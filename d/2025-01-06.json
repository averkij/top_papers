{
    "date": {
        "ru": "6 января",
        "en": "January 6",
        "zh": "1月6日"
    },
    "time_utc": "2025-01-06 05:10",
    "weekday": 0,
    "issue_id": 1506,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2501.01895",
            "title": "EnerVerse: Envisioning Embodied Future Space for Robotics Manipulation",
            "url": "https://huggingface.co/papers/2501.01895",
            "abstract": "We introduce EnerVerse, a comprehensive framework for embodied future space generation specifically designed for robotic manipulation tasks. EnerVerse seamlessly integrates convolutional and bidirectional attention mechanisms for inner-chunk space modeling, ensuring low-level consistency and continuity. Recognizing the inherent redundancy in video data, we propose a sparse memory context combined with a chunkwise unidirectional generative paradigm to enable the generation of infinitely long sequences. To further augment robotic capabilities, we introduce the Free Anchor View (FAV) space, which provides flexible perspectives to enhance observation and analysis. The FAV space mitigates motion modeling ambiguity, removes physical constraints in confined environments, and significantly improves the robot's generalization and adaptability across various tasks and settings. To address the prohibitive costs and labor intensity of acquiring multi-camera observations, we present a data engine pipeline that integrates a generative model with 4D Gaussian Splatting (4DGS). This pipeline leverages the generative model's robust generalization capabilities and the spatial constraints provided by 4DGS, enabling an iterative enhancement of data quality and diversity, thus creating a data flywheel effect that effectively narrows the sim-to-real gap. Finally, our experiments demonstrate that the embodied future space generation prior substantially enhances policy predictive capabilities, resulting in improved overall performance, particularly in long-range robotic manipulation tasks.",
            "score": 9,
            "issue_id": 1506,
            "pub_date": "2025-01-03",
            "pub_date_card": {
                "ru": "3 января",
                "en": "January 3",
                "zh": "1月3日"
            },
            "hash": "bae2a6e63f87958d",
            "authors": [
                "Siyuan Huang",
                "Liliang Chen",
                "Pengfei Zhou",
                "Shengcong Chen",
                "Zhengkai Jiang",
                "Yue Hu",
                "Peng Gao",
                "Hongsheng Li",
                "Maoqing Yao",
                "Guanghui Ren"
            ],
            "affiliations": [
                "AgiBot",
                "CUHK",
                "FDU",
                "HIT",
                "HKUST",
                "SJTU",
                "Shanghai AI Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.01895.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#data",
                    "#robotics"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "EnerVerse: Революция в пространственном моделировании для роботов-манипуляторов",
                    "desc": "EnerVerse - это комплексная система для генерации пространства будущего в задачах роботизированной манипуляции. Она использует сверточные механизмы и двунаправленное внимание для моделирования внутренних фрагментов пространства, обеспечивая согласованность на низком уровне. Система вводит пространство Free Anchor View для гибких перспектив наблюдения и анализа, улучшая обобщение и адаптивность робота. EnerVerse также включает конвейер данных, интегрирующий генеративную модель с 4D Gaussian Splatting для сужения разрыва между симуляцией и реальностью."
                },
                "en": {
                    "title": "Empowering Robots with EnerVerse: A New Era in Space Generation and Manipulation",
                    "desc": "EnerVerse is a new framework designed to help robots better understand and manipulate their environments. It uses advanced techniques like convolutional and bidirectional attention mechanisms to create a consistent model of space. By recognizing that video data often has unnecessary information, EnerVerse employs a sparse memory context to generate long sequences efficiently. Additionally, the Free Anchor View (FAV) space allows robots to observe from different angles, improving their ability to adapt and perform tasks in various settings."
                },
                "zh": {
                    "title": "EnerVerse：提升机器人操作的未来空间生成框架",
                    "desc": "本文介绍了EnerVerse，这是一个专为机器人操作任务设计的未来空间生成框架。EnerVerse结合了卷积和双向注意机制，以确保内部空间建模的一致性和连续性。我们提出了一种稀疏记忆上下文和单向生成范式的结合，能够生成无限长的序列，从而提高机器人的能力。通过引入自由锚视图空间（FAV），我们增强了观察和分析的灵活性，显著改善了机器人在各种任务和环境中的泛化能力和适应性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.01957",
            "title": "VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction",
            "url": "https://huggingface.co/papers/2501.01957",
            "abstract": "Recent Multimodal Large Language Models (MLLMs) have typically focused on integrating visual and textual modalities, with less emphasis placed on the role of speech in enhancing interaction. However, speech plays a crucial role in multimodal dialogue systems, and implementing high-performance in both vision and speech tasks remains a significant challenge due to the fundamental modality differences. In this paper, we propose a carefully designed multi-stage training methodology that progressively trains LLM to understand both visual and speech information, ultimately enabling fluent vision and speech interaction. Our approach not only preserves strong vision-language capacity, but also enables efficient speech-to-speech dialogue capabilities without separate ASR and TTS modules, significantly accelerating multimodal end-to-end response speed. By comparing our method against state-of-the-art counterparts across benchmarks for image, video, and speech tasks, we demonstrate that our model is equipped with both strong visual and speech capabilities, making near real-time vision and speech interaction.",
            "score": 4,
            "issue_id": 1506,
            "pub_date": "2025-01-03",
            "pub_date_card": {
                "ru": "3 января",
                "en": "January 3",
                "zh": "1月3日"
            },
            "hash": "b6690c7efedf5a39",
            "authors": [
                "Chaoyou Fu",
                "Haojia Lin",
                "Xiong Wang",
                "Yi-Fan Zhang",
                "Yunhang Shen",
                "Xiaoyu Liu",
                "Yangze Li",
                "Zuwei Long",
                "Heting Gao",
                "Ke Li",
                "Xiawu Zheng",
                "Rongrong Ji",
                "Xing Sun",
                "Caifeng Shan",
                "Ran He"
            ],
            "affiliations": [
                "CASIA",
                "NJU",
                "Tencent Youtu Lab",
                "XMU"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.01957.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#cv",
                    "#multimodal",
                    "#benchmark",
                    "#audio"
                ],
                "emoji": "🗣️",
                "ru": {
                    "title": "Революция в мультимодальном взаимодействии: речь и зрение в одной модели",
                    "desc": "В статье представлена новая методология обучения мультимодальных языковых моделей, объединяющая визуальную и речевую модальности. Авторы предлагают поэтапный подход к обучению, который позволяет модели эффективно понимать как визуальную, так и речевую информацию. Модель демонстрирует высокую производительность в задачах обработки изображений, видео и речи, превосходя современные аналоги. Этот подход обеспечивает возможность ведения диалога с использованием речи и изображений в режиме, близком к реальному времени."
                },
                "en": {
                    "title": "Enhancing Multimodal Interaction with Speech and Vision Integration",
                    "desc": "This paper introduces a novel training methodology for Multimodal Large Language Models (MLLMs) that enhances their ability to process both visual and speech data. The proposed multi-stage training approach allows the model to progressively learn and integrate information from images, videos, and spoken language, facilitating seamless interaction. By eliminating the need for separate Automatic Speech Recognition (ASR) and Text-to-Speech (TTS) modules, the model achieves faster response times in multimodal dialogues. Experimental results show that this method not only maintains strong vision-language performance but also excels in speech tasks, enabling near real-time interactions."
                },
                "zh": {
                    "title": "实现流畅的视觉与语音交互",
                    "desc": "最近的多模态大型语言模型（MLLMs）主要集中在视觉和文本的整合上，而对语音在增强交互中的作用关注较少。然而，语音在多模态对话系统中起着至关重要的作用，如何在视觉和语音任务中实现高性能仍然是一个重大挑战。本文提出了一种精心设计的多阶段训练方法，逐步训练大型语言模型理解视觉和语音信息，从而实现流畅的视觉和语音交互。我们的方法不仅保持了强大的视觉-语言能力，还实现了高效的语音对话能力，显著加快了多模态端到端的响应速度。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.01904",
            "title": "Virgo: A Preliminary Exploration on Reproducing o1-like MLLM",
            "url": "https://huggingface.co/papers/2501.01904",
            "abstract": "Recently, slow-thinking reasoning systems, built upon large language models (LLMs), have garnered widespread attention by scaling the thinking time during inference. There is also growing interest in adapting this capability to multimodal large language models (MLLMs). Given that MLLMs handle more complex data semantics across different modalities, it is intuitively more challenging to implement multimodal slow-thinking systems.   To address this issue, in this paper, we explore a straightforward approach by fine-tuning a capable MLLM with a small amount of textual long-form thought data, resulting in a multimodal slow-thinking system, Virgo (Visual reasoning with long thought). We find that these long-form reasoning processes, expressed in natural language, can be effectively transferred to MLLMs. Moreover, it seems that such textual reasoning data can be even more effective than visual reasoning data in eliciting the slow-thinking capacities of MLLMs. While this work is preliminary, it demonstrates that slow-thinking capacities are fundamentally associated with the language model component, which can be transferred across modalities or domains. This finding can be leveraged to guide the development of more powerful slow-thinking reasoning systems. We release our resources at https://github.com/RUCAIBox/Virgo.",
            "score": 2,
            "issue_id": 1505,
            "pub_date": "2025-01-03",
            "pub_date_card": {
                "ru": "3 января",
                "en": "January 3",
                "zh": "1月3日"
            },
            "hash": "576423a20b419d0f",
            "authors": [
                "Yifan Du",
                "Zikang Liu",
                "Yifan Li",
                "Wayne Xin Zhao",
                "Yuqi Huo",
                "Bingning Wang",
                "Weipeng Chen",
                "Zheng Liu",
                "Zhongyuan Wang",
                "Ji-Rong Wen"
            ],
            "affiliations": [
                "BAAI",
                "Baichuan AI",
                "Gaoling School of Artificial Intelligence, Renmin University of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.01904.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#multimodal",
                    "#transfer_learning",
                    "#training"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Обучение мультимодальных ИИ длительным рассуждениям через текст",
                    "desc": "Статья описывает исследование в области мультимодальных больших языковых моделей (MLLM) и их способности к медленному мышлению. Авторы предлагают метод Virgo, который позволяет обучить MLLM длительным рассуждениям с помощью небольшого количества текстовых данных. Результаты показывают, что текстовые данные для обучения рассуждениям могут быть даже эффективнее визуальных. Это исследование демонстрирует, что способности к медленному мышлению в основном связаны с языковым компонентом модели и могут переноситься между модальностями."
                },
                "en": {
                    "title": "Unlocking Slow-Thinking in Multimodal Models with Textual Reasoning",
                    "desc": "This paper discusses the development of a multimodal slow-thinking reasoning system called Virgo, which is based on fine-tuning a multimodal large language model (MLLM) using long-form textual reasoning data. The authors found that incorporating long-form reasoning in natural language significantly enhances the slow-thinking capabilities of MLLMs, even more so than using visual reasoning data. This suggests that the slow-thinking abilities are closely linked to the language model aspect, allowing for effective transfer across different data modalities. The research indicates a promising direction for creating advanced reasoning systems that can handle complex data semantics."
                },
                "zh": {
                    "title": "多模态慢思维推理的探索",
                    "desc": "最近，基于大型语言模型（LLMs）的慢思维推理系统引起了广泛关注，尤其是在推理过程中延长思考时间的能力。本文探讨了如何将这种能力应用于多模态大型语言模型（MLLMs），尽管处理不同模态的复杂数据语义更具挑战性。我们通过微调一个强大的MLLM，使用少量的长文本思维数据，成功构建了一个多模态慢思维系统，命名为Virgo（视觉推理与长思维）。研究表明，长文本推理过程可以有效转移到MLLMs，并且这种文本推理数据在激发MLLMs的慢思维能力方面，似乎比视觉推理数据更有效。"
                }
            }
        }
    ],
    "link_prev": "2025-01-03.html",
    "link_next": "2025-01-07.html",
    "link_month": "2025-01.html",
    "short_date_prev": {
        "ru": "03.01",
        "en": "01/03",
        "zh": "1月3日"
    },
    "short_date_next": {
        "ru": "07.01",
        "en": "01/07",
        "zh": "1月7日"
    },
    "categories": {
        "#dataset": 0,
        "#data": 1,
        "#benchmark": 1,
        "#agents": 0,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 1,
        "#audio": 1,
        "#video": 0,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 2,
        "#robotics": 1,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 0,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章介绍了一种新的多模态教科书语料库，用于视觉-语言模型（VLMs）的预训练。与现有的图像-文本对数据相比，这种语料库从网络上的教学视频中提取信息，提供更丰富的基础知识和更好的图像-文本对齐。研究人员使用大语言模型（LLM）提出的分类法系统地收集教学视频，并逐步提取和精炼视觉、音频和文本知识。实验表明，这种视频为中心的教科书在知识和推理密集型任务中表现出色。代码可在GitHub上找到。",
        "title": "2.5 Years in Class: A Multimodal Textbook for Vision-Language Pretraining",
        "pinyin": "这篇文章介绍了一种新的多模态教科书语料库，用于视觉-语言模型（VLMs）的预训练。与现有的图像-文本对数据相比，这种语料库从网络上的教学视频中提取信息，提供更丰富的基础知识和更好的图像-文本对齐。研究人员使用大语言模型（LLM）提出的分类法系统地收集教学视频，并逐步提取和精炼视觉、音频和文本知识。实验表明，这种视频为中心的教科书在知识和推理密集型任务中表现出色。代码可在GitHub上找到。\n\nZhè piān wénzhāng jièshào le yī zhǒng xīn de duō mó tài jiàokēshū yǔliào kù, yòngyú shìjué-yǔyán móxíng (VLMs) de yùxùnliàn. Yǔ xiàn yǒu de túxiàng-wénběn duì shùjù xiāngbǐ, zhè zhǒng yǔliào kù cóng wǎngluò shàng de jiàoxué shìpín zhōng tīqǔ xìnxī, tígōng gèng fēngfù de jīchǔ zhīshi hé gèng hǎo de túxiàng-wénběn duìqí. Yánjiū rényuán shǐyòng dà yǔyán móxíng (LLM) tíchū de fēnlèi fǎ xìtǒng de shōu jí jiàoxué shìpín, bìng zhúbù tīqǔ hé jīngliàn shìjué, yīnpiàn hé wénběn zhīshi. Shíyàn biǎomíng, zhè zhǒng shìpín wéi zhōngxīn de jiàokēshū zài zhīshi hé tuīlǐ mìjī xíng rènwù zhōng biǎoxiàn chūsè. Dàimǎ kě zài GitHub shàng zhǎo dào.",
        "vocab": "[\n    {\"word\": \"多模态\", \"pinyin\": \"duō mó tài\", \"trans\": \"multimodal\"},\n    {\"word\": \"语料库\", \"pinyin\": \"yǔ liào kù\", \"trans\": \"corpus\"},\n    {\"word\": \"预训练\", \"pinyin\": \"yù xùn liàn\", \"trans\": \"pre-training\"},\n    {\"word\": \"视觉-语言模型\", \"pinyin\": \"shì jué yǔ yán mó xíng\", \"trans\": \"vision-language models\"},\n    {\"word\": \"图像-文本对\", \"pinyin\": \"tú xiàng wén běn duì\", \"trans\": \"image-text pairs\"},\n    {\"word\": \"提取\", \"pinyin\": \"tí qū\", \"trans\": \"extract\"},\n    {\"word\": \"基础知识\", \"pinyin\": \"jī chǔ zhī shi\", \"trans\": \"foundational knowledge\"},\n    {\"word\": \"对齐\", \"pinyin\": \"duì qí\", \"trans\": \"alignment\"},\n    {\"word\": \"大语言模型\", \"pinyin\": \"dà yǔ yán mó xíng\", \"trans\": \"large language model\"},\n    {\"word\": \"分类法\", \"pinyin\": \"fēn lèi fǎ\", \"trans\": \"classification method\"},\n    {\"word\": \"系统地\", \"pinyin\": \"xì tǒng de\", \"trans\": \"systematically\"},\n    {\"word\": \"收集\", \"pinyin\": \"shōu jí\", \"trans\": \"collect\"},\n    {\"word\": \"教学视频\", \"pinyin\": \"jiào xué shì pín\", \"trans\": \"educational videos\"},\n    {\"word\": \"逐步\", \"pinyin\": \"zhú bù\", \"trans\": \"step-by-step\"},\n    {\"word\": \"精炼\", \"pinyin\": \"jīng liàn\", \"trans\": \"refine\"},\n    {\"word\": \"视觉\", \"pinyin\": \"shì jué\", \"trans\": \"visual\"},\n    {\"word\": \"音频\", \"pinyin\": \"yīn pín\", \"trans\": \"audio\"},\n    {\"word\": \"文本\", \"pinyin\": \"wén běn\", \"trans\": \"text\"},\n    {\"word\": \"知识\", \"pinyin\": \"zhī shi\", \"trans\": \"knowledge\"},\n    {\"word\": \"推理\", \"pinyin\": \"tuī lǐ\", \"trans\": \"reasoning\"},\n    {\"word\": \"密集型\", \"pinyin\": \"mì jí xíng\", \"trans\": \"intensive\"},\n    {\"word\": \"任务\", \"pinyin\": \"rèn wu\", \"trans\": \"task\"},\n    {\"word\": \"表现\", \"pinyin\": \"biǎo xiàn\", \"trans\": \"performance\"},\n    {\"word\": \"出色\", \"pinyin\": \"chū sè\", \"trans\": \"outstanding\"},\n    {\"word\": \"代码\", \"pinyin\": \"dài mǎ\", \"trans\": \"code\"},\n    {\"word\": \"GitHub\", \"pinyin\": \"GitHub\", \"trans\": \"GitHub\"}\n]",
        "trans": "This article introduces a new multimodal textbook corpus for the pre-training of vision-language models (VLMs). Unlike existing image-text pair data, this corpus extracts information from educational videos on the web, providing richer foundational knowledge and better image-text alignment. Researchers systematically collected educational videos using a classification scheme proposed by large language models (LLMs) and progressively extracted and refined visual, audio, and textual knowledge. Experiments demonstrate that this video-centric textbook performs excellently in knowledge and reasoning-intensive tasks. The code can be found on GitHub.",
        "update_ts": "2025-01-05 12:39"
    }
}