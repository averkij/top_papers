{
    "date": {
        "ru": "6 января",
        "en": "January 6",
        "zh": "1月6日"
    },
    "time_utc": "2025-01-06 10:11",
    "weekday": 0,
    "issue_id": 1511,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2501.01895",
            "title": "EnerVerse: Envisioning Embodied Future Space for Robotics Manipulation",
            "url": "https://huggingface.co/papers/2501.01895",
            "abstract": "We introduce EnerVerse, a comprehensive framework for embodied future space generation specifically designed for robotic manipulation tasks. EnerVerse seamlessly integrates convolutional and bidirectional attention mechanisms for inner-chunk space modeling, ensuring low-level consistency and continuity. Recognizing the inherent redundancy in video data, we propose a sparse memory context combined with a chunkwise unidirectional generative paradigm to enable the generation of infinitely long sequences. To further augment robotic capabilities, we introduce the Free Anchor View (FAV) space, which provides flexible perspectives to enhance observation and analysis. The FAV space mitigates motion modeling ambiguity, removes physical constraints in confined environments, and significantly improves the robot's generalization and adaptability across various tasks and settings. To address the prohibitive costs and labor intensity of acquiring multi-camera observations, we present a data engine pipeline that integrates a generative model with 4D Gaussian Splatting (4DGS). This pipeline leverages the generative model's robust generalization capabilities and the spatial constraints provided by 4DGS, enabling an iterative enhancement of data quality and diversity, thus creating a data flywheel effect that effectively narrows the sim-to-real gap. Finally, our experiments demonstrate that the embodied future space generation prior substantially enhances policy predictive capabilities, resulting in improved overall performance, particularly in long-range robotic manipulation tasks.",
            "score": 31,
            "issue_id": 1506,
            "pub_date": "2025-01-03",
            "pub_date_card": {
                "ru": "3 января",
                "en": "January 3",
                "zh": "1月3日"
            },
            "hash": "bae2a6e63f87958d",
            "authors": [
                "Siyuan Huang",
                "Liliang Chen",
                "Pengfei Zhou",
                "Shengcong Chen",
                "Zhengkai Jiang",
                "Yue Hu",
                "Peng Gao",
                "Hongsheng Li",
                "Maoqing Yao",
                "Guanghui Ren"
            ],
            "affiliations": [
                "AgiBot",
                "CUHK",
                "FDU",
                "HIT",
                "HKUST",
                "SJTU",
                "Shanghai AI Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.01895.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#data",
                    "#robotics"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "EnerVerse: Революция в пространственном моделировании для роботов-манипуляторов",
                    "desc": "EnerVerse - это комплексная система для генерации пространства будущего в задачах роботизированной манипуляции. Она использует сверточные механизмы и двунаправленное внимание для моделирования внутренних фрагментов пространства, обеспечивая согласованность на низком уровне. Система вводит пространство Free Anchor View для гибких перспектив наблюдения и анализа, улучшая обобщение и адаптивность робота. EnerVerse также включает конвейер данных, интегрирующий генеративную модель с 4D Gaussian Splatting для сужения разрыва между симуляцией и реальностью."
                },
                "en": {
                    "title": "Empowering Robots with EnerVerse: A New Era in Space Generation and Manipulation",
                    "desc": "EnerVerse is a new framework designed to help robots better understand and manipulate their environments. It uses advanced techniques like convolutional and bidirectional attention mechanisms to create a consistent model of space. By recognizing that video data often has unnecessary information, EnerVerse employs a sparse memory context to generate long sequences efficiently. Additionally, the Free Anchor View (FAV) space allows robots to observe from different angles, improving their ability to adapt and perform tasks in various settings."
                },
                "zh": {
                    "title": "EnerVerse：提升机器人操作的未来空间生成框架",
                    "desc": "本文介绍了EnerVerse，这是一个专为机器人操作任务设计的未来空间生成框架。EnerVerse结合了卷积和双向注意机制，以确保内部空间建模的一致性和连续性。我们提出了一种稀疏记忆上下文和单向生成范式的结合，能够生成无限长的序列，从而提高机器人的能力。通过引入自由锚视图空间（FAV），我们增强了观察和分析的灵活性，显著改善了机器人在各种任务和环境中的泛化能力和适应性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.01957",
            "title": "VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction",
            "url": "https://huggingface.co/papers/2501.01957",
            "abstract": "Recent Multimodal Large Language Models (MLLMs) have typically focused on integrating visual and textual modalities, with less emphasis placed on the role of speech in enhancing interaction. However, speech plays a crucial role in multimodal dialogue systems, and implementing high-performance in both vision and speech tasks remains a significant challenge due to the fundamental modality differences. In this paper, we propose a carefully designed multi-stage training methodology that progressively trains LLM to understand both visual and speech information, ultimately enabling fluent vision and speech interaction. Our approach not only preserves strong vision-language capacity, but also enables efficient speech-to-speech dialogue capabilities without separate ASR and TTS modules, significantly accelerating multimodal end-to-end response speed. By comparing our method against state-of-the-art counterparts across benchmarks for image, video, and speech tasks, we demonstrate that our model is equipped with both strong visual and speech capabilities, making near real-time vision and speech interaction.",
            "score": 9,
            "issue_id": 1506,
            "pub_date": "2025-01-03",
            "pub_date_card": {
                "ru": "3 января",
                "en": "January 3",
                "zh": "1月3日"
            },
            "hash": "b6690c7efedf5a39",
            "authors": [
                "Chaoyou Fu",
                "Haojia Lin",
                "Xiong Wang",
                "Yi-Fan Zhang",
                "Yunhang Shen",
                "Xiaoyu Liu",
                "Yangze Li",
                "Zuwei Long",
                "Heting Gao",
                "Ke Li",
                "Xiawu Zheng",
                "Rongrong Ji",
                "Xing Sun",
                "Caifeng Shan",
                "Ran He"
            ],
            "affiliations": [
                "CASIA",
                "NJU",
                "Tencent Youtu Lab",
                "XMU"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.01957.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#cv",
                    "#multimodal",
                    "#benchmark",
                    "#audio"
                ],
                "emoji": "🗣️",
                "ru": {
                    "title": "Революция в мультимодальном взаимодействии: речь и зрение в одной модели",
                    "desc": "В статье представлена новая методология обучения мультимодальных языковых моделей, объединяющая визуальную и речевую модальности. Авторы предлагают поэтапный подход к обучению, который позволяет модели эффективно понимать как визуальную, так и речевую информацию. Модель демонстрирует высокую производительность в задачах обработки изображений, видео и речи, превосходя современные аналоги. Этот подход обеспечивает возможность ведения диалога с использованием речи и изображений в режиме, близком к реальному времени."
                },
                "en": {
                    "title": "Enhancing Multimodal Interaction with Speech and Vision Integration",
                    "desc": "This paper introduces a novel training methodology for Multimodal Large Language Models (MLLMs) that enhances their ability to process both visual and speech data. The proposed multi-stage training approach allows the model to progressively learn and integrate information from images, videos, and spoken language, facilitating seamless interaction. By eliminating the need for separate Automatic Speech Recognition (ASR) and Text-to-Speech (TTS) modules, the model achieves faster response times in multimodal dialogues. Experimental results show that this method not only maintains strong vision-language performance but also excels in speech tasks, enabling near real-time interactions."
                },
                "zh": {
                    "title": "实现流畅的视觉与语音交互",
                    "desc": "最近的多模态大型语言模型（MLLMs）主要集中在视觉和文本的整合上，而对语音在增强交互中的作用关注较少。然而，语音在多模态对话系统中起着至关重要的作用，如何在视觉和语音任务中实现高性能仍然是一个重大挑战。本文提出了一种精心设计的多阶段训练方法，逐步训练大型语言模型理解视觉和语音信息，从而实现流畅的视觉和语音交互。我们的方法不仅保持了强大的视觉-语言能力，还实现了高效的语音对话能力，显著加快了多模态端到端的响应速度。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.01073",
            "title": "Graph Generative Pre-trained Transformer",
            "url": "https://huggingface.co/papers/2501.01073",
            "abstract": "Graph generation is a critical task in numerous domains, including molecular design and social network analysis, due to its ability to model complex relationships and structured data. While most modern graph generative models utilize adjacency matrix representations, this work revisits an alternative approach that represents graphs as sequences of node set and edge set. We advocate for this approach due to its efficient encoding of graphs and propose a novel representation. Based on this representation, we introduce the Graph Generative Pre-trained Transformer (G2PT), an auto-regressive model that learns graph structures via next-token prediction. To further exploit G2PT's capabilities as a general-purpose foundation model, we explore fine-tuning strategies for two downstream applications: goal-oriented generation and graph property prediction. We conduct extensive experiments across multiple datasets. Results indicate that G2PT achieves superior generative performance on both generic graph and molecule datasets. Furthermore, G2PT exhibits strong adaptability and versatility in downstream tasks from molecular design to property prediction.",
            "score": 5,
            "issue_id": 1508,
            "pub_date": "2025-01-02",
            "pub_date_card": {
                "ru": "2 января",
                "en": "January 2",
                "zh": "1月2日"
            },
            "hash": "596abc88d57e0650",
            "authors": [
                "Xiaohui Chen",
                "Yinkai Wang",
                "Jiaxing He",
                "Yuanqi Du",
                "Soha Hassoun",
                "Xiaolin Xu",
                "Li-Ping Liu"
            ],
            "affiliations": [
                "Cornell University",
                "Northeastern University",
                "Tufts University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.01073.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#optimization",
                    "#training",
                    "#architecture",
                    "#data",
                    "#graphs"
                ],
                "emoji": "🕸️",
                "ru": {
                    "title": "G2PT: Универсальный трансформер для эффективной генерации графов",
                    "desc": "В статье представлена новая модель генерации графов - Graph Generative Pre-trained Transformer (G2PT). G2PT использует альтернативный подход к представлению графов в виде последовательностей множеств узлов и рёбер вместо матриц смежности. Модель обучается предсказывать следующий токен автореgressивным способом. G2PT показывает превосходные результаты в генерации как общих графов, так и молекул, а также демонстрирует хорошую адаптивность к различным задачам."
                },
                "en": {
                    "title": "Revolutionizing Graph Generation with G2PT",
                    "desc": "This paper focuses on improving graph generation, which is important for tasks like designing molecules and analyzing social networks. Instead of using the common adjacency matrix, it proposes a new way to represent graphs as sequences of node and edge sets, making the encoding more efficient. The authors introduce the Graph Generative Pre-trained Transformer (G2PT), an auto-regressive model that learns to generate graph structures by predicting the next token in a sequence. Through various experiments, they demonstrate that G2PT outperforms existing models in generating graphs and is effective in applications like molecular design and predicting graph properties."
                },
                "zh": {
                    "title": "图生成的创新：G2PT模型",
                    "desc": "图生成在许多领域中非常重要，比如分子设计和社交网络分析，因为它能够建模复杂的关系和结构化数据。本文提出了一种新的图表示方法，将图表示为节点集和边集的序列，而不是传统的邻接矩阵。基于这种表示，我们引入了图生成预训练变换器（G2PT），这是一种通过下一个标记预测学习图结构的自回归模型。实验结果表明，G2PT在通用图和分子数据集上表现出色，并且在分子设计和属性预测等下游任务中具有很强的适应性和多功能性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.21059",
            "title": "VisionReward: Fine-Grained Multi-Dimensional Human Preference Learning for Image and Video Generation",
            "url": "https://huggingface.co/papers/2412.21059",
            "abstract": "We present a general strategy to aligning visual generation models -- both image and video generation -- with human preference. To start with, we build VisionReward -- a fine-grained and multi-dimensional reward model. We decompose human preferences in images and videos into multiple dimensions, each represented by a series of judgment questions, linearly weighted and summed to an interpretable and accurate score. To address the challenges of video quality assessment, we systematically analyze various dynamic features of videos, which helps VisionReward surpass VideoScore by 17.2% and achieve top performance for video preference prediction. Based on VisionReward, we develop a multi-objective preference learning algorithm that effectively addresses the issue of confounding factors within preference data. Our approach significantly outperforms existing image and video scoring methods on both machine metrics and human evaluation. All code and datasets are provided at https://github.com/THUDM/VisionReward.",
            "score": 4,
            "issue_id": 1510,
            "pub_date": "2025-12-30",
            "pub_date_card": {
                "ru": "30 декабря",
                "en": "December 30",
                "zh": "12月30日"
            },
            "hash": "1f3bb267ffa751d9",
            "authors": [
                "Jiazheng Xu",
                "Yu Huang",
                "Jiale Cheng",
                "Yuanming Yang",
                "Jiajun Xu",
                "Yuan Wang",
                "Wenbo Duan",
                "Shen Yang",
                "Qunlin Jin",
                "Shurun Li",
                "Jiayan Teng",
                "Zhuoyi Yang",
                "Wendi Zheng",
                "Xiao Liu",
                "Ming Ding",
                "Xiaohan Zhang",
                "Xiaotao Gu",
                "Shiyu Huang",
                "Minlie Huang",
                "Jie Tang",
                "Yuxiao Dong"
            ],
            "affiliations": [
                "Tsinghua University",
                "Zhipu AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.21059.jpg",
            "data": {
                "categories": [
                    "#rag",
                    "#training",
                    "#open_source",
                    "#cv",
                    "#video",
                    "#optimization",
                    "#alignment"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "VisionReward: многомерная оценка визуального контента с учетом человеческих предпочтений",
                    "desc": "Исследователи представили стратегию для согласования моделей генерации визуального контента с человеческими предпочтениями. Они разработали VisionReward - многомерную модель вознаграждения, которая декомпозирует предпочтения в изображениях и видео на несколько измерений. Для оценки качества видео были проанализированы различные динамические характеристики, что позволило VisionReward превзойти существующие методы на 17.2%. На основе VisionReward был разработан алгоритм многоцелевого обучения предпочтениям, эффективно решающий проблему конфаундинг-факторов в данных о предпочтениях."
                },
                "en": {
                    "title": "Aligning Visual Generation with Human Preferences",
                    "desc": "This paper introduces a method for aligning visual generation models, such as those for images and videos, with human preferences. The authors create a reward model called VisionReward, which breaks down human preferences into multiple dimensions assessed through specific judgment questions. They enhance video quality assessment by analyzing dynamic features, leading to a 17.2% improvement over previous methods. Additionally, a multi-objective preference learning algorithm is developed to manage confounding factors in preference data, resulting in superior performance compared to existing scoring methods."
                },
                "zh": {
                    "title": "视觉生成模型与人类偏好的完美对齐",
                    "desc": "本文提出了一种通用策略，用于将视觉生成模型（包括图像和视频生成）与人类偏好对齐。我们构建了VisionReward，这是一个细粒度和多维度的奖励模型，能够将人类对图像和视频的偏好分解为多个维度。通过分析视频的动态特征，VisionReward在视频偏好预测中超越了现有方法，提升了17.2%的性能。基于VisionReward，我们开发了一种多目标偏好学习算法，有效解决了偏好数据中的混淆因素问题。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.01904",
            "title": "Virgo: A Preliminary Exploration on Reproducing o1-like MLLM",
            "url": "https://huggingface.co/papers/2501.01904",
            "abstract": "Recently, slow-thinking reasoning systems, built upon large language models (LLMs), have garnered widespread attention by scaling the thinking time during inference. There is also growing interest in adapting this capability to multimodal large language models (MLLMs). Given that MLLMs handle more complex data semantics across different modalities, it is intuitively more challenging to implement multimodal slow-thinking systems.   To address this issue, in this paper, we explore a straightforward approach by fine-tuning a capable MLLM with a small amount of textual long-form thought data, resulting in a multimodal slow-thinking system, Virgo (Visual reasoning with long thought). We find that these long-form reasoning processes, expressed in natural language, can be effectively transferred to MLLMs. Moreover, it seems that such textual reasoning data can be even more effective than visual reasoning data in eliciting the slow-thinking capacities of MLLMs. While this work is preliminary, it demonstrates that slow-thinking capacities are fundamentally associated with the language model component, which can be transferred across modalities or domains. This finding can be leveraged to guide the development of more powerful slow-thinking reasoning systems. We release our resources at https://github.com/RUCAIBox/Virgo.",
            "score": 4,
            "issue_id": 1505,
            "pub_date": "2025-01-03",
            "pub_date_card": {
                "ru": "3 января",
                "en": "January 3",
                "zh": "1月3日"
            },
            "hash": "576423a20b419d0f",
            "authors": [
                "Yifan Du",
                "Zikang Liu",
                "Yifan Li",
                "Wayne Xin Zhao",
                "Yuqi Huo",
                "Bingning Wang",
                "Weipeng Chen",
                "Zheng Liu",
                "Zhongyuan Wang",
                "Ji-Rong Wen"
            ],
            "affiliations": [
                "BAAI",
                "Baichuan AI",
                "Gaoling School of Artificial Intelligence, Renmin University of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.01904.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#multimodal",
                    "#transfer_learning",
                    "#training"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Обучение мультимодальных ИИ длительным рассуждениям через текст",
                    "desc": "Статья описывает исследование в области мультимодальных больших языковых моделей (MLLM) и их способности к медленному мышлению. Авторы предлагают метод Virgo, который позволяет обучить MLLM длительным рассуждениям с помощью небольшого количества текстовых данных. Результаты показывают, что текстовые данные для обучения рассуждениям могут быть даже эффективнее визуальных. Это исследование демонстрирует, что способности к медленному мышлению в основном связаны с языковым компонентом модели и могут переноситься между модальностями."
                },
                "en": {
                    "title": "Unlocking Slow-Thinking in Multimodal Models with Textual Reasoning",
                    "desc": "This paper discusses the development of a multimodal slow-thinking reasoning system called Virgo, which is based on fine-tuning a multimodal large language model (MLLM) using long-form textual reasoning data. The authors found that incorporating long-form reasoning in natural language significantly enhances the slow-thinking capabilities of MLLMs, even more so than using visual reasoning data. This suggests that the slow-thinking abilities are closely linked to the language model aspect, allowing for effective transfer across different data modalities. The research indicates a promising direction for creating advanced reasoning systems that can handle complex data semantics."
                },
                "zh": {
                    "title": "多模态慢思维推理的探索",
                    "desc": "最近，基于大型语言模型（LLMs）的慢思维推理系统引起了广泛关注，尤其是在推理过程中延长思考时间的能力。本文探讨了如何将这种能力应用于多模态大型语言模型（MLLMs），尽管处理不同模态的复杂数据语义更具挑战性。我们通过微调一个强大的MLLM，使用少量的长文本思维数据，成功构建了一个多模态慢思维系统，命名为Virgo（视觉推理与长思维）。研究表明，长文本推理过程可以有效转移到MLLMs，并且这种文本推理数据在激发MLLMs的慢思维能力方面，似乎比视觉推理数据更有效。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.01540",
            "title": "BoxingGym: Benchmarking Progress in Automated Experimental Design and Model Discovery",
            "url": "https://huggingface.co/papers/2501.01540",
            "abstract": "Understanding the world and explaining it with scientific theories is a central aspiration of artificial intelligence research. Proposing theories, designing experiments to test them, and then revising them based on data are fundamental to scientific discovery. Despite the significant promise of LLM-based scientific agents, no benchmarks systematically test LLM's ability to propose scientific models, collect experimental data, and revise them in light of new data. We introduce BoxingGym, a benchmark with 10 environments for systematically evaluating both experimental design (e.g. collecting data to test a scientific theory) and model discovery (e.g. proposing and revising scientific theories). To enable tractable and quantitative evaluation, we implement each environment as a generative probabilistic model with which a scientific agent can run interactive experiments. These probabilistic models are drawn from various real-world scientific domains ranging from psychology to ecology. To quantitatively evaluate a scientific agent's ability to collect informative experimental data, we compute the expected information gain (EIG), an information-theoretic quantity which measures how much an experiment reduces uncertainty about the parameters of a generative model. A good scientific theory is a concise and predictive explanation. Therefore, to quantitatively evaluate model discovery, we ask a scientific agent to explain their model and then assess whether this explanation enables another scientific agent to make reliable predictions about this environment. In addition to this explanation-based evaluation, we compute standard model evaluation metrics such as prediction errors. We find that current LLMs, such as GPT-4o, struggle with both experimental design and model discovery. We find that augmenting the LLM-based agent with an explicit statistical model does not reliably improve these results.",
            "score": 2,
            "issue_id": 1510,
            "pub_date": "2025-01-02",
            "pub_date_card": {
                "ru": "2 января",
                "en": "January 2",
                "zh": "1月2日"
            },
            "hash": "0f853b1681ef29b5",
            "authors": [
                "Kanishk Gandhi",
                "Michael Y. Li",
                "Lyle Goodyear",
                "Louise Li",
                "Aditi Bhaskar",
                "Mohammed Zaman",
                "Noah D. Goodman"
            ],
            "affiliations": [
                "Stanford University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.01540.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#data",
                    "#science",
                    "#agents"
                ],
                "emoji": "🧪",
                "ru": {
                    "title": "BoxingGym: новый вызов для ИИ в научном моделировании",
                    "desc": "Статья представляет новый бенчмарк BoxingGym для оценки способности языковых моделей (LLM) к научному открытию. Бенчмарк включает 10 сред, моделирующих различные научные области, и позволяет тестировать планирование экспериментов и построение теорий. Для оценки качества экспериментов используется ожидаемый прирост информации (EIG), а для оценки теорий - их способность объяснять и предсказывать. Результаты показывают, что современные LLM, включая GPT-4, пока слабо справляются с этими задачами."
                },
                "en": {
                    "title": "BoxingGym: Evaluating LLMs in Scientific Discovery",
                    "desc": "This paper introduces BoxingGym, a benchmark designed to evaluate the capabilities of large language models (LLMs) in scientific discovery tasks. It focuses on two main aspects: experimental design, which involves collecting data to test scientific theories, and model discovery, which includes proposing and revising these theories. The benchmark consists of 10 environments modeled as generative probabilistic models from various scientific fields, allowing for interactive experimentation. The study finds that current LLMs, like GPT-4o, face challenges in both areas, and adding a statistical model does not consistently enhance their performance."
                },
                "zh": {
                    "title": "评估人工智能在科学研究中的能力",
                    "desc": "这篇论文探讨了人工智能在科学研究中的应用，特别是大型语言模型（LLM）在提出科学理论和设计实验方面的能力。作者提出了一个名为BoxingGym的基准测试，包含10个环境，用于系统评估实验设计和模型发现的能力。通过计算期望信息增益（EIG），论文量化了科学代理收集实验数据的有效性，并评估其提出的模型是否能进行可靠预测。研究发现，当前的LLM在实验设计和模型发现方面表现不佳，且简单地增加统计模型并未显著改善结果。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.00874",
            "title": "LUSIFER: Language Universal Space Integration for Enhanced Multilingual Embeddings with Large Language Models",
            "url": "https://huggingface.co/papers/2501.00874",
            "abstract": "Recent advancements in large language models (LLMs) based embedding models have established new state-of-the-art benchmarks for text embedding tasks, particularly in dense vector-based retrieval. However, these models predominantly focus on English, leaving multilingual embedding capabilities largely unexplored. To address this limitation, we present LUSIFER, a novel zero-shot approach that adapts LLM-based embedding models for multilingual tasks without requiring multilingual supervision. LUSIFER's architecture combines a multilingual encoder, serving as a language-universal learner, with an LLM-based embedding model optimized for embedding-specific tasks. These components are seamlessly integrated through a minimal set of trainable parameters that act as a connector, effectively transferring the multilingual encoder's language understanding capabilities to the specialized embedding model. Additionally, to comprehensively evaluate multilingual embedding performance, we introduce a new benchmark encompassing 5 primary embedding tasks, 123 diverse datasets, and coverage across 14 languages. Extensive experimental results demonstrate that LUSIFER significantly enhances the multilingual performance across various embedding tasks, particularly for medium and low-resource languages, without requiring explicit multilingual training data.",
            "score": 2,
            "issue_id": 1507,
            "pub_date": "2025-01-01",
            "pub_date_card": {
                "ru": "1 января",
                "en": "January 1",
                "zh": "1月1日"
            },
            "hash": "5bdfec436923a2a6",
            "authors": [
                "Hieu Man",
                "Nghia Trung Ngo",
                "Viet Dac Lai",
                "Ryan A. Rossi",
                "Franck Dernoncourt",
                "Thien Huu Nguyen"
            ],
            "affiliations": [
                "Adobe Research, USA",
                "Dept. of Computer Science, University of Oregon, OR, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.00874.jpg",
            "data": {
                "categories": [
                    "#transfer_learning",
                    "#architecture",
                    "#benchmark",
                    "#multilingual",
                    "#low_resource"
                ],
                "emoji": "🌍",
                "ru": {
                    "title": "Универсальные многоязычные эмбеддинги без многоязычного обучения",
                    "desc": "LUSIFER - это новый подход к созданию многоязычных эмбеддингов без использования многоязычных обучающих данных. Он объединяет многоязычный энкодер и LLM-модель для эмбеддингов через набор обучаемых параметров. Авторы также представили новый бенчмарк для оценки качества многоязычных эмбеддингов, охватывающий 5 основных задач, 123 датасета и 14 языков. Эксперименты показали, что LUSIFER значительно улучшает многоязычную производительность, особенно для языков с ограниченными ресурсами."
                },
                "en": {
                    "title": "LUSIFER: Bridging Multilingual Gaps in Text Embedding",
                    "desc": "This paper introduces LUSIFER, a new method that enhances large language models (LLMs) for multilingual text embedding tasks. Unlike existing models that mainly focus on English, LUSIFER uses a zero-shot approach to adapt LLMs for multiple languages without needing multilingual training data. It combines a multilingual encoder with an LLM-based embedding model, allowing for effective language understanding and embedding performance. The authors also present a comprehensive benchmark to evaluate LUSIFER's performance across various languages and tasks, showing significant improvements, especially for less-resourced languages."
                },
                "zh": {
                    "title": "LUSIFER：无监督多语言嵌入的新突破",
                    "desc": "最近，大型语言模型（LLMs）在文本嵌入任务中取得了新的突破，尤其是在基于密集向量的检索方面。然而，这些模型主要集中在英语上，导致多语言嵌入能力尚未得到充分探索。为了解决这个问题，我们提出了LUSIFER，这是一种新颖的零样本方法，可以在不需要多语言监督的情况下，将LLM嵌入模型适应于多语言任务。LUSIFER的架构结合了一个多语言编码器和一个针对嵌入特定任务优化的LLM嵌入模型，通过一组最小的可训练参数实现无缝连接，有效地将多语言编码器的语言理解能力转移到专门的嵌入模型上。"
                }
            }
        }
    ],
    "link_prev": "2025-01-03.html",
    "link_next": "2025-01-07.html",
    "link_month": "2025-01.html",
    "short_date_prev": {
        "ru": "03.01",
        "en": "01/03",
        "zh": "1月3日"
    },
    "short_date_next": {
        "ru": "07.01",
        "en": "01/07",
        "zh": "1月7日"
    },
    "categories": {
        "#dataset": 1,
        "#data": 3,
        "#benchmark": 3,
        "#agents": 1,
        "#cv": 2,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 1,
        "#plp": 0,
        "#inference": 0,
        "#3d": 1,
        "#audio": 1,
        "#video": 1,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 4,
        "#robotics": 1,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 2,
        "#graphs": 1,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 2,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 1
    },
    "zh": {
        "text": "This article introduces EnerVerse, a framework for generating future space in robotic tasks. It uses attention mechanisms for consistent space modeling and a memory context for long sequence generation. The FAV space enhances robot observation and adaptability. A data engine with 4DGS improves data quality and diversity. Experiments show it boosts performance in long-range robotic tasks.",
        "title": "EnerVerse: Envisioning Embodied Future Space for Robotics Manipulation",
        "pinyin": "Sure, here is the pinyin transcription for the given text:\n\nZhè wénzhāng jièshào EnerVerse, yīgè kuàngjià yǐnqǐ wèilái kōngjiān zài jīqirén rénwù zhōng. Tā shǐyòng zhùyì jīzhì wèi yuáncháng kōngjiān móxíng hé yīgè jìyì qūjiàn wèi cháng xùliè shēngchéng. FAV kōngjiān zēngqiáng jīqirén guānchá hé shìyìngxìng. Yīgè shùjù yǐnqíng yǒu 4DGS gǎishàn shùjù zhìliàng hé duōyàngxìng. Shíyàn xiǎnshì tā zēngqiáng xiàoguǒ zài chángqī jīqirén rénwù zhōng.\n\nPlease note that the pinyin transcription is based on the pronunciation of the Chinese characters that would be used to translate the English text. The actual translation might vary slightly depending on the context and specific terminology used.",
        "vocab": "[\n    {\"word\": \"framework\", \"pinyin\": \"kuàngjià\", \"trans\": \"框架\"},\n    {\"word\": \"generating\", \"pinyin\": \"shēngchéng\", \"trans\": \"生成\"},\n    {\"word\": \"future\", \"pinyin\": \"wèilái\", \"trans\": \"未来\"},\n    {\"word\": \"space\", \"pinyin\": \"kōngjiān\", \"trans\": \"空间\"},\n    {\"word\": \"robotic\", \"pinyin\": \"jīqirén\", \"trans\": \"机器人\"},\n    {\"word\": \"tasks\", \"pinyin\": \"rènwù\", \"trans\": \"任务\"},\n    {\"word\": \"attention\", \"pinyin\": \"zhùyì\", \"trans\": \"注意\"},\n    {\"word\": \"mechanisms\", \"pinyin\": \"jīzhì\", \"trans\": \"机制\"},\n    {\"word\": \"consistent\", \"pinyin\": \"wúguǒ\", \"trans\": \"一致\"},\n    {\"word\": \"modeling\", \"pinyin\": \"móxíng\", \"trans\": \"建模\"},\n    {\"word\": \"memory\", \"pinyin\": \"jìyì\", \"trans\": \"记忆\"},\n    {\"word\": \"context\", \"pinyin\": \"qǔwén\", \"trans\": \"上下文\"},\n    {\"word\": \"sequence\", \"pinyin\": \"xùliè\", \"trans\": \"序列\"},\n    {\"word\": \"generation\", \"pinyin\": \"shēngchéng\", \"trans\": \"生成\"},\n    {\"word\": \"FAV\", \"pinyin\": \"Fēi-Ēi-Wēi\", \"trans\": \"FAV\"},\n    {\"word\": \"enhances\", \"pinyin\": \"zēngqiáng\", \"trans\": \"增强\"},\n    {\"word\": \"observation\", \"pinyin\": \"guānchá\", \"trans\": \"观察\"},\n    {\"word\": \"adaptability\", \"pinyin\": \"shìyìngxìng\", \"trans\": \"适应性\"},\n    {\"word\": \"engine\", \"pinyin\": \"yǐnqíng\", \"trans\": \"引擎\"},\n    {\"word\": \"4DGS\", \"pinyin\": \"Sì-Dī-Jī-Ēs\", \"trans\": \"4DGS\"},\n    {\"word\": \"improves\", \"pinyin\": \"gǎishàn\", \"trans\": \"改善\"},\n    {\"word\": \"quality\", \"pinyin\": \"zhìliàng\", \"trans\": \"质量\"},\n    {\"word\": \"diversity\", \"pinyin\": \"duōyàngxìng\", \"trans\": \"多样性\"},\n    {\"word\": \"experiments\", \"pinyin\": \"shíyàn\", \"trans\": \"实验\"},\n    {\"word\": \"show\", \"pinyin\": \"xiǎnshì\", \"trans\": \"显示\"},\n    {\"word\": \"boosts\", \"pinyin\": \"zēngqiáng\", \"trans\": \"增强\"},\n    {\"word\": \"performance\", \"pinyin\": \"biǎoxiàn\", \"trans\": \"表现\"},\n    {\"word\": \"long-range\", \"pinyin\": \"chángyuǎn\", \"trans\": \"长远\"}\n]",
        "trans": "This article introduces EnerVerse, a framework designed to generate future space in robotic tasks. It employs attention mechanisms to ensure consistent space modeling and utilizes a memory context for generating long sequences. The FAV space enhances robot observation capabilities and adaptability. Additionally, a data engine equipped with 4DGS improves the quality and diversity of data. Experiments demonstrate that it significantly boosts performance in long-range robotic tasks.",
        "update_ts": "2025-01-06 09:11"
    }
}