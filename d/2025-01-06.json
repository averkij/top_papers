{
    "date": {
        "ru": "6 ÑĞ½Ğ²Ğ°Ñ€Ñ",
        "en": "January 6",
        "zh": "1æœˆ6æ—¥"
    },
    "time_utc": "2025-01-06 10:11",
    "weekday": 0,
    "issue_id": 1511,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2501.01895",
            "title": "EnerVerse: Envisioning Embodied Future Space for Robotics Manipulation",
            "url": "https://huggingface.co/papers/2501.01895",
            "abstract": "We introduce EnerVerse, a comprehensive framework for embodied future space generation specifically designed for robotic manipulation tasks. EnerVerse seamlessly integrates convolutional and bidirectional attention mechanisms for inner-chunk space modeling, ensuring low-level consistency and continuity. Recognizing the inherent redundancy in video data, we propose a sparse memory context combined with a chunkwise unidirectional generative paradigm to enable the generation of infinitely long sequences. To further augment robotic capabilities, we introduce the Free Anchor View (FAV) space, which provides flexible perspectives to enhance observation and analysis. The FAV space mitigates motion modeling ambiguity, removes physical constraints in confined environments, and significantly improves the robot's generalization and adaptability across various tasks and settings. To address the prohibitive costs and labor intensity of acquiring multi-camera observations, we present a data engine pipeline that integrates a generative model with 4D Gaussian Splatting (4DGS). This pipeline leverages the generative model's robust generalization capabilities and the spatial constraints provided by 4DGS, enabling an iterative enhancement of data quality and diversity, thus creating a data flywheel effect that effectively narrows the sim-to-real gap. Finally, our experiments demonstrate that the embodied future space generation prior substantially enhances policy predictive capabilities, resulting in improved overall performance, particularly in long-range robotic manipulation tasks.",
            "score": 31,
            "issue_id": 1506,
            "pub_date": "2025-01-03",
            "pub_date_card": {
                "ru": "3 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 3",
                "zh": "1æœˆ3æ—¥"
            },
            "hash": "bae2a6e63f87958d",
            "authors": [
                "Siyuan Huang",
                "Liliang Chen",
                "Pengfei Zhou",
                "Shengcong Chen",
                "Zhengkai Jiang",
                "Yue Hu",
                "Peng Gao",
                "Hongsheng Li",
                "Maoqing Yao",
                "Guanghui Ren"
            ],
            "affiliations": [
                "AgiBot",
                "CUHK",
                "FDU",
                "HIT",
                "HKUST",
                "SJTU",
                "Shanghai AI Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.01895.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#data",
                    "#robotics"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "EnerVerse: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ²-Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ¾Ğ²",
                    "desc": "EnerVerse - ÑÑ‚Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ±ÑƒĞ´ÑƒÑ‰ĞµĞ³Ğ¾ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ²ĞµÑ€Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ Ğ¸ Ğ´Ğ²ÑƒĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ñ… Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¼ ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Free Anchor View Ğ´Ğ»Ñ Ğ³Ğ¸Ğ±ĞºĞ¸Ñ… Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ² Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°, ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°. EnerVerse Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ 4D Gaussian Splatting Ğ´Ğ»Ñ ÑÑƒĞ¶ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ²Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸ĞµĞ¹ Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ."
                },
                "en": {
                    "title": "Empowering Robots with EnerVerse: A New Era in Space Generation and Manipulation",
                    "desc": "EnerVerse is a new framework designed to help robots better understand and manipulate their environments. It uses advanced techniques like convolutional and bidirectional attention mechanisms to create a consistent model of space. By recognizing that video data often has unnecessary information, EnerVerse employs a sparse memory context to generate long sequences efficiently. Additionally, the Free Anchor View (FAV) space allows robots to observe from different angles, improving their ability to adapt and perform tasks in various settings."
                },
                "zh": {
                    "title": "EnerVerseï¼šæå‡æœºå™¨äººæ“ä½œçš„æœªæ¥ç©ºé—´ç”Ÿæˆæ¡†æ¶",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†EnerVerseï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºæœºå™¨äººæ“ä½œä»»åŠ¡è®¾è®¡çš„æœªæ¥ç©ºé—´ç”Ÿæˆæ¡†æ¶ã€‚EnerVerseç»“åˆäº†å·ç§¯å’ŒåŒå‘æ³¨æ„æœºåˆ¶ï¼Œä»¥ç¡®ä¿å†…éƒ¨ç©ºé—´å»ºæ¨¡çš„ä¸€è‡´æ€§å’Œè¿ç»­æ€§ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç¨€ç–è®°å¿†ä¸Šä¸‹æ–‡å’Œå•å‘ç”ŸæˆèŒƒå¼çš„ç»“åˆï¼Œèƒ½å¤Ÿç”Ÿæˆæ— é™é•¿çš„åºåˆ—ï¼Œä»è€Œæé«˜æœºå™¨äººçš„èƒ½åŠ›ã€‚é€šè¿‡å¼•å…¥è‡ªç”±é”šè§†å›¾ç©ºé—´ï¼ˆFAVï¼‰ï¼Œæˆ‘ä»¬å¢å¼ºäº†è§‚å¯Ÿå’Œåˆ†æçš„çµæ´»æ€§ï¼Œæ˜¾è‘—æ”¹å–„äº†æœºå™¨äººåœ¨å„ç§ä»»åŠ¡å’Œç¯å¢ƒä¸­çš„æ³›åŒ–èƒ½åŠ›å’Œé€‚åº”æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.01957",
            "title": "VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction",
            "url": "https://huggingface.co/papers/2501.01957",
            "abstract": "Recent Multimodal Large Language Models (MLLMs) have typically focused on integrating visual and textual modalities, with less emphasis placed on the role of speech in enhancing interaction. However, speech plays a crucial role in multimodal dialogue systems, and implementing high-performance in both vision and speech tasks remains a significant challenge due to the fundamental modality differences. In this paper, we propose a carefully designed multi-stage training methodology that progressively trains LLM to understand both visual and speech information, ultimately enabling fluent vision and speech interaction. Our approach not only preserves strong vision-language capacity, but also enables efficient speech-to-speech dialogue capabilities without separate ASR and TTS modules, significantly accelerating multimodal end-to-end response speed. By comparing our method against state-of-the-art counterparts across benchmarks for image, video, and speech tasks, we demonstrate that our model is equipped with both strong visual and speech capabilities, making near real-time vision and speech interaction.",
            "score": 9,
            "issue_id": 1506,
            "pub_date": "2025-01-03",
            "pub_date_card": {
                "ru": "3 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 3",
                "zh": "1æœˆ3æ—¥"
            },
            "hash": "b6690c7efedf5a39",
            "authors": [
                "Chaoyou Fu",
                "Haojia Lin",
                "Xiong Wang",
                "Yi-Fan Zhang",
                "Yunhang Shen",
                "Xiaoyu Liu",
                "Yangze Li",
                "Zuwei Long",
                "Heting Gao",
                "Ke Li",
                "Xiawu Zheng",
                "Rongrong Ji",
                "Xing Sun",
                "Caifeng Shan",
                "Ran He"
            ],
            "affiliations": [
                "CASIA",
                "NJU",
                "Tencent Youtu Lab",
                "XMU"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.01957.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#cv",
                    "#multimodal",
                    "#benchmark",
                    "#audio"
                ],
                "emoji": "ğŸ—£ï¸",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¸: Ñ€ĞµÑ‡ÑŒ Ğ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ğµ Ğ² Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ°Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¸ Ñ€ĞµÑ‡ĞµĞ²ÑƒÑ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¿Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ ĞºĞ°Ğº Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ, Ñ‚Ğ°Ğº Ğ¸ Ñ€ĞµÑ‡ĞµĞ²ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ñ€ĞµÑ‡Ğ¸, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ° Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ€ĞµÑ‡Ğ¸ Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ, Ğ±Ğ»Ğ¸Ğ·ĞºĞ¾Ğ¼ Ğº Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸."
                },
                "en": {
                    "title": "Enhancing Multimodal Interaction with Speech and Vision Integration",
                    "desc": "This paper introduces a novel training methodology for Multimodal Large Language Models (MLLMs) that enhances their ability to process both visual and speech data. The proposed multi-stage training approach allows the model to progressively learn and integrate information from images, videos, and spoken language, facilitating seamless interaction. By eliminating the need for separate Automatic Speech Recognition (ASR) and Text-to-Speech (TTS) modules, the model achieves faster response times in multimodal dialogues. Experimental results show that this method not only maintains strong vision-language performance but also excels in speech tasks, enabling near real-time interactions."
                },
                "zh": {
                    "title": "å®ç°æµç•…çš„è§†è§‰ä¸è¯­éŸ³äº¤äº’",
                    "desc": "æœ€è¿‘çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸»è¦é›†ä¸­åœ¨è§†è§‰å’Œæ–‡æœ¬çš„æ•´åˆä¸Šï¼Œè€Œå¯¹è¯­éŸ³åœ¨å¢å¼ºäº¤äº’ä¸­çš„ä½œç”¨å…³æ³¨è¾ƒå°‘ã€‚ç„¶è€Œï¼Œè¯­éŸ³åœ¨å¤šæ¨¡æ€å¯¹è¯ç³»ç»Ÿä¸­èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ï¼Œå¦‚ä½•åœ¨è§†è§‰å’Œè¯­éŸ³ä»»åŠ¡ä¸­å®ç°é«˜æ€§èƒ½ä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç²¾å¿ƒè®¾è®¡çš„å¤šé˜¶æ®µè®­ç»ƒæ–¹æ³•ï¼Œé€æ­¥è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ç†è§£è§†è§‰å’Œè¯­éŸ³ä¿¡æ¯ï¼Œä»è€Œå®ç°æµç•…çš„è§†è§‰å’Œè¯­éŸ³äº¤äº’ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…ä¿æŒäº†å¼ºå¤§çš„è§†è§‰-è¯­è¨€èƒ½åŠ›ï¼Œè¿˜å®ç°äº†é«˜æ•ˆçš„è¯­éŸ³å¯¹è¯èƒ½åŠ›ï¼Œæ˜¾è‘—åŠ å¿«äº†å¤šæ¨¡æ€ç«¯åˆ°ç«¯çš„å“åº”é€Ÿåº¦ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.01073",
            "title": "Graph Generative Pre-trained Transformer",
            "url": "https://huggingface.co/papers/2501.01073",
            "abstract": "Graph generation is a critical task in numerous domains, including molecular design and social network analysis, due to its ability to model complex relationships and structured data. While most modern graph generative models utilize adjacency matrix representations, this work revisits an alternative approach that represents graphs as sequences of node set and edge set. We advocate for this approach due to its efficient encoding of graphs and propose a novel representation. Based on this representation, we introduce the Graph Generative Pre-trained Transformer (G2PT), an auto-regressive model that learns graph structures via next-token prediction. To further exploit G2PT's capabilities as a general-purpose foundation model, we explore fine-tuning strategies for two downstream applications: goal-oriented generation and graph property prediction. We conduct extensive experiments across multiple datasets. Results indicate that G2PT achieves superior generative performance on both generic graph and molecule datasets. Furthermore, G2PT exhibits strong adaptability and versatility in downstream tasks from molecular design to property prediction.",
            "score": 5,
            "issue_id": 1508,
            "pub_date": "2025-01-02",
            "pub_date_card": {
                "ru": "2 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 2",
                "zh": "1æœˆ2æ—¥"
            },
            "hash": "596abc88d57e0650",
            "authors": [
                "Xiaohui Chen",
                "Yinkai Wang",
                "Jiaxing He",
                "Yuanqi Du",
                "Soha Hassoun",
                "Xiaolin Xu",
                "Li-Ping Liu"
            ],
            "affiliations": [
                "Cornell University",
                "Northeastern University",
                "Tufts University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.01073.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#optimization",
                    "#training",
                    "#architecture",
                    "#data",
                    "#graphs"
                ],
                "emoji": "ğŸ•¸ï¸",
                "ru": {
                    "title": "G2PT: Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ²",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ² - Graph Generative Pre-trained Transformer (G2PT). G2PT Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ² Ğ² Ğ²Ğ¸Ğ´Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ² ÑƒĞ·Ğ»Ğ¾Ğ² Ğ¸ Ñ€Ñ‘Ğ±ĞµÑ€ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ† ÑĞ¼ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµgressĞ¸Ğ²Ğ½Ñ‹Ğ¼ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ¾Ğ¼. G2PT Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ°Ğº Ğ¾Ğ±Ñ‰Ğ¸Ñ… Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ², Ñ‚Ğ°Ğº Ğ¸ Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ», Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ…Ğ¾Ñ€Ğ¾ÑˆÑƒÑ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼."
                },
                "en": {
                    "title": "Revolutionizing Graph Generation with G2PT",
                    "desc": "This paper focuses on improving graph generation, which is important for tasks like designing molecules and analyzing social networks. Instead of using the common adjacency matrix, it proposes a new way to represent graphs as sequences of node and edge sets, making the encoding more efficient. The authors introduce the Graph Generative Pre-trained Transformer (G2PT), an auto-regressive model that learns to generate graph structures by predicting the next token in a sequence. Through various experiments, they demonstrate that G2PT outperforms existing models in generating graphs and is effective in applications like molecular design and predicting graph properties."
                },
                "zh": {
                    "title": "å›¾ç”Ÿæˆçš„åˆ›æ–°ï¼šG2PTæ¨¡å‹",
                    "desc": "å›¾ç”Ÿæˆåœ¨è®¸å¤šé¢†åŸŸä¸­éå¸¸é‡è¦ï¼Œæ¯”å¦‚åˆ†å­è®¾è®¡å’Œç¤¾äº¤ç½‘ç»œåˆ†æï¼Œå› ä¸ºå®ƒèƒ½å¤Ÿå»ºæ¨¡å¤æ‚çš„å…³ç³»å’Œç»“æ„åŒ–æ•°æ®ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å›¾è¡¨ç¤ºæ–¹æ³•ï¼Œå°†å›¾è¡¨ç¤ºä¸ºèŠ‚ç‚¹é›†å’Œè¾¹é›†çš„åºåˆ—ï¼Œè€Œä¸æ˜¯ä¼ ç»Ÿçš„é‚»æ¥çŸ©é˜µã€‚åŸºäºè¿™ç§è¡¨ç¤ºï¼Œæˆ‘ä»¬å¼•å…¥äº†å›¾ç”Ÿæˆé¢„è®­ç»ƒå˜æ¢å™¨ï¼ˆG2PTï¼‰ï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹å­¦ä¹ å›¾ç»“æ„çš„è‡ªå›å½’æ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒG2PTåœ¨é€šç”¨å›¾å’Œåˆ†å­æ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå¹¶ä¸”åœ¨åˆ†å­è®¾è®¡å’Œå±æ€§é¢„æµ‹ç­‰ä¸‹æ¸¸ä»»åŠ¡ä¸­å…·æœ‰å¾ˆå¼ºçš„é€‚åº”æ€§å’Œå¤šåŠŸèƒ½æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.21059",
            "title": "VisionReward: Fine-Grained Multi-Dimensional Human Preference Learning for Image and Video Generation",
            "url": "https://huggingface.co/papers/2412.21059",
            "abstract": "We present a general strategy to aligning visual generation models -- both image and video generation -- with human preference. To start with, we build VisionReward -- a fine-grained and multi-dimensional reward model. We decompose human preferences in images and videos into multiple dimensions, each represented by a series of judgment questions, linearly weighted and summed to an interpretable and accurate score. To address the challenges of video quality assessment, we systematically analyze various dynamic features of videos, which helps VisionReward surpass VideoScore by 17.2% and achieve top performance for video preference prediction. Based on VisionReward, we develop a multi-objective preference learning algorithm that effectively addresses the issue of confounding factors within preference data. Our approach significantly outperforms existing image and video scoring methods on both machine metrics and human evaluation. All code and datasets are provided at https://github.com/THUDM/VisionReward.",
            "score": 4,
            "issue_id": 1510,
            "pub_date": "2025-12-30",
            "pub_date_card": {
                "ru": "30 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 30",
                "zh": "12æœˆ30æ—¥"
            },
            "hash": "1f3bb267ffa751d9",
            "authors": [
                "Jiazheng Xu",
                "Yu Huang",
                "Jiale Cheng",
                "Yuanming Yang",
                "Jiajun Xu",
                "Yuan Wang",
                "Wenbo Duan",
                "Shen Yang",
                "Qunlin Jin",
                "Shurun Li",
                "Jiayan Teng",
                "Zhuoyi Yang",
                "Wendi Zheng",
                "Xiao Liu",
                "Ming Ding",
                "Xiaohan Zhang",
                "Xiaotao Gu",
                "Shiyu Huang",
                "Minlie Huang",
                "Jie Tang",
                "Yuxiao Dong"
            ],
            "affiliations": [
                "Tsinghua University",
                "Zhipu AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.21059.jpg",
            "data": {
                "categories": [
                    "#rag",
                    "#training",
                    "#open_source",
                    "#cv",
                    "#video",
                    "#optimization",
                    "#alignment"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "VisionReward: Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼ĞµÑ€Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼Ğ¸. ĞĞ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ VisionReward - Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼ĞµÑ€Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ñ Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ… Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğ¹. Ğ”Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±Ñ‹Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ñ‹ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ¾ VisionReward Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾Ğ¹Ñ‚Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° 17.2%. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ VisionReward Ğ±Ñ‹Ğ» Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼, ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€ĞµÑˆĞ°ÑÑ‰Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ĞºĞ¾Ğ½Ñ„Ğ°ÑƒĞ½Ğ´Ğ¸Ğ½Ğ³-Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¾Ğ² Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑÑ…."
                },
                "en": {
                    "title": "Aligning Visual Generation with Human Preferences",
                    "desc": "This paper introduces a method for aligning visual generation models, such as those for images and videos, with human preferences. The authors create a reward model called VisionReward, which breaks down human preferences into multiple dimensions assessed through specific judgment questions. They enhance video quality assessment by analyzing dynamic features, leading to a 17.2% improvement over previous methods. Additionally, a multi-objective preference learning algorithm is developed to manage confounding factors in preference data, resulting in superior performance compared to existing scoring methods."
                },
                "zh": {
                    "title": "è§†è§‰ç”Ÿæˆæ¨¡å‹ä¸äººç±»åå¥½çš„å®Œç¾å¯¹é½",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§é€šç”¨ç­–ç•¥ï¼Œç”¨äºå°†è§†è§‰ç”Ÿæˆæ¨¡å‹ï¼ˆåŒ…æ‹¬å›¾åƒå’Œè§†é¢‘ç”Ÿæˆï¼‰ä¸äººç±»åå¥½å¯¹é½ã€‚æˆ‘ä»¬æ„å»ºäº†VisionRewardï¼Œè¿™æ˜¯ä¸€ä¸ªç»†ç²’åº¦å’Œå¤šç»´åº¦çš„å¥–åŠ±æ¨¡å‹ï¼Œèƒ½å¤Ÿå°†äººç±»å¯¹å›¾åƒå’Œè§†é¢‘çš„åå¥½åˆ†è§£ä¸ºå¤šä¸ªç»´åº¦ã€‚é€šè¿‡åˆ†æè§†é¢‘çš„åŠ¨æ€ç‰¹å¾ï¼ŒVisionRewardåœ¨è§†é¢‘åå¥½é¢„æµ‹ä¸­è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œæå‡äº†17.2%çš„æ€§èƒ½ã€‚åŸºäºVisionRewardï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§å¤šç›®æ ‡åå¥½å­¦ä¹ ç®—æ³•ï¼Œæœ‰æ•ˆè§£å†³äº†åå¥½æ•°æ®ä¸­çš„æ··æ·†å› ç´ é—®é¢˜ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.01904",
            "title": "Virgo: A Preliminary Exploration on Reproducing o1-like MLLM",
            "url": "https://huggingface.co/papers/2501.01904",
            "abstract": "Recently, slow-thinking reasoning systems, built upon large language models (LLMs), have garnered widespread attention by scaling the thinking time during inference. There is also growing interest in adapting this capability to multimodal large language models (MLLMs). Given that MLLMs handle more complex data semantics across different modalities, it is intuitively more challenging to implement multimodal slow-thinking systems.   To address this issue, in this paper, we explore a straightforward approach by fine-tuning a capable MLLM with a small amount of textual long-form thought data, resulting in a multimodal slow-thinking system, Virgo (Visual reasoning with long thought). We find that these long-form reasoning processes, expressed in natural language, can be effectively transferred to MLLMs. Moreover, it seems that such textual reasoning data can be even more effective than visual reasoning data in eliciting the slow-thinking capacities of MLLMs. While this work is preliminary, it demonstrates that slow-thinking capacities are fundamentally associated with the language model component, which can be transferred across modalities or domains. This finding can be leveraged to guide the development of more powerful slow-thinking reasoning systems. We release our resources at https://github.com/RUCAIBox/Virgo.",
            "score": 4,
            "issue_id": 1505,
            "pub_date": "2025-01-03",
            "pub_date_card": {
                "ru": "3 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 3",
                "zh": "1æœˆ3æ—¥"
            },
            "hash": "576423a20b419d0f",
            "authors": [
                "Yifan Du",
                "Zikang Liu",
                "Yifan Li",
                "Wayne Xin Zhao",
                "Yuqi Huo",
                "Bingning Wang",
                "Weipeng Chen",
                "Zheng Liu",
                "Zhongyuan Wang",
                "Ji-Rong Wen"
            ],
            "affiliations": [
                "BAAI",
                "Baichuan AI",
                "Gaoling School of Artificial Intelligence, Renmin University of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.01904.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#multimodal",
                    "#transfer_learning",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ñ‡ĞµÑ€ĞµĞ· Ñ‚ĞµĞºÑÑ‚",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğ¸ Ğ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ğ¼ĞµĞ´Ğ»ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Virgo, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ¸Ñ‚ÑŒ MLLM Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ³Ğ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ´Ğ°Ğ¶Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ĞµĞµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ…. Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ğ¼ĞµĞ´Ğ»ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ² Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¼ ÑĞ²ÑĞ·Ğ°Ğ½Ñ‹ Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ¸Ñ‚ÑŒÑÑ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Unlocking Slow-Thinking in Multimodal Models with Textual Reasoning",
                    "desc": "This paper discusses the development of a multimodal slow-thinking reasoning system called Virgo, which is based on fine-tuning a multimodal large language model (MLLM) using long-form textual reasoning data. The authors found that incorporating long-form reasoning in natural language significantly enhances the slow-thinking capabilities of MLLMs, even more so than using visual reasoning data. This suggests that the slow-thinking abilities are closely linked to the language model aspect, allowing for effective transfer across different data modalities. The research indicates a promising direction for creating advanced reasoning systems that can handle complex data semantics."
                },
                "zh": {
                    "title": "å¤šæ¨¡æ€æ…¢æ€ç»´æ¨ç†çš„æ¢ç´¢",
                    "desc": "æœ€è¿‘ï¼ŒåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ…¢æ€ç»´æ¨ç†ç³»ç»Ÿå¼•èµ·äº†å¹¿æ³›å…³æ³¨ï¼Œå°¤å…¶æ˜¯åœ¨æ¨ç†è¿‡ç¨‹ä¸­å»¶é•¿æ€è€ƒæ—¶é—´çš„èƒ½åŠ›ã€‚æœ¬æ–‡æ¢è®¨äº†å¦‚ä½•å°†è¿™ç§èƒ½åŠ›åº”ç”¨äºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ï¼Œå°½ç®¡å¤„ç†ä¸åŒæ¨¡æ€çš„å¤æ‚æ•°æ®è¯­ä¹‰æ›´å…·æŒ‘æˆ˜æ€§ã€‚æˆ‘ä»¬é€šè¿‡å¾®è°ƒä¸€ä¸ªå¼ºå¤§çš„MLLMï¼Œä½¿ç”¨å°‘é‡çš„é•¿æ–‡æœ¬æ€ç»´æ•°æ®ï¼ŒæˆåŠŸæ„å»ºäº†ä¸€ä¸ªå¤šæ¨¡æ€æ…¢æ€ç»´ç³»ç»Ÿï¼Œå‘½åä¸ºVirgoï¼ˆè§†è§‰æ¨ç†ä¸é•¿æ€ç»´ï¼‰ã€‚ç ”ç©¶è¡¨æ˜ï¼Œé•¿æ–‡æœ¬æ¨ç†è¿‡ç¨‹å¯ä»¥æœ‰æ•ˆè½¬ç§»åˆ°MLLMsï¼Œå¹¶ä¸”è¿™ç§æ–‡æœ¬æ¨ç†æ•°æ®åœ¨æ¿€å‘MLLMsçš„æ…¢æ€ç»´èƒ½åŠ›æ–¹é¢ï¼Œä¼¼ä¹æ¯”è§†è§‰æ¨ç†æ•°æ®æ›´æœ‰æ•ˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.01540",
            "title": "BoxingGym: Benchmarking Progress in Automated Experimental Design and Model Discovery",
            "url": "https://huggingface.co/papers/2501.01540",
            "abstract": "Understanding the world and explaining it with scientific theories is a central aspiration of artificial intelligence research. Proposing theories, designing experiments to test them, and then revising them based on data are fundamental to scientific discovery. Despite the significant promise of LLM-based scientific agents, no benchmarks systematically test LLM's ability to propose scientific models, collect experimental data, and revise them in light of new data. We introduce BoxingGym, a benchmark with 10 environments for systematically evaluating both experimental design (e.g. collecting data to test a scientific theory) and model discovery (e.g. proposing and revising scientific theories). To enable tractable and quantitative evaluation, we implement each environment as a generative probabilistic model with which a scientific agent can run interactive experiments. These probabilistic models are drawn from various real-world scientific domains ranging from psychology to ecology. To quantitatively evaluate a scientific agent's ability to collect informative experimental data, we compute the expected information gain (EIG), an information-theoretic quantity which measures how much an experiment reduces uncertainty about the parameters of a generative model. A good scientific theory is a concise and predictive explanation. Therefore, to quantitatively evaluate model discovery, we ask a scientific agent to explain their model and then assess whether this explanation enables another scientific agent to make reliable predictions about this environment. In addition to this explanation-based evaluation, we compute standard model evaluation metrics such as prediction errors. We find that current LLMs, such as GPT-4o, struggle with both experimental design and model discovery. We find that augmenting the LLM-based agent with an explicit statistical model does not reliably improve these results.",
            "score": 2,
            "issue_id": 1510,
            "pub_date": "2025-01-02",
            "pub_date_card": {
                "ru": "2 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 2",
                "zh": "1æœˆ2æ—¥"
            },
            "hash": "0f853b1681ef29b5",
            "authors": [
                "Kanishk Gandhi",
                "Michael Y. Li",
                "Lyle Goodyear",
                "Louise Li",
                "Aditi Bhaskar",
                "Mohammed Zaman",
                "Noah D. Goodman"
            ],
            "affiliations": [
                "Stanford University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.01540.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#data",
                    "#science",
                    "#agents"
                ],
                "emoji": "ğŸ§ª",
                "ru": {
                    "title": "BoxingGym: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ²Ñ‹Ğ·Ğ¾Ğ² Ğ´Ğ»Ñ Ğ˜Ğ˜ Ğ² Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº BoxingGym Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğº Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ¼Ñƒ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ñ. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 10 ÑÑ€ĞµĞ´, Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ñ… Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğµ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸, Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ğµ Ñ‚ĞµĞ¾Ñ€Ğ¸Ğ¹. Ğ”Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ¾Ğ¶Ğ¸Ğ´Ğ°ĞµĞ¼Ñ‹Ğ¹ Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ (EIG), Ğ° Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ‚ĞµĞ¾Ñ€Ğ¸Ğ¹ - Ğ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑŠÑÑĞ½ÑÑ‚ÑŒ Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ LLM, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ GPT-4, Ğ¿Ğ¾ĞºĞ° ÑĞ»Ğ°Ğ±Ğ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ ÑÑ‚Ğ¸Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "BoxingGym: Evaluating LLMs in Scientific Discovery",
                    "desc": "This paper introduces BoxingGym, a benchmark designed to evaluate the capabilities of large language models (LLMs) in scientific discovery tasks. It focuses on two main aspects: experimental design, which involves collecting data to test scientific theories, and model discovery, which includes proposing and revising these theories. The benchmark consists of 10 environments modeled as generative probabilistic models from various scientific fields, allowing for interactive experimentation. The study finds that current LLMs, like GPT-4o, face challenges in both areas, and adding a statistical model does not consistently enhance their performance."
                },
                "zh": {
                    "title": "è¯„ä¼°äººå·¥æ™ºèƒ½åœ¨ç§‘å­¦ç ”ç©¶ä¸­çš„èƒ½åŠ›",
                    "desc": "è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†äººå·¥æ™ºèƒ½åœ¨ç§‘å­¦ç ”ç©¶ä¸­çš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æå‡ºç§‘å­¦ç†è®ºå’Œè®¾è®¡å®éªŒæ–¹é¢çš„èƒ½åŠ›ã€‚ä½œè€…æå‡ºäº†ä¸€ä¸ªåä¸ºBoxingGymçš„åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«10ä¸ªç¯å¢ƒï¼Œç”¨äºç³»ç»Ÿè¯„ä¼°å®éªŒè®¾è®¡å’Œæ¨¡å‹å‘ç°çš„èƒ½åŠ›ã€‚é€šè¿‡è®¡ç®—æœŸæœ›ä¿¡æ¯å¢ç›Šï¼ˆEIGï¼‰ï¼Œè®ºæ–‡é‡åŒ–äº†ç§‘å­¦ä»£ç†æ”¶é›†å®éªŒæ•°æ®çš„æœ‰æ•ˆæ€§ï¼Œå¹¶è¯„ä¼°å…¶æå‡ºçš„æ¨¡å‹æ˜¯å¦èƒ½è¿›è¡Œå¯é é¢„æµ‹ã€‚ç ”ç©¶å‘ç°ï¼Œå½“å‰çš„LLMåœ¨å®éªŒè®¾è®¡å’Œæ¨¡å‹å‘ç°æ–¹é¢è¡¨ç°ä¸ä½³ï¼Œä¸”ç®€å•åœ°å¢åŠ ç»Ÿè®¡æ¨¡å‹å¹¶æœªæ˜¾è‘—æ”¹å–„ç»“æœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.00874",
            "title": "LUSIFER: Language Universal Space Integration for Enhanced Multilingual Embeddings with Large Language Models",
            "url": "https://huggingface.co/papers/2501.00874",
            "abstract": "Recent advancements in large language models (LLMs) based embedding models have established new state-of-the-art benchmarks for text embedding tasks, particularly in dense vector-based retrieval. However, these models predominantly focus on English, leaving multilingual embedding capabilities largely unexplored. To address this limitation, we present LUSIFER, a novel zero-shot approach that adapts LLM-based embedding models for multilingual tasks without requiring multilingual supervision. LUSIFER's architecture combines a multilingual encoder, serving as a language-universal learner, with an LLM-based embedding model optimized for embedding-specific tasks. These components are seamlessly integrated through a minimal set of trainable parameters that act as a connector, effectively transferring the multilingual encoder's language understanding capabilities to the specialized embedding model. Additionally, to comprehensively evaluate multilingual embedding performance, we introduce a new benchmark encompassing 5 primary embedding tasks, 123 diverse datasets, and coverage across 14 languages. Extensive experimental results demonstrate that LUSIFER significantly enhances the multilingual performance across various embedding tasks, particularly for medium and low-resource languages, without requiring explicit multilingual training data.",
            "score": 2,
            "issue_id": 1507,
            "pub_date": "2025-01-01",
            "pub_date_card": {
                "ru": "1 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 1",
                "zh": "1æœˆ1æ—¥"
            },
            "hash": "5bdfec436923a2a6",
            "authors": [
                "Hieu Man",
                "Nghia Trung Ngo",
                "Viet Dac Lai",
                "Ryan A. Rossi",
                "Franck Dernoncourt",
                "Thien Huu Nguyen"
            ],
            "affiliations": [
                "Adobe Research, USA",
                "Dept. of Computer Science, University of Oregon, OR, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.00874.jpg",
            "data": {
                "categories": [
                    "#transfer_learning",
                    "#architecture",
                    "#benchmark",
                    "#multilingual",
                    "#low_resource"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ Ğ±ĞµĞ· Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "LUSIFER - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ½ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ Ğ¸ LLM-Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ², Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ 5 Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡, 123 Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ğ¸ 14 ÑĞ·Ñ‹ĞºĞ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ LUSIFER Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ² Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµÑÑƒÑ€ÑĞ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "LUSIFER: Bridging Multilingual Gaps in Text Embedding",
                    "desc": "This paper introduces LUSIFER, a new method that enhances large language models (LLMs) for multilingual text embedding tasks. Unlike existing models that mainly focus on English, LUSIFER uses a zero-shot approach to adapt LLMs for multiple languages without needing multilingual training data. It combines a multilingual encoder with an LLM-based embedding model, allowing for effective language understanding and embedding performance. The authors also present a comprehensive benchmark to evaluate LUSIFER's performance across various languages and tasks, showing significant improvements, especially for less-resourced languages."
                },
                "zh": {
                    "title": "LUSIFERï¼šæ— ç›‘ç£å¤šè¯­è¨€åµŒå…¥çš„æ–°çªç ´",
                    "desc": "æœ€è¿‘ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ–‡æœ¬åµŒå…¥ä»»åŠ¡ä¸­å–å¾—äº†æ–°çš„çªç ´ï¼Œå°¤å…¶æ˜¯åœ¨åŸºäºå¯†é›†å‘é‡çš„æ£€ç´¢æ–¹é¢ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹ä¸»è¦é›†ä¸­åœ¨è‹±è¯­ä¸Šï¼Œå¯¼è‡´å¤šè¯­è¨€åµŒå…¥èƒ½åŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†LUSIFERï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„é›¶æ ·æœ¬æ–¹æ³•ï¼Œå¯ä»¥åœ¨ä¸éœ€è¦å¤šè¯­è¨€ç›‘ç£çš„æƒ…å†µä¸‹ï¼Œå°†LLMåµŒå…¥æ¨¡å‹é€‚åº”äºå¤šè¯­è¨€ä»»åŠ¡ã€‚LUSIFERçš„æ¶æ„ç»“åˆäº†ä¸€ä¸ªå¤šè¯­è¨€ç¼–ç å™¨å’Œä¸€ä¸ªé’ˆå¯¹åµŒå…¥ç‰¹å®šä»»åŠ¡ä¼˜åŒ–çš„LLMåµŒå…¥æ¨¡å‹ï¼Œé€šè¿‡ä¸€ç»„æœ€å°çš„å¯è®­ç»ƒå‚æ•°å®ç°æ— ç¼è¿æ¥ï¼Œæœ‰æ•ˆåœ°å°†å¤šè¯­è¨€ç¼–ç å™¨çš„è¯­è¨€ç†è§£èƒ½åŠ›è½¬ç§»åˆ°ä¸“é—¨çš„åµŒå…¥æ¨¡å‹ä¸Šã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-01-03.html",
    "link_next": "2025-01-07.html",
    "link_month": "2025-01.html",
    "short_date_prev": {
        "ru": "03.01",
        "en": "01/03",
        "zh": "1æœˆ3æ—¥"
    },
    "short_date_next": {
        "ru": "07.01",
        "en": "01/07",
        "zh": "1æœˆ7æ—¥"
    },
    "categories": {
        "#dataset": 1,
        "#data": 3,
        "#benchmark": 3,
        "#agents": 1,
        "#cv": 2,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 1,
        "#plp": 0,
        "#inference": 0,
        "#3d": 1,
        "#audio": 1,
        "#video": 1,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 4,
        "#robotics": 1,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 2,
        "#graphs": 1,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 2,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 1
    },
    "zh": {
        "text": "This article introduces EnerVerse, a framework for generating future space in robotic tasks. It uses attention mechanisms for consistent space modeling and a memory context for long sequence generation. The FAV space enhances robot observation and adaptability. A data engine with 4DGS improves data quality and diversity. Experiments show it boosts performance in long-range robotic tasks.",
        "title": "EnerVerse: Envisioning Embodied Future Space for Robotics Manipulation",
        "pinyin": "Sure, here is the pinyin transcription for the given text:\n\nZhÃ¨ wÃ©nzhÄng jiÃ¨shÃ o EnerVerse, yÄ«gÃ¨ kuÃ ngjiÃ  yÇnqÇ wÃ¨ilÃ¡i kÅngjiÄn zÃ i jÄ«qirÃ©n rÃ©nwÃ¹ zhÅng. TÄ shÇyÃ²ng zhÃ¹yÃ¬ jÄ«zhÃ¬ wÃ¨i yuÃ¡nchÃ¡ng kÅngjiÄn mÃ³xÃ­ng hÃ© yÄ«gÃ¨ jÃ¬yÃ¬ qÅ«jiÃ n wÃ¨i chÃ¡ng xÃ¹liÃ¨ shÄ“ngchÃ©ng. FAV kÅngjiÄn zÄ“ngqiÃ¡ng jÄ«qirÃ©n guÄnchÃ¡ hÃ© shÃ¬yÃ¬ngxÃ¬ng. YÄ«gÃ¨ shÃ¹jÃ¹ yÇnqÃ­ng yÇ’u 4DGS gÇishÃ n shÃ¹jÃ¹ zhÃ¬liÃ ng hÃ© duÅyÃ ngxÃ¬ng. ShÃ­yÃ n xiÇnshÃ¬ tÄ zÄ“ngqiÃ¡ng xiÃ oguÇ’ zÃ i chÃ¡ngqÄ« jÄ«qirÃ©n rÃ©nwÃ¹ zhÅng.\n\nPlease note that the pinyin transcription is based on the pronunciation of the Chinese characters that would be used to translate the English text. The actual translation might vary slightly depending on the context and specific terminology used.",
        "vocab": "[\n    {\"word\": \"framework\", \"pinyin\": \"kuÃ ngjiÃ \", \"trans\": \"æ¡†æ¶\"},\n    {\"word\": \"generating\", \"pinyin\": \"shÄ“ngchÃ©ng\", \"trans\": \"ç”Ÿæˆ\"},\n    {\"word\": \"future\", \"pinyin\": \"wÃ¨ilÃ¡i\", \"trans\": \"æœªæ¥\"},\n    {\"word\": \"space\", \"pinyin\": \"kÅngjiÄn\", \"trans\": \"ç©ºé—´\"},\n    {\"word\": \"robotic\", \"pinyin\": \"jÄ«qirÃ©n\", \"trans\": \"æœºå™¨äºº\"},\n    {\"word\": \"tasks\", \"pinyin\": \"rÃ¨nwÃ¹\", \"trans\": \"ä»»åŠ¡\"},\n    {\"word\": \"attention\", \"pinyin\": \"zhÃ¹yÃ¬\", \"trans\": \"æ³¨æ„\"},\n    {\"word\": \"mechanisms\", \"pinyin\": \"jÄ«zhÃ¬\", \"trans\": \"æœºåˆ¶\"},\n    {\"word\": \"consistent\", \"pinyin\": \"wÃºguÇ’\", \"trans\": \"ä¸€è‡´\"},\n    {\"word\": \"modeling\", \"pinyin\": \"mÃ³xÃ­ng\", \"trans\": \"å»ºæ¨¡\"},\n    {\"word\": \"memory\", \"pinyin\": \"jÃ¬yÃ¬\", \"trans\": \"è®°å¿†\"},\n    {\"word\": \"context\", \"pinyin\": \"qÇ”wÃ©n\", \"trans\": \"ä¸Šä¸‹æ–‡\"},\n    {\"word\": \"sequence\", \"pinyin\": \"xÃ¹liÃ¨\", \"trans\": \"åºåˆ—\"},\n    {\"word\": \"generation\", \"pinyin\": \"shÄ“ngchÃ©ng\", \"trans\": \"ç”Ÿæˆ\"},\n    {\"word\": \"FAV\", \"pinyin\": \"FÄ“i-Ä’i-WÄ“i\", \"trans\": \"FAV\"},\n    {\"word\": \"enhances\", \"pinyin\": \"zÄ“ngqiÃ¡ng\", \"trans\": \"å¢å¼º\"},\n    {\"word\": \"observation\", \"pinyin\": \"guÄnchÃ¡\", \"trans\": \"è§‚å¯Ÿ\"},\n    {\"word\": \"adaptability\", \"pinyin\": \"shÃ¬yÃ¬ngxÃ¬ng\", \"trans\": \"é€‚åº”æ€§\"},\n    {\"word\": \"engine\", \"pinyin\": \"yÇnqÃ­ng\", \"trans\": \"å¼•æ“\"},\n    {\"word\": \"4DGS\", \"pinyin\": \"SÃ¬-DÄ«-JÄ«-Ä’s\", \"trans\": \"4DGS\"},\n    {\"word\": \"improves\", \"pinyin\": \"gÇishÃ n\", \"trans\": \"æ”¹å–„\"},\n    {\"word\": \"quality\", \"pinyin\": \"zhÃ¬liÃ ng\", \"trans\": \"è´¨é‡\"},\n    {\"word\": \"diversity\", \"pinyin\": \"duÅyÃ ngxÃ¬ng\", \"trans\": \"å¤šæ ·æ€§\"},\n    {\"word\": \"experiments\", \"pinyin\": \"shÃ­yÃ n\", \"trans\": \"å®éªŒ\"},\n    {\"word\": \"show\", \"pinyin\": \"xiÇnshÃ¬\", \"trans\": \"æ˜¾ç¤º\"},\n    {\"word\": \"boosts\", \"pinyin\": \"zÄ“ngqiÃ¡ng\", \"trans\": \"å¢å¼º\"},\n    {\"word\": \"performance\", \"pinyin\": \"biÇoxiÃ n\", \"trans\": \"è¡¨ç°\"},\n    {\"word\": \"long-range\", \"pinyin\": \"chÃ¡ngyuÇn\", \"trans\": \"é•¿è¿œ\"}\n]",
        "trans": "This article introduces EnerVerse, a framework designed to generate future space in robotic tasks. It employs attention mechanisms to ensure consistent space modeling and utilizes a memory context for generating long sequences. The FAV space enhances robot observation capabilities and adaptability. Additionally, a data engine equipped with 4DGS improves the quality and diversity of data. Experiments demonstrate that it significantly boosts performance in long-range robotic tasks.",
        "update_ts": "2025-01-06 09:11"
    }
}