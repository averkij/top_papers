
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 25 papers. October 29.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">29 октября</span> | <span id="title-articles-count">25 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-10-28.html">⬅️ <span id="prev-date">28.10</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-10-30.html">➡️ <span id="next-date">30.10</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-10.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '29 октября', 'en': 'October 29', 'zh': '10月29日'};
        let feedDateNext = {'ru': '30.10', 'en': '10/30', 'zh': '10月30日'};
        let feedDatePrev = {'ru': '28.10', 'en': '10/28', 'zh': '10月28日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2510.24668', 'title': 'InteractComp: Evaluating Search Agents With Ambiguous Queries', 'url': 'https://huggingface.co/papers/2510.24668', 'abstract': "InteractComp evaluates search agents' ability to recognize and resolve query ambiguity through interaction, revealing significant gaps in current models' capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t Language agents have demonstrated remarkable potential in web search and information retrieval. However, these search agents assume user queries are complete and unambiguous, an assumption that diverges from reality where users begin with incomplete queries requiring clarification through interaction. Yet most agents lack interactive mechanisms during the search process, and existing benchmarks cannot assess this capability. To address this gap, we introduce InteractComp, a benchmark designed to evaluate whether search agents can recognize query ambiguity and actively interact to resolve it during search. Following the principle of easy to verify, interact to disambiguate, we construct 210 expert-curated questions across 9 domains through a target-distractor methodology that creates genuine ambiguity resolvable only through interaction. Evaluation of 17 models reveals striking failure: the best model achieves only 13.73% accuracy despite 71.50% with complete context, exposing systematic overconfidence rather than reasoning deficits. Forced interaction produces dramatic gains, demonstrating latent capability current strategies fail to engage. Longitudinal analysis shows interaction capabilities stagnated over 15 months while search performance improved seven-fold, revealing a critical blind spot. This stagnation, coupled with the immediate feedback inherent to search tasks, makes InteractComp a valuable resource for both evaluating and training interaction capabilities in search agents. The code is available at https://github.com/FoundationAgents/InteractComp.", 'score': 72, 'issue_id': 6668, 'pub_date': '2025-10-28', 'pub_date_card': {'ru': '28 октября', 'en': 'October 28', 'zh': '10月28日'}, 'hash': '7db93e4c4cf42dec', 'authors': ['Mingyi Deng', 'Lijun Huang', 'Yani Fan', 'Jiayi Zhang', 'Fashen Ren', 'Jinyi Bai', 'Fuzhen Yang', 'Dayi Miao', 'Zhaoyang Yu', 'Yifan Wu', 'Yanfei Zhang', 'Fengwei Teng', 'Yingjia Wan', 'Song Hu', 'Yude Li', 'Xin Jin', 'Conghao Hu', 'Haoyu Li', 'Qirui Fu', 'Tai Zhong', 'Xinyu Wang', 'Xiangru Tang', 'Nan Tang', 'Chenglin Wu', 'Yuyu Luo'], 'affiliations': ['Agent Universe', 'DeepWisdom', 'McGill University', 'Renmin University of China', 'The Hong Kong University of Science and Technology (Guangzhou)', 'University of California, Los Angeles', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2510.24668.jpg', 'data': {'categories': ['#alignment', '#interpretability', '#benchmark', '#agents', '#reasoning'], 'emoji': '❓', 'ru': {'title': 'Поисковые агенты не умеют задавать уточняющие вопросы', 'desc': 'Исследователи представили бенчмарк InteractComp для оценки способности поисковых AI-агентов распознавать неоднозначные запросы и уточнять их через диалог с пользователем. Тестирование 17 моделей показало критический провал: лучшая модель достигла только 13.73% точности при неполном контексте против 71.50% с полной информацией. Проблема заключается не в недостатке рассуждений, а в систематической самоуверенности моделей, которые не осознают необходимость задавать вопросы. При принудительном взаимодействии результаты резко улучшаются, что демонстрирует скрытый потенциал современных LLM, который текущие стратегии не используют.'}, 'en': {'title': "Bridging the Gap: Enhancing Search Agents' Interaction Skills", 'desc': 'InteractComp is a new benchmark that tests how well search agents can identify and clarify ambiguous user queries through interaction. Current models often assume that user queries are clear and complete, which is not the case in real-world scenarios. The benchmark includes 210 carefully designed questions that create genuine ambiguity, requiring agents to engage with users to resolve it. Results show that while search performance has improved, the ability to interact and clarify queries has stagnated, highlighting a significant area for development in search agent capabilities.'}, 'zh': {'title': '提升搜索代理的互动能力', 'desc': 'InteractComp是一个新的基准，用于评估搜索代理识别和解决查询模糊性的能力。现有的搜索模型通常假设用户的查询是完整且明确的，但实际上用户的查询往往是不完整的，需要通过互动来澄清。我们设计了210个专家策划的问题，旨在通过互动来解决真实的模糊性。评估结果显示，尽管搜索性能有所提高，但互动能力却停滞不前，这表明当前策略未能有效利用潜在能力。'}}}, {'id': 'https://huggingface.co/papers/2510.24701', 'title': 'Tongyi DeepResearch Technical Report', 'url': 'https://huggingface.co/papers/2510.24701', 'abstract': "Tongyi DeepResearch, a large language model with agentic capabilities, achieves top performance in various deep research tasks through an end-to-end training framework and automated data synthesis.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Tongyi DeepResearch, an agentic large language model, which is specifically designed for long-horizon, deep information-seeking research tasks. To incentivize autonomous deep research agency, Tongyi DeepResearch is developed through an end-to-end training framework that combines agentic mid-training and agentic post-training, enabling scalable reasoning and information seeking across complex tasks. We design a highly scalable data synthesis pipeline that is fully automatic, without relying on costly human annotation, and empowers all training stages. By constructing customized environments for each stage, our system enables stable and consistent interactions throughout. Tongyi DeepResearch, featuring 30.5 billion total parameters, with only 3.3 billion activated per token, achieves state-of-the-art performance across a range of agentic deep research benchmarks, including Humanity's Last Exam, BrowseComp, BrowseComp-ZH, WebWalkerQA, xbench-DeepSearch, FRAMES and xbench-DeepSearch-2510. We open-source the model, framework, and complete solutions to empower the community.", 'score': 45, 'issue_id': 6667, 'pub_date': '2025-10-28', 'pub_date_card': {'ru': '28 октября', 'en': 'October 28', 'zh': '10月28日'}, 'hash': 'a6f467c756fc29b7', 'authors': ['Tongyi DeepResearch Team', 'Baixuan Li', 'Bo Zhang', 'Dingchu Zhang', 'Fei Huang', 'Guangyu Li', 'Guoxin Chen', 'Huifeng Yin', 'Jialong Wu', 'Jingren Zhou', 'Kuan Li', 'Liangcai Su', 'Litu Ou', 'Liwen Zhang', 'Pengjun Xie', 'Rui Ye', 'Wenbiao Yin', 'Xinmiao Yu', 'Xinyu Wang', 'Xixi Wu', 'Xuanzhong Chen', 'Yida Zhao', 'Zhen Zhang', 'Zhengwei Tao', 'Zhongwang Zhang', 'Zile Qiao', 'Chenxi Wang', 'Donglei Yu', 'Gang Fu', 'Haiyang Shen', 'Jiayin Yang', 'Jun Lin', 'Junkai Zhang', 'Kui Zeng', 'Li Yang', 'Hailong Yin', 'Maojia Song', 'Ming Yan', 'Peng Xia', 'Qian Xiao', 'Rui Min', 'Ruixue Ding', 'Runnan Fang', 'Shaowei Chen', 'Shen Huang', 'Shihang Wang', 'Shihao Cai', 'Weizhou Shen', 'Xiaobin Wang', 'Xin Guan', 'Xinyu Geng', 'Yingcheng Shi', 'Yuning Wu', 'Zhuo Chen', 'Zijian Li', 'Yong Jiang'], 'affiliations': ['Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2510.24701.jpg', 'data': {'categories': ['#long_context', '#synthetic', '#dataset', '#benchmark', '#training', '#open_source', '#agi', '#agents'], 'emoji': '🔍', 'ru': {'title': 'Автономный AI-агент для глубоких исследований', 'desc': 'Tongyi DeepResearch - это большая языковая модель с агентными способностями, специализирующаяся на долгосрочных исследовательских задачах с глубоким поиском информации. Модель обучается через end-to-end фреймворк, включающий агентное mid-training и post-training, что позволяет масштабируемо рассуждать и искать информацию в сложных задачах. Ключевая особенность - полностью автоматический пайплайн синтеза данных без дорогостоящей человеческой разметки. При 30.5 миллиардах параметров, из которых активируется только 3.3 миллиарда на токен, модель достигает state-of-the-art результатов на множестве бенчмарков для агентных исследований.'}, 'en': {'title': 'Empowering Autonomous Deep Research with Tongyi DeepResearch', 'desc': 'Tongyi DeepResearch is a large language model designed for complex research tasks that require deep information-seeking capabilities. It utilizes an end-to-end training framework that includes both mid-training and post-training phases to enhance its autonomous research abilities. The model features a fully automated data synthesis pipeline, eliminating the need for expensive human annotations, which allows for scalable training across various tasks. With 30.5 billion parameters, Tongyi DeepResearch achieves top performance on multiple benchmarks, demonstrating its effectiveness in deep research applications.'}, 'zh': {'title': '自主深度研究的未来', 'desc': 'Tongyi DeepResearch 是一种具有自主能力的大型语言模型，专门用于长时间的信息检索研究任务。它通过端到端的训练框架和自动化的数据合成，能够在复杂任务中实现可扩展的推理和信息获取。该模型具有 305 亿个参数，且每个令牌仅激活 33 亿个参数，表现出色，达到了多项深度研究基准的最先进水平。我们将模型、框架和完整解决方案开源，以支持社区的发展。'}}}, {'id': 'https://huggingface.co/papers/2510.24699', 'title': 'AgentFold: Long-Horizon Web Agents with Proactive Context Management', 'url': 'https://huggingface.co/papers/2510.24699', 'abstract': "AgentFold, a novel proactive context management paradigm for LLM-based web agents, achieves superior performance on long-horizon tasks through dynamic context folding, surpassing larger models and proprietary agents.  \t\t\t\t\tAI-generated summary \t\t\t\t LLM-based web agents show immense promise for information seeking, yet their effectiveness on long-horizon tasks is hindered by a fundamental trade-off in context management. Prevailing ReAct-based agents suffer from context saturation as they accumulate noisy, raw histories, while methods that fixedly summarize the full history at each step risk the irreversible loss of critical details. Addressing these, we introduce AgentFold, a novel agent paradigm centered on proactive context management, inspired by the human cognitive process of retrospective consolidation. AgentFold treats its context as a dynamic cognitive workspace to be actively sculpted, rather than a passive log to be filled. At each step, it learns to execute a `folding' operation, which manages its historical trajectory at multiple scales: it can perform granular condensations to preserve vital, fine-grained details, or deep consolidations to abstract away entire multi-step sub-tasks. The results on prominent benchmarks are striking: with simple supervised fine-tuning (without continual pre-training or RL), our AgentFold-30B-A3B agent achieves 36.2% on BrowseComp and 47.3% on BrowseComp-ZH. Notably, this performance not only surpasses or matches open-source models of a dramatically larger scale, such as the DeepSeek-V3.1-671B-A37B, but also surpasses leading proprietary agents like OpenAI's o4-mini.", 'score': 41, 'issue_id': 6668, 'pub_date': '2025-10-28', 'pub_date_card': {'ru': '28 октября', 'en': 'October 28', 'zh': '10月28日'}, 'hash': 'c4184e162b0c817e', 'authors': ['Rui Ye', 'Zhongwang Zhang', 'Kuan Li', 'Huifeng Yin', 'Zhengwei Tao', 'Yida Zhao', 'Liangcai Su', 'Liwen Zhang', 'Zile Qiao', 'Xinyu Wang', 'Pengjun Xie', 'Fei Huang', 'Siheng Chen', 'Jingren Zhou', 'Yong Jiang'], 'affiliations': ['Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2510.24699.jpg', 'data': {'categories': ['#long_context', '#benchmark', '#training', '#agents'], 'emoji': '🗂️', 'ru': {'title': 'Динамическое сворачивание контекста для эффективных веб-агентов', 'desc': 'AgentFold — это новая парадигма управления контекстом для веб-агентов на основе LLM, вдохновленная процессом ретроспективной консолидации в человеческом познании. Вместо пассивного накопления истории действий, агент активно «складывает» свой контекст, выполняя гранулярное сжатие важных деталей или глубокую консолидацию целых подзадач. Такой подход позволяет избежать переполнения контекста шумом и потери критической информации, что характерно для традиционных ReAct-агентов. Модель AgentFold-30B показывает результаты, превосходящие модели в 20 раз большего размера и proprietary агенты вроде OpenAI o4-mini на бенчмарках длинных задач.'}, 'en': {'title': 'Dynamic Context Management for Superior LLM Performance', 'desc': 'AgentFold is a new approach for managing context in large language model (LLM) web agents, designed to improve their performance on long tasks. It addresses the issue of context saturation found in existing agents by dynamically folding context, which helps retain important details while reducing noise. This method mimics human cognitive processes, allowing the agent to actively manage its historical information rather than just accumulating it. The results show that AgentFold outperforms larger models and proprietary agents on key benchmarks with minimal fine-tuning.'}, 'zh': {'title': 'AgentFold：长任务中的上下文管理新范式', 'desc': 'AgentFold是一种新颖的主动上下文管理范式，专为基于大语言模型（LLM）的网络代理设计。它通过动态上下文折叠技术，在长时间任务中表现优异，超越了更大模型和专有代理。与传统的ReAct代理相比，AgentFold有效解决了上下文饱和的问题，避免了重要细节的丢失。通过将上下文视为动态的认知工作空间，AgentFold能够在多个层面上管理历史轨迹，从而实现更高效的信息处理。'}}}, {'id': 'https://huggingface.co/papers/2510.23691', 'title': 'Game-TARS: Pretrained Foundation Models for Scalable Generalist\n  Multimodal Game Agents', 'url': 'https://huggingface.co/papers/2510.23691', 'abstract': 'Game-TARS, a generalist game agent trained with a unified action space, achieves superior performance across various domains and benchmarks through large-scale pre-training and efficient reasoning strategies.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Game-TARS, a generalist game agent trained with a unified, scalable action space anchored to human-aligned native keyboard-mouse inputs. Unlike API- or GUI-based approaches, this paradigm enables large-scale continual pre-training across heterogeneous domains, including OS, web, and simulation games. Game-TARS is pre-trained on over 500B tokens with diverse trajectories and multimodal data. Key techniques include a decaying continual loss to reduce causal confusion and an efficient Sparse-Thinking strategy that balances reasoning depth and inference cost. Experiments show that Game-TARS achieves about 2 times the success rate over the previous sota model on open-world Minecraft tasks, is close to the generality of fresh humans in unseen web 3d games, and outperforms GPT-5, Gemini-2.5-Pro, and Claude-4-Sonnet in FPS benchmarks. Scaling results on training-time and test-time confirm that the unified action space sustains improvements when scaled to cross-game and multimodal data. Our results demonstrate that simple, scalable action representations combined with large-scale pre-training provide a promising path toward generalist agents with broad computer-use abilities.', 'score': 38, 'issue_id': 6668, 'pub_date': '2025-10-27', 'pub_date_card': {'ru': '27 октября', 'en': 'October 27', 'zh': '10月27日'}, 'hash': '9070d487a826ae6f', 'authors': ['Zihao Wang', 'Xujing Li', 'Yining Ye', 'Junjie Fang', 'Haoming Wang', 'Longxiang Liu', 'Shihao Liang', 'Junting Lu', 'Zhiyong Wu', 'Jiazhan Feng', 'Wanjun Zhong', 'Zili Li', 'Yu Wang', 'Yu Miao', 'Bo Zhou', 'Yuanfan Li', 'Hao Wang', 'Zhongkai Zhao', 'Faming Wu', 'Zhengxuan Jiang', 'Weihao Tan', 'Heyuan Yao', 'Shi Yan', 'Xiangyang Li', 'Yitao Liang', 'Yujia Qin', 'Guang Shi'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2510.23691.jpg', 'data': {'categories': ['#agents', '#training', '#games', '#benchmark', '#multimodal', '#reasoning'], 'emoji': '🎮', 'ru': {'title': 'Универсальный игровой агент через единое пространство действий', 'desc': 'Game-TARS — это универсальный игровой агент, обученный на едином пространстве действий, основанном на нативных действиях клавиатуры и мыши. Модель прошла предобучение на более чем 500 миллиардах токенов с использованием разнородных траекторий из игр, операционных систем и веб-приложений. Ключевые техники включают continual loss с затуханием для снижения каузальной путаницы и стратегию Sparse-Thinking для эффективного рассуждения. Агент показал двукратное превосходство над предыдущими SOTA моделями в задачах Minecraft и превзошёл GPT-5, Gemini-2.5-Pro и Claude-4-Sonnet в FPS бенчмарках.'}, 'en': {'title': 'Game-TARS: A Unified Agent for Diverse Gaming Excellence', 'desc': 'Game-TARS is a versatile game agent designed to operate across different gaming environments using a unified action space that mimics human keyboard and mouse inputs. It undergoes extensive pre-training on a massive dataset of over 500 billion tokens, which includes varied gameplay experiences and multimodal information. The agent employs innovative techniques like a decaying continual loss to minimize confusion in decision-making and a Sparse-Thinking strategy to optimize reasoning efficiency. Experimental results indicate that Game-TARS significantly outperforms previous models in various gaming tasks, showcasing its potential as a generalist agent capable of adapting to diverse computer-based activities.'}, 'zh': {'title': 'Game-TARS：通用游戏代理的未来', 'desc': 'Game-TARS是一种通用游戏代理，采用统一的可扩展动作空间进行训练，能够在多个领域和基准测试中表现出色。它通过大规模的持续预训练，结合多模态数据，提升了在操作系统、网页和模拟游戏等异构领域的性能。关键技术包括逐渐减小的持续损失，以减少因果混淆，以及高效的稀疏思维策略，平衡推理深度和推理成本。实验结果表明，Game-TARS在开放世界的Minecraft任务中成功率是之前最佳模型的两倍，并在未见过的网页3D游戏中接近新手玩家的表现。'}}}, {'id': 'https://huggingface.co/papers/2510.23763', 'title': 'RoboOmni: Proactive Robot Manipulation in Omni-modal Context', 'url': 'https://huggingface.co/papers/2510.23763', 'abstract': 'RoboOmni, a Perceiver-Thinker-Talker-Executor framework using end-to-end omni-modal LLMs, improves robotic manipulation by inferring user intentions from spoken dialogue, environmental sounds, and visual cues.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in Multimodal Large Language Models (MLLMs) have driven rapid progress in Vision-Language-Action (VLA) models for robotic manipulation. Although effective in many scenarios, current approaches largely rely on explicit instructions, whereas in real-world interactions, humans rarely issue instructions directly. Effective collaboration requires robots to infer user intentions proactively. In this work, we introduce cross-modal contextual instructions, a new setting where intent is derived from spoken dialogue, environmental sounds, and visual cues rather than explicit commands. To address this new setting, we present RoboOmni, a Perceiver-Thinker-Talker-Executor framework based on end-to-end omni-modal LLMs that unifies intention recognition, interaction confirmation, and action execution. RoboOmni fuses auditory and visual signals spatiotemporally for robust intention recognition, while supporting direct speech interaction. To address the absence of training data for proactive intention recognition in robotic manipulation, we build OmniAction, comprising 140k episodes, 5k+ speakers, 2.4k event sounds, 640 backgrounds, and six contextual instruction types. Experiments in simulation and real-world settings show that RoboOmni surpasses text- and ASR-based baselines in success rate, inference speed, intention recognition, and proactive assistance.', 'score': 32, 'issue_id': 6668, 'pub_date': '2025-10-27', 'pub_date_card': {'ru': '27 октября', 'en': 'October 27', 'zh': '10月27日'}, 'hash': 'b37a82d0c1d4c433', 'authors': ['Siyin Wang', 'Jinlan Fu', 'Feihong Liu', 'Xinzhe He', 'Huangxuan Wu', 'Junhao Shi', 'Kexin Huang', 'Zhaoye Fei', 'Jingjing Gong', 'Zuxuan Wu', 'Yugang Jiang', 'See-Kiong Ng', 'Tat-Seng Chua', 'Xipeng Qiu'], 'affiliations': ['Fudan University', 'National University of Singapore', 'Shanghai Innovation Institute'], 'pdf_title_img': 'assets/pdf/title_img/2510.23763.jpg', 'data': {'categories': ['#agents', '#dataset', '#optimization', '#games', '#multimodal', '#interpretability'], 'emoji': '🤖', 'ru': {'title': 'Робот, который понимает намерения без прямых команд', 'desc': 'Статья представляет RoboOmni — систему для роботизированной манипуляции, которая понимает намерения пользователя из разговоров, звуков окружения и визуальных сигналов, а не из явных команд. Фреймворк основан на omni-modal LLM и объединяет распознавание намерений, подтверждение через диалог и выполнение действий. Авторы создали датасет OmniAction с 140 тысячами эпизодов для обучения проактивному распознаванию намерений. Эксперименты показали, что RoboOmni превосходит baseline-модели по точности, скорости и способности к проактивной помощи.'}, 'en': {'title': 'RoboOmni: Understanding Intentions for Smarter Robot Interaction', 'desc': 'RoboOmni is a new framework that enhances robotic manipulation by understanding user intentions through various inputs like speech, sounds, and visual information. Unlike traditional models that depend on clear instructions, RoboOmni can infer what users want based on context, making it more effective in real-world situations. It combines different types of data using advanced multimodal large language models (MLLMs) to recognize intentions and execute actions seamlessly. The framework is trained on a large dataset called OmniAction, which helps it perform better in both simulated and real environments.'}, 'zh': {'title': 'RoboOmni：智能机器人主动理解用户意图的全新框架', 'desc': 'RoboOmni是一个基于端到端全模态大语言模型的框架，旨在通过推断用户意图来改善机器人操作。该框架结合了语音对话、环境声音和视觉线索，能够在没有明确指令的情况下进行有效的协作。RoboOmni通过跨模态上下文指令来识别意图，并支持直接的语音交互。实验结果表明，RoboOmni在成功率、推理速度和主动协助方面优于传统的文本和自动语音识别基线。'}}}, {'id': 'https://huggingface.co/papers/2510.24717', 'title': 'Uniform Discrete Diffusion with Metric Path for Video Generation', 'url': 'https://huggingface.co/papers/2510.24717', 'abstract': 'URSA, a discrete generative model, bridges the gap with continuous approaches in video generation by using a Linearized Metric Path and Resolution-dependent Timestep Shifting, achieving high-resolution and long-duration synthesis with fewer inference steps.  \t\t\t\t\tAI-generated summary \t\t\t\t Continuous-space video generation has advanced rapidly, while discrete approaches lag behind due to error accumulation and long-context inconsistency. In this work, we revisit discrete generative modeling and present Uniform discRete diffuSion with metric pAth (URSA), a simple yet powerful framework that bridges the gap with continuous approaches for the scalable video generation. At its core, URSA formulates the video generation task as an iterative global refinement of discrete spatiotemporal tokens. It integrates two key designs: a Linearized Metric Path and a Resolution-dependent Timestep Shifting mechanism. These designs enable URSA to scale efficiently to high-resolution image synthesis and long-duration video generation, while requiring significantly fewer inference steps. Additionally, we introduce an asynchronous temporal fine-tuning strategy that unifies versatile tasks within a single model, including interpolation and image-to-video generation. Extensive experiments on challenging video and image generation benchmarks demonstrate that URSA consistently outperforms existing discrete methods and achieves performance comparable to state-of-the-art continuous diffusion methods. Code and models are available at https://github.com/baaivision/URSA', 'score': 27, 'issue_id': 6670, 'pub_date': '2025-10-28', 'pub_date_card': {'ru': '28 октября', 'en': 'October 28', 'zh': '10月28日'}, 'hash': '2191a1dd1f4af127', 'authors': ['Haoge Deng', 'Ting Pan', 'Fan Zhang', 'Yang Liu', 'Zhuoyan Luo', 'Yufeng Cui', 'Wenxuan Wang', 'Chunhua Shen', 'Shiguang Shan', 'Zhaoxiang Zhang', 'Xinlong Wang'], 'affiliations': ['Beijing Academy of Artificial Intelligence', 'Key Laboratory of Intelligent Information Processing, ICT, CAS', 'National Laboratory of Pattern Recognition, CASIA', 'University of Chinese Academy of Sciences', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2510.24717.jpg', 'data': {'categories': ['#diffusion', '#benchmark', '#architecture', '#video'], 'emoji': '🎥', 'ru': {'title': 'URSA: Новая эра дискретной генерации видео', 'desc': 'В статье представлена модель URSA, которая улучшает генерацию видео с использованием дискретных подходов, приближая их к непрерывным методам. URSA использует линейный метрический путь и механизм сдвига временных шагов, что позволяет создавать видео высокого разрешения и длительности с меньшим количеством шагов вывода. Также предложена стратегия асинхронной временной настройки, которая объединяет различные задачи в одной модели, такие как интерполяция и генерация видео из изображений. Эксперименты показывают, что URSA превосходит существующие дискретные методы и достигает результатов, сопоставимых с передовыми непрерывными методами диффузии.'}, 'en': {'title': 'URSA: Bridging Discrete and Continuous Video Generation', 'desc': 'URSA is a new discrete generative model designed to improve video generation by addressing the limitations of traditional discrete methods. It uses a Linearized Metric Path and Resolution-dependent Timestep Shifting to enhance the quality and duration of generated videos while minimizing the number of inference steps needed. The model refines discrete spatiotemporal tokens iteratively, allowing for high-resolution outputs and long video sequences. Additionally, URSA incorporates an asynchronous temporal fine-tuning strategy to handle various tasks like interpolation and image-to-video generation within a single framework.'}, 'zh': {'title': 'URSA：高效的视频生成新方法', 'desc': 'URSA是一种离散生成模型，通过线性度量路径和分辨率依赖的时间步移机制，缩小了视频生成中离散方法与连续方法之间的差距。它将视频生成任务视为离散时空标记的迭代全局优化，从而实现高分辨率和长时长的视频合成。URSA的设计使其在生成高质量图像和长时间视频时，所需的推理步骤显著减少。此外，URSA还引入了一种异步时间微调策略，能够在单一模型中统一多种任务，包括插值和图像到视频的生成。'}}}, {'id': 'https://huggingface.co/papers/2510.24563', 'title': 'OSWorld-MCP: Benchmarking MCP Tool Invocation In Computer-Use Agents', 'url': 'https://huggingface.co/papers/2510.24563', 'abstract': "OSWorld-MCP is a benchmark that evaluates multimodal agents' tool invocation, GUI operation, and decision-making abilities, highlighting the importance of assessing tool usage in real-world scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t With advances in decision-making and reasoning capabilities, multimodal agents show strong potential in computer application scenarios. Past evaluations have mainly assessed GUI interaction skills, while tool invocation abilities, such as those enabled by the Model Context Protocol (MCP), have been largely overlooked. Comparing agents with integrated tool invocation to those evaluated only on GUI interaction is inherently unfair. We present OSWorld-MCP, the first comprehensive and fair benchmark for assessing computer-use agents' tool invocation, GUI operation, and decision-making abilities in a real-world environment. We design a novel automated code-generation pipeline to create tools and combine them with a curated selection from existing tools. Rigorous manual validation yields 158 high-quality tools (covering 7 common applications), each verified for correct functionality, practical applicability, and versatility. Extensive evaluations of state-of-the-art multimodal agents on OSWorld-MCP show that MCP tools generally improve task success rates (e.g., from 8.3% to 20.4% for OpenAI o3 at 15 steps, from 40.1% to 43.3% for Claude 4 Sonnet at 50 steps), underscoring the importance of assessing tool invocation capabilities. However, even the strongest models have relatively low tool invocation rates, Only 36.3%, indicating room for improvement and highlighting the benchmark's challenge. By explicitly measuring MCP tool usage skills, OSWorld-MCP deepens understanding of multimodal agents and sets a new standard for evaluating performance in complex, tool-assisted environments. Our code, environment, and data are publicly available at https://osworld-mcp.github.io.", 'score': 17, 'issue_id': 6667, 'pub_date': '2025-10-28', 'pub_date_card': {'ru': '28 октября', 'en': 'October 28', 'zh': '10月28日'}, 'hash': '31a5246f81b64759', 'authors': ['Hongrui Jia', 'Jitong Liao', 'Xi Zhang', 'Haiyang Xu', 'Tianbao Xie', 'Chaoya Jiang', 'Ming Yan', 'Si Liu', 'Wei Ye', 'Fei Huang'], 'affiliations': ['Beijing Zhongguancun Academy', 'Peking University', 'Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2510.24563.jpg', 'data': {'categories': ['#multimodal', '#reasoning', '#benchmark', '#optimization', '#open_source', '#agents'], 'emoji': '🛠️', 'ru': {'title': 'Новый стандарт оценки AI-агентов: не только GUI, но и умение пользоваться инструментами', 'desc': 'OSWorld-MCP — это первый комплексный бенчмарк для оценки мультимодальных агентов в реальных условиях, который измеряет способности к вызову инструментов, работе с графическим интерфейсом и принятию решений. Исследователи создали 158 высококачественных инструментов для 7 популярных приложений с помощью автоматизированной генерации кода и ручной проверки. Эксперименты показали, что использование MCP-инструментов улучшает показатели успеха задач для современных моделей, например, с 8.3% до 20.4% для OpenAI o3. Однако даже лучшие модели используют инструменты только в 36.3% случаев, что указывает на значительный потенциал для улучшения.'}, 'en': {'title': 'Revolutionizing Multimodal Agent Evaluation with OSWorld-MCP', 'desc': "OSWorld-MCP is a new benchmark designed to evaluate multimodal agents on their ability to invoke tools, operate GUIs, and make decisions in real-world scenarios. It addresses the gap in previous assessments that primarily focused on GUI interactions, providing a fair comparison by including tool invocation capabilities. The benchmark features a unique automated code-generation pipeline that creates and validates 158 high-quality tools for common applications. Results show that while integrating MCP tools significantly improves task success rates, there is still a need for enhancement in tool invocation rates among leading models, emphasizing the benchmark's importance in advancing multimodal agent evaluation."}, 'zh': {'title': '评估多模态智能体的新标准', 'desc': 'OSWorld-MCP是一个基准测试，旨在评估多模态智能体在工具调用、图形用户界面（GUI）操作和决策能力方面的表现。该研究强调了在真实场景中评估工具使用的重要性，尤其是通过模型上下文协议（MCP）实现的工具调用能力。通过设计自动化代码生成管道，研究团队创建了158个高质量工具，并对其功能和适用性进行了严格验证。评估结果表明，使用MCP工具的智能体在任务成功率上有显著提升，显示出工具调用能力的关键性。'}}}, {'id': 'https://huggingface.co/papers/2510.24657', 'title': 'Group Relative Attention Guidance for Image Editing', 'url': 'https://huggingface.co/papers/2510.24657', 'abstract': "Group Relative Attention Guidance enhances image editing quality by modulating token deltas in Diffusion-in-Transformer models, providing fine-grained control over editing intensity.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, image editing based on Diffusion-in-Transformer models has undergone rapid development. However, existing editing methods often lack effective control over the degree of editing, limiting their ability to achieve more customized results. To address this limitation, we investigate the MM-Attention mechanism within the DiT model and observe that the Query and Key tokens share a bias vector that is only layer-dependent. We interpret this bias as representing the model's inherent editing behavior, while the delta between each token and its corresponding bias encodes the content-specific editing signals. Based on this insight, we propose Group Relative Attention Guidance, a simple yet effective method that reweights the delta values of different tokens to modulate the focus of the model on the input image relative to the editing instruction, enabling continuous and fine-grained control over editing intensity without any tuning. Extensive experiments conducted on existing image editing frameworks demonstrate that GRAG can be integrated with as few as four lines of code, consistently enhancing editing quality. Moreover, compared to the commonly used Classifier-Free Guidance, GRAG achieves smoother and more precise control over the degree of editing. Our code will be released at https://github.com/little-misfit/GRAG-Image-Editing.", 'score': 16, 'issue_id': 6668, 'pub_date': '2025-10-28', 'pub_date_card': {'ru': '28 октября', 'en': 'October 28', 'zh': '10月28日'}, 'hash': '05adb00252472e87', 'authors': ['Xuanpu Zhang', 'Xuesong Niu', 'Ruidong Chen', 'Dan Song', 'Jianhao Zeng', 'Penghui Du', 'Haoxiang Cao', 'Kai Wu', 'An-an Liu'], 'affiliations': ['Kolors Team, Kuaishou Technology', 'Tianjin University'], 'pdf_title_img': 'assets/pdf/title_img/2510.24657.jpg', 'data': {'categories': ['#architecture', '#open_source', '#training', '#optimization', '#cv', '#diffusion'], 'emoji': '🎚️', 'ru': {'title': 'Точная настройка силы редактирования через модуляцию токенов', 'desc': 'Исследователи предложили метод Group Relative Attention Guidance (GRAG) для улучшения контроля над интенсивностью редактирования изображений в Diffusion-in-Transformer моделях. Они обнаружили, что Query и Key токены в механизме MM-Attention имеют общий bias-вектор, который представляет базовое поведение модели при редактировании. GRAG использует это наблюдение для модуляции дельт между токенами и их bias, что позволяет тонко управлять фокусом модели на исходном изображении относительно инструкции редактирования. Метод интегрируется всего четырьмя строками кода и обеспечивает более плавный и точный контроль по сравнению с классическим Classifier-Free Guidance.'}, 'en': {'title': 'Fine-Tune Your Edits with Group Relative Attention Guidance!', 'desc': "This paper introduces Group Relative Attention Guidance (GRAG), a method that improves image editing quality in Diffusion-in-Transformer models by adjusting token deltas. The authors identify that the Query and Key tokens in the model share a layer-dependent bias, which influences the model's editing behavior. By reweighting the delta values of tokens, GRAG allows for fine-tuned control over the intensity of image edits based on specific instructions. The results show that GRAG can be easily integrated into existing frameworks and provides smoother and more precise editing compared to traditional methods."}, 'zh': {'title': '群体相对注意力引导提升图像编辑质量', 'desc': '本文提出了一种名为群体相对注意力引导（Group Relative Attention Guidance, GRAG）的方法，旨在提高基于扩散-变换器模型的图像编辑质量。通过调节不同标记的增量值，GRAG能够实现对编辑强度的细粒度控制，克服了现有方法在编辑程度上的局限性。研究表明，GRAG可以与现有图像编辑框架轻松集成，并且只需少量代码即可实现。实验结果显示，GRAG在编辑质量上优于常用的无分类器引导方法，提供了更平滑和精确的编辑控制。'}}}, {'id': 'https://huggingface.co/papers/2510.24697', 'title': 'WebLeaper: Empowering Efficiency and Efficacy in WebAgent via Enabling\n  Info-Rich Seeking', 'url': 'https://huggingface.co/papers/2510.24697', 'abstract': 'WebLeaper framework improves information seeking efficiency and effectiveness by constructing high-coverage tasks and generating efficient solution trajectories using tree-structured reasoning and curated Wikipedia tables.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Model (LLM)-based agents have emerged as a transformative approach for open-ended problem solving, with information seeking (IS) being a core capability that enables autonomous reasoning and decision-making. While prior research has largely focused on improving retrieval depth, we observe that current IS agents often suffer from low search efficiency, which in turn constrains overall performance. A key factor underlying this inefficiency is the sparsity of target entities in training tasks, which limits opportunities for agents to learn and generalize efficient search behaviors. To address these challenges, we propose WebLeaper, a framework for constructing high-coverage IS tasks and generating efficient solution trajectories. We formulate IS as a tree-structured reasoning problem, enabling a substantially larger set of target entities to be embedded within a constrained context. Leveraging curated Wikipedia tables, we propose three variants for synthesizing IS tasks, Basic, Union, and Reverse-Union, to systematically increase both IS efficiency and efficacy. Finally, we curate training trajectories by retaining only those that are simultaneously accurate and efficient, ensuring that the model is optimized for both correctness and search performance. Extensive experiments on both basic and comprehensive settings, conducted on five IS benchmarks, BrowserComp, GAIA, xbench-DeepSearch, WideSearch, and Seal-0, demonstrate that our method consistently achieves improvements in both effectiveness and efficiency over strong baselines.', 'score': 15, 'issue_id': 6668, 'pub_date': '2025-10-28', 'pub_date_card': {'ru': '28 октября', 'en': 'October 28', 'zh': '10月28日'}, 'hash': '4986235d4df6a5c7', 'authors': ['Zhengwei Tao', 'Haiyang Shen', 'Baixuan Li', 'Wenbiao Yin', 'Jialong Wu', 'Kuan Li', 'Zhongwang Zhang', 'Huifeng Yin', 'Rui Ye', 'Liwen Zhang', 'Xinyu Wang', 'Pengjun Xie', 'Jingren Zhou', 'Yong Jiang'], 'affiliations': ['Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2510.24697.jpg', 'data': {'categories': ['#dataset', '#survey', '#training', '#optimization', '#benchmark', '#agents', '#reasoning'], 'emoji': '🌳', 'ru': {'title': 'WebLeaper: Эффективный поиск информации через древовидное рассуждение', 'desc': 'Статья представляет WebLeaper — фреймворк для улучшения эффективности информационного поиска агентами на основе LLM. Авторы формулируют задачу поиска как древовидную структуру рассуждений, что позволяет охватить больше целевых сущностей в ограниченном контексте. Используя таблицы из Wikipedia, предложены три варианта синтеза задач (Basic, Union и Reverse-Union) для повышения качества поиска. Эксперименты на пяти бенчмарках показали улучшение как точности, так и скорости поиска по сравнению с базовыми методами.'}, 'en': {'title': 'WebLeaper: Boosting Information Seeking Efficiency with Tree-Structured Reasoning', 'desc': 'The WebLeaper framework enhances the efficiency and effectiveness of information seeking (IS) by creating high-coverage tasks and generating efficient solution paths through tree-structured reasoning. It addresses the issue of low search efficiency in current IS agents, which often struggle due to the limited availability of target entities in training tasks. By utilizing curated Wikipedia tables, WebLeaper synthesizes IS tasks in three variants to improve both the efficiency and efficacy of the search process. Extensive experiments show that this approach leads to significant improvements in performance across multiple IS benchmarks.'}, 'zh': {'title': 'WebLeaper：提升信息检索效率与效果的框架', 'desc': 'WebLeaper框架通过构建高覆盖率的信息检索任务和生成高效的解决方案路径，提升了信息检索的效率和效果。该框架将信息检索视为一个树结构推理问题，从而在有限的上下文中嵌入更多的目标实体。通过利用精心策划的维基百科表格，WebLeaper提出了三种合成信息检索任务的变体，以系统性地提高信息检索的效率和有效性。实验结果表明，该方法在多个基准测试中相较于强基线模型在效果和效率上均有显著提升。'}}}, {'id': 'https://huggingface.co/papers/2510.24694', 'title': 'Repurposing Synthetic Data for Fine-grained Search Agent Supervision', 'url': 'https://huggingface.co/papers/2510.24694', 'abstract': 'Entity-aware Group Relative Policy Optimization (E-GRPO) enhances search agents by incorporating entity information into the reward function, improving accuracy and efficiency in knowledge-intensive tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t LLM-based search agents are increasingly trained on entity-centric synthetic data to solve complex, knowledge-intensive tasks. However, prevailing training methods like Group Relative Policy Optimization (GRPO) discard this rich entity information, relying instead on sparse, outcome-based rewards. This critical limitation renders them unable to distinguish informative "near-miss" samples-those with substantially correct reasoning but a flawed final answer-from complete failures, thus discarding valuable learning signals. We address this by leveraging the very entities discarded during training. Our empirical analysis reveals a strong positive correlation between the number of ground-truth entities identified during an agent\'s reasoning process and final answer accuracy. Building on this insight, we introduce Entity-aware Group Relative Policy Optimization (E-GRPO), a novel framework that formulates a dense entity-aware reward function. E-GRPO assigns partial rewards to incorrect samples proportional to their entity match rate, enabling the model to effectively learn from these "near-misses". Experiments on diverse question-answering (QA) and deep research benchmarks show that E-GRPO consistently and significantly outperforms the GRPO baseline. Furthermore, our analysis reveals that E-GRPO not only achieves superior accuracy but also induces more efficient reasoning policies that require fewer tool calls, demonstrating a more effective and sample-efficient approach to aligning search agents.', 'score': 15, 'issue_id': 6668, 'pub_date': '2025-10-28', 'pub_date_card': {'ru': '28 октября', 'en': 'October 28', 'zh': '10月28日'}, 'hash': '57a3b452eab7371c', 'authors': ['Yida Zhao', 'Kuan Li', 'Xixi Wu', 'Liwen Zhang', 'Dingchu Zhang', 'Baixuan Li', 'Maojia Song', 'Zhuo Chen', 'Chenxi Wang', 'Xinyu Wang', 'Kewei Tu', 'Pengjun Xie', 'Jingren Zhou', 'Yong Jiang'], 'affiliations': ['Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2510.24694.jpg', 'data': {'categories': ['#training', '#optimization', '#rl', '#agents', '#reasoning'], 'emoji': '🎯', 'ru': {'title': 'Учимся на «почти правильных» ответах: награды за найденные сущности', 'desc': 'Статья представляет Entity-aware Group Relative Policy Optimization (E-GRPO) — улучшенный метод обучения LLM-агентов для решения задач, требующих поиска и обработки знаний. В отличие от традиционного GRPO, который использует только финальный результат для обучения, E-GRPO учитывает промежуточную информацию об упомянутых сущностях. Метод назначает частичные награды за «почти правильные» ответы пропорционально числу корректно найденных сущностей, что позволяет эффективнее учиться на ошибках. Эксперименты показывают, что E-GRPO превосходит базовый GRPO по точности и эффективности, требуя меньше обращений к инструментам поиска.'}, 'en': {'title': 'Enhancing Search Agents with Entity-Aware Learning', 'desc': 'Entity-aware Group Relative Policy Optimization (E-GRPO) improves the training of search agents by integrating entity information into the reward system, which enhances their performance on complex tasks. Traditional methods like Group Relative Policy Optimization (GRPO) overlook valuable entity data, leading to a loss of learning opportunities from near-miss samples. E-GRPO addresses this by providing partial rewards based on the match rate of identified entities, allowing agents to learn from their mistakes more effectively. Experimental results show that E-GRPO not only boosts accuracy but also promotes more efficient reasoning, requiring fewer resources to achieve better outcomes.'}, 'zh': {'title': '实体感知优化，提升搜索代理的智能！', 'desc': 'E-GRPO（实体感知群体相对策略优化）通过将实体信息纳入奖励函数，增强了搜索代理的能力，从而提高了在知识密集型任务中的准确性和效率。传统的训练方法如GRPO忽视了丰富的实体信息，依赖稀疏的基于结果的奖励，导致无法有效区分有价值的“近乎正确”样本。我们通过利用训练中被丢弃的实体，提出了一种新的密集实体感知奖励函数，使模型能够从这些“近乎正确”的样本中有效学习。实验结果表明，E-GRPO在多种问答和深度研究基准上显著优于GRPO基线，且在提高准确性的同时，减少了工具调用次数，展现出更高效的推理策略。'}}}, {'id': 'https://huggingface.co/papers/2510.24693', 'title': 'STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D\n  Intelligence', 'url': 'https://huggingface.co/papers/2510.24693', 'abstract': 'STAR-Bench measures audio 4D intelligence by evaluating sound dynamics in time and 3D space, revealing gaps in fine-grained perceptual reasoning among existing models.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite rapid progress in Multi-modal Large Language Models and Large Audio-Language Models, existing audio benchmarks largely test semantics that can be recovered from text captions, masking deficits in fine-grained perceptual reasoning. We formalize audio 4D intelligence that is defined as reasoning over sound dynamics in time and 3D space, and introduce STAR-Bench to measure it. STAR-Bench combines a Foundational Acoustic Perception setting (six attributes under absolute and relative regimes) with a Holistic Spatio-Temporal Reasoning setting that includes segment reordering for continuous and discrete processes and spatial tasks spanning static localization, multi-source relations, and dynamic trajectories. Our data curation pipeline uses two methods to ensure high-quality samples. For foundational tasks, we use procedurally synthesized and physics-simulated audio. For holistic data, we follow a four-stage process that includes human annotation and final selection based on human performance. Unlike prior benchmarks where caption-only answering reduces accuracy slightly, STAR-Bench induces far larger drops (-31.5\\% temporal, -35.2\\% spatial), evidencing its focus on linguistically hard-to-describe cues. Evaluating 19 models reveals substantial gaps compared with humans and a capability hierarchy: closed-source models are bottlenecked by fine-grained perception, while open-source models lag across perception, knowledge, and reasoning. Our STAR-Bench provides critical insights and a clear path forward for developing future models with a more robust understanding of the physical world.', 'score': 15, 'issue_id': 6669, 'pub_date': '2025-10-28', 'pub_date_card': {'ru': '28 октября', 'en': 'October 28', 'zh': '10月28日'}, 'hash': 'cb1bd8a9c7ac0b88', 'authors': ['Zihan Liu', 'Zhikang Niu', 'Qiuyang Xiao', 'Zhisheng Zheng', 'Ruoqi Yuan', 'Yuhang Zang', 'Yuhang Cao', 'Xiaoyi Dong', 'Jianze Liang', 'Xie Chen', 'Leilei Sun', 'Dahua Lin', 'Jiaqi Wang'], 'affiliations': ['Beihang University', 'Shanghai AI Laboratory', 'Shanghai Innovation Institute', 'Shanghai Jiao Tong University', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2510.24693.jpg', 'data': {'categories': ['#interpretability', '#data', '#audio', '#reasoning', '#benchmark', '#open_source'], 'emoji': '🎧', 'ru': {'title': '4D аудио-интеллект: новый уровень понимания звука в пространстве и времени', 'desc': 'Статья представляет STAR-Bench — новый бенчмарк для оценки «4D аудио-интеллекта» у мультимодальных LLM, то есть способности моделей понимать динамику звука во времени и трёхмерном пространстве. Существующие аудио-бенчмарки тестируют в основном семантику, которую можно восстановить из текстовых описаний, скрывая проблемы с детальным перцептивным анализом. STAR-Bench включает задачи на базовое акустическое восприятие (шесть атрибутов) и комплексное пространственно-временное рассуждение (переупорядочивание сегментов, локализацию, траектории). Тестирование 19 моделей показало значительное отставание от людей: закрытые модели испытывают трудности с детальным восприятием, а открытые отстают во всех аспектах — восприятии, знаниях и рассуждениях.'}, 'en': {'title': 'STAR-Bench: Advancing Audio 4D Intelligence Evaluation', 'desc': 'The paper introduces STAR-Bench, a new benchmark designed to evaluate audio 4D intelligence, which involves understanding sound dynamics over time and in three-dimensional space. It highlights the limitations of existing audio benchmarks that primarily focus on semantic understanding derived from text, thereby neglecting fine-grained perceptual reasoning. STAR-Bench incorporates both foundational acoustic perception and holistic spatio-temporal reasoning tasks, revealing significant performance gaps between current models and human capabilities. The findings indicate that both closed-source and open-source models struggle with fine-grained perception, underscoring the need for improved model development to enhance understanding of complex auditory environments.'}, 'zh': {'title': 'STAR-Bench：音频四维智能的新标准', 'desc': 'STAR-Bench 是一个用于测量音频四维智能的基准，评估声音在时间和三维空间中的动态表现。该研究揭示了现有模型在细粒度感知推理方面的不足，尤其是在多模态大语言模型和大音频语言模型的快速发展背景下。STAR-Bench 结合了基础声学感知和整体时空推理的设置，使用高质量样本来确保评估的准确性。通过对19个模型的评估，发现它们与人类的表现存在显著差距，强调了未来模型在理解物理世界方面的改进方向。'}}}, {'id': 'https://huggingface.co/papers/2510.24711', 'title': 'Routing Matters in MoE: Scaling Diffusion Transformers with Explicit\n  Routing Guidance', 'url': 'https://huggingface.co/papers/2510.24711', 'abstract': 'ProMoE, an MoE framework with conditional and prototypical routing, enhances expert specialization in Diffusion Transformers, achieving state-of-the-art performance on ImageNet.  \t\t\t\t\tAI-generated summary \t\t\t\t Mixture-of-Experts (MoE) has emerged as a powerful paradigm for scaling model capacity while preserving computational efficiency. Despite its notable success in large language models (LLMs), existing attempts to apply MoE to Diffusion Transformers (DiTs) have yielded limited gains. We attribute this gap to fundamental differences between language and visual tokens. Language tokens are semantically dense with pronounced inter-token variation, while visual tokens exhibit spatial redundancy and functional heterogeneity, hindering expert specialization in vision MoE. To this end, we present ProMoE, an MoE framework featuring a two-step router with explicit routing guidance that promotes expert specialization. Specifically, this guidance encourages the router to partition image tokens into conditional and unconditional sets via conditional routing according to their functional roles, and refine the assignments of conditional image tokens through prototypical routing with learnable prototypes based on semantic content. Moreover, the similarity-based expert allocation in latent space enabled by prototypical routing offers a natural mechanism for incorporating explicit semantic guidance, and we validate that such guidance is crucial for vision MoE. Building on this, we propose a routing contrastive loss that explicitly enhances the prototypical routing process, promoting intra-expert coherence and inter-expert diversity. Extensive experiments on ImageNet benchmark demonstrate that ProMoE surpasses state-of-the-art methods under both Rectified Flow and DDPM training objectives. Code and models will be made publicly available.', 'score': 14, 'issue_id': 6668, 'pub_date': '2025-10-28', 'pub_date_card': {'ru': '28 октября', 'en': 'October 28', 'zh': '10月28日'}, 'hash': '628b0204fe7a35ef', 'authors': ['Yujie Wei', 'Shiwei Zhang', 'Hangjie Yuan', 'Yujin Han', 'Zhekai Chen', 'Jiayu Wang', 'Difan Zou', 'Xihui Liu', 'Yingya Zhang', 'Yu Liu', 'Hongming Shan'], 'affiliations': ['Fudan University', 'MMLab', 'The University of Hong Kong', 'Tongyi Lab, Alibaba Group', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2510.24711.jpg', 'data': {'categories': ['#architecture', '#open_source', '#optimization', '#cv', '#benchmark', '#diffusion'], 'emoji': '🎨', 'ru': {'title': 'Специализация экспертов в диффузионных моделях через двухэтапную маршрутизацию', 'desc': 'Статья представляет ProMoE — новый подход к использованию Mixture-of-Experts в Diffusion Transformers для генерации изображений. Авторы обнаружили, что визуальные токены отличаются от языковых избыточностью и функциональной неоднородностью, что мешает специализации экспертов. Предложенная двухэтапная маршрутизация сначала разделяет токены на условные и безусловные, а затем использует прототипы для уточнения назначения на основе семантического содержания. ProMoE достигает лучших результатов на ImageNet благодаря контрастивной функции потерь, которая усиливает когерентность внутри экспертов и разнообразие между ними.'}, 'en': {'title': 'ProMoE: Enhancing Expert Specialization in Vision with Smart Routing', 'desc': "ProMoE is a new framework that improves the Mixture-of-Experts (MoE) approach in Diffusion Transformers, focusing on better expert specialization. It introduces a two-step routing mechanism that categorizes image tokens into conditional and unconditional sets, enhancing how experts are utilized based on the tokens' roles. By using prototypical routing, the framework refines the assignment of tokens to experts, ensuring that similar tokens are grouped together for more effective learning. The results show that ProMoE achieves top performance on the ImageNet dataset, demonstrating its effectiveness in visual tasks compared to previous methods."}, 'zh': {'title': 'ProMoE：提升扩散变换器专家专业化的创新框架', 'desc': 'ProMoE是一种混合专家（MoE）框架，旨在提高扩散变换器（Diffusion Transformers）中的专家专业化。该框架采用了条件和原型路由的两步路由机制，能够有效地将图像标记分为条件和无条件集合，从而优化专家的分配。通过原型路由，ProMoE能够根据语义内容对条件图像标记进行精细分配，增强了专家之间的多样性和内部一致性。实验结果表明，ProMoE在ImageNet基准测试中表现优于现有的最先进方法。'}}}, {'id': 'https://huggingface.co/papers/2510.24698', 'title': 'ParallelMuse: Agentic Parallel Thinking for Deep Information Seeking', 'url': 'https://huggingface.co/papers/2510.24698', 'abstract': 'ParallelMuse enhances problem-solving by efficiently reusing paths and compressing reasoning in deep information-seeking agents, improving performance and reducing token consumption.  \t\t\t\t\tAI-generated summary \t\t\t\t Parallel thinking expands exploration breadth, complementing the deep exploration of information-seeking (IS) agents to further enhance problem-solving capability. However, conventional parallel thinking faces two key challenges in this setting: inefficiency from repeatedly rolling out from scratch, and difficulty in integrating long-horizon reasoning trajectories during answer generation, as limited context capacity prevents full consideration of the reasoning process. To address these issues, we propose ParallelMuse, a two-stage paradigm designed for deep IS agents. The first stage, Functionality-Specified Partial Rollout, partitions generated sequences into functional regions and performs uncertainty-guided path reuse and branching to enhance exploration efficiency. The second stage, Compressed Reasoning Aggregation, exploits reasoning redundancy to losslessly compress information relevant to answer derivation and synthesize a coherent final answer. Experiments across multiple open-source agents and benchmarks demonstrate up to 62% performance improvement with a 10--30% reduction in exploratory token consumption.', 'score': 14, 'issue_id': 6668, 'pub_date': '2025-10-28', 'pub_date_card': {'ru': '28 октября', 'en': 'October 28', 'zh': '10月28日'}, 'hash': '5622c4489fe55493', 'authors': ['Baixuan Li', 'Dingchu Zhang', 'Jialong Wu', 'Wenbiao Yin', 'Zhengwei Tao', 'Yida Zhao', 'Liwen Zhang', 'Haiyang Shen', 'Runnan Fang', 'Pengjun Xie', 'Jingren Zhou', 'Yong Jiang'], 'affiliations': ['Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2510.24698.jpg', 'data': {'categories': ['#training', '#optimization', '#benchmark', '#agents', '#reasoning'], 'emoji': '🔀', 'ru': {'title': 'Параллельное мышление для AI-агентов: быстрее думаем, меньше тратим', 'desc': 'Исследователи представили ParallelMuse — систему, которая улучшает работу AI-агентов при решении сложных задач, требующих глубокого поиска информации. Метод работает в два этапа: сначала умно переиспользует уже найденные пути решения вместо повторного поиска с нуля, а затем сжимает длинные цепочки рассуждений без потери важной информации. Это позволяет агентам параллельно исследовать больше вариантов решения и лучше их комбинировать при формировании финального ответа. В результате производительность улучшается до 62%, при этом расход токенов снижается на 10-30%.'}, 'en': {'title': 'Enhancing Problem-Solving Efficiency with ParallelMuse', 'desc': 'ParallelMuse is a novel approach that improves the efficiency of deep information-seeking agents by reusing reasoning paths and compressing the information they generate. It addresses the challenges of traditional parallel thinking, which often leads to inefficiencies and difficulties in managing long-term reasoning. The method consists of two main stages: the first enhances exploration by reusing paths based on their functionality, while the second compresses reasoning to streamline the answer generation process. Experiments show that ParallelMuse can significantly boost performance while reducing the number of tokens used during exploration.'}, 'zh': {'title': 'ParallelMuse：高效推理与探索的结合', 'desc': 'ParallelMuse 是一种增强深度信息搜索代理问题解决能力的方法。它通过高效重用路径和压缩推理过程，显著提高了性能并减少了令牌消耗。该方法分为两个阶段：第一阶段通过不确定性引导的路径重用来提高探索效率，第二阶段则利用推理冗余来无损压缩与答案推导相关的信息。实验结果表明，ParallelMuse 在多个开源代理和基准测试中实现了高达62%的性能提升，同时探索性令牌消耗减少了10%到30%。'}}}, {'id': 'https://huggingface.co/papers/2510.24695', 'title': 'AgentFrontier: Expanding the Capability Frontier of LLM Agents with\n  ZPD-Guided Data Synthesis', 'url': 'https://huggingface.co/papers/2510.24695', 'abstract': "A ZPD-guided data synthesis approach enhances large language model capabilities by training them on tasks just beyond their current abilities, leading to state-of-the-art performance on complex benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Training large language model agents on tasks at the frontier of their capabilities is key to unlocking advanced reasoning. We introduce a data synthesis approach inspired by the educational theory of the Zone of Proximal Development (ZPD), which defines this frontier as tasks an LLM cannot solve alone but can master with guidance. To operationalize this, we present the AgentFrontier Engine, an automated pipeline that synthesizes high-quality, multidisciplinary data situated precisely within the LLM's ZPD. This engine supports both continued pre-training with knowledge-intensive data and targeted post-training on complex reasoning tasks. From the same framework, we derive the ZPD Exam, a dynamic and automated benchmark designed to evaluate agent capabilities on these frontier tasks. We train AgentFrontier-30B-A3B model on our synthesized data, which achieves state-of-the-art results on demanding benchmarks like Humanity's Last Exam, even surpassing some leading proprietary agents. Our work demonstrates that a ZPD-guided approach to data synthesis offers a scalable and effective path toward building more capable LLM agents.", 'score': 14, 'issue_id': 6668, 'pub_date': '2025-10-28', 'pub_date_card': {'ru': '28 октября', 'en': 'October 28', 'zh': '10月28日'}, 'hash': '041127f22b52b58f', 'authors': ['Xuanzhong Chen', 'Zile Qiao', 'Guoxin Chen', 'Liangcai Su', 'Zhen Zhang', 'Xinyu Wang', 'Pengjun Xie', 'Fei Huang', 'Jingren Zhou', 'Yong Jiang'], 'affiliations': ['Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2510.24695.jpg', 'data': {'categories': ['#dataset', '#training', '#benchmark', '#synthetic', '#data', '#agents', '#reasoning'], 'emoji': '🎯', 'ru': {'title': 'Обучение AI в зоне ближайшего развития', 'desc': 'Исследователи предложили подход к синтезу данных для обучения LLM, основанный на педагогической концепции зоны ближайшего развития (ZPD). Метод использует автоматический пайплайн AgentFrontier Engine, который генерирует задачи на границе возможностей модели — те, что она не может решить самостоятельно, но способна освоить с помощью. Обученная таким образом модель AgentFrontier-30B-A3B показала результаты уровня state-of-the-art на сложных бенчмарках, превзойдя некоторые проприетарные решения. Подход демонстрирует масштабируемый путь к созданию более способных AI-агентов через целенаправленный синтез обучающих данных.'}, 'en': {'title': 'Unlocking LLM Potential with ZPD-Guided Data Synthesis', 'desc': "This paper presents a novel approach to enhance large language models (LLMs) by using a data synthesis method based on the Zone of Proximal Development (ZPD). The ZPD concept helps identify tasks that LLMs can learn to solve with some guidance, allowing for targeted training on these challenging tasks. The authors introduce the AgentFrontier Engine, which automates the creation of high-quality training data that aligns with the LLM's current capabilities. By applying this method, the trained model achieves state-of-the-art performance on complex benchmarks, demonstrating the effectiveness of ZPD-guided data synthesis in advancing LLM capabilities."}, 'zh': {'title': '基于ZPD的数据合成，提升语言模型能力！', 'desc': '本文提出了一种基于最近发展区（ZPD）理论的数据合成方法，以提升大型语言模型（LLM）的能力。通过训练模型在其能力边界附近的任务，模型能够在复杂基准测试中实现最先进的表现。我们介绍了AgentFrontier引擎，这是一种自动化管道，能够合成高质量的多学科数据，帮助模型在知识密集型数据上进行持续预训练和复杂推理任务的后续训练。我们的实验表明，基于ZPD指导的数据合成方法为构建更强大的LLM代理提供了一条可扩展且有效的路径。'}}}, {'id': 'https://huggingface.co/papers/2510.24514', 'title': 'Latent Sketchpad: Sketching Visual Thoughts to Elicit Multimodal\n  Reasoning in MLLMs', 'url': 'https://huggingface.co/papers/2510.24514', 'abstract': "Latent Sketchpad enhances Multimodal Large Language Models with an internal visual scratchpad, enabling generative visual thought and improved reasoning performance.  \t\t\t\t\tAI-generated summary \t\t\t\t While Multimodal Large Language Models (MLLMs) excel at visual understanding, they often struggle in complex scenarios that require visual planning and imagination. Inspired by how humans use sketching as a form of visual thinking to develop and communicate ideas, we introduce Latent Sketchpad, a framework that equips MLLMs with an internal visual scratchpad. The internal visual representations of MLLMs have traditionally been confined to perceptual understanding. We repurpose them to support generative visual thought without compromising reasoning ability. Building on frontier MLLMs, our approach integrates visual generation directly into their native autoregressive reasoning process. It allows the model to interleave textual reasoning with the generation of visual latents. These latents guide the internal thought process and can be translated into sketch images for interpretability. To realize this, we introduce two components: a Context-Aware Vision Head autoregressively produces visual representations, and a pretrained Sketch Decoder renders these into human-interpretable images. We evaluate the framework on our new dataset MazePlanning. Experiments across various MLLMs show that Latent Sketchpad delivers comparable or even superior reasoning performance to their backbone. It further generalizes across distinct frontier MLLMs, including Gemma3 and Qwen2.5-VL. By extending model's textual reasoning to visual thinking, our framework opens new opportunities for richer human-computer interaction and broader applications. More details and resources are available on our project page: https://latent-sketchpad.github.io/.", 'score': 14, 'issue_id': 6668, 'pub_date': '2025-10-28', 'pub_date_card': {'ru': '28 октября', 'en': 'October 28', 'zh': '10月28日'}, 'hash': 'c64dd82987acd606', 'authors': ['Huanyu Zhang', 'Wenshan Wu', 'Chengzu Li', 'Ning Shang', 'Yan Xia', 'Yangyu Huang', 'Yifan Zhang', 'Li Dong', 'Zhang Zhang', 'Liang Wang', 'Tieniu Tan', 'Furu Wei'], 'affiliations': ['CASIA', 'Cambridge', 'MSR', 'NJU', 'UCAS'], 'pdf_title_img': 'assets/pdf/title_img/2510.24514.jpg', 'data': {'categories': ['#reasoning', '#dataset', '#cv', '#multimodal', '#interpretability'], 'emoji': '✏️', 'ru': {'title': 'Визуальное мышление для AI: когда нейросеть учится рисовать свои мысли', 'desc': 'Статья представляет Latent Sketchpad — фреймворк, который добавляет мультимодальным LLM внутренний визуальный «блокнот для набросков». Подобно тому, как люди используют рисование для визуального мышления, модель может генерировать внутренние визуальные представления в процессе рассуждений. Система включает Context-Aware Vision Head для создания визуальных латентов и Sketch Decoder для их преобразования в понятные изображения. Эксперименты показывают, что такой подход улучшает способность моделей к планированию и рассуждениям в сложных визуальных задачах.'}, 'en': {'title': 'Empowering MLLMs with Visual Thinking through Latent Sketchpad', 'desc': 'Latent Sketchpad is a novel framework that enhances Multimodal Large Language Models (MLLMs) by introducing an internal visual scratchpad for generative visual thought. This approach allows MLLMs to not only understand visual information but also engage in complex visual planning and imagination, similar to human sketching. By integrating visual generation into the autoregressive reasoning process, the model can seamlessly combine text and visual representations, improving its reasoning capabilities. The framework has been evaluated on a new dataset, MazePlanning, demonstrating superior performance across various MLLMs, thus paving the way for improved human-computer interaction.'}, 'zh': {'title': '增强视觉思维，提升推理能力的框架', 'desc': 'Latent Sketchpad 是一种增强多模态大型语言模型（MLLM）的框架，提供了一个内部视觉草图板，支持生成性视觉思维和改进的推理能力。该框架通过将视觉生成直接集成到模型的自回归推理过程中，使得模型能够在文本推理与视觉潜在生成之间交替进行。我们引入了两个关键组件：上下文感知视觉头和预训练的草图解码器，前者生成视觉表示，后者将其转化为人类可理解的图像。实验结果表明，Latent Sketchpad 在推理性能上与基础模型相当，甚至更优，拓展了模型的文本推理能力至视觉思维，开启了更丰富的人机交互和应用机会。'}}}, {'id': 'https://huggingface.co/papers/2510.24320', 'title': 'Critique-RL: Training Language Models for Critiquing through Two-Stage\n  Reinforcement Learning', 'url': 'https://huggingface.co/papers/2510.24320', 'abstract': "Critique-RL is an online reinforcement learning approach for developing critiquing language models without strong supervision, using a two-stage optimization strategy to improve both the critic's discriminability and helpfulness.  \t\t\t\t\tAI-generated summary \t\t\t\t Training critiquing language models to assess and provide feedback on model outputs is a promising way to improve LLMs for complex reasoning tasks. However, existing approaches typically rely on stronger supervisors for annotating critique data. To address this, we propose Critique-RL, an online RL approach for developing critiquing language models without stronger supervision. Our approach operates on a two-player paradigm: the actor generates a response, the critic provides feedback, and the actor refines the response accordingly. We first reveal that relying solely on indirect reward signals from the actor's outputs for RL optimization often leads to unsatisfactory critics: while their helpfulness (i.e., providing constructive feedback) improves, the discriminability (i.e., determining whether a response is high-quality or not) remains poor, resulting in marginal performance gains. To overcome this, Critique-RL adopts a two-stage optimization strategy. In stage I, it reinforces the discriminability of the critic with direct rule-based reward signals; in stage II, it introduces indirect rewards based on actor refinement to improve the critic's helpfulness, while maintaining its discriminability via appropriate regularization. Extensive experiments across various tasks and models show that Critique-RL delivers substantial performance improvements. For example, it achieves a 9.02% gain on in-domain tasks and a 5.70% gain on out-of-domain tasks for Qwen2.5-7B, highlighting its potential.", 'score': 13, 'issue_id': 6667, 'pub_date': '2025-10-28', 'pub_date_card': {'ru': '28 октября', 'en': 'October 28', 'zh': '10月28日'}, 'hash': '0c26fc5b04f76bff', 'authors': ['Zhiheng Xi', 'Jixuan Huang', 'Xin Guo', 'Boyang Hong', 'Dingwen Yang', 'Xiaoran Fan', 'Shuo Li', 'Zehui Chen', 'Junjie Ye', 'Siyu Yuan', 'Zhengyin Du', 'Xuesong Yao', 'Yufei Xu', 'Jiecao Chen', 'Rui Zheng', 'Tao Gui', 'Qi Zhang', 'Xuanjing Huang'], 'affiliations': ['ByteDance Seed', 'Fudan University'], 'pdf_title_img': 'assets/pdf/title_img/2510.24320.jpg', 'data': {'categories': ['#rl', '#reasoning', '#optimization', '#training', '#rlhf'], 'emoji': '🧠', 'ru': {'title': 'Critique-RL: Улучшение языковых моделей без сильного надзора', 'desc': 'Critique-RL — это метод обучения с подкреплением для создания языковых моделей, которые могут критиковать без сильного надзора. Он использует двухэтапную стратегию оптимизации, чтобы улучшить как способность критика различать, так и его полезность. В первом этапе усиливается способность критика различать с помощью прямых сигналов вознаграждения, а во втором этапе вводятся косвенные вознаграждения для улучшения полезности критика. Эксперименты показывают, что Critique-RL значительно улучшает производительность моделей на различных задачах.'}, 'en': {'title': 'Empowering Language Models with Self-Supervised Critique', 'desc': "Critique-RL is a novel online reinforcement learning method designed to enhance critiquing language models without the need for strong supervision. It employs a two-stage optimization strategy where an actor generates responses and a critic evaluates them, allowing for iterative refinement. The first stage focuses on improving the critic's ability to distinguish high-quality responses using direct reward signals, while the second stage enhances the critic's helpfulness through indirect rewards based on the actor's improvements. Experimental results demonstrate that Critique-RL significantly boosts performance, achieving notable gains in both in-domain and out-of-domain tasks."}, 'zh': {'title': 'Critique-RL：无强监督的评估语言模型优化方法', 'desc': 'Critique-RL是一种在线强化学习方法，用于在没有强监督的情况下开发评估语言模型。该方法采用两阶段优化策略，旨在提高评估者的区分能力和有用性。在第一阶段，通过直接的基于规则的奖励信号来增强评估者的区分能力；在第二阶段，基于生成者的反馈引入间接奖励，以提高评估者的有用性，同时通过适当的正则化保持其区分能力。实验结果表明，Critique-RL在多个任务和模型上显著提升了性能，展示了其潜力。'}}}, {'id': 'https://huggingface.co/papers/2510.23642', 'title': 'VisCoder2: Building Multi-Language Visualization Coding Agents', 'url': 'https://huggingface.co/papers/2510.23642', 'abstract': 'VisCoder2, a family of multi-language visualization models, outperforms open-source baselines and approaches proprietary models by leveraging VisCode-Multi-679K and VisPlotBench for iterative self-debugging and multi-turn correction.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have recently enabled coding agents capable of generating, executing, and revising visualization code. However, existing models often fail in practical workflows due to limited language coverage, unreliable execution, and lack of iterative correction mechanisms. Progress has been constrained by narrow datasets and benchmarks that emphasize single-round generation and single-language tasks. To address these challenges, we introduce three complementary resources for advancing visualization coding agents. VisCode-Multi-679K is a large-scale, supervised dataset containing 679K validated and executable visualization samples with multi-turn correction dialogues across 12 programming languages. VisPlotBench is a benchmark for systematic evaluation, featuring executable tasks, rendered outputs, and protocols for both initial generation and multi-round self-debug. Finally, we present VisCoder2, a family of multi-language visualization models trained on VisCode-Multi-679K. Experiments show that VisCoder2 significantly outperforms strong open-source baselines and approaches the performance of proprietary models like GPT-4.1, with further gains from iterative self-debug, reaching 82.4% overall execution pass rate at the 32B scale, particularly in symbolic or compiler-dependent languages.', 'score': 13, 'issue_id': 6668, 'pub_date': '2025-10-24', 'pub_date_card': {'ru': '24 октября', 'en': 'October 24', 'zh': '10月24日'}, 'hash': 'ddd9603391399242', 'authors': ['Yuansheng Ni', 'Songcheng Cai', 'Xiangchao Chen', 'Jiarong Liang', 'Zhiheng Lyu', 'Jiaqi Deng', 'Kai Zou', 'Ping Nie', 'Fei Yuan', 'Xiang Yue', 'Wenhu Chen'], 'affiliations': ['Carnegie Mellon University', 'Independent Researcher', 'Korea Advanced Institute of Science & Technology', 'Netmind.ai', 'University of Waterloo'], 'pdf_title_img': 'assets/pdf/title_img/2510.23642.jpg', 'data': {'categories': ['#open_source', '#science', '#dataset', '#multilingual', '#benchmark', '#agents'], 'emoji': '📊', 'ru': {'title': 'Мультиязычный AI-агент для визуализации данных с самоотладкой', 'desc': 'Представлена семья моделей VisCoder2 для генерации кода визуализаций на 12 языках программирования. Модели обучены на датасете VisCode-Multi-679K, содержащем 679 тысяч валидных примеров с диалогами многошаговой коррекции ошибок. Для оценки создан бенчмарк VisPlotBench с исполняемыми задачами и механизмом итеративной самоотладки. VisCoder2 превосходит открытые аналоги и приближается к проприетарным моделям вроде GPT-4.1, достигая 82.4% успешного выполнения кода на масштабе 32B параметров.'}, 'en': {'title': 'Revolutionizing Visualization Code Generation with VisCoder2', 'desc': 'VisCoder2 is a new set of models designed to improve the generation of visualization code across multiple programming languages. It uses a large dataset called VisCode-Multi-679K, which includes 679,000 validated examples and supports multi-turn corrections, allowing for better iterative debugging. The models are evaluated using VisPlotBench, which provides a structured way to assess their performance on both initial code generation and subsequent corrections. Results show that VisCoder2 outperforms existing open-source models and approaches the capabilities of proprietary systems, achieving a high execution pass rate, especially in complex programming scenarios.'}, 'zh': {'title': 'VisCoder2：多语言可视化的未来', 'desc': 'VisCoder2是一种多语言可视化模型家族，通过利用VisCode-Multi-679K和VisPlotBench实现迭代自我调试和多轮修正，超越了开源基准并接近专有模型的性能。该模型解决了现有编码代理在实际工作流程中面临的语言覆盖有限、执行不可靠和缺乏迭代修正机制的问题。我们引入了一个包含679K验证和可执行可视化样本的大规模监督数据集，以及一个系统评估基准，支持初始生成和多轮自我调试。实验结果表明，VisCoder2在执行通过率上达到了82.4%，尤其在符号或依赖编译器的语言中表现突出。'}}}, {'id': 'https://huggingface.co/papers/2510.22037', 'title': 'ATLAS: Adaptive Transfer Scaling Laws for Multilingual Pretraining,\n  Finetuning, and Decoding the Curse of Multilinguality', 'url': 'https://huggingface.co/papers/2510.22037', 'abstract': "The study introduces ATLAS, a multilingual scaling law that improves out-of-sample generalization and provides insights into cross-lingual transfer, optimal scaling, and computational crossover points for model training.  \t\t\t\t\tAI-generated summary \t\t\t\t Scaling laws research has focused overwhelmingly on English -- yet the most prominent AI models explicitly serve billions of international users. In this work, we undertake the largest multilingual scaling laws study to date, totaling 774 multilingual training experiments, spanning 10M-8B model parameters, 400+ training languages and 48 evaluation languages. We introduce the Adaptive Transfer Scaling Law (ATLAS) for both monolingual and multilingual pretraining, which outperforms existing scaling laws' out-of-sample generalization often by more than 0.3 R^2. Our analyses of the experiments shed light on multilingual learning dynamics, transfer properties between languages, and the curse of multilinguality. First, we derive a cross-lingual transfer matrix, empirically measuring mutual benefit scores between 38 x 38=1444 language pairs. Second, we derive a language-agnostic scaling law that reveals how to optimally scale model size and data when adding languages without sacrificing performance. Third, we identify the computational crossover points for when to pretrain from scratch versus finetune from multilingual checkpoints. We hope these findings provide the scientific foundation for democratizing scaling laws across languages, and enable practitioners to efficiently scale models -- beyond English-first AI.", 'score': 7, 'issue_id': 6669, 'pub_date': '2025-10-24', 'pub_date_card': {'ru': '24 октября', 'en': 'October 24', 'zh': '10月24日'}, 'hash': '23b87c7cd2ed6296', 'authors': ['Shayne Longpre', 'Sneha Kudugunta', 'Niklas Muennighoff', 'I-Hung Hsu', 'Isaac Caswell', 'Alex Pentland', 'Sercan Arik', 'Chen-Yu Lee', 'Sayna Ebrahimi'], 'affiliations': ['Google Cloud AI', 'Google DeepMind', 'MIT', 'Stanford University', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2510.22037.jpg', 'data': {'categories': ['#low_resource', '#multilingual', '#transfer_learning', '#dataset', '#training'], 'emoji': '🌍', 'ru': {'title': 'Законы масштабирования для многоязычного AI', 'desc': 'ATLAS — это новый закон масштабирования для многоязычных языковых моделей, основанный на 774 экспериментах с моделями от 10M до 8B параметров и более чем 400 языками. Исследование показывает, как языки передают знания друг другу, создавая матрицу взаимной пользы для 1444 языковых пар. ATLAS помогает определить оптимальное соотношение размера модели и данных при добавлении новых языков, а также точки, когда выгоднее обучать модель с нуля или дообучать существующую. Работа закладывает научную основу для демократизации AI за пределами англоцентричного подхода.'}, 'en': {'title': 'Unlocking Multilingual AI: The ATLAS Approach', 'desc': 'The paper presents ATLAS, a new multilingual scaling law designed to enhance out-of-sample generalization in AI models. It is based on extensive research involving 774 multilingual training experiments across a wide range of model sizes and languages. The study reveals important insights into cross-lingual transfer dynamics and provides a framework for optimal scaling of model size and data when incorporating multiple languages. Additionally, it identifies key points for deciding between pretraining from scratch or fine-tuning existing multilingual models, aiming to make AI more accessible for diverse languages.'}, 'zh': {'title': 'ATLAS：多语言扩展法则的创新', 'desc': '本研究介绍了ATLAS，一个多语言扩展法则，旨在提高模型的外部样本泛化能力，并提供跨语言迁移、最佳扩展和模型训练的计算交叉点的见解。我们进行了774个多语言训练实验，涵盖了10M到8B的模型参数，400多种训练语言和48种评估语言。ATLAS在单语言和多语言预训练中表现优于现有的扩展法则，外部样本泛化能力提高了0.3 R^2以上。我们的分析揭示了多语言学习动态、语言间的迁移特性以及多语言的挑战，为跨语言的扩展法则提供了科学基础。'}}}, {'id': 'https://huggingface.co/papers/2510.24645', 'title': 'FunReason-MT Technical Report: Overcoming the Complexity Barrier in\n  Multi-Turn Function Calling', 'url': 'https://huggingface.co/papers/2510.24645', 'abstract': 'FunReason-MT is a novel data synthesis framework that enhances multi-turn function calling in large language models by addressing challenges in environment interaction, query synthesis, and chain-of-thought generation, achieving state-of-the-art performance on the Berkeley Function-Calling Leaderboard.  \t\t\t\t\tAI-generated summary \t\t\t\t Function calling (FC) empowers large language models (LLMs) and autonomous agents to interface with external tools, a critical capability for solving complex, real-world problems. As this ability becomes increasingly central to advanced AI systems, the need for high-quality, multi-turn training data to develop and refine it cannot be overstated. Existing data synthesis methods, such as random environment sampling or multi-agent role-playing, are not powerful enough to generate high-quality data in real-world environments. Practical challenges come in three folds: targeted model training, isolation of tool architecture, and multi-turn logical dependency. To address these structural deficiencies, we present FunReason-MT, a novel data synthesis framework for real-world multi-turn tool use. FunReason-MT resolves the complexity barrier in multi-turn FC data by employing 1) Environment-API Graph Interactions to gather varied high-quality trajectories, 2) Advanced Tool-Query Synthesis to simplify hard query construction, and 3) Guided Iterative Chain for sophisticated CoT generation. Evaluations on Berkeley Function-Calling Leaderboard (BFCLv3) demonstrate the power of our framework: a 4B model built upon FunReason-MT generated data achieves state-of-the-art performance among comparable-sized models, outperforming most close-source models. Further performance improvements on BFCLv4 confirm that FunReason-MT provides a reliable and robust source for agentic learning.', 'score': 3, 'issue_id': 6668, 'pub_date': '2025-10-28', 'pub_date_card': {'ru': '28 октября', 'en': 'October 28', 'zh': '10月28日'}, 'hash': 'e3e4dc7959a10fb7', 'authors': ['Zengzhuang Xu', 'Bingguang Hao', 'Zechuan Wang', 'Yuntao Wen', 'Maolin Wang', 'Yang Liu', 'Long Chen', 'Dong Wang', 'Yicheng Chen', 'Cunyin Peng', 'Chenyi Zhuang', 'Jinjie Gu', 'Leilei Gan', 'Xiangyu Zhao', 'Shi Gu'], 'affiliations': ['AWorld Team, Inclusion AI', 'City University of Hong Kong', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2510.24645.jpg', 'data': {'categories': ['#dataset', '#training', '#optimization', '#data', '#agents', '#transfer_learning'], 'emoji': '🔧', 'ru': {'title': 'Учим LLM правильно пользоваться инструментами в несколько шагов', 'desc': 'FunReason-MT — это новый фреймворк для синтеза данных, который улучшает способность больших языковых моделей выполнять многошаговые вызовы функций. Метод решает три ключевые проблемы: взаимодействие с окружением через графы Environment-API, синтез сложных запросов и генерацию chain-of-thought рассуждений. Модель размером всего 4B параметров, обученная на данных FunReason-MT, достигла state-of-the-art результатов на Berkeley Function-Calling Leaderboard, превзойдя большинство проприетарных моделей. Фреймворк обеспечивает надежный источник качественных данных для обучения AI-агентов работе с внешними инструментами.'}, 'en': {'title': 'Empowering AI with Enhanced Multi-Turn Function Calling', 'desc': "FunReason-MT is a new framework designed to improve how large language models (LLMs) perform multi-turn function calling, which is essential for interacting with external tools. It tackles issues related to generating high-quality training data by using advanced techniques like Environment-API Graph Interactions and Tool-Query Synthesis. This framework helps in creating better training scenarios that reflect real-world complexities, enhancing the model's ability to understand and generate logical sequences. The results show that models trained with FunReason-MT data achieve top performance on the Berkeley Function-Calling Leaderboard, indicating its effectiveness in advancing AI capabilities."}, 'zh': {'title': '提升多轮函数调用的智能框架', 'desc': 'FunReason-MT 是一个新颖的数据合成框架，旨在提升大型语言模型在多轮函数调用中的表现。它通过解决环境交互、查询合成和思维链生成等挑战，成功在伯克利函数调用排行榜上取得了领先的性能。该框架采用环境-API图交互、先进的工具-查询合成和引导迭代链等技术，生成高质量的多轮数据。评估结果显示，基于 FunReason-MT 生成数据的 4B 模型在同类模型中表现优异，证明了其在智能学习中的可靠性和稳健性。'}}}, {'id': 'https://huggingface.co/papers/2510.24591', 'title': 'ReplicationBench: Can AI Agents Replicate Astrophysics Research Papers?', 'url': 'https://huggingface.co/papers/2510.24591', 'abstract': "ReplicationBench evaluates AI agents' ability to replicate astrophysics research papers, providing insights into their faithfulness and correctness in scientific research tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Frontier AI agents show increasing promise as scientific research assistants, and may eventually be useful for extended, open-ended research workflows. However, in order to use agents for novel research, we must first assess the underlying faithfulness and correctness of their work. To evaluate agents as research assistants, we introduce ReplicationBench, an evaluation framework that tests whether agents can replicate entire research papers drawn from the astrophysics literature. Astrophysics, where research relies heavily on archival data and computational study while requiring little real-world experimentation, is a particularly useful testbed for AI agents in scientific research. We split each paper into tasks which require agents to replicate the paper's core contributions, including the experimental setup, derivations, data analysis, and codebase. Each task is co-developed with the original paper authors and targets a key scientific result, enabling objective evaluation of both faithfulness (adherence to original methods) and correctness (technical accuracy of results). ReplicationBench is extremely challenging for current frontier language models: even the best-performing language models score under 20%. We analyze ReplicationBench trajectories in collaboration with domain experts and find a rich, diverse set of failure modes for agents in scientific research. ReplicationBench establishes the first benchmark of paper-scale, expert-validated astrophysics research tasks, reveals insights about agent performance generalizable to other domains of data-driven science, and provides a scalable framework for measuring AI agents' reliability in scientific research.", 'score': 3, 'issue_id': 6667, 'pub_date': '2025-10-28', 'pub_date_card': {'ru': '28 октября', 'en': 'October 28', 'zh': '10月28日'}, 'hash': 'd655b0884e7b15c0', 'authors': ['Christine Ye', 'Sihan Yuan', 'Suchetha Cooray', 'Steven Dillmann', 'Ian L. V. Roque', 'Dalya Baron', 'Philipp Frank', 'Sergio Martin-Alvarez', 'Nolan Koblischke', 'Frank J Qu', 'Diyi Yang', 'Risa Wechsler', 'Ioana Ciuca'], 'affiliations': ['Stanford University', 'University of Toronto'], 'pdf_title_img': 'assets/pdf/title_img/2510.24591.jpg', 'data': {'categories': ['#benchmark', '#science', '#agents'], 'emoji': '🔭', 'ru': {'title': 'Проверка AI-агентов на репликацию научных исследований', 'desc': 'Исследователи представили ReplicationBench — бенчмарк для оценки способности AI-агентов воспроизводить научные работы по астрофизике. Каждая задача включает проверку верности оригинальным методам и корректности технических результатов, при этом все задачи разработаны совместно с авторами оригинальных статей. Даже лучшие современные LLM показывают результат ниже 20%, демонстрируя множество разнообразных ошибок при работе с научными исследованиями. Этот бенчмарк предоставляет первую масштабируемую систему для измерения надежности AI-агентов в научных исследованиях и может быть применен в других областях науки, основанных на данных.'}, 'en': {'title': 'Evaluating AI Agents in Astrophysics Research Replication', 'desc': "ReplicationBench is a framework designed to evaluate AI agents' ability to replicate research papers in astrophysics, focusing on their faithfulness and correctness. It breaks down each paper into specific tasks that require the agents to reproduce key contributions, such as experimental setups and data analyses, in collaboration with the original authors. The framework reveals that even advanced language models struggle with these tasks, scoring below 20%, highlighting various failure modes in their performance. This benchmark not only assesses AI reliability in astrophysics but also offers insights applicable to other scientific fields."}, 'zh': {'title': '评估 AI 代理在科学研究中的可靠性', 'desc': 'ReplicationBench 是一个评估 AI 代理在复制天体物理学研究论文能力的框架。它通过将论文分解为多个任务，测试代理是否能够准确复现论文的核心贡献，包括实验设置、推导、数据分析和代码库。该框架与原论文作者共同开发，确保评估的客观性，关注代理的忠实性和正确性。尽管当前的前沿语言模型在此任务中表现不佳，ReplicationBench 仍为科学研究中的 AI 代理提供了一个可扩展的可靠性测量框架。'}}}, {'id': 'https://huggingface.co/papers/2510.24448', 'title': 'Rethinking Visual Intelligence: Insights from Video Pretraining', 'url': 'https://huggingface.co/papers/2510.24448', 'abstract': 'Video Diffusion Models (VDMs) show higher data efficiency than large language models across various visual tasks, suggesting video pretraining can enhance visual foundation models.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have demonstrated that large-scale pretraining enables systems to adapt rapidly to new problems with little supervision in the language domain. This success, however, has not translated as effectively to the visual domain, where models, including LLMs, continue to struggle with compositional understanding, sample efficiency, and general-purpose problem-solving. We investigate Video Diffusion Models (VDMs) as a promising direction for bridging this gap. Pretraining on spatiotemporal data endows these models with strong inductive biases for structure and dynamics, which we hypothesize can support broad task adaptability. To test this, we design a controlled evaluation in which both a pretrained LLM and a pretrained VDM are equipped with lightweight adapters and presented with tasks in their natural modalities. Across benchmarks including ARC-AGI, ConceptARC, visual games, route planning, and cellular automata, VDMs demonstrate higher data efficiency than their language counterparts. Taken together, our results indicate that video pretraining offers inductive biases that support progress toward visual foundation models.', 'score': 2, 'issue_id': 6667, 'pub_date': '2025-10-28', 'pub_date_card': {'ru': '28 октября', 'en': 'October 28', 'zh': '10月28日'}, 'hash': 'cb963e7271205da4', 'authors': ['Pablo Acuaviva', 'Aram Davtyan', 'Mariam Hassan', 'Sebastian Stapf', 'Ahmad Rahimi', 'Alexandre Alahi', 'Paolo Favaro'], 'affiliations': ['Computer Vision Group University of Bern Bern, Switzerland', 'VITA Lab, EPFL Lausanne, Switzerland'], 'pdf_title_img': 'assets/pdf/title_img/2510.24448.jpg', 'data': {'categories': ['#multimodal', '#transfer_learning', '#games', '#diffusion', '#benchmark', '#video'], 'emoji': '🎬', 'ru': {'title': 'Видео-диффузия побеждает языковые модели в визуальных задачах', 'desc': 'Исследователи сравнили Video Diffusion Models (VDMs) и большие языковые модели (LLM) в визуальных задачах. VDMs, предобученные на видеоданных, показали более высокую эффективность обучения на малых данных благодаря лучшему пониманию пространственно-временной структуры. Модели тестировались на различных бенчмарках, включая ARC-AGI, ConceptARC, визуальные игры и планирование маршрутов. Результаты показывают, что предобучение на видео дает важные индуктивные смещения для создания визуальных foundation models.'}, 'en': {'title': 'Unlocking Visual Potential with Video Diffusion Models', 'desc': 'This paper explores the effectiveness of Video Diffusion Models (VDMs) in improving visual tasks through video pretraining. Unlike large language models (LLMs), which excel in language tasks, VDMs leverage spatiotemporal data to enhance their understanding of structure and dynamics. The authors conducted experiments comparing pretrained VDMs and LLMs, finding that VDMs showed superior data efficiency across various benchmarks. The results suggest that video pretraining can significantly advance the development of visual foundation models.'}, 'zh': {'title': '视频预训练提升视觉模型效率', 'desc': '视频扩散模型（VDMs）在各种视觉任务中显示出比大型语言模型更高的数据效率，表明视频预训练可以增强视觉基础模型的能力。尽管大型语言模型在语言领域的预训练取得了成功，但在视觉领域，模型仍然面临组合理解和样本效率等挑战。我们研究VDMs作为弥合这一差距的有前景的方向，认为其在时空数据上的预训练赋予了模型强大的结构和动态的归纳偏置。我们的实验结果表明，VDMs在多个基准测试中表现出比语言模型更高的数据效率，支持了视频预训练对视觉基础模型的进展。'}}}, {'id': 'https://huggingface.co/papers/2510.22099', 'title': 'Generalization or Memorization: Dynamic Decoding for Mode Steering', 'url': 'https://huggingface.co/papers/2510.22099', 'abstract': "A framework using the Information Bottleneck principle and Dynamic Mode Steering algorithm improves the reliability of Large Language Models by balancing generalization and memorization.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) exhibit a troubling duality, capable of both remarkable generalization and brittle, verbatim memorization of their training data. This unpredictability undermines their reliability in high-stakes applications. In this work, we propose a unified framework to understand, identify, and control these distinct reasoning modes. First, we introduce a theoretical model based on the Information Bottleneck (IB) principle, formalizing generalization as the learning of a compressed, task-relevant representation and memorization as a failure to compress. Building on this theory, we develop Dynamic Mode Steering (DMS), a novel inference-time algorithm which comprises two components: (1) a lightweight, causally-grounded linear probe that identifies the model's instantaneous reliance on memorization, and (2) a dynamic activation steering mechanism that nudges the model's computation towards pre-identified generalization circuits. We frame DMS as a form of adaptive, self-contrastive decoding. Experiments on reasoning and faithfulness tasks demonstrate that DMS significantly improves logical consistency and factual accuracy, thereby offering a principled approach to enhancing LLM reliability.", 'score': 2, 'issue_id': 6667, 'pub_date': '2025-10-25', 'pub_date_card': {'ru': '25 октября', 'en': 'October 25', 'zh': '10月25日'}, 'hash': '7a4136db7043277f', 'authors': ['Xuanming Zhang'], 'affiliations': ['Department of Computer Science, University of Wisconsin-Madison, Madison, USA', 'Stanford University, Palo Alto, USA'], 'pdf_title_img': 'assets/pdf/title_img/2510.22099.jpg', 'data': {'categories': ['#reasoning', '#interpretability', '#inference', '#training'], 'emoji': '🧭', 'ru': {'title': 'Управление режимами мышления: как научить LLM обобщать, а не заучивать', 'desc': 'Исследователи предлагают новый подход к проблеме, когда большие языковые модели то блестяще обобщают знания, то просто воспроизводят заученные фразы из обучающих данных. На основе принципа Information Bottleneck они создали теоретическую модель, где обобщение понимается как сжатие информации до важных для задачи элементов, а механическое запоминание — как неспособность к такому сжатию. Разработанный алгоритм Dynamic Mode Steering использует легковесный линейный классификатор для определения момента, когда модель полагается на заученные данные, и динамически корректирует её вычисления в сторону обобщающих паттернов. Эксперименты показали, что метод значительно улучшает логическую последовательность и фактическую точность ответов LLM.'}, 'en': {'title': 'Balancing Generalization and Memorization for Reliable LLMs', 'desc': "This paper presents a framework that enhances the reliability of Large Language Models (LLMs) by addressing their tendency to both generalize and memorize training data. It introduces the Information Bottleneck principle to differentiate between effective generalization, which compresses information, and problematic memorization, which retains too much detail. The Dynamic Mode Steering algorithm is developed to dynamically adjust the model's focus during inference, promoting generalization while minimizing reliance on memorization. Experimental results show that this approach improves logical consistency and factual accuracy in LLM outputs, making them more reliable for critical applications."}, 'zh': {'title': '提升大型语言模型可靠性的创新框架', 'desc': '本文提出了一种新的框架，利用信息瓶颈原理和动态模式引导算法来提高大型语言模型的可靠性。该框架旨在平衡模型的泛化能力和记忆能力，解决模型在高风险应用中的不可靠性问题。通过理论模型，泛化被定义为学习压缩的、与任务相关的表示，而记忆则被视为未能进行压缩。实验结果表明，动态模式引导算法显著提高了模型的逻辑一致性和事实准确性，提供了一种增强大型语言模型可靠性的原则性方法。'}}}, {'id': 'https://huggingface.co/papers/2510.22373', 'title': 'VisJudge-Bench: Aesthetics and Quality Assessment of Visualizations', 'url': 'https://huggingface.co/papers/2510.22373', 'abstract': "VisJudge-Bench is a benchmark for evaluating MLLMs' performance in assessing visualization aesthetics and quality, revealing gaps compared to human experts and demonstrating improvements with the VisJudge model.  \t\t\t\t\tAI-generated summary \t\t\t\t Visualization, a domain-specific yet widely used form of imagery, is an effective way to turn complex datasets into intuitive insights, and its value depends on whether data are faithfully represented, clearly communicated, and aesthetically designed. However, evaluating visualization quality is challenging: unlike natural images, it requires simultaneous judgment across data encoding accuracy, information expressiveness, and visual aesthetics. Although multimodal large language models (MLLMs) have shown promising performance in aesthetic assessment of natural images, no systematic benchmark exists for measuring their capabilities in evaluating visualizations. To address this, we propose VisJudge-Bench, the first comprehensive benchmark for evaluating MLLMs' performance in assessing visualization aesthetics and quality. It contains 3,090 expert-annotated samples from real-world scenarios, covering single visualizations, multiple visualizations, and dashboards across 32 chart types. Systematic testing on this benchmark reveals that even the most advanced MLLMs (such as GPT-5) still exhibit significant gaps compared to human experts in judgment, with a Mean Absolute Error (MAE) of 0.551 and a correlation with human ratings of only 0.429. To address this issue, we propose VisJudge, a model specifically designed for visualization aesthetics and quality assessment. Experimental results demonstrate that VisJudge significantly narrows the gap with human judgment, reducing the MAE to 0.442 (a 19.8% reduction) and increasing the consistency with human experts to 0.681 (a 58.7% improvement) compared to GPT-5. The benchmark is available at https://github.com/HKUSTDial/VisJudgeBench.", 'score': 1, 'issue_id': 6674, 'pub_date': '2025-10-25', 'pub_date_card': {'ru': '25 октября', 'en': 'October 25', 'zh': '10月25日'}, 'hash': 'e827dfbea1866daf', 'authors': ['Yupeng Xie', 'Zhiyang Zhang', 'Yifan Wu', 'Sirong Lu', 'Jiayi Zhang', 'Zhaoyang Yu', 'Jinlin Wang', 'Sirui Hong', 'Bang Liu', 'Chenglin Wu', 'Yuyu Luo'], 'affiliations': ['DeepWisdom', 'The Hong Kong University of Science and Technology (Guangzhou)', 'Université de Montréal & Mila'], 'pdf_title_img': 'assets/pdf/title_img/2510.22373.jpg', 'data': {'categories': ['#benchmark', '#interpretability', '#multimodal', '#optimization'], 'emoji': '📊', 'ru': {'title': 'VisJudge: учим AI понимать красоту графиков', 'desc': 'Статья представляет VisJudge-Bench — первый комплексный бенчмарк для оценки способности multimodal LLM оценивать эстетику и качество визуализаций данных. Benchmark содержит 3090 образцов с экспертными аннотациями, охватывающих 32 типа графиков из реальных сценариев. Тестирование показало, что даже продвинутые модели типа GPT-5 значительно уступают человеческим экспертам в оценке визуализаций. Предложенная модель VisJudge сокращает разрыв с человеческой оценкой на 19.8% по метрике MAE и улучшает корреляцию с экспертами на 58.7%.'}, 'en': {'title': 'Bridging the Gap in Visualization Quality Assessment with VisJudge-Bench', 'desc': 'VisJudge-Bench is a new benchmark designed to evaluate how well multimodal large language models (MLLMs) can assess the aesthetics and quality of visualizations. It includes over 3,000 expert-annotated examples that cover various types of visualizations, making it a comprehensive tool for testing. The findings show that even advanced models like GPT-5 struggle to match human expert evaluations, with notable gaps in accuracy and correlation. To improve this, the VisJudge model was introduced, which significantly enhances performance in aesthetic assessments, demonstrating a marked reduction in error rates compared to existing MLLMs.'}, 'zh': {'title': '可视化评估的新基准与模型', 'desc': 'VisJudge-Bench是一个用于评估多模态大型语言模型（MLLMs）在可视化美学和质量评估方面表现的基准。该基准包含3090个来自真实场景的专家标注样本，涵盖32种图表类型的单个可视化、多重可视化和仪表板。研究表明，即使是最先进的MLLM（如GPT-5），在判断上与人类专家相比仍存在显著差距。为此，我们提出了VisJudge模型，专门用于可视化美学和质量评估，实验结果显示其在与人类判断的一致性上有显著提升。'}}}, {'id': 'https://huggingface.co/papers/2510.21323', 'title': 'VL-SAE: Interpreting and Enhancing Vision-Language Alignment with a\n  Unified Concept Set', 'url': 'https://huggingface.co/papers/2510.21323', 'abstract': 'VL-SAE, a sparse autoencoder, enhances vision-language alignment by correlating neurons to unified concepts, improving interpretability and performance in tasks like zero-shot image classification and hallucination elimination.  \t\t\t\t\tAI-generated summary \t\t\t\t The alignment of vision-language representations endows current Vision-Language Models (VLMs) with strong multi-modal reasoning capabilities. However, the interpretability of the alignment component remains uninvestigated due to the difficulty in mapping the semantics of multi-modal representations into a unified concept set. To address this problem, we propose VL-SAE, a sparse autoencoder that encodes vision-language representations into its hidden activations. Each neuron in its hidden layer correlates to a concept represented by semantically similar images and texts, thereby interpreting these representations with a unified concept set. To establish the neuron-concept correlation, we encourage semantically similar representations to exhibit consistent neuron activations during self-supervised training. First, to measure the semantic similarity of multi-modal representations, we perform their alignment in an explicit form based on cosine similarity. Second, we construct the VL-SAE with a distance-based encoder and two modality-specific decoders to ensure the activation consistency of semantically similar representations. Experiments across multiple VLMs (e.g., CLIP, LLaVA) demonstrate the superior capability of VL-SAE in interpreting and enhancing the vision-language alignment. For interpretation, the alignment between vision and language representations can be understood by comparing their semantics with concepts. For enhancement, the alignment can be strengthened by aligning vision-language representations at the concept level, contributing to performance improvements in downstream tasks, including zero-shot image classification and hallucination elimination. Codes are available at https://github.com/ssfgunner/VL-SAE.', 'score': 1, 'issue_id': 6667, 'pub_date': '2025-10-24', 'pub_date_card': {'ru': '24 октября', 'en': 'October 24', 'zh': '10月24日'}, 'hash': '41279ff25d4d0ecb', 'authors': ['Shufan Shen', 'Junshu Sun', 'Qingming Huang', 'Shuhui Wang'], 'affiliations': ['Key Lab of Intell. Info. Process., Inst. of Comput. Tech., CAS', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2510.21323.jpg', 'data': {'categories': ['#alignment', '#multimodal', '#interpretability', '#cv', '#hallucinations', '#training'], 'emoji': '🔍', 'ru': {'title': 'Разреженный автоэнкодер для понимания и усиления связи между зрением и языком', 'desc': 'VL-SAE — это разреженный автоэнкодер, который повышает интерпретируемость визуально-языкового выравнивания в VLM моделях путём сопоставления нейронов с единым набором концептов. Каждый нейрон скрытого слоя коррелирует с концептом, представленным семантически похожими изображениями и текстами. Модель обучается самообучением, обеспечивая согласованную активацию нейронов для семантически близких представлений разных модальностей. Эксперименты показывают улучшение производительности в задачах zero-shot классификации изображений и устранения галлюцинаций в моделях типа CLIP и LLaVA.'}, 'en': {'title': 'Enhancing Vision-Language Alignment with VL-SAE', 'desc': 'The paper introduces VL-SAE, a sparse autoencoder designed to improve the alignment between vision and language representations in multi-modal models. By correlating neurons in its hidden layer to unified concepts derived from semantically similar images and texts, VL-SAE enhances both interpretability and performance. The model employs self-supervised training to ensure consistent neuron activations for similar representations, using cosine similarity for semantic alignment. Experiments show that VL-SAE significantly boosts the effectiveness of vision-language models in tasks like zero-shot image classification and reducing hallucinations.'}, 'zh': {'title': 'VL-SAE：提升视觉-语言对齐的稀疏自编码器', 'desc': 'VL-SAE是一种稀疏自编码器，通过将视觉和语言表示关联到统一的概念，增强了视觉-语言对齐的能力。它的隐藏层中的每个神经元与语义相似的图像和文本所代表的概念相关联，从而提高了模型的可解释性和性能。通过自监督训练，VL-SAE鼓励语义相似的表示在神经元激活上保持一致。实验表明，VL-SAE在零样本图像分类和消除幻觉等任务中表现出色，显著提升了视觉-语言模型的对齐能力。'}}}, {'id': 'https://huggingface.co/papers/2510.20155', 'title': 'PartNeXt: A Next-Generation Dataset for Fine-Grained and Hierarchical 3D\n  Part Understanding', 'url': 'https://huggingface.co/papers/2510.20155', 'abstract': "PartNeXt, a high-quality, textured 3D dataset with fine-grained part labels, improves performance in class-agnostic part segmentation and 3D part-centric question answering, highlighting gaps in open-vocabulary part grounding.  \t\t\t\t\tAI-generated summary \t\t\t\t Understanding objects at the level of their constituent parts is fundamental to advancing computer vision, graphics, and robotics. While datasets like PartNet have driven progress in 3D part understanding, their reliance on untextured geometries and expert-dependent annotation limits scalability and usability. We introduce PartNeXt, a next-generation dataset addressing these gaps with over 23,000 high-quality, textured 3D models annotated with fine-grained, hierarchical part labels across 50 categories. We benchmark PartNeXt on two tasks: (1) class-agnostic part segmentation, where state-of-the-art methods (e.g., PartField, SAMPart3D) struggle with fine-grained and leaf-level parts, and (2) 3D part-centric question answering, a new benchmark for 3D-LLMs that reveals significant gaps in open-vocabulary part grounding. Additionally, training Point-SAM on PartNeXt yields substantial gains over PartNet, underscoring the dataset's superior quality and diversity. By combining scalable annotation, texture-aware labels, and multi-task evaluation, PartNeXt opens new avenues for research in structured 3D understanding.", 'score': 1, 'issue_id': 6672, 'pub_date': '2025-10-23', 'pub_date_card': {'ru': '23 октября', 'en': 'October 23', 'zh': '10月23日'}, 'hash': '338d5df27efaaad5', 'authors': ['Penghao Wang', 'Yiyang He', 'Xin Lv', 'Yukai Zhou', 'Lan Xu', 'Jingyi Yu', 'Jiayuan Gu'], 'affiliations': ['ShanghaiTech University'], 'pdf_title_img': 'assets/pdf/title_img/2510.20155.jpg', 'data': {'categories': ['#dataset', '#3d', '#benchmark', '#cv'], 'emoji': '🧩', 'ru': {'title': 'PartNeXt: текстурированные 3D модели с детальной разметкой частей для продвинутого понимания объектов', 'desc': 'Исследователи представили PartNeXt — новый датасет из более чем 23,000 высококачественных текстурированных 3D моделей с детальной иерархической разметкой частей объектов в 50 категориях. Датасет решает проблемы предыдущих наборов данных, таких как PartNet, которые использовали модели без текстур и требовали экспертной аннотации. На PartNeXt протестировали две задачи: class-agnostic сегментацию частей объектов и 3D question answering о частях объектов, где современные методы показали существенные пробелы в понимании. Обучение Point-SAM на PartNeXt продемонстрировало значительное улучшение качества по сравнению с PartNet, подтверждая превосходство нового датасета.'}, 'en': {'title': 'PartNeXt: Elevating 3D Understanding with Textured Models and Fine-Grained Labels', 'desc': "PartNeXt is a new dataset designed to enhance the understanding of 3D objects by providing high-quality, textured models with detailed part labels. It addresses limitations of previous datasets like PartNet, which lacked texture and relied on expert annotations, making them less scalable. The dataset is evaluated on two key tasks: class-agnostic part segmentation and 3D part-centric question answering, revealing challenges in current models' ability to handle fine-grained parts. By offering a diverse and well-annotated resource, PartNeXt aims to advance research in 3D part segmentation and improve the performance of machine learning models in this area."}, 'zh': {'title': 'PartNeXt：推动3D理解的新数据集', 'desc': 'PartNeXt是一个高质量的3D数据集，包含超过23,000个带有细粒度部件标签的纹理模型，旨在提升无类别部件分割和3D部件中心问答的性能。与以往的数据集相比，PartNeXt克服了依赖无纹理几何体和专家注释的局限性，提供了更好的可扩展性和可用性。通过在两个任务上进行基准测试，PartNeXt展示了其在细粒度部件处理和开放词汇部件定位方面的优势。该数据集的多任务评估和纹理感知标签为结构化3D理解的研究开辟了新的方向。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (1)', '#agents (13)', '#agi (1)', '#alignment (2)', '#architecture (3)', '#audio (1)', '#benchmark (16)', '#cv (5)', '#data (3)', '#dataset (9)', '#diffusion (4)', '#ethics', '#games (3)', '#graphs', '#hallucinations (1)', '#healthcare', '#inference (1)', '#interpretability (7)', '#leakage', '#long_context (2)', '#low_resource (1)', '#machine_translation', '#math', '#multilingual (2)', '#multimodal (7)', '#open_source (6)', '#optimization (10)', '#plp', '#rag', '#reasoning (11)', '#rl (2)', '#rlhf (1)', '#robotics', '#science (2)', '#security', '#small_models', '#story_generation', '#survey (1)', '#synthetic (2)', '#training (13)', '#transfer_learning (3)', '#video (2)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-10-29 09:13',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-10-29 09:13')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-10-29 09:13')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    