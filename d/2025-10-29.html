
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 40 papers. October 29.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ", "Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹", "Ğ¼Ğ¸Ğ½ÑƒÑ‚"],
                hour: ["Ñ‡Ğ°Ñ", "Ñ‡Ğ°ÑĞ°", "Ñ‡Ğ°ÑĞ¾Ğ²"],
                day: ["Ğ´ĞµĞ½ÑŒ", "Ğ´Ğ½Ñ", "Ğ´Ğ½ĞµĞ¹"],
                justNow: "Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ñ‚Ğ¾",
                ago: "Ğ½Ğ°Ğ·Ğ°Ğ´"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["åˆ†é’Ÿ", "åˆ†é’Ÿ", "åˆ†é’Ÿ"],
                hour: ["å°æ—¶", "å°æ—¶", "å°æ—¶"],
                day: ["å¤©", "å¤©", "å¤©"],
                justNow: "åˆšåˆš",
                ago: "å‰"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            } else if (lastDigit === 1) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒÑ";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒĞ¸";
            } else {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ç¯‡è®ºæ–‡"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">ğŸ”º</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">29 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ</span> | <span id="title-articles-count">40 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-10-28.html">â¬…ï¸ <span id="prev-date">28.10</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-10-30.html">â¡ï¸ <span id="next-date">30.10</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-10.html">ğŸ“ˆ <span id='top-month-label'>ĞœĞµÑÑÑ†</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">ğŸ”€ <span id="sort-label-text">Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ</option>
                    <option value="pub_date">Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸</option>
                    <option value="issue_id">Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">ğŸ·ï¸ Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> AâˆªB</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> Aâˆ©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">ğŸ§¹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> âœ–ï¸ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '29 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 29', 'zh': '10æœˆ29æ—¥'};
        let feedDateNext = {'ru': '30.10', 'en': '10/30', 'zh': '10æœˆ30æ—¥'};
        let feedDatePrev = {'ru': '28.10', 'en': '10/28', 'zh': '10æœˆ28æ—¥'};
        let filterLabel = {'ru': 'Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€', 'en': 'Topics', 'zh': 'ä¸»é¢˜ç­›é€‰'}
        let publishedLabel = {'ru': 'ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ñ‚ ', 'en': 'published on ', 'zh': 'å‘è¡¨äº'}
        let sortLabel = {'ru': 'Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾', 'en': 'Sort by', 'zh': 'æ’åºæ–¹å¼'}
        let paperLabel = {'ru': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ', 'en': 'Paper', 'zh': 'è®ºæ–‡'}
        let topMonthLabel = {'ru': 'ĞœĞµÑÑÑ†', 'en': 'Month', 'zh': 'æœˆåº¦è®ºæ–‡'}
        let topDayLabel = {'ru': 'Ğ”ĞµĞ½ÑŒ', 'en': 'Day', 'zh': 'æ—¥åº¦è®ºæ–‡'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2510.24668', 'title': 'InteractComp: Evaluating Search Agents With Ambiguous Queries', 'url': 'https://huggingface.co/papers/2510.24668', 'abstract': "InteractComp evaluates search agents' ability to recognize and resolve query ambiguity through interaction, revealing significant gaps in current models' capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t Language agents have demonstrated remarkable potential in web search and information retrieval. However, these search agents assume user queries are complete and unambiguous, an assumption that diverges from reality where users begin with incomplete queries requiring clarification through interaction. Yet most agents lack interactive mechanisms during the search process, and existing benchmarks cannot assess this capability. To address this gap, we introduce InteractComp, a benchmark designed to evaluate whether search agents can recognize query ambiguity and actively interact to resolve it during search. Following the principle of easy to verify, interact to disambiguate, we construct 210 expert-curated questions across 9 domains through a target-distractor methodology that creates genuine ambiguity resolvable only through interaction. Evaluation of 17 models reveals striking failure: the best model achieves only 13.73% accuracy despite 71.50% with complete context, exposing systematic overconfidence rather than reasoning deficits. Forced interaction produces dramatic gains, demonstrating latent capability current strategies fail to engage. Longitudinal analysis shows interaction capabilities stagnated over 15 months while search performance improved seven-fold, revealing a critical blind spot. This stagnation, coupled with the immediate feedback inherent to search tasks, makes InteractComp a valuable resource for both evaluating and training interaction capabilities in search agents. The code is available at https://github.com/FoundationAgents/InteractComp.", 'score': 82, 'issue_id': 6668, 'pub_date': '2025-10-28', 'pub_date_card': {'ru': '28 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 28', 'zh': '10æœˆ28æ—¥'}, 'hash': '7db93e4c4cf42dec', 'authors': ['Mingyi Deng', 'Lijun Huang', 'Yani Fan', 'Jiayi Zhang', 'Fashen Ren', 'Jinyi Bai', 'Fuzhen Yang', 'Dayi Miao', 'Zhaoyang Yu', 'Yifan Wu', 'Yanfei Zhang', 'Fengwei Teng', 'Yingjia Wan', 'Song Hu', 'Yude Li', 'Xin Jin', 'Conghao Hu', 'Haoyu Li', 'Qirui Fu', 'Tai Zhong', 'Xinyu Wang', 'Xiangru Tang', 'Nan Tang', 'Chenglin Wu', 'Yuyu Luo'], 'affiliations': ['Agent Universe', 'DeepWisdom', 'McGill University', 'Renmin University of China', 'The Hong Kong University of Science and Technology (Guangzhou)', 'University of California, Los Angeles', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2510.24668.jpg', 'data': {'categories': ['#alignment', '#interpretability', '#benchmark', '#agents', '#reasoning'], 'emoji': 'â“', 'ru': {'title': 'ĞŸĞ¾Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ½Ğµ ÑƒĞ¼ĞµÑÑ‚ Ğ·Ğ°Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ ÑƒÑ‚Ğ¾Ñ‡Ğ½ÑÑÑ‰Ğ¸Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº InteractComp Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ñ… AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ½ĞµĞ¾Ğ´Ğ½Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¸ ÑƒÑ‚Ğ¾Ñ‡Ğ½ÑÑ‚ÑŒ Ğ¸Ñ… Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³ Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¼. Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ 17 Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ñ€Ğ¾Ğ²Ğ°Ğ»: Ğ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ° Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ 13.73% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ½ĞµĞ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ² 71.50% Ñ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹. ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğµ Ğ² Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚ĞºĞµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ° Ğ² ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞ°Ğ¼Ğ¾ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ½Ğµ Ğ¾ÑĞ¾Ğ·Ğ½Ğ°ÑÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹. ĞŸÑ€Ğ¸ Ğ¿Ñ€Ğ¸Ğ½ÑƒĞ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ñ€ĞµĞ·ĞºĞ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ÑÑ, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… LLM, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ½Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚.'}, 'en': {'title': "Bridging the Gap: Enhancing Search Agents' Interaction Skills", 'desc': 'InteractComp is a new benchmark that tests how well search agents can identify and clarify ambiguous user queries through interaction. Current models often assume that user queries are clear and complete, which is not the case in real-world scenarios. The benchmark includes 210 carefully designed questions that create genuine ambiguity, requiring agents to engage with users to resolve it. Results show that while search performance has improved, the ability to interact and clarify queries has stagnated, highlighting a significant area for development in search agent capabilities.'}, 'zh': {'title': 'æå‡æœç´¢ä»£ç†çš„äº’åŠ¨èƒ½åŠ›', 'desc': 'InteractCompæ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†ï¼Œç”¨äºè¯„ä¼°æœç´¢ä»£ç†è¯†åˆ«å’Œè§£å†³æŸ¥è¯¢æ¨¡ç³Šæ€§çš„èƒ½åŠ›ã€‚ç°æœ‰çš„æœç´¢æ¨¡å‹é€šå¸¸å‡è®¾ç”¨æˆ·çš„æŸ¥è¯¢æ˜¯å®Œæ•´ä¸”æ˜ç¡®çš„ï¼Œä½†å®é™…ä¸Šç”¨æˆ·çš„æŸ¥è¯¢å¾€å¾€æ˜¯ä¸å®Œæ•´çš„ï¼Œéœ€è¦é€šè¿‡äº’åŠ¨æ¥æ¾„æ¸…ã€‚æˆ‘ä»¬è®¾è®¡äº†210ä¸ªä¸“å®¶ç­–åˆ’çš„é—®é¢˜ï¼Œæ—¨åœ¨é€šè¿‡äº’åŠ¨æ¥è§£å†³çœŸå®çš„æ¨¡ç³Šæ€§ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œå°½ç®¡æœç´¢æ€§èƒ½æœ‰æ‰€æé«˜ï¼Œä½†äº’åŠ¨èƒ½åŠ›å´åœæ»ä¸å‰ï¼Œè¿™è¡¨æ˜å½“å‰ç­–ç•¥æœªèƒ½æœ‰æ•ˆåˆ©ç”¨æ½œåœ¨èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.24701', 'title': 'Tongyi DeepResearch Technical Report', 'url': 'https://huggingface.co/papers/2510.24701', 'abstract': "Tongyi DeepResearch, a large language model with agentic capabilities, achieves top performance in various deep research tasks through an end-to-end training framework and automated data synthesis.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Tongyi DeepResearch, an agentic large language model, which is specifically designed for long-horizon, deep information-seeking research tasks. To incentivize autonomous deep research agency, Tongyi DeepResearch is developed through an end-to-end training framework that combines agentic mid-training and agentic post-training, enabling scalable reasoning and information seeking across complex tasks. We design a highly scalable data synthesis pipeline that is fully automatic, without relying on costly human annotation, and empowers all training stages. By constructing customized environments for each stage, our system enables stable and consistent interactions throughout. Tongyi DeepResearch, featuring 30.5 billion total parameters, with only 3.3 billion activated per token, achieves state-of-the-art performance across a range of agentic deep research benchmarks, including Humanity's Last Exam, BrowseComp, BrowseComp-ZH, WebWalkerQA, xbench-DeepSearch, FRAMES and xbench-DeepSearch-2510. We open-source the model, framework, and complete solutions to empower the community.", 'score': 68, 'issue_id': 6667, 'pub_date': '2025-10-28', 'pub_date_card': {'ru': '28 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 28', 'zh': '10æœˆ28æ—¥'}, 'hash': 'a6f467c756fc29b7', 'authors': ['Tongyi DeepResearch Team', 'Baixuan Li', 'Bo Zhang', 'Dingchu Zhang', 'Fei Huang', 'Guangyu Li', 'Guoxin Chen', 'Huifeng Yin', 'Jialong Wu', 'Jingren Zhou', 'Kuan Li', 'Liangcai Su', 'Litu Ou', 'Liwen Zhang', 'Pengjun Xie', 'Rui Ye', 'Wenbiao Yin', 'Xinmiao Yu', 'Xinyu Wang', 'Xixi Wu', 'Xuanzhong Chen', 'Yida Zhao', 'Zhen Zhang', 'Zhengwei Tao', 'Zhongwang Zhang', 'Zile Qiao', 'Chenxi Wang', 'Donglei Yu', 'Gang Fu', 'Haiyang Shen', 'Jiayin Yang', 'Jun Lin', 'Junkai Zhang', 'Kui Zeng', 'Li Yang', 'Hailong Yin', 'Maojia Song', 'Ming Yan', 'Peng Xia', 'Qian Xiao', 'Rui Min', 'Ruixue Ding', 'Runnan Fang', 'Shaowei Chen', 'Shen Huang', 'Shihang Wang', 'Shihao Cai', 'Weizhou Shen', 'Xiaobin Wang', 'Xin Guan', 'Xinyu Geng', 'Yingcheng Shi', 'Yuning Wu', 'Zhuo Chen', 'Zijian Li', 'Yong Jiang'], 'affiliations': ['Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2510.24701.jpg', 'data': {'categories': ['#long_context', '#synthetic', '#dataset', '#benchmark', '#training', '#open_source', '#agi', '#agents'], 'emoji': 'ğŸ”', 'ru': {'title': 'ĞĞ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ğ¹ AI-Ğ°Ğ³ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹', 'desc': 'Tongyi DeepResearch - ÑÑ‚Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸, ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‰Ğ°ÑÑÑ Ğ½Ğ° Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ğ¼ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ¼ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ñ‡ĞµÑ€ĞµĞ· end-to-end Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ mid-training Ğ¸ post-training, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ Ğ¸ Ğ¸ÑĞºĞ°Ñ‚ÑŒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ - Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ±ĞµĞ· Ğ´Ğ¾Ñ€Ğ¾Ğ³Ğ¾ÑÑ‚Ğ¾ÑÑ‰ĞµĞ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸. ĞŸÑ€Ğ¸ 30.5 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ¸Ğ· ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ğ°ĞºÑ‚Ğ¸Ğ²Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ 3.3 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ° Ğ½Ğ° Ñ‚Ğ¾ĞºĞµĞ½, Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğµ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ² Ğ´Ğ»Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹.'}, 'en': {'title': 'Empowering Autonomous Deep Research with Tongyi DeepResearch', 'desc': 'Tongyi DeepResearch is a large language model designed for complex research tasks that require deep information-seeking capabilities. It utilizes an end-to-end training framework that includes both mid-training and post-training phases to enhance its autonomous research abilities. The model features a fully automated data synthesis pipeline, eliminating the need for expensive human annotations, which allows for scalable training across various tasks. With 30.5 billion parameters, Tongyi DeepResearch achieves top performance on multiple benchmarks, demonstrating its effectiveness in deep research applications.'}, 'zh': {'title': 'è‡ªä¸»æ·±åº¦ç ”ç©¶çš„æœªæ¥', 'desc': 'Tongyi DeepResearch æ˜¯ä¸€ç§å…·æœ‰è‡ªä¸»èƒ½åŠ›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä¸“é—¨ç”¨äºé•¿æ—¶é—´çš„ä¿¡æ¯æ£€ç´¢ç ”ç©¶ä»»åŠ¡ã€‚å®ƒé€šè¿‡ç«¯åˆ°ç«¯çš„è®­ç»ƒæ¡†æ¶å’Œè‡ªåŠ¨åŒ–çš„æ•°æ®åˆæˆï¼Œèƒ½å¤Ÿåœ¨å¤æ‚ä»»åŠ¡ä¸­å®ç°å¯æ‰©å±•çš„æ¨ç†å’Œä¿¡æ¯è·å–ã€‚è¯¥æ¨¡å‹å…·æœ‰ 305 äº¿ä¸ªå‚æ•°ï¼Œä¸”æ¯ä¸ªä»¤ç‰Œä»…æ¿€æ´» 33 äº¿ä¸ªå‚æ•°ï¼Œè¡¨ç°å‡ºè‰²ï¼Œè¾¾åˆ°äº†å¤šé¡¹æ·±åº¦ç ”ç©¶åŸºå‡†çš„æœ€å…ˆè¿›æ°´å¹³ã€‚æˆ‘ä»¬å°†æ¨¡å‹ã€æ¡†æ¶å’Œå®Œæ•´è§£å†³æ–¹æ¡ˆå¼€æºï¼Œä»¥æ”¯æŒç¤¾åŒºçš„å‘å±•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.24699', 'title': 'AgentFold: Long-Horizon Web Agents with Proactive Context Management', 'url': 'https://huggingface.co/papers/2510.24699', 'abstract': "AgentFold, a novel proactive context management paradigm for LLM-based web agents, achieves superior performance on long-horizon tasks through dynamic context folding, surpassing larger models and proprietary agents.  \t\t\t\t\tAI-generated summary \t\t\t\t LLM-based web agents show immense promise for information seeking, yet their effectiveness on long-horizon tasks is hindered by a fundamental trade-off in context management. Prevailing ReAct-based agents suffer from context saturation as they accumulate noisy, raw histories, while methods that fixedly summarize the full history at each step risk the irreversible loss of critical details. Addressing these, we introduce AgentFold, a novel agent paradigm centered on proactive context management, inspired by the human cognitive process of retrospective consolidation. AgentFold treats its context as a dynamic cognitive workspace to be actively sculpted, rather than a passive log to be filled. At each step, it learns to execute a `folding' operation, which manages its historical trajectory at multiple scales: it can perform granular condensations to preserve vital, fine-grained details, or deep consolidations to abstract away entire multi-step sub-tasks. The results on prominent benchmarks are striking: with simple supervised fine-tuning (without continual pre-training or RL), our AgentFold-30B-A3B agent achieves 36.2% on BrowseComp and 47.3% on BrowseComp-ZH. Notably, this performance not only surpasses or matches open-source models of a dramatically larger scale, such as the DeepSeek-V3.1-671B-A37B, but also surpasses leading proprietary agents like OpenAI's o4-mini.", 'score': 52, 'issue_id': 6668, 'pub_date': '2025-10-28', 'pub_date_card': {'ru': '28 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 28', 'zh': '10æœˆ28æ—¥'}, 'hash': 'c4184e162b0c817e', 'authors': ['Rui Ye', 'Zhongwang Zhang', 'Kuan Li', 'Huifeng Yin', 'Zhengwei Tao', 'Yida Zhao', 'Liangcai Su', 'Liwen Zhang', 'Zile Qiao', 'Xinyu Wang', 'Pengjun Xie', 'Fei Huang', 'Siheng Chen', 'Jingren Zhou', 'Yong Jiang'], 'affiliations': ['Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2510.24699.jpg', 'data': {'categories': ['#long_context', '#benchmark', '#training', '#agents'], 'emoji': 'ğŸ—‚ï¸', 'ru': {'title': 'Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑĞ²Ğ¾Ñ€Ğ°Ñ‡Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ²ĞµĞ±-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'AgentFold â€” ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ´Ğ»Ñ Ğ²ĞµĞ±-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LLM, Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ğ°Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ¼ Ñ€ĞµÑ‚Ñ€Ğ¾ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ½ÑĞ¾Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¼ Ğ¿Ğ¾Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¸. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ğ°ÑÑĞ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹, Ğ°Ğ³ĞµĞ½Ñ‚ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Â«ÑĞºĞ»Ğ°Ğ´Ñ‹Ğ²Ğ°ĞµÑ‚Â» ÑĞ²Ğ¾Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚, Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ Ğ³Ñ€Ğ°Ğ½ÑƒĞ»ÑÑ€Ğ½Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ñ… Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹ Ğ¸Ğ»Ğ¸ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºÑƒÑ ĞºĞ¾Ğ½ÑĞ¾Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ Ñ†ĞµĞ»Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ Ğ¿ĞµÑ€ĞµĞ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° ÑˆÑƒĞ¼Ğ¾Ğ¼ Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸, Ñ‡Ñ‚Ğ¾ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ½Ğ¾ Ğ´Ğ»Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ReAct-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ AgentFold-30B Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² 20 Ñ€Ğ°Ğ· Ğ±Ğ¾Ğ»ÑŒÑˆĞµĞ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¸ proprietary Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ²Ñ€Ğ¾Ğ´Ğµ OpenAI o4-mini Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡.'}, 'en': {'title': 'Dynamic Context Management for Superior LLM Performance', 'desc': 'AgentFold is a new approach for managing context in large language model (LLM) web agents, designed to improve their performance on long tasks. It addresses the issue of context saturation found in existing agents by dynamically folding context, which helps retain important details while reducing noise. This method mimics human cognitive processes, allowing the agent to actively manage its historical information rather than just accumulating it. The results show that AgentFold outperforms larger models and proprietary agents on key benchmarks with minimal fine-tuning.'}, 'zh': {'title': 'AgentFoldï¼šé•¿ä»»åŠ¡ä¸­çš„ä¸Šä¸‹æ–‡ç®¡ç†æ–°èŒƒå¼', 'desc': 'AgentFoldæ˜¯ä¸€ç§æ–°é¢–çš„ä¸»åŠ¨ä¸Šä¸‹æ–‡ç®¡ç†èŒƒå¼ï¼Œä¸“ä¸ºåŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç½‘ç»œä»£ç†è®¾è®¡ã€‚å®ƒé€šè¿‡åŠ¨æ€ä¸Šä¸‹æ–‡æŠ˜å æŠ€æœ¯ï¼Œåœ¨é•¿æ—¶é—´ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†æ›´å¤§æ¨¡å‹å’Œä¸“æœ‰ä»£ç†ã€‚ä¸ä¼ ç»Ÿçš„ReActä»£ç†ç›¸æ¯”ï¼ŒAgentFoldæœ‰æ•ˆè§£å†³äº†ä¸Šä¸‹æ–‡é¥±å’Œçš„é—®é¢˜ï¼Œé¿å…äº†é‡è¦ç»†èŠ‚çš„ä¸¢å¤±ã€‚é€šè¿‡å°†ä¸Šä¸‹æ–‡è§†ä¸ºåŠ¨æ€çš„è®¤çŸ¥å·¥ä½œç©ºé—´ï¼ŒAgentFoldèƒ½å¤Ÿåœ¨å¤šä¸ªå±‚é¢ä¸Šç®¡ç†å†å²è½¨è¿¹ï¼Œä»è€Œå®ç°æ›´é«˜æ•ˆçš„ä¿¡æ¯å¤„ç†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.23763', 'title': 'RoboOmni: Proactive Robot Manipulation in Omni-modal Context', 'url': 'https://huggingface.co/papers/2510.23763', 'abstract': 'RoboOmni, a Perceiver-Thinker-Talker-Executor framework using end-to-end omni-modal LLMs, improves robotic manipulation by inferring user intentions from spoken dialogue, environmental sounds, and visual cues.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in Multimodal Large Language Models (MLLMs) have driven rapid progress in Vision-Language-Action (VLA) models for robotic manipulation. Although effective in many scenarios, current approaches largely rely on explicit instructions, whereas in real-world interactions, humans rarely issue instructions directly. Effective collaboration requires robots to infer user intentions proactively. In this work, we introduce cross-modal contextual instructions, a new setting where intent is derived from spoken dialogue, environmental sounds, and visual cues rather than explicit commands. To address this new setting, we present RoboOmni, a Perceiver-Thinker-Talker-Executor framework based on end-to-end omni-modal LLMs that unifies intention recognition, interaction confirmation, and action execution. RoboOmni fuses auditory and visual signals spatiotemporally for robust intention recognition, while supporting direct speech interaction. To address the absence of training data for proactive intention recognition in robotic manipulation, we build OmniAction, comprising 140k episodes, 5k+ speakers, 2.4k event sounds, 640 backgrounds, and six contextual instruction types. Experiments in simulation and real-world settings show that RoboOmni surpasses text- and ASR-based baselines in success rate, inference speed, intention recognition, and proactive assistance.', 'score': 50, 'issue_id': 6668, 'pub_date': '2025-10-27', 'pub_date_card': {'ru': '27 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 27', 'zh': '10æœˆ27æ—¥'}, 'hash': 'b37a82d0c1d4c433', 'authors': ['Siyin Wang', 'Jinlan Fu', 'Feihong Liu', 'Xinzhe He', 'Huangxuan Wu', 'Junhao Shi', 'Kexin Huang', 'Zhaoye Fei', 'Jingjing Gong', 'Zuxuan Wu', 'Yugang Jiang', 'See-Kiong Ng', 'Tat-Seng Chua', 'Xipeng Qiu'], 'affiliations': ['Fudan University', 'National University of Singapore', 'Shanghai Innovation Institute'], 'pdf_title_img': 'assets/pdf/title_img/2510.23763.jpg', 'data': {'categories': ['#agents', '#dataset', '#optimization', '#games', '#multimodal', '#interpretability'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ Ğ¾Ğ±Ğ¾Ñ‚, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ğ¿Ñ€ÑĞ¼Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ RoboOmni â€” ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ¾Ğ², Ğ·Ğ²ÑƒĞºĞ¾Ğ² Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ², Ğ° Ğ½Ğµ Ğ¸Ğ· ÑĞ²Ğ½Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° omni-modal LLM Ğ¸ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğ¹, Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³ Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ OmniAction Ñ 140 Ñ‚Ñ‹ÑÑÑ‡Ğ°Ğ¼Ğ¸ ÑĞ¿Ğ¸Ğ·Ğ¾Ğ´Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ RoboOmni Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ baseline-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ğ¿Ñ€Ğ¾Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ¸.'}, 'en': {'title': 'RoboOmni: Understanding Intentions for Smarter Robot Interaction', 'desc': 'RoboOmni is a new framework that enhances robotic manipulation by understanding user intentions through various inputs like speech, sounds, and visual information. Unlike traditional models that depend on clear instructions, RoboOmni can infer what users want based on context, making it more effective in real-world situations. It combines different types of data using advanced multimodal large language models (MLLMs) to recognize intentions and execute actions seamlessly. The framework is trained on a large dataset called OmniAction, which helps it perform better in both simulated and real environments.'}, 'zh': {'title': 'RoboOmniï¼šæ™ºèƒ½æœºå™¨äººä¸»åŠ¨ç†è§£ç”¨æˆ·æ„å›¾çš„å…¨æ–°æ¡†æ¶', 'desc': 'RoboOmniæ˜¯ä¸€ä¸ªåŸºäºç«¯åˆ°ç«¯å…¨æ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡æ¨æ–­ç”¨æˆ·æ„å›¾æ¥æ”¹å–„æœºå™¨äººæ“ä½œã€‚è¯¥æ¡†æ¶ç»“åˆäº†è¯­éŸ³å¯¹è¯ã€ç¯å¢ƒå£°éŸ³å’Œè§†è§‰çº¿ç´¢ï¼Œèƒ½å¤Ÿåœ¨æ²¡æœ‰æ˜ç¡®æŒ‡ä»¤çš„æƒ…å†µä¸‹è¿›è¡Œæœ‰æ•ˆçš„åä½œã€‚RoboOmnié€šè¿‡è·¨æ¨¡æ€ä¸Šä¸‹æ–‡æŒ‡ä»¤æ¥è¯†åˆ«æ„å›¾ï¼Œå¹¶æ”¯æŒç›´æ¥çš„è¯­éŸ³äº¤äº’ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRoboOmniåœ¨æˆåŠŸç‡ã€æ¨ç†é€Ÿåº¦å’Œä¸»åŠ¨ååŠ©æ–¹é¢ä¼˜äºä¼ ç»Ÿçš„æ–‡æœ¬å’Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«åŸºçº¿ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.23691', 'title': 'Game-TARS: Pretrained Foundation Models for Scalable Generalist\n  Multimodal Game Agents', 'url': 'https://huggingface.co/papers/2510.23691', 'abstract': 'Game-TARS, a generalist game agent trained with a unified action space, achieves superior performance across various domains and benchmarks through large-scale pre-training and efficient reasoning strategies.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Game-TARS, a generalist game agent trained with a unified, scalable action space anchored to human-aligned native keyboard-mouse inputs. Unlike API- or GUI-based approaches, this paradigm enables large-scale continual pre-training across heterogeneous domains, including OS, web, and simulation games. Game-TARS is pre-trained on over 500B tokens with diverse trajectories and multimodal data. Key techniques include a decaying continual loss to reduce causal confusion and an efficient Sparse-Thinking strategy that balances reasoning depth and inference cost. Experiments show that Game-TARS achieves about 2 times the success rate over the previous sota model on open-world Minecraft tasks, is close to the generality of fresh humans in unseen web 3d games, and outperforms GPT-5, Gemini-2.5-Pro, and Claude-4-Sonnet in FPS benchmarks. Scaling results on training-time and test-time confirm that the unified action space sustains improvements when scaled to cross-game and multimodal data. Our results demonstrate that simple, scalable action representations combined with large-scale pre-training provide a promising path toward generalist agents with broad computer-use abilities.', 'score': 43, 'issue_id': 6668, 'pub_date': '2025-10-27', 'pub_date_card': {'ru': '27 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 27', 'zh': '10æœˆ27æ—¥'}, 'hash': '9070d487a826ae6f', 'authors': ['Zihao Wang', 'Xujing Li', 'Yining Ye', 'Junjie Fang', 'Haoming Wang', 'Longxiang Liu', 'Shihao Liang', 'Junting Lu', 'Zhiyong Wu', 'Jiazhan Feng', 'Wanjun Zhong', 'Zili Li', 'Yu Wang', 'Yu Miao', 'Bo Zhou', 'Yuanfan Li', 'Hao Wang', 'Zhongkai Zhao', 'Faming Wu', 'Zhengxuan Jiang', 'Weihao Tan', 'Heyuan Yao', 'Shi Yan', 'Xiangyang Li', 'Yitao Liang', 'Yujia Qin', 'Guang Shi'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2510.23691.jpg', 'data': {'categories': ['#agents', '#training', '#games', '#benchmark', '#multimodal', '#reasoning'], 'emoji': 'ğŸ®', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¸Ğ³Ñ€Ğ¾Ğ²Ğ¾Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚ Ñ‡ĞµÑ€ĞµĞ· ĞµĞ´Ğ¸Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹', 'desc': 'Game-TARS â€” ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¸Ğ³Ñ€Ğ¾Ğ²Ğ¾Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ½Ğ° Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑÑ… ĞºĞ»Ğ°Ğ²Ğ¸Ğ°Ñ‚ÑƒÑ€Ñ‹ Ğ¸ Ğ¼Ñ‹ÑˆĞ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€Ğ¾ÑˆĞ»Ğ° Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 500 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ€Ğ°Ğ·Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ¸Ğ· Ğ¸Ğ³Ñ€, Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¸ Ğ²ĞµĞ±-Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ continual loss Ñ Ğ·Ğ°Ñ‚ÑƒÑ…Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ°ÑƒĞ·Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿ÑƒÑ‚Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Sparse-Thinking Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ³ĞµĞ½Ñ‚ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ» Ğ´Ğ²ÑƒĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ½Ğ°Ğ´ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ SOTA Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Minecraft Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾ÑˆÑ‘Ğ» GPT-5, Gemini-2.5-Pro Ğ¸ Claude-4-Sonnet Ğ² FPS Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ….'}, 'en': {'title': 'Game-TARS: A Unified Agent for Diverse Gaming Excellence', 'desc': 'Game-TARS is a versatile game agent designed to operate across different gaming environments using a unified action space that mimics human keyboard and mouse inputs. It undergoes extensive pre-training on a massive dataset of over 500 billion tokens, which includes varied gameplay experiences and multimodal information. The agent employs innovative techniques like a decaying continual loss to minimize confusion in decision-making and a Sparse-Thinking strategy to optimize reasoning efficiency. Experimental results indicate that Game-TARS significantly outperforms previous models in various gaming tasks, showcasing its potential as a generalist agent capable of adapting to diverse computer-based activities.'}, 'zh': {'title': 'Game-TARSï¼šé€šç”¨æ¸¸æˆä»£ç†çš„æœªæ¥', 'desc': 'Game-TARSæ˜¯ä¸€ç§é€šç”¨æ¸¸æˆä»£ç†ï¼Œé‡‡ç”¨ç»Ÿä¸€çš„å¯æ‰©å±•åŠ¨ä½œç©ºé—´è¿›è¡Œè®­ç»ƒï¼Œèƒ½å¤Ÿåœ¨å¤šä¸ªé¢†åŸŸå’ŒåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ã€‚å®ƒé€šè¿‡å¤§è§„æ¨¡çš„æŒç»­é¢„è®­ç»ƒï¼Œç»“åˆå¤šæ¨¡æ€æ•°æ®ï¼Œæå‡äº†åœ¨æ“ä½œç³»ç»Ÿã€ç½‘é¡µå’Œæ¨¡æ‹Ÿæ¸¸æˆç­‰å¼‚æ„é¢†åŸŸçš„æ€§èƒ½ã€‚å…³é”®æŠ€æœ¯åŒ…æ‹¬é€æ¸å‡å°çš„æŒç»­æŸå¤±ï¼Œä»¥å‡å°‘å› æœæ··æ·†ï¼Œä»¥åŠé«˜æ•ˆçš„ç¨€ç–æ€ç»´ç­–ç•¥ï¼Œå¹³è¡¡æ¨ç†æ·±åº¦å’Œæ¨ç†æˆæœ¬ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGame-TARSåœ¨å¼€æ”¾ä¸–ç•Œçš„Minecraftä»»åŠ¡ä¸­æˆåŠŸç‡æ˜¯ä¹‹å‰æœ€ä½³æ¨¡å‹çš„ä¸¤å€ï¼Œå¹¶åœ¨æœªè§è¿‡çš„ç½‘é¡µ3Dæ¸¸æˆä¸­æ¥è¿‘æ–°æ‰‹ç©å®¶çš„è¡¨ç°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.24717', 'title': 'Uniform Discrete Diffusion with Metric Path for Video Generation', 'url': 'https://huggingface.co/papers/2510.24717', 'abstract': 'URSA, a discrete generative model, bridges the gap with continuous approaches in video generation by using a Linearized Metric Path and Resolution-dependent Timestep Shifting, achieving high-resolution and long-duration synthesis with fewer inference steps.  \t\t\t\t\tAI-generated summary \t\t\t\t Continuous-space video generation has advanced rapidly, while discrete approaches lag behind due to error accumulation and long-context inconsistency. In this work, we revisit discrete generative modeling and present Uniform discRete diffuSion with metric pAth (URSA), a simple yet powerful framework that bridges the gap with continuous approaches for the scalable video generation. At its core, URSA formulates the video generation task as an iterative global refinement of discrete spatiotemporal tokens. It integrates two key designs: a Linearized Metric Path and a Resolution-dependent Timestep Shifting mechanism. These designs enable URSA to scale efficiently to high-resolution image synthesis and long-duration video generation, while requiring significantly fewer inference steps. Additionally, we introduce an asynchronous temporal fine-tuning strategy that unifies versatile tasks within a single model, including interpolation and image-to-video generation. Extensive experiments on challenging video and image generation benchmarks demonstrate that URSA consistently outperforms existing discrete methods and achieves performance comparable to state-of-the-art continuous diffusion methods. Code and models are available at https://github.com/baaivision/URSA', 'score': 34, 'issue_id': 6670, 'pub_date': '2025-10-28', 'pub_date_card': {'ru': '28 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 28', 'zh': '10æœˆ28æ—¥'}, 'hash': '2191a1dd1f4af127', 'authors': ['Haoge Deng', 'Ting Pan', 'Fan Zhang', 'Yang Liu', 'Zhuoyan Luo', 'Yufeng Cui', 'Wenxuan Wang', 'Chunhua Shen', 'Shiguang Shan', 'Zhaoxiang Zhang', 'Xinlong Wang'], 'affiliations': ['Beijing Academy of Artificial Intelligence', 'Key Laboratory of Intelligent Information Processing, ICT, CAS', 'National Laboratory of Pattern Recognition, CASIA', 'University of Chinese Academy of Sciences', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2510.24717.jpg', 'data': {'categories': ['#diffusion', '#benchmark', '#architecture', '#video'], 'emoji': 'ğŸ¥', 'ru': {'title': 'URSA: ĞĞ¾Ğ²Ğ°Ñ ÑÑ€Ğ° Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ URSA, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ¶Ğ°Ñ Ğ¸Ñ… Ğº Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼. URSA Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿ÑƒÑ‚ÑŒ Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ÑĞ´Ğ²Ğ¸Ğ³Ğ° Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¸ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ ÑˆĞ°Ğ³Ğ¾Ğ² Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ°ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ² Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ URSA Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ², ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ñ‹Ñ… Ñ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸.'}, 'en': {'title': 'URSA: Bridging Discrete and Continuous Video Generation', 'desc': 'URSA is a new discrete generative model designed to improve video generation by addressing the limitations of traditional discrete methods. It uses a Linearized Metric Path and Resolution-dependent Timestep Shifting to enhance the quality and duration of generated videos while minimizing the number of inference steps needed. The model refines discrete spatiotemporal tokens iteratively, allowing for high-resolution outputs and long video sequences. Additionally, URSA incorporates an asynchronous temporal fine-tuning strategy to handle various tasks like interpolation and image-to-video generation within a single framework.'}, 'zh': {'title': 'URSAï¼šé«˜æ•ˆçš„è§†é¢‘ç”Ÿæˆæ–°æ–¹æ³•', 'desc': 'URSAæ˜¯ä¸€ç§ç¦»æ•£ç”Ÿæˆæ¨¡å‹ï¼Œé€šè¿‡çº¿æ€§åº¦é‡è·¯å¾„å’Œåˆ†è¾¨ç‡ä¾èµ–çš„æ—¶é—´æ­¥ç§»æœºåˆ¶ï¼Œç¼©å°äº†è§†é¢‘ç”Ÿæˆä¸­ç¦»æ•£æ–¹æ³•ä¸è¿ç»­æ–¹æ³•ä¹‹é—´çš„å·®è·ã€‚å®ƒå°†è§†é¢‘ç”Ÿæˆä»»åŠ¡è§†ä¸ºç¦»æ•£æ—¶ç©ºæ ‡è®°çš„è¿­ä»£å…¨å±€ä¼˜åŒ–ï¼Œä»è€Œå®ç°é«˜åˆ†è¾¨ç‡å’Œé•¿æ—¶é•¿çš„è§†é¢‘åˆæˆã€‚URSAçš„è®¾è®¡ä½¿å…¶åœ¨ç”Ÿæˆé«˜è´¨é‡å›¾åƒå’Œé•¿æ—¶é—´è§†é¢‘æ—¶ï¼Œæ‰€éœ€çš„æ¨ç†æ­¥éª¤æ˜¾è‘—å‡å°‘ã€‚æ­¤å¤–ï¼ŒURSAè¿˜å¼•å…¥äº†ä¸€ç§å¼‚æ­¥æ—¶é—´å¾®è°ƒç­–ç•¥ï¼Œèƒ½å¤Ÿåœ¨å•ä¸€æ¨¡å‹ä¸­ç»Ÿä¸€å¤šç§ä»»åŠ¡ï¼ŒåŒ…æ‹¬æ’å€¼å’Œå›¾åƒåˆ°è§†é¢‘çš„ç”Ÿæˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.24694', 'title': 'Repurposing Synthetic Data for Fine-grained Search Agent Supervision', 'url': 'https://huggingface.co/papers/2510.24694', 'abstract': 'Entity-aware Group Relative Policy Optimization (E-GRPO) enhances search agents by incorporating entity information into the reward function, improving accuracy and efficiency in knowledge-intensive tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t LLM-based search agents are increasingly trained on entity-centric synthetic data to solve complex, knowledge-intensive tasks. However, prevailing training methods like Group Relative Policy Optimization (GRPO) discard this rich entity information, relying instead on sparse, outcome-based rewards. This critical limitation renders them unable to distinguish informative "near-miss" samples-those with substantially correct reasoning but a flawed final answer-from complete failures, thus discarding valuable learning signals. We address this by leveraging the very entities discarded during training. Our empirical analysis reveals a strong positive correlation between the number of ground-truth entities identified during an agent\'s reasoning process and final answer accuracy. Building on this insight, we introduce Entity-aware Group Relative Policy Optimization (E-GRPO), a novel framework that formulates a dense entity-aware reward function. E-GRPO assigns partial rewards to incorrect samples proportional to their entity match rate, enabling the model to effectively learn from these "near-misses". Experiments on diverse question-answering (QA) and deep research benchmarks show that E-GRPO consistently and significantly outperforms the GRPO baseline. Furthermore, our analysis reveals that E-GRPO not only achieves superior accuracy but also induces more efficient reasoning policies that require fewer tool calls, demonstrating a more effective and sample-efficient approach to aligning search agents.', 'score': 20, 'issue_id': 6668, 'pub_date': '2025-10-28', 'pub_date_card': {'ru': '28 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 28', 'zh': '10æœˆ28æ—¥'}, 'hash': '57a3b452eab7371c', 'authors': ['Yida Zhao', 'Kuan Li', 'Xixi Wu', 'Liwen Zhang', 'Dingchu Zhang', 'Baixuan Li', 'Maojia Song', 'Zhuo Chen', 'Chenxi Wang', 'Xinyu Wang', 'Kewei Tu', 'Pengjun Xie', 'Jingren Zhou', 'Yong Jiang'], 'affiliations': ['Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2510.24694.jpg', 'data': {'categories': ['#training', '#optimization', '#rl', '#agents', '#reasoning'], 'emoji': 'ğŸ¯', 'ru': {'title': 'Ğ£Ñ‡Ğ¸Ğ¼ÑÑ Ğ½Ğ° Â«Ğ¿Ğ¾Ñ‡Ñ‚Ğ¸ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ…Â» Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ñ…: Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ·Ğ° Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ½Ñ‹Ğµ ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Entity-aware Group Relative Policy Optimization (E-GRPO) â€” ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ GRPO, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, E-GRPO ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ± ÑƒĞ¿Ğ¾Ğ¼ÑĞ½ÑƒÑ‚Ñ‹Ñ… ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚ÑÑ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡Ğ°ĞµÑ‚ Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ·Ğ° Â«Ğ¿Ğ¾Ñ‡Ñ‚Ğ¸ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹ĞµÂ» Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ¿Ñ€Ğ¾Ğ¿Ğ¾Ñ€Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ‡Ğ¸ÑĞ»Ñƒ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ½Ñ‹Ñ… ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚ĞµĞ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ĞµĞµ ÑƒÑ‡Ğ¸Ñ‚ÑŒÑÑ Ğ½Ğ° Ğ¾ÑˆĞ¸Ğ±ĞºĞ°Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ E-GRPO Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¹ GRPO Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸, Ñ‚Ñ€ĞµĞ±ÑƒÑ Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ¾Ğ±Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğ¹ Ğº Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼ Ğ¿Ğ¾Ğ¸ÑĞºĞ°.'}, 'en': {'title': 'Enhancing Search Agents with Entity-Aware Learning', 'desc': 'Entity-aware Group Relative Policy Optimization (E-GRPO) improves the training of search agents by integrating entity information into the reward system, which enhances their performance on complex tasks. Traditional methods like Group Relative Policy Optimization (GRPO) overlook valuable entity data, leading to a loss of learning opportunities from near-miss samples. E-GRPO addresses this by providing partial rewards based on the match rate of identified entities, allowing agents to learn from their mistakes more effectively. Experimental results show that E-GRPO not only boosts accuracy but also promotes more efficient reasoning, requiring fewer resources to achieve better outcomes.'}, 'zh': {'title': 'å®ä½“æ„ŸçŸ¥ä¼˜åŒ–ï¼Œæå‡æœç´¢ä»£ç†çš„æ™ºèƒ½ï¼', 'desc': 'E-GRPOï¼ˆå®ä½“æ„ŸçŸ¥ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼‰é€šè¿‡å°†å®ä½“ä¿¡æ¯çº³å…¥å¥–åŠ±å‡½æ•°ï¼Œå¢å¼ºäº†æœç´¢ä»£ç†çš„èƒ½åŠ›ï¼Œä»è€Œæé«˜äº†åœ¨çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ä¸­çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚ä¼ ç»Ÿçš„è®­ç»ƒæ–¹æ³•å¦‚GRPOå¿½è§†äº†ä¸°å¯Œçš„å®ä½“ä¿¡æ¯ï¼Œä¾èµ–ç¨€ç–çš„åŸºäºç»“æœçš„å¥–åŠ±ï¼Œå¯¼è‡´æ— æ³•æœ‰æ•ˆåŒºåˆ†æœ‰ä»·å€¼çš„â€œè¿‘ä¹æ­£ç¡®â€æ ·æœ¬ã€‚æˆ‘ä»¬é€šè¿‡åˆ©ç”¨è®­ç»ƒä¸­è¢«ä¸¢å¼ƒçš„å®ä½“ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„å¯†é›†å®ä½“æ„ŸçŸ¥å¥–åŠ±å‡½æ•°ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿä»è¿™äº›â€œè¿‘ä¹æ­£ç¡®â€çš„æ ·æœ¬ä¸­æœ‰æ•ˆå­¦ä¹ ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒE-GRPOåœ¨å¤šç§é—®ç­”å’Œæ·±åº¦ç ”ç©¶åŸºå‡†ä¸Šæ˜¾è‘—ä¼˜äºGRPOåŸºçº¿ï¼Œä¸”åœ¨æé«˜å‡†ç¡®æ€§çš„åŒæ—¶ï¼Œå‡å°‘äº†å·¥å…·è°ƒç”¨æ¬¡æ•°ï¼Œå±•ç°å‡ºæ›´é«˜æ•ˆçš„æ¨ç†ç­–ç•¥ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.24563', 'title': 'OSWorld-MCP: Benchmarking MCP Tool Invocation In Computer-Use Agents', 'url': 'https://huggingface.co/papers/2510.24563', 'abstract': "OSWorld-MCP is a benchmark that evaluates multimodal agents' tool invocation, GUI operation, and decision-making abilities, highlighting the importance of assessing tool usage in real-world scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t With advances in decision-making and reasoning capabilities, multimodal agents show strong potential in computer application scenarios. Past evaluations have mainly assessed GUI interaction skills, while tool invocation abilities, such as those enabled by the Model Context Protocol (MCP), have been largely overlooked. Comparing agents with integrated tool invocation to those evaluated only on GUI interaction is inherently unfair. We present OSWorld-MCP, the first comprehensive and fair benchmark for assessing computer-use agents' tool invocation, GUI operation, and decision-making abilities in a real-world environment. We design a novel automated code-generation pipeline to create tools and combine them with a curated selection from existing tools. Rigorous manual validation yields 158 high-quality tools (covering 7 common applications), each verified for correct functionality, practical applicability, and versatility. Extensive evaluations of state-of-the-art multimodal agents on OSWorld-MCP show that MCP tools generally improve task success rates (e.g., from 8.3% to 20.4% for OpenAI o3 at 15 steps, from 40.1% to 43.3% for Claude 4 Sonnet at 50 steps), underscoring the importance of assessing tool invocation capabilities. However, even the strongest models have relatively low tool invocation rates, Only 36.3%, indicating room for improvement and highlighting the benchmark's challenge. By explicitly measuring MCP tool usage skills, OSWorld-MCP deepens understanding of multimodal agents and sets a new standard for evaluating performance in complex, tool-assisted environments. Our code, environment, and data are publicly available at https://osworld-mcp.github.io.", 'score': 20, 'issue_id': 6667, 'pub_date': '2025-10-28', 'pub_date_card': {'ru': '28 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 28', 'zh': '10æœˆ28æ—¥'}, 'hash': '31a5246f81b64759', 'authors': ['Hongrui Jia', 'Jitong Liao', 'Xi Zhang', 'Haiyang Xu', 'Tianbao Xie', 'Chaoya Jiang', 'Ming Yan', 'Si Liu', 'Wei Ye', 'Fei Huang'], 'affiliations': ['Beijing Zhongguancun Academy', 'Peking University', 'Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2510.24563.jpg', 'data': {'categories': ['#multimodal', '#reasoning', '#benchmark', '#optimization', '#open_source', '#agents'], 'emoji': 'ğŸ› ï¸', 'ru': {'title': 'ĞĞ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²: Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ GUI, Ğ½Ğ¾ Ğ¸ ÑƒĞ¼ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸', 'desc': 'OSWorld-MCP â€” ÑÑ‚Ğ¾ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸Ğ·Ğ¼ĞµÑ€ÑĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ğ²Ñ‹Ğ·Ğ¾Ğ²Ñƒ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ¼ Ğ¸ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ 158 Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ 7 Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ° Ğ¸ Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ MCP-Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»Ğ¸ ÑƒÑĞ¿ĞµÑ…Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ´Ğ»Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, Ñ 8.3% Ğ´Ğ¾ 20.4% Ğ´Ğ»Ñ OpenAI o3. ĞĞ´Ğ½Ğ°ĞºĞ¾ Ğ´Ğ°Ğ¶Ğµ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ² 36.3% ÑĞ»ÑƒÑ‡Ğ°ĞµĞ², Ñ‡Ñ‚Ğ¾ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Revolutionizing Multimodal Agent Evaluation with OSWorld-MCP', 'desc': "OSWorld-MCP is a new benchmark designed to evaluate multimodal agents on their ability to invoke tools, operate GUIs, and make decisions in real-world scenarios. It addresses the gap in previous assessments that primarily focused on GUI interactions, providing a fair comparison by including tool invocation capabilities. The benchmark features a unique automated code-generation pipeline that creates and validates 158 high-quality tools for common applications. Results show that while integrating MCP tools significantly improves task success rates, there is still a need for enhancement in tool invocation rates among leading models, emphasizing the benchmark's importance in advancing multimodal agent evaluation."}, 'zh': {'title': 'è¯„ä¼°å¤šæ¨¡æ€æ™ºèƒ½ä½“çš„æ–°æ ‡å‡†', 'desc': 'OSWorld-MCPæ˜¯ä¸€ä¸ªåŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°å¤šæ¨¡æ€æ™ºèƒ½ä½“åœ¨å·¥å…·è°ƒç”¨ã€å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰æ“ä½œå’Œå†³ç­–èƒ½åŠ›æ–¹é¢çš„è¡¨ç°ã€‚è¯¥ç ”ç©¶å¼ºè°ƒäº†åœ¨çœŸå®åœºæ™¯ä¸­è¯„ä¼°å·¥å…·ä½¿ç”¨çš„é‡è¦æ€§ï¼Œå°¤å…¶æ˜¯é€šè¿‡æ¨¡å‹ä¸Šä¸‹æ–‡åè®®ï¼ˆMCPï¼‰å®ç°çš„å·¥å…·è°ƒç”¨èƒ½åŠ›ã€‚é€šè¿‡è®¾è®¡è‡ªåŠ¨åŒ–ä»£ç ç”Ÿæˆç®¡é“ï¼Œç ”ç©¶å›¢é˜Ÿåˆ›å»ºäº†158ä¸ªé«˜è´¨é‡å·¥å…·ï¼Œå¹¶å¯¹å…¶åŠŸèƒ½å’Œé€‚ç”¨æ€§è¿›è¡Œäº†ä¸¥æ ¼éªŒè¯ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œä½¿ç”¨MCPå·¥å…·çš„æ™ºèƒ½ä½“åœ¨ä»»åŠ¡æˆåŠŸç‡ä¸Šæœ‰æ˜¾è‘—æå‡ï¼Œæ˜¾ç¤ºå‡ºå·¥å…·è°ƒç”¨èƒ½åŠ›çš„å…³é”®æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.24657', 'title': 'Group Relative Attention Guidance for Image Editing', 'url': 'https://huggingface.co/papers/2510.24657', 'abstract': "Group Relative Attention Guidance enhances image editing quality by modulating token deltas in Diffusion-in-Transformer models, providing fine-grained control over editing intensity.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, image editing based on Diffusion-in-Transformer models has undergone rapid development. However, existing editing methods often lack effective control over the degree of editing, limiting their ability to achieve more customized results. To address this limitation, we investigate the MM-Attention mechanism within the DiT model and observe that the Query and Key tokens share a bias vector that is only layer-dependent. We interpret this bias as representing the model's inherent editing behavior, while the delta between each token and its corresponding bias encodes the content-specific editing signals. Based on this insight, we propose Group Relative Attention Guidance, a simple yet effective method that reweights the delta values of different tokens to modulate the focus of the model on the input image relative to the editing instruction, enabling continuous and fine-grained control over editing intensity without any tuning. Extensive experiments conducted on existing image editing frameworks demonstrate that GRAG can be integrated with as few as four lines of code, consistently enhancing editing quality. Moreover, compared to the commonly used Classifier-Free Guidance, GRAG achieves smoother and more precise control over the degree of editing. Our code will be released at https://github.com/little-misfit/GRAG-Image-Editing.", 'score': 19, 'issue_id': 6668, 'pub_date': '2025-10-28', 'pub_date_card': {'ru': '28 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 28', 'zh': '10æœˆ28æ—¥'}, 'hash': '05adb00252472e87', 'authors': ['Xuanpu Zhang', 'Xuesong Niu', 'Ruidong Chen', 'Dan Song', 'Jianhao Zeng', 'Penghui Du', 'Haoxiang Cao', 'Kai Wu', 'An-an Liu'], 'affiliations': ['Kolors Team, Kuaishou Technology', 'Tianjin University'], 'pdf_title_img': 'assets/pdf/title_img/2510.24657.jpg', 'data': {'categories': ['#architecture', '#open_source', '#training', '#optimization', '#cv', '#diffusion'], 'emoji': 'ğŸšï¸', 'ru': {'title': 'Ğ¢Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° ÑĞ¸Ğ»Ñ‹ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ¾Ğ´ÑƒĞ»ÑÑ†Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Group Relative Attention Guidance (GRAG) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ½Ğ°Ğ´ Ğ¸Ğ½Ñ‚ĞµĞ½ÑĞ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Diffusion-in-Transformer Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ½Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Query Ğ¸ Key Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ² Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğµ MM-Attention Ğ¸Ğ¼ĞµÑÑ‚ Ğ¾Ğ±Ñ‰Ğ¸Ğ¹ bias-Ğ²ĞµĞºÑ‚Ğ¾Ñ€, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğµ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸. GRAG Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ğ¾ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ´ĞµĞ»ÑŒÑ‚ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼Ğ¸ Ğ¸ Ğ¸Ñ… bias, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ‚Ğ¾Ğ½ĞºĞ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑ‚ÑŒ Ñ„Ğ¾ĞºÑƒÑĞ¾Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ¼ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¸ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ²ÑĞµĞ³Ğ¾ Ñ‡ĞµÑ‚Ñ‹Ñ€ÑŒĞ¼Ñ ÑÑ‚Ñ€Ğ¾ĞºĞ°Ğ¼Ğ¸ ĞºĞ¾Ğ´Ğ° Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ğ»Ğ°Ğ²Ğ½Ñ‹Ğ¹ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Classifier-Free Guidance.'}, 'en': {'title': 'Fine-Tune Your Edits with Group Relative Attention Guidance!', 'desc': "This paper introduces Group Relative Attention Guidance (GRAG), a method that improves image editing quality in Diffusion-in-Transformer models by adjusting token deltas. The authors identify that the Query and Key tokens in the model share a layer-dependent bias, which influences the model's editing behavior. By reweighting the delta values of tokens, GRAG allows for fine-tuned control over the intensity of image edits based on specific instructions. The results show that GRAG can be easily integrated into existing frameworks and provides smoother and more precise editing compared to traditional methods."}, 'zh': {'title': 'ç¾¤ä½“ç›¸å¯¹æ³¨æ„åŠ›å¼•å¯¼æå‡å›¾åƒç¼–è¾‘è´¨é‡', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºç¾¤ä½“ç›¸å¯¹æ³¨æ„åŠ›å¼•å¯¼ï¼ˆGroup Relative Attention Guidance, GRAGï¼‰çš„æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜åŸºäºæ‰©æ•£-å˜æ¢å™¨æ¨¡å‹çš„å›¾åƒç¼–è¾‘è´¨é‡ã€‚é€šè¿‡è°ƒèŠ‚ä¸åŒæ ‡è®°çš„å¢é‡å€¼ï¼ŒGRAGèƒ½å¤Ÿå®ç°å¯¹ç¼–è¾‘å¼ºåº¦çš„ç»†ç²’åº¦æ§åˆ¶ï¼Œå…‹æœäº†ç°æœ‰æ–¹æ³•åœ¨ç¼–è¾‘ç¨‹åº¦ä¸Šçš„å±€é™æ€§ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒGRAGå¯ä»¥ä¸ç°æœ‰å›¾åƒç¼–è¾‘æ¡†æ¶è½»æ¾é›†æˆï¼Œå¹¶ä¸”åªéœ€å°‘é‡ä»£ç å³å¯å®ç°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGRAGåœ¨ç¼–è¾‘è´¨é‡ä¸Šä¼˜äºå¸¸ç”¨çš„æ— åˆ†ç±»å™¨å¼•å¯¼æ–¹æ³•ï¼Œæä¾›äº†æ›´å¹³æ»‘å’Œç²¾ç¡®çš„ç¼–è¾‘æ§åˆ¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.24697', 'title': 'WebLeaper: Empowering Efficiency and Efficacy in WebAgent via Enabling\n  Info-Rich Seeking', 'url': 'https://huggingface.co/papers/2510.24697', 'abstract': 'WebLeaper framework improves information seeking efficiency and effectiveness by constructing high-coverage tasks and generating efficient solution trajectories using tree-structured reasoning and curated Wikipedia tables.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Model (LLM)-based agents have emerged as a transformative approach for open-ended problem solving, with information seeking (IS) being a core capability that enables autonomous reasoning and decision-making. While prior research has largely focused on improving retrieval depth, we observe that current IS agents often suffer from low search efficiency, which in turn constrains overall performance. A key factor underlying this inefficiency is the sparsity of target entities in training tasks, which limits opportunities for agents to learn and generalize efficient search behaviors. To address these challenges, we propose WebLeaper, a framework for constructing high-coverage IS tasks and generating efficient solution trajectories. We formulate IS as a tree-structured reasoning problem, enabling a substantially larger set of target entities to be embedded within a constrained context. Leveraging curated Wikipedia tables, we propose three variants for synthesizing IS tasks, Basic, Union, and Reverse-Union, to systematically increase both IS efficiency and efficacy. Finally, we curate training trajectories by retaining only those that are simultaneously accurate and efficient, ensuring that the model is optimized for both correctness and search performance. Extensive experiments on both basic and comprehensive settings, conducted on five IS benchmarks, BrowserComp, GAIA, xbench-DeepSearch, WideSearch, and Seal-0, demonstrate that our method consistently achieves improvements in both effectiveness and efficiency over strong baselines.', 'score': 18, 'issue_id': 6668, 'pub_date': '2025-10-28', 'pub_date_card': {'ru': '28 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 28', 'zh': '10æœˆ28æ—¥'}, 'hash': '4986235d4df6a5c7', 'authors': ['Zhengwei Tao', 'Haiyang Shen', 'Baixuan Li', 'Wenbiao Yin', 'Jialong Wu', 'Kuan Li', 'Zhongwang Zhang', 'Huifeng Yin', 'Rui Ye', 'Liwen Zhang', 'Xinyu Wang', 'Pengjun Xie', 'Jingren Zhou', 'Yong Jiang'], 'affiliations': ['Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2510.24697.jpg', 'data': {'categories': ['#dataset', '#survey', '#training', '#optimization', '#benchmark', '#agents', '#reasoning'], 'emoji': 'ğŸŒ³', 'ru': {'title': 'WebLeaper: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ´Ñ€ĞµĞ²Ğ¾Ğ²Ğ¸Ğ´Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ WebLeaper â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LLM. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¿Ğ¾Ğ¸ÑĞºĞ° ĞºĞ°Ğº Ğ´Ñ€ĞµĞ²Ğ¾Ğ²Ğ¸Ğ´Ğ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ğ¸Ñ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ñ… ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ² Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹ Ğ¸Ğ· Wikipedia, Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ñ‹ Ñ‚Ñ€Ğ¸ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ° ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡ (Basic, Union Ğ¸ Reverse-Union) Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿Ğ¾Ğ¸ÑĞºĞ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ¿ÑÑ‚Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ğº Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, Ñ‚Ğ°Ğº Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'WebLeaper: Boosting Information Seeking Efficiency with Tree-Structured Reasoning', 'desc': 'The WebLeaper framework enhances the efficiency and effectiveness of information seeking (IS) by creating high-coverage tasks and generating efficient solution paths through tree-structured reasoning. It addresses the issue of low search efficiency in current IS agents, which often struggle due to the limited availability of target entities in training tasks. By utilizing curated Wikipedia tables, WebLeaper synthesizes IS tasks in three variants to improve both the efficiency and efficacy of the search process. Extensive experiments show that this approach leads to significant improvements in performance across multiple IS benchmarks.'}, 'zh': {'title': 'WebLeaperï¼šæå‡ä¿¡æ¯æ£€ç´¢æ•ˆç‡ä¸æ•ˆæœçš„æ¡†æ¶', 'desc': 'WebLeaperæ¡†æ¶é€šè¿‡æ„å»ºé«˜è¦†ç›–ç‡çš„ä¿¡æ¯æ£€ç´¢ä»»åŠ¡å’Œç”Ÿæˆé«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆè·¯å¾„ï¼Œæå‡äº†ä¿¡æ¯æ£€ç´¢çš„æ•ˆç‡å’Œæ•ˆæœã€‚è¯¥æ¡†æ¶å°†ä¿¡æ¯æ£€ç´¢è§†ä¸ºä¸€ä¸ªæ ‘ç»“æ„æ¨ç†é—®é¢˜ï¼Œä»è€Œåœ¨æœ‰é™çš„ä¸Šä¸‹æ–‡ä¸­åµŒå…¥æ›´å¤šçš„ç›®æ ‡å®ä½“ã€‚é€šè¿‡åˆ©ç”¨ç²¾å¿ƒç­–åˆ’çš„ç»´åŸºç™¾ç§‘è¡¨æ ¼ï¼ŒWebLeaperæå‡ºäº†ä¸‰ç§åˆæˆä¿¡æ¯æ£€ç´¢ä»»åŠ¡çš„å˜ä½“ï¼Œä»¥ç³»ç»Ÿæ€§åœ°æé«˜ä¿¡æ¯æ£€ç´¢çš„æ•ˆç‡å’Œæœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ç›¸è¾ƒäºå¼ºåŸºçº¿æ¨¡å‹åœ¨æ•ˆæœå’Œæ•ˆç‡ä¸Šå‡æœ‰æ˜¾è‘—æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.24698', 'title': 'ParallelMuse: Agentic Parallel Thinking for Deep Information Seeking', 'url': 'https://huggingface.co/papers/2510.24698', 'abstract': 'ParallelMuse enhances problem-solving by efficiently reusing paths and compressing reasoning in deep information-seeking agents, improving performance and reducing token consumption.  \t\t\t\t\tAI-generated summary \t\t\t\t Parallel thinking expands exploration breadth, complementing the deep exploration of information-seeking (IS) agents to further enhance problem-solving capability. However, conventional parallel thinking faces two key challenges in this setting: inefficiency from repeatedly rolling out from scratch, and difficulty in integrating long-horizon reasoning trajectories during answer generation, as limited context capacity prevents full consideration of the reasoning process. To address these issues, we propose ParallelMuse, a two-stage paradigm designed for deep IS agents. The first stage, Functionality-Specified Partial Rollout, partitions generated sequences into functional regions and performs uncertainty-guided path reuse and branching to enhance exploration efficiency. The second stage, Compressed Reasoning Aggregation, exploits reasoning redundancy to losslessly compress information relevant to answer derivation and synthesize a coherent final answer. Experiments across multiple open-source agents and benchmarks demonstrate up to 62% performance improvement with a 10--30% reduction in exploratory token consumption.', 'score': 17, 'issue_id': 6668, 'pub_date': '2025-10-28', 'pub_date_card': {'ru': '28 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 28', 'zh': '10æœˆ28æ—¥'}, 'hash': '5622c4489fe55493', 'authors': ['Baixuan Li', 'Dingchu Zhang', 'Jialong Wu', 'Wenbiao Yin', 'Zhengwei Tao', 'Yida Zhao', 'Liwen Zhang', 'Haiyang Shen', 'Runnan Fang', 'Pengjun Xie', 'Jingren Zhou', 'Yong Jiang'], 'affiliations': ['Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2510.24698.jpg', 'data': {'categories': ['#training', '#optimization', '#benchmark', '#agents', '#reasoning'], 'emoji': 'ğŸ”€', 'ru': {'title': 'ĞŸĞ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²: Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ Ğ´ÑƒĞ¼Ğ°ĞµĞ¼, Ğ¼ĞµĞ½ÑŒÑˆĞµ Ñ‚Ñ€Ğ°Ñ‚Ğ¸Ğ¼', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ ParallelMuse â€” ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ² Ğ´Ğ²Ğ° ÑÑ‚Ğ°Ğ¿Ğ°: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° ÑƒĞ¼Ğ½Ğ¾ Ğ¿ĞµÑ€ĞµĞ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑƒĞ¶Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿ÑƒÑ‚Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ñ Ğ½ÑƒĞ»Ñ, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ ÑĞ¶Ğ¸Ğ¼Ğ°ĞµÑ‚ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ğ²Ğ°Ğ¶Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ¾Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¸ Ğ»ÑƒÑ‡ÑˆĞµ Ğ¸Ñ… ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¸ Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°. Ğ’ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ÑÑ Ğ´Ğ¾ 62%, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ñ€Ğ°ÑÑ…Ğ¾Ğ´ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° 10-30%.'}, 'en': {'title': 'Enhancing Problem-Solving Efficiency with ParallelMuse', 'desc': 'ParallelMuse is a novel approach that improves the efficiency of deep information-seeking agents by reusing reasoning paths and compressing the information they generate. It addresses the challenges of traditional parallel thinking, which often leads to inefficiencies and difficulties in managing long-term reasoning. The method consists of two main stages: the first enhances exploration by reusing paths based on their functionality, while the second compresses reasoning to streamline the answer generation process. Experiments show that ParallelMuse can significantly boost performance while reducing the number of tokens used during exploration.'}, 'zh': {'title': 'ParallelMuseï¼šé«˜æ•ˆæ¨ç†ä¸æ¢ç´¢çš„ç»“åˆ', 'desc': 'ParallelMuse æ˜¯ä¸€ç§å¢å¼ºæ·±åº¦ä¿¡æ¯æœç´¢ä»£ç†é—®é¢˜è§£å†³èƒ½åŠ›çš„æ–¹æ³•ã€‚å®ƒé€šè¿‡é«˜æ•ˆé‡ç”¨è·¯å¾„å’Œå‹ç¼©æ¨ç†è¿‡ç¨‹ï¼Œæ˜¾è‘—æé«˜äº†æ€§èƒ½å¹¶å‡å°‘äº†ä»¤ç‰Œæ¶ˆè€—ã€‚è¯¥æ–¹æ³•åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µé€šè¿‡ä¸ç¡®å®šæ€§å¼•å¯¼çš„è·¯å¾„é‡ç”¨æ¥æé«˜æ¢ç´¢æ•ˆç‡ï¼Œç¬¬äºŒé˜¶æ®µåˆ™åˆ©ç”¨æ¨ç†å†—ä½™æ¥æ— æŸå‹ç¼©ä¸ç­”æ¡ˆæ¨å¯¼ç›¸å…³çš„ä¿¡æ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒParallelMuse åœ¨å¤šä¸ªå¼€æºä»£ç†å’ŒåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†é«˜è¾¾62%çš„æ€§èƒ½æå‡ï¼ŒåŒæ—¶æ¢ç´¢æ€§ä»¤ç‰Œæ¶ˆè€—å‡å°‘äº†10%åˆ°30%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.24695', 'title': 'AgentFrontier: Expanding the Capability Frontier of LLM Agents with\n  ZPD-Guided Data Synthesis', 'url': 'https://huggingface.co/papers/2510.24695', 'abstract': "A ZPD-guided data synthesis approach enhances large language model capabilities by training them on tasks just beyond their current abilities, leading to state-of-the-art performance on complex benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Training large language model agents on tasks at the frontier of their capabilities is key to unlocking advanced reasoning. We introduce a data synthesis approach inspired by the educational theory of the Zone of Proximal Development (ZPD), which defines this frontier as tasks an LLM cannot solve alone but can master with guidance. To operationalize this, we present the AgentFrontier Engine, an automated pipeline that synthesizes high-quality, multidisciplinary data situated precisely within the LLM's ZPD. This engine supports both continued pre-training with knowledge-intensive data and targeted post-training on complex reasoning tasks. From the same framework, we derive the ZPD Exam, a dynamic and automated benchmark designed to evaluate agent capabilities on these frontier tasks. We train AgentFrontier-30B-A3B model on our synthesized data, which achieves state-of-the-art results on demanding benchmarks like Humanity's Last Exam, even surpassing some leading proprietary agents. Our work demonstrates that a ZPD-guided approach to data synthesis offers a scalable and effective path toward building more capable LLM agents.", 'score': 17, 'issue_id': 6668, 'pub_date': '2025-10-28', 'pub_date_card': {'ru': '28 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 28', 'zh': '10æœˆ28æ—¥'}, 'hash': '041127f22b52b58f', 'authors': ['Xuanzhong Chen', 'Zile Qiao', 'Guoxin Chen', 'Liangcai Su', 'Zhen Zhang', 'Xinyu Wang', 'Pengjun Xie', 'Fei Huang', 'Jingren Zhou', 'Yong Jiang'], 'affiliations': ['Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2510.24695.jpg', 'data': {'categories': ['#dataset', '#training', '#benchmark', '#synthetic', '#data', '#agents', '#reasoning'], 'emoji': 'ğŸ¯', 'ru': {'title': 'ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ AI Ğ² Ğ·Ğ¾Ğ½Ğµ Ğ±Ğ»Ğ¸Ğ¶Ğ°Ğ¹ÑˆĞµĞ³Ğ¾ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¸Ğ½Ñ‚ĞµĞ·Ñƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ LLM, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¿ĞµĞ´Ğ°Ğ³Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¸ Ğ·Ğ¾Ğ½Ñ‹ Ğ±Ğ»Ğ¸Ğ¶Ğ°Ğ¹ÑˆĞµĞ³Ğ¾ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ (ZPD). ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½ AgentFrontier Engine, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ½Ğ° Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ â€” Ñ‚Ğµ, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ½Ğ° Ğ½Ğµ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ñ€ĞµÑˆĞ¸Ñ‚ÑŒ ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾, Ğ½Ğ¾ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ¾ÑĞ²Ğ¾Ğ¸Ñ‚ÑŒ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ñ‚Ğ°ĞºĞ¸Ğ¼ Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ AgentFrontier-30B-A3B Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ state-of-the-art Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾Ğ¹Ğ´Ñ Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ Ğ¿ÑƒÑ‚ÑŒ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ñ†ĞµĞ»ĞµĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Unlocking LLM Potential with ZPD-Guided Data Synthesis', 'desc': "This paper presents a novel approach to enhance large language models (LLMs) by using a data synthesis method based on the Zone of Proximal Development (ZPD). The ZPD concept helps identify tasks that LLMs can learn to solve with some guidance, allowing for targeted training on these challenging tasks. The authors introduce the AgentFrontier Engine, which automates the creation of high-quality training data that aligns with the LLM's current capabilities. By applying this method, the trained model achieves state-of-the-art performance on complex benchmarks, demonstrating the effectiveness of ZPD-guided data synthesis in advancing LLM capabilities."}, 'zh': {'title': 'åŸºäºZPDçš„æ•°æ®åˆæˆï¼Œæå‡è¯­è¨€æ¨¡å‹èƒ½åŠ›ï¼', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæœ€è¿‘å‘å±•åŒºï¼ˆZPDï¼‰ç†è®ºçš„æ•°æ®åˆæˆæ–¹æ³•ï¼Œä»¥æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„èƒ½åŠ›ã€‚é€šè¿‡è®­ç»ƒæ¨¡å‹åœ¨å…¶èƒ½åŠ›è¾¹ç•Œé™„è¿‘çš„ä»»åŠ¡ï¼Œæ¨¡å‹èƒ½å¤Ÿåœ¨å¤æ‚åŸºå‡†æµ‹è¯•ä¸­å®ç°æœ€å…ˆè¿›çš„è¡¨ç°ã€‚æˆ‘ä»¬ä»‹ç»äº†AgentFrontierå¼•æ“ï¼Œè¿™æ˜¯ä¸€ç§è‡ªåŠ¨åŒ–ç®¡é“ï¼Œèƒ½å¤Ÿåˆæˆé«˜è´¨é‡çš„å¤šå­¦ç§‘æ•°æ®ï¼Œå¸®åŠ©æ¨¡å‹åœ¨çŸ¥è¯†å¯†é›†å‹æ•°æ®ä¸Šè¿›è¡ŒæŒç»­é¢„è®­ç»ƒå’Œå¤æ‚æ¨ç†ä»»åŠ¡çš„åç»­è®­ç»ƒã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒåŸºäºZPDæŒ‡å¯¼çš„æ•°æ®åˆæˆæ–¹æ³•ä¸ºæ„å»ºæ›´å¼ºå¤§çš„LLMä»£ç†æä¾›äº†ä¸€æ¡å¯æ‰©å±•ä¸”æœ‰æ•ˆçš„è·¯å¾„ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.24693', 'title': 'STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D\n  Intelligence', 'url': 'https://huggingface.co/papers/2510.24693', 'abstract': 'STAR-Bench measures audio 4D intelligence by evaluating sound dynamics in time and 3D space, revealing gaps in fine-grained perceptual reasoning among existing models.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite rapid progress in Multi-modal Large Language Models and Large Audio-Language Models, existing audio benchmarks largely test semantics that can be recovered from text captions, masking deficits in fine-grained perceptual reasoning. We formalize audio 4D intelligence that is defined as reasoning over sound dynamics in time and 3D space, and introduce STAR-Bench to measure it. STAR-Bench combines a Foundational Acoustic Perception setting (six attributes under absolute and relative regimes) with a Holistic Spatio-Temporal Reasoning setting that includes segment reordering for continuous and discrete processes and spatial tasks spanning static localization, multi-source relations, and dynamic trajectories. Our data curation pipeline uses two methods to ensure high-quality samples. For foundational tasks, we use procedurally synthesized and physics-simulated audio. For holistic data, we follow a four-stage process that includes human annotation and final selection based on human performance. Unlike prior benchmarks where caption-only answering reduces accuracy slightly, STAR-Bench induces far larger drops (-31.5\\% temporal, -35.2\\% spatial), evidencing its focus on linguistically hard-to-describe cues. Evaluating 19 models reveals substantial gaps compared with humans and a capability hierarchy: closed-source models are bottlenecked by fine-grained perception, while open-source models lag across perception, knowledge, and reasoning. Our STAR-Bench provides critical insights and a clear path forward for developing future models with a more robust understanding of the physical world.', 'score': 17, 'issue_id': 6669, 'pub_date': '2025-10-28', 'pub_date_card': {'ru': '28 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 28', 'zh': '10æœˆ28æ—¥'}, 'hash': 'cb1bd8a9c7ac0b88', 'authors': ['Zihan Liu', 'Zhikang Niu', 'Qiuyang Xiao', 'Zhisheng Zheng', 'Ruoqi Yuan', 'Yuhang Zang', 'Yuhang Cao', 'Xiaoyi Dong', 'Jianze Liang', 'Xie Chen', 'Leilei Sun', 'Dahua Lin', 'Jiaqi Wang'], 'affiliations': ['Beihang University', 'Shanghai AI Laboratory', 'Shanghai Innovation Institute', 'Shanghai Jiao Tong University', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2510.24693.jpg', 'data': {'categories': ['#interpretability', '#data', '#audio', '#reasoning', '#benchmark', '#open_source'], 'emoji': 'ğŸ§', 'ru': {'title': '4D Ğ°ÑƒĞ´Ğ¸Ğ¾-Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ·Ğ²ÑƒĞºĞ° Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ STAR-Bench â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Â«4D Ğ°ÑƒĞ´Ğ¸Ğ¾-Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°Â» Ñƒ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… LLM, Ñ‚Ğ¾ ĞµÑÑ‚ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºÑƒ Ğ·Ğ²ÑƒĞºĞ° Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¸ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ. Ğ¡ÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ°ÑƒĞ´Ğ¸Ğ¾-Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸ Ñ‚ĞµÑÑ‚Ğ¸Ñ€ÑƒÑÑ‚ Ğ² Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¼ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºÑƒ, ĞºĞ¾Ñ‚Ğ¾Ñ€ÑƒÑ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑŒ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹, ÑĞºÑ€Ñ‹Ğ²Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ñ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¿ĞµÑ€Ñ†ĞµĞ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ¼. STAR-Bench Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ½Ğ° Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğµ Ğ°ĞºÑƒÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ (ÑˆĞµÑÑ‚ÑŒ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ¾Ğ²) Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ (Ğ¿ĞµÑ€ĞµÑƒĞ¿Ğ¾Ñ€ÑĞ´Ğ¾Ñ‡Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ, Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸). Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ 19 Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ñ‚ÑÑ‚Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ‚ Ğ»ÑĞ´ĞµĞ¹: Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸ĞµĞ¼, Ğ° Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¾Ñ‚ÑÑ‚Ğ°ÑÑ‚ Ğ²Ğ¾ Ğ²ÑĞµÑ… Ğ°ÑĞ¿ĞµĞºÑ‚Ğ°Ñ… â€” Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğ¸, Ğ·Ğ½Ğ°Ğ½Ğ¸ÑÑ… Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ….'}, 'en': {'title': 'STAR-Bench: Advancing Audio 4D Intelligence Evaluation', 'desc': 'The paper introduces STAR-Bench, a new benchmark designed to evaluate audio 4D intelligence, which involves understanding sound dynamics over time and in three-dimensional space. It highlights the limitations of existing audio benchmarks that primarily focus on semantic understanding derived from text, thereby neglecting fine-grained perceptual reasoning. STAR-Bench incorporates both foundational acoustic perception and holistic spatio-temporal reasoning tasks, revealing significant performance gaps between current models and human capabilities. The findings indicate that both closed-source and open-source models struggle with fine-grained perception, underscoring the need for improved model development to enhance understanding of complex auditory environments.'}, 'zh': {'title': 'STAR-Benchï¼šéŸ³é¢‘å››ç»´æ™ºèƒ½çš„æ–°æ ‡å‡†', 'desc': 'STAR-Bench æ˜¯ä¸€ä¸ªç”¨äºæµ‹é‡éŸ³é¢‘å››ç»´æ™ºèƒ½çš„åŸºå‡†ï¼Œè¯„ä¼°å£°éŸ³åœ¨æ—¶é—´å’Œä¸‰ç»´ç©ºé—´ä¸­çš„åŠ¨æ€è¡¨ç°ã€‚è¯¥ç ”ç©¶æ­ç¤ºäº†ç°æœ‰æ¨¡å‹åœ¨ç»†ç²’åº¦æ„ŸçŸ¥æ¨ç†æ–¹é¢çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å’Œå¤§éŸ³é¢‘è¯­è¨€æ¨¡å‹çš„å¿«é€Ÿå‘å±•èƒŒæ™¯ä¸‹ã€‚STAR-Bench ç»“åˆäº†åŸºç¡€å£°å­¦æ„ŸçŸ¥å’Œæ•´ä½“æ—¶ç©ºæ¨ç†çš„è®¾ç½®ï¼Œä½¿ç”¨é«˜è´¨é‡æ ·æœ¬æ¥ç¡®ä¿è¯„ä¼°çš„å‡†ç¡®æ€§ã€‚é€šè¿‡å¯¹19ä¸ªæ¨¡å‹çš„è¯„ä¼°ï¼Œå‘ç°å®ƒä»¬ä¸äººç±»çš„è¡¨ç°å­˜åœ¨æ˜¾è‘—å·®è·ï¼Œå¼ºè°ƒäº†æœªæ¥æ¨¡å‹åœ¨ç†è§£ç‰©ç†ä¸–ç•Œæ–¹é¢çš„æ”¹è¿›æ–¹å‘ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.24514', 'title': 'Latent Sketchpad: Sketching Visual Thoughts to Elicit Multimodal\n  Reasoning in MLLMs', 'url': 'https://huggingface.co/papers/2510.24514', 'abstract': "Latent Sketchpad enhances Multimodal Large Language Models with an internal visual scratchpad, enabling generative visual thought and improved reasoning performance.  \t\t\t\t\tAI-generated summary \t\t\t\t While Multimodal Large Language Models (MLLMs) excel at visual understanding, they often struggle in complex scenarios that require visual planning and imagination. Inspired by how humans use sketching as a form of visual thinking to develop and communicate ideas, we introduce Latent Sketchpad, a framework that equips MLLMs with an internal visual scratchpad. The internal visual representations of MLLMs have traditionally been confined to perceptual understanding. We repurpose them to support generative visual thought without compromising reasoning ability. Building on frontier MLLMs, our approach integrates visual generation directly into their native autoregressive reasoning process. It allows the model to interleave textual reasoning with the generation of visual latents. These latents guide the internal thought process and can be translated into sketch images for interpretability. To realize this, we introduce two components: a Context-Aware Vision Head autoregressively produces visual representations, and a pretrained Sketch Decoder renders these into human-interpretable images. We evaluate the framework on our new dataset MazePlanning. Experiments across various MLLMs show that Latent Sketchpad delivers comparable or even superior reasoning performance to their backbone. It further generalizes across distinct frontier MLLMs, including Gemma3 and Qwen2.5-VL. By extending model's textual reasoning to visual thinking, our framework opens new opportunities for richer human-computer interaction and broader applications. More details and resources are available on our project page: https://latent-sketchpad.github.io/.", 'score': 17, 'issue_id': 6668, 'pub_date': '2025-10-28', 'pub_date_card': {'ru': '28 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 28', 'zh': '10æœˆ28æ—¥'}, 'hash': 'c64dd82987acd606', 'authors': ['Huanyu Zhang', 'Wenshan Wu', 'Chengzu Li', 'Ning Shang', 'Yan Xia', 'Yangyu Huang', 'Yifan Zhang', 'Li Dong', 'Zhang Zhang', 'Liang Wang', 'Tieniu Tan', 'Furu Wei'], 'affiliations': ['CASIA', 'Cambridge', 'MSR', 'NJU', 'UCAS'], 'pdf_title_img': 'assets/pdf/title_img/2510.24514.jpg', 'data': {'categories': ['#reasoning', '#dataset', '#cv', '#multimodal', '#interpretability'], 'emoji': 'âœï¸', 'ru': {'title': 'Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ AI: ĞºĞ¾Ğ³Ğ´Ğ° Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ÑŒ ÑƒÑ‡Ğ¸Ñ‚ÑÑ Ñ€Ğ¸ÑĞ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ²Ğ¾Ğ¸ Ğ¼Ñ‹ÑĞ»Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Latent Sketchpad â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ LLM Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Â«Ğ±Ğ»Ğ¾ĞºĞ½Ğ¾Ñ‚ Ğ´Ğ»Ñ Ğ½Ğ°Ğ±Ñ€Ğ¾ÑĞºĞ¾Ğ²Â». ĞŸĞ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾ Ñ‚Ğ¾Ğ¼Ñƒ, ĞºĞ°Ğº Ğ»ÑĞ´Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ñ€Ğ¸ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ, Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Context-Aware Vision Head Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Sketch Decoder Ğ´Ğ»Ñ Ğ¸Ñ… Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ¿Ğ¾Ğ½ÑÑ‚Ğ½Ñ‹Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ….'}, 'en': {'title': 'Empowering MLLMs with Visual Thinking through Latent Sketchpad', 'desc': 'Latent Sketchpad is a novel framework that enhances Multimodal Large Language Models (MLLMs) by introducing an internal visual scratchpad for generative visual thought. This approach allows MLLMs to not only understand visual information but also engage in complex visual planning and imagination, similar to human sketching. By integrating visual generation into the autoregressive reasoning process, the model can seamlessly combine text and visual representations, improving its reasoning capabilities. The framework has been evaluated on a new dataset, MazePlanning, demonstrating superior performance across various MLLMs, thus paving the way for improved human-computer interaction.'}, 'zh': {'title': 'å¢å¼ºè§†è§‰æ€ç»´ï¼Œæå‡æ¨ç†èƒ½åŠ›çš„æ¡†æ¶', 'desc': 'Latent Sketchpad æ˜¯ä¸€ç§å¢å¼ºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„æ¡†æ¶ï¼Œæä¾›äº†ä¸€ä¸ªå†…éƒ¨è§†è§‰è‰å›¾æ¿ï¼Œæ”¯æŒç”Ÿæˆæ€§è§†è§‰æ€ç»´å’Œæ”¹è¿›çš„æ¨ç†èƒ½åŠ›ã€‚è¯¥æ¡†æ¶é€šè¿‡å°†è§†è§‰ç”Ÿæˆç›´æ¥é›†æˆåˆ°æ¨¡å‹çš„è‡ªå›å½’æ¨ç†è¿‡ç¨‹ä¸­ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨æ–‡æœ¬æ¨ç†ä¸è§†è§‰æ½œåœ¨ç”Ÿæˆä¹‹é—´äº¤æ›¿è¿›è¡Œã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šä¸Šä¸‹æ–‡æ„ŸçŸ¥è§†è§‰å¤´å’Œé¢„è®­ç»ƒçš„è‰å›¾è§£ç å™¨ï¼Œå‰è€…ç”Ÿæˆè§†è§‰è¡¨ç¤ºï¼Œåè€…å°†å…¶è½¬åŒ–ä¸ºäººç±»å¯ç†è§£çš„å›¾åƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLatent Sketchpad åœ¨æ¨ç†æ€§èƒ½ä¸Šä¸åŸºç¡€æ¨¡å‹ç›¸å½“ï¼Œç”šè‡³æ›´ä¼˜ï¼Œæ‹“å±•äº†æ¨¡å‹çš„æ–‡æœ¬æ¨ç†èƒ½åŠ›è‡³è§†è§‰æ€ç»´ï¼Œå¼€å¯äº†æ›´ä¸°å¯Œçš„äººæœºäº¤äº’å’Œåº”ç”¨æœºä¼šã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.23642', 'title': 'VisCoder2: Building Multi-Language Visualization Coding Agents', 'url': 'https://huggingface.co/papers/2510.23642', 'abstract': 'VisCoder2, a family of multi-language visualization models, outperforms open-source baselines and approaches proprietary models by leveraging VisCode-Multi-679K and VisPlotBench for iterative self-debugging and multi-turn correction.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have recently enabled coding agents capable of generating, executing, and revising visualization code. However, existing models often fail in practical workflows due to limited language coverage, unreliable execution, and lack of iterative correction mechanisms. Progress has been constrained by narrow datasets and benchmarks that emphasize single-round generation and single-language tasks. To address these challenges, we introduce three complementary resources for advancing visualization coding agents. VisCode-Multi-679K is a large-scale, supervised dataset containing 679K validated and executable visualization samples with multi-turn correction dialogues across 12 programming languages. VisPlotBench is a benchmark for systematic evaluation, featuring executable tasks, rendered outputs, and protocols for both initial generation and multi-round self-debug. Finally, we present VisCoder2, a family of multi-language visualization models trained on VisCode-Multi-679K. Experiments show that VisCoder2 significantly outperforms strong open-source baselines and approaches the performance of proprietary models like GPT-4.1, with further gains from iterative self-debug, reaching 82.4% overall execution pass rate at the 32B scale, particularly in symbolic or compiler-dependent languages.', 'score': 17, 'issue_id': 6668, 'pub_date': '2025-10-24', 'pub_date_card': {'ru': '24 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 24', 'zh': '10æœˆ24æ—¥'}, 'hash': 'ddd9603391399242', 'authors': ['Yuansheng Ni', 'Songcheng Cai', 'Xiangchao Chen', 'Jiarong Liang', 'Zhiheng Lyu', 'Jiaqi Deng', 'Kai Zou', 'Ping Nie', 'Fei Yuan', 'Xiang Yue', 'Wenhu Chen'], 'affiliations': ['Carnegie Mellon University', 'Independent Researcher', 'Korea Advanced Institute of Science & Technology', 'Netmind.ai', 'University of Waterloo'], 'pdf_title_img': 'assets/pdf/title_img/2510.23642.jpg', 'data': {'categories': ['#open_source', '#science', '#dataset', '#multilingual', '#benchmark', '#agents'], 'emoji': 'ğŸ“Š', 'ru': {'title': 'ĞœÑƒĞ»ÑŒÑ‚Ğ¸ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ AI-Ğ°Ğ³ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ ÑĞ°Ğ¼Ğ¾Ğ¾Ñ‚Ğ»Ğ°Ğ´ĞºĞ¾Ğ¹', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ÑĞµĞ¼ÑŒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ VisCoder2 Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¹ Ğ½Ğ° 12 ÑĞ·Ñ‹ĞºĞ°Ñ… Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ñ‹ Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ VisCode-Multi-679K, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰ĞµĞ¼ 679 Ñ‚Ñ‹ÑÑÑ‡ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ñ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°Ğ¼Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ¹ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ğ¸ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº. Ğ”Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº VisPlotBench Ñ Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ¼ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ ÑĞ°Ğ¼Ğ¾Ğ¾Ñ‚Ğ»Ğ°Ğ´ĞºĞ¸. VisCoder2 Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ¶Ğ°ĞµÑ‚ÑÑ Ğº Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ²Ñ€Ğ¾Ğ´Ğµ GPT-4.1, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ 82.4% ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ´Ğ° Ğ½Ğ° Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğµ 32B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ².'}, 'en': {'title': 'Revolutionizing Visualization Code Generation with VisCoder2', 'desc': 'VisCoder2 is a new set of models designed to improve the generation of visualization code across multiple programming languages. It uses a large dataset called VisCode-Multi-679K, which includes 679,000 validated examples and supports multi-turn corrections, allowing for better iterative debugging. The models are evaluated using VisPlotBench, which provides a structured way to assess their performance on both initial code generation and subsequent corrections. Results show that VisCoder2 outperforms existing open-source models and approaches the capabilities of proprietary systems, achieving a high execution pass rate, especially in complex programming scenarios.'}, 'zh': {'title': 'VisCoder2ï¼šå¤šè¯­è¨€å¯è§†åŒ–çš„æœªæ¥', 'desc': 'VisCoder2æ˜¯ä¸€ç§å¤šè¯­è¨€å¯è§†åŒ–æ¨¡å‹å®¶æ—ï¼Œé€šè¿‡åˆ©ç”¨VisCode-Multi-679Kå’ŒVisPlotBenchå®ç°è¿­ä»£è‡ªæˆ‘è°ƒè¯•å’Œå¤šè½®ä¿®æ­£ï¼Œè¶…è¶Šäº†å¼€æºåŸºå‡†å¹¶æ¥è¿‘ä¸“æœ‰æ¨¡å‹çš„æ€§èƒ½ã€‚è¯¥æ¨¡å‹è§£å†³äº†ç°æœ‰ç¼–ç ä»£ç†åœ¨å®é™…å·¥ä½œæµç¨‹ä¸­é¢ä¸´çš„è¯­è¨€è¦†ç›–æœ‰é™ã€æ‰§è¡Œä¸å¯é å’Œç¼ºä¹è¿­ä»£ä¿®æ­£æœºåˆ¶çš„é—®é¢˜ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåŒ…å«679KéªŒè¯å’Œå¯æ‰§è¡Œå¯è§†åŒ–æ ·æœ¬çš„å¤§è§„æ¨¡ç›‘ç£æ•°æ®é›†ï¼Œä»¥åŠä¸€ä¸ªç³»ç»Ÿè¯„ä¼°åŸºå‡†ï¼Œæ”¯æŒåˆå§‹ç”Ÿæˆå’Œå¤šè½®è‡ªæˆ‘è°ƒè¯•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVisCoder2åœ¨æ‰§è¡Œé€šè¿‡ç‡ä¸Šè¾¾åˆ°äº†82.4%ï¼Œå°¤å…¶åœ¨ç¬¦å·æˆ–ä¾èµ–ç¼–è¯‘å™¨çš„è¯­è¨€ä¸­è¡¨ç°çªå‡ºã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.24711', 'title': 'Routing Matters in MoE: Scaling Diffusion Transformers with Explicit\n  Routing Guidance', 'url': 'https://huggingface.co/papers/2510.24711', 'abstract': 'ProMoE, an MoE framework with conditional and prototypical routing, enhances expert specialization in Diffusion Transformers, achieving state-of-the-art performance on ImageNet.  \t\t\t\t\tAI-generated summary \t\t\t\t Mixture-of-Experts (MoE) has emerged as a powerful paradigm for scaling model capacity while preserving computational efficiency. Despite its notable success in large language models (LLMs), existing attempts to apply MoE to Diffusion Transformers (DiTs) have yielded limited gains. We attribute this gap to fundamental differences between language and visual tokens. Language tokens are semantically dense with pronounced inter-token variation, while visual tokens exhibit spatial redundancy and functional heterogeneity, hindering expert specialization in vision MoE. To this end, we present ProMoE, an MoE framework featuring a two-step router with explicit routing guidance that promotes expert specialization. Specifically, this guidance encourages the router to partition image tokens into conditional and unconditional sets via conditional routing according to their functional roles, and refine the assignments of conditional image tokens through prototypical routing with learnable prototypes based on semantic content. Moreover, the similarity-based expert allocation in latent space enabled by prototypical routing offers a natural mechanism for incorporating explicit semantic guidance, and we validate that such guidance is crucial for vision MoE. Building on this, we propose a routing contrastive loss that explicitly enhances the prototypical routing process, promoting intra-expert coherence and inter-expert diversity. Extensive experiments on ImageNet benchmark demonstrate that ProMoE surpasses state-of-the-art methods under both Rectified Flow and DDPM training objectives. Code and models will be made publicly available.', 'score': 16, 'issue_id': 6668, 'pub_date': '2025-10-28', 'pub_date_card': {'ru': '28 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 28', 'zh': '10æœˆ28æ—¥'}, 'hash': '628b0204fe7a35ef', 'authors': ['Yujie Wei', 'Shiwei Zhang', 'Hangjie Yuan', 'Yujin Han', 'Zhekai Chen', 'Jiayu Wang', 'Difan Zou', 'Xihui Liu', 'Yingya Zhang', 'Yu Liu', 'Hongming Shan'], 'affiliations': ['Fudan University', 'MMLab', 'The University of Hong Kong', 'Tongyi Lab, Alibaba Group', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2510.24711.jpg', 'data': {'categories': ['#architecture', '#open_source', '#optimization', '#cv', '#benchmark', '#diffusion'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ¡Ğ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ProMoE â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Mixture-of-Experts Ğ² Diffusion Transformers Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ°ÑÑ‚ÑÑ Ğ¾Ñ‚ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ½ĞµĞ¾Ğ´Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ğ¾ÑÑ‚ÑŒÑ, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑˆĞ°ĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ². ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ°Ñ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ°Ñ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ½Ğ° ÑƒÑĞ»Ğ¾Ğ²Ğ½Ñ‹Ğµ Ğ¸ Ğ±ĞµĞ·ÑƒÑĞ»Ğ¾Ğ²Ğ½Ñ‹Ğµ, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾Ñ‚Ğ¸Ğ¿Ñ‹ Ğ´Ğ»Ñ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ. ProMoE Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° ImageNet Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒÑĞ¸Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ¾Ğ³ĞµÑ€ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ½Ğ¸Ğ¼Ğ¸.'}, 'en': {'title': 'ProMoE: Enhancing Expert Specialization in Vision with Smart Routing', 'desc': "ProMoE is a new framework that improves the Mixture-of-Experts (MoE) approach in Diffusion Transformers, focusing on better expert specialization. It introduces a two-step routing mechanism that categorizes image tokens into conditional and unconditional sets, enhancing how experts are utilized based on the tokens' roles. By using prototypical routing, the framework refines the assignment of tokens to experts, ensuring that similar tokens are grouped together for more effective learning. The results show that ProMoE achieves top performance on the ImageNet dataset, demonstrating its effectiveness in visual tasks compared to previous methods."}, 'zh': {'title': 'ProMoEï¼šæå‡æ‰©æ•£å˜æ¢å™¨ä¸“å®¶ä¸“ä¸šåŒ–çš„åˆ›æ–°æ¡†æ¶', 'desc': 'ProMoEæ˜¯ä¸€ç§æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜æ‰©æ•£å˜æ¢å™¨ï¼ˆDiffusion Transformersï¼‰ä¸­çš„ä¸“å®¶ä¸“ä¸šåŒ–ã€‚è¯¥æ¡†æ¶é‡‡ç”¨äº†æ¡ä»¶å’ŒåŸå‹è·¯ç”±çš„ä¸¤æ­¥è·¯ç”±æœºåˆ¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°å°†å›¾åƒæ ‡è®°åˆ†ä¸ºæ¡ä»¶å’Œæ— æ¡ä»¶é›†åˆï¼Œä»è€Œä¼˜åŒ–ä¸“å®¶çš„åˆ†é…ã€‚é€šè¿‡åŸå‹è·¯ç”±ï¼ŒProMoEèƒ½å¤Ÿæ ¹æ®è¯­ä¹‰å†…å®¹å¯¹æ¡ä»¶å›¾åƒæ ‡è®°è¿›è¡Œç²¾ç»†åˆ†é…ï¼Œå¢å¼ºäº†ä¸“å®¶ä¹‹é—´çš„å¤šæ ·æ€§å’Œå†…éƒ¨ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒProMoEåœ¨ImageNetåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.24320', 'title': 'Critique-RL: Training Language Models for Critiquing through Two-Stage\n  Reinforcement Learning', 'url': 'https://huggingface.co/papers/2510.24320', 'abstract': "Critique-RL is an online reinforcement learning approach for developing critiquing language models without strong supervision, using a two-stage optimization strategy to improve both the critic's discriminability and helpfulness.  \t\t\t\t\tAI-generated summary \t\t\t\t Training critiquing language models to assess and provide feedback on model outputs is a promising way to improve LLMs for complex reasoning tasks. However, existing approaches typically rely on stronger supervisors for annotating critique data. To address this, we propose Critique-RL, an online RL approach for developing critiquing language models without stronger supervision. Our approach operates on a two-player paradigm: the actor generates a response, the critic provides feedback, and the actor refines the response accordingly. We first reveal that relying solely on indirect reward signals from the actor's outputs for RL optimization often leads to unsatisfactory critics: while their helpfulness (i.e., providing constructive feedback) improves, the discriminability (i.e., determining whether a response is high-quality or not) remains poor, resulting in marginal performance gains. To overcome this, Critique-RL adopts a two-stage optimization strategy. In stage I, it reinforces the discriminability of the critic with direct rule-based reward signals; in stage II, it introduces indirect rewards based on actor refinement to improve the critic's helpfulness, while maintaining its discriminability via appropriate regularization. Extensive experiments across various tasks and models show that Critique-RL delivers substantial performance improvements. For example, it achieves a 9.02% gain on in-domain tasks and a 5.70% gain on out-of-domain tasks for Qwen2.5-7B, highlighting its potential.", 'score': 16, 'issue_id': 6667, 'pub_date': '2025-10-28', 'pub_date_card': {'ru': '28 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 28', 'zh': '10æœˆ28æ—¥'}, 'hash': '0c26fc5b04f76bff', 'authors': ['Zhiheng Xi', 'Jixuan Huang', 'Xin Guo', 'Boyang Hong', 'Dingwen Yang', 'Xiaoran Fan', 'Shuo Li', 'Zehui Chen', 'Junjie Ye', 'Siyu Yuan', 'Zhengyin Du', 'Xuesong Yao', 'Yufei Xu', 'Jiecao Chen', 'Rui Zheng', 'Tao Gui', 'Qi Zhang', 'Xuanjing Huang'], 'affiliations': ['ByteDance Seed', 'Fudan University'], 'pdf_title_img': 'assets/pdf/title_img/2510.24320.jpg', 'data': {'categories': ['#rl', '#reasoning', '#optimization', '#training', '#rlhf'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Critique-RL: Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· ÑĞ¸Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ´Ğ·Ğ¾Ñ€Ğ°', 'desc': 'Critique-RL â€” ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¼Ğ¾Ğ³ÑƒÑ‚ ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ¾Ğ²Ğ°Ñ‚ÑŒ Ğ±ĞµĞ· ÑĞ¸Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ´Ğ·Ğ¾Ñ€Ğ°. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ ĞºĞ°Ğº ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ°Ñ‚ÑŒ, Ñ‚Ğ°Ğº Ğ¸ ĞµĞ³Ğ¾ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ÑÑ‚ÑŒ. Ğ’ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ ÑƒÑĞ¸Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ°Ñ‚ÑŒ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ñ€ÑĞ¼Ñ‹Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ° Ğ²Ğ¾ Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ÑÑ ĞºĞ¾ÑĞ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Critique-RL Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ….'}, 'en': {'title': 'Empowering Language Models with Self-Supervised Critique', 'desc': "Critique-RL is a novel online reinforcement learning method designed to enhance critiquing language models without the need for strong supervision. It employs a two-stage optimization strategy where an actor generates responses and a critic evaluates them, allowing for iterative refinement. The first stage focuses on improving the critic's ability to distinguish high-quality responses using direct reward signals, while the second stage enhances the critic's helpfulness through indirect rewards based on the actor's improvements. Experimental results demonstrate that Critique-RL significantly boosts performance, achieving notable gains in both in-domain and out-of-domain tasks."}, 'zh': {'title': 'Critique-RLï¼šæ— å¼ºç›‘ç£çš„è¯„ä¼°è¯­è¨€æ¨¡å‹ä¼˜åŒ–æ–¹æ³•', 'desc': 'Critique-RLæ˜¯ä¸€ç§åœ¨çº¿å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œç”¨äºåœ¨æ²¡æœ‰å¼ºç›‘ç£çš„æƒ…å†µä¸‹å¼€å‘è¯„ä¼°è¯­è¨€æ¨¡å‹ã€‚è¯¥æ–¹æ³•é‡‡ç”¨ä¸¤é˜¶æ®µä¼˜åŒ–ç­–ç•¥ï¼Œæ—¨åœ¨æé«˜è¯„ä¼°è€…çš„åŒºåˆ†èƒ½åŠ›å’Œæœ‰ç”¨æ€§ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œé€šè¿‡ç›´æ¥çš„åŸºäºè§„åˆ™çš„å¥–åŠ±ä¿¡å·æ¥å¢å¼ºè¯„ä¼°è€…çš„åŒºåˆ†èƒ½åŠ›ï¼›åœ¨ç¬¬äºŒé˜¶æ®µï¼ŒåŸºäºç”Ÿæˆè€…çš„åé¦ˆå¼•å…¥é—´æ¥å¥–åŠ±ï¼Œä»¥æé«˜è¯„ä¼°è€…çš„æœ‰ç”¨æ€§ï¼ŒåŒæ—¶é€šè¿‡é€‚å½“çš„æ­£åˆ™åŒ–ä¿æŒå…¶åŒºåˆ†èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCritique-RLåœ¨å¤šä¸ªä»»åŠ¡å’Œæ¨¡å‹ä¸Šæ˜¾è‘—æå‡äº†æ€§èƒ½ï¼Œå±•ç¤ºäº†å…¶æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.21978', 'title': 'Beyond Reasoning Gains: Mitigating General Capabilities Forgetting in\n  Large Reasoning Models', 'url': 'https://huggingface.co/papers/2510.21978', 'abstract': 'RECAP, a dynamic objective reweighting strategy, enhances reinforcement learning with verifiable rewards by preserving general knowledge and improving reasoning through flexible reward trade-offs.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning with verifiable rewards (RLVR) has delivered impressive gains in mathematical and multimodal reasoning and has become a standard post-training paradigm for contemporary language and vision-language models. However, the RLVR recipe introduces a significant risk of capability regression, where models forget foundational skills after prolonged training without employing regularization strategies. We empirically confirm this concern, observing that open-source reasoning models suffer performance degradation on core capabilities such as perception and faithfulness. While imposing regularization terms like KL divergence can help prevent deviation from the base model, these terms are calculated on the current task, thus they do not guarantee broader knowledge. Meanwhile, commonly used experience replay across heterogeneous domains makes it nontrivial to decide how much training focus each objective should receive. To address this, we propose RECAP-a replay strategy with dynamic objective reweighting for general knowledge preservation. Our reweighting mechanism adapts in an online manner using short-horizon signals of convergence and instability, shifting the post-training focus away from saturated objectives and toward underperforming or volatile ones. Our method is end-to-end and readily applicable to existing RLVR pipelines without training additional models or heavy tuning. Extensive experiments on benchmarks based on Qwen2.5-VL-3B and Qwen2.5-VL-7B demonstrate the effectiveness of our method, which not only preserves general capabilities but also improves reasoning by enabling more flexible trade-offs among in-task rewards.', 'score': 12, 'issue_id': 6684, 'pub_date': '2025-10-24', 'pub_date_card': {'ru': '24 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 24', 'zh': '10æœˆ24æ—¥'}, 'hash': '5ed5e8a9d936f8c4', 'authors': ['Hoang Phan', 'Xianjun Yang', 'Kevin Yao', 'Jingyu Zhang', 'Shengjie Bi', 'Xiaocheng Tang', 'Madian Khabsa', 'Lijuan Liu', 'Deren Lei'], 'affiliations': ['Johns Hopkins University', 'Meta Superintelligence Labs', 'New York University'], 'pdf_title_img': 'assets/pdf/title_img/2510.21978.jpg', 'data': {'categories': ['#optimization', '#training', '#benchmark', '#reasoning', '#rl', '#rlhf'], 'emoji': 'âš–ï¸', 'ru': {'title': 'Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿ĞµÑ€ĞµĞ²Ğ·Ğ²ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ†ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ RECAP â€” Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¸ reinforcement learning Ñ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°Ğ¼Ğ¸ (RLVR). Ğ¢Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ñ KL-Ğ´Ğ¸Ğ²ĞµÑ€Ğ³ĞµĞ½Ñ†Ğ¸ĞµĞ¹ Ğ¸ experience replay Ğ½Ğµ Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ñ€ÑƒÑÑ‚ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¸Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. RECAP Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿ĞµÑ€ĞµĞ²Ğ·Ğ²ĞµÑˆĞ¸Ğ²Ğ°ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞºÑ€Ğ°Ñ‚ĞºĞ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸, ÑĞ¼ĞµÑ‰Ğ°Ñ Ñ„Ğ¾ĞºÑƒÑ Ğ¾Ñ‚ Ğ½Ğ°ÑÑ‹Ñ‰ĞµĞ½Ğ½Ñ‹Ñ… Ñ†ĞµĞ»ĞµĞ¹ Ğº Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ½Ñ‹Ğ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Qwen2.5-VL Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ¾Ğ±Ñ‰Ğ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Dynamic Reweighting for Enhanced Reinforcement Learning', 'desc': 'RECAP is a novel strategy designed to enhance reinforcement learning with verifiable rewards by dynamically adjusting the focus on different training objectives. It addresses the issue of capability regression, where models lose foundational skills during extended training periods. By implementing a flexible reward trade-off system, RECAP ensures that models maintain general knowledge while improving their reasoning abilities. The method is easy to integrate into existing reinforcement learning frameworks, requiring no additional model training or extensive tuning.'}, 'zh': {'title': 'RECAPï¼šåŠ¨æ€é‡åŠ æƒï¼Œæå‡å¼ºåŒ–å­¦ä¹ æ¨ç†èƒ½åŠ›', 'desc': 'RECAPæ˜¯ä¸€ç§åŠ¨æ€ç›®æ ‡é‡åŠ æƒç­–ç•¥ï¼Œæ—¨åœ¨é€šè¿‡çµæ´»çš„å¥–åŠ±æƒè¡¡æ¥å¢å¼ºå¼ºåŒ–å­¦ä¹ ä¸­çš„å¯éªŒè¯å¥–åŠ±ã€‚è¯¥æ–¹æ³•è§£å†³äº†åœ¨é•¿æ—¶é—´è®­ç»ƒä¸­æ¨¡å‹èƒ½åŠ›é€€åŒ–çš„é—®é¢˜ï¼Œç¡®ä¿æ¨¡å‹ä¿ç•™åŸºç¡€çŸ¥è¯†å¹¶æ”¹å–„æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡åœ¨çº¿é€‚åº”çš„é‡åŠ æƒæœºåˆ¶ï¼ŒRECAPèƒ½å¤Ÿæ ¹æ®æ”¶æ•›å’Œä¸ç¨³å®šçš„çŸ­æœŸä¿¡å·è°ƒæ•´è®­ç»ƒé‡ç‚¹ï¼Œä»è€Œé¿å…è¿‡åº¦å…³æ³¨é¥±å’Œç›®æ ‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRECAPä¸ä»…èƒ½ä¿æŒæ¨¡å‹çš„é€šç”¨èƒ½åŠ›ï¼Œè¿˜èƒ½é€šè¿‡æ›´çµæ´»çš„ä»»åŠ¡å¥–åŠ±æƒè¡¡æ¥æå‡æ¨ç†æ•ˆæœã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.22037', 'title': 'ATLAS: Adaptive Transfer Scaling Laws for Multilingual Pretraining,\n  Finetuning, and Decoding the Curse of Multilinguality', 'url': 'https://huggingface.co/papers/2510.22037', 'abstract': "The study introduces ATLAS, a multilingual scaling law that improves out-of-sample generalization and provides insights into cross-lingual transfer, optimal scaling, and computational crossover points for model training.  \t\t\t\t\tAI-generated summary \t\t\t\t Scaling laws research has focused overwhelmingly on English -- yet the most prominent AI models explicitly serve billions of international users. In this work, we undertake the largest multilingual scaling laws study to date, totaling 774 multilingual training experiments, spanning 10M-8B model parameters, 400+ training languages and 48 evaluation languages. We introduce the Adaptive Transfer Scaling Law (ATLAS) for both monolingual and multilingual pretraining, which outperforms existing scaling laws' out-of-sample generalization often by more than 0.3 R^2. Our analyses of the experiments shed light on multilingual learning dynamics, transfer properties between languages, and the curse of multilinguality. First, we derive a cross-lingual transfer matrix, empirically measuring mutual benefit scores between 38 x 38=1444 language pairs. Second, we derive a language-agnostic scaling law that reveals how to optimally scale model size and data when adding languages without sacrificing performance. Third, we identify the computational crossover points for when to pretrain from scratch versus finetune from multilingual checkpoints. We hope these findings provide the scientific foundation for democratizing scaling laws across languages, and enable practitioners to efficiently scale models -- beyond English-first AI.", 'score': 11, 'issue_id': 6669, 'pub_date': '2025-10-24', 'pub_date_card': {'ru': '24 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 24', 'zh': '10æœˆ24æ—¥'}, 'hash': '23b87c7cd2ed6296', 'authors': ['Shayne Longpre', 'Sneha Kudugunta', 'Niklas Muennighoff', 'I-Hung Hsu', 'Isaac Caswell', 'Alex Pentland', 'Sercan Arik', 'Chen-Yu Lee', 'Sayna Ebrahimi'], 'affiliations': ['Google Cloud AI', 'Google DeepMind', 'MIT', 'Stanford University', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2510.22037.jpg', 'data': {'categories': ['#low_resource', '#multilingual', '#transfer_learning', '#dataset', '#training'], 'emoji': 'ğŸŒ', 'ru': {'title': 'Ğ—Ğ°ĞºĞ¾Ğ½Ñ‹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ³Ğ¾ AI', 'desc': 'ATLAS â€” ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ·Ğ°ĞºĞ¾Ğ½ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° 774 ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ñ… Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¾Ñ‚ 10M Ğ´Ğ¾ 8B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 400 ÑĞ·Ñ‹ĞºĞ°Ğ¼Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, ĞºĞ°Ğº ÑĞ·Ñ‹ĞºĞ¸ Ğ¿ĞµÑ€ĞµĞ´Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ´Ñ€ÑƒĞ³ Ğ´Ñ€ÑƒĞ³Ñƒ, ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ñƒ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ñ‹ Ğ´Ğ»Ñ 1444 ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¿Ğ°Ñ€. ATLAS Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸Ñ‚ÑŒ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑĞ¾Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ², Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ñ‚Ğ¾Ñ‡ĞºĞ¸, ĞºĞ¾Ğ³Ğ´Ğ° Ğ²Ñ‹Ğ³Ğ¾Ğ´Ğ½ĞµĞµ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ Ğ½ÑƒĞ»Ñ Ğ¸Ğ»Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰ÑƒÑ. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ·Ğ°ĞºĞ»Ğ°Ğ´Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ°ÑƒÑ‡Ğ½ÑƒÑ Ğ¾ÑĞ½Ğ¾Ğ²Ñƒ Ğ´Ğ»Ñ Ğ´ĞµĞ¼Ğ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ AI Ğ·Ğ° Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ°Ğ¼Ğ¸ Ğ°Ğ½Ğ³Ğ»Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°.'}, 'en': {'title': 'Unlocking Multilingual AI: The ATLAS Approach', 'desc': 'The paper presents ATLAS, a new multilingual scaling law designed to enhance out-of-sample generalization in AI models. It is based on extensive research involving 774 multilingual training experiments across a wide range of model sizes and languages. The study reveals important insights into cross-lingual transfer dynamics and provides a framework for optimal scaling of model size and data when incorporating multiple languages. Additionally, it identifies key points for deciding between pretraining from scratch or fine-tuning existing multilingual models, aiming to make AI more accessible for diverse languages.'}, 'zh': {'title': 'ATLASï¼šå¤šè¯­è¨€æ‰©å±•æ³•åˆ™çš„åˆ›æ–°', 'desc': 'æœ¬ç ”ç©¶ä»‹ç»äº†ATLASï¼Œä¸€ä¸ªå¤šè¯­è¨€æ‰©å±•æ³•åˆ™ï¼Œæ—¨åœ¨æé«˜æ¨¡å‹çš„å¤–éƒ¨æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶æä¾›è·¨è¯­è¨€è¿ç§»ã€æœ€ä½³æ‰©å±•å’Œæ¨¡å‹è®­ç»ƒçš„è®¡ç®—äº¤å‰ç‚¹çš„è§è§£ã€‚æˆ‘ä»¬è¿›è¡Œäº†774ä¸ªå¤šè¯­è¨€è®­ç»ƒå®éªŒï¼Œæ¶µç›–äº†10Måˆ°8Bçš„æ¨¡å‹å‚æ•°ï¼Œ400å¤šç§è®­ç»ƒè¯­è¨€å’Œ48ç§è¯„ä¼°è¯­è¨€ã€‚ATLASåœ¨å•è¯­è¨€å’Œå¤šè¯­è¨€é¢„è®­ç»ƒä¸­è¡¨ç°ä¼˜äºç°æœ‰çš„æ‰©å±•æ³•åˆ™ï¼Œå¤–éƒ¨æ ·æœ¬æ³›åŒ–èƒ½åŠ›æé«˜äº†0.3 R^2ä»¥ä¸Šã€‚æˆ‘ä»¬çš„åˆ†ææ­ç¤ºäº†å¤šè¯­è¨€å­¦ä¹ åŠ¨æ€ã€è¯­è¨€é—´çš„è¿ç§»ç‰¹æ€§ä»¥åŠå¤šè¯­è¨€çš„æŒ‘æˆ˜ï¼Œä¸ºè·¨è¯­è¨€çš„æ‰©å±•æ³•åˆ™æä¾›äº†ç§‘å­¦åŸºç¡€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.24702', 'title': 'Agent Data Protocol: Unifying Datasets for Diverse, Effective\n  Fine-tuning of LLM Agents', 'url': 'https://huggingface.co/papers/2510.24702', 'abstract': 'The agent data protocol (ADP) standardizes diverse agent training datasets, enabling improved performance across various tasks without domain-specific tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t Public research results on large-scale supervised finetuning of AI agents remain relatively rare, since the collection of agent training data presents unique challenges. In this work, we argue that the bottleneck is not a lack of underlying data sources, but that a large variety of data is fragmented across heterogeneous formats, tools, and interfaces. To this end, we introduce the agent data protocol (ADP), a light-weight representation language that serves as an "interlingua" between agent datasets in diverse formats and unified agent training pipelines downstream. The design of ADP is expressive enough to capture a large variety of tasks, including API/tool use, browsing, coding, software engineering, and general agentic workflows, while remaining simple to parse and train on without engineering at a per-dataset level. In experiments, we unified a broad collection of 13 existing agent training datasets into ADP format, and converted the standardized ADP data into training-ready formats for multiple agent frameworks. We performed SFT on these data, and demonstrated an average performance gain of ~20% over corresponding base models, and delivers state-of-the-art or near-SOTA performance on standard coding, browsing, tool use, and research benchmarks, without domain-specific tuning. All code and data are released publicly, in the hope that ADP could help lower the barrier to standardized, scalable, and reproducible agent training.', 'score': 10, 'issue_id': 6680, 'pub_date': '2025-10-28', 'pub_date_card': {'ru': '28 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 28', 'zh': '10æœˆ28æ—¥'}, 'hash': '0143eedf3ffbbc3a', 'authors': ['Yueqi Song', 'Ketan Ramaneti', 'Zaid Sheikh', 'Ziru Chen', 'Boyu Gou', 'Tianbao Xie', 'Yiheng Xu', 'Danyang Zhang', 'Apurva Gandhi', 'Fan Yang', 'Joseph Liu', 'Tianyue Ou', 'Zhihao Yuan', 'Frank Xu', 'Shuyan Zhou', 'Xingyao Wang', 'Xiang Yue', 'Tao Yu', 'Huan Sun', 'Yu Su', 'Graham Neubig'], 'affiliations': ['All Hands AI', 'Carnegie Mellon University', 'Duke University', 'Fujitsu Research', 'The Ohio State University', 'University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2510.24702.jpg', 'data': {'categories': ['#open_source', '#agents', '#training', '#dataset', '#optimization', '#benchmark'], 'emoji': 'ğŸ”—', 'ru': {'title': 'Ğ•Ğ´Ğ¸Ğ½Ñ‹Ğ¹ ÑĞ·Ñ‹Ğº Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Agent Data Protocol (ADP) â€” ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚ Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°Ğ»Ğ°ÑÑŒ Ğ½Ğµ Ğ² Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ° Ğ² Ğ¸Ñ… Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ°Ğ¼ Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ°Ğ¼. ADP Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ ĞºĞ°Ğº Â«Ğ¼ĞµĞ¶ÑŠÑĞ·Ñ‹ĞºÂ», Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½Ğ¸Ñ‚ÑŒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ² Ğ´Ğ»Ñ ĞµĞ´Ğ¸Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¾Ñ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ API Ğ´Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Supervised finetuning Ğ½Ğ° 13 Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½Ñ‘Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ» Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ~20% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ±ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¿Ğ¾Ğ´ Ğ´Ğ¾Ğ¼ĞµĞ½.'}, 'en': {'title': 'Standardizing Agent Training for Better Performance', 'desc': 'The paper introduces the Agent Data Protocol (ADP), which standardizes various agent training datasets to enhance performance across multiple tasks without needing specific adjustments for each domain. It identifies that the main issue in agent training is not the lack of data, but rather the fragmentation of data across different formats and tools. ADP acts as a universal language that connects these diverse datasets, making it easier to train agents on a wide range of tasks like coding and browsing. Experiments show that using ADP leads to an average performance improvement of about 20% on various benchmarks, promoting more efficient and reproducible agent training.'}, 'zh': {'title': 'ä»£ç†æ•°æ®åè®®ï¼šæå‡å¤šä»»åŠ¡æ€§èƒ½çš„å…³é”®', 'desc': 'æœ¬æ–‡æå‡ºäº†ä»£ç†æ•°æ®åè®®ï¼ˆADPï¼‰ï¼Œæ—¨åœ¨æ ‡å‡†åŒ–ä¸åŒæ ¼å¼çš„ä»£ç†è®­ç»ƒæ•°æ®é›†ï¼Œä»è€Œæé«˜å¤šä»»åŠ¡çš„æ€§èƒ½ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œæ•°æ®ç¢ç‰‡åŒ–æ˜¯è®­ç»ƒä»£ç†çš„ä¸»è¦ç“¶é¢ˆï¼Œè€Œä¸æ˜¯æ•°æ®æºçš„ç¼ºä¹ã€‚ADPä½œä¸ºä¸€ç§è½»é‡çº§çš„è¡¨ç¤ºè¯­è¨€ï¼Œå¯ä»¥åœ¨å¤šç§æ ¼å¼çš„ä»£ç†æ•°æ®é›†ä¹‹é—´å……å½“â€œä¸­ä»‹â€ï¼Œå¹¶ç®€åŒ–ä¸‹æ¸¸çš„è®­ç»ƒæµç¨‹ã€‚å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨ADPæ ¼å¼ç»Ÿä¸€çš„13ä¸ªç°æœ‰ä»£ç†è®­ç»ƒæ•°æ®é›†ï¼Œç»è¿‡å¾®è°ƒåï¼Œæ¨¡å‹æ€§èƒ½å¹³å‡æå‡çº¦20%ï¼Œåœ¨å¤šä¸ªæ ‡å‡†åŸºå‡†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„è¡¨ç°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.17439', 'title': 'From Spatial to Actions: Grounding Vision-Language-Action Model in\n  Spatial Foundation Priors', 'url': 'https://huggingface.co/papers/2510.17439', 'abstract': 'FALCON enhances vision-language-action models by integrating rich 3D spatial tokens into the action head, improving spatial reasoning and modality transferability.  \t\t\t\t\tAI-generated summary \t\t\t\t Existing vision-language-action (VLA) models act in 3D real-world but are typically built on 2D encoders, leaving a spatial reasoning gap that limits generalization and adaptability. Recent 3D integration techniques for VLAs either require specialized sensors and transfer poorly across modalities, or inject weak cues that lack geometry and degrade vision-language alignment. In this work, we introduce FALCON (From Spatial to Action), a novel paradigm that injects rich 3D spatial tokens into the action head. FALCON leverages spatial foundation models to deliver strong geometric priors from RGB alone, and includes an Embodied Spatial Model that can optionally fuse depth, or pose for higher fidelity when available, without retraining or architectural changes. To preserve language reasoning, spatial tokens are consumed by a Spatial-Enhanced Action Head rather than being concatenated into the vision-language backbone. These designs enable FALCON to address limitations in spatial representation, modality transferability, and alignment. In comprehensive evaluations across three simulation benchmarks and eleven real-world tasks, our proposed FALCON achieves state-of-the-art performance, consistently surpasses competitive baselines, and remains robust under clutter, spatial-prompt conditioning, and variations in object scale and height.', 'score': 10, 'issue_id': 6677, 'pub_date': '2025-10-20', 'pub_date_card': {'ru': '20 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 20', 'zh': '10æœˆ20æ—¥'}, 'hash': '11b29adeb1a77d6e', 'authors': ['Zhengshen Zhang', 'Hao Li', 'Yalun Dai', 'Zhengbang Zhu', 'Lei Zhou', 'Chenchen Liu', 'Dong Wang', 'Francis E. H. Tay', 'Sijin Chen', 'Ziwei Liu', 'Yuxiao Liu', 'Xinghang Li', 'Pan Zhou'], 'affiliations': ['ByteDance Seed', 'NTU', 'NUS', 'SMU', 'THU'], 'pdf_title_img': 'assets/pdf/title_img/2510.17439.jpg', 'data': {'categories': ['#3d', '#reasoning', '#architecture', '#benchmark', '#alignment', '#transfer_learning', '#multimodal'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'FALCON: ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğ¸ Ğ´Ğ»Ñ VLA Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ FALCON, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ² ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Vision-Language-Action (VLA) Ğ·Ğ° ÑÑ‡Ñ‘Ñ‚ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ 3D Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ»ÑƒÑ‡ÑˆĞµ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğº Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ¼Ğ¸Ñ€Ñƒ, Ğ³Ğ´Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ÑÑ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°. FALCON Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‚ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚Ñ‹, Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ñ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¾ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğµ Ğ¸Ğ»Ğ¸ Ğ¿Ğ¾Ğ·Ğµ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸. Ğ’ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğµ, FALCON Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¸ ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ…, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸.'}, 'en': {'title': 'FALCON: Bridging 3D Spatial Understanding with Action in VLA Models', 'desc': 'FALCON is a new approach that improves vision-language-action (VLA) models by incorporating detailed 3D spatial tokens into the action decision-making process. This integration helps the models better understand spatial relationships and enhances their ability to transfer knowledge across different modalities. Unlike previous methods that struggled with spatial reasoning or required complex setups, FALCON uses spatial foundation models to extract geometric information from standard RGB images. The design allows for improved performance in various tasks without needing extensive retraining, making it more adaptable and effective in real-world scenarios.'}, 'zh': {'title': 'FALCONï¼šæå‡ç©ºé—´æ¨ç†ä¸æ¨¡æ€è½¬ç§»çš„åˆ›æ–°æ¨¡å‹', 'desc': 'FALCONæ˜¯ä¸€ç§æ–°é¢–çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼Œæ—¨åœ¨é€šè¿‡å°†ä¸°å¯Œçš„3Dç©ºé—´æ ‡è®°æ•´åˆåˆ°åŠ¨ä½œå¤´ä¸­æ¥å¢å¼ºç©ºé—´æ¨ç†èƒ½åŠ›å’Œæ¨¡æ€å¯è½¬ç§»æ€§ã€‚ç°æœ‰çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹é€šå¸¸åŸºäº2Dç¼–ç å™¨ï¼Œå¯¼è‡´ç©ºé—´æ¨ç†å­˜åœ¨å·®è·ï¼Œé™åˆ¶äº†æ¨¡å‹çš„æ³›åŒ–å’Œé€‚åº”èƒ½åŠ›ã€‚FALCONåˆ©ç”¨ç©ºé—´åŸºç¡€æ¨¡å‹ï¼Œä»RGBå›¾åƒä¸­æå–å¼ºå‡ ä½•å…ˆéªŒï¼Œå¹¶é€šè¿‡ç©ºé—´å¢å¼ºåŠ¨ä½œå¤´å¤„ç†ç©ºé—´æ ‡è®°ï¼Œä»¥ä¿æŒè¯­è¨€æ¨ç†çš„å®Œæ•´æ€§ã€‚ç»è¿‡åœ¨å¤šä¸ªæ¨¡æ‹ŸåŸºå‡†å’ŒçœŸå®ä»»åŠ¡ä¸­çš„å…¨é¢è¯„ä¼°ï¼ŒFALCONåœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†ç«äº‰å¯¹æ‰‹ï¼Œå±•ç°å‡ºå“è¶Šçš„é²æ£’æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.20661', 'title': 'UltraHR-100K: Enhancing UHR Image Synthesis with A Large-Scale\n  High-Quality Dataset', 'url': 'https://huggingface.co/papers/2510.20661', 'abstract': 'A new dataset and frequency-aware post-training method improve fine-grained detail synthesis in ultra-high-resolution text-to-image diffusion models.  \t\t\t\t\tAI-generated summary \t\t\t\t Ultra-high-resolution (UHR) text-to-image (T2I) generation has seen notable progress. However, two key challenges remain : 1) the absence of a large-scale high-quality UHR T2I dataset, and (2) the neglect of tailored training strategies for fine-grained detail synthesis in UHR scenarios. To tackle the first challenge, we introduce UltraHR-100K, a high-quality dataset of 100K UHR images with rich captions, offering diverse content and strong visual fidelity. Each image exceeds 3K resolution and is rigorously curated based on detail richness, content complexity, and aesthetic quality. To tackle the second challenge, we propose a frequency-aware post-training method that enhances fine-detail generation in T2I diffusion models. Specifically, we design (i) Detail-Oriented Timestep Sampling (DOTS) to focus learning on detail-critical denoising steps, and (ii) Soft-Weighting Frequency Regularization (SWFR), which leverages Discrete Fourier Transform (DFT) to softly constrain frequency components, encouraging high-frequency detail preservation. Extensive experiments on our proposed UltraHR-eval4K benchmarks demonstrate that our approach significantly improves the fine-grained detail quality and overall fidelity of UHR image generation. The code is available at https://github.com/NJU-PCALab/UltraHR-100k{here}.', 'score': 9, 'issue_id': 6678, 'pub_date': '2025-10-23', 'pub_date_card': {'ru': '23 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 23', 'zh': '10æœˆ23æ—¥'}, 'hash': 'b8ec9bcd7c4829c5', 'authors': ['Chen Zhao', 'En Ci', 'Yunzhe Xu', 'Tiehan Fan', 'Shanyan Guan', 'Yanhao Ge', 'Jian Yang', 'Ying Tai'], 'affiliations': ['State Key Laboratory of Novel Software Technology, Nanjing University, China', 'vivo Mobile Communication Co., Ltd., China'], 'pdf_title_img': 'assets/pdf/title_img/2510.20661.jpg', 'data': {'categories': ['#training', '#benchmark', '#synthetic', '#dataset', '#diffusion', '#cv'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'Ğ¡Ğ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾ Ğ² Ğ´ĞµÑ‚Ğ°Ğ»ÑÑ…: Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ğ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ´Ğ»Ñ ÑĞ²ĞµÑ€Ñ…Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ UltraHR-100K Ğ¸Ğ· 100 Ñ‚Ñ‹ÑÑÑ‡ ÑƒĞ»ÑŒÑ‚Ñ€Ğ°Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ (Ğ±Ğ¾Ğ»ĞµĞµ 3K) Ñ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ÑĞ¼Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ text-to-image Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ post-training, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¼ĞµĞ»ĞºĞ¸Ñ… Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºÑƒ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ² (DOTS) Ğ¸ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¤ÑƒÑ€ÑŒĞµ (SWFR). ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ diffusion Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ»ÑƒÑ‡ÑˆĞµ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ‚ÑŒ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ñ‹Ğµ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ Ğ´Ğ»Ñ ÑƒĞ»ÑŒÑ‚Ñ€Ğ°Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ğ±Ñ‰ĞµĞ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Enhancing Ultra-High-Resolution Image Generation with New Dataset and Training Techniques', 'desc': 'This paper presents a new dataset called UltraHR-100K, which contains 100,000 ultra-high-resolution images with detailed captions, aimed at improving text-to-image (T2I) generation. The authors address two main challenges in T2I models: the lack of a high-quality dataset and the need for better training methods for fine details. They introduce a frequency-aware post-training technique that includes Detail-Oriented Timestep Sampling (DOTS) and Soft-Weighting Frequency Regularization (SWFR) to enhance detail synthesis. Experimental results show that their methods significantly improve the quality and fidelity of generated images in ultra-high resolutions.'}, 'zh': {'title': 'æå‡è¶…é«˜åˆ†è¾¨ç‡å›¾åƒç”Ÿæˆçš„ç»†èŠ‚è´¨é‡', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ•°æ®é›†å’Œåè®­ç»ƒæ–¹æ³•ï¼Œä»¥æ”¹å–„è¶…é«˜åˆ†è¾¨ç‡æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹ä¸­çš„ç»†èŠ‚åˆæˆã€‚æˆ‘ä»¬å¼•å…¥äº†UltraHR-100Kæ•°æ®é›†ï¼ŒåŒ…å«10ä¸‡å¼ é«˜è´¨é‡çš„è¶…é«˜åˆ†è¾¨ç‡å›¾åƒï¼Œé…æœ‰ä¸°å¯Œçš„æè¿°ï¼Œç¡®ä¿å†…å®¹å¤šæ ·æ€§å’Œè§†è§‰çœŸå®æ„Ÿã€‚ä¸ºäº†è§£å†³ç»†èŠ‚åˆæˆçš„é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é¢‘ç‡æ„ŸçŸ¥çš„åè®­ç»ƒæ–¹æ³•ï¼ŒåŒ…æ‹¬ç»†èŠ‚å¯¼å‘æ—¶é—´æ­¥é‡‡æ ·å’Œè½¯åŠ æƒé¢‘ç‡æ­£åˆ™åŒ–ï¼Œä»¥å¢å¼ºç»†èŠ‚ç”Ÿæˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—æé«˜äº†è¶…é«˜åˆ†è¾¨ç‡å›¾åƒç”Ÿæˆçš„ç»†èŠ‚è´¨é‡å’Œæ•´ä½“çœŸå®æ„Ÿã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.24081', 'title': 'Global PIQA: Evaluating Physical Commonsense Reasoning Across 100+\n  Languages and Cultures', 'url': 'https://huggingface.co/papers/2510.24081', 'abstract': 'Global PIQA is a multilingual commonsense reasoning benchmark that highlights performance gaps of large language models across different cultures and languages.  \t\t\t\t\tAI-generated summary \t\t\t\t To date, there exist almost no culturally-specific evaluation benchmarks for large language models (LLMs) that cover a large number of languages and cultures. In this paper, we present Global PIQA, a participatory commonsense reasoning benchmark for over 100 languages, constructed by hand by 335 researchers from 65 countries around the world. The 116 language varieties in Global PIQA cover five continents, 14 language families, and 23 writing systems. In the non-parallel split of Global PIQA, over 50% of examples reference local foods, customs, traditions, or other culturally-specific elements. We find that state-of-the-art LLMs perform well on Global PIQA in aggregate, but they exhibit weaker performance in lower-resource languages (up to a 37% accuracy gap, despite random chance at 50%). Open models generally perform worse than proprietary models. Global PIQA highlights that in many languages and cultures, everyday knowledge remains an area for improvement, alongside more widely-discussed capabilities such as complex reasoning and expert knowledge. Beyond its uses for LLM evaluation, we hope that Global PIQA provides a glimpse into the wide diversity of cultures in which human language is embedded.', 'score': 8, 'issue_id': 6682, 'pub_date': '2025-10-28', 'pub_date_card': {'ru': '28 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 28', 'zh': '10æœˆ28æ—¥'}, 'hash': '87e621d34cc9f647', 'authors': ['Tyler A. Chang', 'Catherine Arnett', 'Abdelrahman Eldesokey', 'Abdelrahman Sadallah', 'Abeer Kashar', 'Abolade Daud', 'Abosede Grace Olanihun', 'Adamu Labaran Mohammed', 'Adeyemi Praise', 'Adhikarinayum Meerajita Sharma', 'Aditi Gupta', 'Afitab Iyigun', 'Afonso SimplÃ­cio', 'Ahmed Essouaied', 'Aicha Chorana', 'Akhil Eppa', 'Akintunde Oladipo', 'Akshay Ramesh', 'Aleksei Dorkin', 'Alfred Malengo Kondoro', 'Alham Fikri Aji', 'Ali Eren Ã‡etintaÅŸ', 'Allan Hanbury', 'Alou Dembele', 'Alp Niksarli', 'Ãlvaro Arroyo', 'Amin Bajand', 'Amol Khanna', 'Ana Chkhaidze', 'Ana Condez', 'Andiswa Mkhonto', 'Andrew Hoblitzell', 'Andrew Tran', 'Angelos Poulis', 'Anirban Majumder', 'Anna Vacalopoulou', 'Annette Kuuipolani Kanahele Wong', 'Annika Simonsen', 'Anton Kovalev', 'Ashvanth. S', 'Ayodeji Joseph Lana', 'Barkin Kinay', 'Bashar Alhafni', 'Benedict Cibalinda Busole', 'Bernard Ghanem', 'Bharti Nathani', 'Biljana Stojanovska ÄuriÄ‡', 'Bola Agbonile', 'Bragi Bergsson', 'Bruce Torres Fischer', 'Burak Tutar', 'Burcu AlakuÅŸ Ã‡Ä±nar', 'Cade J. Kanoniakapueo Kane', 'Can Udomcharoenchaikit', 'Catherine Arnett', 'Chadi Helwe', 'Chaithra Reddy Nerella', 'Chen Cecilia Liu', 'Chiamaka Glory Nwokolo', 'Cristina EspaÃ±a-Bonet', 'Cynthia Amol', 'DaeYeop Lee', 'Dana Arad', 'Daniil Dzenhaliou', 'Daria Pugacheva', 'Dasol Choi', 'Daud Abolade', 'David Liu', 'David Semedo', 'Deborah Popoola', 'Deividas Mataciunas', 'Delphine Nyaboke', 'Dhyuthy Krishna Kumar', 'Diogo GlÃ³ria-Silva', 'Diogo Tavares', 'Divyanshu Goyal', 'DongGeon Lee', 'Ebele Nwamaka Anajemba', 'Egonu Ngozi Grace', 'Elena Mickel', 'Elena Tutubalina', 'Elias Herranen', 'Emile Anand', 'Emmanuel Habumuremyi', 'Emuobonuvie Maria Ajiboye', 'Eryawan Presma Yulianrifat', 'Esther Adenuga', 'Ewa Rudnicka', 'Faith Olabisi Itiola', 'Faran Taimoor Butt', 'Fathima Thekkekara', 'Fatima Haouari', 'Filbert Aurelian Tjiaranata', 'Firas Laakom', 'Francesca Grasso', 'Francesco Orabona', 'Francesco Periti', 'Gbenga Kayode Solomon', 'Gia Nghia Ngo', 'Gloria Udhehdhe-oze', 'GonÃ§alo Martins', 'Gopi Naga Sai Ram Challagolla', 'Guijin Son', 'Gulnaz Abdykadyrova', 'Hafsteinn Einarsson', 'Hai Hu', 'Hamidreza Saffari', 'Hamza Zaidi', 'Haopeng Zhang', 'Harethah Abu Shairah', 'Harry Vuong', 'Hele-Andra Kuulmets', 'Houda Bouamor', 'Hwanjo Yu', 'Iben Nyholm Debess', 'Ä°brahim Ethem Deveci', 'Ikhlasul Akmal Hanif', 'Ikhyun Cho', 'InÃªs Calvo', 'InÃªs Vieira', 'Isaac Manzi', 'Ismail Daud', 'Itay Itzhak', 'Iuliia', 'Alekseenko', 'Ivan Belashkin', 'Ivan Spada', 'Ivan Zhelyazkov', 'Jacob Brinton', 'Jafar Isbarov', 'Jaka ÄŒibej', 'Jan ÄŒuhel', 'Jan KocoÅ„', 'Jauza Akbar Krito', 'Jebish Purbey', 'Jennifer Mickel', 'Jennifer Za', 'Jenny Kunz', 'Jihae Jeong', 'Jimena Tena DÃ¡valos', 'Jinu Lee', 'JoÃ£o MagalhÃ£es', 'John Yi', 'Jongin Kim', 'Joseph Chataignon', 'Joseph Marvin Imperial', 'Jubeerathan Thevakumar', 'Judith Land', 'Junchen Jiang', 'Jungwhan Kim', 'Kairit Sirts', 'Kamesh R', 'Kamesh V', 'Kanda Patrick Tshinu', 'KÃ¤triin Kukk', 'Kaustubh Ponkshe', 'Kavsar Huseynova', 'Ke He', 'Kelly Buchanan', 'Kengatharaiyer Sarveswaran', 'Kerem Zaman', 'Khalil Mrini', 'Kian Kyars', 'Krister Kruusmaa', 'Kusum Chouhan', 'Lainitha Krishnakumar', 'Laura Castro SÃ¡nchez', 'Laura Porrino Moscoso', 'Leshem Choshen', 'Levent Sencan', 'Lilja Ã˜vrelid', 'Lisa Alazraki', 'Lovina Ehimen-Ugbede', 'Luheerathan Thevakumar', 'Luxshan Thavarasa', 'Mahnoor Malik', 'Mamadou K. Keita', 'Mansi Jangid', 'Marco De Santis', 'Marcos GarcÃ­a', 'Marek Suppa', "Mariam D'Ciofalo", 'Marii Ojastu', 'Maryam Sikander', 'Mausami Narayan', 'Maximos Skandalis', 'Mehak Mehak', 'Mehmet Ä°lteriÅŸ Bozkurt', 'Melaku Bayu Workie', 'Menan Velayuthan', 'Michael Leventhal', 'MichaÅ‚ MarciÅ„czuk', 'Mirna PotoÄnjak', 'Mohammadamin Shafiei', 'Mridul Sharma', 'Mrityunjaya Indoria', 'Muhammad Ravi Shulthan Habibi', 'Murat KoliÄ‡', 'Nada Galant', 'Naphat Permpredanun', 'Narada Maugin', 'Nicholas Kluge CorrÃªa', 'Nikola LjubeÅ¡iÄ‡', 'Nirmal Thomas', 'Nisansa de Silva', 'Nisheeth Joshi', 'Nitish Ponkshe', 'Nizar Habash', 'Nneoma C. Udeze', 'Noel Thomas', 'NoÃ©mi Ligeti-Nagy', 'Nouhoum Coulibaly', 'Nsengiyumva Faustin', 'Odunayo Kareemat Buliaminu', 'Odunayo Ogundepo', 'Oghojafor Godswill Fejiro', 'Ogundipe Blessing Funmilola', "Okechukwu God'spraise", 'Olanrewaju Samuel', 'Olaoye Deborah Oluwaseun', 'Olasoji Akindejoye', 'Olga Popova', 'Olga Snissarenko', 'Onyinye Anulika Chiemezie', 'Orkun Kinay', 'Osman Tursun', 'Owoeye Tobiloba Moses', 'Oyelade Oluwafemi Joshua', 'Oyesanmi Fiyinfoluwa', 'Pablo Gamallo', 'Pablo RodrÃ­guez FernÃ¡ndez', 'Palak Arora', 'Pedro Valente', 'Peter Rupnik', 'Philip Oghenesuowho Ekiugbo', 'Pramit Sahoo', 'Prokopis Prokopidis', 'Pua Niau-Puhipau', 'Quadri Yahya', 'Rachele Mignone', 'Raghav Singhal', 'Ram Mohan Rao Kadiyala', 'Raphael Merx', 'Rapheal Afolayan', 'Ratnavel Rajalakshmi', 'Rishav Ghosh', 'Romina Oji', 'Ron Kekeha Solis', 'Rui Guerra', 'Rushikesh Zawar', "Sa'ad Nasir Bashir", 'Saeed Alzaabi', 'Sahil Sandeep', 'Sai Pavan Batchu', 'SaiSandeep Kantareddy', 'Salsabila Zahirah Pranida', 'Sam Buchanan', 'Samuel Rutunda', 'Sander Land', 'Sarah Sulollari', 'Sardar Ali', 'Saroj Sapkota', 'Saulius Tautvaisas', 'Sayambhu Sen', 'Sayantani Banerjee', 'Sebastien Diarra', 'SenthilNathan. M', 'Sewoong Lee', 'Shaan Shah', 'Shankar Venkitachalam', 'Sharifa Djurabaeva', 'Sharon Ibejih', 'Shivanya Shomir Dutta', 'Siddhant Gupta', 'Silvia Paniagua SuÃ¡rez', 'Sina Ahmadi', 'Sivasuthan Sukumar', 'Siyuan Song', 'Snegha A.', 'Sokratis Sofianopoulos', 'Sona Elza Simon', 'Sonja BenÄina', 'Sophie Gvasalia', 'Sphurti Kirit More', 'Spyros Dragazis', 'Stephan P. Kaufhold', 'Suba. S', 'Sultan AlRashed', 'Surangika Ranathunga', 'Taiga Someya', 'Taja Kuzman PungerÅ¡ek', 'Tal Haklay', "Tasi'u Jibril", 'Tatsuya Aoyama', 'Tea Abashidze', 'Terenz Jomar Dela Cruz', 'Terra Blevins', 'Themistoklis Nikas', 'Theresa Dora Idoko', 'Thu Mai Do', 'Tilek Chubakov', 'Tommaso Gargiani', 'Uma Rathore', 'Uni Johannesen', 'Uwuma Doris Ugwu', 'Vallerie Alexandra Putra', 'Vanya Bannihatti Kumar', 'Varsha Jeyarajalingam', 'Varvara Arzt', 'Vasudevan Nedumpozhimana', 'Viktoria Ondrejova', 'Viktoryia Horbik', 'Vishnu Vardhan Reddy Kummitha', 'Vuk DiniÄ‡', 'Walelign Tewabe Sewunetie', 'Winston Wu', 'Xiaojing Zhao', 'Yacouba Diarra', 'Yaniv Nikankin', 'Yash Mathur', 'Yixi Chen', 'Yiyuan Li', 'Yolanda Xavier', 'Yonatan Belinkov', 'Yusuf Ismail Abayomi', 'Zaid Alyafeai', 'Zhengyang Shan', 'Zhi Rui Tam', 'Zilu Tang', 'Zuzana Nadova', 'Baber Abbasi', 'Stella Biderman', 'David Stap', 'Duygu Ataman', 'Fabian Schmidt', 'Hila Gonen', 'Jiayi Wang', 'David Ifeoluwa Adelani'], 'affiliations': ['EleutherAI', 'UC San Diego'], 'pdf_title_img': 'assets/pdf/title_img/2510.24081.jpg', 'data': {'categories': ['#dataset', '#reasoning', '#ethics', '#multilingual', '#low_resource', '#benchmark'], 'emoji': 'ğŸŒ', 'ru': {'title': 'ĞšÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ¾Ğ±Ğ½Ğ°Ğ¶Ğ°ĞµÑ‚ ÑĞ»Ğ°Ğ±Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Global PIQA â€” ÑÑ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ·Ğ´Ñ€Ğ°Ğ²Ğ¾Ğ³Ğ¾ ÑĞ¼Ñ‹ÑĞ»Ğ° LLM, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ²Ñ€ÑƒÑ‡Ğ½ÑƒÑ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼Ğ¸ Ğ¸Ğ· 65 ÑÑ‚Ñ€Ğ°Ğ½ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 100 ÑĞ·Ñ‹ĞºĞ¾Ğ². Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ 14 ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… ÑĞµĞ¼ĞµĞ¹ Ğ¸ 23 ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¿Ğ¸ÑÑŒĞ¼Ğ°, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ±Ğ¾Ğ»ĞµĞµ 50% Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‚ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ğ¾-ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ñ‹, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ¼ĞµÑÑ‚Ğ½Ğ°Ñ ĞµĞ´Ğ°, Ğ¾Ğ±Ñ‹Ñ‡Ğ°Ğ¸ Ğ¸ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¸. Ğ¡Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ LLM Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² ÑÑ€ĞµĞ´Ğ½ĞµĞ¼, Ğ½Ğ¾ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ°Ğ´ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ (Ğ´Ğ¾ 37%) Ğ½Ğ° Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€ĞµÑÑƒÑ€ÑĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ°Ñ… Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ ÑĞ¾ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ñ‹Ğ¼ ÑƒĞ³Ğ°Ğ´Ñ‹Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ² 50%. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ²ÑĞµĞ´Ğ½ĞµĞ²Ğ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ, ÑƒĞºĞ¾Ñ€ĞµĞ½ĞµĞ½Ğ½Ñ‹Ğµ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ°Ñ…, Ğ¾ÑÑ‚Ğ°ÑÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑŒÑ Ğ´Ğ»Ñ AI-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ°Ñ€Ğ°Ğ²Ğ½Ğµ ÑĞ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸.'}, 'en': {'title': 'Bridging Cultural Gaps in Language Models with Global PIQA', 'desc': 'Global PIQA is a new benchmark designed to evaluate the commonsense reasoning abilities of large language models (LLMs) across more than 100 languages and cultures. It was created by a diverse team of researchers and includes culturally-specific examples that reflect local customs and traditions. The study reveals that while LLMs perform well overall, they struggle significantly with lower-resource languages, showing a notable accuracy gap. This benchmark not only assesses LLM performance but also emphasizes the importance of understanding cultural diversity in language processing.'}, 'zh': {'title': 'å…¨çƒPIQAï¼šè·¨æ–‡åŒ–å¸¸è¯†æ¨ç†çš„æ–°åŸºå‡†', 'desc': 'Global PIQAæ˜¯ä¸€ä¸ªå¤šè¯­è¨€çš„å¸¸è¯†æ¨ç†åŸºå‡†ï¼Œæ—¨åœ¨æ­ç¤ºå¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸åŒæ–‡åŒ–å’Œè¯­è¨€ä¹‹é—´çš„è¡¨ç°å·®è·ã€‚è¯¥åŸºå‡†ç”±æ¥è‡ª65ä¸ªå›½å®¶çš„335åç ”ç©¶äººå‘˜æ‰‹å·¥æ„å»ºï¼Œæ¶µç›–è¶…è¿‡100ç§è¯­è¨€ï¼Œæ¶‰åŠäº”å¤§æ´²ã€14ä¸ªè¯­è¨€å®¶æ—å’Œ23ç§ä¹¦å†™ç³»ç»Ÿã€‚Global PIQAä¸­çš„ç¤ºä¾‹è¶…è¿‡50%æ¶‰åŠå½“åœ°çš„é£Ÿç‰©ã€ä¹ ä¿—å’Œä¼ ç»Ÿç­‰æ–‡åŒ–ç‰¹å¾ï¼Œæ˜¾ç¤ºå‡ºæ–‡åŒ–ç‰¹å®šæ€§çš„é‡è¦æ€§ã€‚ç ”ç©¶å‘ç°ï¼Œå°½ç®¡æœ€å…ˆè¿›çš„è¯­è¨€æ¨¡å‹åœ¨æ•´ä½“ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨ä½èµ„æºè¯­è¨€ä¸­çš„è¡¨ç°è¾ƒå¼±ï¼Œå‡†ç¡®ç‡å·®è·å¯è¾¾37%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.23925', 'title': 'Latent Chain-of-Thought for Visual Reasoning', 'url': 'https://huggingface.co/papers/2510.23925', 'abstract': 'The proposed method reformulates reasoning in Large Vision-Language Models as posterior inference using amortized variational inference and a sparse reward function, improving effectiveness, generalization, and interpretability.  \t\t\t\t\tAI-generated summary \t\t\t\t Chain-of-thought (CoT) reasoning is critical for improving the interpretability and reliability of Large Vision-Language Models (LVLMs). However, existing training algorithms such as SFT, PPO, and GRPO may not generalize well across unseen reasoning tasks and heavily rely on a biased reward model. To address this challenge, we reformulate reasoning in LVLMs as posterior inference and propose a scalable training algorithm based on amortized variational inference. By leveraging diversity-seeking reinforcement learning algorithms, we introduce a novel sparse reward function for token-level learning signals that encourage diverse, high-likelihood latent CoT, overcoming deterministic sampling limitations and avoiding reward hacking. Additionally, we implement a Bayesian inference-scaling strategy that replaces costly Best-of-N and Beam Search with a marginal likelihood to efficiently rank optimal rationales and answers. We empirically demonstrate that the proposed method enhances the state-of-the-art LVLMs on seven reasoning benchmarks, in terms of effectiveness, generalization, and interpretability.', 'score': 8, 'issue_id': 6684, 'pub_date': '2025-10-27', 'pub_date_card': {'ru': '27 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 27', 'zh': '10æœˆ27æ—¥'}, 'hash': '0868d2155ce6b87b', 'authors': ['Guohao Sun', 'Hang Hua', 'Jian Wang', 'Jiebo Luo', 'Sohail Dianat', 'Majid Rabbani', 'Raghuveer Rao', 'Zhiqiang Tao'], 'affiliations': ['DEVCOM Army Research Laboratory', 'Rochester Institute of Technology', 'Snap Inc.', 'University of Rochester'], 'pdf_title_img': 'assets/pdf/title_img/2510.23925.jpg', 'data': {'categories': ['#interpretability', '#training', '#multimodal', '#benchmark', '#reasoning', '#rl', '#rlhf'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ‘Ğ°Ğ¹ĞµÑĞ¾Ğ²ÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… LLM', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LVLMs) Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²ĞºÑƒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ ĞºĞ°Ğº Ğ±Ğ°Ğ¹ĞµÑĞ¾Ğ²ÑĞºĞ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ¸. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ² Ñ‚Ğ¸Ğ¿Ğ° PPO Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ reinforcement learning Ñ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ¾Ğ¹ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¸Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (chain-of-thought). Ğ”Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ°Ñ€Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ¸Ğµ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Best-of-N. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° ÑĞµĞ¼Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ĞµĞ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Revolutionizing Reasoning in LVLMs with Amortized Inference', 'desc': 'This paper presents a new approach to improve Large Vision-Language Models (LVLMs) by treating reasoning as posterior inference through amortized variational inference. The authors highlight the limitations of traditional training methods, which often struggle with generalization and rely on biased reward models. To overcome these issues, they introduce a sparse reward function that promotes diverse reasoning paths and enhances interpretability. Their method shows significant improvements in performance across various reasoning tasks, demonstrating better effectiveness and generalization capabilities.'}, 'zh': {'title': 'æå‡å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œå°†å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰çš„æ¨ç†è¿‡ç¨‹é‡æ–°è¡¨è¿°ä¸ºåéªŒæ¨æ–­ï¼Œé‡‡ç”¨äº†æ‘Šé”€å˜åˆ†æ¨æ–­å’Œç¨€ç–å¥–åŠ±å‡½æ•°ã€‚è¿™ç§æ–¹æ³•æé«˜äº†æ¨¡å‹çš„æœ‰æ•ˆæ€§ã€æ³›åŒ–èƒ½åŠ›å’Œå¯è§£é‡Šæ€§ã€‚é€šè¿‡å¼•å…¥å¤šæ ·æ€§å¯»æ±‚çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„ç¨€ç–å¥–åŠ±å‡½æ•°ï¼Œä»¥ä¿ƒè¿›å¤šæ ·åŒ–çš„é«˜å¯èƒ½æ€§é“¾å¼æ¨ç†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸ƒä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡äº†ç°æœ‰LVLMçš„è¡¨ç°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.24684', 'title': 'SPICE: Self-Play In Corpus Environments Improves Reasoning', 'url': 'https://huggingface.co/papers/2510.24684', 'abstract': "SPICE, a reinforcement learning framework, uses self-play in a corpus environment to continuously improve a model's reasoning capabilities through adversarial dynamics and document grounding.  \t\t\t\t\tAI-generated summary \t\t\t\t Self-improving systems require environmental interaction for continuous adaptation. We introduce SPICE (Self-Play In Corpus Environments), a reinforcement learning framework where a single model acts in two roles: a Challenger that mines documents from a large corpus to generate diverse reasoning tasks, and a Reasoner that solves them. Through adversarial dynamics, the Challenger creates an automatic curriculum at the frontier of the Reasoner's capability, while corpus grounding provides the rich, near-inexhaustible external signal necessary for sustained improvement. Unlike existing ungrounded self-play methods that offer more limited benefits, SPICE achieves consistent gains across mathematical (+8.9%) and general reasoning (+9.8%) benchmarks on multiple model families. Our analysis reveals how document grounding is a key ingredient in SPICE to continuously generate its own increasingly challenging goals and achieve them, enabling sustained self-improvement.", 'score': 5, 'issue_id': 6681, 'pub_date': '2025-10-28', 'pub_date_card': {'ru': '28 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 28', 'zh': '10æœˆ28æ—¥'}, 'hash': 'ee9331019ee63467', 'authors': ['Bo Liu', 'Chuanyang Jin', 'Seungone Kim', 'Weizhe Yuan', 'Wenting Zhao', 'Ilia Kulikov', 'Xian Li', 'Sainbayar Sukhbaatar', 'Jack Lanchantin', 'Jason Weston'], 'affiliations': ['FAIR at Meta', 'National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2510.24684.jpg', 'data': {'categories': ['#optimization', '#rl', '#benchmark', '#reasoning', '#rlhf'], 'emoji': 'ğŸ­', 'ru': {'title': 'Ğ”Ğ²Ğµ Ñ€Ğ¾Ğ»Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· adversarial Ğ¸Ğ³Ñ€Ñƒ Ñ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸', 'desc': 'SPICE - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ reinforcement learning, Ğ³Ğ´Ğµ Ğ¾Ğ´Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸Ğ³Ñ€Ğ°ĞµÑ‚ Ğ´Ğ²Ğµ Ñ€Ğ¾Ğ»Ğ¸: Challenger Ğ¸Ñ‰ĞµÑ‚ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ² ĞºĞ¾Ñ€Ğ¿ÑƒÑĞµ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ½Ğ° reasoning, Ğ° Reasoner Ğ¸Ñ… Ñ€ĞµÑˆĞ°ĞµÑ‚. Ğ‘Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ adversarial Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞµ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑÑ‚Ğ¸Ğ¼Ğ¸ Ñ€Ğ¾Ğ»ÑĞ¼Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ÑÑ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ curriculum learning Ğ½Ğ° Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ - Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸Ğ· ĞºĞ¾Ñ€Ğ¿ÑƒÑĞ° ĞºĞ°Ğº Ğ½ĞµĞ¸ÑÑ‡ĞµÑ€Ğ¿Ğ°ĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ° ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ° Ğ´Ğ»Ñ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ³Ğ¾ self-improvement. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ€Ğ¾ÑÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ½Ğ° Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… (+8.9%) Ğ¸ Ğ¾Ğ±Ñ‰Ğ¸Ñ… (+9.8%) Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ´Ğ»Ñ reasoning.'}, 'en': {'title': 'SPICE: Self-Play for Continuous Reasoning Improvement', 'desc': "SPICE is a reinforcement learning framework that enhances a model's reasoning abilities through self-play in a document-rich environment. It features two roles: a Challenger that generates diverse reasoning tasks from a large corpus, and a Reasoner that attempts to solve these tasks. This adversarial setup creates a dynamic learning environment where the Challenger continuously adapts the difficulty of tasks based on the Reasoner's performance. By grounding the learning process in real documents, SPICE achieves significant improvements in reasoning tasks compared to traditional self-play methods."}, 'zh': {'title': 'SPICEï¼šè‡ªæˆ‘æå‡çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶', 'desc': 'SPICEæ˜¯ä¸€ç§å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡è‡ªæˆ‘å¯¹å¼ˆåœ¨è¯­æ–™ç¯å¢ƒä¸­ä¸æ–­æå‡æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚è¯¥æ¡†æ¶ä¸­ï¼Œå•ä¸€æ¨¡å‹æ‰®æ¼”æŒ‘æˆ˜è€…å’Œæ¨ç†è€…ä¸¤ä¸ªè§’è‰²ï¼ŒæŒ‘æˆ˜è€…ä»å¤§è¯­æ–™åº“ä¸­æŒ–æ˜æ–‡æ¡£ä»¥ç”Ÿæˆå¤šæ ·çš„æ¨ç†ä»»åŠ¡ï¼Œè€Œæ¨ç†è€…åˆ™è´Ÿè´£è§£å†³è¿™äº›ä»»åŠ¡ã€‚é€šè¿‡å¯¹æŠ—åŠ¨æ€ï¼ŒæŒ‘æˆ˜è€…ä¸ºæ¨ç†è€…åˆ›é€ äº†ä¸€ä¸ªè‡ªåŠ¨åŒ–çš„è¯¾ç¨‹ï¼Œä½¿å…¶ä¸æ–­é¢å¯¹æ›´å…·æŒ‘æˆ˜æ€§çš„ç›®æ ‡ã€‚SPICEåœ¨å¤šä¸ªæ¨¡å‹å®¶æ—çš„æ•°å­¦å’Œä¸€èˆ¬æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å‡å–å¾—äº†æ˜¾è‘—çš„æå‡ï¼Œè¡¨æ˜æ–‡æ¡£åŸºç¡€æ˜¯å…¶æŒç»­è‡ªæˆ‘æ”¹è¿›çš„å…³é”®å› ç´ ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.22768', 'title': 'MMPersuade: A Dataset and Evaluation Framework for Multimodal Persuasion', 'url': 'https://huggingface.co/papers/2510.22768', 'abstract': 'MMPersuade is a framework for studying multimodal persuasion in Large Vision-Language Models, revealing insights into their susceptibility and the effectiveness of various persuasive strategies across different contexts.  \t\t\t\t\tAI-generated summary \t\t\t\t As Large Vision-Language Models (LVLMs) are increasingly deployed in domains such as shopping, health, and news, they are exposed to pervasive persuasive content. A critical question is how these models function as persuadees-how and why they can be influenced by persuasive multimodal inputs. Understanding both their susceptibility to persuasion and the effectiveness of different persuasive strategies is crucial, as overly persuadable models may adopt misleading beliefs, override user preferences, or generate unethical or unsafe outputs when exposed to manipulative messages. We introduce MMPersuade, a unified framework for systematically studying multimodal persuasion dynamics in LVLMs. MMPersuade contributes (i) a comprehensive multimodal dataset that pairs images and videos with established persuasion principles across commercial, subjective and behavioral, and adversarial contexts, and (ii) an evaluation framework that quantifies both persuasion effectiveness and model susceptibility via third-party agreement scoring and self-estimated token probabilities on conversation histories. Our study of six leading LVLMs as persuadees yields three key insights: (i) multimodal inputs substantially increase persuasion effectiveness-and model susceptibility-compared to text alone, especially in misinformation scenarios; (ii) stated prior preferences decrease susceptibility, yet multimodal information maintains its persuasive advantage; and (iii) different strategies vary in effectiveness across contexts, with reciprocity being most potent in commercial and subjective contexts, and credibility and logic prevailing in adversarial contexts. By jointly analyzing persuasion effectiveness and susceptibility, MMPersuade provides a principled foundation for developing models that are robust, preference-consistent, and ethically aligned when engaging with persuasive multimodal content.', 'score': 5, 'issue_id': 6682, 'pub_date': '2025-10-26', 'pub_date_card': {'ru': '26 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 26', 'zh': '10æœˆ26æ—¥'}, 'hash': '549393a3e78ca331', 'authors': ['Haoyi Qiu', 'Yilun Zhou', 'Pranav Narayanan Venkit', 'Kung-Hsiang Huang', 'Jiaxin Zhang', 'Nanyun Peng', 'Chien-Sheng Wu'], 'affiliations': ['Salesforce AI Research', 'University of California, Los Angeles'], 'pdf_title_img': 'assets/pdf/title_img/2510.22768.jpg', 'data': {'categories': ['#dataset', '#ethics', '#multimodal', '#alignment', '#benchmark'], 'emoji': 'ğŸ­', 'ru': {'title': 'ĞšĞ°Ğº Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ´ĞµĞ»Ğ°ÑÑ‚ AI-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ğ½ÑƒÑˆĞ°ĞµĞ¼Ñ‹Ğ¼Ğ¸', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ MMPersuade - Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… vision-language Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LVLMs). Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸Ğ¸Ğ¼Ñ‡Ğ¸Ğ²Ñ‹ Ğº ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ´ĞµĞ·Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. Ğ Ğ°Ğ·Ğ½Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ Ğ¿Ğ¾-Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¼Ñƒ: Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ½Ğ¾ÑÑ‚ÑŒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ° Ğ² ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ‡ĞµÑĞºĞ¸Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ñ…, Ğ° Ğ»Ğ¾Ğ³Ğ¸ĞºĞ° Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ - Ğ² Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾Ğ±Ğ¾Ñ€ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… ÑĞ¸Ñ‚ÑƒĞ°Ñ†Ğ¸ÑÑ…. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ñ‹Ğµ Ğ¸ ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ aligned Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ñƒ.'}, 'en': {'title': 'Understanding Persuasion in Large Vision-Language Models', 'desc': 'MMPersuade is a framework designed to explore how Large Vision-Language Models (LVLMs) respond to persuasive multimodal inputs, such as images and videos. The study reveals that these models can be significantly influenced by such inputs, especially in scenarios involving misinformation. It also finds that while having stated preferences can reduce susceptibility to persuasion, multimodal content still holds a persuasive edge. By analyzing the effectiveness of various persuasive strategies, MMPersuade aims to enhance the robustness and ethical alignment of LVLMs when they encounter persuasive content.'}, 'zh': {'title': 'å¤šæ¨¡æ€è¯´æœçš„ç ”ç©¶æ–°æ¡†æ¶', 'desc': 'MMPersuadeæ˜¯ä¸€ä¸ªç ”ç©¶å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨å¤šæ¨¡æ€è¯´æœä¸­çš„æ¡†æ¶ã€‚è¯¥æ¡†æ¶æ­ç¤ºäº†è¿™äº›æ¨¡å‹åœ¨ä¸åŒæƒ…å¢ƒä¸‹çš„æ˜“å—å½±å“æ€§å’Œå„ç§è¯´æœç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå¤šæ¨¡æ€è¾“å…¥æ˜¾è‘—æé«˜äº†è¯´æœæ•ˆæœï¼Œå°¤å…¶æ˜¯åœ¨é”™è¯¯ä¿¡æ¯çš„æƒ…å†µä¸‹ã€‚é€šè¿‡åˆ†æè¯´æœæ•ˆæœå’Œæ¨¡å‹çš„æ˜“å—å½±å“æ€§ï¼ŒMMPersuadeä¸ºå¼€å‘æ›´ç¨³å¥ã€ç¬¦åˆç”¨æˆ·åå¥½å’Œä¼¦ç†çš„æ¨¡å‹æä¾›äº†åŸºç¡€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.24645', 'title': 'FunReason-MT Technical Report: Overcoming the Complexity Barrier in\n  Multi-Turn Function Calling', 'url': 'https://huggingface.co/papers/2510.24645', 'abstract': 'FunReason-MT is a novel data synthesis framework that enhances multi-turn function calling in large language models by addressing challenges in environment interaction, query synthesis, and chain-of-thought generation, achieving state-of-the-art performance on the Berkeley Function-Calling Leaderboard.  \t\t\t\t\tAI-generated summary \t\t\t\t Function calling (FC) empowers large language models (LLMs) and autonomous agents to interface with external tools, a critical capability for solving complex, real-world problems. As this ability becomes increasingly central to advanced AI systems, the need for high-quality, multi-turn training data to develop and refine it cannot be overstated. Existing data synthesis methods, such as random environment sampling or multi-agent role-playing, are not powerful enough to generate high-quality data in real-world environments. Practical challenges come in three folds: targeted model training, isolation of tool architecture, and multi-turn logical dependency. To address these structural deficiencies, we present FunReason-MT, a novel data synthesis framework for real-world multi-turn tool use. FunReason-MT resolves the complexity barrier in multi-turn FC data by employing 1) Environment-API Graph Interactions to gather varied high-quality trajectories, 2) Advanced Tool-Query Synthesis to simplify hard query construction, and 3) Guided Iterative Chain for sophisticated CoT generation. Evaluations on Berkeley Function-Calling Leaderboard (BFCLv3) demonstrate the power of our framework: a 4B model built upon FunReason-MT generated data achieves state-of-the-art performance among comparable-sized models, outperforming most close-source models. Further performance improvements on BFCLv4 confirm that FunReason-MT provides a reliable and robust source for agentic learning.', 'score': 4, 'issue_id': 6668, 'pub_date': '2025-10-28', 'pub_date_card': {'ru': '28 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 28', 'zh': '10æœˆ28æ—¥'}, 'hash': 'e3e4dc7959a10fb7', 'authors': ['Zengzhuang Xu', 'Bingguang Hao', 'Zechuan Wang', 'Yuntao Wen', 'Maolin Wang', 'Yang Liu', 'Long Chen', 'Dong Wang', 'Yicheng Chen', 'Cunyin Peng', 'Chenyi Zhuang', 'Jinjie Gu', 'Leilei Gan', 'Xiangyu Zhao', 'Shi Gu'], 'affiliations': ['AWorld Team, Inclusion AI', 'City University of Hong Kong', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2510.24645.jpg', 'data': {'categories': ['#dataset', '#training', '#optimization', '#data', '#agents', '#transfer_learning'], 'emoji': 'ğŸ”§', 'ru': {'title': 'Ğ£Ñ‡Ğ¸Ğ¼ LLM Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ² Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ÑˆĞ°Ğ³Ğ¾Ğ²', 'desc': 'FunReason-MT â€” ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğµ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ñ‹ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ñ‚Ñ€Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹: Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ñ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸ĞµĞ¼ Ñ‡ĞµÑ€ĞµĞ· Ğ³Ñ€Ğ°Ñ„Ñ‹ Environment-API, ÑĞ¸Ğ½Ñ‚ĞµĞ· ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ chain-of-thought Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ Ğ²ÑĞµĞ³Ğ¾ 4B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… FunReason-MT, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ° state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Berkeley Function-Calling Leaderboard, Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾Ğ¹Ğ´Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ğ¹ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸Ğº ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğ¼Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Empowering AI with Enhanced Multi-Turn Function Calling', 'desc': "FunReason-MT is a new framework designed to improve how large language models (LLMs) perform multi-turn function calling, which is essential for interacting with external tools. It tackles issues related to generating high-quality training data by using advanced techniques like Environment-API Graph Interactions and Tool-Query Synthesis. This framework helps in creating better training scenarios that reflect real-world complexities, enhancing the model's ability to understand and generate logical sequences. The results show that models trained with FunReason-MT data achieve top performance on the Berkeley Function-Calling Leaderboard, indicating its effectiveness in advancing AI capabilities."}, 'zh': {'title': 'æå‡å¤šè½®å‡½æ•°è°ƒç”¨çš„æ™ºèƒ½æ¡†æ¶', 'desc': 'FunReason-MT æ˜¯ä¸€ä¸ªæ–°é¢–çš„æ•°æ®åˆæˆæ¡†æ¶ï¼Œæ—¨åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šè½®å‡½æ•°è°ƒç”¨ä¸­çš„è¡¨ç°ã€‚å®ƒé€šè¿‡è§£å†³ç¯å¢ƒäº¤äº’ã€æŸ¥è¯¢åˆæˆå’Œæ€ç»´é“¾ç”Ÿæˆç­‰æŒ‘æˆ˜ï¼ŒæˆåŠŸåœ¨ä¼¯å…‹åˆ©å‡½æ•°è°ƒç”¨æ’è¡Œæ¦œä¸Šå–å¾—äº†é¢†å…ˆçš„æ€§èƒ½ã€‚è¯¥æ¡†æ¶é‡‡ç”¨ç¯å¢ƒ-APIå›¾äº¤äº’ã€å…ˆè¿›çš„å·¥å…·-æŸ¥è¯¢åˆæˆå’Œå¼•å¯¼è¿­ä»£é“¾ç­‰æŠ€æœ¯ï¼Œç”Ÿæˆé«˜è´¨é‡çš„å¤šè½®æ•°æ®ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼ŒåŸºäº FunReason-MT ç”Ÿæˆæ•°æ®çš„ 4B æ¨¡å‹åœ¨åŒç±»æ¨¡å‹ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¯æ˜äº†å…¶åœ¨æ™ºèƒ½å­¦ä¹ ä¸­çš„å¯é æ€§å’Œç¨³å¥æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.24448', 'title': 'Rethinking Visual Intelligence: Insights from Video Pretraining', 'url': 'https://huggingface.co/papers/2510.24448', 'abstract': 'Video Diffusion Models (VDMs) show higher data efficiency than large language models across various visual tasks, suggesting video pretraining can enhance visual foundation models.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have demonstrated that large-scale pretraining enables systems to adapt rapidly to new problems with little supervision in the language domain. This success, however, has not translated as effectively to the visual domain, where models, including LLMs, continue to struggle with compositional understanding, sample efficiency, and general-purpose problem-solving. We investigate Video Diffusion Models (VDMs) as a promising direction for bridging this gap. Pretraining on spatiotemporal data endows these models with strong inductive biases for structure and dynamics, which we hypothesize can support broad task adaptability. To test this, we design a controlled evaluation in which both a pretrained LLM and a pretrained VDM are equipped with lightweight adapters and presented with tasks in their natural modalities. Across benchmarks including ARC-AGI, ConceptARC, visual games, route planning, and cellular automata, VDMs demonstrate higher data efficiency than their language counterparts. Taken together, our results indicate that video pretraining offers inductive biases that support progress toward visual foundation models.', 'score': 4, 'issue_id': 6667, 'pub_date': '2025-10-28', 'pub_date_card': {'ru': '28 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 28', 'zh': '10æœˆ28æ—¥'}, 'hash': 'cb963e7271205da4', 'authors': ['Pablo Acuaviva', 'Aram Davtyan', 'Mariam Hassan', 'Sebastian Stapf', 'Ahmad Rahimi', 'Alexandre Alahi', 'Paolo Favaro'], 'affiliations': ['Computer Vision Group University of Bern Bern, Switzerland', 'VITA Lab, EPFL Lausanne, Switzerland'], 'pdf_title_img': 'assets/pdf/title_img/2510.24448.jpg', 'data': {'categories': ['#multimodal', '#transfer_learning', '#games', '#diffusion', '#benchmark', '#video'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ’Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ñ Ğ¿Ğ¾Ğ±ĞµĞ¶Ğ´Ğ°ĞµÑ‚ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ»Ğ¸ Video Diffusion Models (VDMs) Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. VDMs, Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¼Ğ°Ğ»Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ¼Ñƒ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ÑÑŒ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ARC-AGI, ConceptARC, Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸Ğ³Ñ€Ñ‹ Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ°ĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğµ Ğ¸Ğ½Ğ´ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… foundation models.'}, 'en': {'title': 'Unlocking Visual Potential with Video Diffusion Models', 'desc': 'This paper explores the effectiveness of Video Diffusion Models (VDMs) in improving visual tasks through video pretraining. Unlike large language models (LLMs), which excel in language tasks, VDMs leverage spatiotemporal data to enhance their understanding of structure and dynamics. The authors conducted experiments comparing pretrained VDMs and LLMs, finding that VDMs showed superior data efficiency across various benchmarks. The results suggest that video pretraining can significantly advance the development of visual foundation models.'}, 'zh': {'title': 'è§†é¢‘é¢„è®­ç»ƒæå‡è§†è§‰æ¨¡å‹æ•ˆç‡', 'desc': 'è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ˆVDMsï¼‰åœ¨å„ç§è§†è§‰ä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºæ¯”å¤§å‹è¯­è¨€æ¨¡å‹æ›´é«˜çš„æ•°æ®æ•ˆç‡ï¼Œè¡¨æ˜è§†é¢‘é¢„è®­ç»ƒå¯ä»¥å¢å¼ºè§†è§‰åŸºç¡€æ¨¡å‹çš„èƒ½åŠ›ã€‚å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è¯­è¨€é¢†åŸŸçš„é¢„è®­ç»ƒå–å¾—äº†æˆåŠŸï¼Œä½†åœ¨è§†è§‰é¢†åŸŸï¼Œæ¨¡å‹ä»ç„¶é¢ä¸´ç»„åˆç†è§£å’Œæ ·æœ¬æ•ˆç‡ç­‰æŒ‘æˆ˜ã€‚æˆ‘ä»¬ç ”ç©¶VDMsä½œä¸ºå¼¥åˆè¿™ä¸€å·®è·çš„æœ‰å‰æ™¯çš„æ–¹å‘ï¼Œè®¤ä¸ºå…¶åœ¨æ—¶ç©ºæ•°æ®ä¸Šçš„é¢„è®­ç»ƒèµ‹äºˆäº†æ¨¡å‹å¼ºå¤§çš„ç»“æ„å’ŒåŠ¨æ€çš„å½’çº³åç½®ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒVDMsåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºæ¯”è¯­è¨€æ¨¡å‹æ›´é«˜çš„æ•°æ®æ•ˆç‡ï¼Œæ”¯æŒäº†è§†é¢‘é¢„è®­ç»ƒå¯¹è§†è§‰åŸºç¡€æ¨¡å‹çš„è¿›å±•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.24591', 'title': 'ReplicationBench: Can AI Agents Replicate Astrophysics Research Papers?', 'url': 'https://huggingface.co/papers/2510.24591', 'abstract': "ReplicationBench evaluates AI agents' ability to replicate astrophysics research papers, providing insights into their faithfulness and correctness in scientific research tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Frontier AI agents show increasing promise as scientific research assistants, and may eventually be useful for extended, open-ended research workflows. However, in order to use agents for novel research, we must first assess the underlying faithfulness and correctness of their work. To evaluate agents as research assistants, we introduce ReplicationBench, an evaluation framework that tests whether agents can replicate entire research papers drawn from the astrophysics literature. Astrophysics, where research relies heavily on archival data and computational study while requiring little real-world experimentation, is a particularly useful testbed for AI agents in scientific research. We split each paper into tasks which require agents to replicate the paper's core contributions, including the experimental setup, derivations, data analysis, and codebase. Each task is co-developed with the original paper authors and targets a key scientific result, enabling objective evaluation of both faithfulness (adherence to original methods) and correctness (technical accuracy of results). ReplicationBench is extremely challenging for current frontier language models: even the best-performing language models score under 20%. We analyze ReplicationBench trajectories in collaboration with domain experts and find a rich, diverse set of failure modes for agents in scientific research. ReplicationBench establishes the first benchmark of paper-scale, expert-validated astrophysics research tasks, reveals insights about agent performance generalizable to other domains of data-driven science, and provides a scalable framework for measuring AI agents' reliability in scientific research.", 'score': 3, 'issue_id': 6667, 'pub_date': '2025-10-28', 'pub_date_card': {'ru': '28 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 28', 'zh': '10æœˆ28æ—¥'}, 'hash': 'd655b0884e7b15c0', 'authors': ['Christine Ye', 'Sihan Yuan', 'Suchetha Cooray', 'Steven Dillmann', 'Ian L. V. Roque', 'Dalya Baron', 'Philipp Frank', 'Sergio Martin-Alvarez', 'Nolan Koblischke', 'Frank J Qu', 'Diyi Yang', 'Risa Wechsler', 'Ioana Ciuca'], 'affiliations': ['Stanford University', 'University of Toronto'], 'pdf_title_img': 'assets/pdf/title_img/2510.24591.jpg', 'data': {'categories': ['#benchmark', '#science', '#agents'], 'emoji': 'ğŸ”­', 'ru': {'title': 'ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ñ€ĞµĞ¿Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ ReplicationBench â€” Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ¿Ğ¾ Ğ°ÑÑ‚Ñ€Ğ¾Ñ„Ğ¸Ğ·Ğ¸ĞºĞµ. ĞšĞ°Ğ¶Ğ´Ğ°Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºÑƒ Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼ Ğ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ², Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ²ÑĞµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ñ‹ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾ Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸ Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ‚Ğ°Ñ‚ĞµĞ¹. Ğ”Ğ°Ğ¶Ğµ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ LLM Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ Ğ½Ğ¸Ğ¶Ğµ 20%, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¿ĞµÑ€Ğ²ÑƒÑ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑÑ… Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½ Ğ² Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ… Ğ½Ğ°ÑƒĞºĞ¸, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Evaluating AI Agents in Astrophysics Research Replication', 'desc': "ReplicationBench is a framework designed to evaluate AI agents' ability to replicate research papers in astrophysics, focusing on their faithfulness and correctness. It breaks down each paper into specific tasks that require the agents to reproduce key contributions, such as experimental setups and data analyses, in collaboration with the original authors. The framework reveals that even advanced language models struggle with these tasks, scoring below 20%, highlighting various failure modes in their performance. This benchmark not only assesses AI reliability in astrophysics but also offers insights applicable to other scientific fields."}, 'zh': {'title': 'è¯„ä¼° AI ä»£ç†åœ¨ç§‘å­¦ç ”ç©¶ä¸­çš„å¯é æ€§', 'desc': 'ReplicationBench æ˜¯ä¸€ä¸ªè¯„ä¼° AI ä»£ç†åœ¨å¤åˆ¶å¤©ä½“ç‰©ç†å­¦ç ”ç©¶è®ºæ–‡èƒ½åŠ›çš„æ¡†æ¶ã€‚å®ƒé€šè¿‡å°†è®ºæ–‡åˆ†è§£ä¸ºå¤šä¸ªä»»åŠ¡ï¼Œæµ‹è¯•ä»£ç†æ˜¯å¦èƒ½å¤Ÿå‡†ç¡®å¤ç°è®ºæ–‡çš„æ ¸å¿ƒè´¡çŒ®ï¼ŒåŒ…æ‹¬å®éªŒè®¾ç½®ã€æ¨å¯¼ã€æ•°æ®åˆ†æå’Œä»£ç åº“ã€‚è¯¥æ¡†æ¶ä¸åŸè®ºæ–‡ä½œè€…å…±åŒå¼€å‘ï¼Œç¡®ä¿è¯„ä¼°çš„å®¢è§‚æ€§ï¼Œå…³æ³¨ä»£ç†çš„å¿ å®æ€§å’Œæ­£ç¡®æ€§ã€‚å°½ç®¡å½“å‰çš„å‰æ²¿è¯­è¨€æ¨¡å‹åœ¨æ­¤ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ï¼ŒReplicationBench ä»ä¸ºç§‘å­¦ç ”ç©¶ä¸­çš„ AI ä»£ç†æä¾›äº†ä¸€ä¸ªå¯æ‰©å±•çš„å¯é æ€§æµ‹é‡æ¡†æ¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.22876', 'title': 'Batch Speculative Decoding Done Right', 'url': 'https://huggingface.co/papers/2510.22876', 'abstract': 'Batch speculative decoding improves LLM inference throughput by managing ragged tensors to maintain output equivalence and reduce realignment overhead.  \t\t\t\t\tAI-generated summary \t\t\t\t Speculative decoding speeds up LLM inference by using a small draft model to propose multiple tokens that a target model verifies in parallel. Extending this idea to batches is essential for production serving, but it introduces the ragged tensor problem: sequences in the same batch accept different numbers of draft tokens, breaking right-alignment and corrupting position IDs, attention masks, and KV-cache state. We show that several existing batch implementations violate output equivalence-the fundamental requirement that speculative decoding must produce identical token sequences to standard autoregressive generation. These violations occur precisely due to improper handling of the ragged tensor problem. In response, we (1) characterize the synchronization requirements that guarantee correctness, (2) present a correctness-first batch speculative decoding EQSPEC that exposes realignment as consuming 40% of overhead, and (3) introduce EXSPEC, which maintains a sliding pool of sequences and dynamically forms same-length groups, to reduce the realignment overhead while preserving per-sequence speculative speedups. On the SpecBench dataset, across Vicuna-7B/68M, Qwen3-8B/0.6B, and GLM-4-9B/0.6B target/draft pairs, our approach achieves up to 3times throughput improvement at batch size 8 compared to batch size 1, with efficient scaling through batch size 8, while maintaining 95% output equivalence. Our method requires no custom kernels and integrates cleanly with existing inference stacks. Our code is available at https://github.com/eBay/spec_dec.', 'score': 2, 'issue_id': 6687, 'pub_date': '2025-10-26', 'pub_date_card': {'ru': '26 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 26', 'zh': '10æœˆ26æ—¥'}, 'hash': '7d222fe69620ec33', 'authors': ['Ranran Haoran Zhang', 'Soumik Dey', 'Ashirbad Mishra', 'Hansi Wu', 'Binbin Li', 'Rui Zhang'], 'affiliations': ['The Pennsylvania State University', 'eBay Inc'], 'pdf_title_img': 'assets/pdf/title_img/2510.22876.jpg', 'data': {'categories': ['#inference', '#optimization', '#training'], 'emoji': 'ğŸ¯', 'ru': {'title': 'ĞŸÑ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ°Ñ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¾Ğ²Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ inference Ğ² LLM', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ batch speculative decoding Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ inference Ğ² LLM. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ½ĞµÑ€Ğ¾Ğ²Ğ½Ñ‹Ñ… Ñ‚ĞµĞ½Ğ·Ğ¾Ñ€Ğ¾Ğ² (ragged tensors), ĞºĞ¾Ğ³Ğ´Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ±Ğ°Ñ‚Ñ‡Ğµ Ğ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°ÑÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¾Ñ‚ draft-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ°Ñ€ÑƒÑˆĞ°ÑÑ‚ ÑĞºĞ²Ğ¸Ğ²Ğ°Ğ»ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ·-Ğ·Ğ° Ğ½ĞµĞ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ÑÑ‚Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ EXSPEC Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ´Ğ¸Ğ½Ğ°ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ´Ğ»Ğ¸Ğ½Ñ‹, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ´Ğ¾ 3x ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ throughput Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğµ Ğ±Ğ°Ñ‚Ñ‡Ğ° 8 Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ 95% ÑĞºĞ²Ğ¸Ğ²Ğ°Ğ»ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ¾Ğ².'}, 'en': {'title': 'Boosting LLM Inference with Batch Speculative Decoding', 'desc': 'This paper presents a method called Batch Speculative Decoding that enhances the efficiency of large language model (LLM) inference by addressing the challenges posed by ragged tensors. It highlights how speculative decoding can be applied in batches to improve throughput while ensuring that the output remains equivalent to standard autoregressive generation. The authors identify synchronization issues that can lead to output discrepancies and propose two solutions: EQSPEC, which focuses on correctness, and EXSPEC, which optimizes realignment overhead. Their approach demonstrates significant performance improvements, achieving up to three times the throughput at batch size 8 while maintaining a high level of output equivalence.'}, 'zh': {'title': 'æ‰¹é‡æ¨æµ‹è§£ç ï¼šæå‡LLMæ¨ç†æ•ˆç‡çš„å…³é”®', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ‰¹é‡æ¨æµ‹è§£ç çš„æ–¹æ³•ï¼Œä»¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†ååé‡ã€‚é€šè¿‡ç®¡ç†ä¸è§„åˆ™å¼ é‡ï¼Œä¿æŒè¾“å‡ºç­‰ä»·æ€§å¹¶å‡å°‘é‡æ–°å¯¹é½çš„å¼€é”€ï¼Œè§£å†³äº†æ‰¹é‡å¤„ç†ä¸­çš„ä¸è§„åˆ™å¼ é‡é—®é¢˜ã€‚æˆ‘ä»¬å±•ç¤ºäº†ç°æœ‰æ‰¹é‡å®ç°å¦‚ä½•å› å¤„ç†ä¸å½“è€Œè¿åè¾“å‡ºç­‰ä»·æ€§ï¼Œå¹¶æå‡ºäº†ä¸¤ç§æ–°æ–¹æ³•æ¥ä¿è¯æ­£ç¡®æ€§å’Œå‡å°‘å¼€é”€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨æ‰¹é‡å¤§å°ä¸º8æ—¶ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ååé‡ä¸Šæé«˜äº†æœ€å¤š3å€ï¼ŒåŒæ—¶ä¿æŒäº†95%çš„è¾“å‡ºç­‰ä»·æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.22795', 'title': 'SAO-Instruct: Free-form Audio Editing using Natural Language\n  Instructions', 'url': 'https://huggingface.co/papers/2510.22795', 'abstract': 'SAO-Instruct, a generative model based on Stable Audio Open, allows flexible audio editing using natural language instructions, outperforming existing methods in both objective and subjective evaluations.  \t\t\t\t\tAI-generated summary \t\t\t\t Generative models have made significant progress in synthesizing high-fidelity audio from short textual descriptions. However, editing existing audio using natural language has remained largely underexplored. Current approaches either require the complete description of the edited audio or are constrained to predefined edit instructions that lack flexibility. In this work, we introduce SAO-Instruct, a model based on Stable Audio Open capable of editing audio clips using any free-form natural language instruction. To train our model, we create a dataset of audio editing triplets (input audio, edit instruction, output audio) using Prompt-to-Prompt, DDPM inversion, and a manual editing pipeline. Although partially trained on synthetic data, our model generalizes well to real in-the-wild audio clips and unseen edit instructions. We demonstrate that SAO-Instruct achieves competitive performance on objective metrics and outperforms other audio editing approaches in a subjective listening study. To encourage future research, we release our code and model weights.', 'score': 2, 'issue_id': 6681, 'pub_date': '2025-10-26', 'pub_date_card': {'ru': '26 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 26', 'zh': '10æœˆ26æ—¥'}, 'hash': '2dd95fced620d404', 'authors': ['Michael UngersbÃ¶ck', 'Florian GrÃ¶tschla', 'Luca A. LanzendÃ¶rfer', 'June Young Yi', 'Changho Choi', 'Roger Wattenhofer'], 'affiliations': ['ETH Zurich', 'Korea University', 'Seoul National University'], 'pdf_title_img': 'assets/pdf/title_img/2510.22795.jpg', 'data': {'categories': ['#dataset', '#open_source', '#audio', '#synthetic'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ°ÑƒĞ´Ğ¸Ğ¾ ÑĞ²Ğ¾Ğ±Ğ¾Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ°Ğ¼Ğ¸', 'desc': 'SAO-Instruct â€” ÑÑ‚Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Stable Audio Open, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¸Ğ· Ñ‚Ñ€Ğ¸Ğ¿Ğ»ĞµÑ‚Ğ¾Ğ² (Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğµ Ğ°ÑƒĞ´Ğ¸Ğ¾, Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚) Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Prompt-to-Prompt Ğ¸ DDPM Ğ¸Ğ½version. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ…Ğ¾Ñ€Ğ¾ÑˆÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°ÑƒĞ´Ğ¸Ğ¾ĞºĞ»Ğ¸Ğ¿Ğ°Ñ…, Ğ½ĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞŸĞ¾ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°Ğ¼ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº Ğ¸ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑĞ»ÑƒÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ñ SAO-Instruct Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾.'}, 'en': {'title': 'Revolutionizing Audio Editing with Natural Language Instructions', 'desc': 'SAO-Instruct is a generative model designed for flexible audio editing using natural language instructions. Unlike previous methods that require detailed descriptions or fixed editing commands, this model allows users to provide any free-form instruction for audio modification. It is trained on a dataset of audio editing triplets, which helps it learn to generate high-quality edited audio from various inputs. The model not only performs well on objective metrics but also excels in subjective evaluations, making it a significant advancement in audio editing technology.'}, 'zh': {'title': 'çµæ´»éŸ³é¢‘ç¼–è¾‘ï¼Œå°½åœ¨SAO-Instruct', 'desc': 'SAO-Instructæ˜¯ä¸€ç§åŸºäºStable Audio Opençš„ç”Ÿæˆæ¨¡å‹ï¼Œèƒ½å¤Ÿçµæ´»åœ°ä½¿ç”¨è‡ªç„¶è¯­è¨€æŒ‡ä»¤è¿›è¡ŒéŸ³é¢‘ç¼–è¾‘ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œå®ƒåœ¨å®¢è§‚å’Œä¸»è§‚è¯„ä¼°ä¸­è¡¨ç°æ›´ä½³ã€‚è¯¥æ¨¡å‹é€šè¿‡åˆ›å»ºéŸ³é¢‘ç¼–è¾‘ä¸‰å…ƒç»„çš„æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œèƒ½å¤Ÿå¤„ç†ä»»æ„è‡ªç”±å½¢å¼çš„ç¼–è¾‘æŒ‡ä»¤ã€‚å°½ç®¡éƒ¨åˆ†è®­ç»ƒä½¿ç”¨äº†åˆæˆæ•°æ®ï¼Œä½†æ¨¡å‹åœ¨çœŸå®éŸ³é¢‘ç‰‡æ®µå’Œæœªè§è¿‡çš„ç¼–è¾‘æŒ‡ä»¤ä¸Šä¹Ÿèƒ½å¾ˆå¥½åœ°æ³›åŒ–ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.22099', 'title': 'Generalization or Memorization: Dynamic Decoding for Mode Steering', 'url': 'https://huggingface.co/papers/2510.22099', 'abstract': "A framework using the Information Bottleneck principle and Dynamic Mode Steering algorithm improves the reliability of Large Language Models by balancing generalization and memorization.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) exhibit a troubling duality, capable of both remarkable generalization and brittle, verbatim memorization of their training data. This unpredictability undermines their reliability in high-stakes applications. In this work, we propose a unified framework to understand, identify, and control these distinct reasoning modes. First, we introduce a theoretical model based on the Information Bottleneck (IB) principle, formalizing generalization as the learning of a compressed, task-relevant representation and memorization as a failure to compress. Building on this theory, we develop Dynamic Mode Steering (DMS), a novel inference-time algorithm which comprises two components: (1) a lightweight, causally-grounded linear probe that identifies the model's instantaneous reliance on memorization, and (2) a dynamic activation steering mechanism that nudges the model's computation towards pre-identified generalization circuits. We frame DMS as a form of adaptive, self-contrastive decoding. Experiments on reasoning and faithfulness tasks demonstrate that DMS significantly improves logical consistency and factual accuracy, thereby offering a principled approach to enhancing LLM reliability.", 'score': 2, 'issue_id': 6667, 'pub_date': '2025-10-25', 'pub_date_card': {'ru': '25 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 25', 'zh': '10æœˆ25æ—¥'}, 'hash': '7a4136db7043277f', 'authors': ['Xuanming Zhang'], 'affiliations': ['Department of Computer Science, University of Wisconsin-Madison, Madison, USA', 'Stanford University, Palo Alto, USA'], 'pdf_title_img': 'assets/pdf/title_img/2510.22099.jpg', 'data': {'categories': ['#reasoning', '#interpretability', '#inference', '#training'], 'emoji': 'ğŸ§­', 'ru': {'title': 'Ğ£Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ°Ğ¼Ğ¸ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ: ĞºĞ°Ğº Ğ½Ğ°ÑƒÑ‡Ğ¸Ñ‚ÑŒ LLM Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°Ñ‚ÑŒ, Ğ° Ğ½Ğµ Ğ·Ğ°ÑƒÑ‡Ğ¸Ğ²Ğ°Ñ‚ÑŒ', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğµ, ĞºĞ¾Ğ³Ğ´Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‚Ğ¾ Ğ±Ğ»ĞµÑÑ‚ÑÑ‰Ğµ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ, Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´ÑÑ‚ Ğ·Ğ°ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ñ„Ñ€Ğ°Ğ·Ñ‹ Ğ¸Ğ· Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ° Information Bottleneck Ğ¾Ğ½Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ³Ğ´Ğµ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°ĞµÑ‚ÑÑ ĞºĞ°Ğº ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ¾ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ğ° Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ğµ â€” ĞºĞ°Ğº Ğ½ĞµÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ñ‚Ğ°ĞºĞ¾Ğ¼Ñƒ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Dynamic Mode Steering Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğ¹ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ğ¹ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚Ğ°, ĞºĞ¾Ğ³Ğ´Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ·Ğ°ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€ÑƒĞµÑ‚ ĞµÑ‘ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ Ğ² ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñƒ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰Ğ¸Ñ… Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² LLM.'}, 'en': {'title': 'Balancing Generalization and Memorization for Reliable LLMs', 'desc': "This paper presents a framework that enhances the reliability of Large Language Models (LLMs) by addressing their tendency to both generalize and memorize training data. It introduces the Information Bottleneck principle to differentiate between effective generalization, which compresses information, and problematic memorization, which retains too much detail. The Dynamic Mode Steering algorithm is developed to dynamically adjust the model's focus during inference, promoting generalization while minimizing reliance on memorization. Experimental results show that this approach improves logical consistency and factual accuracy in LLM outputs, making them more reliable for critical applications."}, 'zh': {'title': 'æå‡å¤§å‹è¯­è¨€æ¨¡å‹å¯é æ€§çš„åˆ›æ–°æ¡†æ¶', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œåˆ©ç”¨ä¿¡æ¯ç“¶é¢ˆåŸç†å’ŒåŠ¨æ€æ¨¡å¼å¼•å¯¼ç®—æ³•æ¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„å¯é æ€§ã€‚è¯¥æ¡†æ¶æ—¨åœ¨å¹³è¡¡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œè®°å¿†èƒ½åŠ›ï¼Œè§£å†³æ¨¡å‹åœ¨é«˜é£é™©åº”ç”¨ä¸­çš„ä¸å¯é æ€§é—®é¢˜ã€‚é€šè¿‡ç†è®ºæ¨¡å‹ï¼Œæ³›åŒ–è¢«å®šä¹‰ä¸ºå­¦ä¹ å‹ç¼©çš„ã€ä¸ä»»åŠ¡ç›¸å…³çš„è¡¨ç¤ºï¼Œè€Œè®°å¿†åˆ™è¢«è§†ä¸ºæœªèƒ½è¿›è¡Œå‹ç¼©ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒåŠ¨æ€æ¨¡å¼å¼•å¯¼ç®—æ³•æ˜¾è‘—æé«˜äº†æ¨¡å‹çš„é€»è¾‘ä¸€è‡´æ€§å’Œäº‹å®å‡†ç¡®æ€§ï¼Œæä¾›äº†ä¸€ç§å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹å¯é æ€§çš„åŸåˆ™æ€§æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.23667', 'title': 'Optimize Any Topology: A Foundation Model for Shape- and Resolution-Free\n  Structural Topology Optimization', 'url': 'https://huggingface.co/papers/2510.23667', 'abstract': 'OAT, a deep-learning framework combining autoencoder, neural-field decoder, and latent-diffusion model, achieves fast and general topology optimization with high performance across various conditions and resolutions.  \t\t\t\t\tAI-generated summary \t\t\t\t Structural topology optimization (TO) is central to engineering design but remains computationally intensive due to complex physics and hard constraints. Existing deep-learning methods are limited to fixed square grids, a few hand-coded boundary conditions, and post-hoc optimization, preventing general deployment. We introduce Optimize Any Topology (OAT), a foundation-model framework that directly predicts minimum-compliance layouts for arbitrary aspect ratios, resolutions, volume fractions, loads, and fixtures. OAT combines a resolution- and shape-agnostic autoencoder with an implicit neural-field decoder and a conditional latent-diffusion model trained on OpenTO, a new corpus of 2.2 million optimized structures covering 2 million unique boundary-condition configurations. On four public benchmarks and two challenging unseen tests, OAT lowers mean compliance up to 90% relative to the best prior models and delivers sub-1 second inference on a single GPU across resolutions from 64 x 64 to 256 x 256 and aspect ratios as high as 10:1. These results establish OAT as a general, fast, and resolution-free framework for physics-aware topology optimization and provide a large-scale dataset to spur further research in generative modeling for inverse design. Code & data can be found at https://github.com/ahnobari/OptimizeAnyTopology.', 'score': 1, 'issue_id': 6678, 'pub_date': '2025-10-26', 'pub_date_card': {'ru': '26 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 26', 'zh': '10æœˆ26æ—¥'}, 'hash': '7fce06878cedad5b', 'authors': ['Amin Heyrani Nobari', 'Lyle Regenwetter', 'Cyril Picard', 'Ligong Han', 'Faez Ahmed'], 'affiliations': ['Massachusetts Institute of Technology, Cambridge, MA, 02139', 'Red Hat AI, MIT-IBM Watson AI Lab, Cambridge, MA, 02139'], 'pdf_title_img': 'assets/pdf/title_img/2510.23667.jpg', 'data': {'categories': ['#inference', '#benchmark', '#dataset', '#diffusion', '#optimization', '#open_source', '#architecture'], 'emoji': 'ğŸ—ï¸', 'ru': {'title': 'OAT: ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ AI-ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¼Ğ³Ğ½Ğ¾Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ»ÑĞ±Ñ‹Ñ… ĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° OAT â€” framework Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ deep learning Ğ´Ğ»Ñ Ñ‚Ğ¾Ğ¿Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ autoencoder, neural-field Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€ Ğ¸ latent-diffusion Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ OpenTO Ğ¸Ğ· 2.2 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€ Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸ÑĞ¼Ğ¸, ÑĞ¾Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸ÑĞ¼Ğ¸ ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½ Ğ¸ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑĞ¼Ğ¸. OAT ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ ÑÑ€ĞµĞ´Ğ½ÑÑ Ğ¿Ğ¾Ğ´Ğ°Ñ‚Ğ»Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ ĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ´Ğ¾ 90% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ inference Ğ¼ĞµĞ½ĞµĞµ Ñ‡ĞµĞ¼ Ğ·Ğ° ÑĞµĞºÑƒĞ½Ğ´Ñƒ Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¼ GPU. Ğ­Ñ‚Ğ¾ Ğ¿ĞµÑ€Ğ²Ğ°Ñ foundation model Ğ´Ğ»Ñ Ñ‚Ğ¾Ğ¿Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ñ Ğ»ÑĞ±Ñ‹Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼Ğ¸ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'Revolutionizing Topology Optimization with OAT', 'desc': 'The paper presents Optimize Any Topology (OAT), a novel deep-learning framework designed for efficient structural topology optimization. OAT integrates an autoencoder, a neural-field decoder, and a latent-diffusion model to predict optimal layouts for various engineering constraints and configurations. Unlike previous methods that are limited to fixed grids and specific conditions, OAT can handle arbitrary aspect ratios and resolutions, significantly improving performance. The framework demonstrates a remarkable reduction in compliance and fast inference times, making it a versatile tool for physics-aware design optimization.'}, 'zh': {'title': 'ä¼˜åŒ–ä»»æ„æ‹“æ‰‘çš„å¿«é€Ÿè§£å†³æ–¹æ¡ˆ', 'desc': 'OATæ˜¯ä¸€ç§æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œç»“åˆäº†è‡ªç¼–ç å™¨ã€ç¥ç»åœºè§£ç å™¨å’Œæ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼Œèƒ½å¤Ÿå¿«é€Ÿä¸”é«˜æ•ˆåœ°è¿›è¡Œæ‹“æ‰‘ä¼˜åŒ–ã€‚å®ƒè§£å†³äº†ä¼ ç»Ÿæ–¹æ³•åœ¨å›ºå®šç½‘æ ¼å’Œè¾¹ç•Œæ¡ä»¶ä¸Šçš„å±€é™æ€§ï¼Œæ”¯æŒä»»æ„æ¯”ä¾‹ã€åˆ†è¾¨ç‡å’Œè´Ÿè½½çš„å¸ƒå±€é¢„æµ‹ã€‚OATåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œç›¸è¾ƒäºä¹‹å‰çš„æœ€ä½³æ¨¡å‹ï¼Œå¹³å‡åˆè§„æ€§é™ä½äº†90%ã€‚è¯¥æ¡†æ¶ä¸ºç‰©ç†æ„ŸçŸ¥çš„æ‹“æ‰‘ä¼˜åŒ–æä¾›äº†ä¸€ä¸ªé€šç”¨ã€å¿«é€Ÿä¸”æ— åˆ†è¾¨ç‡é™åˆ¶çš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶ä¸ºé€†å‘è®¾è®¡çš„ç”Ÿæˆå»ºæ¨¡ç ”ç©¶æä¾›äº†å¤§è§„æ¨¡æ•°æ®é›†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.22590', 'title': 'ATOM: AdapTive and OptiMized dynamic temporal knowledge graph\n  construction using LLMs', 'url': 'https://huggingface.co/papers/2510.22590', 'abstract': 'ATOM is a few-shot, scalable approach for constructing and updating Temporal Knowledge Graphs from unstructured text, improving exhaustivity, stability, and latency.  \t\t\t\t\tAI-generated summary \t\t\t\t In today\'s rapidly expanding data landscape, knowledge extraction from unstructured text is vital for real-time analytics, temporal inference, and dynamic memory frameworks. However, traditional static knowledge graph (KG) construction often overlooks the dynamic and time-sensitive nature of real-world data, limiting adaptability to continuous changes. Moreover, recent zero- or few-shot approaches that avoid domain-specific fine-tuning or reliance on prebuilt ontologies often suffer from instability across multiple runs, as well as incomplete coverage of key facts. To address these challenges, we introduce ATOM (AdapTive and OptiMized), a few-shot and scalable approach that builds and continuously updates Temporal Knowledge Graphs (TKGs) from unstructured texts. ATOM splits input documents into minimal, self-contained "atomic" facts, improving extraction exhaustivity and stability. Then, it constructs atomic TKGs from these facts while employing a dual-time modeling that distinguishes when information is observed from when it is valid. The resulting atomic TKGs are subsequently merged in parallel. Empirical evaluations demonstrate that ATOM achieves ~18% higher exhaustivity, ~17% better stability, and over 90% latency reduction compared to baseline methods, demonstrating a strong scalability potential for dynamic TKG construction.', 'score': 1, 'issue_id': 6687, 'pub_date': '2025-10-26', 'pub_date_card': {'ru': '26 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 26', 'zh': '10æœˆ26æ—¥'}, 'hash': '2f15e1bddecd9786', 'authors': ['Yassir Lairgi', 'Ludovic Moncla', 'Khalid Benabdeslem', 'RÃ©my Cazabet', 'Pierre ClÃ©au'], 'affiliations': ['GAUC, Lyon, France', 'LIRIS, INSA Lyon, UniversitÃ© Claude Bernard Lyon 1, France'], 'pdf_title_img': 'assets/pdf/title_img/2510.22590.jpg', 'data': {'categories': ['#dataset', '#games', '#data', '#transfer_learning', '#optimization', '#multimodal'], 'emoji': 'âš›ï¸', 'ru': {'title': 'ATOM: ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ² Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ATOM Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ² Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸Ğ· Ğ½ĞµÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ° Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ few-shot Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ€Ğ°Ğ·Ğ±Ğ¸ĞµĞ½Ğ¸Ğ¸ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ°Ñ‚Ğ¾Ğ¼Ğ°Ñ€Ğ½Ñ‹Ğµ Ñ„Ğ°ĞºÑ‚Ñ‹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ‚Ñƒ Ğ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ¾Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ°ÑÑ‰ĞµĞµ Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¿ĞµÑ€Ğ¸Ğ¾Ğ´ Ğ¸Ñ… Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ°Ñ‚Ğ¾Ğ¼Ğ°Ñ€Ğ½Ñ‹Ğµ Ğ³Ñ€Ğ°Ñ„Ñ‹ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ‚Ñ‹ Ğ½Ğ° 18%, ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 17% Ğ¸ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ Ğ½Ğ° 90% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'ATOM: Revolutionizing Temporal Knowledge Graphs with Few-Shot Learning', 'desc': "ATOM is a novel approach designed to create and update Temporal Knowledge Graphs (TKGs) from unstructured text using few-shot learning techniques. It addresses the limitations of traditional static knowledge graph construction by focusing on the dynamic nature of real-world data, allowing for continuous updates. By breaking down documents into minimal 'atomic' facts, ATOM enhances the exhaustivity and stability of knowledge extraction. The method employs dual-time modeling to differentiate between the observation and validity of information, resulting in significant improvements in performance metrics such as exhaustivity, stability, and latency."}, 'zh': {'title': 'ATOMï¼šåŠ¨æ€çŸ¥è¯†å›¾è°±æ„å»ºçš„æ–°æ–¹æ³•', 'desc': 'ATOMæ˜¯ä¸€ç§å°‘é‡æ ·æœ¬ã€å¯æ‰©å±•çš„æ–¹æ³•ï¼Œç”¨äºä»éç»“æ„åŒ–æ–‡æœ¬ä¸­æ„å»ºå’Œæ›´æ–°æ—¶é—´çŸ¥è¯†å›¾è°±ã€‚å®ƒé€šè¿‡å°†è¾“å…¥æ–‡æ¡£æ‹†åˆ†ä¸ºæœ€å°çš„ã€è‡ªåŒ…å«çš„â€œåŸå­â€äº‹å®ï¼Œæ¥æé«˜çŸ¥è¯†æå–çš„å…¨é¢æ€§å’Œç¨³å®šæ€§ã€‚ATOMé‡‡ç”¨åŒæ—¶é—´å»ºæ¨¡ï¼ŒåŒºåˆ†ä¿¡æ¯è¢«è§‚å¯Ÿçš„æ—¶é—´å’Œä¿¡æ¯æœ‰æ•ˆçš„æ—¶é—´ï¼Œä»è€Œæ„å»ºåŸå­æ—¶é—´çŸ¥è¯†å›¾è°±ã€‚å®éªŒè¯æ˜ï¼ŒATOMåœ¨å…¨é¢æ€§ã€ç¨³å®šæ€§å’Œå»¶è¿Ÿæ–¹é¢å‡æ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶åœ¨åŠ¨æ€çŸ¥è¯†å›¾è°±æ„å»ºä¸­çš„å¼ºå¤§æ‰©å±•æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.22373', 'title': 'VisJudge-Bench: Aesthetics and Quality Assessment of Visualizations', 'url': 'https://huggingface.co/papers/2510.22373', 'abstract': "VisJudge-Bench is a benchmark for evaluating MLLMs' performance in assessing visualization aesthetics and quality, revealing gaps compared to human experts and demonstrating improvements with the VisJudge model.  \t\t\t\t\tAI-generated summary \t\t\t\t Visualization, a domain-specific yet widely used form of imagery, is an effective way to turn complex datasets into intuitive insights, and its value depends on whether data are faithfully represented, clearly communicated, and aesthetically designed. However, evaluating visualization quality is challenging: unlike natural images, it requires simultaneous judgment across data encoding accuracy, information expressiveness, and visual aesthetics. Although multimodal large language models (MLLMs) have shown promising performance in aesthetic assessment of natural images, no systematic benchmark exists for measuring their capabilities in evaluating visualizations. To address this, we propose VisJudge-Bench, the first comprehensive benchmark for evaluating MLLMs' performance in assessing visualization aesthetics and quality. It contains 3,090 expert-annotated samples from real-world scenarios, covering single visualizations, multiple visualizations, and dashboards across 32 chart types. Systematic testing on this benchmark reveals that even the most advanced MLLMs (such as GPT-5) still exhibit significant gaps compared to human experts in judgment, with a Mean Absolute Error (MAE) of 0.551 and a correlation with human ratings of only 0.429. To address this issue, we propose VisJudge, a model specifically designed for visualization aesthetics and quality assessment. Experimental results demonstrate that VisJudge significantly narrows the gap with human judgment, reducing the MAE to 0.442 (a 19.8% reduction) and increasing the consistency with human experts to 0.681 (a 58.7% improvement) compared to GPT-5. The benchmark is available at https://github.com/HKUSTDial/VisJudgeBench.", 'score': 1, 'issue_id': 6674, 'pub_date': '2025-10-25', 'pub_date_card': {'ru': '25 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 25', 'zh': '10æœˆ25æ—¥'}, 'hash': 'e827dfbea1866daf', 'authors': ['Yupeng Xie', 'Zhiyang Zhang', 'Yifan Wu', 'Sirong Lu', 'Jiayi Zhang', 'Zhaoyang Yu', 'Jinlin Wang', 'Sirui Hong', 'Bang Liu', 'Chenglin Wu', 'Yuyu Luo'], 'affiliations': ['DeepWisdom', 'The Hong Kong University of Science and Technology (Guangzhou)', 'UniversitÃ© de MontrÃ©al & Mila'], 'pdf_title_img': 'assets/pdf/title_img/2510.22373.jpg', 'data': {'categories': ['#benchmark', '#interpretability', '#multimodal', '#optimization'], 'emoji': 'ğŸ“Š', 'ru': {'title': 'VisJudge: ÑƒÑ‡Ğ¸Ğ¼ AI Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ ĞºÑ€Ğ°ÑĞ¾Ñ‚Ñƒ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ VisJudge-Bench â€” Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ multimodal LLM Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ ÑÑÑ‚ĞµÑ‚Ğ¸ĞºÑƒ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Benchmark ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 3090 Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ² Ñ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… 32 Ñ‚Ğ¸Ğ¿Ğ° Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¾Ğ² Ğ¸Ğ· Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ². Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‚Ğ¸Ğ¿Ğ° GPT-5 Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒÑÑ‚ÑƒĞ¿Ğ°ÑÑ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ğ¼ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¹. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ VisJudge ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¾Ğ¹ Ğ½Ğ° 19.8% Ğ¿Ğ¾ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞµ MAE Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ñ Ñ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ğ¼Ğ¸ Ğ½Ğ° 58.7%.'}, 'en': {'title': 'Bridging the Gap in Visualization Quality Assessment with VisJudge-Bench', 'desc': 'VisJudge-Bench is a new benchmark designed to evaluate how well multimodal large language models (MLLMs) can assess the aesthetics and quality of visualizations. It includes over 3,000 expert-annotated examples that cover various types of visualizations, making it a comprehensive tool for testing. The findings show that even advanced models like GPT-5 struggle to match human expert evaluations, with notable gaps in accuracy and correlation. To improve this, the VisJudge model was introduced, which significantly enhances performance in aesthetic assessments, demonstrating a marked reduction in error rates compared to existing MLLMs.'}, 'zh': {'title': 'å¯è§†åŒ–è¯„ä¼°çš„æ–°åŸºå‡†ä¸æ¨¡å‹', 'desc': 'VisJudge-Benchæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¯è§†åŒ–ç¾å­¦å’Œè´¨é‡è¯„ä¼°æ–¹é¢è¡¨ç°çš„åŸºå‡†ã€‚è¯¥åŸºå‡†åŒ…å«3090ä¸ªæ¥è‡ªçœŸå®åœºæ™¯çš„ä¸“å®¶æ ‡æ³¨æ ·æœ¬ï¼Œæ¶µç›–32ç§å›¾è¡¨ç±»å‹çš„å•ä¸ªå¯è§†åŒ–ã€å¤šé‡å¯è§†åŒ–å’Œä»ªè¡¨æ¿ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„MLLMï¼ˆå¦‚GPT-5ï¼‰ï¼Œåœ¨åˆ¤æ–­ä¸Šä¸äººç±»ä¸“å®¶ç›¸æ¯”ä»å­˜åœ¨æ˜¾è‘—å·®è·ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†VisJudgeæ¨¡å‹ï¼Œä¸“é—¨ç”¨äºå¯è§†åŒ–ç¾å­¦å’Œè´¨é‡è¯„ä¼°ï¼Œå®éªŒç»“æœæ˜¾ç¤ºå…¶åœ¨ä¸äººç±»åˆ¤æ–­çš„ä¸€è‡´æ€§ä¸Šæœ‰æ˜¾è‘—æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.22319', 'title': 'GRPO-Guard: Mitigating Implicit Over-Optimization in Flow Matching via\n  Regulated Clipping', 'url': 'https://huggingface.co/papers/2510.22319', 'abstract': 'GRPO-Guard enhances GRPO-based reinforcement learning by normalizing importance ratios and reweighting gradients, mitigating over-optimization in flow-matching models without heavy KL regularization.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, GRPO-based reinforcement learning has shown remarkable progress in optimizing flow-matching models, effectively improving their alignment with task-specific rewards. Within these frameworks, the policy update relies on importance-ratio clipping to constrain overconfident positive and negative gradients. However, in practice, we observe a systematic shift in the importance-ratio distribution-its mean falls below 1 and its variance differs substantially across timesteps. This left-shifted and inconsistent distribution prevents positive-advantage samples from entering the clipped region, causing the mechanism to fail in constraining overconfident positive updates. As a result, the policy model inevitably enters an implicit over-optimization stage-while the proxy reward continues to increase, essential metrics such as image quality and text-prompt alignment deteriorate sharply, ultimately making the learned policy impractical for real-world use. To address this issue, we introduce GRPO-Guard, a simple yet effective enhancement to existing GRPO frameworks. Our method incorporates ratio normalization, which restores a balanced and step-consistent importance ratio, ensuring that PPO clipping properly constrains harmful updates across denoising timesteps. In addition, a gradient reweighting strategy equalizes policy gradients over noise conditions, preventing excessive updates from particular timestep regions. Together, these designs act as a regulated clipping mechanism, stabilizing optimization and substantially mitigating implicit over-optimization without relying on heavy KL regularization. Extensive experiments on multiple diffusion backbones (e.g., SD3.5M, Flux.1-dev) and diverse proxy tasks demonstrate that GRPO-Guard significantly reduces over-optimization while maintaining or even improving generation quality.', 'score': 1, 'issue_id': 6681, 'pub_date': '2025-10-25', 'pub_date_card': {'ru': '25 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 25', 'zh': '10æœˆ25æ—¥'}, 'hash': '4224bc436e599f94', 'authors': ['Jing Wang', 'Jiajun Liang', 'Jie Liu', 'Henglin Liu', 'Gongye Liu', 'Jun Zheng', 'Wanyuan Pang', 'Ao Ma', 'Zhenyu Xie', 'Xintao Wang', 'Meng Wang', 'Pengfei Wan', 'Xiaodan Liang'], 'affiliations': ['CUHK MMLab', 'HKUST', 'Kuaishou Technology', 'Shenzhen Campus of Sun Yat-Sen University', 'Tsinghua University', 'UCAS', 'USTB'], 'pdf_title_img': 'assets/pdf/title_img/2510.22319.jpg', 'data': {'categories': ['#optimization', '#rlhf', '#training', '#rl'], 'emoji': 'ğŸ›¡ï¸', 'ru': {'title': 'Ğ—Ğ°Ñ‰Ğ¸Ñ‚Ğ° Ğ¾Ñ‚ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² reinforcement learning Ğ´Ğ»Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ GRPO-Guard â€” ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ flow-matching Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¹ GRPO ÑÑ‚Ñ€Ğ°Ğ´Ğ°ĞµÑ‚ Ğ¾Ñ‚ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ importance ratios, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ½ĞµÑĞ²Ğ½Ğ¾Ğ¹ Ğ¿ĞµÑ€ĞµĞ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸: proxy reward Ñ€Ğ°ÑÑ‚Ñ‘Ñ‚, Ğ½Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ°Ğ¼ ÑƒÑ…ÑƒĞ´ÑˆĞ°ÑÑ‚ÑÑ. GRPO-Guard Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ñ‡ĞµÑ€ĞµĞ· Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ĞºĞ¾ÑÑ„Ñ„Ğ¸Ñ†Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿ĞµÑ€ĞµĞ²Ğ·Ğ²ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ±ĞµĞ· Ñ‚ÑĞ¶Ñ‘Ğ»Ğ¾Ğ¹ KL-Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° SD3.5M Ğ¸ Flux.1-dev Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¿ĞµÑ€ĞµĞ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'Stabilizing Reinforcement Learning with GRPO-Guard', 'desc': 'The paper introduces GRPO-Guard, an enhancement to GRPO-based reinforcement learning that addresses the issue of over-optimization in flow-matching models. It normalizes importance ratios and reweights gradients to ensure that policy updates are stable and effective, preventing the model from becoming overly confident in its predictions. By correcting the distribution of importance ratios, GRPO-Guard allows for better clipping of gradients, which helps maintain the quality of generated outputs. Experimental results show that this method improves performance across various tasks without the need for heavy KL regularization.'}, 'zh': {'title': 'GRPO-Guardï¼šç¨³å®šä¼˜åŒ–ï¼Œå‡è½»è¿‡åº¦ä¼˜åŒ–çš„åˆ©å™¨', 'desc': 'GRPO-Guard æ˜¯ä¸€ç§å¢å¼º GRPO åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•ï¼Œé€šè¿‡å½’ä¸€åŒ–é‡è¦æ€§æ¯”ç‡å’Œé‡æ–°åŠ æƒæ¢¯åº¦ï¼Œå‡è½»æµåŒ¹é…æ¨¡å‹ä¸­çš„è¿‡åº¦ä¼˜åŒ–é—®é¢˜ï¼Œè€Œæ— éœ€é‡åº¦çš„ KL æ­£åˆ™åŒ–ã€‚åœ¨ GRPO æ¡†æ¶ä¸­ï¼Œç­–ç•¥æ›´æ–°ä¾èµ–äºé‡è¦æ€§æ¯”ç‡çš„è£å‰ªï¼Œä»¥é™åˆ¶è¿‡äºè‡ªä¿¡çš„æ­£è´Ÿæ¢¯åº¦ã€‚ç„¶è€Œï¼Œå®é™…ä¸­æˆ‘ä»¬è§‚å¯Ÿåˆ°é‡è¦æ€§æ¯”ç‡åˆ†å¸ƒçš„ç³»ç»Ÿæ€§åç§»ï¼Œå¯¼è‡´æ­£ä¼˜åŠ¿æ ·æœ¬æ— æ³•è¿›å…¥è£å‰ªåŒºåŸŸï¼Œä»è€Œä½¿æœºåˆ¶æ— æ³•æœ‰æ•ˆçº¦æŸè¿‡åº¦è‡ªä¿¡çš„æ›´æ–°ã€‚GRPO-Guard é€šè¿‡å¼•å…¥æ¯”ç‡å½’ä¸€åŒ–å’Œæ¢¯åº¦é‡æ–°åŠ æƒç­–ç•¥ï¼Œç¨³å®šäº†ä¼˜åŒ–è¿‡ç¨‹ï¼Œæ˜¾è‘—å‡å°‘äº†éšæ€§è¿‡åº¦ä¼˜åŒ–ï¼ŒåŒæ—¶ä¿æŒæˆ–æé«˜äº†ç”Ÿæˆè´¨é‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.21323', 'title': 'VL-SAE: Interpreting and Enhancing Vision-Language Alignment with a\n  Unified Concept Set', 'url': 'https://huggingface.co/papers/2510.21323', 'abstract': 'VL-SAE, a sparse autoencoder, enhances vision-language alignment by correlating neurons to unified concepts, improving interpretability and performance in tasks like zero-shot image classification and hallucination elimination.  \t\t\t\t\tAI-generated summary \t\t\t\t The alignment of vision-language representations endows current Vision-Language Models (VLMs) with strong multi-modal reasoning capabilities. However, the interpretability of the alignment component remains uninvestigated due to the difficulty in mapping the semantics of multi-modal representations into a unified concept set. To address this problem, we propose VL-SAE, a sparse autoencoder that encodes vision-language representations into its hidden activations. Each neuron in its hidden layer correlates to a concept represented by semantically similar images and texts, thereby interpreting these representations with a unified concept set. To establish the neuron-concept correlation, we encourage semantically similar representations to exhibit consistent neuron activations during self-supervised training. First, to measure the semantic similarity of multi-modal representations, we perform their alignment in an explicit form based on cosine similarity. Second, we construct the VL-SAE with a distance-based encoder and two modality-specific decoders to ensure the activation consistency of semantically similar representations. Experiments across multiple VLMs (e.g., CLIP, LLaVA) demonstrate the superior capability of VL-SAE in interpreting and enhancing the vision-language alignment. For interpretation, the alignment between vision and language representations can be understood by comparing their semantics with concepts. For enhancement, the alignment can be strengthened by aligning vision-language representations at the concept level, contributing to performance improvements in downstream tasks, including zero-shot image classification and hallucination elimination. Codes are available at https://github.com/ssfgunner/VL-SAE.', 'score': 1, 'issue_id': 6667, 'pub_date': '2025-10-24', 'pub_date_card': {'ru': '24 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 24', 'zh': '10æœˆ24æ—¥'}, 'hash': '41279ff25d4d0ecb', 'authors': ['Shufan Shen', 'Junshu Sun', 'Qingming Huang', 'Shuhui Wang'], 'affiliations': ['Key Lab of Intell. Info. Process., Inst. of Comput. Tech., CAS', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2510.21323.jpg', 'data': {'categories': ['#alignment', '#multimodal', '#interpretability', '#cv', '#hallucinations', '#training'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ¸Ñ ÑĞ²ÑĞ·Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ·Ñ€ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ¼', 'desc': 'VL-SAE â€” ÑÑ‚Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² VLM Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¿ÑƒÑ‚Ñ‘Ğ¼ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ¾Ğ² Ñ ĞµĞ´Ğ¸Ğ½Ñ‹Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ¼ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ‚Ğ¾Ğ². ĞšĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½ ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ ÑĞ»Ğ¾Ñ ĞºĞ¾Ñ€Ñ€ĞµĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ñ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ‚Ğ¾Ğ¼, Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¼ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸Ğ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°Ğ¼Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ñ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ¾Ğ² Ğ´Ğ»Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ±Ğ»Ğ¸Ğ·ĞºĞ¸Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… zero-shot ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑƒÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ‚Ğ¸Ğ¿Ğ° CLIP Ğ¸ LLaVA.'}, 'en': {'title': 'Enhancing Vision-Language Alignment with VL-SAE', 'desc': 'The paper introduces VL-SAE, a sparse autoencoder designed to improve the alignment between vision and language representations in multi-modal models. By correlating neurons in its hidden layer to unified concepts derived from semantically similar images and texts, VL-SAE enhances both interpretability and performance. The model employs self-supervised training to ensure consistent neuron activations for similar representations, using cosine similarity for semantic alignment. Experiments show that VL-SAE significantly boosts the effectiveness of vision-language models in tasks like zero-shot image classification and reducing hallucinations.'}, 'zh': {'title': 'VL-SAEï¼šæå‡è§†è§‰-è¯­è¨€å¯¹é½çš„ç¨€ç–è‡ªç¼–ç å™¨', 'desc': 'VL-SAEæ˜¯ä¸€ç§ç¨€ç–è‡ªç¼–ç å™¨ï¼Œé€šè¿‡å°†è§†è§‰å’Œè¯­è¨€è¡¨ç¤ºå…³è”åˆ°ç»Ÿä¸€çš„æ¦‚å¿µï¼Œå¢å¼ºäº†è§†è§‰-è¯­è¨€å¯¹é½çš„èƒ½åŠ›ã€‚å®ƒçš„éšè—å±‚ä¸­çš„æ¯ä¸ªç¥ç»å…ƒä¸è¯­ä¹‰ç›¸ä¼¼çš„å›¾åƒå’Œæ–‡æœ¬æ‰€ä»£è¡¨çš„æ¦‚å¿µç›¸å…³è”ï¼Œä»è€Œæé«˜äº†æ¨¡å‹çš„å¯è§£é‡Šæ€§å’Œæ€§èƒ½ã€‚é€šè¿‡è‡ªç›‘ç£è®­ç»ƒï¼ŒVL-SAEé¼“åŠ±è¯­ä¹‰ç›¸ä¼¼çš„è¡¨ç¤ºåœ¨ç¥ç»å…ƒæ¿€æ´»ä¸Šä¿æŒä¸€è‡´ã€‚å®éªŒè¡¨æ˜ï¼ŒVL-SAEåœ¨é›¶æ ·æœ¬å›¾åƒåˆ†ç±»å’Œæ¶ˆé™¤å¹»è§‰ç­‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œæ˜¾è‘—æå‡äº†è§†è§‰-è¯­è¨€æ¨¡å‹çš„å¯¹é½èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.20155', 'title': 'PartNeXt: A Next-Generation Dataset for Fine-Grained and Hierarchical 3D\n  Part Understanding', 'url': 'https://huggingface.co/papers/2510.20155', 'abstract': "PartNeXt, a high-quality, textured 3D dataset with fine-grained part labels, improves performance in class-agnostic part segmentation and 3D part-centric question answering, highlighting gaps in open-vocabulary part grounding.  \t\t\t\t\tAI-generated summary \t\t\t\t Understanding objects at the level of their constituent parts is fundamental to advancing computer vision, graphics, and robotics. While datasets like PartNet have driven progress in 3D part understanding, their reliance on untextured geometries and expert-dependent annotation limits scalability and usability. We introduce PartNeXt, a next-generation dataset addressing these gaps with over 23,000 high-quality, textured 3D models annotated with fine-grained, hierarchical part labels across 50 categories. We benchmark PartNeXt on two tasks: (1) class-agnostic part segmentation, where state-of-the-art methods (e.g., PartField, SAMPart3D) struggle with fine-grained and leaf-level parts, and (2) 3D part-centric question answering, a new benchmark for 3D-LLMs that reveals significant gaps in open-vocabulary part grounding. Additionally, training Point-SAM on PartNeXt yields substantial gains over PartNet, underscoring the dataset's superior quality and diversity. By combining scalable annotation, texture-aware labels, and multi-task evaluation, PartNeXt opens new avenues for research in structured 3D understanding.", 'score': 1, 'issue_id': 6672, 'pub_date': '2025-10-23', 'pub_date_card': {'ru': '23 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 23', 'zh': '10æœˆ23æ—¥'}, 'hash': '338d5df27efaaad5', 'authors': ['Penghao Wang', 'Yiyang He', 'Xin Lv', 'Yukai Zhou', 'Lan Xu', 'Jingyi Yu', 'Jiayuan Gu'], 'affiliations': ['ShanghaiTech University'], 'pdf_title_img': 'assets/pdf/title_img/2510.20155.jpg', 'data': {'categories': ['#dataset', '#3d', '#benchmark', '#cv'], 'emoji': 'ğŸ§©', 'ru': {'title': 'PartNeXt: Ñ‚ĞµĞºÑÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ 3D Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¾Ğ¹ Ñ‡Ğ°ÑÑ‚ĞµĞ¹ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ²', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ PartNeXt â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¸Ğ· Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 23,000 Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… 3D Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¾Ğ¹ Ñ‡Ğ°ÑÑ‚ĞµĞ¹ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² 50 ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸ÑÑ…. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº PartNet, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±ĞµĞ· Ñ‚ĞµĞºÑÑ‚ÑƒÑ€ Ğ¸ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ»Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ğ¾Ğ¹ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸. ĞĞ° PartNeXt Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ´Ğ²Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸: class-agnostic ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ñ‡Ğ°ÑÑ‚ĞµĞ¹ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ 3D question answering Ğ¾ Ñ‡Ğ°ÑÑ‚ÑÑ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ğ³Ğ´Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ñ‹ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Point-SAM Ğ½Ğ° PartNeXt Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ PartNet, Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°Ñ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°.'}, 'en': {'title': 'PartNeXt: Elevating 3D Understanding with Textured Models and Fine-Grained Labels', 'desc': "PartNeXt is a new dataset designed to enhance the understanding of 3D objects by providing high-quality, textured models with detailed part labels. It addresses limitations of previous datasets like PartNet, which lacked texture and relied on expert annotations, making them less scalable. The dataset is evaluated on two key tasks: class-agnostic part segmentation and 3D part-centric question answering, revealing challenges in current models' ability to handle fine-grained parts. By offering a diverse and well-annotated resource, PartNeXt aims to advance research in 3D part segmentation and improve the performance of machine learning models in this area."}, 'zh': {'title': 'PartNeXtï¼šæ¨åŠ¨3Dç†è§£çš„æ–°æ•°æ®é›†', 'desc': 'PartNeXtæ˜¯ä¸€ä¸ªé«˜è´¨é‡çš„3Dæ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡23,000ä¸ªå¸¦æœ‰ç»†ç²’åº¦éƒ¨ä»¶æ ‡ç­¾çš„çº¹ç†æ¨¡å‹ï¼Œæ—¨åœ¨æå‡æ— ç±»åˆ«éƒ¨ä»¶åˆ†å‰²å’Œ3Déƒ¨ä»¶ä¸­å¿ƒé—®ç­”çš„æ€§èƒ½ã€‚ä¸ä»¥å¾€çš„æ•°æ®é›†ç›¸æ¯”ï¼ŒPartNeXtå…‹æœäº†ä¾èµ–æ— çº¹ç†å‡ ä½•ä½“å’Œä¸“å®¶æ³¨é‡Šçš„å±€é™æ€§ï¼Œæä¾›äº†æ›´å¥½çš„å¯æ‰©å±•æ€§å’Œå¯ç”¨æ€§ã€‚é€šè¿‡åœ¨ä¸¤ä¸ªä»»åŠ¡ä¸Šè¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼ŒPartNeXtå±•ç¤ºäº†å…¶åœ¨ç»†ç²’åº¦éƒ¨ä»¶å¤„ç†å’Œå¼€æ”¾è¯æ±‡éƒ¨ä»¶å®šä½æ–¹é¢çš„ä¼˜åŠ¿ã€‚è¯¥æ•°æ®é›†çš„å¤šä»»åŠ¡è¯„ä¼°å’Œçº¹ç†æ„ŸçŸ¥æ ‡ç­¾ä¸ºç»“æ„åŒ–3Dç†è§£çš„ç ”ç©¶å¼€è¾Ÿäº†æ–°çš„æ–¹å‘ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.23828', 'title': "Beyond Understanding: Evaluating the Pragmatic Gap in LLMs' Cultural\n  Processing of Figurative Language", 'url': 'https://huggingface.co/papers/2510.23828', 'abstract': 'We present a comprehensive evaluation of the ability of large language models (LLMs) to process culturally grounded language, specifically to understand and pragmatically use figurative expressions that encode local knowledge and cultural nuance. Using figurative language as a proxy for cultural nuance and local knowledge, we design evaluation tasks for contextual understanding, pragmatic use, and connotation interpretation in Arabic and English. We evaluate 22 open- and closed-source LLMs on Egyptian Arabic idioms, multidialectal Arabic proverbs, and English proverbs. Our results show a consistent hierarchy: the average accuracy for Arabic proverbs is 4.29% lower than for English proverbs, and performance for Egyptian idioms is 10.28% lower than for Arabic proverbs. For the pragmatic use task, accuracy drops by 14.07% relative to understanding, though providing contextual idiomatic sentences improves accuracy by 10.66%. Models also struggle with connotative meaning, reaching at most 85.58% agreement with human annotators on idioms with 100% inter-annotator agreement. These findings demonstrate that figurative language serves as an effective diagnostic for cultural reasoning: while LLMs can often interpret figurative meaning, they face challenges in using it appropriately. To support future research, we release Kinayat, the first dataset of Egyptian Arabic idioms designed for both figurative understanding and pragmatic use evaluation.', 'score': 0, 'issue_id': 6687, 'pub_date': '2025-10-27', 'pub_date_card': {'ru': '27 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 27', 'zh': '10æœˆ27æ—¥'}, 'hash': 'd4ec15f710d0fa0b', 'authors': ['Mena Attia', 'Aashiq Muhamed', 'Mai Alkhamissi', 'Thamar Solorio', 'Mona Diab'], 'affiliations': ['Carnegie Mellon University', 'MBZUAI'], 'pdf_title_img': 'assets/pdf/title_img/2510.23828.jpg', 'data': {'categories': ['#dataset', '#low_resource', '#interpretability', '#alignment', '#multilingual', '#benchmark'], 'emoji': 'ğŸ­', 'ru': {'title': 'ĞšÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ñ‹Ğµ Ğ½ÑĞ°Ğ½ÑÑ‹ â€” Ğ°Ñ…Ğ¸Ğ»Ğ»ĞµÑĞ¾Ğ²Ğ° Ğ¿ÑÑ‚Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ LLM Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¹ ÑĞ·Ñ‹Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ñ‚Ñ€Ğ°Ğ¶Ğ°ĞµÑ‚ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ñ‹Ğµ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ. ĞĞ½Ğ¸ Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ 22 Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¸ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° ĞµĞ³Ğ¸Ğ¿ĞµÑ‚ÑĞºĞ¸Ñ… Ğ¸Ğ´Ğ¸Ğ¾Ğ¼Ğ°Ñ…, Ğ°Ñ€Ğ°Ğ±ÑĞºĞ¸Ñ… Ğ¿Ğ¾ÑĞ»Ğ¾Ğ²Ğ¸Ñ†Ğ°Ñ… Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ´Ğ¸Ğ°Ğ»ĞµĞºÑ‚Ğ¾Ğ² Ğ¸ Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¸Ñ… Ğ¿Ğ¾ÑĞ»Ğ¾Ğ²Ğ¸Ñ†Ğ°Ñ…. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ñ‡Ñ‘Ñ‚ĞºÑƒÑ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ: Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¸Ğ¼Ğ¸ Ğ¿Ğ¾ÑĞ»Ğ¾Ğ²Ğ¸Ñ†Ğ°Ğ¼Ğ¸ Ğ»ÑƒÑ‡ÑˆĞµ Ğ²ÑĞµĞ³Ğ¾, Ğ½Ğ° 4.29% Ñ…ÑƒĞ¶Ğµ Ñ Ğ°Ñ€Ğ°Ğ±ÑĞºĞ¸Ğ¼Ğ¸, Ğ¸ Ğ½Ğ° 10.28% Ñ…ÑƒĞ¶Ğµ Ñ ĞµĞ³Ğ¸Ğ¿ĞµÑ‚ÑĞºĞ¸Ğ¼Ğ¸ Ğ¸Ğ´Ğ¸Ğ¾Ğ¼Ğ°Ğ¼Ğ¸. ĞÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¹ Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ°ÑÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ° Ğ¿Ñ€Ğ°Ğ³Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ²Ñ‹Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ â€” Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ ÑƒĞ¿Ğ°Ğ»Ğ° Ğ½Ğ° 14.07%, Ğ° ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¸Ğµ Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾ ĞºĞ¾Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ¾ Ğ¼Ğ°ĞºÑĞ¸Ğ¼ÑƒĞ¼ 85.58%, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñƒ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… LLM.'}, 'en': {'title': 'Unlocking Cultural Nuance: Evaluating LLMs with Figurative Language', 'desc': 'This paper evaluates how well large language models (LLMs) understand and use culturally specific language, focusing on figurative expressions that reflect local knowledge. The authors created tasks to assess contextual understanding, pragmatic application, and connotation interpretation in both Arabic and English. They tested 22 different LLMs on various idioms and proverbs, revealing that models perform better on English than Arabic, with significant drops in accuracy for pragmatic tasks. The study highlights that while LLMs can grasp figurative meanings, they struggle with appropriate usage, indicating the need for improved cultural reasoning in AI.'}, 'zh': {'title': 'å¤§å‹è¯­è¨€æ¨¡å‹ä¸æ–‡åŒ–è¯­è¨€çš„æŒ‘æˆ˜', 'desc': 'æœ¬æ–‡è¯„ä¼°äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¤„ç†æ–‡åŒ–ç›¸å…³è¯­è¨€çš„èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯ç†è§£å’Œä½¿ç”¨éšå–»è¡¨è¾¾çš„èƒ½åŠ›ã€‚æˆ‘ä»¬è®¾è®¡äº†è¯„ä¼°ä»»åŠ¡ï¼Œè€ƒå¯Ÿæ¨¡å‹åœ¨é˜¿æ‹‰ä¼¯è¯­å’Œè‹±è¯­ä¸­çš„ä¸Šä¸‹æ–‡ç†è§£ã€å®ç”¨ä½¿ç”¨å’Œå†…æ¶µè§£é‡Šã€‚ç»“æœæ˜¾ç¤ºï¼Œé˜¿æ‹‰ä¼¯è°šè¯­çš„å¹³å‡å‡†ç¡®ç‡æ¯”è‹±è¯­è°šè¯­ä½4.29%ï¼Œè€ŒåŸƒåŠæˆè¯­çš„è¡¨ç°æ›´ä½ï¼Œä½äºé˜¿æ‹‰ä¼¯è°šè¯­10.28%ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œéšå–»è¯­è¨€æ˜¯æ–‡åŒ–æ¨ç†çš„æœ‰æ•ˆè¯Šæ–­å·¥å…·ï¼Œå°½ç®¡LLMsèƒ½å¤Ÿè§£é‡Šéšå–»æ„ä¹‰ï¼Œä½†åœ¨é€‚å½“ä½¿ç”¨æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.22264', 'title': 'PatenTEB: A Comprehensive Benchmark and Model Family for Patent Text\n  Embedding', 'url': 'https://huggingface.co/papers/2510.22264', 'abstract': 'PatenTEB is a comprehensive benchmark for patent text embeddings with 15 tasks, and the patembed model family demonstrates strong generalization across various patent-specific challenges.  \t\t\t\t\tAI-generated summary \t\t\t\t Patent text embeddings enable prior art search, technology landscaping, and patent analysis, yet existing benchmarks inadequately capture patent-specific challenges. We introduce PatenTEB, a comprehensive benchmark comprising 15 tasks across retrieval, classification, paraphrase, and clustering, with 2.06 million examples. PatenTEB employs domain-stratified splits, domain specific hard negative mining, and systematic coverage of asymmetric fragment-to-document matching scenarios absent from general embedding benchmarks. We develop the patembed model family through multi-task training, spanning 67M to 344M parameters with context lengths up to 4096 tokens. External validation shows strong generalization: patembed-base achieves state-of-the-art on MTEB BigPatentClustering.v2 (0.494 V-measure vs. 0.445 previous best), while patembed-large achieves 0.377 NDCG@100 on DAPFAM. Systematic ablations reveal that multi-task training improves external generalization despite minor benchmark costs, and that domain-pretrained initialization provides consistent advantages across task families. All resources will be made available at https://github.com/iliass-y/patenteb. Keywords: patent retrieval, sentence embeddings, multi-task learning, asymmetric retrieval, benchmark evaluation, contrastive learning.', 'score': 0, 'issue_id': 6676, 'pub_date': '2025-10-25', 'pub_date_card': {'ru': '25 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 25', 'zh': '10æœˆ25æ—¥'}, 'hash': '8e4bb95367ac7cbc', 'authors': ['Iliass Ayaou', 'Denis Cavallucci'], 'affiliations': ['INSA Strasbourg, France'], 'pdf_title_img': 'assets/pdf/title_img/2510.22264.jpg', 'data': {'categories': ['#benchmark', '#training', '#open_source', '#multimodal', '#transfer_learning', '#dataset'], 'emoji': 'ğŸ“œ', 'ru': {'title': 'PatenTEB: ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¿Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ PatenTEB â€” ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ text embeddings Ğ¿Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ 15 Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ 2 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ². Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸: Ğ°ÑĞ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ…, ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºÑƒ hard negatives Ğ¸ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ğ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ patembed Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼Ğ¸ Ğ¾Ñ‚ 67M Ğ´Ğ¾ 344M Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ´Ğ¾ 4096 Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ multi-task learning. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ multi-task Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¿Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Revolutionizing Patent Analysis with PatenTEB and Patembed Models', 'desc': 'PatenTEB is a new benchmark designed specifically for evaluating patent text embeddings across 15 diverse tasks, including retrieval and classification. The patembed model family, which ranges from 67M to 344M parameters, shows impressive generalization capabilities in handling patent-specific challenges. By utilizing techniques like domain-stratified splits and hard negative mining, PatenTEB addresses gaps in existing benchmarks that do not cater to the unique aspects of patent texts. The results indicate that multi-task training and domain-pretrained models significantly enhance performance on various tasks, demonstrating the effectiveness of this approach in patent analysis.'}, 'zh': {'title': 'ä¸“åˆ©æ–‡æœ¬åµŒå…¥çš„å…¨é¢åŸºå‡†ä¸å¼ºå¤§æ¨¡å‹', 'desc': 'PatenTEBæ˜¯ä¸€ä¸ªå…¨é¢çš„ä¸“åˆ©æ–‡æœ¬åµŒå…¥åŸºå‡†ï¼ŒåŒ…å«15ä¸ªä»»åŠ¡ï¼Œæ—¨åœ¨è§£å†³ä¸“åˆ©ç‰¹æœ‰çš„æŒ‘æˆ˜ã€‚è¯¥åŸºå‡†é€šè¿‡é¢†åŸŸåˆ†å±‚åˆ’åˆ†ã€ç‰¹å®šé¢†åŸŸçš„å›°éš¾è´Ÿæ ·æœ¬æŒ–æ˜ï¼Œä»¥åŠç³»ç»Ÿè¦†ç›–ä¸å¯¹ç§°ç‰‡æ®µä¸æ–‡æ¡£åŒ¹é…åœºæ™¯ï¼Œæä¾›äº†2.06ç™¾ä¸‡ä¸ªç¤ºä¾‹ã€‚patembedæ¨¡å‹ç³»åˆ—é€šè¿‡å¤šä»»åŠ¡è®­ç»ƒå¼€å‘ï¼Œå‚æ•°èŒƒå›´ä»6700ä¸‡åˆ°3.44äº¿ï¼Œæ”¯æŒæœ€é•¿4096ä¸ªæ ‡è®°çš„ä¸Šä¸‹æ–‡ã€‚å¤–éƒ¨éªŒè¯æ˜¾ç¤ºï¼Œpatembedæ¨¡å‹åœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œè¯æ˜äº†å¤šä»»åŠ¡è®­ç»ƒå’Œé¢†åŸŸé¢„è®­ç»ƒåˆå§‹åŒ–çš„æœ‰æ•ˆæ€§ã€‚'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (2)', '#agents (14)', '#agi (1)', '#alignment (5)', '#architecture (5)', '#audio (2)', '#benchmark (27)', '#cv (6)', '#data (4)', '#dataset (18)', '#diffusion (6)', '#ethics (2)', '#games (4)', '#graphs', '#hallucinations (1)', '#healthcare', '#inference (3)', '#interpretability (9)', '#leakage', '#long_context (2)', '#low_resource (3)', '#machine_translation', '#math', '#multilingual (4)', '#multimodal (12)', '#open_source (10)', '#optimization (17)', '#plp', '#rag', '#reasoning (16)', '#rl (6)', '#rlhf (5)', '#robotics', '#science (2)', '#security', '#small_models', '#story_generation', '#survey (1)', '#synthetic (4)', '#training (20)', '#transfer_learning (6)', '#video (2)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            ğŸ”º ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'ğŸ”„ ' + getTimeDiff('2025-10-29 22:13',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ",
                    pub_date: "Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸",
                    issue_id: "Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "è¯„åˆ†",
                    pub_date: "å‘å¸ƒæ—¥æœŸ",
                    issue_id: "HFä¸Šä¼ æ—¥æœŸ"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-10-29 22:13')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-10-29 22:13')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    