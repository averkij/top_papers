{
    "date": {
        "ru": "15 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
        "en": "August 15",
        "zh": "8æœˆ15æ—¥"
    },
    "time_utc": "2025-08-15 06:18",
    "weekday": 4,
    "issue_id": 5367,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2508.10433",
            "title": "We-Math 2.0: A Versatile MathBook System for Incentivizing Visual\n  Mathematical Reasoning",
            "url": "https://huggingface.co/papers/2508.10433",
            "abstract": "We-Math 2.0 enhances MLLMs' mathematical reasoning through a structured knowledge system, model-centric data space modeling, and reinforcement learning, demonstrating competitive performance on benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal Large Language Models (MLLMs) have demonstrated impressive capabilities across various tasks, but still struggle with complex mathematical reasoning. Existing research primarily focuses on dataset construction and method optimization, often overlooking two critical aspects: comprehensive knowledge-driven design and model-centric data space modeling. In this paper, we introduce We-Math 2.0, a unified system that integrates a structured mathematical knowledge system, model-centric data space modeling, and a reinforcement learning (RL)-based training paradigm to comprehensively enhance the mathematical reasoning abilities of MLLMs. The key contributions of We-Math 2.0 are fourfold: (1) MathBook Knowledge System: We construct a five-level hierarchical system encompassing 491 knowledge points and 1,819 fundamental principles. (2) MathBook-Standard & Pro: We develop MathBook-Standard, a dataset that ensures broad conceptual coverage and flexibility through dual expansion. Additionally, we define a three-dimensional difficulty space and generate 7 progressive variants per problem to build MathBook-Pro, a challenging dataset for robust training. (3) MathBook-RL: We propose a two-stage RL framework comprising: (i) Cold-Start Fine-tuning, which aligns the model with knowledge-oriented chain-of-thought reasoning; and (ii) Progressive Alignment RL, leveraging average-reward learning and dynamic data scheduling to achieve progressive alignment across difficulty levels. (4) MathBookEval: We introduce a comprehensive benchmark covering all 491 knowledge points with diverse reasoning step distributions. Experimental results show that MathBook-RL performs competitively with existing baselines on four widely-used benchmarks and achieves strong results on MathBookEval, suggesting promising generalization in mathematical reasoning.",
            "score": 81,
            "issue_id": 5363,
            "pub_date": "2025-08-14",
            "pub_date_card": {
                "ru": "14 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 14",
                "zh": "8æœˆ14æ—¥"
            },
            "hash": "fabdc1bdb3fd820b",
            "authors": [
                "Runqi Qiao",
                "Qiuna Tan",
                "Peiqing Yang",
                "Yanzi Wang",
                "Xiaowan Wang",
                "Enhui Wan",
                "Sitong Zhou",
                "Guanting Dong",
                "Yuchen Zeng",
                "Yida Xu",
                "Jie Wang",
                "Chong Sun",
                "Chen Li",
                "Honggang Zhang"
            ],
            "affiliations": [
                "BUPT",
                "Tsinghua University",
                "WeChat Vision, Tencent Inc."
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.10433.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#optimization",
                    "#benchmark",
                    "#reasoning",
                    "#training",
                    "#data",
                    "#rl"
                ],
                "emoji": "ğŸ§®",
                "ru": {
                    "title": "Ğ£ÑĞ¸Ğ»ĞµĞ½Ğ¸Ğµ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ˜Ğ˜ Ñ‡ĞµÑ€ĞµĞ· ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "We-Math 2.0 - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM). ĞĞ½Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹, Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ±Ğ°Ğ·Ñ‹ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ MathBook, Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… MathBook-Standard Ğ¸ Pro, Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğ³Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ MathBook-RL. We-Math 2.0 Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¸ Ğ½Ğ¾Ğ²Ğ¾Ğ¼ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾Ğ¼ Ñ‚ĞµÑÑ‚Ğµ MathBookEval."
                },
                "en": {
                    "title": "Empowering MLLMs with Enhanced Mathematical Reasoning",
                    "desc": "We-Math 2.0 is a system designed to improve the mathematical reasoning abilities of Multimodal Large Language Models (MLLMs). It incorporates a structured knowledge system, model-centric data space modeling, and a reinforcement learning approach to enhance performance. The system includes a hierarchical knowledge framework, a comprehensive dataset for training, and a two-stage reinforcement learning strategy to align models with reasoning tasks. Experimental results indicate that We-Math 2.0 achieves competitive performance on various benchmarks, demonstrating its effectiveness in advancing mathematical reasoning capabilities."
                },
                "zh": {
                    "title": "We-Math 2.0ï¼šæå‡æ•°å­¦æ¨ç†çš„æ™ºèƒ½ç³»ç»Ÿ",
                    "desc": "We-Math 2.0 æ˜¯ä¸€ä¸ªå¢å¼ºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ•°å­¦æ¨ç†èƒ½åŠ›çš„ç³»ç»Ÿã€‚å®ƒé€šè¿‡æ„å»ºç»“æ„åŒ–çš„æ•°å­¦çŸ¥è¯†ä½“ç³»ã€ä»¥æ¨¡å‹ä¸ºä¸­å¿ƒçš„æ•°æ®ç©ºé—´å»ºæ¨¡å’ŒåŸºäºå¼ºåŒ–å­¦ä¹ çš„è®­ç»ƒæ–¹æ³•æ¥å®ç°è¿™ä¸€ç›®æ ‡ã€‚è¯¥ç³»ç»ŸåŒ…æ‹¬äº”çº§å±‚æ¬¡çš„çŸ¥è¯†ç‚¹å’ŒåŸºæœ¬åŸåˆ™ï¼Œç¡®ä¿äº†å¹¿æ³›çš„æ¦‚å¿µè¦†ç›–å’Œçµæ´»æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒWe-Math 2.0 åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨æ•°å­¦æ¨ç†æ–¹é¢çš„è‰¯å¥½æ³›åŒ–èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.10711",
            "title": "NextStep-1: Toward Autoregressive Image Generation with Continuous\n  Tokens at Scale",
            "url": "https://huggingface.co/papers/2508.10711",
            "abstract": "NextStep-1, a 14B autoregressive model with a 157M flow matching head, achieves state-of-the-art performance in text-to-image generation and image editing by processing discrete text tokens and continuous image tokens.  \t\t\t\t\tAI-generated summary \t\t\t\t Prevailing autoregressive (AR) models for text-to-image generation either rely on heavy, computationally-intensive diffusion models to process continuous image tokens, or employ vector quantization (VQ) to obtain discrete tokens with quantization loss. In this paper, we push the autoregressive paradigm forward with NextStep-1, a 14B autoregressive model paired with a 157M flow matching head, training on discrete text tokens and continuous image tokens with next-token prediction objectives. NextStep-1 achieves state-of-the-art performance for autoregressive models in text-to-image generation tasks, exhibiting strong capabilities in high-fidelity image synthesis. Furthermore, our method shows strong performance in image editing, highlighting the power and versatility of our unified approach. To facilitate open research, we will release our code and models to the community.",
            "score": 59,
            "issue_id": 5364,
            "pub_date": "2025-08-14",
            "pub_date_card": {
                "ru": "14 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 14",
                "zh": "8æœˆ14æ—¥"
            },
            "hash": "47af2f1f4d3a49d6",
            "authors": [
                "NextStep Team",
                "Chunrui Han",
                "Guopeng Li",
                "Jingwei Wu",
                "Quan Sun",
                "Yan Cai",
                "Yuang Peng",
                "Zheng Ge",
                "Deyu Zhou",
                "Haomiao Tang",
                "Hongyu Zhou",
                "Kenkun Liu",
                "Ailin Huang",
                "Bin Wang",
                "Changxin Miao",
                "Deshan Sun",
                "En Yu",
                "Fukun Yin",
                "Gang Yu",
                "Hao Nie",
                "Haoran Lv",
                "Hanpeng Hu",
                "Jia Wang",
                "Jian Zhou",
                "Jianjian Sun",
                "Kaijun Tan",
                "Kang An",
                "Kangheng Lin",
                "Liang Zhao",
                "Mei Chen",
                "Peng Xing",
                "Rui Wang",
                "Shiyu Liu",
                "Shutao Xia",
                "Tianhao You",
                "Wei Ji",
                "Xianfang Zeng",
                "Xin Han",
                "Xuelin Zhang",
                "Yana Wei",
                "Yanming Xu",
                "Yimin Jiang",
                "Yingming Wang",
                "Yu Zhou",
                "Yucheng Han",
                "Ziyang Meng",
                "Binxing Jiao",
                "Daxin Jiang",
                "Xiangyu Zhang",
                "Yibo Zhu"
            ],
            "affiliations": [
                "StepFun"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.10711.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#multimodal",
                    "#cv",
                    "#architecture",
                    "#diffusion",
                    "#training"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹: NextStep-1 Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ñ‚ĞµĞºÑÑ‚ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ",
                    "desc": "NextStep-1 - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ, ÑĞ¾ÑÑ‚Ğ¾ÑÑ‰Ğ°Ñ Ğ¸Ğ· Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ñ‡Ğ°ÑÑ‚Ğ¸ (14 Ğ¼Ğ»Ñ€Ğ´ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²) Ğ¸ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ñ‹ Ğ´Ğ»Ñ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ² (157 Ğ¼Ğ»Ğ½ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²). ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ¸ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², NextStep-1 Ğ½Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚ÑĞ¶ĞµĞ»Ñ‹Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸Ğ»Ğ¸ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğµ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€ÑƒÑÑ‚ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚ÑŒ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ´ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ³Ğ¾ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²Ğ°."
                },
                "en": {
                    "title": "NextStep-1: Revolutionizing Text-to-Image Generation with Autoregressive Power",
                    "desc": "NextStep-1 is a 14 billion parameter autoregressive model designed for text-to-image generation and image editing. It uniquely combines discrete text tokens with continuous image tokens, using a 157 million parameter flow matching head to enhance performance. This model outperforms existing methods by avoiding the computational heaviness of diffusion models and the quantization loss associated with vector quantization. The results demonstrate its ability to generate high-fidelity images and effectively edit them, showcasing the model's versatility and potential for future research."
                },
                "zh": {
                    "title": "NextStep-1ï¼šæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„æ–°çªç ´",
                    "desc": "NextStep-1æ˜¯ä¸€ç§14Bçš„è‡ªå›å½’æ¨¡å‹ï¼Œé…å¤‡157Mçš„æµåŒ¹é…å¤´ï¼Œä¸“æ³¨äºæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå’Œå›¾åƒç¼–è¾‘ã€‚ä¸ä¼ ç»Ÿçš„è‡ªå›å½’æ¨¡å‹ä¸åŒï¼Œå®ƒé€šè¿‡å¤„ç†ç¦»æ•£çš„æ–‡æœ¬æ ‡è®°å’Œè¿ç»­çš„å›¾åƒæ ‡è®°ï¼Œé¿å…äº†é‡åŒ–æŸå¤±ã€‚è¯¥æ¨¡å‹åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜ä¿çœŸçš„å›¾åƒã€‚æ­¤å¤–ï¼ŒNextStep-1åœ¨å›¾åƒç¼–è¾‘æ–¹é¢ä¹Ÿå±•ç°äº†å¼ºå¤§çš„èƒ½åŠ›ï¼Œè¯æ˜äº†å…¶æ–¹æ³•çš„å¼ºå¤§å’Œå¤šæ ·æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.09848",
            "title": "PRELUDE: A Benchmark Designed to Require Global Comprehension and\n  Reasoning over Long Contexts",
            "url": "https://huggingface.co/papers/2508.09848",
            "abstract": "A benchmark called PRELUDE evaluates long-context understanding by assessing the consistency of prequel stories with original books, revealing significant challenges for models compared to humans.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce PRELUDE, a benchmark for evaluating long-context understanding through the task of determining whether a character's prequel story is consistent with the canonical narrative of the original book. Our task poses a stronger demand for global comprehension and deep reasoning than existing benchmarks -- as the prequels are not part of the original story, assessing their plausibility typically requires searching and integrating information that is only indirectly related. Empirically, 88% of instances require evidence from multiple parts of the narrative. Experimental results highlight the challenge of our task: in-context learning, RAG and in-domain training with state-of-the-art LLMs, and commercial DeepResearch services, lag behind humans by >15%. A further human study reveals that models often produce correct answers with flawed reasoning, leading to an over 30% gap in reasoning accuracy compared to humans. These findings underscore the substantial room for improvement in long-context understanding and reasoning.",
            "score": 13,
            "issue_id": 5363,
            "pub_date": "2025-08-13",
            "pub_date_card": {
                "ru": "13 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 13",
                "zh": "8æœˆ13æ—¥"
            },
            "hash": "2ffa20c1920780a2",
            "authors": [
                "Mo Yu",
                "Tsz Ting Chung",
                "Chulun Zhou",
                "Tong Li",
                "Rui Lu",
                "Jiangnan Li",
                "Liyan Xu",
                "Haoshu Lu",
                "Ning Zhang",
                "Jing Li",
                "Jie Zhou"
            ],
            "affiliations": [
                "CUHK",
                "HKUST",
                "NJIT",
                "WeChat AI, Tencent"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.09848.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#benchmark",
                    "#multimodal",
                    "#long_context"
                ],
                "emoji": "ğŸ“š",
                "ru": {
                    "title": "PRELUDE: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ¾Ğ¼",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº PRELUDE Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ—Ğ°Ğ´Ğ°Ñ‡Ğ° Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ĞºĞ²ĞµĞ»Ğ¾Ğ² Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹ Ñ Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ĞºĞ½Ğ¸Ğ³Ğ°Ğ¼Ğ¸, Ñ‡Ñ‚Ğ¾ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ñ‚ÑÑ‚Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¾Ñ‚ Ğ»ÑĞ´ĞµĞ¹ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ Ğ½Ğ° 15%. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ´Ğ°ÑÑ‚ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸, Ñ‡Ñ‚Ğ¾ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ° ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°."
                },
                "en": {
                    "title": "PRELUDE: Bridging the Gap in Long-Context Understanding",
                    "desc": "The paper introduces PRELUDE, a benchmark designed to evaluate how well models understand long contexts by checking if prequel stories align with original narratives. This task requires models to demonstrate global comprehension and deep reasoning, as prequels are not directly part of the original story. The study shows that current state-of-the-art models struggle significantly, with a performance gap of over 15% compared to human reasoning. Additionally, while models may provide correct answers, they often do so with flawed reasoning, highlighting the need for advancements in long-context understanding."
                },
                "zh": {
                    "title": "é•¿æ–‡æœ¬ç†è§£çš„æ–°æŒ‘æˆ˜ï¼šPRELUDEåŸºå‡†",
                    "desc": "PRELUDEæ˜¯ä¸€ä¸ªè¯„ä¼°é•¿æ–‡æœ¬ç†è§£çš„æ–°åŸºå‡†ï¼Œä¸»è¦é€šè¿‡åˆ¤æ–­è§’è‰²çš„å‰ä¼ æ•…äº‹ä¸åŸè‘—çš„å™è¿°æ˜¯å¦ä¸€è‡´æ¥è¿›è¡Œè¯„ä¼°ã€‚è¿™ä¸ªä»»åŠ¡å¯¹æ¨¡å‹çš„å…¨çƒç†è§£å’Œæ·±åº¦æ¨ç†èƒ½åŠ›æå‡ºäº†æ›´é«˜çš„è¦æ±‚ï¼Œå› ä¸ºå‰ä¼ æ•…äº‹å¹¶ä¸æ˜¯åŸæ•…äº‹çš„ä¸€éƒ¨åˆ†ï¼Œè¯„ä¼°å…¶åˆç†æ€§é€šå¸¸éœ€è¦æ•´åˆé—´æ¥ç›¸å…³çš„ä¿¡æ¯ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå½“å‰çš„å…ˆè¿›æ¨¡å‹åœ¨è¿™ä¸€ä»»åŠ¡ä¸Šè¡¨ç°ä¸å¦‚äººç±»ï¼Œæ¨ç†å‡†ç¡®ç‡ç›¸å·®è¶…è¿‡30%ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†åœ¨é•¿æ–‡æœ¬ç†è§£å’Œæ¨ç†æ–¹é¢ä»æœ‰å¾ˆå¤§çš„æ”¹è¿›ç©ºé—´ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.10833",
            "title": "UI-Venus Technical Report: Building High-performance UI Agents with RFT",
            "url": "https://huggingface.co/papers/2508.10833",
            "abstract": "UI-Venus, a multimodal large language model-based UI agent, achieves state-of-the-art performance in UI grounding and navigation tasks using reinforcement fine-tuning and novel self-evolving frameworks.  \t\t\t\t\tAI-generated summary \t\t\t\t We present UI-Venus, a native UI agent that takes only screenshots as input based on a multimodal large language model. UI-Venus achieves SOTA performance on both UI grounding and navigation tasks using only several hundred thousand high-quality training samples through reinforcement finetune (RFT) based on Qwen2.5-VL. Specifically, the 7B and 72B variants of UI-Venus obtain 94.1% / 50.8% and 95.3% / 61.9% on the standard grounding benchmarks, i.e., Screenspot-V2 / Pro, surpassing the previous SOTA baselines including open-source GTA1 and closed-source UI-TARS-1.5.To show UI-Venus's summary and planing ability, we also evaluate it on the AndroidWorld, an online UI navigation arena, on which our 7B and 72B variants achieve 49.1% and 65.9% success rate, also beating existing models.To achieve this, we introduce carefully designed reward functions for both UI grounding and navigation tasks and corresponding efficient data cleaning strategies.To further boost navigation performance, we propose Self-Evolving Trajectory History Alignment \\& Sparse Action Enhancement that refine historical reasoning traces and balances the distribution of sparse but critical actions, leading to more coherent planning and better generalization in complex UI tasks. Our contributions include the publish of SOTA open-source UI agents, comprehensive data cleaning protocols and a novel self-evolving framework for improving navigation performance, which encourage further research and development in the community. Code is available at https://github.com/antgroup/UI-Venus.",
            "score": 11,
            "issue_id": 5364,
            "pub_date": "2025-08-14",
            "pub_date_card": {
                "ru": "14 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 14",
                "zh": "8æœˆ14æ—¥"
            },
            "hash": "23e6ea5081c3c925",
            "authors": [
                "Zhangxuan Gu",
                "Zhengwen Zeng",
                "Zhenyu Xu",
                "Xingran Zhou",
                "Shuheng Shen",
                "Yunfei Liu",
                "Beitong Zhou",
                "Changhua Meng",
                "Tianyu Xia",
                "Weizhi Chen",
                "Yue Wen",
                "Jingya Dou",
                "Fei Tang",
                "Jinzhen Lin",
                "Yulin Liu",
                "Zhenlin Guo",
                "Yichen Gong",
                "Heng Jia",
                "Changlong Gao",
                "Yuan Guo",
                "Yong Deng",
                "Zhenyu Guo",
                "Liang Chen",
                "Weiqiang Wang"
            ],
            "affiliations": [
                "Ant Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.10833.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#open_source",
                    "#rl",
                    "#games",
                    "#optimization",
                    "#multimodal",
                    "#training",
                    "#agents"
                ],
                "emoji": "ğŸ–¥ï¸",
                "ru": {
                    "title": "UI-Venus: Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğ¼Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ°Ğ¼Ğ¸",
                    "desc": "UI-Venus - ÑÑ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğ¼Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ°Ğ¼Ğ¸. ĞĞ½Ğ° Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºĞ¸ Ğº ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ°, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ÑĞºÑ€Ğ¸Ğ½ÑˆĞ¾Ñ‚Ñ‹ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ÑÑÑ‰ĞµĞ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Qwen2.5-VL. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ÑĞ°Ğ¼Ğ¾ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "UI-Venus: Redefining UI Navigation with Multimodal Intelligence",
                    "desc": "UI-Venus is a multimodal large language model-based UI agent that excels in UI grounding and navigation tasks. It utilizes reinforcement fine-tuning and innovative self-evolving frameworks to achieve state-of-the-art performance with only a few hundred thousand high-quality training samples. The model's variants, 7B and 72B, have surpassed previous benchmarks, demonstrating impressive success rates in both grounding and navigation tasks. Key contributions include the introduction of specialized reward functions and data cleaning strategies, along with a novel framework that enhances navigation performance through improved planning and reasoning."
                },
                "zh": {
                    "title": "UI-Venusï¼šç”¨æˆ·ç•Œé¢ä»»åŠ¡çš„æ–°æ ‡æ†",
                    "desc": "UI-Venusæ˜¯ä¸€ç§åŸºäºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„ç”¨æˆ·ç•Œé¢ä»£ç†ï¼Œèƒ½å¤Ÿä»…é€šè¿‡æˆªå›¾è¾“å…¥æ¥å®Œæˆä»»åŠ¡ã€‚å®ƒåœ¨ç”¨æˆ·ç•Œé¢å®šä½å’Œå¯¼èˆªä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œé‡‡ç”¨äº†å¼ºåŒ–å¾®è°ƒå’Œè‡ªæˆ‘æ¼”åŒ–æ¡†æ¶ã€‚UI-Venusçš„7Bå’Œ72Bç‰ˆæœ¬åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†ä¹‹å‰çš„æœ€ä½³æ¨¡å‹ï¼Œæ˜¾ç¤ºå‡ºå…¶å¼ºå¤§çš„èƒ½åŠ›ã€‚è¯¥æ¨¡å‹è¿˜å¼•å…¥äº†ç²¾å¿ƒè®¾è®¡çš„å¥–åŠ±å‡½æ•°å’Œé«˜æ•ˆçš„æ•°æ®æ¸…ç†ç­–ç•¥ï¼Œä»¥æå‡å¯¼èˆªæ€§èƒ½å’Œè§„åˆ’èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.10576",
            "title": "HumanSense: From Multimodal Perception to Empathetic Context-Aware\n  Responses through Reasoning MLLMs",
            "url": "https://huggingface.co/papers/2508.10576",
            "abstract": "HumanSense is a benchmark for evaluating human-centered perception and interaction in Multimodal Large Language Models, focusing on multimodal context understanding and rational feedback through reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t While Multimodal Large Language Models (MLLMs) show immense promise for achieving truly human-like interactions, progress is hindered by the lack of fine-grained evaluation frameworks for human-centered scenarios, encompassing both the understanding of complex human intentions and the provision of empathetic, context-aware responses. Here we introduce HumanSense, a comprehensive benchmark designed to evaluate the human-centered perception and interaction capabilities of MLLMs, with a particular focus on deep understanding of extended multimodal contexts and the formulation of rational feedback. Our evaluation reveals that leading MLLMs still have considerable room for improvement, particularly for advanced interaction-oriented tasks. Supplementing visual input with audio and text information yields substantial improvements, and Omni-modal models show advantages on these tasks. Furthermore, we argue that appropriate feedback stems from a contextual analysis of the interlocutor's needs and emotions, with reasoning ability serving as the key to unlocking it. Accordingly, we employ a multi-stage, modality-progressive reinforcement learning to enhance the reasoning abilities of an Omni model, achieving substantial gains on evaluation results. Additionally, we observe that successful reasoning processes exhibit highly consistent thought patterns. By designing corresponding prompts, we also enhance the performance of non-reasoning models in a training-free manner. Project page: brightpinkhttps://digital-avatar.github.io/ai/HumanSense/",
            "score": 4,
            "issue_id": 5363,
            "pub_date": "2025-08-14",
            "pub_date_card": {
                "ru": "14 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 14",
                "zh": "8æœˆ14æ—¥"
            },
            "hash": "94390c2120c8d69a",
            "authors": [
                "Zheng Qin",
                "Ruobing Zheng",
                "Yabing Wang",
                "Tianqi Li",
                "Yi Yuan",
                "Jingdong Chen",
                "Le Wang"
            ],
            "affiliations": [
                "Ant Group",
                "National Key Laboratory of Human-Machine Hybrid Augmented Intelligence, National Engineering Research Center for Visual Information and Applications, Institute of Artificial Intelligence and Robotics, Xian Jiaotong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.10576.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#benchmark",
                    "#interpretability",
                    "#reasoning",
                    "#rl",
                    "#multimodal"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞÑ†ĞµĞ½ĞºĞ° Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "HumanSense - ÑÑ‚Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ, Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ° Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°, Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (MLLM). ĞĞ½ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ¼ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ğ¸ Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ñ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ğµ MLLM Ğ²ÑĞµ ĞµÑ‰Ğµ Ğ¸Ğ¼ĞµÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğµ, Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾-Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¾Ğ¼Ğ½Ğ¸-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞ¸."
                },
                "en": {
                    "title": "Enhancing Human-Centered Interaction in MLLMs with HumanSense",
                    "desc": "HumanSense is a new benchmark aimed at assessing how well Multimodal Large Language Models (MLLMs) understand and interact in human-centered scenarios. It focuses on evaluating the models' ability to comprehend complex human intentions and provide empathetic responses based on multimodal inputs like text, audio, and visual data. The research shows that while current MLLMs have made progress, they still need improvement in advanced interaction tasks, especially when it comes to reasoning and contextual understanding. By using a multi-stage reinforcement learning approach, the study enhances the reasoning capabilities of these models, leading to better performance in understanding and responding to human needs and emotions."
                },
                "zh": {
                    "title": "æå‡äººæœºäº¤äº’çš„å¤šæ¨¡æ€åŸºå‡†",
                    "desc": "HumanSenseæ˜¯ä¸€ä¸ªåŸºå‡†ï¼Œç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨ä»¥äººä¸ºä¸­å¿ƒçš„æ„ŸçŸ¥å’Œäº¤äº’èƒ½åŠ›ã€‚è¯¥åŸºå‡†ç‰¹åˆ«å…³æ³¨å¯¹å¤æ‚äººç±»æ„å›¾çš„ç†è§£å’Œæä¾›å¯Œæœ‰åŒç†å¿ƒçš„ã€ä¸Šä¸‹æ–‡ç›¸å…³çš„åé¦ˆã€‚ç ”ç©¶è¡¨æ˜ï¼Œå°½ç®¡é¢†å…ˆçš„MLLMsåœ¨è¿™äº›ä»»åŠ¡ä¸Šä»æœ‰å¾ˆå¤§çš„æ”¹è¿›ç©ºé—´ï¼Œä½†é€šè¿‡ç»“åˆè§†è§‰ã€éŸ³é¢‘å’Œæ–‡æœ¬ä¿¡æ¯ï¼Œå¯ä»¥æ˜¾è‘—æå‡å…¶è¡¨ç°ã€‚æˆ‘ä»¬é‡‡ç”¨å¤šé˜¶æ®µçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œå¢å¼ºæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œä»è€Œåœ¨è¯„ä¼°ç»“æœä¸Šå–å¾—äº†æ˜¾è‘—çš„è¿›å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.10860",
            "title": "From Black Box to Transparency: Enhancing Automated Interpreting\n  Assessment with Explainable AI in College Classrooms",
            "url": "https://huggingface.co/papers/2508.10860",
            "abstract": "A multi-dimensional modeling framework enhances automated interpreting quality assessment by integrating feature engineering, data augmentation, and explainable machine learning, focusing on transparency and detailed diagnostic feedback.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in machine learning have spurred growing interests in automated interpreting quality assessment. Nevertheless, existing research suffers from insufficient examination of language use quality, unsatisfactory modeling effectiveness due to data scarcity and imbalance, and a lack of efforts to explain model predictions. To address these gaps, we propose a multi-dimensional modeling framework that integrates feature engineering, data augmentation, and explainable machine learning. This approach prioritizes explainability over ``black box'' predictions by utilizing only construct-relevant, transparent features and conducting Shapley Value (SHAP) analysis. Our results demonstrate strong predictive performance on a novel English-Chinese consecutive interpreting dataset, identifying BLEURT and CometKiwi scores to be the strongest predictive features for fidelity, pause-related features for fluency, and Chinese-specific phraseological diversity metrics for language use. Overall, by placing particular emphasis on explainability, we present a scalable, reliable, and transparent alternative to traditional human evaluation, facilitating the provision of detailed diagnostic feedback for learners and supporting self-regulated learning advantages not afforded by automated scores in isolation.",
            "score": 2,
            "issue_id": 5363,
            "pub_date": "2025-08-14",
            "pub_date_card": {
                "ru": "14 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 14",
                "zh": "8æœˆ14æ—¥"
            },
            "hash": "a06076355558e87a",
            "authors": [
                "Zhaokun Jiang",
                "Ziyin Zhang"
            ],
            "affiliations": [
                "Shanghai Jiao Tong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.10860.jpg",
            "data": {
                "categories": [
                    "#multilingual",
                    "#science",
                    "#dataset",
                    "#optimization",
                    "#interpretability",
                    "#training",
                    "#data"
                ],
                "emoji": "ğŸ—£ï¸",
                "ru": {
                    "title": "ĞŸÑ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ÑƒÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼ĞµÑ€Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ÑƒÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ°. ĞĞ½Ğ° Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ², Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· SHAP Ğ´Ğ»Ñ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ½Ğ¾Ğ²Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ° Ñ Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ³Ğ¾ Ğ½Ğ° ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¸Ğ¹."
                },
                "en": {
                    "title": "Enhancing Automated Interpreting Quality with Explainable AI",
                    "desc": "This paper introduces a multi-dimensional modeling framework aimed at improving the quality assessment of automated interpreting. It combines feature engineering, data augmentation, and explainable machine learning to enhance transparency and provide detailed feedback. The framework addresses issues like data scarcity and the need for clearer model predictions by using relevant features and Shapley Value analysis. The results show that this approach not only predicts interpreting quality effectively but also supports learners with valuable insights for self-improvement."
                },
                "zh": {
                    "title": "æå‡è‡ªåŠ¨åŒ–å£è¯‘è´¨é‡è¯„ä¼°çš„é€æ˜æ€§ä¸å¯é æ€§",
                    "desc": "è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§å¤šç»´å»ºæ¨¡æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜è‡ªåŠ¨åŒ–å£è¯‘è´¨é‡è¯„ä¼°çš„æ•ˆæœã€‚è¯¥æ¡†æ¶ç»“åˆäº†ç‰¹å¾å·¥ç¨‹ã€æ•°æ®å¢å¼ºå’Œå¯è§£é‡Šçš„æœºå™¨å­¦ä¹ ï¼Œå¼ºè°ƒé€æ˜æ€§å’Œè¯¦ç»†çš„è¯Šæ–­åé¦ˆã€‚ç ”ç©¶è¡¨æ˜ï¼Œä½¿ç”¨BLEURTå’ŒCometKiwiè¯„åˆ†ä½œä¸ºå¿ å®åº¦çš„é¢„æµ‹ç‰¹å¾ï¼Œä»¥åŠä¸æµåˆ©åº¦ç›¸å…³çš„åœé¡¿ç‰¹å¾ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡æ¨¡å‹çš„é¢„æµ‹æ€§èƒ½ã€‚é€šè¿‡è¿™ç§æ–¹æ³•ï¼Œè®ºæ–‡ä¸ºå­¦ä¹ è€…æä¾›äº†æ›´å¯é çš„åé¦ˆï¼Œæ”¯æŒè‡ªæˆ‘è°ƒèŠ‚å­¦ä¹ çš„ä¼˜åŠ¿ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.10751",
            "title": "Pass@k Training for Adaptively Balancing Exploration and Exploitation of\n  Large Reasoning Models",
            "url": "https://huggingface.co/papers/2508.10751",
            "abstract": "Using Pass@k as a reward in reinforcement learning with verifiable rewards improves exploration and reveals that exploration and exploitation can mutually enhance each other.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning with verifiable rewards (RLVR), which typically adopts Pass@1 as the reward, has faced the issues in balancing exploration and exploitation, causing policies to prefer conservative actions, converging to a local optimum. Identifying an appropriate reward metric is therefore crucial. Regarding the prior work, although Pass@k has been used in evaluation, its connection to LLM exploration ability in RLVR remains largely overlooked. To investigate this, we first use Pass@k as the reward to train the policy model (i.e., Pass@k Training), and observe the improvement on its exploration ability. Next, we derive an analytical solution for the advantage of Pass@k Training, leading to an efficient and effective process. Building on this, our analysis reveals that exploration and exploitation are not inherently conflicting objectives, while they can mutually enhance each other. Moreover, Pass@k Training with analytical derivation essentially involves directly designing the advantage function. Inspired by this, we preliminarily explore the advantage design for RLVR, showing promising results and highlighting a potential future direction.",
            "score": 1,
            "issue_id": 5367,
            "pub_date": "2025-08-14",
            "pub_date_card": {
                "ru": "14 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 14",
                "zh": "8æœˆ14æ—¥"
            },
            "hash": "643dd15b9c2eddf6",
            "authors": [
                "Zhipeng Chen",
                "Xiaobo Qin",
                "Youbin Wu",
                "Yue Ling",
                "Qinghao Ye",
                "Wayne Xin Zhao",
                "Guang Shi"
            ],
            "affiliations": [
                "ByteDance Seed",
                "Renmin University of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.10751.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#rlhf",
                    "#training",
                    "#optimization",
                    "#reasoning"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Pass@k: ĞºĞ»ÑÑ‡ Ğº Ğ±Ğ°Ğ»Ğ°Ğ½ÑÑƒ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑĞºÑĞ¿Ğ»ÑƒĞ°Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Pass@k Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°Ğ¼Ğ¸ (RLVR). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Pass@k ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ», Ñ‡Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ ÑĞºÑĞ¿Ğ»ÑƒĞ°Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ½Ğ¾ ÑƒÑĞ¸Ğ»Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ´Ñ€ÑƒĞ³ Ğ´Ñ€ÑƒĞ³Ğ°, Ğ° Ğ½Ğµ ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ğ¾Ğ²Ğ°Ñ‚ÑŒ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¿Ñ€ÑĞ¼Ğ¾Ğµ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² RLVR."
                },
                "en": {
                    "title": "Unlocking Exploration: Pass@k Enhances Reinforcement Learning",
                    "desc": "This paper explores the use of Pass@k as a reward in reinforcement learning with verifiable rewards (RLVR) to improve exploration strategies. Traditional methods often rely on Pass@1, which can lead to conservative policies that get stuck in local optima. By employing Pass@k, the authors demonstrate that exploration and exploitation can work together to enhance overall performance. The study also provides an analytical framework for understanding the benefits of this approach and suggests new directions for designing advantage functions in RLVR."
                },
                "zh": {
                    "title": "æ¢ç´¢ä¸åˆ©ç”¨çš„ç›¸äº’å¢å¼ºï¼šPass@kçš„åŠ›é‡",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†åœ¨å¼ºåŒ–å­¦ä¹ ä¸­ä½¿ç”¨å¯éªŒè¯å¥–åŠ±çš„ç­–ç•¥ï¼Œç‰¹åˆ«æ˜¯ä½¿ç”¨Pass@kä½œä¸ºå¥–åŠ±æ¥æ”¹å–„æ¢ç´¢èƒ½åŠ›ã€‚ä¼ ç»Ÿä¸Šï¼Œä½¿ç”¨Pass@1ä½œä¸ºå¥–åŠ±ä¼šå¯¼è‡´ç­–ç•¥åå‘ä¿å®ˆè¡Œä¸ºï¼Œéš¾ä»¥å¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨ã€‚é€šè¿‡å°†Pass@kä½œä¸ºå¥–åŠ±è¿›è¡Œè®­ç»ƒï¼Œç ”ç©¶å‘ç°æ¢ç´¢ä¸åˆ©ç”¨å¯ä»¥ç›¸äº’å¢å¼ºï¼Œè€Œä¸æ˜¯ç›¸äº’å¯¹ç«‹ã€‚æœ¬æ–‡è¿˜æå‡ºäº†ä¸€ç§åˆ†æè§£å†³æ–¹æ¡ˆï¼Œå±•ç¤ºäº†Pass@kè®­ç»ƒçš„ä¼˜åŠ¿ï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶æ–¹å‘æä¾›äº†å¯ç¤ºã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-08-14.html",
    "link_next": "2025-08-18.html",
    "link_month": "2025-08.html",
    "short_date_prev": {
        "ru": "14.08",
        "en": "08/14",
        "zh": "8æœˆ14æ—¥"
    },
    "short_date_next": {
        "ru": "18.08",
        "en": "08/18",
        "zh": "8æœˆ18æ—¥"
    },
    "categories": {
        "#dataset": 2,
        "#data": 3,
        "#benchmark": 3,
        "#agents": 1,
        "#cv": 1,
        "#rl": 4,
        "#rlhf": 2,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 4,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 5,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 2,
        "#reasoning": 4,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 4,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 0
    }
}