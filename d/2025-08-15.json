{
    "date": {
        "ru": "15 –∞–≤–≥—É—Å—Ç–∞",
        "en": "August 15",
        "zh": "8Êúà15Êó•"
    },
    "time_utc": "2025-08-15 02:48",
    "weekday": 4,
    "issue_id": 5363,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2508.10433",
            "title": "We-Math 2.0: A Versatile MathBook System for Incentivizing Visual\n  Mathematical Reasoning",
            "url": "https://huggingface.co/papers/2508.10433",
            "abstract": "We-Math 2.0 enhances MLLMs' mathematical reasoning through a structured knowledge system, model-centric data space modeling, and reinforcement learning, demonstrating competitive performance on benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal Large Language Models (MLLMs) have demonstrated impressive capabilities across various tasks, but still struggle with complex mathematical reasoning. Existing research primarily focuses on dataset construction and method optimization, often overlooking two critical aspects: comprehensive knowledge-driven design and model-centric data space modeling. In this paper, we introduce We-Math 2.0, a unified system that integrates a structured mathematical knowledge system, model-centric data space modeling, and a reinforcement learning (RL)-based training paradigm to comprehensively enhance the mathematical reasoning abilities of MLLMs. The key contributions of We-Math 2.0 are fourfold: (1) MathBook Knowledge System: We construct a five-level hierarchical system encompassing 491 knowledge points and 1,819 fundamental principles. (2) MathBook-Standard & Pro: We develop MathBook-Standard, a dataset that ensures broad conceptual coverage and flexibility through dual expansion. Additionally, we define a three-dimensional difficulty space and generate 7 progressive variants per problem to build MathBook-Pro, a challenging dataset for robust training. (3) MathBook-RL: We propose a two-stage RL framework comprising: (i) Cold-Start Fine-tuning, which aligns the model with knowledge-oriented chain-of-thought reasoning; and (ii) Progressive Alignment RL, leveraging average-reward learning and dynamic data scheduling to achieve progressive alignment across difficulty levels. (4) MathBookEval: We introduce a comprehensive benchmark covering all 491 knowledge points with diverse reasoning step distributions. Experimental results show that MathBook-RL performs competitively with existing baselines on four widely-used benchmarks and achieves strong results on MathBookEval, suggesting promising generalization in mathematical reasoning.",
            "score": 7,
            "issue_id": 5363,
            "pub_date": "2025-08-14",
            "pub_date_card": {
                "ru": "14 –∞–≤–≥—É—Å—Ç–∞",
                "en": "August 14",
                "zh": "8Êúà14Êó•"
            },
            "hash": "fabdc1bdb3fd820b",
            "authors": [
                "Runqi Qiao",
                "Qiuna Tan",
                "Peiqing Yang",
                "Yanzi Wang",
                "Xiaowan Wang",
                "Enhui Wan",
                "Sitong Zhou",
                "Guanting Dong",
                "Yuchen Zeng",
                "Yida Xu",
                "Jie Wang",
                "Chong Sun",
                "Chen Li",
                "Honggang Zhang"
            ],
            "affiliations": [
                "BUPT",
                "Tsinghua University",
                "WeChat Vision, Tencent Inc."
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.10433.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#optimization",
                    "#benchmark",
                    "#reasoning",
                    "#training",
                    "#data",
                    "#rl"
                ],
                "emoji": "üßÆ",
                "ru": {
                    "title": "–£—Å–∏–ª–µ–Ω–∏–µ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –ò–ò —á–µ—Ä–µ–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∑–Ω–∞–Ω–∏—è –∏ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º",
                    "desc": "We-Math 2.0 - —ç—Ç–æ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (MLLM). –û–Ω–∞ –≤–∫–ª—é—á–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å–∏—Å—Ç–µ–º—É –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–Ω–∞–Ω–∏–π, –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö —Å –æ—Ä–∏–µ–Ω—Ç–∞—Ü–∏–µ–π –Ω–∞ –º–æ–¥–µ–ª—å –∏ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º. –°–∏—Å—Ç–µ–º–∞ —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–π –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π MathBook, –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö MathBook-Standard –∏ Pro, –¥–≤—É—Ö—ç—Ç–∞–ø–Ω–æ–≥–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º MathBook-RL. We-Math 2.0 –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ—Å–ø–æ—Å–æ–±–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö –∏ –Ω–æ–≤–æ–º –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–º —Ç–µ—Å—Ç–µ MathBookEval."
                },
                "en": {
                    "title": "Empowering MLLMs with Enhanced Mathematical Reasoning",
                    "desc": "We-Math 2.0 is a system designed to improve the mathematical reasoning abilities of Multimodal Large Language Models (MLLMs). It incorporates a structured knowledge system, model-centric data space modeling, and a reinforcement learning approach to enhance performance. The system includes a hierarchical knowledge framework, a comprehensive dataset for training, and a two-stage reinforcement learning strategy to align models with reasoning tasks. Experimental results indicate that We-Math 2.0 achieves competitive performance on various benchmarks, demonstrating its effectiveness in advancing mathematical reasoning capabilities."
                },
                "zh": {
                    "title": "We-Math 2.0ÔºöÊèêÂçáÊï∞Â≠¶Êé®ÁêÜÁöÑÊô∫ËÉΩÁ≥ªÁªü",
                    "desc": "We-Math 2.0 ÊòØ‰∏Ä‰∏™Â¢ûÂº∫Â§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâÊï∞Â≠¶Êé®ÁêÜËÉΩÂäõÁöÑÁ≥ªÁªü„ÄÇÂÆÉÈÄöËøáÊûÑÂª∫ÁªìÊûÑÂåñÁöÑÊï∞Â≠¶Áü•ËØÜ‰ΩìÁ≥ª„ÄÅ‰ª•Ê®°Âûã‰∏∫‰∏≠ÂøÉÁöÑÊï∞ÊçÆÁ©∫Èó¥Âª∫Ê®°ÂíåÂü∫‰∫éÂº∫ÂåñÂ≠¶‰π†ÁöÑËÆ≠ÁªÉÊñπÊ≥ïÊù•ÂÆûÁé∞Ëøô‰∏ÄÁõÆÊ†á„ÄÇËØ•Á≥ªÁªüÂåÖÊã¨‰∫îÁ∫ßÂ±ÇÊ¨°ÁöÑÁü•ËØÜÁÇπÂíåÂü∫Êú¨ÂéüÂàôÔºåÁ°Æ‰øù‰∫ÜÂπøÊ≥õÁöÑÊ¶ÇÂøµË¶ÜÁõñÂíåÁÅµÊ¥ªÊÄß„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåWe-Math 2.0 Âú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÊòæÁ§∫Âá∫ÂÖ∂Âú®Êï∞Â≠¶Êé®ÁêÜÊñπÈù¢ÁöÑËâØÂ•ΩÊ≥õÂåñËÉΩÂäõ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.09848",
            "title": "PRELUDE: A Benchmark Designed to Require Global Comprehension and\n  Reasoning over Long Contexts",
            "url": "https://huggingface.co/papers/2508.09848",
            "abstract": "A benchmark called PRELUDE evaluates long-context understanding by assessing the consistency of prequel stories with original books, revealing significant challenges for models compared to humans.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce PRELUDE, a benchmark for evaluating long-context understanding through the task of determining whether a character's prequel story is consistent with the canonical narrative of the original book. Our task poses a stronger demand for global comprehension and deep reasoning than existing benchmarks -- as the prequels are not part of the original story, assessing their plausibility typically requires searching and integrating information that is only indirectly related. Empirically, 88% of instances require evidence from multiple parts of the narrative. Experimental results highlight the challenge of our task: in-context learning, RAG and in-domain training with state-of-the-art LLMs, and commercial DeepResearch services, lag behind humans by >15%. A further human study reveals that models often produce correct answers with flawed reasoning, leading to an over 30% gap in reasoning accuracy compared to humans. These findings underscore the substantial room for improvement in long-context understanding and reasoning.",
            "score": 3,
            "issue_id": 5363,
            "pub_date": "2025-08-13",
            "pub_date_card": {
                "ru": "13 –∞–≤–≥—É—Å—Ç–∞",
                "en": "August 13",
                "zh": "8Êúà13Êó•"
            },
            "hash": "2ffa20c1920780a2",
            "authors": [
                "Mo Yu",
                "Tsz Ting Chung",
                "Chulun Zhou",
                "Tong Li",
                "Rui Lu",
                "Jiangnan Li",
                "Liyan Xu",
                "Haoshu Lu",
                "Ning Zhang",
                "Jing Li",
                "Jie Zhou"
            ],
            "affiliations": [
                "CUHK",
                "HKUST",
                "NJIT",
                "WeChat AI, Tencent"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.09848.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#benchmark",
                    "#multimodal",
                    "#long_context"
                ],
                "emoji": "üìö",
                "ru": {
                    "title": "PRELUDE: –Ω–æ–≤—ã–π —Ä—É–±–µ–∂ –≤ –æ—Ü–µ–Ω–∫–µ –≥–ª—É–±–∏–Ω—ã –ø–æ–Ω–∏–º–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–æ–º",
                    "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ PRELUDE –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –º–æ–¥–µ–ª—è–º–∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –ó–∞–¥–∞—á–∞ –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∫–≤–µ–ª–æ–≤ –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π —Å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–º–∏ –∫–Ω–∏–≥–∞–º–∏, —á—Ç–æ —Ç—Ä–µ–±—É–µ—Ç –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ –≥–ª—É–±–æ–∫–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ –æ—Ç—Å—Ç–∞–≤–∞–Ω–∏–µ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –æ—Ç –ª—é–¥–µ–π –±–æ–ª–µ–µ —á–µ–º –Ω–∞ 15%. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –≤—ã—è–≤–∏–ª–æ, —á—Ç–æ –º–æ–¥–µ–ª–∏ —á–∞—Å—Ç–æ –¥–∞—é—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã —Å –æ—à–∏–±–æ—á–Ω—ã–º–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º–∏, —á—Ç–æ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≤ –æ–±–ª–∞—Å—Ç–∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞."
                },
                "en": {
                    "title": "PRELUDE: Bridging the Gap in Long-Context Understanding",
                    "desc": "The paper introduces PRELUDE, a benchmark designed to evaluate how well models understand long contexts by checking if prequel stories align with original narratives. This task requires models to demonstrate global comprehension and deep reasoning, as prequels are not directly part of the original story. The study shows that current state-of-the-art models struggle significantly, with a performance gap of over 15% compared to human reasoning. Additionally, while models may provide correct answers, they often do so with flawed reasoning, highlighting the need for advancements in long-context understanding."
                },
                "zh": {
                    "title": "ÈïøÊñáÊú¨ÁêÜËß£ÁöÑÊñ∞ÊåëÊàòÔºöPRELUDEÂü∫ÂáÜ",
                    "desc": "PRELUDEÊòØ‰∏Ä‰∏™ËØÑ‰º∞ÈïøÊñáÊú¨ÁêÜËß£ÁöÑÊñ∞Âü∫ÂáÜÔºå‰∏ªË¶ÅÈÄöËøáÂà§Êñ≠ËßíËâ≤ÁöÑÂâç‰º†ÊïÖ‰∫ã‰∏éÂéüËëóÁöÑÂèôËø∞ÊòØÂê¶‰∏ÄËá¥Êù•ËøõË°åËØÑ‰º∞„ÄÇËøô‰∏™‰ªªÂä°ÂØπÊ®°ÂûãÁöÑÂÖ®ÁêÉÁêÜËß£ÂíåÊ∑±Â∫¶Êé®ÁêÜËÉΩÂäõÊèêÂá∫‰∫ÜÊõ¥È´òÁöÑË¶ÅÊ±ÇÔºåÂõ†‰∏∫Ââç‰º†ÊïÖ‰∫ãÂπ∂‰∏çÊòØÂéüÊïÖ‰∫ãÁöÑ‰∏ÄÈÉ®ÂàÜÔºåËØÑ‰º∞ÂÖ∂ÂêàÁêÜÊÄßÈÄöÂ∏∏ÈúÄË¶ÅÊï¥ÂêàÈó¥Êé•Áõ∏ÂÖ≥ÁöÑ‰ø°ÊÅØ„ÄÇÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåÂΩìÂâçÁöÑÂÖàËøõÊ®°ÂûãÂú®Ëøô‰∏Ä‰ªªÂä°‰∏äË°®Áé∞‰∏çÂ¶Ç‰∫∫Á±ªÔºåÊé®ÁêÜÂáÜÁ°ÆÁéáÁõ∏Â∑ÆË∂ÖËøá30%„ÄÇËøô‰∫õÂèëÁé∞Âº∫Ë∞É‰∫ÜÂú®ÈïøÊñáÊú¨ÁêÜËß£ÂíåÊé®ÁêÜÊñπÈù¢‰ªçÊúâÂæàÂ§ßÁöÑÊîπËøõÁ©∫Èó¥„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.10860",
            "title": "From Black Box to Transparency: Enhancing Automated Interpreting\n  Assessment with Explainable AI in College Classrooms",
            "url": "https://huggingface.co/papers/2508.10860",
            "abstract": "A multi-dimensional modeling framework enhances automated interpreting quality assessment by integrating feature engineering, data augmentation, and explainable machine learning, focusing on transparency and detailed diagnostic feedback.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in machine learning have spurred growing interests in automated interpreting quality assessment. Nevertheless, existing research suffers from insufficient examination of language use quality, unsatisfactory modeling effectiveness due to data scarcity and imbalance, and a lack of efforts to explain model predictions. To address these gaps, we propose a multi-dimensional modeling framework that integrates feature engineering, data augmentation, and explainable machine learning. This approach prioritizes explainability over ``black box'' predictions by utilizing only construct-relevant, transparent features and conducting Shapley Value (SHAP) analysis. Our results demonstrate strong predictive performance on a novel English-Chinese consecutive interpreting dataset, identifying BLEURT and CometKiwi scores to be the strongest predictive features for fidelity, pause-related features for fluency, and Chinese-specific phraseological diversity metrics for language use. Overall, by placing particular emphasis on explainability, we present a scalable, reliable, and transparent alternative to traditional human evaluation, facilitating the provision of detailed diagnostic feedback for learners and supporting self-regulated learning advantages not afforded by automated scores in isolation.",
            "score": 2,
            "issue_id": 5363,
            "pub_date": "2025-08-14",
            "pub_date_card": {
                "ru": "14 –∞–≤–≥—É—Å—Ç–∞",
                "en": "August 14",
                "zh": "8Êúà14Êó•"
            },
            "hash": "a06076355558e87a",
            "authors": [
                "Zhaokun Jiang",
                "Ziyin Zhang"
            ],
            "affiliations": [
                "Shanghai Jiao Tong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.10860.jpg",
            "data": {
                "categories": [
                    "#multilingual",
                    "#science",
                    "#dataset",
                    "#optimization",
                    "#interpretability",
                    "#training",
                    "#data"
                ],
                "emoji": "üó£Ô∏è",
                "ru": {
                    "title": "–ü—Ä–æ–∑—Ä–∞—á–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ —É—Å—Ç–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞ —Å –ø–æ–º–æ—â—å—é –ò–ò",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–Ω–æ–≥–æ–º–µ—Ä–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ —É—Å—Ç–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞. –û–Ω–∞ –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –∏–Ω–∂–µ–Ω–µ—Ä–∏—é –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—é –¥–∞–Ω–Ω—ã—Ö –∏ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ–µ –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ. –ú–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–æ–∑—Ä–∞—á–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏ –∞–Ω–∞–ª–∏–∑ SHAP –¥–ª—è –æ–±—ä—è—Å–Ω–µ–Ω–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –≤—ã—Å–æ–∫—É—é –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç–µ–ª—å–Ω—É—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –Ω–∞ –Ω–æ–≤–æ–º –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞ —Å –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ –Ω–∞ –∫–∏—Ç–∞–π—Å–∫–∏–π."
                },
                "en": {
                    "title": "Enhancing Automated Interpreting Quality with Explainable AI",
                    "desc": "This paper introduces a multi-dimensional modeling framework aimed at improving the quality assessment of automated interpreting. It combines feature engineering, data augmentation, and explainable machine learning to enhance transparency and provide detailed feedback. The framework addresses issues like data scarcity and the need for clearer model predictions by using relevant features and Shapley Value analysis. The results show that this approach not only predicts interpreting quality effectively but also supports learners with valuable insights for self-improvement."
                },
                "zh": {
                    "title": "ÊèêÂçáËá™Âä®ÂåñÂè£ËØëË¥®ÈáèËØÑ‰º∞ÁöÑÈÄèÊòéÊÄß‰∏éÂèØÈù†ÊÄß",
                    "desc": "ËøôÁØáËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂ§öÁª¥Âª∫Ê®°Ê°ÜÊû∂ÔºåÊó®Âú®ÊèêÈ´òËá™Âä®ÂåñÂè£ËØëË¥®ÈáèËØÑ‰º∞ÁöÑÊïàÊûú„ÄÇËØ•Ê°ÜÊû∂ÁªìÂêà‰∫ÜÁâπÂæÅÂ∑•Á®ã„ÄÅÊï∞ÊçÆÂ¢ûÂº∫ÂíåÂèØËß£ÈáäÁöÑÊú∫Âô®Â≠¶‰π†ÔºåÂº∫Ë∞ÉÈÄèÊòéÊÄßÂíåËØ¶ÁªÜÁöÑËØäÊñ≠ÂèçÈ¶à„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºå‰ΩøÁî®BLEURTÂíåCometKiwiËØÑÂàÜ‰Ωú‰∏∫Âø†ÂÆûÂ∫¶ÁöÑÈ¢ÑÊµãÁâπÂæÅÔºå‰ª•Âèä‰∏éÊµÅÂà©Â∫¶Áõ∏ÂÖ≥ÁöÑÂÅúÈ°øÁâπÂæÅÔºåËÉΩÂ§üÊúâÊïàÊèêÂçáÊ®°ÂûãÁöÑÈ¢ÑÊµãÊÄßËÉΩ„ÄÇÈÄöËøáËøôÁßçÊñπÊ≥ïÔºåËÆ∫Êñá‰∏∫Â≠¶‰π†ËÄÖÊèê‰æõ‰∫ÜÊõ¥ÂèØÈù†ÁöÑÂèçÈ¶àÔºåÊîØÊåÅËá™ÊàëË∞ÉËäÇÂ≠¶‰π†ÁöÑ‰ºòÂäø„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.10576",
            "title": "HumanSense: From Multimodal Perception to Empathetic Context-Aware\n  Responses through Reasoning MLLMs",
            "url": "https://huggingface.co/papers/2508.10576",
            "abstract": "HumanSense is a benchmark for evaluating human-centered perception and interaction in Multimodal Large Language Models, focusing on multimodal context understanding and rational feedback through reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t While Multimodal Large Language Models (MLLMs) show immense promise for achieving truly human-like interactions, progress is hindered by the lack of fine-grained evaluation frameworks for human-centered scenarios, encompassing both the understanding of complex human intentions and the provision of empathetic, context-aware responses. Here we introduce HumanSense, a comprehensive benchmark designed to evaluate the human-centered perception and interaction capabilities of MLLMs, with a particular focus on deep understanding of extended multimodal contexts and the formulation of rational feedback. Our evaluation reveals that leading MLLMs still have considerable room for improvement, particularly for advanced interaction-oriented tasks. Supplementing visual input with audio and text information yields substantial improvements, and Omni-modal models show advantages on these tasks. Furthermore, we argue that appropriate feedback stems from a contextual analysis of the interlocutor's needs and emotions, with reasoning ability serving as the key to unlocking it. Accordingly, we employ a multi-stage, modality-progressive reinforcement learning to enhance the reasoning abilities of an Omni model, achieving substantial gains on evaluation results. Additionally, we observe that successful reasoning processes exhibit highly consistent thought patterns. By designing corresponding prompts, we also enhance the performance of non-reasoning models in a training-free manner. Project page: brightpinkhttps://digital-avatar.github.io/ai/HumanSense/",
            "score": 1,
            "issue_id": 5363,
            "pub_date": "2025-08-14",
            "pub_date_card": {
                "ru": "14 –∞–≤–≥—É—Å—Ç–∞",
                "en": "August 14",
                "zh": "8Êúà14Êó•"
            },
            "hash": "94390c2120c8d69a",
            "authors": [
                "Zheng Qin",
                "Ruobing Zheng",
                "Yabing Wang",
                "Tianqi Li",
                "Yi Yuan",
                "Jingdong Chen",
                "Le Wang"
            ],
            "affiliations": [
                "Ant Group",
                "National Key Laboratory of Human-Machine Hybrid Augmented Intelligence, National Engineering Research Center for Visual Information and Applications, Institute of Artificial Intelligence and Robotics, Xian Jiaotong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.10576.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#benchmark",
                    "#interpretability",
                    "#reasoning",
                    "#rl",
                    "#multimodal"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "–û—Ü–µ–Ω–∫–∞ —á–µ–ª–æ–≤–µ–∫–æ–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –ò–ò-–º–æ–¥–µ–ª—è—Ö",
                    "desc": "HumanSense - —ç—Ç–æ –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è –∏ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è, –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –Ω–∞ —á–µ–ª–æ–≤–µ–∫–∞, –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (MLLM). –û–Ω —Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è –Ω–∞ –≥–ª—É–±–æ–∫–æ–º –ø–æ–Ω–∏–º–∞–Ω–∏–∏ —Å–ª–æ–∂–Ω—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤ –∏ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–∏ —Ä–∞—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–π –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –≤–µ–¥—É—â–∏–µ MLLM –≤—Å–µ –µ—â–µ –∏–º–µ—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è, –æ—Å–æ–±–µ–Ω–Ω–æ –≤ –∑–∞–¥–∞—á–∞—Ö –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–≥–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è. –ê–≤—Ç–æ—Ä—ã –ø—Ä–∏–º–µ–Ω—è—é—Ç –º–Ω–æ–≥–æ—ç—Ç–∞–ø–Ω–æ–µ, –º–æ–¥–∞–ª—å–Ω–æ-–ø—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é –æ–º–Ω–∏-–º–æ–¥–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏, —á—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ–º—É –ø–æ–≤—ã—à–µ–Ω–∏—é —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ—Ü–µ–Ω–∫–∏."
                },
                "en": {
                    "title": "Enhancing Human-Centered Interaction in MLLMs with HumanSense",
                    "desc": "HumanSense is a new benchmark aimed at assessing how well Multimodal Large Language Models (MLLMs) understand and interact in human-centered scenarios. It focuses on evaluating the models' ability to comprehend complex human intentions and provide empathetic responses based on multimodal inputs like text, audio, and visual data. The research shows that while current MLLMs have made progress, they still need improvement in advanced interaction tasks, especially when it comes to reasoning and contextual understanding. By using a multi-stage reinforcement learning approach, the study enhances the reasoning capabilities of these models, leading to better performance in understanding and responding to human needs and emotions."
                },
                "zh": {
                    "title": "ÊèêÂçá‰∫∫Êú∫‰∫§‰∫íÁöÑÂ§öÊ®°ÊÄÅÂü∫ÂáÜ",
                    "desc": "HumanSenseÊòØ‰∏Ä‰∏™Âü∫ÂáÜÔºåÁî®‰∫éËØÑ‰º∞Â§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâÂú®‰ª•‰∫∫‰∏∫‰∏≠ÂøÉÁöÑÊÑüÁü•Âíå‰∫§‰∫íËÉΩÂäõ„ÄÇËØ•Âü∫ÂáÜÁâπÂà´ÂÖ≥Ê≥®ÂØπÂ§çÊùÇ‰∫∫Á±ªÊÑèÂõæÁöÑÁêÜËß£ÂíåÊèê‰æõÂØåÊúâÂêåÁêÜÂøÉÁöÑ„ÄÅ‰∏ä‰∏ãÊñáÁõ∏ÂÖ≥ÁöÑÂèçÈ¶à„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÂ∞ΩÁÆ°È¢ÜÂÖàÁöÑMLLMsÂú®Ëøô‰∫õ‰ªªÂä°‰∏ä‰ªçÊúâÂæàÂ§ßÁöÑÊîπËøõÁ©∫Èó¥Ôºå‰ΩÜÈÄöËøáÁªìÂêàËßÜËßâ„ÄÅÈü≥È¢ëÂíåÊñáÊú¨‰ø°ÊÅØÔºåÂèØ‰ª•ÊòæËëóÊèêÂçáÂÖ∂Ë°®Áé∞„ÄÇÊàë‰ª¨ÈááÁî®Â§öÈò∂ÊÆµÁöÑÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ïÔºåÂ¢ûÂº∫Ê®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõÔºå‰ªéËÄåÂú®ËØÑ‰º∞ÁªìÊûú‰∏äÂèñÂæó‰∫ÜÊòæËëóÁöÑËøõÂ±ï„ÄÇ"
                }
            }
        }
    ],
    "link_prev": "2025-08-14.html",
    "link_next": "2025-08-18.html",
    "link_month": "2025-08.html",
    "short_date_prev": {
        "ru": "14.08",
        "en": "08/14",
        "zh": "8Êúà14Êó•"
    },
    "short_date_next": {
        "ru": "18.08",
        "en": "08/18",
        "zh": "8Êúà18Êó•"
    },
    "categories": {
        "#dataset": 2,
        "#data": 2,
        "#benchmark": 3,
        "#agents": 0,
        "#cv": 0,
        "#rl": 2,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 2,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 2,
        "#reasoning": 3,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 2,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 0
    }
}