{
    "date": {
        "ru": "24 Ğ¸ÑĞ»Ñ",
        "en": "July 24",
        "zh": "7æœˆ24æ—¥"
    },
    "time_utc": "2025-07-24 09:17",
    "weekday": 3,
    "issue_id": 4989,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2507.16863",
            "title": "Pixels, Patterns, but No Poetry: To See The World like Humans",
            "url": "https://huggingface.co/papers/2507.16863",
            "abstract": "The Turing Eye Test evaluates MLLMs' perceptual abilities through synthetic images, revealing that vision tower generalization is a significant gap compared to human perception.  \t\t\t\t\tAI-generated summary \t\t\t\t Achieving human-like perception and reasoning in Multimodal Large Language Models (MLLMs) remains a central challenge in artificial intelligence. While recent research has primarily focused on enhancing reasoning capabilities in MLLMs, a fundamental question persists: Can Multimodal Large Language Models truly perceive the world as humans do? This paper shifts focus from reasoning to perception. Rather than constructing benchmarks specifically for reasoning, we introduce the Turing Eye Test (TET), a challenging perception-oriented benchmark comprising four diagnostic tasks that evaluate MLLMs' performance on synthetic images that humans process intuitively. Our findings reveal that state-of-the-art MLLMs exhibit catastrophic failures on our perceptual tasks trivial for humans. Both in-context learning and training on language backbone-effective for previous benchmarks-fail to improve performance on our tasks, while fine-tuning the vision tower enables rapid adaptation, suggesting that our benchmark poses challenges for vision tower generalization rather than for the knowledge and reasoning capabilities of the language backbone-a key gap between current MLLMs and human perception. We release a representative subset of TET tasks in this version, and will introduce more diverse tasks and methods to enhance visual generalization in future work.",
            "score": 25,
            "issue_id": 4985,
            "pub_date": "2025-07-21",
            "pub_date_card": {
                "ru": "21 Ğ¸ÑĞ»Ñ",
                "en": "July 21",
                "zh": "7æœˆ21æ—¥"
            },
            "hash": "26ca329617f34c34",
            "authors": [
                "Hongcheng Gao",
                "Zihao Huang",
                "Lin Xu",
                "Jingyi Tang",
                "Xinhao Li",
                "Yue Liu",
                "Haoyang Li",
                "Taihang Hu",
                "Minhua Lin",
                "Xinlong Yang",
                "Ge Wu",
                "Balong Bi",
                "Hongyu Chen",
                "Wentao Zhang"
            ],
            "affiliations": [
                "BJTU",
                "BUPT",
                "Nanjing University",
                "Nankai University",
                "National University of Singapore",
                "Peking University",
                "The Pennsylvania State University",
                "University of Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.16863.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#benchmark",
                    "#multimodal",
                    "#reasoning",
                    "#perception",
                    "#cv"
                ],
                "emoji": "ğŸ‘ï¸",
                "ru": {
                    "title": "Ğ¢ĞµÑÑ‚ Ğ¢ÑŒÑÑ€Ğ¸Ğ½Ğ³Ğ° Ğ´Ğ»Ñ Ğ·Ñ€ĞµĞ½Ğ¸Ñ: Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ¿ĞµÑ€Ñ†ĞµĞ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ˜Ğ˜",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ‚ĞµÑÑ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿ĞµÑ€Ñ†ĞµĞ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) - Turing Eye Test (TET). TET Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ²Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸ĞµĞ¼ MLLM Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ MLLM Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ĞºĞ°Ñ‚Ğ°ÑÑ‚Ñ€Ğ¾Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ±Ğ¾Ğ¸ Ğ½Ğ° Ğ¿ĞµÑ€Ñ†ĞµĞ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ‚Ñ€Ğ¸Ğ²Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ»ÑĞ´ĞµĞ¹. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¾ÑĞ½Ğ¾Ğ²Ñ‹ Ğ½Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹, Ğ² Ñ‚Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ĞºĞ°Ğº Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ‡Ğ°ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ."
                },
                "en": {
                    "title": "Bridging the Perception Gap in MLLMs with the Turing Eye Test",
                    "desc": "This paper introduces the Turing Eye Test (TET), a new benchmark designed to evaluate the perceptual abilities of Multimodal Large Language Models (MLLMs) using synthetic images. The study highlights a significant gap in vision tower generalization, where MLLMs struggle with tasks that humans find easy, despite advancements in reasoning capabilities. The results indicate that traditional methods like in-context learning do not enhance performance on perceptual tasks, while fine-tuning the vision tower shows promise for improvement. This research emphasizes the need to focus on perception in MLLMs to bridge the gap between AI and human-like understanding of visual information."
                },
                "zh": {
                    "title": "å›¾çµçœ¼ç›æµ‹è¯•ï¼šæ­ç¤ºMLLMsæ„ŸçŸ¥èƒ½åŠ›çš„å·®è·",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨æ„ŸçŸ¥èƒ½åŠ›æ–¹é¢çš„ä¸è¶³ï¼Œæå‡ºäº†å›¾çµçœ¼ç›æµ‹è¯•ï¼ˆTETï¼‰ä½œä¸ºä¸€ç§æ–°çš„è¯„ä¼°åŸºå‡†ã€‚è¯¥æµ‹è¯•é€šè¿‡å››ä¸ªè¯Šæ–­ä»»åŠ¡ï¼Œè¯„ä¼°MLLMsåœ¨å¤„ç†åˆæˆå›¾åƒæ—¶çš„è¡¨ç°ï¼Œå‘ç°å½“å‰æœ€å…ˆè¿›çš„MLLMsåœ¨è¿™äº›ä»»åŠ¡ä¸Šè¡¨ç°ä¸ä½³ï¼Œè¿œä¸åŠäººç±»ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå°½ç®¡åœ¨æ¨ç†èƒ½åŠ›ä¸Šæœ‰æ‰€æå‡ï¼Œä½†åœ¨è§†è§‰æ„ŸçŸ¥æ–¹é¢ä»å­˜åœ¨æ˜¾è‘—å·®è·ã€‚æœªæ¥çš„å·¥ä½œå°†è‡´åŠ›äºå¼•å…¥æ›´å¤šå¤šæ ·åŒ–çš„ä»»åŠ¡ï¼Œä»¥æé«˜è§†è§‰æ³›åŒ–èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.17744",
            "title": "Yume: An Interactive World Generation Model",
            "url": "https://huggingface.co/papers/2507.17744",
            "abstract": "A framework for generating and exploring interactive video worlds from images using Masked Video Diffusion Transformer, Anti-Artifact Mechanism, Time Travel Sampling, and model acceleration techniques.  \t\t\t\t\tAI-generated summary \t\t\t\t Yume aims to use images, text, or videos to create an interactive, realistic, and dynamic world, which allows exploration and control using peripheral devices or neural signals. In this report, we present a preview version of \\method, which creates a dynamic world from an input image and allows exploration of the world using keyboard actions. To achieve this high-fidelity and interactive video world generation, we introduce a well-designed framework, which consists of four main components, including camera motion quantization, video generation architecture, advanced sampler, and model acceleration. First, we quantize camera motions for stable training and user-friendly interaction using keyboard inputs. Then, we introduce the Masked Video Diffusion Transformer~(MVDT) with a memory module for infinite video generation in an autoregressive manner. After that, training-free Anti-Artifact Mechanism (AAM) and Time Travel Sampling based on Stochastic Differential Equations (TTS-SDE) are introduced to the sampler for better visual quality and more precise control. Moreover, we investigate model acceleration by synergistic optimization of adversarial distillation and caching mechanisms. We use the high-quality world exploration dataset \\sekai to train \\method, and it achieves remarkable results in diverse scenes and applications. All data, codebase, and model weights are available on https://github.com/stdstu12/YUME. Yume will update monthly to achieve its original goal. Project page: https://stdstu12.github.io/YUME-Project/.",
            "score": 23,
            "issue_id": 4986,
            "pub_date": "2025-07-23",
            "pub_date_card": {
                "ru": "23 Ğ¸ÑĞ»Ñ",
                "en": "July 23",
                "zh": "7æœˆ23æ—¥"
            },
            "hash": "367e31a517aa16f3",
            "authors": [
                "Xiaofeng Mao",
                "Shaoheng Lin",
                "Zhen Li",
                "Chuanhao Li",
                "Wenshuo Peng",
                "Tong He",
                "Jiangmiao Pang",
                "Mingmin Chi",
                "Yu Qiao",
                "Kaipeng Zhang"
            ],
            "affiliations": [
                "Fudan University",
                "Shanghai AI Laboratory",
                "Shanghai Innovation Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.17744.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#architecture",
                    "#open_source",
                    "#diffusion",
                    "#cv",
                    "#video"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¸Ñ€Ğ¾Ğ² Ğ¸Ğ· Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Yume Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¸Ñ€Ğ¾Ğ² Ğ¸Ğ· Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Masked Video Diffusion Transformer Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ÑƒÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ğ¾Ğ² Ğ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºÑƒ Ñ Ğ¿ÑƒÑ‚ĞµÑˆĞµÑÑ‚Ğ²Ğ¸ĞµĞ¼ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ ĞºĞ°Ğ¼ĞµÑ€Ñ‹, Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Yume Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¼Ğ¸Ñ€ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ĞºĞ»Ğ°Ğ²Ğ¸Ğ°Ñ‚ÑƒÑ€Ñ‹ Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ¿ĞµÑ‡Ğ°Ñ‚Ğ»ÑÑÑ‰Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ…."
                },
                "en": {
                    "title": "Creating Interactive Video Worlds with Yume",
                    "desc": "This paper presents a framework called Yume that generates interactive video worlds from images, enabling users to explore these worlds using keyboard inputs or neural signals. The framework incorporates a Masked Video Diffusion Transformer (MVDT) for autoregressive video generation, along with an Anti-Artifact Mechanism (AAM) and Time Travel Sampling (TTS-SDE) to enhance visual quality and control. It also includes techniques for camera motion quantization and model acceleration to improve user interaction and performance. The system is trained on a high-quality dataset and aims to provide a dynamic and realistic experience in various applications."
                },
                "zh": {
                    "title": "ç”Ÿæˆäº’åŠ¨è§†é¢‘ä¸–ç•Œçš„æ–°æ¡†æ¶",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œåˆ©ç”¨å›¾åƒç”Ÿæˆå’Œæ¢ç´¢äº’åŠ¨è§†é¢‘ä¸–ç•Œã€‚è¯¥æ¡†æ¶åŒ…æ‹¬å››ä¸ªä¸»è¦ç»„ä»¶ï¼šç›¸æœºè¿åŠ¨é‡åŒ–ã€è§†é¢‘ç”Ÿæˆæ¶æ„ã€å…ˆè¿›çš„é‡‡æ ·å™¨å’Œæ¨¡å‹åŠ é€ŸæŠ€æœ¯ã€‚é€šè¿‡ä½¿ç”¨Masked Video Diffusion Transformerï¼ˆMVDTï¼‰å’Œåä¼ªå½±æœºåˆ¶ï¼Œç³»ç»Ÿèƒ½å¤Ÿç”Ÿæˆé«˜ä¿çœŸåº¦çš„åŠ¨æ€è§†é¢‘ä¸–ç•Œï¼Œå¹¶å…è®¸ç”¨æˆ·é€šè¿‡é”®ç›˜æ“ä½œè¿›è¡Œæ¢ç´¢ã€‚è¯¥æ–¹æ³•åœ¨å¤šç§åœºæ™¯å’Œåº”ç”¨ä¸­è¡¨ç°å‡ºè‰²ï¼Œå±•ç¤ºäº†ç”Ÿæˆäº’åŠ¨è§†é¢‘ä¸–ç•Œçš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.17202",
            "title": "DesignLab: Designing Slides Through Iterative Detection and Correction",
            "url": "https://huggingface.co/papers/2507.17202",
            "abstract": "DesignLab uses fine-tuned large language models to iteratively improve presentation slides through a design reviewer and contributor system, outperforming existing tools.  \t\t\t\t\tAI-generated summary \t\t\t\t Designing high-quality presentation slides can be challenging for non-experts due to the complexity involved in navigating various design choices. Numerous automated tools can suggest layouts and color schemes, yet often lack the ability to refine their own output, which is a key aspect in real-world workflows. We propose DesignLab, which separates the design process into two roles, the design reviewer, who identifies design-related issues, and the design contributor who corrects them. This decomposition enables an iterative loop where the reviewer continuously detects issues and the contributor corrects them, allowing a draft to be further polished with each iteration, reaching qualities that were unattainable. We fine-tune large language models for these roles and simulate intermediate drafts by introducing controlled perturbations, enabling the design reviewer learn design errors and the contributor learn how to fix them. Our experiments show that DesignLab outperforms existing design-generation methods, including a commercial tool, by embracing the iterative nature of designing which can result in polished, professional slides.",
            "score": 23,
            "issue_id": 4986,
            "pub_date": "2025-07-23",
            "pub_date_card": {
                "ru": "23 Ğ¸ÑĞ»Ñ",
                "en": "July 23",
                "zh": "7æœˆ23æ—¥"
            },
            "hash": "1102d80628fa748f",
            "authors": [
                "Jooyeol Yun",
                "Heng Wang",
                "Yotaro Shimose",
                "Jaegul Choo",
                "Shingo Takamatsu"
            ],
            "affiliations": [
                "Korea Advanced Institute of Science and Technology (KAIST)",
                "Sony Group Corporation"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.17202.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#multimodal"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Ğ˜Ğ˜-Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ½Ğ¸Ğº Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ·ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¹",
                    "desc": "DesignLab - ÑÑ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ·ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ»Ğ°Ğ¹Ğ´Ğ¾Ğ². ĞĞ½Ğ° Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğ° Ğ½Ğ° Ğ´Ğ²Ğµ Ñ€Ğ¾Ğ»Ğ¸: Ñ€ĞµÑ†ĞµĞ½Ğ·ĞµĞ½Ñ‚ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğ°, Ğ²Ñ‹ÑĞ²Ğ»ÑÑÑ‰Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹, Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¸Ğ±ÑŒÑÑ‚Ğ¾Ñ€ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğ°, Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‰Ğ¸Ğ¹ Ğ¸Ñ…. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ‚ÑŒ Ñ‡ĞµÑ€Ğ½Ğ¾Ğ²Ğ¸ĞºĞ¸, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°, Ğ½ĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ DesignLab Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğ°, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Iterative Design Excellence with AI",
                    "desc": "DesignLab is a system that enhances the process of creating presentation slides by using fine-tuned large language models. It divides the design task into two roles: a design reviewer who identifies problems and a design contributor who makes corrections. This iterative approach allows for continuous improvement of the slides, leading to higher quality outcomes. Experiments demonstrate that DesignLab surpasses existing design tools by effectively refining designs through repeated feedback and adjustments."
                },
                "zh": {
                    "title": "DesignLabï¼šè¿­ä»£æå‡æ¼”ç¤ºæ–‡ç¨¿çš„æ™ºèƒ½å·¥å…·",
                    "desc": "DesignLab æ˜¯ä¸€ä¸ªåˆ©ç”¨å¾®è°ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡è®¾è®¡å®¡æŸ¥è€…å’Œè´¡çŒ®è€…ç³»ç»Ÿæ¥è¿­ä»£æ”¹è¿›æ¼”ç¤ºæ–‡ç¨¿çš„å·¥å…·ã€‚è¯¥ç³»ç»Ÿå°†è®¾è®¡è¿‡ç¨‹åˆ†ä¸ºä¸¤ä¸ªè§’è‰²ï¼Œå®¡æŸ¥è€…è¯†åˆ«è®¾è®¡é—®é¢˜ï¼Œè´¡çŒ®è€…è¿›è¡Œä¿®æ­£ï¼Œä»è€Œå½¢æˆä¸€ä¸ªè¿­ä»£å¾ªç¯ã€‚æ¯æ¬¡è¿­ä»£ä¸­ï¼Œå®¡æŸ¥è€…ä¸æ–­å‘ç°é—®é¢˜ï¼Œè´¡çŒ®è€…åˆ™è¿›è¡Œä¿®æ­£ï¼Œä½¿å¾—è‰ç¨¿åœ¨æ¯æ¬¡è¿­ä»£ä¸­éƒ½èƒ½å¾—åˆ°è¿›ä¸€æ­¥çš„å®Œå–„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDesignLab åœ¨è®¾è®¡ç”Ÿæˆæ–¹é¢çš„è¡¨ç°ä¼˜äºç°æœ‰å·¥å…·ï¼Œèƒ½å¤Ÿç”Ÿæˆæ›´é«˜è´¨é‡çš„ä¸“ä¸šæ¼”ç¤ºæ–‡ç¨¿ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.17512",
            "title": "Can One Domain Help Others? A Data-Centric Study on Multi-Domain\n  Reasoning via Reinforcement Learning",
            "url": "https://huggingface.co/papers/2507.17512",
            "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a powerful paradigm for enhancing the reasoning capabilities of LLMs. Existing research has predominantly concentrated on isolated reasoning domains such as mathematical problem-solving, coding tasks, or logical reasoning. However, real world reasoning scenarios inherently demand an integrated application of multiple cognitive skills. Despite this, the interplay among these reasoning skills under reinforcement learning remains poorly understood. To bridge this gap, we present a systematic investigation of multi-domain reasoning within the RLVR framework, explicitly focusing on three primary domains: mathematical reasoning, code generation, and logical puzzle solving. We conduct a comprehensive study comprising four key components: (1) Leveraging the GRPO algorithm and the Qwen-2.5-7B model family, our study thoroughly evaluates the models' in-domain improvements and cross-domain generalization capabilities when trained on single-domain datasets. (2) Additionally, we examine the intricate interactions including mutual enhancements and conflicts that emerge during combined cross-domain training. (3) To further understand the influence of SFT on RL, we also analyze and compare performance differences between base and instruct models under identical RL configurations. (4) Furthermore, we delve into critical RL training details, systematically exploring the impacts of curriculum learning strategies, variations in reward design, and language-specific factors. Through extensive experiments, our results offer significant insights into the dynamics governing domain interactions, revealing key factors influencing both specialized and generalizable reasoning performance. These findings provide valuable guidance for optimizing RL methodologies to foster comprehensive, multi-domain reasoning capabilities in LLMs.",
            "score": 20,
            "issue_id": 4983,
            "pub_date": "2025-07-23",
            "pub_date_card": {
                "ru": "23 Ğ¸ÑĞ»Ñ",
                "en": "July 23",
                "zh": "7æœˆ23æ—¥"
            },
            "hash": "d637b6ffc2166e10",
            "authors": [
                "Yu Li",
                "Zhuoshi Pan",
                "Honglin Lin",
                "Mengyuan Sun",
                "Conghui He",
                "Lijun Wu"
            ],
            "affiliations": [
                "OpenDataLab",
                "Shanghai Artificial Intelligence Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.17512.jpg",
            "data": {
                "categories": [
                    "#math",
                    "#training",
                    "#reasoning",
                    "#rl",
                    "#optimization"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞœĞ½Ğ¾Ğ³Ğ¾Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ: Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°Ñ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ˜Ğ˜ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°Ğ¼Ğ¸ (RLVR) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ… Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ ĞºĞ¾Ğ´Ğ° Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸ĞµĞ¼ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² Ñ€Ğ°Ğ¼ĞºĞ°Ñ… RLVR. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ²ĞºĞ»ÑÑ‡Ğ°Ğ»Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºÑƒ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¹ Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ¾Ğ² Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ğ¼Ğ¸, Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸, ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ°ÑĞ¿ĞµĞºÑ‚Ğ¾Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ñ†ĞµĞ½Ğ½Ñ‹Ğµ insights Ğ¾ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ¾Ğ² Ğ¸ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ°Ñ…, Ğ²Ğ»Ğ¸ÑÑÑ‰Ğ¸Ñ… Ğ½Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ…."
                },
                "en": {
                    "title": "Enhancing Multi-Domain Reasoning in LLMs with RLVR",
                    "desc": "This paper introduces Reinforcement Learning with Verifiable Rewards (RLVR) as a method to improve the reasoning abilities of large language models (LLMs) across multiple domains. It highlights the need for integrated reasoning skills in real-world scenarios, moving beyond isolated tasks like math or coding. The study investigates how different reasoning domains interact during training, using the GRPO algorithm and the Qwen-2.5-7B model family to assess improvements and generalization. Key findings reveal important dynamics in multi-domain training, offering insights for enhancing RL techniques to develop more versatile reasoning capabilities in LLMs."
                },
                "zh": {
                    "title": "å¤šé¢†åŸŸæ¨ç†çš„å¼ºåŒ–å­¦ä¹ æ–°æ¢ç´¢",
                    "desc": "å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰æ˜¯ä¸€ç§å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¨ç†èƒ½åŠ›çš„æœ‰æ•ˆæ–¹æ³•ã€‚ç°æœ‰ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨æ•°å­¦é—®é¢˜è§£å†³ã€ç¼–ç ä»»åŠ¡å’Œé€»è¾‘æ¨ç†ç­‰å­¤ç«‹çš„æ¨ç†é¢†åŸŸã€‚ç„¶è€Œï¼Œç°å®ä¸–ç•Œçš„æ¨ç†åœºæ™¯éœ€è¦å¤šç§è®¤çŸ¥æŠ€èƒ½çš„ç»¼åˆåº”ç”¨ã€‚æœ¬æ–‡ç³»ç»Ÿç ”ç©¶äº†RLVRæ¡†æ¶ä¸‹çš„å¤šé¢†åŸŸæ¨ç†ï¼Œé‡ç‚¹åˆ†ææ•°å­¦æ¨ç†ã€ä»£ç ç”Ÿæˆå’Œé€»è¾‘éš¾é¢˜è§£å†³ä¹‹é—´çš„ç›¸äº’ä½œç”¨ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.16331",
            "title": "Re:Form -- Reducing Human Priors in Scalable Formal Software\n  Verification with RL in LLMs: A Preliminary Study on Dafny",
            "url": "https://huggingface.co/papers/2507.16331",
            "abstract": "Formal language-based reasoning and automatic verification improve the reliability and scalability of Large Language Models for generating verifiable programs.  \t\t\t\t\tAI-generated summary \t\t\t\t Existing informal language-based (e.g., human language) Large Language Models (LLMs) trained with Reinforcement Learning (RL) face a significant challenge: their verification processes, which provide crucial training signals, are neither reliable nor scalable. In fact, the prevalent large proprietary models could hardly generate verifiable programs. A promising yet largely uncharted alternative is formal language-based reasoning. Grounding LLMs in rigorous formal systems where generative models operate in formal language spaces (e.g., Dafny) enables the automatic and mathematically provable verification of their reasoning processes and outcomes. This capability is pivotal for achieving large-scale, reliable formal software verification. It is a common practice to employ human-annotated chain-of-thought and other human priors to induce the reasoning and coding capabilities of LLMs. Unfortunately, it becomes unacceptably all-consuming to provide such priors for supervising complex programming tasks. In this work, we systematically explore ways to reduce human priors with the formal language, Dafny, as the main environment for our pilot study. Our pipeline mainly relies on introducing an automatic and scalable data curation pipeline, and careful RL designs integrated with feedback from the formal language verifier. We introduce DafnyComp, a benchmark of compositional formal programs with auto-formalized specifications for specification reasoning. Our supervised fine-tuning (SFT) stage enables even small models (e.g., 0.5B) to generate syntactically valid and verifiable Dafny code, surpassing proprietary models. RL with regularization further improves performance, achieving stronger generalization to out-of-domain tasks and outperforming all strong baselines on the challenging DafnyComp benchmark.",
            "score": 12,
            "issue_id": 4987,
            "pub_date": "2025-07-22",
            "pub_date_card": {
                "ru": "22 Ğ¸ÑĞ»Ñ",
                "en": "July 22",
                "zh": "7æœˆ22æ—¥"
            },
            "hash": "a6439c608d5df566",
            "authors": [
                "Chuanhao Yan",
                "Fengdi Che",
                "Xuhan Huang",
                "Xu Xu",
                "Xin Li",
                "Yizhi Li",
                "Xingwei Qu",
                "Jingzhe Shi",
                "Zhuangzhuang He",
                "Chenghua Lin",
                "Yaodong Yang",
                "Binhang Yuan",
                "Hang Zhao",
                "Yu Qiao",
                "Bowen Zhou",
                "Jie Fu"
            ],
            "affiliations": [
                "Shanghai AI Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.16331.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#dataset",
                    "#reasoning",
                    "#training",
                    "#optimization",
                    "#small_models",
                    "#rlhf",
                    "#benchmark"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ÑÑ‚ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ˜Ğ˜ Ğ² Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ² Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ DafnyComp - Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼. ĞĞ½Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (0.5B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²) Ğ¿Ğ¾ÑĞ»Ğµ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ¸Ğ½Ñ‚Ğ°ĞºÑĞ¸Ñ‡ĞµÑĞºĞ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ñ‹Ğ¹ Ğ¸ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ ĞºĞ¾Ğ´ Ğ½Ğ° ÑĞ·Ñ‹ĞºĞµ Dafny. ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL) Ñ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¸Ñ… Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ."
                },
                "en": {
                    "title": "Enhancing LLMs with Formal Language for Reliable Program Verification",
                    "desc": "This paper discusses how using formal language-based reasoning can enhance the reliability and scalability of Large Language Models (LLMs) in generating verifiable programs. Traditional LLMs, which rely on informal language and Reinforcement Learning, struggle with verification processes that are essential for training. The authors propose a method that utilizes the formal language Dafny, allowing for automatic and mathematically provable verification of the models' outputs. Their approach includes a new benchmark called DafnyComp and demonstrates that even smaller models can produce valid and verifiable code, outperforming existing proprietary models."
                },
                "zh": {
                    "title": "å½¢å¼è¯­è¨€åŠ©åŠ›å¤§å‹è¯­è¨€æ¨¡å‹çš„å¯é æ€§ä¸å¯æ‰©å±•æ€§",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†åŸºäºå½¢å¼è¯­è¨€çš„æ¨ç†å’Œè‡ªåŠ¨éªŒè¯å¦‚ä½•æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç”Ÿæˆå¯éªŒè¯ç¨‹åºçš„å¯é æ€§å’Œå¯æ‰©å±•æ€§ã€‚ç°æœ‰çš„åŸºäºéæ­£å¼è¯­è¨€çš„LLMsåœ¨éªŒè¯è¿‡ç¨‹ä¸­é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼Œå¯¼è‡´å…¶ç”Ÿæˆçš„ç¨‹åºéš¾ä»¥éªŒè¯ã€‚é€šè¿‡å°†LLMsåµŒå…¥ä¸¥æ ¼çš„å½¢å¼ç³»ç»Ÿï¼ˆå¦‚Dafnyï¼‰ï¼Œå¯ä»¥å®ç°å…¶æ¨ç†è¿‡ç¨‹å’Œç»“æœçš„è‡ªåŠ¨åŒ–å’Œæ•°å­¦å¯è¯æ˜çš„éªŒè¯ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œä½¿ç”¨Dafnyä½œä¸ºä¸»è¦ç¯å¢ƒï¼Œç»“åˆè‡ªåŠ¨åŒ–æ•°æ®æ•´ç†å’Œå¼ºåŒ–å­¦ä¹ è®¾è®¡ï¼Œå¯ä»¥æ˜¾è‘—å‡å°‘å¯¹äººç±»å…ˆéªŒçŸ¥è¯†çš„ä¾èµ–ï¼Œæå‡æ¨¡å‹åœ¨å¤æ‚ç¼–ç¨‹ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.17745",
            "title": "Ultra3D: Efficient and High-Fidelity 3D Generation with Part Attention",
            "url": "https://huggingface.co/papers/2507.17745",
            "abstract": "Ultra3D uses VecSet and Part Attention to accelerate 3D voxel generation while maintaining high quality and resolution.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in sparse voxel representations have significantly improved the quality of 3D content generation, enabling high-resolution modeling with fine-grained geometry. However, existing frameworks suffer from severe computational inefficiencies due to the quadratic complexity of attention mechanisms in their two-stage diffusion pipelines. In this work, we propose Ultra3D, an efficient 3D generation framework that significantly accelerates sparse voxel modeling without compromising quality. Our method leverages the compact VecSet representation to efficiently generate a coarse object layout in the first stage, reducing token count and accelerating voxel coordinate prediction. To refine per-voxel latent features in the second stage, we introduce Part Attention, a geometry-aware localized attention mechanism that restricts attention computation within semantically consistent part regions. This design preserves structural continuity while avoiding unnecessary global attention, achieving up to 6.7x speed-up in latent generation. To support this mechanism, we construct a scalable part annotation pipeline that converts raw meshes into part-labeled sparse voxels. Extensive experiments demonstrate that Ultra3D supports high-resolution 3D generation at 1024 resolution and achieves state-of-the-art performance in both visual fidelity and user preference.",
            "score": 6,
            "issue_id": 4984,
            "pub_date": "2025-07-23",
            "pub_date_card": {
                "ru": "23 Ğ¸ÑĞ»Ñ",
                "en": "July 23",
                "zh": "7æœˆ23æ—¥"
            },
            "hash": "73bfee6022eeade3",
            "authors": [
                "Yiwen Chen",
                "Zhihao Li",
                "Yikai Wang",
                "Hu Zhang",
                "Qin Li",
                "Chi Zhang",
                "Guosheng Lin"
            ],
            "affiliations": [
                "Math Magic",
                "Nanyang Technological University",
                "School of Artificial Intelligence, Beijing Normal University",
                "Tsinghua University",
                "Westlake University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.17745.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#3d"
                ],
                "emoji": "ğŸ§Š",
                "ru": {
                    "title": "Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… 3D-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ",
                    "desc": "Ultra3D - ÑÑ‚Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D-ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ VecSet Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Part Attention. VecSet Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒÑĞºĞ¾Ñ€Ğ¸Ñ‚ÑŒ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ³Ñ€ÑƒĞ±Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ĞºĞµÑ‚Ğ° Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ° Ğ½Ğ° Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ, ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°Ñ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². Part Attention Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑĞ¼Ğ¸ Ğ½Ğ° Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½ÑƒÑ Ñ†ĞµĞ»Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ´Ğ¾ 6,7 Ñ€Ğ°Ğ· Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸. Ultra3D Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ 3D-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ (1024^3 Ğ²Ğ¾ĞºÑĞµĞ»ĞµĞ¹) Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Accelerating 3D Voxel Generation with Ultra3D!",
                    "desc": "Ultra3D is a novel framework designed to enhance the efficiency of 3D voxel generation while ensuring high quality and resolution. It utilizes a compact VecSet representation to create a coarse object layout, which reduces the number of tokens and speeds up voxel coordinate prediction. Additionally, the framework introduces Part Attention, a localized attention mechanism that focuses on semantically consistent regions, thus improving computational efficiency and maintaining structural integrity. Experimental results show that Ultra3D can generate high-resolution 3D models at 1024 resolution with significant speed improvements and superior visual quality compared to existing methods."
                },
                "zh": {
                    "title": "Ultra3Dï¼šé«˜æ•ˆçš„3Dä½“ç´ ç”Ÿæˆæ–°æ–¹æ³•",
                    "desc": "Ultra3Dæ˜¯ä¸€ç§é«˜æ•ˆçš„3Dç”Ÿæˆæ¡†æ¶ï¼Œåˆ©ç”¨VecSetå’ŒPart AttentionåŠ é€Ÿç¨€ç–ä½“ç´ å»ºæ¨¡ï¼ŒåŒæ—¶ä¿æŒé«˜è´¨é‡å’Œé«˜åˆ†è¾¨ç‡ã€‚è¯¥æ–¹æ³•åœ¨ç¬¬ä¸€é˜¶æ®µä½¿ç”¨ç´§å‡‘çš„VecSetè¡¨ç¤ºç”Ÿæˆç²—ç•¥çš„ç‰©ä½“å¸ƒå±€ï¼Œä»è€Œå‡å°‘äº†æ ‡è®°æ•°é‡ï¼ŒåŠ å¿«äº†ä½“ç´ åæ ‡é¢„æµ‹ã€‚ç¬¬äºŒé˜¶æ®µå¼•å…¥äº†Part Attentionï¼Œè¿™æ˜¯ä¸€ç§å‡ ä½•æ„ŸçŸ¥çš„å±€éƒ¨æ³¨æ„åŠ›æœºåˆ¶ï¼Œé™åˆ¶äº†æ³¨æ„åŠ›è®¡ç®—åœ¨è¯­ä¹‰ä¸€è‡´çš„éƒ¨åˆ†åŒºåŸŸå†…ã€‚é€šè¿‡è¿™ç§è®¾è®¡ï¼ŒUltra3Dåœ¨æ½œåœ¨ç‰¹å¾ç”Ÿæˆä¸Šå®ç°äº†æœ€é«˜6.7å€çš„åŠ é€Ÿï¼ŒåŒæ—¶æ”¯æŒ1024åˆ†è¾¨ç‡çš„é«˜è´¨é‡3Dç”Ÿæˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.11465",
            "title": "Elevating 3D Models: High-Quality Texture and Geometry Refinement from a\n  Low-Quality Model",
            "url": "https://huggingface.co/papers/2507.11465",
            "abstract": "Elevate3D enhances both texture and geometry of low-quality 3D assets using HFS-SDEdit and monocular geometry predictors, achieving superior refinement quality.  \t\t\t\t\tAI-generated summary \t\t\t\t High-quality 3D assets are essential for various applications in computer graphics and 3D vision but remain scarce due to significant acquisition costs. To address this shortage, we introduce Elevate3D, a novel framework that transforms readily accessible low-quality 3D assets into higher quality. At the core of Elevate3D is HFS-SDEdit, a specialized texture enhancement method that significantly improves texture quality while preserving the appearance and geometry while fixing its degradations. Furthermore, Elevate3D operates in a view-by-view manner, alternating between texture and geometry refinement. Unlike previous methods that have largely overlooked geometry refinement, our framework leverages geometric cues from images refined with HFS-SDEdit by employing state-of-the-art monocular geometry predictors. This approach ensures detailed and accurate geometry that aligns seamlessly with the enhanced texture. Elevate3D outperforms recent competitors by achieving state-of-the-art quality in 3D model refinement, effectively addressing the scarcity of high-quality open-source 3D assets.",
            "score": 6,
            "issue_id": 4984,
            "pub_date": "2025-07-15",
            "pub_date_card": {
                "ru": "15 Ğ¸ÑĞ»Ñ",
                "en": "July 15",
                "zh": "7æœˆ15æ—¥"
            },
            "hash": "3e9488a149f4e7a7",
            "authors": [
                "Nuri Ryu",
                "Jiyun Won",
                "Jooeun Son",
                "Minsu Gong",
                "Joo-Haeng Lee",
                "Sunghyun Cho"
            ],
            "affiliations": [
                "POSTECH, South Korea",
                "Pebblous, South Korea"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.11465.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#open_source",
                    "#synthetic"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "ĞŸÑ€ĞµĞ²Ñ€Ğ°Ñ‰Ğ°ĞµĞ¼ Ğ½Ğ¸Ğ·ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ 3D-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² ÑˆĞµĞ´ĞµĞ²Ñ€Ñ‹",
                    "desc": "Elevate3D - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° 3D-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ HFS-SDEdit Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¸ĞºÑ‚Ğ¾Ñ€Ñ‹ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ Ñ„Ğ¾Ñ€Ğ¼Ñ‹ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾, ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€Ñ‹ Ğ¸ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ñ€Ğ°ĞºÑƒÑ€ÑĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Elevate3D Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¸ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ñ€ĞµÑˆĞ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ½ĞµÑ…Ğ²Ğ°Ñ‚ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… 3D-Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ²."
                },
                "en": {
                    "title": "Transforming Low-Quality 3D Assets into High-Quality Masterpieces",
                    "desc": "Elevate3D is a framework designed to improve the quality of low-resolution 3D assets by enhancing both their texture and geometry. It utilizes a method called HFS-SDEdit for texture enhancement, which maintains the original appearance while correcting any flaws. The framework also incorporates monocular geometry predictors to refine the 3D geometry, ensuring that it matches the improved textures. By alternating between texture and geometry refinement in a view-by-view manner, Elevate3D achieves superior results compared to existing methods, making high-quality 3D assets more accessible."
                },
                "zh": {
                    "title": "Elevate3Dï¼šæå‡ä½è´¨é‡3Dèµ„äº§çš„è´¨é‡",
                    "desc": "Elevate3D æ˜¯ä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œæ—¨åœ¨å°†ä½è´¨é‡çš„ 3D èµ„äº§è½¬åŒ–ä¸ºé«˜è´¨é‡çš„ 3D æ¨¡å‹ã€‚å®ƒçš„æ ¸å¿ƒæ˜¯ HFS-SDEditï¼Œè¿™æ˜¯ä¸€ç§ä¸“é—¨çš„çº¹ç†å¢å¼ºæ–¹æ³•ï¼Œå¯ä»¥æ˜¾è‘—æé«˜çº¹ç†è´¨é‡ï¼ŒåŒæ—¶ä¿æŒå¤–è§‚å’Œå‡ ä½•å½¢çŠ¶ã€‚Elevate3D é‡‡ç”¨é€è§†å›¾çš„æ–¹å¼ï¼Œäº¤æ›¿è¿›è¡Œçº¹ç†å’Œå‡ ä½•çš„ç»†åŒ–ï¼Œç¡®ä¿ä¸¤è€…ä¹‹é—´çš„åè°ƒã€‚é€šè¿‡ä½¿ç”¨å…ˆè¿›çš„å•ç›®å‡ ä½•é¢„æµ‹å™¨ï¼ŒElevate3D åœ¨ 3D æ¨¡å‹ç»†åŒ–æ–¹é¢è¶…è¶Šäº†ç«äº‰å¯¹æ‰‹ï¼Œæä¾›äº†å“è¶Šçš„è´¨é‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.16880",
            "title": "Finding Dori: Memorization in Text-to-Image Diffusion Models Is Less\n  Local Than Assumed",
            "url": "https://huggingface.co/papers/2507.16880",
            "abstract": "Pruning-based defenses in text-to-image diffusion models are ineffective as minor adjustments to text embeddings can re-trigger data replication, necessitating methods that truly remove memorized content.  \t\t\t\t\tAI-generated summary \t\t\t\t Text-to-image diffusion models (DMs) have achieved remarkable success in image generation. However, concerns about data privacy and intellectual property remain due to their potential to inadvertently memorize and replicate training data. Recent mitigation efforts have focused on identifying and pruning weights responsible for triggering replication, based on the assumption that memorization can be localized. Our research assesses the robustness of these pruning-based approaches. We demonstrate that even after pruning, minor adjustments to text embeddings of input prompts are sufficient to re-trigger data replication, highlighting the fragility of these defenses. Furthermore, we challenge the fundamental assumption of memorization locality, by showing that replication can be triggered from diverse locations within the text embedding space, and follows different paths in the model. Our findings indicate that existing mitigation strategies are insufficient and underscore the need for methods that truly remove memorized content, rather than attempting to suppress its retrieval. As a first step in this direction, we introduce a novel adversarial fine-tuning method that iteratively searches for replication triggers and updates the model to increase robustness. Through our research, we provide fresh insights into the nature of memorization in text-to-image DMs and a foundation for building more trustworthy and compliant generative AI.",
            "score": 2,
            "issue_id": 4989,
            "pub_date": "2025-07-22",
            "pub_date_card": {
                "ru": "22 Ğ¸ÑĞ»Ñ",
                "en": "July 22",
                "zh": "7æœˆ22æ—¥"
            },
            "hash": "441dcefad865f158",
            "authors": [
                "Antoni Kowalczuk",
                "Dominik Hintersdorf",
                "Lukas Struppek",
                "Kristian Kersting",
                "Adam Dziedzic",
                "Franziska Boenisch"
            ],
            "affiliations": [
                "CISPA Helmholtz Center for Information Security",
                "Centre for Cognitive Science, Technical University of Darmstadt",
                "Computer Science Department, Technical University of Darmstadt",
                "German Research Center for Artificial Intelligence (DFKI)",
                "Hessian Center for AI (Hessian.AI)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.16880.jpg",
            "data": {
                "categories": [
                    "#security",
                    "#cv",
                    "#training",
                    "#hallucinations",
                    "#data"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ—Ğ°Ñ‰Ğ¸Ñ‚Ğ° Ğ¿Ñ€Ğ¸Ğ²Ğ°Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…: Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ñ‹ Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñ‹ Ğ¾Ñ‚ Ğ½ĞµĞ¶ĞµĞ»Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ Ğ½ĞµÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ¿Ğ¾ÑĞ»Ğµ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ Ğ²ĞµÑĞ¾Ğ², Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ° Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ, Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² Ğ¼Ğ¾Ğ³ÑƒÑ‚ ÑĞ½Ğ¾Ğ²Ğ° Ğ²Ñ‹Ğ·Ğ²Ğ°Ñ‚ÑŒ Ğ½ĞµĞ¶ĞµĞ»Ğ°Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ. Ğ­Ñ‚Ğ¾ ÑÑ‚Ğ°Ğ²Ğ¸Ñ‚ Ğ¿Ğ¾Ğ´ ÑĞ¾Ğ¼Ğ½ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¾ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ñ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¾ÑÑ‚ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ´Ğ¾Ğ²Ğ¾Ğ´ĞºĞ¸ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğº Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Beyond Pruning: A New Approach to Combat Data Replication in AI Models",
                    "desc": "This paper investigates the limitations of pruning-based defenses in text-to-image diffusion models, which are designed to prevent the replication of training data. The authors find that even small changes to text embeddings can reactivate memorized content, revealing the ineffectiveness of current mitigation strategies. They challenge the idea that memorization can be localized within the model, showing that data replication can occur from various points in the embedding space. To address these issues, the paper proposes a new adversarial fine-tuning method aimed at enhancing the model's robustness against such triggers."
                },
                "zh": {
                    "title": "çœŸæ­£å»é™¤è®°å¿†å†…å®¹çš„å¿…è¦æ€§",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­çš„ä¿®å‰ªé˜²å¾¡æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå³ä½¿åœ¨ä¿®å‰ªåï¼Œå¾®å°çš„æ–‡æœ¬åµŒå…¥è°ƒæ•´ä»ç„¶å¯ä»¥é‡æ–°è§¦å‘æ•°æ®å¤åˆ¶ï¼Œæ˜¾ç¤ºå‡ºè¿™äº›é˜²å¾¡æªæ–½çš„è„†å¼±æ€§ã€‚æˆ‘ä»¬è¿˜æŒ‘æˆ˜äº†è®°å¿†å±€éƒ¨æ€§çš„åŸºæœ¬å‡è®¾ï¼Œè¯æ˜å¤åˆ¶å¯ä»¥ä»æ–‡æœ¬åµŒå…¥ç©ºé—´ä¸­çš„ä¸åŒä½ç½®è§¦å‘ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„å¯¹æŠ—æ€§å¾®è°ƒæ–¹æ³•ï¼Œä»¥å¢å¼ºæ¨¡å‹çš„é²æ£’æ€§ï¼ŒçœŸæ­£å»é™¤è®°å¿†å†…å®¹ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-07-23.html",
    "link_next": "2025-07-25.html",
    "link_month": "2025-07.html",
    "short_date_prev": {
        "ru": "23.07",
        "en": "07/23",
        "zh": "7æœˆ23æ—¥"
    },
    "short_date_next": {
        "ru": "25.07",
        "en": "07/25",
        "zh": "7æœˆ25æ—¥"
    },
    "categories": {
        "#dataset": 2,
        "#data": 1,
        "#benchmark": 2,
        "#agents": 0,
        "#cv": 3,
        "#rl": 2,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 2,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 2,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 4,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 3,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 4,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 0,
        "#synthetic": 2,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 1,
        "#science": 0,
        "#low_resource": 0,
        "#perception": 1
    }
}