{
    "date": {
        "ru": "24 июля",
        "en": "July 24",
        "zh": "7月24日"
    },
    "time_utc": "2025-07-24 09:17",
    "weekday": 3,
    "issue_id": 4989,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2507.16863",
            "title": "Pixels, Patterns, but No Poetry: To See The World like Humans",
            "url": "https://huggingface.co/papers/2507.16863",
            "abstract": "The Turing Eye Test evaluates MLLMs' perceptual abilities through synthetic images, revealing that vision tower generalization is a significant gap compared to human perception.  \t\t\t\t\tAI-generated summary \t\t\t\t Achieving human-like perception and reasoning in Multimodal Large Language Models (MLLMs) remains a central challenge in artificial intelligence. While recent research has primarily focused on enhancing reasoning capabilities in MLLMs, a fundamental question persists: Can Multimodal Large Language Models truly perceive the world as humans do? This paper shifts focus from reasoning to perception. Rather than constructing benchmarks specifically for reasoning, we introduce the Turing Eye Test (TET), a challenging perception-oriented benchmark comprising four diagnostic tasks that evaluate MLLMs' performance on synthetic images that humans process intuitively. Our findings reveal that state-of-the-art MLLMs exhibit catastrophic failures on our perceptual tasks trivial for humans. Both in-context learning and training on language backbone-effective for previous benchmarks-fail to improve performance on our tasks, while fine-tuning the vision tower enables rapid adaptation, suggesting that our benchmark poses challenges for vision tower generalization rather than for the knowledge and reasoning capabilities of the language backbone-a key gap between current MLLMs and human perception. We release a representative subset of TET tasks in this version, and will introduce more diverse tasks and methods to enhance visual generalization in future work.",
            "score": 25,
            "issue_id": 4985,
            "pub_date": "2025-07-21",
            "pub_date_card": {
                "ru": "21 июля",
                "en": "July 21",
                "zh": "7月21日"
            },
            "hash": "26ca329617f34c34",
            "authors": [
                "Hongcheng Gao",
                "Zihao Huang",
                "Lin Xu",
                "Jingyi Tang",
                "Xinhao Li",
                "Yue Liu",
                "Haoyang Li",
                "Taihang Hu",
                "Minhua Lin",
                "Xinlong Yang",
                "Ge Wu",
                "Balong Bi",
                "Hongyu Chen",
                "Wentao Zhang"
            ],
            "affiliations": [
                "BJTU",
                "BUPT",
                "Nanjing University",
                "Nankai University",
                "National University of Singapore",
                "Peking University",
                "The Pennsylvania State University",
                "University of Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.16863.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#benchmark",
                    "#multimodal",
                    "#reasoning",
                    "#perception",
                    "#cv"
                ],
                "emoji": "👁️",
                "ru": {
                    "title": "Тест Тьюринга для зрения: проверка перцептивных способностей ИИ",
                    "desc": "Статья представляет новый тест для оценки перцептивных способностей мультимодальных больших языковых моделей (MLLM) - Turing Eye Test (TET). TET использует синтетические изображения для выявления разрыва между восприятием MLLM и человека. Результаты показывают, что современные MLLM демонстрируют катастрофические сбои на перцептивных задачах, тривиальных для людей. Обучение языковой основы не улучшает результаты, в то время как дообучение визуальной части модели позволяет быстро адаптироваться."
                },
                "en": {
                    "title": "Bridging the Perception Gap in MLLMs with the Turing Eye Test",
                    "desc": "This paper introduces the Turing Eye Test (TET), a new benchmark designed to evaluate the perceptual abilities of Multimodal Large Language Models (MLLMs) using synthetic images. The study highlights a significant gap in vision tower generalization, where MLLMs struggle with tasks that humans find easy, despite advancements in reasoning capabilities. The results indicate that traditional methods like in-context learning do not enhance performance on perceptual tasks, while fine-tuning the vision tower shows promise for improvement. This research emphasizes the need to focus on perception in MLLMs to bridge the gap between AI and human-like understanding of visual information."
                },
                "zh": {
                    "title": "图灵眼睛测试：揭示MLLMs感知能力的差距",
                    "desc": "本文探讨了多模态大型语言模型（MLLMs）在感知能力方面的不足，提出了图灵眼睛测试（TET）作为一种新的评估基准。该测试通过四个诊断任务，评估MLLMs在处理合成图像时的表现，发现当前最先进的MLLMs在这些任务上表现不佳，远不及人类。研究表明，尽管在推理能力上有所提升，但在视觉感知方面仍存在显著差距。未来的工作将致力于引入更多多样化的任务，以提高视觉泛化能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.17744",
            "title": "Yume: An Interactive World Generation Model",
            "url": "https://huggingface.co/papers/2507.17744",
            "abstract": "A framework for generating and exploring interactive video worlds from images using Masked Video Diffusion Transformer, Anti-Artifact Mechanism, Time Travel Sampling, and model acceleration techniques.  \t\t\t\t\tAI-generated summary \t\t\t\t Yume aims to use images, text, or videos to create an interactive, realistic, and dynamic world, which allows exploration and control using peripheral devices or neural signals. In this report, we present a preview version of \\method, which creates a dynamic world from an input image and allows exploration of the world using keyboard actions. To achieve this high-fidelity and interactive video world generation, we introduce a well-designed framework, which consists of four main components, including camera motion quantization, video generation architecture, advanced sampler, and model acceleration. First, we quantize camera motions for stable training and user-friendly interaction using keyboard inputs. Then, we introduce the Masked Video Diffusion Transformer~(MVDT) with a memory module for infinite video generation in an autoregressive manner. After that, training-free Anti-Artifact Mechanism (AAM) and Time Travel Sampling based on Stochastic Differential Equations (TTS-SDE) are introduced to the sampler for better visual quality and more precise control. Moreover, we investigate model acceleration by synergistic optimization of adversarial distillation and caching mechanisms. We use the high-quality world exploration dataset \\sekai to train \\method, and it achieves remarkable results in diverse scenes and applications. All data, codebase, and model weights are available on https://github.com/stdstu12/YUME. Yume will update monthly to achieve its original goal. Project page: https://stdstu12.github.io/YUME-Project/.",
            "score": 23,
            "issue_id": 4986,
            "pub_date": "2025-07-23",
            "pub_date_card": {
                "ru": "23 июля",
                "en": "July 23",
                "zh": "7月23日"
            },
            "hash": "367e31a517aa16f3",
            "authors": [
                "Xiaofeng Mao",
                "Shaoheng Lin",
                "Zhen Li",
                "Chuanhao Li",
                "Wenshuo Peng",
                "Tong He",
                "Jiangmiao Pang",
                "Mingmin Chi",
                "Yu Qiao",
                "Kaipeng Zhang"
            ],
            "affiliations": [
                "Fudan University",
                "Shanghai AI Laboratory",
                "Shanghai Innovation Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.17744.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#architecture",
                    "#open_source",
                    "#diffusion",
                    "#cv",
                    "#video"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Создание интерактивных видеомиров из изображений с помощью ИИ",
                    "desc": "Эта статья представляет фреймворк Yume для создания интерактивных видеомиров из изображений. Он использует Masked Video Diffusion Transformer для генерации видео, механизм устранения артефактов и выборку с путешествием во времени для улучшения качества. Фреймворк включает квантизацию движений камеры, архитектуру генерации видео и методы ускорения модели. Yume позволяет исследовать сгенерированный мир с помощью клавиатуры и демонстрирует впечатляющие результаты в различных сценах."
                },
                "en": {
                    "title": "Creating Interactive Video Worlds with Yume",
                    "desc": "This paper presents a framework called Yume that generates interactive video worlds from images, enabling users to explore these worlds using keyboard inputs or neural signals. The framework incorporates a Masked Video Diffusion Transformer (MVDT) for autoregressive video generation, along with an Anti-Artifact Mechanism (AAM) and Time Travel Sampling (TTS-SDE) to enhance visual quality and control. It also includes techniques for camera motion quantization and model acceleration to improve user interaction and performance. The system is trained on a high-quality dataset and aims to provide a dynamic and realistic experience in various applications."
                },
                "zh": {
                    "title": "生成互动视频世界的新框架",
                    "desc": "本文介绍了一种新的框架，利用图像生成和探索互动视频世界。该框架包括四个主要组件：相机运动量化、视频生成架构、先进的采样器和模型加速技术。通过使用Masked Video Diffusion Transformer（MVDT）和反伪影机制，系统能够生成高保真度的动态视频世界，并允许用户通过键盘操作进行探索。该方法在多种场景和应用中表现出色，展示了生成互动视频世界的潜力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.17202",
            "title": "DesignLab: Designing Slides Through Iterative Detection and Correction",
            "url": "https://huggingface.co/papers/2507.17202",
            "abstract": "DesignLab uses fine-tuned large language models to iteratively improve presentation slides through a design reviewer and contributor system, outperforming existing tools.  \t\t\t\t\tAI-generated summary \t\t\t\t Designing high-quality presentation slides can be challenging for non-experts due to the complexity involved in navigating various design choices. Numerous automated tools can suggest layouts and color schemes, yet often lack the ability to refine their own output, which is a key aspect in real-world workflows. We propose DesignLab, which separates the design process into two roles, the design reviewer, who identifies design-related issues, and the design contributor who corrects them. This decomposition enables an iterative loop where the reviewer continuously detects issues and the contributor corrects them, allowing a draft to be further polished with each iteration, reaching qualities that were unattainable. We fine-tune large language models for these roles and simulate intermediate drafts by introducing controlled perturbations, enabling the design reviewer learn design errors and the contributor learn how to fix them. Our experiments show that DesignLab outperforms existing design-generation methods, including a commercial tool, by embracing the iterative nature of designing which can result in polished, professional slides.",
            "score": 23,
            "issue_id": 4986,
            "pub_date": "2025-07-23",
            "pub_date_card": {
                "ru": "23 июля",
                "en": "July 23",
                "zh": "7月23日"
            },
            "hash": "1102d80628fa748f",
            "authors": [
                "Jooyeol Yun",
                "Heng Wang",
                "Yotaro Shimose",
                "Jaegul Choo",
                "Shingo Takamatsu"
            ],
            "affiliations": [
                "Korea Advanced Institute of Science and Technology (KAIST)",
                "Sony Group Corporation"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.17202.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#multimodal"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "ИИ-помощник для создания профессиональных презентаций",
                    "desc": "DesignLab - это система, использующая настроенные большие языковые модели для итеративного улучшения презентационных слайдов. Она разделяет процесс дизайна на две роли: рецензент дизайна, выявляющий проблемы, и контрибьютор дизайна, исправляющий их. Этот подход позволяет постоянно улучшать черновики, достигая качества, недоступного другим методам. Эксперименты показывают, что DesignLab превосходит существующие инструменты генерации дизайна, включая коммерческие решения."
                },
                "en": {
                    "title": "Iterative Design Excellence with AI",
                    "desc": "DesignLab is a system that enhances the process of creating presentation slides by using fine-tuned large language models. It divides the design task into two roles: a design reviewer who identifies problems and a design contributor who makes corrections. This iterative approach allows for continuous improvement of the slides, leading to higher quality outcomes. Experiments demonstrate that DesignLab surpasses existing design tools by effectively refining designs through repeated feedback and adjustments."
                },
                "zh": {
                    "title": "DesignLab：迭代提升演示文稿的智能工具",
                    "desc": "DesignLab 是一个利用微调的大型语言模型，通过设计审查者和贡献者系统来迭代改进演示文稿的工具。该系统将设计过程分为两个角色，审查者识别设计问题，贡献者进行修正，从而形成一个迭代循环。每次迭代中，审查者不断发现问题，贡献者则进行修正，使得草稿在每次迭代中都能得到进一步的完善。实验结果表明，DesignLab 在设计生成方面的表现优于现有工具，能够生成更高质量的专业演示文稿。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.17512",
            "title": "Can One Domain Help Others? A Data-Centric Study on Multi-Domain\n  Reasoning via Reinforcement Learning",
            "url": "https://huggingface.co/papers/2507.17512",
            "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a powerful paradigm for enhancing the reasoning capabilities of LLMs. Existing research has predominantly concentrated on isolated reasoning domains such as mathematical problem-solving, coding tasks, or logical reasoning. However, real world reasoning scenarios inherently demand an integrated application of multiple cognitive skills. Despite this, the interplay among these reasoning skills under reinforcement learning remains poorly understood. To bridge this gap, we present a systematic investigation of multi-domain reasoning within the RLVR framework, explicitly focusing on three primary domains: mathematical reasoning, code generation, and logical puzzle solving. We conduct a comprehensive study comprising four key components: (1) Leveraging the GRPO algorithm and the Qwen-2.5-7B model family, our study thoroughly evaluates the models' in-domain improvements and cross-domain generalization capabilities when trained on single-domain datasets. (2) Additionally, we examine the intricate interactions including mutual enhancements and conflicts that emerge during combined cross-domain training. (3) To further understand the influence of SFT on RL, we also analyze and compare performance differences between base and instruct models under identical RL configurations. (4) Furthermore, we delve into critical RL training details, systematically exploring the impacts of curriculum learning strategies, variations in reward design, and language-specific factors. Through extensive experiments, our results offer significant insights into the dynamics governing domain interactions, revealing key factors influencing both specialized and generalizable reasoning performance. These findings provide valuable guidance for optimizing RL methodologies to foster comprehensive, multi-domain reasoning capabilities in LLMs.",
            "score": 20,
            "issue_id": 4983,
            "pub_date": "2025-07-23",
            "pub_date_card": {
                "ru": "23 июля",
                "en": "July 23",
                "zh": "7月23日"
            },
            "hash": "d637b6ffc2166e10",
            "authors": [
                "Yu Li",
                "Zhuoshi Pan",
                "Honglin Lin",
                "Mengyuan Sun",
                "Conghui He",
                "Lijun Wu"
            ],
            "affiliations": [
                "OpenDataLab",
                "Shanghai Artificial Intelligence Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.17512.jpg",
            "data": {
                "categories": [
                    "#math",
                    "#training",
                    "#reasoning",
                    "#rl",
                    "#optimization"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Многодоменное рассуждение: раскрывая потенциал ИИ через обучение с подкреплением",
                    "desc": "Исследование посвящено обучению с подкреплением с проверяемыми наградами (RLVR) для улучшения способностей больших языковых моделей к рассуждению в нескольких областях одновременно. Авторы провели систематический анализ взаимодействия между математическими рассуждениями, генерацией кода и решением логических задач в рамках RLVR. Эксперименты включали оценку улучшений внутри доменов и обобщения между доменами, изучение взаимодействий при комбинированном обучении, сравнение базовых и инструктированных моделей, а также исследование влияния различных аспектов обучения с подкреплением. Результаты предоставляют ценные insights о динамике взаимодействия доменов и факторах, влияющих на специализированную и обобщенную производительность в рассуждениях."
                },
                "en": {
                    "title": "Enhancing Multi-Domain Reasoning in LLMs with RLVR",
                    "desc": "This paper introduces Reinforcement Learning with Verifiable Rewards (RLVR) as a method to improve the reasoning abilities of large language models (LLMs) across multiple domains. It highlights the need for integrated reasoning skills in real-world scenarios, moving beyond isolated tasks like math or coding. The study investigates how different reasoning domains interact during training, using the GRPO algorithm and the Qwen-2.5-7B model family to assess improvements and generalization. Key findings reveal important dynamics in multi-domain training, offering insights for enhancing RL techniques to develop more versatile reasoning capabilities in LLMs."
                },
                "zh": {
                    "title": "多领域推理的强化学习新探索",
                    "desc": "强化学习与可验证奖励（RLVR）是一种增强大型语言模型（LLMs）推理能力的有效方法。现有研究主要集中在数学问题解决、编码任务和逻辑推理等孤立的推理领域。然而，现实世界的推理场景需要多种认知技能的综合应用。本文系统研究了RLVR框架下的多领域推理，重点分析数学推理、代码生成和逻辑难题解决之间的相互作用。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.16331",
            "title": "Re:Form -- Reducing Human Priors in Scalable Formal Software\n  Verification with RL in LLMs: A Preliminary Study on Dafny",
            "url": "https://huggingface.co/papers/2507.16331",
            "abstract": "Formal language-based reasoning and automatic verification improve the reliability and scalability of Large Language Models for generating verifiable programs.  \t\t\t\t\tAI-generated summary \t\t\t\t Existing informal language-based (e.g., human language) Large Language Models (LLMs) trained with Reinforcement Learning (RL) face a significant challenge: their verification processes, which provide crucial training signals, are neither reliable nor scalable. In fact, the prevalent large proprietary models could hardly generate verifiable programs. A promising yet largely uncharted alternative is formal language-based reasoning. Grounding LLMs in rigorous formal systems where generative models operate in formal language spaces (e.g., Dafny) enables the automatic and mathematically provable verification of their reasoning processes and outcomes. This capability is pivotal for achieving large-scale, reliable formal software verification. It is a common practice to employ human-annotated chain-of-thought and other human priors to induce the reasoning and coding capabilities of LLMs. Unfortunately, it becomes unacceptably all-consuming to provide such priors for supervising complex programming tasks. In this work, we systematically explore ways to reduce human priors with the formal language, Dafny, as the main environment for our pilot study. Our pipeline mainly relies on introducing an automatic and scalable data curation pipeline, and careful RL designs integrated with feedback from the formal language verifier. We introduce DafnyComp, a benchmark of compositional formal programs with auto-formalized specifications for specification reasoning. Our supervised fine-tuning (SFT) stage enables even small models (e.g., 0.5B) to generate syntactically valid and verifiable Dafny code, surpassing proprietary models. RL with regularization further improves performance, achieving stronger generalization to out-of-domain tasks and outperforming all strong baselines on the challenging DafnyComp benchmark.",
            "score": 12,
            "issue_id": 4987,
            "pub_date": "2025-07-22",
            "pub_date_card": {
                "ru": "22 июля",
                "en": "July 22",
                "zh": "7月22日"
            },
            "hash": "a6439c608d5df566",
            "authors": [
                "Chuanhao Yan",
                "Fengdi Che",
                "Xuhan Huang",
                "Xu Xu",
                "Xin Li",
                "Yizhi Li",
                "Xingwei Qu",
                "Jingzhe Shi",
                "Zhuangzhuang He",
                "Chenghua Lin",
                "Yaodong Yang",
                "Binhang Yuan",
                "Hang Zhao",
                "Yu Qiao",
                "Bowen Zhou",
                "Jie Fu"
            ],
            "affiliations": [
                "Shanghai AI Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.16331.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#dataset",
                    "#reasoning",
                    "#training",
                    "#optimization",
                    "#small_models",
                    "#rlhf",
                    "#benchmark"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Формальные языки повышают надежность ИИ в программировании",
                    "desc": "Статья исследует использование формальных языков для улучшения надежности и масштабируемости больших языковых моделей (LLM) при генерации проверяемых программ. Авторы представляют DafnyComp - набор данных композиционных формальных программ для обучения спецификационным рассуждениям. Они показывают, что даже небольшие модели (0.5B параметров) после дообучения способны генерировать синтаксически корректный и верифицируемый код на языке Dafny. Применение обучения с подкреплением (RL) с регуляризацией дополнительно улучшает производительность моделей и их обобщающую способность."
                },
                "en": {
                    "title": "Enhancing LLMs with Formal Language for Reliable Program Verification",
                    "desc": "This paper discusses how using formal language-based reasoning can enhance the reliability and scalability of Large Language Models (LLMs) in generating verifiable programs. Traditional LLMs, which rely on informal language and Reinforcement Learning, struggle with verification processes that are essential for training. The authors propose a method that utilizes the formal language Dafny, allowing for automatic and mathematically provable verification of the models' outputs. Their approach includes a new benchmark called DafnyComp and demonstrates that even smaller models can produce valid and verifiable code, outperforming existing proprietary models."
                },
                "zh": {
                    "title": "形式语言助力大型语言模型的可靠性与可扩展性",
                    "desc": "本文探讨了基于形式语言的推理和自动验证如何提高大型语言模型（LLMs）生成可验证程序的可靠性和可扩展性。现有的基于非正式语言的LLMs在验证过程中面临重大挑战，导致其生成的程序难以验证。通过将LLMs嵌入严格的形式系统（如Dafny），可以实现其推理过程和结果的自动化和数学可证明的验证。我们的研究表明，使用Dafny作为主要环境，结合自动化数据整理和强化学习设计，可以显著减少对人类先验知识的依赖，提升模型在复杂编程任务中的表现。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.17745",
            "title": "Ultra3D: Efficient and High-Fidelity 3D Generation with Part Attention",
            "url": "https://huggingface.co/papers/2507.17745",
            "abstract": "Ultra3D uses VecSet and Part Attention to accelerate 3D voxel generation while maintaining high quality and resolution.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in sparse voxel representations have significantly improved the quality of 3D content generation, enabling high-resolution modeling with fine-grained geometry. However, existing frameworks suffer from severe computational inefficiencies due to the quadratic complexity of attention mechanisms in their two-stage diffusion pipelines. In this work, we propose Ultra3D, an efficient 3D generation framework that significantly accelerates sparse voxel modeling without compromising quality. Our method leverages the compact VecSet representation to efficiently generate a coarse object layout in the first stage, reducing token count and accelerating voxel coordinate prediction. To refine per-voxel latent features in the second stage, we introduce Part Attention, a geometry-aware localized attention mechanism that restricts attention computation within semantically consistent part regions. This design preserves structural continuity while avoiding unnecessary global attention, achieving up to 6.7x speed-up in latent generation. To support this mechanism, we construct a scalable part annotation pipeline that converts raw meshes into part-labeled sparse voxels. Extensive experiments demonstrate that Ultra3D supports high-resolution 3D generation at 1024 resolution and achieves state-of-the-art performance in both visual fidelity and user preference.",
            "score": 6,
            "issue_id": 4984,
            "pub_date": "2025-07-23",
            "pub_date_card": {
                "ru": "23 июля",
                "en": "July 23",
                "zh": "7月23日"
            },
            "hash": "73bfee6022eeade3",
            "authors": [
                "Yiwen Chen",
                "Zhihao Li",
                "Yikai Wang",
                "Hu Zhang",
                "Qin Li",
                "Chi Zhang",
                "Guosheng Lin"
            ],
            "affiliations": [
                "Math Magic",
                "Nanyang Technological University",
                "School of Artificial Intelligence, Beijing Normal University",
                "Tsinghua University",
                "Westlake University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.17745.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#3d"
                ],
                "emoji": "🧊",
                "ru": {
                    "title": "Ускоренная генерация высококачественных 3D-моделей с помощью локализованного внимания",
                    "desc": "Ultra3D - это эффективный фреймворк для генерации 3D-контента, использующий представление VecSet и механизм Part Attention. VecSet позволяет ускорить создание грубого макета объекта на первом этапе, уменьшая количество токенов. Part Attention ограничивает вычисление внимания семантически согласованными областями на втором этапе, сохраняя структурную целостность. Метод достигает ускорения до 6,7 раз при генерации латентных представлений по сравнению с существующими подходами. Ultra3D поддерживает генерацию 3D-моделей высокого разрешения (1024^3 вокселей) и демонстрирует высокое качество результатов."
                },
                "en": {
                    "title": "Accelerating 3D Voxel Generation with Ultra3D!",
                    "desc": "Ultra3D is a novel framework designed to enhance the efficiency of 3D voxel generation while ensuring high quality and resolution. It utilizes a compact VecSet representation to create a coarse object layout, which reduces the number of tokens and speeds up voxel coordinate prediction. Additionally, the framework introduces Part Attention, a localized attention mechanism that focuses on semantically consistent regions, thus improving computational efficiency and maintaining structural integrity. Experimental results show that Ultra3D can generate high-resolution 3D models at 1024 resolution with significant speed improvements and superior visual quality compared to existing methods."
                },
                "zh": {
                    "title": "Ultra3D：高效的3D体素生成新方法",
                    "desc": "Ultra3D是一种高效的3D生成框架，利用VecSet和Part Attention加速稀疏体素建模，同时保持高质量和高分辨率。该方法在第一阶段使用紧凑的VecSet表示生成粗略的物体布局，从而减少了标记数量，加快了体素坐标预测。第二阶段引入了Part Attention，这是一种几何感知的局部注意力机制，限制了注意力计算在语义一致的部分区域内。通过这种设计，Ultra3D在潜在特征生成上实现了最高6.7倍的加速，同时支持1024分辨率的高质量3D生成。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.11465",
            "title": "Elevating 3D Models: High-Quality Texture and Geometry Refinement from a\n  Low-Quality Model",
            "url": "https://huggingface.co/papers/2507.11465",
            "abstract": "Elevate3D enhances both texture and geometry of low-quality 3D assets using HFS-SDEdit and monocular geometry predictors, achieving superior refinement quality.  \t\t\t\t\tAI-generated summary \t\t\t\t High-quality 3D assets are essential for various applications in computer graphics and 3D vision but remain scarce due to significant acquisition costs. To address this shortage, we introduce Elevate3D, a novel framework that transforms readily accessible low-quality 3D assets into higher quality. At the core of Elevate3D is HFS-SDEdit, a specialized texture enhancement method that significantly improves texture quality while preserving the appearance and geometry while fixing its degradations. Furthermore, Elevate3D operates in a view-by-view manner, alternating between texture and geometry refinement. Unlike previous methods that have largely overlooked geometry refinement, our framework leverages geometric cues from images refined with HFS-SDEdit by employing state-of-the-art monocular geometry predictors. This approach ensures detailed and accurate geometry that aligns seamlessly with the enhanced texture. Elevate3D outperforms recent competitors by achieving state-of-the-art quality in 3D model refinement, effectively addressing the scarcity of high-quality open-source 3D assets.",
            "score": 6,
            "issue_id": 4984,
            "pub_date": "2025-07-15",
            "pub_date_card": {
                "ru": "15 июля",
                "en": "July 15",
                "zh": "7月15日"
            },
            "hash": "3e9488a149f4e7a7",
            "authors": [
                "Nuri Ryu",
                "Jiyun Won",
                "Jooeun Son",
                "Minsu Gong",
                "Joo-Haeng Lee",
                "Sunghyun Cho"
            ],
            "affiliations": [
                "POSTECH, South Korea",
                "Pebblous, South Korea"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.11465.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#open_source",
                    "#synthetic"
                ],
                "emoji": "🔬",
                "ru": {
                    "title": "Превращаем низкокачественные 3D-модели в шедевры",
                    "desc": "Elevate3D - это новая система для улучшения качества 3D-моделей низкого разрешения. Она использует метод HFS-SDEdit для улучшения текстур и предикторы геометрии для уточнения формы объектов. Система работает последовательно, улучшая текстуры и геометрию для каждого ракурса модели. Elevate3D превосходит существующие методы и помогает решить проблему нехватки качественных 3D-ресурсов."
                },
                "en": {
                    "title": "Transforming Low-Quality 3D Assets into High-Quality Masterpieces",
                    "desc": "Elevate3D is a framework designed to improve the quality of low-resolution 3D assets by enhancing both their texture and geometry. It utilizes a method called HFS-SDEdit for texture enhancement, which maintains the original appearance while correcting any flaws. The framework also incorporates monocular geometry predictors to refine the 3D geometry, ensuring that it matches the improved textures. By alternating between texture and geometry refinement in a view-by-view manner, Elevate3D achieves superior results compared to existing methods, making high-quality 3D assets more accessible."
                },
                "zh": {
                    "title": "Elevate3D：提升低质量3D资产的质量",
                    "desc": "Elevate3D 是一个新颖的框架，旨在将低质量的 3D 资产转化为高质量的 3D 模型。它的核心是 HFS-SDEdit，这是一种专门的纹理增强方法，可以显著提高纹理质量，同时保持外观和几何形状。Elevate3D 采用逐视图的方式，交替进行纹理和几何的细化，确保两者之间的协调。通过使用先进的单目几何预测器，Elevate3D 在 3D 模型细化方面超越了竞争对手，提供了卓越的质量。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.16880",
            "title": "Finding Dori: Memorization in Text-to-Image Diffusion Models Is Less\n  Local Than Assumed",
            "url": "https://huggingface.co/papers/2507.16880",
            "abstract": "Pruning-based defenses in text-to-image diffusion models are ineffective as minor adjustments to text embeddings can re-trigger data replication, necessitating methods that truly remove memorized content.  \t\t\t\t\tAI-generated summary \t\t\t\t Text-to-image diffusion models (DMs) have achieved remarkable success in image generation. However, concerns about data privacy and intellectual property remain due to their potential to inadvertently memorize and replicate training data. Recent mitigation efforts have focused on identifying and pruning weights responsible for triggering replication, based on the assumption that memorization can be localized. Our research assesses the robustness of these pruning-based approaches. We demonstrate that even after pruning, minor adjustments to text embeddings of input prompts are sufficient to re-trigger data replication, highlighting the fragility of these defenses. Furthermore, we challenge the fundamental assumption of memorization locality, by showing that replication can be triggered from diverse locations within the text embedding space, and follows different paths in the model. Our findings indicate that existing mitigation strategies are insufficient and underscore the need for methods that truly remove memorized content, rather than attempting to suppress its retrieval. As a first step in this direction, we introduce a novel adversarial fine-tuning method that iteratively searches for replication triggers and updates the model to increase robustness. Through our research, we provide fresh insights into the nature of memorization in text-to-image DMs and a foundation for building more trustworthy and compliant generative AI.",
            "score": 2,
            "issue_id": 4989,
            "pub_date": "2025-07-22",
            "pub_date_card": {
                "ru": "22 июля",
                "en": "July 22",
                "zh": "7月22日"
            },
            "hash": "441dcefad865f158",
            "authors": [
                "Antoni Kowalczuk",
                "Dominik Hintersdorf",
                "Lukas Struppek",
                "Kristian Kersting",
                "Adam Dziedzic",
                "Franziska Boenisch"
            ],
            "affiliations": [
                "CISPA Helmholtz Center for Information Security",
                "Centre for Cognitive Science, Technical University of Darmstadt",
                "Computer Science Department, Technical University of Darmstadt",
                "German Research Center for Artificial Intelligence (DFKI)",
                "Hessian Center for AI (Hessian.AI)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.16880.jpg",
            "data": {
                "categories": [
                    "#security",
                    "#cv",
                    "#training",
                    "#hallucinations",
                    "#data"
                ],
                "emoji": "🔐",
                "ru": {
                    "title": "Защита приватности в генеративных моделях: новые вызовы и решения",
                    "desc": "Исследование показывает, что существующие методы защиты от нежелательного воспроизведения данных в диффузионных моделях генерации изображений по тексту неэффективны. Авторы демонстрируют, что даже после удаления весов, ответственных за воспроизведение, небольшие изменения текстовых эмбеддингов могут снова вызвать нежелательную генерацию. Это ставит под сомнение предположение о локальности запоминания в модели. Предлагается новый метод состязательной доводки для повышения устойчивости модели к воспроизведению данных."
                },
                "en": {
                    "title": "Beyond Pruning: A New Approach to Combat Data Replication in AI Models",
                    "desc": "This paper investigates the limitations of pruning-based defenses in text-to-image diffusion models, which are designed to prevent the replication of training data. The authors find that even small changes to text embeddings can reactivate memorized content, revealing the ineffectiveness of current mitigation strategies. They challenge the idea that memorization can be localized within the model, showing that data replication can occur from various points in the embedding space. To address these issues, the paper proposes a new adversarial fine-tuning method aimed at enhancing the model's robustness against such triggers."
                },
                "zh": {
                    "title": "真正去除记忆内容的必要性",
                    "desc": "本文探讨了文本到图像扩散模型中的修剪防御方法的有效性。研究表明，即使在修剪后，微小的文本嵌入调整仍然可以重新触发数据复制，显示出这些防御措施的脆弱性。我们还挑战了记忆局部性的基本假设，证明复制可以从文本嵌入空间中的不同位置触发。为此，我们提出了一种新的对抗性微调方法，以增强模型的鲁棒性，真正去除记忆内容。"
                }
            }
        }
    ],
    "link_prev": "2025-07-23.html",
    "link_next": "2025-07-25.html",
    "link_month": "2025-07.html",
    "short_date_prev": {
        "ru": "23.07",
        "en": "07/23",
        "zh": "7月23日"
    },
    "short_date_next": {
        "ru": "25.07",
        "en": "07/25",
        "zh": "7月25日"
    },
    "categories": {
        "#dataset": 2,
        "#data": 1,
        "#benchmark": 2,
        "#agents": 0,
        "#cv": 3,
        "#rl": 2,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 2,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 2,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 4,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 3,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 4,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 0,
        "#synthetic": 2,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 1,
        "#science": 0,
        "#low_resource": 0,
        "#perception": 1
    }
}