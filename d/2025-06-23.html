
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 18 papers. June 23.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">23 июня</span> | <span id="title-articles-count">18 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-06-20.html">⬅️ <span id="prev-date">20.06</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-06-24.html">➡️ <span id="next-date">24.06</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-06.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '23 июня', 'en': 'June 23', 'zh': '6月23日'};
        let feedDateNext = {'ru': '24.06', 'en': '06/24', 'zh': '6月24日'};
        let feedDatePrev = {'ru': '20.06', 'en': '06/20', 'zh': '6月20日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2506.16406', 'title': 'Drag-and-Drop LLMs: Zero-Shot Prompt-to-Weights', 'url': 'https://huggingface.co/papers/2506.16406', 'abstract': 'Drag-and-Drop LLMs generate task-specific parameters through prompt-conditioned parameter generation, achieving significant efficiency gains and cross-domain generalization without per-task training.  \t\t\t\t\tAI-generated summary \t\t\t\t Modern Parameter-Efficient Fine-Tuning (PEFT) methods such as low-rank adaptation (LoRA) reduce the cost of customizing large language models (LLMs), yet still require a separate optimization run for every downstream dataset. We introduce Drag-and-Drop LLMs (\\textit{DnD)}, a prompt-conditioned parameter generator that eliminates per-task training by mapping a handful of unlabeled task prompts directly to LoRA weight updates. A lightweight text encoder distills each prompt batch into condition embeddings, which are then transformed by a cascaded hyper-convolutional decoder into the full set of LoRA matrices. Once trained in a diverse collection of prompt-checkpoint pairs, DnD produces task-specific parameters in seconds, yielding i) up to 12,000times lower overhead than full fine-tuning, ii) average gains up to 30\\% in performance over the strongest training LoRAs on unseen common-sense reasoning, math, coding, and multimodal benchmarks, and iii) robust cross-domain generalization despite never seeing the target data or labels. Our results demonstrate that prompt-conditioned parameter generation is a viable alternative to gradient-based adaptation for rapidly specializing LLMs. Our project is available at https://jerryliang24.github.io/DnD{https://jerryliang24.github.io/DnD}.', 'score': 75, 'issue_id': 4426, 'pub_date': '2025-06-19', 'pub_date_card': {'ru': '19 июня', 'en': 'June 19', 'zh': '6月19日'}, 'hash': '7ec3cdc6c01d1d79', 'authors': ['Zhiyuan Liang', 'Dongwen Tang', 'Yuhao Zhou', 'Xuanlei Zhao', 'Mingjia Shi', 'Wangbo Zhao', 'Zekai Li', 'Peihao Wang', 'Konstantin Schürholt', 'Damian Borth', 'Michael M. Bronstein', 'Yang You', 'Zhangyang Wang', 'Kai Wang'], 'affiliations': ['National University of Singapore', 'Oxford University', 'UT Austin', 'University of St. Gallen'], 'pdf_title_img': 'assets/pdf/title_img/2506.16406.jpg', 'data': {'categories': ['#transfer_learning', '#multimodal', '#training', '#optimization'], 'emoji': '🎭', 'ru': {'title': 'Революция в настройке ЯМ: быстрая адаптация без дополнительного обучения', 'desc': 'Статья представляет новый метод Drag-and-Drop LLMs (DnD) для эффективной настройки больших языковых моделей. DnD использует генератор параметров, обусловленный промптами, для создания обновлений весов LoRA без необходимости обучения для каждой задачи. Этот подход обеспечивает значительное снижение вычислительных затрат и улучшение производительности на различных тестах по сравнению с традиционными методами тонкой настройки. DnD также демонстрирует надежную межпредметную генерализацию без доступа к целевым данным или меткам.'}, 'en': {'title': 'Efficient Task-Specific Adaptation with Drag-and-Drop LLMs', 'desc': 'The paper introduces Drag-and-Drop LLMs (DnD), a novel approach that generates task-specific parameters without the need for separate training on each task. By using prompt-conditioned parameter generation, DnD maps unlabeled task prompts directly to updates for low-rank adaptation (LoRA) weights. This method significantly reduces the computational overhead, achieving up to 12,000 times less than traditional fine-tuning while improving performance by an average of 30% on various benchmarks. DnD demonstrates strong cross-domain generalization, effectively adapting to new tasks without prior exposure to their data or labels.'}, 'zh': {'title': '拖放大语言模型：高效的任务特定参数生成', 'desc': '本文介绍了一种新的机器学习方法，称为拖放大语言模型（DnD），它通过提示条件生成参数，消除了每个任务的训练需求。DnD使用轻量级文本编码器将提示批次提炼为条件嵌入，并通过级联超卷积解码器转换为完整的LoRA矩阵。与传统的参数高效微调方法相比，DnD在性能上有显著提升，并且在未见过的任务上也能保持良好的泛化能力。该方法在多个基准测试中表现出色，证明了提示条件参数生成是快速专门化大语言模型的有效替代方案。'}}}, {'id': 'https://huggingface.co/papers/2506.16035', 'title': 'Vision-Guided Chunking Is All You Need: Enhancing RAG with Multimodal\n  Document Understanding', 'url': 'https://huggingface.co/papers/2506.16035', 'abstract': 'A novel multimodal document chunking approach using Large Multimodal Models (LMMs) enhances RAG performance by accurately processing complex PDF documents, including multi-page tables and embedded visuals.  \t\t\t\t\tAI-generated summary \t\t\t\t Retrieval-Augmented Generation (RAG) systems have revolutionized information retrieval and question answering, but traditional text-based chunking methods struggle with complex document structures, multi-page tables, embedded figures, and contextual dependencies across page boundaries. We present a novel multimodal document chunking approach that leverages Large Multimodal Models (LMMs) to process PDF documents in batches while maintaining semantic coherence and structural integrity. Our method processes documents in configurable page batches with cross-batch context preservation, enabling accurate handling of tables spanning multiple pages, embedded visual elements, and procedural content. We evaluate our approach on a curated dataset of PDF documents with manually crafted queries, demonstrating improvements in chunk quality and downstream RAG performance. Our vision-guided approach achieves better accuracy compared to traditional vanilla RAG systems, with qualitative analysis showing superior preservation of document structure and semantic coherence.', 'score': 53, 'issue_id': 4427, 'pub_date': '2025-06-19', 'pub_date_card': {'ru': '19 июня', 'en': 'June 19', 'zh': '6月19日'}, 'hash': '08abcd6ec6f0d9e1', 'authors': ['Vishesh Tripathi', 'Tanmay Odapally', 'Indraneel Das', 'Uday Allu', 'Biddwan Ahmed'], 'affiliations': ['AI Research Team, Yellow.ai'], 'pdf_title_img': 'assets/pdf/title_img/2506.16035.jpg', 'data': {'categories': ['#dataset', '#long_context', '#optimization', '#rag', '#multimodal'], 'emoji': '📄', 'ru': {'title': 'Умное разбиение документов для улучшения RAG-систем', 'desc': 'Предложен новый подход к разбиению сложных PDF-документов на фрагменты с использованием мультимодальных языковых моделей (LMM). Метод обрабатывает документы партиями страниц, сохраняя контекст между партиями, что позволяет корректно обрабатывать многостраничные таблицы и встроенные визуальные элементы. Эксперименты показали улучшение качества фрагментов и повышение эффективности систем генерации с извлечением (RAG). Новый подход превосходит традиционные методы RAG в точности и сохранении структуры документа.'}, 'en': {'title': 'Enhancing RAG with Multimodal Document Chunking', 'desc': 'This paper introduces a new method for breaking down complex PDF documents using Large Multimodal Models (LMMs). Traditional methods struggle with intricate layouts, such as multi-page tables and embedded visuals, which can lead to poor information retrieval. The proposed approach processes documents in batches while keeping the context intact across pages, ensuring that the meaning and structure are preserved. Evaluations show that this method significantly enhances the quality of document chunks and improves the performance of Retrieval-Augmented Generation (RAG) systems.'}, 'zh': {'title': '多模态文档分块，提升信息检索新高度', 'desc': '本文提出了一种新颖的多模态文档分块方法，利用大型多模态模型（LMMs）来处理复杂的PDF文档。该方法能够有效处理多页表格和嵌入视觉元素，同时保持语义一致性和结构完整性。通过配置页面批次进行处理，我们的方法能够跨批次保留上下文，从而提高了分块质量和后续的检索增强生成（RAG）性能。实验结果表明，与传统的RAG系统相比，我们的方法在准确性和文档结构保留方面表现更优。'}}}, {'id': 'https://huggingface.co/papers/2506.16054', 'title': 'PAROAttention: Pattern-Aware ReOrdering for Efficient Sparse and\n  Quantized Attention in Visual Generation Models', 'url': 'https://huggingface.co/papers/2506.16054', 'abstract': 'PAROAttention reorganizes visual attention patterns to enable efficient sparsification and quantization, reducing memory and computational costs with minimal impact on performance.  \t\t\t\t\tAI-generated summary \t\t\t\t In visual generation, the quadratic complexity of attention mechanisms results in high memory and computational costs, especially for longer token sequences required in high-resolution image or multi-frame video generation. To address this, prior research has explored techniques such as sparsification and quantization. However, these techniques face significant challenges under low density and reduced bitwidths. Through systematic analysis, we identify that the core difficulty stems from the dispersed and irregular characteristics of visual attention patterns. Therefore, instead of introducing specialized sparsification and quantization design to accommodate such patterns, we propose an alternative strategy: *reorganizing* the attention pattern to alleviate the challenges. Inspired by the local aggregation nature of visual feature extraction, we design a novel **Pattern-Aware token ReOrdering (PARO)** technique, which unifies the diverse attention patterns into a hardware-friendly block-wise pattern. This unification substantially simplifies and enhances both sparsification and quantization. We evaluate the performance-efficiency trade-offs of various design choices and finalize a methodology tailored for the unified pattern. Our approach, **PAROAttention**, achieves video and image generation with lossless metrics, and nearly identical results from full-precision (FP) baselines, while operating at notably lower density (~20%-30%) and bitwidth (**INT8/INT4**), achieving a **1.9x** to **2.7x** end-to-end latency speedup.', 'score': 43, 'issue_id': 4428, 'pub_date': '2025-06-19', 'pub_date_card': {'ru': '19 июня', 'en': 'June 19', 'zh': '6月19日'}, 'hash': '141661e70ce4b65f', 'authors': ['Tianchen Zhao', 'Ke Hong', 'Xinhao Yang', 'Xuefeng Xiao', 'Huixia Li', 'Feng Ling', 'Ruiqi Xie', 'Siqi Chen', 'Hongyu Zhu', 'Yichong Zhang', 'Yu Wang'], 'affiliations': ['ByteDance', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2506.16054.jpg', 'data': {'categories': ['#optimization', '#cv', '#inference', '#architecture', '#video'], 'emoji': '🔬', 'ru': {'title': 'Эффективное внимание: меньше вычислений, та же точность', 'desc': 'Статья представляет новый метод PAROAttention для оптимизации механизмов внимания в задачах генерации изображений и видео. Авторы предлагают реорганизовать паттерны внимания в блочную структуру, что упрощает разреживание и квантование. Это позволяет значительно снизить требования к памяти и вычислительным ресурсам без существенной потери качества. Эксперименты показывают ускорение в 1.9-2.7 раза по сравнению с базовыми моделями полной точности при использовании низкой плотности (20-30%) и 8/4-битного квантования.'}, 'en': {'title': 'Revolutionizing Visual Attention for Efficient AI', 'desc': 'PAROAttention is a novel approach that reorganizes visual attention patterns to improve the efficiency of sparsification and quantization in machine learning models. By addressing the challenges posed by the irregular characteristics of attention mechanisms, this method simplifies the process of reducing memory and computational costs. The technique, known as Pattern-Aware token ReOrdering (PARO), consolidates diverse attention patterns into a more manageable block-wise format. As a result, PAROAttention achieves high-quality image and video generation with significantly reduced resource requirements, maintaining performance comparable to full-precision models.'}, 'zh': {'title': 'PAROAttention：高效的视觉注意力重组', 'desc': 'PAROAttention是一种通过重新组织视觉注意力模式来提高稀疏化和量化效率的方法。这种方法能够在减少内存和计算成本的同时，尽量不影响性能。研究表明，视觉注意力模式的分散和不规则特性是造成高计算成本的主要原因。PARO技术通过将多样的注意力模式统一为硬件友好的块状模式，显著简化了稀疏化和量化过程。'}}}, {'id': 'https://huggingface.co/papers/2506.09049', 'title': 'VIKI-R: Coordinating Embodied Multi-Agent Cooperation via Reinforcement\n  Learning', 'url': 'https://huggingface.co/papers/2506.09049', 'abstract': 'VIKI-Bench and VIKI-R provide a benchmark and framework for evaluating and improving visual-driven cooperation among diverse embodied agents using vision-language models and reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t Coordinating multiple embodied agents in dynamic environments remains a core challenge in artificial intelligence, requiring both perception-driven reasoning and scalable cooperation strategies. While recent works have leveraged large language models (LLMs) for multi-agent planning, a few have begun to explore vision-language models (VLMs) for visual reasoning. However, these VLM-based approaches remain limited in their support for diverse embodiment types. In this work, we introduce VIKI-Bench, the first hierarchical benchmark tailored for embodied multi-agent cooperation, featuring three structured levels: agent activation, task planning, and trajectory perception. VIKI-Bench includes diverse robot embodiments, multi-view visual observations, and structured supervision signals to evaluate reasoning grounded in visual inputs. To demonstrate the utility of VIKI-Bench, we propose VIKI-R, a two-stage framework that fine-tunes a pretrained vision-language model (VLM) using Chain-of-Thought annotated demonstrations, followed by reinforcement learning under multi-level reward signals. Our extensive experiments show that VIKI-R significantly outperforms baselines method across all task levels. Furthermore, we show that reinforcement learning enables the emergence of compositional cooperation patterns among heterogeneous agents. Together, VIKI-Bench and VIKI-R offer a unified testbed and method for advancing multi-agent, visual-driven cooperation in embodied AI systems.', 'score': 29, 'issue_id': 4425, 'pub_date': '2025-06-10', 'pub_date_card': {'ru': '10 июня', 'en': 'June 10', 'zh': '6月10日'}, 'hash': '91caa1a0b1cb2b54', 'authors': ['Li Kang', 'Xiufeng Song', 'Heng Zhou', 'Yiran Qin', 'Jie Yang', 'Xiaohong Liu', 'Philip Torr', 'Lei Bai', 'Zhenfei Yin'], 'affiliations': ['Shanghai Artificial Intelligence Laboratory', 'Shanghai Jiao Tong University', 'The Chinese University of Hong Kong, Shenzhen', 'University of Oxford', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2506.09049.jpg', 'data': {'categories': ['#reasoning', '#agents', '#games', '#rl', '#benchmark', '#cv'], 'emoji': '🤖', 'ru': {'title': 'Визуальная кооперация агентов с помощью глубокого обучения', 'desc': 'VIKI-Bench представляет собой иерархический бенчмарк для оценки кооперации между воплощенными агентами с использованием визуально-языковых моделей. VIKI-R - это двухэтапный фреймворк, который дообучает предобученную визуально-языковую модель с помощью аннотированных демонстраций и обучения с подкреплением. Эксперименты показывают, что VIKI-R значительно превосходит базовые методы на всех уровнях задач. Обучение с подкреплением позволяет появиться композиционным паттернам кооперации между разнородными агентами.'}, 'en': {'title': 'Enhancing Multi-Agent Cooperation with VIKI-Bench and VIKI-R', 'desc': 'This paper introduces VIKI-Bench and VIKI-R, a benchmark and framework designed to enhance cooperation among various embodied agents using vision-language models and reinforcement learning. VIKI-Bench is structured into three levels: agent activation, task planning, and trajectory perception, allowing for comprehensive evaluation of visual reasoning in multi-agent scenarios. VIKI-R fine-tunes a pretrained vision-language model with Chain-of-Thought demonstrations and applies reinforcement learning to optimize performance across different tasks. The results demonstrate that VIKI-R significantly improves cooperation patterns among diverse agents, showcasing the potential of visual-driven strategies in embodied AI.'}, 'zh': {'title': 'VIKI-Bench与VIKI-R：推动具身智能体的视觉驱动合作', 'desc': 'VIKI-Bench和VIKI-R是用于评估和改善多样化具身智能体之间视觉驱动合作的基准和框架。该研究提出了一个分层基准，包含代理激活、任务规划和轨迹感知三个结构化层次。VIKI-R是一个两阶段框架，通过链式思维注释示例微调预训练的视觉语言模型，并在多层次奖励信号下进行强化学习。实验结果表明，VIKI-R在所有任务层次上显著优于基线方法，并且强化学习促进了异构智能体之间的组合合作模式。'}}}, {'id': 'https://huggingface.co/papers/2506.17201', 'title': 'Hunyuan-GameCraft: High-dynamic Interactive Game Video Generation with\n  Hybrid History Condition', 'url': 'https://huggingface.co/papers/2506.17201', 'abstract': 'Hunyuan-GameCraft is a novel framework for high-dynamic interactive video generation in game environments that addresses limitations in dynamics, generality, and efficiency through unified input representation, hybrid history-conditioned training, and model distillation.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in diffusion-based and controllable video generation have enabled high-quality and temporally coherent video synthesis, laying the groundwork for immersive interactive gaming experiences. However, current methods face limitations in dynamics, generality, long-term consistency, and efficiency, which limit the ability to create various gameplay videos. To address these gaps, we introduce Hunyuan-GameCraft, a novel framework for high-dynamic interactive video generation in game environments. To achieve fine-grained action control, we unify standard keyboard and mouse inputs into a shared camera representation space, facilitating smooth interpolation between various camera and movement operations. Then we propose a hybrid history-conditioned training strategy that extends video sequences autoregressively while preserving game scene information. Additionally, to enhance inference efficiency and playability, we achieve model distillation to reduce computational overhead while maintaining consistency across long temporal sequences, making it suitable for real-time deployment in complex interactive environments. The model is trained on a large-scale dataset comprising over one million gameplay recordings across over 100 AAA games, ensuring broad coverage and diversity, then fine-tuned on a carefully annotated synthetic dataset to enhance precision and control. The curated game scene data significantly improves the visual fidelity, realism and action controllability. Extensive experiments demonstrate that Hunyuan-GameCraft significantly outperforms existing models, advancing the realism and playability of interactive game video generation.', 'score': 23, 'issue_id': 4425, 'pub_date': '2025-06-20', 'pub_date_card': {'ru': '20 июня', 'en': 'June 20', 'zh': '6月20日'}, 'hash': 'def0daf064c03fe6', 'authors': ['Jiaqi Li', 'Junshu Tang', 'Zhiyong Xu', 'Longhuang Wu', 'Yuan Zhou', 'Shuai Shao', 'Tianbao Yu', 'Zhiguo Cao', 'Qinglin Lu'], 'affiliations': ['Huazhong University of Science and Technology', 'Tencent Hunyuan'], 'pdf_title_img': 'assets/pdf/title_img/2506.17201.jpg', 'data': {'categories': ['#inference', '#video', '#diffusion', '#synthetic', '#dataset', '#games', '#training'], 'emoji': '🎮', 'ru': {'title': 'Революция в генерации интерактивного игрового видео', 'desc': 'Hunyuan-GameCraft - это новая система для генерации интерактивного видео в игровых средах с высокой динамикой. Она решает проблемы ограничений в динамике, универсальности и эффективности с помощью унифицированного представления ввода, гибридного обучения с учетом истории и дистилляции модели. Система обучается на масштабном наборе данных из более чем миллиона записей геймплея из более 100 ААА-игр, а затем дообучается на тщательно размеченном синтетическом наборе данных для повышения точности и контроля. Эксперименты показывают, что Hunyuan-GameCraft значительно превосходит существующие модели, повышая реалистичность и играбельность интерактивной генерации игрового видео.'}, 'en': {'title': 'Revolutionizing Interactive Video Generation in Gaming', 'desc': 'Hunyuan-GameCraft is a new framework designed for generating interactive videos in gaming environments, overcoming issues related to dynamics, generality, and efficiency. It uses a unified input representation that combines keyboard and mouse controls, allowing for smooth transitions in camera and movement actions. The framework employs a hybrid history-conditioned training method to extend video sequences while keeping important game scene details intact. Additionally, model distillation is utilized to improve computational efficiency, making it suitable for real-time applications in complex gaming scenarios.'}, 'zh': {'title': 'Hunyuan-GameCraft：提升互动游戏视频生成的真实感与可玩性', 'desc': 'Hunyuan-GameCraft是一个新颖的框架，旨在解决游戏环境中高动态互动视频生成的局限性。它通过统一输入表示、混合历史条件训练和模型蒸馏来提高动态性、通用性和效率。该框架能够实现精细的动作控制，并在复杂的互动环境中保持长时间的一致性。经过大规模数据集的训练，Hunyuan-GameCraft在生成互动游戏视频的真实感和可玩性方面显著优于现有模型。'}}}, {'id': 'https://huggingface.co/papers/2506.16310', 'title': 'Optimizing Multilingual Text-To-Speech with Accents & Emotions', 'url': 'https://huggingface.co/papers/2506.16310', 'abstract': 'A new TTS architecture improves accent accuracy and emotion recognition for Hindi and Indian English by integrating phoneme alignment, culture-sensitive emotion embeddings, and dynamic accent code switching.  \t\t\t\t\tAI-generated summary \t\t\t\t State-of-the-art text-to-speech (TTS) systems realize high naturalness in monolingual environments, synthesizing speech with correct multilingual accents (especially for Indic languages) and context-relevant emotions still poses difficulty owing to cultural nuance discrepancies in current frameworks. This paper introduces a new TTS architecture integrating accent along with preserving transliteration with multi-scale emotion modelling, in particularly tuned for Hindi and Indian English accent. Our approach extends the Parler-TTS model by integrating A language-specific phoneme alignment hybrid encoder-decoder architecture, and culture-sensitive emotion embedding layers trained on native speaker corpora, as well as incorporating a dynamic accent code switching with residual vector quantization. Quantitative tests demonstrate 23.7% improvement in accent accuracy (Word Error Rate reduction from 15.4% to 11.8%) and 85.3% emotion recognition accuracy from native listeners, surpassing METTS and VECL-TTS baselines. The novelty of the system is that it can mix code in real time - generating statements such as "Namaste, let\'s talk about <Hindi phrase>" with uninterrupted accent shifts while preserving emotional consistency. Subjective evaluation with 200 users reported a mean opinion score (MOS) of 4.2/5 for cultural correctness, much better than existing multilingual systems (p<0.01). This research makes cross-lingual synthesis more feasible by showcasing scalable accent-emotion disentanglement, with direct application in South Asian EdTech and accessibility software.', 'score': 21, 'issue_id': 4436, 'pub_date': '2025-06-19', 'pub_date_card': {'ru': '19 июня', 'en': 'June 19', 'zh': '6月19日'}, 'hash': '5aff84e0535327a9', 'authors': ['Pranav Pawar', 'Akshansh Dwivedi', 'Jenish Boricha', 'Himanshu Gohil', 'Aditya Dubey'], 'affiliations': ['Dwarkadas J. Sanghvi College of Engineering, Mumbai, India'], 'pdf_title_img': 'assets/pdf/title_img/2506.16310.jpg', 'data': {'categories': ['#low_resource', '#machine_translation', '#multilingual', '#audio'], 'emoji': '🗣️', 'ru': {'title': 'Улучшение синтеза речи для индийских языков с учетом акцента и эмоций', 'desc': 'Статья представляет новую архитектуру синтеза речи (TTS) для хинди и индийского английского. Система интегрирует выравнивание фонем, культурно-чувствительные эмбеддинги эмоций и динамическое переключение акцентов. Количественные тесты показывают улучшение точности акцента на 23.7% и 85.3% точности распознавания эмоций. Субъективная оценка 200 пользователей дала среднюю оценку 4.2/5 за культурную корректность.'}, 'en': {'title': 'Enhancing TTS for Hindi and Indian English with Cultural Nuance', 'desc': 'This paper presents a new text-to-speech (TTS) architecture that enhances accent accuracy and emotion recognition specifically for Hindi and Indian English. It integrates phoneme alignment, culture-sensitive emotion embeddings, and dynamic accent code switching to address the challenges of synthesizing speech with cultural nuances. The proposed system shows a significant improvement in accent accuracy and emotion recognition, outperforming existing models. This innovation allows for real-time accent shifts while maintaining emotional consistency, making it particularly useful for applications in South Asian education technology and accessibility.'}, 'zh': {'title': '提升印地语与印度英语的语音合成准确性', 'desc': '这篇论文提出了一种新的文本到语音（TTS）架构，旨在提高印地语和印度英语的口音准确性和情感识别能力。该系统通过整合音素对齐、文化敏感的情感嵌入和动态口音切换，克服了现有框架在多语言环境中的局限性。研究表明，该模型在口音准确性上提高了23.7%，情感识别准确率达到了85.3%。这项研究为跨语言合成提供了更可行的解决方案，特别适用于南亚的教育科技和无障碍软件。'}}}, {'id': 'https://huggingface.co/papers/2506.17206', 'title': 'DreamCube: 3D Panorama Generation via Multi-plane Synchronization', 'url': 'https://huggingface.co/papers/2506.17206', 'abstract': 'Multi-plane synchronization extends 2D foundation models to 3D panorama generation, introducing DreamCube to achieve diverse appearances and accurate geometry.  \t\t\t\t\tAI-generated summary \t\t\t\t 3D panorama synthesis is a promising yet challenging task that demands high-quality and diverse visual appearance and geometry of the generated omnidirectional content. Existing methods leverage rich image priors from pre-trained 2D foundation models to circumvent the scarcity of 3D panoramic data, but the incompatibility between 3D panoramas and 2D single views limits their effectiveness. In this work, we demonstrate that by applying multi-plane synchronization to the operators from 2D foundation models, their capabilities can be seamlessly extended to the omnidirectional domain. Based on this design, we further introduce DreamCube, a multi-plane RGB-D diffusion model for 3D panorama generation, which maximizes the reuse of 2D foundation model priors to achieve diverse appearances and accurate geometry while maintaining multi-view consistency. Extensive experiments demonstrate the effectiveness of our approach in panoramic image generation, panoramic depth estimation, and 3D scene generation.', 'score': 15, 'issue_id': 4425, 'pub_date': '2025-06-20', 'pub_date_card': {'ru': '20 июня', 'en': 'June 20', 'zh': '6月20日'}, 'hash': 'bbec38fbe4e9ddd8', 'authors': ['Yukun Huang', 'Yanning Zhou', 'Jianan Wang', 'Kaiyi Huang', 'Xihui Liu'], 'affiliations': ['Astribot', 'Tencent', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2506.17206.jpg', 'data': {'categories': ['#optimization', '#diffusion', '#3d', '#cv'], 'emoji': '🌐', 'ru': {'title': 'Многоплоскостная синхронизация: мост между 2D и 3D в генерации панорам', 'desc': 'Эта статья представляет новый подход к генерации трехмерных панорам с использованием многоплоскостной синхронизации двумерных фундаментальных моделей. Авторы предлагают модель DreamCube, которая максимально использует преимущества двумерных моделей для создания разнообразных и геометрически точных трехмерных панорам. Метод решает проблему несовместимости между трехмерными панорамами и двумерными изображениями, расширяя возможности существующих алгоритмов. Эксперименты показывают эффективность подхода в генерации панорамных изображений, оценке глубины и создании трехмерных сцен.'}, 'en': {'title': 'DreamCube: Bridging 2D and 3D for Stunning Panoramas', 'desc': 'This paper presents a method called DreamCube that enhances 2D foundation models for generating 3D panoramas. It addresses the challenge of creating high-quality and diverse omnidirectional content by using multi-plane synchronization. This technique allows the model to effectively utilize existing 2D image data to improve the geometry and appearance of 3D images. The results show that DreamCube can generate consistent panoramic images and accurately estimate depth, making it a significant advancement in 3D scene generation.'}, 'zh': {'title': '多平面同步，开启三维全景新视界', 'desc': '本论文提出了一种名为DreamCube的多平面同步方法，旨在将二维基础模型扩展到三维全景生成。该方法通过利用预训练的二维模型的丰富图像先验，克服了三维全景数据稀缺的问题。我们的方法能够实现多样化的视觉效果和准确的几何形状，同时保持多视图的一致性。实验结果表明，我们的技术在全景图像生成、深度估计和三维场景生成方面表现出色。'}}}, {'id': 'https://huggingface.co/papers/2506.16504', 'title': 'Hunyuan3D 2.5: Towards High-Fidelity 3D Assets Generation with Ultimate\n  Details', 'url': 'https://huggingface.co/papers/2506.16504', 'abstract': 'Hunyuan3D 2.5, a suite of 3D diffusion models, advances shape and texture generation with a new LATTICE model and physical-based rendering in a multi-view architecture.  \t\t\t\t\tAI-generated summary \t\t\t\t In this report, we present Hunyuan3D 2.5, a robust suite of 3D diffusion models aimed at generating high-fidelity and detailed textured 3D assets. Hunyuan3D 2.5 follows two-stages pipeline of its previous version Hunyuan3D 2.0, while demonstrating substantial advancements in both shape and texture generation. In terms of shape generation, we introduce a new shape foundation model -- LATTICE, which is trained with scaled high-quality datasets, model-size, and compute. Our largest model reaches 10B parameters and generates sharp and detailed 3D shape with precise image-3D following while keeping mesh surface clean and smooth, significantly closing the gap between generated and handcrafted 3D shapes. In terms of texture generation, it is upgraded with phyiscal-based rendering (PBR) via a novel multi-view architecture extended from Hunyuan3D 2.0 Paint model. Our extensive evaluation shows that Hunyuan3D 2.5 significantly outperforms previous methods in both shape and end-to-end texture generation.', 'score': 12, 'issue_id': 4424, 'pub_date': '2025-06-19', 'pub_date_card': {'ru': '19 июня', 'en': 'June 19', 'zh': '6月19日'}, 'hash': '7ea16d5712b67fcc', 'authors': ['Zeqiang Lai', 'Yunfei Zhao', 'Haolin Liu', 'Zibo Zhao', 'Qingxiang Lin', 'Huiwen Shi', 'Xianghui Yang', 'Mingxin Yang', 'Shuhui Yang', 'Yifei Feng', 'Sheng Zhang', 'Xin Huang', 'Di Luo', 'Fan Yang', 'Fang Yang', 'Lifu Wang', 'Sicong Liu', 'Yixuan Tang', 'Yulin Cai', 'Zebin He', 'Tian Liu', 'Yuhong Liu', 'Jie Jiang', 'Linus', 'Jingwei Huang', 'Chunchao Guo'], 'affiliations': ['Tencent'], 'pdf_title_img': 'assets/pdf/title_img/2506.16504.jpg', 'data': {'categories': ['#diffusion', '#3d'], 'emoji': '🧊', 'ru': {'title': 'Новый уровень 3D-генерации: точные формы и реалистичные текстуры', 'desc': 'Hunyuan3D 2.5 представляет собой набор моделей диффузии для 3D-генерации, улучшающий создание форм и текстур. Ключевое нововведение - модель LATTICE для генерации форм, обученная на масштабных высококачественных датасетах. Для генерации текстур используется физически корректный рендеринг (PBR) в многоракурсной архитектуре. Согласно оценкам, Hunyuan3D 2.5 значительно превосходит предыдущие методы в генерации как форм, так и текстур.'}, 'en': {'title': 'Revolutionizing 3D Asset Generation with Hunyuan3D 2.5', 'desc': 'Hunyuan3D 2.5 is a suite of advanced 3D diffusion models designed to enhance the generation of high-quality 3D shapes and textures. It introduces the LATTICE model, a new foundation for shape generation that utilizes large, high-quality datasets and boasts a model size of up to 10 billion parameters. This model produces sharp and detailed 3D shapes while maintaining clean mesh surfaces, bridging the gap between generated and handcrafted designs. Additionally, the suite incorporates physical-based rendering in a multi-view architecture to improve texture generation, demonstrating significant improvements over previous versions.'}, 'zh': {'title': 'Hunyuan3D 2.5：形状与纹理生成的新突破', 'desc': 'Hunyuan3D 2.5 是一套先进的 3D 扩散模型，旨在生成高保真和细致的纹理 3D 资产。它在形状生成方面引入了新的基础模型 LATTICE，经过大规模高质量数据集的训练，能够生成清晰且细致的 3D 形状。纹理生成方面，采用了基于物理的渲染（PBR）技术，结合了多视角架构，显著提升了纹理质量。综合评估表明，Hunyuan3D 2.5 在形状和纹理生成方面均优于之前的方法。'}}}, {'id': 'https://huggingface.co/papers/2506.17218', 'title': 'Machine Mental Imagery: Empower Multimodal Reasoning with Latent Visual\n  Tokens', 'url': 'https://huggingface.co/papers/2506.17218', 'abstract': "Mirage enhances vision-language models by integrating latent visual tokens into text decoding to improve multimodal reasoning without generating explicit images.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-language models (VLMs) excel at multimodal understanding, yet their text-only decoding forces them to verbalize visual reasoning, limiting performance on tasks that demand visual imagination. Recent attempts train VLMs to render explicit images, but the heavy image-generation pre-training often hinders the reasoning ability. Inspired by the way humans reason with mental imagery-the internal construction and manipulation of visual cues-we investigate whether VLMs can reason through interleaved multimodal trajectories without producing explicit images. To this end, we present a Machine Mental Imagery framework, dubbed as Mirage, which augments VLM decoding with latent visual tokens alongside ordinary text. Concretely, whenever the model chooses to ``think visually'', it recasts its hidden states as next tokens, thereby continuing a multimodal trajectory without generating pixel-level images. Begin by supervising the latent tokens through distillation from ground-truth image embeddings, we then switch to text-only supervision to make the latent trajectory align tightly with the task objective. A subsequent reinforcement learning stage further enhances the multimodal reasoning capability. Experiments on diverse benchmarks demonstrate that Mirage unlocks stronger multimodal reasoning without explicit image generation.", 'score': 10, 'issue_id': 4439, 'pub_date': '2025-06-20', 'pub_date_card': {'ru': '20 июня', 'en': 'June 20', 'zh': '6月20日'}, 'hash': '8f0feb2578145e77', 'authors': ['Zeyuan Yang', 'Xueyang Yu', 'Delin Chen', 'Maohao Shen', 'Chuang Gan'], 'affiliations': ['Massachusetts Institute of Technology', 'University of Massachusetts, Amherst'], 'pdf_title_img': 'assets/pdf/title_img/2506.17218.jpg', 'data': {'categories': ['#multimodal', '#rl', '#reasoning', '#games'], 'emoji': '🧠', 'ru': {'title': 'Ментальные образы для ИИ: улучшение мультимодального мышления без генерации изображений', 'desc': "Статья представляет новый подход под названием Mirage для улучшения мультимодальных языковых моделей. Mirage интегрирует латентные визуальные токены в процесс декодирования текста, что позволяет улучшить мультимодальные рассуждения без генерации явных изображений. Этот метод вдохновлен человеческой способностью к ментальным образам и позволяет модели 'мыслить визуально' во время обработки текста. Эксперименты показывают, что Mirage значительно улучшает способности моделей к мультимодальным рассуждениям."}, 'en': {'title': 'Unlocking Multimodal Reasoning with Mental Imagery', 'desc': 'The paper introduces Mirage, a framework that enhances vision-language models (VLMs) by incorporating latent visual tokens into the text decoding process. This approach allows the models to perform multimodal reasoning without the need to generate explicit images, which can limit their performance. By mimicking human mental imagery, Mirage enables VLMs to interleave visual and textual information effectively. The framework is trained using a combination of distillation from image embeddings and reinforcement learning, resulting in improved reasoning capabilities across various tasks.'}, 'zh': {'title': 'Mirage：无图像生成的多模态推理增强', 'desc': 'Mirage是一种增强视觉-语言模型的方法，通过将潜在视觉标记整合到文本解码中，提升多模态推理能力，而无需生成明确的图像。该方法借鉴了人类通过心理意象进行推理的方式，允许模型在不生成图像的情况下进行多模态推理。具体来说，当模型选择“视觉思考”时，它会将隐藏状态重新转换为下一个标记，从而继续多模态轨迹。实验结果表明，Mirage在多种基准测试中表现出更强的多模态推理能力。'}}}, {'id': 'https://huggingface.co/papers/2506.15745', 'title': 'InfiniPot-V: Memory-Constrained KV Cache Compression for Streaming Video\n  Understanding', 'url': 'https://huggingface.co/papers/2506.15745', 'abstract': 'InfiniPot-V is a training-free, query-agnostic framework that compresses the key-value cache during video encoding to maintain a fixed memory cap for streaming video understanding, enhancing real-time performance and accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Modern multimodal large language models (MLLMs) can reason over hour-long video, yet their key-value (KV) cache grows linearly with time--quickly exceeding the fixed memory of phones, AR glasses, and edge robots. Prior compression schemes either assume the whole video and user query are available offline or must first build the full cache, so memory still scales with stream length. InfiniPot-V is the first training-free, query-agnostic framework that enforces a hard, length-independent memory cap for streaming video understanding. During video encoding it monitors the cache and, once a user-set threshold is reached, runs a lightweight compression pass that (i) removes temporally redundant tokens via Temporal-axis Redundancy (TaR) metric and (ii) keeps semantically significant tokens via Value-Norm (VaN) ranking. Across four open-source MLLMs and four long-video and two streaming-video benchmarks, InfiniPot-V cuts peak GPU memory by up to 94%, sustains real-time generation, and matches or surpasses full-cache accuracy--even in multi-turn dialogues. By dissolving the KV cache bottleneck without retraining or query knowledge, InfiniPot-V closes the gap for on-device streaming video assistants.', 'score': 8, 'issue_id': 4431, 'pub_date': '2025-06-18', 'pub_date_card': {'ru': '18 июня', 'en': 'June 18', 'zh': '6月18日'}, 'hash': 'f6babb4a5e89bb3f', 'authors': ['Minsoo Kim', 'Kyuhong Shim', 'Jungwook Choi', 'Simyung Chang'], 'affiliations': ['Hanyang University', 'Qualcomm AI Research, Qualcomm Korea', 'Sungkyunkwan University'], 'pdf_title_img': 'assets/pdf/title_img/2506.15745.jpg', 'data': {'categories': ['#training', '#multimodal', '#optimization', '#long_context', '#video'], 'emoji': '🎥', 'ru': {'title': 'Эффективное сжатие кэша для потокового анализа видео без потери точности', 'desc': 'InfiniPot-V - это фреймворк для сжатия кэша ключ-значение при кодировании видео, не требующий обучения и независимый от запросов. Он поддерживает фиксированный объем памяти для потокового анализа видео, улучшая производительность и точность в реальном времени. Фреймворк использует метрику временной избыточности (TaR) для удаления избыточных токенов и ранжирование по норме значений (VaN) для сохранения семантически значимых токенов. InfiniPot-V сокращает пиковое использование памяти GPU до 94%, обеспечивая генерацию в реальном времени и сохраняя или превосходя точность полного кэша.'}, 'en': {'title': 'Streamline Video Understanding with InfiniPot-V!', 'desc': 'InfiniPot-V is a novel framework designed to optimize memory usage during video encoding for real-time video understanding. It operates without the need for training or prior knowledge of user queries, making it flexible and efficient. The framework compresses the key-value cache by removing redundant information while preserving important data, ensuring that memory usage remains constant regardless of video length. This approach significantly reduces GPU memory requirements while maintaining high accuracy in video processing tasks.'}, 'zh': {'title': 'InfiniPot-V：实时视频理解的内存压缩新方案', 'desc': 'InfiniPot-V 是一个无需训练且与查询无关的框架，旨在压缩视频编码中的关键值缓存，以保持固定的内存限制，从而提升实时性能和准确性。该框架通过监控缓存，当达到用户设定的阈值时，执行轻量级压缩，去除时间上冗余的标记，并保留语义上重要的标记。与以往的压缩方案不同，InfiniPot-V 不需要离线获取整个视频或用户查询，因此能够有效控制内存使用。通过在多个开源多模态大语言模型和视频基准测试中验证，InfiniPot-V 显著降低了 GPU 内存峰值，同时保持实时生成和准确性。'}}}, {'id': 'https://huggingface.co/papers/2506.15442', 'title': 'Hunyuan3D 2.1: From Images to High-Fidelity 3D Assets with\n  Production-Ready PBR Material', 'url': 'https://huggingface.co/papers/2506.15442', 'abstract': 'The tutorial provides a comprehensive guide on using Hunyuan3D 2.1 for generating high-resolution, textured 3D models, covering data preparation, model architecture, training, evaluation, and deployment.  \t\t\t\t\tAI-generated summary \t\t\t\t 3D AI-generated content (AIGC) is a passionate field that has significantly accelerated the creation of 3D models in gaming, film, and design. Despite the development of several groundbreaking models that have revolutionized 3D generation, the field remains largely accessible only to researchers, developers, and designers due to the complexities involved in collecting, processing, and training 3D models. To address these challenges, we introduce Hunyuan3D 2.1 as a case study in this tutorial. This tutorial offers a comprehensive, step-by-step guide on processing 3D data, training a 3D generative model, and evaluating its performance using Hunyuan3D 2.1, an advanced system for producing high-resolution, textured 3D assets. The system comprises two core components: the Hunyuan3D-DiT for shape generation and the Hunyuan3D-Paint for texture synthesis. We will explore the entire workflow, including data preparation, model architecture, training strategies, evaluation metrics, and deployment. By the conclusion of this tutorial, you will have the knowledge to finetune or develop a robust 3D generative model suitable for applications in gaming, virtual reality, and industrial design.', 'score': 7, 'issue_id': 4431, 'pub_date': '2025-06-18', 'pub_date_card': {'ru': '18 июня', 'en': 'June 18', 'zh': '6月18日'}, 'hash': 'c5c9b452dd58395e', 'authors': ['Team Hunyuan3D', 'Shuhui Yang', 'Mingxin Yang', 'Yifei Feng', 'Xin Huang', 'Sheng Zhang', 'Zebin He', 'Di Luo', 'Haolin Liu', 'Yunfei Zhao', 'Qingxiang Lin', 'Zeqiang Lai', 'Xianghui Yang', 'Huiwen Shi', 'Zibo Zhao', 'Bowen Zhang', 'Hongyu Yan', 'Lifu Wang', 'Sicong Liu', 'Jihong Zhang', 'Meng Chen', 'Liang Dong', 'Yiwen Jia', 'Yulin Cai', 'Jiaao Yu', 'Yixuan Tang', 'Dongyuan Guo', 'Junlin Yu', 'Hao Zhang', 'Zheng Ye', 'Peng He', 'Runzhou Wu', 'Shida Wei', 'Chao Zhang', 'Yonghao Tan', 'Yifu Sun', 'Lin Niu', 'Shirui Huang', 'Bojian Zheng', 'Shu Liu', 'Shilin Chen', 'Xiang Yuan', 'Xiaofeng Yang', 'Kai Liu', 'Jianchen Zhu', 'Peng Chen', 'Tian Liu', 'Di Wang', 'Yuhong Liu', 'Linus', 'Jie Jiang', 'Jingwei Huang', 'Chunchao Guo'], 'affiliations': ['Tencent Hunyuan'], 'pdf_title_img': 'assets/pdf/title_img/2506.15442.jpg', 'data': {'categories': ['#training', '#3d', '#architecture', '#synthetic', '#games', '#data'], 'emoji': '🎨', 'ru': {'title': 'Революция в 3D-моделировании: доступное руководство по Hunyuan3D 2.1', 'desc': 'Статья представляет собой подробное руководство по использованию Hunyuan3D 2.1 для генерации высококачественных текстурированных 3D-моделей. В ней рассматриваются этапы подготовки данных, архитектура модели, процесс обучения, методы оценки и развертывания. Система Hunyuan3D 2.1 состоит из двух основных компонентов: Hunyuan3D-DiT для генерации форм и Hunyuan3D-Paint для синтеза текстур. Цель руководства - сделать технологию 3D-генерации более доступной для исследователей, разработчиков и дизайнеров.'}, 'en': {'title': 'Unlocking 3D Model Creation with Hunyuan3D 2.1', 'desc': 'This paper presents a tutorial on Hunyuan3D 2.1, a system designed for generating high-resolution, textured 3D models. It addresses the complexities of 3D model creation by providing a detailed guide on data preparation, model architecture, training, evaluation, and deployment. The system features two main components: Hunyuan3D-DiT for shape generation and Hunyuan3D-Paint for texture synthesis. By following this tutorial, users will learn how to finetune or create effective 3D generative models for various applications such as gaming and virtual reality.'}, 'zh': {'title': '掌握Hunyuan3D 2.1，轻松生成高质量3D模型', 'desc': '本教程全面介绍了如何使用Hunyuan3D 2.1生成高分辨率、带纹理的3D模型，涵盖了数据准备、模型架构、训练、评估和部署等方面。尽管3D AI生成内容领域取得了显著进展，但由于收集、处理和训练3D模型的复杂性，仍然主要面向研究人员和开发者。Hunyuan3D 2.1作为案例研究，提供了逐步指导，帮助用户处理3D数据、训练生成模型并评估其性能。通过本教程，您将掌握微调或开发适用于游戏、虚拟现实和工业设计的强大3D生成模型的知识。'}}}, {'id': 'https://huggingface.co/papers/2506.17202', 'title': 'UniFork: Exploring Modality Alignment for Unified Multimodal\n  Understanding and Generation', 'url': 'https://huggingface.co/papers/2506.17202', 'abstract': 'A Y-shaped architecture, UniFork, balances shared learning and task specialization for unified image understanding and generation, outperforming conventional fully shared Transformer models.  \t\t\t\t\tAI-generated summary \t\t\t\t Unified image understanding and generation has emerged as a promising paradigm in multimodal artificial intelligence. Despite recent progress, the optimal architectural design for such unified models remains an open challenge. In this work, we start by analyzing the modality alignment behaviors of task-specific expert models for understanding and generation, as well as current unified models. Our analysis reveals a crucial observation: understanding tasks benefit from a progressively increasing modality alignment across network depth, which helps build up semantic information for better comprehension; In contrast, generation tasks follow a different trend: modality alignment increases in the early layers but decreases in the deep layers to recover spatial details. These divergent alignment patterns create a fundamental conflict in fully shared Transformer backbones, where a uniform representational flow often leads to performance compromises across two tasks. Motivated by this finding, we introduce UniFork, a novel Y-shaped architecture that shares the shallow layers for cross-task representation learning, while employing task-specific branches in deeper layers to avoid task interference. This design effectively balances shared learning and task specialization. Through extensive ablation experiments, we demonstrate that Unifork consistently outperforms conventional fully shared Transformer architectures, and achieves performance on par with or better than task-specific models.', 'score': 5, 'issue_id': 4432, 'pub_date': '2025-06-20', 'pub_date_card': {'ru': '20 июня', 'en': 'June 20', 'zh': '6月20日'}, 'hash': '3796f42ead32837a', 'authors': ['Teng Li', 'Quanfeng Lu', 'Lirui Zhao', 'Hao Li', 'Xizhou Zhu', 'Yu Qiao', 'Jun Zhang', 'Wenqi Shao'], 'affiliations': ['HKUST', 'SJTU', 'Shanghai AI Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2506.17202.jpg', 'data': {'categories': ['#optimization', '#architecture', '#multimodal'], 'emoji': '🍴', 'ru': {'title': 'UniFork: оптимальный баланс между общим обучением и специализацией задач', 'desc': 'UniFork - это новая Y-образная архитектура для объединенного понимания и генерации изображений. Она сочетает общие слои для обучения между задачами и специализированные ветви для избежания интерференции. Анализ показал, что задачи понимания и генерации требуют разных паттернов выравнивания модальностей. UniFork превосходит обычные полностью разделяемые трансформерные модели и достигает результатов на уровне специализированных моделей.'}, 'en': {'title': 'UniFork: Balancing Shared Learning and Task Specialization in Image AI', 'desc': 'The paper introduces UniFork, a Y-shaped architecture designed to improve unified image understanding and generation in machine learning. It addresses the challenge of balancing shared learning and task specialization by analyzing how different tasks align modalities at various network depths. The findings show that understanding tasks benefit from deeper alignment, while generation tasks require a different approach, leading to conflicts in traditional models. UniFork resolves these issues by sharing shallow layers for common learning and using specialized branches for deeper layers, resulting in superior performance compared to fully shared Transformer models.'}, 'zh': {'title': 'UniFork：平衡共享学习与任务专门化的创新架构', 'desc': '本文提出了一种新的Y形架构UniFork，旨在平衡共享学习和任务专门化，以实现统一的图像理解和生成。研究发现，理解任务在网络深度上需要逐渐增加的模态对齐，而生成任务则在早期层增加模态对齐但在深层减少，以恢复空间细节。这种对齐模式的差异在传统的完全共享Transformer模型中造成了性能妥协。UniFork通过在浅层共享表示学习，在深层采用任务特定的分支，有效地解决了这一问题，实验结果表明其性能优于传统模型。'}}}, {'id': 'https://huggingface.co/papers/2506.17213', 'title': 'Long-term Traffic Simulation with Interleaved Autoregressive Motion and\n  Scenario Generation', 'url': 'https://huggingface.co/papers/2506.17213', 'abstract': 'InfGen, a unified next-token prediction model, enables stable long-term traffic simulation by interleaving closed-loop motion simulation and scene generation.  \t\t\t\t\tAI-generated summary \t\t\t\t An ideal traffic simulator replicates the realistic long-term point-to-point trip that a self-driving system experiences during deployment. Prior models and benchmarks focus on closed-loop motion simulation for initial agents in a scene. This is problematic for long-term simulation. Agents enter and exit the scene as the ego vehicle enters new regions. We propose InfGen, a unified next-token prediction model that performs interleaved closed-loop motion simulation and scene generation. InfGen automatically switches between closed-loop motion simulation and scene generation mode. It enables stable long-term rollout simulation. InfGen performs at the state-of-the-art in short-term (9s) traffic simulation, and significantly outperforms all other methods in long-term (30s) simulation. The code and model of InfGen will be released at https://orangesodahub.github.io/InfGen', 'score': 4, 'issue_id': 4431, 'pub_date': '2025-06-20', 'pub_date_card': {'ru': '20 июня', 'en': 'June 20', 'zh': '6月20日'}, 'hash': '0940c34932f5181b', 'authors': ['Xiuyu Yang', 'Shuhan Tan', 'Philipp Krähenbühl'], 'affiliations': ['UT Austin'], 'pdf_title_img': 'assets/pdf/title_img/2506.17213.jpg', 'data': {'categories': ['#training', '#optimization', '#agents', '#open_source', '#video'], 'emoji': '🚗', 'ru': {'title': 'InfGen: Революция в долгосрочном моделировании дорожного движения', 'desc': 'InfGen - это унифицированная модель предсказания следующего токена, которая позволяет проводить стабильное долгосрочное моделирование дорожного движения. Она объединяет симуляцию движения в замкнутом цикле и генерацию сцен. InfGen автоматически переключается между режимами симуляции движения и генерации сцен, что обеспечивает стабильное долгосрочное моделирование. Модель демонстрирует лучшие результаты как в краткосрочном (9 секунд), так и в долгосрочном (30 секунд) моделировании трафика.'}, 'en': {'title': 'InfGen: Revolutionizing Long-Term Traffic Simulation with Unified Prediction', 'desc': 'InfGen is a novel model designed for traffic simulation that predicts the next movement of vehicles in a scene. It combines two key processes: closed-loop motion simulation, which tracks the movement of vehicles, and scene generation, which creates the environment around them. This interleaving allows InfGen to maintain stability during long-term simulations, addressing the challenges faced by previous models that struggled with dynamic agent interactions. The model demonstrates superior performance in both short-term and long-term traffic simulations, making it a significant advancement in the field.'}, 'zh': {'title': 'InfGen：稳定的长期交通仿真新方法', 'desc': 'InfGen是一种统一的下一个标记预测模型，能够通过交替进行闭环运动仿真和场景生成，实现稳定的长期交通仿真。传统模型主要关注初始代理的闭环运动仿真，这在长期仿真中存在问题，因为代理在自驾车进入新区域时会进出场景。InfGen能够自动切换闭环运动仿真和场景生成模式，从而实现稳定的长期仿真。该模型在短期（9秒）交通仿真中表现出色，并在长期（30秒）仿真中显著优于其他方法。'}}}, {'id': 'https://huggingface.co/papers/2506.15925', 'title': 'Reranking-based Generation for Unbiased Perspective Summarization', 'url': 'https://huggingface.co/papers/2506.15925', 'abstract': 'Reranking and preference tuning improve the quality of perspective summaries generated by LLMs, as measured by language model-based metrics that outperform traditional ones.  \t\t\t\t\tAI-generated summary \t\t\t\t Generating unbiased summaries in real-world settings such as political perspective summarization remains a crucial application of Large Language Models (LLMs). Yet, existing evaluation frameworks rely on traditional metrics for measuring key attributes such as coverage and faithfulness without verifying their applicability, and efforts to develop improved summarizers are still nascent. We address these gaps by (1) identifying reliable metrics for measuring perspective summary quality, and (2) investigating the efficacy of LLM-based methods beyond zero-shot inference. Namely, we build a test set for benchmarking metric reliability using human annotations and show that traditional metrics underperform compared to language model-based metrics, which prove to be strong evaluators. Using these metrics, we show that reranking-based methods yield strong results, and preference tuning with synthetically generated and reranking-labeled data further boosts performance. Our findings aim to contribute to the reliable evaluation and development of perspective summarization methods.', 'score': 4, 'issue_id': 4425, 'pub_date': '2025-06-19', 'pub_date_card': {'ru': '19 июня', 'en': 'June 19', 'zh': '6月19日'}, 'hash': '8cb1a06d9566bcfc', 'authors': ['Narutatsu Ri', 'Nicholas Deas', 'Kathleen McKeown'], 'affiliations': ['Department of Computer Science, Columbia University'], 'pdf_title_img': 'assets/pdf/title_img/2506.15925.jpg', 'data': {'categories': ['#synthetic', '#multimodal', '#dataset', '#interpretability', '#alignment', '#training', '#benchmark'], 'emoji': '🔍', 'ru': {'title': 'Улучшение генерации обзоров с разных точек зрения с помощью ранжирования и настройки LLM', 'desc': 'Исследование посвящено улучшению качества генерации кратких обзоров с учетом различных точек зрения с помощью больших языковых моделей (LLM). Авторы разработали надежные метрики для оценки качества таких обзоров, основанные на языковых моделях. Они показали, что методы ранжирования и настройки предпочтений с использованием синтетических данных значительно повышают эффективность генерации. Результаты исследования вносят вклад в развитие методов надежной оценки и создания систем суммаризации с учетом различных перспектив.'}, 'en': {'title': 'Enhancing Perspective Summaries with Reranking and Preference Tuning', 'desc': 'This paper discusses how to improve the quality of summaries generated by Large Language Models (LLMs) for political perspectives. It highlights the limitations of traditional evaluation metrics, which do not effectively measure important aspects like coverage and faithfulness. The authors propose new, reliable metrics based on language models and demonstrate that these outperform traditional methods. They also show that using reranking and preference tuning techniques can significantly enhance the performance of LLM-generated summaries.'}, 'zh': {'title': '提升观点摘要质量的关键在于重排序与偏好调优', 'desc': '本文探讨了如何提高大型语言模型（LLMs）生成的观点摘要的质量。我们发现，传统的评估指标在测量摘要的覆盖率和忠实度时效果不佳，而基于语言模型的指标表现更为出色。通过建立一个基准测试集并进行人类标注，我们验证了这些新指标的可靠性。最终，我们提出了重排序和偏好调优的方法，显著提升了摘要生成的效果。'}}}, {'id': 'https://huggingface.co/papers/2506.09930', 'title': 'From Intention to Execution: Probing the Generalization Boundaries of\n  Vision-Language-Action Models', 'url': 'https://huggingface.co/papers/2506.09930', 'abstract': 'A unified benchmark suite evaluates Vision-Language-Action models\' generalization and motor execution capabilities, highlighting the disparity between perceptual understanding and precise action execution.  \t\t\t\t\tAI-generated summary \t\t\t\t One promise that Vision-Language-Action (VLA) models hold over traditional imitation learning for robotics is to leverage the broad generalization capabilities of large Vision-Language Models (VLMs) to produce versatile, "generalist" robot policies. However, current evaluations of VLAs remain insufficient. Traditional imitation learning benchmarks are unsuitable due to the lack of language instructions. Emerging benchmarks for VLAs that incorporate language often come with limited evaluation tasks and do not intend to investigate how much VLM pretraining truly contributes to the generalization capabilities of the downstream robotic policy. Meanwhile, much research relies on real-world robot setups designed in isolation by different institutions, which creates a barrier for reproducibility and accessibility. To address this gap, we introduce a unified probing suite of 50 simulation-based tasks across 10 subcategories spanning language instruction, vision, and objects. We systematically evaluate several state-of-the-art VLA architectures on this suite to understand their generalization capability. Our results show that while VLM backbones endow VLAs with robust perceptual understanding and high level planning, which we refer to as good intentions, this does not reliably translate into precise motor execution: when faced with out-of-distribution observations, policies often exhibit coherent intentions, but falter in action execution. Moreover, finetuning on action data can erode the original VLM\'s generalist reasoning abilities. We release our task suite and evaluation code to serve as a standardized benchmark for future VLAs and to drive research on closing the perception-to-action gap. More information, including the source code, can be found at https://ai4ce.github.io/INT-ACT/', 'score': 4, 'issue_id': 4438, 'pub_date': '2025-06-11', 'pub_date_card': {'ru': '11 июня', 'en': 'June 11', 'zh': '6月11日'}, 'hash': '62044c4dafedefe4', 'authors': ['Irving Fang', 'Juexiao Zhang', 'Shengbang Tong', 'Chen Feng'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2506.09930.jpg', 'data': {'categories': ['#cv', '#agi', '#optimization', '#survey', '#robotics', '#agents', '#benchmark', '#games'], 'emoji': '🤖', 'ru': {'title': 'Разрыв между намерением и действием: оценка моделей VLA в робототехнике', 'desc': 'Статья представляет унифицированный набор тестов для оценки моделей Vision-Language-Action (VLA) в робототехнике. Исследование показывает, что несмотря на хорошее понимание и планирование высокого уровня, модели VLA часто не справляются с точным выполнением действий при встрече с нестандартными ситуациями. Авторы разработали набор из 50 задач в симуляции для оценки обобщающей способности VLA моделей. Результаты указывают на разрыв между перцептивным пониманием и точным выполнением действий в робототехнике.'}, 'en': {'title': 'Bridging the Gap: Evaluating Vision-Language-Action Models for Better Robot Execution', 'desc': "This paper presents a unified benchmark suite designed to evaluate Vision-Language-Action (VLA) models, focusing on their ability to generalize and execute motor actions. It highlights the gap between a model's perceptual understanding, derived from large Vision-Language Models (VLMs), and its actual performance in executing precise actions. The study reveals that while VLA models can plan effectively, they struggle with action execution, especially when faced with unfamiliar scenarios. The authors provide a comprehensive set of simulation-based tasks to facilitate standardized evaluations and encourage further research in bridging the perception-to-action divide."}, 'zh': {'title': '缩小感知与动作之间的差距', 'desc': '本文提出了一个统一的基准套件，用于评估视觉-语言-动作（VLA）模型的泛化能力和运动执行能力。研究发现，尽管大型视觉-语言模型（VLM）能够提供良好的感知理解和高层次规划，但在面对不同分布的观察时，模型的动作执行能力常常不足。我们设计了50个基于仿真的任务，涵盖语言指令、视觉和物体等多个子类别，以系统性地评估当前最先进的VLA架构。最终，我们希望通过这个基准套件推动未来VLA的研究，缩小感知与动作之间的差距。'}}}, {'id': 'https://huggingface.co/papers/2506.17113', 'title': 'MEXA: Towards General Multimodal Reasoning with Dynamic Multi-Expert\n  Aggregation', 'url': 'https://huggingface.co/papers/2506.17113', 'abstract': 'MEXA is a training-free framework that aggregates outputs from specialized expert models using a Large Reasoning Model for effective multimodal reasoning across various domains.  \t\t\t\t\tAI-generated summary \t\t\t\t Combining pre-trained expert models offers substantial potential for scalable multimodal reasoning, but building a unified framework remains challenging due to the increasing diversity of input modalities and task complexity. For instance, medical diagnosis requires precise reasoning over structured clinical tables, while financial forecasting depends on interpreting plot-based data to make informed predictions. To tackle this challenge, we introduce MEXA, a training-free framework that performs modality- and task-aware aggregation of multiple expert models to enable effective multimodal reasoning across diverse and distinct domains. MEXA dynamically selects expert models based on the input modality and the task-specific reasoning demands (i.e., skills). Each expert model, specialized in a modality task pair, generates interpretable textual reasoning outputs. MEXA then aggregates and reasons over these outputs using a Large Reasoning Model (LRM) to produce the final answer. This modular design allows flexible and transparent multimodal reasoning across diverse domains without additional training overhead. We extensively evaluate our approach on diverse multimodal benchmarks, including Video Reasoning, Audio Reasoning, 3D Understanding, and Medical QA. MEXA consistently delivers performance improvements over strong multimodal baselines, highlighting the effectiveness and broad applicability of our expert-driven selection and aggregation in diverse multimodal reasoning tasks.', 'score': 2, 'issue_id': 4439, 'pub_date': '2025-06-20', 'pub_date_card': {'ru': '20 июня', 'en': 'June 20', 'zh': '6月20日'}, 'hash': '55a36cc78de699ca', 'authors': ['Shoubin Yu', 'Yue Zhang', 'Ziyang Wang', 'Jaehong Yoon', 'Mohit Bansal'], 'affiliations': ['UNC Chapel Hill'], 'pdf_title_img': 'assets/pdf/title_img/2506.17113.jpg', 'data': {'categories': ['#multimodal', '#reasoning', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'MEXA: Умное объединение экспертов для мультимодального ИИ', 'desc': 'MEXA - это фреймворк для мультимодального рассуждения, который не требует дополнительного обучения. Он агрегирует выходные данные специализированных экспертных моделей, используя крупную модель рассуждений (LRM). MEXA динамически выбирает экспертные модели на основе входной модальности и требований к конкретной задаче. Этот подход позволяет гибко и прозрачно выполнять мультимодальные рассуждения в различных областях без дополнительных затрат на обучение.'}, 'en': {'title': 'MEXA: Expert-Driven Multimodal Reasoning Without Training', 'desc': 'MEXA is a novel framework designed for multimodal reasoning that does not require additional training. It effectively combines outputs from specialized expert models tailored to different input modalities and tasks. By dynamically selecting the appropriate expert models based on the specific reasoning needs, MEXA aggregates their outputs using a Large Reasoning Model to generate coherent and interpretable results. This approach enhances performance across various domains, such as medical diagnosis and financial forecasting, while maintaining flexibility and transparency in the reasoning process.'}, 'zh': {'title': 'MEXA：无训练的多模态推理框架', 'desc': 'MEXA是一个无需训练的框架，通过使用大型推理模型聚合来自专业专家模型的输出，以实现有效的多模态推理。该框架能够根据输入的模态和特定任务的推理需求动态选择专家模型，从而处理不同领域的复杂任务。每个专家模型专注于特定的模态任务对，生成可解释的文本推理输出。MEXA通过聚合这些输出，利用大型推理模型生成最终答案，展现了在多种多模态基准测试中的优越性能。'}}}, {'id': 'https://huggingface.co/papers/2506.17090', 'title': 'Better Language Model Inversion by Compactly Representing Next-Token\n  Distributions', 'url': 'https://huggingface.co/papers/2506.17090', 'abstract': "A new method called Prompt Inversion from Logprob Sequences (PILS) recovers hidden prompts in language models by analyzing the low-dimensional subspace of the model's next-token probabilities, achieving higher recovery rates and better generalization than previous methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Language model inversion seeks to recover hidden prompts using only language model outputs. This capability has implications for security and accountability in language model deployments, such as leaking private information from an API-protected language model's system message. We propose a new method -- prompt inversion from logprob sequences (PILS) -- that recovers hidden prompts by gleaning clues from the model's next-token probabilities over the course of multiple generation steps. Our method is enabled by a key insight: The vector-valued outputs of a language model occupy a low-dimensional subspace. This enables us to losslessly compress the full next-token probability distribution over multiple generation steps using a linear map, allowing more output information to be used for inversion. Our approach yields massive gains over previous state-of-the-art methods for recovering hidden prompts, achieving 2--3.5 times higher exact recovery rates across test sets, in one case increasing the recovery rate from 17% to 60%. Our method also exhibits surprisingly good generalization behavior; for instance, an inverter trained on 16 generations steps gets 5--27 points higher prompt recovery when we increase the number of steps to 32 at test time. Furthermore, we demonstrate strong performance of our method on the more challenging task of recovering hidden system messages. We also analyze the role of verbatim repetition in prompt recovery and propose a new method for cross-family model transfer for logit-based inverters. Our findings show that next-token probabilities are a considerably more vulnerable attack surface for inversion attacks than previously known.", 'score': 1, 'issue_id': 4443, 'pub_date': '2025-06-20', 'pub_date_card': {'ru': '20 июня', 'en': 'June 20', 'zh': '6月20日'}, 'hash': '1da03dddd925b45e', 'authors': ['Murtaza Nazir', 'Matthew Finlayson', 'John X. Morris', 'Xiang Ren', 'Swabha Swayamdipta'], 'affiliations': ['Cornell University'], 'pdf_title_img': 'assets/pdf/title_img/2506.17090.jpg', 'data': {'categories': ['#security', '#rlhf', '#hallucinations', '#data', '#transfer_learning', '#architecture'], 'emoji': '🔍', 'ru': {'title': 'PILS: Новый метод для эффективного восстановления скрытых промптов в языковых моделях', 'desc': 'Новый метод PILS (Prompt Inversion from Logprob Sequences) позволяет восстанавливать скрытые промпты в языковых моделях путем анализа низкоразмерного подпространства вероятностей следующего токена модели. Этот метод достигает более высоких показателей восстановления и лучшей обобщаемости по сравнению с предыдущими подходами. PILS использует ключевое наблюдение о том, что векторные выходы языковой модели занимают низкоразмерное подпространство, что позволяет сжать распределение вероятностей следующего токена на протяжении нескольких шагов генерации. Метод демонстрирует значительные улучшения в задаче восстановления скрытых промптов, увеличивая точность восстановления в 2-3.5 раза по сравнению с предыдущими методами.'}, 'en': {'title': 'Unlocking Hidden Prompts with PILS: A New Era in Language Model Inversion', 'desc': "The paper introduces a novel technique called Prompt Inversion from Logprob Sequences (PILS) that enhances the recovery of hidden prompts in language models by utilizing the model's next-token probabilities. By recognizing that these probabilities exist within a low-dimensional subspace, the method allows for efficient compression of information, leading to significantly improved recovery rates compared to existing approaches. PILS achieves recovery rates that are 2 to 3.5 times higher, demonstrating its effectiveness even in more complex scenarios like hidden system message recovery. Additionally, the method shows strong generalization capabilities, indicating that it can adapt well to varying conditions during testing."}, 'zh': {'title': '揭示隐藏提示的新方法：PILS', 'desc': '本文提出了一种新方法，称为基于对数概率序列的提示反演（PILS），旨在通过分析语言模型的下一个标记概率的低维子空间来恢复隐藏的提示。该方法在恢复率和泛化能力上优于以往的技术，能够有效地从多个生成步骤中提取信息。研究表明，语言模型的向量输出占据了一个低维子空间，这使得我们能够无损压缩完整的下一个标记概率分布。我们的实验结果显示，PILS在恢复隐藏提示方面的准确率提高了2到3.5倍，且在更复杂的任务中也表现出色。'}}}, {'id': 'https://huggingface.co/papers/2506.16349', 'title': 'Watermarking Autoregressive Image Generation', 'url': 'https://huggingface.co/papers/2506.16349', 'abstract': 'A novel watermarking technique for autoregressive image generation models achieves reliable detection through improved reverse cycle-consistency and synchronization layers.  \t\t\t\t\tAI-generated summary \t\t\t\t Watermarking the outputs of generative models has emerged as a promising approach for tracking their provenance. Despite significant interest in autoregressive image generation models and their potential for misuse, no prior work has attempted to watermark their outputs at the token level. In this work, we present the first such approach by adapting language model watermarking techniques to this setting. We identify a key challenge: the lack of reverse cycle-consistency (RCC), wherein re-tokenizing generated image tokens significantly alters the token sequence, effectively erasing the watermark. To address this and to make our method robust to common image transformations, neural compression, and removal attacks, we introduce (i) a custom tokenizer-detokenizer finetuning procedure that improves RCC, and (ii) a complementary watermark synchronization layer. As our experiments demonstrate, our approach enables reliable and robust watermark detection with theoretically grounded p-values.', 'score': 1, 'issue_id': 4443, 'pub_date': '2025-06-19', 'pub_date_card': {'ru': '19 июня', 'en': 'June 19', 'zh': '6月19日'}, 'hash': '9da42722b7d365e0', 'authors': ['Nikola Jovanović', 'Ismail Labiad', 'Tomáš Souček', 'Martin Vechev', 'Pierre Fernandez'], 'affiliations': ['ETH Zurich', 'Meta FAIR', 'Université Paris-Saclay'], 'pdf_title_img': 'assets/pdf/title_img/2506.16349.jpg', 'data': {'categories': ['#security', '#multimodal', '#cv', '#training'], 'emoji': '🖼️', 'ru': {'title': 'Надежная защита авторства генеративных изображений', 'desc': 'Эта статья представляет новую технику водяных знаков для авторегрессионных моделей генерации изображений. Основная проблема заключалась в отсутствии обратной цикличной согласованности при ретокенизации сгенерированных токенов изображения. Для решения этой проблемы авторы разработали специальную процедуру дообучения токенизатора-детокенизатора и дополнительный слой синхронизации водяных знаков. Эксперименты показали, что предложенный подход обеспечивает надежное и устойчивое обнаружение водяных знаков с теоретически обоснованными p-значениями.'}, 'en': {'title': 'Watermarking Autoregressive Models: A New Era of Image Provenance', 'desc': 'This paper introduces a new watermarking technique specifically designed for autoregressive image generation models. The method enhances reverse cycle-consistency (RCC) to ensure that the watermark remains intact even after re-tokenizing the generated images. Additionally, it incorporates a synchronization layer to improve the robustness of the watermark against various image transformations and attacks. Experimental results show that this approach allows for reliable detection of watermarks, supported by statistically significant p-values.'}, 'zh': {'title': '自回归图像生成模型的可靠水印技术', 'desc': '本文提出了一种新颖的水印技术，专门用于自回归图像生成模型。该技术通过改进的反向循环一致性和同步层，实现了可靠的水印检测。我们首次将语言模型的水印技术应用于图像生成的输出，解决了重标记生成图像令牌时水印被消除的问题。实验结果表明，我们的方法在面对常见图像变换和攻击时，能够保持水印的可靠性和鲁棒性。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (3)', '#agents (3)', '#agi (1)', '#alignment (1)', '#architecture (4)', '#audio (1)', '#benchmark (4)', '#cv (5)', '#data (2)', '#dataset (3)', '#diffusion (3)', '#ethics', '#games (5)', '#graphs', '#hallucinations (1)', '#healthcare', '#inference (2)', '#interpretability (1)', '#leakage', '#long_context (2)', '#low_resource (1)', '#machine_translation (1)', '#math', '#multilingual (1)', '#multimodal (8)', '#open_source (1)', '#optimization (8)', '#plp', '#rag (1)', '#reasoning (3)', '#rl (2)', '#rlhf (1)', '#robotics (1)', '#science', '#security (2)', '#small_models', '#story_generation', '#survey (1)', '#synthetic (3)', '#training (7)', '#transfer_learning (2)', '#video (4)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-06-23 22:11',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-06-23 22:11')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-06-23 22:11')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    