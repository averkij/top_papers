{
    "date": {
        "ru": "12 декабря",
        "en": "December 12",
        "zh": "12月12日"
    },
    "time_utc": "2024-12-12 11:09",
    "weekday": 3,
    "issue_id": 1088,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2412.07760",
            "title": "SynCamMaster: Synchronizing Multi-Camera Video Generation from Diverse Viewpoints",
            "url": "https://huggingface.co/papers/2412.07760",
            "abstract": "Recent advancements in video diffusion models have shown exceptional abilities in simulating real-world dynamics and maintaining 3D consistency. This progress inspires us to investigate the potential of these models to ensure dynamic consistency across various viewpoints, a highly desirable feature for applications such as virtual filming. Unlike existing methods focused on multi-view generation of single objects for 4D reconstruction, our interest lies in generating open-world videos from arbitrary viewpoints, incorporating 6 DoF camera poses. To achieve this, we propose a plug-and-play module that enhances a pre-trained text-to-video model for multi-camera video generation, ensuring consistent content across different viewpoints. Specifically, we introduce a multi-view synchronization module to maintain appearance and geometry consistency across these viewpoints. Given the scarcity of high-quality training data, we design a hybrid training scheme that leverages multi-camera images and monocular videos to supplement Unreal Engine-rendered multi-camera videos. Furthermore, our method enables intriguing extensions, such as re-rendering a video from novel viewpoints. We also release a multi-view synchronized video dataset, named SynCamVideo-Dataset. Project page: https://jianhongbai.github.io/SynCamMaster/.",
            "score": 26,
            "issue_id": 1081,
            "pub_date": "2024-12-10",
            "pub_date_card": {
                "ru": "10 декабря",
                "en": "December 10",
                "zh": "12月10日"
            },
            "hash": "5ac69027d8ae0669",
            "authors": [
                "Jianhong Bai",
                "Menghan Xia",
                "Xintao Wang",
                "Ziyang Yuan",
                "Xiao Fu",
                "Zuozhu Liu",
                "Haoji Hu",
                "Pengfei Wan",
                "Di Zhang"
            ],
            "affiliations": [
                "CUHK",
                "Kuaishou Technology",
                "Tsinghua University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.07760.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#dataset",
                    "#3d",
                    "#open_source",
                    "#video"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Согласованная генерация видео с множества ракурсов",
                    "desc": "Статья представляет новый подход к генерации мультиракурсных видео с использованием диффузионных моделей. Авторы разработали модуль, который улучшает предобученную модель text-to-video для создания согласованного контента с разных точек обзора. Они применяют гибридную схему обучения, используя мультиракурсные изображения и монокулярные видео в дополнение к рендерам из Unreal Engine. Метод также позволяет перерендерить видео с новых ракурсов и включает выпуск нового датасета SynCamVideo-Dataset."
                },
                "en": {
                    "title": "Dynamic Consistency in Multi-View Video Generation",
                    "desc": "This paper explores the use of video diffusion models to create videos that maintain dynamic consistency from multiple viewpoints, which is important for applications like virtual filming. The authors propose a new module that enhances existing text-to-video models, allowing them to generate videos that are consistent in appearance and geometry across different camera angles. They introduce a multi-view synchronization module to ensure that the content remains coherent, even when viewed from various perspectives. Additionally, they present a hybrid training approach that combines different types of video data to improve the model's performance and release a new dataset for multi-view synchronized videos."
                },
                "zh": {
                    "title": "实现多视角视频的一致性",
                    "desc": "最近视频扩散模型的进展显示出在模拟现实世界动态和保持三维一致性方面的卓越能力。我们研究这些模型在不同视角下确保动态一致性的潜力，这对于虚拟拍摄等应用非常重要。与现有方法不同，我们关注的是从任意视角生成开放世界视频，并引入六自由度相机姿态。为此，我们提出了一个可插拔模块，增强了预训练的文本到视频模型，以实现多相机视频生成，并确保不同视角下内容的一致性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.08580",
            "title": "LAION-SG: An Enhanced Large-Scale Dataset for Training Complex Image-Text Models with Structural Annotations",
            "url": "https://huggingface.co/papers/2412.08580",
            "abstract": "Recent advances in text-to-image (T2I) generation have shown remarkable success in producing high-quality images from text. However, existing T2I models show decayed performance in compositional image generation involving multiple objects and intricate relationships. We attribute this problem to limitations in existing datasets of image-text pairs, which lack precise inter-object relationship annotations with prompts only. To address this problem, we construct LAION-SG, a large-scale dataset with high-quality structural annotations of scene graphs (SG), which precisely describe attributes and relationships of multiple objects, effectively representing the semantic structure in complex scenes. Based on LAION-SG, we train a new foundation model SDXL-SG to incorporate structural annotation information into the generation process. Extensive experiments show advanced models trained on our LAION-SG boast significant performance improvements in complex scene generation over models on existing datasets. We also introduce CompSG-Bench, a benchmark that evaluates models on compositional image generation, establishing a new standard for this domain.",
            "score": 21,
            "issue_id": 1081,
            "pub_date": "2024-12-11",
            "pub_date_card": {
                "ru": "11 декабря",
                "en": "December 11",
                "zh": "12月11日"
            },
            "hash": "07b05e5ae44a52c7",
            "authors": [
                "Zejian Li",
                "Chenye Meng",
                "Yize Li",
                "Ling Yang",
                "Shengyuan Zhang",
                "Jiarui Ma",
                "Jiayi Li",
                "Guang Yang",
                "Changyuan Yang",
                "Zhiyuan Yang",
                "Jinxiong Chang",
                "Lingyun Sun"
            ],
            "affiliations": [
                "Alibaba Group",
                "Ant Group",
                "Jiangnan University",
                "Peking University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.08580.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#cv",
                    "#synthetic",
                    "#benchmark",
                    "#games"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "Новый уровень генерации сложных сцен с помощью графов",
                    "desc": "Исследователи представили новый подход к генерации изображений по тексту, который улучшает композиционную генерацию сложных сцен с несколькими объектами. Они создали датасет LAION-SG с высококачественными структурными аннотациями в виде графов сцен. На основе этого датасета была обучена новая фундаментальная модель SDXL-SG, которая демонстрирует значительное улучшение в генерации сложных сцен по сравнению с существующими моделями. Также авторы представили бенчмарк CompSG-Bench для оценки моделей в задаче композиционной генерации изображений."
                },
                "en": {
                    "title": "Enhancing Text-to-Image Generation with Structured Scene Graphs",
                    "desc": "This paper addresses the challenges faced by text-to-image (T2I) models in generating complex images with multiple objects and their relationships. The authors identify that existing datasets lack detailed annotations for inter-object relationships, which hampers model performance. To overcome this, they introduce LAION-SG, a new dataset that includes comprehensive scene graph annotations, enhancing the understanding of object attributes and relationships. They also present a new model, SDXL-SG, trained on this dataset, which shows significant improvements in generating intricate scenes, along with a new benchmark, CompSG-Bench, for evaluating compositional image generation."
                },
                "zh": {
                    "title": "构建高质量数据集，提升图像生成能力",
                    "desc": "最近在文本到图像生成（T2I）方面取得了显著进展，能够从文本生成高质量图像。然而，现有的T2I模型在生成包含多个对象和复杂关系的图像时表现不佳。我们认为这个问题源于现有图像-文本对数据集的局限性，这些数据集缺乏精确的对象间关系注释。为了解决这个问题，我们构建了LAION-SG，这是一个具有高质量结构注释的大规模数据集，能够有效表示复杂场景中的语义结构，并基于此训练了新的基础模型SDXL-SG。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.08443",
            "title": "POINTS1.5: Building a Vision-Language Model towards Real World Applications",
            "url": "https://huggingface.co/papers/2412.08443",
            "abstract": "Vision-language models have made significant strides recently, demonstrating superior performance across a range of tasks, e.g. optical character recognition and complex diagram analysis. Building on this trend, we introduce a new vision-language model, POINTS1.5, designed to excel in various real-world applications. POINTS1.5 is an enhancement of POINTS1.0 and incorporates several key innovations: i) We replace the original CLIP vision encoder, which had a fixed image resolution, with a NaViT-style vision encoder that supports native dynamic high resolution. This allows POINTS1.5 to process images of any resolution without needing to split them into tiles. ii) We add bilingual support to POINTS1.5, significantly enhancing its capability in Chinese. Due to the scarcity of open-source Chinese datasets for vision-language models, we collect numerous images from the Internet and annotate them using a combination of manual and automatic methods. iii) We propose a set of rigorous filtering methods for visual instruction tuning datasets. We comprehensively evaluate all these filtering methods, and choose the most effective ones to obtain the final visual instruction tuning set. Thanks to these innovations, POINTS1.5 significantly outperforms POINTS1.0 and demonstrates strong performance across a range of real-world applications. Notably, POINTS1.5-7B is trained on fewer than 4 billion tokens and ranks first on the OpenCompass leaderboard among models with fewer than 10 billion parameters",
            "score": 19,
            "issue_id": 1083,
            "pub_date": "2024-12-11",
            "pub_date_card": {
                "ru": "11 декабря",
                "en": "December 11",
                "zh": "12月11日"
            },
            "hash": "02dbe9638e613a10",
            "authors": [
                "Yuan Liu",
                "Le Tian",
                "Xiao Zhou",
                "Xinyu Gao",
                "Kavio Yu",
                "Yang Yu",
                "Jie Zhou"
            ],
            "affiliations": [
                "Pattern Recognition Center, WeChat AI, Tencent Inc, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.08443.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#cv",
                    "#low_resource",
                    "#architecture",
                    "#data",
                    "#multilingual",
                    "#training",
                    "#open_source"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "POINTS1.5: Новый уровень мультимодального искусственного интеллекта",
                    "desc": "Статья представляет новую мультимодальную модель POINTS1.5, улучшенную версию POINTS1.0. Ключевые инновации включают замену энкодера изображений на NaViT для поддержки динамического высокого разрешения, добавление двуязычной поддержки (английский и китайский) и применение строгих методов фильтрации для наборов данных визуального обучения. Модель POINTS1.5 демонстрирует превосходную производительность в различных реальных приложениях, несмотря на обучение на менее чем 4 миллиардах токенов. POINTS1.5-7B занимает первое место в рейтинге OpenCompass среди моделей с менее чем 10 миллиардами параметров."
                },
                "en": {
                    "title": "POINTS1.5: Elevating Vision-Language Models with Dynamic Resolution and Bilingual Support",
                    "desc": "The paper presents POINTS1.5, an advanced vision-language model that improves upon its predecessor, POINTS1.0. It features a NaViT-style vision encoder that allows for dynamic high-resolution image processing, eliminating the need for image tiling. Additionally, POINTS1.5 introduces bilingual support, particularly enhancing its performance in Chinese by utilizing a newly curated dataset. The model also employs rigorous filtering methods for visual instruction tuning datasets, leading to superior performance in various real-world applications and achieving top rankings in benchmark evaluations."
                },
                "zh": {
                    "title": "POINTS1.5：视觉语言模型的新突破",
                    "desc": "本文介绍了一种新的视觉语言模型POINTS1.5，该模型在多个实际应用中表现优异。与之前的版本POINTS1.0相比，POINTS1.5进行了多项重要改进，包括使用支持动态高分辨率的NaViT风格视觉编码器，能够处理任意分辨率的图像。该模型还增加了对中文的双语支持，通过收集和注释大量图像来提升其中文能力。此外，本文提出了一套严格的视觉指令调优数据集过滤方法，确保最终数据集的有效性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.07744",
            "title": "StyleMaster: Stylize Your Video with Artistic Generation and Translation",
            "url": "https://huggingface.co/papers/2412.07744",
            "abstract": "Style control has been popular in video generation models. Existing methods often generate videos far from the given style, cause content leakage, and struggle to transfer one video to the desired style. Our first observation is that the style extraction stage matters, whereas existing methods emphasize global style but ignore local textures. In order to bring texture features while preventing content leakage, we filter content-related patches while retaining style ones based on prompt-patch similarity; for global style extraction, we generate a paired style dataset through model illusion to facilitate contrastive learning, which greatly enhances the absolute style consistency. Moreover, to fill in the image-to-video gap, we train a lightweight motion adapter on still videos, which implicitly enhances stylization extent, and enables our image-trained model to be seamlessly applied to videos. Benefited from these efforts, our approach, StyleMaster, not only achieves significant improvement in both style resemblance and temporal coherence, but also can easily generalize to video style transfer with a gray tile ControlNet. Extensive experiments and visualizations demonstrate that StyleMaster significantly outperforms competitors, effectively generating high-quality stylized videos that align with textual content and closely resemble the style of reference images. Our project page is at https://zixuan-ye.github.io/stylemaster",
            "score": 10,
            "issue_id": 1084,
            "pub_date": "2024-12-10",
            "pub_date_card": {
                "ru": "10 декабря",
                "en": "December 10",
                "zh": "12月10日"
            },
            "hash": "1eac9c28026939fc",
            "authors": [
                "Zixuan Ye",
                "Huijuan Huang",
                "Xintao Wang",
                "Pengfei Wan",
                "Di Zhang",
                "Wenhan Luo"
            ],
            "affiliations": [
                "Hong Kong University of Science and Technology",
                "KuaiShou Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.07744.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#style_transfer",
                    "#video",
                    "#multimodal"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "StyleMaster: Совершенствование стилевого контроля в генерации видео",
                    "desc": "StyleMaster - это новый подход к стилевому контролю в генерации видео. Метод улучшает извлечение стиля, используя фильтрацию патчей на основе сходства с промптом и контрастное обучение на парных стилевых данных. Легковесный адаптер движения позволяет применять модель, обученную на изображениях, к видео. StyleMaster значительно превосходит конкурентов по соответствию стилю и временной согласованности генерируемых видео."
                },
                "en": {
                    "title": "StyleMaster: Elevating Video Style Transfer with Texture and Consistency",
                    "desc": "This paper presents StyleMaster, a novel approach to video style transfer that addresses common issues in existing methods, such as content leakage and poor style adherence. The authors emphasize the importance of local texture features in addition to global style, proposing a method that filters content-related patches while preserving style-related ones. They introduce a paired style dataset for contrastive learning to enhance style consistency and develop a lightweight motion adapter to bridge the gap between image and video stylization. Extensive experiments show that StyleMaster significantly improves style resemblance and temporal coherence, outperforming existing models in generating high-quality stylized videos."
                },
                "zh": {
                    "title": "StyleMaster：提升视频风格转移的创新方法",
                    "desc": "本论文介绍了一种名为StyleMaster的视频生成模型，旨在改善视频的风格转移效果。我们发现，现有方法在风格提取阶段往往忽视局部纹理，导致生成的视频与目标风格不符。为了解决这个问题，我们通过过滤内容相关的图像块来保留风格特征，并利用对比学习增强全局风格的一致性。此外，我们还训练了一个轻量级的运动适配器，使得图像训练的模型能够无缝应用于视频生成。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.06234",
            "title": "Generative Densification: Learning to Densify Gaussians for High-Fidelity Generalizable 3D Reconstruction",
            "url": "https://huggingface.co/papers/2412.06234",
            "abstract": "Generalized feed-forward Gaussian models have achieved significant progress in sparse-view 3D reconstruction by leveraging prior knowledge from large multi-view datasets. However, these models often struggle to represent high-frequency details due to the limited number of Gaussians. While the densification strategy used in per-scene 3D Gaussian splatting (3D-GS) optimization can be adapted to the feed-forward models, it may not be ideally suited for generalized scenarios. In this paper, we propose Generative Densification, an efficient and generalizable method to densify Gaussians generated by feed-forward models. Unlike the 3D-GS densification strategy, which iteratively splits and clones raw Gaussian parameters, our method up-samples feature representations from the feed-forward models and generates their corresponding fine Gaussians in a single forward pass, leveraging the embedded prior knowledge for enhanced generalization. Experimental results on both object-level and scene-level reconstruction tasks demonstrate that our method outperforms state-of-the-art approaches with comparable or smaller model sizes, achieving notable improvements in representing fine details.",
            "score": 8,
            "issue_id": 1083,
            "pub_date": "2024-12-09",
            "pub_date_card": {
                "ru": "9 декабря",
                "en": "December 9",
                "zh": "12月9日"
            },
            "hash": "1da969b562c1f280",
            "authors": [
                "Seungtae Nam",
                "Xiangyu Sun",
                "Gyeongjin Kang",
                "Younggeun Lee",
                "Seungjun Oh",
                "Eunbyung Park"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2412.06234.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#3d",
                    "#dataset"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Генеративное уплотнение: новый шаг в 3D-реконструкции с высоким разрешением",
                    "desc": "Статья представляет новый метод под названием 'Генеративное уплотнение' для улучшения реконструкции 3D-объектов при ограниченном количестве ракурсов. Этот подход позволяет эффективно уплотнять гауссовы распределения, генерируемые моделями прямого распространения, улучшая детализацию реконструкции. В отличие от существующих методов, 'Генеративное уплотнение' использует встроенные априорные знания для повышения обобщающей способности. Экспериментальные результаты показывают, что предложенный метод превосходит современные подходы как для объектов, так и для сцен."
                },
                "en": {
                    "title": "Enhancing 3D Reconstruction with Generative Densification",
                    "desc": "This paper introduces Generative Densification, a novel method aimed at improving the representation of high-frequency details in sparse-view 3D reconstruction using feed-forward Gaussian models. Traditional methods struggle with detail due to a limited number of Gaussians, but our approach efficiently up-samples feature representations in a single forward pass. By leveraging prior knowledge from large datasets, this method enhances generalization and performance in both object-level and scene-level tasks. Experimental results show that Generative Densification outperforms existing techniques while maintaining comparable or smaller model sizes."
                },
                "zh": {
                    "title": "生成稠密化：提升3D重建细节的高效方法",
                    "desc": "本文提出了一种名为生成稠密化的高效方法，用于增强前馈高斯模型在稀疏视图3D重建中的表现。与传统的3D高斯稠密化策略不同，我们的方法通过单次前向传播从前馈模型中上采样特征表示，生成相应的细节高斯。该方法利用嵌入的先验知识，提升了模型的泛化能力，能够更好地表示高频细节。实验结果表明，我们的方法在物体级和场景级重建任务中均优于现有的最先进方法，且模型规模相对较小。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.07825",
            "title": "3DSRBench: A Comprehensive 3D Spatial Reasoning Benchmark",
            "url": "https://huggingface.co/papers/2412.07825",
            "abstract": "3D spatial reasoning is the ability to analyze and interpret the positions, orientations, and spatial relationships of objects within the 3D space. This allows models to develop a comprehensive understanding of the 3D scene, enabling their applicability to a broader range of areas, such as autonomous navigation, robotics, and AR/VR. While large multi-modal models (LMMs) have achieved remarkable progress in a wide range of image and video understanding tasks, their capabilities to perform 3D spatial reasoning on diverse natural images are less studied. In this work we present the first comprehensive 3D spatial reasoning benchmark, 3DSRBench, with 2,772 manually annotated visual question-answer pairs across 12 question types. We conduct robust and thorough evaluation of 3D spatial reasoning capabilities by balancing the data distribution and adopting a novel FlipEval strategy. To further study the robustness of 3D spatial reasoning w.r.t. camera 3D viewpoints, our 3DSRBench includes two subsets with 3D spatial reasoning questions on paired images with common and uncommon viewpoints. We benchmark a wide range of open-sourced and proprietary LMMs, uncovering their limitations in various aspects of 3D awareness, such as height, orientation, location, and multi-object reasoning, as well as their degraded performance on images with uncommon camera viewpoints. Our 3DSRBench provide valuable findings and insights about the future development of LMMs with strong 3D reasoning capabilities. Our project page and dataset is available https://3dsrbench.github.io.",
            "score": 7,
            "issue_id": 1082,
            "pub_date": "2024-12-10",
            "pub_date_card": {
                "ru": "10 декабря",
                "en": "December 10",
                "zh": "12月10日"
            },
            "hash": "91db0c60d08d4efd",
            "authors": [
                "Wufei Ma",
                "Haoyu Chen",
                "Guofeng Zhang",
                "Celso M de Melo",
                "Alan Yuille",
                "Jieneng Chen"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "DEVCOM Army Research Laboratory",
                "Johns Hopkins University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.07825.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#benchmark",
                    "#3d",
                    "#reasoning"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "3DSRBench: новый стандарт для оценки 3D пространственного мышления у LMM",
                    "desc": "Эта статья представляет первый комплексный benchmark для оценки способностей больших мультимодальных моделей (LMM) к 3D пространственному мышлению на разнообразных естественных изображениях. Авторы создали датасет 3DSRBench, содержащий 2772 вручную размеченных пар вопрос-ответ по 12 типам вопросов, связанных с 3D пространственным мышлением. Исследование включает оценку робастности моделей к различным ракурсам камеры и использует стратегию FlipEval для тщательного тестирования. Результаты показывают ограничения существующих LMM в различных аспектах 3D восприятия, таких как высота, ориентация, расположение объектов и рассуждения о нескольких объектах."
                },
                "en": {
                    "title": "Enhancing 3D Spatial Reasoning in AI Models",
                    "desc": "This paper introduces 3DSRBench, a new benchmark designed to evaluate 3D spatial reasoning in large multi-modal models (LMMs). It includes 2,772 annotated visual question-answer pairs that cover various question types related to spatial relationships in 3D environments. The study highlights the limitations of current LMMs in understanding aspects like height, orientation, and location, especially when dealing with images taken from uncommon viewpoints. By providing a structured evaluation framework, this work aims to enhance the development of models with improved 3D reasoning capabilities."
                },
                "zh": {
                    "title": "推动3D空间推理的未来发展",
                    "desc": "3D空间推理是分析和理解三维空间中物体位置、方向和空间关系的能力。本文提出了第一个全面的3D空间推理基准，3DSRBench，包含2772个手动标注的视觉问答对，涵盖12种问题类型。我们通过平衡数据分布和采用新颖的FlipEval策略，对3D空间推理能力进行了全面评估。研究结果揭示了现有大型多模态模型在3D意识方面的局限性，并为未来的模型发展提供了重要见解。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.08486",
            "title": "Learning Flow Fields in Attention for Controllable Person Image Generation",
            "url": "https://huggingface.co/papers/2412.08486",
            "abstract": "Controllable person image generation aims to generate a person image conditioned on reference images, allowing precise control over the person's appearance or pose. However, prior methods often distort fine-grained textural details from the reference image, despite achieving high overall image quality. We attribute these distortions to inadequate attention to corresponding regions in the reference image. To address this, we thereby propose learning flow fields in attention (Leffa), which explicitly guides the target query to attend to the correct reference key in the attention layer during training. Specifically, it is realized via a regularization loss on top of the attention map within a diffusion-based baseline. Our extensive experiments show that Leffa achieves state-of-the-art performance in controlling appearance (virtual try-on) and pose (pose transfer), significantly reducing fine-grained detail distortion while maintaining high image quality. Additionally, we show that our loss is model-agnostic and can be used to improve the performance of other diffusion models.",
            "score": 6,
            "issue_id": 1086,
            "pub_date": "2024-12-11",
            "pub_date_card": {
                "ru": "11 декабря",
                "en": "December 11",
                "zh": "12月11日"
            },
            "hash": "ff329acbd2056afe",
            "authors": [
                "Zijian Zhou",
                "Shikun Liu",
                "Xiao Han",
                "Haozhe Liu",
                "Kam Woh Ng",
                "Tian Xie",
                "Yuren Cong",
                "Hang Li",
                "Mengmeng Xu",
                "Juan-Manuel Pérez-Rúa",
                "Aditya Patel",
                "Tao Xiang",
                "Miaojing Shi",
                "Sen He"
            ],
            "affiliations": [
                "Kings College London",
                "Meta AI",
                "Tongji University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.08486.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#optimization",
                    "#training",
                    "#cv"
                ],
                "emoji": "👤",
                "ru": {
                    "title": "Точный контроль деталей при генерации изображений людей",
                    "desc": "Статья представляет новый метод под названием Leffa для улучшения контролируемой генерации изображений людей. Метод использует обучение полей потока в слоях внимания, чтобы точнее передавать детали из референсного изображения. Leffa реализуется через регуляризационную функцию потерь поверх карты внимания в базовой модели диффузии. Эксперименты показывают, что Leffa достигает современного уровня производительности в контроле внешнего вида и позы, значительно уменьшая искажения мелких деталей."
                },
                "en": {
                    "title": "Enhancing Image Generation with Targeted Attention",
                    "desc": "This paper presents a method called Learning Flow Fields in Attention (Leffa) for controllable person image generation. The goal is to create images of people that accurately reflect the appearance and pose of reference images without losing important details. Previous methods struggled with distorting fine textures, which this approach aims to fix by improving how the model focuses on specific areas of the reference image. The authors demonstrate that Leffa not only enhances the quality of generated images but is also adaptable to other diffusion models, making it a versatile solution in the field."
                },
                "zh": {
                    "title": "精确控制人物图像生成的关键",
                    "desc": "可控的人物图像生成旨在根据参考图像生成特定外观或姿势的人物图像。以往的方法虽然在整体图像质量上表现良好，但常常会扭曲参考图像中的细节纹理。我们认为这种扭曲是由于对参考图像中相应区域关注不足造成的。为了解决这个问题，我们提出了一种在注意力机制中学习流场的方法（Leffa），通过在训练过程中引导目标查询关注正确的参考关键点，从而显著减少细节扭曲，同时保持高图像质量。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.07797",
            "title": "Mogo: RQ Hierarchical Causal Transformer for High-Quality 3D Human Motion Generation",
            "url": "https://huggingface.co/papers/2412.07797",
            "abstract": "In the field of text-to-motion generation, Bert-type Masked Models (MoMask, MMM) currently produce higher-quality outputs compared to GPT-type autoregressive models (T2M-GPT). However, these Bert-type models often lack the streaming output capability required for applications in video game and multimedia environments, a feature inherent to GPT-type models. Additionally, they demonstrate weaker performance in out-of-distribution generation. To surpass the quality of BERT-type models while leveraging a GPT-type structure, without adding extra refinement models that complicate scaling data, we propose a novel architecture, Mogo (Motion Only Generate Once), which generates high-quality lifelike 3D human motions by training a single transformer model. Mogo consists of only two main components: 1) RVQ-VAE, a hierarchical residual vector quantization variational autoencoder, which discretizes continuous motion sequences with high precision; 2) Hierarchical Causal Transformer, responsible for generating the base motion sequences in an autoregressive manner while simultaneously inferring residuals across different layers. Experimental results demonstrate that Mogo can generate continuous and cyclic motion sequences up to 260 frames (13 seconds), surpassing the 196 frames (10 seconds) length limitation of existing datasets like HumanML3D. On the HumanML3D test set, Mogo achieves a FID score of 0.079, outperforming both the GPT-type model T2M-GPT (FID = 0.116), AttT2M (FID = 0.112) and the BERT-type model MMM (FID = 0.080). Furthermore, our model achieves the best quantitative performance in out-of-distribution generation.",
            "score": 6,
            "issue_id": 1080,
            "pub_date": "2024-12-05",
            "pub_date_card": {
                "ru": "5 декабря",
                "en": "December 5",
                "zh": "12月5日"
            },
            "hash": "ff9bb8b603f9d972",
            "authors": [
                "Dongjie Fu"
            ],
            "affiliations": [
                "Mogo AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.07797.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#3d",
                    "#games",
                    "#architecture"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Mogo: Революция в генерации движений из текста",
                    "desc": "Статья представляет новую архитектуру Mogo для генерации высококачественных трехмерных движений человека на основе текста. Mogo использует RVQ-VAE для дискретизации непрерывных последовательностей движений и иерархический причинный трансформер для генерации базовых последовательностей движений. Эксперименты показывают, что Mogo превосходит существующие модели по качеству генерации, включая GPT-подобные и BERT-подобные модели. Модель также демонстрирует лучшие результаты при генерации вне распределения обучающих данных."
                },
                "en": {
                    "title": "Mogo: Revolutionizing Text-to-Motion with High-Quality 3D Generation",
                    "desc": "This paper introduces Mogo, a new architecture for generating high-quality 3D human motions from text. Mogo combines a hierarchical residual vector quantization variational autoencoder (RVQ-VAE) with a hierarchical causal transformer to produce continuous and cyclic motion sequences efficiently. Unlike existing Bert-type models, Mogo maintains the streaming output capability of GPT-type models while improving performance in out-of-distribution scenarios. Experimental results show that Mogo not only generates longer motion sequences but also achieves superior quality metrics compared to both GPT-type and Bert-type models."
                },
                "zh": {
                    "title": "Mogo：高效生成高质量3D人类动作的创新架构",
                    "desc": "在文本到动作生成领域，Bert类型的模型（如MoMask, MMM）虽然输出质量较高，但缺乏流式输出能力，无法满足视频游戏和多媒体环境的需求。相比之下，GPT类型的自回归模型（如T2M-GPT）具备这一特性，但在生成质量上稍逊一筹。为了解决这一问题，我们提出了一种新架构Mogo（Motion Only Generate Once），它通过训练单一的变换器模型生成高质量的3D人类动作。Mogo结合了高精度的层次残差向量量化变分自编码器和层次因果变换器，能够生成连续且循环的动作序列，超越了现有数据集的限制。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.08646",
            "title": "StreamChat: Chatting with Streaming Video",
            "url": "https://huggingface.co/papers/2412.08646",
            "abstract": "This paper presents StreamChat, a novel approach that enhances the interaction capabilities of Large Multimodal Models (LMMs) with streaming video content. In streaming interaction scenarios, existing methods rely solely on visual information available at the moment a question is posed, resulting in significant delays as the model remains unaware of subsequent changes in the streaming video. StreamChat addresses this limitation by innovatively updating the visual context at each decoding step, ensuring that the model utilizes up-to-date video content throughout the decoding process. Additionally, we introduce a flexible and efficient crossattention-based architecture to process dynamic streaming inputs while maintaining inference efficiency for streaming interactions. Furthermore, we construct a new dense instruction dataset to facilitate the training of streaming interaction models, complemented by a parallel 3D-RoPE mechanism that encodes the relative temporal information of visual and text tokens. Experimental results demonstrate that StreamChat achieves competitive performance on established image and video benchmarks and exhibits superior capabilities in streaming interaction scenarios compared to state-of-the-art video LMM.",
            "score": 5,
            "issue_id": 1086,
            "pub_date": "2024-12-11",
            "pub_date_card": {
                "ru": "11 декабря",
                "en": "December 11",
                "zh": "12月11日"
            },
            "hash": "6d48f15bab7c3545",
            "authors": [
                "Jihao Liu",
                "Zhiding Yu",
                "Shiyi Lan",
                "Shihao Wang",
                "Rongyao Fang",
                "Jan Kautz",
                "Hongsheng Li",
                "Jose M. Alvare"
            ],
            "affiliations": [
                "CPII under InnoHK",
                "CUHK MMLab",
                "NVIDIA",
                "Shanghai AI Laboratory",
                "The Hong Kong Polytechnic University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.08646.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#multimodal",
                    "#dataset",
                    "#architecture",
                    "#benchmark"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "StreamChat: Революция в потоковом видеовзаимодействии с ИИ",
                    "desc": "Статья представляет StreamChat - новый подход к улучшению взаимодействия больших мультимодальных моделей (LMM) с потоковым видео. StreamChat обновляет визуальный контекст на каждом шаге декодирования, обеспечивая использование актуального содержания видео. Авторы вводят архитектуру на основе кросс-внимания для обработки динамических потоковых входных данных и создают новый набор данных для обучения моделей потокового взаимодействия. Экспериментальные результаты показывают превосходство StreamChat в сценариях потокового взаимодействия по сравнению с современными видео LMM."
                },
                "en": {
                    "title": "StreamChat: Real-Time Interaction with Streaming Video",
                    "desc": "This paper introduces StreamChat, a new method that improves how Large Multimodal Models (LMMs) interact with live video. Traditional approaches only use the visual information available at the time a question is asked, which can cause delays as they miss changes in the video. StreamChat solves this by updating the visual context continuously during the decoding process, allowing the model to respond with the most current video content. It also features a cross-attention architecture for efficient processing of dynamic inputs and a new dataset for training, leading to better performance in streaming interactions compared to existing models."
                },
                "zh": {
                    "title": "StreamChat：实时视频交互的新突破",
                    "desc": "本文提出了一种新方法StreamChat，旨在增强大型多模态模型（LMM）与流媒体视频内容的交互能力。在流媒体交互场景中，现有方法仅依赖于提问时可用的视觉信息，导致模型无法及时获取视频中的后续变化，从而产生显著延迟。StreamChat通过在每个解码步骤中创新性地更新视觉上下文，确保模型在解码过程中利用最新的视频内容。此外，我们引入了一种灵活高效的基于交叉注意力的架构，以处理动态流媒体输入，同时保持流媒体交互的推理效率。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.08629",
            "title": "FlowEdit: Inversion-Free Text-Based Editing Using Pre-Trained Flow Models",
            "url": "https://huggingface.co/papers/2412.08629",
            "abstract": "Editing real images using a pre-trained text-to-image (T2I) diffusion/flow model often involves inverting the image into its corresponding noise map. However, inversion by itself is typically insufficient for obtaining satisfactory results, and therefore many methods additionally intervene in the sampling process. Such methods achieve improved results but are not seamlessly transferable between model architectures. Here, we introduce FlowEdit, a text-based editing method for pre-trained T2I flow models, which is inversion-free, optimization-free and model agnostic. Our method constructs an ODE that directly maps between the source and target distributions (corresponding to the source and target text prompts) and achieves a lower transport cost than the inversion approach. This leads to state-of-the-art results, as we illustrate with Stable Diffusion 3 and FLUX. Code and examples are available on the project's webpage.",
            "score": 2,
            "issue_id": 1088,
            "pub_date": "2024-12-11",
            "pub_date_card": {
                "ru": "11 декабря",
                "en": "December 11",
                "zh": "12月11日"
            },
            "hash": "b08b9bb78c9561f4",
            "authors": [
                "Vladimir Kulikov",
                "Matan Kleiner",
                "Inbar Huberman-Spiegelglas",
                "Tomer Michaeli"
            ],
            "affiliations": [
                "Technion Israel Institute of Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.08629.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#architecture",
                    "#cv",
                    "#open_source",
                    "#diffusion",
                    "#multimodal"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "FlowEdit: эффективное редактирование изображений без инверсии",
                    "desc": "Статья представляет FlowEdit - новый метод редактирования изображений с помощью текстовых запросов для предобученных моделей text-to-image. В отличие от существующих подходов, FlowEdit не требует инверсии изображения и оптимизации, а также применим к различным архитектурам моделей. Метод основан на построении ODE, напрямую отображающего распределения исходного и целевого текстовых запросов. FlowEdit достигает лучших результатов по сравнению с методами, основанными на инверсии, что продемонстрировано на моделях Stable Diffusion 3 и FLUX."
                },
                "en": {
                    "title": "Seamless Image Editing with FlowEdit: No Inversion Needed!",
                    "desc": "This paper presents FlowEdit, a novel method for editing images using pre-trained text-to-image (T2I) flow models without the need for inversion or optimization. Traditional methods often require converting images into noise maps, which can be inefficient and model-specific. FlowEdit instead utilizes an ordinary differential equation (ODE) to directly connect the source and target distributions based on text prompts, resulting in a more efficient editing process. The approach demonstrates superior performance compared to existing methods, as shown with models like Stable Diffusion 3 and FLUX."
                },
                "zh": {
                    "title": "无反演的文本图像编辑新方法",
                    "desc": "本文介绍了一种名为FlowEdit的文本编辑方法，专为预训练的文本到图像（T2I）流模型设计。与传统的图像反演方法不同，FlowEdit不需要反演和优化，且对模型架构不敏感。该方法通过构建一个常微分方程（ODE），直接在源分布和目标分布之间进行映射，从而降低了传输成本。实验结果表明，FlowEdit在Stable Diffusion 3和FLUX上达到了最先进的效果。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.07147",
            "title": "MIT-10M: A Large Scale Parallel Corpus of Multilingual Image Translation",
            "url": "https://huggingface.co/papers/2412.07147",
            "abstract": "Image Translation (IT) holds immense potential across diverse domains, enabling the translation of textual content within images into various languages. However, existing datasets often suffer from limitations in scale, diversity, and quality, hindering the development and evaluation of IT models. To address this issue, we introduce MIT-10M, a large-scale parallel corpus of multilingual image translation with over 10M image-text pairs derived from real-world data, which has undergone extensive data cleaning and multilingual translation validation. It contains 840K images in three sizes, 28 categories, tasks with three levels of difficulty and 14 languages image-text pairs, which is a considerable improvement on existing datasets. We conduct extensive experiments to evaluate and train models on MIT-10M. The experimental results clearly indicate that our dataset has higher adaptability when it comes to evaluating the performance of the models in tackling challenging and complex image translation tasks in the real world. Moreover, the performance of the model fine-tuned with MIT-10M has tripled compared to the baseline model, further confirming its superiority.",
            "score": 1,
            "issue_id": 1087,
            "pub_date": "2024-12-10",
            "pub_date_card": {
                "ru": "10 декабря",
                "en": "December 10",
                "zh": "12月10日"
            },
            "hash": "cf26c855c674b8aa",
            "authors": [
                "Bo Li",
                "Shaolin Zhu",
                "Lijie Wen"
            ],
            "affiliations": [
                "Baidu Inc., Beijing, China",
                "College of Intelligence and Computing, Tianjin University, Tianjin, China",
                "School of Software, Tsinghua University, Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.07147.jpg",
            "data": {
                "categories": [
                    "#machine_translation",
                    "#training",
                    "#multilingual",
                    "#benchmark",
                    "#dataset",
                    "#data",
                    "#synthetic"
                ],
                "emoji": "🌐",
                "ru": {
                    "title": "MIT-10M: новый стандарт для машинного перевода текста на изображениях",
                    "desc": "Статья представляет новый набор данных MIT-10M для задачи перевода текста на изображениях. Этот датасет содержит более 10 миллионов пар изображение-текст на 14 языках, охватывающих 28 категорий и три уровня сложности. MIT-10M значительно превосходит существующие наборы данных по масштабу, разнообразию и качеству. Эксперименты показали, что модели, обученные на MIT-10M, демонстрируют трехкратное улучшение производительности по сравнению с базовыми моделями."
                },
                "en": {
                    "title": "Unlocking Multilingual Image Translation with MIT-10M",
                    "desc": "This paper presents MIT-10M, a new large-scale dataset designed for multilingual image translation (IT). It consists of over 10 million image-text pairs, which are carefully curated to enhance diversity and quality, addressing the limitations of existing datasets. The dataset includes images across 28 categories and supports 14 languages, making it suitable for various IT tasks of different complexities. Experimental results show that models trained on MIT-10M significantly outperform baseline models, demonstrating its effectiveness in real-world image translation scenarios."
                },
                "zh": {
                    "title": "MIT-10M：提升图像翻译的多语言数据集",
                    "desc": "本文介绍了MIT-10M，这是一个大规模的多语言图像翻译平行语料库，包含超过1000万对图像和文本。该数据集经过严格的数据清理和多语言翻译验证，解决了现有数据集在规模、多样性和质量上的不足。MIT-10M包含840K张图像，涵盖28个类别和14种语言，提供了不同难度级别的任务。实验结果表明，使用MIT-10M微调的模型在处理复杂的图像翻译任务时，性能显著提升，达到了基线模型的三倍。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.08503",
            "title": "StyleStudio: Text-Driven Style Transfer with Selective Control of Style Elements",
            "url": "https://huggingface.co/papers/2412.08503",
            "abstract": "Text-driven style transfer aims to merge the style of a reference image with content described by a text prompt. Recent advancements in text-to-image models have improved the nuance of style transformations, yet significant challenges remain, particularly with overfitting to reference styles, limiting stylistic control, and misaligning with textual content. In this paper, we propose three complementary strategies to address these issues. First, we introduce a cross-modal Adaptive Instance Normalization (AdaIN) mechanism for better integration of style and text features, enhancing alignment. Second, we develop a Style-based Classifier-Free Guidance (SCFG) approach that enables selective control over stylistic elements, reducing irrelevant influences. Finally, we incorporate a teacher model during early generation stages to stabilize spatial layouts and mitigate artifacts. Our extensive evaluations demonstrate significant improvements in style transfer quality and alignment with textual prompts. Furthermore, our approach can be integrated into existing style transfer frameworks without fine-tuning.",
            "score": 0,
            "issue_id": 1088,
            "pub_date": "2024-12-11",
            "pub_date_card": {
                "ru": "11 декабря",
                "en": "December 11",
                "zh": "12月11日"
            },
            "hash": "7b217a36b7d6db44",
            "authors": [
                "Mingkun Lei",
                "Xue Song",
                "Beier Zhu",
                "Hao Wang",
                "Chi Zhang"
            ],
            "affiliations": [
                "AGI Lab, Westlake University",
                "Fudan University",
                "Nanyang Technological University",
                "The Hong Kong University of Science and Technology (Guangzhou)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.08503.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#multimodal"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "Улучшенный перенос стиля изображений с помощью текстового контроля",
                    "desc": "Статья предлагает новый подход к переносу стиля изображения на основе текстового описания. Авторы вводят три стратегии: кросс-модальный механизм AdaIN для лучшей интеграции стиля и текста, подход Style-based Classifier-Free Guidance для селективного контроля стилистических элементов, и использование учительской модели для стабилизации пространственной компоновки. Эксперименты показывают значительные улучшения в качестве переноса стиля и соответствии текстовым промптам. Предложенный метод может быть интегрирован в существующие фреймворки без дополнительного обучения."
                },
                "en": {
                    "title": "Enhancing Text-Driven Style Transfer with Adaptive Techniques",
                    "desc": "This paper focuses on improving text-driven style transfer, which combines the style of an image with content from a text description. The authors identify challenges such as overfitting to styles and misalignment with text, and propose three strategies to overcome these issues. They introduce a cross-modal Adaptive Instance Normalization (AdaIN) for better feature integration, a Style-based Classifier-Free Guidance (SCFG) for selective stylistic control, and a teacher model to stabilize outputs. The results show enhanced style transfer quality and better alignment with text, and the methods can be easily integrated into existing frameworks."
                },
                "zh": {
                    "title": "提升文本驱动风格迁移的质量与对齐性",
                    "desc": "本文研究了文本驱动的风格迁移，旨在将参考图像的风格与文本提示描述的内容相结合。尽管最近的文本到图像模型在风格转换的细微差别上取得了进展，但仍面临过拟合、风格控制有限和文本内容不对齐等挑战。为了解决这些问题，我们提出了三种互补策略，包括跨模态自适应实例归一化机制、基于风格的无分类器引导方法以及在生成早期阶段引入教师模型。我们的评估结果显示，所提方法在风格迁移质量和与文本提示的对齐性上有显著提升，并且可以无缝集成到现有的风格迁移框架中。"
                }
            }
        }
    ],
    "link_prev": "2024-12-11.html",
    "link_next": "2024-12-13.html",
    "link_month": "2024-12.html",
    "short_date_prev": {
        "ru": "11.12",
        "en": "12/11",
        "zh": "12月11日"
    },
    "short_date_next": {
        "ru": "13.12",
        "en": "12/13",
        "zh": "12月13日"
    },
    "categories": {
        "#dataset": 6,
        "#data": 2,
        "#benchmark": 4,
        "#agents": 0,
        "#cv": 5,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 4,
        "#audio": 0,
        "#video": 3,
        "#multimodal": 4,
        "#math": 0,
        "#multilingual": 2,
        "#architecture": 4,
        "#healthcare": 0,
        "#training": 4,
        "#robotics": 0,
        "#agi": 0,
        "#games": 2,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 4,
        "#survey": 0,
        "#diffusion": 3,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 2,
        "#machine_translation": 1,
        "#leakage": 0,
        "#open_source": 4,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 1,
        "#style_transfer": 1
    },
    "zh": {
        "text": "最近的视频扩散模型在模拟现实世界动态和维持3D一致性方面表现出色。这激发了我们研究这些模型在确保不同视角动态一致性方面的潜力，这对虚拟拍摄等应用非常有用。我们提出了一个插播模块，增强预训练的文本到视频模型，用于多摄像头视频生成，确保不同视角的内容一致。我们引入了多视角同步模块，维持外观和几何一致性。由于高质量训练数据的稀缺，我们设计了混合训练方案，利用多摄像头图像和单目视频补充虚幻引擎渲染的多摄像头视频。我们还发布了一个多视角同步视频数据集，命名为SynCamVideo-Dataset。",
        "title": "SynCamMaster: Synchronizing Multi-Camera Video Generation from Diverse Viewpoints",
        "pinyin": "Zùijìn de shìpín kuòsàn móxíng zài mónǐ xiànshì shìjiè dòngtài hé wéichí 3D yīzhìxìng fāngmiàn biǎoxiàn chūsè. Zhè jīfāle wǒmen yánjiū zhèxiē móxíng zài quèshǒu bùtóng shìjiǎo dòngtài yīzhìxìng fāngmiàn de qiánlì, zhè duì xūnǐ pāishè děng yìngyòng fēicháng yǒuyòng. Wǒmen tíchūle yīgè chābō mókuài, zēngqiáng yùjiàoxùn de wénběn dào shìpín móxíng, yòngyú duō shèxiàngtóu shìpín shēngchéng, quèshǒu bùtóng shìjiǎo de nèiróng yīzhì. Wǒmen yǐnrùle duō shìjiǎo tóngbù mókuài, wéichí wàiguǎn hé jǐhé yīzhìxìng. Yóuyú gāo zhìliàng xùnliàn shùjù de xīquē, wǒmen shèjìle hùn hé xùnliàn fāng'àn, lìyòng duō shèxiàngtóu túxiàng hé dānmù shìpín bǔchōng xūhuàn yǐnqíng xuànchǔ de duō shèxiàngtóu shìpín. Wǒmen hái fābùle yīgè duō shìjiǎo tóngbù shìpín shùjùjí, mìngmíng wéi SynCamVideo-Dataset.",
        "vocab": "[\n    {\"word\": \"视频扩散模型\", \"pinyin\": \"shìpín kuòsàn móxíng\", \"trans\": \"video diffusion model\"},\n    {\"word\": \"模拟\", \"pinyin\": \"mónǐ\", \"trans\": \"simulate\"},\n    {\"word\": \"现实世界\", \"pinyin\": \"xiànshí shìjiè\", \"trans\": \"real world\"},\n    {\"word\": \"动态\", \"pinyin\": \"dòngtài\", \"trans\": \"dynamics\"},\n    {\"word\": \"维持\", \"pinyin\": \"wéichí\", \"trans\": \"maintain\"},\n    {\"word\": \"3D一致性\", \"pinyin\": \"3D yīzhìxìng\", \"trans\": \"3D consistency\"},\n    {\"word\": \"表现出色\", \"pinyin\": \"biǎoxiàn chūsè\", \"trans\": \"perform excellently\"},\n    {\"word\": \"激发\", \"pinyin\": \"jīfā\", \"trans\": \"inspire\"},\n    {\"word\": \"潜力\", \"pinyin\": \"qiánlì\", \"trans\": \"potential\"},\n    {\"word\": \"虚拟拍摄\", \"pinyin\": \"xūnǐ pāishè\", \"trans\": \"virtual shooting\"},\n    {\"word\": \"应用\", \"pinyin\": \"yìngyòng\", \"trans\": \"application\"},\n    {\"word\": \"提出\", \"pinyin\": \"tíchū\", \"trans\": \"propose\"},\n    {\"word\": \"插播模块\", \"pinyin\": \"chābō mókuài\", \"trans\": \"interpolation module\"},\n    {\"word\": \"增强\", \"pinyin\": \"zēngqiáng\", \"trans\": \"enhance\"},\n    {\"word\": \"预训练\", \"pinyin\": \"yù xùnliàn\", \"trans\": \"pre-trained\"},\n    {\"word\": \"文本到视频模型\", \"pinyin\": \"wénběn dào shìpín móxíng\", \"trans\": \"text-to-video model\"},\n    {\"word\": \"多摄像头视频生成\", \"pinyin\": \"duō shèxiàngtou shìpín shēngchéng\", \"trans\": \"multi-camera video generation\"},\n    {\"word\": \"确保\", \"pinyin\": \"quèbǎo\", \"trans\": \"ensure\"},\n    {\"word\": \"内容一致\", \"pinyin\": \"nèiróng yīzhì\", \"trans\": \"content consistency\"},\n    {\"word\": \"引入\", \"pinyin\": \"yǐnrù\", \"trans\": \"introduce\"},\n    {\"word\": \"同步模块\", \"pinyin\": \"tóngbù mókuài\", \"trans\": \"synchronization module\"},\n    {\"word\": \"外观\", \"pinyin\": \"wàiguǎn\", \"trans\": \"appearance\"},\n    {\"word\": \"几何一致性\", \"pinyin\": \"jǐhé yīzhìxìng\", \"trans\": \"geometric consistency\"},\n    {\"word\": \"稀缺\", \"pinyin\": \"xīquē\", \"trans\": \"scarce\"},\n    {\"word\": \"设计\", \"pinyin\": \"shèjì\", \"trans\": \"design\"},\n    {\"word\": \"混合训练方案\", \"pinyin\": \"hùnhé xùnliàn fāng'àn\", \"trans\": \"hybrid training scheme\"},\n    {\"word\": \"利用\", \"pinyin\": \"lìyòng\", \"trans\": \"utilize\"},\n    {\"word\": \"多摄像头图像\", \"pinyin\": \"duō shèxiàngtou túxiàng\", \"trans\": \"multi-camera images\"},\n    {\"word\": \"单目视频\", \"pinyin\": \"dānmù shìpín\", \"trans\": \"monocular video\"},\n    {\"word\": \"补充\", \"pinyin\": \"bǔchōng\", \"trans\": \"supplement\"},\n    {\"word\": \"虚幻引擎\", \"pinyin\": \"xūhuàn yǐnqíng\", \"trans\": \"Unreal Engine\"},\n    {\"word\": \"渲染\", \"pinyin\": \"xuànxiàn\", \"trans\": \"render\"},\n    {\"word\": \"发布\", \"pinyin\": \"fābù\", \"trans\": \"release\"},\n    {\"word\": \"数据集\", \"pinyin\": \"shùjùjí\", \"trans\": \"dataset\"},\n    {\"word\": \"命名为\", \"pinyin\": \"mìngmíng wéi\", \"trans\": \"named as\"},\n    {\"word\": \"SynCamVideo-Dataset\", \"pinyin\": \"SynCamVideo-Dataset\", \"trans\": \"SynCamVideo-Dataset\"}\n]",
        "trans": "Recent video diffusion models have shown excellent performance in simulating real-world dynamics and maintaining 3D consistency. This has inspired us to explore the potential of these models in ensuring dynamic consistency across different viewpoints, which is particularly useful for applications such as virtual cinematography. We propose an interpolation module that enhances pre-trained text-to-video models for multi-camera video generation, ensuring content consistency across different viewpoints. We introduce a multi-view synchronization module to maintain consistency in appearance and geometry. Due to the scarcity of high-quality training data, we designed a hybrid training scheme that utilizes multi-camera images and monocular videos to supplement Unreal Engine-rendered multi-camera videos. Additionally, we release a multi-view synchronized video dataset named SynCamVideo-Dataset.",
        "update_ts": "2024-12-12 09:11"
    }
}