{
    "date": {
        "ru": "6 октября",
        "en": "October 6",
        "zh": "10月6日"
    },
    "time_utc": "2025-10-06 02:19",
    "weekday": 0,
    "issue_id": 6252,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2510.01068",
            "title": "Compose Your Policies! Improving Diffusion-based or Flow-based Robot\n  Policies via Test-time Distribution-level Composition",
            "url": "https://huggingface.co/papers/2510.01068",
            "abstract": "General Policy Composition (GPC) enhances robotic control performance by combining pre-trained diffusion-based policies without additional training, leading to superior results across various benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion-based models for robotic control, including vision-language-action (VLA) and vision-action (VA) policies, have demonstrated significant capabilities. Yet their advancement is constrained by the high cost of acquiring large-scale interaction datasets. This work introduces an alternative paradigm for enhancing policy performance without additional model training. Perhaps surprisingly, we demonstrate that the composed policies can exceed the performance of either parent policy. Our contribution is threefold. First, we establish a theoretical foundation showing that the convex composition of distributional scores from multiple diffusion models can yield a superior one-step functional objective compared to any individual score. A Gr\\\"onwall-type bound is then used to show that this single-step improvement propagates through entire generation trajectories, leading to systemic performance gains. Second, motivated by these results, we propose General Policy Composition (GPC), a training-free method that enhances performance by combining the distributional scores of multiple pre-trained policies via a convex combination and test-time search. GPC is versatile, allowing for the plug-and-play composition of heterogeneous policies, including VA and VLA models, as well as those based on diffusion or flow-matching, irrespective of their input visual modalities. Third, we provide extensive empirical validation. Experiments on Robomimic, PushT, and RoboTwin benchmarks, alongside real-world robotic evaluations, confirm that GPC consistently improves performance and adaptability across a diverse set of tasks. Further analysis of alternative composition operators and weighting strategies offers insights into the mechanisms underlying the success of GPC. These results establish GPC as a simple yet effective method for improving control performance by leveraging existing policies.",
            "score": 3,
            "issue_id": 6252,
            "pub_date": "2025-10-01",
            "pub_date_card": {
                "ru": "1 октября",
                "en": "October 1",
                "zh": "10月1日"
            },
            "hash": "f7a26368ff58e67e",
            "authors": [
                "Jiahang Cao",
                "Yize Huang",
                "Hanzhong Guo",
                "Rui Zhang",
                "Mu Nan",
                "Weijian Mai",
                "Jiaxu Wang",
                "Hao Cheng",
                "Jingkai Sun",
                "Gang Han",
                "Wen Zhao",
                "Qiang Zhang",
                "Yijie Guo",
                "Qihao Zheng",
                "Chunfeng Song",
                "Xiao Li",
                "Ping Luo",
                "Andrew F. Luo"
            ],
            "affiliations": [
                "Beijing Innovation Center of Humanoid Robotics",
                "Shanghai AI Lab",
                "Shanghai Jiaotong University",
                "The Hong Kong University of Science and Technology",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.01068.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#robotics",
                    "#optimization",
                    "#benchmark",
                    "#diffusion",
                    "#agents"
                ],
                "emoji": "🤝",
                "ru": {
                    "title": "Композиция policy без обучения превосходит отдельные модели",
                    "desc": "Статья представляет метод General Policy Composition (GPC), который позволяет улучшить производительность робототехнических систем путём композиции нескольких предобученных diffusion-моделей без дополнительного обучения. Авторы доказывают теоретически, что выпуклая комбинация распределений от разных моделей может превзойти каждую отдельную policy. GPC работает с гетерогенными моделями — vision-language-action (VLA) и vision-action (VA), основанными на diffusion или flow-matching. Эксперименты на бенчмарках Robomimic, PushT, RoboTwin и реальных роботах подтверждают стабильное улучшение качества управления."
                },
                "en": {
                    "title": "Enhancing Robotic Control with Policy Composition",
                    "desc": "General Policy Composition (GPC) is a novel approach that enhances robotic control by combining pre-trained diffusion-based policies without the need for additional training. This method leverages the strengths of multiple policies, including vision-language-action and vision-action models, to achieve superior performance on various benchmarks. The theoretical foundation of GPC shows that combining distributional scores from different models can lead to better outcomes than using any single model alone. Extensive experiments demonstrate that GPC not only improves performance but also increases adaptability across diverse robotic tasks, making it a versatile tool in the field of robotic control."
                },
                "zh": {
                    "title": "通用策略组合：提升机器人控制性能的新方法",
                    "desc": "本文提出了一种名为通用策略组合（GPC）的方法，旨在通过结合预训练的扩散模型策略来提升机器人控制性能，而无需额外的训练。研究表明，组合后的策略在多个基准测试中表现优于单独的父策略。GPC利用凸组合的方式，将多个策略的分布得分进行结合，从而实现系统性的性能提升。通过在多个机器人任务上的实验证明，GPC在提高适应性和性能方面表现出色。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.03230",
            "title": "Improving GUI Grounding with Explicit Position-to-Coordinate Mapping",
            "url": "https://huggingface.co/papers/2510.03230",
            "abstract": "Explicit coordinate markers and improved spatial encoding enhance GUI grounding accuracy across diverse resolutions and platforms.  \t\t\t\t\tAI-generated summary \t\t\t\t GUI grounding, the task of mapping natural-language instructions to pixel coordinates, is crucial for autonomous agents, yet remains difficult for current VLMs. The core bottleneck is reliable patch-to-pixel mapping, which breaks when extrapolating to high-resolution displays unseen during training. Current approaches generate coordinates as text tokens directly from visual features, forcing the model to infer complex position-to-pixel mappings implicitly; as a result, accuracy degrades and failures proliferate on new resolutions. We address this with two complementary innovations. First, RULER tokens serve as explicit coordinate markers, letting the model reference positions similar to gridlines on a map and adjust rather than generate coordinates from scratch. Second, Interleaved MRoPE (I-MRoPE) improves spatial encoding by ensuring that width and height dimensions are represented equally, addressing the asymmetry of standard positional schemes. Experiments on ScreenSpot, ScreenSpot-V2, and ScreenSpot-Pro show consistent gains in grounding accuracy, with the largest improvements on high-resolution interfaces. By providing explicit spatial guidance rather than relying on implicit learning, our approach enables more reliable GUI automation across diverse resolutions and platforms.",
            "score": 1,
            "issue_id": 6252,
            "pub_date": "2025-10-03",
            "pub_date_card": {
                "ru": "3 октября",
                "en": "October 3",
                "zh": "10月3日"
            },
            "hash": "8911479d98450376",
            "authors": [
                "Suyuchen Wang",
                "Tianyu Zhang",
                "Ahmed Masry",
                "Christopher Pal",
                "Spandana Gella",
                "Bang Liu",
                "Perouz Taslakian"
            ],
            "affiliations": [
                "CIFAR AI Chair",
                "McGill University",
                "Mila - Quebec AI Institute",
                "Polytechnique Montreal",
                "ServiceNow",
                "Universite de Montreal",
                "York University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.03230.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#agents",
                    "#cv",
                    "#optimization"
                ],
                "emoji": "🎯",
                "ru": {
                    "title": "Явные координаты вместо угадывания: как научить модели точно находить элементы интерфейса",
                    "desc": "Статья посвящена проблеме GUI grounding — задаче сопоставления текстовых инструкций с координатами пикселей на экране, что критично для автономных AI-агентов. Основная сложность заключается в том, что современные vision-language модели плохо экстраполируют на высокие разрешения экранов, не встреченные при обучении. Авторы предлагают два решения: RULER tokens — явные маркеры координат, работающие как линии сетки на карте, и Interleaved MRoPE (I-MRoPE) — улучшенное позиционное кодирование, которое равномерно представляет ширину и высоту. Эксперименты показывают значительное улучшение точности определения элементов интерфейса, особенно на экранах высокого разрешения."
                },
                "en": {
                    "title": "Enhancing GUI Grounding with Explicit Spatial Markers",
                    "desc": "This paper focuses on improving GUI grounding, which is the process of translating natural language commands into specific pixel locations on a screen. The authors identify that existing vision-language models (VLMs) struggle with high-resolution displays due to their reliance on implicit mappings from visual features to pixel coordinates. To overcome this, they introduce RULER tokens as explicit coordinate markers, allowing the model to reference positions more accurately. Additionally, they propose Interleaved MRoPE (I-MRoPE) to enhance spatial encoding, ensuring that both width and height are treated equally, leading to significant improvements in grounding accuracy across various resolutions."
                },
                "zh": {
                    "title": "提升GUI定位准确性的创新方法",
                    "desc": "本文探讨了图形用户界面（GUI）定位的挑战，尤其是在高分辨率显示器上的准确性问题。当前的视觉语言模型（VLMs）在将自然语言指令映射到像素坐标时，面临着可靠的补丁到像素映射的瓶颈。为了解决这个问题，作者提出了两种创新方法：使用RULER标记作为明确的坐标标记，以及改进空间编码的交错MRoPE（I-MRoPE）。实验结果表明，这些方法在不同分辨率和平台上显著提高了GUI定位的准确性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.03120",
            "title": "SurveyBench: How Well Can LLM(-Agents) Write Academic Surveys?",
            "url": "https://huggingface.co/papers/2510.03120",
            "abstract": "A new evaluation framework, SurveyBench, assesses the quality of automatically generated academic surveys using a quiz-driven approach, revealing deficiencies in current LLM4Survey methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Academic survey writing, which distills vast literature into a coherent and insightful narrative, remains a labor-intensive and intellectually demanding task. While recent approaches, such as general DeepResearch agents and survey-specialized methods, can generate surveys automatically (a.k.a. LLM4Survey), their outputs often fall short of human standards and there lacks a rigorous, reader-aligned benchmark for thoroughly revealing their deficiencies. To fill the gap, we propose a fine-grained, quiz-driven evaluation framework SurveyBench, featuring (1) typical survey topics source from recent 11,343 arXiv papers and corresponding 4,947 high-quality surveys; (2) a multifaceted metric hierarchy that assesses the outline quality (e.g., coverage breadth, logical coherence), content quality (e.g., synthesis granularity, clarity of insights), and non-textual richness; and (3) a dual-mode evaluation protocol that includes content-based and quiz-based answerability tests, explicitly aligned with readers' informational needs. Results show SurveyBench effectively challenges existing LLM4Survey approaches (e.g., on average 21% lower than human in content-based evaluation).",
            "score": 1,
            "issue_id": 6252,
            "pub_date": "2025-10-03",
            "pub_date_card": {
                "ru": "3 октября",
                "en": "October 3",
                "zh": "10月3日"
            },
            "hash": "9114023adb7490f9",
            "authors": [
                "Zhaojun Sun",
                "Xuzhou Zhu",
                "Xuanhe Zhou",
                "Xin Tong",
                "Shuo Wang",
                "Jie Fu",
                "Guoliang Li",
                "Zhiyuan Liu",
                "Fan Wu"
            ],
            "affiliations": [
                "Shanghai AI Laboratory",
                "Shanghai Jiao Tong University",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.03120.jpg",
            "data": {
                "categories": [
                    "#survey",
                    "#benchmark"
                ],
                "emoji": "📊",
                "ru": {
                    "title": "SurveyBench: бенчмарк для проверки AI-генерации научных обзоров через викторины",
                    "desc": "Исследователи представили SurveyBench — новый фреймворк для оценки качества автоматически сгенерированных научных обзоров с помощью LLM. Система использует quiz-driven подход и оценивает структуру обзора, качество контента и наличие нетекстовых элементов на основе 11,343 статей с arXiv и 4,947 высококачественных обзоров. Результаты показывают, что современные LLM4Survey методы в среднем на 21% хуже справляются с задачей по сравнению с человеком. Фреймворк включает двухрежимную оценку, которая проверяет способность сгенерированных обзоров отвечать на вопросы читателей."
                },
                "en": {
                    "title": "SurveyBench: Elevating AI-Generated Academic Surveys",
                    "desc": "The paper introduces SurveyBench, a new evaluation framework designed to assess the quality of automatically generated academic surveys. It highlights the limitations of current LLM4Survey methods, which often fail to meet human standards in survey writing. SurveyBench utilizes a quiz-driven approach and a comprehensive metric hierarchy to evaluate both outline and content quality, ensuring alignment with reader needs. The results demonstrate that existing methods significantly underperform compared to human-generated surveys, with an average score 21% lower in content-based evaluations."
                },
                "zh": {
                    "title": "SurveyBench：提升自动生成学术调查的评估标准",
                    "desc": "本论文提出了一种新的评估框架SurveyBench，用于评估自动生成的学术调查的质量。该框架采用基于测验的方法，揭示了当前LLM4Survey方法的不足之处。SurveyBench通过分析来自11,343篇arXiv论文的典型调查主题和4,947篇高质量调查，建立了多层次的评估指标体系。研究结果表明，SurveyBench在内容评估中平均比人类低21%，有效挑战了现有的LLM4Survey方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.02665",
            "title": "Self-Improvement in Multimodal Large Language Models: A Survey",
            "url": "https://huggingface.co/papers/2510.02665",
            "abstract": "A survey of self-improvement methods in Multimodal Large Language Models (MLLMs) from data collection, organization, and model optimization perspectives.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in self-improvement for Large Language Models (LLMs) have efficiently enhanced model capabilities without significantly increasing costs, particularly in terms of human effort. While this area is still relatively young, its extension to the multimodal domain holds immense potential for leveraging diverse data sources and developing more general self-improving models. This survey is the first to provide a comprehensive overview of self-improvement in Multimodal LLMs (MLLMs). We provide a structured overview of the current literature and discuss methods from three perspectives: 1) data collection, 2) data organization, and 3) model optimization, to facilitate the further development of self-improvement in MLLMs. We also include commonly used evaluations and downstream applications. Finally, we conclude by outlining open challenges and future research directions.",
            "score": 1,
            "issue_id": 6252,
            "pub_date": "2025-10-03",
            "pub_date_card": {
                "ru": "3 октября",
                "en": "October 3",
                "zh": "10月3日"
            },
            "hash": "a7980db6477e39f7",
            "authors": [
                "Shijian Deng",
                "Kai Wang",
                "Tianyu Yang",
                "Harsh Singh",
                "Yapeng Tian"
            ],
            "affiliations": [
                "Mohamed bin Zayed University of Artificial Intelligence",
                "The University of Texas at Dallas",
                "University of Notre Dame",
                "University of Toronto"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.02665.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#survey",
                    "#optimization",
                    "#multimodal",
                    "#data",
                    "#dataset"
                ],
                "emoji": "🔄",
                "ru": {
                    "title": "Мультимодальные LLM учатся сами: обзор методов самосовершенствования",
                    "desc": "Статья представляет первый комплексный обзор методов самосовершенствования мультимодальных LLM. Авторы систематизируют существующие подходы с трёх ключевых точек зрения: сбор данных, организация данных и оптимизация моделей. Самосовершенствование позволяет улучшать возможности моделей без значительного увеличения затрат и человеческих усилий. В работе также обсуждаются методы оценки, практические применения и открытые проблемы в этой развивающейся области исследований."
                },
                "en": {
                    "title": "Unlocking Potential: Self-Improvement in Multimodal Language Models",
                    "desc": "This paper surveys self-improvement methods in Multimodal Large Language Models (MLLMs), focusing on how to enhance model performance through better data handling and optimization techniques. It highlights the importance of efficiently collecting and organizing diverse data sources to improve model capabilities without incurring high costs. The authors provide a structured overview of existing literature and categorize methods into three main areas: data collection, data organization, and model optimization. Additionally, the paper discusses evaluation metrics and potential applications, while identifying challenges and future research opportunities in the field."
                },
                "zh": {
                    "title": "多模态语言模型的自我改进潜力",
                    "desc": "本论文对多模态大型语言模型（MLLMs）中的自我改进方法进行了全面调查。我们从数据收集、数据组织和模型优化三个角度，系统性地回顾了当前文献，探讨了如何有效提升模型能力。尽管这一领域仍在发展中，但其在多模态领域的扩展具有巨大的潜力，可以利用多样的数据源。最后，我们总结了当前面临的挑战和未来的研究方向。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.02571",
            "title": "How Confident are Video Models? Empowering Video Models to Express their\n  Uncertainty",
            "url": "https://huggingface.co/papers/2510.02571",
            "abstract": "A framework for uncertainty quantification in generative video models is introduced, including a metric for calibration, a black-box method called S-QUBED, and a benchmark dataset, demonstrating improved uncertainty estimates and task accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Generative video models demonstrate impressive text-to-video capabilities, spurring widespread adoption in many real-world applications. However, like large language models (LLMs), video generation models tend to hallucinate, producing plausible videos even when they are factually wrong. Although uncertainty quantification (UQ) of LLMs has been extensively studied in prior work, no UQ method for video models exists, raising critical safety concerns. To our knowledge, this paper represents the first work towards quantifying the uncertainty of video models. We present a framework for uncertainty quantification of generative video models, consisting of: (i) a metric for evaluating the calibration of video models based on robust rank correlation estimation with no stringent modeling assumptions; (ii) a black-box UQ method for video models (termed S-QUBED), which leverages latent modeling to rigorously decompose predictive uncertainty into its aleatoric and epistemic components; and (iii) a UQ dataset to facilitate benchmarking calibration in video models. By conditioning the generation task in the latent space, we disentangle uncertainty arising due to vague task specifications from that arising from lack of knowledge. Through extensive experiments on benchmark video datasets, we demonstrate that S-QUBED computes calibrated total uncertainty estimates that are negatively correlated with the task accuracy and effectively computes the aleatoric and epistemic constituents.",
            "score": 1,
            "issue_id": 6252,
            "pub_date": "2025-10-02",
            "pub_date_card": {
                "ru": "2 октября",
                "en": "October 2",
                "zh": "10月2日"
            },
            "hash": "6e5849ca43586c8a",
            "authors": [
                "Zhiting Mei",
                "Ola Shorinwa",
                "Anirudha Majumdar"
            ],
            "affiliations": [
                "Princeton University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.02571.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#benchmark",
                    "#hallucinations",
                    "#dataset",
                    "#video"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Когда AI не уверен в своём видео",
                    "desc": "Исследователи представили первый фреймворк для количественной оценки неопределённости в генеративных видеомоделях, которые, как и LLM, склонны к галлюцинациям. Разработан метод S-QUBED, разделяющий неопределённость на алеаторную (из-за неясных формулировок задачи) и эпистемическую (из-за недостатка знаний модели) компоненты через моделирование в латентном пространстве. Предложена метрика калибровки на основе ранговой корреляции и создан специальный benchmark-датасет для оценки. Эксперименты показали, что метод даёт калиброванные оценки неопределённости, которые коррелируют с точностью выполнения задач."
                },
                "en": {
                    "title": "Quantifying Uncertainty in Generative Video Models for Safer AI",
                    "desc": "This paper introduces a new framework for measuring uncertainty in generative video models, which is crucial for ensuring their reliability in real-world applications. It presents a novel metric for assessing how well these models predict uncertainty, along with a black-box method called S-QUBED that separates different types of uncertainty. The framework also includes a benchmark dataset to evaluate the performance of video models in terms of their uncertainty calibration. Through experiments, the authors show that S-QUBED provides accurate uncertainty estimates that correlate with the models' task performance, addressing safety concerns in video generation."
                },
                "zh": {
                    "title": "生成视频模型的不确定性量化新框架",
                    "desc": "本文提出了一种用于生成视频模型的不确定性量化框架，包括一个用于校准的度量标准、一种称为S-QUBED的黑箱方法，以及一个基准数据集。生成视频模型在文本到视频的能力上表现出色，但也存在幻觉现象，即生成的内容可能在事实上一无是处。尽管对大型语言模型的不确定性量化已有大量研究，但目前尚无针对视频模型的不确定性量化方法，这引发了安全隐患。我们的研究首次量化了视频模型的不确定性，并通过实验验证了S-QUBED在校准总不确定性估计方面的有效性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.03204",
            "title": "FocusAgent: Simple Yet Effective Ways of Trimming the Large Context of\n  Web Agents",
            "url": "https://huggingface.co/papers/2510.03204",
            "abstract": "FocusAgent uses a lightweight LLM retriever to extract relevant content from web page observations, improving efficiency and security in web agents.  \t\t\t\t\tAI-generated summary \t\t\t\t Web agents powered by large language models (LLMs) must process lengthy web page observations to complete user goals; these pages often exceed tens of thousands of tokens. This saturates context limits and increases computational cost processing; moreover, processing full pages exposes agents to security risks such as prompt injection. Existing pruning strategies either discard relevant content or retain irrelevant context, leading to suboptimal action prediction. We introduce FocusAgent, a simple yet effective approach that leverages a lightweight LLM retriever to extract the most relevant lines from accessibility tree (AxTree) observations, guided by task goals. By pruning noisy and irrelevant content, FocusAgent enables efficient reasoning while reducing vulnerability to injection attacks. Experiments on WorkArena and WebArena benchmarks show that FocusAgent matches the performance of strong baselines, while reducing observation size by over 50%. Furthermore, a variant of FocusAgent significantly reduces the success rate of prompt-injection attacks, including banner and pop-up attacks, while maintaining task success performance in attack-free settings. Our results highlight that targeted LLM-based retrieval is a practical and robust strategy for building web agents that are efficient, effective, and secure.",
            "score": 0,
            "issue_id": 6252,
            "pub_date": "2025-10-03",
            "pub_date_card": {
                "ru": "3 октября",
                "en": "October 3",
                "zh": "10月3日"
            },
            "hash": "cff617954ead65b4",
            "authors": [
                "Imene Kerboua",
                "Sahar Omidi Shayegan",
                "Megh Thakkar",
                "Xing Han Lù",
                "Léo Boisvert",
                "Massimo Caccia",
                "Jérémy Espinas",
                "Alexandre Aussem",
                "Véronique Eglin",
                "Alexandre Lacoste"
            ],
            "affiliations": [
                "Esker",
                "LIRIS - CNRS, INSA Lyon, Universite Claude Bernard Lyon 1",
                "McGill University",
                "Mila - Quebec AI Institute",
                "Polytechnique Montréal",
                "ServiceNow Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.03204.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#inference",
                    "#benchmark",
                    "#agents",
                    "#security",
                    "#reasoning"
                ],
                "emoji": "🎯",
                "ru": {
                    "title": "Фокусировка внимания веб-агентов для эффективности и безопасности",
                    "desc": "FocusAgent — это подход для создания веб-агентов на основе LLM, который использует лёгкий retriever для извлечения релевантного контента из веб-страниц. Проблема в том, что веб-страницы часто содержат десятки тысяч токенов, что создаёт нагрузку на контекст и увеличивает вычислительные затраты, а также открывает уязвимости для prompt injection атак. FocusAgent фильтрует accessibility tree наблюдений, оставляя только важные строки согласно цели задачи, сокращая размер наблюдений более чем на 50% без потери качества. Эксперименты показывают, что метод не только сохраняет производительность на бенчмарках WorkArena и WebArena, но и значительно повышает защищённость от инъекций промптов."
                },
                "en": {
                    "title": "Efficient and Secure Web Agents with FocusAgent",
                    "desc": "FocusAgent is a novel approach that enhances the efficiency and security of web agents using a lightweight LLM retriever. It extracts the most relevant information from lengthy web page observations, which often contain excessive tokens that can overwhelm processing capabilities. By focusing on task-specific content and eliminating irrelevant data, FocusAgent minimizes computational costs and reduces the risk of security threats like prompt injection. Experimental results demonstrate that it not only maintains performance comparable to existing methods but also significantly decreases the amount of data processed, leading to safer and more effective web interactions."
                },
                "zh": {
                    "title": "FocusAgent：高效安全的网页代理解决方案",
                    "desc": "FocusAgent 是一种轻量级的 LLM 检索器，旨在从网页观察中提取相关内容，从而提高网络代理的效率和安全性。传统的大型语言模型在处理长达数万标记的网页时，容易导致上下文限制饱和和计算成本增加，同时也增加了安全风险。FocusAgent 通过从可访问性树（AxTree）观察中提取最相关的行，减少了噪声和无关内容，使推理过程更加高效，并降低了注入攻击的脆弱性。实验结果表明，FocusAgent 在保持任务成功率的同时，观察大小减少超过 50%，并显著降低了提示注入攻击的成功率。"
                }
            }
        }
    ],
    "link_prev": "2025-10-03.html",
    "link_next": "2025-10-07.html",
    "link_month": "2025-10.html",
    "short_date_prev": {
        "ru": "03.10",
        "en": "10/03",
        "zh": "10月3日"
    },
    "short_date_next": {
        "ru": "07.10",
        "en": "10/07",
        "zh": "10月7日"
    },
    "categories": {
        "#dataset": 2,
        "#data": 1,
        "#benchmark": 4,
        "#agents": 3,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 1,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 2,
        "#robotics": 1,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 1,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 4,
        "#survey": 2,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    }
}