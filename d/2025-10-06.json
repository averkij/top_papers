{
    "date": {
        "ru": "6 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
        "en": "October 6",
        "zh": "10æœˆ6æ—¥"
    },
    "time_utc": "2025-10-06 02:19",
    "weekday": 0,
    "issue_id": 6252,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2510.01068",
            "title": "Compose Your Policies! Improving Diffusion-based or Flow-based Robot\n  Policies via Test-time Distribution-level Composition",
            "url": "https://huggingface.co/papers/2510.01068",
            "abstract": "General Policy Composition (GPC) enhances robotic control performance by combining pre-trained diffusion-based policies without additional training, leading to superior results across various benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion-based models for robotic control, including vision-language-action (VLA) and vision-action (VA) policies, have demonstrated significant capabilities. Yet their advancement is constrained by the high cost of acquiring large-scale interaction datasets. This work introduces an alternative paradigm for enhancing policy performance without additional model training. Perhaps surprisingly, we demonstrate that the composed policies can exceed the performance of either parent policy. Our contribution is threefold. First, we establish a theoretical foundation showing that the convex composition of distributional scores from multiple diffusion models can yield a superior one-step functional objective compared to any individual score. A Gr\\\"onwall-type bound is then used to show that this single-step improvement propagates through entire generation trajectories, leading to systemic performance gains. Second, motivated by these results, we propose General Policy Composition (GPC), a training-free method that enhances performance by combining the distributional scores of multiple pre-trained policies via a convex combination and test-time search. GPC is versatile, allowing for the plug-and-play composition of heterogeneous policies, including VA and VLA models, as well as those based on diffusion or flow-matching, irrespective of their input visual modalities. Third, we provide extensive empirical validation. Experiments on Robomimic, PushT, and RoboTwin benchmarks, alongside real-world robotic evaluations, confirm that GPC consistently improves performance and adaptability across a diverse set of tasks. Further analysis of alternative composition operators and weighting strategies offers insights into the mechanisms underlying the success of GPC. These results establish GPC as a simple yet effective method for improving control performance by leveraging existing policies.",
            "score": 3,
            "issue_id": 6252,
            "pub_date": "2025-10-01",
            "pub_date_card": {
                "ru": "1 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 1",
                "zh": "10æœˆ1æ—¥"
            },
            "hash": "f7a26368ff58e67e",
            "authors": [
                "Jiahang Cao",
                "Yize Huang",
                "Hanzhong Guo",
                "Rui Zhang",
                "Mu Nan",
                "Weijian Mai",
                "Jiaxu Wang",
                "Hao Cheng",
                "Jingkai Sun",
                "Gang Han",
                "Wen Zhao",
                "Qiang Zhang",
                "Yijie Guo",
                "Qihao Zheng",
                "Chunfeng Song",
                "Xiao Li",
                "Ping Luo",
                "Andrew F. Luo"
            ],
            "affiliations": [
                "Beijing Innovation Center of Humanoid Robotics",
                "Shanghai AI Lab",
                "Shanghai Jiaotong University",
                "The Hong Kong University of Science and Technology",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.01068.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#robotics",
                    "#optimization",
                    "#benchmark",
                    "#diffusion",
                    "#agents"
                ],
                "emoji": "ğŸ¤",
                "ru": {
                    "title": "ĞšĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ policy Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ General Policy Composition (GPC), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… diffusion-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸, Ñ‡Ñ‚Ğ¾ Ğ²Ñ‹Ğ¿ÑƒĞºĞ»Ğ°Ñ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¹ Ğ¾Ñ‚ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾Ğ¹Ñ‚Ğ¸ ĞºĞ°Ğ¶Ğ´ÑƒÑ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½ÑƒÑ policy. GPC Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ñ Ğ³ĞµÑ‚ĞµÑ€Ğ¾Ğ³ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ â€” vision-language-action (VLA) Ğ¸ vision-action (VA), Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ½Ğ° diffusion Ğ¸Ğ»Ğ¸ flow-matching. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Robomimic, PushT, RoboTwin Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ñ… Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Enhancing Robotic Control with Policy Composition",
                    "desc": "General Policy Composition (GPC) is a novel approach that enhances robotic control by combining pre-trained diffusion-based policies without the need for additional training. This method leverages the strengths of multiple policies, including vision-language-action and vision-action models, to achieve superior performance on various benchmarks. The theoretical foundation of GPC shows that combining distributional scores from different models can lead to better outcomes than using any single model alone. Extensive experiments demonstrate that GPC not only improves performance but also increases adaptability across diverse robotic tasks, making it a versatile tool in the field of robotic control."
                },
                "zh": {
                    "title": "é€šç”¨ç­–ç•¥ç»„åˆï¼šæå‡æœºå™¨äººæ§åˆ¶æ€§èƒ½çš„æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºé€šç”¨ç­–ç•¥ç»„åˆï¼ˆGPCï¼‰çš„æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡ç»“åˆé¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ç­–ç•¥æ¥æå‡æœºå™¨äººæ§åˆ¶æ€§èƒ½ï¼Œè€Œæ— éœ€é¢å¤–çš„è®­ç»ƒã€‚ç ”ç©¶è¡¨æ˜ï¼Œç»„åˆåçš„ç­–ç•¥åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºå•ç‹¬çš„çˆ¶ç­–ç•¥ã€‚GPCåˆ©ç”¨å‡¸ç»„åˆçš„æ–¹å¼ï¼Œå°†å¤šä¸ªç­–ç•¥çš„åˆ†å¸ƒå¾—åˆ†è¿›è¡Œç»“åˆï¼Œä»è€Œå®ç°ç³»ç»Ÿæ€§çš„æ€§èƒ½æå‡ã€‚é€šè¿‡åœ¨å¤šä¸ªæœºå™¨äººä»»åŠ¡ä¸Šçš„å®éªŒè¯æ˜ï¼ŒGPCåœ¨æé«˜é€‚åº”æ€§å’Œæ€§èƒ½æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.03230",
            "title": "Improving GUI Grounding with Explicit Position-to-Coordinate Mapping",
            "url": "https://huggingface.co/papers/2510.03230",
            "abstract": "Explicit coordinate markers and improved spatial encoding enhance GUI grounding accuracy across diverse resolutions and platforms.  \t\t\t\t\tAI-generated summary \t\t\t\t GUI grounding, the task of mapping natural-language instructions to pixel coordinates, is crucial for autonomous agents, yet remains difficult for current VLMs. The core bottleneck is reliable patch-to-pixel mapping, which breaks when extrapolating to high-resolution displays unseen during training. Current approaches generate coordinates as text tokens directly from visual features, forcing the model to infer complex position-to-pixel mappings implicitly; as a result, accuracy degrades and failures proliferate on new resolutions. We address this with two complementary innovations. First, RULER tokens serve as explicit coordinate markers, letting the model reference positions similar to gridlines on a map and adjust rather than generate coordinates from scratch. Second, Interleaved MRoPE (I-MRoPE) improves spatial encoding by ensuring that width and height dimensions are represented equally, addressing the asymmetry of standard positional schemes. Experiments on ScreenSpot, ScreenSpot-V2, and ScreenSpot-Pro show consistent gains in grounding accuracy, with the largest improvements on high-resolution interfaces. By providing explicit spatial guidance rather than relying on implicit learning, our approach enables more reliable GUI automation across diverse resolutions and platforms.",
            "score": 1,
            "issue_id": 6252,
            "pub_date": "2025-10-03",
            "pub_date_card": {
                "ru": "3 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 3",
                "zh": "10æœˆ3æ—¥"
            },
            "hash": "8911479d98450376",
            "authors": [
                "Suyuchen Wang",
                "Tianyu Zhang",
                "Ahmed Masry",
                "Christopher Pal",
                "Spandana Gella",
                "Bang Liu",
                "Perouz Taslakian"
            ],
            "affiliations": [
                "CIFAR AI Chair",
                "McGill University",
                "Mila - Quebec AI Institute",
                "Polytechnique Montreal",
                "ServiceNow",
                "Universite de Montreal",
                "York University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.03230.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#agents",
                    "#cv",
                    "#optimization"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "Ğ¯Ğ²Ğ½Ñ‹Ğµ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚Ñ‹ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ÑƒĞ³Ğ°Ğ´Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ: ĞºĞ°Ğº Ğ½Ğ°ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğµ GUI grounding â€” Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ñ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚Ğ°Ğ¼Ğ¸ Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹ Ğ½Ğ° ÑĞºÑ€Ğ°Ğ½Ğµ, Ñ‡Ñ‚Ğ¾ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ vision-language Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ»Ğ¾Ñ…Ğ¾ ÑĞºÑÑ‚Ñ€Ğ°Ğ¿Ğ¾Ğ»Ğ¸Ñ€ÑƒÑÑ‚ Ğ½Ğ° Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğµ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑĞºÑ€Ğ°Ğ½Ğ¾Ğ², Ğ½Ğµ Ğ²ÑÑ‚Ñ€ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ²Ğ° Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ: RULER tokens â€” ÑĞ²Ğ½Ñ‹Ğµ Ğ¼Ğ°Ñ€ĞºĞµÑ€Ñ‹ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‰Ğ¸Ğµ ĞºĞ°Ğº Ğ»Ğ¸Ğ½Ğ¸Ğ¸ ÑĞµÑ‚ĞºĞ¸ Ğ½Ğ° ĞºĞ°Ñ€Ñ‚Ğµ, Ğ¸ Interleaved MRoPE (I-MRoPE) â€” ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ñ€Ğ°Ğ²Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑˆĞ¸Ñ€Ğ¸Ğ½Ñƒ Ğ¸ Ğ²Ñ‹ÑĞ¾Ñ‚Ñƒ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ°, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ½Ğ° ÑĞºÑ€Ğ°Ğ½Ğ°Ñ… Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Enhancing GUI Grounding with Explicit Spatial Markers",
                    "desc": "This paper focuses on improving GUI grounding, which is the process of translating natural language commands into specific pixel locations on a screen. The authors identify that existing vision-language models (VLMs) struggle with high-resolution displays due to their reliance on implicit mappings from visual features to pixel coordinates. To overcome this, they introduce RULER tokens as explicit coordinate markers, allowing the model to reference positions more accurately. Additionally, they propose Interleaved MRoPE (I-MRoPE) to enhance spatial encoding, ensuring that both width and height are treated equally, leading to significant improvements in grounding accuracy across various resolutions."
                },
                "zh": {
                    "title": "æå‡GUIå®šä½å‡†ç¡®æ€§çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰å®šä½çš„æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨é«˜åˆ†è¾¨ç‡æ˜¾ç¤ºå™¨ä¸Šçš„å‡†ç¡®æ€§é—®é¢˜ã€‚å½“å‰çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å°†è‡ªç„¶è¯­è¨€æŒ‡ä»¤æ˜ å°„åˆ°åƒç´ åæ ‡æ—¶ï¼Œé¢ä¸´ç€å¯é çš„è¡¥ä¸åˆ°åƒç´ æ˜ å°„çš„ç“¶é¢ˆã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½œè€…æå‡ºäº†ä¸¤ç§åˆ›æ–°æ–¹æ³•ï¼šä½¿ç”¨RULERæ ‡è®°ä½œä¸ºæ˜ç¡®çš„åæ ‡æ ‡è®°ï¼Œä»¥åŠæ”¹è¿›ç©ºé—´ç¼–ç çš„äº¤é”™MRoPEï¼ˆI-MRoPEï¼‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™äº›æ–¹æ³•åœ¨ä¸åŒåˆ†è¾¨ç‡å’Œå¹³å°ä¸Šæ˜¾è‘—æé«˜äº†GUIå®šä½çš„å‡†ç¡®æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.03120",
            "title": "SurveyBench: How Well Can LLM(-Agents) Write Academic Surveys?",
            "url": "https://huggingface.co/papers/2510.03120",
            "abstract": "A new evaluation framework, SurveyBench, assesses the quality of automatically generated academic surveys using a quiz-driven approach, revealing deficiencies in current LLM4Survey methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Academic survey writing, which distills vast literature into a coherent and insightful narrative, remains a labor-intensive and intellectually demanding task. While recent approaches, such as general DeepResearch agents and survey-specialized methods, can generate surveys automatically (a.k.a. LLM4Survey), their outputs often fall short of human standards and there lacks a rigorous, reader-aligned benchmark for thoroughly revealing their deficiencies. To fill the gap, we propose a fine-grained, quiz-driven evaluation framework SurveyBench, featuring (1) typical survey topics source from recent 11,343 arXiv papers and corresponding 4,947 high-quality surveys; (2) a multifaceted metric hierarchy that assesses the outline quality (e.g., coverage breadth, logical coherence), content quality (e.g., synthesis granularity, clarity of insights), and non-textual richness; and (3) a dual-mode evaluation protocol that includes content-based and quiz-based answerability tests, explicitly aligned with readers' informational needs. Results show SurveyBench effectively challenges existing LLM4Survey approaches (e.g., on average 21% lower than human in content-based evaluation).",
            "score": 1,
            "issue_id": 6252,
            "pub_date": "2025-10-03",
            "pub_date_card": {
                "ru": "3 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 3",
                "zh": "10æœˆ3æ—¥"
            },
            "hash": "9114023adb7490f9",
            "authors": [
                "Zhaojun Sun",
                "Xuzhou Zhu",
                "Xuanhe Zhou",
                "Xin Tong",
                "Shuo Wang",
                "Jie Fu",
                "Guoliang Li",
                "Zhiyuan Liu",
                "Fan Wu"
            ],
            "affiliations": [
                "Shanghai AI Laboratory",
                "Shanghai Jiao Tong University",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.03120.jpg",
            "data": {
                "categories": [
                    "#survey",
                    "#benchmark"
                ],
                "emoji": "ğŸ“Š",
                "ru": {
                    "title": "SurveyBench: Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ AI-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ¸ĞºÑ‚Ğ¾Ñ€Ğ¸Ğ½Ñ‹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ SurveyBench â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ LLM. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ quiz-driven Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğ°, ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ¸ Ğ½Ğ°Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ½ĞµÑ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ 11,343 ÑÑ‚Ğ°Ñ‚ĞµĞ¹ Ñ arXiv Ğ¸ 4,947 Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ LLM4Survey Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² ÑÑ€ĞµĞ´Ğ½ĞµĞ¼ Ğ½Ğ° 21% Ñ…ÑƒĞ¶Ğµ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ĞµĞ¹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¼. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´Ğ²ÑƒÑ…Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğ¾Ğ² Ğ¾Ñ‚Ğ²ĞµÑ‡Ğ°Ñ‚ÑŒ Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ñ‡Ğ¸Ñ‚Ğ°Ñ‚ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "SurveyBench: Elevating AI-Generated Academic Surveys",
                    "desc": "The paper introduces SurveyBench, a new evaluation framework designed to assess the quality of automatically generated academic surveys. It highlights the limitations of current LLM4Survey methods, which often fail to meet human standards in survey writing. SurveyBench utilizes a quiz-driven approach and a comprehensive metric hierarchy to evaluate both outline and content quality, ensuring alignment with reader needs. The results demonstrate that existing methods significantly underperform compared to human-generated surveys, with an average score 21% lower in content-based evaluations."
                },
                "zh": {
                    "title": "SurveyBenchï¼šæå‡è‡ªåŠ¨ç”Ÿæˆå­¦æœ¯è°ƒæŸ¥çš„è¯„ä¼°æ ‡å‡†",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„è¯„ä¼°æ¡†æ¶SurveyBenchï¼Œç”¨äºè¯„ä¼°è‡ªåŠ¨ç”Ÿæˆçš„å­¦æœ¯è°ƒæŸ¥çš„è´¨é‡ã€‚è¯¥æ¡†æ¶é‡‡ç”¨åŸºäºæµ‹éªŒçš„æ–¹æ³•ï¼Œæ­ç¤ºäº†å½“å‰LLM4Surveyæ–¹æ³•çš„ä¸è¶³ä¹‹å¤„ã€‚SurveyBenché€šè¿‡åˆ†ææ¥è‡ª11,343ç¯‡arXivè®ºæ–‡çš„å…¸å‹è°ƒæŸ¥ä¸»é¢˜å’Œ4,947ç¯‡é«˜è´¨é‡è°ƒæŸ¥ï¼Œå»ºç«‹äº†å¤šå±‚æ¬¡çš„è¯„ä¼°æŒ‡æ ‡ä½“ç³»ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒSurveyBenchåœ¨å†…å®¹è¯„ä¼°ä¸­å¹³å‡æ¯”äººç±»ä½21%ï¼Œæœ‰æ•ˆæŒ‘æˆ˜äº†ç°æœ‰çš„LLM4Surveyæ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.02665",
            "title": "Self-Improvement in Multimodal Large Language Models: A Survey",
            "url": "https://huggingface.co/papers/2510.02665",
            "abstract": "A survey of self-improvement methods in Multimodal Large Language Models (MLLMs) from data collection, organization, and model optimization perspectives.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in self-improvement for Large Language Models (LLMs) have efficiently enhanced model capabilities without significantly increasing costs, particularly in terms of human effort. While this area is still relatively young, its extension to the multimodal domain holds immense potential for leveraging diverse data sources and developing more general self-improving models. This survey is the first to provide a comprehensive overview of self-improvement in Multimodal LLMs (MLLMs). We provide a structured overview of the current literature and discuss methods from three perspectives: 1) data collection, 2) data organization, and 3) model optimization, to facilitate the further development of self-improvement in MLLMs. We also include commonly used evaluations and downstream applications. Finally, we conclude by outlining open challenges and future research directions.",
            "score": 1,
            "issue_id": 6252,
            "pub_date": "2025-10-03",
            "pub_date_card": {
                "ru": "3 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 3",
                "zh": "10æœˆ3æ—¥"
            },
            "hash": "a7980db6477e39f7",
            "authors": [
                "Shijian Deng",
                "Kai Wang",
                "Tianyu Yang",
                "Harsh Singh",
                "Yapeng Tian"
            ],
            "affiliations": [
                "Mohamed bin Zayed University of Artificial Intelligence",
                "The University of Texas at Dallas",
                "University of Notre Dame",
                "University of Toronto"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.02665.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#survey",
                    "#optimization",
                    "#multimodal",
                    "#data",
                    "#dataset"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ LLM ÑƒÑ‡Ğ°Ñ‚ÑÑ ÑĞ°Ğ¼Ğ¸: Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² ÑĞ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² ÑĞ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… LLM. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ñ Ñ‚Ñ€Ñ‘Ñ… ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ñ‚Ğ¾Ñ‡ĞµĞº Ğ·Ñ€ĞµĞ½Ğ¸Ñ: ÑĞ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ¡Ğ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ‚ÑŒ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… ÑƒÑĞ¸Ğ»Ğ¸Ğ¹. Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ğ±ÑÑƒĞ¶Ğ´Ğ°ÑÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ² ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°ÑÑ‰ĞµĞ¹ÑÑ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Unlocking Potential: Self-Improvement in Multimodal Language Models",
                    "desc": "This paper surveys self-improvement methods in Multimodal Large Language Models (MLLMs), focusing on how to enhance model performance through better data handling and optimization techniques. It highlights the importance of efficiently collecting and organizing diverse data sources to improve model capabilities without incurring high costs. The authors provide a structured overview of existing literature and categorize methods into three main areas: data collection, data organization, and model optimization. Additionally, the paper discusses evaluation metrics and potential applications, while identifying challenges and future research opportunities in the field."
                },
                "zh": {
                    "title": "å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹çš„è‡ªæˆ‘æ”¹è¿›æ½œåŠ›",
                    "desc": "æœ¬è®ºæ–‡å¯¹å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸­çš„è‡ªæˆ‘æ”¹è¿›æ–¹æ³•è¿›è¡Œäº†å…¨é¢è°ƒæŸ¥ã€‚æˆ‘ä»¬ä»æ•°æ®æ”¶é›†ã€æ•°æ®ç»„ç»‡å’Œæ¨¡å‹ä¼˜åŒ–ä¸‰ä¸ªè§’åº¦ï¼Œç³»ç»Ÿæ€§åœ°å›é¡¾äº†å½“å‰æ–‡çŒ®ï¼Œæ¢è®¨äº†å¦‚ä½•æœ‰æ•ˆæå‡æ¨¡å‹èƒ½åŠ›ã€‚å°½ç®¡è¿™ä¸€é¢†åŸŸä»åœ¨å‘å±•ä¸­ï¼Œä½†å…¶åœ¨å¤šæ¨¡æ€é¢†åŸŸçš„æ‰©å±•å…·æœ‰å·¨å¤§çš„æ½œåŠ›ï¼Œå¯ä»¥åˆ©ç”¨å¤šæ ·çš„æ•°æ®æºã€‚æœ€åï¼Œæˆ‘ä»¬æ€»ç»“äº†å½“å‰é¢ä¸´çš„æŒ‘æˆ˜å’Œæœªæ¥çš„ç ”ç©¶æ–¹å‘ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.02571",
            "title": "How Confident are Video Models? Empowering Video Models to Express their\n  Uncertainty",
            "url": "https://huggingface.co/papers/2510.02571",
            "abstract": "A framework for uncertainty quantification in generative video models is introduced, including a metric for calibration, a black-box method called S-QUBED, and a benchmark dataset, demonstrating improved uncertainty estimates and task accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Generative video models demonstrate impressive text-to-video capabilities, spurring widespread adoption in many real-world applications. However, like large language models (LLMs), video generation models tend to hallucinate, producing plausible videos even when they are factually wrong. Although uncertainty quantification (UQ) of LLMs has been extensively studied in prior work, no UQ method for video models exists, raising critical safety concerns. To our knowledge, this paper represents the first work towards quantifying the uncertainty of video models. We present a framework for uncertainty quantification of generative video models, consisting of: (i) a metric for evaluating the calibration of video models based on robust rank correlation estimation with no stringent modeling assumptions; (ii) a black-box UQ method for video models (termed S-QUBED), which leverages latent modeling to rigorously decompose predictive uncertainty into its aleatoric and epistemic components; and (iii) a UQ dataset to facilitate benchmarking calibration in video models. By conditioning the generation task in the latent space, we disentangle uncertainty arising due to vague task specifications from that arising from lack of knowledge. Through extensive experiments on benchmark video datasets, we demonstrate that S-QUBED computes calibrated total uncertainty estimates that are negatively correlated with the task accuracy and effectively computes the aleatoric and epistemic constituents.",
            "score": 1,
            "issue_id": 6252,
            "pub_date": "2025-10-02",
            "pub_date_card": {
                "ru": "2 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 2",
                "zh": "10æœˆ2æ—¥"
            },
            "hash": "6e5849ca43586c8a",
            "authors": [
                "Zhiting Mei",
                "Ola Shorinwa",
                "Anirudha Majumdar"
            ],
            "affiliations": [
                "Princeton University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.02571.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#benchmark",
                    "#hallucinations",
                    "#dataset",
                    "#video"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "ĞšĞ¾Ğ³Ğ´Ğ° AI Ğ½Ğµ ÑƒĞ²ĞµÑ€ĞµĞ½ Ğ² ÑĞ²Ğ¾Ñ‘Ğ¼ Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ, ĞºĞ°Ğº Ğ¸ LLM, ÑĞºĞ»Ğ¾Ğ½Ğ½Ñ‹ Ğº Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸ÑĞ¼. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ S-QUBED, Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑÑÑ‰Ğ¸Ğ¹ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ°Ğ»ĞµĞ°Ñ‚Ğ¾Ñ€Ğ½ÑƒÑ (Ğ¸Ğ·-Ğ·Ğ° Ğ½ĞµÑÑĞ½Ñ‹Ñ… Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ¾Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸) Ğ¸ ÑĞ¿Ğ¸ÑÑ‚ĞµĞ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ (Ğ¸Ğ·-Ğ·Ğ° Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚ĞºĞ° Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸) ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹ Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ² Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ° ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€Ğ°Ğ½Ğ³Ğ¾Ğ²Ğ¾Ğ¹ ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ğ¸ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ benchmark-Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ°Ñ‘Ñ‚ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ĞºĞ¾Ñ€Ñ€ĞµĞ»Ğ¸Ñ€ÑƒÑÑ‚ Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡."
                },
                "en": {
                    "title": "Quantifying Uncertainty in Generative Video Models for Safer AI",
                    "desc": "This paper introduces a new framework for measuring uncertainty in generative video models, which is crucial for ensuring their reliability in real-world applications. It presents a novel metric for assessing how well these models predict uncertainty, along with a black-box method called S-QUBED that separates different types of uncertainty. The framework also includes a benchmark dataset to evaluate the performance of video models in terms of their uncertainty calibration. Through experiments, the authors show that S-QUBED provides accurate uncertainty estimates that correlate with the models' task performance, addressing safety concerns in video generation."
                },
                "zh": {
                    "title": "ç”Ÿæˆè§†é¢‘æ¨¡å‹çš„ä¸ç¡®å®šæ€§é‡åŒ–æ–°æ¡†æ¶",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºç”Ÿæˆè§†é¢‘æ¨¡å‹çš„ä¸ç¡®å®šæ€§é‡åŒ–æ¡†æ¶ï¼ŒåŒ…æ‹¬ä¸€ä¸ªç”¨äºæ ¡å‡†çš„åº¦é‡æ ‡å‡†ã€ä¸€ç§ç§°ä¸ºS-QUBEDçš„é»‘ç®±æ–¹æ³•ï¼Œä»¥åŠä¸€ä¸ªåŸºå‡†æ•°æ®é›†ã€‚ç”Ÿæˆè§†é¢‘æ¨¡å‹åœ¨æ–‡æœ¬åˆ°è§†é¢‘çš„èƒ½åŠ›ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†ä¹Ÿå­˜åœ¨å¹»è§‰ç°è±¡ï¼Œå³ç”Ÿæˆçš„å†…å®¹å¯èƒ½åœ¨äº‹å®ä¸Šä¸€æ— æ˜¯å¤„ã€‚å°½ç®¡å¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸ç¡®å®šæ€§é‡åŒ–å·²æœ‰å¤§é‡ç ”ç©¶ï¼Œä½†ç›®å‰å°šæ— é’ˆå¯¹è§†é¢‘æ¨¡å‹çš„ä¸ç¡®å®šæ€§é‡åŒ–æ–¹æ³•ï¼Œè¿™å¼•å‘äº†å®‰å…¨éšæ‚£ã€‚æˆ‘ä»¬çš„ç ”ç©¶é¦–æ¬¡é‡åŒ–äº†è§†é¢‘æ¨¡å‹çš„ä¸ç¡®å®šæ€§ï¼Œå¹¶é€šè¿‡å®éªŒéªŒè¯äº†S-QUBEDåœ¨æ ¡å‡†æ€»ä¸ç¡®å®šæ€§ä¼°è®¡æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.03204",
            "title": "FocusAgent: Simple Yet Effective Ways of Trimming the Large Context of\n  Web Agents",
            "url": "https://huggingface.co/papers/2510.03204",
            "abstract": "FocusAgent uses a lightweight LLM retriever to extract relevant content from web page observations, improving efficiency and security in web agents.  \t\t\t\t\tAI-generated summary \t\t\t\t Web agents powered by large language models (LLMs) must process lengthy web page observations to complete user goals; these pages often exceed tens of thousands of tokens. This saturates context limits and increases computational cost processing; moreover, processing full pages exposes agents to security risks such as prompt injection. Existing pruning strategies either discard relevant content or retain irrelevant context, leading to suboptimal action prediction. We introduce FocusAgent, a simple yet effective approach that leverages a lightweight LLM retriever to extract the most relevant lines from accessibility tree (AxTree) observations, guided by task goals. By pruning noisy and irrelevant content, FocusAgent enables efficient reasoning while reducing vulnerability to injection attacks. Experiments on WorkArena and WebArena benchmarks show that FocusAgent matches the performance of strong baselines, while reducing observation size by over 50%. Furthermore, a variant of FocusAgent significantly reduces the success rate of prompt-injection attacks, including banner and pop-up attacks, while maintaining task success performance in attack-free settings. Our results highlight that targeted LLM-based retrieval is a practical and robust strategy for building web agents that are efficient, effective, and secure.",
            "score": 0,
            "issue_id": 6252,
            "pub_date": "2025-10-03",
            "pub_date_card": {
                "ru": "3 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 3",
                "zh": "10æœˆ3æ—¥"
            },
            "hash": "cff617954ead65b4",
            "authors": [
                "Imene Kerboua",
                "Sahar Omidi Shayegan",
                "Megh Thakkar",
                "Xing Han LÃ¹",
                "LÃ©o Boisvert",
                "Massimo Caccia",
                "JÃ©rÃ©my Espinas",
                "Alexandre Aussem",
                "VÃ©ronique Eglin",
                "Alexandre Lacoste"
            ],
            "affiliations": [
                "Esker",
                "LIRIS - CNRS, INSA Lyon, Universite Claude Bernard Lyon 1",
                "McGill University",
                "Mila - Quebec AI Institute",
                "Polytechnique MontrÃ©al",
                "ServiceNow Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.03204.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#inference",
                    "#benchmark",
                    "#agents",
                    "#security",
                    "#reasoning"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "Ğ¤Ğ¾ĞºÑƒÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²ĞµĞ±-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸",
                    "desc": "FocusAgent â€” ÑÑ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ²ĞµĞ±-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LLM, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ»Ñ‘Ğ³ĞºĞ¸Ğ¹ retriever Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ¸Ğ· Ğ²ĞµĞ±-ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†. ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ²ĞµĞ±-ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ñ‡Ğ°ÑÑ‚Ğ¾ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‚ Ğ´ĞµÑÑÑ‚ĞºĞ¸ Ñ‚Ñ‹ÑÑÑ‡ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ Ğ½Ğ°Ğ³Ñ€ÑƒĞ·ĞºÑƒ Ğ½Ğ° ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ¸ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ prompt injection Ğ°Ñ‚Ğ°Ğº. FocusAgent Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€ÑƒĞµÑ‚ accessibility tree Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğ¹, Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğµ ÑÑ‚Ñ€Ğ¾ĞºĞ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ½Ğ¾ Ñ†ĞµĞ»Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ Ğ½Ğ° 50% Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… WorkArena Ğ¸ WebArena, Ğ½Ğ¾ Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ·Ğ°Ñ‰Ğ¸Ñ‰Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚ Ğ¸Ğ½ÑŠĞµĞºÑ†Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Efficient and Secure Web Agents with FocusAgent",
                    "desc": "FocusAgent is a novel approach that enhances the efficiency and security of web agents using a lightweight LLM retriever. It extracts the most relevant information from lengthy web page observations, which often contain excessive tokens that can overwhelm processing capabilities. By focusing on task-specific content and eliminating irrelevant data, FocusAgent minimizes computational costs and reduces the risk of security threats like prompt injection. Experimental results demonstrate that it not only maintains performance comparable to existing methods but also significantly decreases the amount of data processed, leading to safer and more effective web interactions."
                },
                "zh": {
                    "title": "FocusAgentï¼šé«˜æ•ˆå®‰å…¨çš„ç½‘é¡µä»£ç†è§£å†³æ–¹æ¡ˆ",
                    "desc": "FocusAgent æ˜¯ä¸€ç§è½»é‡çº§çš„ LLM æ£€ç´¢å™¨ï¼Œæ—¨åœ¨ä»ç½‘é¡µè§‚å¯Ÿä¸­æå–ç›¸å…³å†…å®¹ï¼Œä»è€Œæé«˜ç½‘ç»œä»£ç†çš„æ•ˆç‡å’Œå®‰å…¨æ€§ã€‚ä¼ ç»Ÿçš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†é•¿è¾¾æ•°ä¸‡æ ‡è®°çš„ç½‘é¡µæ—¶ï¼Œå®¹æ˜“å¯¼è‡´ä¸Šä¸‹æ–‡é™åˆ¶é¥±å’Œå’Œè®¡ç®—æˆæœ¬å¢åŠ ï¼ŒåŒæ—¶ä¹Ÿå¢åŠ äº†å®‰å…¨é£é™©ã€‚FocusAgent é€šè¿‡ä»å¯è®¿é—®æ€§æ ‘ï¼ˆAxTreeï¼‰è§‚å¯Ÿä¸­æå–æœ€ç›¸å…³çš„è¡Œï¼Œå‡å°‘äº†å™ªå£°å’Œæ— å…³å†…å®¹ï¼Œä½¿æ¨ç†è¿‡ç¨‹æ›´åŠ é«˜æ•ˆï¼Œå¹¶é™ä½äº†æ³¨å…¥æ”»å‡»çš„è„†å¼±æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFocusAgent åœ¨ä¿æŒä»»åŠ¡æˆåŠŸç‡çš„åŒæ—¶ï¼Œè§‚å¯Ÿå¤§å°å‡å°‘è¶…è¿‡ 50%ï¼Œå¹¶æ˜¾è‘—é™ä½äº†æç¤ºæ³¨å…¥æ”»å‡»çš„æˆåŠŸç‡ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-10-03.html",
    "link_next": "2025-10-07.html",
    "link_month": "2025-10.html",
    "short_date_prev": {
        "ru": "03.10",
        "en": "10/03",
        "zh": "10æœˆ3æ—¥"
    },
    "short_date_next": {
        "ru": "07.10",
        "en": "10/07",
        "zh": "10æœˆ7æ—¥"
    },
    "categories": {
        "#dataset": 2,
        "#data": 1,
        "#benchmark": 4,
        "#agents": 3,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 1,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 2,
        "#robotics": 1,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 1,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 4,
        "#survey": 2,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    }
}