{
    "date": {
        "ru": "12 ÑĞ½Ğ²Ğ°Ñ€Ñ",
        "en": "January 12",
        "zh": "1æœˆ12æ—¥"
    },
    "time_utc": "2026-01-12 05:31",
    "weekday": 0,
    "issue_id": 524,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2601.05432",
            "title": "Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization",
            "url": "https://huggingface.co/papers/2601.05432",
            "abstract": "Large vision-language models are enhanced for image geolocalization by incorporating map-based reasoning and agent-in-the-map loop optimization, achieving superior accuracy compared to existing models.  \t\t\t\t\tAI-generated summary \t\t\t\t The image geolocalization task aims to predict the location where an image was taken anywhere on Earth using visual clues. Existing large vision-language model (LVLM) approaches leverage world knowledge, chain-of-thought reasoning, and agentic capabilities, but overlook a common strategy used by humans -- using maps. In this work, we first equip the model Thinking with Map ability and formulate it as an agent-in-the-map loop. We develop a two-stage optimization scheme for it, including agentic reinforcement learning (RL) followed by parallel test-time scaling (TTS). The RL strengthens the agentic capability of model to improve sampling efficiency, and the parallel TTS enables the model to explore multiple candidate paths before making the final prediction, which is crucial for geolocalization. To evaluate our method on up-to-date and in-the-wild images, we further present MAPBench, a comprehensive geolocalization training and evaluation benchmark composed entirely of real-world images. Experimental results show that our method outperforms existing open- and closed-source models on most metrics, specifically improving Acc@500m from 8.0\\% to 22.1\\% compared to Gemini-3-Pro with Google Search/Map grounded mode.",
            "score": 32,
            "issue_id": 522,
            "pub_date": "2026-01-08",
            "pub_date_card": {
                "ru": "8 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 8",
                "zh": "1æœˆ8æ—¥"
            },
            "hash": "6130091a1c7cf83b",
            "authors": [
                "Yuxiang Ji",
                "Yong Wang",
                "Ziyu Ma",
                "Yiming Hu",
                "Hailang Huang",
                "Xuecai Hu",
                "Guanhua Chen",
                "Liaoni Wu",
                "Xiangxiang Chu"
            ],
            "affiliations": [
                "AMAP, Alibaba Group",
                "Southern University of Science and Technology",
                "Xiamen University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.05432.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#optimization",
                    "#agents",
                    "#cv",
                    "#dataset",
                    "#multimodal",
                    "#rl",
                    "#open_source",
                    "#reasoning"
                ],
                "emoji": "ğŸ—ºï¸",
                "ru": {
                    "title": "ĞšĞ°Ñ€Ñ‚Ñ‹ ĞºĞ°Ğº ĞºĞ¾Ğ¼Ğ¿Ğ°Ñ: Ğ½Ğ°ÑƒÑ‡Ğ¸Ğ¼ AI Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ ÑĞµĞ±Ñ Ğ½Ğ° Ğ¿Ğ»Ğ°Ğ½ĞµÑ‚Ğµ",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ³ĞµĞ¾Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ°Ñ€Ñ‚ Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾ÑĞ½Ğ°Ñ‰ĞµĞ½Ğ° ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ ĞºĞ°Ñ€Ñ‚Ğ°Ğ¼Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ñ†Ğ¸ĞºĞ» Â«Ğ°Ğ³ĞµĞ½Ñ‚-Ğ²-ĞºĞ°Ñ€Ñ‚ĞµÂ», Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ Ñ‚Ğ°Ğº Ğ¶Ğµ, ĞºĞ°Ğº Ğ»ÑĞ´Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¸ Ğ¼ĞµÑÑ‚Ğ¾Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ. Ğ”Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ°Ñ ÑÑ…ĞµĞ¼Ğ°: Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Ğ´Ğ»Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ³Ğ¸Ğ¿Ğ¾Ñ‚ĞµĞ·. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº MAPBench Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ğ´Ğ¾Ğ±Ğ¸Ğ»Ğ¸ÑÑŒ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Mapping the Future of Image Geolocalization",
                    "desc": "This paper presents an advanced approach to image geolocalization by integrating map-based reasoning into large vision-language models (LVLMs). The authors introduce a novel agent-in-the-map loop optimization that enhances the model's ability to utilize maps, a strategy often employed by humans. They implement a two-stage optimization process that includes reinforcement learning to boost the model's agentic capabilities and parallel test-time scaling for efficient path exploration. The proposed method significantly improves accuracy on real-world images, as demonstrated by their new benchmark, MAPBench, achieving notable performance gains over existing models."
                },
                "zh": {
                    "title": "åœ°å›¾æ€ç»´åŠ©åŠ›å›¾åƒåœ°ç†å®šä½",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§å¢å¼ºçš„å¤§å‹è§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œç”¨äºå›¾åƒåœ°ç†å®šä½ï¼Œé€šè¿‡å¼•å…¥åŸºäºåœ°å›¾çš„æ¨ç†å’Œåœ°å›¾ä¸­çš„ä»£ç†å¾ªç¯ä¼˜åŒ–ï¼Œæ˜¾è‘—æé«˜äº†å‡†ç¡®æ€§ã€‚å›¾åƒåœ°ç†å®šä½ä»»åŠ¡æ—¨åœ¨åˆ©ç”¨è§†è§‰çº¿ç´¢é¢„æµ‹å›¾åƒæ‹æ‘„åœ°ç‚¹ï¼Œç°æœ‰æ¨¡å‹é€šå¸¸å¿½è§†äº†äººç±»å¸¸ç”¨çš„åœ°å›¾ç­–ç•¥ã€‚æˆ‘ä»¬ä¸ºæ¨¡å‹èµ‹äºˆäº†åœ°å›¾æ€ç»´èƒ½åŠ›ï¼Œå¹¶å°†å…¶å½¢å¼åŒ–ä¸ºä»£ç†-åœ°å›¾å¾ªç¯ï¼Œé‡‡ç”¨äº†ä¸¤é˜¶æ®µä¼˜åŒ–æ–¹æ¡ˆï¼ŒåŒ…æ‹¬ä»£ç†å¼ºåŒ–å­¦ä¹ å’Œå¹¶è¡Œæµ‹è¯•æ—¶é—´ç¼©æ”¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤§å¤šæ•°æŒ‡æ ‡ä¸Šè¶…è¶Šäº†ç°æœ‰æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯åœ¨500ç±³å‡†ç¡®ç‡ä¸Šä»8.0%æå‡è‡³22.1%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.06021",
            "title": "Chaining the Evidence: Robust Reinforcement Learning for Deep Search Agents with Citation-Aware Rubric Rewards",
            "url": "https://huggingface.co/papers/2601.06021",
            "abstract": "A citation-aware reward framework and policy optimization method improve deep search agents' reasoning comprehensiveness and factual accuracy while reducing shortcut exploitation and hallucinations.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning (RL) has emerged as a critical technique for enhancing LLM-based deep search agents. However, existing approaches primarily rely on binary outcome rewards, which fail to capture the comprehensiveness and factuality of agents' reasoning process, and often lead to undesirable behaviors such as shortcut exploitation and hallucinations. To address these limitations, we propose Citation-aware Rubric Rewards (CaRR), a fine-grained reward framework for deep search agents that emphasizes reasoning comprehensiveness, factual grounding, and evidence connectivity. CaRR decomposes complex questions into verifiable single-hop rubrics and requires agents to satisfy these rubrics by explicitly identifying hidden entities, supporting them with correct citations, and constructing complete evidence chains that link to the predicted answer. We further introduce Citation-aware Group Relative Policy Optimization (C-GRPO), which combines CaRR and outcome rewards for training robust deep search agents. Experiments show that C-GRPO consistently outperforms standard outcome-based RL baselines across multiple deep search benchmarks. Our analysis also validates that C-GRPO effectively discourages shortcut exploitation, promotes comprehensive, evidence-grounded reasoning, and exhibits strong generalization to open-ended deep research tasks. Our code and data are available at https://github.com/THUDM/CaRR.",
            "score": 25,
            "issue_id": 522,
            "pub_date": "2026-01-09",
            "pub_date_card": {
                "ru": "9 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 9",
                "zh": "1æœˆ9æ—¥"
            },
            "hash": "2c4d9d165dc12ad2",
            "authors": [
                "Jiajie Zhang",
                "Xin Lv",
                "Ling Feng",
                "Lei Hou",
                "Juanzi Li"
            ],
            "affiliations": [
                "Tsinghua University",
                "Zhipu AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.06021.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#benchmark",
                    "#optimization",
                    "#hallucinations",
                    "#rl",
                    "#training",
                    "#open_source",
                    "#reasoning"
                ],
                "emoji": "ğŸ”—",
                "ru": {
                    "title": "ĞĞ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ñ Ğ¾ÑĞ¾Ğ·Ğ½Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ†Ğ¸Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ñ‡ĞµÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ°",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´ Citation-aware Rubric Rewards (CaRR), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ‚Ñ‹ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ² Ğ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ†Ğ¸Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ C-GRPO, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ CaRR Ñ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¾Ñ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¾Ğ¶Ğ¸Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ÑŒ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ² Ñ ÑĞ²Ğ½Ñ‹Ğ¼Ğ¸ ÑÑÑ‹Ğ»ĞºĞ°Ğ¼Ğ¸ Ğ½Ğ° Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑÑ€Ğ»Ñ‹ĞºĞ¾Ğ², Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸."
                },
                "en": {
                    "title": "Enhancing Deep Search Agents with Citation-Aware Rewards",
                    "desc": "This paper introduces a new reward framework called Citation-aware Rubric Rewards (CaRR) to enhance the reasoning abilities of deep search agents in reinforcement learning. Traditional methods use simple binary rewards, which can lead to poor reasoning and inaccuracies, such as shortcut exploitation and hallucinations. CaRR focuses on breaking down complex questions into simpler, verifiable components that require agents to provide evidence and citations for their answers. Additionally, the paper presents Citation-aware Group Relative Policy Optimization (C-GRPO), which integrates CaRR with outcome rewards to train more effective and reliable deep search agents."
                },
                "zh": {
                    "title": "æå‡æ·±åº¦æœç´¢ä»£ç†æ¨ç†çš„å…¨é¢æ€§ä¸å‡†ç¡®æ€§",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¥–åŠ±æ¡†æ¶ï¼Œç§°ä¸ºå¼•ç”¨æ„è¯†è¯„åˆ†å¥–åŠ±ï¼ˆCaRRï¼‰ï¼Œæ—¨åœ¨æé«˜æ·±åº¦æœç´¢ä»£ç†çš„æ¨ç†å…¨é¢æ€§å’Œäº‹å®å‡†ç¡®æ€§ã€‚ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ä¸»è¦ä¾èµ–äºäºŒå…ƒç»“æœå¥–åŠ±ï¼Œæ— æ³•æœ‰æ•ˆæ•æ‰ä»£ç†æ¨ç†è¿‡ç¨‹çš„å¤æ‚æ€§ï¼Œå®¹æ˜“å¯¼è‡´æ·å¾„åˆ©ç”¨å’Œå¹»è§‰ç°è±¡ã€‚CaRRé€šè¿‡å°†å¤æ‚é—®é¢˜åˆ†è§£ä¸ºå¯éªŒè¯çš„å•æ­¥è¯„åˆ†ï¼Œè¦æ±‚ä»£ç†æ˜ç¡®è¯†åˆ«éšè—å®ä½“ï¼Œå¹¶ç”¨æ­£ç¡®çš„å¼•ç”¨æ”¯æŒå®ƒä»¬ï¼Œä»è€Œæ„å»ºå®Œæ•´çš„è¯æ®é“¾ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†å¼•ç”¨æ„è¯†ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆC-GRPOï¼‰ï¼Œç»“åˆCaRRå’Œç»“æœå¥–åŠ±ï¼Œè®­ç»ƒå‡ºæ›´å¼ºå¤§çš„æ·±åº¦æœç´¢ä»£ç†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.06002",
            "title": "The Molecular Structure of Thought: Mapping the Topology of Long Chain-of-Thought Reasoning",
            "url": "https://huggingface.co/papers/2601.06002",
            "abstract": "Large language models struggle with long chain-of-thought reasoning due to unstable structural patterns, but a molecular-inspired approach using effective semantic isomers and distribution-transfer-graph methods improves training stability and performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) often fail to learn effective long chain-of-thought (Long CoT) reasoning from human or non-Long-CoT LLMs imitation. To understand this, we propose that effective and learnable Long CoT trajectories feature stable molecular-like structures in unified view, which are formed by three interaction types: Deep-Reasoning (covalent-like), Self-Reflection (hydrogen-bond-like), and Self-Exploration (van der Waals-like). Analysis of distilled trajectories reveals these structures emerge from Long CoT fine-tuning, not keyword imitation. We introduce Effective Semantic Isomers and show that only bonds promoting fast entropy convergence support stable Long CoT learning, while structural competition impairs training. Drawing on these findings, we present Mole-Syn, a distribution-transfer-graph method that guides synthesis of effective Long CoT structures, boosting performance and RL stability across benchmarks.",
            "score": 21,
            "issue_id": 522,
            "pub_date": "2026-01-09",
            "pub_date_card": {
                "ru": "9 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 9",
                "zh": "1æœˆ9æ—¥"
            },
            "hash": "3f57cbb3429682eb",
            "authors": [
                "Qiguang Chen",
                "Yantao Du",
                "Ziniu Li",
                "Jinhao Liu",
                "Songyao Duan",
                "Jiarui Guo",
                "Minghao Liu",
                "Jiaheng Liu",
                "Tong Yang",
                "Ge Zhang",
                "Libo Qin",
                "Wanxiang Che",
                "Wenhao Huang"
            ],
            "affiliations": [
                "2077AI Foundation",
                "ByteDance Seed China",
                "Central South University",
                "LARG, SCIR, Harbin Institute of Technology",
                "M-A-P",
                "Nanjing University",
                "Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.06002.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#reasoning",
                    "#optimization",
                    "#training"
                ],
                "emoji": "ğŸ§¬",
                "ru": {
                    "title": "ĞœĞ¾Ğ»ĞµĞºÑƒĞ»ÑÑ€Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² LLM",
                    "desc": "Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ°Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸Ğ·ÑƒÑÑ‚ÑÑ Ğ½ĞµÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ»ÑÑ€Ğ½Ğ¾-Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ ĞºĞ°Ğº ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ñ Ñ‚Ñ€ĞµĞ¼Ñ Ñ‚Ğ¸Ğ¿Ğ°Ğ¼Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹: Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ, ÑĞ°Ğ¼Ğ¾Ğ¾Ñ‚Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¸ ÑĞ°Ğ¼Ğ¾Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ. ĞĞ½Ğ¸ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ÑĞ²ÑĞ·Ğ¸, ÑĞ¿Ğ¾ÑĞ¾Ğ±ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ¹ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸, Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ÑÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ. ĞœĞµÑ‚Ğ¾Ğ´ Mole-Syn Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³Ñ€Ğ°Ñ„ Ñ‚Ñ€Ğ°Ğ½ÑÑ„ĞµÑ€Ñ‚Ğ° Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼."
                },
                "en": {
                    "title": "Molecular Structures Enhance Long Chain-of-Thought Reasoning in LLMs",
                    "desc": "This paper addresses the challenges that large language models (LLMs) face in performing long chain-of-thought (Long CoT) reasoning. It introduces a molecular-inspired framework that utilizes stable structural patterns, likening reasoning trajectories to molecular structures formed by different types of interactions. The authors propose Effective Semantic Isomers to enhance training stability, emphasizing that certain bonds are crucial for effective learning. Additionally, they present Mole-Syn, a method that improves the synthesis of Long CoT structures, leading to better performance and reinforcement learning stability."
                },
                "zh": {
                    "title": "åˆ†å­å¯å‘çš„é•¿é“¾æ¨ç†æå‡æ–¹æ³•",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é•¿é“¾æ¨ç†æ–¹é¢è¡¨ç°ä¸ä½³ï¼Œä¸»è¦æ˜¯å› ä¸ºå…¶ç»“æ„æ¨¡å¼ä¸ç¨³å®šã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ†å­å¯å‘çš„æ–¹æ³•ï¼Œé€šè¿‡æœ‰æ•ˆçš„è¯­ä¹‰å¼‚æ„ä½“å’Œåˆ†å¸ƒè½¬ç§»å›¾æ–¹æ³•æ¥æé«˜è®­ç»ƒçš„ç¨³å®šæ€§å’Œæ€§èƒ½ã€‚ç ”ç©¶è¡¨æ˜ï¼Œç¨³å®šçš„åˆ†å­ç»“æ„å¯ä»¥é€šè¿‡æ·±åº¦æ¨ç†ã€è‡ªæˆ‘åæ€å’Œè‡ªæˆ‘æ¢ç´¢ä¸‰ç§äº¤äº’ç±»å‹å½¢æˆï¼Œä»è€Œä¿ƒè¿›é•¿é“¾æ¨ç†çš„å­¦ä¹ ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬æå‡ºäº†Mole-Synæ–¹æ³•ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæŒ‡å¯¼é•¿é“¾æ¨ç†ç»“æ„çš„åˆæˆï¼Œæå‡æ¨¡å‹åœ¨å„é¡¹åŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°å’Œå¼ºåŒ–å­¦ä¹ çš„ç¨³å®šæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.05808",
            "title": "EnvScaler: Scaling Tool-Interactive Environments for LLM Agent via Programmatic Synthesis",
            "url": "https://huggingface.co/papers/2601.05808",
            "abstract": "EnvScaler automates the creation of tool-interaction environments through programmatic synthesis, enhancing LLM performance in complex multi-turn, multi-tool tasks via supervised fine-tuning and reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) are expected to be trained to act as agents in various real-world environments, but this process relies on rich and varied tool-interaction sandboxes. However, access to real systems is often restricted; LLM-simulated environments are prone to hallucinations and inconsistencies; and manually built sandboxes are hard to scale. In this paper, we propose EnvScaler, an automated framework for scalable tool-interaction environments via programmatic synthesis. EnvScaler comprises two components. First, SkelBuilder constructs diverse environment skeletons through topic mining, logic modeling, and quality evaluation. Then, ScenGenerator generates multiple task scenarios and rule-based trajectory validation functions for each environment. With EnvScaler, we synthesize 191 environments and about 7K scenarios, and apply them to Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) for Qwen3 series models. Results on three benchmarks show that EnvScaler significantly improves LLMs' ability to solve tasks in complex environments involving multi-turn, multi-tool interactions. We release our code and data at https://github.com/RUC-NLPIR/EnvScaler.",
            "score": 20,
            "issue_id": 522,
            "pub_date": "2026-01-09",
            "pub_date_card": {
                "ru": "9 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 9",
                "zh": "1æœˆ9æ—¥"
            },
            "hash": "7fb43c28ab2879e9",
            "authors": [
                "Xiaoshuai Song",
                "Haofei Chang",
                "Guanting Dong",
                "Yutao Zhu",
                "Zhicheng Dou",
                "Ji-Rong Wen"
            ],
            "affiliations": [
                "Gaoling School of Artificial Intelligence, Renmin University of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.05808.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#dataset",
                    "#agents",
                    "#training"
                ],
                "emoji": "ğŸ—ï¸",
                "ru": {
                    "title": "ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LLM",
                    "desc": "EnvScaler â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑÑ€ĞµĞ´ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ğ´Ğ²ÑƒÑ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ²: SkelBuilder Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ ÑĞºĞµĞ»ĞµÑ‚Ñ‹ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ°Ğ¹Ğ½Ğ¸Ğ½Ğ³ Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ğº Ğ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ğ° ScenGenerator ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ² Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹. Ğ¡Ğ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¸ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ÑÑ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ LLM Ñ‡ĞµÑ€ĞµĞ· supervised fine-tuning Ğ¸ reinforcement learning. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ LLM Ñ€ĞµÑˆĞ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½Ñ‡Ğ°Ñ‚Ñ‹Ğ¼ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ĞµĞ¼ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Automating Tool-Interaction Environments for Enhanced LLM Performance",
                    "desc": "EnvScaler is a framework designed to automate the creation of environments where large language models (LLMs) can interact with various tools. It uses programmatic synthesis to generate diverse and scalable tool-interaction environments, addressing the limitations of manual sandbox creation. The framework consists of two main components: SkelBuilder, which creates environment skeletons, and ScenGenerator, which produces task scenarios and validation functions. By applying EnvScaler, researchers have synthesized numerous environments and scenarios, leading to significant improvements in LLM performance on complex multi-turn tasks."
                },
                "zh": {
                    "title": "è‡ªåŠ¨åŒ–å·¥å…·äº¤äº’ç¯å¢ƒçš„åˆæˆä¸ä¼˜åŒ–",
                    "desc": "EnvScaler æ˜¯ä¸€ä¸ªè‡ªåŠ¨åŒ–æ¡†æ¶ï¼Œé€šè¿‡ç¨‹åºåˆæˆæ¥åˆ›å»ºå¯æ‰©å±•çš„å·¥å…·äº¤äº’ç¯å¢ƒï¼Œä»è€Œæå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤æ‚å¤šè½®ã€å¤šå·¥å…·ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚è¯¥æ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦ç»„ä»¶ï¼šSkelBuilder ç”¨äºé€šè¿‡ä¸»é¢˜æŒ–æ˜ã€é€»è¾‘å»ºæ¨¡å’Œè´¨é‡è¯„ä¼°æ„å»ºå¤šæ ·åŒ–çš„ç¯å¢ƒéª¨æ¶ï¼›ScenGenerator åˆ™ä¸ºæ¯ä¸ªç¯å¢ƒç”Ÿæˆå¤šä¸ªä»»åŠ¡åœºæ™¯å’ŒåŸºäºè§„åˆ™çš„è½¨è¿¹éªŒè¯å‡½æ•°ã€‚é€šè¿‡ EnvScalerï¼Œæˆ‘ä»¬åˆæˆäº† 191 ä¸ªç¯å¢ƒå’Œçº¦ 7000 ä¸ªåœºæ™¯ï¼Œå¹¶å°†å…¶åº”ç”¨äºç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ï¼Œæ˜¾è‘—æé«˜äº† LLM åœ¨å¤æ‚ç¯å¢ƒä¸­çš„ä»»åŠ¡è§£å†³èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®å·²åœ¨ https://github.com/RUC-NLPIR/EnvScaler å‘å¸ƒã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.05930",
            "title": "Can We Predict Before Executing Machine Learning Agents?",
            "url": "https://huggingface.co/papers/2601.05930",
            "abstract": "Autonomous machine learning agents overcome execution bottlenecks by predicting outcomes before physical execution, achieving faster convergence and improved performance through a predict-then-verify approach.  \t\t\t\t\tAI-generated summary \t\t\t\t Autonomous machine learning agents have revolutionized scientific discovery, yet they remain constrained by a Generate-Execute-Feedback paradigm. Previous approaches suffer from a severe Execution Bottleneck, as hypothesis evaluation relies strictly on expensive physical execution. To bypass these physical constraints, we internalize execution priors to substitute costly runtime checks with instantaneous predictive reasoning, drawing inspiration from World Models. In this work, we formalize the task of Data-centric Solution Preference and construct a comprehensive corpus of 18,438 pairwise comparisons. We demonstrate that LLMs exhibit significant predictive capabilities when primed with a Verified Data Analysis Report, achieving 61.5% accuracy and robust confidence calibration. Finally, we instantiate this framework in FOREAGENT, an agent that employs a Predict-then-Verify loop, achieving a 6x acceleration in convergence while surpassing execution-based baselines by +6%. Our code and dataset will be publicly available soon at https://github.com/zjunlp/predict-before-execute.",
            "score": 12,
            "issue_id": 522,
            "pub_date": "2026-01-09",
            "pub_date_card": {
                "ru": "9 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 9",
                "zh": "1æœˆ9æ—¥"
            },
            "hash": "f17fbcab3adc48b7",
            "authors": [
                "Jingsheng Zheng",
                "Jintian Zhang",
                "Yujie Luo",
                "Yuren Mao",
                "Yunjun Gao",
                "Lun Du",
                "Huajun Chen",
                "Ningyu Zhang"
            ],
            "affiliations": [
                "Ant Group",
                "Zhejiang University",
                "Zhejiang University - Ant Group Joint Laboratory of Knowledge Graph"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.05930.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#agents",
                    "#dataset",
                    "#open_source",
                    "#science"
                ],
                "emoji": "ğŸ”®",
                "ru": {
                    "title": "ĞŸÑ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ñ‚ÑŒ Ğ¿Ñ€ĞµĞ¶Ğ´Ğµ, Ñ‡ĞµĞ¼ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ÑŒ: ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ĞµĞµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ",
                    "desc": "ĞĞ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑÑ‚Ñ€Ğ°Ğ´Ğ°ÑÑ‚ Ğ¾Ñ‚ ÑƒĞ·ĞºĞ¾Ğ³Ğ¾ Ğ¼ĞµÑÑ‚Ğ° Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ğ³Ğ´Ğ° Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ³Ğ¸Ğ¿Ğ¾Ñ‚ĞµĞ· Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ´Ğ¾Ñ€Ğ¾Ğ³Ğ¾ÑÑ‚Ğ¾ÑÑ‰Ğ¸Ñ… Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ´Ğ¾ Ğ¸Ñ… Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¸Ñ€Ğ° Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ”Ğ»Ñ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¾Ğ½Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¸Ğ· 18,438 Ğ¿Ğ¾Ğ¿Ğ°Ñ€Ğ½Ñ‹Ñ… ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ LLM Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ 61.5% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚ FOREAGENT Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ†Ğ¸ĞºĞ» Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ-Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ 6-ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ½Ğ° 6% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Predict Before You Execute: Accelerating Machine Learning Agents",
                    "desc": "This paper introduces autonomous machine learning agents that enhance performance by predicting outcomes before actual execution, thus addressing the Execution Bottleneck in traditional methods. By using a Predict-then-Verify approach, these agents can achieve faster convergence and improved results without relying solely on costly physical execution. The authors formalize the concept of Data-centric Solution Preference and create a dataset of 18,438 comparisons to evaluate predictive capabilities. The proposed agent, FOREAGENT, demonstrates a significant acceleration in convergence and outperforms existing execution-based methods."
                },
                "zh": {
                    "title": "é¢„æµ‹å…ˆè¡Œï¼Œæ‰§è¡Œæ›´å¿«ï¼",
                    "desc": "è‡ªä¸»æœºå™¨å­¦ä¹ ä»£ç†é€šè¿‡åœ¨ç‰©ç†æ‰§è¡Œä¹‹å‰é¢„æµ‹ç»“æœï¼Œå…‹æœäº†æ‰§è¡Œç“¶é¢ˆï¼Œä»è€Œå®ç°æ›´å¿«çš„æ”¶æ•›å’Œæ›´å¥½çš„æ€§èƒ½ã€‚è¿™ç§æ–¹æ³•é‡‡ç”¨äº†é¢„æµ‹-éªŒè¯çš„ç­–ç•¥ï¼Œé¿å…äº†ä¾èµ–æ˜‚è´µçš„ç‰©ç†æ‰§è¡Œæ¥è¯„ä¼°å‡è®¾ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåŒ…å«18,438å¯¹æ¯”è¾ƒçš„ç»¼åˆè¯­æ–™åº“ï¼Œå¹¶è¯æ˜äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç»è¿‡éªŒè¯çš„æ•°æ®åˆ†ææŠ¥å‘Šçš„å¼•å¯¼ä¸‹ï¼Œå…·æœ‰æ˜¾è‘—çš„é¢„æµ‹èƒ½åŠ›ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬åœ¨FOREAGENTä¸­å®ç°äº†è¿™ä¸€æ¡†æ¶ï¼Œè¯¥ä»£ç†é€šè¿‡é¢„æµ‹-éªŒè¯å¾ªç¯å®ç°äº†6å€çš„æ”¶æ•›åŠ é€Ÿï¼Œå¹¶è¶…è¶Šäº†åŸºäºæ‰§è¡Œçš„åŸºçº¿ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.05905",
            "title": "Illusions of Confidence? Diagnosing LLM Truthfulness via Neighborhood Consistency",
            "url": "https://huggingface.co/papers/2601.05905",
            "abstract": "Large language models exhibit brittle beliefs under contextual perturbations, which are better measured by structural consistency metrics and addressed through structure-aware training methods.  \t\t\t\t\tAI-generated summary \t\t\t\t As Large Language Models (LLMs) are increasingly deployed in real-world settings, correctness alone is insufficient. Reliable deployment requires maintaining truthful beliefs under contextual perturbations. Existing evaluations largely rely on point-wise confidence like Self-Consistency, which can mask brittle belief. We show that even facts answered with perfect self-consistency can rapidly collapse under mild contextual interference. To address this gap, we propose Neighbor-Consistency Belief (NCB), a structural measure of belief robustness that evaluates response coherence across a conceptual neighborhood. To validate the efficiency of NCB, we introduce a new cognitive stress-testing protocol that probes outputs stability under contextual interference. Experiments across multiple LLMs show that the performance of high-NCB data is relatively more resistant to interference. Finally, we present Structure-Aware Training (SAT), which optimizes context-invariant belief structure and reduces long-tail knowledge brittleness by approximately 30%. Code will be available at https://github.com/zjunlp/belief.",
            "score": 9,
            "issue_id": 522,
            "pub_date": "2026-01-09",
            "pub_date_card": {
                "ru": "9 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 9",
                "zh": "1æœˆ9æ—¥"
            },
            "hash": "30111934330923b5",
            "authors": [
                "Haoming Xu",
                "Ningyuan Zhao",
                "Yunzhi Yao",
                "Weihong Xu",
                "Hongru Wang",
                "Xinle Deng",
                "Shumin Deng",
                "Jeff Z. Pan",
                "Huajun Chen",
                "Ningyu Zhang"
            ],
            "affiliations": [
                "NUS-NCS Joint Lab",
                "National University of Singapore",
                "University of Edinburgh",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.05905.jpg",
            "data": {
                "categories": [],
                "emoji": "ğŸ—ï¸",
                "ru": {
                    "title": "Ğ£ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ğ¾Ğ¹ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ñ…Ñ€ÑƒĞ¿ĞºĞ¸Ğµ ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ğ»Ğ¾Ñ…Ğ¾ Ğ¸Ğ·Ğ¼ĞµÑ€ÑÑÑ‚ÑÑ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼Ğ¸ Ğ²Ñ€Ğ¾Ğ´Ğµ Self-Consistency. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ Neighbor-Consistency Belief (NCB), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾ĞºÑ€ĞµÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ„Ğ°ĞºÑ‚Ğ¾Ğ². ĞĞ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ» ÑÑ‚Ñ€ĞµÑÑ-Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ´ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸ĞµĞ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ¼ĞµÑ…. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Structure-Aware Training (SAT) Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾-Ğ¸Ğ½Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ñ…Ñ€ÑƒĞ¿ĞºĞ¾ÑÑ‚ÑŒ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ½Ğ¾ 30%."
                },
                "en": {
                    "title": "Building Robust Beliefs in Language Models",
                    "desc": "This paper discusses the limitations of Large Language Models (LLMs) in maintaining accurate beliefs when faced with slight changes in context. It introduces a new metric called Neighbor-Consistency Belief (NCB) to measure the robustness of these beliefs by assessing how consistent responses are within a related conceptual framework. The authors demonstrate that traditional evaluation methods can overlook fragile beliefs that may collapse under minor contextual shifts. To improve the stability of LLMs, they propose Structure-Aware Training (SAT), which enhances the models' ability to maintain consistent beliefs, reducing knowledge brittleness significantly."
                },
                "zh": {
                    "title": "å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹çš„ä¿¡å¿µç¨³å®šæ€§",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é¢å¯¹ä¸Šä¸‹æ–‡å¹²æ‰°æ—¶è¡¨ç°å‡ºè„†å¼±çš„ä¿¡å¿µï¼Œè¿™ç§ç°è±¡å¯ä»¥é€šè¿‡ç»“æ„ä¸€è‡´æ€§æŒ‡æ ‡æ¥æ›´å¥½åœ°è¡¡é‡ã€‚ç°æœ‰çš„è¯„ä¼°æ–¹æ³•ä¸»è¦ä¾èµ–äºç‚¹å¯¹ç‚¹çš„è‡ªä¿¡åº¦ï¼Œå¯èƒ½æ©ç›–äº†ä¿¡å¿µçš„è„†å¼±æ€§ã€‚æˆ‘ä»¬æå‡ºäº†é‚»å±…ä¸€è‡´æ€§ä¿¡å¿µï¼ˆNCBï¼‰ï¼Œè¿™æ˜¯ä¸€ç§è¯„ä¼°å“åº”ä¸€è‡´æ€§çš„ç»“æ„æ€§æŒ‡æ ‡ï¼Œèƒ½å¤Ÿåœ¨æ¦‚å¿µé‚»åŸŸå†…è¿›è¡Œè¯„ä¼°ã€‚é€šè¿‡ç»“æ„æ„ŸçŸ¥è®­ç»ƒï¼ˆSATï¼‰ï¼Œæˆ‘ä»¬ä¼˜åŒ–äº†ä¸Šä¸‹æ–‡ä¸å˜çš„ä¿¡å¿µç»“æ„ï¼Œæ˜¾è‘—å‡å°‘äº†çŸ¥è¯†çš„è„†å¼±æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.05573",
            "title": "Orient Anything V2: Unifying Orientation and Rotation Understanding",
            "url": "https://huggingface.co/papers/2601.05573",
            "abstract": "Orient Anything V2 enhances 3D orientation understanding through scalable 3D asset synthesis, symmetry-aware periodic distribution fitting, and multi-frame relative rotation prediction, achieving state-of-the-art performance across multiple benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t This work presents Orient Anything V2, an enhanced foundation model for unified understanding of object 3D orientation and rotation from single or paired images. Building upon Orient Anything V1, which defines orientation via a single unique front face, V2 extends this capability to handle objects with diverse rotational symmetries and directly estimate relative rotations. These improvements are enabled by four key innovations: 1) Scalable 3D assets synthesized by generative models, ensuring broad category coverage and balanced data distribution; 2) An efficient, model-in-the-loop annotation system that robustly identifies 0 to N valid front faces for each object; 3) A symmetry-aware, periodic distribution fitting objective that captures all plausible front-facing orientations, effectively modeling object rotational symmetry; 4) A multi-frame architecture that directly predicts relative object rotations. Extensive experiments show that Orient Anything V2 achieves state-of-the-art zero-shot performance on orientation estimation, 6DoF pose estimation, and object symmetry recognition across 11 widely used benchmarks. The model demonstrates strong generalization, significantly broadening the applicability of orientation estimation in diverse downstream tasks.",
            "score": 6,
            "issue_id": 522,
            "pub_date": "2026-01-09",
            "pub_date_card": {
                "ru": "9 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 9",
                "zh": "1æœˆ9æ—¥"
            },
            "hash": "6d51bd0357e81bbe",
            "authors": [
                "Zehan Wang",
                "Ziang Zhang",
                "Jiayang Xu",
                "Jialei Wang",
                "Tianyu Pang",
                "Chao Du",
                "HengShuang Zhao",
                "Zhou Zhao"
            ],
            "affiliations": [
                "Sea AI Lab",
                "Shanghai AI Lab",
                "The University of Hong Kong",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.05573.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#cv",
                    "#dataset",
                    "#3d",
                    "#multimodal",
                    "#synthetic",
                    "#open_source"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ğ¾Ğ¹ Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ ÑƒÑ‡Ñ‘Ñ‚Ğ¾Ğ¼ ÑĞ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸",
                    "desc": "Orient Anything V2 â€” ÑÑ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ°Ñ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ğ¾Ğ¹ Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ 3D-Ğ°ĞºÑ‚Ğ¸Ğ²Ñ‹, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ¸ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ€Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ ÑĞ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ´Ğ»Ñ Ğ¿ĞµÑ€Ğ¸Ğ¾Ğ´Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ¾Ğ²Ğ¾Ñ€Ğ¾Ñ‚Ñ‹ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ĞºĞ°Ğ´Ñ€Ğ°Ñ… Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° 11 Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸, Ğ¿Ğ¾Ğ·Ñ‹ Ğ¸ ÑĞ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¸Ğ»ÑŒĞ½ÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Revolutionizing 3D Orientation Understanding with Orient Anything V2",
                    "desc": "Orient Anything V2 is a machine learning model designed to improve the understanding of 3D object orientation and rotation from images. It builds on its predecessor by allowing for the handling of objects with various rotational symmetries and estimating relative rotations directly. Key innovations include the use of generative models for creating diverse 3D assets, a robust annotation system for identifying valid orientations, and a multi-frame architecture for predicting rotations. The model has shown exceptional performance in orientation estimation and related tasks across multiple benchmarks, demonstrating its versatility in real-world applications."
                },
                "zh": {
                    "title": "å¢å¼ºä¸‰ç»´æ–¹å‘ç†è§£çš„é©å‘½æ€§æ¨¡å‹",
                    "desc": "Orient Anything V2 æ˜¯ä¸€ä¸ªå¢å¼ºçš„åŸºç¡€æ¨¡å‹ï¼Œæ—¨åœ¨ç»Ÿä¸€ç†è§£ç‰©ä½“çš„ä¸‰ç»´æ–¹å‘å’Œæ—‹è½¬ã€‚ä¸ä¹‹å‰çš„ç‰ˆæœ¬ç›¸æ¯”ï¼ŒV2 èƒ½å¤Ÿå¤„ç†å…·æœ‰ä¸åŒæ—‹è½¬å¯¹ç§°æ€§çš„ç‰©ä½“ï¼Œå¹¶ç›´æ¥ä¼°è®¡ç›¸å¯¹æ—‹è½¬ã€‚è¯¥æ¨¡å‹é€šè¿‡å››ä¸ªå…³é”®åˆ›æ–°å®ç°äº†è¿™äº›æ”¹è¿›ï¼ŒåŒ…æ‹¬å¯æ‰©å±•çš„ä¸‰ç»´èµ„äº§åˆæˆã€æœ‰æ•ˆçš„æ ‡æ³¨ç³»ç»Ÿã€å¯¹ç§°æ„ŸçŸ¥çš„åˆ†å¸ƒæ‹Ÿåˆç›®æ ‡ä»¥åŠå¤šå¸§æ¶æ„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOrient Anything V2 åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„é›¶æ ·æœ¬æ€§èƒ½ï¼Œæ˜¾è‘—æé«˜äº†æ–¹å‘ä¼°è®¡çš„é€‚ç”¨æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.04786",
            "title": "AgentOCR: Reimagining Agent History via Optical Self-Compression",
            "url": "https://huggingface.co/papers/2601.04786",
            "abstract": "AgentOCR reduces token consumption in agentic systems by representing interaction history as visual tokens and employing visual caching and self-compression techniques.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large language models (LLMs) enable agentic systems trained with reinforcement learning (RL) over multi-turn interaction trajectories, but practical deployment is bottlenecked by rapidly growing textual histories that inflate token budgets and memory usage. We introduce AgentOCR, a framework that exploits the superior information density of visual tokens by representing the accumulated observation-action history as a compact rendered image. To make multi-turn rollouts scalable, AgentOCR proposes segment optical caching. By decomposing history into hashable segments and maintaining a visual cache, this mechanism eliminates redundant re-rendering. Beyond fixed rendering, AgentOCR introduces agentic self-compression, where the agent actively emits a compression rate and is trained with compression-aware reward to adaptively balance task success and token efficiency. We conduct extensive experiments on challenging agentic benchmarks, ALFWorld and search-based QA. Remarkably, results demonstrate that AgentOCR preserves over 95\\% of text-based agent performance while substantially reducing token consumption (>50\\%), yielding consistent token and memory efficiency. Our further analysis validates a 20x rendering speedup from segment optical caching and the effective strategic balancing of self-compression.",
            "score": 5,
            "issue_id": 523,
            "pub_date": "2026-01-08",
            "pub_date_card": {
                "ru": "8 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 8",
                "zh": "1æœˆ8æ—¥"
            },
            "hash": "a8a1ae23f28f1d1a",
            "authors": [
                "Lang Feng",
                "Fuchao Yang",
                "Feng Chen",
                "Xin Cheng",
                "Haiyang Xu",
                "Zhenglin Wan",
                "Ming Yan",
                "Bo An"
            ],
            "affiliations": [
                "Nanyang Technological University, Singapore",
                "Tongyi Lab, Alibaba Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.04786.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#agents",
                    "#inference",
                    "#rl"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…",
                    "desc": "AgentOCR â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…, Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒÑ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ² ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ…. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ğ¾Ğ¿Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ğ¸ÑĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ Ğ¿ĞµÑ€ĞµÑÑ‡Ñ‘Ñ‚ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ñ…ĞµÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹. ĞĞ³ĞµĞ½Ñ‚ ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ ÑÑ‚ĞµĞ¿ĞµĞ½ÑŒ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğº ÑĞ¶Ğ°Ñ‚Ğ¸Ñ, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑƒÑĞ¿ĞµÑ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞĞ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… AgentOCR ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ 95% Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ Ğ½Ğ° 50%."
                },
                "en": {
                    "title": "Efficient Interaction with Visual Tokens in Agentic Systems",
                    "desc": "AgentOCR is a novel framework designed to enhance the efficiency of agentic systems by minimizing token usage during multi-turn interactions. It achieves this by converting interaction history into visual tokens, which are more information-dense than text. The framework employs segment optical caching to avoid redundant rendering of similar histories, significantly speeding up processing. Additionally, AgentOCR incorporates self-compression techniques that allow the agent to adaptively manage its performance and token efficiency, leading to substantial reductions in token consumption while maintaining high performance levels."
                },
                "zh": {
                    "title": "AgentOCRï¼šé«˜æ•ˆçš„è§†è§‰æ ‡è®°ä¸è‡ªæˆ‘å‹ç¼©æŠ€æœ¯",
                    "desc": "AgentOCR æ˜¯ä¸€ç§æ–°æ¡†æ¶ï¼Œé€šè¿‡å°†äº¤äº’å†å²è¡¨ç¤ºä¸ºè§†è§‰æ ‡è®°ï¼Œæ˜¾è‘—å‡å°‘äº†ä»£ç†ç³»ç»Ÿä¸­çš„ä»¤ç‰Œæ¶ˆè€—ã€‚å®ƒåˆ©ç”¨è§†è§‰ç¼“å­˜å’Œè‡ªæˆ‘å‹ç¼©æŠ€æœ¯ï¼Œä½¿å¾—å¤šè½®äº¤äº’çš„å¤„ç†æ›´åŠ é«˜æ•ˆã€‚é€šè¿‡å°†å†å²åˆ†è§£ä¸ºå¯å“ˆå¸Œçš„æ®µå¹¶ç»´æŠ¤è§†è§‰ç¼“å­˜ï¼ŒAgentOCR é¿å…äº†å†—ä½™çš„é‡æ–°æ¸²æŸ“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAgentOCR åœ¨ä¿æŒé«˜æ€§èƒ½çš„åŒæ—¶ï¼Œä»¤ç‰Œæ¶ˆè€—å‡å°‘è¶…è¿‡ 50%ï¼Œå¹¶å®ç°äº†æ˜¾è‘—çš„å†…å­˜æ•ˆç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.05848",
            "title": "Goal Force: Teaching Video Models To Accomplish Physics-Conditioned Goals",
            "url": "https://huggingface.co/papers/2601.05848",
            "abstract": "Video generation models trained on synthetic physics primitives demonstrate zero-shot generalization to complex real-world scenarios by modeling force propagation through time and space.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in video generation have enabled the development of ``world models'' capable of simulating potential futures for robotics and planning. However, specifying precise goals for these models remains a challenge; text instructions are often too abstract to capture physical nuances, while target images are frequently infeasible to specify for dynamic tasks. To address this, we introduce Goal Force, a novel framework that allows users to define goals via explicit force vectors and intermediate dynamics, mirroring how humans conceptualize physical tasks. We train a video generation model on a curated dataset of synthetic causal primitives-such as elastic collisions and falling dominos-teaching it to propagate forces through time and space. Despite being trained on simple physics data, our model exhibits remarkable zero-shot generalization to complex, real-world scenarios, including tool manipulation and multi-object causal chains. Our results suggest that by grounding video generation in fundamental physical interactions, models can emerge as implicit neural physics simulators, enabling precise, physics-aware planning without reliance on external engines. We release all datasets, code, model weights, and interactive video demos at our project page.",
            "score": 4,
            "issue_id": 522,
            "pub_date": "2026-01-09",
            "pub_date_card": {
                "ru": "9 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 9",
                "zh": "1æœˆ9æ—¥"
            },
            "hash": "fedc48a7e2d815ae",
            "authors": [
                "Nate Gillman",
                "Yinghua Zhou",
                "Zitian Tang",
                "Evan Luo",
                "Arjan Chakravarthy",
                "Daksh Aggarwal",
                "Michael Freeman",
                "Charles Herrmann",
                "Chen Sun"
            ],
            "affiliations": [
                "Brown University",
                "Cornell University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.05848.jpg",
            "data": {
                "categories": [
                    "#robotics",
                    "#dataset",
                    "#synthetic",
                    "#video",
                    "#training",
                    "#open_source"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "ĞÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ… Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ² Ñ„Ğ¸Ğ·Ğ¸ĞºĞ¸ Ğº Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ°",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Goal Force â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ñ‡ĞµÑ€ĞµĞ· ÑĞ²Ğ½Ñ‹Ğµ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ñ‹ ÑĞ¸Ğ» Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½ÑƒÑ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºÑƒ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼ Ğ·Ğ°Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ñ†ĞµĞ»Ğ¸ Ğ² Ñ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ°Ñ… Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°Ğ»Ğ°ÑÑŒ Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ğ²Ğ°Ñ… Ñ„Ğ¸Ğ·Ğ¸ĞºĞ¸ (ÑƒĞ¿Ñ€ÑƒĞ³Ğ¸Ğµ ÑÑ‚Ğ¾Ğ»ĞºĞ½Ğ¾Ğ²ĞµĞ½Ğ¸Ñ, Ğ¿Ğ°Ğ´Ğ°ÑÑ‰Ğ¸Ğµ ĞºĞ¾ÑÑ‚ÑÑˆĞºĞ¸ Ğ´Ğ¾Ğ¼Ğ¸Ğ½Ğ¾) Ğ´Ğ»Ñ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ ÑĞ¸Ğ» Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ. ĞĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ… Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…, Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ° Ğ¿Ğ¾Ñ€Ğ°Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ğ½ÑƒĞ»ĞµĞ²ÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ¸ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-ÑĞ»ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ·Ğ°Ğ·ĞµĞ¼Ğ»ĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°ĞºĞ¾Ğ½Ğ°Ñ… Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ ĞºĞ°Ğº Ğ½ĞµÑĞ²Ğ½Ñ‹Ğµ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ²Ñ‹Ğµ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ñ‹ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Empowering Video Generation with Physics-Based Goal Setting",
                    "desc": "This paper presents a new framework called Goal Force for video generation models that allows users to set goals using explicit force vectors. By training on simple physics scenarios, the model learns to simulate how forces propagate over time and space. Remarkably, it can apply this knowledge to complex real-world tasks without needing additional training, demonstrating zero-shot generalization. This approach positions the model as an implicit neural physics simulator, enhancing its ability to plan and execute dynamic tasks based on physical interactions."
                },
                "zh": {
                    "title": "é€šè¿‡åŠ›å‘é‡å®ç°ç›®æ ‡å®šä¹‰çš„åˆ›æ–°æ¡†æ¶",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºGoal Forceçš„æ–°æ¡†æ¶ï¼Œå…è®¸ç”¨æˆ·é€šè¿‡æ˜ç¡®çš„åŠ›å‘é‡å’Œä¸­é—´åŠ¨æ€æ¥å®šä¹‰ç›®æ ‡ã€‚è¿™ç§æ–¹æ³•æ¨¡ä»¿äº†äººç±»å¯¹ç‰©ç†ä»»åŠ¡çš„æ¦‚å¿µåŒ–ï¼Œè§£å†³äº†ä¼ ç»Ÿæ–‡æœ¬æŒ‡ä»¤å’Œç›®æ ‡å›¾åƒåœ¨åŠ¨æ€ä»»åŠ¡ä¸­éš¾ä»¥å…·ä½“åŒ–çš„é—®é¢˜ã€‚æˆ‘ä»¬è®­ç»ƒçš„æ¨¡å‹åœ¨ç®€å•çš„ç‰©ç†æ•°æ®ä¸Šè¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿåœ¨å¤æ‚çš„ç°å®åœºæ™¯ä¸­å®ç°é›¶-shotæ³›åŒ–ï¼ŒåŒ…æ‹¬å·¥å…·æ“ä½œå’Œå¤šç‰©ä½“å› æœé“¾ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œé€šè¿‡å°†è§†é¢‘ç”Ÿæˆä¸åŸºæœ¬ç‰©ç†äº¤äº’ç›¸ç»“åˆï¼Œæ¨¡å‹å¯ä»¥ä½œä¸ºéšå¼ç¥ç»ç‰©ç†æ¨¡æ‹Ÿå™¨ï¼Œå®ç°ç²¾ç¡®çš„ç‰©ç†æ„ŸçŸ¥è§„åˆ’ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.05966",
            "title": "VideoAR: Autoregressive Video Generation via Next-Frame & Scale Prediction",
            "url": "https://huggingface.co/papers/2601.05966",
            "abstract": "VideoAR presents a large-scale visual autoregressive framework for video generation that combines multi-scale next-frame prediction with autoregressive modeling, achieving state-of-the-art results with improved efficiency and temporal consistency.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in video generation have been dominated by diffusion and flow-matching models, which produce high-quality results but remain computationally intensive and difficult to scale. In this work, we introduce VideoAR, the first large-scale Visual Autoregressive (VAR) framework for video generation that combines multi-scale next-frame prediction with autoregressive modeling. VideoAR disentangles spatial and temporal dependencies by integrating intra-frame VAR modeling with causal next-frame prediction, supported by a 3D multi-scale tokenizer that efficiently encodes spatio-temporal dynamics. To improve long-term consistency, we propose Multi-scale Temporal RoPE, Cross-Frame Error Correction, and Random Frame Mask, which collectively mitigate error propagation and stabilize temporal coherence. Our multi-stage pretraining pipeline progressively aligns spatial and temporal learning across increasing resolutions and durations. Empirically, VideoAR achieves new state-of-the-art results among autoregressive models, improving FVD on UCF-101 from 99.5 to 88.6 while reducing inference steps by over 10x, and reaching a VBench score of 81.74-competitive with diffusion-based models an order of magnitude larger. These results demonstrate that VideoAR narrows the performance gap between autoregressive and diffusion paradigms, offering a scalable, efficient, and temporally consistent foundation for future video generation research.",
            "score": 3,
            "issue_id": 522,
            "pub_date": "2026-01-09",
            "pub_date_card": {
                "ru": "9 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 9",
                "zh": "1æœˆ9æ—¥"
            },
            "hash": "e8ed88c8c20040fb",
            "authors": [
                "Longbin Ji",
                "Xiaoxiong Liu",
                "Junyuan Shang",
                "Shuohuan Wang",
                "Yu Sun",
                "Hua Wu",
                "Haifeng Wang"
            ],
            "affiliations": [
                "Baidu"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.05966.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#benchmark",
                    "#video",
                    "#training"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "ĞĞ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ² ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "VideoAR Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¿ĞµÑ€Ğ²ÑƒÑ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½ÑƒÑ Ğ°Ğ²Ñ‚Ğ°Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ ĞºĞ°Ğ´Ñ€Ğ° Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ 3D Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ğ¸ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸. Ğ”Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Temporal RoPE Ğ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²ĞºÑƒ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ğ´Ñ€Ğ°Ğ¼Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑĞ½Ğ¸Ğ¶Ğ°ÑÑ‚ Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº. VideoAR Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ², ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ñ‹Ñ… Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ñ‚Ñ€ĞµĞ±ÑƒÑ Ğ² 10 Ñ€Ğ°Ğ· Ğ¼ĞµĞ½ÑŒÑˆĞµ ÑˆĞ°Ğ³Ğ¾Ğ² Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "VideoAR: Efficient and Consistent Video Generation with Autoregressive Modeling",
                    "desc": "VideoAR is a novel framework for generating videos using a visual autoregressive approach that enhances efficiency and temporal consistency. It integrates multi-scale next-frame prediction with autoregressive modeling to effectively manage spatial and temporal dependencies. The framework employs techniques like Multi-scale Temporal RoPE and Cross-Frame Error Correction to reduce errors and maintain coherence over time. VideoAR sets new benchmarks in video generation, outperforming previous autoregressive models while significantly cutting down on computational requirements."
                },
                "zh": {
                    "title": "VideoARï¼šé«˜æ•ˆä¸€è‡´çš„è§†é¢‘ç”Ÿæˆæ–°æ¡†æ¶",
                    "desc": "VideoARæ˜¯ä¸€ç§å¤§è§„æ¨¡çš„è§†è§‰è‡ªå›å½’æ¡†æ¶ï¼Œç”¨äºè§†é¢‘ç”Ÿæˆï¼Œç»“åˆäº†å¤šå°ºåº¦çš„ä¸‹ä¸€å¸§é¢„æµ‹å’Œè‡ªå›å½’å»ºæ¨¡ï¼Œå–å¾—äº†å…ˆè¿›çš„æ•ˆç‡å’Œæ—¶é—´ä¸€è‡´æ€§ã€‚è¯¥æ–¹æ³•é€šè¿‡å°†å¸§å†…è‡ªå›å½’å»ºæ¨¡ä¸å› æœä¸‹ä¸€å¸§é¢„æµ‹ç›¸ç»“åˆï¼Œè§£è€¦äº†ç©ºé—´å’Œæ—¶é—´ä¾èµ–æ€§ã€‚ä¸ºäº†æé«˜é•¿æœŸä¸€è‡´æ€§ï¼ŒVideoARæå‡ºäº†å¤šå°ºåº¦æ—¶é—´RoPEã€è·¨å¸§è¯¯å·®ä¿®æ­£å’Œéšæœºå¸§æ©ç ç­‰æŠ€æœ¯ï¼Œå‡å°‘äº†é”™è¯¯ä¼ æ’­å¹¶ç¨³å®šäº†æ—¶é—´è¿è´¯æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVideoARåœ¨è‡ªå›å½’æ¨¡å‹ä¸­è¾¾åˆ°äº†æ–°çš„æœ€å…ˆè¿›ç»“æœï¼Œæ˜¾è‘—æé«˜äº†æ€§èƒ½å¹¶å‡å°‘äº†æ¨ç†æ­¥éª¤ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.04888",
            "title": "SmartSearch: Process Reward-Guided Query Refinement for Search Agents",
            "url": "https://huggingface.co/papers/2601.04888",
            "abstract": "SmartSearch enhances LLM-based search agents through process rewards and query refinement mechanisms that improve intermediate search query quality via a three-stage curriculum learning approach.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language model (LLM)-based search agents have proven promising for addressing knowledge-intensive problems by incorporating information retrieval capabilities. Existing works largely focus on optimizing the reasoning paradigms of search agents, yet the quality of intermediate search queries during reasoning remains overlooked. As a result, the generated queries often remain inaccurate, leading to unexpected retrieval results and ultimately limiting search agents' overall effectiveness. To mitigate this issue, we introduce SmartSearch, a framework built upon two key mechanisms: (1) Process rewards, which provide fine-grained supervision for the quality of each intermediate search query through Dual-Level Credit Assessment. (2) Query refinement, which promotes the optimization of query generation by selectively refining low-quality search queries and regenerating subsequent search rounds based on these refinements. To enable the search agent to progressively internalize the ability to improve query quality under the guidance of process rewards, we design a three-stage curriculum learning framework. This framework guides the agent through a progression from imitation, to alignment, and ultimately to generalization. Experimental results show that SmartSearch consistently surpasses existing baselines, and additional quantitative analyses further confirm its significant gains in both search efficiency and query quality. The code is available at https://github.com/MYVAE/SmartSearch.",
            "score": 3,
            "issue_id": 522,
            "pub_date": "2026-01-08",
            "pub_date_card": {
                "ru": "8 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 8",
                "zh": "1æœˆ8æ—¥"
            },
            "hash": "1d9a399da8a29c5c",
            "authors": [
                "Tongyu Wen",
                "Guanting Dong",
                "Zhicheng Dou"
            ],
            "affiliations": [
                "Renmin University of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.04888.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#optimization",
                    "#open_source"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ½Ñ‹Ğµ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ¸ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²",
                    "desc": "SmartSearch â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ LLM-based Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ½Ñ‹Ğµ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ñ Ğ´Ğ²ÑƒÑ…ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¾Ğ¹ ĞºÑ€ĞµĞ´Ğ¸Ñ‚Ğ° Ğ´Ğ»Ñ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ° Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞœĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´ĞµĞ»Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğ¸Ğ·ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ¸ÑĞºĞ¸ Ğ¸ Ğ¿ĞµÑ€ĞµĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ğµ Ñ€Ğ°ÑƒĞ½Ğ´Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¹. Ğ¢Ñ€Ñ‘Ñ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ curriculum learning Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ¼Ğ¸Ñ‚Ğ°Ñ†Ğ¸Ñ, Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸ÑĞ¼ Ğ² ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²."
                },
                "en": {
                    "title": "Enhancing Search Quality with SmartSearch",
                    "desc": "SmartSearch is a framework designed to improve the performance of large language model (LLM)-based search agents by enhancing the quality of their intermediate search queries. It introduces two main mechanisms: process rewards, which provide detailed feedback on the quality of each query, and query refinement, which helps to improve low-quality queries through selective adjustments. The framework employs a three-stage curriculum learning approach that guides the search agent from basic imitation to more complex generalization of query improvement. Experimental results demonstrate that SmartSearch outperforms existing methods, leading to better search efficiency and higher quality queries."
                },
                "zh": {
                    "title": "SmartSearchï¼šæå‡æœç´¢ä»£ç†çš„æŸ¥è¯¢è´¨é‡",
                    "desc": "SmartSearch æ˜¯ä¸€ç§å¢å¼ºåŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœç´¢ä»£ç†çš„æ¡†æ¶ã€‚å®ƒé€šè¿‡è¿‡ç¨‹å¥–åŠ±å’ŒæŸ¥è¯¢ä¼˜åŒ–æœºåˆ¶ï¼Œæé«˜ä¸­é—´æœç´¢æŸ¥è¯¢çš„è´¨é‡ã€‚è¯¥æ¡†æ¶é‡‡ç”¨ä¸‰é˜¶æ®µçš„è¯¾ç¨‹å­¦ä¹ æ–¹æ³•ï¼Œå¸®åŠ©æœç´¢ä»£ç†é€æ­¥æŒæ¡æ”¹è¿›æŸ¥è¯¢è´¨é‡çš„èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSmartSearch åœ¨æœç´¢æ•ˆç‡å’ŒæŸ¥è¯¢è´¨é‡ä¸Šå‡ä¼˜äºç°æœ‰çš„åŸºçº¿æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.04720",
            "title": "Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking",
            "url": "https://huggingface.co/papers/2601.04720",
            "abstract": "The Qwen3-VL-Embedding and Qwen3-VL-Reranker models form an end-to-end multimodal search pipeline, leveraging multi-stage training and cross-attention mechanisms to achieve high-precision retrieval across diverse modalities.  \t\t\t\t\tAI-generated summary \t\t\t\t In this report, we introduce the Qwen3-VL-Embedding and Qwen3-VL-Reranker model series, the latest extensions of the Qwen family built on the Qwen3-VL foundation model. Together, they provide an end-to-end pipeline for high-precision multimodal search by mapping diverse modalities, including text, images, document images, and video, into a unified representation space. The Qwen3-VL-Embedding model employs a multi-stage training paradigm, progressing from large-scale contrastive pre-training to reranking model distillation, to generate semantically rich high-dimensional vectors. It supports Matryoshka Representation Learning, enabling flexible embedding dimensions, and handles inputs up to 32k tokens. Complementing this, Qwen3-VL-Reranker performs fine-grained relevance estimation for query-document pairs using a cross-encoder architecture with cross-attention mechanisms. Both model series inherit the multilingual capabilities of Qwen3-VL, supporting more than 30 languages, and are released in 2B and 8B parameter sizes to accommodate diverse deployment requirements. Empirical evaluations demonstrate that the Qwen3-VL-Embedding series achieves state-of-the-art results across diverse multimodal embedding evaluation benchmarks. Specifically, Qwen3-VL-Embedding-8B attains an overall score of 77.8 on MMEB-V2, ranking first among all models (as of January 8, 2025). This report presents the architecture, training methodology, and practical capabilities of the series, demonstrating their effectiveness on various multimodal retrieval tasks, including image-text retrieval, visual question answering, and video-text matching.",
            "score": 2,
            "issue_id": 524,
            "pub_date": "2026-01-08",
            "pub_date_card": {
                "ru": "8 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 8",
                "zh": "1æœˆ8æ—¥"
            },
            "hash": "f64f73630c899742",
            "authors": [
                "Mingxin Li",
                "Yanzhao Zhang",
                "Dingkun Long",
                "Keqin Chen",
                "Sibo Song",
                "Shuai Bai",
                "Zhibo Yang",
                "Pengjun Xie",
                "An Yang",
                "Dayiheng Liu",
                "Jingren Zhou",
                "Junyang Lin"
            ],
            "affiliations": [
                "Tongyi Lab, Alibaba Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.04720.jpg",
            "data": {
                "categories": [
                    "#multilingual",
                    "#benchmark",
                    "#rag",
                    "#open_source",
                    "#training",
                    "#architecture",
                    "#multimodal"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ•Ğ´Ğ¸Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ²Ğ¾ Ğ²ÑĞµÑ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑÑ…",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Qwen3-VL-Embedding Ğ¸ Qwen3-VL-Reranker, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ°, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑ Ñ‚ĞµĞºÑÑ‚, Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² ĞµĞ´Ğ¸Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Qwen3-VL-Embedding Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ¾Ğ¹ Ğ¸ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿ĞµÑ€ĞµÑ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ±Ğ¾Ğ³Ğ°Ñ‚Ñ‹Ñ… Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹. Qwen3-VL-Reranker Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ-Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ĞºÑ€Ğ¾ÑÑ-ĞºĞ¾Ğ´ĞµÑ€Ğ° Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ² ĞºÑ€Ğ¾ÑÑ-Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. ĞĞ±Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ÑÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ 30 ÑĞ·Ñ‹ĞºĞ¾Ğ², Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹ Ğ² Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°Ñ… 2B Ğ¸ 8B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ²."
                },
                "en": {
                    "title": "Revolutionizing Multimodal Search with Qwen3 Models",
                    "desc": "The Qwen3-VL-Embedding and Qwen3-VL-Reranker models create a powerful system for searching across different types of data, like text and images. They use advanced training techniques and cross-attention to improve how well they find relevant information. The embedding model generates detailed representations of data, while the reranker fine-tunes the results to ensure the best matches are highlighted. Together, they support multiple languages and have shown top performance in multimodal search tasks."
                },
                "zh": {
                    "title": "é«˜ç²¾åº¦å¤šæ¨¡æ€æœç´¢çš„æœªæ¥",
                    "desc": "Qwen3-VL-Embeddingå’ŒQwen3-VL-Rerankeræ¨¡å‹æ„æˆäº†ä¸€ä¸ªç«¯åˆ°ç«¯çš„å¤šæ¨¡æ€æœç´¢ç®¡é“ï¼Œåˆ©ç”¨å¤šé˜¶æ®µè®­ç»ƒå’Œäº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œå®ç°é«˜ç²¾åº¦çš„æ£€ç´¢ã€‚è¿™äº›æ¨¡å‹å°†æ–‡æœ¬ã€å›¾åƒã€æ–‡æ¡£å›¾åƒå’Œè§†é¢‘ç­‰å¤šç§æ¨¡æ€æ˜ å°„åˆ°ç»Ÿä¸€çš„è¡¨ç¤ºç©ºé—´ã€‚Qwen3-VL-Embeddingæ¨¡å‹é‡‡ç”¨å¤šé˜¶æ®µè®­ç»ƒï¼Œä»å¤§è§„æ¨¡å¯¹æ¯”é¢„è®­ç»ƒåˆ°é‡æ’åºæ¨¡å‹è’¸é¦ï¼Œç”Ÿæˆè¯­ä¹‰ä¸°å¯Œçš„é«˜ç»´å‘é‡ã€‚Qwen3-VL-Rerankeråˆ™ä½¿ç”¨äº¤å‰ç¼–ç å™¨æ¶æ„è¿›è¡Œç»†ç²’åº¦ç›¸å…³æ€§ä¼°è®¡ï¼Œæ”¯æŒå¤šè¯­è¨€ï¼Œé€‚åº”ä¸åŒçš„éƒ¨ç½²éœ€æ±‚ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.05637",
            "title": "GenCtrl -- A Formal Controllability Toolkit for Generative Models",
            "url": "https://huggingface.co/papers/2601.05637",
            "abstract": "Generative models' controllability is theoretically analyzed through a framework that estimates controllable sets with distribution-free bounds, revealing that controllability is fragile and context-dependent.  \t\t\t\t\tAI-generated summary \t\t\t\t As generative models become ubiquitous, there is a critical need for fine-grained control over the generation process. Yet, while controlled generation methods from prompting to fine-tuning proliferate, a fundamental question remains unanswered: are these models truly controllable in the first place? In this work, we provide a theoretical framework to formally answer this question. Framing human-model interaction as a control process, we propose a novel algorithm to estimate the controllable sets of models in a dialogue setting. Notably, we provide formal guarantees on the estimation error as a function of sample complexity: we derive probably-approximately correct bounds for controllable set estimates that are distribution-free, employ no assumptions except for output boundedness, and work for any black-box nonlinear control system (i.e., any generative model). We empirically demonstrate the theoretical framework on different tasks in controlling dialogue processes, for both language models and text-to-image generation. Our results show that model controllability is surprisingly fragile and highly dependent on the experimental setting. This highlights the need for rigorous controllability analysis, shifting the focus from simply attempting control to first understanding its fundamental limits.",
            "score": 1,
            "issue_id": 522,
            "pub_date": "2026-01-09",
            "pub_date_card": {
                "ru": "9 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 9",
                "zh": "1æœˆ9æ—¥"
            },
            "hash": "30ec0c4c1e6d1f97",
            "authors": [
                "Emily Cheng",
                "Carmen Amo Alonso",
                "Federico Danieli",
                "Arno Blaas",
                "Luca Zappella",
                "Pau Rodriguez",
                "Xavier Suau"
            ],
            "affiliations": [
                "Apple",
                "Stanford",
                "Universitat Pompeu Fabra"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.05637.jpg",
            "data": {
                "categories": [],
                "emoji": "ğŸ®",
                "ru": {
                    "title": "Ğ£Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: Ğ¾Ñ‚ Ğ¸Ğ»Ğ»ÑĞ·Ğ¸Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğº Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ĞµĞ³Ğ¾ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶Ğ¸Ğ¼Ñ‹Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ½Ğ¾-Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ñ‹Ñ… Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ† Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒÑÑ‚ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºĞ°Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ² ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…. Ğ“Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ Ğ´Ğ»Ñ Ğ»ÑĞ±Ñ‹Ñ… Ñ‡Ñ‘Ñ€Ğ½Ñ‹Ñ… ÑÑ‰Ğ¸ĞºĞ¾Ğ² Ğ½ĞµĞ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ĞºÑ€Ğ¾Ğ¼Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ…Ñ€ÑƒĞ¿ĞºĞ° Ğ¸ ÑĞ¸Ğ»ÑŒĞ½Ğ¾ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ñ‚ Ğ¾Ñ‚ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°, Ñ‡Ñ‚Ğ¾ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ° Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ."
                },
                "en": {
                    "title": "Understanding the Fragility of Generative Model Controllability",
                    "desc": "This paper analyzes how controllable generative models really are, using a new theoretical framework. It introduces an algorithm that estimates the sets of outputs that can be controlled in dialogue systems, providing guarantees on the accuracy of these estimates. The findings reveal that the ability to control these models is often fragile and varies greatly depending on the context. This emphasizes the importance of understanding the limits of model controllability before trying to exert control over them."
                },
                "zh": {
                    "title": "ç”Ÿæˆæ¨¡å‹çš„å¯æ§æ€§ï¼šè„†å¼±è€Œä¾èµ–ä¸Šä¸‹æ–‡",
                    "desc": "æœ¬æ–‡é€šè¿‡ä¸€ä¸ªç†è®ºæ¡†æ¶åˆ†æç”Ÿæˆæ¨¡å‹çš„å¯æ§æ€§ï¼Œä¼°è®¡å¯æ§é›†å¹¶æä¾›æ— åˆ†å¸ƒç•Œé™çš„ä¿è¯ã€‚ç ”ç©¶è¡¨æ˜ï¼Œç”Ÿæˆæ¨¡å‹çš„å¯æ§æ€§æ˜¯è„†å¼±çš„ï¼Œå¹¶ä¸”é«˜åº¦ä¾èµ–äºä¸Šä¸‹æ–‡ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°ç®—æ³•ï¼Œç”¨äºåœ¨å¯¹è¯è®¾ç½®ä¸­ä¼°è®¡æ¨¡å‹çš„å¯æ§é›†ï¼Œå¹¶åœ¨ä¸åŒä»»åŠ¡ä¸­è¿›è¡Œäº†å®è¯éªŒè¯ã€‚ç»“æœæ˜¾ç¤ºï¼Œæ¨¡å‹çš„å¯æ§æ€§å¹¶ä¸å¦‚é¢„æœŸï¼Œå¼ºè°ƒäº†å¯¹å¯æ§æ€§è¿›è¡Œä¸¥æ ¼åˆ†æçš„é‡è¦æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.05503",
            "title": "Over-Searching in Search-Augmented Large Language Models",
            "url": "https://huggingface.co/papers/2601.05503",
            "abstract": "Search-augmented large language models suffer from over-searching behavior that wastes computational resources and introduces hallucinations, with findings showing varied impacts across model types and conversation contexts.  \t\t\t\t\tAI-generated summary \t\t\t\t Search-augmented large language models (LLMs) excel at knowledge-intensive tasks by integrating external retrieval. However, they often over-search -- unnecessarily invoking search tool even when it does not improve response quality, which leads to computational inefficiency and hallucinations by incorporating irrelevant context. In this work, we conduct a systematic evaluation of over-searching across multiple dimensions, including query types, model categories, retrieval conditions, and multi-turn conversations. Our finding shows: (i) search generally improves answer accuracy on answerable queries but harms abstention on unanswerable ones; (ii) over-searching is more pronounced in complex reasoning models and deep research systems, is exacerbated by noisy retrieval, and compounds across turns in multi-turn conversations; and (iii) the composition of retrieved evidence is crucial, as the presence of negative evidence improves abstention. To quantify over-searching, we introduce Tokens Per Correctness (TPC), an evaluation metric that captures the performance-cost trade-off for search-augmented LLMs. Lastly, we investigate mitigation approaches at both the query and retrieval levels and release the OverSearchQA to foster continued research into efficient search-augmented LLMs.",
            "score": 1,
            "issue_id": 522,
            "pub_date": "2026-01-09",
            "pub_date_card": {
                "ru": "9 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 9",
                "zh": "1æœˆ9æ—¥"
            },
            "hash": "33cf12cd1c7618ab",
            "authors": [
                "Roy Xie",
                "Deepak Gopinath",
                "David Qiu",
                "Dong Lin",
                "Haitian Sun",
                "Saloni Potdar",
                "Bhuwan Dhingra"
            ],
            "affiliations": [
                "Apple",
                "Duke University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.05503.jpg",
            "data": {
                "categories": [
                    "#rag",
                    "#benchmark",
                    "#hallucinations",
                    "#optimization"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞšĞ¾Ğ³Ğ´Ğ° Ğ¿Ğ¾Ğ¸ÑĞº Ğ²Ñ€ĞµĞ´Ğ¸Ñ‚: ĞºĞ°Ğº Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ Ñ‡Ñ€ĞµĞ·Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ¾Ğ±Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¿Ğ¾Ğ¸ÑĞº-Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ½Ñ‹Ñ… LLM",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ñ‡Ñ€ĞµĞ·Ğ¼ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ¼, ĞºĞ¾Ğ³Ğ´Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ÑÑ Ğº Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğ¼ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ°Ğ¼ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ¿ÑƒÑÑ‚Ñ€Ğ°Ñ‚Ğ°Ğ¼ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ğ¸ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸ÑĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´ÑÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ½Ğ°Ğ´-Ğ¿Ğ¾Ğ¸ÑĞºĞ°, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞº ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ¾Ñ‚Ğ²ĞµÑ‡Ğ°ĞµĞ¼Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°Ñ…, Ğ½Ğ¾ Ğ²Ñ€ĞµĞ´Ğ¸Ñ‚ Ğ¾Ñ‚ĞºĞ°Ğ·Ñƒ Ğ¾Ñ‚ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° Ğ½Ğ° Ğ½ĞµĞ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹, Ğ° ÑÑ„Ñ„ĞµĞºÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹Ñ€Ğ°Ğ¶ĞµĞ½ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° ÑƒÑĞ¸Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¸ Ğ·Ğ°ÑˆÑƒĞ¼Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°Ñ… Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸ Ğ½Ğ°ĞºĞ°Ğ¿Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¾Ğ±Ğ¾Ñ€Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°Ñ…, Ğ½Ğ¾ Ğ½Ğ°Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾ Ğ¾Ñ‚ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ. Ğ”Ğ»Ñ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ° Tokens Per Correctness, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ñ‚Ñ€Ğ°Ğ¶Ğ°ĞµÑ‚ ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Optimizing Search for Smarter Language Models",
                    "desc": "This paper addresses the issue of over-searching in search-augmented large language models (LLMs), which can lead to wasted computational resources and hallucinations. The authors systematically evaluate how over-searching affects different model types and conversation contexts, revealing that while search can improve accuracy for answerable queries, it can negatively impact responses for unanswerable ones. They introduce a new metric, Tokens Per Correctness (TPC), to measure the efficiency of search-augmented LLMs and highlight the importance of the quality of retrieved evidence. The study also explores strategies to mitigate over-searching at both the query and retrieval levels, contributing to the development of more efficient LLMs."
                },
                "zh": {
                    "title": "ä¼˜åŒ–æœç´¢ï¼Œæå‡è¯­è¨€æ¨¡å‹æ•ˆç‡",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†å¢å¼ºæœç´¢çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä½¿ç”¨æœç´¢å·¥å…·æ—¶çš„è¿‡åº¦æœç´¢è¡Œä¸ºï¼Œè¿™ç§è¡Œä¸ºä¼šæµªè´¹è®¡ç®—èµ„æºå¹¶å¼•å…¥å¹»è§‰ã€‚ç ”ç©¶å‘ç°ï¼Œæœç´¢é€šå¸¸èƒ½æé«˜å¯å›ç­”é—®é¢˜çš„å‡†ç¡®æ€§ï¼Œä½†åœ¨æ— æ³•å›ç­”çš„é—®é¢˜ä¸Šå´ä¼šå¯¼è‡´é”™è¯¯çš„æ”¾å¼ƒã€‚è¿‡åº¦æœç´¢åœ¨å¤æ‚æ¨ç†æ¨¡å‹å’Œæ·±åº¦ç ”ç©¶ç³»ç»Ÿä¸­æ›´ä¸ºæ˜æ˜¾ï¼Œå¹¶ä¸”åœ¨å¤šè½®å¯¹è¯ä¸­ä¼šåŠ å‰§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„è¯„ä¼°æŒ‡æ ‡â€”â€”æ¯ä¸ªæ­£ç¡®ç­”æ¡ˆçš„ä»¤ç‰Œæ•°ï¼ˆTPCï¼‰ï¼Œå¹¶æ¢è®¨äº†åœ¨æŸ¥è¯¢å’Œæ£€ç´¢å±‚é¢ä¸Šçš„ç¼“è§£æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.04823",
            "title": "DR-LoRA: Dynamic Rank LoRA for Mixture-of-Experts Adaptation",
            "url": "https://huggingface.co/papers/2601.04823",
            "abstract": "DR-LoRA dynamically adjusts LoRA ranks for experts in Mixture-of-Experts models based on task-specific demands, improving parameter efficiency and performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Mixture-of-Experts (MoE) has become a prominent paradigm for scaling Large Language Models (LLMs). Parameter-efficient fine-tuning (PEFT), such as LoRA, is widely adopted to adapt pretrained MoE LLMs to downstream tasks. However, existing approaches assign identical LoRA ranks to all experts, overlooking the intrinsic functional specialization within MoE LLMs. This uniform allocation leads to resource mismatch, task-relevant experts are under-provisioned while less relevant ones receive redundant parameters. We propose a Dynamic Rank LoRA framework named DR-LoRA, which dynamically grows expert LoRA ranks during fine-tuning based on task-specific demands. DR-LoRA employs an Expert Saliency Scoring mechanism that integrates expert routing frequency and LoRA rank importance to quantify each expert's demand for additional capacity. Experts with higher saliency scores are prioritized for rank expansion, enabling the automatic formation of a heterogeneous rank distribution tailored to the target task. Experiments on multiple benchmarks demonstrate that DR-LoRA consistently outperforms standard LoRA and static allocation strategies under the same parameter budget, achieving superior task performance with more efficient parameter utilization.",
            "score": 1,
            "issue_id": 522,
            "pub_date": "2026-01-08",
            "pub_date_card": {
                "ru": "8 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 8",
                "zh": "1æœˆ8æ—¥"
            },
            "hash": "81ecc151e1820135",
            "authors": [
                "Guanzhi Deng",
                "Bo Li",
                "Ronghao Chen",
                "Huacan Wang",
                "Linqi Song",
                "Lijie Wen"
            ],
            "affiliations": [
                "City University of Hong Kong",
                "Peking University",
                "Tsinghua University",
                "University of Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.04823.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ğ°Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ²: Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€Ğ°Ğ½Ğ³Ğ¸ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ´ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ DR-LoRA Ğ´Ğ»Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ñ€Ğ°Ğ½Ğ³Ğ¾Ğ² Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ğ¾Ğ² LoRA Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Mixture-of-Experts Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ fine-tuning. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ñƒ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ½Ğ³Ğ° Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ° Ğ² Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµÑ‚ Ğ³ĞµÑ‚ĞµÑ€Ğ¾Ğ³ĞµĞ½Ğ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ½Ğ³Ğ¾Ğ², Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒÑÑÑŒ Ğº ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ, Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ€Ğ°Ğ²Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ²ÑĞµĞ¼ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ğ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ DR-LoRA Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸ Ñ‚Ğ¾Ğ¹ Ğ¶Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğ°, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ»ÑƒÑ‡ÑˆÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²."
                },
                "en": {
                    "title": "Dynamic Expert Tuning for Efficient MoE Performance",
                    "desc": "The paper introduces DR-LoRA, a method that optimizes the use of parameters in Mixture-of-Experts (MoE) models by dynamically adjusting the LoRA ranks of different experts based on the specific needs of a task. Traditional approaches assign the same LoRA rank to all experts, which can lead to inefficiencies as some experts may be over-allocated while others are under-utilized. DR-LoRA addresses this issue by using an Expert Saliency Scoring mechanism that evaluates how much additional capacity each expert requires based on their relevance to the task. The results show that DR-LoRA significantly improves performance and parameter efficiency compared to standard LoRA methods and static rank assignments."
                },
                "zh": {
                    "title": "åŠ¨æ€è°ƒæ•´ä¸“å®¶ç­‰çº§ï¼Œæå‡æ¨¡å‹æ€§èƒ½",
                    "desc": "DR-LoRAæ˜¯ä¸€ç§åŠ¨æ€è°ƒæ•´Mixture-of-Expertsæ¨¡å‹ä¸­LoRAç­‰çº§çš„æ–¹æ³•ï¼Œæ—¨åœ¨æ ¹æ®ç‰¹å®šä»»åŠ¡çš„éœ€æ±‚æé«˜å‚æ•°æ•ˆç‡å’Œæ€§èƒ½ã€‚ä¼ ç»Ÿæ–¹æ³•ä¸ºæ‰€æœ‰ä¸“å®¶åˆ†é…ç›¸åŒçš„LoRAç­‰çº§ï¼Œå¿½è§†äº†ä¸“å®¶ä¹‹é—´çš„åŠŸèƒ½ä¸“é—¨åŒ–ï¼Œå¯¼è‡´èµ„æºåˆ†é…ä¸å‡ã€‚DR-LoRAé€šè¿‡ä¸“å®¶æ˜¾è‘—æ€§è¯„åˆ†æœºåˆ¶ï¼ŒåŠ¨æ€è°ƒæ•´ä¸“å®¶çš„LoRAç­‰çº§ï¼Œä¼˜å…ˆæ‰©å±•éœ€æ±‚æ›´é«˜çš„ä¸“å®¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDR-LoRAåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºæ ‡å‡†LoRAå’Œé™æ€åˆ†é…ç­–ç•¥ï¼Œèƒ½å¤Ÿåœ¨ç›¸åŒå‚æ•°é¢„ç®—ä¸‹å®ç°æ›´é«˜çš„ä»»åŠ¡æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.05870",
            "title": "IIB-LPO: Latent Policy Optimization via Iterative Information Bottleneck",
            "url": "https://huggingface.co/papers/2601.05870",
            "abstract": "Latent Policy Optimization via Iterative Information Bottleneck addresses exploration collapse in LLM reasoning by enabling topological branching of reasoning trajectories through information bottleneck principles.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in Reinforcement Learning with Verifiable Rewards (RLVR) for Large Language Model (LLM) reasoning have been hindered by a persistent challenge: exploration collapse. The semantic homogeneity of random rollouts often traps models in narrow, over-optimized behaviors. While existing methods leverage policy entropy to encourage exploration, they face inherent limitations. Global entropy regularization is susceptible to reward hacking, which can induce meaningless verbosity, whereas local token-selective updates struggle with the strong inductive bias of pre-trained models. To address this, we propose Latent Policy Optimization via Iterative Information Bottleneck (IIB-LPO), a novel approach that shifts exploration from statistical perturbation of token distributions to topological branching of reasoning trajectories. IIB-LPO triggers latent branching at high-entropy states to diversify reasoning paths and employs the Information Bottleneck principle both as a trajectory filter and a self-reward mechanism, ensuring concise and informative exploration. Empirical results across four mathematical reasoning benchmarks demonstrate that IIB-LPO achieves state-of-the-art performance, surpassing prior methods by margins of up to 5.3% in accuracy and 7.4% in diversity metrics.",
            "score": 0,
            "issue_id": 522,
            "pub_date": "2026-01-09",
            "pub_date_card": {
                "ru": "9 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 9",
                "zh": "1æœˆ9æ—¥"
            },
            "hash": "d2103f98d8252f93",
            "authors": [
                "Huilin Deng",
                "Hongchen Luo",
                "Yue Zhu",
                "Long Li",
                "Zhuoyue Chen",
                "Xinghao Zhao",
                "Ming Li",
                "Jihai Zhang",
                "Mengchang Wang",
                "Yang Cao",
                "Yu Kang"
            ],
            "affiliations": [
                "DAMO Academy, Alibaba Group",
                "Northeastern University",
                "Shanghai Jiao Tong University",
                "University of Science and Technology of China",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.05870.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#math",
                    "#rl",
                    "#training",
                    "#reasoning"
                ],
                "emoji": "ğŸŒ¿",
                "ru": {
                    "title": "Ğ’ĞµÑ‚Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ ÑƒĞ·ĞºĞ¾Ğµ Ğ¼ĞµÑÑ‚Ğ¾",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ IIB-LPO Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ ĞºĞ¾Ğ»Ğ»Ğ°Ğ¿ÑĞ° Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°Ğ¼Ğ¸. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ² Ñ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸, Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ·ĞºĞ¾Ğ³Ğ¾ Ğ¼ĞµÑÑ‚Ğ° Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ¾Ğ¿Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²ĞµÑ‚Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ. Ğ˜Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ ÑƒĞ·ĞºĞ¾Ğµ Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ ĞºĞ°Ğº Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ÑĞ°Ğ¼Ğ¾Ğ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸Ğ¸, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ±Ğ°Ğ»Ğ°Ğ½Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºÑ€Ğ°Ñ‚ĞºĞ¾ÑÑ‚ÑŒÑ Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 5,3% Ğ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ½Ğ° 7,4% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Branching Out: Enhancing LLM Reasoning with IIB-LPO",
                    "desc": "This paper introduces Latent Policy Optimization via Iterative Information Bottleneck (IIB-LPO) to tackle the issue of exploration collapse in Large Language Model (LLM) reasoning. The authors highlight that traditional methods often lead to over-optimized behaviors due to semantic homogeneity in random rollouts. IIB-LPO innovatively shifts the focus from merely adjusting token distributions to creating diverse reasoning paths through topological branching at high-entropy states. The approach not only filters trajectories using the Information Bottleneck principle but also incorporates a self-reward mechanism, resulting in improved accuracy and diversity in reasoning tasks."
                },
                "zh": {
                    "title": "é€šè¿‡ä¿¡æ¯ç“¶é¢ˆå®ç°æ½œåœ¨ç­–ç•¥ä¼˜åŒ–ï¼Œæ‰“ç ´æ¢ç´¢å´©æºƒ",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œç§°ä¸ºè¿­ä»£ä¿¡æ¯ç“¶é¢ˆä¸‹çš„æ½œåœ¨ç­–ç•¥ä¼˜åŒ–ï¼ˆIIB-LPOï¼‰ï¼Œæ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†ä¸­çš„æ¢ç´¢å´©æºƒé—®é¢˜ã€‚é€šè¿‡ä¿¡æ¯ç“¶é¢ˆåŸç†ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨é«˜ç†µçŠ¶æ€ä¸‹è§¦å‘æ½œåœ¨åˆ†æ”¯ï¼Œä»è€Œå¤šæ ·åŒ–æ¨ç†è·¯å¾„ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒIIB-LPOä¸å†ä¾èµ–äºç»Ÿè®¡æ‰°åŠ¨ï¼Œè€Œæ˜¯é€šè¿‡æ‹“æ‰‘åˆ†æ”¯æ¥å¢å¼ºæ¢ç´¢èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒIIB-LPOåœ¨å››ä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå‡†ç¡®ç‡å’Œå¤šæ ·æ€§æŒ‡æ ‡å‡è¶…è¿‡äº†ä¹‹å‰çš„æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.05851",
            "title": "Router-Suggest: Dynamic Routing for Multimodal Auto-Completion in Visually-Grounded Dialogs",
            "url": "https://huggingface.co/papers/2601.05851",
            "abstract": "Multimodal auto-completion leverages visual and textual context to improve real-time prediction accuracy in conversational interfaces, with a router framework enabling efficient model selection based on dialog context.  \t\t\t\t\tAI-generated summary \t\t\t\t Real-time multimodal auto-completion is essential for digital assistants, chatbots, design tools, and healthcare consultations, where user inputs rely on shared visual context. We introduce Multimodal Auto-Completion (MAC), a task that predicts upcoming characters in live chats using partially typed text and visual cues. Unlike traditional text-only auto-completion (TAC), MAC grounds predictions in multimodal context to better capture user intent. To enable this task, we adapt MMDialog and ImageChat to create benchmark datasets. We evaluate leading vision-language models (VLMs) against strong textual baselines, highlighting trade-offs in accuracy and efficiency. We present Router-Suggest, a router framework that dynamically selects between textual models and VLMs based on dialog context, along with a lightweight variant for resource-constrained environments. Router-Suggest achieves a 2.3x to 10x speedup over the best-performing VLM. A user study shows that VLMs significantly excel over textual models on user satisfaction, notably saving user typing effort and improving the quality of completions in multi-turn conversations. These findings underscore the need for multimodal context in auto-completions, leading to smarter, user-aware assistants.",
            "score": 0,
            "issue_id": 522,
            "pub_date": "2026-01-09",
            "pub_date_card": {
                "ru": "9 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 9",
                "zh": "1æœˆ9æ—¥"
            },
            "hash": "7a9a52f4b2a51b55",
            "authors": [
                "Sandeep Mishra",
                "Devichand Budagam",
                "Anubhab Mandal",
                "Bishal Santra",
                "Pawan Goyal",
                "Manish Gupta"
            ],
            "affiliations": [
                "IIT Kharagpur",
                "Microsoft"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.05851.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#multimodal"
                ],
                "emoji": "ğŸ’¬",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ğ¾Ğµ Ğ°Ğ²Ñ‚Ğ¾Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ (MAC), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ğµ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ñ‹ Ğ² Ñ‡Ğ°Ñ‚Ğµ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ Ğ½Ğ°Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² MMDialog Ğ¸ ImageChat, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ñ… vision-language Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¸ Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ±Ğ°Ğ·ĞµĞ»Ğ°Ğ¹Ğ½Ğ°Ğ¼Ğ¸. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ²ĞºĞ»Ğ°Ğ´ â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Router-Suggest, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¸ VLM Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°, Ñ‡Ñ‚Ğ¾ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ² 2.3-10x Ñ€Ğ°Ğ·. ĞŸĞ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ğ¾ ÑƒĞ´Ğ¾Ğ²Ğ»ĞµÑ‚Ğ²Ğ¾Ñ€Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ¸ ÑĞ½Ğ¸Ğ¶Ğ°ÑÑ‚ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ¿ĞµÑ‡Ğ°Ñ‚ÑŒ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¾Ğ±Ğ¾Ñ€Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ°Ñ…."
                },
                "en": {
                    "title": "Enhancing Conversations with Multimodal Auto-Completion",
                    "desc": "This paper introduces Multimodal Auto-Completion (MAC), a method that enhances real-time predictions in conversational interfaces by utilizing both visual and textual information. Unlike traditional text-only auto-completion, MAC integrates visual cues to better understand user intent and improve prediction accuracy. The authors develop a router framework called Router-Suggest, which efficiently selects between vision-language models (VLMs) and textual models based on the context of the conversation. Their findings demonstrate that VLMs significantly outperform traditional models in user satisfaction and typing efficiency, highlighting the importance of multimodal context in creating smarter digital assistants."
                },
                "zh": {
                    "title": "å¤šæ¨¡æ€è‡ªåŠ¨è¡¥å…¨ï¼šæ™ºèƒ½åŠ©æ‰‹çš„æ–°æ–¹å‘",
                    "desc": "å¤šæ¨¡æ€è‡ªåŠ¨è¡¥å…¨åˆ©ç”¨è§†è§‰å’Œæ–‡æœ¬ä¸Šä¸‹æ–‡æ¥æé«˜å¯¹è¯ç•Œé¢çš„å®æ—¶é¢„æµ‹å‡†ç¡®æ€§ã€‚æˆ‘ä»¬æå‡ºäº†å¤šæ¨¡æ€è‡ªåŠ¨è¡¥å…¨ï¼ˆMACï¼‰ä»»åŠ¡ï¼Œé€šè¿‡éƒ¨åˆ†è¾“å…¥æ–‡æœ¬å’Œè§†è§‰çº¿ç´¢é¢„æµ‹å³å°†è¾“å…¥çš„å­—ç¬¦ã€‚ä¸ä¼ ç»Ÿçš„ä»…æ–‡æœ¬è‡ªåŠ¨è¡¥å…¨ï¼ˆTACï¼‰ä¸åŒï¼ŒMACåœ¨å¤šæ¨¡æ€ä¸Šä¸‹æ–‡ä¸­è¿›è¡Œé¢„æµ‹ï¼Œæ›´å¥½åœ°æ•æ‰ç”¨æˆ·æ„å›¾ã€‚æˆ‘ä»¬è¿˜ä»‹ç»äº†Router-Suggestæ¡†æ¶ï¼Œæ ¹æ®å¯¹è¯ä¸Šä¸‹æ–‡åŠ¨æ€é€‰æ‹©æ–‡æœ¬æ¨¡å‹å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼Œæ˜¾è‘—æé«˜äº†æ•ˆç‡å’Œç”¨æˆ·æ»¡æ„åº¦ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.02760",
            "title": "AnyDepth: Depth Estimation Made Easy",
            "url": "https://huggingface.co/papers/2601.02760",
            "abstract": "A lightweight monocular depth estimation framework uses DINOv3 as visual encoder and a compact transformer decoder to achieve higher accuracy with reduced computational overhead and improved data quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Monocular depth estimation aims to recover the depth information of 3D scenes from 2D images. Recent work has made significant progress, but its reliance on large-scale datasets and complex decoders has limited its efficiency and generalization ability. In this paper, we propose a lightweight and data-centric framework for zero-shot monocular depth estimation. We first adopt DINOv3 as the visual encoder to obtain high-quality dense features. Secondly, to address the inherent drawbacks of the complex structure of the DPT, we design the Simple Depth Transformer (SDT), a compact transformer-based decoder. Compared to the DPT, it uses a single-path feature fusion and upsampling process to reduce the computational overhead of cross-scale feature fusion, achieving higher accuracy while reducing the number of parameters by approximately 85%-89%. Furthermore, we propose a quality-based filtering strategy to filter out harmful samples, thereby reducing dataset size while improving overall training quality. Extensive experiments on five benchmarks demonstrate that our framework surpasses the DPT in accuracy. This work highlights the importance of balancing model design and data quality for achieving efficient and generalizable zero-shot depth estimation. Code: https://github.com/AIGeeksGroup/AnyDepth. Website: https://aigeeksgroup.github.io/AnyDepth.",
            "score": 0,
            "issue_id": 524,
            "pub_date": "2026-01-06",
            "pub_date_card": {
                "ru": "6 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 6",
                "zh": "1æœˆ6æ—¥"
            },
            "hash": "a6ad530d65a90d73",
            "authors": [
                "Zeyu Ren",
                "Zeyu Zhang",
                "Wukai Li",
                "Qingxiang Liu",
                "Hao Tang"
            ],
            "affiliations": [
                "Peking University",
                "Shanghai University of Engineering Science",
                "The University of Melbourne"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.02760.jpg",
            "data": {
                "categories": [
                    "#small_models",
                    "#3d",
                    "#training",
                    "#data",
                    "#architecture",
                    "#cv"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ñ‡ĞµÑ€ĞµĞ· Ğ±Ğ°Ğ»Ğ°Ğ½Ñ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ»Ñ‘Ğ³ĞºĞ¸Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ½Ğ¾ĞºÑƒĞ»ÑÑ€Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ DINOv3 Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ° Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Simple Depth Transformer (SDT) â€” ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ½Ñ‹Ğ¹ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ½Ğ° 85-89% Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ DPT, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ğº Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ĞµĞ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ¿ÑÑ‚Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ´Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ zero-shot Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹."
                },
                "en": {
                    "title": "Efficient Monocular Depth Estimation with DINOv3 and SDT",
                    "desc": "This paper presents a lightweight framework for monocular depth estimation that utilizes DINOv3 as a visual encoder and a compact transformer decoder called Simple Depth Transformer (SDT). The framework aims to improve accuracy while significantly reducing computational costs and the size of the dataset required for training. By employing a single-path feature fusion and upsampling process, the SDT minimizes the complexity of traditional decoders, achieving a reduction in parameters by 85%-89%. The authors also introduce a quality-based filtering strategy to enhance data quality, demonstrating that their approach outperforms existing methods in accuracy across multiple benchmarks."
                },
                "zh": {
                    "title": "è½»é‡çº§å•ç›®æ·±åº¦ä¼°è®¡çš„é«˜æ•ˆè§£å†³æ–¹æ¡ˆ",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§è½»é‡çº§çš„å•ç›®æ·±åº¦ä¼°è®¡æ¡†æ¶ï¼Œä½¿ç”¨DINOv3ä½œä¸ºè§†è§‰ç¼–ç å™¨å’Œç´§å‡‘çš„å˜æ¢è§£ç å™¨ï¼Œä»¥æé«˜å‡†ç¡®æ€§å¹¶å‡å°‘è®¡ç®—å¼€é”€ã€‚è¯¥æ¡†æ¶é€šè¿‡å•è·¯å¾„ç‰¹å¾èåˆå’Œä¸Šé‡‡æ ·è¿‡ç¨‹ï¼Œæ˜¾è‘—é™ä½äº†è·¨å°ºåº¦ç‰¹å¾èåˆçš„è®¡ç®—å¤æ‚åº¦ï¼ŒåŒæ—¶å‚æ•°æ•°é‡å‡å°‘çº¦85%-89%ã€‚æ­¤å¤–ï¼Œæå‡ºäº†ä¸€ç§åŸºäºè´¨é‡çš„è¿‡æ»¤ç­–ç•¥ï¼Œä»¥å»é™¤æœ‰å®³æ ·æœ¬ï¼Œä»è€Œå‡å°‘æ•°æ®é›†å¤§å°å¹¶æé«˜æ•´ä½“è®­ç»ƒè´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å‡†ç¡®æ€§ä¸Šè¶…è¶Šäº†ä¼ ç»Ÿçš„DPTæ¨¡å‹ï¼Œå¼ºè°ƒäº†æ¨¡å‹è®¾è®¡ä¸æ•°æ®è´¨é‡ä¹‹é—´çš„å¹³è¡¡å¯¹äºé«˜æ•ˆå’Œå¯æ³›åŒ–çš„é›¶-shotæ·±åº¦ä¼°è®¡çš„é‡è¦æ€§ã€‚"
                }
            }
        }
    ],
    "link_prev": "2026-01-09.html",
    "link_next": "2026-01-13.html",
    "link_month": "2026-01.html",
    "short_date_prev": {
        "ru": "09.01",
        "en": "01/09",
        "zh": "1æœˆ9æ—¥"
    },
    "short_date_next": {
        "ru": "13.01",
        "en": "01/13",
        "zh": "1æœˆ13æ—¥"
    },
    "categories": {
        "#dataset": 6,
        "#data": 1,
        "#benchmark": 8,
        "#agents": 4,
        "#cv": 4,
        "#rl": 5,
        "#rlhf": 1,
        "#rag": 2,
        "#plp": 0,
        "#inference": 1,
        "#3d": 2,
        "#audio": 0,
        "#video": 2,
        "#multimodal": 4,
        "#math": 1,
        "#multilingual": 1,
        "#architecture": 5,
        "#healthcare": 0,
        "#training": 9,
        "#robotics": 1,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 5,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 6,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 2,
        "#long_context": 0,
        "#synthetic": 2,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 7,
        "#small_models": 1,
        "#science": 1,
        "#low_resource": 0
    }
}