
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 40 papers. June 13.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">13 июня</span> | <span id="title-articles-count">40 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-06-12.html">⬅️ <span id="prev-date">12.06</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-06-16.html">➡️ <span id="next-date">16.06</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-06.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '13 июня', 'en': 'June 13', 'zh': '6月13日'};
        let feedDateNext = {'ru': '16.06', 'en': '06/16', 'zh': '6月16日'};
        let feedDatePrev = {'ru': '12.06', 'en': '06/12', 'zh': '6月12日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2506.09513', 'title': 'ReasonMed: A 370K Multi-Agent Generated Dataset for Advancing Medical\n  Reasoning', 'url': 'https://huggingface.co/papers/2506.09513', 'abstract': 'ReasonMed, a large medical reasoning dataset, enhances the accuracy of medical question answering models by combining detailed reasoning paths with concise summaries, setting new benchmarks for model performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Though reasoning-based large language models (LLMs) have excelled in mathematics and programming, their capabilities in knowledge-intensive medical question answering remain underexplored. To address this, we introduce ReasonMed, the largest medical reasoning dataset, comprising 370k high-quality examples distilled from 1.7 million initial reasoning paths generated by various LLMs. ReasonMed is constructed through a multi-agent verification and refinement process, where we design an Error Refiner to enhance the reasoning paths by identifying and correcting error-prone steps flagged by a verifier. Leveraging ReasonMed, we systematically investigate best practices for training medical reasoning models and find that combining detailed Chain-of-Thought (CoT) reasoning with concise answer summaries yields the most effective fine-tuning strategy. Based on this strategy, we train ReasonMed-7B, which sets a new benchmark for sub-10B models, outperforming the prior best by 4.17\\% and even exceeding LLaMA3.1-70B on PubMedQA by 4.60\\%.', 'score': 57, 'issue_id': 4272, 'pub_date': '2025-06-11', 'pub_date_card': {'ru': '11 июня', 'en': 'June 11', 'zh': '6月11日'}, 'hash': '7c6aa342a51b1d59', 'authors': ['Yu Sun', 'Xingyu Qian', 'Weiwen Xu', 'Hao Zhang', 'Chenghao Xiao', 'Long Li', 'Yu Rong', 'Wenbing Huang', 'Qifeng Bai', 'Tingyang Xu'], 'affiliations': ['Alibaba DAMO Academy', 'Beĳing Key Laboratory of Research on Large Models', 'Engineering Research Center of Next-Generation Intelligent Search and Recommendation', 'Gaoling School of', 'Hupan Lab', 'Renmin University of China', 'School of Basic Medical Sciences, Lanzhou University'], 'pdf_title_img': 'assets/pdf/title_img/2506.09513.jpg', 'data': {'categories': ['#dataset', '#optimization', '#benchmark', '#healthcare', '#reasoning', '#training'], 'emoji': '🩺', 'ru': {'title': 'ReasonMed: Прорыв в медицинских вопросно-ответных системах на основе ИИ', 'desc': 'ReasonMed - это крупнейший набор данных для медицинских рассуждений, состоящий из 370 тысяч высококачественных примеров. Он был создан с помощью многоагентного процесса верификации и уточнения, включающего специальный Error Refiner для улучшения цепочек рассуждений. Исследование показало, что комбинация подробных рассуждений по методу Chain-of-Thought с краткими сводками ответов является наиболее эффективной стратегией для обучения моделей медицинских рассуждений. На основе этой стратегии была обучена модель ReasonMed-7B, которая превзошла предыдущие модели на 4.17% и даже превзошла LLaMA3.1-70B на 4.60% в задаче PubMedQA.'}, 'en': {'title': 'Enhancing Medical AI with ReasonMed: A New Benchmark in Reasoning', 'desc': 'ReasonMed is a comprehensive medical reasoning dataset designed to improve the performance of medical question answering models. It consists of 370,000 high-quality examples derived from 1.7 million initial reasoning paths created by various large language models (LLMs). The dataset is refined through a multi-agent process that includes an Error Refiner to correct mistakes in reasoning paths. By combining detailed Chain-of-Thought reasoning with concise summaries, ReasonMed-7B achieves superior results, surpassing previous benchmarks for smaller models and even outperforming larger models on specific tasks.'}, 'zh': {'title': 'ReasonMed：提升医学问答模型的新基准', 'desc': 'ReasonMed是一个大型医学推理数据集，旨在提高医学问答模型的准确性。它结合了详细的推理路径和简洁的总结，创造了新的模型性能基准。该数据集包含370,000个高质量示例，经过多代理验证和精炼过程构建而成。通过结合详细的思维链推理和简洁的答案总结，ReasonMed-7B模型在医学问答任务中表现优异，超越了之前的最佳模型。'}}}, {'id': 'https://huggingface.co/papers/2506.10954', 'title': 'SWE-Factory: Your Automated Factory for Issue Resolution Training Data\n  and Evaluation Benchmarks', 'url': 'https://huggingface.co/papers/2506.10954', 'abstract': 'An automated pipeline, SWE-Factory, is introduced to facilitate the creation of large-scale datasets for evaluating and training Large Language Models in GitHub issue resolution tasks, offering efficient environment building, standardized grading, and automated validation.  \t\t\t\t\tAI-generated summary \t\t\t\t Constructing large-scale datasets for the GitHub issue resolution task is crucial for both training and evaluating the software engineering capabilities of Large Language Models (LLMs). However, the traditional process for creating such benchmarks is notoriously challenging and labor-intensive, particularly in the stages of setting up evaluation environments, grading test outcomes, and validating task instances. In this paper, we propose SWE-Factory, an automated pipeline designed to address these challenges. To tackle these issues, our pipeline integrates three core automated components. First, we introduce SWE-Builder, a multi-agent system that automates evaluation environment construction, which employs four specialized agents that work in a collaborative, iterative loop and leverages an environment memory pool to enhance efficiency. Second, we introduce a standardized, exit-code-based grading method that eliminates the need for manually writing custom parsers. Finally, we automate the fail2pass validation process using these reliable exit code signals. Experiments on 671 issues across four programming languages show that our pipeline can effectively construct valid task instances; for example, with GPT-4.1-mini, our SWE-Builder constructs 269 valid instances at 0.045 per instance, while with Gemini-2.5-flash, it achieves comparable performance at the lowest cost of 0.024 per instance. We also demonstrate that our exit-code-based grading achieves 100% accuracy compared to manual inspection, and our automated fail2pass validation reaches a precision of 0.92 and a recall of 1.00. We hope our automated pipeline will accelerate the collection of large-scale, high-quality GitHub issue resolution datasets for both training and evaluation. Our code and datasets are released at https://github.com/DeepSoftwareAnalytics/swe-factory.', 'score': 36, 'issue_id': 4272, 'pub_date': '2025-06-12', 'pub_date_card': {'ru': '12 июня', 'en': 'June 12', 'zh': '6月12日'}, 'hash': 'dfb4cf3e253468bd', 'authors': ['Lianghong Guo', 'Yanlin Wang', 'Caihua Li', 'Pengyu Yang', 'Jiachi Chen', 'Wei Tao', 'Yingtian Zou', 'Duyu Tang', 'Zibin Zheng'], 'affiliations': ['Huawei', 'Independent Researcher', 'Sun Yat-sen University'], 'pdf_title_img': 'assets/pdf/title_img/2506.10954.jpg', 'data': {'categories': ['#data', '#science', '#dataset', '#agents', '#benchmark', '#open_source'], 'emoji': '🏭', 'ru': {'title': 'SWE-Factory: автоматизация создания датасетов для LLM в разработке ПО', 'desc': 'SWE-Factory - это автоматизированный конвейер для создания масштабных датасетов для оценки и обучения больших языковых моделей (LLM) в задачах разрешения проблем на GitHub. Он включает в себя SWE-Builder - мультиагентную систему для автоматизированного построения сред оценки, стандартизированный метод оценки на основе кодов выхода и автоматизированную валидацию fail2pass. Эксперименты показали эффективность конвейера в создании валидных экземпляров задач, точность оценки и высокую производительность валидации. Этот подход призван ускорить сбор масштабных, качественных датасетов для обучения и оценки LLM в задачах разработки программного обеспечения.'}, 'en': {'title': 'Automating Dataset Creation for LLMs in GitHub Issue Resolution', 'desc': 'The paper presents SWE-Factory, an automated pipeline designed to streamline the creation of large-scale datasets for training and evaluating Large Language Models (LLMs) in GitHub issue resolution tasks. It addresses the challenges of environment setup, grading, and validation by integrating three automated components: SWE-Builder for environment construction, a standardized exit-code-based grading system, and an automated fail2pass validation process. Experiments demonstrate that SWE-Factory can efficiently generate valid task instances at a low cost while achieving high accuracy in grading and validation. This innovation aims to enhance the quality and speed of dataset collection for LLM training and evaluation.'}, 'zh': {'title': '自动化管道加速GitHub问题解决数据集构建', 'desc': '本文介绍了一种名为SWE-Factory的自动化管道，旨在简化大规模数据集的创建，以评估和训练大型语言模型在GitHub问题解决任务中的表现。传统的数据集构建过程繁琐且耗时，尤其是在环境搭建、结果评分和任务验证阶段。SWE-Factory通过集成三个核心自动化组件来解决这些问题，包括自动化环境构建的多代理系统SWE-Builder、基于退出代码的标准化评分方法，以及自动化的fail2pass验证过程。实验结果表明，该管道能够有效构建有效的任务实例，并在评分和验证方面表现出高准确率和高精度。'}}}, {'id': 'https://huggingface.co/papers/2506.09993', 'title': 'Text-Aware Image Restoration with Diffusion Models', 'url': 'https://huggingface.co/papers/2506.09993', 'abstract': 'The proposed Text-Aware Image Restoration (TAIR) system integrates a multi-task diffusion framework with a text-spotting module to enhance both image recovery and textual fidelity, outperforming existing diffusion-based methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Image restoration aims to recover degraded images. However, existing diffusion-based restoration methods, despite great success in natural image restoration, often struggle to faithfully reconstruct textual regions in degraded images. Those methods frequently generate plausible but incorrect text-like patterns, a phenomenon we refer to as text-image hallucination. In this paper, we introduce Text-Aware Image Restoration (TAIR), a novel restoration task that requires the simultaneous recovery of visual contents and textual fidelity. To tackle this task, we present SA-Text, a large-scale benchmark of 100K high-quality scene images densely annotated with diverse and complex text instances. Furthermore, we propose a multi-task diffusion framework, called TeReDiff, that integrates internal features from diffusion models into a text-spotting module, enabling both components to benefit from joint training. This allows for the extraction of rich text representations, which are utilized as prompts in subsequent denoising steps. Extensive experiments demonstrate that our approach consistently outperforms state-of-the-art restoration methods, achieving significant gains in text recognition accuracy. See our project page: https://cvlab-kaist.github.io/TAIR/', 'score': 32, 'issue_id': 4272, 'pub_date': '2025-06-11', 'pub_date_card': {'ru': '11 июня', 'en': 'June 11', 'zh': '6月11日'}, 'hash': '91f1bb97ef062632', 'authors': ['Jaewon Min', 'Jin Hyeon Kim', 'Paul Hyunbin Cho', 'Jaeeun Lee', 'Jihye Park', 'Minkyu Park', 'Sangpil Kim', 'Hyunhee Park', 'Seungryong Kim'], 'affiliations': ['KAIST AI', 'Korea University', 'Samsung Electronics', 'Yonsei University'], 'pdf_title_img': 'assets/pdf/title_img/2506.09993.jpg', 'data': {'categories': ['#dataset', '#cv', '#benchmark', '#diffusion', '#hallucinations'], 'emoji': '📝', 'ru': {'title': 'Восстановление изображений с сохранением текстовой информации', 'desc': 'Предложенная система восстановления изображений с учетом текста (TAIR) объединяет многозадачную диффузионную модель с модулем распознавания текста для улучшения как восстановления изображения, так и точности текста. Система превосходит существующие методы на основе диффузии, которые часто создают правдоподобные, но неверные текстоподобные паттерны. Авторы представили SA-Text - крупномасштабный набор данных из 100 тысяч высококачественных изображений с разнообразными текстовыми аннотациями. Предложенная модель TeReDiff интегрирует внутренние признаки диффузионных моделей в модуль распознавания текста, что позволяет извлекать богатые текстовые представления для последующих шагов шумоподавления.'}, 'en': {'title': 'Restoring Images with Textual Precision', 'desc': 'The Text-Aware Image Restoration (TAIR) system addresses the challenge of restoring images while maintaining the accuracy of textual information. Traditional diffusion-based methods often produce incorrect text patterns, leading to what is known as text-image hallucination. TAIR introduces a multi-task diffusion framework, TeReDiff, which combines image restoration with a text-spotting module to improve both visual and textual fidelity. By leveraging a large-scale dataset of annotated images, TAIR significantly enhances text recognition accuracy compared to existing methods.'}, 'zh': {'title': '文本感知图像修复：提升图像与文本的双重恢复', 'desc': '本文提出了一种名为文本感知图像修复（TAIR）的系统，旨在同时恢复图像内容和文本的准确性。现有的扩散基础修复方法在自然图像修复方面表现良好，但在处理图像中的文本区域时常常出现错误的文本模式。TAIR系统结合了多任务扩散框架和文本检测模块，通过联合训练提高了文本识别的准确性。实验结果表明，TAIR在图像修复和文本保真度方面均优于现有的修复方法。'}}}, {'id': 'https://huggingface.co/papers/2506.10857', 'title': 'VRBench: A Benchmark for Multi-Step Reasoning in Long Narrative Videos', 'url': 'https://huggingface.co/papers/2506.10857', 'abstract': "VRBench evaluates long video understanding by assessing multi-step reasoning capabilities across temporal and procedural validity using human-labeled question-answering pairs and reasoning chains.  \t\t\t\t\tAI-generated summary \t\t\t\t We present VRBench, the first long narrative video benchmark crafted for evaluating large models' multi-step reasoning capabilities, addressing limitations in existing evaluations that overlook temporal reasoning and procedural validity. It comprises 1,010 long videos (with an average duration of 1.6 hours), along with 9,468 human-labeled multi-step question-answering pairs and 30,292 reasoning steps with timestamps. These videos are curated via a multi-stage filtering process including expert inter-rater reviewing to prioritize plot coherence. We develop a human-AI collaborative framework that generates coherent reasoning chains, each requiring multiple temporally grounded steps, spanning seven types (e.g., event attribution, implicit inference). VRBench designs a multi-phase evaluation pipeline that assesses models at both the outcome and process levels. Apart from the MCQs for the final results, we propose a progress-level LLM-guided scoring metric to evaluate the quality of the reasoning chain from multiple dimensions comprehensively. Through extensive evaluations of 12 LLMs and 16 VLMs on VRBench, we undertake a thorough analysis and provide valuable insights that advance the field of multi-step reasoning.", 'score': 28, 'issue_id': 4272, 'pub_date': '2025-06-12', 'pub_date_card': {'ru': '12 июня', 'en': 'June 12', 'zh': '6月12日'}, 'hash': '7eabd83f69c00df7', 'authors': ['Jiashuo Yu', 'Yue Wu', 'Meng Chu', 'Zhifei Ren', 'Zizheng Huang', 'Pei Chu', 'Ruijie Zhang', 'Yinan He', 'Qirui Li', 'Songze Li', 'Zhenxiang Li', 'Zhongying Tu', 'Conghui He', 'Yu Qiao', 'Yali Wang', 'Yi Wang', 'Limin Wang'], 'affiliations': ['Nanjing University', 'Shanghai Artificial Intelligence Laboratory', 'Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2506.10857.jpg', 'data': {'categories': ['#long_context', '#benchmark', '#reasoning', '#video', '#multimodal'], 'emoji': '🎬', 'ru': {'title': 'VRBench: Оценка глубокого понимания видео через многоступенчатые рассуждения', 'desc': 'VRBench - это новый бенчмарк для оценки способностей моделей машинного обучения к многоступенчатым рассуждениям на основе длинных видео. Он включает в себя 1010 длинных видео со средней продолжительностью 1,6 часа, а также 9468 пар вопросов-ответов и 30292 шага рассуждений с временными метками, размеченных людьми. VRBench оценивает модели как на уровне конечных результатов, так и на уровне процесса рассуждений, используя многофазный конвейер оценки. Бенчмарк был протестирован на 12 языковых моделях (LLM) и 16 визуально-языковых моделях (VLM), предоставив ценные выводы для развития области многоступенчатых рассуждений.'}, 'en': {'title': 'VRBench: Advancing Multi-Step Reasoning in Long Video Understanding', 'desc': 'VRBench is a new benchmark designed to evaluate how well large models understand long videos through multi-step reasoning. It includes 1,010 long videos and thousands of human-labeled question-answering pairs, focusing on both temporal reasoning and procedural validity. The framework allows for the generation of coherent reasoning chains that require multiple steps, assessing models not just on final answers but also on the reasoning process. By testing various large language models (LLMs) and vision-language models (VLMs), VRBench aims to provide insights that enhance the understanding of multi-step reasoning in video comprehension.'}, 'zh': {'title': 'VRBench：长视频理解的新基准', 'desc': 'VRBench是一个用于评估长视频理解的基准，专注于多步骤推理能力，特别是时间推理和程序有效性。该基准包含1010个长视频，平均时长为1.6小时，以及9468个人工标注的多步骤问答对和30292个带时间戳的推理步骤。通过多阶段筛选过程，确保视频情节连贯性，并开发了一个人机协作框架，生成需要多个时间基础步骤的连贯推理链。VRBench设计了一个多阶段评估流程，综合评估模型的结果和过程，推动了多步骤推理领域的发展。'}}}, {'id': 'https://huggingface.co/papers/2506.10540', 'title': 'AniMaker: Automated Multi-Agent Animated Storytelling with MCTS-Driven\n  Clip Generation', 'url': 'https://huggingface.co/papers/2506.10540', 'abstract': "AniMaker, a multi-agent framework using MCTS-Gen and AniEval, generates coherent storytelling videos from text input, outperforming existing models with better quality and efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite rapid advancements in video generation models, generating coherent storytelling videos that span multiple scenes and characters remains challenging. Current methods often rigidly convert pre-generated keyframes into fixed-length clips, resulting in disjointed narratives and pacing issues. Furthermore, the inherent instability of video generation models means that even a single low-quality clip can significantly degrade the entire output animation's logical coherence and visual continuity. To overcome these obstacles, we introduce AniMaker, a multi-agent framework enabling efficient multi-candidate clip generation and storytelling-aware clip selection, thus creating globally consistent and story-coherent animation solely from text input. The framework is structured around specialized agents, including the Director Agent for storyboard generation, the Photography Agent for video clip generation, the Reviewer Agent for evaluation, and the Post-Production Agent for editing and voiceover. Central to AniMaker's approach are two key technical components: MCTS-Gen in Photography Agent, an efficient Monte Carlo Tree Search (MCTS)-inspired strategy that intelligently navigates the candidate space to generate high-potential clips while optimizing resource usage; and AniEval in Reviewer Agent, the first framework specifically designed for multi-shot animation evaluation, which assesses critical aspects such as story-level consistency, action completion, and animation-specific features by considering each clip in the context of its preceding and succeeding clips. Experiments demonstrate that AniMaker achieves superior quality as measured by popular metrics including VBench and our proposed AniEval framework, while significantly improving the efficiency of multi-candidate generation, pushing AI-generated storytelling animation closer to production standards.", 'score': 26, 'issue_id': 4275, 'pub_date': '2025-06-12', 'pub_date_card': {'ru': '12 июня', 'en': 'June 12', 'zh': '6月12日'}, 'hash': '5c11232a01ad90bb', 'authors': ['Haoyuan Shi', 'Yunxin Li', 'Xinyu Chen', 'Longyue Wang', 'Baotian Hu', 'Min Zhang'], 'affiliations': ['Alibaba International Digital Commerce, Hangzhou, China', 'Harbin Institute of Technology, Shenzhen, China'], 'pdf_title_img': 'assets/pdf/title_img/2506.10540.jpg', 'data': {'categories': ['#optimization', '#video', '#multimodal', '#story_generation', '#agents'], 'emoji': '🎬', 'ru': {'title': 'AniMaker: умная система для создания анимационных историй из текста', 'desc': 'AniMaker - это многоагентная система для создания анимационных видеороликов по текстовому описанию. Она использует метод Монте-Карло (MCTS-Gen) для эффективной генерации видеоклипов и специальную систему оценки AniEval для выбора наиболее подходящих фрагментов. AniMaker состоит из нескольких агентов, включая режиссера, оператора, рецензента и монтажера. Эксперименты показывают, что AniMaker превосходит существующие модели по качеству и эффективности генерации анимационных историй.'}, 'en': {'title': 'AniMaker: Crafting Coherent Stories from Text with AI', 'desc': 'AniMaker is a multi-agent framework designed to generate coherent storytelling videos from text input, addressing challenges in video generation. It utilizes specialized agents for different tasks, such as storyboard creation and video clip generation, ensuring a consistent narrative flow. The framework incorporates MCTS-Gen for efficient clip generation and AniEval for evaluating animation quality, focusing on story coherence and visual continuity. Experiments show that AniMaker outperforms existing models in both quality and efficiency, making AI-generated storytelling animation more viable for production.'}, 'zh': {'title': 'AniMaker：高效生成连贯故事视频的智能框架', 'desc': 'AniMaker是一个多智能体框架，利用MCTS-Gen和AniEval，从文本输入生成连贯的故事视频，超越了现有模型的质量和效率。该框架通过专门的智能体，如导演智能体、摄影智能体、评审智能体和后期制作智能体，来实现高效的多候选片段生成和故事意识片段选择。AniMaker的核心技术包括MCTS-Gen，它是一种高效的蒙特卡洛树搜索策略，能够智能地导航候选空间，生成高潜力的片段，同时优化资源使用；以及AniEval，这是第一个专门为多镜头动画评估设计的框架。实验表明，AniMaker在质量和效率上均显著优于现有方法，推动了AI生成的故事动画更接近生产标准。'}}}, {'id': 'https://huggingface.co/papers/2506.10274', 'title': 'Discrete Audio Tokens: More Than a Survey!', 'url': 'https://huggingface.co/papers/2506.10274', 'abstract': 'A systematic review and benchmark of discrete audio tokenizers across speech, music, and general audio domains is presented, covering their taxonomy, evaluation metrics, and limitations.  \t\t\t\t\tAI-generated summary \t\t\t\t Discrete audio tokens are compact representations that aim to preserve perceptual quality, phonetic content, and speaker characteristics while enabling efficient storage and inference, as well as competitive performance across diverse downstream tasks.They provide a practical alternative to continuous features, enabling the integration of speech and audio into modern large language models (LLMs). As interest in token-based audio processing grows, various tokenization methods have emerged, and several surveys have reviewed the latest progress in the field. However, existing studies often focus on specific domains or tasks and lack a unified comparison across various benchmarks. This paper presents a systematic review and benchmark of discrete audio tokenizers, covering three domains: speech, music, and general audio. We propose a taxonomy of tokenization approaches based on encoder-decoder, quantization techniques, training paradigm, streamability, and application domains. We evaluate tokenizers on multiple benchmarks for reconstruction, downstream performance, and acoustic language modeling, and analyze trade-offs through controlled ablation studies. Our findings highlight key limitations, practical considerations, and open challenges, providing insight and guidance for future research in this rapidly evolving area. For more information, including our main results and tokenizer database, please refer to our website: https://poonehmousavi.github.io/dates-website/.', 'score': 20, 'issue_id': 4282, 'pub_date': '2025-06-12', 'pub_date_card': {'ru': '12 июня', 'en': 'June 12', 'zh': '6月12日'}, 'hash': '636451c0543dc586', 'authors': ['Pooneh Mousavi', 'Gallil Maimon', 'Adel Moumen', 'Darius Petermann', 'Jiatong Shi', 'Haibin Wu', 'Haici Yang', 'Anastasia Kuznetsova', 'Artem Ploujnikov', 'Ricard Marxer', 'Bhuvana Ramabhadran', 'Benjamin Elizalde', 'Loren Lugosch', 'Jinyu Li', 'Cem Subakan', 'Phil Woodland', 'Minje Kim', 'Hung-yi Lee', 'Shinji Watanabe', 'Yossi Adi', 'Mirco Ravanelli'], 'affiliations': ['Apple', 'Carnegie Mellon University', 'Concordia University', 'Google', 'Indiana University', 'Laval University', 'Microsoft', 'Mila-Quebec AI Institute', 'National Taiwan University', 'The Hebrew University of Jerusalem', 'University of Cambridge', 'University of Illinois at Urbana-Champaign', 'Université de Montréal', 'Université de Toulon'], 'pdf_title_img': 'assets/pdf/title_img/2506.10274.jpg', 'data': {'categories': ['#survey', '#benchmark', '#audio'], 'emoji': '🎵', 'ru': {'title': 'Аудиотокенизация: от речи до музыки', 'desc': 'В статье представлен систематический обзор и сравнительный анализ дискретных аудиотокенизаторов в областях речи, музыки и общего аудио. Авторы предлагают таксономию подходов к токенизации на основе различных критериев, включая архитектуру энкодер-декодер и методы квантования. Проводится оценка токенизаторов по нескольким критериям, таким как качество реконструкции и эффективность в нисходящих задачах. Исследование выявляет ключевые ограничения и открытые проблемы в этой быстро развивающейся области машинного обучения.'}, 'en': {'title': 'Unlocking the Power of Discrete Audio Tokenization', 'desc': 'This paper reviews and benchmarks different methods of discrete audio tokenization used in speech, music, and general audio. Discrete audio tokens are efficient representations that maintain important audio characteristics while allowing for better storage and processing in large language models. The authors categorize various tokenization techniques and evaluate their performance across multiple tasks, highlighting their strengths and weaknesses. The study aims to provide a comprehensive understanding of the current landscape of audio tokenizers and identify future research directions.'}, 'zh': {'title': '离散音频标记器的系统评估与比较', 'desc': '本文系统回顾并基准测试了离散音频标记器在语音、音乐和一般音频领域的表现。离散音频标记是紧凑的表示方式，旨在保留感知质量、语音内容和说话者特征，同时实现高效存储和推理。我们提出了一种基于编码器-解码器、量化技术、训练范式、流式处理和应用领域的标记化方法分类。研究结果揭示了关键的局限性和未来研究的挑战，为音频处理领域的进一步发展提供了指导。'}}}, {'id': 'https://huggingface.co/papers/2506.10952', 'title': 'Domain2Vec: Vectorizing Datasets to Find the Optimal Data Mixture\n  without Training', 'url': 'https://huggingface.co/papers/2506.10952', 'abstract': 'Domain2Vec decomposes datasets into meta-domains to optimize language model pretraining and downstream performance with reduced computational cost.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce~Domain2Vec, a novel approach that decomposes any dataset into a linear combination of several meta-domains, a new concept designed to capture the key underlying features of datasets. Domain2Vec maintains a vocabulary of meta-domains and uses a classifier to decompose any given dataset into a domain vector that corresponds to a distribution over this vocabulary. These domain vectors enable the identification of the optimal data mixture for language model (LM) pretraining in a training-free manner under the \\textbf{Distribution Alignment Assumption} (DA^{2}), which suggests that when the data distributions of the training set and the validation set are better aligned, a lower validation loss is achieved. Moreover, Domain2vec can be seamlessly integrated into previous works to model the relationship between domain vectors and LM performance, greatly enhancing the efficiency and scalability of previous methods. Extensive experiments demonstrate that Domain2Vec helps find the data mixture that enhances downstream task performance with minimal computational overhead. Specifically, Domain2Vec achieves the same validation loss on Pile-CC using only 51.5% of the computation required when training on the original mixture of The Pile dataset. Under equivalent compute budget, Domain2Vec improves downstream performance by an average of 2.83%.', 'score': 18, 'issue_id': 4273, 'pub_date': '2025-06-12', 'pub_date_card': {'ru': '12 июня', 'en': 'June 12', 'zh': '6月12日'}, 'hash': 'f9cd5a16ae7e2cc3', 'authors': ['Mozhi Zhang', 'Howe Tissue', 'Lu Wang', 'Xipeng Qiu'], 'affiliations': ['Ritzz-AI', 'School of Computer Science, Fudan University, Shanghai, China'], 'pdf_title_img': 'assets/pdf/title_img/2506.10952.jpg', 'data': {'categories': ['#transfer_learning', '#dataset', '#training', '#optimization', '#data'], 'emoji': '🧩', 'ru': {'title': 'Умное разложение данных для эффективного обучения языковых моделей', 'desc': 'Domain2Vec - это новый подход к декомпозиции датасетов на мета-домены для оптимизации предобучения языковых моделей. Он использует классификатор для представления датасета в виде распределения по словарю мета-доменов. Это позволяет эффективно находить оптимальную смесь данных для предобучения, улучшая производительность на целевых задачах при меньших вычислительных затратах. Эксперименты показали, что Domain2Vec достигает той же валидационной ошибки на Pile-CC, используя лишь 51.5% вычислений по сравнению с обучением на оригинальной смеси данных The Pile.'}, 'en': {'title': 'Optimize Language Models with Domain2Vec!', 'desc': "Domain2Vec is a new method that breaks down datasets into smaller parts called meta-domains to improve the training of language models. It uses a classifier to create a domain vector that represents the dataset's features, allowing for better alignment between training and validation data distributions. This approach helps in selecting the best combination of data for pretraining language models while using less computational power. Experiments show that Domain2Vec can achieve similar performance with significantly reduced computational costs, enhancing efficiency in machine learning tasks."}, 'zh': {'title': 'Domain2Vec：优化语言模型的高效数据分解方法', 'desc': 'Domain2Vec是一种新颖的方法，它将数据集分解为多个元域的线性组合，以优化语言模型的预训练和下游性能，同时降低计算成本。该方法维护一个元域词汇表，并使用分类器将给定数据集分解为对应于该词汇表的域向量。这些域向量能够在不进行训练的情况下，根据分布对齐假设（DA²）识别出最佳的数据混合，从而降低验证损失。此外，Domain2Vec可以无缝集成到之前的工作中，建模域向量与语言模型性能之间的关系，显著提高了效率和可扩展性。'}}}, {'id': 'https://huggingface.co/papers/2506.10910', 'title': 'Magistral', 'url': 'https://huggingface.co/papers/2506.10910', 'abstract': "A scalable reinforcement learning pipeline for training reasoning models demonstrates improvements in multimodal understanding, instruction following, and function calling without relying on existing implementations.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Magistral, Mistral's first reasoning model and our own scalable reinforcement learning (RL) pipeline. Instead of relying on existing implementations and RL traces distilled from prior models, we follow a ground up approach, relying solely on our own models and infrastructure. Notably, we demonstrate a stack that enabled us to explore the limits of pure RL training of LLMs, present a simple method to force the reasoning language of the model, and show that RL on text data alone maintains most of the initial checkpoint's capabilities. We find that RL on text maintains or improves multimodal understanding, instruction following and function calling. We present Magistral Medium, trained for reasoning on top of Mistral Medium 3 with RL alone, and we open-source Magistral Small (Apache 2.0) which further includes cold-start data from Magistral Medium.", 'score': 18, 'issue_id': 4276, 'pub_date': '2025-06-12', 'pub_date_card': {'ru': '12 июня', 'en': 'June 12', 'zh': '6月12日'}, 'hash': '59d3abdb994da001', 'authors': ['Mistral-AI', ':', 'Abhinav Rastogi', 'Albert Q. Jiang', 'Andy Lo', 'Gabrielle Berrada', 'Guillaume Lample', 'Jason Rute', 'Joep Barmentlo', 'Karmesh Yadav', 'Kartik Khandelwal', 'Khyathi Raghavi Chandu', 'Léonard Blier', 'Lucile Saulnier', 'Matthieu Dinot', 'Maxime Darrin', 'Neha Gupta', 'Roman Soletskyi', 'Sagar Vaze', 'Teven Le Scao', 'Yihan Wang', 'Adam Yang', 'Alexander H. Liu', 'Alexandre Sablayrolles', 'Amélie Héliou', 'Amélie Martin', 'Andy Ehrenberg', 'Anmol Agarwal', 'Antoine Roux', 'Arthur Darcet', 'Arthur Mensch', 'Baptiste Bout', 'Baptiste Rozière', 'Baudouin De Monicault', 'Chris Bamford', 'Christian Wallenwein', 'Christophe Renaudin', 'Clémence Lanfranchi', 'Darius Dabert', 'Devon Mizelle', 'Diego de las Casas', 'Elliot Chane-Sane', 'Emilien Fugier', 'Emma Bou Hanna', 'Gauthier Delerce', 'Gauthier Guinet', 'Georgii Novikov', 'Guillaume Martin', 'Himanshu Jaju', 'Jan Ludziejewski', 'Jean-Hadrien Chabran', 'Jean-Malo Delignon', 'Joachim Studnia', 'Jonas Amar', 'Josselin Somerville Roberts', 'Julien Denize', 'Karan Saxena', 'Kush Jain', 'Lingxiao Zhao', 'Louis Martin', 'Luyu Gao', 'Lélio Renard Lavaud', 'Marie Pellat', 'Mathilde Guillaumin', 'Mathis Felardos', 'Maximilian Augustin', 'Mickaël Seznec', 'Nikhil Raghuraman', 'Olivier Duchenne', 'Patricia Wang', 'Patrick von Platen', 'Patryk Saffer', 'Paul Jacob', 'Paul Wambergue', 'Paula Kurylowicz', 'Pavankumar Reddy Muddireddy', 'Philomène Chagniot', 'Pierre Stock', 'Pravesh Agrawal', 'Romain Sauvestre', 'Rémi Delacourt', 'Sanchit Gandhi', 'Sandeep Subramanian', 'Shashwat Dalal', 'Siddharth Gandhi', 'Soham Ghosh', 'Srijan Mishra', 'Sumukh Aithal', 'Szymon Antoniak', 'Thibault Schueller', 'Thibaut Lavril', 'Thomas Robert', 'Thomas Wang', 'Timothée Lacroix', 'Valeriia Nemychnikova', 'Victor Paltz', 'Virgile Richard', 'Wen-Ding Li', 'William Marshall', 'Xuanyu Zhang', 'Yunhao Tang'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2506.10910.jpg', 'data': {'categories': ['#optimization', '#training', '#rl', '#reasoning', '#open_source', '#multimodal'], 'emoji': '🧠', 'ru': {'title': 'Обучение с подкреплением открывает новые горизонты для моделей рассуждений', 'desc': 'Исследователи представили Magistral - модель рассуждений, обученную с помощью масштабируемого конвейера обучения с подкреплением (RL). Этот подход не опирается на существующие реализации, а использует только собственные модели и инфраструктуру компании. Эксперименты показали, что RL на текстовых данных сохраняет или улучшает мультимодальное понимание, следование инструкциям и вызов функций. Авторы открыли исходный код Magistral Small и представили Magistral Medium для задач рассуждения.'}, 'en': {'title': 'Revolutionizing Reasoning with Pure Reinforcement Learning', 'desc': 'This paper presents Magistral, a new reasoning model developed using a scalable reinforcement learning (RL) pipeline. The authors emphasize a novel approach that does not depend on previous implementations or RL traces from other models, focusing instead on their own infrastructure. They demonstrate that training with pure RL on text data can preserve or enhance capabilities in multimodal understanding, instruction following, and function calling. Additionally, they introduce Magistral Medium, which is built on Mistral Medium 3, and make Magistral Small available as open-source software.'}, 'zh': {'title': '可扩展的强化学习管道，提升推理模型能力', 'desc': '本文介绍了Magistral，这是Mistral的第一个推理模型，以及我们自己的可扩展强化学习（RL）管道。我们采用自下而上的方法，完全依赖于自己的模型和基础设施，而不是现有的实现和从先前模型中提取的RL轨迹。研究表明，纯文本数据的RL训练能够保持或改善多模态理解、指令跟随和功能调用的能力。我们还发布了Magistral Medium和开源的Magistral Small，进一步支持推理训练。'}}}, {'id': 'https://huggingface.co/papers/2506.10357', 'title': 'Optimus-3: Towards Generalist Multimodal Minecraft Agents with Scalable\n  Task Experts', 'url': 'https://huggingface.co/papers/2506.10357', 'abstract': "Optimus-3, an agent using knowledge-enhanced data generation, Mixture-of-Experts routing, and multimodal reasoning-augmented reinforcement learning, achieves superior performance across various tasks in Minecraft.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, agents based on multimodal large language models (MLLMs) have achieved remarkable progress across various domains. However, building a generalist agent with capabilities such as perception, planning, action, grounding, and reflection in open-world environments like Minecraft remains challenges: insufficient domain-specific data, interference among heterogeneous tasks, and visual diversity in open-world settings. In this paper, we address these challenges through three key contributions. 1) We propose a knowledge-enhanced data generation pipeline to provide scalable and high-quality training data for agent development. 2) To mitigate interference among heterogeneous tasks, we introduce a Mixture-of-Experts (MoE) architecture with task-level routing. 3) We develop a Multimodal Reasoning-Augmented Reinforcement Learning approach to enhance the agent's reasoning ability for visual diversity in Minecraft. Built upon these innovations, we present Optimus-3, a general-purpose agent for Minecraft. Extensive experimental results demonstrate that Optimus-3 surpasses both generalist multimodal large language models and existing state-of-the-art agents across a wide range of tasks in the Minecraft environment. Project page: https://cybertronagent.github.io/Optimus-3.github.io/", 'score': 17, 'issue_id': 4272, 'pub_date': '2025-06-12', 'pub_date_card': {'ru': '12 июня', 'en': 'June 12', 'zh': '6月12日'}, 'hash': '145045d8e634c76b', 'authors': ['Zaijing Li', 'Yuquan Xie', 'Rui Shao', 'Gongwei Chen', 'Weili Guan', 'Dongmei Jiang', 'Liqiang Nie'], 'affiliations': ['Harbin Institute of Technology, Shenzhen', 'Peng Cheng Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2506.10357.jpg', 'data': {'categories': ['#optimization', '#architecture', '#agents', '#rag', '#rl', '#reasoning', '#multimodal', '#games'], 'emoji': '🤖', 'ru': {'title': 'Optimus-3: Универсальный ИИ-агент покоряет Minecraft', 'desc': 'Статья представляет Optimus-3 - интеллектуального агента для игры Minecraft, использующего усовершенствованные методы машинного обучения. Агент применяет генерацию данных с использованием знаний, маршрутизацию на основе смеси экспертов и обучение с подкреплением, дополненное мультимодальными рассуждениями. Optimus-3 демонстрирует превосходную производительность в различных задачах в открытом мире Minecraft. Это достижение преодолевает ключевые проблемы, такие как недостаток специфических данных, интерференция между разнородными задачами и визуальное разнообразие в открытых средах.'}, 'en': {'title': 'Optimus-3: Mastering Minecraft with Advanced AI Techniques', 'desc': 'Optimus-3 is a general-purpose agent designed for the open-world environment of Minecraft, utilizing advanced techniques in machine learning. It incorporates a knowledge-enhanced data generation pipeline to create high-quality training data, addressing the challenge of insufficient domain-specific data. The agent employs a Mixture-of-Experts (MoE) architecture to effectively manage interference among diverse tasks, allowing for better performance. Additionally, it uses Multimodal Reasoning-Augmented Reinforcement Learning to improve its reasoning capabilities, enabling it to handle the visual diversity present in Minecraft.'}, 'zh': {'title': 'Optimus-3：在Minecraft中超越极限的智能体', 'desc': '本文介绍了Optimus-3，一个利用知识增强数据生成、专家混合路由和多模态推理增强强化学习的智能体。该智能体在Minecraft等开放世界环境中表现出色，解决了领域特定数据不足、异构任务干扰和视觉多样性等挑战。我们提出了一种知识增强的数据生成管道，以提供可扩展的高质量训练数据，并引入了任务级路由的专家混合架构来减轻任务间的干扰。此外，我们开发了多模态推理增强的强化学习方法，以提升智能体在视觉多样性方面的推理能力。'}}}, {'id': 'https://huggingface.co/papers/2506.10741', 'title': 'PosterCraft: Rethinking High-Quality Aesthetic Poster Generation in a\n  Unified Framework', 'url': 'https://huggingface.co/papers/2506.10741', 'abstract': 'PosterCraft improves aesthetic poster generation through a unified, modular pipeline with enhanced text rendering, region-aware fine-tuning, aesthetic reinforcement learning, and joint vision-language refinement.  \t\t\t\t\tAI-generated summary \t\t\t\t Generating aesthetic posters is more challenging than simple design images: it requires not only precise text rendering but also the seamless integration of abstract artistic content, striking layouts, and overall stylistic harmony. To address this, we propose PosterCraft, a unified framework that abandons prior modular pipelines and rigid, predefined layouts, allowing the model to freely explore coherent, visually compelling compositions. PosterCraft employs a carefully designed, cascaded workflow to optimize the generation of high-aesthetic posters: (i) large-scale text-rendering optimization on our newly introduced Text-Render-2M dataset; (ii) region-aware supervised fine-tuning on HQ-Poster100K; (iii) aesthetic-text-reinforcement learning via best-of-n preference optimization; and (iv) joint vision-language feedback refinement. Each stage is supported by a fully automated data-construction pipeline tailored to its specific needs, enabling robust training without complex architectural modifications. Evaluated on multiple experiments, PosterCraft significantly outperforms open-source baselines in rendering accuracy, layout coherence, and overall visual appeal-approaching the quality of SOTA commercial systems. Our code, models, and datasets can be found in the Project page: https://ephemeral182.github.io/PosterCraft', 'score': 15, 'issue_id': 4276, 'pub_date': '2025-06-12', 'pub_date_card': {'ru': '12 июня', 'en': 'June 12', 'zh': '6月12日'}, 'hash': '5c43d96d0a604ef8', 'authors': ['SiXiang Chen', 'Jianyu Lai', 'Jialin Gao', 'Tian Ye', 'Haoyu Chen', 'Hengyu Shi', 'Shitong Shao', 'Yunlong Lin', 'Song Fei', 'Zhaohu Xing', 'Yeying Jin', 'Junfeng Luo', 'Xiaoming Wei', 'Lei Zhu'], 'affiliations': ['Meituan', 'National University of Singapore', 'The Hong Kong University of Science and Technology', 'The Hong Kong University of Science and Technology (Guangzhou)', 'Xiamen University'], 'pdf_title_img': 'assets/pdf/title_img/2506.10741.jpg', 'data': {'categories': ['#optimization', '#training', '#rl', '#architecture', '#open_source', '#dataset', '#data', '#multimodal'], 'emoji': '🎨', 'ru': {'title': 'Искусственный интеллект создает эстетичные постеры нового уровня', 'desc': 'PosterCraft - это унифицированная система для создания эстетичных постеров с использованием искусственного интеллекта. Она включает в себя оптимизацию рендеринга текста, обучение с подкреплением для улучшения эстетики и совместную обработку визуальной и текстовой информации. Система использует каскадный рабочий процесс, включающий обучение на больших наборах данных и тонкую настройку с учетом регионов изображения. PosterCraft превосходит существующие открытые решения по точности рендеринга, согласованности макета и общей визуальной привлекательности.'}, 'en': {'title': "Elevating Poster Design with PosterCraft's Unified Framework", 'desc': 'PosterCraft is a novel framework designed to enhance the generation of aesthetic posters by integrating advanced techniques in text rendering and layout optimization. It utilizes a modular pipeline that includes region-aware fine-tuning and aesthetic reinforcement learning to improve the visual quality of the generated images. The framework operates on a cascaded workflow, leveraging large-scale datasets for training and optimizing each component for better performance. Experimental results show that PosterCraft surpasses existing open-source models in rendering accuracy and overall aesthetic appeal, making it competitive with state-of-the-art commercial systems.'}, 'zh': {'title': 'PosterCraft：美学海报生成的新突破', 'desc': 'PosterCraft 是一个改进美学海报生成的统一模块化框架。它通过增强的文本渲染、区域感知微调、美学强化学习和联合视觉-语言优化，提升了海报的生成质量。该框架允许模型自由探索视觉上引人注目的组合，克服了传统模块化管道的局限性。经过多项实验评估，PosterCraft 在渲染精度、布局一致性和整体视觉吸引力方面显著优于开源基线，接近最先进的商业系统的质量。'}}}, {'id': 'https://huggingface.co/papers/2506.10974', 'title': 'AutoMind: Adaptive Knowledgeable Agent for Automated Data Science', 'url': 'https://huggingface.co/papers/2506.10974', 'abstract': 'AutoMind, a flexible and knowledgeable LLM-agent framework, improves automated data science through expert knowledge integration, strategic solution exploration, and adaptive coding, outperforming existing systems.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Model (LLM) agents have shown great potential in addressing real-world data science problems. LLM-driven data science agents promise to automate the entire machine learning pipeline, yet their real-world effectiveness remains limited. Existing frameworks depend on rigid, pre-defined workflows and inflexible coding strategies; consequently, they excel only on relatively simple, classical problems and fail to capture the empirical expertise that human practitioners bring to complex, innovative tasks. In this work, we introduce AutoMind, an adaptive, knowledgeable LLM-agent framework that overcomes these deficiencies through three key advances: (1) a curated expert knowledge base that grounds the agent in domain expert knowledge, (2) an agentic knowledgeable tree search algorithm that strategically explores possible solutions, and (3) a self-adaptive coding strategy that dynamically tailors code generation to task complexity. Evaluations on two automated data science benchmarks demonstrate that AutoMind delivers superior performance versus state-of-the-art baselines. Additional analyses confirm favorable effectiveness, efficiency, and qualitative solution quality, highlighting AutoMind as an efficient and robust step toward fully automated data science.', 'score': 11, 'issue_id': 4276, 'pub_date': '2025-06-12', 'pub_date_card': {'ru': '12 июня', 'en': 'June 12', 'zh': '6月12日'}, 'hash': '50bcb63544ac7586', 'authors': ['Yixin Ou', 'Yujie Luo', 'Jingsheng Zheng', 'Lanning Wei', 'Shuofei Qiao', 'Jintian Zhang', 'Da Zheng', 'Huajun Chen', 'Ningyu Zhang'], 'affiliations': ['Ant Group', 'Zhejiang University', 'Zhejiang University - Ant Group Joint Laboratory of Knowledge Graph'], 'pdf_title_img': 'assets/pdf/title_img/2506.10974.jpg', 'data': {'categories': ['#science', '#training', '#dataset', '#benchmark', '#agents'], 'emoji': '🤖', 'ru': {'title': 'AutoMind: ИИ-ассистент нового поколения для автоматизации науки о данных', 'desc': 'AutoMind - это новая гибкая система на основе больших языковых моделей для автоматизации задач в области науки о данных. Она интегрирует экспертные знания, использует стратегический поиск решений и адаптивную генерацию кода. AutoMind превосходит существующие системы благодаря использованию базы знаний, алгоритма поиска на основе дерева решений и самонастраивающейся стратегии кодирования. Система показала высокую эффективность на тестовых наборах данных для автоматизированной науки о данных.'}, 'en': {'title': 'AutoMind: Revolutionizing Automated Data Science with Expert Knowledge and Adaptability', 'desc': 'AutoMind is a new framework designed to enhance automated data science by integrating expert knowledge and adapting its approach based on the complexity of tasks. It utilizes a curated knowledge base to inform its decisions, allowing it to tackle more complex problems than traditional systems. The framework employs a knowledgeable tree search algorithm to explore various solutions strategically, improving its problem-solving capabilities. Evaluations show that AutoMind outperforms existing methods, making it a significant advancement in the field of automated machine learning.'}, 'zh': {'title': 'AutoMind：自动化数据科学的新突破', 'desc': 'AutoMind是一个灵活且知识丰富的LLM代理框架，旨在通过整合专家知识、战略性解决方案探索和自适应编码来提升自动化数据科学的能力。与现有系统相比，AutoMind在处理复杂和创新任务时表现更为出色，克服了传统框架的局限性。它通过建立一个经过筛选的专家知识库、采用智能的知识树搜索算法以及动态调整编码策略，来适应不同任务的复杂性。评估结果表明，AutoMind在自动化数据科学基准测试中超越了现有的最先进方法，展现出高效和稳健的特性。'}}}, {'id': 'https://huggingface.co/papers/2506.10821', 'title': 'VideoDeepResearch: Long Video Understanding With Agentic Tool Using', 'url': 'https://huggingface.co/papers/2506.10821', 'abstract': "VideoDeepResearch, a text-only reasoning model with modular tools, surpasses existing baselines in long video understanding tasks without extending context windows or enhancing visual perception capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t Long video understanding (LVU) presents a significant challenge for current multi-modal large language models (MLLMs) due to the task's inherent complexity and context window constraint. It is widely assumed that addressing LVU tasks requires foundation MLLMs with extended context windows, strong visual perception capabilities, and proficient domain expertise. In this work, we challenge this common belief by introducing VideoDeepResearch, a novel agentic framework for long video understanding. Our approach relies solely on a text-only large reasoning model (LRM) combined with a modular multi-modal toolkit, including multimodal retrievers and visual perceivers, all of which are readily available in practice. For each LVU task, the system formulates a problem-solving strategy through reasoning, while selectively accessing and utilizing essential video content via tool using. We conduct extensive experiments on popular LVU benchmarks, including MLVU, Video-MME, and LVBench. Our results demonstrate that VideoDeepResearch achieves substantial improvements over existing MLLM baselines, surpassing the previous state-of-the-art by 9.6%, 6.6%, and 3.9% on MLVU (test), LVBench, and LongVideoBench, respectively. These findings highlight the promise of agentic systems in overcoming key challenges in LVU problems.", 'score': 11, 'issue_id': 4273, 'pub_date': '2025-06-12', 'pub_date_card': {'ru': '12 июня', 'en': 'June 12', 'zh': '6月12日'}, 'hash': 'e1e5003f31573e97', 'authors': ['Huaying Yuan', 'Zheng Liu', 'Junjie Zhou', 'Ji-Rong Wen', 'Zhicheng Dou'], 'affiliations': ['Beijing Academy of Artificial Intelligence', 'Beijing University of Posts and Telecommunications', 'Gaoling School of Artificial Intelligence, Renmin University of China'], 'pdf_title_img': 'assets/pdf/title_img/2506.10821.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#long_context', '#video', '#multimodal', '#agents'], 'emoji': '🎥', 'ru': {'title': 'Агентный подход превосходит мультимодальные модели в понимании длинных видео', 'desc': 'VideoDeepResearch - это новая агентная система для понимания длинных видео, основанная на текстовой модели рассуждений и модульном наборе инструментов. Система формирует стратегию решения задач через рассуждения, выборочно обращаясь к важному видеоконтенту. Эксперименты показали значительное улучшение результатов по сравнению с существующими мультимодальными языковыми моделями на нескольких бенчмарках. Исследование демонстрирует потенциал агентных систем в преодолении ключевых проблем понимания длинных видео.'}, 'en': {'title': 'Revolutionizing Long Video Understanding with Text-Only Reasoning', 'desc': 'VideoDeepResearch is a novel framework designed to improve long video understanding (LVU) tasks without relying on extended context windows or advanced visual perception capabilities. It utilizes a text-only large reasoning model (LRM) in conjunction with a modular toolkit that includes multimodal retrievers and visual perceivers. This system formulates problem-solving strategies through reasoning and selectively accesses relevant video content as needed. Experimental results show that VideoDeepResearch significantly outperforms existing multi-modal large language models (MLLMs) on various LVU benchmarks, demonstrating its effectiveness in tackling complex video understanding challenges.'}, 'zh': {'title': '突破长视频理解的全新框架', 'desc': 'VideoDeepResearch是一种新型的长视频理解框架，它仅依赖文本推理模型和模块化工具，而不需要扩展上下文窗口或增强视觉感知能力。该系统通过推理制定问题解决策略，并利用多模态工具选择性地访问和使用视频内容。我们在多个长视频理解基准上进行了广泛实验，结果显示VideoDeepResearch在性能上显著超越了现有的多模态大语言模型基线。该研究表明，代理系统在解决长视频理解问题中具有很大的潜力。'}}}, {'id': 'https://huggingface.co/papers/2506.09967', 'title': 'Resa: Transparent Reasoning Models via SAEs', 'url': 'https://huggingface.co/papers/2506.09967', 'abstract': "SAE-Tuning efficiently elicits strong reasoning in language models by leveraging sparse autoencoders, enabling cost-effective performance gains without extensive retraining.  \t\t\t\t\tAI-generated summary \t\t\t\t How cost-effectively can we elicit strong reasoning in language models by leveraging their underlying representations? We answer this question with Resa, a family of 1.5B reasoning models trained via a novel and efficient sparse autoencoder tuning (SAE-Tuning) procedure. This method first trains an SAE to capture reasoning abilities from a source model, and then uses the trained SAE to guide a standard supervised fine-tuning process to elicit such abilities in a target model, all using verified question-answer data without any reasoning traces. Notably, when applied to certain base models before further RL post-training, SAE-Tuning retains >97% of its RL-trained counterpart's reasoning performance while reducing training costs by >2000x to roughly \\1 and training time by >450x to around 20 minutes. Furthermore, when applied to lightly RL-trained models (e.g., within 1 hour on 2 GPUs), it enables reasoning performance such as 43.33% Pass@1 on AIME24 and 90% Pass@1 on AMC23 for only around 1 additional cost. Surprisingly, the reasoning abilities extracted via SAEs are potentially both generalizable and modular. Generality means abilities extracted from one dataset still elevate performance on a larger and overlapping corpus. Modularity means abilities extracted from Qwen or Qwen-Math can be attached to the R1-Distill model at test time, without any retraining, and yield comparable gains. Extensive ablations validate these findings and all artifacts are fully open-sourced.", 'score': 11, 'issue_id': 4274, 'pub_date': '2025-06-11', 'pub_date_card': {'ru': '11 июня', 'en': 'June 11', 'zh': '6月11日'}, 'hash': '03a45cae6d11be64', 'authors': ['Shangshang Wang', 'Julian Asilis', 'Ömer Faruk Akgül', 'Enes Burak Bilgin', 'Ollie Liu', 'Deqing Fu', 'Willie Neiswanger'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2506.09967.jpg', 'data': {'categories': ['#rl', '#training', '#open_source', '#optimization', '#small_models', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Эффективное обучение рассуждению с помощью разреженных автоэнкодеров', 'desc': 'Эта статья представляет метод SAE-Tuning для эффективного улучшения способностей языковых моделей к рассуждению. Метод использует разреженные автоэнкодеры (SAE) для извлечения навыков рассуждения из исходной модели и применения их к целевой модели. SAE-Tuning позволяет достичь производительности, сравнимой с RL-обучением, при значительно меньших затратах времени и ресурсов. Исследование также показывает, что извлеченные навыки рассуждения обладают свойствами обобщаемости и модульности.'}, 'en': {'title': 'Efficient Reasoning Enhancement in Language Models with SAE-Tuning', 'desc': 'The paper introduces SAE-Tuning, a method that enhances reasoning capabilities in language models using sparse autoencoders. This approach allows for significant performance improvements without the need for extensive retraining, achieving cost reductions of over 2000 times and time savings of over 450 times. By training a sparse autoencoder to capture reasoning skills from a source model, the method effectively guides the fine-tuning of a target model using verified question-answer data. The results show that the reasoning abilities gained are both generalizable across datasets and modular, allowing for easy integration into different models without retraining.'}, 'zh': {'title': '高效推理：稀疏自编码器调优的力量', 'desc': 'SAE-Tuning是一种高效的稀疏自编码器调优方法，能够在语言模型中引发强大的推理能力，而无需进行大量的重新训练。该方法首先训练一个稀疏自编码器（SAE），以从源模型中捕捉推理能力，然后利用训练好的SAE指导标准的监督微调过程，从而在目标模型中引发这些能力。通过这种方式，SAE-Tuning在保持推理性能的同时，显著降低了训练成本和时间。研究表明，提取的推理能力具有可泛化和模块化的特性，可以在不同的数据集和模型之间灵活应用。'}}}, {'id': 'https://huggingface.co/papers/2506.10890', 'title': 'CreatiPoster: Towards Editable and Controllable Multi-Layer Graphic\n  Design Generation', 'url': 'https://huggingface.co/papers/2506.10890', 'abstract': 'CreatiPoster generates high-quality, editable, and customizable graphic compositions from text or assets, outperforming existing tools and templates.  \t\t\t\t\tAI-generated summary \t\t\t\t Graphic design plays a crucial role in both commercial and personal contexts, yet creating high-quality, editable, and aesthetically pleasing graphic compositions remains a time-consuming and skill-intensive task, especially for beginners. Current AI tools automate parts of the workflow, but struggle to accurately incorporate user-supplied assets, maintain editability, and achieve professional visual appeal. Commercial systems, like Canva Magic Design, rely on vast template libraries, which are impractical for replicate. In this paper, we introduce CreatiPoster, a framework that generates editable, multi-layer compositions from optional natural-language instructions or assets. A protocol model, an RGBA large multimodal model, first produces a JSON specification detailing every layer (text or asset) with precise layout, hierarchy, content and style, plus a concise background prompt. A conditional background model then synthesizes a coherent background conditioned on this rendered foreground layers. We construct a benchmark with automated metrics for graphic-design generation and show that CreatiPoster surpasses leading open-source approaches and proprietary commercial systems. To catalyze further research, we release a copyright-free corpus of 100,000 multi-layer designs. CreatiPoster supports diverse applications such as canvas editing, text overlay, responsive resizing, multilingual adaptation, and animated posters, advancing the democratization of AI-assisted graphic design. Project homepage: https://github.com/graphic-design-ai/creatiposter', 'score': 10, 'issue_id': 4272, 'pub_date': '2025-06-12', 'pub_date_card': {'ru': '12 июня', 'en': 'June 12', 'zh': '6月12日'}, 'hash': '22fffd0088d280e0', 'authors': ['Zhao Zhang', 'Yutao Cheng', 'Dexiang Hong', 'Maoke Yang', 'Gonglei Shi', 'Lei Ma', 'Hui Zhang', 'Jie Shao', 'Xinglong Wu'], 'affiliations': ['ByteDance, Fudan University', 'ByteDance, Intelligent Creation'], 'pdf_title_img': 'assets/pdf/title_img/2506.10890.jpg', 'data': {'categories': ['#dataset', '#cv', '#benchmark', '#open_source', '#multimodal'], 'emoji': '🎨', 'ru': {'title': 'CreatiPoster: ИИ-революция в графическом дизайне', 'desc': 'CreatiPoster - это новая система искусственного интеллекта для генерации высококачественных графических композиций на основе текста или готовых ресурсов. Она использует модель протокола для создания JSON-спецификации каждого слоя и условную модель фона для синтеза согласованного фона. CreatiPoster превосходит существующие инструменты и шаблоны, обеспечивая редактируемость и профессиональный визуальный appeal. Система поддерживает различные приложения, включая редактирование холста, наложение текста и адаптивное изменение размера.'}, 'en': {'title': 'Revolutionizing Graphic Design with AI-Generated Custom Compositions', 'desc': 'CreatiPoster is a novel framework that generates high-quality, editable graphic designs from user inputs like text or images. It utilizes a protocol model to create a detailed JSON specification for each design layer, ensuring precise layout and style. A conditional background model then generates a cohesive background that complements the foreground elements. This approach not only enhances the editability and visual appeal of designs but also outperforms existing tools and templates in the market.'}, 'zh': {'title': 'CreatiPoster：让图形设计更简单', 'desc': 'CreatiPoster 是一个生成高质量、可编辑和可定制图形作品的框架，能够从文本或资产中创建多层次的图形设计。与现有工具相比，它在用户提供的资产整合、可编辑性和视觉吸引力方面表现更佳。该框架使用协议模型生成详细的 JSON 规范，描述每一层的布局、层次、内容和风格。通过提供一个无版权的 100,000 个多层设计的语料库，CreatiPoster 促进了 AI 辅助图形设计的进一步研究和应用。'}}}, {'id': 'https://huggingface.co/papers/2506.10960', 'title': 'ChineseHarm-Bench: A Chinese Harmful Content Detection Benchmark', 'url': 'https://huggingface.co/papers/2506.10960', 'abstract': 'A benchmark for Chinese harmful content detection, coupled with a knowledge-augmented baseline, improves the performance of smaller models without extensive resources.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have been increasingly applied to automated harmful content detection tasks, assisting moderators in identifying policy violations and improving the overall efficiency and accuracy of content review. However, existing resources for harmful content detection are predominantly focused on English, with Chinese datasets remaining scarce and often limited in scope. We present a comprehensive, professionally annotated benchmark for Chinese content harm detection, which covers six representative categories and is constructed entirely from real-world data. Our annotation process further yields a knowledge rule base that provides explicit expert knowledge to assist LLMs in Chinese harmful content detection. In addition, we propose a knowledge-augmented baseline that integrates both human-annotated knowledge rules and implicit knowledge from large language models, enabling smaller models to achieve performance comparable to state-of-the-art LLMs. Code and data are available at https://github.com/zjunlp/ChineseHarm-bench.', 'score': 9, 'issue_id': 4276, 'pub_date': '2025-06-12', 'pub_date_card': {'ru': '12 июня', 'en': 'June 12', 'zh': '6月12日'}, 'hash': 'db6960a49f9467ee', 'authors': ['Kangwei Liu', 'Siyuan Cheng', 'Bozhong Tian', 'Xiaozhuan Liang', 'Yuyang Yin', 'Meng Han', 'Ningyu Zhang', 'Bryan Hooi', 'Xi Chen', 'Shumin Deng'], 'affiliations': ['National University of Singapore', 'Tencent', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2506.10960.jpg', 'data': {'categories': ['#multilingual', '#small_models', '#low_resource', '#ethics', '#dataset', '#benchmark'], 'emoji': '🇨🇳', 'ru': {'title': 'Улучшение обнаружения вредоносного контента на китайском языке с помощью знаний', 'desc': 'Представлен новый бенчмарк для обнаружения вредоносного контента на китайском языке, включающий шесть категорий и построенный на реальных данных. Процесс аннотации позволил создать базу знаний с экспертными правилами для помощи языковым моделям. Предложен метод дополнения знаниями, объединяющий аннотированные правила и неявные знания больших языковых моделей. Это позволяет небольшим моделям достигать производительности, сравнимой с современными крупными языковыми моделями, без использования значительных ресурсов.'}, 'en': {'title': 'Empowering Small Models for Chinese Harm Detection', 'desc': 'This paper introduces a new benchmark for detecting harmful content in Chinese, addressing the lack of resources in this area compared to English. It features a dataset that is professionally annotated and covers six categories of harmful content, using real-world examples. The authors also develop a knowledge-augmented baseline that combines expert knowledge with insights from large language models, allowing smaller models to perform effectively without needing extensive resources. This approach enhances the accuracy of harmful content detection in Chinese, making it more accessible for various applications.'}, 'zh': {'title': '提升中文有害内容检测的基准与方法', 'desc': '本论文提出了一个针对中文有害内容检测的基准数据集，涵盖六个代表性类别，并完全基于真实世界数据进行专业标注。现有的有害内容检测资源主要集中在英语，中文数据集相对稀缺且范围有限。我们还构建了一个知识增强的基线模型，结合了人工标注的知识规则和大型语言模型的隐性知识，使得较小的模型在性能上能够与最先进的模型相媲美。该研究为中文有害内容检测提供了重要的资源和方法，提升了内容审核的效率和准确性。'}}}, {'id': 'https://huggingface.co/papers/2506.09344', 'title': 'Ming-Omni: A Unified Multimodal Model for Perception and Generation', 'url': 'https://huggingface.co/papers/2506.09344', 'abstract': 'Ming-Omni is a unified multimodal model with dedicated encoders and modality-specific routers that can process images, text, audio, and video, and performs tasks like speech and image generation, context-aware chatting, and versatile image editing.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose Ming-Omni, a unified multimodal model capable of processing images, text, audio, and video, while demonstrating strong proficiency in both speech and image generation. Ming-Omni employs dedicated encoders to extract tokens from different modalities, which are then processed by Ling, an MoE architecture equipped with newly proposed modality-specific routers. This design enables a single model to efficiently process and fuse multimodal inputs within a unified framework, thereby facilitating diverse tasks without requiring separate models, task-specific fine-tuning, or structural redesign. Importantly, Ming-Omni extends beyond conventional multimodal models by supporting audio and image generation. This is achieved through the integration of an advanced audio decoder for natural-sounding speech and Ming-Lite-Uni for high-quality image generation, which also allow the model to engage in context-aware chatting, perform text-to-speech conversion, and conduct versatile image editing. Our experimental results showcase Ming-Omni offers a powerful solution for unified perception and generation across all modalities. Notably, our proposed Ming-Omni is the first open-source model we are aware of to match GPT-4o in modality support, and we release all code and model weights to encourage further research and development in the community.', 'score': 9, 'issue_id': 4273, 'pub_date': '2025-06-11', 'pub_date_card': {'ru': '11 июня', 'en': 'June 11', 'zh': '6月11日'}, 'hash': 'b9166301d93dd2bb', 'authors': ['Inclusion AI', 'Biao Gong', 'Cheng Zou', 'Chuanyang Zheng', 'Chunluan Zhou', 'Canxiang Yan', 'Chunxiang Jin', 'Chunjie Shen', 'Dandan Zheng', 'Fudong Wang', 'Furong Xu', 'GuangMing Yao', 'Jun Zhou', 'Jingdong Chen', 'Jianxin Sun', 'Jiajia Liu', 'Jianjiang Zhu', 'Jun Peng', 'Kaixiang Ji', 'Kaiyou Song', 'Kaimeng Ren', 'Libin Wang', 'Lixiang Ru', 'Lele Xie', 'Longhua Tan', 'Lyuxin Xue', 'Lan Wang', 'Mochen Bai', 'Ning Gao', 'Pei Chen', 'Qingpei Guo', 'Qinglong Zhang', 'Qiang Xu', 'Rui Liu', 'Ruijie Xiong', 'Sirui Gao', 'Tinghao Liu', 'Taisong Li', 'Weilong Chai', 'Xinyu Xiao', 'Xiaomei Wang', 'Xiaoxue Chen', 'Xiao Lu', 'Xiaoyu Li', 'Xingning Dong', 'Xuzheng Yu', 'Yi Yuan', 'Yuting Gao', 'Yunxiao Sun', 'Yipeng Chen', 'Yifei Wu', 'Yongjie Lyu', 'Ziping Ma', 'Zipeng Feng', 'Zhijiang Fang', 'Zhihao Qiu', 'Ziyuan Huang', 'Zhengyu He'], 'affiliations': ['Ant Group', 'Inclusion AI'], 'pdf_title_img': 'assets/pdf/title_img/2506.09344.jpg', 'data': {'categories': ['#audio', '#open_source', '#video', '#cv', '#multimodal'], 'emoji': '🤖', 'ru': {'title': 'Единая модель для всех модальностей: восприятие и генерация в одном', 'desc': 'Статья представляет Ming-Omni - унифицированную мультимодальную модель, способную обрабатывать изображения, текст, аудио и видео. Модель использует специализированные энкодеры для извлечения токенов из разных модальностей и архитектуру MoE с модальноспецифичными роутерами для их обработки. Ming-Omni поддерживает генерацию аудио и изображений, а также контекстно-зависимый чат и редактирование изображений. Экспериментальные результаты показывают, что Ming-Omni предлагает мощное решение для унифицированного восприятия и генерации во всех модальностях.'}, 'en': {'title': 'Ming-Omni: One Model, Many Modalities!', 'desc': 'Ming-Omni is a cutting-edge multimodal model designed to handle various types of data, including images, text, audio, and video. It utilizes dedicated encoders to extract information from these different modalities and employs a mixture of experts (MoE) architecture with modality-specific routers for efficient processing. This innovative design allows Ming-Omni to perform a wide range of tasks, such as generating speech and images, engaging in context-aware conversations, and editing images, all within a single framework. By being open-source and matching the capabilities of advanced models like GPT-4o, Ming-Omni aims to foster further research and development in the field of multimodal AI.'}, 'zh': {'title': 'Ming-Omni：统一多模态处理的强大解决方案', 'desc': 'Ming-Omni是一种统一的多模态模型，能够处理图像、文本、音频和视频。它使用专用编码器提取不同模态的特征，并通过新提出的模态特定路由器进行处理。该模型支持语音和图像生成，能够进行上下文感知的对话和多功能的图像编辑。Ming-Omni是首个开源模型，能够在多模态支持上与GPT-4o相媲美，促进了社区的进一步研究和发展。'}}}, {'id': 'https://huggingface.co/papers/2506.10953', 'title': 'Build the web for agents, not agents for the web', 'url': 'https://huggingface.co/papers/2506.10953', 'abstract': 'A paradigm shift in web agent research is proposed, advocating for the development of Agentic Web Interfaces (AWIs) to optimize interaction for AI agents within web environments.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in Large Language Models (LLMs) and multimodal counterparts have spurred significant interest in developing web agents -- AI systems capable of autonomously navigating and completing tasks within web environments. While holding tremendous promise for automating complex web interactions, current approaches face substantial challenges due to the fundamental mismatch between human-designed interfaces and LLM capabilities. Current methods struggle with the inherent complexity of web inputs, whether processing massive DOM trees, relying on screenshots augmented with additional information, or bypassing the user interface entirely through API interactions. This position paper advocates for a paradigm shift in web agent research: rather than forcing web agents to adapt to interfaces designed for humans, we should develop a new interaction paradigm specifically optimized for agentic capabilities. To this end, we introduce the concept of an Agentic Web Interface (AWI), an interface specifically designed for agents to navigate a website. We establish six guiding principles for AWI design, emphasizing safety, efficiency, and standardization, to account for the interests of all primary stakeholders. This reframing aims to overcome fundamental limitations of existing interfaces, paving the way for more efficient, reliable, and transparent web agent design, which will be a collaborative effort involving the broader ML community.', 'score': 6, 'issue_id': 4274, 'pub_date': '2025-06-12', 'pub_date_card': {'ru': '12 июня', 'en': 'June 12', 'zh': '6月12日'}, 'hash': '77dd06867c379745', 'authors': ['Xing Han Lù', 'Gaurav Kamath', 'Marius Mosbach', 'Siva Reddy'], 'affiliations': ['Equal Advising', 'McGill University', 'Mila', 'Quebec AI Institute'], 'pdf_title_img': 'assets/pdf/title_img/2506.10953.jpg', 'data': {'categories': ['#agents', '#agi', '#optimization', '#multimodal'], 'emoji': '🌐', 'ru': {'title': 'Агентные Веб-Интерфейсы: революция во взаимодействии ИИ с веб-средой', 'desc': 'Статья предлагает новую парадигму для веб-агентов, вводя концепцию Агентных Веб-Интерфейсов (AWI). AWI оптимизированы для взаимодействия ИИ-агентов с веб-средой, преодолевая ограничения интерфейсов, созданных для людей. Авторы устанавливают шесть принципов дизайна AWI, учитывающих безопасность, эффективность и стандартизацию. Этот подход нацелен на создание более надежных и прозрачных веб-агентов, что потребует совместных усилий ML-сообщества.'}, 'en': {'title': 'Redefining Web Interaction for AI Agents with AWIs', 'desc': 'This paper proposes a new approach to web agent research by introducing Agentic Web Interfaces (AWIs), which are specifically designed for AI agents to interact with web environments. Current web interfaces are not optimized for the capabilities of AI, leading to inefficiencies and challenges in task completion. The authors outline six guiding principles for designing AWIs, focusing on safety, efficiency, and standardization to benefit all stakeholders involved. This shift aims to enhance the performance and reliability of web agents, encouraging collaboration within the machine learning community.'}, 'zh': {'title': '为代理设计优化网络交互界面', 'desc': '本文提出了网络代理研究的范式转变，倡导开发代理网络接口（AWI），以优化人工智能代理在网络环境中的交互。随着大型语言模型（LLM）和多模态模型的进步，开发能够自主导航和完成任务的网络代理引起了广泛关注。当前的方法面临着人类设计的界面与LLM能力之间的根本不匹配问题，导致处理复杂网络输入时的困难。本文提出的AWI概念旨在为代理设计专门的交互界面，以提高安全性、效率和标准化，推动更高效、可靠和透明的网络代理设计。'}}}, {'id': 'https://huggingface.co/papers/2506.10178', 'title': 'Attention, Please! Revisiting Attentive Probing for Masked Image\n  Modeling', 'url': 'https://huggingface.co/papers/2506.10178', 'abstract': 'Efficient probing, a simplified multi-query cross-attention mechanism, enhances evaluation of self-supervised learning models by improving speed, performance, and interpretability.  \t\t\t\t\tAI-generated summary \t\t\t\t As fine-tuning (FT) becomes increasingly impractical at scale, probing is emerging as the preferred evaluation protocol for self-supervised learning (SSL). Yet, the standard linear probing (LP) fails to adequately reflect the potential of models trained with Masked Image Modeling (MIM), due to the distributed nature of patch tokens. This motivates the need for attentive probing, an alternative that uses attention to selectively aggregate patch-level features. Despite its growing adoption, attentive probing remains under-explored, with existing methods suffering from excessive parameterization and poor computational efficiency.   In this work, we revisit attentive probing through the lens of the accuracy-efficiency trade-off. We conduct a systematic study of existing methods, analyzing their mechanisms and benchmarking their performance. We introduce efficient probing (EP), a multi-query cross-attention mechanism that eliminates redundant projections, reduces the number of trainable parameters, and achieves up to a 10times speed-up over conventional multi-head attention. Despite its simplicity, EP outperforms LP and prior attentive probing approaches across seven benchmarks, generalizes well beyond MIM to diverse pre-training paradigms, produces interpretable attention maps, and achieves strong gains in low-shot and layer-wise settings. Code available at https://github.com/billpsomas/efficient-probing.', 'score': 5, 'issue_id': 4280, 'pub_date': '2025-06-11', 'pub_date_card': {'ru': '11 июня', 'en': 'June 11', 'zh': '6月11日'}, 'hash': 'e98eb04f1204de95', 'authors': ['Bill Psomas', 'Dionysis Christopoulos', 'Eirini Baltzi', 'Ioannis Kakogeorgiou', 'Tilemachos Aravanis', 'Nikos Komodakis', 'Konstantinos Karantzalos', 'Yannis Avrithis', 'Giorgos Tolias'], 'affiliations': ['Archimedes, Athena RC', 'Czech Technical University in Prague', 'IACM-FORTH', 'IARAI', 'IIT, NCSR Demokritos', 'National Technical University of Athens', 'University of Crete'], 'pdf_title_img': 'assets/pdf/title_img/2506.10178.jpg', 'data': {'categories': ['#interpretability', '#training', '#optimization', '#benchmark', '#architecture'], 'emoji': '🔍', 'ru': {'title': 'Эффективное зондирование: быстрый и точный метод оценки самообучающихся моделей', 'desc': 'Статья представляет эффективный метод зондирования (efficient probing) для оценки моделей самообучения. Этот метод использует упрощенный механизм кросс-внимания с множественными запросами, что улучшает скорость, производительность и интерпретируемость оценки. Эффективное зондирование превосходит линейное зондирование и предыдущие подходы с использованием внимания на семи эталонных тестах. Метод хорошо обобщается на различные парадигмы предварительного обучения и показывает сильные результаты в условиях малого количества обучающих примеров.'}, 'en': {'title': 'Efficient Probing: Speed and Performance in Self-Supervised Learning', 'desc': 'This paper introduces efficient probing (EP), a new method that enhances the evaluation of self-supervised learning (SSL) models by using a simplified multi-query cross-attention mechanism. Traditional linear probing (LP) does not fully capture the capabilities of models trained with Masked Image Modeling (MIM) due to the complexity of patch tokens. EP addresses this by reducing unnecessary parameters and improving computational efficiency, achieving up to a 10x speed increase compared to standard multi-head attention. The results show that EP not only outperforms LP and previous probing methods but also provides interpretable attention maps and performs well in various settings.'}, 'zh': {'title': '高效探测：提升自监督学习评估的利器', 'desc': '本论文提出了一种高效探测（Efficient Probing）的方法，旨在提升自监督学习模型的评估效率和性能。传统的线性探测方法无法充分反映使用遮挡图像建模训练的模型的潜力，因此需要一种新的关注机制来选择性地聚合特征。高效探测通过多查询交叉注意力机制，减少冗余投影和可训练参数，从而实现更快的计算速度。实验结果表明，高效探测在多个基准测试中表现优于传统方法，并且在低样本和逐层设置中也取得了显著的提升。'}}}, {'id': 'https://huggingface.co/papers/2506.09942', 'title': 'VerIF: Verification Engineering for Reinforcement Learning in\n  Instruction Following', 'url': 'https://huggingface.co/papers/2506.09942', 'abstract': 'VerIF, a hybrid verification method combining rule-based and LLM-based approaches, enhances instruction-following RL with significant performance improvements and generalization.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning with verifiable rewards (RLVR) has become a key technique for enhancing large language models (LLMs), with verification engineering playing a central role. However, best practices for RL in instruction following remain underexplored. In this work, we explore the verification challenge in RL for instruction following and propose VerIF, a verification method that combines rule-based code verification with LLM-based verification from a large reasoning model (e.g., QwQ-32B). To support this approach, we construct a high-quality instruction-following dataset, VerInstruct, containing approximately 22,000 instances with associated verification signals. We apply RL training with VerIF to two models, achieving significant improvements across several representative instruction-following benchmarks. The trained models reach state-of-the-art performance among models of comparable size and generalize well to unseen constraints. We further observe that their general capabilities remain unaffected, suggesting that RL with VerIF can be integrated into existing RL recipes to enhance overall model performance. We have released our datasets, codes, and models to facilitate future research at https://github.com/THU-KEG/VerIF.', 'score': 5, 'issue_id': 4272, 'pub_date': '2025-06-11', 'pub_date_card': {'ru': '11 июня', 'en': 'June 11', 'zh': '6月11日'}, 'hash': '5430c32ec46dccaa', 'authors': ['Hao Peng', 'Yunjia Qi', 'Xiaozhi Wang', 'Bin Xu', 'Lei Hou', 'Juanzi Li'], 'affiliations': ['Department of Computer Science and Technology, Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2506.09942.jpg', 'data': {'categories': ['#dataset', '#optimization', '#rl', '#benchmark', '#open_source', '#reasoning', '#training', '#rlhf'], 'emoji': '🤖', 'ru': {'title': 'VerIF: Гибридная верификация для улучшения RL в следовании инструкциям', 'desc': 'Статья представляет VerIF - гибридный метод верификации, сочетающий подходы на основе правил и больших языковых моделей (LLM) для улучшения обучения с подкреплением (RL) в задаче следования инструкциям. Авторы создали набор данных VerInstruct с около 22 000 примеров и сигналами верификации. Применение RL с VerIF к двум моделям показало значительное улучшение производительности на нескольких эталонных тестах и хорошую обобщаемость. Метод может быть интегрирован в существующие рецепты RL для повышения общей эффективности моделей.'}, 'en': {'title': 'VerIF: Boosting Instruction-Following RL with Hybrid Verification', 'desc': 'This paper introduces VerIF, a novel hybrid verification method that merges rule-based and large language model (LLM) approaches to improve reinforcement learning (RL) in instruction-following tasks. The authors highlight the importance of verification engineering in enhancing LLMs through reinforcement learning with verifiable rewards (RLVR). They present a new dataset, VerInstruct, which contains around 22,000 instruction-following instances with verification signals to support their method. The results show that models trained with VerIF achieve state-of-the-art performance and maintain strong generalization capabilities, indicating that this approach can effectively enhance existing RL frameworks.'}, 'zh': {'title': 'VerIF：提升指令跟随的强化学习新方法', 'desc': '本文提出了一种名为VerIF的混合验证方法，结合了基于规则的验证和基于大型语言模型（LLM）的验证，显著提升了指令跟随的强化学习（RL）性能和泛化能力。我们构建了一个高质量的指令跟随数据集VerInstruct，包含约22,000个实例及其验证信号，以支持这一方法。通过使用VerIF进行RL训练，我们在多个代表性的指令跟随基准上取得了显著的性能提升，训练后的模型在同类模型中达到了最先进的表现，并且对未见约束具有良好的泛化能力。我们的研究表明，VerIF可以与现有的RL方法结合，进一步增强模型的整体性能。'}}}, {'id': 'https://huggingface.co/papers/2506.06952', 'title': 'LaTtE-Flow: Layerwise Timestep-Expert Flow-based Transformer', 'url': 'https://huggingface.co/papers/2506.06952', 'abstract': 'LaTtE-Flow, a new architecture, unifies image understanding and generation with high performance and faster inference by using a Layerwise Timestep Experts flow-based Transformer and Timestep-Conditioned Residual Attention mechanism.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in multimodal foundation models unifying image understanding and generation have opened exciting avenues for tackling a wide range of vision-language tasks within a single framework. Despite progress, existing unified models typically require extensive pretraining and struggle to achieve the same level of performance compared to models dedicated to each task. Additionally, many of these models suffer from slow image generation speeds, limiting their practical deployment in real-time or resource-constrained settings. In this work, we propose Layerwise Timestep-Expert Flow-based Transformer (LaTtE-Flow), a novel and efficient architecture that unifies image understanding and generation within a single multimodal model. LaTtE-Flow builds upon powerful pretrained Vision-Language Models (VLMs) to inherit strong multimodal understanding capabilities, and extends them with a novel Layerwise Timestep Experts flow-based architecture for efficient image generation. LaTtE-Flow distributes the flow-matching process across specialized groups of Transformer layers, each responsible for a distinct subset of timesteps. This design significantly improves sampling efficiency by activating only a small subset of layers at each sampling timestep. To further enhance performance, we propose a Timestep-Conditioned Residual Attention mechanism for efficient information reuse across layers. Experiments demonstrate that LaTtE-Flow achieves strong performance on multimodal understanding tasks, while achieving competitive image generation quality with around 6x faster inference speed compared to recent unified multimodal models.', 'score': 5, 'issue_id': 4288, 'pub_date': '2025-06-08', 'pub_date_card': {'ru': '8 июня', 'en': 'June 8', 'zh': '6月8日'}, 'hash': 'e034c1178056e190', 'authors': ['Ying Shen', 'Zhiyang Xu', 'Jiuhai Chen', 'Shizhe Diao', 'Jiaxin Zhang', 'Yuguang Yao', 'Joy Rimchala', 'Ismini Lourentzou', 'Lifu Huang'], 'affiliations': ['Intuit AI Research', 'Nvidia', 'UC Davis', 'University of Illinois Urbana-Champaign', 'University of Maryland', 'Virginia Tech'], 'pdf_title_img': 'assets/pdf/title_img/2506.06952.jpg', 'data': {'categories': ['#games', '#cv', '#multimodal', '#architecture', '#optimization'], 'emoji': '🖼️', 'ru': {'title': 'Единая архитектура для быстрого понимания и генерации изображений', 'desc': 'LaTtE-Flow - это новая архитектура, объединяющая понимание и генерацию изображений с высокой производительностью и более быстрым выводом. Она использует поточный трансформер с экспертами по временным шагам и слоям, а также механизм остаточного внимания, обусловленного временными шагами. LaTtE-Flow основывается на предобученных мультимодальных моделях для наследования сильных возможностей понимания. Эксперименты показывают, что LaTtE-Flow достигает высокой производительности в задачах мультимодального понимания и конкурентоспособного качества генерации изображений примерно в 6 раз быстрее по сравнению с недавними унифицированными мультимодальными моделями.'}, 'en': {'title': 'Unifying Image Understanding and Generation with Speed and Efficiency', 'desc': 'LaTtE-Flow is a new architecture that combines image understanding and generation into one efficient model. It uses a Layerwise Timestep Experts flow-based Transformer to improve the speed and performance of image generation tasks. By activating only specific layers for different timesteps, it enhances sampling efficiency, making it faster than previous models. Additionally, the Timestep-Conditioned Residual Attention mechanism allows for better information sharing across layers, leading to strong results in multimodal tasks.'}, 'zh': {'title': '高效统一图像理解与生成的LaTtE-Flow架构', 'desc': 'LaTtE-Flow是一种新型架构，旨在统一图像理解和生成，具有高性能和更快的推理速度。它采用了分层时间专家流式Transformer和时间条件残差注意力机制，提升了图像生成的效率。通过将流匹配过程分布到专门的Transformer层组中，LaTtE-Flow在每个采样时间步只激活少量层，从而显著提高了采样效率。实验结果表明，LaTtE-Flow在多模态理解任务上表现出色，同时在图像生成质量上也具有竞争力，推理速度比现有统一多模态模型快约6倍。'}}}, {'id': 'https://huggingface.co/papers/2506.10568', 'title': 'DreamActor-H1: High-Fidelity Human-Product Demonstration Video\n  Generation via Motion-designed Diffusion Transformers', 'url': 'https://huggingface.co/papers/2506.10568', 'abstract': 'A Diffusion Transformer-based framework generates high-fidelity human-product demonstration videos by preserving identities and spatial relationships, using masked cross-attention and structured text encoding.  \t\t\t\t\tAI-generated summary \t\t\t\t In e-commerce and digital marketing, generating high-fidelity human-product demonstration videos is important for effective product presentation. However, most existing frameworks either fail to preserve the identities of both humans and products or lack an understanding of human-product spatial relationships, leading to unrealistic representations and unnatural interactions. To address these challenges, we propose a Diffusion Transformer (DiT)-based framework. Our method simultaneously preserves human identities and product-specific details, such as logos and textures, by injecting paired human-product reference information and utilizing an additional masked cross-attention mechanism. We employ a 3D body mesh template and product bounding boxes to provide precise motion guidance, enabling intuitive alignment of hand gestures with product placements. Additionally, structured text encoding is used to incorporate category-level semantics, enhancing 3D consistency during small rotational changes across frames. Trained on a hybrid dataset with extensive data augmentation strategies, our approach outperforms state-of-the-art techniques in maintaining the identity integrity of both humans and products and generating realistic demonstration motions. Project page: https://submit2025-dream.github.io/DreamActor-H1/.', 'score': 4, 'issue_id': 4281, 'pub_date': '2025-06-12', 'pub_date_card': {'ru': '12 июня', 'en': 'June 12', 'zh': '6月12日'}, 'hash': '83c202462f081ecf', 'authors': ['Lizhen Wang', 'Zhurong Xia', 'Tianshu Hu', 'Pengrui Wang', 'Pengfei Wang', 'Zerong Zheng', 'Ming Zhou'], 'affiliations': ['ByteDance Intelligent Creation'], 'pdf_title_img': 'assets/pdf/title_img/2506.10568.jpg', 'data': {'categories': ['#dataset', '#diffusion', '#video', '#3d'], 'emoji': '🎥', 'ru': {'title': 'Реалистичные видео-демонстрации продуктов с помощью диффузионных трансформеров', 'desc': 'Предложена новая архитектура на основе Diffusion Transformer для генерации высококачественных видео с демонстрацией продуктов. Метод использует маскированное кросс-внимание и структурированное кодирование текста для сохранения идентичности людей и продуктов, а также их пространственных отношений. Применяется шаблон 3D-меша тела и ограничивающие рамки продуктов для точного управления движениями. Подход превосходит современные методы в сохранении идентичности и генерации реалистичных демонстрационных движений.'}, 'en': {'title': 'Realistic Human-Product Videos with Diffusion Transformers', 'desc': 'This paper presents a Diffusion Transformer-based framework designed to create realistic human-product demonstration videos for e-commerce. The framework addresses the common issues of identity preservation and spatial relationships between humans and products by using masked cross-attention and structured text encoding. By incorporating 3D body mesh templates and product bounding boxes, the method ensures accurate motion guidance and alignment of gestures with products. The approach is trained on a diverse dataset, achieving superior results in generating high-fidelity videos that maintain the integrity of both human and product identities.'}, 'zh': {'title': '生成高保真演示视频的创新框架', 'desc': '本文提出了一种基于扩散变换器（Diffusion Transformer, DiT）的框架，用于生成高保真的人类与产品演示视频。该方法通过注入配对的人类与产品参考信息，结合掩蔽交叉注意力机制，能够同时保留人类身份和产品细节。我们使用3D身体网格模板和产品边界框提供精确的运动指导，从而实现手势与产品位置的直观对齐。此外，结构化文本编码用于引入类别级语义，增强了在小旋转变化下的3D一致性。'}}}, {'id': 'https://huggingface.co/papers/2506.09952', 'title': 'UniPre3D: Unified Pre-training of 3D Point Cloud Models with Cross-Modal\n  Gaussian Splatting', 'url': 'https://huggingface.co/papers/2506.09952', 'abstract': "UniPre3D is a unified pre-training method for 3D point clouds and models of any scale, using Gaussian primitives and 2D feature integration for effective performance across object and scene tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t The scale diversity of point cloud data presents significant challenges in developing unified representation learning techniques for 3D vision. Currently, there are few unified 3D models, and no existing pre-training method is equally effective for both object- and scene-level point clouds. In this paper, we introduce UniPre3D, the first unified pre-training method that can be seamlessly applied to point clouds of any scale and 3D models of any architecture. Our approach predicts Gaussian primitives as the pre-training task and employs differentiable Gaussian splatting to render images, enabling precise pixel-level supervision and end-to-end optimization. To further regulate the complexity of the pre-training task and direct the model's focus toward geometric structures, we integrate 2D features from pre-trained image models to incorporate well-established texture knowledge. We validate the universal effectiveness of our proposed method through extensive experiments across a variety of object- and scene-level tasks, using diverse point cloud models as backbones. Code is available at https://github.com/wangzy22/UniPre3D.", 'score': 4, 'issue_id': 4279, 'pub_date': '2025-06-11', 'pub_date_card': {'ru': '11 июня', 'en': 'June 11', 'zh': '6月11日'}, 'hash': 'fc746da54c59982b', 'authors': ['Ziyi Wang', 'Yanran Zhang', 'Jie Zhou', 'Jiwen Lu'], 'affiliations': ['Department of Automation, Tsinghua University, China'], 'pdf_title_img': 'assets/pdf/title_img/2506.09952.jpg', 'data': {'categories': ['#dataset', '#optimization', '#training', '#3d'], 'emoji': '🌐', 'ru': {'title': 'Единый метод предобучения для 3D-данных любого масштаба', 'desc': 'UniPre3D - это унифицированный метод предварительного обучения для облаков точек и 3D-моделей любого масштаба. Он использует гауссовы примитивы и интеграцию 2D-признаков для эффективной работы как с объектами, так и со сценами. Метод применяет дифференцируемое гауссово сплаттинг для рендеринга изображений, что позволяет осуществлять точный попиксельный контроль и сквозную оптимизацию. UniPre3D показал универсальную эффективность в различных задачах на уровне объектов и сцен с использованием разнообразных моделей облаков точек в качестве основы.'}, 'en': {'title': 'UniPre3D: Unified Pre-Training for All 3D Scales', 'desc': "UniPre3D is a novel pre-training method designed for 3D point clouds and models, addressing the challenges posed by varying scales in 3D vision. It uniquely predicts Gaussian primitives as part of its pre-training task and utilizes differentiable Gaussian splatting for accurate image rendering. By integrating 2D features from pre-trained image models, it enhances the model's understanding of geometric structures and textures. Extensive experiments demonstrate its effectiveness across both object and scene-level tasks, making it a versatile solution for 3D representation learning."}, 'zh': {'title': '统一预训练，提升3D视觉表现', 'desc': 'UniPre3D是一种统一的预训练方法，旨在处理各种规模的3D点云和模型。该方法通过预测高斯原语作为预训练任务，并使用可微分的高斯点云渲染技术，实现了精确的像素级监督。为了增强模型对几何结构的关注，UniPre3D还整合了来自预训练图像模型的2D特征，利用已有的纹理知识。通过广泛的实验验证，我们的方法在对象和场景任务中表现出普遍的有效性。'}}}, {'id': 'https://huggingface.co/papers/2506.08060', 'title': 'Eliciting Fine-Tuned Transformer Capabilities via Inference-Time\n  Techniques', 'url': 'https://huggingface.co/papers/2506.08060', 'abstract': 'Transformers can approximate supervised fine-tuning capabilities through in-context learning without altering model parameters, supported by theoretical bounds and practical techniques.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models have transformed natural language processing, yet supervised fine-tuning (SFT) remains computationally intensive. This paper formally proves that capabilities acquired through SFT can be approximated by a base transformer model using inference-time techniques, specifically in-context learning (ICL), without altering model parameters, under idealized assumptions including unbounded computational resources and access to the fine-tuning dataset. We extend these results to practical scenarios with finite context lengths and partial dataset access. For text generation tasks with fixed output length l, datasets of size Oleft( m V{varepsilon^2} log m{delta} right) or, with bounded context, Oleft( l log V{varepsilon^2} log 1{delta} right) suffice to approximate fine-tuned behavior across m contexts within error varepsilon, where V is the vocabulary size and delta is the failure probability. For linear classification, datasets of size Oleft( d{varepsilon} right) or, with fixed context, Oleft( 1{varepsilon^2} log 1{delta} right) are sufficient, where d is the input dimension. Grounded in the Turing completeness of transformers, these results provide a theoretical foundation for resource-efficient deployment of large language models, with practical techniques like retrieval-augmented generation bridging theory to real-world applications.', 'score': 4, 'issue_id': 4272, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 июня', 'en': 'June 9', 'zh': '6月9日'}, 'hash': '5c9cde8c4bcbdc6e', 'authors': ['Asankhaya Sharma'], 'affiliations': ['Patched Codes, Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2506.08060.jpg', 'data': {'categories': ['#optimization', '#transfer_learning', '#rag', '#inference', '#training'], 'emoji': '🧠', 'ru': {'title': 'Трансформеры: обучение в контексте как альтернатива тонкой настройке', 'desc': 'Статья исследует способность трансформеров аппроксимировать возможности обучения с учителем без изменения параметров модели, используя обучение в контексте. Авторы предоставляют теоретические границы и практические методы для этого подхода. Исследование охватывает сценарии с ограниченной длиной контекста и частичным доступом к набору данных. Результаты обосновывают эффективное развертывание больших языковых моделей и связывают теорию с практическими приложениями.'}, 'en': {'title': 'Transformers: Fine-Tuning Efficiency through In-Context Learning', 'desc': "This paper explores how transformers can mimic the performance of supervised fine-tuning (SFT) through a method called in-context learning (ICL) without changing the model's parameters. It provides theoretical proofs that under certain ideal conditions, a base transformer can achieve results similar to those obtained through SFT. The authors extend their findings to practical situations, showing that smaller datasets can still approximate fine-tuned behavior effectively. This research highlights the potential for more efficient use of large language models in real-world applications by leveraging retrieval-augmented generation techniques."}, 'zh': {'title': '变换器模型：高效近似监督微调的未来', 'desc': '本论文探讨了变换器模型如何通过上下文学习（ICL）在不改变模型参数的情况下，近似监督微调（SFT）的能力。研究表明，在理想条件下，变换器模型可以利用推理时的技术来模拟SFT的效果。我们还扩展了这些结果到实际场景，考虑有限的上下文长度和部分数据集访问。通过理论证明，这为大语言模型的资源高效部署提供了基础，结合检索增强生成等实用技术，将理论与实际应用相结合。'}}}, {'id': 'https://huggingface.co/papers/2506.10674', 'title': 'TeleMath: A Benchmark for Large Language Models in Telecom Mathematical\n  Problem Solving', 'url': 'https://huggingface.co/papers/2506.10674', 'abstract': 'A benchmark dataset called TeleMath evaluates Large Language Models in domain-specific mathematical problems within telecommunications, showing that models designed for mathematical reasoning perform better than general-purpose models.  \t\t\t\t\tAI-generated summary \t\t\t\t The increasing adoption of artificial intelligence in telecommunications has raised interest in the capability of Large Language Models (LLMs) to address domain-specific, mathematically intensive tasks. Although recent advancements have improved the performance of LLMs in general mathematical reasoning, their effectiveness within specialized domains, such as signal processing, network optimization, and performance analysis, remains largely unexplored. To address this gap, we introduce TeleMath, the first benchmark dataset specifically designed to evaluate LLM performance in solving mathematical problems with numerical solutions in the telecommunications domain. Comprising 500 question-answer (QnA) pairs, TeleMath covers a wide spectrum of topics in the telecommunications field. This paper outlines the proposed QnAs generation pipeline, starting from a selected seed of problems crafted by Subject Matter Experts. The evaluation of a wide range of open-source LLMs reveals that best performance on TeleMath is achieved by recent models explicitly designed for mathematical or logical reasoning. In contrast, general-purpose models, even those with a large number of parameters, often struggle with these challenges. We have released the dataset and the evaluation code to ease result reproducibility and support future research.', 'score': 3, 'issue_id': 4285, 'pub_date': '2025-06-12', 'pub_date_card': {'ru': '12 июня', 'en': 'June 12', 'zh': '6月12日'}, 'hash': 'fc14281fcee53b0d', 'authors': ['Vincenzo Colle', 'Mohamed Sana', 'Nicola Piovesan', 'Antonio De Domenico', 'Fadhel Ayed', 'Merouane Debbah'], 'affiliations': ['Khalifa University of Science and Technology, Abu Dhabi, UAE', 'Paris Research Center, Huawei Technologies, Boulogne-Billancourt, France', 'Università degli Studi di Cassino del Lazio Meridionale, Cassino, Italy'], 'pdf_title_img': 'assets/pdf/title_img/2506.10674.jpg', 'data': {'categories': ['#reasoning', '#dataset', '#open_source', '#math', '#benchmark'], 'emoji': '📡', 'ru': {'title': 'TeleMath: Измеряем математические способности ИИ в телекоммуникациях', 'desc': 'Представлен новый набор данных TeleMath для оценки способности больших языковых моделей (LLM) решать математические задачи в области телекоммуникаций. Набор содержит 500 пар вопрос-ответ, охватывающих широкий спектр тем в телекоммуникационной сфере. Исследование показало, что модели, специально разработанные для математических рассуждений, превосходят модели общего назначения в решении этих задач. Авторы опубликовали набор данных и код для оценки, чтобы облегчить воспроизводимость результатов и поддержать дальнейшие исследования.'}, 'en': {'title': 'TeleMath: Evaluating LLMs in Telecommunications Mathematics', 'desc': 'The paper introduces TeleMath, a benchmark dataset aimed at assessing Large Language Models (LLMs) on mathematical problems specific to the telecommunications sector. It highlights that LLMs tailored for mathematical reasoning outperform general-purpose models when tackling domain-specific tasks. The dataset consists of 500 question-answer pairs covering various telecommunications topics, created with input from Subject Matter Experts. The findings suggest that specialized models are more effective in solving these complex mathematical challenges compared to their general counterparts.'}, 'zh': {'title': '专注电信数学，提升模型表现', 'desc': '本文介绍了一个名为TeleMath的基准数据集，旨在评估大型语言模型（LLMs）在电信领域特定数学问题上的表现。研究表明，专为数学推理设计的模型在解决这些问题时表现优于通用模型。TeleMath包含500个问答对，涵盖电信领域的广泛主题，填补了LLMs在专业领域应用的空白。我们还发布了数据集和评估代码，以支持未来的研究和结果的可重复性。'}}}, {'id': 'https://huggingface.co/papers/2506.08234', 'title': 'Compound AI Systems Optimization: A Survey of Methods, Challenges, and\n  Future Directions', 'url': 'https://huggingface.co/papers/2506.08234', 'abstract': 'Recent advancements in optimizing compound AI systems highlight challenges in integrating various components, with an emphasis on natural language feedback methods for non-differentiable systems.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in large language models (LLMs) and AI systems have led to a paradigm shift in the design and optimization of complex AI workflows. By integrating multiple components, compound AI systems have become increasingly adept at performing sophisticated tasks. However, as these systems grow in complexity, new challenges arise in optimizing not only individual components but also their interactions. While traditional optimization methods such as supervised fine-tuning (SFT) and reinforcement learning (RL) remain foundational, the rise of natural language feedback introduces promising new approaches, especially for optimizing non-differentiable systems. This paper provides a systematic review of recent progress in optimizing compound AI systems, encompassing both numerical and language-based techniques. We formalize the notion of compound AI system optimization, classify existing methods along several key dimensions, and highlight open research challenges and future directions in this rapidly evolving field. A list of surveyed papers is publicly available at https://github.com/MiuLab/AISysOpt-Survey.', 'score': 3, 'issue_id': 4279, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 июня', 'en': 'June 9', 'zh': '6月9日'}, 'hash': 'a098935231ad146d', 'authors': ['Yu-Ang Lee', 'Guan-Ting Yi', 'Mei-Yi Liu', 'Jui-Chao Lu', 'Guan-Bo Yang', 'Yun-Nung Chen'], 'affiliations': ['National Taiwan University, Taipei, Taiwan'], 'pdf_title_img': 'assets/pdf/title_img/2506.08234.jpg', 'data': {'categories': ['#survey', '#rlhf', '#training', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Новые горизонты оптимизации сложных систем ИИ', 'desc': 'Статья рассматривает последние достижения в оптимизации сложных систем искусственного интеллекта. Авторы анализируют проблемы интеграции различных компонентов, уделяя особое внимание методам обратной связи на естественном языке для недифференцируемых систем. В работе представлен систематический обзор современных подходов к оптимизации составных систем ИИ, включая как численные, так и языковые методы. Исследователи формализуют понятие оптимизации составных систем ИИ, классифицируют существующие методы и выделяют открытые проблемы в этой быстро развивающейся области.'}, 'en': {'title': 'Optimizing Complex AI Systems with Natural Language Feedback', 'desc': 'This paper discusses the recent progress in optimizing compound AI systems, which are complex systems made up of multiple interacting components. It highlights the challenges faced in integrating these components, particularly when using natural language feedback methods for systems that are not easily differentiable. The authors review traditional optimization techniques like supervised fine-tuning and reinforcement learning, while also exploring new approaches that leverage natural language. They aim to formalize the concept of compound AI system optimization and identify future research directions in this evolving field.'}, 'zh': {'title': '优化复合AI系统的新方法探索', 'desc': '最近在复合人工智能系统优化方面的进展突显了整合各种组件的挑战，特别是在非可微系统中使用自然语言反馈方法。随着大型语言模型和人工智能系统的发展，复合人工智能系统在执行复杂任务方面变得更加高效。尽管传统的优化方法如监督微调和强化学习仍然是基础，但自然语言反馈的兴起为优化非可微系统提供了新的可能性。本文系统回顾了复合人工智能系统优化的最新进展，分类现有方法，并强调了该领域的开放研究挑战和未来方向。'}}}, {'id': 'https://huggingface.co/papers/2506.06950', 'title': 'What Makes a Good Natural Language Prompt?', 'url': 'https://huggingface.co/papers/2506.06950', 'abstract': 'A framework for evaluating and optimizing natural language prompts in large language models is proposed, revealing correlations between prompt properties and their impact on reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t As large language models (LLMs) have progressed towards more human-like and human--AI communications have become prevalent, prompting has emerged as a decisive component. However, there is limited conceptual consensus on what exactly quantifies natural language prompts. We attempt to address this question by conducting a meta-analysis surveying more than 150 prompting-related papers from leading NLP and AI conferences from 2022 to 2025 and blogs. We propose a property- and human-centric framework for evaluating prompt quality, encompassing 21 properties categorized into six dimensions. We then examine how existing studies assess their impact on LLMs, revealing their imbalanced support across models and tasks, and substantial research gaps. Further, we analyze correlations among properties in high-quality natural language prompts, deriving prompting recommendations. We then empirically explore multi-property prompt enhancements in reasoning tasks, observing that single-property enhancements often have the greatest impact. Finally, we discover that instruction-tuning on property-enhanced prompts can result in better reasoning models. Our findings establish a foundation for property-centric prompt evaluation and optimization, bridging the gaps between human--AI communication and opening new prompting research directions.', 'score': 3, 'issue_id': 4275, 'pub_date': '2025-06-07', 'pub_date_card': {'ru': '7 июня', 'en': 'June 7', 'zh': '6月7日'}, 'hash': '265555e63c6771ba', 'authors': ['Do Xuan Long', 'Duy Dinh', 'Ngoc-Hai Nguyen', 'Kenji Kawaguchi', 'Nancy F. Chen', 'Shafiq Joty', 'Min-Yen Kan'], 'affiliations': ['Institute for Infocomm Research (I2R), A*STAR', 'National University of Singapore', 'Salesforce AI Research'], 'pdf_title_img': 'assets/pdf/title_img/2506.06950.jpg', 'data': {'categories': ['#optimization', '#benchmark', '#survey', '#reasoning', '#training'], 'emoji': '🧠', 'ru': {'title': 'Оптимизация промптов для улучшения рассуждений языковых моделей', 'desc': 'Предложена система оценки и оптимизации естественно-языковых промптов для больших языковых моделей. Авторы проанализировали более 150 статей и выделили 21 свойство промптов, сгруппированных в 6 категорий. Исследование показало корреляции между свойствами качественных промптов и их влиянием на задачи рассуждения. Обнаружено, что обучение моделей на улучшенных промптах повышает их способности к рассуждению.'}, 'en': {'title': 'Optimizing Prompts for Smarter AI Reasoning', 'desc': 'This paper presents a framework for evaluating and optimizing natural language prompts used in large language models (LLMs). It identifies 21 properties of prompts, organized into six dimensions, that influence their effectiveness in reasoning tasks. The authors conducted a meta-analysis of over 150 studies to highlight the inconsistencies in how prompt quality is assessed across different models and tasks. Their findings suggest that enhancing prompts based on specific properties can significantly improve LLM performance, particularly through instruction-tuning techniques.'}, 'zh': {'title': '优化提示，提升推理能力！', 'desc': '本文提出了一个评估和优化大型语言模型中自然语言提示的框架，揭示了提示属性与推理任务之间的相关性。我们通过对2022至2025年间150多篇与提示相关的论文进行元分析，探讨了自然语言提示的量化标准。该框架包含21个属性，分为六个维度，旨在评估提示质量。研究发现，单一属性的增强对推理任务的影响最大，而基于属性增强的指令调优可以提升推理模型的表现。'}}}, {'id': 'https://huggingface.co/papers/2506.10911', 'title': 'NoLoCo: No-all-reduce Low Communication Training Method for Large Models', 'url': 'https://huggingface.co/papers/2506.10911', 'abstract': 'NoLoCo is a novel optimization method that eliminates explicit parameter synchronization and reduces communication overhead during the training of large language models, achieving faster convergence rates and reduced idling time compared to existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Training large language models is generally done via optimization methods on clusters containing tens of thousands of accelerators, communicating over a high-bandwidth interconnect. Scaling up these clusters is expensive and can become impractical, imposing limits on the size of models that can be trained. Several recent studies have proposed training methods that are less communication intensive, avoiding the need for a highly connected compute cluster. These state-of-the-art low communication training methods still employ a synchronization step for model parameters, which, when performed over all model replicas, can become costly on a low-bandwidth network.   In this work, we propose a novel optimization method, NoLoCo, that does not explicitly synchronize all model parameters during training and, as a result, does not require any collective communication. NoLoCo implicitly synchronizes model weights via a novel variant of the Nesterov momentum optimizer by partially averaging model weights with a randomly selected other one. We provide both a theoretical convergence analysis for our proposed optimizer as well as empirical results from language model training.   We benchmark NoLoCo on a wide range of accelerator counts and model sizes, between 125M to 6.8B parameters. Our method requires significantly less communication overhead than fully sharded data parallel training or even widely used low communication training method, DiLoCo. The synchronization step itself is estimated to be one magnitude faster than the all-reduce used in DiLoCo for few hundred accelerators training over the internet. We also do not have any global blocking communication that reduces accelerator idling time. Compared to DiLoCo, we also observe up to 4% faster convergence rate with wide range of model sizes and accelerator counts.', 'score': 2, 'issue_id': 4288, 'pub_date': '2025-06-12', 'pub_date_card': {'ru': '12 июня', 'en': 'June 12', 'zh': '6月12日'}, 'hash': 'f39f533144e06990', 'authors': ['Jari Kolehmainen', 'Nikolay Blagoev', 'John Donaghy', 'Oğuzhan Ersoy', 'Christopher Nies'], 'affiliations': ['Gensyn'], 'pdf_title_img': 'assets/pdf/title_img/2506.10911.jpg', 'data': {'categories': ['#benchmark', '#training', '#optimization'], 'emoji': '🚀', 'ru': {'title': 'NoLoCo: Эффективное обучение больших языковых моделей без явной синхронизации', 'desc': 'NoLoCo - это новый метод оптимизации для обучения больших языковых моделей, который устраняет необходимость явной синхронизации параметров и снижает накладные расходы на коммуникацию. Метод использует вариант оптимизатора Нестерова, частично усредняя веса модели со случайно выбранной другой моделью. NoLoCo демонстрирует более быструю сходимость и меньшее время простоя по сравнению с существующими методами. Эксперименты показали эффективность NoLoCo для моделей размером от 125 млн до 6,8 млрд параметров на различном количестве ускорителей.'}, 'en': {'title': 'NoLoCo: Faster Training with Less Communication!', 'desc': 'NoLoCo is an innovative optimization technique designed for training large language models without the need for explicit parameter synchronization. By avoiding collective communication, it significantly reduces communication overhead and minimizes idling time among accelerators. The method utilizes a modified Nesterov momentum optimizer that implicitly synchronizes model weights through partial averaging with randomly selected weights. Empirical results demonstrate that NoLoCo achieves faster convergence rates and is more efficient than existing low communication training methods, such as DiLoCo.'}, 'zh': {'title': 'NoLoCo：高效的无同步优化方法', 'desc': 'NoLoCo是一种新颖的优化方法，旨在消除显式的参数同步，从而减少在大型语言模型训练过程中的通信开销。与现有方法相比，NoLoCo实现了更快的收敛速度和更少的空闲时间。该方法通过一种新型的Nesterov动量优化器变体，隐式地同步模型权重，部分平均与随机选择的其他权重。我们的实验结果表明，NoLoCo在不同的加速器数量和模型规模下，通信开销显著低于传统的全分片数据并行训练方法。'}}}, {'id': 'https://huggingface.co/papers/2506.10036', 'title': 'Token Perturbation Guidance for Diffusion Models', 'url': 'https://huggingface.co/papers/2506.10036', 'abstract': 'Token Perturbation Guidance (TPG) enhances diffusion model generation quality without training, by perturbing intermediate token representations, achieving CFG-like performance and improving unconditional generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Classifier-free guidance (CFG) has become an essential component of modern diffusion models to enhance both generation quality and alignment with input conditions. However, CFG requires specific training procedures and is limited to conditional generation. To address these limitations, we propose Token Perturbation Guidance (TPG), a novel method that applies perturbation matrices directly to intermediate token representations within the diffusion network. TPG employs a norm-preserving shuffling operation to provide effective and stable guidance signals that improve generation quality without architectural changes. As a result, TPG is training-free and agnostic to input conditions, making it readily applicable to both conditional and unconditional generation. We further analyze the guidance term provided by TPG and show that its effect on sampling more closely resembles CFG compared to existing training-free guidance techniques. Extensive experiments on SDXL and Stable Diffusion 2.1 show that TPG achieves nearly a 2times improvement in FID for unconditional generation over the SDXL baseline, while closely matching CFG in prompt alignment. These results establish TPG as a general, condition-agnostic guidance method that brings CFG-like benefits to a broader class of diffusion models. The code is available at https://github.com/TaatiTeam/Token-Perturbation-Guidance', 'score': 2, 'issue_id': 4277, 'pub_date': '2025-06-10', 'pub_date_card': {'ru': '10 июня', 'en': 'June 10', 'zh': '6月10日'}, 'hash': '3637011ee12cd77a', 'authors': ['Javad Rajabi', 'Soroush Mehraban', 'Seyedmorteza Sadat', 'Babak Taati'], 'affiliations': ['ETH Zürich', 'KITE Research Institute', 'University of Toronto', 'Vector Institute for Artificial Intelligence'], 'pdf_title_img': 'assets/pdf/title_img/2506.10036.jpg', 'data': {'categories': ['#optimization', '#training', '#diffusion', '#cv'], 'emoji': '🔀', 'ru': {'title': 'Улучшение генерации диффузионных моделей без переобучения', 'desc': 'Статья представляет новый метод под названием Token Perturbation Guidance (TPG) для улучшения качества генерации диффузионных моделей. TPG применяет матрицы возмущения к промежуточным токенным представлениям в сети диффузии, что позволяет улучшить качество генерации без изменения архитектуры или дополнительного обучения. Метод эффективен как для условной, так и для безусловной генерации, достигая производительности, сравнимой с Classifier-free guidance (CFG). Эксперименты на моделях SDXL и Stable Diffusion 2.1 показывают значительное улучшение метрики FID для безусловной генерации.'}, 'en': {'title': 'Enhancing Diffusion Models with Token Perturbation Guidance', 'desc': 'Token Perturbation Guidance (TPG) is a new method that improves the quality of images generated by diffusion models without needing additional training. It works by applying perturbation matrices to the intermediate token representations, which helps guide the generation process effectively. Unlike Classifier-free Guidance (CFG), TPG does not require specific training and can be used for both conditional and unconditional generation tasks. Experiments show that TPG significantly enhances generation quality, achieving nearly double the improvement in FID scores compared to existing methods, while maintaining alignment with prompts.'}, 'zh': {'title': '令牌扰动引导：无训练的生成质量提升', 'desc': '本文提出了一种新的方法，称为令牌扰动引导（TPG），旨在提高扩散模型的生成质量，而无需进行训练。TPG通过对扩散网络中间令牌表示施加扰动矩阵，提供有效且稳定的引导信号，从而改善生成效果。与传统的无分类器引导方法相比，TPG在无条件生成任务中表现出接近分类器无关引导的性能。实验结果表明，TPG在生成质量上显著优于现有基线，且适用于条件和无条件生成。'}}}, {'id': 'https://huggingface.co/papers/2506.07795', 'title': 'LLM Unlearning Should Be Form-Independent', 'url': 'https://huggingface.co/papers/2506.07795', 'abstract': "Form-Dependent Bias limits the effectiveness of LLM unlearning across different knowledge expressions, and Rank-one Concept Redirection (ROCR) is proposed as a form-independent solution that enhances unlearning efficacy.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Model (LLM) unlearning aims to erase or suppress undesirable knowledge within the model, offering promise for controlling harmful or private information to prevent misuse. However, recent studies highlight its limited efficacy in real-world scenarios, hindering practical adoption. In this study, we identify a pervasive issue underlying many downstream failures: the effectiveness of existing unlearning methods heavily depends on the form of training samples and frequently fails to generalize to alternate expressions of the same knowledge. We formally characterize this problem as Form-Dependent Bias and systematically investigate its specific manifestation patterns across various downstream tasks. To quantify its prevalence and support future research, we introduce ORT, a novel benchmark designed to evaluate the robustness of unlearning methods against variations in knowledge expression. Results reveal that Form-Dependent Bias is both widespread and severe among current techniques.   We argue that LLM unlearning should be form-independent to address the endless forms of downstream tasks encountered in real-world security-critical scenarios. Towards this goal, we introduce Rank-one Concept Redirection (ROCR), a novel training-free method, as a promising solution path. ROCR performs unlearning by targeting the invariants in downstream tasks, specifically the activated dangerous concepts. It is capable of modifying model parameters within seconds to redirect the model's perception of a specific unlearning target concept to another harmless concept. Extensive experiments demonstrate that ROCR significantly improves unlearning effectiveness compared to traditional methods while generating highly natural outputs.", 'score': 2, 'issue_id': 4279, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 июня', 'en': 'June 9', 'zh': '6月9日'}, 'hash': '7314028832938fb2', 'authors': ['Xiaotian Ye', 'Mengqi Zhang', 'Shu Wu'], 'affiliations': ['New Laboratory of Pattern Recognition (NLPR), State Key Laboratory of Multimodal Artificial Intelligence Systems (MAIS), Institute of Automation, Chinese Academy of Sciences', 'School of Computer Science, Beijing University of Posts and Telecommunications', 'Shandong University'], 'pdf_title_img': 'assets/pdf/title_img/2506.07795.jpg', 'data': {'categories': ['#security', '#rlhf', '#ethics', '#benchmark', '#hallucinations', '#training'], 'emoji': '🧠', 'ru': {'title': 'Преодоление зависимости от формы в разобучении языковых моделей', 'desc': 'Исследование выявило проблему зависимости эффективности методов разобучения больших языковых моделей от формы обучающих примеров. Это явление, названное Form-Dependent Bias, ограничивает возможности подавления нежелательных знаний в моделях. Для решения этой проблемы авторы предлагают новый метод Rank-one Concept Redirection (ROCR), который не зависит от формы представления знаний. ROCR перенаправляет восприятие модели от опасных концепций к безвредным, значительно улучшая эффективность разобучения по сравнению с традиционными методами.'}, 'en': {'title': 'Unlearning Without Limits: ROCR for Form-Independent Knowledge Management', 'desc': 'This paper addresses the challenge of unlearning in Large Language Models (LLMs), specifically focusing on the limitations caused by Form-Dependent Bias, which affects the effectiveness of unlearning methods across different knowledge expressions. The authors propose a new method called Rank-one Concept Redirection (ROCR) that aims to enhance unlearning efficacy by being form-independent, allowing it to generalize better across various tasks. They introduce a benchmark called ORT to evaluate the robustness of unlearning techniques against different expressions of knowledge. Experimental results show that ROCR outperforms traditional unlearning methods, providing a more effective and efficient way to manage harmful or private information in LLMs.'}, 'zh': {'title': '形式无关的去学习新方法', 'desc': '本文探讨了大型语言模型（LLM）在去除不良知识时面临的挑战，特别是形式依赖偏差的问题。研究表明，现有的去学习方法在不同知识表达形式下的有效性有限，导致其在实际应用中的效果不佳。为了解决这一问题，提出了一种新的方法——Rank-one Concept Redirection（ROCR），旨在实现形式无关的去学习。实验结果显示，ROCR在去学习的有效性上显著优于传统方法，同时生成的输出也更加自然。'}}}, {'id': 'https://huggingface.co/papers/2506.06694', 'title': 'Breaking Data Silos: Towards Open and Scalable Mobility Foundation\n  Models via Generative Continual Learning', 'url': 'https://huggingface.co/papers/2506.06694', 'abstract': 'MoveGCL is a privacy-preserving framework using generative continual learning and a Mixture-of-Experts Transformer for training mobility foundation models without sharing raw data.  \t\t\t\t\tAI-generated summary \t\t\t\t Foundation models have revolutionized fields such as natural language processing and computer vision by enabling general-purpose learning across diverse tasks and datasets. However, building analogous models for human mobility remains challenging due to the privacy-sensitive nature of mobility data and the resulting data silos across institutions. To bridge this gap, we propose MoveGCL, a scalable and privacy-preserving framework for training mobility foundation models via generative continual learning. Without sharing raw data, MoveGCL enables decentralized and progressive model evolution by replaying synthetic trajectories generated from a frozen teacher model, and reinforces knowledge retention through a tailored distillation strategy that mitigates catastrophic forgetting. To address the heterogeneity of mobility patterns, MoveGCL incorporates a Mixture-of-Experts Transformer with a mobility-aware expert routing mechanism, and employs a layer-wise progressive adaptation strategy to stabilize continual updates. Experiments on six real-world urban datasets demonstrate that MoveGCL achieves performance comparable to joint training and significantly outperforms federated learning baselines, while offering strong privacy protection. MoveGCL marks a crucial step toward unlocking foundation models for mobility, offering a practical blueprint for open, scalable, and privacy-preserving model development in the era of foundation models.', 'score': 2, 'issue_id': 4275, 'pub_date': '2025-06-07', 'pub_date_card': {'ru': '7 июня', 'en': 'June 7', 'zh': '6月7日'}, 'hash': 'f878d8e46d64a439', 'authors': ['Yuan Yuan', 'Yukun Liu', 'Chonghua Han', 'Jie Feng', 'Yong Li'], 'affiliations': ['Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2506.06694.jpg', 'data': {'categories': ['#dataset', '#synthetic', '#data', '#training', '#open_source', '#architecture'], 'emoji': '🚶', 'ru': {'title': 'Защита приватности при обучении моделей мобильности', 'desc': 'MoveGCL - это фреймворк для обучения фундаментальных моделей мобильности с сохранением конфиденциальности данных. Он использует генеративное непрерывное обучение и трансформер с механизмом смеси экспертов для работы с разнородными паттернами мобильности. MoveGCL позволяет развивать модель децентрализованно, воспроизводя синтетические траектории из замороженной учительской модели. Эксперименты показали, что MoveGCL достигает результатов, сравнимых с совместным обучением, значительно превосходя базовые методы федеративного обучения.'}, 'en': {'title': 'Unlocking Mobility Models with Privacy-Preserving Learning', 'desc': 'MoveGCL is a framework designed to train mobility foundation models while ensuring data privacy. It uses generative continual learning to create synthetic data from a teacher model, allowing for model updates without sharing sensitive raw data. The framework employs a Mixture-of-Experts Transformer to adapt to various mobility patterns and includes strategies to prevent catastrophic forgetting during training. Experiments show that MoveGCL performs well compared to traditional methods while maintaining strong privacy protections.'}, 'zh': {'title': 'MoveGCL：隐私保护的移动基础模型训练框架', 'desc': 'MoveGCL是一个保护隐私的框架，利用生成持续学习和混合专家Transformer来训练移动基础模型，而无需共享原始数据。该框架通过重放从冻结教师模型生成的合成轨迹，实现去中心化和渐进式模型演化，并通过定制的蒸馏策略增强知识保留，减少灾难性遗忘。为了应对移动模式的异质性，MoveGCL结合了移动感知的专家路由机制和逐层渐进适应策略，以稳定持续更新。实验结果表明，MoveGCL在六个真实城市数据集上的表现与联合训练相当，显著优于联邦学习基线，同时提供强有力的隐私保护。'}}}, {'id': 'https://huggingface.co/papers/2506.10737', 'title': 'TaxoAdapt: Aligning LLM-Based Multidimensional Taxonomy Construction to\n  Evolving Research Corpora', 'url': 'https://huggingface.co/papers/2506.10737', 'abstract': "TaxoAdapt dynamically adapts an LLM-generated taxonomy for scientific literature across multiple dimensions, improving granularity and coherence compared to existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t The rapid evolution of scientific fields introduces challenges in organizing and retrieving scientific literature. While expert-curated taxonomies have traditionally addressed this need, the process is time-consuming and expensive. Furthermore, recent automatic taxonomy construction methods either (1) over-rely on a specific corpus, sacrificing generalizability, or (2) depend heavily on the general knowledge of large language models (LLMs) contained within their pre-training datasets, often overlooking the dynamic nature of evolving scientific domains. Additionally, these approaches fail to account for the multi-faceted nature of scientific literature, where a single research paper may contribute to multiple dimensions (e.g., methodology, new tasks, evaluation metrics, benchmarks). To address these gaps, we propose TaxoAdapt, a framework that dynamically adapts an LLM-generated taxonomy to a given corpus across multiple dimensions. TaxoAdapt performs iterative hierarchical classification, expanding both the taxonomy width and depth based on corpus' topical distribution. We demonstrate its state-of-the-art performance across a diverse set of computer science conferences over the years to showcase its ability to structure and capture the evolution of scientific fields. As a multidimensional method, TaxoAdapt generates taxonomies that are 26.51% more granularity-preserving and 50.41% more coherent than the most competitive baselines judged by LLMs.", 'score': 1, 'issue_id': 4283, 'pub_date': '2025-06-12', 'pub_date_card': {'ru': '12 июня', 'en': 'June 12', 'zh': '6月12日'}, 'hash': 'ba81aecb13322a55', 'authors': ['Priyanka Kargupta', 'Nan Zhang', 'Yunyi Zhang', 'Rui Zhang', 'Prasenjit Mitra', 'Jiawei Han'], 'affiliations': ['The Pennsylvania State University', 'University of Illinois at Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2506.10737.jpg', 'data': {'categories': ['#science', '#multimodal', '#dataset'], 'emoji': '🌳', 'ru': {'title': 'Динамическая адаптация таксономий для эволюционирующей научной литературы', 'desc': 'TaxoAdapt - это фреймворк для динамической адаптации таксономии, сгенерированной большой языковой моделью (LLM), к заданному корпусу научной литературы по нескольким измерениям. Он использует итеративную иерархическую классификацию для расширения ширины и глубины таксономии на основе тематического распределения корпуса. TaxoAdapt демонстрирует современные результаты на разнообразных компьютерных конференциях, отражая эволюцию научных областей. По сравнению с конкурентными методами, таксономии TaxoAdapt на 26.51% лучше сохраняют гранулярность и на 50.41% более согласованны по оценке LLM.'}, 'en': {'title': 'Dynamic Taxonomy Adaptation for Evolving Science', 'desc': "TaxoAdapt is a novel framework that enhances the organization of scientific literature by dynamically adapting taxonomies generated by large language models (LLMs). It addresses the limitations of traditional expert-curated taxonomies and existing automatic methods, which often lack generalizability and fail to capture the evolving nature of scientific fields. By employing iterative hierarchical classification, TaxoAdapt expands the taxonomy's width and depth based on the topical distribution of the corpus, allowing for a more nuanced representation of research contributions. The results show that TaxoAdapt achieves significantly higher granularity and coherence compared to leading methods, making it a powerful tool for structuring scientific knowledge."}, 'zh': {'title': '动态适应科学文献分类法的创新方法', 'desc': 'TaxoAdapt 是一种动态调整大型语言模型（LLM）生成的分类法的方法，旨在提高科学文献的组织和检索效率。它通过迭代的层次分类，基于文献的主题分布扩展分类法的宽度和深度，从而更好地适应快速发展的科学领域。与传统的专家策划分类法相比，TaxoAdapt 在细粒度和一致性方面表现出色，分别提高了 26.51% 和 50.41%。该方法能够处理科学文献的多维特性，使得单篇研究论文可以在多个维度上进行贡献。'}}}, {'id': 'https://huggingface.co/papers/2506.10728', 'title': 'Beyond True or False: Retrieval-Augmented Hierarchical Analysis of\n  Nuanced Claims', 'url': 'https://huggingface.co/papers/2506.10728', 'abstract': 'ClaimSpect is a retrieval-augmented generation-based framework that constructs a hierarchical structure of aspects for claims, enriching them with diverse perspectives from a corpus.  \t\t\t\t\tAI-generated summary \t\t\t\t Claims made by individuals or entities are oftentimes nuanced and cannot be clearly labeled as entirely "true" or "false" -- as is frequently the case with scientific and political claims. However, a claim (e.g., "vaccine A is better than vaccine B") can be dissected into its integral aspects and sub-aspects (e.g., efficacy, safety, distribution), which are individually easier to validate. This enables a more comprehensive, structured response that provides a well-rounded perspective on a given problem while also allowing the reader to prioritize specific angles of interest within the claim (e.g., safety towards children). Thus, we propose ClaimSpect, a retrieval-augmented generation-based framework for automatically constructing a hierarchy of aspects typically considered when addressing a claim and enriching them with corpus-specific perspectives. This structure hierarchically partitions an input corpus to retrieve relevant segments, which assist in discovering new sub-aspects. Moreover, these segments enable the discovery of varying perspectives towards an aspect of the claim (e.g., support, neutral, or oppose) and their respective prevalence (e.g., "how many biomedical papers believe vaccine A is more transportable than B?"). We apply ClaimSpect to a wide variety of real-world scientific and political claims featured in our constructed dataset, showcasing its robustness and accuracy in deconstructing a nuanced claim and representing perspectives within a corpus. Through real-world case studies and human evaluation, we validate its effectiveness over multiple baselines.', 'score': 1, 'issue_id': 4283, 'pub_date': '2025-06-12', 'pub_date_card': {'ru': '12 июня', 'en': 'June 12', 'zh': '6月12日'}, 'hash': 'f5822dc8f4d101b9', 'authors': ['Priyanka Kargupta', 'Runchu Tian', 'Jiawei Han'], 'affiliations': ['Department of Computer Science, University of Illinois at Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2506.10728.jpg', 'data': {'categories': ['#science', '#reasoning', '#rag', '#dataset'], 'emoji': '🔍', 'ru': {'title': 'Разложение сложных утверждений на структурированные аспекты', 'desc': 'ClaimSpect - это фреймворк, основанный на генерации с усилением извлечения, который создает иерархическую структуру аспектов для утверждений, обогащая их разнообразными перспективами из корпуса. Он использует рекурсивный подход для разбиения сложных утверждений на более простые аспекты и подаспекты, которые легче проверить. ClaimSpect применяет методы обработки естественного языка и машинного обучения для автоматического построения иерархии аспектов и извлечения релевантных сегментов из корпуса. Система позволяет получить более полное и структурированное представление о сложных утверждениях, особенно в научной и политической сферах.'}, 'en': {'title': 'Deconstructing Claims for Clearer Perspectives', 'desc': 'ClaimSpect is a framework that uses retrieval-augmented generation to break down complex claims into a structured hierarchy of aspects and sub-aspects. This approach allows for a nuanced analysis of claims, such as those in science and politics, by focusing on individual components like efficacy and safety. By retrieving relevant information from a corpus, ClaimSpect enriches these aspects with diverse perspectives, helping users understand varying viewpoints and their prevalence. The framework has been tested on real-world claims, demonstrating its ability to provide comprehensive and accurate insights into complex issues.'}, 'zh': {'title': 'ClaimSpect：解构声明的智能框架', 'desc': 'ClaimSpect 是一个基于检索增强生成的框架，旨在为声明构建层次结构的方面，并从语料库中丰富多样的视角。声明通常是复杂的，不能简单地标记为“真”或“假”，但可以将其分解为更易验证的基本方面。该框架通过分层划分输入语料库，检索相关片段，帮助发现新的子方面和不同的观点。我们在多个真实的科学和政治声明中应用 ClaimSpect，展示了其在解构复杂声明和表示语料库中观点的有效性。'}}}, {'id': 'https://huggingface.co/papers/2506.08373', 'title': 'Draft-based Approximate Inference for LLMs', 'url': 'https://huggingface.co/papers/2506.08373', 'abstract': "A new framework using draft models enhances approximate inference for long-context LLMs by better predicting token and key-value pair importance, improving accuracy while maintaining memory and compute efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Optimizing inference for long-context Large Language Models (LLMs) is increasingly important due to the quadratic compute and linear memory complexity of Transformers. Existing approximation methods, such as key-value (KV) cache dropping, sparse attention, and prompt compression, typically rely on rough predictions of token or KV pair importance. We propose a novel framework for approximate LLM inference that leverages small draft models to more accurately predict the importance of tokens and KV pairs. Specifically, we introduce two instantiations of our proposed framework: (i) SpecKV, which leverages a draft output to accurately assess the importance of each KV pair for more effective KV cache dropping, and (ii) SpecPC, which uses the draft model's attention activations to identify and discard unimportant prompt tokens. To the best of our knowledge, this is the first work to use draft models for approximate LLM inference acceleration, extending their utility beyond traditional lossless speculative decoding. We motivate our methods with theoretical and empirical analyses, and show a strong correlation between the attention patterns of draft and target models. Extensive experiments on long-context benchmarks show that our methods consistently achieve higher accuracy than existing baselines, while preserving the same improvements in memory usage, latency, and throughput. Our code is available at https://github.com/furiosa-ai/draft-based-approx-llm.", 'score': 1, 'issue_id': 4272, 'pub_date': '2025-06-10', 'pub_date_card': {'ru': '10 июня', 'en': 'June 10', 'zh': '6月10日'}, 'hash': '02a9a3f798ba1509', 'authors': ['Kevin Galim', 'Ethan Ewer', 'Wonjun Kang', 'Minjae Lee', 'Hyung Il Koo', 'Kangwook Lee'], 'affiliations': ['Ajou University', 'FuriosaAI', 'Seoul National University', 'UW-Madison'], 'pdf_title_img': 'assets/pdf/title_img/2506.08373.jpg', 'data': {'categories': ['#optimization', '#long_context', '#benchmark', '#architecture', '#training', '#inference'], 'emoji': '🚀', 'ru': {'title': 'Ускорение вывода ИИ с помощью умных черновиков', 'desc': 'Предложена новая система для приближенного вывода в больших языковых моделях с длинным контекстом, использующая вспомогательные модели-черновики. Система точнее предсказывает важность токенов и пар ключ-значение, что повышает точность при сохранении эффективности использования памяти и вычислений. Представлены два варианта реализации: SpecKV для более эффективного отбрасывания кэша ключ-значение и SpecPC для идентификации и удаления неважных токенов запроса. Эксперименты показывают, что предложенные методы превосходят существующие базовые подходы по точности при сохранении преимуществ в использовании памяти, задержке и пропускной способности.'}, 'en': {'title': 'Enhancing LLM Inference with Draft Models for Efficiency and Accuracy', 'desc': 'This paper introduces a new framework that uses draft models to enhance approximate inference in long-context Large Language Models (LLMs). By accurately predicting the importance of tokens and key-value (KV) pairs, the framework improves the accuracy of LLMs while keeping memory and computational efficiency in check. The authors present two specific implementations: SpecKV for effective KV cache dropping and SpecPC for identifying unimportant prompt tokens. Their experiments demonstrate that this approach outperforms existing methods in accuracy while maintaining low resource usage.'}, 'zh': {'title': '利用草稿模型提升长上下文LLM推理效率', 'desc': '本文提出了一种新的框架，利用草稿模型来增强长上下文大语言模型（LLM）的近似推理能力。通过更准确地预测令牌和键值对的重要性，该方法提高了推理的准确性，同时保持了内存和计算效率。我们介绍了两种具体实现：SpecKV和SpecPC，分别用于优化键值缓存和提示令牌的选择。实验结果表明，该方法在准确性、内存使用、延迟和吞吐量方面均优于现有基线。'}}}, {'id': 'https://huggingface.co/papers/2506.06561', 'title': 'LaMP-Cap: Personalized Figure Caption Generation With Multimodal Figure\n  Profiles', 'url': 'https://huggingface.co/papers/2506.06561', 'abstract': "LaMP-Cap introduces a dataset for personalized figure caption generation using multimodal profiles to improve the quality of AI-generated captions.  \t\t\t\t\tAI-generated summary \t\t\t\t Figure captions are crucial for helping readers understand and remember a figure's key message. Many models have been developed to generate these captions, helping authors compose better quality captions more easily. Yet, authors almost always need to revise generic AI-generated captions to match their writing style and the domain's style, highlighting the need for personalization. Despite language models' personalization (LaMP) advances, these technologies often focus on text-only settings and rarely address scenarios where both inputs and profiles are multimodal. This paper introduces LaMP-Cap, a dataset for personalized figure caption generation with multimodal figure profiles. For each target figure, LaMP-Cap provides not only the needed inputs, such as figure images, but also up to three other figures from the same document--each with its image, caption, and figure-mentioning paragraphs--as a profile to characterize the context. Experiments with four LLMs show that using profile information consistently helps generate captions closer to the original author-written ones. Ablation studies reveal that images in the profile are more helpful than figure-mentioning paragraphs, highlighting the advantage of using multimodal profiles over text-only ones.", 'score': 1, 'issue_id': 4272, 'pub_date': '2025-06-06', 'pub_date_card': {'ru': '6 июня', 'en': 'June 6', 'zh': '6月6日'}, 'hash': '12942392f309be51', 'authors': ["Ho Yin 'Sam' Ng", 'Ting-Yao Hsu', 'Aashish Anantha Ramakrishnan', 'Branislav Kveton', 'Nedim Lipka', 'Franck Dernoncourt', 'Dongwon Lee', 'Tong Yu', 'Sungchul Kim', 'Ryan A. Rossi', "Ting-Hao 'Kenneth' Huang"], 'affiliations': ['Adobe Research', 'Pennsylvania State University'], 'pdf_title_img': 'assets/pdf/title_img/2506.06561.jpg', 'data': {'categories': ['#optimization', '#dataset', '#multimodal', '#interpretability', '#games'], 'emoji': '🖼️', 'ru': {'title': 'Персонализированные подписи к изображениям: мультимодальный подход', 'desc': 'LaMP-Cap представляет датасет для персонализированной генерации подписей к изображениям с использованием мультимодальных профилей. Эксперименты показали, что использование профильной информации помогает генерировать подписи, более близкие к оригинальным авторским. Исследование выявило, что изображения в профиле более полезны, чем текстовые параграфы, упоминающие рисунки. Это подчеркивает преимущество использования мультимодальных профилей по сравнению с чисто текстовыми.'}, 'en': {'title': 'Personalized Captions Through Multimodal Contexts', 'desc': "LaMP-Cap is a new dataset designed to enhance the generation of personalized figure captions by utilizing multimodal profiles. It provides not only the target figure images but also additional contextual figures and their associated captions and paragraphs. This approach allows AI models to create captions that better reflect the author's style and the specific domain. Experiments demonstrate that incorporating profile information, especially images, significantly improves the quality of AI-generated captions compared to traditional text-only methods."}, 'zh': {'title': '个性化图形标题生成的新突破', 'desc': 'LaMP-Cap是一个用于个性化图形标题生成的数据集，旨在通过多模态资料提高AI生成标题的质量。图形标题对于帮助读者理解和记住图形的关键信息至关重要。尽管已有许多模型可以生成这些标题，但作者通常需要修改通用的AI生成标题以匹配他们的写作风格。LaMP-Cap提供了图像和相关图形的上下文资料，实验表明，使用这些多模态资料可以生成更接近作者原始写作的标题。'}}}, {'id': 'https://huggingface.co/papers/2506.05982', 'title': 'MCA-Bench: A Multimodal Benchmark for Evaluating CAPTCHA Robustness\n  Against VLM-based Attacks', 'url': 'https://huggingface.co/papers/2506.05982', 'abstract': 'MCA-Bench is a multimodal benchmark suite for CAPTCHA security evaluation which fine-tunes specialized cracking agents using a shared vision-language model.  \t\t\t\t\tAI-generated summary \t\t\t\t As automated attack techniques rapidly advance, CAPTCHAs remain a critical defense mechanism against malicious bots. However, existing CAPTCHA schemes encompass a diverse range of modalities -- from static distorted text and obfuscated images to interactive clicks, sliding puzzles, and logic-based questions -- yet the community still lacks a unified, large-scale, multimodal benchmark to rigorously evaluate their security robustness. To address this gap, we introduce MCA-Bench, a comprehensive and reproducible benchmarking suite that integrates heterogeneous CAPTCHA types into a single evaluation protocol. Leveraging a shared vision-language model backbone, we fine-tune specialized cracking agents for each CAPTCHA category, enabling consistent, cross-modal assessments. Extensive experiments reveal that MCA-Bench effectively maps the vulnerability spectrum of modern CAPTCHA designs under varied attack settings, and crucially offers the first quantitative analysis of how challenge complexity, interaction depth, and model solvability interrelate. Based on these findings, we propose three actionable design principles and identify key open challenges, laying the groundwork for systematic CAPTCHA hardening, fair benchmarking, and broader community collaboration. Datasets and code are available online.', 'score': 1, 'issue_id': 4272, 'pub_date': '2025-06-06', 'pub_date_card': {'ru': '6 июня', 'en': 'June 6', 'zh': '6月6日'}, 'hash': '6cf7938ff751b2ff', 'authors': ['Zonglin Wu', 'Yule Xue', 'Xin Wei', 'Yiren Song'], 'affiliations': ['National University of Singapore', 'Southwest University'], 'pdf_title_img': 'assets/pdf/title_img/2506.05982.jpg', 'data': {'categories': ['#agents', '#benchmark', '#security', '#open_source', '#multimodal'], 'emoji': '🔐', 'ru': {'title': 'Единый мультимодальный бенчмарк для оценки безопасности CAPTCHA', 'desc': 'MCA-Bench - это комплексный набор инструментов для оценки безопасности CAPTCHA, использующий мультимодальный подход. Он объединяет различные типы CAPTCHA в единый протокол оценки, используя общую основу модели машинного зрения и обработки естественного языка. MCA-Bench позволяет дообучать специализированных агентов для взлома каждой категории CAPTCHA, обеспечивая последовательную оценку между различными модальностями. Эксперименты показывают, что MCA-Bench эффективно отображает спектр уязвимостей современных CAPTCHA и предлагает количественный анализ взаимосвязи между сложностью задачи, глубиной взаимодействия и способностью модели решать задачи.'}, 'en': {'title': 'Strengthening CAPTCHA Security with MCA-Bench', 'desc': 'MCA-Bench is a new tool designed to evaluate the security of different types of CAPTCHAs against automated attacks. It combines various CAPTCHA formats, such as text, images, and interactive puzzles, into one comprehensive testing framework. By using a shared vision-language model, it fine-tunes specific agents to crack each type of CAPTCHA, allowing for consistent comparisons across different modalities. The results provide insights into how the complexity and interaction of CAPTCHAs affect their vulnerability, helping to improve their design and security.'}, 'zh': {'title': 'MCA-Bench：CAPTCHA安全评估的新基准', 'desc': 'MCA-Bench是一个多模态基准测试套件，用于评估CAPTCHA的安全性。它通过共享的视觉-语言模型微调专门的破解代理，以便对不同类型的CAPTCHA进行一致的评估。该研究填补了现有CAPTCHA评估中缺乏统一大规模基准的空白，提供了对现代CAPTCHA设计脆弱性的定量分析。基于实验结果，提出了三个可行的设计原则，并识别了关键的开放挑战，为CAPTCHA的系统性强化和公平基准测试奠定了基础。'}}}, {'id': 'https://huggingface.co/papers/2506.10978', 'title': 'Fine-Grained Perturbation Guidance via Attention Head Selection', 'url': 'https://huggingface.co/papers/2506.10978', 'abstract': 'The paper proposes HeadHunter, a systematic framework for selecting attention heads in Diffusion Transformer architectures to enable precise control over image generation quality and style, outperforming existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent guidance methods in diffusion models steer reverse sampling by perturbing the model to construct an implicit weak model and guide generation away from it. Among these approaches, attention perturbation has demonstrated strong empirical performance in unconditional scenarios where classifier-free guidance is not applicable. However, existing attention perturbation methods lack principled approaches for determining where perturbations should be applied, particularly in Diffusion Transformer (DiT) architectures where quality-relevant computations are distributed across layers. In this paper, we investigate the granularity of attention perturbations, ranging from the layer level down to individual attention heads, and discover that specific heads govern distinct visual concepts such as structure, style, and texture quality. Building on this insight, we propose "HeadHunter", a systematic framework for iteratively selecting attention heads that align with user-centric objectives, enabling fine-grained control over generation quality and visual attributes. In addition, we introduce SoftPAG, which linearly interpolates each selected head\'s attention map toward an identity matrix, providing a continuous knob to tune perturbation strength and suppress artifacts. Our approach not only mitigates the oversmoothing issues of existing layer-level perturbation but also enables targeted manipulation of specific visual styles through compositional head selection. We validate our method on modern large-scale DiT-based text-to-image models including Stable Diffusion 3 and FLUX.1, demonstrating superior performance in both general quality enhancement and style-specific guidance. Our work provides the first head-level analysis of attention perturbation in diffusion models, uncovering interpretable specialization within attention layers and enabling practical design of effective perturbation strategies.', 'score': 0, 'issue_id': 4281, 'pub_date': '2025-06-12', 'pub_date_card': {'ru': '12 июня', 'en': 'June 12', 'zh': '6月12日'}, 'hash': 'a646cb2c7b825b74', 'authors': ['Donghoon Ahn', 'Jiwon Kang', 'Sanghyun Lee', 'Minjae Kim', 'Jaewon Min', 'Wooseok Jang', 'Saungwu Lee', 'Sayak Paul', 'Susung Hong', 'Seungryong Kim'], 'affiliations': ['HuggingFace', 'KAIST', 'KAIST AI', 'Korea University', 'Krea AI', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2506.10978.jpg', 'data': {'categories': ['#interpretability', '#cv', '#training', '#diffusion', '#optimization', '#architecture'], 'emoji': '🎯', 'ru': {'title': 'Точное управление генерацией изображений через выбор голов внимания', 'desc': 'Статья представляет HeadHunter - систему для выбора голов внимания в архитектурах Diffusion Transformer для точного контроля качества и стиля генерации изображений. Авторы обнаружили, что определенные головы внимания отвечают за различные визуальные концепции, такие как структура, стиль и качество текстуры. HeadHunter позволяет итеративно выбирать головы внимания в соответствии с целями пользователя, обеспечивая детальный контроль над генерацией. Метод превосходит существующие подходы как в улучшении общего качества, так и в управлении конкретными стилями.'}, 'en': {'title': 'HeadHunter: Precision Control in Image Generation with Attention Heads', 'desc': 'The paper introduces HeadHunter, a framework designed to select specific attention heads in Diffusion Transformer architectures for better control over image generation quality and style. It addresses the limitations of existing attention perturbation methods by providing a systematic approach to determine where perturbations should be applied, focusing on individual attention heads rather than entire layers. The authors demonstrate that different heads are responsible for distinct visual concepts, allowing for targeted manipulation of attributes like structure and texture. By implementing SoftPAG, they offer a method to fine-tune perturbation strength, leading to improved image quality and style guidance in large-scale text-to-image models.'}, 'zh': {'title': '精准控制图像生成的注意力头选择框架', 'desc': '本文提出了HeadHunter，这是一个系统化框架，用于选择扩散变换器架构中的注意力头，以实现对图像生成质量和风格的精确控制，超越了现有方法。我们研究了注意力扰动的粒度，从层级到单个注意力头，发现特定的头控制着不同的视觉概念，如结构、风格和纹理质量。基于这一发现，我们提出了“HeadHunter”，通过迭代选择与用户目标一致的注意力头，实现对生成质量和视觉属性的细粒度控制。此外，我们引入了SoftPAG，提供了一个连续的调节工具，以调整扰动强度并抑制伪影。'}}}, {'id': 'https://huggingface.co/papers/2506.10920', 'title': 'Decomposing MLP Activations into Interpretable Features via\n  Semi-Nonnegative Matrix Factorization', 'url': 'https://huggingface.co/papers/2506.10920', 'abstract': "SNMF is used to identify interpretable features in LLMs by directly decomposing MLP activations, outperforming SAEs and supervised methods in causal evaluations and aligning with human-interpretable concepts.  \t\t\t\t\tAI-generated summary \t\t\t\t A central goal for mechanistic interpretability has been to identify the right units of analysis in large language models (LLMs) that causally explain their outputs. While early work focused on individual neurons, evidence that neurons often encode multiple concepts has motivated a shift toward analyzing directions in activation space. A key question is how to find directions that capture interpretable features in an unsupervised manner. Current methods rely on dictionary learning with sparse autoencoders (SAEs), commonly trained over residual stream activations to learn directions from scratch. However, SAEs often struggle in causal evaluations and lack intrinsic interpretability, as their learning is not explicitly tied to the computations of the model. Here, we tackle these limitations by directly decomposing MLP activations with semi-nonnegative matrix factorization (SNMF), such that the learned features are (a) sparse linear combinations of co-activated neurons, and (b) mapped to their activating inputs, making them directly interpretable. Experiments on Llama 3.1, Gemma 2 and GPT-2 show that SNMF derived features outperform SAEs and a strong supervised baseline (difference-in-means) on causal steering, while aligning with human-interpretable concepts. Further analysis reveals that specific neuron combinations are reused across semantically-related features, exposing a hierarchical structure in the MLP's activation space. Together, these results position SNMF as a simple and effective tool for identifying interpretable features and dissecting concept representations in LLMs.", 'score': 0, 'issue_id': 4285, 'pub_date': '2025-06-12', 'pub_date_card': {'ru': '12 июня', 'en': 'June 12', 'zh': '6月12日'}, 'hash': 'ce3cd0a96d1ecc71', 'authors': ['Or Shafran', 'Atticus Geiger', 'Mor Geva'], 'affiliations': ['Blavatnik School of Computer Science and AI, Tel Aviv University', 'Pr(Ai)2R Group'], 'pdf_title_img': 'assets/pdf/title_img/2506.10920.jpg', 'data': {'categories': ['#architecture', '#data', '#training', '#interpretability'], 'emoji': '🧠', 'ru': {'title': 'SNMF: ключ к интерпретации больших языковых моделей', 'desc': 'В статье предлагается метод полуотрицательной матричной факторизации (SNMF) для выявления интерпретируемых признаков в больших языковых моделях (LLM). SNMF напрямую разлагает активации многослойного перцептрона (MLP), что позволяет находить более интерпретируемые и каузально значимые признаки по сравнению с разреженными автоэнкодерами (SAE) и методами обучения с учителем. Эксперименты на моделях Llama 3.1, Gemma 2 и GPT-2 показывают, что признаки, полученные с помощью SNMF, лучше поддаются каузальному управлению и соответствуют понятным человеку концепциям. Анализ также выявляет иерархическую структуру в пространстве активаций MLP, где определенные комбинации нейронов повторно используются для семантически связанных признаков.'}, 'en': {'title': 'Unlocking Interpretability in LLMs with SNMF', 'desc': 'This paper introduces semi-nonnegative matrix factorization (SNMF) as a method to extract interpretable features from large language models (LLMs) by analyzing multi-layer perceptron (MLP) activations. Unlike sparse autoencoders (SAEs), which often fail in causal evaluations, SNMF directly decomposes activations into sparse linear combinations of neurons, making the features more interpretable. The study demonstrates that SNMF outperforms SAEs and supervised methods in identifying causal relationships and aligns better with human-understandable concepts. Additionally, it reveals a hierarchical structure in the activation space, showing how certain neuron combinations are reused across related features.'}, 'zh': {'title': 'SNMF：揭示大型语言模型的可解释特征', 'desc': '本论文提出了一种名为半非负矩阵分解（SNMF）的方法，用于在大型语言模型（LLMs）中识别可解释的特征。与稀疏自编码器（SAEs）和监督方法相比，SNMF在因果评估中表现更好，并且与人类可解释的概念对齐。该方法通过直接分解多层感知器（MLP）的激活，学习到的特征是稀疏的线性组合，并且可以直接映射到其激活输入上，从而提高了可解释性。实验结果表明，SNMF在识别可解释特征和解析概念表示方面是一个简单而有效的工具。'}}}, {'id': 'https://huggingface.co/papers/2506.10600', 'title': 'EmbodiedGen: Towards a Generative 3D World Engine for Embodied\n  Intelligence', 'url': 'https://huggingface.co/papers/2506.10600', 'abstract': 'EmbodiedGen is a platform that generates high-quality, photorealistic 3D assets at low cost, enabling scalable and realistic embodied AI research through generative AI techniques.  \t\t\t\t\tAI-generated summary \t\t\t\t Constructing a physically realistic and accurately scaled simulated 3D world is crucial for the training and evaluation of embodied intelligence tasks. The diversity, realism, low cost accessibility and affordability of 3D data assets are critical for achieving generalization and scalability in embodied AI. However, most current embodied intelligence tasks still rely heavily on traditional 3D computer graphics assets manually created and annotated, which suffer from high production costs and limited realism. These limitations significantly hinder the scalability of data driven approaches. We present EmbodiedGen, a foundational platform for interactive 3D world generation. It enables the scalable generation of high-quality, controllable and photorealistic 3D assets with accurate physical properties and real-world scale in the Unified Robotics Description Format (URDF) at low cost. These assets can be directly imported into various physics simulation engines for fine-grained physical control, supporting downstream tasks in training and evaluation. EmbodiedGen is an easy-to-use, full-featured toolkit composed of six key modules: Image-to-3D, Text-to-3D, Texture Generation, Articulated Object Generation, Scene Generation and Layout Generation. EmbodiedGen generates diverse and interactive 3D worlds composed of generative 3D assets, leveraging generative AI to address the challenges of generalization and evaluation to the needs of embodied intelligence related research. Code is available at https://horizonrobotics.github.io/robot_lab/embodied_gen/index.html.', 'score': 0, 'issue_id': 4285, 'pub_date': '2025-06-12', 'pub_date_card': {'ru': '12 июня', 'en': 'June 12', 'zh': '6月12日'}, 'hash': '34b11f20c290edff', 'authors': ['Wang Xinjie', 'Liu Liu', 'Cao Yu', 'Wu Ruiqi', 'Qin Wenkang', 'Wang Dehui', 'Sui Wei', 'Su Zhizhong'], 'affiliations': ['D-Robotics', 'GigaAI', 'Horizon Robotics', 'Shanghai Jiao Tong University', 'VCIP, CS, Nankai University'], 'pdf_title_img': 'assets/pdf/title_img/2506.10600.jpg', 'data': {'categories': ['#games', '#3d', '#agents', '#synthetic'], 'emoji': '🤖', 'ru': {'title': 'Генеративный ИИ для создания реалистичных 3D-миров', 'desc': 'EmbodiedGen - это платформа для генерации фотореалистичных 3D-активов высокого качества с низкой стоимостью. Она использует методы генеративного искусственного интеллекта для создания масштабируемых и реалистичных сред для исследований в области воплощенного ИИ. Платформа состоит из шести ключевых модулей, включая преобразование изображений и текста в 3D, генерацию текстур и сцен. EmbodiedGen позволяет создавать разнообразные интерактивные 3D-миры, решая проблемы обобщения и оценки для исследований воплощенного интеллекта.'}, 'en': {'title': 'Revolutionizing 3D Asset Generation for Embodied AI', 'desc': 'EmbodiedGen is a platform designed to create high-quality, photorealistic 3D assets efficiently, which is essential for training embodied AI systems. It addresses the limitations of traditional 3D graphics by providing a scalable and cost-effective solution for generating diverse 3D environments. The platform includes six modules that facilitate the generation of 3D objects and scenes, ensuring they have accurate physical properties for realistic simulations. By leveraging generative AI techniques, EmbodiedGen enhances the generalization and evaluation capabilities of embodied intelligence research.'}, 'zh': {'title': 'EmbodiedGen：低成本生成高质量3D资产的解决方案', 'desc': 'EmbodiedGen是一个生成高质量、逼真的3D资产的平台，旨在降低成本并促进可扩展的具身人工智能研究。该平台通过生成式人工智能技术，构建物理真实且准确缩放的3D世界，以支持具身智能任务的训练和评估。EmbodiedGen提供了六个关键模块，能够生成多样化和互动的3D世界，解决了传统3D图形资产的高成本和有限真实感的问题。通过使用EmbodiedGen，研究人员可以更高效地生成所需的3D数据资产，从而推动具身智能领域的发展。'}}}, {'id': 'https://huggingface.co/papers/2506.10378', 'title': 'Discovering Hierarchical Latent Capabilities of Language Models via\n  Causal Representation Learning', 'url': 'https://huggingface.co/papers/2506.10378', 'abstract': 'The study proposes a causal representation learning framework to evaluate language model capabilities through latent factors, emphasizing the importance of controlling for base model variations to uncover underlying causal relationships.  \t\t\t\t\tAI-generated summary \t\t\t\t Faithful evaluation of language model capabilities is crucial for deriving actionable insights that can inform model development. However, rigorous causal evaluations in this domain face significant methodological challenges, including complex confounding effects and prohibitive computational costs associated with extensive retraining. To tackle these challenges, we propose a causal representation learning framework wherein observed benchmark performance is modeled as a linear transformation of a few latent capability factors. Crucially, these latent factors are identified as causally interrelated after appropriately controlling for the base model as a common confounder. Applying this approach to a comprehensive dataset encompassing over 1500 models evaluated across six benchmarks from the Open LLM Leaderboard, we identify a concise three-node linear causal structure that reliably explains the observed performance variations. Further interpretation of this causal structure provides substantial scientific insights beyond simple numerical rankings: specifically, we reveal a clear causal direction starting from general problem-solving capabilities, advancing through instruction-following proficiency, and culminating in mathematical reasoning ability. Our results underscore the essential role of carefully controlling base model variations during evaluation, a step critical to accurately uncovering the underlying causal relationships among latent model capabilities.', 'score': 0, 'issue_id': 4275, 'pub_date': '2025-06-12', 'pub_date_card': {'ru': '12 июня', 'en': 'June 12', 'zh': '6月12日'}, 'hash': '50915ea9038be86d', 'authors': ['Jikai Jin', 'Vasilis Syrgkanis', 'Sham Kakade', 'Hanlin Zhang'], 'affiliations': ['Harvard University', 'Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2506.10378.jpg', 'data': {'categories': ['#science', '#dataset', '#interpretability', '#benchmark', '#reasoning', '#math'], 'emoji': '🧠', 'ru': {'title': 'Раскрытие причинно-следственных связей в способностях языковых моделей', 'desc': 'Исследование предлагает каузальную модель представления для оценки возможностей языковых моделей через латентные факторы. Авторы подчеркивают важность контроля вариаций базовой модели для выявления причинно-следственных связей. Применяя этот подход к большому набору данных, они выявили трехузловую линейную каузальную структуру, объясняющую наблюдаемые различия в производительности. Результаты показывают четкую причинную связь от общих способностей решения задач через следование инструкциям к математическим рассуждениям.'}, 'en': {'title': 'Uncovering Causal Relationships in Language Model Performance', 'desc': 'This paper introduces a causal representation learning framework designed to assess the capabilities of language models by examining latent factors. It highlights the necessity of controlling for variations in base models to accurately identify causal relationships. The authors analyze a dataset of over 1500 models across six benchmarks, revealing a three-node linear causal structure that explains performance differences. Their findings emphasize the importance of understanding the causal pathways from general problem-solving to specific abilities like instruction-following and mathematical reasoning.'}, 'zh': {'title': '揭示语言模型能力的因果关系', 'desc': '本研究提出了一种因果表示学习框架，用于通过潜在因素评估语言模型的能力。我们强调控制基础模型变异的重要性，以揭示潜在的因果关系。通过对超过1500个模型在六个基准测试中的表现进行分析，我们识别出一个简洁的三节点线性因果结构，能够可靠地解释观察到的性能变化。我们的结果表明，在评估过程中仔细控制基础模型的变异是揭示潜在模型能力之间因果关系的关键步骤。'}}}, {'id': 'https://huggingface.co/papers/2506.08862', 'title': 'StreamSplat: Towards Online Dynamic 3D Reconstruction from Uncalibrated\n  Video Streams', 'url': 'https://huggingface.co/papers/2506.08862', 'abstract': 'StreamSplat, a fully feed-forward framework, addresses real-time 3D scene reconstruction from uncalibrated video with accurate dynamics and long-term stability.  \t\t\t\t\tAI-generated summary \t\t\t\t Real-time reconstruction of dynamic 3D scenes from uncalibrated video streams is crucial for numerous real-world applications. However, existing methods struggle to jointly address three key challenges: 1) processing uncalibrated inputs in real time, 2) accurately modeling dynamic scene evolution, and 3) maintaining long-term stability and computational efficiency. To this end, we introduce StreamSplat, the first fully feed-forward framework that transforms uncalibrated video streams of arbitrary length into dynamic 3D Gaussian Splatting (3DGS) representations in an online manner, capable of recovering scene dynamics from temporally local observations. We propose two key technical innovations: a probabilistic sampling mechanism in the static encoder for 3DGS position prediction, and a bidirectional deformation field in the dynamic decoder that enables robust and efficient dynamic modeling. Extensive experiments on static and dynamic benchmarks demonstrate that StreamSplat consistently outperforms prior works in both reconstruction quality and dynamic scene modeling, while uniquely supporting online reconstruction of arbitrarily long video streams. Code and models are available at https://github.com/nickwzk/StreamSplat.', 'score': 0, 'issue_id': 4279, 'pub_date': '2025-06-10', 'pub_date_card': {'ru': '10 июня', 'en': 'June 10', 'zh': '6月10日'}, 'hash': 'fc6ac5b21c6b00ff', 'authors': ['Zike Wu', 'Qi Yan', 'Xuanyu Yi', 'Lele Wang', 'Renjie Liao'], 'affiliations': ['Canada CIFAR AI Chair', 'Nanyang Technological University', 'University of British Columbia', 'Vector Institute for AI'], 'pdf_title_img': 'assets/pdf/title_img/2506.08862.jpg', 'data': {'categories': ['#benchmark', '#architecture', '#video', '#3d'], 'emoji': '🎥', 'ru': {'title': 'Революция в 3D-реконструкции: от видео к динамическим сценам в реальном времени', 'desc': 'StreamSplat - это новая система для реконструкции динамических 3D-сцен из неоткалиброванного видео в режиме реального времени. Она использует полностью прямую архитектуру нейронной сети для преобразования видеопотока в динамическое представление на основе 3D Gaussian Splatting. Ключевые инновации включают вероятностный механизм выборки для предсказания позиций 3DGS и двунаправленное поле деформации для моделирования динамики. StreamSplat превосходит существующие методы по качеству реконструкции и эффективности, поддерживая обработку видеопотоков произвольной длины.'}, 'en': {'title': 'StreamSplat: Real-Time 3D Scene Reconstruction Made Easy!', 'desc': 'StreamSplat is a novel framework designed for real-time 3D scene reconstruction from uncalibrated video inputs. It effectively addresses the challenges of processing uncalibrated data, accurately modeling dynamic changes in scenes, and ensuring long-term stability. The framework utilizes a feed-forward approach, incorporating a probabilistic sampling mechanism for predicting 3D positions and a bidirectional deformation field for dynamic modeling. Experimental results show that StreamSplat outperforms existing methods in both reconstruction quality and the ability to handle long video streams.'}, 'zh': {'title': 'StreamSplat：实时动态三维场景重建的新突破', 'desc': 'StreamSplat 是一个完全前馈的框架，旨在从未校准的视频中实时重建动态三维场景。该方法解决了处理未校准输入、准确建模动态场景演变以及保持长期稳定性等三个关键挑战。通过引入静态编码器中的概率采样机制和动态解码器中的双向变形场，StreamSplat 实现了高效的动态建模。实验结果表明，StreamSplat 在重建质量和动态场景建模方面均优于现有方法，并支持任意长度视频流的在线重建。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (4)', '#agents (8)', '#agi (1)', '#alignment', '#architecture (9)', '#audio (2)', '#benchmark (19)', '#cv (6)', '#data (5)', '#dataset (17)', '#diffusion (4)', '#ethics (2)', '#games (4)', '#graphs', '#hallucinations (2)', '#healthcare (1)', '#inference (2)', '#interpretability (5)', '#leakage', '#long_context (3)', '#low_resource (1)', '#machine_translation', '#math (2)', '#multilingual (1)', '#multimodal (13)', '#open_source (10)', '#optimization (20)', '#plp', '#rag (3)', '#reasoning (11)', '#rl (5)', '#rlhf (3)', '#robotics', '#science (5)', '#security (2)', '#small_models (2)', '#story_generation (1)', '#survey (3)', '#synthetic (2)', '#training (19)', '#transfer_learning (2)', '#video (6)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-06-13 20:13',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-06-13 20:13')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-06-13 20:13')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    