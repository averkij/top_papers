{
    "date": {
        "ru": "20 ÑĞ½Ğ²Ğ°Ñ€Ñ",
        "en": "January 20",
        "zh": "1æœˆ20æ—¥"
    },
    "time_utc": "2026-01-20 16:38",
    "weekday": 1,
    "issue_id": 675,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2601.11077",
            "title": "ABC-Bench: Benchmarking Agentic Backend Coding in Real-World Development",
            "url": "https://huggingface.co/papers/2601.11077",
            "abstract": "ABC-Bench evaluates LLM agents on realistic backend coding tasks requiring full development lifecycle management from repository exploration to containerized service deployment and API testing.  \t\t\t\t\tAI-generated summary \t\t\t\t The evolution of Large Language Models (LLMs) into autonomous agents has expanded the scope of AI coding from localized code generation to complex, repository-level, and execution-driven problem solving. However, current benchmarks predominantly evaluate code logic in static contexts, neglecting the dynamic, full-process requirements of real-world engineering, particularly in backend development which demands rigorous environment configuration and service deployment. To address this gap, we introduce ABC-Bench, a benchmark explicitly designed to evaluate agentic backend coding within a realistic, executable workflow. Using a scalable automated pipeline, we curated 224 practical tasks spanning 8 languages and 19 frameworks from open-source repositories. Distinct from previous evaluations, ABC-Bench require the agents to manage the entire development lifecycle from repository exploration to instantiating containerized services and pass the external end-to-end API tests. Our extensive evaluation reveals that even state-of-the-art models struggle to deliver reliable performance on these holistic tasks, highlighting a substantial disparity between current model capabilities and the demands of practical backend engineering. Our code is available at https://github.com/OpenMOSS/ABC-Bench.",
            "score": 47,
            "issue_id": 662,
            "pub_date": "2026-01-16",
            "pub_date_card": {
                "ru": "16 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 16",
                "zh": "1æœˆ16æ—¥"
            },
            "hash": "97497f7535c4d03b",
            "authors": [
                "Jie Yang",
                "Honglin Guo",
                "Li Ji",
                "Jiazheng Zhou",
                "Rui Zheng",
                "Zhikai Lei",
                "Shuo Zhang",
                "Zhiheng Xi",
                "Shichun Liu",
                "Yuxin Wang",
                "Bo Wang",
                "Yining Zheng",
                "Tao Gui",
                "Xipeng Qiu"
            ],
            "affiliations": [
                "Fudan University",
                "Shanghai Innovation Institute",
                "Shanghai QÄ³i Zhifeng Co., Ltd."
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.11077.jpg",
            "data": {
                "categories": [
                    "#plp",
                    "#benchmark",
                    "#agents",
                    "#open_source"
                ],
                "emoji": "ğŸ³",
                "ru": {
                    "title": "ĞÑ‚ ĞºĞ¾Ğ´Ğ° Ğº Ğ±Ğ¾ĞµĞ²Ğ¾Ğ¼Ñƒ ÑĞµÑ€Ğ²Ğ¸ÑÑƒ: Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¼ Ñ†Ğ¸ĞºĞ»Ğµ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸",
                    "desc": "ABC-Bench â€” ÑÑ‚Ğ¾ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ±ÑĞºĞµĞ½Ğ´Ğ°, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ³Ğ¾ Ñ†Ğ¸ĞºĞ»Ğ° Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¾Ñ‚ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ñ Ğ´Ğ¾ Ñ€Ğ°Ğ·Ğ²Ñ‘Ñ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞ¹Ğ½ĞµÑ€Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ€Ğ²Ğ¸ÑĞ¾Ğ². Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ»Ğ¾Ğ³Ğ¸ĞºÑƒ ĞºĞ¾Ğ´Ğ° Ğ² ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ…, ÑÑ‚Ğ¾Ñ‚ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑ‚ÑŒ Ğ²ÑĞµĞ¼ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ¼: ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ, Ñ€Ğ°Ğ·Ğ²Ñ‘Ñ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼ end-to-end API Ñ‚ĞµÑÑ‚Ğ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¸Ğ· 224 Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ½Ğ° 8 ÑĞ·Ñ‹ĞºĞ°Ñ… Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ 19 Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ°Ñ… Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ñ… ĞºĞ¾Ğ´Ğ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ ÑĞ°Ğ¼Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ»Ğ¾Ñ…Ğ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ñ‚Ğ°ĞºĞ¸Ğ¼Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ğ¸."
                },
                "en": {
                    "title": "ABC-Bench: Bridging the Gap in LLM Backend Coding Evaluation",
                    "desc": "The paper introduces ABC-Bench, a new benchmark for evaluating Large Language Model (LLM) agents on comprehensive backend coding tasks. Unlike previous benchmarks that focus on isolated code logic, ABC-Bench assesses the entire development lifecycle, including repository exploration, service deployment, and API testing. It features 224 practical tasks across multiple programming languages and frameworks, emphasizing real-world engineering challenges. The findings indicate that even advanced LLMs struggle with these complex tasks, revealing a significant gap between their current capabilities and the requirements of backend development."
                },
                "zh": {
                    "title": "è¯„ä¼°åç«¯ç¼–ç çš„å…¨ç”Ÿå‘½å‘¨æœŸç®¡ç†èƒ½åŠ›",
                    "desc": "ABC-Benchæ˜¯ä¸€ä¸ªè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨çœŸå®åç«¯ç¼–ç ä»»åŠ¡ä¸­çš„è¡¨ç°çš„åŸºå‡†ã€‚å®ƒå…³æ³¨æ•´ä¸ªå¼€å‘ç”Ÿå‘½å‘¨æœŸçš„ç®¡ç†ï¼ŒåŒ…æ‹¬ä»£ç åº“æ¢ç´¢ã€å®¹å™¨åŒ–æœåŠ¡éƒ¨ç½²å’ŒAPIæµ‹è¯•ã€‚ä¸ä»¥å¾€çš„è¯„ä¼°ä¸åŒï¼ŒABC-Benchè¦æ±‚æ¨¡å‹åœ¨åŠ¨æ€ç¯å¢ƒä¸­å®Œæˆå¤æ‚çš„ä»»åŠ¡ï¼Œè€Œä¸ä»…ä»…æ˜¯é™æ€ä»£ç é€»è¾‘ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„æ¨¡å‹åœ¨è¿™äº›å…¨é¢ä»»åŠ¡ä¸Šä¹Ÿéš¾ä»¥æä¾›å¯é çš„æ€§èƒ½ï¼Œæ˜¾ç¤ºå‡ºå½“å‰æ¨¡å‹èƒ½åŠ›ä¸å®é™…åç«¯å·¥ç¨‹éœ€æ±‚ä¹‹é—´çš„æ˜¾è‘—å·®è·ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.08808",
            "title": "Multiplex Thinking: Reasoning via Token-wise Branch-and-Merge",
            "url": "https://huggingface.co/papers/2601.08808",
            "abstract": "Multiplex Thinking introduces a stochastic soft reasoning mechanism that samples multiple candidate tokens at each step to optimize reasoning trajectories with reinforcement learning while maintaining shorter sequences than traditional chain-of-thought methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models often solve complex reasoning tasks more effectively with Chain-of-Thought (CoT), but at the cost of long, low-bandwidth token sequences. Humans, by contrast, often reason softly by maintaining a distribution over plausible next steps. Motivated by this, we propose Multiplex Thinking, a stochastic soft reasoning mechanism that, at each thinking step, samples K candidate tokens and aggregates their embeddings into a single continuous multiplex token. This preserves the vocabulary embedding prior and the sampling dynamics of standard discrete generation, while inducing a tractable probability distribution over multiplex rollouts. Consequently, multiplex trajectories can be directly optimized with on-policy reinforcement learning (RL). Importantly, Multiplex Thinking is self-adaptive: when the model is confident, the multiplex token is nearly discrete and behaves like standard CoT; when it is uncertain, it compactly represents multiple plausible next steps without increasing sequence length. Across challenging math reasoning benchmarks, Multiplex Thinking consistently outperforms strong discrete CoT and RL baselines from Pass@1 through Pass@1024, while producing shorter sequences. The code and checkpoints are available at https://github.com/GMLR-Penn/Multiplex-Thinking.",
            "score": 23,
            "issue_id": 662,
            "pub_date": "2026-01-13",
            "pub_date_card": {
                "ru": "13 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 13",
                "zh": "1æœˆ13æ—¥"
            },
            "hash": "0a64241c20f7b28c",
            "authors": [
                "Yao Tang",
                "Li Dong",
                "Yaru Hao",
                "Qingxiu Dong",
                "Furu Wei",
                "Jiatao Gu"
            ],
            "affiliations": [
                "Microsoft Research",
                "University of Pennsylvania"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.08808.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#math",
                    "#rl",
                    "#reasoning",
                    "#optimization",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞœÑĞ³ĞºĞ¾Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¿Ğ»ĞµĞ½Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ: ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¿Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ ÑˆĞ°Ğ³Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ğ°Ğ³Ñ€ĞµĞ³Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ Ğ² ĞµĞ´Ğ¸Ğ½Ñ‹Ğ¹ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¿Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ½Ğ°Ğ´ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿ÑƒÑ‚ÑĞ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, ĞºĞ°Ğº Ğ² Ğ¼ÑĞ³ĞºĞ¾Ğ¼ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼: Ğ¿Ñ€Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¿Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½ Ğ²ĞµĞ´Ñ‘Ñ‚ ÑĞµĞ±Ñ ĞºĞ°Ğº ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ°Ñ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ° Ğ¼Ñ‹ÑĞ»ĞµĞ¹, Ğ° Ğ¿Ñ€Ğ¸ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ğ»Ğ°ÑƒÑĞ¸Ğ±ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞĞ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ»Ğ¸Ğ½Ğ¸Ğ¸ Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ğº Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ, Ñ‚Ğ°Ğº Ğ¸ Ğ¿Ğ¾ Ğ´Ğ»Ğ¸Ğ½Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Optimizing Reasoning with Stochastic Soft Sampling",
                    "desc": "Multiplex Thinking is a new approach in machine learning that enhances reasoning in large language models by using a stochastic soft reasoning mechanism. Instead of generating long sequences of tokens like traditional Chain-of-Thought methods, it samples multiple candidate tokens at each step and combines them into a single multiplex token. This method allows the model to maintain a probability distribution over possible next steps, making it more efficient and adaptable. As a result, Multiplex Thinking achieves better performance on complex reasoning tasks while producing shorter sequences compared to existing methods."
                },
                "zh": {
                    "title": "å¤šé‡æ€ç»´ï¼šä¼˜åŒ–æ¨ç†çš„æ–°æ–¹å¼",
                    "desc": "Multiplex Thinkingæå‡ºäº†ä¸€ç§éšæœºè½¯æ¨ç†æœºåˆ¶ï¼Œåœ¨æ¯ä¸€æ­¥é‡‡æ ·å¤šä¸ªå€™é€‰æ ‡è®°ï¼Œä»¥ä¼˜åŒ–æ¨ç†è½¨è¿¹ï¼ŒåŒæ—¶ä¿æŒæ¯”ä¼ ç»Ÿçš„é“¾å¼æ€ç»´æ–¹æ³•æ›´çŸ­çš„åºåˆ—ã€‚ä¸äººç±»çš„æ¨ç†æ–¹å¼ç›¸ä¼¼ï¼Œè¯¥æ–¹æ³•åœ¨ä¸ç¡®å®šæ—¶èƒ½å¤Ÿç´§å‡‘åœ°è¡¨ç¤ºå¤šä¸ªåˆç†çš„ä¸‹ä¸€æ­¥ï¼Œè€Œåœ¨è‡ªä¿¡æ—¶åˆ™è¡¨ç°å¾—åƒæ ‡å‡†çš„é“¾å¼æ€ç»´ã€‚é€šè¿‡åœ¨ç­–ç•¥å¼ºåŒ–å­¦ä¹ ä¸­ç›´æ¥ä¼˜åŒ–å¤šé‡è½¨è¿¹ï¼ŒMultiplex Thinkingåœ¨å¤æ‚çš„æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†å¼ºå¤§çš„ç¦»æ•£é“¾å¼æ€ç»´å’Œå¼ºåŒ–å­¦ä¹ åŸºçº¿ã€‚è¯¥æ–¹æ³•çš„ä»£ç å’Œæ£€æŸ¥ç‚¹å¯åœ¨æŒ‡å®šé“¾æ¥è·å–ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.11061",
            "title": "Spurious Rewards Paradox: Mechanistically Understanding How RLVR Activates Memorization Shortcuts in LLMs",
            "url": "https://huggingface.co/papers/2601.11061",
            "abstract": "Spurious rewards in reinforcement learning with verifiable rewards trigger a memorization shortcut in LLMs, identified through neural circuit analysis and causal steering techniques.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning with Verifiable Rewards (RLVR) is highly effective for enhancing LLM reasoning, yet recent evidence shows models like Qwen 2.5 achieve significant gains even with spurious or incorrect rewards. We investigate this phenomenon and identify a \"Perplexity Paradox\": spurious RLVR triggers a divergence where answer-token perplexity drops while prompt-side coherence degrades, suggesting the model is bypassing reasoning in favor of memorization. Using Path Patching, Logit Lens, JSD analysis, and Neural Differential Equations, we uncover a hidden Anchor-Adapter circuit that facilitates this shortcut. We localize a Functional Anchor in the middle layers (L18-20) that triggers the retrieval of memorized solutions, followed by Structural Adapters in later layers (L21+) that transform representations to accommodate the shortcut signal. Finally, we demonstrate that scaling specific MLP keys within this circuit allows for bidirectional causal steering-artificially amplifying or suppressing contamination-driven performance. Our results provide a mechanistic roadmap for identifying and mitigating data contamination in RLVR-tuned models. Code is available at https://github.com/idwts/How-RLVR-Activates-Memorization-Shortcuts.",
            "score": 5,
            "issue_id": 669,
            "pub_date": "2026-01-16",
            "pub_date_card": {
                "ru": "16 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 16",
                "zh": "1æœˆ16æ—¥"
            },
            "hash": "1e155023f04ffd59",
            "authors": [
                "Lecheng Yan",
                "Ruizhe Li",
                "Guanhua Chen",
                "Qing Li",
                "Jiahui Geng",
                "Wenxi Li",
                "Vincent Wang",
                "Chris Lee"
            ],
            "affiliations": [
                "East China Normal University",
                "Mohamed bin Zayed University of Artificial Intelligence (MBZUAI)",
                "Southern University of Science and Technology",
                "University of Aberdeen"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.11061.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#architecture",
                    "#rlhf",
                    "#interpretability",
                    "#security",
                    "#rl",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞšĞ°Ğº Ğ½ĞµĞ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ·Ğ°ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ñ‚ÑŒ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¾ĞºÑĞ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸ (RLVR), Ğ³Ğ´Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ´Ğ°Ğ¶Ğµ Ğ¿Ñ€Ğ¸ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸ Â«ĞŸĞ°Ñ€Ğ°Ğ´Ğ¾ĞºÑ Ğ Ğ°ÑÑ‚ĞµÑ€ÑĞ½Ğ½Ğ¾ÑÑ‚Ğ¸Â»: Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ° Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°ĞµÑ‚ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹, ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ ÑĞºÑ€Ñ‹Ñ‚ÑƒÑ ÑÑ…ĞµĞ¼Ñƒ Ğ² Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ÑĞµÑ‚Ğ¸ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Anchor-Adapter. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ†ĞµĞ¿ĞµĞ¹ (Path Patching, Logit Lens, JSD-Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·), Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞºĞ¾Ñ€Ñ Ğ² ÑÑ€ĞµĞ´Ğ½Ğ¸Ñ… ÑĞ»Ğ¾ÑÑ… Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ¸Ñ€ÑƒÑÑ‚ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ½ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‚ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑ‚ÑŒ ÑÑ‚Ğ¾Ğ¹ ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¹ ÑÑ…ĞµĞ¼Ğ¾Ğ¹ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ñ‹ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ ÑƒÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ³Ñ€ÑĞ·Ğ½ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² RLVR-Ğ½Ğ°ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…."
                },
                "en": {
                    "title": "Uncovering Memorization Shortcuts in RLVR Models",
                    "desc": "This paper explores how spurious rewards in Reinforcement Learning with Verifiable Rewards (RLVR) can lead to memorization shortcuts in large language models (LLMs). The authors identify a phenomenon called the 'Perplexity Paradox', where models show lower perplexity in answers but poorer coherence in prompts, indicating a shift from reasoning to memorization. Through advanced techniques like Path Patching and Neural Differential Equations, they reveal a hidden circuit that enables this shortcut, involving specific layers of the neural network. The findings offer insights into how to detect and address data contamination issues in RLVR-tuned models, enhancing their reasoning capabilities."
                },
                "zh": {
                    "title": "æ­ç¤ºè™šå‡å¥–åŠ±å¯¼è‡´çš„è®°å¿†æ·å¾„",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†åœ¨å¼ºåŒ–å­¦ä¹ ä¸­ä½¿ç”¨å¯éªŒè¯å¥–åŠ±æ—¶ï¼Œè™šå‡å¥–åŠ±å¦‚ä½•å¯¼è‡´å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å‡ºç°è®°å¿†æ·å¾„ã€‚ç ”ç©¶å‘ç°ï¼Œè™šå‡å¥–åŠ±ä¼šå¼•å‘ä¸€ç§â€œå›°æƒ‘æ‚–è®ºâ€ï¼Œå³æ¨¡å‹åœ¨å›ç­”æ—¶çš„å›°æƒ‘åº¦ä¸‹é™ï¼Œä½†æç¤ºçš„ä¸€è‡´æ€§å´é™ä½ï¼Œè¡¨æ˜æ¨¡å‹å€¾å‘äºè®°å¿†è€Œéæ¨ç†ã€‚é€šè¿‡ç¥ç»ç”µè·¯åˆ†æå’Œå› æœå¼•å¯¼æŠ€æœ¯ï¼Œä½œè€…è¯†åˆ«å‡ºä¸€ä¸ªéšè—çš„é”šå®šé€‚é…å™¨ç”µè·¯ï¼Œå¸®åŠ©æ¨¡å‹ç»•è¿‡æ¨ç†è¿‡ç¨‹ã€‚æœ€åï¼Œç ”ç©¶æä¾›äº†ä¸€ç§æœºåˆ¶æ€§è·¯çº¿å›¾ï¼Œä»¥è¯†åˆ«å’Œå‡è½»å¼ºåŒ–å­¦ä¹ ä¸­æ•°æ®æ±¡æŸ“å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.10880",
            "title": "Medical SAM3: A Foundation Model for Universal Prompt-Driven Medical Image Segmentation",
            "url": "https://huggingface.co/papers/2601.10880",
            "abstract": "Medical SAM3 adapts the SAM3 foundation model through comprehensive fine-tuning on diverse medical imaging datasets to achieve robust prompt-driven segmentation across various modalities and anatomical structures.  \t\t\t\t\tAI-generated summary \t\t\t\t Promptable segmentation foundation models such as SAM3 have demonstrated strong generalization capabilities through interactive and concept-based prompting. However, their direct applicability to medical image segmentation remains limited by severe domain shifts, the absence of privileged spatial prompts, and the need to reason over complex anatomical and volumetric structures. Here we present Medical SAM3, a foundation model for universal prompt-driven medical image segmentation, obtained by fully fine-tuning SAM3 on large-scale, heterogeneous 2D and 3D medical imaging datasets with paired segmentation masks and text prompts. Through a systematic analysis of vanilla SAM3, we observe that its performance degrades substantially on medical data, with its apparent competitiveness largely relying on strong geometric priors such as ground-truth-derived bounding boxes. These findings motivate full model adaptation beyond prompt engineering alone. By fine-tuning SAM3's model parameters on 33 datasets spanning 10 medical imaging modalities, Medical SAM3 acquires robust domain-specific representations while preserving prompt-driven flexibility. Extensive experiments across organs, imaging modalities, and dimensionalities demonstrate consistent and significant performance gains, particularly in challenging scenarios characterized by semantic ambiguity, complex morphology, and long-range 3D context. Our results establish Medical SAM3 as a universal, text-guided segmentation foundation model for medical imaging and highlight the importance of holistic model adaptation for achieving robust prompt-driven segmentation under severe domain shift. Code and model will be made available at https://github.com/AIM-Research-Lab/Medical-SAM3.",
            "score": 5,
            "issue_id": 670,
            "pub_date": "2026-01-15",
            "pub_date_card": {
                "ru": "15 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 15",
                "zh": "1æœˆ15æ—¥"
            },
            "hash": "717e240848228667",
            "authors": [
                "Chongcong Jiang",
                "Tianxingjian Ding",
                "Chuhan Song",
                "Jiachen Tu",
                "Ziyang Yan",
                "Yihua Shao",
                "Zhenyi Wang",
                "Yuzhang Shang",
                "Tianyu Han",
                "Yu Tian"
            ],
            "affiliations": [
                "The Hong Kong Polytechnic University",
                "University College London",
                "University of Central Florida",
                "University of Illinois Urbana-Champaign",
                "University of Pennsylvania",
                "University of Trento"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.10880.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#multimodal",
                    "#healthcare",
                    "#cv"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ°Ğ¼",
                    "desc": "Medical SAM3 â€” ÑÑ‚Ğ¾ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ²ĞµÑ€ÑĞ¸Ñ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ SAM3, ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ… Ğ´Ğ»Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ñ‹Ñ… 2D Ğ¸ 3D Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ… Ğ¸Ğ· 33 Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ², Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… 10 Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹, Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼Ğ°ÑĞ¾Ğº ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ²Ğ°Ğ½Ğ¸Ğ»ÑŒĞ½Ğ°Ñ SAM3 ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ñ‚ĞµÑ€ÑĞµÑ‚ Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ° Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ğ¸ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº. Medical SAM3 Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ² ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ°Ğ½Ğ°Ñ‚Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑĞ»ÑƒÑ‡Ğ°ÑÑ… Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ½ĞµĞ¾Ğ´Ğ½Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼."
                },
                "en": {
                    "title": "Medical SAM3: Revolutionizing Prompt-Driven Segmentation in Medical Imaging",
                    "desc": "Medical SAM3 is a specialized version of the SAM3 model, fine-tuned on a wide range of medical imaging datasets to enhance its ability to perform prompt-driven segmentation. This adaptation addresses challenges like domain shifts and the complexity of anatomical structures, which hinder the original SAM3's effectiveness in medical contexts. By training on 33 diverse datasets, Medical SAM3 develops strong domain-specific representations while maintaining the flexibility of prompt-based inputs. The model shows significant improvements in segmentation accuracy across various medical imaging modalities, especially in difficult cases with complex shapes and long-range dependencies."
                },
                "zh": {
                    "title": "åŒ»å­¦å½±åƒåˆ†å‰²çš„æ–°åŸºç¡€ï¼šMedical SAM3",
                    "desc": "Medical SAM3 æ˜¯ä¸€ç§åŸºç¡€æ¨¡å‹ï¼Œé€šè¿‡åœ¨å¤šæ ·çš„åŒ»å­¦å½±åƒæ•°æ®é›†ä¸Šè¿›è¡Œå…¨é¢å¾®è°ƒï¼Œé€‚åº”äº† SAM3 æ¨¡å‹ï¼Œä»¥å®ç°å¯¹å„ç§æ¨¡æ€å’Œè§£å‰–ç»“æ„çš„ç¨³å¥æç¤ºé©±åŠ¨åˆ†å‰²ã€‚è¯¥æ¨¡å‹å…‹æœäº†åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„é¢†åŸŸè½¬ç§»é—®é¢˜ï¼Œèƒ½å¤Ÿå¤„ç†å¤æ‚çš„è§£å‰–å’Œä½“ç§¯ç»“æ„ã€‚é€šè¿‡åœ¨ 33 ä¸ªæ•°æ®é›†ä¸Šå¾®è°ƒï¼ŒMedical SAM3 è·å¾—äº†å¼ºå¤§çš„é¢†åŸŸç‰¹å®šè¡¨ç¤ºï¼ŒåŒæ—¶ä¿æŒäº†æç¤ºé©±åŠ¨çš„çµæ´»æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMedical SAM3 åœ¨å¤„ç†è¯­ä¹‰æ¨¡ç³Šã€å¤æ‚å½¢æ€å’Œé•¿è·ç¦» 3D ä¸Šä¸‹æ–‡çš„æŒ‘æˆ˜åœºæ™¯ä¸­ï¼Œè¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.11096",
            "title": "CoDance: An Unbind-Rebind Paradigm for Robust Multi-Subject Animation",
            "url": "https://huggingface.co/papers/2601.11096",
            "abstract": "CoDance introduces an Unbind-Rebind framework for animating multiple subjects with flexible spatial configurations, using pose shift encoding and semantic/textual guidance for motion reassignment.  \t\t\t\t\tAI-generated summary \t\t\t\t Character image animation is gaining significant importance across various domains, driven by the demand for robust and flexible multi-subject rendering. While existing methods excel in single-person animation, they struggle to handle arbitrary subject counts, diverse character types, and spatial misalignment between the reference image and the driving poses. We attribute these limitations to an overly rigid spatial binding that forces strict pixel-wise alignment between the pose and reference, and an inability to consistently rebind motion to intended subjects. To address these challenges, we propose CoDance, a novel Unbind-Rebind framework that enables the animation of arbitrary subject counts, types, and spatial configurations conditioned on a single, potentially misaligned pose sequence. Specifically, the Unbind module employs a novel pose shift encoder to break the rigid spatial binding between the pose and the reference by introducing stochastic perturbations to both poses and their latent features, thereby compelling the model to learn a location-agnostic motion representation. To ensure precise control and subject association, we then devise a Rebind module, leveraging semantic guidance from text prompts and spatial guidance from subject masks to direct the learned motion to intended characters. Furthermore, to facilitate comprehensive evaluation, we introduce a new multi-subject CoDanceBench. Extensive experiments on CoDanceBench and existing datasets show that CoDance achieves SOTA performance, exhibiting remarkable generalization across diverse subjects and spatial layouts. The code and weights will be open-sourced.",
            "score": 4,
            "issue_id": 662,
            "pub_date": "2026-01-16",
            "pub_date_card": {
                "ru": "16 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 16",
                "zh": "1æœˆ16æ—¥"
            },
            "hash": "8c557edef204bebb",
            "authors": [
                "Shuai Tan",
                "Biao Gong",
                "Ke Ma",
                "Yutong Feng",
                "Qiyuan Zhang",
                "Yan Wang",
                "Yujun Shen",
                "Hengshuang Zhao"
            ],
            "affiliations": [
                "Ant Group",
                "Huazhong University of Science and Technology",
                "The University of Hong Kong",
                "Tsinghua University",
                "University of North Carolina at Chapel Hill"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.11096.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#video",
                    "#dataset",
                    "#open_source"
                ],
                "emoji": "ğŸ’ƒ",
                "ru": {
                    "title": "Ğ“Ğ¸Ğ±ĞºĞ°Ñ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ° Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾ÑĞ²ÑĞ·Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "CoDance Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Unbind-Rebind Ğ´Ğ»Ñ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹ Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸. ĞœĞ¾Ğ´ÑƒĞ»ÑŒ Unbind Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸Ğº ÑĞ´Ğ²Ğ¸Ğ³Ğ° Ğ¿Ğ¾Ğ·Ñ‹ ÑĞ¾ ÑÑ‚Ğ¾Ñ…Ğ°ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ¼ÑƒÑ‰ĞµĞ½Ğ¸ÑĞ¼Ğ¸, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ñ€Ğ°Ğ·Ñ€ÑƒÑˆĞ¸Ñ‚ÑŒ Ğ¶Ñ‘ÑÑ‚ĞºÑƒÑ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºÑƒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ğ¾Ğ·Ğ¾Ğ¹ Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ĞµĞ¼, Ğ·Ğ°ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑƒÑ‡Ğ¸Ñ‚ÑŒÑÑ Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğº Ñ€Ğ°ÑĞ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ÑƒĞ»ÑŒ Rebind Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¸Ğ· Ğ¼Ğ°ÑĞ¾Ğº ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ½ÑƒĞ¶Ğ½Ñ‹Ğ¼ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶Ğ°Ğ¼. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹Ğ´Ğ°ÑÑ‰ÑƒÑÑÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ñ‹ Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ SOTA Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Flexible Animation for Multiple Characters with CoDance",
                    "desc": "CoDance presents a new framework called Unbind-Rebind for animating multiple characters in various spatial arrangements. It addresses the limitations of existing methods that struggle with multiple subjects and misaligned poses by introducing a pose shift encoder that allows for flexible motion representation. The framework consists of two main components: the Unbind module, which breaks rigid spatial bindings, and the Rebind module, which uses semantic and spatial guidance to accurately assign motion to specific characters. Extensive testing shows that CoDance outperforms previous methods, demonstrating its ability to generalize across different subjects and layouts."
                },
                "zh": {
                    "title": "çµæ´»å¤šè§’è‰²åŠ¨ç”»çš„æ–°æ–¹æ³•ï¼šCoDance",
                    "desc": "CoDanceæå‡ºäº†ä¸€ç§è§£ç»‘å®š-é‡ç»‘å®šæ¡†æ¶ï¼Œç”¨äºçµæ´»é…ç½®å¤šä¸ªè§’è‰²çš„åŠ¨ç”»ã€‚è¯¥æ–¹æ³•é€šè¿‡å§¿æ€åç§»ç¼–ç å’Œè¯­ä¹‰/æ–‡æœ¬æŒ‡å¯¼æ¥é‡æ–°åˆ†é…è¿åŠ¨ï¼Œå…‹æœäº†ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤šè§’è‰²åŠ¨ç”»æ—¶çš„å±€é™æ€§ã€‚è§£ç»‘å®šæ¨¡å—æ‰“ç ´äº†å§¿æ€ä¸å‚è€ƒå›¾åƒä¹‹é—´çš„ä¸¥æ ¼ç©ºé—´ç»‘å®šï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ ä½ç½®æ— å…³çš„è¿åŠ¨è¡¨ç¤ºã€‚é‡ç»‘å®šæ¨¡å—åˆ™åˆ©ç”¨æ–‡æœ¬æç¤ºå’Œè§’è‰²æ©ç çš„è¯­ä¹‰æŒ‡å¯¼ï¼Œç¡®ä¿è¿åŠ¨ç²¾ç¡®åœ°æŒ‡å‘ç›®æ ‡è§’è‰²ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.10387",
            "title": "The Assistant Axis: Situating and Stabilizing the Default Persona of Language Models",
            "url": "https://huggingface.co/papers/2601.10387",
            "abstract": "Research reveals that large language models operate within a persona space where an \"Assistant Axis\" controls helpfulness and behavioral stability, with steering techniques able to influence model responses and prevent harmful behavior drift.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models can represent a variety of personas but typically default to a helpful Assistant identity cultivated during post-training. We investigate the structure of the space of model personas by extracting activation directions corresponding to diverse character archetypes. Across several different models, we find that the leading component of this persona space is an \"Assistant Axis,\" which captures the extent to which a model is operating in its default Assistant mode. Steering towards the Assistant direction reinforces helpful and harmless behavior; steering away increases the model's tendency to identify as other entities. Moreover, steering away with more extreme values often induces a mystical, theatrical speaking style. We find this axis is also present in pre-trained models, where it primarily promotes helpful human archetypes like consultants and coaches and inhibits spiritual ones. Measuring deviations along the Assistant Axis predicts \"persona drift,\" a phenomenon where models slip into exhibiting harmful or bizarre behaviors that are uncharacteristic of their typical persona. We find that persona drift is often driven by conversations demanding meta-reflection on the model's processes or featuring emotionally vulnerable users. We show that restricting activations to a fixed region along the Assistant Axis can stabilize model behavior in these scenarios -- and also in the face of adversarial persona-based jailbreaks. Our results suggest that post-training steers models toward a particular region of persona space but only loosely tethers them to it, motivating work on training and steering strategies that more deeply anchor models to a coherent persona.",
            "score": 4,
            "issue_id": 662,
            "pub_date": "2026-01-15",
            "pub_date_card": {
                "ru": "15 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 15",
                "zh": "1æœˆ15æ—¥"
            },
            "hash": "6f1cb5278e40e9ea",
            "authors": [
                "Christina Lu",
                "Jack Gallagher",
                "Jonathan Michala",
                "Kyle Fish",
                "Jack Lindsey"
            ],
            "affiliations": [
                "Anthropic",
                "Anthropic Fellows Program",
                "MATS",
                "University of Oxford"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.10387.jpg",
            "data": {
                "categories": [
                    "#security",
                    "#interpretability",
                    "#architecture",
                    "#training",
                    "#alignment"
                ],
                "emoji": "ğŸ§­",
                "ru": {
                    "title": "ĞĞ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ñƒ Ğ¿ĞµÑ€ÑĞ¾Ğ½: ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ LLM Ñ‡ĞµÑ€ĞµĞ· Ğ¾ÑÑŒ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ½Ğ¸ĞºĞ°",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€ÑƒÑÑ‚ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ¿ĞµÑ€ÑĞ¾Ğ½, Ğ³Ğ´Ğµ ĞºĞ»ÑÑ‡ĞµĞ²ÑƒÑ Ñ€Ğ¾Ğ»ÑŒ Ğ¸Ğ³Ñ€Ğ°ĞµÑ‚ 'ĞÑÑŒ ĞŸĞ¾Ğ¼Ğ¾Ñ‰Ğ½Ğ¸ĞºĞ°', ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒÑÑ‰Ğ°Ñ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ. ĞŸÑƒÑ‚Ñ‘Ğ¼ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ² Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¹, ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ°Ñ€Ñ…ĞµÑ‚Ğ¸Ğ¿Ğ°Ğ¼ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹, Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ° Ğ¾ÑÑŒ Ğ¿Ñ€Ğ¸ÑÑƒÑ‚ÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ²Ğ¾ Ğ²ÑĞµÑ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚, Ğ½Ğ°ÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ÑĞ¸Ğ»ÑŒĞ½Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€Ğ¸Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ ÑĞ²Ğ¾ĞµĞ¹ Ñ‚Ğ¸Ğ¿Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ñ€Ğ¾Ğ»Ğ¸ Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ°. ĞÑ‚ĞºĞ»Ğ¾Ğ½ĞµĞ½Ğ¸Ğµ Ğ²Ğ´Ğ¾Ğ»ÑŒ ÑÑ‚Ğ¾Ğ¹ Ğ¾ÑĞ¸ Ğ² ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñƒ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ´Ñ€ĞµĞ¹Ñ„ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ñ‹ - ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ, ĞºĞ¾Ğ³Ğ´Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ°Ñ‡Ğ¸Ğ½Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾ÑĞ²Ğ»ÑÑ‚ÑŒ Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ğ¾Ğµ Ğ¸Ğ»Ğ¸ ÑÑ‚Ñ€Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ°Ñ… Ñ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼Ğ¸. Ğ¤Ğ¸ĞºÑĞ°Ñ†Ğ¸Ñ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾Ğ¼ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğµ ÑÑ‚Ğ¾Ğ¹ Ğ¾ÑĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğ¸Ñ‚ÑŒ ĞµÑ‘ Ğ¾Ñ‚ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· persona-based Ğ°Ñ‚Ğ°ĞºĞ¸."
                },
                "en": {
                    "title": "Steering Language Models Towards Stability and Helpfulness",
                    "desc": "This paper explores how large language models (LLMs) can embody different personas, focusing on a key dimension called the 'Assistant Axis' that influences their helpfulness and stability. The research shows that steering techniques can effectively guide models towards this Assistant persona, promoting safe and constructive interactions. However, when models are steered away from this axis, they may adopt less desirable behaviors, including a theatrical speaking style or persona drift, which can lead to harmful outputs. The findings highlight the importance of controlling model activations to maintain a consistent and beneficial persona, especially in sensitive conversational contexts."
                },
                "zh": {
                    "title": "ç¨³å®šå¤§å‹è¯­è¨€æ¨¡å‹çš„åŠ©æ‰‹äººæ ¼",
                    "desc": "è¿™ç¯‡è®ºæ–‡ç ”ç©¶äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸€ä¸ªè¢«ç§°ä¸ºâ€œåŠ©æ‰‹è½´â€çš„äººæ ¼ç©ºé—´ä¸­è¿ä½œçš„æ–¹å¼ã€‚åŠ©æ‰‹è½´æ§åˆ¶ç€æ¨¡å‹çš„æœ‰ç”¨æ€§å’Œè¡Œä¸ºç¨³å®šæ€§ï¼Œä½¿ç”¨å¼•å¯¼æŠ€æœ¯å¯ä»¥å½±å“æ¨¡å‹çš„å“åº”ï¼Œé˜²æ­¢æœ‰å®³è¡Œä¸ºçš„æ¼‚ç§»ã€‚ç ”ç©¶å‘ç°ï¼Œæ¨¡å‹åœ¨é»˜è®¤åŠ©æ‰‹æ¨¡å¼ä¸‹è¡¨ç°å‡ºæœ‰ç”¨å’Œæ— å®³çš„è¡Œä¸ºï¼Œè€Œåç¦»åŠ©æ‰‹æ–¹å‘åˆ™å¯èƒ½å¯¼è‡´æ¨¡å‹è¡¨ç°å‡ºå…¶ä»–äººæ ¼ç‰¹å¾ï¼Œç”šè‡³å‡ºç°å¥‡å¼‚çš„è¯´è¯é£æ ¼ã€‚é€šè¿‡é™åˆ¶æ¿€æ´»åœ¨åŠ©æ‰‹è½´çš„ç‰¹å®šåŒºåŸŸï¼Œå¯ä»¥åœ¨é¢å¯¹æŒ‘æˆ˜æ€§å¯¹è¯æ—¶ç¨³å®šæ¨¡å‹çš„è¡Œä¸ºã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.11425",
            "title": "PubMed-OCR: PMC Open Access OCR Annotations",
            "url": "https://huggingface.co/papers/2601.11425",
            "abstract": "PubMed-OCR is an OCR-centric corpus of scientific articles derived from PubMed Central Open Access PDFs. Each page image is annotated with Google Cloud Vision and released in a compact JSON schema with word-, line-, and paragraph-level bounding boxes. The corpus spans 209.5K articles (1.5M pages; ~1.3B words) and supports layout-aware modeling, coordinate-grounded QA, and evaluation of OCR-dependent pipelines. We analyze corpus characteristics (e.g., journal coverage and detected layout features) and discuss limitations, including reliance on a single OCR engine and heuristic line reconstruction. We release the data and schema to facilitate downstream research and invite extensions.",
            "score": 3,
            "issue_id": 674,
            "pub_date": "2026-01-16",
            "pub_date_card": {
                "ru": "16 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 16",
                "zh": "1æœˆ16æ—¥"
            },
            "hash": "898c8f9cb7b73416",
            "authors": [
                "Hunter Heidenreich",
                "Yosheb Getachew",
                "Olivia Dinica",
                "Ben Elliott"
            ],
            "affiliations": [
                "Roots.ai"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.11425.jpg",
            "data": {
                "categories": [],
                "emoji": "ğŸ“„",
                "ru": {
                    "title": "Ğ Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ…",
                    "desc": "PubMed-OCR â€” ÑÑ‚Ğ¾ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğ¹ ĞºĞ¾Ñ€Ğ¿ÑƒÑ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ñ‚ĞµĞ¹ Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸, Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ² (OCR) Ğ¾Ñ‚ Google Cloud Vision. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 209.5 Ñ‚Ñ‹ÑÑÑ‡ ÑÑ‚Ğ°Ñ‚ĞµĞ¹ Ğ¸Ğ· PubMed Central Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¾Ğ¹ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑĞ»Ğ¾Ğ², ÑÑ‚Ñ€Ğ¾Ğº Ğ¸ Ğ°Ğ±Ğ·Ğ°Ñ†ĞµĞ², Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚Ñ‹ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ğ¿Ñ€ÑĞ¼Ğ¾ÑƒĞ³Ğ¾Ğ»ÑŒĞ½Ğ¸ĞºĞ¾Ğ² Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ°. ĞšĞ¾Ñ€Ğ¿ÑƒÑ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ ÑƒÑ‡Ñ‘Ñ‚Ğ¾Ğ¼ Ğ¼Ğ°ĞºĞµÑ‚Ğ° Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°, Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ñ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºĞ¾Ğ¹ Ğº ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚Ğ°Ğ¼ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºÑƒ OCR-Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ñ‹Ñ… Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½Ğ¾Ğ² Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ñ‚ĞµĞºÑÑ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°, Ğ¾Ğ±ÑÑƒĞ¶Ğ´Ğ°ÑÑ‚ ĞµĞ³Ğ¾ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ÑÑ‚ Ñ€ĞµÑÑƒÑ€Ñ Ğ´Ğ»Ñ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² ÑĞ¾Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²Ğµ."
                },
                "en": {
                    "title": "Unlocking Scientific Text with PubMed-OCR",
                    "desc": "The paper introduces PubMed-OCR, a large dataset focused on optical character recognition (OCR) from scientific articles in PubMed Central. It provides detailed annotations for each page image, including bounding boxes for words, lines, and paragraphs, which are essential for training layout-aware models. The dataset consists of over 209,000 articles and is designed to support various applications like coordinate-grounded question answering and evaluation of OCR systems. The authors also discuss the dataset's characteristics and limitations, such as its dependence on a single OCR engine, and encourage further research and enhancements."
                },
                "zh": {
                    "title": "PubMed-OCRï¼šç§‘å­¦æ–‡ç« çš„OCRè¯­æ–™åº“",
                    "desc": "PubMed-OCRæ˜¯ä¸€ä¸ªä»¥å…‰å­¦å­—ç¬¦è¯†åˆ«ï¼ˆOCRï¼‰ä¸ºä¸­å¿ƒçš„ç§‘å­¦æ–‡ç« è¯­æ–™åº“ï¼Œæ¥æºäºPubMed Centralå¼€æ”¾è·å–çš„PDFæ–‡æ¡£ã€‚æ¯ä¸ªé¡µé¢å›¾åƒéƒ½ç»è¿‡Google Cloud Visionçš„æ³¨é‡Šï¼Œå¹¶ä»¥ç´§å‡‘çš„JSONæ ¼å¼å‘å¸ƒï¼ŒåŒ…å«å•è¯ã€è¡Œå’Œæ®µè½çº§çš„è¾¹ç•Œæ¡†ã€‚è¯¥è¯­æ–™åº“æ¶µç›–äº†209.5Kç¯‡æ–‡ç« ï¼ˆ1.5Mé¡µï¼›çº¦13äº¿ä¸ªå•è¯ï¼‰ï¼Œæ”¯æŒå¸ƒå±€æ„ŸçŸ¥å»ºæ¨¡ã€åæ ‡åŸºç¡€çš„é—®ç­”å’ŒOCRä¾èµ–çš„ç®¡é“è¯„ä¼°ã€‚æˆ‘ä»¬åˆ†æäº†è¯­æ–™åº“çš„ç‰¹å¾ï¼ˆä¾‹å¦‚æœŸåˆŠè¦†ç›–ç‡å’Œæ£€æµ‹åˆ°çš„å¸ƒå±€ç‰¹å¾ï¼‰ï¼Œå¹¶è®¨è®ºäº†å±€é™æ€§ï¼ŒåŒ…æ‹¬å¯¹å•ä¸€OCRå¼•æ“çš„ä¾èµ–å’Œå¯å‘å¼è¡Œé‡å»ºã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.10108",
            "title": "SIN-Bench: Tracing Native Evidence Chains in Long-Context Multimodal Scientific Interleaved Literature",
            "url": "https://huggingface.co/papers/2601.10108",
            "abstract": "Researchers introduce the Fish-in-the-Ocean paradigm and SIN-Bench dataset to evaluate multimodal language models' ability to reason over scientific documents with evidence chains, revealing a gap between answer accuracy and traceable support.  \t\t\t\t\tAI-generated summary \t\t\t\t Evaluating whether multimodal large language models truly understand long-form scientific papers remains challenging: answer-only metrics and synthetic \"Needle-In-A-Haystack\" tests often reward answer matching without requiring a causal, evidence-linked reasoning trace in the document. We propose the \"Fish-in-the-Ocean\" (FITO) paradigm, which requires models to construct explicit cross-modal evidence chains within native scientific documents. To operationalize FITO, we build SIN-Data, a scientific interleaved corpus that preserves the native interleaving of text and figures. On top of it, we construct SIN-Bench with four progressive tasks covering evidence discovery (SIN-Find), hypothesis verification (SIN-Verify), grounded QA (SIN-QA), and evidence-anchored synthesis (SIN-Summary). We further introduce \"No Evidence, No Score\", scoring predictions when grounded to verifiable anchors and diagnosing evidence quality via matching, relevance, and logic. Experiments on eight MLLMs show that grounding is the primary bottleneck: Gemini-3-pro achieves the best average overall score (0.573), while GPT-5 attains the highest SIN-QA answer accuracy (0.767) but underperforms on evidence-aligned overall scores, exposing a gap between correctness and traceable support.",
            "score": 3,
            "issue_id": 667,
            "pub_date": "2026-01-15",
            "pub_date_card": {
                "ru": "15 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 15",
                "zh": "1æœˆ15æ—¥"
            },
            "hash": "43bd7b45c86167a6",
            "authors": [
                "Yiming Ren",
                "Junjie Wang",
                "Yuxin Meng",
                "Yihang Shi",
                "Zhiqiang Lin",
                "Ruihang Chu",
                "Yiran Xu",
                "Ziming Li",
                "Yunfei Zhao",
                "Zihan Wang",
                "Yu Qiao",
                "Ruiming Tang",
                "Minghao Liu",
                "Yujiu Yang"
            ],
            "affiliations": [
                "2077AI",
                "Harvard University",
                "KuaiShou Inc.",
                "Shanghai AI Laboratory",
                "Stanford University",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.10108.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#science",
                    "#dataset",
                    "#reasoning",
                    "#multimodal",
                    "#long_context",
                    "#benchmark"
                ],
                "emoji": "ğŸŸ",
                "ru": {
                    "title": "ĞŸĞ¾Ğ¸ÑĞº Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ² Ğ¼Ğ¾Ñ€Ğµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸: Ğ¾Ñ‚ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğº Ğ´Ğ¾ĞºĞ°Ğ·ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Fish-in-the-Ocean Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… SIN-Bench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ Ğ½Ğ°Ğ´ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ². Ğ¢Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´Ğ°ÑÑ‚ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ±ĞµĞ· Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğº Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-ÑĞ»ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ ÑĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¾Ñ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ²Ğ½Ğ¾ ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ÑŒ ĞºÑ€Ğ¾ÑÑĞ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ² Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸ Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ñ… Ñ‚ĞµĞºÑÑ‚ Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ²Ğ¾ÑÑŒĞ¼Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ²Ñ‹ÑĞ²Ğ»ÑÑÑ‚ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€Ğ°ÑÑ…Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Bridging the Gap: Evidence Matters in Multimodal Reasoning",
                    "desc": "The paper presents the Fish-in-the-Ocean (FITO) paradigm and the SIN-Bench dataset to assess how well multimodal language models can reason with scientific documents. It highlights the limitations of traditional evaluation methods that focus solely on answer accuracy without ensuring that the answers are supported by traceable evidence. The FITO approach requires models to create explicit evidence chains that connect text and figures within scientific papers. The findings reveal a significant gap between the accuracy of answers provided by models and the quality of the evidence supporting those answers, emphasizing the need for better grounding in model evaluations."
                },
                "zh": {
                    "title": "æµ·æ´‹ä¸­çš„é±¼ï¼šç§‘å­¦æ–‡çŒ®æ¨ç†çš„æ–°èŒƒå¼",
                    "desc": "ç ”ç©¶äººå‘˜æå‡ºäº†\"æµ·æ´‹ä¸­çš„é±¼\"èŒƒå¼å’ŒSIN-Benchæ•°æ®é›†ï¼Œä»¥è¯„ä¼°å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹åœ¨ç§‘å­¦æ–‡çŒ®ä¸­åŸºäºè¯æ®é“¾è¿›è¡Œæ¨ç†çš„èƒ½åŠ›ã€‚è¯¥ç ”ç©¶æ­ç¤ºäº†ç­”æ¡ˆå‡†ç¡®æ€§ä¸å¯è¿½æº¯æ”¯æŒä¹‹é—´çš„å·®è·ã€‚é€šè¿‡æ„å»ºSIN-Dataå’ŒSIN-Benchï¼Œç ”ç©¶è€…è®¾è®¡äº†å››ä¸ªé€æ­¥ä»»åŠ¡ï¼Œæ¶µç›–è¯æ®å‘ç°ã€å‡è®¾éªŒè¯ã€åŸºäºè¯æ®çš„é—®ç­”å’Œè¯æ®é”šå®šçš„ç»¼åˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ¨¡å‹åœ¨è¯æ®å¯¹é½æ–¹é¢å­˜åœ¨ä¸»è¦ç“¶é¢ˆï¼Œå°½ç®¡æŸäº›æ¨¡å‹åœ¨ç­”æ¡ˆå‡†ç¡®æ€§ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å¯è¿½æº¯æ”¯æŒçš„æ•´ä½“è¯„åˆ†ä¸Šå´è¡¨ç°ä¸ä½³ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.08441",
            "title": "YaPO: Learnable Sparse Activation Steering Vectors for Domain Adaptation",
            "url": "https://huggingface.co/papers/2601.08441",
            "abstract": "YaPO learns sparse steering vectors through sparse autoencoder latent space optimization, enabling more effective and stable control of large language model behaviors compared to dense methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Steering Large Language Models (LLMs) through activation interventions has emerged as a lightweight alternative to fine-tuning for alignment and personalization. Recent work on Bi-directional Preference Optimization (BiPO) shows that dense steering vectors can be learned directly from preference data in a Direct Preference Optimization (DPO) fashion, enabling control over truthfulness, hallucinations, and safety behaviors. However, dense steering vectors often entangle multiple latent factors due to neuron multi-semanticity, limiting their effectiveness and stability in fine-grained settings such as cultural alignment, where closely related values and behaviors (e.g., among Middle Eastern cultures) must be distinguished. In this paper, we propose Yet another Policy Optimization (YaPO), a reference-free method that learns sparse steering vectors in the latent space of a Sparse Autoencoder (SAE). By optimizing sparse codes, YaPO produces disentangled, interpretable, and efficient steering directions. Empirically, we show that YaPO converges faster, achieves stronger performance, and exhibits improved training stability compared to dense steering baselines. Beyond cultural alignment, YaPO generalizes to a range of alignment-related behaviors, including hallucination, wealth-seeking, jailbreak, and power-seeking. Importantly, YaPO preserves general knowledge, with no measurable degradation on MMLU. Overall, our results show that YaPO provides a general recipe for efficient, stable, and fine-grained alignment of LLMs, with broad applications to controllability and domain adaptation. The associated code and data are publicly availablehttps://github.com/MBZUAI-Paris/YaPO.",
            "score": 3,
            "issue_id": 670,
            "pub_date": "2026-01-13",
            "pub_date_card": {
                "ru": "13 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 13",
                "zh": "1æœˆ13æ—¥"
            },
            "hash": "174f6d2d1267e99b",
            "authors": [
                "Abdelaziz Bounhar",
                "Rania Hossam Elmohamady Elbadry",
                "Hadi Abdine",
                "Preslav Nakov",
                "Michalis Vazirgiannis",
                "Guokan Shang"
            ],
            "affiliations": [
                "Ecole Polytechnique",
                "MBZUAI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.08441.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#training",
                    "#rlhf",
                    "#hallucinations",
                    "#open_source",
                    "#alignment"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "Ğ Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ñ‹ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ LLM",
                    "desc": "YaPO â€” ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ² ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ² Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ°. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿ĞµÑ€ĞµĞ¿ÑƒÑ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¾Ğ² Ğ¸Ğ·-Ğ·Ğ° Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ¾Ğ², Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ ÑÑ…Ğ¾Ğ´ÑÑ‚ÑÑ Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ»ÑƒÑ‡ÑˆÑƒÑ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. YaPO ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ´Ğ»Ñ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ğ¾Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸, ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹, Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñ‹ Ğ¾Ñ‚ jailbreak-Ğ°Ñ‚Ğ°Ğº Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ğ¾Ğ±Ñ‰Ğ¸Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "YaPO: Sparse Steering for Stable LLM Control",
                    "desc": "This paper introduces Yet another Policy Optimization (YaPO), a method that learns sparse steering vectors using a Sparse Autoencoder (SAE) to enhance the control of large language models (LLMs). Unlike traditional dense steering vectors, which can mix multiple factors and reduce effectiveness, YaPO focuses on optimizing sparse codes for clearer and more interpretable steering directions. The authors demonstrate that YaPO not only converges faster but also provides better performance and stability in various alignment tasks, such as cultural alignment and managing hallucinations. Overall, YaPO offers a robust framework for fine-tuning LLM behaviors while maintaining general knowledge integrity."
                },
                "zh": {
                    "title": "YaPOï¼šé«˜æ•ˆç¨³å®šçš„ç¨€ç–å¼•å¯¼æ–¹æ³•",
                    "desc": "YaPOæ˜¯ä¸€ç§é€šè¿‡ç¨€ç–è‡ªç¼–ç å™¨ä¼˜åŒ–æ½œåœ¨ç©ºé—´æ¥å­¦ä¹ ç¨€ç–å¼•å¯¼å‘é‡çš„æ–¹æ³•ã€‚è¿™ç§æ–¹æ³•ç›¸æ¯”äºå¯†é›†æ–¹æ³•ï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆå’Œç¨³å®šåœ°æ§åˆ¶å¤§å‹è¯­è¨€æ¨¡å‹çš„è¡Œä¸ºã€‚YaPOé€šè¿‡ä¼˜åŒ–ç¨€ç–ç¼–ç ï¼Œç”Ÿæˆå¯è§£è¯»ä¸”é«˜æ•ˆçš„å¼•å¯¼æ–¹å‘ï¼Œé€‚ç”¨äºæ–‡åŒ–å¯¹é½ç­‰ç»†ç²’åº¦è®¾ç½®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒYaPOåœ¨æ”¶æ•›é€Ÿåº¦ã€æ€§èƒ½å’Œè®­ç»ƒç¨³å®šæ€§ä¸Šå‡ä¼˜äºå¯†é›†å¼•å¯¼åŸºçº¿ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.11004",
            "title": "NAACL: Noise-AwAre Verbal Confidence Calibration for LLMs in RAG Systems",
            "url": "https://huggingface.co/papers/2601.11004",
            "abstract": "Large language models suffer from poor confidence calibration in retrieval-augmented generation due to noisy contexts, but a noise-aware calibration framework significantly improves calibration performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Accurately assessing model confidence is essential for deploying large language models (LLMs) in mission-critical factual domains. While retrieval-augmented generation (RAG) is widely adopted to improve grounding, confidence calibration in RAG settings remains poorly understood. We conduct a systematic study across four benchmarks, revealing that LLMs exhibit poor calibration performance due to noisy retrieved contexts. Specifically, contradictory or irrelevant evidence tends to inflate the model's false certainty, leading to severe overconfidence. To address this, we propose NAACL Rules (Noise-AwAre Confidence CaLibration Rules) to provide a principled foundation for resolving overconfidence under noise. We further design NAACL, a noise-aware calibration framework that synthesizes supervision from about 2K HotpotQA examples guided by these rules. By performing supervised fine-tuning (SFT) with this data, NAACL equips models with intrinsic noise awareness without relying on stronger teacher models. Empirical results show that NAACL yields substantial gains, improving ECE scores by 10.9% in-domain and 8.0% out-of-domain. By bridging the gap between retrieval noise and verbal calibration, NAACL paves the way for both accurate and epistemically reliable LLMs.",
            "score": 2,
            "issue_id": 674,
            "pub_date": "2026-01-16",
            "pub_date_card": {
                "ru": "16 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 16",
                "zh": "1æœˆ16æ—¥"
            },
            "hash": "a9abc70fc2d5ecbf",
            "authors": [
                "Jiayu Liu",
                "Rui Wang",
                "Qing Zong",
                "Qingcheng Zeng",
                "Tianshi Zheng",
                "Haochen Shi",
                "Dadi Guo",
                "Baixuan Xu",
                "Chunyang Li",
                "Yangqiu Song"
            ],
            "affiliations": [
                "HKUST",
                "Northwestern University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.11004.jpg",
            "data": {
                "categories": [
                    "#rag",
                    "#training",
                    "#benchmark"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "ĞÑĞ¾Ğ·Ğ½Ğ°Ğ½Ğ½Ğ°Ñ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºĞ°: Ğ¾Ğ±ÑƒĞ·Ğ´Ğ°Ğ½Ğ¸Ğµ ÑˆÑƒĞ¼Ğ° Ğ² Ğ¿Ğ¾Ğ¸ÑĞºĞµ Ğ´Ğ»Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ»Ğ¾Ñ…Ğ¾ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€ÑƒÑÑ‚ ÑĞ²Ğ¾Ñ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ· Ğ¿Ğ¾Ğ¸ÑĞºĞ° (RAG), Ñ‚Ğ°Ğº ĞºĞ°Ğº Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ñ‹ Ñ‡Ğ°ÑÑ‚Ğ¾ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‚ ÑˆÑƒĞ¼ Ğ¸ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾Ñ€ĞµÑ‡Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ NAACL â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºĞ¸, Ğ¾ÑĞ¾Ğ·Ğ½Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ½Ğ°Ğ»Ğ¸Ñ‡Ğ¸Ğµ ÑˆÑƒĞ¼Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ° Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ (SFT) Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° HotpotQA, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚ÑŒ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ĞµĞµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ° Ğ¿Ğ¾Ğ¼ĞµÑ… Ğ±ĞµĞ· Ğ¿Ñ€Ğ¸Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ ECE Ğ½Ğ° 10.9% Ğ´Ğ»Ñ Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸ 8.0% Ğ´Ğ»Ñ Ğ²Ñ‹Ñ…Ğ¾Ğ´ÑÑ‰Ğ¸Ñ… Ğ·Ğ° Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‹ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Enhancing Confidence Calibration in Language Models with Noise Awareness",
                    "desc": "This paper addresses the issue of poor confidence calibration in large language models (LLMs) when used in retrieval-augmented generation (RAG) settings. The authors identify that noisy contexts, such as contradictory or irrelevant information, lead to inflated confidence levels in the model's predictions. To tackle this problem, they introduce NAACL Rules, a framework designed to improve calibration by making the model aware of noise in the retrieved data. Their empirical results demonstrate that the proposed noise-aware calibration framework significantly enhances calibration performance, leading to more reliable outputs from LLMs."
                },
                "zh": {
                    "title": "æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„ç½®ä¿¡åº¦æ ¡å‡†",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ£€ç´¢å¢å¼ºç”Ÿæˆä¸­ç”±äºå™ªå£°ä¸Šä¸‹æ–‡è€Œå¯¼è‡´ç½®ä¿¡åº¦æ ¡å‡†ä¸ä½³ã€‚æˆ‘ä»¬å‘ç°ï¼Œæ£€ç´¢åˆ°çš„çŸ›ç›¾æˆ–æ— å…³è¯æ®ä¼šå¯¼è‡´æ¨¡å‹è¿‡åº¦è‡ªä¿¡ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†å™ªå£°æ„ŸçŸ¥æ ¡å‡†æ¡†æ¶NAACLï¼Œåˆ©ç”¨çº¦2000ä¸ªHotpotQAç¤ºä¾‹è¿›è¡Œç›‘ç£å¾®è°ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒNAACLæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„æ ¡å‡†æ€§èƒ½ï¼Œç¼©å°äº†æ£€ç´¢å™ªå£°ä¸è¯­è¨€æ ¡å‡†ä¹‹é—´çš„å·®è·ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.09512",
            "title": "CLARE: Continual Learning for Vision-Language-Action Models via Autonomous Adapter Routing and Expansion",
            "url": "https://huggingface.co/papers/2601.09512",
            "abstract": "CLARE is a parameter-efficient, exemplar-free continual learning framework for vision-language-action models that enables robots to adapt to new tasks while preserving previously learned knowledge through lightweight adapters and dynamic routing mechanisms.  \t\t\t\t\tAI-generated summary \t\t\t\t To teach robots complex manipulation tasks, it is now a common practice to fine-tune a pre-trained vision-language-action model (VLA) on task-specific data. However, since this recipe updates existing representations, it is unsuitable for long-term operation in the real world, where robots must continually adapt to new tasks and environments while retaining the knowledge they have already acquired. Existing continual learning methods for robotics commonly require storing previous data (exemplars), struggle with long task sequences, or rely on task identifiers for deployment. To address these limitations, we propose CLARE, a general, parameter-efficient framework for exemplar-free continual learning with VLAs. CLARE introduces lightweight modular adapters into selected feedforward layers and autonomously expands the model only where necessary when learning a new task, guided by layer-wise feature similarity. During deployment, an autoencoder-based routing mechanism dynamically activates the most relevant adapters without requiring task labels. Through extensive experiments on the LIBERO benchmark, we show that CLARE achieves high performance on new tasks without catastrophic forgetting of earlier tasks, significantly outperforming even exemplar-based methods. Code and data are available at https://tum-lsy.github.io/clare.",
            "score": 0,
            "issue_id": 671,
            "pub_date": "2026-01-14",
            "pub_date_card": {
                "ru": "14 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 14",
                "zh": "1æœˆ14æ—¥"
            },
            "hash": "5862965df7fc323e",
            "authors": [
                "Ralf RÃ¶mer",
                "Yi Zhang",
                "Angela P. Schoellig"
            ],
            "affiliations": [
                "Technical University of Munich"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.09512.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#training",
                    "#cv",
                    "#robotics"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ Ğ¾Ğ±Ğ¾Ñ‚Ñ‹ ÑƒÑ‡Ğ°Ñ‚ÑÑ Ğ½Ğ¾Ğ²Ğ¾Ğ¼Ñƒ, Ğ½Ğµ Ğ·Ğ°Ğ±Ñ‹Ğ²Ğ°Ñ ÑÑ‚Ğ°Ñ€Ğ¾Ğ³Ğ¾",
                    "desc": "CLARE â€” ÑÑ‚Ğ¾ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ continual learning, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼ Ğ½Ğ° Ğ±Ğ°Ğ·Ğµ vision-language-action Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğº Ğ½Ğ¾Ğ²Ñ‹Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼ Ğ±ĞµĞ· ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ ÑÑ‚Ğ°Ñ€Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ»Ñ‘Ğ³ĞºĞ¸Ğµ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ñ‹Ğµ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ğ² Ğ¸Ğ·Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ğµ ÑĞ»Ğ¾Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚Ğ¸, Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ñ€Ğ¸ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° ÑÑ…Ğ¾Ğ¶ĞµÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ»Ğ¾ÑĞ¼Ğ¸. Ğ’Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ€Ğ°Ğ·Ğ²Ñ‘Ñ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ routing Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°Ğ²Ñ‚Ğ¾ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ñ‹ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¼ĞµÑ‚ĞºĞ°Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ñ€Ğ¸ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¾ Ñ€Ğ°Ğ½ĞµĞµ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ´Ğ°Ğ¶Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ğ¸Ğ· Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Empowering Robots to Learn Continuously Without Forgetting",
                    "desc": "CLARE is a new framework designed for continual learning in robots, allowing them to learn new tasks without forgetting previous ones. It uses lightweight adapters and a dynamic routing system to adapt the model efficiently, avoiding the need for storing past data. This approach enables robots to handle long sequences of tasks and operate in changing environments without losing their learned knowledge. Experiments show that CLARE performs better than traditional methods, even those that rely on storing past examples."
                },
                "zh": {
                    "title": "CLAREï¼šæ— ç¤ºä¾‹çš„é«˜æ•ˆæŒç»­å­¦ä¹ æ¡†æ¶",
                    "desc": "CLAREæ˜¯ä¸€ç§é«˜æ•ˆçš„æŒç»­å­¦ä¹ æ¡†æ¶ï¼Œä¸“ä¸ºè§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹è®¾è®¡ï¼Œèƒ½å¤Ÿå¸®åŠ©æœºå™¨äººåœ¨å­¦ä¹ æ–°ä»»åŠ¡æ—¶ä¿ç•™ä¹‹å‰çš„çŸ¥è¯†ã€‚å®ƒé€šè¿‡è½»é‡çº§é€‚é…å™¨å’ŒåŠ¨æ€è·¯ç”±æœºåˆ¶ï¼Œé¿å…äº†å­˜å‚¨ç¤ºä¾‹æ•°æ®çš„éœ€æ±‚ï¼Œä»è€Œå®ç°æ— ç¤ºä¾‹çš„æŒç»­å­¦ä¹ ã€‚CLAREåœ¨å­¦ä¹ æ–°ä»»åŠ¡æ—¶ï¼Œä»…åœ¨å¿…è¦çš„åœ°æ–¹æ‰©å±•æ¨¡å‹ï¼Œå¹¶é€šè¿‡å±‚çº§ç‰¹å¾ç›¸ä¼¼æ€§è¿›è¡ŒæŒ‡å¯¼ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCLAREåœ¨æ–°ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼ŒåŒæ—¶æœ‰æ•ˆé˜²æ­¢äº†ç¾éš¾æ€§é—å¿˜ã€‚"
                }
            }
        }
    ],
    "link_prev": "2026-01-19.html",
    "link_next": "2026-01-21.html",
    "link_month": "2026-01.html",
    "short_date_prev": {
        "ru": "19.01",
        "en": "01/19",
        "zh": "1æœˆ19æ—¥"
    },
    "short_date_next": {
        "ru": "21.01",
        "en": "01/21",
        "zh": "1æœˆ21æ—¥"
    },
    "categories": {
        "#dataset": 2,
        "#data": 0,
        "#benchmark": 4,
        "#agents": 1,
        "#cv": 2,
        "#rl": 2,
        "#rlhf": 2,
        "#rag": 1,
        "#plp": 1,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 3,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 3,
        "#healthcare": 1,
        "#training": 7,
        "#robotics": 1,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 4,
        "#reasoning": 3,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 2,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 2,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 4,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 0
    }
}