{
    "date": {
        "ru": "8 ноября",
        "en": "November 8",
        "zh": "11月8日"
    },
    "time_utc": "2024-11-08 02:42",
    "weekday": 4,
    "issue_id": 464,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2411.05003",
            "title": "ReCapture: Generative Video Camera Controls for User-Provided Videos using Masked Video Fine-Tuning",
            "url": "https://huggingface.co/papers/2411.05003",
            "abstract": "Recently, breakthroughs in video modeling have allowed for controllable camera trajectories in generated videos. However, these methods cannot be directly applied to user-provided videos that are not generated by a video model. In this paper, we present ReCapture, a method for generating new videos with novel camera trajectories from a single user-provided video. Our method allows us to re-generate the reference video, with all its existing scene motion, from vastly different angles and with cinematic camera motion. Notably, using our method we can also plausibly hallucinate parts of the scene that were not observable in the reference video. Our method works by (1) generating a noisy anchor video with a new camera trajectory using multiview diffusion models or depth-based point cloud rendering and then (2) regenerating the anchor video into a clean and temporally consistent reangled video using our proposed masked video fine-tuning technique.",
            "score": 7,
            "issue_id": 464,
            "pub_date": "2024-11-07",
            "pub_date_card": {
                "ru": "7 ноября",
                "en": "November 7",
                "zh": "11月7日"
            },
            "hash": "f71f2e0f1addbe57",
            "data": {
                "categories": [
                    "#video",
                    "#diffusion",
                    "#hallucinations"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Переснимаем реальность: новые ракурсы для любого видео",
                    "desc": "ReCapture - это метод генерации новых видео с измененными траекториями камеры на основе одного пользовательского видео. Он позволяет перегенерировать исходное видео с другими углами обзора и кинематографическим движением камеры, включая правдоподобное восстановление невидимых частей сцены. Метод работает в два этапа: сначала создается шумное опорное видео с новой траекторией камеры, а затем оно очищается и делается временно согласованным. ReCapture использует мультивидовые диффузионные модели и рендеринг облака точек на основе глубины."
                },
                "en": {
                    "title": "ReCapture: Transforming User Videos with New Camera Perspectives",
                    "desc": "This paper introduces ReCapture, a novel method for generating new videos with different camera angles from a single user-provided video. It leverages multiview diffusion models and depth-based point cloud rendering to create an initial noisy video with a new camera trajectory. The method then refines this video using a masked video fine-tuning technique to ensure temporal consistency and clarity. Additionally, ReCapture can convincingly generate parts of the scene that were not visible in the original video, enhancing the overall viewing experience."
                },
                "zh": {
                    "title": "ReCapture：从用户视频生成新视角的魔法",
                    "desc": "最近在视频建模方面取得了突破，使得生成视频中的相机轨迹可控。然而，这些方法无法直接应用于用户提供的非生成视频。本文提出了一种名为ReCapture的方法，可以从单个用户提供的视频生成具有新相机轨迹的新视频。该方法不仅能够从不同角度重新生成参考视频，还能合理地幻觉出参考视频中不可见的场景部分。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.04709",
            "title": "TIP-I2V: A Million-Scale Real Text and Image Prompt Dataset for Image-to-Video Generation",
            "url": "https://huggingface.co/papers/2411.04709",
            "abstract": "Video generation models are revolutionizing content creation, with image-to-video models drawing increasing attention due to their enhanced controllability, visual consistency, and practical applications. However, despite their popularity, these models rely on user-provided text and image prompts, and there is currently no dedicated dataset for studying these prompts. In this paper, we introduce TIP-I2V, the first large-scale dataset of over 1.70 million unique user-provided Text and Image Prompts specifically for Image-to-Video generation. Additionally, we provide the corresponding generated videos from five state-of-the-art image-to-video models. We begin by outlining the time-consuming and costly process of curating this large-scale dataset. Next, we compare TIP-I2V to two popular prompt datasets, VidProM (text-to-video) and DiffusionDB (text-to-image), highlighting differences in both basic and semantic information. This dataset enables advancements in image-to-video research. For instance, to develop better models, researchers can use the prompts in TIP-I2V to analyze user preferences and evaluate the multi-dimensional performance of their trained models; and to enhance model safety, they may focus on addressing the misinformation issue caused by image-to-video models. The new research inspired by TIP-I2V and the differences with existing datasets emphasize the importance of a specialized image-to-video prompt dataset. The project is publicly available at https://tip-i2v.github.io.",
            "score": 1,
            "issue_id": 464,
            "pub_date": "2024-11-05",
            "pub_date_card": {
                "ru": "5 ноября",
                "en": "November 5",
                "zh": "11月5日"
            },
            "hash": "fcc8e4daf79a82b9",
            "data": {
                "categories": [
                    "#dataset",
                    "#video"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "TIP-I2V: Революция в изучении промптов для генерации видео из изображений",
                    "desc": "Исследователи представили TIP-I2V - первый крупномасштабный набор данных, содержащий более 1,70 миллиона уникальных пользовательских текстовых и изображений-промптов для генерации видео из изображений. Датасет также включает соответствующие сгенерированные видео от пяти современных моделей преобразования изображений в видео. TIP-I2V позволяет анализировать предпочтения пользователей, оценивать многомерную производительность обученных моделей и решать проблемы безопасности, связанные с дезинформацией. Этот набор данных подчеркивает важность специализированного датасета промптов для генерации видео из изображений и открывает новые возможности для исследований в этой области."
                },
                "en": {
                    "title": "Empowering Image-to-Video Generation with TIP-I2V Dataset",
                    "desc": "This paper presents TIP-I2V, the first large-scale dataset containing over 1.70 million unique user-provided text and image prompts for image-to-video generation. The dataset aims to enhance the controllability and visual consistency of video generation models by providing a rich source of prompts. It also includes generated videos from five advanced image-to-video models, facilitating comparative analysis and model evaluation. By addressing the lack of dedicated datasets, TIP-I2V supports research in user preferences and model safety, particularly in mitigating misinformation issues."
                },
                "zh": {
                    "title": "TIP-I2V：图像到视频生成的新数据集",
                    "desc": "视频生成模型正在改变内容创作，图像到视频模型因其更好的可控性和视觉一致性而受到关注。尽管这些模型很受欢迎，但目前缺乏专门用于研究用户提供的文本和图像提示的数据集。本文介绍了TIP-I2V，这是第一个大规模的数据集，包含超过170万个独特的用户提供的文本和图像提示，专门用于图像到视频生成。该数据集的推出将推动图像到视频研究的进展，帮助研究人员分析用户偏好并评估模型性能。"
                }
            }
        }
    ],
    "link_prev": "2024-11-07.html",
    "link_next": "2024-11-11.html",
    "link_month": "2024-11.html",
    "short_date_prev": {
        "ru": "07.11",
        "en": "11/07",
        "zh": "11月7日"
    },
    "short_date_next": {
        "ru": "11.11",
        "en": "11/11",
        "zh": "11月11日"
    },
    "categories": {
        "#dataset": 1,
        "#data": 0,
        "#benchmark": 0,
        "#agents": 0,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 2,
        "#multimodal": 0,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#medicine": 0,
        "#training": 0,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#edge_computing": 0,
        "#optimization": 0,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 0,
        "#synthetic": 0,
        "#translation": 0
    },
    "zh": {
        "text": "这篇文章介绍了一种改进的检索增强生成（RAG）方法，称为HtmlRAG。传统的RAG系统从网页检索信息，提取纯文本喂给大语言模型（LLMs）。然而，这会丢失HTML中的结构和语义信息。HtmlRAG直接使用HTML格式的知识，保留更多信息。但HTML包含额外的标签和噪声，作者提出了清理和压缩策略来解决这个问题。实验证明，HtmlRAG在六个问答数据集上表现更好。",
        "title": "HtmlRAG: HTML is Better Than Plain Text for Modeling Retrieved Knowledge in RAG Systems",
        "pinyin": "这篇文章介绍了一种改进的检索增强生成（RAG）方法，称为HtmlRAG。传统的RAG系统从网页检索信息，提取纯文本喂给大语言模型（LLMs）。然而，这会丢失HTML中的结构和语义信息。HtmlRAG直接使用HTML格式的知识，保留更多信息。但HTML包含额外的标签和噪声，作者提出了清理和压缩策略来解决这个问题。实验证明，HtmlRAG在六个问答数据集上表现更好。\n\nzhè piān wén zhāng jiè shào le yī zhǒng gǎi jìn de jiǎn suǒ zēng qiáng shēng chéng (RAG) fāng fǎ, chēng wéi HtmlRAG. chuántǒng de RAG xì tǒng cóng wǎng yè jiǎn suǒ xìn xī, tī qǔ chún wén běn wèi gěi dà yǔ yán mó xìng (LLMs). rán ér, zhè huì diū shī HTML zhōng de jiè gòu hé yǔ yì xìn xī. HtmlRAG zhí jiē shǐ yòng HTML gē shì de zhī shì, bǎo liú gèng duō xìn xī. dàn HTML bāo hán é xiǎo de biǎo qiān hé zào shēng, zuò zhě tí chū le qīng lǐ hé yā suō cè lüè lái jiě jué zhè gè wèn tí. shí yàn zhèng míng, HtmlRAG zài liù gè wèn dá shù jù zhōng biǎo xiàn gèng hǎo.",
        "vocab": "[\n    {\"word\": \"改进\", \"pinyin\": \"gǎi jìn\", \"trans\": \"improvement\"},\n    {\"word\": \"检索\", \"pinyin\": \"jiǎn suǒ\", \"trans\": \"retrieval\"},\n    {\"word\": \"增强\", \"pinyin\": \"zēng qiáng\", \"trans\": \"enhancement\"},\n    {\"word\": \"生成\", \"pinyin\": \"shēng chéng\", \"trans\": \"generation\"},\n    {\"word\": \"方法\", \"pinyin\": \"fāng fǎ\", \"trans\": \"method\"},\n    {\"word\": \"称为\", \"pinyin\": \"chēng wéi\", \"trans\": \"called\"},\n    {\"word\": \"传统\", \"pinyin\": \"chuán tǒng\", \"trans\": \"traditional\"},\n    {\"word\": \"系统\", \"pinyin\": \"xì tǒng\", \"trans\": \"system\"},\n    {\"word\": \"网页\", \"pinyin\": \"wǎng yè\", \"trans\": \"webpage\"},\n    {\"word\": \"提取\", \"pinyin\": \"tí qu\", \"trans\": \"extract\"},\n    {\"word\": \"纯文本\", \"pinyin\": \"chún wén běn\", \"trans\": \"pure text\"},\n    {\"word\": \"喂给\", \"pinyin\": \"wèi gěi\", \"trans\": \"feed\"},\n    {\"word\": \"大语言模型\", \"pinyin\": \"dà yǔ yán mó xíng\", \"trans\": \"large language model\"},\n    {\"word\": \"丢失\", \"pinyin\": \"diū shī\", \"trans\": \"lose\"},\n    {\"word\": \"结构\", \"pinyin\": \"jié gòu\", \"trans\": \"structure\"},\n    {\"word\": \"语义\", \"pinyin\": \"yǔ yì\", \"trans\": \"semantics\"},\n    {\"word\": \"信息\", \"pinyin\": \"xìn xī\", \"trans\": \"information\"},\n    {\"word\": \"直接\", \"pinyin\": \"zhí jiē\", \"trans\": \"directly\"},\n    {\"word\": \"格式\", \"pinyin\": \"gé shì\", \"trans\": \"format\"},\n    {\"word\": \"知识\", \"pinyin\": \"zhī shì\", \"trans\": \"knowledge\"},\n    {\"word\": \"保留\", \"pinyin\": \"bǎo liú\", \"trans\": \"retain\"},\n    {\"word\": \"额外\", \"pinyin\": \"é wài\", \"trans\": \"extra\"},\n    {\"word\": \"标签\", \"pinyin\": \"biāo qiān\", \"trans\": \"tag\"},\n    {\"word\": \"噪声\", \"pinyin\": \"zào shēng\", \"trans\": \"noise\"},\n    {\"word\": \"提出\", \"pinyin\": \"tí chū\", \"trans\": \"propose\"},\n    {\"word\": \"清理\", \"pinyin\": \"qīng lǐ\", \"trans\": \"clean\"},\n    {\"word\": \"压缩\", \"pinyin\": \"yā suō\", \"trans\": \"compress\"},\n    {\"word\": \"策略\", \"pinyin\": \"cè lüè\", \"trans\": \"strategy\"},\n    {\"word\": \"解决\", \"pinyin\": \"jiě jué\", \"trans\": \"solve\"},\n    {\"word\": \"问题\", \"pinyin\": \"wèn tí\", \"trans\": \"problem\"},\n    {\"word\": \"实验\", \"pinyin\": \"shí yàn\", \"trans\": \"experiment\"},\n    {\"word\": \"证明\", \"pinyin\": \"zhèng míng\", \"trans\": \"prove\"},\n    {\"word\": \"表现\", \"pinyin\": \"biǎo xiàn\", \"trans\": \"performance\"},\n    {\"word\": \"数据集\", \"pinyin\": \"shù jù jí\", \"trans\": \"dataset\"}\n]",
        "trans": "This article introduces an improved Retrieval-Augmented Generation (RAG) method called HtmlRAG. Traditional RAG systems retrieve information from web pages and extract plain text to feed into large language models (LLMs). However, this approach loses the structural and semantic information present in HTML. HtmlRAG directly uses knowledge in HTML format, preserving more information. But since HTML contains additional tags and noise, the authors propose cleaning and compression strategies to address this issue. Experiments show that HtmlRAG performs better on six question-answering datasets.",
        "update_ts": "2024-11-07 10:12"
    }
}