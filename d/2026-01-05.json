{
    "date": {
        "ru": "5 ÑĞ½Ğ²Ğ°Ñ€Ñ",
        "en": "January 5",
        "zh": "1æœˆ5æ—¥"
    },
    "time_utc": "2026-01-05 05:02",
    "weekday": 0,
    "issue_id": 406,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2512.24615",
            "title": "Youtu-Agent: Scaling Agent Productivity with Automated Generation and Hybrid Policy Optimization",
            "url": "https://huggingface.co/papers/2512.24615",
            "abstract": "Existing Large Language Model (LLM) agent frameworks face two significant challenges: high configuration costs and static capabilities. Building a high-quality agent often requires extensive manual effort in tool integration and prompt engineering, while deployed agents struggle to adapt to dynamic environments without expensive fine-tuning. To address these issues, we propose Youtu-Agent, a modular framework designed for the automated generation and continuous evolution of LLM agents. Youtu-Agent features a structured configuration system that decouples execution environments, toolkits, and context management, enabling flexible reuse and automated synthesis. We introduce two generation paradigms: a Workflow mode for standard tasks and a Meta-Agent mode for complex, non-standard requirements, capable of automatically generating tool code, prompts, and configurations. Furthermore, Youtu-Agent establishes a hybrid policy optimization system: (1) an Agent Practice module that enables agents to accumulate experience and improve performance through in-context optimization without parameter updates; and (2) an Agent RL module that integrates with distributed training frameworks to enable scalable and stable reinforcement learning of any Youtu-Agents in an end-to-end, large-scale manner. Experiments demonstrate that Youtu-Agent achieves state-of-the-art performance on WebWalkerQA (71.47\\%) and GAIA (72.8\\%) using open-weight models. Our automated generation pipeline achieves over 81\\% tool synthesis success rate, while the Practice module improves performance on AIME 2024/2025 by +2.7\\% and +5.4\\% respectively. Moreover, our Agent RL training achieves 40\\% speedup with steady performance improvement on 7B LLMs, enhancing coding/reasoning and searching capabilities respectively up to 35\\% and 21\\% on Maths and general/multi-hop QA benchmarks.",
            "score": 57,
            "issue_id": 405,
            "pub_date": "2026-12-31",
            "pub_date_card": {
                "ru": "31 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 31",
                "zh": "12æœˆ31æ—¥"
            },
            "hash": "40053cb88310e90a",
            "authors": [
                "Yuchen Shi",
                "Yuzheng Cai",
                "Siqi Cai",
                "Zihan Xu",
                "Lichao Chen",
                "Yulei Qin",
                "Zhijian Zhou",
                "Xiang Fei",
                "Chaofan Qiu",
                "Xiaoyu Tan",
                "Gang Li",
                "Zongyi Li",
                "Haojia Lin",
                "Guocan Cai",
                "Yong Mao",
                "Yunsheng Wu",
                "Ke Li",
                "Xing Sun"
            ],
            "affiliations": [
                "Tencent"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.24615.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#reasoning",
                    "#benchmark",
                    "#optimization",
                    "#training",
                    "#agents"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ±ĞµĞ· Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Youtu-Agent â€” Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ñ€ĞµÑˆĞ°ÑÑ‰Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ½Ğ° ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ, Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ³Ğ¸Ğ±ĞºĞ¾ Ğ¿ĞµÑ€ĞµĞ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹ Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´Ğ²Ğ° Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸: Ñ€ĞµĞ¶Ğ¸Ğ¼ Workflow Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ñ€ĞµĞ¶Ğ¸Ğ¼ Meta-Agent Ğ´Ğ»Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ ĞºĞ¾Ğ´Ğ° Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ¸ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¹. ĞšÑ€Ğ¾Ğ¼Ğµ Ñ‚Ğ¾Ğ³Ğ¾, Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ°Ñ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Agent Practice Ğ´Ğ»Ñ Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ¿Ñ‹Ñ‚Ğ° Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Agent RL Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ² Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾Ğ¹ ÑÑ€ĞµĞ´Ğµ."
                },
                "en": {
                    "title": "Youtu-Agent: Revolutionizing LLM Agent Flexibility and Performance",
                    "desc": "The paper introduces Youtu-Agent, a modular framework that addresses the challenges of high configuration costs and static capabilities in existing Large Language Model (LLM) agent frameworks. It allows for automated generation and continuous evolution of LLM agents through a structured configuration system that separates execution environments, toolkits, and context management. Youtu-Agent employs two generation paradigms: Workflow mode for standard tasks and Meta-Agent mode for complex tasks, enabling the automatic creation of tool code and prompts. Additionally, it features a hybrid policy optimization system that enhances agent performance through in-context optimization and scalable reinforcement learning, achieving state-of-the-art results in various benchmarks."
                },
                "zh": {
                    "title": "Youtu-Agentï¼šè‡ªåŠ¨åŒ–ç”Ÿæˆä¸æŒç»­æ¼”åŒ–çš„LLMä»£ç†æ¡†æ¶",
                    "desc": "ç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†æ¡†æ¶é¢ä¸´ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šé«˜é…ç½®æˆæœ¬å’Œé™æ€èƒ½åŠ›ã€‚æ„å»ºé«˜è´¨é‡ä»£ç†é€šå¸¸éœ€è¦å¤§é‡çš„æ‰‹åŠ¨å·¥å…·é›†æˆå’Œæç¤ºå·¥ç¨‹ï¼Œè€Œå·²éƒ¨ç½²çš„ä»£ç†åœ¨åŠ¨æ€ç¯å¢ƒä¸­éš¾ä»¥é€‚åº”ï¼Œä¸”ä¸æ˜“è¿›è¡Œæ˜‚è´µçš„å¾®è°ƒã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Youtu-Agentï¼Œä¸€ä¸ªæ—¨åœ¨è‡ªåŠ¨ç”Ÿæˆå’ŒæŒç»­æ¼”åŒ–LLMä»£ç†çš„æ¨¡å—åŒ–æ¡†æ¶ã€‚Youtu-Agenté€šè¿‡ç»“æ„åŒ–é…ç½®ç³»ç»Ÿå®ç°æ‰§è¡Œç¯å¢ƒã€å·¥å…·åŒ…å’Œä¸Šä¸‹æ–‡ç®¡ç†çš„è§£è€¦ï¼Œæ”¯æŒçµæ´»é‡ç”¨å’Œè‡ªåŠ¨åˆæˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.24330",
            "title": "SenseNova-MARS: Empowering Multimodal Agentic Reasoning and Search via Reinforcement Learning",
            "url": "https://huggingface.co/papers/2512.24330",
            "abstract": "While Vision-Language Models (VLMs) can solve complex tasks through agentic reasoning, their capabilities remain largely constrained to text-oriented chain-of-thought or isolated tool invocation. They fail to exhibit the human-like proficiency required to seamlessly interleave dynamic tool manipulation with continuous reasoning, particularly in knowledge-intensive and visually complex scenarios that demand coordinated external tools such as search and image cropping. In this work, we introduce SenseNova-MARS, a novel Multimodal Agentic Reasoning and Search framework that empowers VLMs with interleaved visual reasoning and tool-use capabilities via reinforcement learning (RL). Specifically, SenseNova-MARS dynamically integrates the image search, text search, and image crop tools to tackle fine-grained and knowledge-intensive visual understanding challenges. In the RL stage, we propose the Batch-Normalized Group Sequence Policy Optimization (BN-GSPO) algorithm to improve the training stability and advance the model's ability to invoke tools and reason effectively. To comprehensively evaluate the agentic VLMs on complex visual tasks, we introduce the HR-MMSearch benchmark, the first search-oriented benchmark composed of high-resolution images with knowledge-intensive and search-driven questions. Experiments demonstrate that SenseNova-MARS achieves state-of-the-art performance on open-source search and fine-grained image understanding benchmarks. Specifically, on search-oriented benchmarks, SenseNova-MARS-8B scores 67.84 on MMSearch and 41.64 on HR-MMSearch, surpassing proprietary models such as Gemini-3-Flash and GPT-5. SenseNova-MARS represents a promising step toward agentic VLMs by providing effective and robust tool-use capabilities. To facilitate further research in this field, we will release all code, models, and datasets.",
            "score": 26,
            "issue_id": 405,
            "pub_date": "2026-12-30",
            "pub_date_card": {
                "ru": "30 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 30",
                "zh": "12æœˆ30æ—¥"
            },
            "hash": "ff90ddac66fbb641",
            "authors": [
                "Yong Xien Chng",
                "Tao Hu",
                "Wenwen Tong",
                "Xueheng Li",
                "Jiandong Chen",
                "Haojia Yu",
                "Jiefan Lu",
                "Hewei Guo",
                "Hanming Deng",
                "Chengjun Xie",
                "Gao Huang",
                "Dahua Lin",
                "Lewei Lu"
            ],
            "affiliations": [
                "SenseTime Research",
                "Tsinghua University",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.24330.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#reasoning",
                    "#benchmark",
                    "#dataset",
                    "#cv",
                    "#open_source",
                    "#optimization",
                    "#training",
                    "#multimodal",
                    "#agents"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞĞ³ĞµĞ½Ñ‚ÑĞºĞ¾Ğµ Ğ²Ğ¸Ğ´ĞµĞ½Ğ¸Ğµ: Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° SenseNova-MARS â€” Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚ÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² (Ğ¿Ğ¾Ğ¸ÑĞº Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¿Ğ¾Ğ¸ÑĞº Ñ‚ĞµĞºÑÑ‚Ğ°, ĞºĞ°Ğ´Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ) Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ BN-GSPO Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğº Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑĞ¼. Ğ”Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ°Ğ³ĞµĞ½Ñ‚ÑĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ²ĞµĞ´ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº HR-MMSearch, ÑĞ¾ÑÑ‚Ğ¾ÑÑ‰Ğ¸Ğ¹ Ğ¸Ğ· Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ ÑĞ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼Ğ¸, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ SenseNova-MARS Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ñ… Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ´Ğ°Ğ¶Ğµ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Gemini Ğ¸ GPT-5."
                },
                "en": {
                    "title": "Empowering VLMs with Dynamic Tool Manipulation and Reasoning",
                    "desc": "This paper presents SenseNova-MARS, a new framework designed to enhance Vision-Language Models (VLMs) by enabling them to perform dynamic tool manipulation alongside continuous reasoning. Unlike traditional VLMs that struggle with complex tasks requiring coordinated tool use, SenseNova-MARS integrates image search, text search, and image cropping tools through reinforcement learning. The authors introduce a novel training algorithm called Batch-Normalized Group Sequence Policy Optimization (BN-GSPO) to improve the model's stability and effectiveness in invoking tools. The framework is evaluated using the HR-MMSearch benchmark, demonstrating superior performance in knowledge-intensive visual tasks compared to existing models."
                },
                "zh": {
                    "title": "æå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„æ™ºèƒ½æ¨ç†èƒ½åŠ›",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„å¤šæ¨¡æ€æ™ºèƒ½æ¨ç†å’Œæœç´¢æ¡†æ¶SenseNova-MARSï¼Œæ—¨åœ¨æå‡è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨åŠ¨æ€å·¥å…·æ“ä½œä¸è¿ç»­æ¨ç†ä¸­çš„èƒ½åŠ›ã€‚é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ï¼ŒSenseNova-MARSèƒ½å¤Ÿå°†å›¾åƒæœç´¢ã€æ–‡æœ¬æœç´¢å’Œå›¾åƒè£å‰ªå·¥å…·åŠ¨æ€æ•´åˆï¼Œä»¥åº”å¯¹å¤æ‚çš„è§†è§‰ç†è§£ä»»åŠ¡ã€‚æˆ‘ä»¬æå‡ºçš„Batch-Normalized Group Sequence Policy Optimizationï¼ˆBN-GSPOï¼‰ç®—æ³•æé«˜äº†è®­ç»ƒçš„ç¨³å®šæ€§ï¼Œå¢å¼ºäº†æ¨¡å‹è°ƒç”¨å·¥å…·å’Œæœ‰æ•ˆæ¨ç†çš„èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSenseNova-MARSåœ¨å¼€æ”¾æºä»£ç çš„æœç´¢å’Œç»†ç²’åº¦å›¾åƒç†è§£åŸºå‡†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå±•ç¤ºäº†å…¶åœ¨æ™ºèƒ½VLMsé¢†åŸŸçš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.00664",
            "title": "Avatar Forcing: Real-Time Interactive Head Avatar Generation for Natural Conversation",
            "url": "https://huggingface.co/papers/2601.00664",
            "abstract": "A framework called Avatar Forcing uses diffusion forcing and direct preference optimization with synthetic losing samples to enable real-time, expressive multimodal interactions in talking head avatars without labeled data.  \t\t\t\t\tAI-generated summary \t\t\t\t Talking head generation creates lifelike avatars from static portraits for virtual communication and content creation. However, current models do not yet convey the feeling of truly interactive communication, often generating one-way responses that lack emotional engagement. We identify two key challenges toward truly interactive avatars: generating motion in real-time under causal constraints and learning expressive, vibrant reactions without additional labeled data. To address these challenges, we propose Avatar Forcing, a new framework for interactive head avatar generation that models real-time user-avatar interactions through diffusion forcing. This design allows the avatar to process real-time multimodal inputs, including the user's audio and motion, with low latency for instant reactions to both verbal and non-verbal cues such as speech, nods, and laughter. Furthermore, we introduce a direct preference optimization method that leverages synthetic losing samples constructed by dropping user conditions, enabling label-free learning of expressive interaction. Experimental results demonstrate that our framework enables real-time interaction with low latency (approximately 500ms), achieving 6.8X speedup compared to the baseline, and produces reactive and expressive avatar motion, which is preferred over 80% against the baseline.",
            "score": 22,
            "issue_id": 405,
            "pub_date": "2026-01-02",
            "pub_date_card": {
                "ru": "2 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 2",
                "zh": "1æœˆ2æ—¥"
            },
            "hash": "f2dbe0c7e036ea97",
            "authors": [
                "Taekyung Ki",
                "Sangwon Jang",
                "Jaehyeong Jo",
                "Jaehong Yoon",
                "Sung Ju Hwang"
            ],
            "affiliations": [
                "DeepAuto.ai",
                "KAIST",
                "NTU Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.00664.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#optimization",
                    "#training",
                    "#multimodal",
                    "#synthetic",
                    "#diffusion",
                    "#rlhf"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "Ğ˜Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ°Ğ²Ğ°Ñ‚Ğ°Ñ€Ñ‹ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¸Ğ½ÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ",
                    "desc": "Avatar Forcing â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ°Ğ²Ğ°Ñ‚Ğ°Ñ€Ğ¾Ğ² Ñ Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰ĞµĞ¹ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¸Ğ½ÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸ÑÑ… Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ°Ğ²Ğ°Ñ‚Ğ°Ñ€Ñƒ Ñ€ĞµĞ°Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğ° Ñ€ĞµÑ‡ÑŒ, ĞºĞ¸Ğ²ĞºĞ¸ Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ½ĞµĞ²ĞµÑ€Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ñ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¾Ñ‚Ñ€Ğ¸Ñ†Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ€Ğ°Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ñ€ĞµĞ°ĞºÑ†Ğ¸ÑĞ¼ Ğ±ĞµĞ· Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ² 6.8 Ñ€Ğ°Ğ·Ğ° Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ 80% Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğµ Ğ² ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¸ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Real-Time, Expressive Interactions with Avatar Forcing",
                    "desc": "The paper introduces Avatar Forcing, a novel framework designed to enhance real-time interactions with talking head avatars. It utilizes diffusion forcing and direct preference optimization to generate expressive responses without the need for labeled data. The framework addresses challenges in creating lifelike avatars that can react to both verbal and non-verbal cues in real-time. Experimental results show significant improvements in interaction speed and expressiveness, making avatars more engaging for users."
                },
                "zh": {
                    "title": "å®æ—¶äº¤äº’ï¼Œç”ŸåŠ¨å¤´åƒçš„æœªæ¥",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºAvatar Forcingçš„æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°å®æ—¶ã€å¯Œæœ‰è¡¨ç°åŠ›çš„å¤šæ¨¡æ€äº¤äº’ï¼Œç‰¹åˆ«æ˜¯åœ¨å¯¹è¯å¤´åƒç”Ÿæˆä¸­ã€‚è¯¥æ¡†æ¶ç»“åˆäº†æ‰©æ•£å¼ºåˆ¶å’Œç›´æ¥åå¥½ä¼˜åŒ–ï¼Œèƒ½å¤Ÿåœ¨æ²¡æœ‰æ ‡è®°æ•°æ®çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡åˆæˆçš„å¤±è´¥æ ·æœ¬è¿›è¡Œå­¦ä¹ ã€‚Avatar Forcingèƒ½å¤Ÿå¤„ç†ç”¨æˆ·çš„éŸ³é¢‘å’ŒåŠ¨ä½œè¾“å…¥ï¼Œå®ç°ä½å»¶è¿Ÿçš„å³æ—¶ååº”ï¼Œæå‡äº†äº¤äº’çš„çœŸå®æ„Ÿå’Œæƒ…æ„Ÿå‚ä¸åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å®æ—¶äº¤äº’ä¸­è¡¨ç°å‡ºæ˜¾è‘—çš„é€Ÿåº¦æå‡å’Œæ›´å…·è¡¨ç°åŠ›çš„å¤´åƒåŠ¨ä½œã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.24271",
            "title": "Taming Hallucinations: Boosting MLLMs' Video Understanding via Counterfactual Video Generation",
            "url": "https://huggingface.co/papers/2512.24271",
            "abstract": "Multimodal Large Language Models (MLLMs) have made remarkable progress in video understanding. However, they suffer from a critical vulnerability: an over-reliance on language priors, which can lead to visual ungrounded hallucinations, especially when processing counterfactual videos that defy common sense. This limitation, stemming from the intrinsic data imbalance between text and video, is challenging to address due to the substantial cost of collecting and annotating counterfactual data. To address this, we introduce DualityForge, a novel counterfactual data synthesis framework that employs controllable, diffusion-based video editing to transform real-world videos into counterfactual scenarios. By embedding structured contextual information into the video editing and QA generation processes, the framework automatically produces high-quality QA pairs together with original-edited video pairs for contrastive training. Based on this, we build DualityVidQA, a large-scale video dataset designed to reduce MLLM hallucinations. In addition, to fully exploit the contrastive nature of our paired data, we propose Duality-Normalized Advantage Training (DNA-Train), a two-stage SFT-RL training regime where the RL phase applies pair-wise ell_1 advantage normalization, thereby enabling a more stable and efficient policy optimization. Experiments on DualityVidQA-Test demonstrate that our method substantially reduces model hallucinations on counterfactual videos, yielding a relative improvement of 24.0% over the Qwen2.5-VL-7B baseline. Moreover, our approach achieves significant gains across both hallucination and general-purpose benchmarks, indicating strong generalization capability. We will open-source our dataset and code.",
            "score": 19,
            "issue_id": 405,
            "pub_date": "2026-12-30",
            "pub_date_card": {
                "ru": "30 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 30",
                "zh": "12æœˆ30æ—¥"
            },
            "hash": "01479f4c4234799d",
            "authors": [
                "Zhe Huang",
                "Hao Wen",
                "Aiming Hao",
                "Bingze Song",
                "Meiqi Wu",
                "Jiahong Wu",
                "Xiangxiang Chu",
                "Sheng Lu",
                "Haoqian Wang"
            ],
            "affiliations": [
                "AMAP, Alibaba Group",
                "Beihang University",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.24271.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#dataset",
                    "#video",
                    "#open_source",
                    "#training",
                    "#multimodal",
                    "#synthetic",
                    "#diffusion",
                    "#hallucinations"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ‘Ğ¾Ñ€ÑŒĞ±Ğ° Ñ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ‡ĞµÑ€ĞµĞ· ÑĞ¸Ğ½Ñ‚ĞµĞ· ĞºĞ¾Ğ½Ñ‚Ñ€Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…",
                    "desc": "Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ‡Ñ€ĞµĞ·Ğ¼ĞµÑ€Ğ½Ğ¾ Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ÑÑ‚ÑÑ Ğ½Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ DualityForge â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° ĞºĞ¾Ğ½Ñ‚Ñ€Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ½ĞµĞ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ² Ğ¸Ğ· Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ DualityVidQA Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ DNA-Train, ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ supervised fine-tuning Ğ¸ reinforcement learning Ñ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¿Ğ°Ñ€Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ½Ğ° 24% Ğ¸ Ñ…Ğ¾Ñ€Ğ¾ÑˆÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…."
                },
                "en": {
                    "title": "Transforming Video Understanding with Counterfactual Data Synthesis",
                    "desc": "This paper addresses the limitations of Multimodal Large Language Models (MLLMs) in video understanding, particularly their tendency to generate visual hallucinations when faced with counterfactual scenarios. The authors introduce DualityForge, a framework that synthesizes counterfactual data through advanced video editing techniques, allowing for the creation of high-quality question-answer pairs. They also present DualityVidQA, a large-scale dataset aimed at training MLLMs to reduce hallucinations by providing contrastive examples. Additionally, the proposed Duality-Normalized Advantage Training (DNA-Train) enhances the training process, leading to significant improvements in model performance on both hallucination and general benchmarks."
                },
                "zh": {
                    "title": "å‡å°‘å¹»è§‰ï¼Œæå‡è§†é¢‘ç†è§£çš„åŒé‡åŠ›é‡",
                    "desc": "å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†é¢‘ç†è§£æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å®ƒä»¬å­˜åœ¨ä¸€ä¸ªå…³é”®çš„å¼±ç‚¹ï¼šè¿‡åº¦ä¾èµ–è¯­è¨€å…ˆéªŒï¼Œå¯¼è‡´åœ¨å¤„ç†è¿åå¸¸è¯†çš„åäº‹å®è§†é¢‘æ—¶å‡ºç°è§†è§‰æ— æ ¹çš„å¹»è§‰ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†DualityForgeï¼Œä¸€ä¸ªæ–°é¢–çš„åäº‹å®æ•°æ®åˆæˆæ¡†æ¶ï¼Œé€šè¿‡å¯æ§çš„æ‰©æ•£è§†é¢‘ç¼–è¾‘å°†çœŸå®è§†é¢‘è½¬åŒ–ä¸ºåäº‹å®åœºæ™¯ã€‚è¯¥æ¡†æ¶åœ¨è§†é¢‘ç¼–è¾‘å’Œé—®ç­”ç”Ÿæˆè¿‡ç¨‹ä¸­åµŒå…¥ç»“æ„åŒ–ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œè‡ªåŠ¨ç”Ÿæˆé«˜è´¨é‡çš„é—®ç­”å¯¹åŠåŸå§‹-ç¼–è¾‘è§†é¢‘å¯¹ï¼Œä»¥ä¾¿è¿›è¡Œå¯¹æ¯”è®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—å‡å°‘äº†æ¨¡å‹åœ¨åäº‹å®è§†é¢‘ä¸Šçš„å¹»è§‰ï¼Œæ˜¾ç¤ºå‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.00417",
            "title": "Deep Delta Learning",
            "url": "https://huggingface.co/papers/2601.00417",
            "abstract": "The efficacy of deep residual networks is fundamentally predicated on the identity shortcut connection. While this mechanism effectively mitigates the vanishing gradient problem, it imposes a strictly additive inductive bias on feature transformations, thereby limiting the network's capacity to model complex state transitions. In this paper, we introduce Deep Delta Learning (DDL), a novel architecture that generalizes the standard residual connection by modulating the identity shortcut with a learnable, data-dependent geometric transformation. This transformation, termed the Delta Operator, constitutes a rank-1 perturbation of the identity matrix, parameterized by a reflection direction vector k(X) and a gating scalar Î²(X). We provide a spectral analysis of this operator, demonstrating that the gate Î²(X) enables dynamic interpolation between identity mapping, orthogonal projection, and geometric reflection. Furthermore, we restructure the residual update as a synchronous rank-1 injection, where the gate acts as a dynamic step size governing both the erasure of old information and the writing of new features. This unification empowers the network to explicitly control the spectrum of its layer-wise transition operator, enabling the modeling of complex, non-monotonic dynamics while preserving the stable training characteristics of gated residual architectures.",
            "score": 4,
            "issue_id": 405,
            "pub_date": "2026-01-01",
            "pub_date_card": {
                "ru": "1 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 1",
                "zh": "1æœˆ1æ—¥"
            },
            "hash": "b59181a06be243db",
            "authors": [
                "Yifan Zhang",
                "Yifeng Liu",
                "Mengdi Wang",
                "Quanquan Gu"
            ],
            "affiliations": [
                "Princeton University",
                "University of California, Los Angeles"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.00417.jpg",
            "data": {
                "categories": [
                    "#architecture"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ ÑĞ²ÑĞ·Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Deep Delta Learning (DDL) â€” Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑĞ²ÑĞ·ĞµĞ¹ Ğ² Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ÑÑ… Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ¾Ğ´ÑƒĞ»ÑÑ†Ğ¸Ñ Ñ‚Ğ¾Ğ¶Ğ´ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ğ¼ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼. Ğ”ĞµĞ»ÑŒÑ‚Ğ°-Ğ¾Ğ¿ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ñ€Ğ°Ğ½Ğ³-1 Ğ²Ğ¾Ğ·Ğ¼ÑƒÑ‰ĞµĞ½Ğ¸Ğµ ĞµĞ´Ğ¸Ğ½Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ñ‹, Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ¼ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞºĞ°Ğ»ÑÑ€Ğ½Ğ¾Ğ¹ Ğ²ĞµĞ½Ñ‚Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ„ÑƒĞ½ĞºÑ†Ğ¸ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ·Ğ°Ğ²Ğ¸ÑÑÑ‚ Ğ¾Ñ‚ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ¡Ğ¿ĞµĞºÑ‚Ñ€Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ²ĞµĞ½Ñ‚Ğ¸Ğ»ÑŒ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚Ğ¾Ğ¶Ğ´ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ĞµĞ¼, Ğ¾Ñ€Ñ‚Ğ¾Ğ³Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾ĞµĞºÑ†Ğ¸ĞµĞ¹ Ğ¸ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¾Ñ‚Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ĞµĞ¼. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞµÑ‚Ğ¸ ÑĞ²Ğ½Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ¿ĞµĞºÑ‚Ñ€ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ¾Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ»Ğ¾ÑĞ¼Ğ¸, Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒÑ ÑĞ»Ğ¾Ğ¶Ğ½ÑƒÑ Ğ½ĞµĞ»Ğ¸Ğ½ĞµĞ¹Ğ½ÑƒÑ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºÑƒ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Empowering Residual Networks with Dynamic Transformations",
                    "desc": "This paper presents Deep Delta Learning (DDL), a new architecture that enhances deep residual networks by introducing a learnable transformation to the identity shortcut connection. The Delta Operator, a key component of DDL, allows for dynamic adjustments to feature transformations, overcoming limitations of traditional residual connections. By using a gating scalar and a reflection direction vector, DDL can interpolate between various transformations, improving the network's ability to model complex dynamics. This approach maintains the stability of training while enabling richer feature representation and more effective learning of non-linear relationships."
                },
                "zh": {
                    "title": "æ·±åº¦å¢é‡å­¦ä¹ ï¼šè¶…è¶Šä¼ ç»Ÿæ®‹å·®è¿æ¥çš„åˆ›æ–°",
                    "desc": "æ·±åº¦æ®‹å·®ç½‘ç»œçš„æœ‰æ•ˆæ€§ä¾èµ–äºèº«ä»½å¿«æ·è¿æ¥ã€‚è™½ç„¶è¿™ç§æœºåˆ¶æœ‰æ•ˆåœ°ç¼“è§£äº†æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼Œä½†å®ƒå¯¹ç‰¹å¾å˜æ¢æ–½åŠ äº†ä¸¥æ ¼çš„åŠ æ³•å½’çº³åç½®ï¼Œä»è€Œé™åˆ¶äº†ç½‘ç»œå»ºæ¨¡å¤æ‚çŠ¶æ€è½¬å˜çš„èƒ½åŠ›ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ¶æ„â€”â€”æ·±åº¦å¢é‡å­¦ä¹ ï¼ˆDDLï¼‰ï¼Œé€šè¿‡å¯å­¦ä¹ çš„æ•°æ®ä¾èµ–å‡ ä½•å˜æ¢æ¥æ¨å¹¿æ ‡å‡†æ®‹å·®è¿æ¥ã€‚æˆ‘ä»¬å¼•å…¥çš„å¢é‡ç®—å­ä½¿ç½‘ç»œèƒ½å¤ŸåŠ¨æ€æ§åˆ¶å±‚é—´è¿‡æ¸¡æ“ä½œçš„è°±ï¼Œä»è€Œå»ºæ¨¡å¤æ‚çš„éå•è°ƒåŠ¨æ€ï¼ŒåŒæ—¶ä¿æŒé—¨æ§æ®‹å·®æ¶æ„çš„ç¨³å®šè®­ç»ƒç‰¹æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.00671",
            "title": "Fast-weight Product Key Memory",
            "url": "https://huggingface.co/papers/2601.00671",
            "abstract": "FwPKM introduces a dynamic, fast-weight episodic memory mechanism for sequence modeling that balances storage capacity and efficiency, achieving strong performance on long-context tasks like Needle in a Haystack evaluations.  \t\t\t\t\tAI-generated summary \t\t\t\t Sequence modeling layers in modern language models typically face a trade-off between storage capacity and computational efficiency. While Softmax attention offers unbounded storage at prohibitive quadratic costs, linear variants provide efficiency but suffer from limited, fixed-size storage. We propose Fast-weight Product Key Memory (FwPKM), a novel architecture that resolves this tension by transforming the sparse Product Key Memory (PKM) from a static module into a dynamic, \"fast-weight\" episodic memory. Unlike PKM, FwPKM updates its parameters dynamically at both training and inference time via local chunk-level gradient descent, allowing the model to rapidly memorize and retrieve new key-value pairs from input sequences. Experiments reveal that FwPKM functions as an effective episodic memory that complements the semantic memory of standard modules, yielding significant perplexity reductions on long-context datasets. Notably, in Needle in a Haystack evaluations, FwPKM generalizes to 128K-token contexts despite being trained on only 4K-token sequences.",
            "score": 1,
            "issue_id": 405,
            "pub_date": "2026-01-02",
            "pub_date_card": {
                "ru": "2 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 2",
                "zh": "1æœˆ2æ—¥"
            },
            "hash": "072aff4bbbc7f7a5",
            "authors": [
                "Tianyu Zhao",
                "Llion Jones"
            ],
            "affiliations": [
                "Sakana AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.00671.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#architecture",
                    "#long_context"
                ],
                "emoji": "ğŸ“š",
                "ru": {
                    "title": "Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ°Ñ ÑĞ¿Ğ¸Ğ·Ğ¾Ğ´Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° FwPKM, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Product Key Memory Ğ² Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ¿Ğ¸Ğ·Ğ¾Ğ´Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ñ Ğ±Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¼Ğ¸ Ğ²ĞµÑĞ°Ğ¼Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑĞµÑ‚ ÑĞ²Ğ¾Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Ñ‡ĞµÑ€ĞµĞ· Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ ÑĞ¿ÑƒÑĞº Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ±Ğ»Ğ¾ĞºĞ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾ Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ñ‚ÑŒ Ğ¸ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°Ñ‚ÑŒ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ğ°Ñ€Ñ‹ ĞºĞ»ÑÑ‡-Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ Ğ¸Ğ· Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞ°ĞµÑ‚ ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‘Ğ¼ĞºĞ¾ÑÑ‚ÑŒÑ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ² ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¸ Ñ ĞºĞ²Ğ°Ğ´Ñ€Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Softmax-Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ´Ğ¾ 128K Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° 4K-Ñ‚Ğ¾ĞºĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑÑ…."
                },
                "en": {
                    "title": "Dynamic Memory for Efficient Sequence Modeling",
                    "desc": "FwPKM introduces a new memory mechanism for sequence modeling that enhances both storage and efficiency. It transforms the traditional static Product Key Memory into a dynamic, fast-weight episodic memory that can adapt during training and inference. This allows the model to quickly memorize and retrieve information from long input sequences, addressing the limitations of existing attention mechanisms. Experiments show that FwPKM significantly improves performance on long-context tasks, demonstrating its ability to handle large amounts of data effectively."
                },
                "zh": {
                    "title": "åŠ¨æ€å¿«é€Ÿæƒé‡è®°å¿†ï¼Œæå‡åºåˆ—å»ºæ¨¡æ•ˆç‡",
                    "desc": "FwPKMæå‡ºäº†ä¸€ç§åŠ¨æ€çš„å¿«é€Ÿæƒé‡æƒ…èŠ‚è®°å¿†æœºåˆ¶ï¼Œç”¨äºåºåˆ—å»ºæ¨¡ï¼Œå¹³è¡¡äº†å­˜å‚¨å®¹é‡å’Œæ•ˆç‡ã€‚åœ¨ç°ä»£è¯­è¨€æ¨¡å‹ä¸­ï¼Œåºåˆ—å»ºæ¨¡å±‚é€šå¸¸é¢ä¸´å­˜å‚¨å®¹é‡ä¸è®¡ç®—æ•ˆç‡ä¹‹é—´çš„æƒè¡¡ã€‚FwPKMé€šè¿‡å°†ç¨€ç–çš„äº§å“é”®è®°å¿†ï¼ˆPKMï¼‰ä»é™æ€æ¨¡å—è½¬å˜ä¸ºåŠ¨æ€çš„â€œå¿«é€Ÿæƒé‡â€æƒ…èŠ‚è®°å¿†ï¼Œè§£å†³äº†è¿™ä¸€çŸ›ç›¾ã€‚å®éªŒè¡¨æ˜ï¼ŒFwPKMä½œä¸ºä¸€ç§æœ‰æ•ˆçš„æƒ…èŠ‚è®°å¿†ï¼Œæ˜¾è‘—é™ä½äº†é•¿ä¸Šä¸‹æ–‡æ•°æ®é›†çš„å›°æƒ‘åº¦ã€‚"
                }
            }
        }
    ],
    "link_prev": "2026-01-02.html",
    "link_next": "2026-01-06.html",
    "link_month": "2026-01.html",
    "short_date_prev": {
        "ru": "02.01",
        "en": "01/02",
        "zh": "1æœˆ2æ—¥"
    },
    "short_date_next": {
        "ru": "06.01",
        "en": "01/06",
        "zh": "1æœˆ6æ—¥"
    },
    "categories": {
        "#dataset": 2,
        "#data": 0,
        "#benchmark": 2,
        "#agents": 2,
        "#cv": 1,
        "#rl": 3,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 2,
        "#multimodal": 3,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 5,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 2,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 4,
        "#survey": 0,
        "#diffusion": 2,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 1,
        "#synthetic": 2,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    }
}