{
    "date": {
        "ru": "7 января",
        "en": "January 7",
        "zh": "1月7日"
    },
    "time_utc": "2025-01-07 07:10",
    "weekday": 1,
    "issue_id": 1531,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2501.02045",
            "title": "METAGENE-1: Metagenomic Foundation Model for Pandemic Monitoring",
            "url": "https://huggingface.co/papers/2501.02045",
            "abstract": "We pretrain METAGENE-1, a 7-billion-parameter autoregressive transformer model, which we refer to as a metagenomic foundation model, on a novel corpus of diverse metagenomic DNA and RNA sequences comprising over 1.5 trillion base pairs. This dataset is sourced from a large collection of human wastewater samples, processed and sequenced using deep metagenomic (next-generation) sequencing methods. Unlike genomic models that focus on individual genomes or curated sets of specific species, the aim of METAGENE-1 is to capture the full distribution of genomic information present within this wastewater, to aid in tasks relevant to pandemic monitoring and pathogen detection. We carry out byte-pair encoding (BPE) tokenization on our dataset, tailored for metagenomic sequences, and then pretrain our model. In this paper, we first detail the pretraining dataset, tokenization strategy, and model architecture, highlighting the considerations and design choices that enable the effective modeling of metagenomic data. We then show results of pretraining this model on our metagenomic dataset, providing details about our losses, system metrics, and training stability over the course of pretraining. Finally, we demonstrate the performance of METAGENE-1, which achieves state-of-the-art results on a set of genomic benchmarks and new evaluations focused on human-pathogen detection and genomic sequence embedding, showcasing its potential for public health applications in pandemic monitoring, biosurveillance, and early detection of emerging health threats.",
            "score": 7,
            "issue_id": 1528,
            "pub_date": "2025-01-03",
            "pub_date_card": {
                "ru": "3 января",
                "en": "January 3",
                "zh": "1月3日"
            },
            "hash": "60a3568f555ed60f",
            "authors": [
                "Ollie Liu",
                "Sami Jaghouar",
                "Johannes Hagemann",
                "Shangshang Wang",
                "Jason Wiemels",
                "Jeff Kaufman",
                "Willie Neiswanger"
            ],
            "affiliations": [
                "Nucleic Acid Observatory",
                "Prime Intellect",
                "University of Southern California"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.02045.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#data",
                    "#training",
                    "#architecture",
                    "#science",
                    "#dataset",
                    "#healthcare"
                ],
                "emoji": "🧬",
                "ru": {
                    "title": "METAGENE-1: Метагеномная модель для мониторинга здоровья населения",
                    "desc": "METAGENE-1 - это автореграссивная трансформерная модель с 7 миллиардами параметров, обученная на разнообразных метагеномных последовательностях ДНК и РНК. Модель создана для анализа геномной информации из образцов сточных вод с целью мониторинга пандемий и обнаружения патогенов. Авторы описывают процесс предобучения, включая токенизацию и архитектуру модели, а также демонстрируют результаты на различных геномных задачах. METAGENE-1 показывает высокую эффективность в обнаружении патогенов человека и встраивании геномных последовательностей, что открывает перспективы для применения в общественном здравоохранении."
                },
                "en": {
                    "title": "Unlocking Metagenomics: METAGENE-1 for Pandemic Preparedness",
                    "desc": "The paper introduces METAGENE-1, a large autoregressive transformer model designed for metagenomic data analysis. It is pretrained on a vast dataset of metagenomic DNA and RNA sequences derived from human wastewater, totaling over 1.5 trillion base pairs. The model aims to enhance pandemic monitoring and pathogen detection by capturing the diverse genomic information present in wastewater samples. The authors detail their tokenization strategy and model architecture, demonstrating that METAGENE-1 achieves state-of-the-art performance in genomic benchmarks and applications related to public health."
                },
                "zh": {
                    "title": "METAGENE-1：元基因组基础模型助力公共卫生监测",
                    "desc": "我们预训练了METAGENE-1，这是一个拥有70亿参数的自回归变换器模型，称为元基因组基础模型。该模型在一个包含超过1.5万亿碱基对的多样化元基因组DNA和RNA序列的新数据集上进行训练，这些数据来自大量人类废水样本。METAGENE-1的目标是捕捉废水中存在的基因组信息的完整分布，以帮助进行疫情监测和病原体检测。我们展示了该模型在元基因组数据集上的预训练结果，证明其在公共卫生应用中的潜力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.03006",
            "title": "TransPixar: Advancing Text-to-Video Generation with Transparency",
            "url": "https://huggingface.co/papers/2501.03006",
            "abstract": "Text-to-video generative models have made significant strides, enabling diverse applications in entertainment, advertising, and education. However, generating RGBA video, which includes alpha channels for transparency, remains a challenge due to limited datasets and the difficulty of adapting existing models. Alpha channels are crucial for visual effects (VFX), allowing transparent elements like smoke and reflections to blend seamlessly into scenes. We introduce TransPixar, a method to extend pretrained video models for RGBA generation while retaining the original RGB capabilities. TransPixar leverages a diffusion transformer (DiT) architecture, incorporating alpha-specific tokens and using LoRA-based fine-tuning to jointly generate RGB and alpha channels with high consistency. By optimizing attention mechanisms, TransPixar preserves the strengths of the original RGB model and achieves strong alignment between RGB and alpha channels despite limited training data. Our approach effectively generates diverse and consistent RGBA videos, advancing the possibilities for VFX and interactive content creation.",
            "score": 6,
            "issue_id": 1527,
            "pub_date": "2025-01-06",
            "pub_date_card": {
                "ru": "6 января",
                "en": "January 6",
                "zh": "1月6日"
            },
            "hash": "e85e5fa9a03d5d04",
            "authors": [
                "Luozhou Wang",
                "Yijun Li",
                "Zhifei Chen",
                "Jui-Hsien Wang",
                "Zhifei Zhang",
                "He Zhang",
                "Zhe Lin",
                "Yingcong Chen"
            ],
            "affiliations": [
                "Adobe Research",
                "HKUST",
                "HKUST(GZ)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.03006.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#architecture",
                    "#training",
                    "#diffusion",
                    "#video"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "TransPixar: Прорыв в генерации RGBA-видео для визуальных эффектов",
                    "desc": "TransPixar - это новый метод генерации RGBA-видео, расширяющий возможности предобученных видеомоделей. Он использует архитектуру диффузионного трансформера (DiT) и токены, специфичные для альфа-канала, для совместной генерации RGB и альфа-каналов с высокой согласованностью. Метод применяет тонкую настройку на основе LoRA и оптимизирует механизмы внимания для сохранения сильных сторон исходной RGB-модели. TransPixar эффективно генерирует разнообразные и согласованные RGBA-видео, открывая новые возможности для создания визуальных эффектов и интерактивного контента."
                },
                "en": {
                    "title": "TransPixar: Bridging RGB and Alpha for Enhanced Video Generation",
                    "desc": "This paper presents TransPixar, a novel method for generating RGBA videos, which include transparency information crucial for visual effects. The challenge lies in the limited datasets and the need to adapt existing models to handle alpha channels effectively. TransPixar utilizes a diffusion transformer architecture and incorporates alpha-specific tokens, allowing it to generate both RGB and alpha channels simultaneously. By optimizing attention mechanisms and employing LoRA-based fine-tuning, TransPixar achieves high consistency between RGB and alpha outputs, enhancing the quality of video generation for applications in VFX and interactive media."
                },
                "zh": {
                    "title": "TransPixar：生成高质量RGBA视频的新方法",
                    "desc": "本文介绍了一种名为TransPixar的方法，旨在生成包含透明通道的RGBA视频。传统的视频生成模型在处理透明效果时面临挑战，TransPixar通过扩展预训练模型来解决这一问题。该方法利用扩散变换器架构，结合特定的透明通道标记，并通过LoRA微调实现RGB和透明通道的高一致性生成。最终，TransPixar在有限的数据集上优化了注意力机制，成功生成多样且一致的RGBA视频，推动了视觉特效和互动内容创作的可能性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.02976",
            "title": "STAR: Spatial-Temporal Augmentation with Text-to-Video Models for Real-World Video Super-Resolution",
            "url": "https://huggingface.co/papers/2501.02976",
            "abstract": "Image diffusion models have been adapted for real-world video super-resolution to tackle over-smoothing issues in GAN-based methods. However, these models struggle to maintain temporal consistency, as they are trained on static images, limiting their ability to capture temporal dynamics effectively. Integrating text-to-video (T2V) models into video super-resolution for improved temporal modeling is straightforward. However, two key challenges remain: artifacts introduced by complex degradations in real-world scenarios, and compromised fidelity due to the strong generative capacity of powerful T2V models (e.g., CogVideoX-5B). To enhance the spatio-temporal quality of restored videos, we introduce~\\name (Spatial-Temporal Augmentation with T2V models for Real-world video super-resolution), a novel approach that leverages T2V models for real-world video super-resolution, achieving realistic spatial details and robust temporal consistency. Specifically, we introduce a Local Information Enhancement Module (LIEM) before the global attention block to enrich local details and mitigate degradation artifacts. Moreover, we propose a Dynamic Frequency (DF) Loss to reinforce fidelity, guiding the model to focus on different frequency components across diffusion steps. Extensive experiments demonstrate~\\name~outperforms state-of-the-art methods on both synthetic and real-world datasets.",
            "score": 5,
            "issue_id": 1527,
            "pub_date": "2025-01-06",
            "pub_date_card": {
                "ru": "6 января",
                "en": "January 6",
                "zh": "1月6日"
            },
            "hash": "13ac412646c508f5",
            "authors": [
                "Rui Xie",
                "Yinhong Liu",
                "Penghao Zhou",
                "Chen Zhao",
                "Jun Zhou",
                "Kai Zhang",
                "Zhenyu Zhang",
                "Jian Yang",
                "Zhenheng Yang",
                "Ying Tai"
            ],
            "affiliations": [
                "ByteDance",
                "Nanjing University",
                "Southwest University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.02976.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#optimization",
                    "#diffusion",
                    "#multimodal",
                    "#video"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Качественное суперразрешение видео с помощью T2V моделей",
                    "desc": "Представлена новая методика STAR для суперразрешения видео в реальных условиях с использованием моделей text-to-video. Предложен модуль LIEM для улучшения локальных деталей и устранения артефактов деградации. Введена функция потерь Dynamic Frequency для усиления точности восстановления на разных частотах. Эксперименты показывают превосходство STAR над современными методами на синтетических и реальных датасетах."
                },
                "en": {
                    "title": "Enhancing Video Quality with T2V Models for Real-World Super-Resolution",
                    "desc": "This paper presents a new method called Spatial-Temporal Augmentation with T2V models for Real-world video super-resolution, which aims to improve video quality by addressing issues of over-smoothing and temporal consistency. Traditional image diffusion models struggle with video because they are designed for static images, leading to challenges in capturing motion dynamics. The proposed approach incorporates a Local Information Enhancement Module to enhance local details and reduce artifacts, along with a Dynamic Frequency Loss to maintain fidelity across different frequency components. Experimental results show that this method outperforms existing techniques in both synthetic and real-world scenarios, providing better spatial and temporal quality in restored videos."
                },
                "zh": {
                    "title": "提升视频超分辨率的时空一致性",
                    "desc": "本文提出了一种新方法，名为~\\name~，用于提高真实世界视频超分辨率的时空质量。该方法结合了文本到视频（T2V）模型，以解决传统生成对抗网络（GAN）方法中的过平滑问题。通过引入局部信息增强模块（LIEM）和动态频率损失（DF Loss），该方法能够有效改善视频的局部细节和时间一致性。实验结果表明，~\\name~在合成和真实世界数据集上均优于现有的最先进方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.02497",
            "title": "Test-time Computing: from System-1 Thinking to System-2 Thinking",
            "url": "https://huggingface.co/papers/2501.02497",
            "abstract": "The remarkable performance of the o1 model in complex reasoning demonstrates that test-time computing scaling can further unlock the model's potential, enabling powerful System-2 thinking. However, there is still a lack of comprehensive surveys for test-time computing scaling. We trace the concept of test-time computing back to System-1 models. In System-1 models, test-time computing addresses distribution shifts and improves robustness and generalization through parameter updating, input modification, representation editing, and output calibration. In System-2 models, it enhances the model's reasoning ability to solve complex problems through repeated sampling, self-correction, and tree search. We organize this survey according to the trend of System-1 to System-2 thinking, highlighting the key role of test-time computing in the transition from System-1 models to weak System-2 models, and then to strong System-2 models. We also point out a few possible future directions.",
            "score": 4,
            "issue_id": 1528,
            "pub_date": "2025-01-05",
            "pub_date_card": {
                "ru": "5 января",
                "en": "January 5",
                "zh": "1月5日"
            },
            "hash": "7d9414c60fe7701d",
            "authors": [
                "Yixin Ji",
                "Juntao Li",
                "Hai Ye",
                "Kaixin Wu",
                "Jia Xu",
                "Linjian Mo",
                "Min Zhang"
            ],
            "affiliations": [
                "Ant Group",
                "Department of Computer Science, National University of Singapore",
                "School of Computer Science and Technology, Soochow University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.02497.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#math",
                    "#survey",
                    "#training"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Масштабирование вычислений: путь к мышлению System-2",
                    "desc": "Эта статья рассматривает масштабирование вычислений во время тестирования для улучшения производительности моделей машинного обучения. Авторы прослеживают эволюцию этой концепции от моделей System-1 до моделей System-2. В работе описываются различные методы, такие как обновление параметров, модификация входных данных и древовидный поиск. Исследование подчеркивает ключевую роль вычислений во время тестирования в переходе от моделей System-1 к сильным моделям System-2."
                },
                "en": {
                    "title": "Unlocking Model Potential: The Power of Test-Time Computing",
                    "desc": "This paper explores the concept of test-time computing scaling and its impact on machine learning models, particularly in enhancing reasoning capabilities. It distinguishes between System-1 models, which focus on improving robustness and generalization through techniques like parameter updating and output calibration, and System-2 models, which utilize methods such as repeated sampling and self-correction for complex problem-solving. The authors trace the evolution from System-1 to System-2 thinking, emphasizing how test-time computing plays a crucial role in this transition. Additionally, the paper identifies potential future research directions in this area."
                },
                "zh": {
                    "title": "测试时计算：从系统-1到强系统-2的关键转变",
                    "desc": "这篇论文探讨了测试时计算扩展对机器学习模型的影响，特别是在复杂推理中的应用。作者指出，测试时计算可以通过参数更新、输入修改、表示编辑和输出校准来提高模型的鲁棒性和泛化能力。对于系统-2模型，测试时计算通过重复采样、自我修正和树搜索来增强模型的推理能力。论文还强调了测试时计算在从系统-1模型向弱系统-2模型再到强系统-2模型转变中的关键作用，并提出了一些未来的研究方向。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.02157",
            "title": "Personalized Graph-Based Retrieval for Large Language Models",
            "url": "https://huggingface.co/papers/2501.02157",
            "abstract": "As large language models (LLMs) evolve, their ability to deliver personalized and context-aware responses offers transformative potential for improving user experiences. Existing personalization approaches, however, often rely solely on user history to augment the prompt, limiting their effectiveness in generating tailored outputs, especially in cold-start scenarios with sparse data. To address these limitations, we propose Personalized Graph-based Retrieval-Augmented Generation (PGraphRAG), a framework that leverages user-centric knowledge graphs to enrich personalization. By directly integrating structured user knowledge into the retrieval process and augmenting prompts with user-relevant context, PGraphRAG enhances contextual understanding and output quality. We also introduce the Personalized Graph-based Benchmark for Text Generation, designed to evaluate personalized text generation tasks in real-world settings where user history is sparse or unavailable. Experimental results show that PGraphRAG significantly outperforms state-of-the-art personalization methods across diverse tasks, demonstrating the unique advantages of graph-based retrieval for personalization.",
            "score": 3,
            "issue_id": 1527,
            "pub_date": "2025-01-04",
            "pub_date_card": {
                "ru": "4 января",
                "en": "January 4",
                "zh": "1月4日"
            },
            "hash": "65e3736cfc1e3295",
            "authors": [
                "Steven Au",
                "Cameron J. Dimacali",
                "Ojasmitha Pedirappagari",
                "Namyong Park",
                "Franck Dernoncourt",
                "Yu Wang",
                "Nikos Kanakaris",
                "Hanieh Deilamsalehy",
                "Ryan A. Rossi",
                "Nesreen K. Ahmed"
            ],
            "affiliations": [
                "Adobe Research",
                "Cisco AI Research",
                "Meta AI",
                "University of California Santa Cruz",
                "University of Oregon",
                "University of Southern California"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.02157.jpg",
            "data": {
                "categories": [
                    "#rag",
                    "#optimization",
                    "#graphs",
                    "#multimodal",
                    "#benchmark",
                    "#games"
                ],
                "emoji": "🕸️",
                "ru": {
                    "title": "Графы знаний на службе персонализации языковых моделей",
                    "desc": "Статья представляет новый подход к персонализации ответов больших языковых моделей (LLM) под названием PGraphRAG. В отличие от существующих методов, полагающихся на историю пользователя, PGraphRAG использует ориентированные на пользователя графы знаний для обогащения контекста. Этот метод улучшает понимание контекста и качество генерируемых ответов, особенно в сценариях с ограниченными данными о пользователе. Экспериментальные результаты показывают, что PGraphRAG превосходит современные методы персонализации в различных задачах."
                },
                "en": {
                    "title": "Revolutionizing Personalization with Graph-based Retrieval",
                    "desc": "This paper introduces a new framework called Personalized Graph-based Retrieval-Augmented Generation (PGraphRAG) that enhances the personalization of large language models (LLMs). Unlike traditional methods that depend only on user history, PGraphRAG utilizes user-centric knowledge graphs to provide richer context for generating responses. By integrating structured user information into the retrieval process, it improves the model's understanding and the quality of its outputs, especially in situations where user data is limited. The authors also present a benchmark for evaluating personalized text generation, showing that PGraphRAG outperforms existing methods in various tasks."
                },
                "zh": {
                    "title": "个性化图谱提升生成质量",
                    "desc": "随着大型语言模型的发展，它们在提供个性化和上下文感知的响应方面展现出巨大的潜力。现有的个性化方法通常仅依赖用户历史数据来增强提示，这在数据稀疏的冷启动场景中效果有限。为了解决这些问题，我们提出了个性化图谱检索增强生成（PGraphRAG）框架，利用以用户为中心的知识图谱来丰富个性化。实验结果表明，PGraphRAG在多种任务中显著优于现有的个性化方法，展示了基于图谱的检索在个性化中的独特优势。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.02832",
            "title": "Samba-asr state-of-the-art speech recognition leveraging structured state-space models",
            "url": "https://huggingface.co/papers/2501.02832",
            "abstract": "We propose Samba ASR, the first state-of-the-art Automatic Speech Recognition (ASR) model leveraging the novel Mamba architecture as both encoder and decoder, built on the foundation of state-space models (SSMs). Unlike transformer-based ASR models, which rely on self-attention mechanisms to capture dependencies, Samba ASR effectively models both local and global temporal dependencies using efficient state-space dynamics, achieving remarkable performance gains. By addressing the limitations of transformers, such as quadratic scaling with input length and difficulty in handling long-range dependencies, Samba ASR achieves superior accuracy and efficiency.   Experimental results demonstrate that Samba ASR surpasses existing open-source transformer-based ASR models across various standard benchmarks, establishing it as the new state of the art in ASR. Extensive evaluations on benchmark datasets show significant improvements in Word Error Rate (WER), with competitive performance even in low-resource scenarios. Furthermore, the computational efficiency and parameter optimization of the Mamba architecture make Samba ASR a scalable and robust solution for diverse ASR tasks.   Our contributions include:   A new Samba ASR architecture demonstrating the superiority of SSMs over transformer-based models for speech sequence processing. A comprehensive evaluation on public benchmarks showcasing state-of-the-art performance. An analysis of computational efficiency, robustness to noise, and sequence generalization. This work highlights the viability of Mamba SSMs as a transformer-free alternative for efficient and accurate ASR. By leveraging state-space modeling advancements, Samba ASR sets a new benchmark for ASR performance and future research.",
            "score": 2,
            "issue_id": 1530,
            "pub_date": "2025-01-06",
            "pub_date_card": {
                "ru": "6 января",
                "en": "January 6",
                "zh": "1月6日"
            },
            "hash": "ed3c4a6192d0c5f9",
            "authors": [
                "Syed Abdul Gaffar Shakhadri",
                "Kruthika KR",
                "Kartik Basavaraj Angadi"
            ],
            "affiliations": [
                "SandLogic Technologies Pvt Ltd"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.02832.jpg",
            "data": {
                "categories": [
                    "#audio",
                    "#architecture",
                    "#benchmark",
                    "#low_resource",
                    "#open_source"
                ],
                "emoji": "🎙️",
                "ru": {
                    "title": "Samba ASR: революция в распознавании речи с помощью моделей пространства состояний",
                    "desc": "Представлена модель Samba ASR - первая современная система автоматического распознавания речи, использующая архитектуру Mamba в качестве энкодера и декодера на основе моделей пространства состояний (SSM). В отличие от трансформерных моделей, Samba ASR эффективно моделирует локальные и глобальные временные зависимости, достигая значительных улучшений производительности. Экспериментальные результаты показывают, что Samba ASR превосходит существующие модели с открытым исходным кодом на основе трансформеров по различным стандартным показателям. Модель демонстрирует значительное снижение показателя Word Error Rate (WER) и высокую эффективность даже при ограниченных ресурсах."
                },
                "en": {
                    "title": "Samba ASR: Redefining Speech Recognition with State-Space Models",
                    "desc": "Samba ASR is a groundbreaking Automatic Speech Recognition model that utilizes the innovative Mamba architecture, which functions as both the encoder and decoder. This model departs from traditional transformer-based approaches by employing state-space models (SSMs) to effectively capture both local and global temporal dependencies, leading to enhanced performance. By overcoming the challenges associated with transformers, such as their inefficiency with long input sequences, Samba ASR achieves superior accuracy and efficiency in recognizing speech. Extensive testing shows that Samba ASR not only outperforms existing transformer-based models but also excels in low-resource environments, making it a robust solution for various ASR applications."
                },
                "zh": {
                    "title": "Samba ASR：超越变换器的语音识别新标杆",
                    "desc": "我们提出了Samba ASR，这是第一个利用新型Mamba架构作为编码器和解码器的最先进自动语音识别（ASR）模型。与基于变换器的ASR模型不同，Samba ASR通过高效的状态空间动态建模局部和全局时间依赖关系，从而实现显著的性能提升。该模型克服了变换器在处理长距离依赖和输入长度的平方扩展等方面的局限性，展现出更高的准确性和效率。实验结果表明，Samba ASR在多个标准基准测试中超越了现有的开源变换器ASR模型，确立了其在ASR领域的新标杆。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.02690",
            "title": "GS-DiT: Advancing Video Generation with Pseudo 4D Gaussian Fields through Efficient Dense 3D Point Tracking",
            "url": "https://huggingface.co/papers/2501.02690",
            "abstract": "4D video control is essential in video generation as it enables the use of sophisticated lens techniques, such as multi-camera shooting and dolly zoom, which are currently unsupported by existing methods. Training a video Diffusion Transformer (DiT) directly to control 4D content requires expensive multi-view videos. Inspired by Monocular Dynamic novel View Synthesis (MDVS) that optimizes a 4D representation and renders videos according to different 4D elements, such as camera pose and object motion editing, we bring pseudo 4D Gaussian fields to video generation. Specifically, we propose a novel framework that constructs a pseudo 4D Gaussian field with dense 3D point tracking and renders the Gaussian field for all video frames. Then we finetune a pretrained DiT to generate videos following the guidance of the rendered video, dubbed as GS-DiT. To boost the training of the GS-DiT, we also propose an efficient Dense 3D Point Tracking (D3D-PT) method for the pseudo 4D Gaussian field construction. Our D3D-PT outperforms SpatialTracker, the state-of-the-art sparse 3D point tracking method, in accuracy and accelerates the inference speed by two orders of magnitude. During the inference stage, GS-DiT can generate videos with the same dynamic content while adhering to different camera parameters, addressing a significant limitation of current video generation models. GS-DiT demonstrates strong generalization capabilities and extends the 4D controllability of Gaussian splatting to video generation beyond just camera poses. It supports advanced cinematic effects through the manipulation of the Gaussian field and camera intrinsics, making it a powerful tool for creative video production. Demos are available at https://wkbian.github.io/Projects/GS-DiT/.",
            "score": 1,
            "issue_id": 1530,
            "pub_date": "2025-01-05",
            "pub_date_card": {
                "ru": "5 января",
                "en": "January 5",
                "zh": "1月5日"
            },
            "hash": "b4c147a2637166a8",
            "authors": [
                "Weikang Bian",
                "Zhaoyang Huang",
                "Xiaoyu Shi",
                "Yijin Li",
                "Fu-Yun Wang",
                "Hongsheng Li"
            ],
            "affiliations": [
                "Avolution AI",
                "Centre for Perceptual and Interactive Intelligence",
                "Multimedia Laboratory, The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.02690.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#games",
                    "#diffusion",
                    "#3d"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Революция в генерации видео: 4D-контроль с помощью гауссовых полей",
                    "desc": "Эта статья представляет инновационный подход к генерации видео с 4D-контролем, используя псевдо-4D гауссовы поля и модель Diffusion Transformer (DiT). Авторы предлагают метод Dense 3D Point Tracking (D3D-PT) для эффективного построения гауссовых полей, превосходящий существующие решения по точности и скорости. Разработанная система GS-DiT позволяет генерировать видео с одинаковым динамическим содержанием, но с разными параметрами камеры, что открывает новые возможности для создания кинематографических эффектов. Метод демонстрирует сильные обобщающие способности и расширяет возможности 4D-контроля в генерации видео."
                },
                "en": {
                    "title": "Revolutionizing Video Generation with 4D Control",
                    "desc": "This paper introduces a new method for generating videos that can be controlled in four dimensions (4D), which includes both camera movement and object motion. The authors propose a framework called GS-DiT that utilizes pseudo 4D Gaussian fields to enhance video generation, allowing for advanced cinematic effects. They also present a Dense 3D Point Tracking (D3D-PT) technique that improves the accuracy and speed of tracking 3D points compared to existing methods. Overall, GS-DiT enables the creation of dynamic videos with flexible camera parameters, significantly advancing the capabilities of video generation models."
                },
                "zh": {
                    "title": "伪4D高斯场：视频生成的新突破",
                    "desc": "本论文提出了一种新颖的框架，利用伪4D高斯场进行视频生成，以支持复杂的镜头技术。我们通过密集的3D点跟踪构建伪4D高斯场，并为所有视频帧渲染该高斯场。为了提升GS-DiT的训练效果，我们还提出了一种高效的密集3D点跟踪方法，显著提高了准确性和推理速度。GS-DiT能够在不同的相机参数下生成具有相同动态内容的视频，扩展了视频生成的4D可控性，成为创意视频制作的强大工具。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.01830",
            "title": "Auto-RT: Automatic Jailbreak Strategy Exploration for Red-Teaming Large Language Models",
            "url": "https://huggingface.co/papers/2501.01830",
            "abstract": "Automated red-teaming has become a crucial approach for uncovering vulnerabilities in large language models (LLMs). However, most existing methods focus on isolated safety flaws, limiting their ability to adapt to dynamic defenses and uncover complex vulnerabilities efficiently. To address this challenge, we propose Auto-RT, a reinforcement learning framework that automatically explores and optimizes complex attack strategies to effectively uncover security vulnerabilities through malicious queries. Specifically, we introduce two key mechanisms to reduce exploration complexity and improve strategy optimization: 1) Early-terminated Exploration, which accelerate exploration by focusing on high-potential attack strategies; and 2) Progressive Reward Tracking algorithm with intermediate downgrade models, which dynamically refine the search trajectory toward successful vulnerability exploitation. Extensive experiments across diverse LLMs demonstrate that, by significantly improving exploration efficiency and automatically optimizing attack strategies, Auto-RT detects a boarder range of vulnerabilities, achieving a faster detection speed and 16.63\\% higher success rates compared to existing methods.",
            "score": 1,
            "issue_id": 1529,
            "pub_date": "2025-01-03",
            "pub_date_card": {
                "ru": "3 января",
                "en": "January 3",
                "zh": "1月3日"
            },
            "hash": "5b08b81c52ec8da8",
            "authors": [
                "Yanjiang Liu",
                "Shuhen Zhou",
                "Yaojie Lu",
                "Huijia Zhu",
                "Weiqiang Wang",
                "Hongyu Lin",
                "Ben He",
                "Xianpei Han",
                "Le Sun"
            ],
            "affiliations": [
                "Ant Group",
                "Chinese Information Processing Laboratory, Institute of Software, Chinese Academy of Sciences, Beijing, China",
                "University of Chinese Academy of Sciences, Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.01830.jpg",
            "data": {
                "categories": [
                    "#security",
                    "#rl",
                    "#rlhf"
                ],
                "emoji": "🛡️",
                "ru": {
                    "title": "Auto-RT: Умная защита больших языковых моделей",
                    "desc": "Авторы представляют Auto-RT - фреймворк на основе обучения с подкреплением для автоматизированного поиска уязвимостей в больших языковых моделях (LLM). Система использует механизмы раннего прекращения исследования и прогрессивного отслеживания наград для оптимизации стратегий атак. Auto-RT превосходит существующие методы, обнаруживая более широкий спектр уязвимостей с большей скоростью и на 16.63% более высоким уровнем успеха. Этот подход позволяет эффективно выявлять сложные уязвимости в LLM через вредоносные запросы."
                },
                "en": {
                    "title": "Auto-RT: Revolutionizing Vulnerability Detection in LLMs",
                    "desc": "This paper presents Auto-RT, a reinforcement learning framework designed to enhance automated red-teaming for large language models (LLMs). Unlike traditional methods that target isolated safety flaws, Auto-RT efficiently uncovers complex vulnerabilities by optimizing attack strategies through malicious queries. It introduces two innovative mechanisms: Early-terminated Exploration to prioritize promising attack strategies, and Progressive Reward Tracking to refine the search process dynamically. Experimental results show that Auto-RT significantly improves exploration efficiency and detection success rates, outperforming existing approaches."
                },
                "zh": {
                    "title": "自动化红队：高效发现语言模型漏洞的利器",
                    "desc": "自动化红队技术在发现大型语言模型（LLMs）中的漏洞方面变得至关重要。现有方法大多集中于孤立的安全缺陷，限制了其适应动态防御和高效发现复杂漏洞的能力。为了解决这个问题，我们提出了Auto-RT，一个强化学习框架，能够自动探索和优化复杂的攻击策略，通过恶意查询有效发现安全漏洞。我们的实验表明，Auto-RT显著提高了探索效率和攻击策略的自动优化，检测到更广泛的漏洞，检测速度更快，成功率提高了16.63%。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.01790",
            "title": "Ingredients: Blending Custom Photos with Video Diffusion Transformers",
            "url": "https://huggingface.co/papers/2501.01790",
            "abstract": "This paper presents a powerful framework to customize video creations by incorporating multiple specific identity (ID) photos, with video diffusion Transformers, referred to as Ingredients. Generally, our method consists of three primary modules: (i) a facial extractor that captures versatile and precise facial features for each human ID from both global and local perspectives; (ii) a multi-scale projector that maps face embeddings into the contextual space of image query in video diffusion transformers; (iii) an ID router that dynamically combines and allocates multiple ID embedding to the corresponding space-time regions. Leveraging a meticulously curated text-video dataset and a multi-stage training protocol, Ingredients demonstrates superior performance in turning custom photos into dynamic and personalized video content. Qualitative evaluations highlight the advantages of proposed method, positioning it as a significant advancement toward more effective generative video control tools in Transformer-based architecture, compared to existing methods. The data, code, and model weights are publicly available at: https://github.com/feizc/Ingredients.",
            "score": 1,
            "issue_id": 1528,
            "pub_date": "2025-01-03",
            "pub_date_card": {
                "ru": "3 января",
                "en": "January 3",
                "zh": "1月3日"
            },
            "hash": "dd1ccebdd2fcf276",
            "authors": [
                "Zhengcong Fei",
                "Debang Li",
                "Di Qiu",
                "Changqian Yu",
                "Mingyuan Fan"
            ],
            "affiliations": [
                "Kunlun Inc. Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.01790.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#training",
                    "#architecture",
                    "#video",
                    "#dataset",
                    "#diffusion",
                    "#multimodal"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Персонализированное видео из фотографий: новый уровень контроля в генеративных моделях",
                    "desc": "Статья представляет новый метод под названием Ingredients для создания персонализированных видео с использованием нескольких фотографий конкретных людей. Метод состоит из трех основных модулей: экстрактора лицевых признаков, многомасштабного проектора и маршрутизатора идентификаторов. Ingredients использует тщательно подобранный набор данных текст-видео и многоэтапный протокол обучения для достижения превосходных результатов. Качественная оценка показывает преимущества предложенного метода по сравнению с существующими подходами в области генеративного контроля видео на основе архитектуры Transformer."
                },
                "en": {
                    "title": "Transforming Photos into Personalized Videos with Ingredients",
                    "desc": "This paper introduces a novel framework called Ingredients for creating personalized videos using multiple identity photos. It employs a facial extractor to accurately capture facial features, a multi-scale projector to integrate these features into video diffusion transformers, and an ID router to manage the allocation of identity embeddings across different time and space regions in the video. The framework is trained on a carefully selected text-video dataset, enhancing its ability to generate dynamic video content from custom images. The results show that Ingredients outperforms existing methods, marking a significant step forward in generative video control using Transformer architectures."
                },
                "zh": {
                    "title": "个性化视频创作的新突破",
                    "desc": "本文提出了一种强大的框架，通过结合多个特定身份照片，定制视频创作，称为Ingredients。该方法主要由三个模块组成：面部提取器、多个尺度投影器和身份路由器，分别用于提取面部特征、映射面部嵌入和动态分配身份嵌入。通过精心策划的文本-视频数据集和多阶段训练协议，Ingredients在将自定义照片转化为动态个性化视频内容方面表现出色。定性评估显示，该方法在基于Transformer的架构中，相较于现有方法，显著提升了生成视频控制工具的有效性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.02506",
            "title": "ToolHop: A Query-Driven Benchmark for Evaluating Large Language Models in Multi-Hop Tool Use",
            "url": "https://huggingface.co/papers/2501.02506",
            "abstract": "Effective evaluation of multi-hop tool use is critical for analyzing the understanding, reasoning, and function-calling capabilities of large language models (LLMs). However, progress has been hindered by a lack of reliable evaluation datasets. To address this, we present ToolHop, a dataset comprising 995 user queries and 3,912 associated tools, specifically designed for rigorous evaluation of multi-hop tool use. ToolHop ensures diverse queries, meaningful interdependencies, locally executable tools, detailed feedback, and verifiable answers through a novel query-driven data construction approach that includes tool creation, document refinement, and code generation. We evaluate 14 LLMs across five model families (i.e., LLaMA3.1, Qwen2.5, Gemini1.5, Claude3.5, and GPT), uncovering significant challenges in handling multi-hop tool-use scenarios. The leading model, GPT-4o, achieves an accuracy of 49.04%, underscoring substantial room for improvement. Further analysis reveals variations in tool-use strategies for various families, offering actionable insights to guide the development of more effective approaches. Code and data can be found in https://huggingface.co/bytedance-research/ToolHop.",
            "score": 0,
            "issue_id": 1529,
            "pub_date": "2025-01-05",
            "pub_date_card": {
                "ru": "5 января",
                "en": "January 5",
                "zh": "1月5日"
            },
            "hash": "f785173226e5f9fc",
            "authors": [
                "Junjie Ye",
                "Zhengyin Du",
                "Xuesong Yao",
                "Weijian Lin",
                "Yufei Xu",
                "Zehui Chen",
                "Zaiyuan Wang",
                "Sining Zhu",
                "Zhiheng Xi",
                "Siyu Yuan",
                "Tao Gui",
                "Qi Zhang",
                "Xuanjing Huang",
                "Jiechao Chen"
            ],
            "affiliations": [
                "ByteDance",
                "Institute of Modern Languages and Linguistics, Fudan University",
                "School of Computer Science, Fudan University",
                "School of Data Science, Fudan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.02506.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#reasoning",
                    "#dataset",
                    "#optimization"
                ],
                "emoji": "🛠️",
                "ru": {
                    "title": "ToolHop: новый стандарт для оценки многоэтапного использования инструментов в LLM",
                    "desc": "Статья представляет новый набор данных ToolHop для оценки многоэтапного использования инструментов большими языковыми моделями (LLM). ToolHop содержит 995 пользовательских запросов и 3912 связанных инструментов, обеспечивая разнообразие запросов, взаимозависимости и возможность локального выполнения. Авторы оценили 14 LLM из пяти семейств моделей, выявив значительные трудности в обработке сценариев многоэтапного использования инструментов. Лучшая модель, GPT-4o, достигла точности 49.04%, что указывает на большой потенциал для улучшения."
                },
                "en": {
                    "title": "ToolHop: Advancing Multi-Hop Tool Use Evaluation for LLMs",
                    "desc": "This paper introduces ToolHop, a new dataset designed to evaluate how well large language models (LLMs) can use multiple tools in a single task. It includes 995 user queries and 3,912 tools, focusing on diverse and interdependent queries that can be executed locally. The authors tested 14 different LLMs, revealing that even the best-performing model, GPT-4o, only achieved 49.04% accuracy, indicating significant challenges in multi-hop tool use. The findings highlight different strategies employed by various model families, providing insights for future improvements in LLM capabilities."
                },
                "zh": {
                    "title": "ToolHop：多跳工具使用的有效评估数据集",
                    "desc": "本文介绍了ToolHop数据集，该数据集包含995个用户查询和3912个相关工具，旨在有效评估大型语言模型（LLMs）在多跳工具使用中的理解、推理和功能调用能力。通过新颖的查询驱动数据构建方法，ToolHop确保了查询的多样性、工具的局部可执行性和可验证的答案。我们对14个不同模型（如LLaMA3.1、Qwen2.5等）进行了评估，发现它们在处理多跳工具使用场景时面临显著挑战。尽管GPT-4o模型的准确率为49.04%，但仍有很大的改进空间，分析还揭示了不同模型家族在工具使用策略上的差异，为未来的研究提供了有价值的见解。"
                }
            }
        }
    ],
    "link_prev": "2025-01-06.html",
    "link_next": "2025-01-08.html",
    "link_month": "2025-01.html",
    "short_date_prev": {
        "ru": "06.01",
        "en": "01/06",
        "zh": "1月6日"
    },
    "short_date_next": {
        "ru": "08.01",
        "en": "01/08",
        "zh": "1月8日"
    },
    "categories": {
        "#dataset": 3,
        "#data": 1,
        "#benchmark": 4,
        "#agents": 0,
        "#cv": 1,
        "#rl": 1,
        "#rlhf": 1,
        "#rag": 1,
        "#plp": 0,
        "#inference": 0,
        "#3d": 1,
        "#audio": 1,
        "#video": 4,
        "#multimodal": 3,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 4,
        "#healthcare": 1,
        "#training": 4,
        "#robotics": 0,
        "#agi": 0,
        "#games": 2,
        "#interpretability": 0,
        "#reasoning": 2,
        "#transfer_learning": 0,
        "#graphs": 1,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 4,
        "#survey": 1,
        "#diffusion": 4,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 1
    },
    "zh": {
        "text": "This article introduces EnerVerse, a framework for generating future space in robotic tasks. It uses attention mechanisms for consistent space modeling and a memory context for long sequence generation. The FAV space enhances robot observation and adaptability. A data engine with 4DGS improves data quality and diversity. Experiments show it boosts performance in long-range robotic tasks.",
        "title": "EnerVerse: Envisioning Embodied Future Space for Robotics Manipulation",
        "pinyin": "Sure, here is the pinyin transcription for the given text:\n\nZhè wénzhāng jièshào EnerVerse, yīgè kuàngjià yǐnqǐ wèilái kōngjiān zài jīqirén rénwù zhōng. Tā shǐyòng zhùyì jīzhì wèi yuáncháng kōngjiān móxíng hé yīgè jìyì qūjiàn wèi cháng xùliè shēngchéng. FAV kōngjiān zēngqiáng jīqirén guānchá hé shìyìngxìng. Yīgè shùjù yǐnqíng yǒu 4DGS gǎishàn shùjù zhìliàng hé duōyàngxìng. Shíyàn xiǎnshì tā zēngqiáng xiàoguǒ zài chángqī jīqirén rénwù zhōng.\n\nPlease note that the pinyin transcription is based on the pronunciation of the Chinese characters that would be used to translate the English text. The actual translation might vary slightly depending on the context and specific terminology used.",
        "vocab": "[\n    {\"word\": \"framework\", \"pinyin\": \"kuàngjià\", \"trans\": \"框架\"},\n    {\"word\": \"generating\", \"pinyin\": \"shēngchéng\", \"trans\": \"生成\"},\n    {\"word\": \"future\", \"pinyin\": \"wèilái\", \"trans\": \"未来\"},\n    {\"word\": \"space\", \"pinyin\": \"kōngjiān\", \"trans\": \"空间\"},\n    {\"word\": \"robotic\", \"pinyin\": \"jīqirén\", \"trans\": \"机器人\"},\n    {\"word\": \"tasks\", \"pinyin\": \"rènwù\", \"trans\": \"任务\"},\n    {\"word\": \"attention\", \"pinyin\": \"zhùyì\", \"trans\": \"注意\"},\n    {\"word\": \"mechanisms\", \"pinyin\": \"jīzhì\", \"trans\": \"机制\"},\n    {\"word\": \"consistent\", \"pinyin\": \"wúguǒ\", \"trans\": \"一致\"},\n    {\"word\": \"modeling\", \"pinyin\": \"móxíng\", \"trans\": \"建模\"},\n    {\"word\": \"memory\", \"pinyin\": \"jìyì\", \"trans\": \"记忆\"},\n    {\"word\": \"context\", \"pinyin\": \"qǔwén\", \"trans\": \"上下文\"},\n    {\"word\": \"sequence\", \"pinyin\": \"xùliè\", \"trans\": \"序列\"},\n    {\"word\": \"generation\", \"pinyin\": \"shēngchéng\", \"trans\": \"生成\"},\n    {\"word\": \"FAV\", \"pinyin\": \"Fēi-Ēi-Wēi\", \"trans\": \"FAV\"},\n    {\"word\": \"enhances\", \"pinyin\": \"zēngqiáng\", \"trans\": \"增强\"},\n    {\"word\": \"observation\", \"pinyin\": \"guānchá\", \"trans\": \"观察\"},\n    {\"word\": \"adaptability\", \"pinyin\": \"shìyìngxìng\", \"trans\": \"适应性\"},\n    {\"word\": \"engine\", \"pinyin\": \"yǐnqíng\", \"trans\": \"引擎\"},\n    {\"word\": \"4DGS\", \"pinyin\": \"Sì-Dī-Jī-Ēs\", \"trans\": \"4DGS\"},\n    {\"word\": \"improves\", \"pinyin\": \"gǎishàn\", \"trans\": \"改善\"},\n    {\"word\": \"quality\", \"pinyin\": \"zhìliàng\", \"trans\": \"质量\"},\n    {\"word\": \"diversity\", \"pinyin\": \"duōyàngxìng\", \"trans\": \"多样性\"},\n    {\"word\": \"experiments\", \"pinyin\": \"shíyàn\", \"trans\": \"实验\"},\n    {\"word\": \"show\", \"pinyin\": \"xiǎnshì\", \"trans\": \"显示\"},\n    {\"word\": \"boosts\", \"pinyin\": \"zēngqiáng\", \"trans\": \"增强\"},\n    {\"word\": \"performance\", \"pinyin\": \"biǎoxiàn\", \"trans\": \"表现\"},\n    {\"word\": \"long-range\", \"pinyin\": \"chángyuǎn\", \"trans\": \"长远\"}\n]",
        "trans": "This article introduces EnerVerse, a framework designed to generate future space in robotic tasks. It employs attention mechanisms to ensure consistent space modeling and utilizes a memory context for generating long sequences. The FAV space enhances robot observation capabilities and adaptability. Additionally, a data engine equipped with 4DGS improves the quality and diversity of data. Experiments demonstrate that it significantly boosts performance in long-range robotic tasks.",
        "update_ts": "2025-01-06 09:11"
    }
}