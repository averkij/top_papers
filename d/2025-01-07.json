{
    "date": {
        "ru": "7 ÑĞ½Ğ²Ğ°Ñ€Ñ",
        "en": "January 7",
        "zh": "1æœˆ7æ—¥"
    },
    "time_utc": "2025-01-07 07:10",
    "weekday": 1,
    "issue_id": 1531,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2501.02045",
            "title": "METAGENE-1: Metagenomic Foundation Model for Pandemic Monitoring",
            "url": "https://huggingface.co/papers/2501.02045",
            "abstract": "We pretrain METAGENE-1, a 7-billion-parameter autoregressive transformer model, which we refer to as a metagenomic foundation model, on a novel corpus of diverse metagenomic DNA and RNA sequences comprising over 1.5 trillion base pairs. This dataset is sourced from a large collection of human wastewater samples, processed and sequenced using deep metagenomic (next-generation) sequencing methods. Unlike genomic models that focus on individual genomes or curated sets of specific species, the aim of METAGENE-1 is to capture the full distribution of genomic information present within this wastewater, to aid in tasks relevant to pandemic monitoring and pathogen detection. We carry out byte-pair encoding (BPE) tokenization on our dataset, tailored for metagenomic sequences, and then pretrain our model. In this paper, we first detail the pretraining dataset, tokenization strategy, and model architecture, highlighting the considerations and design choices that enable the effective modeling of metagenomic data. We then show results of pretraining this model on our metagenomic dataset, providing details about our losses, system metrics, and training stability over the course of pretraining. Finally, we demonstrate the performance of METAGENE-1, which achieves state-of-the-art results on a set of genomic benchmarks and new evaluations focused on human-pathogen detection and genomic sequence embedding, showcasing its potential for public health applications in pandemic monitoring, biosurveillance, and early detection of emerging health threats.",
            "score": 7,
            "issue_id": 1528,
            "pub_date": "2025-01-03",
            "pub_date_card": {
                "ru": "3 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 3",
                "zh": "1æœˆ3æ—¥"
            },
            "hash": "60a3568f555ed60f",
            "authors": [
                "Ollie Liu",
                "Sami Jaghouar",
                "Johannes Hagemann",
                "Shangshang Wang",
                "Jason Wiemels",
                "Jeff Kaufman",
                "Willie Neiswanger"
            ],
            "affiliations": [
                "Nucleic Acid Observatory",
                "Prime Intellect",
                "University of Southern California"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.02045.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#data",
                    "#training",
                    "#architecture",
                    "#science",
                    "#dataset",
                    "#healthcare"
                ],
                "emoji": "ğŸ§¬",
                "ru": {
                    "title": "METAGENE-1: ĞœĞµÑ‚Ğ°Ğ³ĞµĞ½Ğ¾Ğ¼Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³Ğ° Ğ·Ğ´Ğ¾Ñ€Ğ¾Ğ²ÑŒÑ Ğ½Ğ°ÑĞµĞ»ĞµĞ½Ğ¸Ñ",
                    "desc": "METAGENE-1 - ÑÑ‚Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€Ğ°ÑÑĞ¸Ğ²Ğ½Ğ°Ñ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ 7 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ°Ğ³ĞµĞ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑÑ… Ğ”ĞĞš Ğ¸ Ğ ĞĞš. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ° Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ³ĞµĞ½Ğ¾Ğ¼Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ· Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ² ÑÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ´ Ñ Ñ†ĞµĞ»ÑŒÑ Ğ¼Ğ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³Ğ° Ğ¿Ğ°Ğ½Ğ´ĞµĞ¼Ğ¸Ğ¹ Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ñ‚Ğ¾Ğ³ĞµĞ½Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ³ĞµĞ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. METAGENE-1 Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¸ Ğ¿Ğ°Ñ‚Ğ¾Ğ³ĞµĞ½Ğ¾Ğ² Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¸ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ³ĞµĞ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹, Ñ‡Ñ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ñ‹ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ² Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ·Ğ´Ñ€Ğ°Ğ²Ğ¾Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Unlocking Metagenomics: METAGENE-1 for Pandemic Preparedness",
                    "desc": "The paper introduces METAGENE-1, a large autoregressive transformer model designed for metagenomic data analysis. It is pretrained on a vast dataset of metagenomic DNA and RNA sequences derived from human wastewater, totaling over 1.5 trillion base pairs. The model aims to enhance pandemic monitoring and pathogen detection by capturing the diverse genomic information present in wastewater samples. The authors detail their tokenization strategy and model architecture, demonstrating that METAGENE-1 achieves state-of-the-art performance in genomic benchmarks and applications related to public health."
                },
                "zh": {
                    "title": "METAGENE-1ï¼šå…ƒåŸºå› ç»„åŸºç¡€æ¨¡å‹åŠ©åŠ›å…¬å…±å«ç”Ÿç›‘æµ‹",
                    "desc": "æˆ‘ä»¬é¢„è®­ç»ƒäº†METAGENE-1ï¼Œè¿™æ˜¯ä¸€ä¸ªæ‹¥æœ‰70äº¿å‚æ•°çš„è‡ªå›å½’å˜æ¢å™¨æ¨¡å‹ï¼Œç§°ä¸ºå…ƒåŸºå› ç»„åŸºç¡€æ¨¡å‹ã€‚è¯¥æ¨¡å‹åœ¨ä¸€ä¸ªåŒ…å«è¶…è¿‡1.5ä¸‡äº¿ç¢±åŸºå¯¹çš„å¤šæ ·åŒ–å…ƒåŸºå› ç»„DNAå’ŒRNAåºåˆ—çš„æ–°æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œè¿™äº›æ•°æ®æ¥è‡ªå¤§é‡äººç±»åºŸæ°´æ ·æœ¬ã€‚METAGENE-1çš„ç›®æ ‡æ˜¯æ•æ‰åºŸæ°´ä¸­å­˜åœ¨çš„åŸºå› ç»„ä¿¡æ¯çš„å®Œæ•´åˆ†å¸ƒï¼Œä»¥å¸®åŠ©è¿›è¡Œç–«æƒ…ç›‘æµ‹å’Œç—…åŸä½“æ£€æµ‹ã€‚æˆ‘ä»¬å±•ç¤ºäº†è¯¥æ¨¡å‹åœ¨å…ƒåŸºå› ç»„æ•°æ®é›†ä¸Šçš„é¢„è®­ç»ƒç»“æœï¼Œè¯æ˜å…¶åœ¨å…¬å…±å«ç”Ÿåº”ç”¨ä¸­çš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.03006",
            "title": "TransPixar: Advancing Text-to-Video Generation with Transparency",
            "url": "https://huggingface.co/papers/2501.03006",
            "abstract": "Text-to-video generative models have made significant strides, enabling diverse applications in entertainment, advertising, and education. However, generating RGBA video, which includes alpha channels for transparency, remains a challenge due to limited datasets and the difficulty of adapting existing models. Alpha channels are crucial for visual effects (VFX), allowing transparent elements like smoke and reflections to blend seamlessly into scenes. We introduce TransPixar, a method to extend pretrained video models for RGBA generation while retaining the original RGB capabilities. TransPixar leverages a diffusion transformer (DiT) architecture, incorporating alpha-specific tokens and using LoRA-based fine-tuning to jointly generate RGB and alpha channels with high consistency. By optimizing attention mechanisms, TransPixar preserves the strengths of the original RGB model and achieves strong alignment between RGB and alpha channels despite limited training data. Our approach effectively generates diverse and consistent RGBA videos, advancing the possibilities for VFX and interactive content creation.",
            "score": 6,
            "issue_id": 1527,
            "pub_date": "2025-01-06",
            "pub_date_card": {
                "ru": "6 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 6",
                "zh": "1æœˆ6æ—¥"
            },
            "hash": "e85e5fa9a03d5d04",
            "authors": [
                "Luozhou Wang",
                "Yijun Li",
                "Zhifei Chen",
                "Jui-Hsien Wang",
                "Zhifei Zhang",
                "He Zhang",
                "Zhe Lin",
                "Yingcong Chen"
            ],
            "affiliations": [
                "Adobe Research",
                "HKUST",
                "HKUST(GZ)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.03006.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#architecture",
                    "#training",
                    "#diffusion",
                    "#video"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "TransPixar: ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ RGBA-Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ„Ñ„ĞµĞºÑ‚Ğ¾Ğ²",
                    "desc": "TransPixar - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ RGBA-Ğ²Ğ¸Ğ´ĞµĞ¾, Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑÑÑ‰Ğ¸Ğ¹ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ° (DiT) Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹, ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ°Ğ»ÑŒÑ„Ğ°-ĞºĞ°Ğ½Ğ°Ğ»Ğ°, Ğ´Ğ»Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ RGB Ğ¸ Ğ°Ğ»ÑŒÑ„Ğ°-ĞºĞ°Ğ½Ğ°Ğ»Ğ¾Ğ² Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒÑ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ñ‚Ğ¾Ğ½ĞºÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LoRA Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ RGB-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. TransPixar ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ RGBA-Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°Ñ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ„Ñ„ĞµĞºÑ‚Ğ¾Ğ² Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°."
                },
                "en": {
                    "title": "TransPixar: Bridging RGB and Alpha for Enhanced Video Generation",
                    "desc": "This paper presents TransPixar, a novel method for generating RGBA videos, which include transparency information crucial for visual effects. The challenge lies in the limited datasets and the need to adapt existing models to handle alpha channels effectively. TransPixar utilizes a diffusion transformer architecture and incorporates alpha-specific tokens, allowing it to generate both RGB and alpha channels simultaneously. By optimizing attention mechanisms and employing LoRA-based fine-tuning, TransPixar achieves high consistency between RGB and alpha outputs, enhancing the quality of video generation for applications in VFX and interactive media."
                },
                "zh": {
                    "title": "TransPixarï¼šç”Ÿæˆé«˜è´¨é‡RGBAè§†é¢‘çš„æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºTransPixarçš„æ–¹æ³•ï¼Œæ—¨åœ¨ç”ŸæˆåŒ…å«é€æ˜é€šé“çš„RGBAè§†é¢‘ã€‚ä¼ ç»Ÿçš„è§†é¢‘ç”Ÿæˆæ¨¡å‹åœ¨å¤„ç†é€æ˜æ•ˆæœæ—¶é¢ä¸´æŒ‘æˆ˜ï¼ŒTransPixaré€šè¿‡æ‰©å±•é¢„è®­ç»ƒæ¨¡å‹æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚è¯¥æ–¹æ³•åˆ©ç”¨æ‰©æ•£å˜æ¢å™¨æ¶æ„ï¼Œç»“åˆç‰¹å®šçš„é€æ˜é€šé“æ ‡è®°ï¼Œå¹¶é€šè¿‡LoRAå¾®è°ƒå®ç°RGBå’Œé€æ˜é€šé“çš„é«˜ä¸€è‡´æ€§ç”Ÿæˆã€‚æœ€ç»ˆï¼ŒTransPixaråœ¨æœ‰é™çš„æ•°æ®é›†ä¸Šä¼˜åŒ–äº†æ³¨æ„åŠ›æœºåˆ¶ï¼ŒæˆåŠŸç”Ÿæˆå¤šæ ·ä¸”ä¸€è‡´çš„RGBAè§†é¢‘ï¼Œæ¨åŠ¨äº†è§†è§‰ç‰¹æ•ˆå’Œäº’åŠ¨å†…å®¹åˆ›ä½œçš„å¯èƒ½æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.02976",
            "title": "STAR: Spatial-Temporal Augmentation with Text-to-Video Models for Real-World Video Super-Resolution",
            "url": "https://huggingface.co/papers/2501.02976",
            "abstract": "Image diffusion models have been adapted for real-world video super-resolution to tackle over-smoothing issues in GAN-based methods. However, these models struggle to maintain temporal consistency, as they are trained on static images, limiting their ability to capture temporal dynamics effectively. Integrating text-to-video (T2V) models into video super-resolution for improved temporal modeling is straightforward. However, two key challenges remain: artifacts introduced by complex degradations in real-world scenarios, and compromised fidelity due to the strong generative capacity of powerful T2V models (e.g., CogVideoX-5B). To enhance the spatio-temporal quality of restored videos, we introduce~\\name (Spatial-Temporal Augmentation with T2V models for Real-world video super-resolution), a novel approach that leverages T2V models for real-world video super-resolution, achieving realistic spatial details and robust temporal consistency. Specifically, we introduce a Local Information Enhancement Module (LIEM) before the global attention block to enrich local details and mitigate degradation artifacts. Moreover, we propose a Dynamic Frequency (DF) Loss to reinforce fidelity, guiding the model to focus on different frequency components across diffusion steps. Extensive experiments demonstrate~\\name~outperforms state-of-the-art methods on both synthetic and real-world datasets.",
            "score": 5,
            "issue_id": 1527,
            "pub_date": "2025-01-06",
            "pub_date_card": {
                "ru": "6 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 6",
                "zh": "1æœˆ6æ—¥"
            },
            "hash": "13ac412646c508f5",
            "authors": [
                "Rui Xie",
                "Yinhong Liu",
                "Penghao Zhou",
                "Chen Zhao",
                "Jun Zhou",
                "Kai Zhang",
                "Zhenyu Zhang",
                "Jian Yang",
                "Zhenheng Yang",
                "Ying Tai"
            ],
            "affiliations": [
                "ByteDance",
                "Nanjing University",
                "Southwest University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.02976.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#optimization",
                    "#diffusion",
                    "#multimodal",
                    "#video"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "ĞšĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ ÑÑƒĞ¿ĞµÑ€Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ T2V Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¸ĞºĞ° STAR Ğ´Ğ»Ñ ÑÑƒĞ¿ĞµÑ€Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ text-to-video. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ LIEM Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹ Ğ¸ ÑƒÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ğ¾Ğ² Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸Ğ¸. Ğ’Ğ²ĞµĞ´ĞµĞ½Ğ° Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Dynamic Frequency Ğ´Ğ»Ñ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ°Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ STAR Ğ½Ğ°Ğ´ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ…."
                },
                "en": {
                    "title": "Enhancing Video Quality with T2V Models for Real-World Super-Resolution",
                    "desc": "This paper presents a new method called Spatial-Temporal Augmentation with T2V models for Real-world video super-resolution, which aims to improve video quality by addressing issues of over-smoothing and temporal consistency. Traditional image diffusion models struggle with video because they are designed for static images, leading to challenges in capturing motion dynamics. The proposed approach incorporates a Local Information Enhancement Module to enhance local details and reduce artifacts, along with a Dynamic Frequency Loss to maintain fidelity across different frequency components. Experimental results show that this method outperforms existing techniques in both synthetic and real-world scenarios, providing better spatial and temporal quality in restored videos."
                },
                "zh": {
                    "title": "æå‡è§†é¢‘è¶…åˆ†è¾¨ç‡çš„æ—¶ç©ºä¸€è‡´æ€§",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œåä¸º~\\name~ï¼Œç”¨äºæé«˜çœŸå®ä¸–ç•Œè§†é¢‘è¶…åˆ†è¾¨ç‡çš„æ—¶ç©ºè´¨é‡ã€‚è¯¥æ–¹æ³•ç»“åˆäº†æ–‡æœ¬åˆ°è§†é¢‘ï¼ˆT2Vï¼‰æ¨¡å‹ï¼Œä»¥è§£å†³ä¼ ç»Ÿç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰æ–¹æ³•ä¸­çš„è¿‡å¹³æ»‘é—®é¢˜ã€‚é€šè¿‡å¼•å…¥å±€éƒ¨ä¿¡æ¯å¢å¼ºæ¨¡å—ï¼ˆLIEMï¼‰å’ŒåŠ¨æ€é¢‘ç‡æŸå¤±ï¼ˆDF Lossï¼‰ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆæ”¹å–„è§†é¢‘çš„å±€éƒ¨ç»†èŠ‚å’Œæ—¶é—´ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œ~\\name~åœ¨åˆæˆå’ŒçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šå‡ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.02497",
            "title": "Test-time Computing: from System-1 Thinking to System-2 Thinking",
            "url": "https://huggingface.co/papers/2501.02497",
            "abstract": "The remarkable performance of the o1 model in complex reasoning demonstrates that test-time computing scaling can further unlock the model's potential, enabling powerful System-2 thinking. However, there is still a lack of comprehensive surveys for test-time computing scaling. We trace the concept of test-time computing back to System-1 models. In System-1 models, test-time computing addresses distribution shifts and improves robustness and generalization through parameter updating, input modification, representation editing, and output calibration. In System-2 models, it enhances the model's reasoning ability to solve complex problems through repeated sampling, self-correction, and tree search. We organize this survey according to the trend of System-1 to System-2 thinking, highlighting the key role of test-time computing in the transition from System-1 models to weak System-2 models, and then to strong System-2 models. We also point out a few possible future directions.",
            "score": 4,
            "issue_id": 1528,
            "pub_date": "2025-01-05",
            "pub_date_card": {
                "ru": "5 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 5",
                "zh": "1æœˆ5æ—¥"
            },
            "hash": "7d9414c60fe7701d",
            "authors": [
                "Yixin Ji",
                "Juntao Li",
                "Hai Ye",
                "Kaixin Wu",
                "Jia Xu",
                "Linjian Mo",
                "Min Zhang"
            ],
            "affiliations": [
                "Ant Group",
                "Department of Computer Science, National University of Singapore",
                "School of Computer Science and Technology, Soochow University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.02497.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#math",
                    "#survey",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹: Ğ¿ÑƒÑ‚ÑŒ Ğº Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ System-2",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°ÑÑ‚ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¸ Ğ¾Ñ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ System-1 Ğ´Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ System-2. Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ÑÑ‚ÑÑ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ´Ñ€ĞµĞ²Ğ¾Ğ²Ğ¸Ğ´Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ»ÑÑ‡ĞµĞ²ÑƒÑ Ñ€Ğ¾Ğ»ÑŒ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğµ Ğ¾Ñ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ System-1 Ğº ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ System-2."
                },
                "en": {
                    "title": "Unlocking Model Potential: The Power of Test-Time Computing",
                    "desc": "This paper explores the concept of test-time computing scaling and its impact on machine learning models, particularly in enhancing reasoning capabilities. It distinguishes between System-1 models, which focus on improving robustness and generalization through techniques like parameter updating and output calibration, and System-2 models, which utilize methods such as repeated sampling and self-correction for complex problem-solving. The authors trace the evolution from System-1 to System-2 thinking, emphasizing how test-time computing plays a crucial role in this transition. Additionally, the paper identifies potential future research directions in this area."
                },
                "zh": {
                    "title": "æµ‹è¯•æ—¶è®¡ç®—ï¼šä»ç³»ç»Ÿ-1åˆ°å¼ºç³»ç»Ÿ-2çš„å…³é”®è½¬å˜",
                    "desc": "è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†æµ‹è¯•æ—¶è®¡ç®—æ‰©å±•å¯¹æœºå™¨å­¦ä¹ æ¨¡å‹çš„å½±å“ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚æ¨ç†ä¸­çš„åº”ç”¨ã€‚ä½œè€…æŒ‡å‡ºï¼Œæµ‹è¯•æ—¶è®¡ç®—å¯ä»¥é€šè¿‡å‚æ•°æ›´æ–°ã€è¾“å…¥ä¿®æ”¹ã€è¡¨ç¤ºç¼–è¾‘å’Œè¾“å‡ºæ ¡å‡†æ¥æé«˜æ¨¡å‹çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚å¯¹äºç³»ç»Ÿ-2æ¨¡å‹ï¼Œæµ‹è¯•æ—¶è®¡ç®—é€šè¿‡é‡å¤é‡‡æ ·ã€è‡ªæˆ‘ä¿®æ­£å’Œæ ‘æœç´¢æ¥å¢å¼ºæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚è®ºæ–‡è¿˜å¼ºè°ƒäº†æµ‹è¯•æ—¶è®¡ç®—åœ¨ä»ç³»ç»Ÿ-1æ¨¡å‹å‘å¼±ç³»ç»Ÿ-2æ¨¡å‹å†åˆ°å¼ºç³»ç»Ÿ-2æ¨¡å‹è½¬å˜ä¸­çš„å…³é”®ä½œç”¨ï¼Œå¹¶æå‡ºäº†ä¸€äº›æœªæ¥çš„ç ”ç©¶æ–¹å‘ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.02157",
            "title": "Personalized Graph-Based Retrieval for Large Language Models",
            "url": "https://huggingface.co/papers/2501.02157",
            "abstract": "As large language models (LLMs) evolve, their ability to deliver personalized and context-aware responses offers transformative potential for improving user experiences. Existing personalization approaches, however, often rely solely on user history to augment the prompt, limiting their effectiveness in generating tailored outputs, especially in cold-start scenarios with sparse data. To address these limitations, we propose Personalized Graph-based Retrieval-Augmented Generation (PGraphRAG), a framework that leverages user-centric knowledge graphs to enrich personalization. By directly integrating structured user knowledge into the retrieval process and augmenting prompts with user-relevant context, PGraphRAG enhances contextual understanding and output quality. We also introduce the Personalized Graph-based Benchmark for Text Generation, designed to evaluate personalized text generation tasks in real-world settings where user history is sparse or unavailable. Experimental results show that PGraphRAG significantly outperforms state-of-the-art personalization methods across diverse tasks, demonstrating the unique advantages of graph-based retrieval for personalization.",
            "score": 3,
            "issue_id": 1527,
            "pub_date": "2025-01-04",
            "pub_date_card": {
                "ru": "4 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 4",
                "zh": "1æœˆ4æ—¥"
            },
            "hash": "65e3736cfc1e3295",
            "authors": [
                "Steven Au",
                "Cameron J. Dimacali",
                "Ojasmitha Pedirappagari",
                "Namyong Park",
                "Franck Dernoncourt",
                "Yu Wang",
                "Nikos Kanakaris",
                "Hanieh Deilamsalehy",
                "Ryan A. Rossi",
                "Nesreen K. Ahmed"
            ],
            "affiliations": [
                "Adobe Research",
                "Cisco AI Research",
                "Meta AI",
                "University of California Santa Cruz",
                "University of Oregon",
                "University of Southern California"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.02157.jpg",
            "data": {
                "categories": [
                    "#rag",
                    "#optimization",
                    "#graphs",
                    "#multimodal",
                    "#benchmark",
                    "#games"
                ],
                "emoji": "ğŸ•¸ï¸",
                "ru": {
                    "title": "Ğ“Ñ€Ğ°Ñ„Ñ‹ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ½Ğ° ÑĞ»ÑƒĞ¶Ğ±Ğµ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ PGraphRAG. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ÑÑ‰Ğ¸Ñ…ÑÑ Ğ½Ğ° Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ, PGraphRAG Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ Ğ³Ñ€Ğ°Ñ„Ñ‹ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¾Ğ±Ğ¾Ğ³Ğ°Ñ‰ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ², Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ PGraphRAG Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…."
                },
                "en": {
                    "title": "Revolutionizing Personalization with Graph-based Retrieval",
                    "desc": "This paper introduces a new framework called Personalized Graph-based Retrieval-Augmented Generation (PGraphRAG) that enhances the personalization of large language models (LLMs). Unlike traditional methods that depend only on user history, PGraphRAG utilizes user-centric knowledge graphs to provide richer context for generating responses. By integrating structured user information into the retrieval process, it improves the model's understanding and the quality of its outputs, especially in situations where user data is limited. The authors also present a benchmark for evaluating personalized text generation, showing that PGraphRAG outperforms existing methods in various tasks."
                },
                "zh": {
                    "title": "ä¸ªæ€§åŒ–å›¾è°±æå‡ç”Ÿæˆè´¨é‡",
                    "desc": "éšç€å¤§å‹è¯­è¨€æ¨¡å‹çš„å‘å±•ï¼Œå®ƒä»¬åœ¨æä¾›ä¸ªæ€§åŒ–å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„å“åº”æ–¹é¢å±•ç°å‡ºå·¨å¤§çš„æ½œåŠ›ã€‚ç°æœ‰çš„ä¸ªæ€§åŒ–æ–¹æ³•é€šå¸¸ä»…ä¾èµ–ç”¨æˆ·å†å²æ•°æ®æ¥å¢å¼ºæç¤ºï¼Œè¿™åœ¨æ•°æ®ç¨€ç–çš„å†·å¯åŠ¨åœºæ™¯ä¸­æ•ˆæœæœ‰é™ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸ªæ€§åŒ–å›¾è°±æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆPGraphRAGï¼‰æ¡†æ¶ï¼Œåˆ©ç”¨ä»¥ç”¨æˆ·ä¸ºä¸­å¿ƒçš„çŸ¥è¯†å›¾è°±æ¥ä¸°å¯Œä¸ªæ€§åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPGraphRAGåœ¨å¤šç§ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰çš„ä¸ªæ€§åŒ–æ–¹æ³•ï¼Œå±•ç¤ºäº†åŸºäºå›¾è°±çš„æ£€ç´¢åœ¨ä¸ªæ€§åŒ–ä¸­çš„ç‹¬ç‰¹ä¼˜åŠ¿ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.02832",
            "title": "Samba-asr state-of-the-art speech recognition leveraging structured state-space models",
            "url": "https://huggingface.co/papers/2501.02832",
            "abstract": "We propose Samba ASR, the first state-of-the-art Automatic Speech Recognition (ASR) model leveraging the novel Mamba architecture as both encoder and decoder, built on the foundation of state-space models (SSMs). Unlike transformer-based ASR models, which rely on self-attention mechanisms to capture dependencies, Samba ASR effectively models both local and global temporal dependencies using efficient state-space dynamics, achieving remarkable performance gains. By addressing the limitations of transformers, such as quadratic scaling with input length and difficulty in handling long-range dependencies, Samba ASR achieves superior accuracy and efficiency.   Experimental results demonstrate that Samba ASR surpasses existing open-source transformer-based ASR models across various standard benchmarks, establishing it as the new state of the art in ASR. Extensive evaluations on benchmark datasets show significant improvements in Word Error Rate (WER), with competitive performance even in low-resource scenarios. Furthermore, the computational efficiency and parameter optimization of the Mamba architecture make Samba ASR a scalable and robust solution for diverse ASR tasks.   Our contributions include:   A new Samba ASR architecture demonstrating the superiority of SSMs over transformer-based models for speech sequence processing. A comprehensive evaluation on public benchmarks showcasing state-of-the-art performance. An analysis of computational efficiency, robustness to noise, and sequence generalization. This work highlights the viability of Mamba SSMs as a transformer-free alternative for efficient and accurate ASR. By leveraging state-space modeling advancements, Samba ASR sets a new benchmark for ASR performance and future research.",
            "score": 2,
            "issue_id": 1530,
            "pub_date": "2025-01-06",
            "pub_date_card": {
                "ru": "6 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 6",
                "zh": "1æœˆ6æ—¥"
            },
            "hash": "ed3c4a6192d0c5f9",
            "authors": [
                "Syed Abdul Gaffar Shakhadri",
                "Kruthika KR",
                "Kartik Basavaraj Angadi"
            ],
            "affiliations": [
                "SandLogic Technologies Pvt Ltd"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.02832.jpg",
            "data": {
                "categories": [
                    "#audio",
                    "#architecture",
                    "#benchmark",
                    "#low_resource",
                    "#open_source"
                ],
                "emoji": "ğŸ™ï¸",
                "ru": {
                    "title": "Samba ASR: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ñ€ĞµÑ‡Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Samba ASR - Ğ¿ĞµÑ€Ğ²Ğ°Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑ‡Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Mamba Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ° Ğ¸ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ (SSM). Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Samba ASR ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Samba ASR Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¿Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑĞ¼. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»Ñ Word Error Rate (WER) Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ°Ğ¶Ğµ Ğ¿Ñ€Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ°Ñ…."
                },
                "en": {
                    "title": "Samba ASR: Redefining Speech Recognition with State-Space Models",
                    "desc": "Samba ASR is a groundbreaking Automatic Speech Recognition model that utilizes the innovative Mamba architecture, which functions as both the encoder and decoder. This model departs from traditional transformer-based approaches by employing state-space models (SSMs) to effectively capture both local and global temporal dependencies, leading to enhanced performance. By overcoming the challenges associated with transformers, such as their inefficiency with long input sequences, Samba ASR achieves superior accuracy and efficiency in recognizing speech. Extensive testing shows that Samba ASR not only outperforms existing transformer-based models but also excels in low-resource environments, making it a robust solution for various ASR applications."
                },
                "zh": {
                    "title": "Samba ASRï¼šè¶…è¶Šå˜æ¢å™¨çš„è¯­éŸ³è¯†åˆ«æ–°æ ‡æ†",
                    "desc": "æˆ‘ä»¬æå‡ºäº†Samba ASRï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªåˆ©ç”¨æ–°å‹Mambaæ¶æ„ä½œä¸ºç¼–ç å™¨å’Œè§£ç å™¨çš„æœ€å…ˆè¿›è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹ã€‚ä¸åŸºäºå˜æ¢å™¨çš„ASRæ¨¡å‹ä¸åŒï¼ŒSamba ASRé€šè¿‡é«˜æ•ˆçš„çŠ¶æ€ç©ºé—´åŠ¨æ€å»ºæ¨¡å±€éƒ¨å’Œå…¨å±€æ—¶é—´ä¾èµ–å…³ç³»ï¼Œä»è€Œå®ç°æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚è¯¥æ¨¡å‹å…‹æœäº†å˜æ¢å™¨åœ¨å¤„ç†é•¿è·ç¦»ä¾èµ–å’Œè¾“å…¥é•¿åº¦çš„å¹³æ–¹æ‰©å±•ç­‰æ–¹é¢çš„å±€é™æ€§ï¼Œå±•ç°å‡ºæ›´é«˜çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSamba ASRåœ¨å¤šä¸ªæ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†ç°æœ‰çš„å¼€æºå˜æ¢å™¨ASRæ¨¡å‹ï¼Œç¡®ç«‹äº†å…¶åœ¨ASRé¢†åŸŸçš„æ–°æ ‡æ†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.02690",
            "title": "GS-DiT: Advancing Video Generation with Pseudo 4D Gaussian Fields through Efficient Dense 3D Point Tracking",
            "url": "https://huggingface.co/papers/2501.02690",
            "abstract": "4D video control is essential in video generation as it enables the use of sophisticated lens techniques, such as multi-camera shooting and dolly zoom, which are currently unsupported by existing methods. Training a video Diffusion Transformer (DiT) directly to control 4D content requires expensive multi-view videos. Inspired by Monocular Dynamic novel View Synthesis (MDVS) that optimizes a 4D representation and renders videos according to different 4D elements, such as camera pose and object motion editing, we bring pseudo 4D Gaussian fields to video generation. Specifically, we propose a novel framework that constructs a pseudo 4D Gaussian field with dense 3D point tracking and renders the Gaussian field for all video frames. Then we finetune a pretrained DiT to generate videos following the guidance of the rendered video, dubbed as GS-DiT. To boost the training of the GS-DiT, we also propose an efficient Dense 3D Point Tracking (D3D-PT) method for the pseudo 4D Gaussian field construction. Our D3D-PT outperforms SpatialTracker, the state-of-the-art sparse 3D point tracking method, in accuracy and accelerates the inference speed by two orders of magnitude. During the inference stage, GS-DiT can generate videos with the same dynamic content while adhering to different camera parameters, addressing a significant limitation of current video generation models. GS-DiT demonstrates strong generalization capabilities and extends the 4D controllability of Gaussian splatting to video generation beyond just camera poses. It supports advanced cinematic effects through the manipulation of the Gaussian field and camera intrinsics, making it a powerful tool for creative video production. Demos are available at https://wkbian.github.io/Projects/GS-DiT/.",
            "score": 1,
            "issue_id": 1530,
            "pub_date": "2025-01-05",
            "pub_date_card": {
                "ru": "5 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 5",
                "zh": "1æœˆ5æ—¥"
            },
            "hash": "b4c147a2637166a8",
            "authors": [
                "Weikang Bian",
                "Zhaoyang Huang",
                "Xiaoyu Shi",
                "Yijin Li",
                "Fu-Yun Wang",
                "Hongsheng Li"
            ],
            "affiliations": [
                "Avolution AI",
                "Centre for Perceptual and Interactive Intelligence",
                "Multimedia Laboratory, The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.02690.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#games",
                    "#diffusion",
                    "#3d"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾: 4D-ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ñ‹Ñ… Ğ¿Ğ¾Ğ»ĞµĞ¹",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ 4D-ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¿ÑĞµĞ²Ğ´Ğ¾-4D Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ñ‹ Ğ¿Ğ¾Ğ»Ñ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Diffusion Transformer (DiT). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Dense 3D Point Tracking (D3D-PT) Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ñ Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ñ‹Ñ… Ğ¿Ğ¾Ğ»ĞµĞ¹, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‰Ğ¸Ğ¹ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° GS-DiT Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¾Ğ´Ğ¸Ğ½Ğ°ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸ĞµĞ¼, Ğ½Ğ¾ Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼Ğ¸ ĞºĞ°Ğ¼ĞµÑ€Ñ‹, Ñ‡Ñ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ĞºĞ¸Ğ½ĞµĞ¼Ğ°Ñ‚Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ„Ñ„ĞµĞºÑ‚Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰Ğ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ 4D-ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "Revolutionizing Video Generation with 4D Control",
                    "desc": "This paper introduces a new method for generating videos that can be controlled in four dimensions (4D), which includes both camera movement and object motion. The authors propose a framework called GS-DiT that utilizes pseudo 4D Gaussian fields to enhance video generation, allowing for advanced cinematic effects. They also present a Dense 3D Point Tracking (D3D-PT) technique that improves the accuracy and speed of tracking 3D points compared to existing methods. Overall, GS-DiT enables the creation of dynamic videos with flexible camera parameters, significantly advancing the capabilities of video generation models."
                },
                "zh": {
                    "title": "ä¼ª4Dé«˜æ–¯åœºï¼šè§†é¢‘ç”Ÿæˆçš„æ–°çªç ´",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œåˆ©ç”¨ä¼ª4Dé«˜æ–¯åœºè¿›è¡Œè§†é¢‘ç”Ÿæˆï¼Œä»¥æ”¯æŒå¤æ‚çš„é•œå¤´æŠ€æœ¯ã€‚æˆ‘ä»¬é€šè¿‡å¯†é›†çš„3Dç‚¹è·Ÿè¸ªæ„å»ºä¼ª4Dé«˜æ–¯åœºï¼Œå¹¶ä¸ºæ‰€æœ‰è§†é¢‘å¸§æ¸²æŸ“è¯¥é«˜æ–¯åœºã€‚ä¸ºäº†æå‡GS-DiTçš„è®­ç»ƒæ•ˆæœï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„å¯†é›†3Dç‚¹è·Ÿè¸ªæ–¹æ³•ï¼Œæ˜¾è‘—æé«˜äº†å‡†ç¡®æ€§å’Œæ¨ç†é€Ÿåº¦ã€‚GS-DiTèƒ½å¤Ÿåœ¨ä¸åŒçš„ç›¸æœºå‚æ•°ä¸‹ç”Ÿæˆå…·æœ‰ç›¸åŒåŠ¨æ€å†…å®¹çš„è§†é¢‘ï¼Œæ‰©å±•äº†è§†é¢‘ç”Ÿæˆçš„4Då¯æ§æ€§ï¼Œæˆä¸ºåˆ›æ„è§†é¢‘åˆ¶ä½œçš„å¼ºå¤§å·¥å…·ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.01830",
            "title": "Auto-RT: Automatic Jailbreak Strategy Exploration for Red-Teaming Large Language Models",
            "url": "https://huggingface.co/papers/2501.01830",
            "abstract": "Automated red-teaming has become a crucial approach for uncovering vulnerabilities in large language models (LLMs). However, most existing methods focus on isolated safety flaws, limiting their ability to adapt to dynamic defenses and uncover complex vulnerabilities efficiently. To address this challenge, we propose Auto-RT, a reinforcement learning framework that automatically explores and optimizes complex attack strategies to effectively uncover security vulnerabilities through malicious queries. Specifically, we introduce two key mechanisms to reduce exploration complexity and improve strategy optimization: 1) Early-terminated Exploration, which accelerate exploration by focusing on high-potential attack strategies; and 2) Progressive Reward Tracking algorithm with intermediate downgrade models, which dynamically refine the search trajectory toward successful vulnerability exploitation. Extensive experiments across diverse LLMs demonstrate that, by significantly improving exploration efficiency and automatically optimizing attack strategies, Auto-RT detects a boarder range of vulnerabilities, achieving a faster detection speed and 16.63\\% higher success rates compared to existing methods.",
            "score": 1,
            "issue_id": 1529,
            "pub_date": "2025-01-03",
            "pub_date_card": {
                "ru": "3 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 3",
                "zh": "1æœˆ3æ—¥"
            },
            "hash": "5b08b81c52ec8da8",
            "authors": [
                "Yanjiang Liu",
                "Shuhen Zhou",
                "Yaojie Lu",
                "Huijia Zhu",
                "Weiqiang Wang",
                "Hongyu Lin",
                "Ben He",
                "Xianpei Han",
                "Le Sun"
            ],
            "affiliations": [
                "Ant Group",
                "Chinese Information Processing Laboratory, Institute of Software, Chinese Academy of Sciences, Beijing, China",
                "University of Chinese Academy of Sciences, Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.01830.jpg",
            "data": {
                "categories": [
                    "#security",
                    "#rl",
                    "#rlhf"
                ],
                "emoji": "ğŸ›¡ï¸",
                "ru": {
                    "title": "Auto-RT: Ğ£Ğ¼Ğ½Ğ°Ñ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Auto-RT - Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM). Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ Ñ€Ğ°Ğ½Ğ½ĞµĞ³Ğ¾ Ğ¿Ñ€ĞµĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹ Ğ°Ñ‚Ğ°Ğº. Auto-RT Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¸Ğ¹ ÑĞ¿ĞµĞºÑ‚Ñ€ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹ Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞµĞ¹ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ğ½Ğ° 16.63% Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğ¼ ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ¼ ÑƒÑĞ¿ĞµÑ…Ğ°. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ²Ñ‹ÑĞ²Ğ»ÑÑ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² LLM Ñ‡ĞµÑ€ĞµĞ· Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ñ‹Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹."
                },
                "en": {
                    "title": "Auto-RT: Revolutionizing Vulnerability Detection in LLMs",
                    "desc": "This paper presents Auto-RT, a reinforcement learning framework designed to enhance automated red-teaming for large language models (LLMs). Unlike traditional methods that target isolated safety flaws, Auto-RT efficiently uncovers complex vulnerabilities by optimizing attack strategies through malicious queries. It introduces two innovative mechanisms: Early-terminated Exploration to prioritize promising attack strategies, and Progressive Reward Tracking to refine the search process dynamically. Experimental results show that Auto-RT significantly improves exploration efficiency and detection success rates, outperforming existing approaches."
                },
                "zh": {
                    "title": "è‡ªåŠ¨åŒ–çº¢é˜Ÿï¼šé«˜æ•ˆå‘ç°è¯­è¨€æ¨¡å‹æ¼æ´çš„åˆ©å™¨",
                    "desc": "è‡ªåŠ¨åŒ–çº¢é˜ŸæŠ€æœ¯åœ¨å‘ç°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­çš„æ¼æ´æ–¹é¢å˜å¾—è‡³å…³é‡è¦ã€‚ç°æœ‰æ–¹æ³•å¤§å¤šé›†ä¸­äºå­¤ç«‹çš„å®‰å…¨ç¼ºé™·ï¼Œé™åˆ¶äº†å…¶é€‚åº”åŠ¨æ€é˜²å¾¡å’Œé«˜æ•ˆå‘ç°å¤æ‚æ¼æ´çš„èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Auto-RTï¼Œä¸€ä¸ªå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œèƒ½å¤Ÿè‡ªåŠ¨æ¢ç´¢å’Œä¼˜åŒ–å¤æ‚çš„æ”»å‡»ç­–ç•¥ï¼Œé€šè¿‡æ¶æ„æŸ¥è¯¢æœ‰æ•ˆå‘ç°å®‰å…¨æ¼æ´ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒAuto-RTæ˜¾è‘—æé«˜äº†æ¢ç´¢æ•ˆç‡å’Œæ”»å‡»ç­–ç•¥çš„è‡ªåŠ¨ä¼˜åŒ–ï¼Œæ£€æµ‹åˆ°æ›´å¹¿æ³›çš„æ¼æ´ï¼Œæ£€æµ‹é€Ÿåº¦æ›´å¿«ï¼ŒæˆåŠŸç‡æé«˜äº†16.63%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.01790",
            "title": "Ingredients: Blending Custom Photos with Video Diffusion Transformers",
            "url": "https://huggingface.co/papers/2501.01790",
            "abstract": "This paper presents a powerful framework to customize video creations by incorporating multiple specific identity (ID) photos, with video diffusion Transformers, referred to as Ingredients. Generally, our method consists of three primary modules: (i) a facial extractor that captures versatile and precise facial features for each human ID from both global and local perspectives; (ii) a multi-scale projector that maps face embeddings into the contextual space of image query in video diffusion transformers; (iii) an ID router that dynamically combines and allocates multiple ID embedding to the corresponding space-time regions. Leveraging a meticulously curated text-video dataset and a multi-stage training protocol, Ingredients demonstrates superior performance in turning custom photos into dynamic and personalized video content. Qualitative evaluations highlight the advantages of proposed method, positioning it as a significant advancement toward more effective generative video control tools in Transformer-based architecture, compared to existing methods. The data, code, and model weights are publicly available at: https://github.com/feizc/Ingredients.",
            "score": 1,
            "issue_id": 1528,
            "pub_date": "2025-01-03",
            "pub_date_card": {
                "ru": "3 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 3",
                "zh": "1æœˆ3æ—¥"
            },
            "hash": "dd1ccebdd2fcf276",
            "authors": [
                "Zhengcong Fei",
                "Debang Li",
                "Di Qiu",
                "Changqian Yu",
                "Mingyuan Fan"
            ],
            "affiliations": [
                "Kunlun Inc. Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.01790.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#training",
                    "#architecture",
                    "#video",
                    "#dataset",
                    "#diffusion",
                    "#multimodal"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "ĞŸĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· Ñ„Ğ¾Ñ‚Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ğ¹: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ingredients Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ñ„Ğ¾Ñ‚Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ğ¹ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ»ÑĞ´ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ñ‚Ñ€ĞµÑ… Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ÑƒĞ»ĞµĞ¹: ÑĞºÑÑ‚Ñ€Ğ°ĞºÑ‚Ğ¾Ñ€Ğ° Ğ»Ğ¸Ñ†ĞµĞ²Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ², Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¾Ñ€Ğ° Ğ¸ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ° Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ¾Ğ². Ingredients Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚-Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ» Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ². ĞšĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Transformer."
                },
                "en": {
                    "title": "Transforming Photos into Personalized Videos with Ingredients",
                    "desc": "This paper introduces a novel framework called Ingredients for creating personalized videos using multiple identity photos. It employs a facial extractor to accurately capture facial features, a multi-scale projector to integrate these features into video diffusion transformers, and an ID router to manage the allocation of identity embeddings across different time and space regions in the video. The framework is trained on a carefully selected text-video dataset, enhancing its ability to generate dynamic video content from custom images. The results show that Ingredients outperforms existing methods, marking a significant step forward in generative video control using Transformer architectures."
                },
                "zh": {
                    "title": "ä¸ªæ€§åŒ–è§†é¢‘åˆ›ä½œçš„æ–°çªç ´",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§å¼ºå¤§çš„æ¡†æ¶ï¼Œé€šè¿‡ç»“åˆå¤šä¸ªç‰¹å®šèº«ä»½ç…§ç‰‡ï¼Œå®šåˆ¶è§†é¢‘åˆ›ä½œï¼Œç§°ä¸ºIngredientsã€‚è¯¥æ–¹æ³•ä¸»è¦ç”±ä¸‰ä¸ªæ¨¡å—ç»„æˆï¼šé¢éƒ¨æå–å™¨ã€å¤šä¸ªå°ºåº¦æŠ•å½±å™¨å’Œèº«ä»½è·¯ç”±å™¨ï¼Œåˆ†åˆ«ç”¨äºæå–é¢éƒ¨ç‰¹å¾ã€æ˜ å°„é¢éƒ¨åµŒå…¥å’ŒåŠ¨æ€åˆ†é…èº«ä»½åµŒå…¥ã€‚é€šè¿‡ç²¾å¿ƒç­–åˆ’çš„æ–‡æœ¬-è§†é¢‘æ•°æ®é›†å’Œå¤šé˜¶æ®µè®­ç»ƒåè®®ï¼ŒIngredientsåœ¨å°†è‡ªå®šä¹‰ç…§ç‰‡è½¬åŒ–ä¸ºåŠ¨æ€ä¸ªæ€§åŒ–è§†é¢‘å†…å®¹æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚å®šæ€§è¯„ä¼°æ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨åŸºäºTransformerçš„æ¶æ„ä¸­ï¼Œç›¸è¾ƒäºç°æœ‰æ–¹æ³•ï¼Œæ˜¾è‘—æå‡äº†ç”Ÿæˆè§†é¢‘æ§åˆ¶å·¥å…·çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.02506",
            "title": "ToolHop: A Query-Driven Benchmark for Evaluating Large Language Models in Multi-Hop Tool Use",
            "url": "https://huggingface.co/papers/2501.02506",
            "abstract": "Effective evaluation of multi-hop tool use is critical for analyzing the understanding, reasoning, and function-calling capabilities of large language models (LLMs). However, progress has been hindered by a lack of reliable evaluation datasets. To address this, we present ToolHop, a dataset comprising 995 user queries and 3,912 associated tools, specifically designed for rigorous evaluation of multi-hop tool use. ToolHop ensures diverse queries, meaningful interdependencies, locally executable tools, detailed feedback, and verifiable answers through a novel query-driven data construction approach that includes tool creation, document refinement, and code generation. We evaluate 14 LLMs across five model families (i.e., LLaMA3.1, Qwen2.5, Gemini1.5, Claude3.5, and GPT), uncovering significant challenges in handling multi-hop tool-use scenarios. The leading model, GPT-4o, achieves an accuracy of 49.04%, underscoring substantial room for improvement. Further analysis reveals variations in tool-use strategies for various families, offering actionable insights to guide the development of more effective approaches. Code and data can be found in https://huggingface.co/bytedance-research/ToolHop.",
            "score": 0,
            "issue_id": 1529,
            "pub_date": "2025-01-05",
            "pub_date_card": {
                "ru": "5 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 5",
                "zh": "1æœˆ5æ—¥"
            },
            "hash": "f785173226e5f9fc",
            "authors": [
                "Junjie Ye",
                "Zhengyin Du",
                "Xuesong Yao",
                "Weijian Lin",
                "Yufei Xu",
                "Zehui Chen",
                "Zaiyuan Wang",
                "Sining Zhu",
                "Zhiheng Xi",
                "Siyu Yuan",
                "Tao Gui",
                "Qi Zhang",
                "Xuanjing Huang",
                "Jiechao Chen"
            ],
            "affiliations": [
                "ByteDance",
                "Institute of Modern Languages and Linguistics, Fudan University",
                "School of Computer Science, Fudan University",
                "School of Data Science, Fudan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.02506.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#reasoning",
                    "#dataset",
                    "#optimization"
                ],
                "emoji": "ğŸ› ï¸",
                "ru": {
                    "title": "ToolHop: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² LLM",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ToolHop Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ (LLM). ToolHop ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 995 Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸ 3912 ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ², Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ»Ğ¸ 14 LLM Ğ¸Ğ· Ğ¿ÑÑ‚Ğ¸ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ²Ñ‹ÑĞ²Ğ¸Ğ² Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ›ÑƒÑ‡ÑˆĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, GPT-4o, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ° Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ 49.04%, Ñ‡Ñ‚Ğ¾ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "ToolHop: Advancing Multi-Hop Tool Use Evaluation for LLMs",
                    "desc": "This paper introduces ToolHop, a new dataset designed to evaluate how well large language models (LLMs) can use multiple tools in a single task. It includes 995 user queries and 3,912 tools, focusing on diverse and interdependent queries that can be executed locally. The authors tested 14 different LLMs, revealing that even the best-performing model, GPT-4o, only achieved 49.04% accuracy, indicating significant challenges in multi-hop tool use. The findings highlight different strategies employed by various model families, providing insights for future improvements in LLM capabilities."
                },
                "zh": {
                    "title": "ToolHopï¼šå¤šè·³å·¥å…·ä½¿ç”¨çš„æœ‰æ•ˆè¯„ä¼°æ•°æ®é›†",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ToolHopæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«995ä¸ªç”¨æˆ·æŸ¥è¯¢å’Œ3912ä¸ªç›¸å…³å·¥å…·ï¼Œæ—¨åœ¨æœ‰æ•ˆè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šè·³å·¥å…·ä½¿ç”¨ä¸­çš„ç†è§£ã€æ¨ç†å’ŒåŠŸèƒ½è°ƒç”¨èƒ½åŠ›ã€‚é€šè¿‡æ–°é¢–çš„æŸ¥è¯¢é©±åŠ¨æ•°æ®æ„å»ºæ–¹æ³•ï¼ŒToolHopç¡®ä¿äº†æŸ¥è¯¢çš„å¤šæ ·æ€§ã€å·¥å…·çš„å±€éƒ¨å¯æ‰§è¡Œæ€§å’Œå¯éªŒè¯çš„ç­”æ¡ˆã€‚æˆ‘ä»¬å¯¹14ä¸ªä¸åŒæ¨¡å‹ï¼ˆå¦‚LLaMA3.1ã€Qwen2.5ç­‰ï¼‰è¿›è¡Œäº†è¯„ä¼°ï¼Œå‘ç°å®ƒä»¬åœ¨å¤„ç†å¤šè·³å·¥å…·ä½¿ç”¨åœºæ™¯æ—¶é¢ä¸´æ˜¾è‘—æŒ‘æˆ˜ã€‚å°½ç®¡GPT-4oæ¨¡å‹çš„å‡†ç¡®ç‡ä¸º49.04%ï¼Œä½†ä»æœ‰å¾ˆå¤§çš„æ”¹è¿›ç©ºé—´ï¼Œåˆ†æè¿˜æ­ç¤ºäº†ä¸åŒæ¨¡å‹å®¶æ—åœ¨å·¥å…·ä½¿ç”¨ç­–ç•¥ä¸Šçš„å·®å¼‚ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-01-06.html",
    "link_next": "2025-01-08.html",
    "link_month": "2025-01.html",
    "short_date_prev": {
        "ru": "06.01",
        "en": "01/06",
        "zh": "1æœˆ6æ—¥"
    },
    "short_date_next": {
        "ru": "08.01",
        "en": "01/08",
        "zh": "1æœˆ8æ—¥"
    },
    "categories": {
        "#dataset": 3,
        "#data": 1,
        "#benchmark": 4,
        "#agents": 0,
        "#cv": 1,
        "#rl": 1,
        "#rlhf": 1,
        "#rag": 1,
        "#plp": 0,
        "#inference": 0,
        "#3d": 1,
        "#audio": 1,
        "#video": 4,
        "#multimodal": 3,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 4,
        "#healthcare": 1,
        "#training": 4,
        "#robotics": 0,
        "#agi": 0,
        "#games": 2,
        "#interpretability": 0,
        "#reasoning": 2,
        "#transfer_learning": 0,
        "#graphs": 1,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 4,
        "#survey": 1,
        "#diffusion": 4,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 1
    },
    "zh": {
        "text": "This article introduces EnerVerse, a framework for generating future space in robotic tasks. It uses attention mechanisms for consistent space modeling and a memory context for long sequence generation. The FAV space enhances robot observation and adaptability. A data engine with 4DGS improves data quality and diversity. Experiments show it boosts performance in long-range robotic tasks.",
        "title": "EnerVerse: Envisioning Embodied Future Space for Robotics Manipulation",
        "pinyin": "Sure, here is the pinyin transcription for the given text:\n\nZhÃ¨ wÃ©nzhÄng jiÃ¨shÃ o EnerVerse, yÄ«gÃ¨ kuÃ ngjiÃ  yÇnqÇ wÃ¨ilÃ¡i kÅngjiÄn zÃ i jÄ«qirÃ©n rÃ©nwÃ¹ zhÅng. TÄ shÇyÃ²ng zhÃ¹yÃ¬ jÄ«zhÃ¬ wÃ¨i yuÃ¡nchÃ¡ng kÅngjiÄn mÃ³xÃ­ng hÃ© yÄ«gÃ¨ jÃ¬yÃ¬ qÅ«jiÃ n wÃ¨i chÃ¡ng xÃ¹liÃ¨ shÄ“ngchÃ©ng. FAV kÅngjiÄn zÄ“ngqiÃ¡ng jÄ«qirÃ©n guÄnchÃ¡ hÃ© shÃ¬yÃ¬ngxÃ¬ng. YÄ«gÃ¨ shÃ¹jÃ¹ yÇnqÃ­ng yÇ’u 4DGS gÇishÃ n shÃ¹jÃ¹ zhÃ¬liÃ ng hÃ© duÅyÃ ngxÃ¬ng. ShÃ­yÃ n xiÇnshÃ¬ tÄ zÄ“ngqiÃ¡ng xiÃ oguÇ’ zÃ i chÃ¡ngqÄ« jÄ«qirÃ©n rÃ©nwÃ¹ zhÅng.\n\nPlease note that the pinyin transcription is based on the pronunciation of the Chinese characters that would be used to translate the English text. The actual translation might vary slightly depending on the context and specific terminology used.",
        "vocab": "[\n    {\"word\": \"framework\", \"pinyin\": \"kuÃ ngjiÃ \", \"trans\": \"æ¡†æ¶\"},\n    {\"word\": \"generating\", \"pinyin\": \"shÄ“ngchÃ©ng\", \"trans\": \"ç”Ÿæˆ\"},\n    {\"word\": \"future\", \"pinyin\": \"wÃ¨ilÃ¡i\", \"trans\": \"æœªæ¥\"},\n    {\"word\": \"space\", \"pinyin\": \"kÅngjiÄn\", \"trans\": \"ç©ºé—´\"},\n    {\"word\": \"robotic\", \"pinyin\": \"jÄ«qirÃ©n\", \"trans\": \"æœºå™¨äºº\"},\n    {\"word\": \"tasks\", \"pinyin\": \"rÃ¨nwÃ¹\", \"trans\": \"ä»»åŠ¡\"},\n    {\"word\": \"attention\", \"pinyin\": \"zhÃ¹yÃ¬\", \"trans\": \"æ³¨æ„\"},\n    {\"word\": \"mechanisms\", \"pinyin\": \"jÄ«zhÃ¬\", \"trans\": \"æœºåˆ¶\"},\n    {\"word\": \"consistent\", \"pinyin\": \"wÃºguÇ’\", \"trans\": \"ä¸€è‡´\"},\n    {\"word\": \"modeling\", \"pinyin\": \"mÃ³xÃ­ng\", \"trans\": \"å»ºæ¨¡\"},\n    {\"word\": \"memory\", \"pinyin\": \"jÃ¬yÃ¬\", \"trans\": \"è®°å¿†\"},\n    {\"word\": \"context\", \"pinyin\": \"qÇ”wÃ©n\", \"trans\": \"ä¸Šä¸‹æ–‡\"},\n    {\"word\": \"sequence\", \"pinyin\": \"xÃ¹liÃ¨\", \"trans\": \"åºåˆ—\"},\n    {\"word\": \"generation\", \"pinyin\": \"shÄ“ngchÃ©ng\", \"trans\": \"ç”Ÿæˆ\"},\n    {\"word\": \"FAV\", \"pinyin\": \"FÄ“i-Ä’i-WÄ“i\", \"trans\": \"FAV\"},\n    {\"word\": \"enhances\", \"pinyin\": \"zÄ“ngqiÃ¡ng\", \"trans\": \"å¢å¼º\"},\n    {\"word\": \"observation\", \"pinyin\": \"guÄnchÃ¡\", \"trans\": \"è§‚å¯Ÿ\"},\n    {\"word\": \"adaptability\", \"pinyin\": \"shÃ¬yÃ¬ngxÃ¬ng\", \"trans\": \"é€‚åº”æ€§\"},\n    {\"word\": \"engine\", \"pinyin\": \"yÇnqÃ­ng\", \"trans\": \"å¼•æ“\"},\n    {\"word\": \"4DGS\", \"pinyin\": \"SÃ¬-DÄ«-JÄ«-Ä’s\", \"trans\": \"4DGS\"},\n    {\"word\": \"improves\", \"pinyin\": \"gÇishÃ n\", \"trans\": \"æ”¹å–„\"},\n    {\"word\": \"quality\", \"pinyin\": \"zhÃ¬liÃ ng\", \"trans\": \"è´¨é‡\"},\n    {\"word\": \"diversity\", \"pinyin\": \"duÅyÃ ngxÃ¬ng\", \"trans\": \"å¤šæ ·æ€§\"},\n    {\"word\": \"experiments\", \"pinyin\": \"shÃ­yÃ n\", \"trans\": \"å®éªŒ\"},\n    {\"word\": \"show\", \"pinyin\": \"xiÇnshÃ¬\", \"trans\": \"æ˜¾ç¤º\"},\n    {\"word\": \"boosts\", \"pinyin\": \"zÄ“ngqiÃ¡ng\", \"trans\": \"å¢å¼º\"},\n    {\"word\": \"performance\", \"pinyin\": \"biÇoxiÃ n\", \"trans\": \"è¡¨ç°\"},\n    {\"word\": \"long-range\", \"pinyin\": \"chÃ¡ngyuÇn\", \"trans\": \"é•¿è¿œ\"}\n]",
        "trans": "This article introduces EnerVerse, a framework designed to generate future space in robotic tasks. It employs attention mechanisms to ensure consistent space modeling and utilizes a memory context for generating long sequences. The FAV space enhances robot observation capabilities and adaptability. Additionally, a data engine equipped with 4DGS improves the quality and diversity of data. Experiments demonstrate that it significantly boosts performance in long-range robotic tasks.",
        "update_ts": "2025-01-06 09:11"
    }
}