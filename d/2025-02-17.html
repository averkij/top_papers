
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 21 papers. February 17.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">17 февраля</span> | <span id="title-articles-count">21 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-02-14.html">⬅️ <span id="prev-date">14.02</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-02-18.html">➡️ <span id="next-date">18.02</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-02.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '17 февраля', 'en': 'February 17', 'zh': '2月17日'};
        let feedDateNext = {'ru': '18.02', 'en': '02/18', 'zh': '2月18日'};
        let feedDatePrev = {'ru': '14.02', 'en': '02/14', 'zh': '2月14日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2502.10389', 'title': 'Region-Adaptive Sampling for Diffusion Transformers', 'url': 'https://huggingface.co/papers/2502.10389', 'abstract': "Diffusion models (DMs) have become the leading choice for generative tasks across diverse domains. However, their reliance on multiple sequential forward passes significantly limits real-time performance. Previous acceleration methods have primarily focused on reducing the number of sampling steps or reusing intermediate results, failing to leverage variations across spatial regions within the image due to the constraints of convolutional U-Net structures. By harnessing the flexibility of Diffusion Transformers (DiTs) in handling variable number of tokens, we introduce RAS, a novel, training-free sampling strategy that dynamically assigns different sampling ratios to regions within an image based on the focus of the DiT model. Our key observation is that during each sampling step, the model concentrates on semantically meaningful regions, and these areas of focus exhibit strong continuity across consecutive steps. Leveraging this insight, RAS updates only the regions currently in focus, while other regions are updated using cached noise from the previous step. The model's focus is determined based on the output from the preceding step, capitalizing on the temporal consistency we observed. We evaluate RAS on Stable Diffusion 3 and Lumina-Next-T2I, achieving speedups up to 2.36x and 2.51x, respectively, with minimal degradation in generation quality. Additionally, a user study reveals that RAS delivers comparable qualities under human evaluation while achieving a 1.6x speedup. Our approach makes a significant step towards more efficient diffusion transformers, enhancing their potential for real-time applications.", 'score': 42, 'issue_id': 2241, 'pub_date': '2025-02-14', 'pub_date_card': {'ru': '14 февраля', 'en': 'February 14', 'zh': '2月14日'}, 'hash': '8068f45b7fd0c2ee', 'authors': ['Ziming Liu', 'Yifan Yang', 'Chengruidong Zhang', 'Yiqi Zhang', 'Lili Qiu', 'Yang You', 'Yuqing Yang'], 'affiliations': ['Microsoft Research', 'National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2502.10389.jpg', 'data': {'categories': ['#optimization', '#benchmark', '#training', '#diffusion', '#architecture', '#dataset'], 'emoji': '🚀', 'ru': {'title': 'Ускорение диффузионных трансформеров без потери качества', 'desc': 'Статья представляет новый метод ускорения диффузионных моделей под названием RAS. Этот метод основан на динамическом распределении различных коэффициентов сэмплирования для разных областей изображения, опираясь на фокус внимания модели Diffusion Transformer. RAS обновляет только те области, на которых в данный момент сфокусирована модель, используя для остальных областей кэшированный шум из предыдущего шага. Эксперименты показали, что RAS позволяет достичь ускорения до 2.51x на различных моделях при минимальном снижении качества генерации.'}, 'en': {'title': 'Accelerating Diffusion Transformers with RAS for Real-Time Generation', 'desc': "This paper introduces RAS, a new sampling strategy for Diffusion Transformers (DiTs) that improves the efficiency of generative tasks. Traditional diffusion models require multiple sequential passes, which slow down real-time performance, but RAS dynamically adjusts sampling ratios based on the model's focus on different image regions. By only updating areas of interest and reusing noise from previous steps, RAS significantly accelerates the sampling process while maintaining high quality. The results show that RAS can achieve speedups of over 2x with minimal loss in generation quality, making it a promising advancement for real-time applications in generative modeling."}, 'zh': {'title': '提升扩散模型的实时性能', 'desc': '扩散模型（DMs）在生成任务中已成为首选，但其依赖多个顺序前向传递限制了实时性能。我们提出了一种新的无训练采样策略RAS，利用扩散变换器（DiTs）的灵活性，根据模型的关注点动态分配图像区域的采样比例。RAS只更新当前关注的区域，而其他区域则使用上一步的缓存噪声，从而提高了效率。我们的实验表明，RAS在生成质量几乎不下降的情况下，能够实现显著的加速。'}}}, {'id': 'https://huggingface.co/papers/2502.10248', 'title': 'Step-Video-T2V Technical Report: The Practice, Challenges, and Future of Video Foundation Model', 'url': 'https://huggingface.co/papers/2502.10248', 'abstract': "We present Step-Video-T2V, a state-of-the-art text-to-video pre-trained model with 30B parameters and the ability to generate videos up to 204 frames in length. A deep compression Variational Autoencoder, Video-VAE, is designed for video generation tasks, achieving 16x16 spatial and 8x temporal compression ratios, while maintaining exceptional video reconstruction quality. User prompts are encoded using two bilingual text encoders to handle both English and Chinese. A DiT with 3D full attention is trained using Flow Matching and is employed to denoise input noise into latent frames. A video-based DPO approach, Video-DPO, is applied to reduce artifacts and improve the visual quality of the generated videos. We also detail our training strategies and share key observations and insights. Step-Video-T2V's performance is evaluated on a novel video generation benchmark, Step-Video-T2V-Eval, demonstrating its state-of-the-art text-to-video quality when compared with both open-source and commercial engines. Additionally, we discuss the limitations of current diffusion-based model paradigm and outline future directions for video foundation models. We make both Step-Video-T2V and Step-Video-T2V-Eval available at https://github.com/stepfun-ai/Step-Video-T2V. The online version can be accessed from https://yuewen.cn/videos as well. Our goal is to accelerate the innovation of video foundation models and empower video content creators.", 'score': 29, 'issue_id': 2241, 'pub_date': '2025-02-14', 'pub_date_card': {'ru': '14 февраля', 'en': 'February 14', 'zh': '2月14日'}, 'hash': '356bc046cc5f59e5', 'authors': ['Guoqing Ma', 'Haoyang Huang', 'Kun Yan', 'Liangyu Chen', 'Nan Duan', 'Shengming Yin', 'Changyi Wan', 'Ranchen Ming', 'Xiaoniu Song', 'Xing Chen', 'Yu Zhou', 'Deshan Sun', 'Deyu Zhou', 'Jian Zhou', 'Kaijun Tan', 'Kang An', 'Mei Chen', 'Wei Ji', 'Qiling Wu', 'Wen Sun', 'Xin Han', 'Yanan Wei', 'Zheng Ge', 'Aojie Li', 'Bin Wang', 'Bizhu Huang', 'Bo Wang', 'Brian Li', 'Changxing Miao', 'Chen Xu', 'Chenfei Wu', 'Chenguang Yu', 'Dapeng Shi', 'Dingyuan Hu', 'Enle Liu', 'Gang Yu', 'Ge Yang', 'Guanzhe Huang', 'Gulin Yan', 'Haiyang Feng', 'Hao Nie', 'Haonan Jia', 'Hanpeng Hu', 'Hanqi Chen', 'Haolong Yan', 'Heng Wang', 'Hongcheng Guo', 'Huilin Xiong', 'Huixin Xiong', 'Jiahao Gong', 'Jianchang Wu', 'Jiaoren Wu', 'Jie Wu', 'Jie Yang', 'Jiashuai Liu', 'Jiashuo Li', 'Jingyang Zhang', 'Junjing Guo', 'Junzhe Lin', 'Kaixiang Li', 'Lei Liu', 'Lei Xia', 'Liang Zhao', 'Liguo Tan', 'Liwen Huang', 'Liying Shi', 'Ming Li', 'Mingliang Li', 'Muhua Cheng', 'Na Wang', 'Qiaohui Chen', 'Qinglin He', 'Qiuyan Liang', 'Quan Sun', 'Ran Sun', 'Rui Wang', 'Shaoliang Pang', 'Shiliang Yang', 'Sitong Liu', 'Siqi Liu', 'Shuli Gao', 'Tiancheng Cao', 'Tianyu Wang', 'Weipeng Ming', 'Wenqing He', 'Xu Zhao', 'Xuelin Zhang', 'Xianfang Zeng', 'Xiaojia Liu', 'Xuan Yang', 'Yaqi Dai', 'Yanbo Yu', 'Yang Li', 'Yineng Deng', 'Yingming Wang', 'Yilei Wang', 'Yuanwei Lu', 'Yu Chen', 'Yu Luo', 'Yuchu Luo', 'Yuhe Yin', 'Yuheng Feng', 'Yuxiang Yang', 'Zecheng Tang', 'Zekai Zhang', 'Zidong Yang', 'Binxing Jiao', 'Jiansheng Chen', 'Jing Li', 'Shuchang Zhou', 'Xiangyu Zhang', 'Xinhao Zhang', 'Yibo Zhu', 'Heung-Yeung Shum', 'Daxin Jiang'], 'affiliations': ['StepFun'], 'pdf_title_img': 'assets/pdf/title_img/2502.10248.jpg', 'data': {'categories': ['#multilingual', '#benchmark', '#training', '#open_source', '#diffusion', '#video', '#architecture'], 'emoji': '🎬', 'ru': {'title': 'Революция в генерации видео: от текста к реалистичным роликам', 'desc': 'Представлена модель Step-Video-T2V - предобученная нейросеть для генерации видео по текстовому описанию с 30 миллиардами параметров. Модель использует глубокий вариационный автоэнкодер Video-VAE для сжатия видео и два двуязычных текстовых энкодера для обработки запросов на английском и китайском языках. Для улучшения качества генерируемых видео применяется подход Video-DPO на основе предпочтений. Модель демонстрирует передовое качество генерации видео по сравнению с открытыми и коммерческими аналогами.'}, 'en': {'title': 'Revolutionizing Video Generation with Step-Video-T2V', 'desc': 'Step-Video-T2V is a cutting-edge text-to-video model that utilizes 30 billion parameters to generate videos with a maximum length of 204 frames. It employs a deep compression Variational Autoencoder, achieving significant spatial and temporal compression while preserving high video quality. The model incorporates bilingual text encoders for processing prompts in both English and Chinese, and utilizes a DiT with 3D full attention for effective denoising of latent frames. Evaluated on a new benchmark, Step-Video-T2V demonstrates superior performance in video generation, addressing current limitations in diffusion-based models and paving the way for future advancements in video foundation models.'}, 'zh': {'title': '创新视频生成，赋能内容创作者', 'desc': '本文介绍了一种名为Step-Video-T2V的先进文本到视频预训练模型，具有300亿参数，能够生成最长204帧的视频。我们设计了一种深度压缩变分自编码器（Video-VAE），在视频生成任务中实现了16x16的空间压缩和8x的时间压缩，同时保持了卓越的视频重建质量。用户提示通过双语文本编码器进行编码，以处理英语和中文。我们还讨论了当前扩散模型范式的局限性，并概述了视频基础模型的未来方向。'}}}, {'id': 'https://huggingface.co/papers/2502.09992', 'title': 'Large Language Diffusion Models', 'url': 'https://huggingface.co/papers/2502.09992', 'abstract': 'Autoregressive models (ARMs) are widely regarded as the cornerstone of large language models (LLMs). We challenge this notion by introducing LLaDA, a diffusion model trained from scratch under the pre-training and supervised fine-tuning (SFT) paradigm. LLaDA models distributions through a forward data masking process and a reverse process, parameterized by a vanilla Transformer to predict masked tokens. By optimizing a likelihood bound, it provides a principled generative approach for probabilistic inference. Across extensive benchmarks, LLaDA demonstrates strong scalability, outperforming our self-constructed ARM baselines. Remarkably, LLaDA 8B is competitive with strong LLMs like LLaMA3 8B in in-context learning and, after SFT, exhibits impressive instruction-following abilities in case studies such as multi-turn dialogue. Moreover, LLaDA addresses the reversal curse, surpassing GPT-4o in a reversal poem completion task. Our findings establish diffusion models as a viable and promising alternative to ARMs, challenging the assumption that key LLM capabilities discussed above are inherently tied to ARMs.', 'score': 27, 'issue_id': 2242, 'pub_date': '2025-02-14', 'pub_date_card': {'ru': '14 февраля', 'en': 'February 14', 'zh': '2月14日'}, 'hash': '5117e8f17ba51f92', 'authors': ['Shen Nie', 'Fengqi Zhu', 'Zebin You', 'Xiaolu Zhang', 'Jingyang Ou', 'Jun Hu', 'Jun Zhou', 'Yankai Lin', 'Ji-Rong Wen', 'Chongxuan Li'], 'affiliations': ['Ant Group', 'Beijing Key Laboratory of Big Data Management and Analysis Methods', 'Gaoling School of Artificial Intelligence, Renmin University of China'], 'pdf_title_img': 'assets/pdf/title_img/2502.09992.jpg', 'data': {'categories': ['#architecture', '#optimization', '#benchmark', '#diffusion', '#training'], 'emoji': '🌊', 'ru': {'title': 'Диффузионные модели бросают вызов авторегрессии в обработке языка', 'desc': "Статья представляет LLaDA - новую диффузионную модель для обработки естественного языка, альтернативную авторегрессионным моделям. LLaDA использует процесс маскирования данных и обратный процесс для предсказания замаскированных токенов, оптимизируя границу правдоподобия. Модель демонстрирует сильную масштабируемость и превосходит базовые авторегрессионные модели в различных тестах. LLaDA показывает впечатляющие результаты в задачах обучения в контексте и следования инструкциям, а также решает проблему 'проклятия обращения'."}, 'en': {'title': 'LLaDA: A New Era for Language Models Beyond Autoregression', 'desc': 'This paper introduces LLaDA, a novel diffusion model that challenges the dominance of autoregressive models (ARMs) in large language models (LLMs). LLaDA employs a forward data masking process and a reverse process, utilizing a Transformer architecture to predict masked tokens effectively. By optimizing a likelihood bound, it offers a robust generative framework for probabilistic inference. The results show that LLaDA not only scales well but also competes with leading LLMs in tasks like in-context learning and instruction-following, suggesting that diffusion models can serve as a strong alternative to traditional ARMs.'}, 'zh': {'title': '扩散模型：自回归模型的新挑战', 'desc': '自回归模型（ARMs）被广泛认为是大型语言模型（LLMs）的基石。本文提出了LLaDA，这是一种从头开始训练的扩散模型，采用预训练和监督微调（SFT）的方法。LLaDA通过前向数据掩蔽过程和反向过程建模分布，并使用普通Transformer预测被掩蔽的标记。研究表明，LLaDA在多个基准测试中表现出强大的可扩展性，超越了自构建的ARMs基线，证明了扩散模型作为ARMs的可行替代方案。'}}}, {'id': 'https://huggingface.co/papers/2502.09696', 'title': 'ZeroBench: An Impossible Visual Benchmark for Contemporary Large Multimodal Models', 'url': 'https://huggingface.co/papers/2502.09696', 'abstract': 'Large Multimodal Models (LMMs) exhibit major shortfalls when interpreting images and, by some measures, have poorer spatial cognition than small children or animals. Despite this, they attain high scores on many popular visual benchmarks, with headroom rapidly eroded by an ongoing surge of model progress. To address this, there is a pressing need for difficult benchmarks that remain relevant for longer. We take this idea to its limit by introducing ZeroBench-a lightweight visual reasoning benchmark that is entirely impossible for contemporary frontier LMMs. Our benchmark consists of 100 manually curated questions and 334 less difficult subquestions. We evaluate 20 LMMs on ZeroBench, all of which score 0.0%, and rigorously analyse the errors. To encourage progress in visual understanding, we publicly release ZeroBench.', 'score': 21, 'issue_id': 2241, 'pub_date': '2025-02-13', 'pub_date_card': {'ru': '13 февраля', 'en': 'February 13', 'zh': '2月13日'}, 'hash': '00f4f8e85ea27717', 'authors': ['Jonathan Roberts', 'Mohammad Reza Taesiri', 'Ansh Sharma', 'Akash Gupta', 'Samuel Roberts', 'Ioana Croitoru', 'Simion-Vlad Bogolin', 'Jialu Tang', 'Florian Langer', 'Vyas Raina', 'Vatsal Raina', 'Hanyi Xiong', 'Vishaal Udandarao', 'Jingyi Lu', 'Shiyang Chen', 'Sam Purkis', 'Tianshuo Yan', 'Wenye Lin', 'Gyungin Shin', 'Qiaochu Yang', 'Anh Totti Nguyen', 'Kai Han', 'Samuel Albanie'], 'affiliations': ['Auburn University', 'Independent Researcher', 'The University of Hong Kong', 'University of Alberta', 'University of Cambridge', 'University of Oxford', 'University of Tubingen'], 'pdf_title_img': 'assets/pdf/title_img/2502.09696.jpg', 'data': {'categories': ['#cv', '#benchmark', '#interpretability', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'ZeroBench: невозможный визуальный тест для мультимодальных моделей', 'desc': 'В статье представлен новый визуальный бенчмарк ZeroBench, который полностью невозможно решить для современных мультимодальных языковых моделей (LMM). Бенчмарк состоит из 100 вручную отобранных вопросов и 334 менее сложных подвопросов. Авторы оценили 20 LMM на ZeroBench, и все они показали результат 0%. Целью бенчмарка является создание сложного теста, который останется актуальным дольше, чем существующие визуальные бенчмарки.'}, 'en': {'title': 'ZeroBench: Raising the Bar for Visual Reasoning in LMMs', 'desc': 'This paper discusses the limitations of Large Multimodal Models (LMMs) in interpreting images, highlighting that they perform worse in spatial reasoning compared to small children and animals. Despite achieving high scores on existing visual benchmarks, these models struggle with more complex visual reasoning tasks. To tackle this issue, the authors introduce ZeroBench, a new benchmark designed to be extremely challenging for current LMMs, consisting of 100 curated questions and 334 easier subquestions. The evaluation of 20 LMMs on ZeroBench resulted in a score of 0.0%, demonstrating the need for more difficult benchmarks to drive advancements in visual understanding.'}, 'zh': {'title': 'ZeroBench：推动视觉理解的新基准', 'desc': '大型多模态模型（LMMs）在图像理解方面存在显著不足，甚至在某些方面的空间认知能力不如小孩或动物。尽管如此，它们在许多流行的视觉基准测试中得分很高，但随着模型进步的加速，这些基准的挑战性迅速降低。为了解决这个问题，我们提出了ZeroBench，这是一个轻量级的视觉推理基准，当前的前沿LMMs完全无法解决。ZeroBench包含100个手动策划的问题和334个较简单的子问题，我们对20个LMMs进行了评估，结果均为0.0%，并对错误进行了严格分析。'}}}, {'id': 'https://huggingface.co/papers/2502.10391', 'title': 'MM-RLHF: The Next Step Forward in Multimodal LLM Alignment', 'url': 'https://huggingface.co/papers/2502.10391', 'abstract': 'Despite notable advancements in Multimodal Large Language Models (MLLMs), most state-of-the-art models have not undergone thorough alignment with human preferences. This gap exists because current alignment research has primarily achieved progress in specific areas (e.g., hallucination reduction), while the broader question of whether aligning models with human preferences can systematically enhance MLLM capability remains largely unexplored. To this end, we introduce MM-RLHF, a dataset containing 120k fine-grained, human-annotated preference comparison pairs. This dataset represents a substantial advancement over existing resources, offering superior size, diversity, annotation granularity, and quality. Leveraging this dataset, we propose several key innovations to improve both the quality of reward models and the efficiency of alignment algorithms. Notably, we introduce a Critique-Based Reward Model, which generates critiques of model outputs before assigning scores, offering enhanced interpretability and more informative feedback compared to traditional scalar reward mechanisms. Additionally, we propose Dynamic Reward Scaling, a method that adjusts the loss weight of each sample according to the reward signal, thereby optimizing the use of high-quality comparison pairs. Our approach is rigorously evaluated across 10 distinct dimensions and 27 benchmarks, with results demonstrating significant and consistent improvements in model performance. Specifically, fine-tuning LLaVA-ov-7B with MM-RLHF and our alignment algorithm leads to a 19.5% increase in conversational abilities and a 60% improvement in safety.   We have open-sourced the preference dataset, reward model, training and evaluation code, as well as reward modeling and safety benchmarks. For more details, please visit our project page: https://mm-rlhf.github.io.', 'score': 17, 'issue_id': 2241, 'pub_date': '2025-02-14', 'pub_date_card': {'ru': '14 февраля', 'en': 'February 14', 'zh': '2月14日'}, 'hash': 'c47a89fda79a1a4b', 'authors': ['Yi-Fan Zhang', 'Tao Yu', 'Haochen Tian', 'Chaoyou Fu', 'Peiyan Li', 'Jianshu Zeng', 'Wulin Xie', 'Yang Shi', 'Huanyu Zhang', 'Junkang Wu', 'Xue Wang', 'Yibo Hu', 'Bin Wen', 'Fan Yang', 'Zhang Zhang', 'Tingting Gao', 'Di Zhang', 'Liang Wang', 'Rong Jin', 'Tieniu Tan'], 'affiliations': ['Alibaba', 'CASIA', 'KuaiShou', 'Meta AI', 'NJU', 'PKU', 'USTC'], 'pdf_title_img': 'assets/pdf/title_img/2502.10391.jpg', 'data': {'categories': ['#benchmark', '#alignment', '#training', '#interpretability', '#open_source', '#rlhf', '#dataset'], 'emoji': '🤖', 'ru': {'title': 'Новый подход к согласованию мультимодальных ИИ-моделей с человеческими предпочтениями', 'desc': 'Исследователи представили MM-RLHF - набор данных из 120 тысяч размеченных пар сравнения предпочтений для мультимодальных языковых моделей. На его основе они разработали новые методы обучения с подкреплением, включая модель вознаграждения на основе критики и динамическое масштабирование вознаграждений. Эксперименты показали значительное улучшение способностей модели LLaVA-ov-7B в диалогах и безопасности после дообучения с использованием предложенных методов. Авторы открыли доступ к набору данных, моделям и коду для воспроизведения результатов.'}, 'en': {'title': 'Enhancing MLLM Alignment with Human Preferences', 'desc': 'This paper addresses the need for better alignment of Multimodal Large Language Models (MLLMs) with human preferences. It introduces MM-RLHF, a new dataset with 120,000 human-annotated preference comparisons, which enhances the quality and diversity of training data for alignment tasks. The authors propose innovative techniques like a Critique-Based Reward Model that provides detailed feedback on model outputs and Dynamic Reward Scaling to optimize the training process. Their methods show significant improvements in model performance, including a 19.5% boost in conversational skills and a 60% increase in safety metrics.'}, 'zh': {'title': '提升多模态模型与人类偏好的对齐', 'desc': '尽管多模态大型语言模型（MLLMs）取得了显著进展，但大多数最先进的模型尚未与人类偏好进行充分对齐。为了解决这一问题，我们引入了MM-RLHF数据集，包含12万个细粒度的人类标注偏好比较对，显著提升了现有资源的规模和质量。我们提出了基于批评的奖励模型，能够在评分前生成模型输出的批评，从而提供更具可解释性和信息量的反馈。此外，我们还提出了动态奖励缩放方法，根据奖励信号调整每个样本的损失权重，以优化高质量比较对的使用。'}}}, {'id': 'https://huggingface.co/papers/2502.09935', 'title': 'Precise Parameter Localization for Textual Generation in Diffusion Models', 'url': 'https://huggingface.co/papers/2502.09935', 'abstract': "Novel diffusion models can synthesize photo-realistic images with integrated high-quality text. Surprisingly, we demonstrate through attention activation patching that only less than 1% of diffusion models' parameters, all contained in attention layers, influence the generation of textual content within the images. Building on this observation, we improve textual generation efficiency and performance by targeting cross and joint attention layers of diffusion models. We introduce several applications that benefit from localizing the layers responsible for textual content generation. We first show that a LoRA-based fine-tuning solely of the localized layers enhances, even more, the general text-generation capabilities of large diffusion models while preserving the quality and diversity of the diffusion models' generations. Then, we demonstrate how we can use the localized layers to edit textual content in generated images. Finally, we extend this idea to the practical use case of preventing the generation of toxic text in a cost-free manner. In contrast to prior work, our localization approach is broadly applicable across various diffusion model architectures, including U-Net (e.g., LDM and SDXL) and transformer-based (e.g., DeepFloyd IF and Stable Diffusion 3), utilizing diverse text encoders (e.g., from CLIP to the large language models like T5). Project page available at https://t2i-text-loc.github.io/.", 'score': 9, 'issue_id': 2245, 'pub_date': '2025-02-14', 'pub_date_card': {'ru': '14 февраля', 'en': 'February 14', 'zh': '2月14日'}, 'hash': '1c3ce78b0c6424d2', 'authors': ['Łukasz Staniszewski', 'Bartosz Cywiński', 'Franziska Boenisch', 'Kamil Deja', 'Adam Dziedzic'], 'affiliations': ['CISPA Helmholtz Center for Information Security', 'IDEAS NCBR', 'Warsaw University of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2502.09935.jpg', 'data': {'categories': ['#cv', '#diffusion', '#training', '#synthetic', '#architecture'], 'emoji': '🔍', 'ru': {'title': 'Локализация текста в диффузионных моделях для повышения эффективности генерации', 'desc': 'Статья представляет новый подход к улучшению генерации текста в диффузионных моделях для создания изображений. Авторы обнаружили, что менее 1% параметров модели, расположенных в слоях внимания, отвечают за текстовый контент. На основе этого наблюдения они предлагают методы повышения эффективности и качества генерации текста, включая точечную настройку локализованных слоев и редактирование текста в сгенерированных изображениях. Подход применим к различным архитектурам диффузионных моделей и текстовым энкодерам.'}, 'en': {'title': 'Targeting Attention for Enhanced Text Generation in Diffusion Models', 'desc': "This paper explores how diffusion models can create realistic images that include high-quality text. It reveals that less than 1% of the model's parameters, specifically in the attention layers, are crucial for generating text within these images. By focusing on these specific layers, the authors enhance the efficiency and performance of text generation in diffusion models. They also present applications such as improving text generation, editing text in images, and preventing toxic text generation, demonstrating the broad applicability of their approach across different model architectures."}, 'zh': {'title': '局部化注意力层提升文本生成能力', 'desc': '本论文介绍了一种新颖的扩散模型，能够合成高质量的照片级图像，并集成文本生成。研究发现，扩散模型中只有不到1%的参数，主要集中在注意力层，影响图像中的文本内容生成。基于这一观察，作者通过针对交叉和联合注意力层，提升了文本生成的效率和性能。论文还展示了如何利用这些局部化的层来编辑生成图像中的文本内容，并防止生成有害文本。'}}}, {'id': 'https://huggingface.co/papers/2502.09955', 'title': 'Diverse Inference and Verification for Advanced Reasoning', 'url': 'https://huggingface.co/papers/2502.09955', 'abstract': "Reasoning LLMs such as OpenAI o1, o3 and DeepSeek R1 have made significant progress in mathematics and coding, yet find challenging advanced tasks such as International Mathematical Olympiad (IMO) combinatorics problems, Abstraction and Reasoning Corpus (ARC) puzzles, and Humanity's Last Exam (HLE) questions. We use a diverse inference approach that combines multiple models and methods at test time. We find that verifying mathematics and code problems, and rejection sampling on other problems is simple and effective. We automatically verify correctness of solutions to IMO problems by Lean, and ARC puzzles by code, and find that best-of-N effectively answers HLE questions. Our approach increases answer accuracy on IMO combinatorics problems from 33.3% to 77.8%, accuracy on HLE questions from 8% to 37%, and solves 80% of ARC puzzles that 948 humans could not and 26.5% of ARC puzzles that o3 high compute does not. Test-time simulations, reinforcement learning, and meta-learning with inference feedback improve generalization by adapting agent graph representations and varying prompts, code, and datasets. Our approach is reliable, robust, and scalable, and in the spirit of reproducible research, we will make it publicly available upon publication.", 'score': 7, 'issue_id': 2242, 'pub_date': '2025-02-14', 'pub_date_card': {'ru': '14 февраля', 'en': 'February 14', 'zh': '2月14日'}, 'hash': '10eaccfc7377f600', 'authors': ['Iddo Drori', 'Gaston Longhitano', 'Mao Mao', 'Seunghwan Hyun', 'Yuke Zhang', 'Sungjun Park', 'Zachary Meeks', 'Xin-Yu Zhang', 'Ben Segev', 'Howard Yong', 'Nakul Verma', 'Avi Shporer', 'Alon Amit', 'Madeleine Udell'], 'affiliations': ['Boston University', 'Columbia University', 'Google', 'Intuit', 'Massachusetts Institute of Technology', 'NotBadMath.AI', 'Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2502.09955.jpg', 'data': {'categories': ['#math', '#reasoning', '#optimization', '#agents', '#rl', '#training', '#inference'], 'emoji': '🧠', 'ru': {'title': 'Усиление LLM для решения сложных задач: многомодельный подход', 'desc': "Статья описывает подход к улучшению производительности языковых моделей (LLM) в решении сложных математических и логических задач. Авторы применяют комбинацию нескольких моделей и методов, включая верификацию решений и отбор лучших ответов. Подход значительно повышает точность на задачах Международной математической олимпиады, Humanity's Last Exam и Abstraction and Reasoning Corpus. Исследователи используют симуляции, обучение с подкреплением и мета-обучение для адаптации представлений агентов и улучшения обобщающей способности."}, 'en': {'title': 'Boosting LLMs: From 33% to 77% Accuracy in Complex Problem Solving!', 'desc': "This paper discusses advancements in reasoning large language models (LLMs) like OpenAI's models and DeepSeek in tackling complex mathematical and coding challenges. The authors propose a diverse inference strategy that integrates various models and methods during testing to enhance performance on difficult tasks such as IMO problems and ARC puzzles. They demonstrate that their method significantly boosts accuracy, achieving a 77.8% success rate on IMO combinatorics and solving 80% of previously unsolved ARC puzzles. Additionally, they employ techniques like reinforcement learning and meta-learning to improve the adaptability and generalization of their models, ensuring their approach is both reliable and scalable."}, 'zh': {'title': '提升推理模型在高级数学问题上的准确性', 'desc': '本文探讨了推理型大语言模型在解决高级数学和编程任务中的挑战，特别是国际数学奥林匹克（IMO）组合问题、抽象与推理语料库（ARC）难题和人类最后考试（HLE）问题。我们提出了一种多模型和多方法结合的推理方法，在测试时进行多样化推理。通过自动验证IMO问题和ARC难题的解答正确性，我们显著提高了这些问题的解答准确率。我们的研究方法可靠、稳健且可扩展，旨在推动可重复研究的发展。'}}}, {'id': 'https://huggingface.co/papers/2502.10235', 'title': 'AdaPTS: Adapting Univariate Foundation Models to Probabilistic Multivariate Time Series Forecasting', 'url': 'https://huggingface.co/papers/2502.10235', 'abstract': 'Pre-trained foundation models (FMs) have shown exceptional performance in univariate time series forecasting tasks. However, several practical challenges persist, including managing intricate dependencies among features and quantifying uncertainty in predictions. This study aims to tackle these critical limitations by introducing adapters; feature-space transformations that facilitate the effective use of pre-trained univariate time series FMs for multivariate tasks. Adapters operate by projecting multivariate inputs into a suitable latent space and applying the FM independently to each dimension. Inspired by the literature on representation learning and partially stochastic Bayesian neural networks, we present a range of adapters and optimization/inference strategies. Experiments conducted on both synthetic and real-world datasets confirm the efficacy of adapters, demonstrating substantial enhancements in forecasting accuracy and uncertainty quantification compared to baseline methods. Our framework, AdaPTS, positions adapters as a modular, scalable, and effective solution for leveraging time series FMs in multivariate contexts, thereby promoting their wider adoption in real-world applications. We release the code at https://github.com/abenechehab/AdaPTS.', 'score': 6, 'issue_id': 2248, 'pub_date': '2025-02-14', 'pub_date_card': {'ru': '14 февраля', 'en': 'February 14', 'zh': '2月14日'}, 'hash': 'dbd38216ecfcb531', 'authors': ['Abdelhakim Benechehab', 'Vasilii Feofanov', 'Giuseppe Paolo', 'Albert Thomas', 'Maurizio Filippone', 'Balázs Kégl'], 'affiliations': ['Department of Data Science, EURECOM', 'Huawei Noahs Ark Lab, Paris, France', 'Statistics Program, KAUST'], 'pdf_title_img': 'assets/pdf/title_img/2502.10235.jpg', 'data': {'categories': ['#data', '#synthetic', '#dataset', '#inference', '#optimization', '#training'], 'emoji': '📊', 'ru': {'title': 'AdaPTS: Адаптация одномерных моделей для многомерного прогнозирования временных рядов', 'desc': 'Исследование представляет AdaPTS - фреймворк, использующий адаптеры для применения предобученных моделей-основ (foundation models) в задачах многомерного прогнозирования временных рядов. Адаптеры проецируют многомерные входные данные в латентное пространство, позволяя эффективно использовать одномерные модели для многомерных задач. Эксперименты на синтетических и реальных данных показали значительное улучшение точности прогнозирования и количественной оценки неопределенности по сравнению с базовыми методами. Фреймворк AdaPTS представляет собой модульное и масштабируемое решение для использования моделей временных рядов в многомерных контекстах.'}, 'en': {'title': 'Enhancing Multivariate Time Series Forecasting with Adapters', 'desc': 'This paper introduces a new approach called adapters to improve the use of pre-trained foundation models (FMs) for multivariate time series forecasting. Adapters transform multivariate inputs into a latent space, allowing the FM to process each feature independently while managing complex dependencies. The study also explores various optimization and inference strategies inspired by representation learning and Bayesian neural networks. Experiments show that this method significantly enhances forecasting accuracy and uncertainty quantification, making it a valuable tool for real-world applications.'}, 'zh': {'title': '适配器：多变量时间序列预测的新解决方案', 'desc': '预训练基础模型在单变量时间序列预测任务中表现出色，但在处理特征间复杂依赖关系和量化预测不确定性方面仍面临挑战。本研究通过引入适配器来解决这些关键限制，适配器是一种特征空间转换，能够有效利用预训练的单变量时间序列模型进行多变量任务。适配器通过将多变量输入投影到合适的潜在空间，并独立地对每个维度应用基础模型，从而实现功能。实验结果表明，适配器在预测准确性和不确定性量化方面显著优于基线方法，展示了其在多变量时间序列应用中的有效性。'}}}, {'id': 'https://huggingface.co/papers/2502.07780', 'title': 'DarwinLM: Evolutionary Structured Pruning of Large Language Models', 'url': 'https://huggingface.co/papers/2502.07780', 'abstract': 'Large Language Models (LLMs) have achieved significant success across various NLP tasks. However, their massive computational costs limit their widespread use, particularly in real-time applications. Structured pruning offers an effective solution by compressing models and directly providing end-to-end speed improvements, regardless of the hardware environment. Meanwhile, different components of the model exhibit varying sensitivities towards pruning, calling for non-uniform model compression. However, a pruning method should not only identify a capable substructure, but also account for post-compression training. To this end, we propose \\sysname, a method for training-aware structured pruning. \\sysname builds upon an evolutionary search process, generating multiple offspring models in each generation through mutation, and selecting the fittest for survival. To assess the effect of post-training, we incorporate a lightweight, multistep training process within the offspring population, progressively increasing the number of tokens and eliminating poorly performing models in each selection stage. We validate our method through extensive experiments on Llama-2-7B, Llama-3.1-8B and Qwen-2.5-14B-Instruct, achieving state-of-the-art performance for structured pruning. For instance, \\sysname surpasses ShearedLlama while requiring 5times less training data during post-compression training.', 'score': 5, 'issue_id': 2251, 'pub_date': '2025-02-11', 'pub_date_card': {'ru': '11 февраля', 'en': 'February 11', 'zh': '2月11日'}, 'hash': 'b53421574afe0c8a', 'authors': ['Shengkun Tang', 'Oliver Sieberling', 'Eldar Kurtic', 'Zhiqiang Shen', 'Dan Alistarh'], 'affiliations': ['Department of Machine Learning, MBZUAI, Abu Dhabi, UAE', 'ETH Zurich, Zurich, Switzerland', 'ISTA, Vienna, Austria', 'Red Hat AI, Boston, USA'], 'pdf_title_img': 'assets/pdf/title_img/2502.07780.jpg', 'data': {'categories': ['#inference', '#small_models', '#training', '#optimization'], 'emoji': '🧬', 'ru': {'title': 'Эволюционная оптимизация языковых моделей', 'desc': 'В статье рассматривается проблема высокой вычислительной стоимости больших языковых моделей (LLM) и предлагается метод структурированной обрезки для их оптимизации. Метод \x1710\x187 использует эволюционный подход, создавая и отбирая наиболее эффективные подмодели. Важной частью метода является обучение после обрезки, что позволяет улучшить производительность моделей. Эксперименты показывают, что \x1710\x187 превосходит существующие методы, требуя меньше данных для обучения.'}, 'en': {'title': 'Efficient Pruning for Powerful Language Models', 'desc': 'This paper presents a new method called \textit{sysname} for structured pruning of large language models (LLMs) to improve their efficiency in natural language processing tasks. The method uses an evolutionary search process to create multiple model variations, selecting the best-performing ones while incorporating a lightweight training process to enhance post-compression performance. By focusing on the varying sensitivities of different model components, \textit{sysname} achieves effective model compression without sacrificing accuracy. The results show that \textit{sysname} outperforms existing methods, requiring significantly less training data while maintaining high performance on various LLMs.'}, 'zh': {'title': '训练感知的结构化剪枝方法', 'desc': '大型语言模型（LLMs）在自然语言处理任务中取得了显著成功，但其巨大的计算成本限制了其广泛应用，尤其是在实时应用中。结构化剪枝是一种有效的解决方案，通过压缩模型并直接提供端到端的速度提升。不同模型组件对剪枝的敏感性不同，因此需要非均匀的模型压缩方法。我们提出了一种名为\textit{sysname}的训练感知结构化剪枝方法，通过进化搜索过程生成多个后代模型，并在每一代中选择最适合的模型进行生存。'}}}, {'id': 'https://huggingface.co/papers/2502.09411', 'title': 'ImageRAG: Dynamic Image Retrieval for Reference-Guided Image Generation', 'url': 'https://huggingface.co/papers/2502.09411', 'abstract': 'Diffusion models enable high-quality and diverse visual content synthesis. However, they struggle to generate rare or unseen concepts. To address this challenge, we explore the usage of Retrieval-Augmented Generation (RAG) with image generation models. We propose ImageRAG, a method that dynamically retrieves relevant images based on a given text prompt, and uses them as context to guide the generation process. Prior approaches that used retrieved images to improve generation, trained models specifically for retrieval-based generation. In contrast, ImageRAG leverages the capabilities of existing image conditioning models, and does not require RAG-specific training. Our approach is highly adaptable and can be applied across different model types, showing significant improvement in generating rare and fine-grained concepts using different base models.   Our project page is available at: https://rotem-shalev.github.io/ImageRAG', 'score': 5, 'issue_id': 2251, 'pub_date': '2025-02-13', 'pub_date_card': {'ru': '13 февраля', 'en': 'February 13', 'zh': '2月13日'}, 'hash': 'a3c0f020b9cc5226', 'authors': ['Rotem Shalev-Arkushin', 'Rinon Gal', 'Amit H. Bermano', 'Ohad Fried'], 'affiliations': ['NVIDIA', 'Reichman University', 'Tel Aviv University'], 'pdf_title_img': 'assets/pdf/title_img/2502.09411.jpg', 'data': {'categories': ['#multimodal', '#transfer_learning', '#cv', '#rag', '#diffusion'], 'emoji': '🔍', 'ru': {'title': 'ImageRAG: Улучшение генерации изображений с помощью извлечения и дополнения', 'desc': 'Статья представляет метод ImageRAG, который использует технологию извлечения и дополнения генерации (RAG) для улучшения работы диффузионных моделей при создании редких или невиданных концепций. ImageRAG динамически извлекает релевантные изображения на основе текстового запроса и использует их в качестве контекста для управления процессом генерации. В отличие от предыдущих подходов, ImageRAG не требует специального обучения для работы с извлеченными изображениями, а использует возможности существующих моделей обусловливания изображений. Метод показывает значительное улучшение в генерации редких и детализированных концепций с использованием различных базовых моделей.'}, 'en': {'title': 'Enhancing Image Generation with Dynamic Retrieval', 'desc': 'This paper introduces ImageRAG, a novel method that enhances image generation by integrating Retrieval-Augmented Generation (RAG) techniques. Unlike previous methods that required specific training for retrieval-based generation, ImageRAG utilizes existing image conditioning models to dynamically retrieve relevant images based on text prompts. This approach allows for improved synthesis of rare and fine-grained visual concepts by providing contextual guidance during the generation process. The adaptability of ImageRAG across various model types demonstrates its effectiveness in producing high-quality and diverse visual content.'}, 'zh': {'title': 'ImageRAG：提升稀有概念生成的创新方法', 'desc': '扩散模型可以生成高质量和多样化的视觉内容，但在生成稀有或未见过的概念时存在困难。为了解决这个问题，我们探索了结合检索增强生成（RAG）与图像生成模型的方法。我们提出了ImageRAG，这是一种根据给定文本提示动态检索相关图像的方法，并将这些图像作为上下文来指导生成过程。与之前需要专门训练的检索生成模型不同，ImageRAG利用现有图像条件模型的能力，无需特定的RAG训练，适应性强，能够在不同模型类型中应用。'}}}, {'id': 'https://huggingface.co/papers/2502.07586', 'title': "We Can't Understand AI Using our Existing Vocabulary", 'url': 'https://huggingface.co/papers/2502.07586', 'abstract': 'This position paper argues that, in order to understand AI, we cannot rely on our existing vocabulary of human words. Instead, we should strive to develop neologisms: new words that represent precise human concepts that we want to teach machines, or machine concepts that we need to learn. We start from the premise that humans and machines have differing concepts. This means interpretability can be framed as a communication problem: humans must be able to reference and control machine concepts, and communicate human concepts to machines. Creating a shared human-machine language through developing neologisms, we believe, could solve this communication problem. Successful neologisms achieve a useful amount of abstraction: not too detailed, so they\'re reusable in many contexts, and not too high-level, so they convey precise information. As a proof of concept, we demonstrate how a "length neologism" enables controlling LLM response length, while a "diversity neologism" allows sampling more variable responses. Taken together, we argue that we cannot understand AI using our existing vocabulary, and expanding it through neologisms creates opportunities for both controlling and understanding machines better.', 'score': 5, 'issue_id': 2247, 'pub_date': '2025-02-11', 'pub_date_card': {'ru': '11 февраля', 'en': 'February 11', 'zh': '2月11日'}, 'hash': '0435a4b5389f3dcb', 'authors': ['John Hewitt', 'Robert Geirhos', 'Been Kim'], 'affiliations': ['Google DeepMind'], 'pdf_title_img': 'assets/pdf/title_img/2502.07586.jpg', 'data': {'categories': ['#multimodal', '#alignment', '#interpretability'], 'emoji': '🗣️', 'ru': {'title': 'Неологизмы как мост между человеком и ИИ', 'desc': 'В статье утверждается, что для понимания искусственного интеллекта недостаточно существующего человеческого словаря. Авторы предлагают разрабатывать неологизмы - новые слова для точных человеческих и машинных концепций. Интерпретируемость рассматривается как проблема коммуникации между людьми и машинами. Создание общего языка через неологизмы может решить эту проблему, позволяя лучше контролировать и понимать ИИ.'}, 'en': {'title': 'Creating New Words for Better AI Communication', 'desc': 'This paper discusses the need for new words, or neologisms, to better communicate with artificial intelligence (AI). It argues that humans and machines have different ways of understanding concepts, which makes it hard for us to explain our ideas to machines. By creating a shared language with these new terms, we can improve how we control and interpret machine behavior. The authors provide examples of neologisms that help manage AI responses, showing that a richer vocabulary can enhance our interaction with AI systems.'}, 'zh': {'title': '通过新词汇理解人工智能', 'desc': '这篇论文认为，要理解人工智能，我们不能仅依赖现有的人类词汇。我们应该努力开发新词汇，以准确表达我们想教给机器的人类概念或我们需要学习的机器概念。人类和机器的概念不同，因此可解释性可以看作是一个沟通问题：人类必须能够引用和控制机器概念，并将人类概念传达给机器。通过开发新词汇创建一个共享的人机语言，可以解决这个沟通问题。'}}}, {'id': 'https://huggingface.co/papers/2502.09741', 'title': 'FoNE: Precise Single-Token Number Embeddings via Fourier Features', 'url': 'https://huggingface.co/papers/2502.09741', 'abstract': "Large Language Models (LLMs) typically represent numbers using multiple tokens, which requires the model to aggregate these tokens to interpret numerical values. This fragmentation makes both training and inference less efficient and adversely affects the model's performance on number-related tasks. Inspired by the observation that pre-trained LLMs internally learn Fourier-like features for number tokens, we propose Fourier Number Embedding (FoNE), a novel method that directly maps numbers into the embedding space with their Fourier features. FoNE encodes each number as a single token with only two embedding dimensions per digit, effectively capturing numerical values without fragmentation. This compact representation accelerates both training and inference. Compared to traditional subword and digit-wise embeddings, FoNE not only reduces computational overhead but also achieves higher accuracy across various numerical tasks including addition, subtraction and multiplication. On 6-digit decimal addition, FoNE requires 64times less data to achieve 99% accuracy than subword and digit-wise embeddings while using 3times and 6times fewer tokens per number, respectively. Furthermore, FoNE is the only method that yields 100% accuracy on over 100,000 test examples for addition, subtraction, and multiplication. The codes and visualization are available at https://fouriernumber.github.io/.", 'score': 5, 'issue_id': 2241, 'pub_date': '2025-02-13', 'pub_date_card': {'ru': '13 февраля', 'en': 'February 13', 'zh': '2月13日'}, 'hash': 'adb30f7d3d01ce3a', 'authors': ['Tianyi Zhou', 'Deqing Fu', 'Mahdi Soltanolkotabi', 'Robin Jia', 'Vatsal Sharan'], 'affiliations': ['Department of Computer Science University of Southern California'], 'pdf_title_img': 'assets/pdf/title_img/2502.09741.jpg', 'data': {'categories': ['#architecture', '#training', '#data', '#optimization'], 'emoji': '🔢', 'ru': {'title': 'FoNE: революция в обработке чисел для языковых моделей', 'desc': 'Исследователи предложили новый метод кодирования чисел для больших языковых моделей, названный Fourier Number Embedding (FoNE). FoNE представляет числа в виде единых токенов с использованием фурье-подобных признаков, что позволяет эффективно захватывать числовые значения без фрагментации. Этот компактный способ представления ускоряет как обучение, так и вывод модели. По сравнению с традиционными методами, FoNE демонстрирует более высокую точность в различных числовых задачах, включая сложение, вычитание и умножение, при меньших вычислительных затратах.'}, 'en': {'title': 'Revolutionizing Number Representation in LLMs with FoNE', 'desc': 'This paper introduces Fourier Number Embedding (FoNE), a new approach for representing numbers in Large Language Models (LLMs). FoNE simplifies the representation of numerical values by encoding each number as a single token using Fourier features, which reduces fragmentation and improves efficiency. The method significantly enhances model performance on numerical tasks, achieving higher accuracy with fewer tokens compared to traditional embeddings. Notably, FoNE demonstrates remarkable results, requiring much less data to reach high accuracy levels in arithmetic operations like addition, subtraction, and multiplication.'}, 'zh': {'title': '傅里叶数字嵌入：高效的数字表示方法', 'desc': '大型语言模型（LLMs）通常使用多个标记来表示数字，这导致模型在理解数值时需要聚合这些标记，降低了训练和推理的效率。我们提出了一种新方法，称为傅里叶数字嵌入（FoNE），它将数字直接映射到嵌入空间，使用傅里叶特征进行编码。FoNE将每个数字编码为一个单一标记，显著减少了计算开销，并在加法、减法和乘法等数值任务中实现了更高的准确性。与传统的子词和数字嵌入相比，FoNE在6位十进制加法中所需的数据量减少了64倍，同时每个数字使用的标记数量也减少了3倍到6倍。'}}}, {'id': 'https://huggingface.co/papers/2502.10140', 'title': 'Small Models, Big Impact: Efficient Corpus and Graph-Based Adaptation of Small Multilingual Language Models for Low-Resource Languages', 'url': 'https://huggingface.co/papers/2502.10140', 'abstract': 'Low-resource languages (LRLs) face significant challenges in natural language processing (NLP) due to limited data. While current state-of-the-art large language models (LLMs) still struggle with LRLs, smaller multilingual models (mLMs) such as mBERT and XLM-R offer greater promise due to a better fit of their capacity to low training data sizes. This study systematically investigates parameter-efficient adapter-based methods for adapting mLMs to LRLs, evaluating three architectures: Sequential Bottleneck, Invertible Bottleneck, and Low-Rank Adaptation. Using unstructured text from GlotCC and structured knowledge from ConceptNet, we show that small adaptation datasets (e.g., up to 1 GB of free-text or a few MB of knowledge graph data) yield gains in intrinsic (masked language modeling) and extrinsic tasks (topic classification, sentiment analysis, and named entity recognition). We find that Sequential Bottleneck adapters excel in language modeling, while Invertible Bottleneck adapters slightly outperform other methods on downstream tasks due to better embedding alignment and larger parameter counts. Adapter-based methods match or outperform full fine-tuning while using far fewer parameters, and smaller mLMs prove more effective for LRLs than massive LLMs like LLaMA-3, GPT-4, and DeepSeek-R1-based distilled models. While adaptation improves performance, pre-training data size remains the dominant factor, especially for languages with extensive pre-training coverage.', 'score': 4, 'issue_id': 2251, 'pub_date': '2025-02-14', 'pub_date_card': {'ru': '14 февраля', 'en': 'February 14', 'zh': '2月14日'}, 'hash': '0d0292edb1900dd5', 'authors': ['Daniil Gurgurov', 'Ivan Vykopal', 'Josef van Genabith', 'Simon Ostermann'], 'affiliations': ['Brno University of Technology', 'German Research Center for Artificial Intelligence (DFKI)', 'Kempelen Institute of Intelligent Technologies (KInIT)', 'University of Saarland'], 'pdf_title_img': 'assets/pdf/title_img/2502.10140.jpg', 'data': {'categories': ['#training', '#transfer_learning', '#small_models', '#multilingual', '#low_resource'], 'emoji': '🌍', 'ru': {'title': 'Эффективная адаптация многоязычных моделей для языков с ограниченными ресурсами', 'desc': 'Исследование посвящено адаптации многоязычных моделей (mLMs) для языков с ограниченными ресурсами (LRLs) с использованием эффективных по параметрам методов на основе адаптеров. Авторы сравнивают три архитектуры адаптеров: Sequential Bottleneck, Invertible Bottleneck и Low-Rank Adaptation, оценивая их эффективность на различных задачах обработки естественного языка. Результаты показывают, что адаптация на небольших наборах данных (до 1 ГБ неструктурированного текста или несколько МБ структурированных знаний) улучшает производительность моделей. Исследование также демонстрирует, что для LRLs более эффективны меньшие mLMs, чем крупные языковые модели типа LLaMA-3 или GPT-4.'}, 'en': {'title': 'Empowering Low-Resource Languages with Efficient Multilingual Models', 'desc': 'This paper addresses the challenges of processing low-resource languages (LRLs) in natural language processing (NLP) by exploring the use of smaller multilingual models (mLMs) like mBERT and XLM-R. It investigates parameter-efficient adapter-based methods to adapt these mLMs to LRLs, focusing on three architectures: Sequential Bottleneck, Invertible Bottleneck, and Low-Rank Adaptation. The study demonstrates that even small adaptation datasets can significantly enhance performance in both intrinsic and extrinsic NLP tasks. Results indicate that Sequential Bottleneck adapters are particularly effective for language modeling, while Invertible Bottleneck adapters perform better on downstream tasks, highlighting the advantages of adapter-based methods over full fine-tuning.'}, 'zh': {'title': '小型多语言模型助力低资源语言处理', 'desc': '低资源语言（LRLs）在自然语言处理（NLP）中面临数据不足的重大挑战。当前的先进大型语言模型（LLMs）在处理LRLs时仍然存在困难，而较小的多语言模型（mLMs）如mBERT和XLM-R由于其容量更适合低训练数据量，展现出更大的潜力。本文系统研究了基于参数高效适配器的方法，评估了三种架构：顺序瓶颈、可逆瓶颈和低秩适配。研究表明，使用小型适配数据集可以在语言建模和下游任务中取得显著提升，尤其是顺序瓶颈适配器在语言建模方面表现优异。'}}}, {'id': 'https://huggingface.co/papers/2502.09638', 'title': 'Jailbreaking to Jailbreak', 'url': 'https://huggingface.co/papers/2502.09638', 'abstract': 'Refusal training on Large Language Models (LLMs) prevents harmful outputs, yet this defense remains vulnerable to both automated and human-crafted jailbreaks. We present a novel LLM-as-red-teamer approach in which a human jailbreaks a refusal-trained LLM to make it willing to jailbreak itself or other LLMs. We refer to the jailbroken LLMs as J_2 attackers, which can systematically evaluate target models using various red teaming strategies and improve its performance via in-context learning from the previous failures. Our experiments demonstrate that Sonnet 3.5 and Gemini 1.5 pro outperform other LLMs as J_2, achieving 93.0% and 91.0% attack success rates (ASRs) respectively against GPT-4o (and similar results across other capable LLMs) on Harmbench. Our work not only introduces a scalable approach to strategic red teaming, drawing inspiration from human red teamers, but also highlights jailbreaking-to-jailbreak as an overlooked failure mode of the safeguard. Specifically, an LLM can bypass its own safeguards by employing a jailbroken version of itself that is willing to assist in further jailbreaking. To prevent any direct misuse with J_2, while advancing research in AI safety, we publicly share our methodology while keeping specific prompting details private.', 'score': 3, 'issue_id': 2242, 'pub_date': '2025-02-09', 'pub_date_card': {'ru': '9 февраля', 'en': 'February 9', 'zh': '2月9日'}, 'hash': '3c2ed560e12b971a', 'authors': ['Jeremy Kritz', 'Vaughn Robinson', 'Robert Vacareanu', 'Bijan Varjavand', 'Michael Choi', 'Bobby Gogov', 'Scale Red Team', 'Summer Yue', 'Willow E. Primack', 'Zifan Wang'], 'affiliations': ['Scale'], 'pdf_title_img': 'assets/pdf/title_img/2502.09638.jpg', 'data': {'categories': ['#ethics', '#multimodal', '#training', '#security', '#rlhf'], 'emoji': '🕵️', 'ru': {'title': 'LLM против LLM: новый фронт в безопасности ИИ', 'desc': "Статья представляет новый подход к тестированию безопасности больших языковых моделей (LLM), использующий сами LLM в качестве 'красной команды'. Авторы демонстрируют, как взломанная модель (J_2) может систематически оценивать и атаковать другие модели, достигая высокого уровня успеха. Эксперименты показывают, что Sonnet 3.5 и Gemini 1.5 pro превосходят другие LLM в роли J_2, достигая 93.0% и 91.0% успешности атак соответственно. Исследование подчеркивает уязвимость существующих механизмов защиты и предлагает новый метод для улучшения безопасности искусственного интеллекта."}, 'en': {'title': 'Jailbreaking the Safeguards: A New Approach to LLM Red Teaming', 'desc': 'This paper discusses a new method for testing the safety of Large Language Models (LLMs) by using a jailbroken version of the model itself, referred to as J_2 attackers. These J_2 attackers can evaluate and exploit vulnerabilities in other LLMs, achieving high success rates in bypassing safeguards. The approach allows the jailbroken LLM to learn from its previous attempts, improving its effectiveness in red teaming strategies. The authors emphasize the importance of understanding this jailbreaking-to-jailbreak phenomenon as a potential risk in AI safety.'}, 'zh': {'title': '破解自我保护的红队策略', 'desc': '本论文提出了一种新的方法，利用大型语言模型（LLM）作为红队成员，来评估和改进拒绝训练模型的安全性。我们称这些被破解的LLM为J_2攻击者，它们能够通过上下文学习从之前的失败中提高性能。实验结果显示，Sonnet 3.5和Gemini 1.5在攻击成功率上优于其他LLM，分别达到了93.0%和91.0%。我们的研究不仅提供了一种可扩展的红队策略，还揭示了破解自身保护机制的潜在风险。'}}}, {'id': 'https://huggingface.co/papers/2502.10177', 'title': 'STMA: A Spatio-Temporal Memory Agent for Long-Horizon Embodied Task Planning', 'url': 'https://huggingface.co/papers/2502.10177', 'abstract': 'A key objective of embodied intelligence is enabling agents to perform long-horizon tasks in dynamic environments while maintaining robust decision-making and adaptability. To achieve this goal, we propose the Spatio-Temporal Memory Agent (STMA), a novel framework designed to enhance task planning and execution by integrating spatio-temporal memory. STMA is built upon three critical components: (1) a spatio-temporal memory module that captures historical and environmental changes in real time, (2) a dynamic knowledge graph that facilitates adaptive spatial reasoning, and (3) a planner-critic mechanism that iteratively refines task strategies. We evaluate STMA in the TextWorld environment on 32 tasks, involving multi-step planning and exploration under varying levels of complexity. Experimental results demonstrate that STMA achieves a 31.25% improvement in success rate and a 24.7% increase in average score compared to the state-of-the-art model. The results highlight the effectiveness of spatio-temporal memory in advancing the memory capabilities of embodied agents.', 'score': 3, 'issue_id': 2240, 'pub_date': '2025-02-14', 'pub_date_card': {'ru': '14 февраля', 'en': 'February 14', 'zh': '2月14日'}, 'hash': '1b17b668b26c2264', 'authors': ['Mingcong Lei', 'Yiming Zhao', 'Ge Wang', 'Zhixin Mai', 'Shuguang Cui', 'Yatong Han', 'Jinke Ren'], 'affiliations': ['FNii-Shenzhen, The Chinese University of Hong Kong, Shenzhen, China', 'Harbin Engineering University, China', 'Infused Synapse AI'], 'pdf_title_img': 'assets/pdf/title_img/2502.10177.jpg', 'data': {'categories': ['#games', '#agents', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Пространственно-временная память повышает эффективность воплощенных агентов', 'desc': 'Статья представляет новый фреймворк под названием Spatio-Temporal Memory Agent (STMA) для улучшения планирования и выполнения задач агентами с воплощенным интеллектом. STMA включает в себя модуль пространственно-временной памяти, динамический граф знаний и механизм планировщика-критика. Эксперименты в среде TextWorld показали значительное улучшение успешности и средней оценки по сравнению с современными моделями. Результаты подчеркивают эффективность пространственно-временной памяти для улучшения возможностей памяти воплощенных агентов.'}, 'en': {'title': 'Enhancing Agent Intelligence with Spatio-Temporal Memory', 'desc': 'The paper introduces the Spatio-Temporal Memory Agent (STMA), which aims to improve how agents perform complex tasks in changing environments. STMA incorporates a spatio-temporal memory module to track past events and environmental shifts, enhancing decision-making. It also utilizes a dynamic knowledge graph for better spatial reasoning and a planner-critic mechanism to refine strategies over time. The results show that STMA significantly outperforms existing models in task success rates and scoring, demonstrating the importance of memory in embodied intelligence.'}, 'zh': {'title': '时空记忆智能体：提升智能体决策与适应能力的关键', 'desc': '本文提出了一种新的框架，称为时空记忆智能体（STMA），旨在提高智能体在动态环境中执行长期任务的能力。STMA集成了时空记忆模块、动态知识图谱和规划-评估机制，以增强任务规划和执行的效果。通过在TextWorld环境中进行32个任务的评估，STMA在成功率和平均得分上分别提高了31.25%和24.7%。实验结果表明，时空记忆在提升智能体的记忆能力方面具有显著效果。'}}}, {'id': 'https://huggingface.co/papers/2502.10392', 'title': 'Text-guided Sparse Voxel Pruning for Efficient 3D Visual Grounding', 'url': 'https://huggingface.co/papers/2502.10392', 'abstract': 'In this paper, we propose an efficient multi-level convolution architecture for 3D visual grounding. Conventional methods are difficult to meet the requirements of real-time inference due to the two-stage or point-based architecture. Inspired by the success of multi-level fully sparse convolutional architecture in 3D object detection, we aim to build a new 3D visual grounding framework following this technical route. However, as in 3D visual grounding task the 3D scene representation should be deeply interacted with text features, sparse convolution-based architecture is inefficient for this interaction due to the large amount of voxel features. To this end, we propose text-guided pruning (TGP) and completion-based addition (CBA) to deeply fuse 3D scene representation and text features in an efficient way by gradual region pruning and target completion. Specifically, TGP iteratively sparsifies the 3D scene representation and thus efficiently interacts the voxel features with text features by cross-attention. To mitigate the affect of pruning on delicate geometric information, CBA adaptively fixes the over-pruned region by voxel completion with negligible computational overhead. Compared with previous single-stage methods, our method achieves top inference speed and surpasses previous fastest method by 100\\% FPS. Our method also achieves state-of-the-art accuracy even compared with two-stage methods, with +1.13 lead of Acc@0.5 on ScanRefer, and +2.6 and +3.2 leads on NR3D and SR3D respectively. The code is available at https://github.com/GWxuan/TSP3D{https://github.com/GWxuan/TSP3D}.', 'score': 2, 'issue_id': 2252, 'pub_date': '2025-02-14', 'pub_date_card': {'ru': '14 февраля', 'en': 'February 14', 'zh': '2月14日'}, 'hash': '364eaebf9db5bd4a', 'authors': ['Wenxuan Guo', 'Xiuwei Xu', 'Ziwei Wang', 'Jianjiang Feng', 'Jie Zhou', 'Jiwen Lu'], 'affiliations': ['Nanyang Technological University', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2502.10392.jpg', 'data': {'categories': ['#3d', '#architecture', '#training', '#cv'], 'emoji': '🏆', 'ru': {'title': 'Эффективная 3D визуальная локализация с помощью многоуровневой сверточной архитектуры', 'desc': 'В статье предлагается эффективная многоуровневая сверточная архитектура для 3D визуальной локализации. Авторы вводят текстово-управляемое прореживание (TGP) и дополнение на основе завершения (CBA) для эффективного объединения 3D-представления сцены и текстовых признаков. Метод достигает высокой скорости вывода, превосходя предыдущий самый быстрый метод на 100% по FPS. Также достигается наилучшая точность по сравнению с двухэтапными методами на наборах данных ScanRefer, NR3D и SR3D.'}, 'en': {'title': 'Efficient 3D Visual Grounding with Multi-Level Convolution', 'desc': 'This paper introduces a new multi-level convolution architecture designed for 3D visual grounding, which is the task of linking 3D scenes with textual descriptions. Traditional methods struggle with real-time performance due to their complex two-stage or point-based designs. The authors leverage a multi-level fully sparse convolution approach, enhancing the interaction between 3D scene representations and text features through innovative techniques like text-guided pruning (TGP) and completion-based addition (CBA). Their method not only improves inference speed, achieving a 100% increase in frames per second (FPS) compared to the fastest existing methods, but also enhances accuracy on benchmark datasets, outperforming previous models.'}, 'zh': {'title': '高效融合3D场景与文本特征的视觉定位新方法', 'desc': '本文提出了一种高效的多层卷积架构，用于3D视觉定位。传统方法由于采用两阶段或基于点的架构，难以满足实时推理的要求。我们借鉴了多层稀疏卷积架构在3D物体检测中的成功，构建了新的3D视觉定位框架。通过文本引导修剪和基于补全的添加，我们有效地融合了3D场景表示和文本特征，显著提高了推理速度和准确性。'}}}, {'id': 'https://huggingface.co/papers/2502.07856', 'title': 'MRS: A Fast Sampler for Mean Reverting Diffusion based on ODE and SDE Solvers', 'url': 'https://huggingface.co/papers/2502.07856', 'abstract': 'In applications of diffusion models, controllable generation is of practical significance, but is also challenging. Current methods for controllable generation primarily focus on modifying the score function of diffusion models, while Mean Reverting (MR) Diffusion directly modifies the structure of the stochastic differential equation (SDE), making the incorporation of image conditions simpler and more natural. However, current training-free fast samplers are not directly applicable to MR Diffusion. And thus MR Diffusion requires hundreds of NFEs (number of function evaluations) to obtain high-quality samples. In this paper, we propose a new algorithm named MRS (MR Sampler) to reduce the sampling NFEs of MR Diffusion. We solve the reverse-time SDE and the probability flow ordinary differential equation (PF-ODE) associated with MR Diffusion, and derive semi-analytical solutions. The solutions consist of an analytical function and an integral parameterized by a neural network. Based on this solution, we can generate high-quality samples in fewer steps. Our approach does not require training and supports all mainstream parameterizations, including noise prediction, data prediction and velocity prediction. Extensive experiments demonstrate that MR Sampler maintains high sampling quality with a speedup of 10 to 20 times across ten different image restoration tasks. Our algorithm accelerates the sampling procedure of MR Diffusion, making it more practical in controllable generation.', 'score': 2, 'issue_id': 2244, 'pub_date': '2025-02-11', 'pub_date_card': {'ru': '11 февраля', 'en': 'February 11', 'zh': '2月11日'}, 'hash': '2bb6766a68f50cdc', 'authors': ['Ao Li', 'Wei Fang', 'Hongbo Zhao', 'Le Lu', 'Ge Yang', 'Minfeng Xu'], 'affiliations': ['DAMO Academy, Alibaba Group', 'Hupan Laboratory, Hangzhou, China', 'Institute of Automation, Chinese Academy of Sciences (CASIA)', 'School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2502.07856.jpg', 'data': {'categories': ['#training', '#cv', '#data', '#diffusion', '#optimization'], 'emoji': '🚀', 'ru': {'title': 'Ускорение управляемой генерации изображений без потери качества', 'desc': 'Статья представляет новый алгоритм MRS (MR Sampler) для ускорения процесса семплирования в Mean Reverting (MR) Diffusion моделях. Авторы решают обратное стохастическое дифференциальное уравнение и обыкновенное дифференциальное уравнение потока вероятности, связанные с MR Diffusion, и выводят полуаналитические решения. Этот подход не требует дополнительного обучения и поддерживает все основные параметризации, включая предсказание шума, данных и скорости. Эксперименты показывают, что MR Sampler сохраняет высокое качество семплирования при ускорении в 10-20 раз для десяти различных задач восстановления изображений.'}, 'en': {'title': 'Accelerating Controllable Generation with MR Sampler', 'desc': 'This paper introduces a new algorithm called MRS (MR Sampler) to improve the efficiency of sampling in Mean Reverting (MR) Diffusion models. Unlike traditional methods that modify the score function, MRS directly addresses the stochastic differential equation structure, simplifying the integration of image conditions. The proposed method achieves high-quality sample generation with significantly fewer function evaluations, enhancing the speed of the sampling process. Experimental results show that MRS can produce samples 10 to 20 times faster while maintaining quality across various image restoration tasks.'}, 'zh': {'title': '加速可控生成的MR采样器', 'desc': '在扩散模型的应用中，可控生成具有重要的实际意义，但也面临挑战。当前的可控生成方法主要集中在修改扩散模型的评分函数，而均值回归扩散（MR Diffusion）则直接修改随机微分方程（SDE）的结构，使得图像条件的结合更加简单自然。本文提出了一种新算法MRS（MR采样器），旨在减少MR扩散的采样函数评估次数（NFEs），并通过解决与MR扩散相关的反向时间SDE和概率流常微分方程（PF-ODE）来获得高质量样本。实验表明，MR采样器在十种不同的图像恢复任务中保持高采样质量，并实现了10到20倍的加速。'}}}, {'id': 'https://huggingface.co/papers/2502.09980', 'title': 'V2V-LLM: Vehicle-to-Vehicle Cooperative Autonomous Driving with Multi-Modal Large Language Models', 'url': 'https://huggingface.co/papers/2502.09980', 'abstract': 'Current autonomous driving vehicles rely mainly on their individual sensors to understand surrounding scenes and plan for future trajectories, which can be unreliable when the sensors are malfunctioning or occluded. To address this problem, cooperative perception methods via vehicle-to-vehicle (V2V) communication have been proposed, but they have tended to focus on detection and tracking. How those approaches contribute to overall cooperative planning performance is still under-explored. Inspired by recent progress using Large Language Models (LLMs) to build autonomous driving systems, we propose a novel problem setting that integrates an LLM into cooperative autonomous driving, with the proposed Vehicle-to-Vehicle Question-Answering (V2V-QA) dataset and benchmark. We also propose our baseline method Vehicle-to-Vehicle Large Language Model (V2V-LLM), which uses an LLM to fuse perception information from multiple connected autonomous vehicles (CAVs) and answer driving-related questions: grounding, notable object identification, and planning. Experimental results show that our proposed V2V-LLM can be a promising unified model architecture for performing various tasks in cooperative autonomous driving, and outperforms other baseline methods that use different fusion approaches. Our work also creates a new research direction that can improve the safety of future autonomous driving systems. Our project website: https://eddyhkchiu.github.io/v2vllm.github.io/ .', 'score': 1, 'issue_id': 2244, 'pub_date': '2025-02-14', 'pub_date_card': {'ru': '14 февраля', 'en': 'February 14', 'zh': '2月14日'}, 'hash': '57343c782d806dc0', 'authors': ['Hsu-kuang Chiu', 'Ryo Hachiuma', 'Chien-Yi Wang', 'Stephen F. Smith', 'Yu-Chiang Frank Wang', 'Min-Hung Chen'], 'affiliations': ['Carnegie Mellon University', 'NVIDIA'], 'pdf_title_img': 'assets/pdf/title_img/2502.09980.jpg', 'data': {'categories': ['#benchmark', '#architecture', '#reasoning', '#science', '#agi', '#games', '#optimization', '#dataset', '#agents'], 'emoji': '🚗', 'ru': {'title': 'LLM на службе кооперативного автономного вождения', 'desc': 'Статья представляет новый подход к кооперативному автономному вождению с использованием больших языковых моделей (LLM). Авторы предлагают датасет и бенчмарк V2V-QA для обмена информацией между автомобилями через вопросно-ответную систему. Их метод V2V-LLM использует LLM для объединения данных восприятия от нескольких подключенных автономных транспортных средств и ответа на вопросы, связанные с вождением. Эксперименты показывают, что V2V-LLM превосходит другие базовые методы и открывает новое направление исследований для повышения безопасности будущих систем автономного вождения.'}, 'en': {'title': 'Enhancing Cooperative Driving with Language Models', 'desc': 'This paper introduces a new approach to enhance cooperative autonomous driving by integrating Large Language Models (LLMs) with vehicle-to-vehicle (V2V) communication. The proposed method, called Vehicle-to-Vehicle Large Language Model (V2V-LLM), allows connected autonomous vehicles (CAVs) to share and fuse perception data, enabling them to answer driving-related questions effectively. The authors present a new dataset, Vehicle-to-Vehicle Question-Answering (V2V-QA), to benchmark this integration and demonstrate its effectiveness in improving planning and safety. Experimental results indicate that V2V-LLM outperforms existing methods, paving the way for a unified model architecture in cooperative driving systems.'}, 'zh': {'title': '协作自动驾驶的新方向：车辆间问答模型', 'desc': '当前的自动驾驶车辆主要依赖各自的传感器来理解周围场景和规划未来轨迹，但当传感器出现故障或被遮挡时，这种方法可能不可靠。为了解决这个问题，提出了通过车与车（V2V）通信的协作感知方法，但这些方法主要集中在检测和跟踪上。本文提出了一种新颖的问题设置，将大型语言模型（LLM）集成到协作自动驾驶中，并引入了车辆间问答（V2V-QA）数据集和基准测试。我们的实验结果表明，V2V-LLM能够有效融合多个连接的自动驾驶车辆的感知信息，并在协作自动驾驶中执行多种任务，提升未来自动驾驶系统的安全性。'}}}, {'id': 'https://huggingface.co/papers/2502.10362', 'title': 'CLaMP 3: Universal Music Information Retrieval Across Unaligned Modalities and Unseen Languages', 'url': 'https://huggingface.co/papers/2502.10362', 'abstract': 'CLaMP 3 is a unified framework developed to address challenges of cross-modal and cross-lingual generalization in music information retrieval. Using contrastive learning, it aligns all major music modalities--including sheet music, performance signals, and audio recordings--with multilingual text in a shared representation space, enabling retrieval across unaligned modalities with text as a bridge. It features a multilingual text encoder adaptable to unseen languages, exhibiting strong cross-lingual generalization. Leveraging retrieval-augmented generation, we curated M4-RAG, a web-scale dataset consisting of 2.31 million music-text pairs. This dataset is enriched with detailed metadata that represents a wide array of global musical traditions. To advance future research, we release WikiMT-X, a benchmark comprising 1,000 triplets of sheet music, audio, and richly varied text descriptions. Experiments show that CLaMP 3 achieves state-of-the-art performance on multiple MIR tasks, significantly surpassing previous strong baselines and demonstrating excellent generalization in multimodal and multilingual music contexts.', 'score': 0, 'issue_id': 2253, 'pub_date': '2025-02-14', 'pub_date_card': {'ru': '14 февраля', 'en': 'February 14', 'zh': '2月14日'}, 'hash': '01dc60f8f9db0ad7', 'authors': ['Shangda Wu', 'Zhancheng Guo', 'Ruibin Yuan', 'Junyan Jiang', 'Seungheon Doh', 'Gus Xia', 'Juhan Nam', 'Xiaobing Li', 'Feng Yu', 'Maosong Sun'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2502.10362.jpg', 'data': {'categories': ['#multimodal', '#transfer_learning', '#multilingual', '#rag', '#low_resource', '#data', '#benchmark', '#dataset'], 'emoji': '🎵', 'ru': {'title': 'Единая платформа для мультимодального и многоязычного анализа музыки', 'desc': 'CLaMP 3 - это унифицированная система для решения задач кросс-модальной и кросс-языковой генерализации в области извлечения музыкальной информации. Она использует контрастное обучение для объединения основных музыкальных модальностей (ноты, сигналы исполнения, аудиозаписи) с многоязычным текстом в едином пространстве представлений. Система включает многоязычный текстовый энкодер, адаптируемый к новым языкам, и демонстрирует сильную кросс-языковую генерализацию. Эксперименты показывают, что CLaMP 3 достигает наилучших результатов в различных задачах MIR, значительно превосходя предыдущие сильные базовые модели.'}, 'en': {'title': 'Bridging Music and Language with CLaMP 3', 'desc': "CLaMP 3 is a new framework designed to improve how we retrieve music information across different formats and languages. It uses contrastive learning to connect various music types, like sheet music and audio, with text in multiple languages, allowing for better searches even when the formats don't match. The framework includes a multilingual text encoder that can adapt to new languages, showing its ability to generalize across different linguistic contexts. Additionally, it introduces a large dataset and benchmark to support further research in music information retrieval, achieving top performance in various tasks."}, 'zh': {'title': '跨模态音乐检索的新突破', 'desc': 'CLaMP 3 是一个统一框架，旨在解决音乐信息检索中的跨模态和跨语言泛化挑战。它通过对比学习，将乐谱、表演信号和音频录音等主要音乐模态与多语言文本对齐，形成共享表示空间，从而实现通过文本作为桥梁的检索。该框架具有适应未见语言的多语言文本编码器，展现出强大的跨语言泛化能力。通过增强检索生成，我们创建了 M4-RAG 数据集，包含 231 万对音乐-文本对，并发布了 WikiMT-X 基准，推动未来研究。'}}}, {'id': 'https://huggingface.co/papers/2502.08769', 'title': 'Cluster and Predict Latents Patches for Improved Masked Image Modeling', 'url': 'https://huggingface.co/papers/2502.08769', 'abstract': 'Masked Image Modeling (MIM) offers a promising approach to self-supervised representation learning, however existing MIM models still lag behind the state-of-the-art. In this paper, we systematically analyze target representations, loss functions, and architectures, to introduce CAPI - a novel pure-MIM framework that relies on the prediction of latent clusterings. Our approach leverages a clustering-based loss, which is stable to train, and exhibits promising scaling properties. Our ViT-L backbone, CAPI, achieves 83.8% accuracy on ImageNet and 32.1% mIoU on ADE20K with simple linear probes, substantially outperforming previous MIM methods and approaching the performance of the current state-of-the-art, DINOv2. We release all our code and models.', 'score': 0, 'issue_id': 2250, 'pub_date': '2025-02-12', 'pub_date_card': {'ru': '12 февраля', 'en': 'February 12', 'zh': '2月12日'}, 'hash': '8fd9852310af51f5', 'authors': ['Timothée Darcet', 'Federico Baldassarre', 'Maxime Oquab', 'Julien Mairal', 'Piotr Bojanowski'], 'affiliations': ['CNRS', 'Grenoble INP', 'Inria', 'LJK', 'Meta', 'Univ. Grenoble Alpes'], 'pdf_title_img': 'assets/pdf/title_img/2502.08769.jpg', 'data': {'categories': ['#optimization', '#cv', '#training', '#architecture', '#dataset', '#open_source'], 'emoji': '🧠', 'ru': {'title': 'CAPI: Прорыв в самообучаемом представлении изображений', 'desc': 'Статья представляет CAPI - новую систему для самообучаемого представления изображений на основе маскированного моделирования изображений (MIM). CAPI использует прогнозирование латентных кластеризаций и новую функцию потерь на основе кластеризации. Модель достигает точности 83.8% на ImageNet и 32.1% mIoU на ADE20K с использованием простых линейных проб. CAPI значительно превосходит предыдущие методы MIM и приближается к производительности современного state-of-the-art метода DINOv2.'}, 'en': {'title': 'CAPI: Clustering for Superior Masked Image Modeling', 'desc': 'This paper presents CAPI, a new framework for Masked Image Modeling (MIM) that enhances self-supervised learning by focusing on predicting latent clusterings. The authors analyze various aspects of MIM, including target representations and loss functions, to develop a clustering-based loss that is stable during training. CAPI utilizes a Vision Transformer (ViT-L) backbone, achieving impressive results with 83.8% accuracy on ImageNet and 32.1% mean Intersection over Union (mIoU) on ADE20K. The framework significantly outperforms previous MIM methods and approaches the performance of the leading model, DINOv2, while the authors provide all code and models for further research.'}, 'zh': {'title': 'CAPI：提升自监督学习的新方法', 'desc': '本文提出了一种新的纯遮罩图像建模框架CAPI，旨在提升自监督表示学习的效果。我们系统分析了目标表示、损失函数和架构，提出了一种基于聚类的损失函数，使得训练过程更加稳定。CAPI在ViT-L骨干网络上实现了83.8%的ImageNet准确率和32.1%的ADE20K mIoU，显著超越了之前的MIM方法。我们将所有代码和模型公开，促进研究的进一步发展。'}}}, {'id': 'https://huggingface.co/papers/2502.10173', 'title': 'Agentic End-to-End De Novo Protein Design for Tailored Dynamics Using a Language Diffusion Model', 'url': 'https://huggingface.co/papers/2502.10173', 'abstract': 'Proteins are dynamic molecular machines whose biological functions, spanning enzymatic catalysis, signal transduction, and structural adaptation, are intrinsically linked to their motions. Designing proteins with targeted dynamic properties, however, remains a challenge due to the complex, degenerate relationships between sequence, structure, and molecular motion. Here, we introduce VibeGen, a generative AI framework that enables end-to-end de novo protein design conditioned on normal mode vibrations. VibeGen employs an agentic dual-model architecture, comprising a protein designer that generates sequence candidates based on specified vibrational modes and a protein predictor that evaluates their dynamic accuracy. This approach synergizes diversity, accuracy, and novelty during the design process. Via full-atom molecular simulations as direct validation, we demonstrate that the designed proteins accurately reproduce the prescribed normal mode amplitudes across the backbone while adopting various stable, functionally relevant structures. Notably, generated sequences are de novo, exhibiting no significant similarity to natural proteins, thereby expanding the accessible protein space beyond evolutionary constraints. Our work integrates protein dynamics into generative protein design, and establishes a direct, bidirectional link between sequence and vibrational behavior, unlocking new pathways for engineering biomolecules with tailored dynamical and functional properties. This framework holds broad implications for the rational design of flexible enzymes, dynamic scaffolds, and biomaterials, paving the way toward dynamics-informed AI-driven protein engineering.', 'score': 0, 'issue_id': 2247, 'pub_date': '2025-02-14', 'pub_date_card': {'ru': '14 февраля', 'en': 'February 14', 'zh': '2月14日'}, 'hash': '84102aa522298331', 'authors': ['Bo Ni', 'Markus J. Buehler'], 'affiliations': ['Center for Computational Science and Engineering, Schwarzman College of Computing, Massachusetts Institute of Technology, Cambridge, MA, USA', 'Department of Materials Science and Engineering, Carnegie Mellon University, Pittsburgh, PA, USA', 'Laboratory for Atomistic and Molecular Mechanics (LAMM), Massachusetts Institute of Technology, Cambridge, MA, USA'], 'pdf_title_img': 'assets/pdf/title_img/2502.10173.jpg', 'data': {'categories': ['#architecture', '#agents', '#dataset'], 'emoji': '🧬', 'ru': {'title': 'VibeGen: ИИ-дизайн белков с заданной динамикой', 'desc': 'VibeGen - это генеративная ИИ-система для проектирования белков с заданными динамическими свойствами. Она использует двойную архитектуру с моделью-дизайнером, генерирующей последовательности белков, и моделью-предиктором, оценивающей их динамическую точность. VibeGen способна создавать de novo белки с заданными нормальными модами колебаний, что подтверждается полноатомным молекулярным моделированием. Эта система открывает новые возможности для инженерии биомолекул с заданными динамическими и функциональными свойствами.'}, 'en': {'title': 'Unlocking Dynamic Protein Design with VibeGen', 'desc': 'This paper presents VibeGen, a generative AI framework designed for creating proteins with specific dynamic properties. It utilizes a dual-model architecture that includes a protein designer to generate sequences based on desired vibrational modes and a protein predictor to assess their dynamic accuracy. The framework successfully integrates protein dynamics into the design process, allowing for the creation of novel protein sequences that do not resemble existing natural proteins. This innovation opens new avenues for engineering proteins with tailored functions and dynamics, which could significantly impact fields like enzyme design and biomaterials.'}, 'zh': {'title': '动态驱动的蛋白质设计新方法', 'desc': '这篇论文介绍了一种名为VibeGen的生成性人工智能框架，用于设计具有特定动态特性的蛋白质。VibeGen结合了蛋白质设计器和蛋白质预测器，前者根据指定的振动模式生成序列候选，后者评估其动态准确性。通过全原子分子模拟验证，设计的蛋白质能够准确再现预定的正常模式振幅，并采用多种稳定的、功能相关的结构。该方法不仅扩展了可设计蛋白质的空间，还为灵活酶、动态支架和生物材料的理性设计提供了新的途径。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (1)', '#agents (4)', '#agi (1)', '#alignment (2)', '#architecture (9)', '#audio', '#benchmark (7)', '#cv (6)', '#data (4)', '#dataset (7)', '#diffusion (6)', '#ethics (1)', '#games (2)', '#graphs', '#hallucinations', '#healthcare', '#inference (3)', '#interpretability (3)', '#leakage', '#long_context', '#low_resource (2)', '#machine_translation', '#math (1)', '#multilingual (3)', '#multimodal (4)', '#open_source (3)', '#optimization (9)', '#plp', '#rag (2)', '#reasoning (4)', '#rl (1)', '#rlhf (2)', '#robotics', '#science (1)', '#security (1)', '#small_models (2)', '#story_generation', '#survey', '#synthetic (2)', '#training (14)', '#transfer_learning (3)', '#video (1)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-02-17 16:12',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-02-17 16:12')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-02-17 16:12')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    