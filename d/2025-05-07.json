{
    "date": {
        "ru": "7 Ğ¼Ğ°Ñ",
        "en": "May 7",
        "zh": "5æœˆ7æ—¥"
    },
    "time_utc": "2025-05-07 06:16",
    "weekday": 2,
    "issue_id": 3628,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2505.03318",
            "title": "Unified Multimodal Chain-of-Thought Reward Model through Reinforcement\n  Fine-Tuning",
            "url": "https://huggingface.co/papers/2505.03318",
            "abstract": "Recent advances in multimodal Reward Models (RMs) have shown significant promise in delivering reward signals to align vision models with human preferences. However, current RMs are generally restricted to providing direct responses or engaging in shallow reasoning processes with limited depth, often leading to inaccurate reward signals. We posit that incorporating explicit long chains of thought (CoT) into the reward reasoning process can significantly strengthen their reliability and robustness. Furthermore, we believe that once RMs internalize CoT reasoning, their direct response accuracy can also be improved through implicit reasoning capabilities. To this end, this paper proposes UnifiedReward-Think, the first unified multimodal CoT-based reward model, capable of multi-dimensional, step-by-step long-chain reasoning for both visual understanding and generation reward tasks. Specifically, we adopt an exploration-driven reinforcement fine-tuning approach to elicit and incentivize the model's latent complex reasoning ability: (1) We first use a small amount of image generation preference data to distill the reasoning process of GPT-4o, which is then used for the model's cold start to learn the format and structure of CoT reasoning. (2) Subsequently, by leveraging the model's prior knowledge and generalization capabilities, we prepare large-scale unified multimodal preference data to elicit the model's reasoning process across various vision tasks. During this phase, correct reasoning outputs are retained for rejection sampling to refine the model (3) while incorrect predicted samples are finally used for Group Relative Policy Optimization (GRPO) based reinforcement fine-tuning, enabling the model to explore diverse reasoning paths and optimize for correct and robust solutions. Extensive experiments across various vision reward tasks demonstrate the superiority of our model.",
            "score": 40,
            "issue_id": 3624,
            "pub_date": "2025-05-06",
            "pub_date_card": {
                "ru": "6 Ğ¼Ğ°Ñ",
                "en": "May 6",
                "zh": "5æœˆ6æ—¥"
            },
            "hash": "f0871f80f0b8fdd9",
            "authors": [
                "Yibin Wang",
                "Zhimin Li",
                "Yuhang Zang",
                "Chunyu Wang",
                "Qinglin Lu",
                "Cheng Jin",
                "Jiaqi Wang"
            ],
            "affiliations": [
                "Fudan University",
                "Hunyuan, Tencent",
                "Shanghai AI Lab",
                "Shanghai Innovation Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.03318.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#alignment",
                    "#multimodal",
                    "#training",
                    "#optimization",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ UnifiedReward-Think - Ğ¿ĞµÑ€Ğ²Ğ°Ñ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (CoT). ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼ĞµÑ€Ğ½Ñ‹Ğµ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ ÑÑ‚Ğ¸Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğº ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Empowering Vision Models with Long-Chain Reasoning",
                    "desc": "This paper introduces UnifiedReward-Think, a novel multimodal reward model that enhances the alignment of vision models with human preferences through long-chain reasoning. By integrating explicit chains of thought (CoT) into the reward reasoning process, the model improves the accuracy and reliability of reward signals. The approach involves a two-step training process: first, distilling reasoning from a small dataset, and then fine-tuning with large-scale multimodal preference data. Experimental results show that this method significantly outperforms existing models in various vision tasks, demonstrating its effectiveness in complex reasoning scenarios."
                },
                "zh": {
                    "title": "é•¿é“¾æ€ç»´æå‡å¤šæ¨¡æ€å¥–åŠ±æ¨¡å‹çš„å¯é æ€§",
                    "desc": "æœ€è¿‘åœ¨å¤šæ¨¡æ€å¥–åŠ±æ¨¡å‹ï¼ˆRMsï¼‰æ–¹é¢çš„è¿›å±•æ˜¾ç¤ºå‡ºå°†è§†è§‰æ¨¡å‹ä¸äººç±»åå¥½å¯¹é½çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œç›®å‰çš„RMsé€šå¸¸åªèƒ½æä¾›ç›´æ¥å“åº”æˆ–è¿›è¡Œæµ…å±‚æ¨ç†ï¼Œå¯¼è‡´å¥–åŠ±ä¿¡å·ä¸å‡†ç¡®ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œå°†æ˜ç¡®çš„é•¿é“¾æ€ç»´ï¼ˆCoTï¼‰çº³å…¥å¥–åŠ±æ¨ç†è¿‡ç¨‹å¯ä»¥æ˜¾è‘—å¢å¼ºå…¶å¯é æ€§å’Œç¨³å¥æ€§ã€‚æœ¬æ–‡æå‡ºäº†UnifiedReward-Thinkï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç»Ÿä¸€çš„åŸºäºCoTçš„å¤šæ¨¡æ€å¥–åŠ±æ¨¡å‹ï¼Œèƒ½å¤Ÿè¿›è¡Œå¤šç»´åº¦ã€é€æ­¥çš„é•¿é“¾æ¨ç†ï¼Œé€‚ç”¨äºè§†è§‰ç†è§£å’Œç”Ÿæˆå¥–åŠ±ä»»åŠ¡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.03335",
            "title": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data",
            "url": "https://huggingface.co/papers/2505.03335",
            "abstract": "Reinforcement learning with verifiable rewards (RLVR) has shown promise in enhancing the reasoning capabilities of large language models by learning directly from outcome-based rewards. Recent RLVR works that operate under the zero setting avoid supervision in labeling the reasoning process, but still depend on manually curated collections of questions and answers for training. The scarcity of high-quality, human-produced examples raises concerns about the long-term scalability of relying on human supervision, a challenge already evident in the domain of language model pretraining. Furthermore, in a hypothetical future where AI surpasses human intelligence, tasks provided by humans may offer limited learning potential for a superintelligent system. To address these concerns, we propose a new RLVR paradigm called Absolute Zero, in which a single model learns to propose tasks that maximize its own learning progress and improves reasoning by solving them, without relying on any external data. Under this paradigm, we introduce the Absolute Zero Reasoner (AZR), a system that self-evolves its training curriculum and reasoning ability by using a code executor to both validate proposed code reasoning tasks and verify answers, serving as an unified source of verifiable reward to guide open-ended yet grounded learning. Despite being trained entirely without external data, AZR achieves overall SOTA performance on coding and mathematical reasoning tasks, outperforming existing zero-setting models that rely on tens of thousands of in-domain human-curated examples. Furthermore, we demonstrate that AZR can be effectively applied across different model scales and is compatible with various model classes.",
            "score": 36,
            "issue_id": 3624,
            "pub_date": "2025-05-06",
            "pub_date_card": {
                "ru": "6 Ğ¼Ğ°Ñ",
                "en": "May 6",
                "zh": "5æœˆ6æ—¥"
            },
            "hash": "b53e736d1884218d",
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "categories": [
                    "#rlhf",
                    "#training",
                    "#rl",
                    "#math",
                    "#optimization",
                    "#reasoning"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ¡Ğ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ÑÑ Ğ˜Ğ˜: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°Ğ¼Ğ¸ (RLVR) Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Absolute Zero. Ğ’ Ñ€Ğ°Ğ¼ĞºĞ°Ñ… ÑÑ‚Ğ¾Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ´Ğ»Ñ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ½Ğµ Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ÑÑÑŒ Ğ½Ğ° Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Absolute Zero Reasoner (AZR), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ²Ğ¾Ñ ÑƒÑ‡ĞµĞ±Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ñƒ Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒ ĞºĞ¾Ğ´Ğ° Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². ĞĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸, AZR Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Self-Learning AI: No Data, No Problem!",
                    "desc": "This paper introduces a new approach in reinforcement learning called Absolute Zero, which allows a model to learn and improve its reasoning skills without needing external data or human supervision. The proposed Absolute Zero Reasoner (AZR) autonomously generates tasks that enhance its learning and validates its own reasoning through a code executor. This self-sufficient learning method leads to state-of-the-art performance in coding and mathematical reasoning tasks, surpassing models that rely on large datasets of human-created examples. The findings suggest that AZR can adapt to different model sizes and types, showcasing its versatility and potential for future AI development."
                },
                "zh": {
                    "title": "ç»å¯¹é›¶ï¼šè‡ªæˆ‘è¿›åŒ–çš„æ¨ç†æ¨¡å‹",
                    "desc": "å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿç›´æ¥ä»ç»“æœå¯¼å‘çš„å¥–åŠ±ä¸­å­¦ä¹ ã€‚æœ€è¿‘çš„RLVRç ”ç©¶åœ¨é›¶è®¾ç½®ä¸‹è¿è¡Œï¼Œé¿å…äº†å¯¹æ¨ç†è¿‡ç¨‹çš„ç›‘ç£ï¼Œä½†ä»ä¾èµ–äºäººå·¥ç­–åˆ’çš„é—®é¢˜å’Œç­”æ¡ˆé›†åˆè¿›è¡Œè®­ç»ƒã€‚ç”±äºé«˜è´¨é‡äººç±»ç”Ÿæˆç¤ºä¾‹çš„ç¨€ç¼ºæ€§ï¼Œä¾èµ–äººç±»ç›‘ç£çš„é•¿æœŸå¯æ‰©å±•æ€§å—åˆ°è´¨ç–‘ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„RLVRèŒƒå¼ï¼Œç§°ä¸ºç»å¯¹é›¶ï¼ˆAbsolute Zeroï¼‰ï¼Œè¯¥èŒƒå¼ä¸‹çš„æ¨¡å‹èƒ½å¤Ÿè‡ªæˆ‘æå‡ºä»»åŠ¡ä»¥æœ€å¤§åŒ–å­¦ä¹ è¿›å±•ï¼Œå¹¶é€šè¿‡è§£å†³è¿™äº›ä»»åŠ¡æ¥æå‡æ¨ç†èƒ½åŠ›ï¼Œè€Œæ— éœ€ä¾èµ–ä»»ä½•å¤–éƒ¨æ•°æ®ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.03730",
            "title": "FlexiAct: Towards Flexible Action Control in Heterogeneous Scenarios",
            "url": "https://huggingface.co/papers/2505.03730",
            "abstract": "Action customization involves generating videos where the subject performs actions dictated by input control signals. Current methods use pose-guided or global motion customization but are limited by strict constraints on spatial structure, such as layout, skeleton, and viewpoint consistency, reducing adaptability across diverse subjects and scenarios. To overcome these limitations, we propose FlexiAct, which transfers actions from a reference video to an arbitrary target image. Unlike existing methods, FlexiAct allows for variations in layout, viewpoint, and skeletal structure between the subject of the reference video and the target image, while maintaining identity consistency. Achieving this requires precise action control, spatial structure adaptation, and consistency preservation. To this end, we introduce RefAdapter, a lightweight image-conditioned adapter that excels in spatial adaptation and consistency preservation, surpassing existing methods in balancing appearance consistency and structural flexibility. Additionally, based on our observations, the denoising process exhibits varying levels of attention to motion (low frequency) and appearance details (high frequency) at different timesteps. So we propose FAE (Frequency-aware Action Extraction), which, unlike existing methods that rely on separate spatial-temporal architectures, directly achieves action extraction during the denoising process. Experiments demonstrate that our method effectively transfers actions to subjects with diverse layouts, skeletons, and viewpoints. We release our code and model weights to support further research at https://shiyi-zh0408.github.io/projectpages/FlexiAct/",
            "score": 15,
            "issue_id": 3624,
            "pub_date": "2025-05-06",
            "pub_date_card": {
                "ru": "6 Ğ¼Ğ°Ñ",
                "en": "May 6",
                "zh": "5æœˆ6æ—¥"
            },
            "hash": "2329fe6d2462c9c9",
            "authors": [
                "Shiyi Zhang",
                "Junhao Zhuang",
                "Zhaoyang Zhang",
                "Ying Shan",
                "Yansong Tang"
            ],
            "affiliations": [
                "Tencent ARC Lab, China",
                "Tsinghua Shenzhen International Graduate School, Tsinghua University, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.03730.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#transfer_learning",
                    "#multimodal",
                    "#video"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "Ğ“Ğ¸Ğ±ĞºĞ¸Ğ¹ Ğ¿ĞµÑ€ĞµĞ½Ğ¾Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸ Ğ¸ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑĞ¼Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ FlexiAct - Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ° Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ñ Ñ€ĞµÑ„ĞµÑ€ĞµĞ½ÑĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½Ğ¾Ğµ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², FlexiAct Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ²Ğ°Ñ€ÑŒĞ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½Ğ¾Ğ²ĞºÑƒ, Ñ€Ğ°ĞºÑƒÑ€Ñ Ğ¸ ÑĞºĞµĞ»ĞµÑ‚Ğ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ¾Ğ¼ Ñ€ĞµÑ„ĞµÑ€ĞµĞ½ÑĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğ¼ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ĞµĞ¼, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ. Ğ”Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ñ†ĞµĞ»Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ RefAdapter - Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€, Ğ¾Ğ±ÑƒÑĞ»Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²ĞºĞµ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ³Ğ¾ Ğ²Ğ¸Ğ´Ğ° Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ğ¾Ğ¹ Ğ³Ğ¸Ğ±ĞºĞ¾ÑÑ‚Ğ¸. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ FAE (Frequency-aware Action Extraction) Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ½ĞµĞ¿Ğ¾ÑÑ€ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "FlexiAct: Flexible Action Transfer for Diverse Video Customization",
                    "desc": "The paper presents FlexiAct, a novel approach for customizing action videos by transferring actions from a reference video to a target image, regardless of differences in layout, viewpoint, and skeletal structure. This method addresses the limitations of existing techniques that require strict spatial consistency, allowing for greater adaptability across various subjects and scenarios. FlexiAct utilizes a lightweight image-conditioned adapter called RefAdapter to ensure identity consistency while adapting spatial structures. Additionally, it introduces Frequency-aware Action Extraction (FAE) to enhance action extraction during the denoising process, achieving superior results in maintaining both appearance and structural flexibility."
                },
                "zh": {
                    "title": "çµæ´»çš„åŠ¨ä½œè½¬ç§»ï¼Œæ‰“ç ´ç©ºé—´é™åˆ¶",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºFlexiActçš„æ–¹æ³•ï¼Œç”¨äºæ ¹æ®è¾“å…¥æ§åˆ¶ä¿¡å·ç”Ÿæˆè§†é¢‘ï¼Œå…è®¸åœ¨ä¸åŒå¸ƒå±€ã€è§†è§’å’Œéª¨æ¶ç»“æ„ä¹‹é—´è¿›è¡ŒåŠ¨ä½œè½¬ç§»ã€‚ä¸ç°æœ‰æ–¹æ³•ä¸åŒï¼ŒFlexiActèƒ½å¤Ÿåœ¨ä¿æŒèº«ä»½ä¸€è‡´æ€§çš„åŒæ—¶ï¼Œé€‚åº”ç›®æ ‡å›¾åƒçš„ç©ºé—´ç»“æ„å˜åŒ–ã€‚ä¸ºå®ç°è¿™ä¸€ç›®æ ‡ï¼Œæ–‡ç« å¼•å…¥äº†RefAdapterï¼Œä¸€ä¸ªè½»é‡çº§çš„å›¾åƒæ¡ä»¶é€‚é…å™¨ï¼Œèƒ½å¤Ÿåœ¨å¤–è§‚ä¸€è‡´æ€§å’Œç»“æ„çµæ´»æ€§ä¹‹é—´å–å¾—è‰¯å¥½å¹³è¡¡ã€‚æ­¤å¤–ï¼Œæå‡ºçš„FAEæ–¹æ³•åœ¨å»å™ªè¿‡ç¨‹ä¸­ç›´æ¥æå–åŠ¨ä½œï¼Œå…‹æœäº†ä¼ ç»Ÿæ–¹æ³•çš„å±€é™æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.03005",
            "title": "RADLADS: Rapid Attention Distillation to Linear Attention Decoders at\n  Scale",
            "url": "https://huggingface.co/papers/2505.03005",
            "abstract": "We present Rapid Attention Distillation to Linear Attention Decoders at Scale (RADLADS), a protocol for rapidly converting softmax attention transformers into linear attention decoder models, along with two new RWKV-variant architectures, and models converted from popular Qwen2.5 open source models in 7B, 32B, and 72B sizes. Our conversion process requires only 350-700M tokens, less than 0.005% of the token count used to train the original teacher models. Converting to our 72B linear attention model costs less than \\$2,000 USD at today's prices, yet quality at inference remains close to the original transformer. These models achieve state-of-the-art downstream performance across a set of standard benchmarks for linear attention models of their size. We release all our models on HuggingFace under the Apache 2.0 license, with the exception of our 72B models which are also governed by the Qwen License Agreement.   Models at https://huggingface.co/collections/recursal/radlads-6818ee69e99e729ba8a87102 Training Code at https://github.com/recursal/RADLADS-paper",
            "score": 11,
            "issue_id": 3625,
            "pub_date": "2025-05-05",
            "pub_date_card": {
                "ru": "5 Ğ¼Ğ°Ñ",
                "en": "May 5",
                "zh": "5æœˆ5æ—¥"
            },
            "hash": "0fe1c0b1575b6708",
            "authors": [
                "Daniel Goldstein",
                "Eric Alcaide",
                "Janna Lu",
                "Eugene Cheah"
            ],
            "affiliations": [
                "Dalle Molle Institute for Artificial Intelligence USI-SUPSI",
                "EleutherAI",
                "George Mason University",
                "Recursal AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.03005.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#open_source",
                    "#inference",
                    "#benchmark",
                    "#optimization"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Rapid Attention Distillation to Linear Attention Decoders at Scale (RADLADS) Ğ´Ğ»Ñ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ñ ÑĞ¾Ñ„Ñ‚Ğ¼Ğ°ĞºÑ-Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ² Ñ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ´Ğ²Ğµ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ RWKV Ğ¸ ĞºĞ¾Ğ½Ğ²ĞµÑ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Qwen2.5 Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ 7B, 32B Ğ¸ 72B. ĞŸÑ€Ğ¾Ñ†ĞµÑÑ ĞºĞ¾Ğ½Ğ²ĞµÑ€Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ²ÑĞµĞ³Ğ¾ 350-700 Ğ¼Ğ»Ğ½ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ ÑĞ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµĞ½ĞµĞµ 0,005% Ğ¾Ñ‚ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…."
                },
                "en": {
                    "title": "Transforming Transformers: Efficient Linear Attention Models with RADLADS",
                    "desc": "The paper introduces Rapid Attention Distillation to Linear Attention Decoders at Scale (RADLADS), a method for transforming softmax attention transformers into efficient linear attention models. This conversion process is highly efficient, requiring only a small fraction of the original training data, specifically 350-700M tokens. Despite the reduced training cost, the resulting 72B linear attention model maintains performance levels comparable to its transformer counterparts. The authors also present new RWKV-variant architectures and make their models available on HuggingFace, demonstrating state-of-the-art results on standard benchmarks for linear attention models."
                },
                "zh": {
                    "title": "å¿«é€Ÿè½¬æ¢ï¼Œçº¿æ€§æ³¨æ„åŠ›çš„æœªæ¥",
                    "desc": "æˆ‘ä»¬æå‡ºäº†ä¸€ç§å¿«é€Ÿæ³¨æ„åŠ›è’¸é¦åˆ°çº¿æ€§æ³¨æ„è§£ç å™¨çš„åè®®ï¼ˆRADLADSï¼‰ï¼Œå¯ä»¥è¿…é€Ÿå°†è½¯æœ€å¤§æ³¨æ„åŠ›å˜æ¢å™¨è½¬æ¢ä¸ºçº¿æ€§æ³¨æ„è§£ç æ¨¡å‹ã€‚æˆ‘ä»¬çš„è½¬æ¢è¿‡ç¨‹åªéœ€350-700Mä¸ªæ ‡è®°ï¼Œè¿œä½äºåŸå§‹æ•™å¸ˆæ¨¡å‹è®­ç»ƒæ‰€éœ€çš„0.005%çš„æ ‡è®°æ•°é‡ã€‚è½¬æ¢ä¸ºæˆ‘ä»¬çš„72Bçº¿æ€§æ³¨æ„æ¨¡å‹çš„æˆæœ¬ä¸åˆ°2000ç¾å…ƒï¼Œä½†æ¨ç†è´¨é‡ä»æ¥è¿‘åŸå§‹å˜æ¢å™¨ã€‚æˆ‘ä»¬åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†åŒç±»æœ€ä½³çš„ä¸‹æ¸¸æ€§èƒ½ï¼Œå¹¶å°†æ‰€æœ‰æ¨¡å‹å‘å¸ƒåœ¨HuggingFaceä¸Šã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.02922",
            "title": "RetroInfer: A Vector-Storage Approach for Scalable Long-Context LLM\n  Inference",
            "url": "https://huggingface.co/papers/2505.02922",
            "abstract": "The growing context lengths of large language models (LLMs) pose significant challenges for efficient inference, primarily due to GPU memory and bandwidth constraints. We present RetroInfer, a novel system that reconceptualizes the key-value (KV) cache as a vector storage system which exploits the inherent attention sparsity to accelerate long-context LLM inference. At its core is the wave index, an Attention-aWare VEctor index that enables efficient and accurate retrieval of critical tokens through techniques such as tripartite attention approximation, accuracy-bounded attention estimation, and segmented clustering. Complementing this is the wave buffer, which coordinates KV cache placement and overlaps computation and data transfer across GPU and CPU to sustain high throughput. Unlike prior sparsity-based methods that struggle with token selection and hardware coordination, RetroInfer delivers robust performance without compromising model accuracy. Experiments on long-context benchmarks show up to 4.5X speedup over full attention within GPU memory limits and up to 10.5X over sparse attention baselines when KV cache is extended to CPU memory, all while preserving full-attention-level accuracy.",
            "score": 8,
            "issue_id": 3624,
            "pub_date": "2025-05-05",
            "pub_date_card": {
                "ru": "5 Ğ¼Ğ°Ñ",
                "en": "May 5",
                "zh": "5æœˆ5æ—¥"
            },
            "hash": "d7e3545dcade10b4",
            "authors": [
                "Yaoqi Chen",
                "Jinkai Zhang",
                "Baotong Lu",
                "Qianxi Zhang",
                "Chengruidong Zhang",
                "Jingjia Luo",
                "Di Liu",
                "Huiqiang Jiang",
                "Qi Chen",
                "Jing Liu",
                "Bailu Ding",
                "Xiao Yan",
                "Jiawei Jiang",
                "Chen Chen",
                "Mingxing Zhang",
                "Yuqing Yang",
                "Fan Yang",
                "Mao Yang"
            ],
            "affiliations": [
                "Microsoft Research",
                "Shanghai Jiao Tong University",
                "Tsinghua University",
                "University of Science and Technology of China",
                "Wuhan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.02922.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#inference",
                    "#benchmark",
                    "#architecture",
                    "#optimization"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "RetroInfer: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° LLM Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼",
                    "desc": "RetroInfer - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿ĞµÑ€ĞµĞ¾ÑĞ¼Ñ‹ÑĞ»Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºÑÑˆ ĞºĞ»ÑÑ‡-Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ ĞºĞ°Ğº ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° LLM Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼. Ğ’ Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ»ĞµĞ¶Ğ¸Ñ‚ Ğ²Ğ¾Ğ»Ğ½Ğ¾Ğ²Ğ¾Ğ¹ Ğ¸Ğ½Ğ´ĞµĞºÑ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². RetroInfer Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ²Ğ¾Ğ»Ğ½Ğ¾Ğ²Ğ¾Ğ¹ Ğ±ÑƒÑ„ĞµÑ€ Ğ´Ğ»Ñ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ ĞºÑÑˆĞ° Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ GPU Ğ¸ CPU. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ´Ğ¾ 4.5 Ñ€Ğ°Ğ· Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ² Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ°Ñ… Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ GPU Ğ¸ Ğ´Ğ¾ 10.5 Ñ€Ğ°Ğ· Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Accelerating Long-Context Inference with RetroInfer",
                    "desc": "This paper introduces RetroInfer, a system designed to improve the efficiency of large language models (LLMs) during inference by addressing GPU memory and bandwidth limitations. It innovatively redefines the key-value (KV) cache as a vector storage system that leverages attention sparsity to speed up processing of long contexts. The core component, the wave index, utilizes advanced techniques for token retrieval, ensuring both efficiency and accuracy. Additionally, the wave buffer optimizes the coordination of KV cache and computation, achieving significant speed improvements while maintaining high model accuracy."
                },
                "zh": {
                    "title": "é«˜æ•ˆæ¨ç†ï¼Œçªç ´ä¸Šä¸‹æ–‡é™åˆ¶ï¼",
                    "desc": "éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸Šä¸‹æ–‡é•¿åº¦çš„å¢åŠ ï¼Œæ¨ç†æ•ˆç‡é¢ä¸´æ˜¾è‘—æŒ‘æˆ˜ï¼Œä¸»è¦æ˜¯ç”±äºGPUå†…å­˜å’Œå¸¦å®½çš„é™åˆ¶ã€‚æˆ‘ä»¬æå‡ºäº†RetroInferï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„ç³»ç»Ÿï¼Œå°†å…³é”®å€¼ï¼ˆKVï¼‰ç¼“å­˜é‡æ–°æ¦‚å¿µåŒ–ä¸ºå‘é‡å­˜å‚¨ç³»ç»Ÿï¼Œåˆ©ç”¨å†…åœ¨çš„æ³¨æ„åŠ›ç¨€ç–æ€§æ¥åŠ é€Ÿé•¿ä¸Šä¸‹æ–‡LLMæ¨ç†ã€‚å…¶æ ¸å¿ƒæ˜¯æ³¢åŠ¨ç´¢å¼•ï¼ˆwave indexï¼‰ï¼Œä¸€ç§æ³¨æ„åŠ›æ„ŸçŸ¥å‘é‡ç´¢å¼•ï¼Œèƒ½å¤Ÿé€šè¿‡ä¸‰æ–¹æ³¨æ„åŠ›è¿‘ä¼¼ã€ç²¾åº¦å—é™çš„æ³¨æ„åŠ›ä¼°è®¡å’Œåˆ†æ®µèšç±»ç­‰æŠ€æœ¯é«˜æ•ˆå‡†ç¡®åœ°æ£€ç´¢å…³é”®æ ‡è®°ã€‚ä¸ä»¥å¾€åœ¨æ ‡è®°é€‰æ‹©å’Œç¡¬ä»¶åè°ƒä¸Šå­˜åœ¨å›°éš¾çš„ç¨€ç–æ€§æ–¹æ³•ä¸åŒï¼ŒRetroInferåœ¨ä¸å½±å“æ¨¡å‹å‡†ç¡®æ€§çš„æƒ…å†µä¸‹æä¾›äº†å¼ºå¤§çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.02214",
            "title": "An Empirical Study of Qwen3 Quantization",
            "url": "https://huggingface.co/papers/2505.02214",
            "abstract": "The Qwen series has emerged as a leading family of open-source Large Language Models (LLMs), demonstrating remarkable capabilities in natural language understanding tasks. With the recent release of Qwen3, which exhibits superior performance across diverse benchmarks, there is growing interest in deploying these models efficiently in resource-constrained environments. Low-bit quantization presents a promising solution, yet its impact on Qwen3's performance remains underexplored. This study conducts a systematic evaluation of Qwen3's robustness under various quantization settings, aiming to uncover both opportunities and challenges in compressing this state-of-the-art model. We rigorously assess 5 existing classic post-training quantization techniques applied to Qwen3, spanning bit-widths from 1 to 8 bits, and evaluate their effectiveness across multiple datasets. Our findings reveal that while Qwen3 maintains competitive performance at moderate bit-widths, it experiences notable degradation in linguistic tasks under ultra-low precision, underscoring the persistent hurdles in LLM compression. These results emphasize the need for further research to mitigate performance loss in extreme quantization scenarios. We anticipate that this empirical analysis will provide actionable insights for advancing quantization methods tailored to Qwen3 and future LLMs, ultimately enhancing their practicality without compromising accuracy. Our project is released on https://github.com/Efficient-ML/Qwen3-Quantization and https://huggingface.co/collections/Efficient-ML/qwen3-quantization-68164450decb1c868788cb2b.",
            "score": 5,
            "issue_id": 3627,
            "pub_date": "2025-05-04",
            "pub_date_card": {
                "ru": "4 Ğ¼Ğ°Ñ",
                "en": "May 4",
                "zh": "5æœˆ4æ—¥"
            },
            "hash": "be32ffc34f30d354",
            "authors": [
                "Xingyu Zheng",
                "Yuye Li",
                "Haoran Chu",
                "Yue Feng",
                "Xudong Ma",
                "Jie Luo",
                "Jinyang Guo",
                "Haotong Qin",
                "Michele Magno",
                "Xianglong Liu"
            ],
            "affiliations": [
                "Beihang University",
                "ETH ZÃ¼rich",
                "Xidian University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.02214.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#optimization",
                    "#training",
                    "#open_source",
                    "#low_resource"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "ĞšĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Qwen3: Ğ±Ğ°Ğ»Ğ°Ğ½Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ¾Ñ†ĞµĞ½ĞºĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Qwen3, Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¸Ğ· Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ»Ğ¸ 5 ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¿Ğ¾ÑÑ‚-Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ±Ğ¸Ñ‚Ğ¾Ğ²Ğ¾Ğ¹ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğ¾Ğ¹ Ğ¾Ñ‚ 1 Ğ´Ğ¾ 8 Ğ±Ğ¸Ñ‚. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Qwen3 ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ ÑƒĞ¼ĞµÑ€ĞµĞ½Ğ½Ğ¾Ğ¼ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸, Ğ½Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ñ‚ĞµÑ€ÑĞµÑ‚ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¿Ñ€Ğ¸ ÑĞ²ĞµÑ€Ñ…Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞ¸Ñ… Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ¾Ğº Ğ´Ğ»Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ ÑĞºÑÑ‚Ñ€ĞµĞ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ LLM."
                },
                "en": {
                    "title": "Unlocking Efficiency: Evaluating Qwen3's Performance Under Quantization",
                    "desc": "The Qwen series represents a significant advancement in open-source Large Language Models (LLMs), particularly with the introduction of Qwen3, which excels in natural language understanding tasks. This paper investigates the effects of low-bit quantization on Qwen3's performance, focusing on how different quantization techniques impact its robustness. By evaluating five classic post-training quantization methods across various bit-widths, the study reveals that while Qwen3 performs well at moderate bit-widths, it struggles with linguistic tasks at ultra-low precision. The findings highlight the challenges of compressing LLMs and suggest the need for further research to improve quantization strategies without sacrificing model accuracy."
                },
                "zh": {
                    "title": "æ¢ç´¢Qwen3çš„é‡åŒ–æŒ‘æˆ˜ä¸æœºé‡",
                    "desc": "Qwenç³»åˆ—æ˜¯ä¸€ä¸ªé¢†å…ˆçš„å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œåœ¨è‡ªç„¶è¯­è¨€ç†è§£ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚æœ€è¿‘å‘å¸ƒçš„Qwen3åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå¸å¼•äº†åœ¨èµ„æºå—é™ç¯å¢ƒä¸­é«˜æ•ˆéƒ¨ç½²çš„å…³æ³¨ã€‚æœ¬æ–‡ç³»ç»Ÿè¯„ä¼°äº†Qwen3åœ¨ä¸åŒé‡åŒ–è®¾ç½®ä¸‹çš„é²æ£’æ€§ï¼Œæ¢è®¨äº†å‹ç¼©è¿™ä¸€å…ˆè¿›æ¨¡å‹çš„æœºé‡ä¸æŒ‘æˆ˜ã€‚ç ”ç©¶å‘ç°ï¼Œå°½ç®¡åœ¨ä¸­ç­‰ä½å®½ä¸‹Qwen3çš„æ€§èƒ½ä»å…·ç«äº‰åŠ›ï¼Œä½†åœ¨è¶…ä½ç²¾åº¦ä¸‹è¯­è¨€ä»»åŠ¡çš„è¡¨ç°æ˜¾è‘—ä¸‹é™ï¼Œå¼ºè°ƒäº†LLMå‹ç¼©ä¸­çš„æŒç»­éš¾é¢˜ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.03735",
            "title": "Multi-Agent System for Comprehensive Soccer Understanding",
            "url": "https://huggingface.co/papers/2505.03735",
            "abstract": "Recent advancements in AI-driven soccer understanding have demonstrated rapid progress, yet existing research predominantly focuses on isolated or narrow tasks. To bridge this gap, we propose a comprehensive framework for holistic soccer understanding. Specifically, we make the following contributions in this paper: (i) we construct SoccerWiki, the first large-scale multimodal soccer knowledge base, integrating rich domain knowledge about players, teams, referees, and venues to enable knowledge-driven reasoning; (ii) we present SoccerBench, the largest and most comprehensive soccer-specific benchmark, featuring around 10K standardized multimodal (text, image, video) multi-choice QA pairs across 13 distinct understanding tasks, curated through automated pipelines and manual verification; (iii) we introduce SoccerAgent, a novel multi-agent system that decomposes complex soccer questions via collaborative reasoning, leveraging domain expertise from SoccerWiki and achieving robust performance; (iv) extensive evaluations and ablations that benchmark state-of-the-art MLLMs on SoccerBench, highlighting the superiority of our proposed agentic system. All data and code are publicly available at: https://jyrao.github.io/SoccerAgent/.",
            "score": 3,
            "issue_id": 3625,
            "pub_date": "2025-05-06",
            "pub_date_card": {
                "ru": "6 Ğ¼Ğ°Ñ",
                "en": "May 6",
                "zh": "5æœˆ6æ—¥"
            },
            "hash": "f62fbd87d3f6f548",
            "authors": [
                "Jiayuan Rao",
                "Zifeng Li",
                "Haoning Wu",
                "Ya Zhang",
                "Yanfeng Wang",
                "Weidi Xie"
            ],
            "affiliations": [
                "SAI, Shanghai Jiao Tong University, Shanghai, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.03735.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#survey",
                    "#benchmark",
                    "#dataset",
                    "#reasoning",
                    "#multimodal",
                    "#agents"
                ],
                "emoji": "âš½",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ˜Ğ˜-Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğµ Ñ„ÑƒÑ‚Ğ±Ğ¾Ğ»Ğ°: Ğ¾Ñ‚ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğº Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ„ÑƒÑ‚Ğ±Ğ¾Ğ»Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ SoccerWiki - Ğ¿ĞµÑ€Ğ²ÑƒÑ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½ÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ±Ğ°Ğ·Ñƒ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¾ Ñ„ÑƒÑ‚Ğ±Ğ¾Ğ»Ğµ, Ğ¸ SoccerBench - Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ğ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ„ÑƒÑ‚Ğ±Ğ¾Ğ»Ğ° Ğ˜Ğ˜-ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼Ğ¸. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¾Ğ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ SoccerAgent - Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¾ Ñ„ÑƒÑ‚Ğ±Ğ¾Ğ»Ğµ Ğ¿ÑƒÑ‚ĞµĞ¼ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ½Ğ°Ğ´ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ„ÑƒÑ‚Ğ±Ğ¾Ğ»Ğ°."
                },
                "en": {
                    "title": "Revolutionizing Soccer Understanding with AI",
                    "desc": "This paper presents a new framework for understanding soccer using AI, addressing the limitations of previous research that focused on narrow tasks. The authors introduce SoccerWiki, a large multimodal knowledge base that contains detailed information about various aspects of soccer, enabling better reasoning. They also create SoccerBench, a comprehensive benchmark with thousands of multimodal question-answer pairs to evaluate soccer understanding tasks. Finally, the paper introduces SoccerAgent, a multi-agent system that collaborates to answer complex soccer questions, demonstrating improved performance through extensive evaluations."
                },
                "zh": {
                    "title": "å…¨é¢æå‡è¶³çƒç†è§£çš„æ™ºèƒ½æ¡†æ¶",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ä¸ªå…¨é¢çš„è¶³çƒç†è§£æ¡†æ¶ï¼Œä»¥å¡«è¡¥ç°æœ‰ç ”ç©¶çš„ç©ºç™½ã€‚æˆ‘ä»¬æ„å»ºäº†SoccerWikiï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå¤§è§„æ¨¡çš„å¤šæ¨¡æ€è¶³çƒçŸ¥è¯†åº“ï¼Œæ•´åˆäº†å…³äºçƒå‘˜ã€çƒé˜Ÿã€è£åˆ¤å’Œåœºé¦†çš„ä¸°å¯Œé¢†åŸŸçŸ¥è¯†ã€‚æˆ‘ä»¬è¿˜æ¨å‡ºäº†SoccerBenchï¼Œè¿™æ˜¯æœ€å¤§çš„è¶³çƒç‰¹å®šåŸºå‡†ï¼ŒåŒ…å«çº¦10,000ä¸ªæ ‡å‡†åŒ–çš„å¤šæ¨¡æ€å¤šé€‰é—®ç­”å¯¹ï¼Œæ¶µç›–13ä¸ªä¸åŒçš„ç†è§£ä»»åŠ¡ã€‚æœ€åï¼Œæˆ‘ä»¬ä»‹ç»äº†SoccerAgentï¼Œä¸€ä¸ªæ–°é¢–çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œé€šè¿‡åä½œæ¨ç†åˆ†è§£å¤æ‚çš„è¶³çƒé—®é¢˜ï¼Œå±•ç¤ºäº†å…¶å“è¶Šçš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.03164",
            "title": "InfoVids: Reimagining the Viewer Experience with Alternative\n  Visualization-Presenter Relationships",
            "url": "https://huggingface.co/papers/2505.03164",
            "abstract": "Traditional data presentations typically separate the presenter and visualization into two separate spaces--the 3D world and a 2D screen--enforcing visualization-centric stories. To create a more human-centric viewing experience, we establish a more equitable relationship between the visualization and the presenter through our InfoVids. These infographics-inspired informational videos are crafted to redefine relationships between the presenter and visualizations. As we design InfoVids, we explore how the use of layout, form, and interactions affects the viewer experience. We compare InfoVids against their baseline 2D `slides' equivalents across 9 metrics with 30 participants and provide practical, long-term insights from an autobiographical perspective. Our mixed methods analyses reveal that this paradigm reduced viewer attention splitting, shifted the focus from the visualization to the presenter, and led to more interactive, natural, and engaging full-body data performances for viewers. Ultimately, InfoVids helped viewers re-imagine traditional dynamics between the presenter and visualizations.",
            "score": 2,
            "issue_id": 3625,
            "pub_date": "2025-05-06",
            "pub_date_card": {
                "ru": "6 Ğ¼Ğ°Ñ",
                "en": "May 6",
                "zh": "5æœˆ6æ—¥"
            },
            "hash": "377a2c082764680a",
            "authors": [
                "Ji Won Chung",
                "Tongyu Zhou",
                "Ivy Chen",
                "Kevin Hsu",
                "Ryan A. Rossi",
                "Alexa Siu",
                "Shunan Guo",
                "Franck Dernoncourt",
                "James Tompkin",
                "Jeff Huang"
            ],
            "affiliations": [
                "Adobe Research",
                "Brown University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.03164.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#video"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "InfoVids: ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¸Ğ·Ğ¼Ñƒ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°",
                    "desc": "Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ InfoVids - Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ½Ñ„Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¾Ğ¹. ĞĞ½Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ğ²Ğ°Ğ½Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€ĞµĞ·ĞµĞ½Ñ‚Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸Ğ·ÑƒÑ‡Ğ°ÑÑ‚, ĞºĞ°Ğº Ğ¼Ğ°ĞºĞµÑ‚, Ñ„Ğ¾Ñ€Ğ¼Ğ° Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ»Ğ¸ÑÑÑ‚ Ğ½Ğ° Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ Ğ·Ñ€Ğ¸Ñ‚ĞµĞ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ InfoVids ÑĞ½Ğ¸Ğ¶Ğ°ÑÑ‚ Ñ€Ğ°ÑÑĞµĞ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑÑÑ‚ Ñ„Ğ¾ĞºÑƒÑ Ğ½Ğ° Ğ¿Ñ€ĞµĞ·ĞµĞ½Ñ‚Ğ°Ñ‚Ğ¾Ñ€Ğ° Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°ÑÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ¸ ÑƒĞ²Ğ»ĞµĞºĞ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing Data Presentation with InfoVids",
                    "desc": "This paper introduces InfoVids, a new format for presenting data that integrates the presenter and visualizations in a more cohesive manner. By using infographics-inspired videos, the authors aim to enhance viewer engagement and reduce distractions that typically arise from traditional 2D slides. The study evaluates InfoVids against standard presentation methods across various metrics, revealing that they foster a more interactive and natural experience for viewers. The findings suggest that this innovative approach can transform the dynamics of data presentation, making it more human-centric and effective."
                },
                "zh": {
                    "title": "é‡æ–°å®šä¹‰æ¼”ç¤ºè€…ä¸å¯è§†åŒ–çš„å…³ç³»",
                    "desc": "ä¼ ç»Ÿçš„æ•°æ®å±•ç¤ºé€šå¸¸å°†æ¼”ç¤ºè€…å’Œå¯è§†åŒ–åˆ†å¼€ï¼Œåˆ†åˆ«åœ¨3Dä¸–ç•Œå’Œ2Då±å¹•ä¸­è¿›è¡Œï¼Œå¼ºè°ƒä»¥å¯è§†åŒ–ä¸ºä¸­å¿ƒçš„å™è¿°ã€‚ä¸ºäº†åˆ›é€ æ›´ä»¥äººä¸ºæœ¬çš„è§‚çœ‹ä½“éªŒï¼Œæˆ‘ä»¬é€šè¿‡InfoVidså»ºç«‹äº†å¯è§†åŒ–ä¸æ¼”ç¤ºè€…ä¹‹é—´æ›´å¹³ç­‰çš„å…³ç³»ã€‚è¿™äº›å—ä¿¡æ¯å›¾å¯å‘çš„ä¿¡æ¯è§†é¢‘æ—¨åœ¨é‡æ–°å®šä¹‰æ¼”ç¤ºè€…ä¸å¯è§†åŒ–ä¹‹é—´çš„å…³ç³»ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼ŒInfoVidså‡å°‘äº†è§‚ä¼—çš„æ³¨æ„åŠ›åˆ†æ•£ï¼Œä½¿ç„¦ç‚¹ä»å¯è§†åŒ–è½¬å‘æ¼”ç¤ºè€…ï¼Œå¹¶ä¸ºè§‚ä¼—æä¾›äº†æ›´äº’åŠ¨ã€è‡ªç„¶å’Œå¼•äººå…¥èƒœçš„æ•°æ®è¡¨ç°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.02311",
            "title": "Invoke Interfaces Only When Needed: Adaptive Invocation for Large\n  Language Models in Question Answering",
            "url": "https://huggingface.co/papers/2505.02311",
            "abstract": "The collaborative paradigm of large and small language models (LMs) effectively balances performance and cost, yet its pivotal challenge lies in precisely pinpointing the moment of invocation when hallucinations arise in small LMs. Previous optimization efforts primarily focused on post-processing techniques, which were separate from the reasoning process of LMs, resulting in high computational costs and limited effectiveness. In this paper, we propose a practical invocation evaluation metric called AttenHScore, which calculates the accumulation and propagation of hallucinations during the generation process of small LMs, continuously amplifying potential reasoning errors. By dynamically adjusting the detection threshold, we achieve more accurate real-time invocation of large LMs. Additionally, considering the limited reasoning capacity of small LMs, we leverage uncertainty-aware knowledge reorganization to assist them better capture critical information from different text chunks. Extensive experiments reveal that our AttenHScore outperforms most baseline in enhancing real-time hallucination detection capabilities across multiple QA datasets, especially when addressing complex queries. Moreover, our strategies eliminate the need for additional model training and display flexibility in adapting to various transformer-based LMs.",
            "score": 2,
            "issue_id": 3624,
            "pub_date": "2025-05-05",
            "pub_date_card": {
                "ru": "5 Ğ¼Ğ°Ñ",
                "en": "May 5",
                "zh": "5æœˆ5æ—¥"
            },
            "hash": "2a0b4af232b71fc8",
            "authors": [
                "Jihao Zhao",
                "Chunlai Zhou",
                "Biao Qin"
            ],
            "affiliations": [
                "School of Information, Renmin University of China, Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.02311.jpg",
            "data": {
                "categories": [
                    "#hallucinations",
                    "#small_models",
                    "#training",
                    "#optimization",
                    "#reasoning"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ğ¾Ğµ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ AttenHScore Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ğ¼Ğ°Ğ»Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½ĞµĞµ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² ĞºĞ¾Ğ»Ğ»Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ñ€ĞµĞ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‡ÑŒ Ğ¼Ğ°Ğ»Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ»ÑƒÑ‡ÑˆĞµ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ ĞºĞ»ÑÑ‡ĞµĞ²ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ AttenHScore Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¸ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼."
                },
                "en": {
                    "title": "Enhancing Hallucination Detection in Language Models with AttenHScore",
                    "desc": "This paper introduces a new metric called AttenHScore to improve the detection of hallucinations in small language models (LMs) during their generation process. Hallucinations refer to incorrect or nonsensical outputs produced by LMs, and the proposed metric helps identify when these errors occur in real-time. By adjusting the detection threshold dynamically, the method enhances the invocation of larger LMs to provide more accurate responses. Additionally, the paper discusses how uncertainty-aware knowledge reorganization can help small LMs better understand and utilize critical information from text, leading to improved performance without requiring extra training."
                },
                "zh": {
                    "title": "æå‡å°å‹è¯­è¨€æ¨¡å‹çš„å¹»è§‰æ£€æµ‹èƒ½åŠ›",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è¯„ä¼°æŒ‡æ ‡AttenHScoreï¼Œç”¨äºåœ¨å°å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆè¿‡ç¨‹ä¸­æ£€æµ‹å’Œä¼ æ’­å¹»è§‰ã€‚é€šè¿‡åŠ¨æ€è°ƒæ•´æ£€æµ‹é˜ˆå€¼ï¼Œæˆ‘ä»¬èƒ½å¤Ÿæ›´å‡†ç¡®åœ°å®æ—¶è°ƒç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä»è€Œæé«˜å¹»è§‰æ£€æµ‹çš„èƒ½åŠ›ã€‚æˆ‘ä»¬è¿˜åˆ©ç”¨ä¸ç¡®å®šæ€§æ„ŸçŸ¥çš„çŸ¥è¯†é‡ç»„ï¼Œå¸®åŠ©å°å‹è¯­è¨€æ¨¡å‹æ›´å¥½åœ°æ•æ‰å…³é”®ä¿¡æ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAttenHScoreåœ¨å¤šä¸ªé—®ç­”æ•°æ®é›†ä¸Šä¼˜äºå¤§å¤šæ•°åŸºçº¿æ–¹æ³•ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†å¤æ‚æŸ¥è¯¢æ—¶ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-05-06.html",
    "link_next": "2025-05-08.html",
    "link_month": "2025-05.html",
    "short_date_prev": {
        "ru": "06.05",
        "en": "05/06",
        "zh": "5æœˆ6æ—¥"
    },
    "short_date_next": {
        "ru": "08.05",
        "en": "05/08",
        "zh": "5æœˆ8æ—¥"
    },
    "categories": {
        "#dataset": 1,
        "#data": 0,
        "#benchmark": 3,
        "#agents": 1,
        "#cv": 0,
        "#rl": 1,
        "#rlhf": 2,
        "#rag": 0,
        "#plp": 0,
        "#inference": 3,
        "#3d": 0,
        "#audio": 0,
        "#video": 2,
        "#multimodal": 4,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 5,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 4,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 6,
        "#survey": 1,
        "#diffusion": 0,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 4,
        "#small_models": 1,
        "#science": 0,
        "#low_resource": 1
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§åä¸ºVoilaçš„è¯­éŸ³AIä»£ç†ã€‚å®ƒèƒ½è‡ªåŠ¨ã€å®æ—¶ä¸”å¯Œæœ‰æƒ…æ„Ÿåœ°ä¸äººäº’åŠ¨ã€‚Voilaé‡‡ç”¨ç«¯åˆ°ç«¯çš„æ¶æ„ï¼Œå®ç°ä½å»¶è¿Ÿçš„å…¨åŒå·¥å¯¹è¯ã€‚å®ƒç»“åˆäº†å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å’Œå¼ºå¤§çš„å£°å­¦å»ºæ¨¡ï¼Œæ”¯æŒè‡ªç„¶çš„è¯­éŸ³ç”Ÿæˆã€‚Voilaè¿˜æ”¯æŒè¶…è¿‡ä¸€ç™¾ä¸‡ç§é¢„è®¾å£°éŸ³å’Œé«˜æ•ˆçš„å®šåˆ¶æ–°å£°éŸ³ã€‚",
        "title": "Voila: Voice-Language Foundation Models for Real-Time Autonomous\n  Interaction and Voice Role-Play",
        "pinyin": "ZhÃ¨ piÄn wÃ©nzhÄng jiÃ¨shÃ o le yÄ« zhÇ’ng mÃ­ngwÃ¨i Voila de yÇ”yÄ«n AI dÃ ilÇ. TÄ nÃ©ng zÃ¬dÃ²ng, shÃ­shÃ­ qiÄ› fÃ¹yÇ’u qÃ­nggÇn de yÇ” rÃ©n hÃ¹dÃ²ng. Voila cÇiyÃ²ng duÄn dÃ o duÄn de jiÃ gÃ²u, shÃ­xiÃ n dÄ« yÃ¡nchÃ­ de quÃ¡n shuÄngxiÃ ng duÃ¬huÃ . TÄ jiÃ©hÃ© le dÃ  yÇ”yÃ¡n mÃ³xÃ­ng de tuÄ«lÇ nÃ©nglÃ¬ hÃ© qiÃ¡ngdÃ  de shÄ“ngxuÃ© jiÃ nmÃ³, zhÄ«chÃ­ zÃ¬rÃ¡n de yÇ”yÄ«n shÄ“ngchÃ©ng. Voila hÃ¡i zhÄ«chÃ­ chÄoguÃ² yÄ«bÇiwÃ n zhÇ’ng yÃ¹shÃ¨ shÄ“ngyÄ«n hÃ© gÄoxiÃ o de dÃ¬ngzhÃ¬ xÄ«n shÄ“ngyÄ«n.",
        "vocab": "[\n    {\"word\": \"è¯­éŸ³\", \"pinyin\": \"yÇ”yÄ«n\", \"trans\": \"voice\"},\n    {\"word\": \"AI\", \"pinyin\": \"Ä“i-Ã i\", \"trans\": \"artificial intelligence\"},\n    {\"word\": \"ä»£ç†\", \"pinyin\": \"dÃ ilÇ\", \"trans\": \"agent\"},\n    {\"word\": \"è‡ªåŠ¨\", \"pinyin\": \"zÃ¬dÃ²ng\", \"trans\": \"automatic\"},\n    {\"word\": \"å®æ—¶\", \"pinyin\": \"shÃ­shÃ­\", \"trans\": \"real-time\"},\n    {\"word\": \"å¯Œæœ‰\", \"pinyin\": \"fÃ¹yÇ’u\", \"trans\": \"rich in\"},\n    {\"word\": \"æƒ…æ„Ÿ\", \"pinyin\": \"qÃ­nggÇn\", \"trans\": \"emotion\"},\n    {\"word\": \"äº’åŠ¨\", \"pinyin\": \"hÃ¹dÃ²ng\", \"trans\": \"interaction\"},\n    {\"word\": \"ç«¯åˆ°ç«¯\", \"pinyin\": \"duÄndÃ oduÄn\", \"trans\": \"end-to-end\"},\n    {\"word\": \"æ¶æ„\", \"pinyin\": \"jiÃ gÃ²u\", \"trans\": \"architecture\"},\n    {\"word\": \"å®ç°\", \"pinyin\": \"shÃ­xiÃ n\", \"trans\": \"achieve\"},\n    {\"word\": \"ä½å»¶è¿Ÿ\", \"pinyin\": \"dÄ« yÃ¡nchÃ­\", \"trans\": \"low latency\"},\n    {\"word\": \"å…¨åŒå·¥\", \"pinyin\": \"quÃ¡n shuÄnggÅng\", \"trans\": \"full duplex\"},\n    {\"word\": \"å¯¹è¯\", \"pinyin\": \"duÃ¬huÃ \", \"trans\": \"dialogue\"},\n    {\"word\": \"ç»“åˆ\", \"pinyin\": \"jiÃ©hÃ©\", \"trans\": \"combine\"},\n    {\"word\": \"å¤§è¯­è¨€æ¨¡å‹\", \"pinyin\": \"dÃ  yÇ”yÃ¡n mÃ³xÃ­ng\", \"trans\": \"large language model\"},\n    {\"word\": \"æ¨ç†\", \"pinyin\": \"tuÄ«lÇ\", \"trans\": \"reasoning\"},\n    {\"word\": \"èƒ½åŠ›\", \"pinyin\": \"nÃ©nglÃ¬\", \"trans\": \"ability\"},\n    {\"word\": \"å¼ºå¤§\", \"pinyin\": \"qiÃ¡ngdÃ \", \"trans\": \"powerful\"},\n    {\"word\": \"å£°å­¦\", \"pinyin\": \"shÄ“ngxuÃ©\", \"trans\": \"acoustics\"},\n    {\"word\": \"å»ºæ¨¡\", \"pinyin\": \"jiÃ nmÃ³\", \"trans\": \"modeling\"},\n    {\"word\": \"æ”¯æŒ\", \"pinyin\": \"zhÄ«chÃ­\", \"trans\": \"support\"},\n    {\"word\": \"è‡ªç„¶\", \"pinyin\": \"zÃ¬rÃ¡n\", \"trans\": \"natural\"},\n    {\"word\": \"ç”Ÿæˆ\", \"pinyin\": \"shÄ“ngchÃ©ng\", \"trans\": \"generation\"},\n    {\"word\": \"è¶…è¿‡\", \"pinyin\": \"chÄoguÃ²\", \"trans\": \"exceed\"},\n    {\"word\": \"é¢„è®¾\", \"pinyin\": \"yÃ¹shÃ¨\", \"trans\": \"preset\"},\n    {\"word\": \"å£°éŸ³\", \"pinyin\": \"shÄ“ngyÄ«n\", \"trans\": \"sound\"},\n    {\"word\": \"é«˜æ•ˆ\", \"pinyin\": \"gÄoxiÃ o\", \"trans\": \"efficient\"},\n    {\"word\": \"å®šåˆ¶\", \"pinyin\": \"dÃ¬ngzhÃ¬\", \"trans\": \"customize\"},\n    {\"word\": \"æ–°\", \"pinyin\": \"xÄ«n\", \"trans\": \"new\"}\n]",
        "trans": "This article introduces a voice AI agent called Voila. It can interact with people automatically, in real-time, and with emotional richness. Voila employs an end-to-end architecture to achieve low-latency, full-duplex conversations. It combines the reasoning capabilities of large language models with powerful acoustic modeling to support natural voice generation. Voila also supports over a million preset voices and efficient customization of new voices.",
        "update_ts": "2025-05-06 09:12"
    }
}