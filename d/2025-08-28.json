{
    "date": {
        "ru": "28 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
        "en": "August 28",
        "zh": "8æœˆ28æ—¥"
    },
    "time_utc": "2025-08-28 20:13",
    "weekday": 3,
    "issue_id": 5603,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2508.15882",
            "title": "Beyond Transcription: Mechanistic Interpretability in ASR",
            "url": "https://huggingface.co/papers/2508.15882",
            "abstract": "Interpretability methods like logit lens, linear probing, and activation patching are applied to ASR to uncover internal dynamics, repetition hallucinations, and semantic biases, enhancing model transparency and robustness.  \t\t\t\t\tAI-generated summary \t\t\t\t Interpretability methods have recently gained significant attention, particularly in the context of large language models, enabling insights into linguistic representations, error detection, and model behaviors such as hallucinations and repetitions. However, these techniques remain underexplored in automatic speech recognition (ASR), despite their potential to advance both the performance and interpretability of ASR systems. In this work, we adapt and systematically apply established interpretability methods such as logit lens, linear probing, and activation patching, to examine how acoustic and semantic information evolves across layers in ASR systems. Our experiments reveal previously unknown internal dynamics, including specific encoder-decoder interactions responsible for repetition hallucinations and semantic biases encoded deep within acoustic representations. These insights demonstrate the benefits of extending and applying interpretability techniques to speech recognition, opening promising directions for future research on improving model transparency and robustness.",
            "score": 67,
            "issue_id": 5593,
            "pub_date": "2025-08-21",
            "pub_date_card": {
                "ru": "21 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 21",
                "zh": "8æœˆ21æ—¥"
            },
            "hash": "092f970748bd5392",
            "authors": [
                "Neta Glazer",
                "Yael Segal-Feldman",
                "Hilit Segev",
                "Aviv Shamsian",
                "Asaf Buchnick",
                "Gill Hetz",
                "Ethan Fetaya",
                "Joseph Keshet",
                "Aviv Navon"
            ],
            "affiliations": [
                "aiOla Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.15882.jpg",
            "data": {
                "categories": [
                    "#hallucinations",
                    "#audio",
                    "#interpretability"
                ],
                "emoji": "ğŸ™ï¸",
                "ru": {
                    "title": "Ğ—Ğ°Ğ³Ğ»ÑĞ½ÑƒÑ‚ÑŒ Ğ²Ğ½ÑƒÑ‚Ñ€ÑŒ ASR: Ñ€Ğ°ÑĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ Ñ‚Ğ°Ğ¹Ğ½ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑ‡Ğ¸",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº logit lens, Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğµ Ğ·Ğ¾Ğ½Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ Ğ¿Ğ°Ñ‚Ñ‡Ğ¸Ğ½Ğ³, Ğº ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑ‡Ğ¸ (ASR). Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ÑÑ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºÑƒ ASR-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ° Ğ¸ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€Ğ°, Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ·Ğ° Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ, Ğ·Ğ°ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ² Ğ°ĞºÑƒÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑÑ… Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ñ… ÑĞ»Ğ¾ĞµĞ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼ ASR."
                },
                "en": {
                    "title": "Unlocking the Secrets of Speech Recognition Models",
                    "desc": "This paper explores the use of interpretability methods in automatic speech recognition (ASR) systems to enhance understanding of their internal workings. Techniques like logit lens, linear probing, and activation patching are applied to reveal how acoustic and semantic information changes throughout the model's layers. The study uncovers important dynamics, such as interactions between the encoder and decoder that lead to issues like repetition hallucinations and semantic biases. By applying these methods, the research highlights the potential for improving the transparency and robustness of ASR models, paving the way for future advancements in the field."
                },
                "zh": {
                    "title": "æå‡ASRç³»ç»Ÿé€æ˜åº¦ä¸é²æ£’æ€§çš„å¯è§£é‡Šæ€§æ–¹æ³•",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†å¯è§£é‡Šæ€§æ–¹æ³•åœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ä¸­çš„åº”ç”¨ï¼ŒåŒ…æ‹¬logit lensã€çº¿æ€§æ¢æµ‹å’Œæ¿€æ´»è¡¥ä¸ç­‰æŠ€æœ¯ã€‚è¿™äº›æ–¹æ³•å¸®åŠ©æˆ‘ä»¬æ­ç¤ºäº†ASRç³»ç»Ÿå†…éƒ¨çš„åŠ¨æ€å˜åŒ–ã€é‡å¤å¹»è§‰å’Œè¯­ä¹‰åè§ï¼Œä»è€Œæé«˜äº†æ¨¡å‹çš„é€æ˜åº¦å’Œé²æ£’æ€§ã€‚å°½ç®¡å¯è§£é‡Šæ€§æ–¹æ³•åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­å¾—åˆ°äº†å¹¿æ³›å…³æ³¨ï¼Œä½†åœ¨ASRé¢†åŸŸçš„åº”ç”¨ä»ç„¶è¾ƒå°‘ã€‚é€šè¿‡ç³»ç»Ÿåœ°åº”ç”¨è¿™äº›æ–¹æ³•ï¼Œæˆ‘ä»¬å‘ç°äº†ASRç³»ç»Ÿä¸­å£°å­¦å’Œè¯­ä¹‰ä¿¡æ¯åœ¨ä¸åŒå±‚æ¬¡ä¸Šçš„æ¼”å˜ï¼Œæ¨åŠ¨äº†æœªæ¥ç ”ç©¶çš„æ–¹å‘ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.19652",
            "title": "Self-Rewarding Vision-Language Model via Reasoning Decomposition",
            "url": "https://huggingface.co/papers/2508.19652",
            "abstract": "Vision-SR1 uses reinforcement learning to enhance visual reasoning in vision-language models by decomposing the process into visual perception and language reasoning stages, improving accuracy and reducing hallucinations.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language Models (VLMs) often suffer from visual hallucinations, saying things that are not actually in the image, and language shortcuts, where they skip the visual part and just rely on text priors. These issues arise because most post-training methods for VLMs rely on simple verifiable answer matching and supervise only final outputs, leaving intermediate visual reasoning without explicit guidance. As a result, VLMs receive sparse visual signals and often learn to prioritize language-based reasoning over visual perception. To mitigate this, some existing methods add visual supervision using human annotations or distilled labels from external large models. However, human annotations are labor-intensive and costly, and because external signals cannot adapt to the evolving policy, they cause distributional shifts that can lead to reward hacking. In this paper, we introduce Vision-SR1, a self-rewarding method that improves visual reasoning without relying on external visual supervisions via reinforcement learning. Vision-SR1 decomposes VLM reasoning into two stages: visual perception and language reasoning. The model is first prompted to produce self-contained visual perceptions that are sufficient to answer the question without referring back the input image. To validate this self-containment, the same VLM model is then re-prompted to perform language reasoning using only the generated perception as input to compute reward. This self-reward is combined with supervision on final outputs, providing a balanced training signal that strengthens both visual perception and language reasoning. Our experiments demonstrate that Vision-SR1 improves visual reasoning, mitigates visual hallucinations, and reduces reliance on language shortcuts across diverse vision-language tasks.",
            "score": 59,
            "issue_id": 5585,
            "pub_date": "2025-08-27",
            "pub_date_card": {
                "ru": "27 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 27",
                "zh": "8æœˆ27æ—¥"
            },
            "hash": "990502a2cc19d192",
            "authors": [
                "Zongxia Li",
                "Wenhao Yu",
                "Chengsong Huang",
                "Rui Liu",
                "Zhenwen Liang",
                "Fuxiao Liu",
                "Jingxi Che",
                "Dian Yu",
                "Jordan Boyd-Graber",
                "Haitao Mi",
                "Dong Yu"
            ],
            "affiliations": [
                "Tencent AI Lab, Seattle",
                "University of Maryland, College Park",
                "Washington University in St. Louis"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.19652.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#hallucinations",
                    "#training",
                    "#multimodal",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ˜Ğ˜ Ğ±ĞµĞ· Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹",
                    "desc": "Vision-SR1 - ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞĞ½ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ñ‹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ĞµÑ‚ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ°Ğ¼Ğ¾Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğµ, Ğ½Ğµ Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ÑÑÑŒ Ğ½Ğ° Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Vision-SR1 ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ, ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ĞµÑ‚ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… ÑˆĞ°Ğ±Ğ»Ğ¾Ğ½Ğ¾Ğ² Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°."
                },
                "en": {
                    "title": "Enhancing Visual Reasoning with Self-Rewarding Learning",
                    "desc": "Vision-SR1 is a novel approach that enhances visual reasoning in vision-language models (VLMs) by using reinforcement learning. It breaks down the reasoning process into two distinct stages: visual perception and language reasoning, allowing for more focused training. By generating self-contained visual perceptions, the model can validate its understanding without needing to refer back to the original image. This method reduces visual hallucinations and reliance on language shortcuts, leading to improved accuracy in various vision-language tasks."
                },
                "zh": {
                    "title": "Vision-SR1ï¼šæå‡è§†è§‰æ¨ç†çš„è‡ªæˆ‘å¥–åŠ±æ–¹æ³•",
                    "desc": "Vision-SR1æ˜¯ä¸€ç§åˆ©ç”¨å¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•ï¼Œæ—¨åœ¨å¢å¼ºè§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„è§†è§‰æ¨ç†èƒ½åŠ›ã€‚è¯¥æ–¹æ³•å°†æ¨ç†è¿‡ç¨‹åˆ†è§£ä¸ºè§†è§‰æ„ŸçŸ¥å’Œè¯­è¨€æ¨ç†ä¸¤ä¸ªé˜¶æ®µï¼Œä»è€Œæé«˜äº†æ¨¡å‹çš„å‡†ç¡®æ€§å¹¶å‡å°‘äº†å¹»è§‰ç°è±¡ã€‚é€šè¿‡è‡ªæˆ‘å¥–åŠ±æœºåˆ¶ï¼ŒVision-SR1ä¸ä¾èµ–å¤–éƒ¨è§†è§‰ç›‘ç£ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°è®­ç»ƒæ¨¡å‹è¿›è¡Œæ›´å¥½çš„è§†è§‰æ„ŸçŸ¥å’Œè¯­è¨€æ¨ç†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVision-SR1åœ¨å¤šç§è§†è§‰è¯­è¨€ä»»åŠ¡ä¸­æ˜¾è‘—æ”¹å–„äº†è§†è§‰æ¨ç†èƒ½åŠ›ï¼Œé™ä½äº†å¯¹è¯­è¨€æ·å¾„çš„ä¾èµ–ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.20072",
            "title": "Discrete Diffusion VLA: Bringing Discrete Diffusion to Action Decoding\n  in Vision-Language-Action Policies",
            "url": "https://huggingface.co/papers/2508.20072",
            "abstract": "Discrete Diffusion VLA uses a single-transformer policy with discrete diffusion to model actions, improving decoding order, consistency, and performance over autoregressive and continuous diffusion methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language-Action (VLA) models adapt large vision-language backbones to map images and instructions to robot actions. However, prevailing VLA decoders either generate actions autoregressively in a fixed left-to-right order or attach continuous diffusion or flow matching heads outside the backbone, demanding specialized training and iterative sampling that hinder a unified, scalable architecture. We present Discrete Diffusion VLA, a single-transformer policy that models discretized action chunks with discrete diffusion and is trained with the same cross-entropy objective as the VLM backbone. The design retains diffusion's progressive refinement paradigm while remaining natively compatible with the discrete token interface of VLMs. Our method achieves an adaptive decoding order that resolves easy action elements before harder ones and uses secondary remasking to revisit uncertain predictions across refinement rounds, which improves consistency and enables robust error correction. This unified decoder preserves pretrained vision language priors, supports parallel decoding, breaks the autoregressive bottleneck, and reduces the number of function evaluations. Discrete Diffusion VLA achieves 96.3% avg. SR on LIBERO, 71.2% visual matching on SimplerEnv Fractal and 49.3% overall on SimplerEnv Bridge, improving over both autoregressive and continuous diffusion baselines. These findings indicate that discrete-diffusion action decoder supports precise action modeling and consistent training, laying groundwork for scaling VLA to larger models and datasets.",
            "score": 19,
            "issue_id": 5586,
            "pub_date": "2025-08-27",
            "pub_date_card": {
                "ru": "27 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 27",
                "zh": "8æœˆ27æ—¥"
            },
            "hash": "ec096752edb34e60",
            "authors": [
                "Zhixuan Liang",
                "Yizhuo Li",
                "Tianshuo Yang",
                "Chengyue Wu",
                "Sitong Mao",
                "Liuao Pei",
                "Xiaokang Yang",
                "Jiangmiao Pang",
                "Yao Mu",
                "Ping Luo"
            ],
            "affiliations": [
                "Huawei Cloud Computing Technologies Co., Ltd.",
                "Shanghai AI Laboratory",
                "Shanghai Jiao Tong University",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.20072.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#diffusion",
                    "#games",
                    "#cv",
                    "#agents",
                    "#multimodal"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ”Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ğ°Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ñ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ² VLA Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Discrete Diffusion VLA - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ·Ñ€ĞµĞ½Ğ¸Ñ-ÑĞ·Ñ‹ĞºĞ°-Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ (Vision-Language-Action, VLA). ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞµĞ´Ğ¸Ğ½Ñ‹Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ Ñ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸ĞµĞ¹ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑ‚ÑŒ Ğ¿Ğ¾Ñ€ÑĞ´Ğ¾Ğº Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸. Discrete Diffusion VLA Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ VLA Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Revolutionizing Action Modeling with Discrete Diffusion VLA",
                    "desc": "The paper introduces Discrete Diffusion VLA, a novel approach that utilizes a single-transformer policy to model actions through discrete diffusion. This method enhances the decoding process by allowing actions to be generated in an adaptive order, addressing simpler actions before more complex ones. By maintaining compatibility with the discrete token interface of vision-language models (VLMs), it simplifies training and improves performance without the need for specialized iterative sampling. The results demonstrate significant improvements in action modeling accuracy and consistency, paving the way for scaling VLA applications to larger datasets and models."
                },
                "zh": {
                    "title": "ç¦»æ•£æ‰©æ•£VLAï¼šæå‡åŠ¨ä½œå»ºæ¨¡ä¸ä¸€è‡´æ€§",
                    "desc": "ç¦»æ•£æ‰©æ•£VLAä½¿ç”¨å•ä¸€å˜æ¢å™¨ç­–ç•¥ï¼Œé€šè¿‡ç¦»æ•£æ‰©æ•£æ¥å»ºæ¨¡åŠ¨ä½œï¼Œæ”¹å–„äº†è§£ç é¡ºåºã€ä¸€è‡´æ€§å’Œæ€§èƒ½ï¼Œä¼˜äºè‡ªå›å½’å’Œè¿ç»­æ‰©æ•£æ–¹æ³•ã€‚è¯¥æ¨¡å‹å°†å›¾åƒå’ŒæŒ‡ä»¤æ˜ å°„åˆ°æœºå™¨äººåŠ¨ä½œï¼Œé‡‡ç”¨ä¸è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ç›¸åŒçš„äº¤å‰ç†µç›®æ ‡è¿›è¡Œè®­ç»ƒã€‚è®¾è®¡ä¿ç•™äº†æ‰©æ•£çš„æ¸è¿›ç»†åŒ–èŒƒå¼ï¼ŒåŒæ—¶ä¸VLMçš„ç¦»æ•£ä»¤ç‰Œæ¥å£å…¼å®¹ã€‚ç¦»æ•£æ‰©æ•£VLAåœ¨LIBEROä¸Šå®ç°äº†96.3%çš„å¹³å‡æˆåŠŸç‡ï¼Œè¡¨æ˜å…¶æ”¯æŒç²¾ç¡®çš„åŠ¨ä½œå»ºæ¨¡å’Œä¸€è‡´çš„è®­ç»ƒï¼Œä¸ºå°†VLAæ‰©å±•åˆ°æ›´å¤§æ¨¡å‹å’Œæ•°æ®é›†å¥ å®šäº†åŸºç¡€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.19320",
            "title": "MIDAS: Multimodal Interactive Digital-human Synthesis via Real-time\n  Autoregressive Video Generation",
            "url": "https://huggingface.co/papers/2508.19320",
            "abstract": "An autoregressive video generation framework with multimodal control and low-latency extrapolation uses a modified large language model and a deep compression autoencoder to achieve high efficiency and fine-grained controllability.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, interactive digital human video generation has attracted widespread attention and achieved remarkable progress. However, building such a practical system that can interact with diverse input signals in real time remains challenging to existing methods, which often struggle with high latency, heavy computational cost, and limited controllability. In this work, we introduce an autoregressive video generation framework that enables interactive multimodal control and low-latency extrapolation in a streaming manner. With minimal modifications to a standard large language model (LLM), our framework accepts multimodal condition encodings including audio, pose, and text, and outputs spatially and semantically coherent representations to guide the denoising process of a diffusion head. To support this, we construct a large-scale dialogue dataset of approximately 20,000 hours from multiple sources, providing rich conversational scenarios for training. We further introduce a deep compression autoencoder with up to 64times reduction ratio, which effectively alleviates the long-horizon inference burden of the autoregressive model. Extensive experiments on duplex conversation, multilingual human synthesis, and interactive world model highlight the advantages of our approach in low latency, high efficiency, and fine-grained multimodal controllability.",
            "score": 18,
            "issue_id": 5592,
            "pub_date": "2025-08-26",
            "pub_date_card": {
                "ru": "26 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 26",
                "zh": "8æœˆ26æ—¥"
            },
            "hash": "41330ab593be1a2d",
            "authors": [
                "Ming Chen",
                "Liyuan Cui",
                "Wenyuan Zhang",
                "Haoxian Zhang",
                "Yan Zhou",
                "Xiaohan Li",
                "Xiaoqiang Liu",
                "Pengfei Wan"
            ],
            "affiliations": [
                "Kuaishou Technology",
                "Tsinghua University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.19320.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#inference",
                    "#long_context",
                    "#diffusion",
                    "#video",
                    "#multimodal",
                    "#low_resource"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "Ğ˜Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ñ†Ğ¸Ñ„Ñ€Ğ¾Ğ²Ñ‹Ğµ Ğ»ÑĞ´Ğ¸: Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾, ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸ Ğ¿Ğ¾Ğ´ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ñ†Ğ¸Ñ„Ñ€Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ»ÑĞ´ÑŒĞ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°ĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ğ¹ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ½Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ² Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞµ, Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¼ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğµ."
                },
                "en": {
                    "title": "Real-Time Interactive Video Generation with Multimodal Control",
                    "desc": "This paper presents a novel autoregressive video generation framework that allows for real-time interaction using multiple input types, such as audio and text. It modifies a large language model to accept these multimodal inputs and produce coherent video outputs efficiently. The framework incorporates a deep compression autoencoder to significantly reduce the computational load, enabling low-latency performance. Extensive testing demonstrates its effectiveness in generating interactive digital human videos with high efficiency and precise control over various modalities."
                },
                "zh": {
                    "title": "ä½å»¶è¿Ÿå¤šæ¨¡æ€è§†é¢‘ç”Ÿæˆæ–°æ¡†æ¶",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªå›å½’è§†é¢‘ç”Ÿæˆæ¡†æ¶ï¼Œèƒ½å¤Ÿå®ç°å¤šæ¨¡æ€æ§åˆ¶å’Œä½å»¶è¿Ÿçš„å¤–æ¨ã€‚è¯¥æ¡†æ¶å¯¹æ ‡å‡†çš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œäº†æœ€å°ä¿®æ”¹ï¼Œèƒ½å¤Ÿå¤„ç†éŸ³é¢‘ã€å§¿æ€å’Œæ–‡æœ¬ç­‰å¤šç§è¾“å…¥ä¿¡å·ï¼Œå¹¶ç”Ÿæˆç©ºé—´å’Œè¯­ä¹‰ä¸€è‡´çš„è¡¨ç¤ºã€‚é€šè¿‡æ„å»ºä¸€ä¸ªåŒ…å«çº¦20,000å°æ—¶å¯¹è¯çš„æ•°æ®é›†ï¼Œæ”¯æŒä¸°å¯Œçš„å¯¹è¯åœºæ™¯è®­ç»ƒã€‚æ­¤å¤–ï¼Œé‡‡ç”¨æ·±åº¦å‹ç¼©è‡ªç¼–ç å™¨ï¼Œæ˜¾è‘—é™ä½äº†è‡ªå›å½’æ¨¡å‹çš„æ¨ç†è´Ÿæ‹…ï¼Œæå‡äº†æ•ˆç‡å’Œå¯æ§æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.20096",
            "title": "CODA: Coordinating the Cerebrum and Cerebellum for a Dual-Brain Computer\n  Use Agent with Decoupled Reinforcement Learning",
            "url": "https://huggingface.co/papers/2508.20096",
            "abstract": "CODA, a trainable compositional framework, combines a generalist planner and specialist executor to achieve robust execution and cross-domain generalization in scientific computing GUIs.  \t\t\t\t\tAI-generated summary \t\t\t\t Autonomous agents for Graphical User Interfaces (GUIs) face significant challenges in specialized domains such as scientific computing, where both long-horizon planning and precise execution are required. Existing approaches suffer from a trade-off: generalist agents excel at planning but perform poorly in execution, while specialized agents demonstrate the opposite weakness. Recent compositional frameworks attempt to bridge this gap by combining a planner and an actor, but they are typically static and non-trainable, which prevents adaptation from experience. This is a critical limitation given the scarcity of high-quality data in scientific domains. To address these limitations, we introduce CODA, a novel and trainable compositional framework that integrates a generalist planner (Cerebrum) with a specialist executor (Cerebellum), trained via a dedicated two-stage pipeline. In the first stage, Specialization, we apply a decoupled GRPO approach to train an expert planner for each scientific application individually, bootstrapping from a small set of task trajectories. In the second stage, Generalization, we aggregate all successful trajectories from the specialized experts to build a consolidated dataset, which is then used for supervised fine-tuning of the final planner. This equips CODA with both robust execution and cross-domain generalization. Evaluated on four challenging applications from the ScienceBoard benchmark, CODA significantly outperforms baselines and establishes a new state of the art among open-source models.",
            "score": 13,
            "issue_id": 5586,
            "pub_date": "2025-08-27",
            "pub_date_card": {
                "ru": "27 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 27",
                "zh": "8æœˆ27æ—¥"
            },
            "hash": "8449940bace28db5",
            "authors": [
                "Zeyi Sun",
                "Yuhang Cao",
                "Jianze Liang",
                "Qiushi Sun",
                "Ziyu Liu",
                "Zhixiong Zhang",
                "Yuhang Zang",
                "Xiaoyi Dong",
                "Kai Chen",
                "Dahua Lin",
                "Jiaqi Wang"
            ],
            "affiliations": [
                "Shanghai AI Laboratory",
                "Shanghai Jiao Tong University",
                "The Chinese University of Hong Kong",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.20096.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#optimization",
                    "#benchmark",
                    "#training",
                    "#agents",
                    "#science"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "CODA: Ğ£Ğ¼Ğ½Ñ‹Ğ¹ Ñ‚Ğ°Ğ½Ğ´ĞµĞ¼ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ° Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»Ñ Ğ´Ğ»Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… GUI",
                    "desc": "CODA - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ğ°Ñ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ°Ñ…. ĞĞ½Ğ° ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸Ğº (Cerebrum) Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒ (Cerebellum), Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ğµ Ğ¿Ğ¾ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğ¼Ñƒ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ñƒ. ĞĞ° Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ğ¹ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸Ğº, Ğ° Ğ½Ğ° Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸Ğº Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ°Ğ³Ñ€ĞµĞ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. CODA Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ ScienceBoard, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğµ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ ĞºÑ€Ğ¾ÑÑ-Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ."
                },
                "en": {
                    "title": "CODA: Bridging Planning and Execution for Robust AI in Science",
                    "desc": "CODA is a novel framework designed to improve the performance of autonomous agents in scientific computing GUIs by combining a generalist planner and a specialist executor. It addresses the limitations of existing methods that struggle with the trade-off between planning and execution. CODA employs a two-stage training process: first, it specializes the planner for individual tasks, and then it generalizes by aggregating successful task trajectories for fine-tuning. This approach allows CODA to achieve robust execution and effective cross-domain generalization, outperforming previous models in benchmark evaluations."
                },
                "zh": {
                    "title": "CODAï¼šç§‘å­¦è®¡ç®—çš„æ™ºèƒ½æ‰§è¡Œæ–°æ¡†æ¶",
                    "desc": "CODAæ˜¯ä¸€ä¸ªå¯è®­ç»ƒçš„ç»„åˆæ¡†æ¶ï¼Œç»“åˆäº†é€šç”¨è§„åˆ’å™¨å’Œä¸“ä¸šæ‰§è¡Œå™¨ï¼Œä»¥å®ç°ç§‘å­¦è®¡ç®—å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰çš„ç¨³å¥æ‰§è¡Œå’Œè·¨é¢†åŸŸæ³›åŒ–ã€‚ç°æœ‰æ–¹æ³•åœ¨é€šç”¨ä»£ç†å’Œä¸“ä¸šä»£ç†ä¹‹é—´å­˜åœ¨æƒè¡¡ï¼Œé€šç”¨ä»£ç†åœ¨è§„åˆ’ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä½†æ‰§è¡Œèƒ½åŠ›è¾ƒå·®ï¼Œè€Œä¸“ä¸šä»£ç†åˆ™ç›¸åã€‚CODAé€šè¿‡ä¸€ä¸ªä¸¤é˜¶æ®µçš„è®­ç»ƒæµç¨‹ï¼Œé¦–å…ˆä¸ºæ¯ä¸ªç§‘å­¦åº”ç”¨è®­ç»ƒä¸“å®¶è§„åˆ’å™¨ï¼Œç„¶åèšåˆæˆåŠŸçš„è½¨è¿¹è¿›è¡Œç›‘ç£å¾®è°ƒï¼Œä»è€Œå…‹æœäº†æ•°æ®ç¨€ç¼ºçš„é™åˆ¶ã€‚ç»è¿‡è¯„ä¼°ï¼ŒCODAåœ¨å¤šä¸ªæŒ‘æˆ˜æ€§åº”ç”¨ä¸­æ˜¾è‘—è¶…è¶Šäº†åŸºçº¿ï¼Œç¡®ç«‹äº†å¼€æºæ¨¡å‹çš„æ–°æ ‡å‡†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.19228",
            "title": "Predicting the Order of Upcoming Tokens Improves Language Modeling",
            "url": "https://huggingface.co/papers/2508.19228",
            "abstract": "Token Order Prediction (TOP) improves language model training by ordering upcoming tokens, outperforming both Next-Token Prediction (NTP) and Multi-Token Prediction (MTP) across benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Multi-Token Prediction (MTP) has been proposed as an auxiliary objective to improve next-token prediction (NTP) in language model training but shows inconsistent improvements, underperforming in standard NLP benchmarks. We argue that MTP's exact future token prediction is too difficult as an auxiliary loss. Instead, we propose Token Order Prediction (TOP), which trains models to order upcoming tokens by their proximity using a learning-to-rank loss. TOP requires only a single additional unembedding layer compared to MTP's multiple transformer layers. We pretrain models of 340M, 1.8B, and 7B parameters using NTP, MTP, and TOP objectives. Results on eight standard NLP benchmarks show that TOP overall outperforms both NTP and MTP even at scale. Our code is available at https://github.com/zaydzuhri/token-order-prediction",
            "score": 12,
            "issue_id": 5587,
            "pub_date": "2025-08-26",
            "pub_date_card": {
                "ru": "26 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 26",
                "zh": "8æœˆ26æ—¥"
            },
            "hash": "697f532512cebd78",
            "authors": [
                "Zayd M. K. Zuhri",
                "Erland Hilman Fuadi",
                "Alham Fikri Aji"
            ],
            "affiliations": [
                "MBZUAI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.19228.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#benchmark",
                    "#optimization"
                ],
                "emoji": "ğŸ”¢",
                "ru": {
                    "title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞ° Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ - Token Order Prediction (TOP). TOP Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ¾ÑÑ‰Ğ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ¿Ğ¾ Ğ¸Ñ… Ğ±Ğ»Ğ¸Ğ·Ğ¾ÑÑ‚Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ loss Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ learning-to-rank. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ĞºĞ°Ğº ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ° (NTP), Ñ‚Ğ°Ğº Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² (MTP) Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°. TOP Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²ÑĞµĞ³Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ ÑĞ»Ğ¾Ñ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ MTP, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ ĞµĞ³Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼."
                },
                "en": {
                    "title": "Revolutionizing Language Models with Token Order Prediction",
                    "desc": "Token Order Prediction (TOP) is a new approach to improve language model training by focusing on the order of upcoming tokens rather than predicting them exactly. This method uses a learning-to-rank loss to train models, making it simpler and more effective than Multi-Token Prediction (MTP), which has shown inconsistent results. TOP only requires one additional unembedding layer, making it more efficient than MTP's multiple transformer layers. Experiments with models of various sizes demonstrate that TOP consistently outperforms both Next-Token Prediction (NTP) and MTP across multiple NLP benchmarks."
                },
                "zh": {
                    "title": "ä»¤ç‰Œé¡ºåºé¢„æµ‹ï¼šæå‡è¯­è¨€æ¨¡å‹çš„æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è¯­è¨€æ¨¡å‹è®­ç»ƒæ–¹æ³•â€”â€”ä»¤ç‰Œé¡ºåºé¢„æµ‹ï¼ˆTOPï¼‰ï¼Œæ—¨åœ¨é€šè¿‡å¯¹å³å°†åˆ°æ¥çš„ä»¤ç‰Œè¿›è¡Œæ’åºæ¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚ä¸å¤šä»¤ç‰Œé¢„æµ‹ï¼ˆMTPï¼‰ç›¸æ¯”ï¼ŒTOPåœ¨å¤šä¸ªæ ‡å‡†è‡ªç„¶è¯­è¨€å¤„ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°æ›´ä½³ï¼Œå› ä¸ºMTPçš„ç²¾ç¡®æœªæ¥ä»¤ç‰Œé¢„æµ‹è¿‡äºå›°éš¾ã€‚TOPä½¿ç”¨å­¦ä¹ æ’åºæŸå¤±ï¼Œä»…éœ€ä¸€ä¸ªé¢å¤–çš„è§£åµŒå…¥å±‚ï¼Œç›¸æ¯”äºMTPçš„å¤šä¸ªå˜æ¢å™¨å±‚ï¼Œç»“æ„æ›´ä¸ºç®€åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTOPåœ¨340Mã€1.8Bå’Œ7Bå‚æ•°çš„æ¨¡å‹é¢„è®­ç»ƒä¸­å‡ä¼˜äºä¼ ç»Ÿçš„ä¸‹ä¸€ä»¤ç‰Œé¢„æµ‹ï¼ˆNTPï¼‰å’ŒMTPã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.17924",
            "title": "Gaze into the Heart: A Multi-View Video Dataset for rPPG and Health\n  Biomarkers Estimation",
            "url": "https://huggingface.co/papers/2508.17924",
            "abstract": "A large-scale multi-view video dataset for rPPG and health biomarkers estimation is introduced, enabling efficient rPPG model training and comparison with existing approaches.  \t\t\t\t\tAI-generated summary \t\t\t\t Progress in remote PhotoPlethysmoGraphy (rPPG) is limited by the critical issues of existing publicly available datasets: small size, privacy concerns with facial videos, and lack of diversity in conditions. The paper introduces a novel comprehensive large-scale multi-view video dataset for rPPG and health biomarkers estimation. Our dataset comprises 3600 synchronized video recordings from 600 subjects, captured under varied conditions (resting and post-exercise) using multiple consumer-grade cameras at different angles. To enable multimodal analysis of physiological states, each recording is paired with a 100 Hz PPG signal and extended health metrics, such as electrocardiogram, arterial blood pressure, biomarkers, temperature, oxygen saturation, respiratory rate, and stress level. Using this data, we train an efficient rPPG model and compare its quality with existing approaches in cross-dataset scenarios. The public release of our dataset and model should significantly speed up the progress in the development of AI medical assistants.",
            "score": 11,
            "issue_id": 5594,
            "pub_date": "2025-08-25",
            "pub_date_card": {
                "ru": "25 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 25",
                "zh": "8æœˆ25æ—¥"
            },
            "hash": "126632113a93dfc3",
            "authors": [
                "Konstantin Egorov",
                "Stepan Botman",
                "Pavel Blinov",
                "Galina Zubkova",
                "Anton Ivaschenko",
                "Alexander Kolsanov",
                "Andrey Savchenko"
            ],
            "affiliations": [
                "ISP RAS Research Center for Trusted Artificial Intelligence, Moscow, Russia",
                "Samara State Medical University, Samara, Russia",
                "Sber AI Lab, Moscow, Russia"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.17924.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#multimodal",
                    "#dataset",
                    "#healthcare"
                ],
                "emoji": "ğŸ“¹",
                "ru": {
                    "title": "Ğ‘Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ñ€Ñ‹Ğ²Ğ° Ğ² Ğ´Ğ¸ÑÑ‚Ğ°Ğ½Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ñ„Ğ¾Ñ‚Ğ¾Ğ¿Ğ»ĞµÑ‚Ğ¸Ğ·Ğ¼Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ğ¸",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ñ‹Ğ¹ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ rPPG Ğ¸ Ğ±Ğ¸Ğ¾Ğ¼Ğ°Ñ€ĞºĞµÑ€Ğ¾Ğ² Ğ·Ğ´Ğ¾Ñ€Ğ¾Ğ²ÑŒÑ. ĞĞ°Ğ±Ğ¾Ñ€ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 3600 ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹ Ğ¾Ñ‚ 600 ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ…, ÑĞ½ÑÑ‚Ñ‹Ñ… Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ ĞºĞ°Ğ¼ĞµÑ€Ğ°Ğ¼Ğ¸ Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑƒĞ³Ğ»Ğ¾Ğ². ĞšĞ°Ğ¶Ğ´Ğ°Ñ Ğ·Ğ°Ğ¿Ğ¸ÑÑŒ ÑĞ¾Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ¶Ğ´Ğ°ĞµÑ‚ÑÑ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ¼ PPG 100 Ğ“Ñ† Ğ¸ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑĞ¼Ğ¸ Ğ·Ğ´Ğ¾Ñ€Ğ¾Ğ²ÑŒÑ. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±ÑƒÑ‡Ğ¸Ğ»Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ rPPG Ğ¸ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ»Ğ¸ ĞµĞµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ² ĞºÑ€Ğ¾ÑÑ-Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…."
                },
                "en": {
                    "title": "Revolutionizing rPPG with a Comprehensive Multi-View Dataset",
                    "desc": "This paper presents a new large-scale multi-view video dataset designed for training models in remote PhotoPlethysmoGraphy (rPPG) and estimating health biomarkers. The dataset includes 3600 synchronized video recordings from 600 subjects, captured under various conditions, which addresses the limitations of existing datasets such as size and diversity. Each video is paired with a 100 Hz PPG signal and additional health metrics, allowing for comprehensive analysis of physiological states. The authors demonstrate the effectiveness of their rPPG model by comparing it with existing methods, aiming to enhance the development of AI medical assistants."
                },
                "zh": {
                    "title": "å¤§è§„æ¨¡å¤šè§†è§’è§†é¢‘æ•°æ®é›†åŠ©åŠ›rPPGç ”ç©¶",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªå¤§è§„æ¨¡çš„å¤šè§†è§’è§†é¢‘æ•°æ®é›†ï¼Œç”¨äºè¿œç¨‹å…‰ç”µå®¹ç§¯æè®°æ³•ï¼ˆrPPGï¼‰å’Œå¥åº·ç”Ÿç‰©æ ‡å¿—ç‰©çš„ä¼°è®¡ã€‚è¯¥æ•°æ®é›†åŒ…å«3600ä¸ªåŒæ­¥è§†é¢‘å½•åˆ¶ï¼Œæ¥è‡ª600ä¸ªå—è¯•è€…ï¼Œæ¶µç›–äº†ä¸åŒçš„æ¡ä»¶ï¼ˆé™æ¯å’Œè¿åŠ¨åï¼‰ï¼Œå¹¶ä½¿ç”¨å¤šå°æ¶ˆè´¹çº§ç›¸æœºä»ä¸åŒè§’åº¦æ‹æ‘„ã€‚æ¯ä¸ªå½•åˆ¶éƒ½é…æœ‰100 Hzçš„PPGä¿¡å·å’Œæ‰©å±•çš„å¥åº·æŒ‡æ ‡ï¼Œå¦‚å¿ƒç”µå›¾ã€åŠ¨è„‰è¡€å‹ã€ç”Ÿç‰©æ ‡å¿—ç‰©ã€ä½“æ¸©ã€æ°§é¥±å’Œåº¦ã€å‘¼å¸é¢‘ç‡å’Œå‹åŠ›æ°´å¹³ã€‚é€šè¿‡è¿™äº›æ•°æ®ï¼Œæˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªé«˜æ•ˆçš„rPPGæ¨¡å‹ï¼Œå¹¶åœ¨è·¨æ•°æ®é›†åœºæ™¯ä¸­ä¸ç°æœ‰æ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.19982",
            "title": "Diffusion Language Models Know the Answer Before Decoding",
            "url": "https://huggingface.co/papers/2508.19982",
            "abstract": "Prophet, a training-free fast decoding paradigm for diffusion language models, reduces inference time by leveraging early answer convergence without sacrificing quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion language models (DLMs) have recently emerged as an alternative to autoregressive approaches, offering parallel sequence generation and flexible token orders. However, their inference remains slower than that of autoregressive models, primarily due to the cost of bidirectional attention and the large number of refinement steps required for high quality outputs. In this work, we highlight and leverage an overlooked property of DLMs early answer convergence: in many cases, the correct answer can be internally identified by half steps before the final decoding step, both under semi-autoregressive and random remasking schedules. For example, on GSM8K and MMLU, up to 97% and 99% of instances, respectively, can be decoded correctly using only half of the refinement steps. Building on this observation, we introduce Prophet, a training-free fast decoding paradigm that enables early commit decoding. Specifically, Prophet dynamically decides whether to continue refinement or to go \"all-in\" (i.e., decode all remaining tokens in one step), using the confidence gap between the top-2 prediction candidates as the criterion. It integrates seamlessly into existing DLM implementations, incurs negligible overhead, and requires no additional training. Empirical evaluations of LLaDA-8B and Dream-7B across multiple tasks show that Prophet reduces the number of decoding steps by up to 3.4x while preserving high generation quality. These results recast DLM decoding as a problem of when to stop sampling, and demonstrate that early decode convergence provides a simple yet powerful mechanism for accelerating DLM inference, complementary to existing speedup techniques. Our code is publicly available at https://github.com/pixeli99/Prophet.",
            "score": 10,
            "issue_id": 5588,
            "pub_date": "2025-08-27",
            "pub_date_card": {
                "ru": "27 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 27",
                "zh": "8æœˆ27æ—¥"
            },
            "hash": "f8a30e4eb7eff789",
            "authors": [
                "Pengxiang Li",
                "Yefan Zhou",
                "Dilxat Muhtar",
                "Lu Yin",
                "Shilin Yan",
                "Li Shen",
                "Yi Liang",
                "Soroush Vosoughi",
                "Shiwei Liu"
            ],
            "affiliations": [
                "Dartmouth College",
                "ELLIS Institute Tubingen",
                "Google DeepMind",
                "Max Planck Institute for Intelligent Systems",
                "Sun Yat-sen University",
                "The Hong Kong Polytechnic University",
                "University of Surrey"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.19982.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#training",
                    "#diffusion",
                    "#inference",
                    "#optimization"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "Prophet: ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑĞ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Prophet - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (DLM). ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ¾ Ñ€Ğ°Ğ½Ğ½ĞµĞ¹ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ², Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞ¸Ñ‚ÑŒ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€Ğ°Ğ½ÑŒÑˆĞµ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. Prophet Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚, ĞºĞ¾Ğ³Ğ´Ğ° Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ¾ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑŒ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²ÑĞµ Ğ¾ÑÑ‚Ğ°Ğ²ÑˆĞ¸ĞµÑÑ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ·Ğ° Ğ¾Ğ´Ğ¸Ğ½ ÑˆĞ°Ğ³. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° DLM Ğ´Ğ¾ 3.4 Ñ€Ğ°Ğ·, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Accelerating Diffusion Language Models with Early Answer Convergence",
                    "desc": "This paper introduces Prophet, a new method for speeding up the decoding process in diffusion language models (DLMs) without requiring additional training. It takes advantage of early answer convergence, where the correct output can often be identified before completing all refinement steps. By dynamically deciding when to stop refining based on the confidence of predictions, Prophet can reduce the number of decoding steps significantly while maintaining high output quality. The results show that this approach can make DLMs as fast as autoregressive models, enhancing their practical usability."
                },
                "zh": {
                    "title": "Prophetï¼šåŠ é€Ÿæ‰©æ•£è¯­è¨€æ¨¡å‹è§£ç çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºProphetçš„å¿«é€Ÿè§£ç æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜æ‰©æ•£è¯­è¨€æ¨¡å‹çš„æ¨ç†é€Ÿåº¦ã€‚é€šè¿‡åˆ©ç”¨æ—©æœŸç­”æ¡ˆæ”¶æ•›çš„ç‰¹æ€§ï¼ŒProphetèƒ½å¤Ÿåœ¨ä¸ç‰ºç‰²ç”Ÿæˆè´¨é‡çš„æƒ…å†µä¸‹ï¼Œå‡å°‘è§£ç æ­¥éª¤ã€‚ç ”ç©¶è¡¨æ˜ï¼Œåœ¨è®¸å¤šæƒ…å†µä¸‹ï¼Œæ­£ç¡®ç­”æ¡ˆå¯ä»¥åœ¨æœ€ç»ˆè§£ç æ­¥éª¤ä¹‹å‰çš„åŠä¸ªæ­¥éª¤å†…è¢«è¯†åˆ«ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒProphetåœ¨å¤šä¸ªä»»åŠ¡ä¸­å°†è§£ç æ­¥éª¤å‡å°‘äº†æœ€å¤š3.4å€ï¼ŒåŒæ—¶ä¿æŒäº†é«˜è´¨é‡çš„ç”Ÿæˆæ•ˆæœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.19229",
            "title": "StepWiser: Stepwise Generative Judges for Wiser Reasoning",
            "url": "https://huggingface.co/papers/2508.19229",
            "abstract": "A generative judge model, StepWiser, uses reinforcement learning to provide step-by-step reasoning feedback, improving both training and inference performance of policy models.  \t\t\t\t\tAI-generated summary \t\t\t\t As models increasingly leverage multi-step reasoning strategies to solve complex problems, supervising the logical validity of these intermediate steps has become a critical research challenge. Process reward models address this by providing step-by-step feedback, but current approaches have two major drawbacks: they typically function as classifiers without providing explanations, and their reliance on supervised fine-tuning with static datasets limits generalization. Inspired by recent advances, we reframe stepwise reward modeling from a classification task to a reasoning task itself. We thus propose a generative judge that reasons about the policy model's reasoning steps (i.e., meta-reasons), outputting thinking tokens before delivering a final verdict. Our model, StepWiser, is trained by reinforcement learning using relative outcomes of rollouts. We show it provides (i) better judgment accuracy on intermediate steps than existing methods; (ii) can be used to improve the policy model at training time; and (iii) improves inference-time search.",
            "score": 10,
            "issue_id": 5586,
            "pub_date": "2025-08-26",
            "pub_date_card": {
                "ru": "26 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 26",
                "zh": "8æœˆ26æ—¥"
            },
            "hash": "6ffd72787adea812",
            "authors": [
                "Wei Xiong",
                "Wenting Zhao",
                "Weizhe Yuan",
                "Olga Golovneva",
                "Tong Zhang",
                "Jason Weston",
                "Sainbayar Sukhbaatar"
            ],
            "affiliations": [
                "FAIR at Meta",
                "NYU",
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.19229.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#inference",
                    "#training",
                    "#reasoning",
                    "#rl"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞœĞµÑ‚Ğ°-Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ˜Ğ˜",
                    "desc": "StepWiser - ÑÑ‚Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ-ÑÑƒĞ´ÑŒÑ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾ÑÑĞ½ĞµĞ½Ğ¸Ñ ÑĞ²Ğ¾Ğ¸Ñ… Ğ¾Ñ†ĞµĞ½Ğ¾Ğº, Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ ÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸. StepWiser Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ‚ÑŒÑÑ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹-Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğµ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿ĞµÑ€ĞµĞ¾ÑĞ¼Ñ‹ÑĞ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ° Ğ½Ğµ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "StepWiser: Enhancing Reasoning with Generative Feedback",
                    "desc": "The paper introduces StepWiser, a generative judge model that enhances the performance of policy models through reinforcement learning. It addresses the challenge of supervising multi-step reasoning by providing detailed feedback on each reasoning step, rather than just classifying them. Unlike traditional methods that rely on static datasets, StepWiser reframes the task to focus on reasoning itself, allowing for better generalization. The results demonstrate that StepWiser improves judgment accuracy, aids in training policy models, and enhances inference-time search capabilities."
                },
                "zh": {
                    "title": "é€æ­¥æ¨ç†çš„ç”Ÿæˆæ€§è¯„åˆ¤æ¨¡å‹",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§ç”Ÿæˆæ€§è¯„åˆ¤æ¨¡å‹StepWiserï¼Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹ æä¾›é€æ­¥æ¨ç†åé¦ˆï¼Œä»è€Œæå‡ç­–ç•¥æ¨¡å‹çš„è®­ç»ƒå’Œæ¨ç†æ€§èƒ½ã€‚éšç€æ¨¡å‹è¶Šæ¥è¶Šå¤šåœ°é‡‡ç”¨å¤šæ­¥æ¨ç†ç­–ç•¥æ¥è§£å†³å¤æ‚é—®é¢˜ï¼Œç›‘ç£è¿™äº›ä¸­é—´æ­¥éª¤çš„é€»è¾‘æœ‰æ•ˆæ€§æˆä¸ºä¸€ä¸ªé‡è¦çš„ç ”ç©¶æŒ‘æˆ˜ã€‚ç°æœ‰çš„è¿‡ç¨‹å¥–åŠ±æ¨¡å‹è™½ç„¶æä¾›é€æ­¥åé¦ˆï¼Œä½†é€šå¸¸ä»…ä½œä¸ºåˆ†ç±»å™¨ï¼Œç¼ºä¹è§£é‡Šï¼Œå¹¶ä¸”ä¾èµ–é™æ€æ•°æ®é›†çš„ç›‘ç£å¾®è°ƒé™åˆ¶äº†å…¶æ³›åŒ–èƒ½åŠ›ã€‚StepWiseré€šè¿‡å°†é€æ­¥å¥–åŠ±å»ºæ¨¡é‡æ–°æ„å»ºä¸ºæ¨ç†ä»»åŠ¡ï¼Œèƒ½å¤Ÿåœ¨ç»™å‡ºæœ€ç»ˆåˆ¤æ–­ä¹‹å‰è¾“å‡ºæ€è€ƒä»¤ç‰Œï¼Œä»è€Œæé«˜ä¸­é—´æ­¥éª¤çš„åˆ¤æ–­å‡†ç¡®æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.19493",
            "title": "Mind the Third Eye! Benchmarking Privacy Awareness in MLLM-powered\n  Smartphone Agents",
            "url": "https://huggingface.co/papers/2508.19493",
            "abstract": "A large-scale benchmark evaluates the privacy awareness of smartphone agents powered by Multimodal Large Language Models, revealing significant gaps in their ability to protect sensitive user information.  \t\t\t\t\tAI-generated summary \t\t\t\t Smartphones bring significant convenience to users but also enable devices to extensively record various types of personal information. Existing smartphone agents powered by Multimodal Large Language Models (MLLMs) have achieved remarkable performance in automating different tasks. However, as the cost, these agents are granted substantial access to sensitive users' personal information during this operation. To gain a thorough understanding of the privacy awareness of these agents, we present the first large-scale benchmark encompassing 7,138 scenarios to the best of our knowledge. In addition, for privacy context in scenarios, we annotate its type (e.g., Account Credentials), sensitivity level, and location. We then carefully benchmark seven available mainstream smartphone agents. Our results demonstrate that almost all benchmarked agents show unsatisfying privacy awareness (RA), with performance remaining below 60% even with explicit hints. Overall, closed-source agents show better privacy ability than open-source ones, and Gemini 2.0-flash achieves the best, achieving an RA of 67%. We also find that the agents' privacy detection capability is highly related to scenario sensitivity level, i.e., the scenario with a higher sensitivity level is typically more identifiable. We hope the findings enlighten the research community to rethink the unbalanced utility-privacy tradeoff about smartphone agents. Our code and benchmark are available at https://zhixin-l.github.io/SAPA-Bench.",
            "score": 9,
            "issue_id": 5586,
            "pub_date": "2025-08-27",
            "pub_date_card": {
                "ru": "27 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 27",
                "zh": "8æœˆ27æ—¥"
            },
            "hash": "d7a43858b898f00a",
            "authors": [
                "Zhixin Lin",
                "Jungang Li",
                "Shidong Pan",
                "Yibo Shi",
                "Yue Yao",
                "Dongliang Xu"
            ],
            "affiliations": [
                "Columbia University",
                "Hong Kong University of Science and Technology",
                "Hong Kong University of Science and Technology (Guangzhou)",
                "Shandong University",
                "Xian Jiaotong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.19493.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#benchmark",
                    "#ethics",
                    "#agents",
                    "#multimodal"
                ],
                "emoji": "ğŸ”’",
                "ru": {
                    "title": "Ğ¡Ğ¼Ğ°Ñ€Ñ‚Ñ„Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° MLLM Ğ½Ğµ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸Ğ²Ğ°Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸",
                    "desc": "Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ¼Ğ°Ñ€Ñ‚Ñ„Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (MLLM), Ğ·Ğ°Ñ‰Ğ¸Ñ‰Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ´ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¸Ğ· 7138 ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ² Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¾ÑĞ²ĞµĞ´Ğ¾Ğ¼Ğ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ°Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ½ĞµÑƒĞ´Ğ¾Ğ²Ğ»ĞµÑ‚Ğ²Ğ¾Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ğ¾ÑĞ²ĞµĞ´Ğ¾Ğ¼Ğ»ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ°Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ½Ğµ Ğ¿Ñ€ĞµĞ²Ñ‹ÑˆĞ°ÑÑ‰ÑƒÑ 60% Ğ´Ğ°Ğ¶Ğµ Ñ ÑĞ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ°Ğ¼Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ñ‹ Ğ² ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¼Ğ°Ñ€Ñ‚Ñ„Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ·Ğ°Ñ‰Ğ¸Ñ‰Ğ°Ñ‚ÑŒ Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Rethinking Privacy in Smartphone Agents: A Call for Better Awareness",
                    "desc": "This paper evaluates the privacy awareness of smartphone agents that use Multimodal Large Language Models (MLLMs). It presents a large-scale benchmark consisting of 7,138 scenarios to assess how well these agents protect sensitive user information. The findings reveal that most agents perform poorly in privacy awareness, with scores below 60%, and closed-source agents generally outperform open-source ones. The study highlights the need for a better balance between utility and privacy in the design of smartphone agents."
                },
                "zh": {
                    "title": "æ™ºèƒ½æ‰‹æœºåŠ©æ‰‹çš„éšç§ä¿æŠ¤èƒ½åŠ›äºŸå¾…æå‡",
                    "desc": "è¿™ç¯‡è®ºæ–‡è¯„ä¼°äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹é©±åŠ¨çš„æ™ºèƒ½æ‰‹æœºåŠ©æ‰‹åœ¨éšç§ä¿æŠ¤æ–¹é¢çš„èƒ½åŠ›ã€‚ç ”ç©¶å‘ç°ï¼Œè¿™äº›åŠ©æ‰‹åœ¨å¤„ç†æ•æ„Ÿç”¨æˆ·ä¿¡æ¯æ—¶å­˜åœ¨æ˜¾è‘—çš„éšç§æ„è¯†ç¼ºå£ã€‚é€šè¿‡å¯¹7138ä¸ªåœºæ™¯è¿›è¡Œå¤§è§„æ¨¡åŸºå‡†æµ‹è¯•ï¼Œç»“æœæ˜¾ç¤ºå‡ ä¹æ‰€æœ‰è¢«æµ‹è¯•çš„åŠ©æ‰‹éšç§æ„è¯†è¡¨ç°ä¸ä½³ï¼Œå¾—åˆ†å‡ä½äº60%ã€‚ç ”ç©¶è¿˜å‘ç°ï¼Œé—­æºåŠ©æ‰‹çš„éšç§ä¿æŠ¤èƒ½åŠ›æ™®éä¼˜äºå¼€æºåŠ©æ‰‹ï¼Œä¸”åœºæ™¯çš„æ•æ„Ÿæ€§ä¸éšç§æ£€æµ‹èƒ½åŠ›å¯†åˆ‡ç›¸å…³ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.20088",
            "title": "AudioStory: Generating Long-Form Narrative Audio with Large Language\n  Models",
            "url": "https://huggingface.co/papers/2508.20088",
            "abstract": "AudioStory integrates large language models with text-to-audio systems to generate coherent, long-form audio narratives through a unified framework with decoupled bridging mechanisms and end-to-end training.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in text-to-audio (TTA) generation excel at synthesizing short audio clips but struggle with long-form narrative audio, which requires temporal coherence and compositional reasoning. To address this gap, we propose AudioStory, a unified framework that integrates large language models (LLMs) with TTA systems to generate structured, long-form audio narratives. AudioStory possesses strong instruction-following reasoning generation capabilities. It employs LLMs to decompose complex narrative queries into temporally ordered sub-tasks with contextual cues, enabling coherent scene transitions and emotional tone consistency. AudioStory has two appealing features: (1) Decoupled bridging mechanism: AudioStory disentangles LLM-diffuser collaboration into two specialized components, i.e., a bridging query for intra-event semantic alignment and a residual query for cross-event coherence preservation. (2) End-to-end training: By unifying instruction comprehension and audio generation within a single end-to-end framework, AudioStory eliminates the need for modular training pipelines while enhancing synergy between components. Furthermore, we establish a benchmark AudioStory-10K, encompassing diverse domains such as animated soundscapes and natural sound narratives. Extensive experiments show the superiority of AudioStory on both single-audio generation and narrative audio generation, surpassing prior TTA baselines in both instruction-following ability and audio fidelity. Our code is available at https://github.com/TencentARC/AudioStory",
            "score": 8,
            "issue_id": 5587,
            "pub_date": "2025-08-27",
            "pub_date_card": {
                "ru": "27 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 27",
                "zh": "8æœˆ27æ—¥"
            },
            "hash": "0289231c11d7f612",
            "authors": [
                "Yuxin Guo",
                "Teng Wang",
                "Yuying Ge",
                "Shijie Ma",
                "Yixiao Ge",
                "Wei Zou",
                "Ying Shan"
            ],
            "affiliations": [
                "ARC Lab, Tencent",
                "MAIS, Institute of Automation, CAS, Beijing",
                "School of Artificial Intelligence, University of Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.20088.jpg",
            "data": {
                "categories": [
                    "#audio",
                    "#benchmark",
                    "#story_generation",
                    "#multimodal",
                    "#long_context"
                ],
                "emoji": "ğŸ§",
                "ru": {
                    "title": "AudioStory: Ğ˜Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ğ˜Ğ˜ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ½Ğ°Ñ€Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ¾Ğ²",
                    "desc": "AudioStory - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ°Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ°ÑƒĞ´Ğ¸Ğ¾ (TTA) Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ¿Ğ¾Ğ²ĞµÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ LLM Ğ´Ğ»Ñ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ½Ğ°Ñ€Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ½Ğ° ÑƒĞ¿Ğ¾Ñ€ÑĞ´Ğ¾Ñ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ°Ğ¼Ğ¸, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ñ‹ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑÑ†ĞµĞ½Ğ°Ğ¼Ğ¸ Ğ¸ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ. AudioStory Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ²ÑĞ·Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑĞºĞ²Ğ¾Ğ·Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ AudioStory Ğ½Ğ°Ğ´ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ TTA-Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ² ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ°ÑƒĞ´Ğ¸Ğ¾."
                },
                "en": {
                    "title": "Transforming Text into Engaging Audio Narratives with AudioStory",
                    "desc": "AudioStory is a novel framework that combines large language models (LLMs) with text-to-audio (TTA) systems to create long, coherent audio narratives. It addresses the challenge of maintaining temporal coherence and compositional reasoning in audio storytelling. The framework features a decoupled bridging mechanism that separates the tasks of semantic alignment and coherence preservation, allowing for better scene transitions and emotional consistency. Additionally, it utilizes end-to-end training to streamline the process, enhancing the collaboration between components and improving overall audio quality and instruction-following capabilities."
                },
                "zh": {
                    "title": "AudioStoryï¼šç”Ÿæˆè¿è´¯é•¿ç¯‡éŸ³é¢‘å™äº‹çš„åˆ›æ–°æ¡†æ¶",
                    "desc": "AudioStoryæ˜¯ä¸€ä¸ªå°†å¤§å‹è¯­è¨€æ¨¡å‹ä¸æ–‡æœ¬åˆ°éŸ³é¢‘ç³»ç»Ÿç»“åˆçš„æ¡†æ¶ï¼Œæ—¨åœ¨ç”Ÿæˆè¿è´¯çš„é•¿ç¯‡éŸ³é¢‘å™äº‹ã€‚å®ƒé€šè¿‡è§£è€¦çš„æ¡¥æ¥æœºåˆ¶å’Œç«¯åˆ°ç«¯çš„è®­ç»ƒï¼Œè§£å†³äº†çŸ­éŸ³é¢‘ç”Ÿæˆä¸é•¿éŸ³é¢‘å™äº‹ä¹‹é—´çš„å·®è·ã€‚è¯¥ç³»ç»Ÿèƒ½å¤Ÿå°†å¤æ‚çš„å™äº‹æŸ¥è¯¢åˆ†è§£ä¸ºæœ‰åºçš„å­ä»»åŠ¡ï¼Œä»è€Œå®ç°åœºæ™¯çš„è¿è´¯è¿‡æ¸¡å’Œæƒ…æ„ŸåŸºè°ƒçš„ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAudioStoryåœ¨éŸ³é¢‘ç”Ÿæˆå’Œå™äº‹éŸ³é¢‘ç”Ÿæˆæ–¹é¢çš„è¡¨ç°ä¼˜äºä¹‹å‰çš„åŸºçº¿ï¼Œå±•ç°äº†å…¶å“è¶Šçš„æŒ‡ä»¤è·Ÿéšèƒ½åŠ›å’ŒéŸ³é¢‘ä¿çœŸåº¦ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.19527",
            "title": "MotionFlux: Efficient Text-Guided Motion Generation through Rectified\n  Flow Matching and Preference Alignment",
            "url": "https://huggingface.co/papers/2508.19527",
            "abstract": "TAPO and MotionFLUX form a unified system that enhances semantic consistency and motion quality in text-driven motion generation while achieving real-time synthesis.  \t\t\t\t\tAI-generated summary \t\t\t\t Motion generation is essential for animating virtual characters and embodied agents. While recent text-driven methods have made significant strides, they often struggle with achieving precise alignment between linguistic descriptions and motion semantics, as well as with the inefficiencies of slow, multi-step inference. To address these issues, we introduce TMR++ Aligned Preference Optimization (TAPO), an innovative framework that aligns subtle motion variations with textual modifiers and incorporates iterative adjustments to reinforce semantic grounding. To further enable real-time synthesis, we propose MotionFLUX, a high-speed generation framework based on deterministic rectified flow matching. Unlike traditional diffusion models, which require hundreds of denoising steps, MotionFLUX constructs optimal transport paths between noise distributions and motion spaces, facilitating real-time synthesis. The linearized probability paths reduce the need for multi-step sampling typical of sequential methods, significantly accelerating inference time without sacrificing motion quality. Experimental results demonstrate that, together, TAPO and MotionFLUX form a unified system that outperforms state-of-the-art approaches in both semantic consistency and motion quality, while also accelerating generation speed. The code and pretrained models will be released.",
            "score": 6,
            "issue_id": 5587,
            "pub_date": "2025-08-27",
            "pub_date_card": {
                "ru": "27 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 27",
                "zh": "8æœˆ27æ—¥"
            },
            "hash": "7410456922eb42b6",
            "authors": [
                "Zhiting Gao",
                "Dan Song",
                "Diqiong Jiang",
                "Chao Xue",
                "An-An Liu"
            ],
            "affiliations": [
                "China University of Petroleum, China",
                "Tiandy Technologies, China",
                "Tianjin University, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.19527.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#games",
                    "#inference",
                    "#multimodal",
                    "#agents",
                    "#optimization"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹: Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¾Ğ´Ğ½Ğ¾Ğ¼ Ñ„Ğ»Ğ°ĞºĞ¾Ğ½Ğµ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹. TAPO (TMR++ Aligned Preference Optimization) ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ĞµĞ¼, Ğ° MotionFLUX Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ñ€Ğ°Ğ½ÑĞ¿Ğ¾Ñ€Ñ‚Ğ½Ñ‹Ğµ Ğ¿ÑƒÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸ ÑˆÑƒĞ¼Ğ° Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°Ğ¼Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ ÑÑ‚Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ½Ğ°Ğ´ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ² Ğ¿Ğ»Ğ°Ğ½Ğµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Real-Time Motion Generation with Semantic Precision",
                    "desc": "This paper presents a unified system called TAPO and MotionFLUX that improves the quality and consistency of motion generation from text descriptions. TAPO uses Aligned Preference Optimization to ensure that subtle motion changes accurately reflect the meanings of the text, enhancing semantic alignment. MotionFLUX introduces a fast generation method that avoids the slow multi-step processes of traditional models by using optimal transport paths for real-time synthesis. Together, these innovations allow for high-quality motion generation that is both semantically consistent and efficient, outperforming existing methods."
                },
                "zh": {
                    "title": "å®æ—¶è¿åŠ¨ç”Ÿæˆçš„æ–°çªç ´",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºTAPOå’ŒMotionFLUXçš„ç»Ÿä¸€ç³»ç»Ÿï¼Œæ—¨åœ¨æé«˜æ–‡æœ¬é©±åŠ¨çš„è¿åŠ¨ç”Ÿæˆä¸­çš„è¯­ä¹‰ä¸€è‡´æ€§å’Œè¿åŠ¨è´¨é‡ï¼ŒåŒæ—¶å®ç°å®æ—¶åˆæˆã€‚TAPOé€šè¿‡å¯¹ç»†å¾®è¿åŠ¨å˜åŒ–ä¸æ–‡æœ¬ä¿®é¥°ç¬¦çš„å¯¹é½ï¼Œç»“åˆè¿­ä»£è°ƒæ•´ï¼Œå¢å¼ºäº†è¯­ä¹‰åŸºç¡€ã€‚MotionFLUXåˆ™æ˜¯ä¸€ç§åŸºäºç¡®å®šæ€§ä¿®æ­£æµåŒ¹é…çš„é«˜é€Ÿç”Ÿæˆæ¡†æ¶ï¼Œèƒ½å¤Ÿå¿«é€Ÿæ„å»ºå™ªå£°åˆ†å¸ƒä¸è¿åŠ¨ç©ºé—´ä¹‹é—´çš„æœ€ä½³ä¼ è¾“è·¯å¾„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿåœ¨è¯­ä¹‰ä¸€è‡´æ€§ã€è¿åŠ¨è´¨é‡å’Œç”Ÿæˆé€Ÿåº¦ä¸Šå‡ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.18179",
            "title": "SEAM: Semantically Equivalent Across Modalities Benchmark for\n  Vision-Language Models",
            "url": "https://huggingface.co/papers/2508.18179",
            "abstract": "SEAM is a benchmark that evaluates vision-language models' reasoning consistency across modalities using semantically equivalent inputs, revealing systematic modality imbalance and visual hallucinations.  \t\t\t\t\tAI-generated summary \t\t\t\t Evaluating whether vision-language models (VLMs) reason consistently across representations is challenging because modality comparisons are typically confounded by task differences and asymmetric information. We introduce SEAM, a benchmark that pairs semantically equivalent inputs across four domains that have existing standardized textual and visual notations. By employing distinct notation systems across modalities, in contrast to OCR-based image-text pairing, SEAM provides a rigorous comparative assessment of the textual-symbolic and visual-spatial reasoning capabilities of VLMs. Across 21 contemporary models, we observe systematic modality imbalance: vision frequently lags language in overall performance, despite the problems containing semantically equivalent information, and cross-modal agreement is relatively low. Our error analysis reveals two main drivers: textual perception failures from tokenization in domain notation and visual perception failures that induce hallucinations. We also show that our results are largely robust to visual transformations. SEAM establishes a controlled, semantically equivalent setting for measuring and improving modality-agnostic reasoning.",
            "score": 6,
            "issue_id": 5601,
            "pub_date": "2025-08-25",
            "pub_date_card": {
                "ru": "25 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 25",
                "zh": "8æœˆ25æ—¥"
            },
            "hash": "f4c3fc42a2de9fce",
            "authors": [
                "Zhenwei Tang",
                "Difan Jiao",
                "Blair Yang",
                "Ashton Anderson"
            ],
            "affiliations": [
                "Coolwei AI Lab",
                "Department of Computer Science, University of Toronto"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.18179.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#multimodal",
                    "#hallucinations",
                    "#benchmark",
                    "#cv"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "SEAM: Ğ¢Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ˜Ğ˜",
                    "desc": "SEAM - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµÑÑ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞºĞ²Ğ¸Ğ²Ğ°Ğ»ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑÑ…. Ğ¢ĞµÑÑ‚ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ´Ğ¸ÑĞ±Ğ°Ğ»Ğ°Ğ½Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ² ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… vision-language Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. SEAM Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ… Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ ÑÑ‚Ñ€Ğ¾Ğ³ÑƒÑ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾-ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ¾Ñ‚ÑÑ‚Ğ°ĞµÑ‚ Ğ¾Ñ‚ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¿Ğ¾ Ğ¾Ğ±Ñ‰ĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ğ½ĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞºĞ²Ğ¸Ğ²Ğ°Ğ»ĞµĞ½Ñ‚Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ."
                },
                "en": {
                    "title": "SEAM: Bridging the Gap in Vision-Language Reasoning",
                    "desc": "The paper introduces SEAM, a benchmark designed to evaluate the reasoning consistency of vision-language models (VLMs) across different modalities. It addresses the challenge of comparing modalities by using semantically equivalent inputs from four standardized domains, allowing for a fair assessment of both textual-symbolic and visual-spatial reasoning. The study finds that vision often underperforms compared to language, highlighting issues like visual hallucinations and tokenization errors in text perception. SEAM aims to provide a controlled environment to enhance the understanding and improvement of reasoning capabilities in VLMs."
                },
                "zh": {
                    "title": "SEAMï¼šè¯„ä¼°è§†è§‰-è¯­è¨€æ¨¡å‹æ¨ç†ä¸€è‡´æ€§çš„åŸºå‡†",
                    "desc": "SEAMæ˜¯ä¸€ä¸ªåŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨ä¸åŒæ¨¡æ€ä¸‹çš„æ¨ç†ä¸€è‡´æ€§ã€‚å®ƒé€šè¿‡é…å¯¹å››ä¸ªé¢†åŸŸä¸­è¯­ä¹‰ç­‰ä»·çš„è¾“å…¥ï¼Œæ­ç¤ºäº†æ¨¡æ€ä¸å¹³è¡¡å’Œè§†è§‰å¹»è§‰çš„é—®é¢˜ã€‚ç ”ç©¶å‘ç°ï¼Œå°½ç®¡è¾“å…¥ä¿¡æ¯æ˜¯ç­‰ä»·çš„ï¼Œè§†è§‰æ¨¡æ€çš„è¡¨ç°é€šå¸¸ä½äºè¯­è¨€æ¨¡æ€ï¼Œä¸”è·¨æ¨¡æ€çš„ä¸€è‡´æ€§è¾ƒä½ã€‚é€šè¿‡é”™è¯¯åˆ†æï¼Œæˆ‘ä»¬è¯†åˆ«å‡ºæ–‡æœ¬æ„ŸçŸ¥å’Œè§†è§‰æ„ŸçŸ¥çš„å¤±è´¥æ˜¯å¯¼è‡´è¿™äº›é—®é¢˜çš„ä¸»è¦åŸå› ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.20033",
            "title": "DeepScholar-Bench: A Live Benchmark and Automated Evaluation for\n  Generative Research Synthesis",
            "url": "https://huggingface.co/papers/2508.20033",
            "abstract": "DeepScholar-bench evaluates generative research synthesis systems by assessing their performance in creating related work sections from recent ArXiv papers, focusing on knowledge synthesis, retrieval quality, and verifiability.  \t\t\t\t\tAI-generated summary \t\t\t\t The ability to research and synthesize knowledge is central to human expertise and progress. An emerging class of systems promises these exciting capabilities through generative research synthesis, performing retrieval over the live web and synthesizing discovered sources into long-form, cited summaries. However, evaluating such systems remains an open challenge: existing question-answering benchmarks focus on short-form factual responses, while expert-curated datasets risk staleness and data contamination. Both fail to capture the complexity and evolving nature of real research synthesis tasks. In this work, we introduce DeepScholar-bench, a live benchmark and holistic, automated evaluation framework designed to evaluate generative research synthesis. DeepScholar-bench draws queries from recent, high-quality ArXiv papers and focuses on a real research synthesis task: generating the related work sections of a paper by retrieving, synthesizing, and citing prior research. Our evaluation framework holistically assesses performance across three key dimensions, knowledge synthesis, retrieval quality, and verifiability. We also develop DeepScholar-base, a reference pipeline implemented efficiently using the LOTUS API. Using the DeepScholar-bench framework, we perform a systematic evaluation of prior open-source systems, search AI's, OpenAI's DeepResearch, and DeepScholar-base. We find that DeepScholar-base establishes a strong baseline, attaining competitive or higher performance than each other method. We also find that DeepScholar-bench remains far from saturated, with no system exceeding a score of 19% across all metrics. These results underscore the difficulty of DeepScholar-bench, as well as its importance for progress towards AI systems capable of generative research synthesis. We make our code available at https://github.com/guestrin-lab/deepscholar-bench.",
            "score": 3,
            "issue_id": 5587,
            "pub_date": "2025-08-27",
            "pub_date_card": {
                "ru": "27 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 27",
                "zh": "8æœˆ27æ—¥"
            },
            "hash": "10b7a89ab4fa5d36",
            "authors": [
                "Liana Patel",
                "Negar Arabzadeh",
                "Harshit Gupta",
                "Ankita Sundar",
                "Ion Stoica",
                "Matei Zaharia",
                "Carlos Guestrin"
            ],
            "affiliations": [
                "Stanford University",
                "UC Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.20033.jpg",
            "data": {
                "categories": [
                    "#rag",
                    "#benchmark",
                    "#science",
                    "#open_source"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ˜Ğ˜-ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ² Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ¼ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğµ",
                    "desc": "DeepScholar-bench - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½ĞµĞ´Ğ°Ğ²Ğ½Ğ¸Ğµ ÑÑ‚Ğ°Ñ‚ÑŒĞ¸ Ñ ArXiv Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ğ´ĞµĞ»Ğ¾Ğ² 'Ğ¡Ğ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹'. ĞÑ†ĞµĞ½ĞºĞ° Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ Ğ¿Ğ¾ Ñ‚Ñ€ĞµĞ¼ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼ Ğ°ÑĞ¿ĞµĞºÑ‚Ğ°Ğ¼: ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹, ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ğ¾ÑÑ‚ÑŒ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ DeepScholar-base, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Evaluating AI's Ability to Synthesize Research Knowledge",
                    "desc": "DeepScholar-bench is a new evaluation framework designed to assess generative research synthesis systems, which create related work sections from recent research papers. It focuses on three main aspects: knowledge synthesis, retrieval quality, and verifiability, addressing the limitations of existing benchmarks that do not capture the complexity of real research tasks. The framework uses queries from high-quality ArXiv papers to evaluate how well these systems can retrieve and synthesize information into coherent summaries with proper citations. The results show that while DeepScholar-base performs competitively, there is still significant room for improvement in generative research synthesis systems, highlighting the challenges in this area."
                },
                "zh": {
                    "title": "DeepScholar-benchï¼šæ¨åŠ¨ç”Ÿæˆç ”ç©¶ç»¼åˆçš„è¯„ä¼°æ–°æ ‡å‡†",
                    "desc": "DeepScholar-bench æ˜¯ä¸€ä¸ªè¯„ä¼°ç”Ÿæˆç ”ç©¶ç»¼åˆç³»ç»Ÿçš„åŸºå‡†ï¼Œä¸“æ³¨äºä»æœ€æ–°çš„ ArXiv è®ºæ–‡ä¸­åˆ›å»ºç›¸å…³å·¥ä½œéƒ¨åˆ†çš„æ€§èƒ½ã€‚è¯¥æ¡†æ¶è¯„ä¼°çŸ¥è¯†ç»¼åˆã€æ£€ç´¢è´¨é‡å’Œå¯éªŒè¯æ€§ä¸‰ä¸ªå…³é”®ç»´åº¦ã€‚é€šè¿‡ä»é«˜è´¨é‡çš„ ArXiv è®ºæ–‡ä¸­æå–æŸ¥è¯¢ï¼ŒDeepScholar-bench æ—¨åœ¨è§£å†³ç°æœ‰è¯„ä¼°æ–¹æ³•æ— æ³•æ•æ‰çœŸå®ç ”ç©¶ç»¼åˆä»»åŠ¡å¤æ‚æ€§çš„æŒ‘æˆ˜ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒDeepScholar-base åœ¨å„é¡¹æŒ‡æ ‡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾ç¤ºå‡ºç”Ÿæˆç ”ç©¶ç»¼åˆç³»ç»Ÿçš„å·¨å¤§æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.19559",
            "title": "Taming the Chaos: Coordinated Autoscaling for Heterogeneous and\n  Disaggregated LLM Inference",
            "url": "https://huggingface.co/papers/2508.19559",
            "abstract": "HeteroScale, a coordinated autoscaling framework, improves GPU utilization and efficiency in serving large language models by addressing challenges in heterogeneous hardware and network constraints in Prefill-Decode architectures.  \t\t\t\t\tAI-generated summary \t\t\t\t Serving Large Language Models (LLMs) is a GPU-intensive task where traditional autoscalers fall short, particularly for modern Prefill-Decode (P/D) disaggregated architectures. This architectural shift, while powerful, introduces significant operational challenges, including inefficient use of heterogeneous hardware, network bottlenecks, and critical imbalances between prefill and decode stages. We introduce HeteroScale, a coordinated autoscaling framework that addresses the core challenges of P/D disaggregated serving. HeteroScale combines a topology-aware scheduler that adapts to heterogeneous hardware and network constraints with a novel metric-driven policy derived from the first large-scale empirical study of autoscaling signals in production. By leveraging a single, robust metric to jointly scale prefill and decode pools, HeteroScale maintains architectural balance while ensuring efficient, adaptive resource management. Deployed in a massive production environment on tens of thousands of GPUs, HeteroScale has proven its effectiveness, increasing average GPU utilization by a significant 26.6 percentage points and saving hundreds of thousands of GPU-hours daily, all while upholding stringent service level objectives.",
            "score": 3,
            "issue_id": 5587,
            "pub_date": "2025-08-27",
            "pub_date_card": {
                "ru": "27 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 27",
                "zh": "8æœˆ27æ—¥"
            },
            "hash": "d2eb0c8a7e647879",
            "authors": [
                "Rongzhi Li",
                "Ruogu Du",
                "Zefang Chu",
                "Sida Zhao",
                "Chunlei Han",
                "Zuocheng Shi",
                "Yiwen Shao",
                "Huanle Han",
                "Long Huang",
                "Zherui Liu",
                "Shufan Liu"
            ],
            "affiliations": [
                "ByteDance Seed",
                "National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.19559.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#architecture",
                    "#inference",
                    "#optimization"
                ],
                "emoji": "âš–ï¸",
                "ru": {
                    "title": "Ğ‘Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ° Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑĞ»ÑƒĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "HeteroScale - ÑÑ‚Ğ¾ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑĞ»ÑƒĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ½Ğ° Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ³ĞµÑ‚ĞµÑ€Ğ¾Ğ³ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ğ¾Ñ€ÑƒĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑĞµÑ‚ĞµĞ²Ñ‹Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°Ñ… Prefill-Decode. HeteroScale Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸Ğº Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ñ‚Ğ¾Ğ¿Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸ Ğ¸ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºÑƒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº, Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ· ÑĞ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ’Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ğµ HeteroScale Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ ÑÑ€ĞµĞ´Ğµ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑĞ¸Ğ»Ğ¾ ÑƒÑ‚Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ GPU Ğ¸ ÑÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ğ»Ğ¾ ÑĞ¾Ñ‚Ğ½Ğ¸ Ñ‚Ñ‹ÑÑÑ‡ GPU-Ñ‡Ğ°ÑĞ¾Ğ² ĞµĞ¶ĞµĞ´Ğ½ĞµĞ²Ğ½Ğ¾."
                },
                "en": {
                    "title": "HeteroScale: Optimizing GPU Efficiency for Large Language Models",
                    "desc": "HeteroScale is a new framework designed to improve the efficiency of GPU usage when serving large language models, especially in complex Prefill-Decode architectures. It tackles the challenges posed by different types of hardware and network limitations that can lead to inefficient resource allocation. By using a smart scheduler that understands the system's topology and a unique metric-driven policy, HeteroScale can effectively balance the demands of both the prefill and decode stages. In real-world applications, it has significantly boosted GPU utilization and reduced wasted resources, demonstrating its effectiveness in large-scale environments."
                },
                "zh": {
                    "title": "HeteroScaleï¼šæå‡å¤§è¯­è¨€æ¨¡å‹GPUæ•ˆç‡çš„æ™ºèƒ½æ‰©å±•æ¡†æ¶",
                    "desc": "HeteroScaleæ˜¯ä¸€ä¸ªåè°ƒçš„è‡ªåŠ¨æ‰©å±•æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å¤§è¯­è¨€æ¨¡å‹çš„GPUåˆ©ç”¨ç‡å’Œæ•ˆç‡ã€‚å®ƒè§£å†³äº†åœ¨Prefill-Decodeæ¶æ„ä¸­ï¼Œå¼‚æ„ç¡¬ä»¶å’Œç½‘ç»œé™åˆ¶å¸¦æ¥çš„æŒ‘æˆ˜ã€‚é€šè¿‡ç»“åˆæ‹“æ‰‘æ„ŸçŸ¥è°ƒåº¦å™¨å’ŒåŸºäºæ–°æŒ‡æ ‡çš„ç­–ç•¥ï¼ŒHeteroScaleèƒ½å¤Ÿæœ‰æ•ˆç®¡ç†é¢„å¡«å……å’Œè§£ç é˜¶æ®µçš„èµ„æºã€‚åœ¨å¤§è§„æ¨¡ç”Ÿäº§ç¯å¢ƒä¸­ï¼ŒHeteroScaleæ˜¾è‘—æé«˜äº†GPUåˆ©ç”¨ç‡ï¼Œå¹¶èŠ‚çœäº†å¤§é‡çš„GPUæ—¶é—´ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-08-27.html",
    "link_next": "2025-08-29.html",
    "link_month": "2025-08.html",
    "short_date_prev": {
        "ru": "27.08",
        "en": "08/27",
        "zh": "8æœˆ27æ—¥"
    },
    "short_date_next": {
        "ru": "29.08",
        "en": "08/29",
        "zh": "8æœˆ29æ—¥"
    },
    "categories": {
        "#dataset": 2,
        "#data": 1,
        "#benchmark": 6,
        "#agents": 4,
        "#cv": 2,
        "#rl": 2,
        "#rlhf": 0,
        "#rag": 1,
        "#plp": 0,
        "#inference": 5,
        "#3d": 0,
        "#audio": 2,
        "#video": 2,
        "#multimodal": 8,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 1,
        "#training": 7,
        "#robotics": 0,
        "#agi": 0,
        "#games": 2,
        "#interpretability": 1,
        "#reasoning": 3,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 0,
        "#optimization": 6,
        "#survey": 0,
        "#diffusion": 3,
        "#alignment": 0,
        "#story_generation": 1,
        "#hallucinations": 3,
        "#long_context": 2,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 4,
        "#small_models": 0,
        "#science": 2,
        "#low_resource": 1
    }
}