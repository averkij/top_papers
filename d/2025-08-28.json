{
    "date": {
        "ru": "28 августа",
        "en": "August 28",
        "zh": "8月28日"
    },
    "time_utc": "2025-08-28 20:13",
    "weekday": 3,
    "issue_id": 5603,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2508.15882",
            "title": "Beyond Transcription: Mechanistic Interpretability in ASR",
            "url": "https://huggingface.co/papers/2508.15882",
            "abstract": "Interpretability methods like logit lens, linear probing, and activation patching are applied to ASR to uncover internal dynamics, repetition hallucinations, and semantic biases, enhancing model transparency and robustness.  \t\t\t\t\tAI-generated summary \t\t\t\t Interpretability methods have recently gained significant attention, particularly in the context of large language models, enabling insights into linguistic representations, error detection, and model behaviors such as hallucinations and repetitions. However, these techniques remain underexplored in automatic speech recognition (ASR), despite their potential to advance both the performance and interpretability of ASR systems. In this work, we adapt and systematically apply established interpretability methods such as logit lens, linear probing, and activation patching, to examine how acoustic and semantic information evolves across layers in ASR systems. Our experiments reveal previously unknown internal dynamics, including specific encoder-decoder interactions responsible for repetition hallucinations and semantic biases encoded deep within acoustic representations. These insights demonstrate the benefits of extending and applying interpretability techniques to speech recognition, opening promising directions for future research on improving model transparency and robustness.",
            "score": 67,
            "issue_id": 5593,
            "pub_date": "2025-08-21",
            "pub_date_card": {
                "ru": "21 августа",
                "en": "August 21",
                "zh": "8月21日"
            },
            "hash": "092f970748bd5392",
            "authors": [
                "Neta Glazer",
                "Yael Segal-Feldman",
                "Hilit Segev",
                "Aviv Shamsian",
                "Asaf Buchnick",
                "Gill Hetz",
                "Ethan Fetaya",
                "Joseph Keshet",
                "Aviv Navon"
            ],
            "affiliations": [
                "aiOla Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.15882.jpg",
            "data": {
                "categories": [
                    "#hallucinations",
                    "#audio",
                    "#interpretability"
                ],
                "emoji": "🎙️",
                "ru": {
                    "title": "Заглянуть внутрь ASR: раскрытие тайн распознавания речи",
                    "desc": "В статье рассматривается применение методов интерпретируемости, таких как logit lens, линейное зондирование и активационное патчинг, к системам автоматического распознавания речи (ASR). Исследование раскрывает внутреннюю динамику ASR-моделей, включая взаимодействие энкодера и декодера, ответственное за галлюцинации повторений. Авторы обнаружили семантические смещения, закодированные в акустических представлениях глубоких слоев модели. Результаты демонстрируют потенциал методов интерпретируемости для повышения прозрачности и надежности систем ASR."
                },
                "en": {
                    "title": "Unlocking the Secrets of Speech Recognition Models",
                    "desc": "This paper explores the use of interpretability methods in automatic speech recognition (ASR) systems to enhance understanding of their internal workings. Techniques like logit lens, linear probing, and activation patching are applied to reveal how acoustic and semantic information changes throughout the model's layers. The study uncovers important dynamics, such as interactions between the encoder and decoder that lead to issues like repetition hallucinations and semantic biases. By applying these methods, the research highlights the potential for improving the transparency and robustness of ASR models, paving the way for future advancements in the field."
                },
                "zh": {
                    "title": "提升ASR系统透明度与鲁棒性的可解释性方法",
                    "desc": "本文探讨了可解释性方法在自动语音识别（ASR）中的应用，包括logit lens、线性探测和激活补丁等技术。这些方法帮助我们揭示了ASR系统内部的动态变化、重复幻觉和语义偏见，从而提高了模型的透明度和鲁棒性。尽管可解释性方法在大型语言模型中得到了广泛关注，但在ASR领域的应用仍然较少。通过系统地应用这些方法，我们发现了ASR系统中声学和语义信息在不同层次上的演变，推动了未来研究的方向。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.19652",
            "title": "Self-Rewarding Vision-Language Model via Reasoning Decomposition",
            "url": "https://huggingface.co/papers/2508.19652",
            "abstract": "Vision-SR1 uses reinforcement learning to enhance visual reasoning in vision-language models by decomposing the process into visual perception and language reasoning stages, improving accuracy and reducing hallucinations.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language Models (VLMs) often suffer from visual hallucinations, saying things that are not actually in the image, and language shortcuts, where they skip the visual part and just rely on text priors. These issues arise because most post-training methods for VLMs rely on simple verifiable answer matching and supervise only final outputs, leaving intermediate visual reasoning without explicit guidance. As a result, VLMs receive sparse visual signals and often learn to prioritize language-based reasoning over visual perception. To mitigate this, some existing methods add visual supervision using human annotations or distilled labels from external large models. However, human annotations are labor-intensive and costly, and because external signals cannot adapt to the evolving policy, they cause distributional shifts that can lead to reward hacking. In this paper, we introduce Vision-SR1, a self-rewarding method that improves visual reasoning without relying on external visual supervisions via reinforcement learning. Vision-SR1 decomposes VLM reasoning into two stages: visual perception and language reasoning. The model is first prompted to produce self-contained visual perceptions that are sufficient to answer the question without referring back the input image. To validate this self-containment, the same VLM model is then re-prompted to perform language reasoning using only the generated perception as input to compute reward. This self-reward is combined with supervision on final outputs, providing a balanced training signal that strengthens both visual perception and language reasoning. Our experiments demonstrate that Vision-SR1 improves visual reasoning, mitigates visual hallucinations, and reduces reliance on language shortcuts across diverse vision-language tasks.",
            "score": 59,
            "issue_id": 5585,
            "pub_date": "2025-08-27",
            "pub_date_card": {
                "ru": "27 августа",
                "en": "August 27",
                "zh": "8月27日"
            },
            "hash": "990502a2cc19d192",
            "authors": [
                "Zongxia Li",
                "Wenhao Yu",
                "Chengsong Huang",
                "Rui Liu",
                "Zhenwen Liang",
                "Fuxiao Liu",
                "Jingxi Che",
                "Dian Yu",
                "Jordan Boyd-Graber",
                "Haitao Mi",
                "Dong Yu"
            ],
            "affiliations": [
                "Tencent AI Lab, Seattle",
                "University of Maryland, College Park",
                "Washington University in St. Louis"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.19652.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#hallucinations",
                    "#training",
                    "#multimodal",
                    "#reasoning"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Улучшение визуального мышления ИИ без внешних аннотаций",
                    "desc": "Vision-SR1 - это метод улучшения визуального мышления в мультимодальных моделях с помощью обучения с подкреплением. Он разделяет процесс на этапы визуального восприятия и языкового рассуждения, что повышает точность и уменьшает галлюцинации. Метод использует самовознаграждение, не полагаясь на внешние визуальные аннотации. Эксперименты показывают, что Vision-SR1 улучшает визуальное мышление, снижает визуальные галлюцинации и уменьшает зависимость от языковых шаблонов в различных задачах компьютерного зрения и обработки естественного языка."
                },
                "en": {
                    "title": "Enhancing Visual Reasoning with Self-Rewarding Learning",
                    "desc": "Vision-SR1 is a novel approach that enhances visual reasoning in vision-language models (VLMs) by using reinforcement learning. It breaks down the reasoning process into two distinct stages: visual perception and language reasoning, allowing for more focused training. By generating self-contained visual perceptions, the model can validate its understanding without needing to refer back to the original image. This method reduces visual hallucinations and reliance on language shortcuts, leading to improved accuracy in various vision-language tasks."
                },
                "zh": {
                    "title": "Vision-SR1：提升视觉推理的自我奖励方法",
                    "desc": "Vision-SR1是一种利用强化学习的方法，旨在增强视觉语言模型中的视觉推理能力。该方法将推理过程分解为视觉感知和语言推理两个阶段，从而提高了模型的准确性并减少了幻觉现象。通过自我奖励机制，Vision-SR1不依赖外部视觉监督，能够有效地训练模型进行更好的视觉感知和语言推理。实验结果表明，Vision-SR1在多种视觉语言任务中显著改善了视觉推理能力，降低了对语言捷径的依赖。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.20072",
            "title": "Discrete Diffusion VLA: Bringing Discrete Diffusion to Action Decoding\n  in Vision-Language-Action Policies",
            "url": "https://huggingface.co/papers/2508.20072",
            "abstract": "Discrete Diffusion VLA uses a single-transformer policy with discrete diffusion to model actions, improving decoding order, consistency, and performance over autoregressive and continuous diffusion methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language-Action (VLA) models adapt large vision-language backbones to map images and instructions to robot actions. However, prevailing VLA decoders either generate actions autoregressively in a fixed left-to-right order or attach continuous diffusion or flow matching heads outside the backbone, demanding specialized training and iterative sampling that hinder a unified, scalable architecture. We present Discrete Diffusion VLA, a single-transformer policy that models discretized action chunks with discrete diffusion and is trained with the same cross-entropy objective as the VLM backbone. The design retains diffusion's progressive refinement paradigm while remaining natively compatible with the discrete token interface of VLMs. Our method achieves an adaptive decoding order that resolves easy action elements before harder ones and uses secondary remasking to revisit uncertain predictions across refinement rounds, which improves consistency and enables robust error correction. This unified decoder preserves pretrained vision language priors, supports parallel decoding, breaks the autoregressive bottleneck, and reduces the number of function evaluations. Discrete Diffusion VLA achieves 96.3% avg. SR on LIBERO, 71.2% visual matching on SimplerEnv Fractal and 49.3% overall on SimplerEnv Bridge, improving over both autoregressive and continuous diffusion baselines. These findings indicate that discrete-diffusion action decoder supports precise action modeling and consistent training, laying groundwork for scaling VLA to larger models and datasets.",
            "score": 19,
            "issue_id": 5586,
            "pub_date": "2025-08-27",
            "pub_date_card": {
                "ru": "27 августа",
                "en": "August 27",
                "zh": "8月27日"
            },
            "hash": "ec096752edb34e60",
            "authors": [
                "Zhixuan Liang",
                "Yizhuo Li",
                "Tianshuo Yang",
                "Chengyue Wu",
                "Sitong Mao",
                "Liuao Pei",
                "Xiaokang Yang",
                "Jiangmiao Pang",
                "Yao Mu",
                "Ping Luo"
            ],
            "affiliations": [
                "Huawei Cloud Computing Technologies Co., Ltd.",
                "Shanghai AI Laboratory",
                "Shanghai Jiao Tong University",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.20072.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#diffusion",
                    "#games",
                    "#cv",
                    "#agents",
                    "#multimodal"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Дискретная диффузия для точного моделирования действий в VLA задачах",
                    "desc": "Статья представляет Discrete Diffusion VLA - новый подход к моделированию действий в задачах зрения-языка-действия (Vision-Language-Action, VLA). Модель использует единый трансформер с дискретной диффузией для генерации дискретизированных фрагментов действий. Этот метод позволяет адаптивно определять порядок декодирования, улучшая согласованность и производительность по сравнению с авторегрессивными методами и методами непрерывной диффузии. Discrete Diffusion VLA достигает высоких результатов на нескольких бенчмарках, демонстрируя потенциал для масштабирования VLA моделей."
                },
                "en": {
                    "title": "Revolutionizing Action Modeling with Discrete Diffusion VLA",
                    "desc": "The paper introduces Discrete Diffusion VLA, a novel approach that utilizes a single-transformer policy to model actions through discrete diffusion. This method enhances the decoding process by allowing actions to be generated in an adaptive order, addressing simpler actions before more complex ones. By maintaining compatibility with the discrete token interface of vision-language models (VLMs), it simplifies training and improves performance without the need for specialized iterative sampling. The results demonstrate significant improvements in action modeling accuracy and consistency, paving the way for scaling VLA applications to larger datasets and models."
                },
                "zh": {
                    "title": "离散扩散VLA：提升动作建模与一致性",
                    "desc": "离散扩散VLA使用单一变换器策略，通过离散扩散来建模动作，改善了解码顺序、一致性和性能，优于自回归和连续扩散方法。该模型将图像和指令映射到机器人动作，采用与视觉语言模型（VLM）相同的交叉熵目标进行训练。设计保留了扩散的渐进细化范式，同时与VLM的离散令牌接口兼容。离散扩散VLA在LIBERO上实现了96.3%的平均成功率，表明其支持精确的动作建模和一致的训练，为将VLA扩展到更大模型和数据集奠定了基础。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.19320",
            "title": "MIDAS: Multimodal Interactive Digital-human Synthesis via Real-time\n  Autoregressive Video Generation",
            "url": "https://huggingface.co/papers/2508.19320",
            "abstract": "An autoregressive video generation framework with multimodal control and low-latency extrapolation uses a modified large language model and a deep compression autoencoder to achieve high efficiency and fine-grained controllability.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, interactive digital human video generation has attracted widespread attention and achieved remarkable progress. However, building such a practical system that can interact with diverse input signals in real time remains challenging to existing methods, which often struggle with high latency, heavy computational cost, and limited controllability. In this work, we introduce an autoregressive video generation framework that enables interactive multimodal control and low-latency extrapolation in a streaming manner. With minimal modifications to a standard large language model (LLM), our framework accepts multimodal condition encodings including audio, pose, and text, and outputs spatially and semantically coherent representations to guide the denoising process of a diffusion head. To support this, we construct a large-scale dialogue dataset of approximately 20,000 hours from multiple sources, providing rich conversational scenarios for training. We further introduce a deep compression autoencoder with up to 64times reduction ratio, which effectively alleviates the long-horizon inference burden of the autoregressive model. Extensive experiments on duplex conversation, multilingual human synthesis, and interactive world model highlight the advantages of our approach in low latency, high efficiency, and fine-grained multimodal controllability.",
            "score": 18,
            "issue_id": 5592,
            "pub_date": "2025-08-26",
            "pub_date_card": {
                "ru": "26 августа",
                "en": "August 26",
                "zh": "8月26日"
            },
            "hash": "41330ab593be1a2d",
            "authors": [
                "Ming Chen",
                "Liyuan Cui",
                "Wenyuan Zhang",
                "Haoxian Zhang",
                "Yan Zhou",
                "Xiaohan Li",
                "Xiaoqiang Liu",
                "Pengfei Wan"
            ],
            "affiliations": [
                "Kuaishou Technology",
                "Tsinghua University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.19320.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#inference",
                    "#long_context",
                    "#diffusion",
                    "#video",
                    "#multimodal",
                    "#low_resource"
                ],
                "emoji": "🎭",
                "ru": {
                    "title": "Интерактивные цифровые люди: быстро, эффективно и под полным контролем",
                    "desc": "Статья представляет новую систему для интерактивной генерации видео с цифровыми людьми. Авторы предлагают авторегрессионную модель на основе модифицированной большой языковой модели (LLM), которая принимает мультимодальные входные данные и генерирует видео с низкой задержкой. Система использует глубокий автоэнкодер сжатия для повышения эффективности и уменьшения вычислительной нагрузки. Эксперименты показывают преимущества подхода в низкой задержке, высокой эффективности и точном мультимодальном контроле."
                },
                "en": {
                    "title": "Real-Time Interactive Video Generation with Multimodal Control",
                    "desc": "This paper presents a novel autoregressive video generation framework that allows for real-time interaction using multiple input types, such as audio and text. It modifies a large language model to accept these multimodal inputs and produce coherent video outputs efficiently. The framework incorporates a deep compression autoencoder to significantly reduce the computational load, enabling low-latency performance. Extensive testing demonstrates its effectiveness in generating interactive digital human videos with high efficiency and precise control over various modalities."
                },
                "zh": {
                    "title": "低延迟多模态视频生成新框架",
                    "desc": "本文提出了一种自回归视频生成框架，能够实现多模态控制和低延迟的外推。该框架对标准的大型语言模型进行了最小修改，能够处理音频、姿态和文本等多种输入信号，并生成空间和语义一致的表示。通过构建一个包含约20,000小时对话的数据集，支持丰富的对话场景训练。此外，采用深度压缩自编码器，显著降低了自回归模型的推理负担，提升了效率和可控性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.20096",
            "title": "CODA: Coordinating the Cerebrum and Cerebellum for a Dual-Brain Computer\n  Use Agent with Decoupled Reinforcement Learning",
            "url": "https://huggingface.co/papers/2508.20096",
            "abstract": "CODA, a trainable compositional framework, combines a generalist planner and specialist executor to achieve robust execution and cross-domain generalization in scientific computing GUIs.  \t\t\t\t\tAI-generated summary \t\t\t\t Autonomous agents for Graphical User Interfaces (GUIs) face significant challenges in specialized domains such as scientific computing, where both long-horizon planning and precise execution are required. Existing approaches suffer from a trade-off: generalist agents excel at planning but perform poorly in execution, while specialized agents demonstrate the opposite weakness. Recent compositional frameworks attempt to bridge this gap by combining a planner and an actor, but they are typically static and non-trainable, which prevents adaptation from experience. This is a critical limitation given the scarcity of high-quality data in scientific domains. To address these limitations, we introduce CODA, a novel and trainable compositional framework that integrates a generalist planner (Cerebrum) with a specialist executor (Cerebellum), trained via a dedicated two-stage pipeline. In the first stage, Specialization, we apply a decoupled GRPO approach to train an expert planner for each scientific application individually, bootstrapping from a small set of task trajectories. In the second stage, Generalization, we aggregate all successful trajectories from the specialized experts to build a consolidated dataset, which is then used for supervised fine-tuning of the final planner. This equips CODA with both robust execution and cross-domain generalization. Evaluated on four challenging applications from the ScienceBoard benchmark, CODA significantly outperforms baselines and establishes a new state of the art among open-source models.",
            "score": 13,
            "issue_id": 5586,
            "pub_date": "2025-08-27",
            "pub_date_card": {
                "ru": "27 августа",
                "en": "August 27",
                "zh": "8月27日"
            },
            "hash": "8449940bace28db5",
            "authors": [
                "Zeyi Sun",
                "Yuhang Cao",
                "Jianze Liang",
                "Qiushi Sun",
                "Ziyu Liu",
                "Zhixiong Zhang",
                "Yuhang Zang",
                "Xiaoyi Dong",
                "Kai Chen",
                "Dahua Lin",
                "Jiaqi Wang"
            ],
            "affiliations": [
                "Shanghai AI Laboratory",
                "Shanghai Jiao Tong University",
                "The Chinese University of Hong Kong",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.20096.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#optimization",
                    "#benchmark",
                    "#training",
                    "#agents",
                    "#science"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "CODA: Умный тандем планировщика и исполнителя для научных GUI",
                    "desc": "CODA - это новая обучаемая композиционная архитектура для автономных агентов в научных графических интерфейсах. Она сочетает обобщенный планировщик (Cerebrum) и специализированный исполнитель (Cerebellum), обучаемые по двухэтапному алгоритму. На первом этапе специализации для каждого приложения обучается экспертный планировщик, а на втором этапе обобщения финальный планировщик дообучается на агрегированном наборе данных. CODA превосходит базовые модели на бенчмарке ScienceBoard, демонстрируя надежное выполнение задач и кросс-доменную генерализацию."
                },
                "en": {
                    "title": "CODA: Bridging Planning and Execution for Robust AI in Science",
                    "desc": "CODA is a novel framework designed to improve the performance of autonomous agents in scientific computing GUIs by combining a generalist planner and a specialist executor. It addresses the limitations of existing methods that struggle with the trade-off between planning and execution. CODA employs a two-stage training process: first, it specializes the planner for individual tasks, and then it generalizes by aggregating successful task trajectories for fine-tuning. This approach allows CODA to achieve robust execution and effective cross-domain generalization, outperforming previous models in benchmark evaluations."
                },
                "zh": {
                    "title": "CODA：科学计算的智能执行新框架",
                    "desc": "CODA是一个可训练的组合框架，结合了通用规划器和专业执行器，以实现科学计算图形用户界面（GUI）的稳健执行和跨领域泛化。现有方法在通用代理和专业代理之间存在权衡，通用代理在规划上表现优异，但执行能力较差，而专业代理则相反。CODA通过一个两阶段的训练流程，首先为每个科学应用训练专家规划器，然后聚合成功的轨迹进行监督微调，从而克服了数据稀缺的限制。经过评估，CODA在多个挑战性应用中显著超越了基线，确立了开源模型的新标准。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.19228",
            "title": "Predicting the Order of Upcoming Tokens Improves Language Modeling",
            "url": "https://huggingface.co/papers/2508.19228",
            "abstract": "Token Order Prediction (TOP) improves language model training by ordering upcoming tokens, outperforming both Next-Token Prediction (NTP) and Multi-Token Prediction (MTP) across benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Multi-Token Prediction (MTP) has been proposed as an auxiliary objective to improve next-token prediction (NTP) in language model training but shows inconsistent improvements, underperforming in standard NLP benchmarks. We argue that MTP's exact future token prediction is too difficult as an auxiliary loss. Instead, we propose Token Order Prediction (TOP), which trains models to order upcoming tokens by their proximity using a learning-to-rank loss. TOP requires only a single additional unembedding layer compared to MTP's multiple transformer layers. We pretrain models of 340M, 1.8B, and 7B parameters using NTP, MTP, and TOP objectives. Results on eight standard NLP benchmarks show that TOP overall outperforms both NTP and MTP even at scale. Our code is available at https://github.com/zaydzuhri/token-order-prediction",
            "score": 12,
            "issue_id": 5587,
            "pub_date": "2025-08-26",
            "pub_date_card": {
                "ru": "26 августа",
                "en": "August 26",
                "zh": "8月26日"
            },
            "hash": "697f532512cebd78",
            "authors": [
                "Zayd M. K. Zuhri",
                "Erland Hilman Fuadi",
                "Alham Fikri Aji"
            ],
            "affiliations": [
                "MBZUAI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.19228.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#benchmark",
                    "#optimization"
                ],
                "emoji": "🔢",
                "ru": {
                    "title": "Улучшение языковых моделей через предсказание порядка токенов",
                    "desc": "В статье предлагается новый метод обучения языковых моделей - Token Order Prediction (TOP). TOP обучает модели ранжировать предстоящие токены по их близости, используя loss функцию learning-to-rank. Этот подход превосходит как стандартное предсказание следующего токена (NTP), так и предсказание нескольких токенов (MTP) на различных бенчмарках обработки естественного языка. TOP требует добавления всего одного слоя по сравнению с MTP, что делает его более эффективным."
                },
                "en": {
                    "title": "Revolutionizing Language Models with Token Order Prediction",
                    "desc": "Token Order Prediction (TOP) is a new approach to improve language model training by focusing on the order of upcoming tokens rather than predicting them exactly. This method uses a learning-to-rank loss to train models, making it simpler and more effective than Multi-Token Prediction (MTP), which has shown inconsistent results. TOP only requires one additional unembedding layer, making it more efficient than MTP's multiple transformer layers. Experiments with models of various sizes demonstrate that TOP consistently outperforms both Next-Token Prediction (NTP) and MTP across multiple NLP benchmarks."
                },
                "zh": {
                    "title": "令牌顺序预测：提升语言模型的新方法",
                    "desc": "本文提出了一种新的语言模型训练方法——令牌顺序预测（TOP），旨在通过对即将到来的令牌进行排序来提高模型性能。与多令牌预测（MTP）相比，TOP在多个标准自然语言处理基准测试中表现更佳，因为MTP的精确未来令牌预测过于困难。TOP使用学习排序损失，仅需一个额外的解嵌入层，相比于MTP的多个变换器层，结构更为简化。实验结果表明，TOP在340M、1.8B和7B参数的模型预训练中均优于传统的下一令牌预测（NTP）和MTP。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.17924",
            "title": "Gaze into the Heart: A Multi-View Video Dataset for rPPG and Health\n  Biomarkers Estimation",
            "url": "https://huggingface.co/papers/2508.17924",
            "abstract": "A large-scale multi-view video dataset for rPPG and health biomarkers estimation is introduced, enabling efficient rPPG model training and comparison with existing approaches.  \t\t\t\t\tAI-generated summary \t\t\t\t Progress in remote PhotoPlethysmoGraphy (rPPG) is limited by the critical issues of existing publicly available datasets: small size, privacy concerns with facial videos, and lack of diversity in conditions. The paper introduces a novel comprehensive large-scale multi-view video dataset for rPPG and health biomarkers estimation. Our dataset comprises 3600 synchronized video recordings from 600 subjects, captured under varied conditions (resting and post-exercise) using multiple consumer-grade cameras at different angles. To enable multimodal analysis of physiological states, each recording is paired with a 100 Hz PPG signal and extended health metrics, such as electrocardiogram, arterial blood pressure, biomarkers, temperature, oxygen saturation, respiratory rate, and stress level. Using this data, we train an efficient rPPG model and compare its quality with existing approaches in cross-dataset scenarios. The public release of our dataset and model should significantly speed up the progress in the development of AI medical assistants.",
            "score": 11,
            "issue_id": 5594,
            "pub_date": "2025-08-25",
            "pub_date_card": {
                "ru": "25 августа",
                "en": "August 25",
                "zh": "8月25日"
            },
            "hash": "126632113a93dfc3",
            "authors": [
                "Konstantin Egorov",
                "Stepan Botman",
                "Pavel Blinov",
                "Galina Zubkova",
                "Anton Ivaschenko",
                "Alexander Kolsanov",
                "Andrey Savchenko"
            ],
            "affiliations": [
                "ISP RAS Research Center for Trusted Artificial Intelligence, Moscow, Russia",
                "Samara State Medical University, Samara, Russia",
                "Sber AI Lab, Moscow, Russia"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.17924.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#multimodal",
                    "#dataset",
                    "#healthcare"
                ],
                "emoji": "📹",
                "ru": {
                    "title": "Большие данные для прорыва в дистанционной фотоплетизмографии",
                    "desc": "Представлен масштабный многоракурсный видеонабор данных для оценки rPPG и биомаркеров здоровья. Набор включает 3600 синхронизированных видеозаписей от 600 субъектов в различных условиях, снятых несколькими камерами с разных углов. Каждая запись сопровождается сигналом PPG 100 Гц и расширенными показателями здоровья. На основе этих данных авторы обучили эффективную модель rPPG и сравнили ее качество с существующими подходами в кросс-датасетных сценариях."
                },
                "en": {
                    "title": "Revolutionizing rPPG with a Comprehensive Multi-View Dataset",
                    "desc": "This paper presents a new large-scale multi-view video dataset designed for training models in remote PhotoPlethysmoGraphy (rPPG) and estimating health biomarkers. The dataset includes 3600 synchronized video recordings from 600 subjects, captured under various conditions, which addresses the limitations of existing datasets such as size and diversity. Each video is paired with a 100 Hz PPG signal and additional health metrics, allowing for comprehensive analysis of physiological states. The authors demonstrate the effectiveness of their rPPG model by comparing it with existing methods, aiming to enhance the development of AI medical assistants."
                },
                "zh": {
                    "title": "大规模多视角视频数据集助力rPPG研究",
                    "desc": "本文介绍了一个大规模的多视角视频数据集，用于远程光电容积描记法（rPPG）和健康生物标志物的估计。该数据集包含3600个同步视频录制，来自600个受试者，涵盖了不同的条件（静息和运动后），并使用多台消费级相机从不同角度拍摄。每个录制都配有100 Hz的PPG信号和扩展的健康指标，如心电图、动脉血压、生物标志物、体温、氧饱和度、呼吸频率和压力水平。通过这些数据，我们训练了一个高效的rPPG模型，并在跨数据集场景中与现有方法进行了比较。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.19982",
            "title": "Diffusion Language Models Know the Answer Before Decoding",
            "url": "https://huggingface.co/papers/2508.19982",
            "abstract": "Prophet, a training-free fast decoding paradigm for diffusion language models, reduces inference time by leveraging early answer convergence without sacrificing quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion language models (DLMs) have recently emerged as an alternative to autoregressive approaches, offering parallel sequence generation and flexible token orders. However, their inference remains slower than that of autoregressive models, primarily due to the cost of bidirectional attention and the large number of refinement steps required for high quality outputs. In this work, we highlight and leverage an overlooked property of DLMs early answer convergence: in many cases, the correct answer can be internally identified by half steps before the final decoding step, both under semi-autoregressive and random remasking schedules. For example, on GSM8K and MMLU, up to 97% and 99% of instances, respectively, can be decoded correctly using only half of the refinement steps. Building on this observation, we introduce Prophet, a training-free fast decoding paradigm that enables early commit decoding. Specifically, Prophet dynamically decides whether to continue refinement or to go \"all-in\" (i.e., decode all remaining tokens in one step), using the confidence gap between the top-2 prediction candidates as the criterion. It integrates seamlessly into existing DLM implementations, incurs negligible overhead, and requires no additional training. Empirical evaluations of LLaDA-8B and Dream-7B across multiple tasks show that Prophet reduces the number of decoding steps by up to 3.4x while preserving high generation quality. These results recast DLM decoding as a problem of when to stop sampling, and demonstrate that early decode convergence provides a simple yet powerful mechanism for accelerating DLM inference, complementary to existing speedup techniques. Our code is publicly available at https://github.com/pixeli99/Prophet.",
            "score": 10,
            "issue_id": 5588,
            "pub_date": "2025-08-27",
            "pub_date_card": {
                "ru": "27 августа",
                "en": "August 27",
                "zh": "8月27日"
            },
            "hash": "f8a30e4eb7eff789",
            "authors": [
                "Pengxiang Li",
                "Yefan Zhou",
                "Dilxat Muhtar",
                "Lu Yin",
                "Shilin Yan",
                "Li Shen",
                "Yi Liang",
                "Soroush Vosoughi",
                "Shiwei Liu"
            ],
            "affiliations": [
                "Dartmouth College",
                "ELLIS Institute Tubingen",
                "Google DeepMind",
                "Max Planck Institute for Intelligent Systems",
                "Sun Yat-sen University",
                "The Hong Kong Polytechnic University",
                "University of Surrey"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.19982.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#training",
                    "#diffusion",
                    "#inference",
                    "#optimization"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "Prophet: ускорение вывода диффузионных языковых моделей без компромиссов",
                    "desc": "Статья представляет Prophet - новый метод быстрого декодирования для диффузионных языковых моделей (DLM). Он использует свойство ранней сходимости ответов, позволяя завершить декодирование раньше без потери качества. Prophet динамически определяет, когда можно остановить уточнение и декодировать все оставшиеся токены за один шаг. Этот подход сокращает время вывода DLM до 3.4 раз, сохраняя высокое качество генерации."
                },
                "en": {
                    "title": "Accelerating Diffusion Language Models with Early Answer Convergence",
                    "desc": "This paper introduces Prophet, a new method for speeding up the decoding process in diffusion language models (DLMs) without requiring additional training. It takes advantage of early answer convergence, where the correct output can often be identified before completing all refinement steps. By dynamically deciding when to stop refining based on the confidence of predictions, Prophet can reduce the number of decoding steps significantly while maintaining high output quality. The results show that this approach can make DLMs as fast as autoregressive models, enhancing their practical usability."
                },
                "zh": {
                    "title": "Prophet：加速扩散语言模型解码的创新方法",
                    "desc": "本文介绍了一种名为Prophet的快速解码方法，旨在提高扩散语言模型的推理速度。通过利用早期答案收敛的特性，Prophet能够在不牺牲生成质量的情况下，减少解码步骤。研究表明，在许多情况下，正确答案可以在最终解码步骤之前的半个步骤内被识别。实验结果显示，Prophet在多个任务中将解码步骤减少了最多3.4倍，同时保持了高质量的生成效果。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.19229",
            "title": "StepWiser: Stepwise Generative Judges for Wiser Reasoning",
            "url": "https://huggingface.co/papers/2508.19229",
            "abstract": "A generative judge model, StepWiser, uses reinforcement learning to provide step-by-step reasoning feedback, improving both training and inference performance of policy models.  \t\t\t\t\tAI-generated summary \t\t\t\t As models increasingly leverage multi-step reasoning strategies to solve complex problems, supervising the logical validity of these intermediate steps has become a critical research challenge. Process reward models address this by providing step-by-step feedback, but current approaches have two major drawbacks: they typically function as classifiers without providing explanations, and their reliance on supervised fine-tuning with static datasets limits generalization. Inspired by recent advances, we reframe stepwise reward modeling from a classification task to a reasoning task itself. We thus propose a generative judge that reasons about the policy model's reasoning steps (i.e., meta-reasons), outputting thinking tokens before delivering a final verdict. Our model, StepWiser, is trained by reinforcement learning using relative outcomes of rollouts. We show it provides (i) better judgment accuracy on intermediate steps than existing methods; (ii) can be used to improve the policy model at training time; and (iii) improves inference-time search.",
            "score": 10,
            "issue_id": 5586,
            "pub_date": "2025-08-26",
            "pub_date_card": {
                "ru": "26 августа",
                "en": "August 26",
                "zh": "8月26日"
            },
            "hash": "6ffd72787adea812",
            "authors": [
                "Wei Xiong",
                "Wenting Zhao",
                "Weizhe Yuan",
                "Olga Golovneva",
                "Tong Zhang",
                "Jason Weston",
                "Sainbayar Sukhbaatar"
            ],
            "affiliations": [
                "FAIR at Meta",
                "NYU",
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.19229.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#inference",
                    "#training",
                    "#reasoning",
                    "#rl"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Мета-рассуждения для улучшения пошагового мышления ИИ",
                    "desc": "StepWiser - это генеративная модель-судья, использующая обучение с подкреплением для оценки промежуточных шагов рассуждения других моделей. Она генерирует пояснения своих оценок, что улучшает точность суждений по сравнению с существующими методами. StepWiser может применяться для улучшения обучения моделей-исполнителей и оптимизации поиска при выводе. Этот подход переосмысливает задачу пошаговой оценки как задачу рассуждения, а не классификации."
                },
                "en": {
                    "title": "StepWiser: Enhancing Reasoning with Generative Feedback",
                    "desc": "The paper introduces StepWiser, a generative judge model that enhances the performance of policy models through reinforcement learning. It addresses the challenge of supervising multi-step reasoning by providing detailed feedback on each reasoning step, rather than just classifying them. Unlike traditional methods that rely on static datasets, StepWiser reframes the task to focus on reasoning itself, allowing for better generalization. The results demonstrate that StepWiser improves judgment accuracy, aids in training policy models, and enhances inference-time search capabilities."
                },
                "zh": {
                    "title": "逐步推理的生成性评判模型",
                    "desc": "本文提出了一种生成性评判模型StepWiser，利用强化学习提供逐步推理反馈，从而提升策略模型的训练和推理性能。随着模型越来越多地采用多步推理策略来解决复杂问题，监督这些中间步骤的逻辑有效性成为一个重要的研究挑战。现有的过程奖励模型虽然提供逐步反馈，但通常仅作为分类器，缺乏解释，并且依赖静态数据集的监督微调限制了其泛化能力。StepWiser通过将逐步奖励建模重新构建为推理任务，能够在给出最终判断之前输出思考令牌，从而提高中间步骤的判断准确性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.19493",
            "title": "Mind the Third Eye! Benchmarking Privacy Awareness in MLLM-powered\n  Smartphone Agents",
            "url": "https://huggingface.co/papers/2508.19493",
            "abstract": "A large-scale benchmark evaluates the privacy awareness of smartphone agents powered by Multimodal Large Language Models, revealing significant gaps in their ability to protect sensitive user information.  \t\t\t\t\tAI-generated summary \t\t\t\t Smartphones bring significant convenience to users but also enable devices to extensively record various types of personal information. Existing smartphone agents powered by Multimodal Large Language Models (MLLMs) have achieved remarkable performance in automating different tasks. However, as the cost, these agents are granted substantial access to sensitive users' personal information during this operation. To gain a thorough understanding of the privacy awareness of these agents, we present the first large-scale benchmark encompassing 7,138 scenarios to the best of our knowledge. In addition, for privacy context in scenarios, we annotate its type (e.g., Account Credentials), sensitivity level, and location. We then carefully benchmark seven available mainstream smartphone agents. Our results demonstrate that almost all benchmarked agents show unsatisfying privacy awareness (RA), with performance remaining below 60% even with explicit hints. Overall, closed-source agents show better privacy ability than open-source ones, and Gemini 2.0-flash achieves the best, achieving an RA of 67%. We also find that the agents' privacy detection capability is highly related to scenario sensitivity level, i.e., the scenario with a higher sensitivity level is typically more identifiable. We hope the findings enlighten the research community to rethink the unbalanced utility-privacy tradeoff about smartphone agents. Our code and benchmark are available at https://zhixin-l.github.io/SAPA-Bench.",
            "score": 9,
            "issue_id": 5586,
            "pub_date": "2025-08-27",
            "pub_date_card": {
                "ru": "27 августа",
                "en": "August 27",
                "zh": "8月27日"
            },
            "hash": "d7a43858b898f00a",
            "authors": [
                "Zhixin Lin",
                "Jungang Li",
                "Shidong Pan",
                "Yibo Shi",
                "Yue Yao",
                "Dongliang Xu"
            ],
            "affiliations": [
                "Columbia University",
                "Hong Kong University of Science and Technology",
                "Hong Kong University of Science and Technology (Guangzhou)",
                "Shandong University",
                "Xian Jiaotong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.19493.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#benchmark",
                    "#ethics",
                    "#agents",
                    "#multimodal"
                ],
                "emoji": "🔒",
                "ru": {
                    "title": "Смартфонные агенты на MLLM не справляются с защитой приватности",
                    "desc": "Это исследование оценивает способность смартфонных агентов, основанных на мультимодальных больших языковых моделях (MLLM), защищать конфиденциальную информацию пользователей. Авторы создали масштабный бенчмарк из 7138 сценариев для оценки осведомленности агентов о приватности. Результаты показывают, что большинство агентов демонстрируют неудовлетворительную осведомленность о приватности, не превышающую 60% даже с явными подсказками. Исследование выявило значительные пробелы в способности смартфонных агентов защищать чувствительные данные пользователей."
                },
                "en": {
                    "title": "Rethinking Privacy in Smartphone Agents: A Call for Better Awareness",
                    "desc": "This paper evaluates the privacy awareness of smartphone agents that use Multimodal Large Language Models (MLLMs). It presents a large-scale benchmark consisting of 7,138 scenarios to assess how well these agents protect sensitive user information. The findings reveal that most agents perform poorly in privacy awareness, with scores below 60%, and closed-source agents generally outperform open-source ones. The study highlights the need for a better balance between utility and privacy in the design of smartphone agents."
                },
                "zh": {
                    "title": "智能手机助手的隐私保护能力亟待提升",
                    "desc": "这篇论文评估了多模态大型语言模型驱动的智能手机助手在隐私保护方面的能力。研究发现，这些助手在处理敏感用户信息时存在显著的隐私意识缺口。通过对7138个场景进行大规模基准测试，结果显示几乎所有被测试的助手隐私意识表现不佳，得分均低于60%。研究还发现，闭源助手的隐私保护能力普遍优于开源助手，且场景的敏感性与隐私检测能力密切相关。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.20088",
            "title": "AudioStory: Generating Long-Form Narrative Audio with Large Language\n  Models",
            "url": "https://huggingface.co/papers/2508.20088",
            "abstract": "AudioStory integrates large language models with text-to-audio systems to generate coherent, long-form audio narratives through a unified framework with decoupled bridging mechanisms and end-to-end training.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in text-to-audio (TTA) generation excel at synthesizing short audio clips but struggle with long-form narrative audio, which requires temporal coherence and compositional reasoning. To address this gap, we propose AudioStory, a unified framework that integrates large language models (LLMs) with TTA systems to generate structured, long-form audio narratives. AudioStory possesses strong instruction-following reasoning generation capabilities. It employs LLMs to decompose complex narrative queries into temporally ordered sub-tasks with contextual cues, enabling coherent scene transitions and emotional tone consistency. AudioStory has two appealing features: (1) Decoupled bridging mechanism: AudioStory disentangles LLM-diffuser collaboration into two specialized components, i.e., a bridging query for intra-event semantic alignment and a residual query for cross-event coherence preservation. (2) End-to-end training: By unifying instruction comprehension and audio generation within a single end-to-end framework, AudioStory eliminates the need for modular training pipelines while enhancing synergy between components. Furthermore, we establish a benchmark AudioStory-10K, encompassing diverse domains such as animated soundscapes and natural sound narratives. Extensive experiments show the superiority of AudioStory on both single-audio generation and narrative audio generation, surpassing prior TTA baselines in both instruction-following ability and audio fidelity. Our code is available at https://github.com/TencentARC/AudioStory",
            "score": 8,
            "issue_id": 5587,
            "pub_date": "2025-08-27",
            "pub_date_card": {
                "ru": "27 августа",
                "en": "August 27",
                "zh": "8月27日"
            },
            "hash": "0289231c11d7f612",
            "authors": [
                "Yuxin Guo",
                "Teng Wang",
                "Yuying Ge",
                "Shijie Ma",
                "Yixiao Ge",
                "Wei Zou",
                "Ying Shan"
            ],
            "affiliations": [
                "ARC Lab, Tencent",
                "MAIS, Institute of Automation, CAS, Beijing",
                "School of Artificial Intelligence, University of Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.20088.jpg",
            "data": {
                "categories": [
                    "#audio",
                    "#benchmark",
                    "#story_generation",
                    "#multimodal",
                    "#long_context"
                ],
                "emoji": "🎧",
                "ru": {
                    "title": "AudioStory: Интеграция ИИ для создания длинных аудионарративов",
                    "desc": "AudioStory - это унифицированная система, объединяющая большие языковые модели (LLM) с системами преобразования текста в аудио (TTA) для создания структурированных длинных аудиоповествований. Она использует LLM для декомпозиции сложных нарративных запросов на упорядоченные подзадачи с контекстными подсказками, обеспечивая согласованные переходы между сценами и эмоциональную последовательность. AudioStory применяет механизм разделенного связывания и сквозное обучение для улучшения взаимодействия между компонентами. Эксперименты показывают превосходство AudioStory над предыдущими TTA-моделями в способности следовать инструкциям и качестве аудио."
                },
                "en": {
                    "title": "Transforming Text into Engaging Audio Narratives with AudioStory",
                    "desc": "AudioStory is a novel framework that combines large language models (LLMs) with text-to-audio (TTA) systems to create long, coherent audio narratives. It addresses the challenge of maintaining temporal coherence and compositional reasoning in audio storytelling. The framework features a decoupled bridging mechanism that separates the tasks of semantic alignment and coherence preservation, allowing for better scene transitions and emotional consistency. Additionally, it utilizes end-to-end training to streamline the process, enhancing the collaboration between components and improving overall audio quality and instruction-following capabilities."
                },
                "zh": {
                    "title": "AudioStory：生成连贯长篇音频叙事的创新框架",
                    "desc": "AudioStory是一个将大型语言模型与文本到音频系统结合的框架，旨在生成连贯的长篇音频叙事。它通过解耦的桥接机制和端到端的训练，解决了短音频生成与长音频叙事之间的差距。该系统能够将复杂的叙事查询分解为有序的子任务，从而实现场景的连贯过渡和情感基调的一致性。实验结果表明，AudioStory在音频生成和叙事音频生成方面的表现优于之前的基线，展现了其卓越的指令跟随能力和音频保真度。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.19527",
            "title": "MotionFlux: Efficient Text-Guided Motion Generation through Rectified\n  Flow Matching and Preference Alignment",
            "url": "https://huggingface.co/papers/2508.19527",
            "abstract": "TAPO and MotionFLUX form a unified system that enhances semantic consistency and motion quality in text-driven motion generation while achieving real-time synthesis.  \t\t\t\t\tAI-generated summary \t\t\t\t Motion generation is essential for animating virtual characters and embodied agents. While recent text-driven methods have made significant strides, they often struggle with achieving precise alignment between linguistic descriptions and motion semantics, as well as with the inefficiencies of slow, multi-step inference. To address these issues, we introduce TMR++ Aligned Preference Optimization (TAPO), an innovative framework that aligns subtle motion variations with textual modifiers and incorporates iterative adjustments to reinforce semantic grounding. To further enable real-time synthesis, we propose MotionFLUX, a high-speed generation framework based on deterministic rectified flow matching. Unlike traditional diffusion models, which require hundreds of denoising steps, MotionFLUX constructs optimal transport paths between noise distributions and motion spaces, facilitating real-time synthesis. The linearized probability paths reduce the need for multi-step sampling typical of sequential methods, significantly accelerating inference time without sacrificing motion quality. Experimental results demonstrate that, together, TAPO and MotionFLUX form a unified system that outperforms state-of-the-art approaches in both semantic consistency and motion quality, while also accelerating generation speed. The code and pretrained models will be released.",
            "score": 6,
            "issue_id": 5587,
            "pub_date": "2025-08-27",
            "pub_date_card": {
                "ru": "27 августа",
                "en": "August 27",
                "zh": "8月27日"
            },
            "hash": "7410456922eb42b6",
            "authors": [
                "Zhiting Gao",
                "Dan Song",
                "Diqiong Jiang",
                "Chao Xue",
                "An-An Liu"
            ],
            "affiliations": [
                "China University of Petroleum, China",
                "Tiandy Technologies, China",
                "Tianjin University, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.19527.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#games",
                    "#inference",
                    "#multimodal",
                    "#agents",
                    "#optimization"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Революция в генерации движений: точность и скорость в одном флаконе",
                    "desc": "Статья представляет новую систему для генерации движений на основе текстовых описаний. TAPO (TMR++ Aligned Preference Optimization) улучшает семантическое соответствие между текстом и движением, а MotionFLUX обеспечивает синтез в реальном времени. Система использует инновационные методы, такие как оптимальные транспортные пути между распределениями шума и пространствами движений. Экспериментальные результаты показывают превосходство этой системы над современными подходами в плане семантической согласованности и качества движений."
                },
                "en": {
                    "title": "Real-Time Motion Generation with Semantic Precision",
                    "desc": "This paper presents a unified system called TAPO and MotionFLUX that improves the quality and consistency of motion generation from text descriptions. TAPO uses Aligned Preference Optimization to ensure that subtle motion changes accurately reflect the meanings of the text, enhancing semantic alignment. MotionFLUX introduces a fast generation method that avoids the slow multi-step processes of traditional models by using optimal transport paths for real-time synthesis. Together, these innovations allow for high-quality motion generation that is both semantically consistent and efficient, outperforming existing methods."
                },
                "zh": {
                    "title": "实时运动生成的新突破",
                    "desc": "本文介绍了一种名为TAPO和MotionFLUX的统一系统，旨在提高文本驱动的运动生成中的语义一致性和运动质量，同时实现实时合成。TAPO通过对细微运动变化与文本修饰符的对齐，结合迭代调整，增强了语义基础。MotionFLUX则是一种基于确定性修正流匹配的高速生成框架，能够快速构建噪声分布与运动空间之间的最佳传输路径。实验结果表明，该系统在语义一致性、运动质量和生成速度上均优于现有的最先进方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.18179",
            "title": "SEAM: Semantically Equivalent Across Modalities Benchmark for\n  Vision-Language Models",
            "url": "https://huggingface.co/papers/2508.18179",
            "abstract": "SEAM is a benchmark that evaluates vision-language models' reasoning consistency across modalities using semantically equivalent inputs, revealing systematic modality imbalance and visual hallucinations.  \t\t\t\t\tAI-generated summary \t\t\t\t Evaluating whether vision-language models (VLMs) reason consistently across representations is challenging because modality comparisons are typically confounded by task differences and asymmetric information. We introduce SEAM, a benchmark that pairs semantically equivalent inputs across four domains that have existing standardized textual and visual notations. By employing distinct notation systems across modalities, in contrast to OCR-based image-text pairing, SEAM provides a rigorous comparative assessment of the textual-symbolic and visual-spatial reasoning capabilities of VLMs. Across 21 contemporary models, we observe systematic modality imbalance: vision frequently lags language in overall performance, despite the problems containing semantically equivalent information, and cross-modal agreement is relatively low. Our error analysis reveals two main drivers: textual perception failures from tokenization in domain notation and visual perception failures that induce hallucinations. We also show that our results are largely robust to visual transformations. SEAM establishes a controlled, semantically equivalent setting for measuring and improving modality-agnostic reasoning.",
            "score": 6,
            "issue_id": 5601,
            "pub_date": "2025-08-25",
            "pub_date_card": {
                "ru": "25 августа",
                "en": "August 25",
                "zh": "8月25日"
            },
            "hash": "f4c3fc42a2de9fce",
            "authors": [
                "Zhenwei Tang",
                "Difan Jiao",
                "Blair Yang",
                "Ashton Anderson"
            ],
            "affiliations": [
                "Coolwei AI Lab",
                "Department of Computer Science, University of Toronto"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.18179.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#multimodal",
                    "#hallucinations",
                    "#benchmark",
                    "#cv"
                ],
                "emoji": "🔬",
                "ru": {
                    "title": "SEAM: Точная оценка мультимодального мышления ИИ",
                    "desc": "SEAM - это новый эталонный тест для оценки согласованности рассуждений мультимодальных моделей на семантически эквивалентных входных данных в различных модальностях. Тест выявляет систематический дисбаланс между модальностями и визуальные галлюцинации в современных vision-language моделях. SEAM использует стандартизированные текстовые и визуальные нотации в четырех предметных областях, что позволяет проводить строгую сравнительную оценку текстово-символических и визуально-пространственных способностей моделей. Результаты показывают, что визуальная модальность часто отстает от языковой по общей производительности, несмотря на семантически эквивалентную информацию."
                },
                "en": {
                    "title": "SEAM: Bridging the Gap in Vision-Language Reasoning",
                    "desc": "The paper introduces SEAM, a benchmark designed to evaluate the reasoning consistency of vision-language models (VLMs) across different modalities. It addresses the challenge of comparing modalities by using semantically equivalent inputs from four standardized domains, allowing for a fair assessment of both textual-symbolic and visual-spatial reasoning. The study finds that vision often underperforms compared to language, highlighting issues like visual hallucinations and tokenization errors in text perception. SEAM aims to provide a controlled environment to enhance the understanding and improvement of reasoning capabilities in VLMs."
                },
                "zh": {
                    "title": "SEAM：评估视觉-语言模型推理一致性的基准",
                    "desc": "SEAM是一个基准测试，用于评估视觉-语言模型在不同模态下的推理一致性。它通过配对四个领域中语义等价的输入，揭示了模态不平衡和视觉幻觉的问题。研究发现，尽管输入信息是等价的，视觉模态的表现通常低于语言模态，且跨模态的一致性较低。通过错误分析，我们识别出文本感知和视觉感知的失败是导致这些问题的主要原因。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.20033",
            "title": "DeepScholar-Bench: A Live Benchmark and Automated Evaluation for\n  Generative Research Synthesis",
            "url": "https://huggingface.co/papers/2508.20033",
            "abstract": "DeepScholar-bench evaluates generative research synthesis systems by assessing their performance in creating related work sections from recent ArXiv papers, focusing on knowledge synthesis, retrieval quality, and verifiability.  \t\t\t\t\tAI-generated summary \t\t\t\t The ability to research and synthesize knowledge is central to human expertise and progress. An emerging class of systems promises these exciting capabilities through generative research synthesis, performing retrieval over the live web and synthesizing discovered sources into long-form, cited summaries. However, evaluating such systems remains an open challenge: existing question-answering benchmarks focus on short-form factual responses, while expert-curated datasets risk staleness and data contamination. Both fail to capture the complexity and evolving nature of real research synthesis tasks. In this work, we introduce DeepScholar-bench, a live benchmark and holistic, automated evaluation framework designed to evaluate generative research synthesis. DeepScholar-bench draws queries from recent, high-quality ArXiv papers and focuses on a real research synthesis task: generating the related work sections of a paper by retrieving, synthesizing, and citing prior research. Our evaluation framework holistically assesses performance across three key dimensions, knowledge synthesis, retrieval quality, and verifiability. We also develop DeepScholar-base, a reference pipeline implemented efficiently using the LOTUS API. Using the DeepScholar-bench framework, we perform a systematic evaluation of prior open-source systems, search AI's, OpenAI's DeepResearch, and DeepScholar-base. We find that DeepScholar-base establishes a strong baseline, attaining competitive or higher performance than each other method. We also find that DeepScholar-bench remains far from saturated, with no system exceeding a score of 19% across all metrics. These results underscore the difficulty of DeepScholar-bench, as well as its importance for progress towards AI systems capable of generative research synthesis. We make our code available at https://github.com/guestrin-lab/deepscholar-bench.",
            "score": 3,
            "issue_id": 5587,
            "pub_date": "2025-08-27",
            "pub_date_card": {
                "ru": "27 августа",
                "en": "August 27",
                "zh": "8月27日"
            },
            "hash": "10b7a89ab4fa5d36",
            "authors": [
                "Liana Patel",
                "Negar Arabzadeh",
                "Harshit Gupta",
                "Ankita Sundar",
                "Ion Stoica",
                "Matei Zaharia",
                "Carlos Guestrin"
            ],
            "affiliations": [
                "Stanford University",
                "UC Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.20033.jpg",
            "data": {
                "categories": [
                    "#rag",
                    "#benchmark",
                    "#science",
                    "#open_source"
                ],
                "emoji": "🔬",
                "ru": {
                    "title": "Новый бенчмарк для оценки ИИ-систем в научном синтезе",
                    "desc": "DeepScholar-bench - это новый метод оценки систем генеративного синтеза исследований. Он использует недавние статьи с ArXiv для создания запросов и фокусируется на задаче генерации разделов 'Связанные работы'. Оценка производится по трем ключевым аспектам: синтез знаний, качество поиска и проверяемость. Авторы также разработали базовую систему DeepScholar-base, которая показала конкурентоспособные результаты по сравнению с другими методами."
                },
                "en": {
                    "title": "Evaluating AI's Ability to Synthesize Research Knowledge",
                    "desc": "DeepScholar-bench is a new evaluation framework designed to assess generative research synthesis systems, which create related work sections from recent research papers. It focuses on three main aspects: knowledge synthesis, retrieval quality, and verifiability, addressing the limitations of existing benchmarks that do not capture the complexity of real research tasks. The framework uses queries from high-quality ArXiv papers to evaluate how well these systems can retrieve and synthesize information into coherent summaries with proper citations. The results show that while DeepScholar-base performs competitively, there is still significant room for improvement in generative research synthesis systems, highlighting the challenges in this area."
                },
                "zh": {
                    "title": "DeepScholar-bench：推动生成研究综合的评估新标准",
                    "desc": "DeepScholar-bench 是一个评估生成研究综合系统的基准，专注于从最新的 ArXiv 论文中创建相关工作部分的性能。该框架评估知识综合、检索质量和可验证性三个关键维度。通过从高质量的 ArXiv 论文中提取查询，DeepScholar-bench 旨在解决现有评估方法无法捕捉真实研究综合任务复杂性的挑战。研究表明，DeepScholar-base 在各项指标上表现优异，显示出生成研究综合系统的巨大潜力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.19559",
            "title": "Taming the Chaos: Coordinated Autoscaling for Heterogeneous and\n  Disaggregated LLM Inference",
            "url": "https://huggingface.co/papers/2508.19559",
            "abstract": "HeteroScale, a coordinated autoscaling framework, improves GPU utilization and efficiency in serving large language models by addressing challenges in heterogeneous hardware and network constraints in Prefill-Decode architectures.  \t\t\t\t\tAI-generated summary \t\t\t\t Serving Large Language Models (LLMs) is a GPU-intensive task where traditional autoscalers fall short, particularly for modern Prefill-Decode (P/D) disaggregated architectures. This architectural shift, while powerful, introduces significant operational challenges, including inefficient use of heterogeneous hardware, network bottlenecks, and critical imbalances between prefill and decode stages. We introduce HeteroScale, a coordinated autoscaling framework that addresses the core challenges of P/D disaggregated serving. HeteroScale combines a topology-aware scheduler that adapts to heterogeneous hardware and network constraints with a novel metric-driven policy derived from the first large-scale empirical study of autoscaling signals in production. By leveraging a single, robust metric to jointly scale prefill and decode pools, HeteroScale maintains architectural balance while ensuring efficient, adaptive resource management. Deployed in a massive production environment on tens of thousands of GPUs, HeteroScale has proven its effectiveness, increasing average GPU utilization by a significant 26.6 percentage points and saving hundreds of thousands of GPU-hours daily, all while upholding stringent service level objectives.",
            "score": 3,
            "issue_id": 5587,
            "pub_date": "2025-08-27",
            "pub_date_card": {
                "ru": "27 августа",
                "en": "August 27",
                "zh": "8月27日"
            },
            "hash": "d2eb0c8a7e647879",
            "authors": [
                "Rongzhi Li",
                "Ruogu Du",
                "Zefang Chu",
                "Sida Zhao",
                "Chunlei Han",
                "Zuocheng Shi",
                "Yiwen Shao",
                "Huanle Han",
                "Long Huang",
                "Zherui Liu",
                "Shufan Liu"
            ],
            "affiliations": [
                "ByteDance Seed",
                "National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.19559.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#architecture",
                    "#inference",
                    "#optimization"
                ],
                "emoji": "⚖️",
                "ru": {
                    "title": "Балансировка ресурсов для эффективного обслуживания языковых моделей",
                    "desc": "HeteroScale - это координированная система автомасштабирования для эффективного обслуживания больших языковых моделей (LLM). Она решает проблемы использования гетерогенного оборудования и сетевых ограничений в архитектурах Prefill-Decode. HeteroScale включает планировщик с учетом топологии и новую политику на основе метрик, полученных из эмпирического исследования. Внедрение HeteroScale в производственной среде значительно повысило утилизацию GPU и сэкономило сотни тысяч GPU-часов ежедневно."
                },
                "en": {
                    "title": "HeteroScale: Optimizing GPU Efficiency for Large Language Models",
                    "desc": "HeteroScale is a new framework designed to improve the efficiency of GPU usage when serving large language models, especially in complex Prefill-Decode architectures. It tackles the challenges posed by different types of hardware and network limitations that can lead to inefficient resource allocation. By using a smart scheduler that understands the system's topology and a unique metric-driven policy, HeteroScale can effectively balance the demands of both the prefill and decode stages. In real-world applications, it has significantly boosted GPU utilization and reduced wasted resources, demonstrating its effectiveness in large-scale environments."
                },
                "zh": {
                    "title": "HeteroScale：提升大语言模型GPU效率的智能扩展框架",
                    "desc": "HeteroScale是一个协调的自动扩展框架，旨在提高大语言模型的GPU利用率和效率。它解决了在Prefill-Decode架构中，异构硬件和网络限制带来的挑战。通过结合拓扑感知调度器和基于新指标的策略，HeteroScale能够有效管理预填充和解码阶段的资源。在大规模生产环境中，HeteroScale显著提高了GPU利用率，并节省了大量的GPU时间。"
                }
            }
        }
    ],
    "link_prev": "2025-08-27.html",
    "link_next": "2025-08-29.html",
    "link_month": "2025-08.html",
    "short_date_prev": {
        "ru": "27.08",
        "en": "08/27",
        "zh": "8月27日"
    },
    "short_date_next": {
        "ru": "29.08",
        "en": "08/29",
        "zh": "8月29日"
    },
    "categories": {
        "#dataset": 2,
        "#data": 1,
        "#benchmark": 6,
        "#agents": 4,
        "#cv": 2,
        "#rl": 2,
        "#rlhf": 0,
        "#rag": 1,
        "#plp": 0,
        "#inference": 5,
        "#3d": 0,
        "#audio": 2,
        "#video": 2,
        "#multimodal": 8,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 1,
        "#training": 7,
        "#robotics": 0,
        "#agi": 0,
        "#games": 2,
        "#interpretability": 1,
        "#reasoning": 3,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 0,
        "#optimization": 6,
        "#survey": 0,
        "#diffusion": 3,
        "#alignment": 0,
        "#story_generation": 1,
        "#hallucinations": 3,
        "#long_context": 2,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 4,
        "#small_models": 0,
        "#science": 2,
        "#low_resource": 1
    }
}