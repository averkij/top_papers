
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 24 papers. February 25.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ", "Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹", "Ğ¼Ğ¸Ğ½ÑƒÑ‚"],
                hour: ["Ñ‡Ğ°Ñ", "Ñ‡Ğ°ÑĞ°", "Ñ‡Ğ°ÑĞ¾Ğ²"],
                day: ["Ğ´ĞµĞ½ÑŒ", "Ğ´Ğ½Ñ", "Ğ´Ğ½ĞµĞ¹"],
                justNow: "Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ñ‚Ğ¾",
                ago: "Ğ½Ğ°Ğ·Ğ°Ğ´"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["åˆ†é’Ÿ", "åˆ†é’Ÿ", "åˆ†é’Ÿ"],
                hour: ["å°æ—¶", "å°æ—¶", "å°æ—¶"],
                day: ["å¤©", "å¤©", "å¤©"],
                justNow: "åˆšåˆš",
                ago: "å‰"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            } else if (lastDigit === 1) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒÑ";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒĞ¸";
            } else {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ç¯‡è®ºæ–‡"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">ğŸ”º</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">25 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ</span> | <span id="title-articles-count">24 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2026-02-24.html">â¬…ï¸ <span id="prev-date">24.02</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2026-02-26.html">â¡ï¸ <span id="next-date">26.02</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2026-02.html">ğŸ“ˆ <span id='top-month-label'>ĞœĞµÑÑÑ†</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">ğŸ”€ <span id="sort-label-text">Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ</option>
                    <option value="pub_date">Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸</option>
                    <option value="issue_id">Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">ğŸ·ï¸ Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> AâˆªB</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> Aâˆ©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">ğŸ§¹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> âœ–ï¸ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '25 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 25', 'zh': '2æœˆ25æ—¥'};
        let feedDateNext = {'ru': '26.02', 'en': '02/26', 'zh': '2æœˆ26æ—¥'};
        let feedDatePrev = {'ru': '24.02', 'en': '02/24', 'zh': '2æœˆ24æ—¥'};
        let filterLabel = {'ru': 'Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€', 'en': 'Topics', 'zh': 'ä¸»é¢˜ç­›é€‰'}
        let publishedLabel = {'ru': 'ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ñ‚ ', 'en': 'published on ', 'zh': 'å‘è¡¨äº'}
        let sortLabel = {'ru': 'Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾', 'en': 'Sort by', 'zh': 'æ’åºæ–¹å¼'}
        let paperLabel = {'ru': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ', 'en': 'Paper', 'zh': 'è®ºæ–‡'}
        let topMonthLabel = {'ru': 'ĞœĞµÑÑÑ†', 'en': 'Month', 'zh': 'æœˆåº¦è®ºæ–‡'}
        let topDayLabel = {'ru': 'Ğ”ĞµĞ½ÑŒ', 'en': 'Day', 'zh': 'æ—¥åº¦è®ºæ–‡'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2602.21193', 'title': 'On Data Engineering for Scaling LLM Terminal Capabilities', 'url': 'https://huggingface.co/papers/2602.21193', 'abstract': 'Abstract Researchers developed a synthetic task generation pipeline and analyzed data strategies to improve terminal agent performance, creating a large-scale dataset and models that outperform larger counterparts on benchmark tests.  \t\t\t\t\tAI-generated summary Despite rapid recent progress in the terminal capabilities of large language models, the training data strategies behind state-of-the-art terminal agents remain largely undisclosed. We address this gap through a systematic study of data engineering practices for terminal agents, making two key contributions: (1) Terminal-Task-Gen, a lightweight synthetic task generation pipeline that supports seed-based and skill-based task construction, and (2) a comprehensive analysis of data and training strategies, including filtering, curriculum learning, long context training, and scaling behavior. Our pipeline yields Terminal-Corpus, a large-scale open-source dataset for terminal tasks. Using this dataset, we train Nemotron-Terminal, a family of models initialized from Qwen3(8B, 14B, 32B) that achieve substantial gains on Terminal-Bench 2.0: Nemotron-Terminal-8B improves from 2.5% to 13.0% Nemotron-Terminal-14B improves from 4.0% to 20.2%, and Nemotron-Terminal-32B improves from 3.4% to 27.4%, matching the performance of significantly larger models. To accelerate research in this domain, we open-source our model checkpoints and most of our synthetic datasets at https://huggingface.co/collections/nvidia/nemotron-terminal.', 'score': 28, 'issue_id': 1212, 'pub_date': '2026-02-24', 'pub_date_card': {'ru': '24 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 24', 'zh': '2æœˆ24æ—¥'}, 'hash': 'efca03aa1b046219', 'authors': ['Renjie Pi', 'Grace Lam', 'Mohammad Shoeybi', 'Pooya Jannaty', 'Bryan Catanzaro', 'Wei Ping'], 'affiliations': ['NVIDIA'], 'pdf_title_img': 'assets/pdf/title_img/2602.21193.jpg', 'data': {'categories': ['#open_source', '#agents', '#benchmark', '#long_context', '#training', '#synthetic', '#dataset', '#data'], 'emoji': 'âš™ï¸', 'ru': {'title': 'Ğ˜ÑĞºÑƒÑÑÑ‚Ğ²Ğ¾ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…: ĞºĞ°Ğº Ğ¼Ğ°Ğ»Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑƒÑ‡Ğ°Ñ‚ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ°Ğ»Ğ¾Ğ¼', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±Ğ°Ğ·Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Terminal-Task-Gen â€” Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğ¹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ğ´Ğ»Ñ ĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ğ²Ğ¾Ğº Ğ¸ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ², Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ, curriculum learning Ğ¸ Ğ´Ğ¾Ğ»Ğ³Ğ¾ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ. Ğ¡Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Terminal-Corpus Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ» Ğ¾Ğ±ÑƒÑ‡Ğ¸Ñ‚ÑŒ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Nemotron-Terminal, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ Terminal-Bench 2.0, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ³Ğ¾Ñ€Ğ°Ğ·Ğ´Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ’ÑĞµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ Ñ‡Ğ°ÑÑ‚ÑŒ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² Ğ±Ñ‹Ğ»Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²Ğ°.'}, 'en': {'title': 'Boosting Terminal Agents with Synthetic Task Generation', 'desc': 'This paper presents a new method for generating synthetic tasks to enhance the performance of terminal agents in machine learning. The authors introduce Terminal-Task-Gen, a pipeline that allows for the creation of tasks based on specific skills and seeds. They also analyze various data strategies, such as filtering and curriculum learning, to optimize training processes. The resulting models, Nemotron-Terminal, demonstrate significant performance improvements on benchmark tests, even outperforming larger models, and the authors provide open access to their datasets and model checkpoints for further research.'}, 'zh': {'title': 'æå‡ç»ˆç«¯ä»£ç†æ€§èƒ½çš„åˆæˆä»»åŠ¡ç”Ÿæˆä¸æ•°æ®ç­–ç•¥', 'desc': 'ç ”ç©¶äººå‘˜å¼€å‘äº†ä¸€ç§åˆæˆä»»åŠ¡ç”Ÿæˆç®¡é“ï¼Œå¹¶åˆ†æäº†æ•°æ®ç­–ç•¥ä»¥æé«˜ç»ˆç«¯ä»£ç†çš„æ€§èƒ½ï¼Œåˆ›å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡æ•°æ®é›†å’Œæ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹åœ¨åŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†æ›´å¤§çš„å¯¹æ‰‹ã€‚æˆ‘ä»¬æå‡ºäº†Terminal-Task-Genï¼Œè¿™æ˜¯ä¸€ç§è½»é‡çº§çš„åˆæˆä»»åŠ¡ç”Ÿæˆç®¡é“ï¼Œæ”¯æŒåŸºäºç§å­å’ŒæŠ€èƒ½çš„ä»»åŠ¡æ„å»ºã€‚é€šè¿‡æˆ‘ä»¬çš„ç®¡é“ç”Ÿæˆäº†Terminal-Corpusï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºç»ˆç«¯ä»»åŠ¡çš„å¤§è§„æ¨¡å¼€æºæ•°æ®é›†ã€‚ä½¿ç”¨è¯¥æ•°æ®é›†ï¼Œæˆ‘ä»¬è®­ç»ƒäº†Nemotron-Terminalç³»åˆ—æ¨¡å‹ï¼Œæ˜¾è‘—æé«˜äº†åœ¨Terminal-Bench 2.0ä¸Šçš„è¡¨ç°ï¼Œå±•ç¤ºäº†æ•°æ®å·¥ç¨‹å®è·µçš„é‡è¦æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.12192', 'title': 'Query-focused and Memory-aware Reranker for Long Context Processing', 'url': 'https://huggingface.co/papers/2602.12192', 'abstract': 'Abstract A lightweight reranking framework uses attention scores from selected heads to estimate passage-query relevance, achieving strong performance across multiple domains and benchmarks.  \t\t\t\t\tAI-generated summary Built upon the existing analysis of retrieval heads in large language models, we propose an alternative reranking framework that trains models to estimate passage-query relevance using the attention scores of selected heads. This approach provides a listwise solution that leverages holistic information within the entire candidate shortlist during ranking. At the same time, it naturally produces continuous relevance scores, enabling training on arbitrary retrieval datasets without requiring Likert-scale supervision. Our framework is lightweight and effective, requiring only small-scale models (e.g., 4B parameters) to achieve strong performance. Extensive experiments demonstrate that our method outperforms existing state-of-the-art pointwise and listwise rerankers across multiple domains, including Wikipedia and long narrative datasets. It further establishes a new state-of-the-art on the LoCoMo benchmark that assesses the capabilities of dialogue understanding and memory usage. We further demonstrate that our framework supports flexible extensions. For example, augmenting candidate passages with contextual information further improves ranking accuracy, while training attention heads from middle layers enhances efficiency without sacrificing performance.', 'score': 28, 'issue_id': 1218, 'pub_date': '2026-02-12', 'pub_date_card': {'ru': '12 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 12', 'zh': '2æœˆ12æ—¥'}, 'hash': '00604083b95f31dd', 'authors': ['Yuqing Li', 'Jiangnan Li', 'Mo Yu', 'Guoxuan Ding', 'Zheng Lin', 'Weiping Wang', 'Jie Zhou'], 'affiliations': ['Institute of Information Engineering, Chinese Academy of Sciences', 'Pattern Recognition Center, WeChat AI, Tencent Inc', 'School of Cyber Security, University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2602.12192.jpg', 'data': {'categories': ['#small_models', '#training', '#rag', '#benchmark'], 'emoji': 'ğŸ”„', 'ru': {'title': 'Ğ›Ñ‘Ğ³ĞºĞ¾Ğµ Ğ¿ĞµÑ€ĞµÑ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ³Ğ¾Ğ»Ğ¾Ğ²Ñ‹ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ±ĞµĞ· Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ»Ñ‘Ğ³ĞºĞ¸Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¿ĞµÑ€ĞµÑ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ· Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ³Ğ¾Ğ»Ğ¾Ğ² Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ° Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ° Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑĞ¿Ğ¸ÑĞºĞ° ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ¾Ğ², ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ñ Ğ²ÑÑ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸, Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ ÑˆĞºĞ°Ğ»Ğ¾Ğ¹ Ğ›Ğ°Ğ¹ĞºĞµÑ€Ñ‚Ğ°. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞ° 4 Ğ¼Ğ»Ñ€Ğ´ Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğµ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ², Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ñ‹ Wikipedia Ğ¸ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ³Ğ¸Ğ±ĞºĞ¸Ğµ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ: Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ»Ğ¾Ñ‘Ğ² Ğ¸Ğ· ÑĞµÑ€ĞµĞ´Ğ¸Ğ½Ñ‹ ÑĞµÑ‚Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Efficient Reranking with Attention Scores for Enhanced Passage-Query Relevance', 'desc': 'This paper presents a lightweight reranking framework that utilizes attention scores from selected heads in large language models to assess the relevance of passages to queries. By focusing on holistic information from the entire shortlist of candidates, the framework offers a listwise approach that generates continuous relevance scores, allowing for training on various datasets without needing strict supervision. The method is efficient, requiring only smaller models to achieve competitive performance across different benchmarks, including Wikipedia and narrative datasets. Additionally, the framework shows flexibility for enhancements, such as incorporating contextual information and optimizing attention heads for better efficiency.'}, 'zh': {'title': 'è½»é‡çº§é‡æ’åºæ¡†æ¶ï¼Œæå‡æ£€ç´¢ç›¸å…³æ€§', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§è½»é‡çº§çš„é‡æ’åºæ¡†æ¶ï¼Œåˆ©ç”¨é€‰å®šå¤´éƒ¨çš„æ³¨æ„åŠ›åˆ†æ•°æ¥ä¼°è®¡æ®µè½ä¸æŸ¥è¯¢çš„ç›¸å…³æ€§ã€‚è¿™ç§æ–¹æ³•é€šè¿‡æ•´ä½“ä¿¡æ¯çš„åˆ©ç”¨ï¼Œæä¾›äº†ä¸€ç§åˆ—è¡¨å¼çš„è§£å†³æ–¹æ¡ˆï¼Œèƒ½å¤Ÿåœ¨æ’åæ—¶è€ƒè™‘æ•´ä¸ªå€™é€‰åˆ—è¡¨ã€‚è¯¥æ¡†æ¶ç”Ÿæˆè¿ç»­çš„ç›¸å…³æ€§åˆ†æ•°ï¼Œä½¿å¾—åœ¨ä»»æ„æ£€ç´¢æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒæˆä¸ºå¯èƒ½ï¼Œè€Œæ— éœ€ä¾èµ–Likerté‡è¡¨çš„ç›‘ç£ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªé¢†åŸŸå’ŒåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›çš„é‡æ’åºæ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶é«˜æ•ˆæ€§å’Œçµæ´»æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.21015', 'title': 'From Perception to Action: An Interactive Benchmark for Vision Reasoning', 'url': 'https://huggingface.co/papers/2602.21015', 'abstract': "Abstract Current vision-language models lack capability to understand physical structures and causal constraints needed for complex, interactive 3D tasks, as demonstrated by the CHAIN benchmark evaluating structured action planning under physical constraints.  \t\t\t\t\tAI-generated summary Understanding the physical structure is essential for real-world applications such as embodied agents, interactive design, and long-horizon manipulation. Yet, prevailing Vision-Language Model (VLM) evaluations still center on structure-agnostic, single-turn setups (e.g., VQA), which fail to assess agents' ability to reason about how geometry, contact, and support relations jointly constrain what actions are possible in a dynamic environment. To address this gap, we introduce the Causal Hierarchy of Actions and Interactions (CHAIN) benchmark, an interactive 3D, physics-driven testbed designed to evaluate whether models can understand, plan, and execute structured action sequences grounded in physical constraints. CHAIN shifts evaluation from passive perception to active problem solving, spanning tasks such as interlocking mechanical puzzles and 3D stacking and packing. We conduct a comprehensive study of state-of-the-art VLMs and diffusion-based models under unified interactive settings. Our results show that top-performing models still struggle to internalize physical structure and causal constraints, often failing to produce reliable long-horizon plans and cannot robustly translate perceived structure into effective actions. The project is available at https://social-ai-studio.github.io/CHAIN/.", 'score': 19, 'issue_id': 1212, 'pub_date': '2026-02-24', 'pub_date_card': {'ru': '24 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 24', 'zh': '2æœˆ24æ—¥'}, 'hash': '28d09e54597b0fc5', 'authors': ['Yuhao Wu', 'Maojia Song', 'Yihuai Lan', 'Lei Wang', 'Zhiqiang Hu', 'Yao Xiao', 'Heng Zhou', 'Weihua Zheng', 'Dylan Raharja', 'Soujanya Poria', 'Roy Ka-Wei Lee'], 'affiliations': ['Nanyang Technological University (NTU), Singapore', 'Singapore Management University (SMU), Singapore', 'Singapore University of Technology and Design (SUTD), Singapore', 'University of Science and Technology of China (USTC), China'], 'pdf_title_img': 'assets/pdf/title_img/2602.21015.jpg', 'data': {'categories': ['#agents', '#multimodal', '#robotics', '#benchmark', '#3d', '#cv'], 'emoji': 'ğŸ§©', 'ru': {'title': 'Ğ¤Ğ¸Ğ·Ğ¸ĞºĞ° Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹: Ğ¾Ñ‚ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğº Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº CHAIN Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ° Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ¸ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-ÑĞ»ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ² Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ğ¾Ğ¹ ÑÑ€ĞµĞ´Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ vision-language Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğµ Ğ¼Ğ¾Ğ³ÑƒÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… ÑƒÑ‡Ñ‘Ñ‚Ğ° Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ĞºĞ¾Ğ½Ñ‚Ğ°ĞºÑ‚Ğ¾Ğ² Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¾Ñ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğ»Ğ¾Ğ¼Ğ¾Ğº Ğ´Ğ¾ ÑƒĞºĞ»Ğ°Ğ´ĞºĞ¸ Ğ¸ ÑƒĞ¿Ğ°ĞºĞ¾Ğ²ĞºĞ¸ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼, Ğ° Ğ½Ğµ Ğ¿Ğ°ÑÑĞ¸Ğ²Ğ½Ğ¾Ğ¼ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ VLM-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ Ğ½Ğµ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸Ğ½ÑÑ‚Ñ‹Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ² ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ.'}, 'en': {'title': 'Empowering AI with Physical Understanding for 3D Interaction', 'desc': 'This paper addresses the limitations of current vision-language models (VLMs) in understanding physical structures and causal relationships necessary for complex 3D tasks. It introduces the Causal Hierarchy of Actions and Interactions (CHAIN) benchmark, which evaluates models on their ability to plan and execute actions based on physical constraints in an interactive 3D environment. The study reveals that even state-of-the-art models struggle with long-horizon planning and translating perceived structures into effective actions. By shifting the focus from passive perception to active problem-solving, this benchmark aims to enhance the evaluation of AI in real-world applications.'}, 'zh': {'title': 'ç†è§£ç‰©ç†ç»“æ„ï¼Œæå‡äº¤äº’èƒ½åŠ›', 'desc': 'å½“å‰çš„è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨ç†è§£ç‰©ç†ç»“æ„å’Œå› æœçº¦æŸæ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œè¿™å¯¹äºå¤æ‚çš„äº¤äº’å¼3Dä»»åŠ¡è‡³å…³é‡è¦ã€‚æˆ‘ä»¬æå‡ºäº†å› æœè¡ŒåŠ¨ä¸äº¤äº’å±‚æ¬¡ï¼ˆCHAINï¼‰åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°æ¨¡å‹åœ¨ç‰©ç†çº¦æŸä¸‹ç†è§£ã€è§„åˆ’å’Œæ‰§è¡Œç»“æ„åŒ–è¡ŒåŠ¨åºåˆ—çš„èƒ½åŠ›ã€‚CHAINåŸºå‡†å°†è¯„ä¼°ä»è¢«åŠ¨æ„ŸçŸ¥è½¬å‘ä¸»åŠ¨é—®é¢˜è§£å†³ï¼Œæ¶µç›–æœºæ¢°æ‹¼å›¾å’Œ3Då †å ç­‰ä»»åŠ¡ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå°½ç®¡ä¸€äº›é¡¶å°–æ¨¡å‹è¡¨ç°è‰¯å¥½ï¼Œä½†ä»ç„¶éš¾ä»¥å†…åŒ–ç‰©ç†ç»“æ„å’Œå› æœçº¦æŸï¼Œæ— æ³•å¯é åœ°ç”Ÿæˆé•¿æ—¶é—´çš„è®¡åˆ’ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.20739', 'title': 'PyVision-RL: Forging Open Agentic Vision Models via RL', 'url': 'https://huggingface.co/papers/2602.20739', 'abstract': 'Abstract PyVision-RL framework addresses interaction collapse in multimodal models through enhanced reinforcement learning techniques and efficient video processing strategies.  \t\t\t\t\tAI-generated summary Reinforcement learning for agentic multimodal models often suffers from interaction collapse, where models learn to reduce tool usage and multi-turn reasoning, limiting the benefits of agentic behavior. We introduce PyVision-RL, a reinforcement learning framework for open-weight multimodal models that stabilizes training and sustains interaction. Our approach combines an oversampling-filtering-ranking rollout strategy with an accumulative tool reward to prevent collapse and encourage multi-turn tool use. Using a unified training pipeline, we develop PyVision-Image and PyVision-Video for image and video understanding. For video reasoning, PyVision-Video employs on-demand context construction, selectively sampling task-relevant frames during reasoning to significantly reduce visual token usage. Experiments show strong performance and improved efficiency, demonstrating that sustained interaction and on-demand visual processing are critical for scalable multimodal agents.', 'score': 19, 'issue_id': 1212, 'pub_date': '2026-02-24', 'pub_date_card': {'ru': '24 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 24', 'zh': '2æœˆ24æ—¥'}, 'hash': '4f9be2e1a2a14149', 'authors': ['Shitian Zhao', 'Shaoheng Lin', 'Ming Li', 'Haoquan Zhang', 'Wenshuo Peng', 'Kaipeng Zhang', 'Chen Wei'], 'affiliations': ['CUHK', 'Rice University', 'Shanda AI Research, Tokyo', 'Shanghai AI Lab', 'THU', 'UMD'], 'pdf_title_img': 'assets/pdf/title_img/2602.20739.jpg', 'data': {'categories': ['#open_source', '#agents', '#multimodal', '#rl', '#reasoning', '#video', '#training', '#optimization'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ¡Ñ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ÑÑ PyVision-RL â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ĞºĞ¾Ğ»Ğ»Ğ°Ğ¿ÑĞ° Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ, ĞºĞ¾Ğ³Ğ´Ğ° Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ¿ĞµÑ€ĞµÑÑ‚Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ-Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸-Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ½Ğ°ĞºĞ¾Ğ¿Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ·Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ”Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ·Ğ° ÑÑ‡Ñ‘Ñ‚ Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ° Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ğµ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ñ‹ Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'Enhancing Multimodal Interaction with PyVision-RL', 'desc': 'The PyVision-RL framework tackles the problem of interaction collapse in multimodal models by utilizing advanced reinforcement learning techniques. This issue arises when models minimize tool usage and multi-turn reasoning, which hinders their ability to act effectively. PyVision-RL introduces a novel strategy that combines oversampling, filtering, and ranking to stabilize training and promote sustained interaction. Additionally, it features a unique approach for video processing that selectively samples relevant frames, enhancing efficiency and performance in multimodal tasks.'}, 'zh': {'title': 'æŒç»­äº¤äº’ï¼Œæå‡å¤šæ¨¡æ€æ™ºèƒ½ä½“çš„èƒ½åŠ›', 'desc': 'PyVision-RLæ¡†æ¶é€šè¿‡å¢å¼ºçš„å¼ºåŒ–å­¦ä¹ æŠ€æœ¯å’Œé«˜æ•ˆçš„è§†é¢‘å¤„ç†ç­–ç•¥ï¼Œè§£å†³äº†å¤šæ¨¡æ€æ¨¡å‹ä¸­çš„äº¤äº’å´©æºƒé—®é¢˜ã€‚è¯¥æ¡†æ¶æ—¨åœ¨ç¨³å®šè®­ç»ƒè¿‡ç¨‹ï¼Œå¹¶ä¿æŒæ¨¡å‹çš„äº¤äº’èƒ½åŠ›ï¼Œé¿å…å·¥å…·ä½¿ç”¨å‡å°‘å’Œå¤šè½®æ¨ç†çš„é™åˆ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†è¿‡é‡‡æ ·-è¿‡æ»¤-æ’åºçš„å›æ”¾ç­–ç•¥å’Œç´¯ç§¯å·¥å…·å¥–åŠ±ï¼Œä»¥é¼“åŠ±å¤šè½®å·¥å…·ä½¿ç”¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒæŒç»­çš„äº¤äº’å’ŒæŒ‰éœ€è§†è§‰å¤„ç†å¯¹äºå¯æ‰©å±•çš„å¤šæ¨¡æ€æ™ºèƒ½ä½“è‡³å…³é‡è¦ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.14337', 'title': 'LongCLI-Bench: A Preliminary Benchmark and Study for Long-horizon Agentic Programming in Command-Line Interfaces', 'url': 'https://huggingface.co/papers/2602.14337', 'abstract': "Abstract LongCLI-Bench evaluates AI agents' ability to complete complex, multi-step programming tasks through command-line interfaces with detailed failure analysis and human-agent collaboration insights.  \t\t\t\t\tAI-generated summary Recent advances in AI-assisted programming have empowered agents to execute complex workflows via command-line interfaces, however, existing benchmarks are limited by short task horizons, data contamination from GitHub scraping, and a lack of fine-grained evaluation metrics, fail to rigorously evaluate the long-horizon planning and execution capabilities essential for realistic software engineering. To address these gaps, we introduce LongCLI-Bench, a comprehensive benchmark designed to evaluate agentic capabilities across long-horizon, realistic tasks. We curated 20 high-quality, long-horizon tasks from over 1,000 computer science assignments and real-world workflows, covering four engineering categories: from scratch, feature addition, bug fixing, and refactoring. We propose a dual-set testing protocol for LongCLI-Bench, which measures requirement fulfillment (fail-to-pass) and regression avoidance (pass-to-pass), and incorporates step-level scoring to pinpoint execution failures. Extensive experiments reveal that even state-of-the-art agents achieve pass rates below 20% in LongCLI-Bench. Step-level analysis further indicates that the majority of tasks stall at less than 30% completion, highlighting that critical failures often occur in the early stages. Although self-correction offers marginal gains, human-agent collaboration through plan injection and interactive guidance yields significantly higher improvements. These results highlight that future research must emphasize the development of synergistic human-agent workflows alongside advances in agents' planning and execution capabilities to overcome key challenges in long-horizon task performance.", 'score': 10, 'issue_id': 1212, 'pub_date': '2026-02-15', 'pub_date_card': {'ru': '15 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 15', 'zh': '2æœˆ15æ—¥'}, 'hash': '62e389ea8e6d8823', 'authors': ['Yukang Feng', 'Jianwen Sun', 'Zelai Yang', 'Jiaxin Ai', 'Chuanhao Li', 'Zizhen Li', 'Fanrui Zhang', 'Kang He', 'Rui Ma', 'Jifan Lin', 'Jie Sun', 'Yang Xiao', 'Sizhuo Zhou', 'Wenxiao Wu', 'Yiming Liu', 'Pengfei Liu', 'Yu Qiao', 'Shenglin Zhang', 'Kaipeng Zhang'], 'affiliations': ['NKU', 'SII', 'SJTU', 'Shanda AI Research Tokyo', 'Shanghai AI Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2602.14337.jpg', 'data': {'categories': ['#agents', '#plp', '#long_context', '#benchmark'], 'emoji': 'ğŸ› ï¸', 'ru': {'title': 'Ğ­Ñ‚Ğ°Ğ»Ğ¾Ğ½ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğº Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ğ½Ğ¾Ğ¼Ñƒ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ LongCLI-Bench â€” ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ¾ĞºÑƒ. ĞĞ°Ğ±Ğ¾Ñ€ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 20 Ğ²Ñ‹ÑĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ñ Ğ½ÑƒĞ»Ñ, Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹, Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ¸ Ñ€ĞµÑ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³ ĞºĞ¾Ğ´Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ²ÑƒÑ…ÑĞµÑ‚Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ» Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¾Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸Ğ·Ğ¼ĞµÑ€ÑĞµÑ‚ ĞºĞ°Ğº Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹, Ñ‚Ğ°Ğº Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ¼ĞµĞ½ĞµĞµ 20% ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚Ğ¸, Ğ¾Ğ´Ğ½Ğ°ĞºĞ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹.'}, 'en': {'title': 'Enhancing AI Agents with LongCLI-Bench: A New Era in Programming Tasks', 'desc': 'The paper introduces LongCLI-Bench, a new benchmark for evaluating AI agents on complex programming tasks using command-line interfaces. It addresses limitations of existing benchmarks by focusing on long-horizon tasks and providing detailed metrics for evaluating agent performance. The study reveals that even advanced AI agents struggle with these tasks, achieving less than 20% success rates, particularly stalling in early execution stages. The findings suggest that enhancing human-agent collaboration can significantly improve task completion rates, indicating a need for future research to focus on synergistic workflows between humans and AI.'}, 'zh': {'title': 'é•¿æ—¶é—´ä»»åŠ¡æ€§èƒ½çš„è¯„ä¼°ä¸äººæœºåä½œçš„é‡è¦æ€§', 'desc': 'LongCLI-Bench æ˜¯ä¸€ä¸ªè¯„ä¼° AI ä»£ç†åœ¨å‘½ä»¤è¡Œç•Œé¢ä¸­å®Œæˆå¤æ‚å¤šæ­¥éª¤ç¼–ç¨‹ä»»åŠ¡èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚è¯¥ç ”ç©¶æŒ‡å‡ºï¼Œç°æœ‰çš„åŸºå‡†æµ‹è¯•åœ¨é•¿æ—¶é—´ä»»åŠ¡è§„åˆ’å’Œæ‰§è¡Œèƒ½åŠ›çš„è¯„ä¼°ä¸Šå­˜åœ¨ä¸è¶³ã€‚é€šè¿‡ä» 1000 å¤šä¸ªè®¡ç®—æœºç§‘å­¦ä½œä¸šä¸­æŒ‘é€‰å‡º 20 ä¸ªé«˜è´¨é‡çš„é•¿æ—¶é—´ä»»åŠ¡ï¼ŒLongCLI-Bench æä¾›äº†æ›´ç»†è‡´çš„è¯„ä¼°æŒ‡æ ‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„ä»£ç†åœ¨ LongCLI-Bench ä¸­çš„é€šè¿‡ç‡ä¹Ÿä½äº 20%ï¼Œå¼ºè°ƒäº†äººæœºåä½œåœ¨æå‡ä»»åŠ¡å®Œæˆç‡æ–¹é¢çš„é‡è¦æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.18940', 'title': 'DREAM: Deep Research Evaluation with Agentic Metrics', 'url': 'https://huggingface.co/papers/2602.18940', 'abstract': 'Abstract Deep Research Agents generate analyst-grade reports, yet evaluating them remains challenging due to the absence of a single ground truth and the multidimensional nature of research quality. Recent benchmarks propose distinct methodologies, yet they suffer from the Mirage of Synthesis, where strong surface-level fluency and citation alignment can obscure underlying factual and reasoning defects. We characterize this gap by introducing a taxonomy across four verticals that exposes a critical capability mismatch: static evaluators inherently lack the tool-use capabilities required to assess temporal validity and factual correctness. To address this, we propose DREAM (Deep Research Evaluation with Agentic Metrics), a framework that instantiates the principle of capability parity by making evaluation itself agentic. DREAM structures assessment through an evaluation protocol combining query-agnostic metrics with adaptive metrics generated by a tool-calling agent, enabling temporally aware coverage, grounded verification, and systematic reasoning probes. Controlled evaluations demonstrate DREAM is significantly more sensitive to factual and temporal decay than existing benchmarks, offering a scalable, reference-free evaluation paradigm.  \t\t\t\t\tAI-generated summary Deep Research Agents generate analyst-grade reports, yet evaluating them remains challenging due to the absence of a single ground truth and the multidimensional nature of research quality. Recent benchmarks propose distinct methodologies, yet they suffer from the Mirage of Synthesis, where strong surface-level fluency and citation alignment can obscure underlying factual and reasoning defects. We characterize this gap by introducing a taxonomy across four verticals that exposes a critical capability mismatch: static evaluators inherently lack the tool-use capabilities required to assess temporal validity and factual correctness. To address this, we propose DREAM (Deep Research Evaluation with Agentic Metrics), a framework that instantiates the principle of capability parity by making evaluation itself agentic. DREAM structures assessment through an evaluation protocol combining query-agnostic metrics with adaptive metrics generated by a tool-calling agent, enabling temporally aware coverage, grounded verification, and systematic reasoning probes. Controlled evaluations demonstrate DREAM is significantly more sensitive to factual and temporal decay than existing benchmarks, offering a scalable, reference-free evaluation paradigm.', 'score': 9, 'issue_id': 1214, 'pub_date': '2026-02-21', 'pub_date_card': {'ru': '21 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 21', 'zh': '2æœˆ21æ—¥'}, 'hash': 'a904e28b8a40ea88', 'authors': ['Elad Ben Avraham', 'Changhao Li', 'Ron Dorfman', 'Roy Ganz', 'Oren Nuriel', 'Amir Dudai', 'Aviad Aberdam', 'Noah Flynn', 'Elman Mansimov', 'Adi Kalyanpur', 'Ron Litman'], 'affiliations': ['AWS Agentic AI', 'Georgia Institute of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2602.18940.jpg', 'data': {'categories': [], 'emoji': 'ğŸ”', 'ru': {'title': 'ĞĞ³ĞµĞ½Ñ‚Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ğ¸ÑÑ‚Ğ¸Ğ½Ñ‹ Ğ² Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚Ğ°Ñ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚Ğ¾Ğ², Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Deep Research Agents. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸ ÑÑ‚Ñ€Ğ°Ğ´Ğ°ÑÑ‚ Ğ¾Ñ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ½Ğ¾Ğ¹ Ğ±ĞµĞ³Ğ»Ğ¾ÑÑ‚Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ†Ğ¸Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑĞºÑ€Ñ‹Ğ²Ğ°ÑÑ‚ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ğµ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº DREAM, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿ Ğ¿Ğ°Ñ€Ğ¸Ñ‚ĞµÑ‚Ğ° Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹, Ğ´ĞµĞ»Ğ°Ñ ÑĞ°Ğ¼Ñƒ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ñ„Ğ°ĞºÑ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ DREAM Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ»ÑƒÑ‡ÑˆĞµ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ¸ ÑƒÑÑ‚Ğ°Ñ€ĞµĞ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸.'}, 'en': {'title': 'DREAM: Elevating Research Evaluation with Agentic Metrics', 'desc': "This paper discusses the challenges in evaluating reports generated by Deep Research Agents due to the lack of a single ground truth and the complex nature of research quality. It highlights the issue of the 'Mirage of Synthesis', where reports may appear fluent and well-cited but can still contain factual inaccuracies and flawed reasoning. The authors introduce a new framework called DREAM, which aims to improve evaluation by incorporating agentic metrics that assess temporal validity and factual correctness. Through controlled evaluations, DREAM is shown to be more effective than existing benchmarks in detecting factual and temporal decay, providing a scalable and reference-free evaluation method."}, 'zh': {'title': 'DREAMï¼šæ™ºèƒ½è¯„ä¼°æ·±åº¦ç ”ç©¶çš„æœªæ¥', 'desc': 'æ·±åº¦ç ”ç©¶ä»£ç†ç”Ÿæˆåˆ†æå¸ˆçº§åˆ«çš„æŠ¥å‘Šï¼Œä½†ç”±äºç¼ºä¹å•ä¸€çš„çœŸå®æ ‡å‡†å’Œç ”ç©¶è´¨é‡çš„å¤šç»´ç‰¹æ€§ï¼Œè¯„ä¼°è¿™äº›æŠ¥å‘Šä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ç°æœ‰çš„åŸºå‡†æ–¹æ³•å­˜åœ¨åˆæˆå¹»è§‰çš„é—®é¢˜ï¼Œè¡¨é¢æµç•…æ€§å’Œå¼•ç”¨å¯¹é½å¯èƒ½æ©ç›–äº‹å®å’Œæ¨ç†çš„ç¼ºé™·ã€‚æˆ‘ä»¬é€šè¿‡å¼•å…¥ä¸€ä¸ªå››ä¸ªç»´åº¦çš„åˆ†ç±»æ³•æ¥æè¿°è¿™ä¸€å·®è·ï¼Œæ­ç¤ºäº†é™æ€è¯„ä¼°è€…åœ¨è¯„ä¼°æ—¶é—´æœ‰æ•ˆæ€§å’Œäº‹å®æ­£ç¡®æ€§æ–¹é¢çš„èƒ½åŠ›ä¸åŒ¹é…ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†DREAMæ¡†æ¶ï¼Œé€šè¿‡å°†è¯„ä¼°æœ¬èº«å˜ä¸ºä»£ç†åŒ–ï¼Œç»“åˆæŸ¥è¯¢æ— å…³çš„æŒ‡æ ‡å’Œç”±å·¥å…·è°ƒç”¨ä»£ç†ç”Ÿæˆçš„è‡ªé€‚åº”æŒ‡æ ‡ï¼Œæä¾›äº†ä¸€ä¸ªå¯æ‰©å±•çš„ã€æ— å‚è€ƒçš„è¯„ä¼°èŒƒå¼ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.16990', 'title': 'Conv-FinRe: A Conversational and Longitudinal Benchmark for Utility-Grounded Financial Recommendation', 'url': 'https://huggingface.co/papers/2602.16990', 'abstract': "Abstract A new conversational financial recommendation benchmark evaluates large language models' ability to balance rational decision-making with user behavior alignment using multi-view references derived from real market data and human decision trajectories.  \t\t\t\t\tAI-generated summary Most recommendation benchmarks evaluate how well a model imitates user behavior. In financial advisory, however, observed actions can be noisy or short-sighted under market volatility and may conflict with a user's long-term goals. Treating what users chose as the sole ground truth, therefore, conflates behavioral imitation with decision quality. We introduce Conv-FinRe, a conversational and longitudinal benchmark for stock recommendation that evaluates LLMs beyond behavior matching. Given an onboarding interview, step-wise market context, and advisory dialogues, models must generate rankings over a fixed investment horizon. Crucially, Conv-FinRe provides multi-view references that distinguish descriptive behavior from normative utility grounded in investor-specific risk preferences, enabling diagnosis of whether an LLM follows rational analysis, mimics user noise, or is driven by market momentum. We build the benchmark from real market data and human decision trajectories, instantiate controlled advisory conversations, and evaluate a suite of state-of-the-art LLMs. Results reveal a persistent tension between rational decision quality and behavioral alignment: models that perform well on utility-based ranking often fail to match user choices, whereas behaviorally aligned models can overfit short-term noise. The dataset is publicly released on Hugging Face, and the codebase is available on GitHub.", 'score': 7, 'issue_id': 1212, 'pub_date': '2026-02-19', 'pub_date_card': {'ru': '19 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 19', 'zh': '2æœˆ19æ—¥'}, 'hash': 'f9665a7ad14a7cb3', 'authors': ['Yan Wang', 'Yi Han', 'Lingfei Qian', 'Yueru He', 'Xueqing Peng', 'Dongji Feng', 'Zhuohan Xie', 'Vincent Jim Zhang', 'Rosie Guo', 'Fengran Mo', 'Jimin Huang', 'Yankai Chen', 'Xue Liu', 'Jian-Yun Nie'], 'affiliations': ['MBZUAI', 'McGill University', 'The Fin AI USA', 'University of Montreal'], 'pdf_title_img': 'assets/pdf/title_img/2602.16990.jpg', 'data': {'categories': [], 'emoji': 'ğŸ“ˆ', 'ru': {'title': 'Ğ‘Ğ°Ğ»Ğ°Ğ½Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸ĞµĞ¼ Ğ² Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸ÑÑ… LLM', 'desc': 'ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Conv-FinRe â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ Ğ°ĞºÑ†Ğ¸ÑĞ¼ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½ÑÑƒĞ»ÑŒÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ¾Ğ»Ğ¶Ğ½Ñ‹ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸ĞµĞ¼ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ½Ğ²ĞµÑÑ‚Ğ¾Ñ€Ğ° Ğ¸ Ğ¸Ğ¼Ğ¸Ñ‚Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ ÑˆÑƒĞ¼Ğ½Ñ‹Ğ¼ Ğ¸Ğ»Ğ¸ Ğ±Ğ»Ğ¸Ğ·Ğ¾Ñ€ÑƒĞºĞ¸Ğ¼. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸-Ñ€ĞµÑ„ĞµÑ€ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€Ñ‹Ğ½ĞºĞ° Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ°Ñ‚ÑŒ Ğ½Ğ°ÑÑ‚Ğ¾ÑÑ‰ĞµĞµ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ¾Ñ‚ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ÑÑ‚Ğ¸. ĞÑ†ĞµĞ½ĞºĞ° ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… LLM Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾Ñ€ĞµÑ‡Ğ¸Ğµ: Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¸Ğ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ¿Ğ»Ğ¾Ñ…Ğ¾ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‚ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ñƒ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹, Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ, Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ÑÑ Ğ½Ğ° ĞºÑ€Ğ°Ñ‚ĞºĞ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ñ€Ñ‹Ğ½Ğ¾Ñ‡Ğ½Ñ‹Ğµ ĞºĞ¾Ğ»ĞµĞ±Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'Balancing Rationality and User Behavior in Financial Recommendations', 'desc': "This paper introduces Conv-FinRe, a new benchmark for evaluating large language models (LLMs) in the context of financial recommendations. Unlike traditional benchmarks that focus solely on how well models mimic user behavior, Conv-FinRe assesses the models' ability to balance rational decision-making with user preferences over time. It incorporates multi-view references derived from real market data and human decision-making patterns, allowing for a nuanced evaluation of model performance. The findings highlight a trade-off between making rational investment recommendations and aligning with user behavior, revealing that models excelling in one area often struggle in the other."}, 'zh': {'title': 'ç†æ€§å†³ç­–ä¸ç”¨æˆ·è¡Œä¸ºçš„å¹³è¡¡æŒ‘æˆ˜', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„å¯¹è¯å¼é‡‘èæ¨èåŸºå‡†Conv-FinReï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç†æ€§å†³ç­–ä¸ç”¨æˆ·è¡Œä¸ºä¸€è‡´æ€§ä¹‹é—´çš„å¹³è¡¡èƒ½åŠ›ã€‚è¯¥åŸºå‡†ä½¿ç”¨çœŸå®å¸‚åœºæ•°æ®å’Œäººç±»å†³ç­–è½¨è¿¹ç”Ÿæˆå¤šè§†è§’å‚è€ƒï¼Œå¸®åŠ©åŒºåˆ†æè¿°æ€§è¡Œä¸ºä¸åŸºäºæŠ•èµ„è€…ç‰¹å®šé£é™©åå¥½çš„è§„èŒƒæ•ˆç”¨ã€‚é€šè¿‡å¯¹æ¨¡å‹åœ¨å›ºå®šæŠ•èµ„æœŸé™å†…ç”Ÿæˆæ¨èæ’åçš„è¯„ä¼°ï¼Œç ”ç©¶å‘ç°ç†æ€§å†³ç­–è´¨é‡ä¸è¡Œä¸ºä¸€è‡´æ€§ä¹‹é—´å­˜åœ¨æŒç»­çš„ç´§å¼ å…³ç³»ã€‚æœ€ç»ˆï¼Œç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè™½ç„¶ä¸€äº›æ¨¡å‹åœ¨æ•ˆç”¨æ’åä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†å¾€å¾€æ— æ³•åŒ¹é…ç”¨æˆ·é€‰æ‹©ï¼Œè€Œè¡Œä¸ºä¸€è‡´æ€§å¼ºçš„æ¨¡å‹åˆ™å¯èƒ½è¿‡åº¦æ‹ŸåˆçŸ­æœŸå™ªå£°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.20309', 'title': 'QuantVLA: Scale-Calibrated Post-Training Quantization for Vision-Language-Action Models', 'url': 'https://huggingface.co/papers/2602.20309', 'abstract': 'Abstract QuantVLA is a post-training quantization framework for vision-language-action models that enables efficient deployment through selective quantization, attention temperature matching, and output head balancing while maintaining performance and reducing memory and latency.  \t\t\t\t\tAI-generated summary Vision-language-action (VLA) models unify perception, language, and control for embodied agents but face significant challenges in practical deployment due to rapidly increasing compute and memory demands, especially as models scale to longer horizons and larger backbones. To address these bottlenecks, we introduce QuantVLA, a training-free post-training quantization (PTQ) framework that, to our knowledge, is the first PTQ approach for VLA systems and the first to successfully quantize a diffusion transformer (DiT) action head. QuantVLA incorporates three scale-calibrated components: (1) a selective quantization layout that integerizes all linear layers in both the language backbone and the DiT while keeping attention projections in floating point to preserve the original operator schedule; (2) attention temperature matching, a lightweight per-head scaling mechanism that stabilizes attention logits and is folded into the dequantization scales at inference; and (3) output head balancing, a per-layer residual interface calibration that mitigates post-projection energy drift. The framework requires no additional training, uses only a small unlabeled calibration buffer, and supports integer kernels for low-bit weights and activations while leaving the architecture unchanged. Across representative VLA models on LIBERO, QuantVLA exceeds the task success rates of full-precision baselines, achieves about 70% relative memory savings on the quantized components, and delivers a 1.22x speedup in end-to-end inference latency, providing a practical pathway toward scalable low-bit embodied intelligence under strict compute, memory, and power constraints.', 'score': 5, 'issue_id': 1218, 'pub_date': '2026-02-23', 'pub_date_card': {'ru': '23 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 23', 'zh': '2æœˆ23æ—¥'}, 'hash': '49f20af61574a93d', 'authors': ['Jingxuan Zhang', 'Yunta Hsieh', 'Zhongwei Wang', 'Haokun Lin', 'Xin Wang', 'Ziqi Wang', 'Yingtie Lei', 'Mi Zhang'], 'affiliations': ['City University of Hong Kong', 'Indiana University', 'The Ohio State University', 'University of Michigan'], 'pdf_title_img': 'assets/pdf/title_img/2602.20309.jpg', 'data': {'categories': ['#inference', '#multimodal', '#open_source', '#robotics', '#optimization'], 'emoji': 'âš¡', 'ru': {'title': 'ĞšĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ğ¼Ğ¸', 'desc': 'QuantVLA â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰ĞµĞ³Ğ¾ÑÑ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ vision-language-action, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‚ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ, ÑĞ·Ñ‹Ğº Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰Ñ‘Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚Ñ€Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°: ÑĞµĞ»ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ñ… ÑĞ»Ğ¾Ñ‘Ğ² Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ attention-Ğ¿Ñ€Ğ¾ĞµĞºÑ†Ğ¸Ğ¹ Ğ² Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, Ñ‚ĞµĞ¼Ğ¿ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ğ½Ğ¾Ğµ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ğµ attention Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ‚Ğ¾Ğ² Ğ¸ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²ĞºÑƒ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ³Ğ¾Ğ»Ğ¾Ğ² Ğ´Ğ»Ñ ĞºĞ¾Ğ¼Ğ¿ĞµĞ½ÑĞ°Ñ†Ğ¸Ğ¸ ÑĞ½ĞµÑ€Ğ³ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ´Ñ€ĞµĞ¹Ñ„Ğ°. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ Ğ±ÑƒÑ„ĞµÑ€Ğ¾Ğ¼. ĞĞ° Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸ĞºĞµ QuantVLA ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ½Ğ° 70%, ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½Ñ Ğ² 1.22 Ñ€Ğ°Ğ·Ğ° Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ¼ ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ²Ñ‘Ñ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ….'}, 'en': {'title': 'Efficient Deployment of VLA Models with QuantVLA', 'desc': 'QuantVLA is a novel post-training quantization framework designed specifically for vision-language-action (VLA) models, which integrate perception, language, and control for intelligent agents. It addresses the challenges of deploying large-scale models by implementing selective quantization, which optimizes memory usage while maintaining performance. The framework also includes attention temperature matching and output head balancing to enhance the stability and efficiency of the model during inference. By requiring no additional training and achieving significant memory savings and speed improvements, QuantVLA paves the way for efficient low-bit deployment of advanced AI systems.'}, 'zh': {'title': 'é«˜æ•ˆéƒ¨ç½²è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹çš„é‡åŒ–è§£å†³æ–¹æ¡ˆ', 'desc': 'QuantVLAæ˜¯ä¸€ä¸ªåè®­ç»ƒé‡åŒ–æ¡†æ¶ï¼Œä¸“ä¸ºè§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹è®¾è®¡ï¼Œæ—¨åœ¨æé«˜éƒ¨ç½²æ•ˆç‡ã€‚å®ƒé€šè¿‡é€‰æ‹©æ€§é‡åŒ–ã€æ³¨æ„åŠ›æ¸©åº¦åŒ¹é…å’Œè¾“å‡ºå¤´å¹³è¡¡æ¥é™ä½å†…å­˜å’Œå»¶è¿Ÿï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹æ€§èƒ½ã€‚è¯¥æ¡†æ¶æ— éœ€é¢å¤–è®­ç»ƒï¼Œä»…éœ€å°‘é‡æœªæ ‡è®°çš„æ ¡å‡†æ•°æ®ï¼Œå¹¶æ”¯æŒä½ä½æƒé‡å’Œæ¿€æ´»çš„æ•´æ•°å†…æ ¸ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒQuantVLAåœ¨LIBEROæ•°æ®é›†ä¸Šè¶…è¶Šäº†å…¨ç²¾åº¦åŸºçº¿ï¼Œæ˜¾è‘—èŠ‚çœäº†å†…å­˜å¹¶åŠ å¿«äº†æ¨ç†é€Ÿåº¦ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.21198', 'title': 'Learning from Trials and Errors: Reflective Test-Time Planning for Embodied LLMs', 'url': 'https://huggingface.co/papers/2602.21198', 'abstract': 'Abstract Reflective Test-Time Planning enhances robot decision-making by integrating multiple reflection mechanisms that enable learning from experience and improving long-horizon task performance.  \t\t\t\t\tAI-generated summary Embodied LLMs endow robots with high-level task reasoning, but they cannot reflect on what went wrong or why, turning deployment into a sequence of independent trials where mistakes repeat rather than accumulate into experience. Drawing upon human reflective practitioners, we introduce Reflective Test-Time Planning, which integrates two modes of reflection: reflection-in-action, where the agent uses test-time scaling to generate and score multiple candidate actions using internal reflections before execution; and reflection-on-action, which uses test-time training to update both its internal reflection model and its action policy based on external reflections after execution. We also include retrospective reflection, allowing the agent to re-evaluate earlier decisions and perform model updates with hindsight for proper long-horizon credit assignment. Experiments on our newly-designed Long-Horizon Household benchmark and MuJoCo Cupboard Fitting benchmark show significant gains over baseline models, with ablative studies validating the complementary roles of reflection-in-action and reflection-on-action. Qualitative analyses, including real-robot trials, highlight behavioral correction through reflection.', 'score': 3, 'issue_id': 1212, 'pub_date': '2026-02-24', 'pub_date_card': {'ru': '24 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 24', 'zh': '2æœˆ24æ—¥'}, 'hash': '6a95fb8f0218e443', 'authors': ['Yining Hong', 'Huang Huang', 'Manling Li', 'Li Fei-Fei', 'Jiajun Wu', 'Yejin Choi'], 'affiliations': ['Northwestern University', 'Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2602.21198.jpg', 'data': {'categories': ['#agents', '#robotics', '#dataset', '#benchmark'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ° Ñ‡ĞµÑ€ĞµĞ· Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ñ: ĞºĞ°Ğº Ğ½Ğ°ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ¼Ğ°ÑˆĞ¸Ğ½Ñƒ ÑƒÑ‡Ğ¸Ñ‚ÑŒÑÑ Ğ½Ğ° Ğ¾ÑˆĞ¸Ğ±ĞºĞ°Ñ…', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Reflective Test-Time Planning, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ° Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ² Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ¸. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ¸ Ñ‚Ğ¸Ğ¿Ğ° Ñ€Ğ°Ğ·Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ: Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ, ĞºĞ¾Ğ³Ğ´Ğ° Ğ°Ğ³ĞµĞ½Ñ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ¾Ğ² Ğ¿ĞµÑ€ĞµĞ´ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸ĞµĞ¼; Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ñ Ğ¿Ğ¾ÑĞ»Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºÑƒ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ¾Ñ‚Ğ·Ñ‹Ğ²Ğ¾Ğ²; Ğ¸ Ñ€ĞµÑ‚Ñ€Ğ¾ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ñ Ğ´Ğ»Ñ Ğ¿ĞµÑ€ĞµĞ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾ÑˆĞ»Ñ‹Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼ Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ ÑƒÑ‡Ğ¸Ñ‚ÑŒÑÑ Ğ½Ğ° ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾ÑˆĞ¸Ğ±ĞºĞ°Ñ… Ğ¸ Ğ½Ğ°ĞºĞ°Ğ¿Ğ»Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ¿Ñ‹Ñ‚ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ¾Ğ´Ğ½Ğ¸Ñ… Ğ¸ Ñ‚ĞµÑ… Ğ¶Ğµ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Long-Horizon Household Ğ¸ MuJoCo Cupboard Fitting Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²ĞºÑƒ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ñ.'}, 'en': {'title': 'Enhancing Robot Decision-Making Through Reflective Learning', 'desc': 'This paper presents Reflective Test-Time Planning, a method that improves robot decision-making by incorporating reflection mechanisms. It combines two types of reflection: reflection-in-action, which allows robots to evaluate multiple actions in real-time, and reflection-on-action, which updates their strategies based on past experiences. Additionally, it introduces retrospective reflection for better long-term decision-making by reassessing earlier choices. Experiments demonstrate that this approach significantly enhances performance in complex tasks compared to traditional methods.'}, 'zh': {'title': 'åæ€æå‡æœºå™¨äººå†³ç­–èƒ½åŠ›', 'desc': 'åæ€æµ‹è¯•æ—¶è§„åˆ’ï¼ˆReflective Test-Time Planningï¼‰é€šè¿‡æ•´åˆå¤šç§åæ€æœºåˆ¶ï¼Œå¢å¼ºäº†æœºå™¨äººå†³ç­–èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿä»ç»éªŒä¸­å­¦ä¹ å¹¶æé«˜é•¿æ—¶é—´ä»»åŠ¡çš„è¡¨ç°ã€‚è¯¥æ–¹æ³•åŒ…æ‹¬ä¸¤ç§åæ€æ¨¡å¼ï¼šè¡ŒåŠ¨ä¸­çš„åæ€ï¼ˆreflection-in-actionï¼‰ï¼Œåœ¨æ‰§è¡Œå‰ç”Ÿæˆå’Œè¯„åˆ†å¤šä¸ªå€™é€‰åŠ¨ä½œï¼›ä»¥åŠè¡ŒåŠ¨åçš„åæ€ï¼ˆreflection-on-actionï¼‰ï¼Œåœ¨æ‰§è¡Œåæ›´æ–°å†…éƒ¨åæ€æ¨¡å‹å’Œè¡ŒåŠ¨ç­–ç•¥ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†å›é¡¾æ€§åæ€ï¼Œå…è®¸ä»£ç†é‡æ–°è¯„ä¼°æ—©æœŸå†³ç­–å¹¶è¿›è¡Œæ¨¡å‹æ›´æ–°ï¼Œä»¥ä¾¿æ›´å¥½åœ°åˆ†é…é•¿æœŸä¿¡ç”¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ–°è®¾è®¡çš„é•¿æ—¶é—´å®¶åº­åŸºå‡†å’ŒMuJoCoæ©±æŸœæ‹ŸåˆåŸºå‡†ä¸Šæ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.21185', 'title': 'The Diffusion Duality, Chapter II: Î¨-Samplers and Efficient Curriculum', 'url': 'https://huggingface.co/papers/2602.21185', 'abstract': "Abstract Discrete diffusion models with predictor-corrector samplers surpass traditional methods in generation quality and efficiency, challenging assumptions about masked diffusion's necessity in language modeling.  \t\t\t\t\tAI-generated summary Uniform-state discrete diffusion models excel at few-step generation and guidance due to their ability to self-correct, making them preferred over autoregressive or Masked diffusion models in these settings. However, their sampling quality plateaus with ancestral samplers as the number of steps increases. We introduce a family of Predictor-Corrector (PC) samplers for discrete diffusion that generalize prior methods and apply to arbitrary noise processes. When paired with uniform-state diffusion, our samplers outperform ancestral sampling on both language and image modeling, achieving lower generative perplexity at matched unigram entropy on OpenWebText and better FID/IS scores on CIFAR10. Crucially, unlike conventional samplers, our PC methods continue to improve with more sampling steps. Taken together, these findings call into question the assumption that Masked diffusion is the inevitable future of diffusion-based language modeling. Beyond sampling, we develop a memory-efficient curriculum for the Gaussian relaxation training phase, reducing training time by 25% and memory by 33% compared to Duo while maintaining comparable perplexity on OpenWebText and LM1B and strong downstream performance. We release code, checkpoints, and a video-tutorial on: https://s-sahoo.com/duo-ch2", 'score': 3, 'issue_id': 1213, 'pub_date': '2026-02-24', 'pub_date_card': {'ru': '24 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 24', 'zh': '2æœˆ24æ—¥'}, 'hash': '4b44e83c2b923e24', 'authors': ['Justin Deschenaux', 'Caglar Gulcehre', 'Subham Sekhar Sahoo'], 'affiliations': ['Cornell Tech, NY', 'EPFL, Lausanne, Switzerland', 'Microsoft AI'], 'pdf_title_img': 'assets/pdf/title_img/2602.21185.jpg', 'data': {'categories': ['#diffusion', '#optimization', '#open_source'], 'emoji': 'ğŸ”„', 'ru': {'title': 'Predictor-Corrector ÑÑĞ¼Ğ¿Ğ»ĞµÑ€Ñ‹ Ğ´Ğ»Ñ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ñ‹ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Predictor-Corrector Ğ´Ğ»Ñ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ÑÑ Ğº Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°Ğ¼ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑˆÑƒĞ¼Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ñ€Ğ°Ğ²Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ñ‹Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ PC-ÑÑĞ¼Ğ¿Ğ»ĞµÑ€Ğ°Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ğµ Ğ¸ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºĞ°Ğº Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ñ‚Ğ°Ğº Ğ¸ Ğ¿Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ğ¿ĞµÑ€Ğ¿Ğ»ĞµĞºÑĞ¸Ğ¸ Ğ¸ FID-Ğ¾Ñ†ĞµĞ½Ğ¾Ğº. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğµ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ‚ÑŒÑÑ Ñ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ‡Ğ¸ÑĞ»Ğ° ÑˆĞ°Ğ³Ğ¾Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ² Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ². Ğ”Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ ĞºÑƒÑ€Ğ¸ĞºÑƒĞ»ÑƒĞ¼ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ğ¾Ğ¹ Ñ€ĞµĞ»Ğ°ĞºÑĞ°Ñ†Ğ¸ĞµĞ¹, ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ğ² Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° 25% Ğ¸ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ½Ğ° 33%.'}, 'en': {'title': 'Revolutionizing Diffusion Models with Predictor-Corrector Samplers', 'desc': 'This paper presents a new approach to discrete diffusion models using Predictor-Corrector (PC) samplers, which enhance the quality and efficiency of generative tasks in language and image modeling. Unlike traditional methods, these PC samplers improve performance with more sampling steps, challenging the reliance on masked diffusion techniques. The authors demonstrate that their models achieve lower generative perplexity and better FID/IS scores, indicating superior output quality. Additionally, they introduce a memory-efficient training curriculum that significantly reduces training time and memory usage while maintaining performance.'}, 'zh': {'title': 'é¢„æµ‹-æ ¡æ­£é‡‡æ ·å™¨ï¼šæ‰©æ•£æ¨¡å‹çš„æ–°æœªæ¥', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„ç¦»æ•£æ‰©æ•£æ¨¡å‹ï¼Œä½¿ç”¨é¢„æµ‹-æ ¡æ­£é‡‡æ ·å™¨æ¥æé«˜ç”Ÿæˆè´¨é‡å’Œæ•ˆç‡ã€‚è¿™ç§æ–¹æ³•æŒ‘æˆ˜äº†ä¼ ç»Ÿçš„æ©è”½æ‰©æ•£åœ¨è¯­è¨€å»ºæ¨¡ä¸­çš„å¿…è¦æ€§ï¼Œå°¤å…¶åœ¨å°‘æ­¥ç”Ÿæˆå’Œå¼•å¯¼æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚ä¸ä¼ ç»Ÿçš„ç¥–å…ˆé‡‡æ ·å™¨ç›¸æ¯”ï¼Œé¢„æµ‹-æ ¡æ­£é‡‡æ ·å™¨åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­èƒ½å¤ŸæŒç»­æ”¹è¿›ï¼Œå°¤å…¶åœ¨å¤„ç†è¯­è¨€å’Œå›¾åƒå»ºæ¨¡æ—¶æ•ˆæœæ˜¾è‘—ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§å†…å­˜é«˜æ•ˆçš„è®­ç»ƒè¯¾ç¨‹ï¼Œæ˜¾è‘—å‡å°‘äº†è®­ç»ƒæ—¶é—´å’Œå†…å­˜æ¶ˆè€—ï¼ŒåŒæ—¶ä¿æŒäº†è‰¯å¥½çš„ç”Ÿæˆæ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.20731', 'title': 'Communication-Inspired Tokenization for Structured Image Representations', 'url': 'https://huggingface.co/papers/2602.20731', 'abstract': 'Abstract COMiT framework learns structured discrete visual tokens through iterative encoding and flow-matching decoding, improving object-centric representation and compositional generalization.  \t\t\t\t\tAI-generated summary Discrete image tokenizers have emerged as a key component of modern vision and multimodal systems, providing a sequential interface for transformer-based architectures. However, most existing approaches remain primarily optimized for reconstruction and compression, often yielding tokens that capture local texture rather than object-level semantic structure. Inspired by the incremental and compositional nature of human communication, we introduce COMmunication inspired Tokenization (COMiT), a framework for learning structured discrete visual token sequences. COMiT constructs a latent message within a fixed token budget by iteratively observing localized image crops and recurrently updating its discrete representation. At each step, the model integrates new visual information while refining and reorganizing the existing token sequence. After several encoding iterations, the final message conditions a flow-matching decoder that reconstructs the full image. Both encoding and decoding are implemented within a single transformer model and trained end-to-end using a combination of flow-matching reconstruction and semantic representation alignment losses. Our experiments demonstrate that while semantic alignment provides grounding, attentive sequential tokenization is critical for inducing interpretable, object-centric token structure and substantially improving compositional generalization and relational reasoning over prior methods.', 'score': 3, 'issue_id': 1217, 'pub_date': '2026-02-24', 'pub_date_card': {'ru': '24 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 24', 'zh': '2æœˆ24æ—¥'}, 'hash': '87eee566f546d126', 'authors': ['Aram Davtyan', 'Yusuf Sahin', 'Yasaman Haghighi', 'Sebastian Stapf', 'Pablo Acuaviva', 'Alexandre Alahi', 'Paolo Favaro'], 'affiliations': ['Computer Vision Group, University of Bern, Switzerland', 'VITA Lab, EPFL, Switzerland'], 'pdf_title_img': 'assets/pdf/title_img/2602.20731.jpg', 'data': {'categories': ['#multimodal', '#interpretability', '#reasoning', '#architecture', '#cv', '#training'], 'emoji': 'ğŸ§©', 'ru': {'title': 'ĞšĞ¾Ğ¼Ğ¼ÑƒĞ½Ğ¸ĞºĞ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚-Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'COMiT - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğ¼ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹, Ğ° Ğ½Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€Ñ‹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒĞµÑ‚ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑÑ Ñ‚Ğ¾ĞºĞµĞ½-Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ flow-matching Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€ Ğ´Ğ»Ñ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. ĞĞ±Ğ° Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ñ‹ Ğ² ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¼ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğµ Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ÑÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ losses Ğ´Ğ»Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¾Ğ±ÑŠĞµĞºÑ‚-Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Enhancing Object-Centric Representation with COMiT Framework', 'desc': 'The COMiT framework introduces a novel approach to learning structured discrete visual tokens for better object-centric representation in images. It uses an iterative process of encoding localized image crops and updating a sequence of tokens, allowing the model to refine its understanding of the visual content. By integrating new information at each step, COMiT enhances the semantic structure of the tokens, which aids in compositional generalization and relational reasoning. The framework employs a flow-matching decoder to reconstruct images, demonstrating improved performance over traditional methods that focus mainly on reconstruction and compression.'}, 'zh': {'title': 'æå‡å¯¹è±¡ä¸­å¿ƒè¡¨ç¤ºçš„COMiTæ¡†æ¶', 'desc': 'COMiTæ¡†æ¶é€šè¿‡è¿­ä»£ç¼–ç å’ŒæµåŒ¹é…è§£ç å­¦ä¹ ç»“æ„åŒ–çš„ç¦»æ•£è§†è§‰æ ‡è®°ï¼Œæå‡äº†ä»¥å¯¹è±¡ä¸ºä¸­å¿ƒçš„è¡¨ç¤ºå’Œç»„åˆæ³›åŒ–èƒ½åŠ›ã€‚è¯¥æ–¹æ³•å€Ÿé‰´äº†äººç±»æ²Ÿé€šçš„å¢é‡å’Œç»„åˆç‰¹æ€§ï¼Œæ„å»ºäº†ä¸€ä¸ªå›ºå®šæ ‡è®°é¢„ç®—å†…çš„æ½œåœ¨ä¿¡æ¯ã€‚æ¨¡å‹åœ¨æ¯ä¸€æ­¥ä¸­æ•´åˆæ–°çš„è§†è§‰ä¿¡æ¯ï¼ŒåŒæ—¶ä¼˜åŒ–å’Œé‡ç»„ç°æœ‰çš„æ ‡è®°åºåˆ—ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯­ä¹‰å¯¹é½æä¾›äº†åŸºç¡€ï¼Œè€Œå…³æ³¨çš„åºåˆ—æ ‡è®°åŒ–å¯¹äºç”Ÿæˆå¯è§£é‡Šçš„å¯¹è±¡ä¸­å¿ƒæ ‡è®°ç»“æ„å’Œæ˜¾è‘—æé«˜ç»„åˆæ³›åŒ–èƒ½åŠ›è‡³å…³é‡è¦ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.21201', 'title': 'Aletheia tackles FirstProof autonomously', 'url': 'https://huggingface.co/papers/2602.21201', 'abstract': 'Abstract  We report the performance of Aletheia (Feng et al., 2026b), a mathematics research agent powered by Gemini 3 Deep Think, on the inaugural FirstProof challenge. Within the allowed timeframe of the challenge, Aletheia autonomously solved 6 problems (2, 5, 7, 8, 9, 10) out of 10 according to majority expert assessments; we note that experts were not unanimous on Problem 8 (only). For full transparency, we explain our interpretation of FirstProof and disclose details about our experiments as well as our evaluation. Raw prompts and outputs are available at https://github.com/google-deepmind/superhuman/tree/main/aletheia.', 'score': 1, 'issue_id': 1212, 'pub_date': '2026-02-24', 'pub_date_card': {'ru': '24 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 24', 'zh': '2æœˆ24æ—¥'}, 'hash': '04a1706ab5b55298', 'authors': ['Tony Feng', 'Junehyuk Jung', 'Sang-hyun Kim', 'Carlo Pagano', 'Sergei Gukov', 'Chiang-Chiang Tsai', 'David Woodruff', 'Adel Javanmard', 'Aryan Mokhtari', 'Dawsen Hwang', 'Yuri Chervonyi', 'Jonathan N. Lee', 'Garrett Bingham', 'Trieu H. Trinh', 'Vahab Mirrokni', 'Quoc V. Le', 'Thang Luong'], 'affiliations': ['Google DeepMind'], 'pdf_title_img': 'assets/pdf/title_img/2602.21201.jpg', 'data': {'categories': [], 'emoji': 'ğŸ§®', 'ru': {'title': 'ĞœĞ°ÑˆĞ¸Ğ½Ñ‹ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ´Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ñ‚ĞµĞ¾Ñ€ĞµĞ¼Ñ‹: Ğ°Ğ³ĞµĞ½Ñ‚ Aletheia Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Aletheia, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Gemini 3 Deep Think, Ğ½Ğ° ĞºĞ¾Ğ½ĞºÑƒÑ€ÑĞµ FirstProof. ĞĞ³ĞµĞ½Ñ‚ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾ Ñ€ĞµÑˆĞ¸Ğ» 6 Ğ¸Ğ· 10 Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞµ Ğ² ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğµ ÑÑ€Ğ¾ĞºĞ¸, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ñƒ Ñ‚ĞµĞ¾Ñ€ĞµĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ»Ğ¸ Ğ¿Ğ¾Ğ»Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ² Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… LLM Ñ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡.'}, 'en': {'title': 'Aletheia: Autonomous Math Problem Solver Shines in FirstProof Challenge', 'desc': "The paper discusses Aletheia, a mathematics research agent that utilizes the Gemini 3 Deep Think model. During the FirstProof challenge, Aletheia successfully solved 6 out of 10 mathematical problems, demonstrating its capability in autonomous problem-solving. The authors provide a detailed explanation of their methodology and evaluation process, ensuring transparency in their findings. Additionally, they share raw prompts and outputs to facilitate further research and understanding of Aletheia's performance."}, 'zh': {'title': 'Aletheiaï¼šæ•°å­¦ç ”ç©¶çš„æ™ºèƒ½ä»£ç†', 'desc': 'æœ¬æ–‡æŠ¥å‘Šäº†Aletheiaï¼ˆFengç­‰ï¼Œ2026bï¼‰åœ¨é¦–å±ŠFirstProofæŒ‘æˆ˜èµ›ä¸­çš„è¡¨ç°ã€‚Aletheiaæ˜¯ä¸€ä¸ªç”±Gemini 3 Deep Thinké©±åŠ¨çš„æ•°å­¦ç ”ç©¶ä»£ç†ï¼Œåœ¨è§„å®šçš„æ—¶é—´å†…è‡ªä¸»è§£å†³äº†10ä¸ªé—®é¢˜ä¸­çš„6ä¸ªã€‚æ ¹æ®å¤šæ•°ä¸“å®¶çš„è¯„ä¼°ï¼Œåªæœ‰ç¬¬8ä¸ªé—®é¢˜çš„ä¸“å®¶æ„è§ä¸ä¸€è‡´ã€‚æˆ‘ä»¬æä¾›äº†å¯¹FirstProofçš„è§£é‡Šï¼Œå¹¶æŠ«éœ²äº†å®éªŒå’Œè¯„ä¼°çš„è¯¦ç»†ä¿¡æ¯ï¼ŒåŸå§‹æç¤ºå’Œè¾“å‡ºå¯åœ¨æŒ‡å®šé“¾æ¥ä¸­æ‰¾åˆ°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.21196', 'title': 'Untied Ulysses: Memory-Efficient Context Parallelism via Headwise Chunking', 'url': 'https://huggingface.co/papers/2602.21196', 'abstract': 'Abstract UPipe enables efficient processing of long sequences in Transformer models through fine-grained chunking at the attention head level, significantly reducing activation memory usage while maintaining training speed.  \t\t\t\t\tAI-generated summary Efficiently processing long sequences with Transformer models usually requires splitting the computations across accelerators via context parallelism. The dominant approaches in this family of methods, such as Ring Attention or DeepSpeed Ulysses, enable scaling over the context dimension but do not focus on memory efficiency, which limits the sequence lengths they can support. More advanced techniques, such as Fully Pipelined Distributed Transformer or activation offloading, can further extend the possible context length at the cost of training throughput. In this paper, we present UPipe, a simple yet effective context parallelism technique that performs fine-grained chunking at the attention head level. This technique significantly reduces the activation memory usage of self-attention, breaking the activation memory barrier and unlocking much longer context lengths. Our approach reduces intermediate tensor memory usage in the attention layer by as much as 87.5% for 32B Transformers, while matching previous context parallelism techniques in terms of training speed. UPipe can support the context length of 5M tokens when training Llama3-8B on a single 8timesH100 node, improving upon prior methods by over 25%.', 'score': 1, 'issue_id': 1215, 'pub_date': '2026-02-24', 'pub_date_card': {'ru': '24 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 24', 'zh': '2æœˆ24æ—¥'}, 'hash': 'bf66e62600d95f2d', 'authors': ['Ravi Ghadia', 'Maksim Abraham', 'Sergei Vorobyov', 'Max Ryabinin'], 'affiliations': ['Together AI'], 'pdf_title_img': 'assets/pdf/title_img/2602.21196.jpg', 'data': {'categories': ['#inference', '#long_context', '#architecture', '#training', '#optimization'], 'emoji': 'âš¡', 'ru': {'title': 'ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ±Ğ¸ĞµĞ½Ğ¸Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ', 'desc': 'UPipe â€” ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»Ğ¸Ğ·Ğ¼Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ³Ğ¾Ğ»Ğ¾Ğ² Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ñ‡ĞµĞ½ÑŒ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. Ğ¢ĞµÑ…Ğ½Ğ¸ĞºĞ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¹ Ğ² ÑĞ»Ğ¾Ğµ self-attention, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼ÑƒÑ Ñ Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»Ğ¸Ğ·Ğ¼Ğ°. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ñ‚ĞµĞ½Ğ·Ğ¾Ñ€Ğ¾Ğ² Ğ½Ğ° 87.5% Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¢Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ¸Ğ· 5 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¼ ÑƒĞ·Ğ»Ğµ Ğ¸Ğ· Ğ²Ğ¾ÑÑŒĞ¼Ğ¸ GPU, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹.'}, 'en': {'title': 'Unlocking Long Sequences with UPipe Efficiency', 'desc': 'UPipe is a novel technique designed to enhance the efficiency of processing long sequences in Transformer models by implementing fine-grained chunking at the attention head level. This method significantly reduces the activation memory required during self-attention, allowing for longer context lengths without sacrificing training speed. By minimizing intermediate tensor memory usage by up to 87.5% for large models, UPipe effectively breaks the activation memory barrier. As a result, it enables the training of models with context lengths of up to 5 million tokens, outperforming existing methods by over 25%.'}, 'zh': {'title': 'UPipeï¼šçªç ´é•¿åºåˆ—å¤„ç†çš„å†…å­˜é™åˆ¶', 'desc': 'UPipeæ˜¯ä¸€ç§æ–°é¢–çš„ä¸Šä¸‹æ–‡å¹¶è¡ŒæŠ€æœ¯ï¼Œèƒ½å¤Ÿåœ¨Transformeræ¨¡å‹ä¸­é€šè¿‡åœ¨æ³¨æ„åŠ›å¤´çº§åˆ«è¿›è¡Œç»†ç²’åº¦åˆ†å—ï¼Œæœ‰æ•ˆå¤„ç†é•¿åºåˆ—ã€‚è¯¥æ–¹æ³•æ˜¾è‘—é™ä½äº†è‡ªæ³¨æ„åŠ›çš„æ¿€æ´»å†…å­˜ä½¿ç”¨ï¼Œçªç ´äº†æ¿€æ´»å†…å­˜çš„é™åˆ¶ï¼Œä»è€Œæ”¯æŒæ›´é•¿çš„ä¸Šä¸‹æ–‡é•¿åº¦ã€‚ä¸ä¹‹å‰çš„ä¸Šä¸‹æ–‡å¹¶è¡ŒæŠ€æœ¯ç›¸æ¯”ï¼ŒUPipeåœ¨è®­ç»ƒé€Ÿåº¦ä¸Šä¿æŒä¸€è‡´ï¼ŒåŒæ—¶åœ¨32Bçš„Transformerä¸­å‡å°‘äº†å¤šè¾¾87.5%çš„ä¸­é—´å¼ é‡å†…å­˜ä½¿ç”¨ã€‚é€šè¿‡åœ¨å•ä¸ª8å€H100èŠ‚ç‚¹ä¸Šè®­ç»ƒLlama3-8Bï¼ŒUPipeèƒ½å¤Ÿæ”¯æŒ5Mä¸ªæ ‡è®°çš„ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œæå‡äº†è¶…è¿‡25%çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.20945', 'title': 'The Art of Efficient Reasoning: Data, Reward, and Optimization', 'url': 'https://huggingface.co/papers/2602.20945', 'abstract': 'Abstract Large language models benefit from scaled chain-of-thought reasoning through efficient training methods that balance trajectory length and accuracy using reinforcement learning with reward shaping.  \t\t\t\t\tAI-generated summary Large Language Models (LLMs) consistently benefit from scaled Chain-of-Thought (CoT) reasoning, but also suffer from heavy computational overhead. To address this issue, efficient reasoning aims to incentivize short yet accurate thinking trajectories, typically through reward shaping with Reinforcement Learning (RL). In this paper, we systematically investigate the mechanics of efficient reasoning for LLMs. For comprehensive evaluation, we advocate for more fine-grained metrics, including length distribution conditioned on correctness and performance across a wide spectrum of token budgets ranging from 2k to 32k. First, we reveal that the training process follows a two-stage paradigm: length adaptation and reasoning refinement. After that, we conduct extensive experiments (about 0.2 million GPU hours) in a unified protocol, deconstructing training prompts and rollouts, reward shaping, and optimization strategies. In particular, a key finding is to train on relatively easier prompts, ensuring the density of positive reward signals and thus avoiding the length collapse. Meanwhile, the learned length bias can be generalized across domains. We distill all findings into valuable insights and practical guidelines, and further validate them across the Qwen3 series, ranging from 0.6B to 30B, demonstrating the robustness and generalization.', 'score': 1, 'issue_id': 1212, 'pub_date': '2026-02-24', 'pub_date_card': {'ru': '24 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 24', 'zh': '2æœˆ24æ—¥'}, 'hash': '677987c12bb77e28', 'authors': ['Taiqiang Wu', 'Zenan Zu', 'Bo Zhou', 'Ngai Wong'], 'affiliations': ['Tencent', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2602.20945.jpg', 'data': {'categories': ['#rl', '#training', '#benchmark'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ LLM Ñ‡ĞµÑ€ĞµĞ· RL', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚ÑÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (Chain-of-Thought) Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ÑĞ¾ÑÑ‚Ğ¾ÑÑ‰Ğ¸Ğ¹ Ğ¸Ğ· Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ğ¸ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ñ… ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ»Ğ¾Ğ¶Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ ĞºĞ¾Ğ»Ğ»Ğ°Ğ¿Ñ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‚ÑÑ Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ² (Ğ¾Ñ‚ 0.6B Ğ´Ğ¾ 30B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²), Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ñ€Ğ¾Ğ±ustĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°.'}, 'en': {'title': 'Efficient Reasoning for Smarter Language Models', 'desc': 'This paper explores how large language models (LLMs) can improve their reasoning abilities while reducing computational costs. It introduces efficient reasoning techniques that use reinforcement learning with reward shaping to encourage shorter, more accurate thought processes. The authors conduct extensive experiments to analyze the training dynamics, revealing a two-stage process of length adaptation followed by reasoning refinement. Their findings provide practical guidelines for training LLMs effectively, ensuring that they maintain a balance between prompt difficulty and reward density to enhance performance across various tasks.'}, 'zh': {'title': 'é«˜æ•ˆæ¨ç†ï¼šä¼˜åŒ–å¤§è¯­è¨€æ¨¡å‹çš„æ€ç»´è½¨è¿¹', 'desc': 'è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é«˜æ•ˆæ¨ç†ä¸­çš„æœºåˆ¶ï¼Œç‰¹åˆ«æ˜¯å¦‚ä½•é€šè¿‡å¼ºåŒ–å­¦ä¹ å’Œå¥–åŠ±å¡‘é€ æ¥å¹³è¡¡æ¨ç†é•¿åº¦å’Œå‡†ç¡®æ€§ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè®­ç»ƒè¿‡ç¨‹åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šé•¿åº¦é€‚åº”å’Œæ¨ç†ç²¾ç‚¼ã€‚ä½œè€…è¿›è¡Œäº†å¤§é‡å®éªŒï¼Œå‘ç°è®­ç»ƒç›¸å¯¹ç®€å•çš„æç¤ºå¯ä»¥æé«˜æ­£å¥–åŠ±ä¿¡å·çš„å¯†åº¦ï¼Œä»è€Œé¿å…æ¨ç†é•¿åº¦çš„å´©æºƒã€‚æœ€ç»ˆï¼Œç ”ç©¶ç»“æœä¸ºå¤§è¯­è¨€æ¨¡å‹çš„é«˜æ•ˆæ¨ç†æä¾›äº†å®ç”¨çš„æŒ‡å¯¼å’Œè§è§£ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.20743', 'title': 'Adaptive Text Anonymization: Learning Privacy-Utility Trade-offs via Prompt Optimization', 'url': 'https://huggingface.co/papers/2602.20743', 'abstract': 'Abstract Adaptive text anonymization framework automatically adjusts anonymization strategies based on privacy-utility requirements using prompt optimization for language models across diverse domains and constraints.  \t\t\t\t\tAI-generated summary Anonymizing textual documents is a highly context-sensitive problem: the appropriate balance between privacy protection and utility preservation varies with the data domain, privacy objectives, and downstream application. However, existing anonymization methods rely on static, manually designed strategies that lack the flexibility to adjust to diverse requirements and often fail to generalize across domains. We introduce adaptive text anonymization, a new task formulation in which anonymization strategies are automatically adapted to specific privacy-utility requirements. We propose a framework for task-specific prompt optimization that automatically constructs anonymization instructions for language models, enabling adaptation to different privacy goals, domains, and downstream usage patterns. To evaluate our approach, we present a benchmark spanning five datasets with diverse domains, privacy constraints, and utility objectives. Across all evaluated settings, our framework consistently achieves a better privacy-utility trade-off than existing baselines, while remaining computationally efficient and effective on open-source language models, with performance comparable to larger closed-source models. Additionally, we show that our method can discover novel anonymization strategies that explore different points along the privacy-utility trade-off frontier.', 'score': 1, 'issue_id': 1216, 'pub_date': '2026-02-24', 'pub_date_card': {'ru': '24 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 24', 'zh': '2æœˆ24æ—¥'}, 'hash': '8a89271a7a2880ad', 'authors': ['Gabriel Loiseau', 'Damien Sileo', 'Damien Riquet', 'Maxime Meyer', 'Marc Tommasi'], 'affiliations': ['CNRS', 'Centrale Lille', 'Hornetsecurity', 'Inria', 'Univ. Lille'], 'pdf_title_img': 'assets/pdf/title_img/2602.20743.jpg', 'data': {'categories': [], 'emoji': 'ğŸ”', 'ru': {'title': 'ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ°Ğ½Ğ¾Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ°Ğ½Ğ¾Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ğ¾Ğ´ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°ĞµÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ´ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ°Ğ½Ğ¾Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ¾Ğ¼ĞµĞ½Ğ¾Ğ² Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ Ñ Ğ¿ÑÑ‚ÑŒÑ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ğ¼Ğ¸, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¹ Ğ±Ğ°Ğ»Ğ°Ğ½Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€Ğ¸Ğ²Ğ°Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²ĞµĞ½ ĞºĞ°Ğº Ğ´Ğ»Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ´Ğ»Ñ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğ¾Ğ²Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ°Ğ½Ğ¾Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ğµ ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑĞ° Ğ¿Ñ€Ğ¸Ğ²Ğ°Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ-Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ÑÑ‚ÑŒ.'}, 'en': {'title': 'Dynamic Anonymization: Balancing Privacy and Utility', 'desc': 'This paper presents an adaptive text anonymization framework that tailors anonymization strategies based on specific privacy and utility needs. It addresses the limitations of traditional methods that use fixed strategies, which often do not perform well across different contexts. The proposed framework utilizes prompt optimization for language models, allowing it to dynamically adjust to various privacy goals and application requirements. Evaluation results demonstrate that this approach outperforms existing methods in achieving a better balance between privacy protection and utility preservation while maintaining efficiency.'}, 'zh': {'title': 'è‡ªé€‚åº”æ–‡æœ¬åŒ¿ååŒ–ï¼šéšç§ä¸æ•ˆç”¨çš„æœ€ä½³å¹³è¡¡', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªé€‚åº”æ–‡æœ¬åŒ¿ååŒ–æ¡†æ¶ï¼Œèƒ½å¤Ÿæ ¹æ®éšç§å’Œæ•ˆç”¨çš„éœ€æ±‚è‡ªåŠ¨è°ƒæ•´åŒ¿ååŒ–ç­–ç•¥ã€‚ç°æœ‰çš„åŒ¿ååŒ–æ–¹æ³•é€šå¸¸ä¾èµ–äºé™æ€çš„æ‰‹åŠ¨è®¾è®¡ç­–ç•¥ï¼Œç¼ºä¹çµæ´»æ€§ï¼Œéš¾ä»¥é€‚åº”ä¸åŒçš„æ•°æ®é¢†åŸŸå’Œåº”ç”¨åœºæ™¯ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡ä»»åŠ¡ç‰¹å®šçš„æç¤ºä¼˜åŒ–ï¼Œè‡ªåŠ¨ç”Ÿæˆé€‚åˆè¯­è¨€æ¨¡å‹çš„åŒ¿ååŒ–æŒ‡ä»¤ï¼Œä»è€Œå®ç°å¯¹ä¸åŒéšç§ç›®æ ‡å’Œé¢†åŸŸçš„é€‚åº”ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶åœ¨éšç§å’Œæ•ˆç”¨çš„æƒè¡¡ä¸Šä¼˜äºç°æœ‰åŸºçº¿ï¼ŒåŒæ—¶åœ¨è®¡ç®—æ•ˆç‡å’Œæ•ˆæœä¸Šä¸å¤§å‹é—­æºæ¨¡å‹ç›¸å½“ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.20424', 'title': "Implicit Intelligence -- Evaluating Agents on What Users Don't Say", 'url': 'https://huggingface.co/papers/2602.20424', 'abstract': 'Abstract AI agents struggle to interpret implicitly specified real-world requests that require contextual reasoning beyond explicit instructions, as demonstrated by an evaluation framework using interactive YAML-defined worlds.  \t\t\t\t\tAI-generated summary Real-world requests to AI agents are fundamentally underspecified. Natural human communication relies on shared context and unstated constraints that speakers expect listeners to infer. Current agentic benchmarks test explicit instruction-following but fail to evaluate whether agents can reason about implicit requirements spanning accessibility needs, privacy boundaries, catastrophic risks, and contextual constraints. We present Implicit Intelligence, an evaluation framework testing whether AI agents can move beyond prompt-following to become genuine goal-fulfillers, paired with Agent-as-a-World (AaW), a harness where interactive worlds are defined in human-readable YAML files and simulated by language models. Our scenarios feature apparent simplicity in user requests, hidden complexity in correct solutions, and discoverability of constraints through environmental exploration. Evaluating 16 frontier and open-weight models across 205 scenarios, we find that even the best-performing model achieves only 48.3% scenario pass rate, revealing substantial room for improvement in bridging the gap between literal instruction-following and human-like contextual reasoning.', 'score': 1, 'issue_id': 1212, 'pub_date': '2026-02-23', 'pub_date_card': {'ru': '23 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 23', 'zh': '2æœˆ23æ—¥'}, 'hash': 'ae9b77b695d3f18a', 'authors': ['Ved Sirdeshmukh', 'Marc Wetter'], 'affiliations': ['Labelbox'], 'pdf_title_img': 'assets/pdf/title_img/2602.20424.jpg', 'data': {'categories': ['#agents', '#reasoning', '#alignment', '#benchmark'], 'emoji': 'ğŸ¯', 'ru': {'title': 'ĞÑ‚ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼ Ğº ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ¼Ñƒ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ: Ğ¾Ñ†ĞµĞ½ĞºĞ° ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° AI Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ AI Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½ĞµÑĞ²Ğ½Ğ¾ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ·Ğ° Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ°Ğ¼Ğ¸ ÑĞ²Ğ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Implicit Intelligence Ğ¸ ÑÑ€ĞµĞ´Ñƒ Agent-as-a-World, Ğ³Ğ´Ğµ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼Ğ¸Ñ€Ñ‹ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑÑ‚ÑÑ Ğ² YAML-Ñ„Ğ°Ğ¹Ğ»Ğ°Ñ… Ğ¸ ÑĞ¸Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒÑÑ‚ÑÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. Ğ—Ğ°Ğ´Ğ°Ñ‡Ğ¸ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‚ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾ÑÑ‚ÑŒÑ, Ğ¿Ñ€Ğ¸Ğ²Ğ°Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ñ€Ğ¸ÑĞºĞ°Ğ¼Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ¾Ğ»Ğ¶Ğ½Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ñ‚ÑŒ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ñ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸ĞµĞ¼. ĞÑ†ĞµĞ½ĞºĞ° 16 ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° 205 ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ°, Ñ‡Ñ‚Ğ¾ Ğ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ 48.3% ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚Ğ¸, Ğ²Ñ‹ÑĞ²Ğ»ÑÑ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ±ÑƒĞºĞ²Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼ Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼, Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ñ‹Ğ¼ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¼Ñƒ.'}, 'en': {'title': 'Bridging the Gap: From Instruction-Following to Contextual Reasoning in AI', 'desc': "This paper introduces a new evaluation framework called Implicit Intelligence, which assesses AI agents' ability to understand and fulfill real-world requests that are not explicitly stated. It highlights the limitations of current benchmarks that focus solely on following direct instructions, neglecting the need for contextual reasoning. The framework uses interactive YAML-defined worlds to create scenarios where user requests appear simple but contain hidden complexities and constraints. The study reveals that even the best AI models struggle with these tasks, achieving only a 48.3% success rate, indicating a significant need for improvement in AI's contextual understanding capabilities."}, 'zh': {'title': 'è¶…è¶ŠæŒ‡ä»¤ï¼šå®ç°ç±»äººä¸Šä¸‹æ–‡æ¨ç†çš„AIä»£ç†', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†äººå·¥æ™ºèƒ½ä»£ç†åœ¨å¤„ç†éšå«è¯·æ±‚æ—¶çš„æŒ‘æˆ˜ï¼Œè¿™äº›è¯·æ±‚éœ€è¦è¶…è¶Šæ˜ç¡®æŒ‡ä»¤çš„ä¸Šä¸‹æ–‡æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºäº†éšå«æ™ºèƒ½ï¼ˆImplicit Intelligenceï¼‰è¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨æµ‹è¯•AIä»£ç†æ˜¯å¦èƒ½å¤Ÿè¶…è¶Šç®€å•çš„æŒ‡ä»¤éµå¾ªï¼Œæˆä¸ºçœŸæ­£çš„ç›®æ ‡å®ç°è€…ã€‚é€šè¿‡ä½¿ç”¨äººç±»å¯è¯»çš„YAMLæ–‡ä»¶å®šä¹‰çš„äº¤äº’å¼ä¸–ç•Œï¼Œæˆ‘ä»¬æ¨¡æ‹Ÿäº†ç”¨æˆ·è¯·æ±‚çš„è¡¨é¢ç®€å•æ€§å’Œè§£å†³æ–¹æ¡ˆçš„éšè—å¤æ‚æ€§ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œå³ä½¿æ˜¯è¡¨ç°æœ€å¥½çš„æ¨¡å‹ï¼Œå…¶åœºæ™¯é€šè¿‡ç‡ä¹Ÿä»…ä¸º48.3%ï¼Œè¡¨æ˜åœ¨å®ç°ç±»äººä¸Šä¸‹æ–‡æ¨ç†æ–¹é¢ä»æœ‰å¾ˆå¤§çš„æ”¹è¿›ç©ºé—´ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.18735', 'title': 'LaS-Comp: Zero-shot 3D Completion with Latent-Spatial Consistency', 'url': 'https://huggingface.co/papers/2602.18735', 'abstract': 'Abstract LaS-Comp presents a zero-shot 3D shape completion method using 3D foundation models with a two-stage approach for faithful reconstruction and seamless boundary refinement.  \t\t\t\t\tAI-generated summary This paper introduces LaS-Comp, a zero-shot and category-agnostic approach that leverages the rich geometric priors of 3D foundation models to enable 3D shape completion across diverse types of partial observations. Our contributions are threefold: First,  harnesses these powerful generative priors for completion through a complementary two-stage design: (i) an explicit replacement stage that preserves the partial observation geometry to ensure faithful completion; and (ii) an implicit refinement stage ensures seamless boundaries between the observed and synthesized regions. Second, our framework is training-free and compatible with different 3D foundation models. Third, we introduce Omni-Comp, a comprehensive benchmark combining real-world and synthetic data with diverse and challenging partial patterns, enabling a more thorough and realistic evaluation. Both quantitative and qualitative experiments demonstrate that our approach outperforms previous state-of-the-art approaches. Our code and data will be available at https://github.com/DavidYan2001/LaS-Comp{LaS-Comp}.', 'score': 1, 'issue_id': 1215, 'pub_date': '2026-02-21', 'pub_date_card': {'ru': '21 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 21', 'zh': '2æœˆ21æ—¥'}, 'hash': '8863ef8453658d6a', 'authors': ['Weilong Yan', 'Haipeng Li', 'Hao Xu', 'Nianjin Ye', 'Yihao Ai', 'Shuaicheng Liu', 'Jingyu Hu'], 'affiliations': ['Changhong Intelligent Robot', 'National University of Singapore', 'The Chinese University of Hong Kong', 'University of Electronic Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2602.18735.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#3d'], 'emoji': 'ğŸ§©', 'ru': {'title': 'Ğ’Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ñ„Ğ¾Ñ€Ğ¼ Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸', 'desc': 'LaS-Comp Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ñ„Ğ¾Ñ€Ğ¼ Ñ Ğ½ÑƒĞ»ĞµĞ²Ñ‹Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ 3D Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ¼. ĞĞ° Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞ²Ğ½Ğ¾ Ğ·Ğ°Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ñ‡Ğ°ÑÑ‚Ğ¸, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ĞµĞ¼Ñ‹Ñ… Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ¾Ğ², Ğ° Ğ½Ğ° Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ğ½ĞµÑĞ²Ğ½Ğ¾ ÑƒÑ‚Ğ¾Ñ‡Ğ½ÑĞµÑ‚ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑĞ¼Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ 3D Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Omni-Comp Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Revolutionizing 3D Shape Completion with LaS-Comp', 'desc': "LaS-Comp is a novel method for completing 3D shapes without needing prior training on specific categories. It uses a two-stage process: the first stage focuses on accurately replacing missing parts while maintaining the original shape's geometry, and the second stage refines the boundaries for a smooth transition between existing and newly generated areas. This approach utilizes the capabilities of 3D foundation models, making it adaptable to various types of incomplete shapes. Additionally, the paper introduces a new benchmark, Omni-Comp, to evaluate the performance of 3D shape completion methods more effectively."}, 'zh': {'title': 'æ— æ ·æœ¬3Då½¢çŠ¶è¡¥å…¨çš„æ–°æ–¹æ³•', 'desc': 'LaS-Compæ˜¯ä¸€ç§é›¶æ ·æœ¬çš„3Då½¢çŠ¶è¡¥å…¨æ–¹æ³•ï¼Œåˆ©ç”¨3DåŸºç¡€æ¨¡å‹è¿›è¡Œå‡†ç¡®é‡å»ºå’Œæ— ç¼è¾¹ç•Œç»†åŒ–ã€‚è¯¥æ–¹æ³•é‡‡ç”¨ä¸¤é˜¶æ®µè®¾è®¡ï¼Œç¬¬ä¸€é˜¶æ®µé€šè¿‡æ˜¾å¼æ›¿æ¢ä¿æŒéƒ¨åˆ†è§‚å¯Ÿå‡ ä½•å½¢çŠ¶ï¼Œç¡®ä¿è¡¥å…¨çš„çœŸå®æ€§ï¼›ç¬¬äºŒé˜¶æ®µåˆ™é€šè¿‡éšå¼ç»†åŒ–ç¡®ä¿è§‚å¯ŸåŒºåŸŸä¸åˆæˆåŒºåŸŸä¹‹é—´çš„æ— ç¼è¿æ¥ã€‚è¯¥æ¡†æ¶æ— éœ€è®­ç»ƒï¼Œå…¼å®¹ä¸åŒçš„3DåŸºç¡€æ¨¡å‹ï¼Œå¹¶å¼•å…¥äº†Omni-CompåŸºå‡†ï¼Œç»“åˆçœŸå®å’Œåˆæˆæ•°æ®ï¼Œæä¾›å¤šæ ·åŒ–çš„éƒ¨åˆ†æ¨¡å¼ä»¥è¿›è¡Œå…¨é¢è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLaS-Compåœ¨å®šé‡å’Œå®šæ€§ä¸Šå‡ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.16813', 'title': 'One-step Language Modeling via Continuous Denoising', 'url': 'https://huggingface.co/papers/2602.16813', 'abstract': 'Abstract Flow-based language models outperform discrete diffusion models in both quality and speed by using Euclidean denoising over one-hot token encodings with improved training stability through time reparameterization.  \t\t\t\t\tAI-generated summary Language models based on discrete diffusion have attracted widespread interest for their potential to provide faster generation than autoregressive models. In practice, however, they exhibit a sharp degradation of sample quality in the few-step regime, failing to realize this promise. Here we show that language models leveraging flow-based continuous denoising can outperform discrete diffusion in both quality and speed. By revisiting the fundamentals of flows over discrete modalities, we build a flow-based language model (FLM) that performs Euclidean denoising over one-hot token encodings. We show that the model can be trained by predicting the clean data via a cross entropy objective, where we introduce a simple time reparameterization that greatly improves training stability and generation quality. By distilling FLM into its associated flow map, we obtain a distilled flow map language model (FMLM) capable of few-step generation. On the LM1B and OWT language datasets, FLM attains generation quality matching state-of-the-art discrete diffusion models. With FMLM, our approach outperforms recent few-step language models across the board, with one-step generation exceeding their 8-step quality. Our work calls into question the widely held hypothesis that discrete diffusion processes are necessary for generative modeling over discrete modalities, and paves the way toward accelerated flow-based language modeling at scale. Code is available at https://github.com/david3684/flm.', 'score': 1, 'issue_id': 1215, 'pub_date': '2026-02-18', 'pub_date_card': {'ru': '18 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 18', 'zh': '2æœˆ18æ—¥'}, 'hash': '482276a862339421', 'authors': ['Chanhyuk Lee', 'Jaehoon Yoo', 'Manan Agarwal', 'Sheel Shah', 'Jerry Huang', 'Aditi Raghunathan', 'Seunghoon Hong', 'Nicholas M. Boffi', 'Jinwoo Kim'], 'affiliations': ['Organization 2', 'University 1'], 'pdf_title_img': 'assets/pdf/title_img/2602.16813.jpg', 'data': {'categories': ['#diffusion', '#architecture', '#open_source', '#training', '#optimization'], 'emoji': 'ğŸŒŠ', 'ru': {'title': 'ĞŸĞ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾Ğ±ĞµĞ´Ğ¸Ğ»Ğ¸ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½ÑƒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ñ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞ·Ñ‹ĞºĞ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ² (FLM), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½ÑƒÑ Ğ´ĞµĞ½Ğ¾Ğ¹Ğ·Ğ°Ñ†Ğ¸Ñ Ğ² ĞµĞ²ĞºĞ»Ğ¸Ğ´Ğ¾Ğ²Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ½Ğ°Ğ´ one-hot ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ½ÑƒÑ‚Ğ¾ Ñ‡ĞµÑ€ĞµĞ· Ğ¿ĞµÑ€ĞµÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ° Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ FLM Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ğº, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ Ñ Ğ¼Ğ°Ğ»Ñ‹Ğ¼ Ñ‡Ğ¸ÑĞ»Ğ¾Ğ¼ ÑˆĞ°Ğ³Ğ¾Ğ². Ğ”Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ²ĞµÑ€ÑĞ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (FMLM) ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ‚ĞµĞºÑÑ‚ Ğ·Ğ° Ğ¾Ğ´Ğ¸Ğ½ ÑˆĞ°Ğ³ Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼, Ğ¿Ñ€ĞµĞ²Ñ‹ÑˆĞ°ÑÑ‰Ğ¸Ğ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ²Ğ¾ÑÑŒĞ¼Ğ¸ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ².'}, 'en': {'title': 'Flow-based Models: Faster and Better Language Generation!', 'desc': 'This paper presents a new type of language model called the flow-based language model (FLM), which uses continuous denoising techniques to improve both the quality and speed of text generation. Unlike traditional discrete diffusion models, FLM employs Euclidean denoising over one-hot token encodings, leading to better training stability through a method called time reparameterization. The authors demonstrate that FLM can generate high-quality text even in few-step scenarios, outperforming existing models. Additionally, they introduce a distilled version of the model, the flow map language model (FMLM), which achieves superior performance in generating text quickly.'}, 'zh': {'title': 'æµæ¨¡å‹è¶…è¶Šç¦»æ•£æ‰©æ•£ï¼Œæå‡ç”Ÿæˆè´¨é‡ä¸é€Ÿåº¦', 'desc': 'è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºæµçš„è¯­è¨€æ¨¡å‹ï¼ˆFLMï¼‰ï¼Œå®ƒé€šè¿‡å¯¹ä¸€çƒ­ç¼–ç è¿›è¡Œæ¬§å‡ é‡Œå¾—å»å™ªï¼Œè¶…è¶Šäº†ç¦»æ•£æ‰©æ•£æ¨¡å‹åœ¨è´¨é‡å’Œé€Ÿåº¦ä¸Šçš„è¡¨ç°ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒFLMåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¼•å…¥äº†æ—¶é—´é‡å‚æ•°åŒ–ï¼Œæ˜¾è‘—æé«˜äº†è®­ç»ƒçš„ç¨³å®šæ€§å’Œç”Ÿæˆè´¨é‡ã€‚é€šè¿‡å°†FLMè’¸é¦ä¸ºæµæ˜ å°„è¯­è¨€æ¨¡å‹ï¼ˆFMLMï¼‰ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿåœ¨å°‘æ­¥ç”Ÿæˆä¸­è¡¨ç°å‡ºè‰²ï¼Œç”šè‡³åœ¨ä¸€æ­¥ç”Ÿæˆä¸­è¶…è¿‡äº†å…¶ä»–æ¨¡å‹çš„å…«æ­¥è´¨é‡ã€‚æ­¤ç ”ç©¶æŒ‘æˆ˜äº†ç¦»æ•£æ‰©æ•£è¿‡ç¨‹åœ¨ç”Ÿæˆå»ºæ¨¡ä¸­çš„å¿…è¦æ€§ï¼Œä¸ºå¤§è§„æ¨¡æµåŸºè¯­è¨€å»ºæ¨¡é“ºå¹³äº†é“è·¯ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.16603', 'title': 'FlowPrefill: Decoupling Preemption from Prefill Scheduling Granularity to Mitigate Head-of-Line Blocking in LLM Serving', 'url': 'https://huggingface.co/papers/2602.16603', 'abstract': 'Abstract FlowPrefill addresses head-of-line blocking in large language model serving by decoupling preemption granularity from scheduling frequency through operator-level preemption and event-driven scheduling, achieving up to 5.6x better goodput than existing systems.  \t\t\t\t\tAI-generated summary The growing demand for large language models (LLMs) requires serving systems to handle many concurrent requests with diverse service level objectives (SLOs). This exacerbates head-of-line (HoL) blocking during the compute-intensive prefill phase, where long-running requests monopolize resources and delay higher-priority ones, leading to widespread time-to-first-token (TTFT) SLO violations. While chunked prefill enables interruptibility, it introduces an inherent trade-off between responsiveness and throughput: reducing chunk size improves response latency but degrades computational efficiency, whereas increasing chunk size maximizes throughput but exacerbates blocking. This necessitates an adaptive preemption mechanism. However, dynamically balancing execution granularity against scheduling overheads remains a key challenge.   In this paper, we propose FlowPrefill, a TTFT-goodput-optimized serving system that resolves this conflict by decoupling preemption granularity from scheduling frequency. To achieve adaptive prefill scheduling, FlowPrefill introduces two key innovations: 1) Operator-Level Preemption, which leverages operator boundaries to enable fine-grained execution interruption without the efficiency loss associated with fixed small chunking; and 2) Event-Driven Scheduling, which triggers scheduling decisions only upon request arrival or completion events, thereby supporting efficient preemption responsiveness while minimizing control-plane overhead. Evaluation on real-world production traces shows that FlowPrefill improves maximum goodput by up to 5.6times compared to state-of-the-art systems while satisfying heterogeneous SLOs.', 'score': 1, 'issue_id': 1212, 'pub_date': '2026-02-18', 'pub_date_card': {'ru': '18 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 18', 'zh': '2æœˆ18æ—¥'}, 'hash': '0813b8a077b15c47', 'authors': ['Chia-chi Hsieh', 'Zan Zong', 'Xinyang Chen', 'Jianjiang Li', 'Jidong Zhai', 'Lijie Wen'], 'affiliations': ['Tsinghua University', 'University of Science and Technology Beijing'], 'pdf_title_img': 'assets/pdf/title_img/2602.16603.jpg', 'data': {'categories': ['#optimization', '#inference'], 'emoji': 'âš¡', 'ru': {'title': 'Ğ Ğ°ÑÑ†ĞµĞ¿Ğ»ĞµĞ½Ğ¸Ñ Ğ³Ñ€Ğ°Ğ½ÑƒĞ»ÑÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ñ‚ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ñ‹ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ â€” Ğ¿Ğ¾Ğ±ĞµĞ´Ğ° Ğ½Ğ°Ğ´ Ğ¾Ñ‡ĞµÑ€ĞµĞ´Ğ½Ğ¾Ğ¹ Ğ±Ğ»Ğ¾ĞºĞ¸Ñ€Ğ¾Ğ²ĞºĞ¾Ğ¹', 'desc': 'FlowPrefill â€” ÑÑ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¾Ğ±ÑĞ»ÑƒĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ±Ğ»Ğ¾ĞºĞ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ¾Ñ‡ĞµÑ€ĞµĞ´Ğ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ¿Ñ€ĞµÑ„Ğ¸Ğ»Ğ»Ğ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ÑĞµÑ‚Ğ¸ Ğ¸ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹Ğ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑĞ¿Ğ¸ÑĞ°Ğ½Ğ¸ĞµĞ¼, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ³Ğ¸Ğ±ĞºĞ¾ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ñ€ĞµĞ¼ĞµĞ½ĞµĞ¼ Ğ¾Ñ‚ĞºĞ»Ğ¸ĞºĞ° Ğ¸ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ½Ğ¾Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒÑ. Ğ”ĞµĞºĞ¾ÑƒĞ¿Ğ»Ğ¸Ğ½Ğ³ Ğ³Ñ€Ğ°Ğ½ÑƒĞ»ÑÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ñ‚ĞµÑĞ½ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ñ‹ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑĞµÑ‚ ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ Ğ±Ğ»Ğ¾ĞºĞ° Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹. Ğ’ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ½Ğ¾Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² 5.6 Ñ€Ğ°Ğ·Ğ° Ğ²Ñ‹ÑˆĞµ, Ñ‡ĞµĞ¼ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ÑƒĞ´Ğ¾Ğ²Ğ»ĞµÑ‚Ğ²Ğ¾Ñ€ÑÑ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğº ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¾Ğ±ÑĞ»ÑƒĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'Decoupling Preemption for Enhanced Language Model Serving', 'desc': 'FlowPrefill is a novel serving system designed to improve the efficiency of large language models by addressing head-of-line blocking during the prefill phase. It introduces operator-level preemption, allowing for fine-grained interruption of long-running requests without sacrificing computational efficiency. Additionally, it employs event-driven scheduling to make scheduling decisions based on request events, which enhances responsiveness while reducing overhead. The system demonstrates a significant increase in goodput, achieving up to 5.6 times better performance than existing solutions while meeting diverse service level objectives.'}, 'zh': {'title': 'FlowPrefillï¼šä¼˜åŒ–å¤§è¯­è¨€æ¨¡å‹æœåŠ¡çš„åˆ›æ–°æ–¹æ¡ˆ', 'desc': 'FlowPrefill æ˜¯ä¸€ç§ä¼˜åŒ–å¤§è¯­è¨€æ¨¡å‹æœåŠ¡çš„ç³»ç»Ÿï¼Œæ—¨åœ¨è§£å†³å¤´éƒ¨é˜»å¡é—®é¢˜ã€‚å®ƒé€šè¿‡æ“ä½œçº§åˆ«çš„æŠ¢å å’Œäº‹ä»¶é©±åŠ¨è°ƒåº¦ï¼Œå°†æŠ¢å ç²’åº¦ä¸è°ƒåº¦é¢‘ç‡è§£è€¦ï¼Œä»è€Œæé«˜äº†ç³»ç»Ÿçš„ååé‡ã€‚è¯¥ç³»ç»Ÿåœ¨å¤„ç†å¹¶å‘è¯·æ±‚æ—¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå¹³è¡¡å“åº”æ—¶é—´å’Œè®¡ç®—æ•ˆç‡ï¼Œé¿å…äº†é•¿æ—¶é—´è¯·æ±‚å¯¹é«˜ä¼˜å…ˆçº§è¯·æ±‚çš„é˜»å¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFlowPrefill çš„æœ€å¤§ååé‡æ¯”ç°æœ‰ç³»ç»Ÿæé«˜äº† 5.6 å€ï¼ŒåŒæ—¶æ»¡è¶³äº†ä¸åŒçš„æœåŠ¡æ°´å¹³ç›®æ ‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.21204', 'title': 'Test-Time Training with KV Binding Is Secretly Linear Attention', 'url': 'https://huggingface.co/papers/2602.21204', 'abstract': 'Abstract Test-time training is reinterpreted as learned linear attention rather than memorization, offering architectural simplifications and improved efficiency.  \t\t\t\t\tAI-generated summary Test-time training (TTT) with KV binding as sequence modeling layer is commonly interpreted as a form of online meta-learning that memorizes a key-value mapping at test time. However, our analysis reveals multiple phenomena that contradict this memorization-based interpretation. Motivated by these findings, we revisit the formulation of TTT and show that a broad class of TTT architectures can be expressed as a form of learned linear attention operator. Beyond explaining previously puzzling model behaviors, this perspective yields multiple practical benefits: it enables principled architectural simplifications, admits fully parallel formulations that preserve performance while improving efficiency, and provides a systematic reduction of diverse TTT variants to a standard linear attention form. Overall, our results reframe TTT not as test-time memorization, but as learned linear attention with enhanced representational capacity.', 'score': 0, 'issue_id': 1213, 'pub_date': '2026-02-24', 'pub_date_card': {'ru': '24 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 24', 'zh': '2æœˆ24æ—¥'}, 'hash': '2e3d59e73d47a15a', 'authors': ['Junchen Liu', 'Sven Elflein', 'Or Litany', 'Zan Gojcic', 'Ruilong Li'], 'affiliations': ['NVIDIA', 'Technion', 'University of Toronto', 'Vector Institute'], 'pdf_title_img': 'assets/pdf/title_img/2602.21204.jpg', 'data': {'categories': [], 'emoji': 'âš¡', 'ru': {'title': 'Test-time Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ĞºĞ°Ğº Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ñ', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿ĞµÑ€ĞµĞ¾ÑĞ¼Ñ‹ÑĞ»ÑĞµÑ‚ÑÑ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ test-time training (Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ) ĞºĞ°Ğº Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğµ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ, Ğ° Ğ½Ğµ ĞºĞ°Ğº Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¸Ğ¹ ĞºĞ»Ğ°ÑÑ TTT Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ·Ğ¸Ñ‚ÑŒ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ¿ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€ learned linear attention, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ÑŠÑÑĞ½ÑĞµÑ‚ Ñ€Ğ°Ğ½ĞµĞµ Ğ½ĞµĞ¿Ğ¾Ğ½ÑÑ‚Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­Ñ‚Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒĞ¿Ñ€Ğ¾ÑÑ‚Ğ¸Ñ‚ÑŒ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ, ÑĞ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ TTT Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ñ‹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒÑÑ‚ test-time training ĞºĞ°Ğº Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ½Ğ¾Ğ¹ Ñ€ĞµĞ¿Ñ€ĞµĞ·ĞµĞ½Ñ‚Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'Reframing Test-Time Training: From Memorization to Learned Linear Attention', 'desc': 'This paper reinterprets test-time training (TTT) as a learned linear attention mechanism instead of a memorization process. The authors argue that TTT can be better understood through the lens of sequence modeling, which reveals inconsistencies in the traditional view of TTT as merely memorizing key-value pairs. By framing TTT in this way, the paper highlights architectural simplifications and efficiency improvements that can be achieved. Ultimately, this new perspective enhances the representational capacity of TTT architectures, making them more effective and easier to implement.'}, 'zh': {'title': 'æµ‹è¯•æ—¶è®­ç»ƒï¼šå­¦ä¹ çš„çº¿æ€§æ³¨æ„åŠ›', 'desc': 'æœ¬æ–‡é‡æ–°è§£é‡Šäº†æµ‹è¯•æ—¶è®­ç»ƒï¼ˆTTTï¼‰ï¼Œå°†å…¶è§†ä¸ºä¸€ç§å­¦ä¹ çš„çº¿æ€§æ³¨æ„åŠ›ï¼Œè€Œéç®€å•çš„è®°å¿†ã€‚æˆ‘ä»¬å‘ç°TTTçš„å¤šç§ç°è±¡ä¸è®°å¿†çš„è§£é‡Šç›¸çŸ›ç›¾ï¼Œå› æ­¤é‡æ–°å®¡è§†äº†TTTçš„å…¬å¼ã€‚é€šè¿‡è¿™ç§æ–°çš„è§†è§’ï¼Œæˆ‘ä»¬èƒ½å¤Ÿç®€åŒ–æ¶æ„ï¼Œæé«˜æ•ˆç‡ï¼Œå¹¶å°†å¤šç§TTTå˜ä½“å½’çº³ä¸ºæ ‡å‡†çš„çº¿æ€§æ³¨æ„åŠ›å½¢å¼ã€‚æ€»çš„æ¥è¯´ï¼Œç ”ç©¶ç»“æœè¡¨æ˜TTTæ˜¯ä¸€ç§å…·æœ‰å¢å¼ºè¡¨ç¤ºèƒ½åŠ›çš„å­¦ä¹ çº¿æ€§æ³¨æ„åŠ›ï¼Œè€Œéä»…ä»…æ˜¯æµ‹è¯•æ—¶çš„è®°å¿†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.21053', 'title': 'OCR-Agent: Agentic OCR with Capability and Memory Reflection', 'url': 'https://huggingface.co/papers/2602.21053', 'abstract': "Abstract A novel iterative self-correction framework enhances vision-language models' reasoning robustness through capability reflection and memory reflection mechanisms, achieving superior performance on visual understanding benchmarks.  \t\t\t\t\tAI-generated summary Large Vision-Language Models (VLMs) have demonstrated significant potential on complex visual understanding tasks through iterative optimization methods.However, these models generally lack effective self-correction mechanisms, making it difficult for them to independently rectify cognitive biases. Consequently, during multi-turn revisions, they often fall into repetitive and ineffective attempts, failing to achieve stable improvements in answer quality.To address this issue, we propose a novel iterative self-correction framework that endows models with two key capabilities: Capability Reflection and Memory Reflection. This framework guides the model to first diagnose errors and generate a correction plan via Capability Reflection, then leverage Memory Reflection to review past attempts to avoid repetition and explore new solutions, and finally, optimize the answer through rigorous re-reasoning. Experiments on the challenging OCRBench v2 benchmark show that OCR-Agent outperforms the current open-source SOTA model InternVL3-8B by +2.0 on English and +1.2 on Chinese subsets, while achieving state-of-the-art results in Visual Understanding (79.9) and Reasoning (66.5) - surpassing even larger fine-tuned models. Our method demonstrates that structured, self-aware reflection can significantly enhance VLMs' reasoning robustness without additional training. Code: https://github.com/AIGeeksGroup/OCR-Agent.", 'score': 0, 'issue_id': 1214, 'pub_date': '2026-02-24', 'pub_date_card': {'ru': '24 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 24', 'zh': '2æœˆ24æ—¥'}, 'hash': '0b66e816c9fedef7', 'authors': ['Shimin Wen', 'Zeyu Zhang', 'Xingdou Bian', 'Hongjie Zhu', 'Lulu He', 'Layi Shama', 'Daji Ergu', 'Ying Cai'], 'affiliations': ['AI Geeks', 'Southwest Minzu University'], 'pdf_title_img': 'assets/pdf/title_img/2602.21053.jpg', 'data': {'categories': ['#reasoning', '#agents', '#multimodal', '#interpretability', '#open_source', '#benchmark', '#cv', '#training'], 'emoji': 'ğŸ”„', 'ru': {'title': 'Ğ¡Ğ°Ğ¼Ğ¾ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ñ: ĞºĞ°Ğº Ğ½Ğ°ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑƒÑ‡Ğ¸Ñ‚ÑŒÑÑ Ğ½Ğ° Ğ¾ÑˆĞ¸Ğ±ĞºĞ°Ñ…', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ°ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (VLM). Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²Ğ° Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ°: Ğ¾Ñ‚Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ»Ğ°Ğ½ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ğ¸, Ğ¸ Ğ¾Ñ‚Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰ĞµĞµ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑˆĞ»Ñ‹Ñ… Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚Ğ¾Ğº Ğ¸ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ OCR-Agent Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ OCRBench v2, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ InternVL3-8B Ğ½Ğ° 2.0 Ğ¿ÑƒĞ½ĞºÑ‚Ğ° Ğ½Ğ° Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ Ğ¸ Ğ½Ğ° 1.2 Ğ¿ÑƒĞ½ĞºÑ‚Ğ° Ğ½Ğ° ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¾Ğ¼. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ ÑĞ°Ğ¼Ğ¾ÑĞ¾Ğ·Ğ½Ğ°ÑÑ‰ĞµĞµ Ğ¾Ñ‚Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ VLM Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Enhancing VLMs with Self-Correction for Robust Reasoning', 'desc': 'This paper introduces an innovative iterative self-correction framework designed to improve the reasoning capabilities of vision-language models (VLMs). The framework incorporates two mechanisms: Capability Reflection, which helps the model identify and plan corrections for its errors, and Memory Reflection, which allows the model to learn from past attempts to avoid repeating mistakes. By applying these mechanisms, the model can refine its answers through a structured re-reasoning process, leading to enhanced performance on visual understanding tasks. Experimental results show that this approach significantly outperforms existing state-of-the-art models on benchmark tests, demonstrating the effectiveness of self-aware reflection in machine learning.'}, 'zh': {'title': 'è‡ªæˆ‘ä¿®æ­£ï¼Œæå‡æ¨ç†èƒ½åŠ›ï¼', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„è¿­ä»£è‡ªæˆ‘ä¿®æ­£æ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºè§†è§‰è¯­è¨€æ¨¡å‹çš„æ¨ç†é²æ£’æ€§ã€‚è¯¥æ¡†æ¶é€šè¿‡èƒ½åŠ›åæ€å’Œè®°å¿†åæ€æœºåˆ¶ï¼Œå¸®åŠ©æ¨¡å‹è¯†åˆ«é”™è¯¯å¹¶åˆ¶å®šä¿®æ­£è®¡åˆ’ï¼Œä»è€Œé¿å…é‡å¤å’Œæ¢ç´¢æ–°è§£å†³æ–¹æ¡ˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è§†è§‰ç†è§£å’Œæ¨ç†ä»»åŠ¡ä¸Šè¶…è¶Šäº†å½“å‰çš„å¼€æºæœ€ä¼˜æ¨¡å‹ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œç»“æ„åŒ–çš„è‡ªæˆ‘åæ€å¯ä»¥åœ¨ä¸å¢åŠ é¢å¤–è®­ç»ƒçš„æƒ…å†µä¸‹æ˜¾è‘—æé«˜è§†è§‰è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.21042', 'title': 'OmniOCR: Generalist OCR for Ethnic Minority Languages', 'url': 'https://huggingface.co/papers/2602.21042', 'abstract': 'Abstract OmniOCR presents a universal framework for ethnic minority scripts using Dynamic LoRA and sparsity regularization to achieve state-of-the-art accuracy with improved parameter efficiency in low-resource settings.  \t\t\t\t\tAI-generated summary Optical character recognition (OCR) has advanced rapidly with deep learning and multimodal models, yet most methods focus on well-resourced scripts such as Latin and Chinese. Ethnic minority languages remain underexplored due to complex writing systems, scarce annotations, and diverse historical and modern forms, making generalization in low-resource or zero-shot settings challenging. To address these challenges, we present OmniOCR, a universal framework for ethnic minority scripts. OmniOCR introduces Dynamic Low-Rank Adaptation (Dynamic LoRA) to allocate model capacity across layers and scripts, enabling effective adaptation while preserving knowledge.A sparsity regularization prunes redundant updates, ensuring compact and efficient adaptation without extra inference cost. Evaluations on TibetanMNIST, Shui, ancient Yi, and Dongba show that OmniOCR outperforms zero-shot foundation models and standard post training, achieving state-of-the-art accuracy with superior parameter efficiency, and compared with the state-of-the-art baseline models, it improves accuracy by 39%-66% on these four datasets. Code: https://github.com/AIGeeksGroup/OmniOCR.', 'score': 0, 'issue_id': 1214, 'pub_date': '2026-02-24', 'pub_date_card': {'ru': '24 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 24', 'zh': '2æœˆ24æ—¥'}, 'hash': 'fd07d66995672ecb', 'authors': ['Bonan Liu', 'Zeyu Zhang', 'Bingbing Meng', 'Han Wang', 'Hanshuo Zhang', 'Chengping Wang', 'Daji Ergu', 'Ying Cai'], 'affiliations': ['AI Geeks', 'Southwest Minzu University'], 'pdf_title_img': 'assets/pdf/title_img/2602.21042.jpg', 'data': {'categories': ['#multilingual', '#small_models', '#transfer_learning', '#low_resource', '#optimization', '#open_source', '#cv', '#training'], 'emoji': 'ğŸ§©', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€ĞµĞ´ĞºĞ¸Ñ… Ğ¿Ğ¸ÑÑŒĞ¼ĞµĞ½Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ', 'desc': 'OmniOCR Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ½Ğ° ÑĞ·Ñ‹ĞºĞ°Ñ… ÑÑ‚Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Dynamic LoRA Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¸ÑÑŒĞ¼ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ¿Ñ€Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞµ. ĞĞ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‚Ğ¸Ğ±ĞµÑ‚ÑĞºĞ¾Ğ³Ğ¾, ÑˆÑƒĞ¹, Ğ´Ñ€ĞµĞ²Ğ½ĞµĞ³Ğ¾ Ğ¸ ÑÑƒÑ‡Ğ¶Ğ¾Ñƒ Ğ¿Ğ¸ÑÑŒĞ¼Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²Ğ° Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸ĞµĞ¼ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 39-66% Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ². ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²ĞµĞ½ Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ½Ğ¸Ğ·ĞºĞ¸Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ğ¸ Ğ½ÑƒĞ»ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… ÑĞºÑ€Ğ¸Ğ¿Ñ‚Ğ°Ñ….'}, 'en': {'title': 'Empowering Ethnic Minority Scripts with OmniOCR', 'desc': 'OmniOCR is a new framework designed to improve optical character recognition (OCR) for ethnic minority scripts, which often lack sufficient resources and annotations. It utilizes Dynamic Low-Rank Adaptation (Dynamic LoRA) to efficiently allocate model capacity, allowing for better adaptation to various writing systems. Additionally, it employs sparsity regularization to reduce unnecessary updates, making the model more compact and efficient during inference. The framework has demonstrated significant improvements in accuracy, outperforming existing models by 39%-66% on several challenging datasets.'}, 'zh': {'title': 'OmniOCRï¼šå°‘æ•°æ°‘æ—æ–‡å­—çš„æ™ºèƒ½è¯†åˆ«æ–°æ¡†æ¶', 'desc': 'OmniOCRæ˜¯ä¸€ä¸ªé’ˆå¯¹å°‘æ•°æ°‘æ—æ–‡å­—çš„é€šç”¨æ¡†æ¶ï¼Œé‡‡ç”¨åŠ¨æ€ä½ç§©é€‚åº”ï¼ˆDynamic LoRAï¼‰å’Œç¨€ç–æ­£åˆ™åŒ–æŠ€æœ¯ï¼Œä»¥åœ¨ä½èµ„æºç¯å¢ƒä¸­å®ç°æœ€å…ˆè¿›çš„å‡†ç¡®æ€§å’Œæ›´é«˜çš„å‚æ•°æ•ˆç‡ã€‚è¯¥æ¡†æ¶è§£å†³äº†å°‘æ•°æ°‘æ—è¯­è¨€åœ¨å¤æ‚ä¹¦å†™ç³»ç»Ÿã€ç¨€ç¼ºæ³¨é‡Šå’Œå¤šæ ·åŒ–å½¢å¼ä¸‹çš„æŒ‘æˆ˜ï¼Œä½¿å¾—åœ¨ä½èµ„æºæˆ–é›¶æ ·æœ¬è®¾ç½®ä¸­å®ç°æ³›åŒ–å˜å¾—æ›´åŠ å¯è¡Œã€‚é€šè¿‡åŠ¨æ€åˆ†é…æ¨¡å‹å®¹é‡ï¼ŒOmniOCRèƒ½å¤Ÿæœ‰æ•ˆé€‚åº”ä¸åŒçš„ä¹¦å†™ç³»ç»Ÿï¼ŒåŒæ—¶ä¿æŒçŸ¥è¯†çš„å®Œæ•´æ€§ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼ŒOmniOCRåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¶…è¶Šäº†é›¶æ ·æœ¬åŸºç¡€æ¨¡å‹å’Œæ ‡å‡†åè®­ç»ƒï¼Œå‡†ç¡®æ€§æé«˜äº†39%-66%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.20903', 'title': 'TextPecker: Rewarding Structural Anomaly Quantification for Enhancing Visual Text Rendering', 'url': 'https://huggingface.co/papers/2602.20903', 'abstract': 'Abstract TextPecker enhances visual text rendering by addressing structural anomalies through a reinforcement learning approach that improves text-to-image generation quality and fidelity.  \t\t\t\t\tAI-generated summary Visual Text Rendering (VTR) remains a critical challenge in text-to-image generation, where even advanced models frequently produce text with structural anomalies such as distortion, blurriness, and misalignment. However, we find that leading MLLMs and specialist OCR models largely fail to perceive these structural anomalies, creating a critical bottleneck for both VTR evaluation and RL-based optimization. As a result, even state-of-the-art generators (e.g., SeedDream4.0, Qwen-Image) still struggle to render structurally faithful text. To address this, we propose TextPecker, a plug-and-play structural anomaly perceptive RL strategy that mitigates noisy reward signals and works with any textto-image generator. To enable this capability, we construct a recognition dataset with character-level structural-anomaly annotations and develop a stroke-editing synthesis engine to expand structural-error coverage. Experiments show that TextPecker consistently improves diverse text-to-image models; even on the well-optimized Qwen-Image, it significantly yields average gains of 4% in structural fidelity and 8.7% in semantic alignment for Chinese text rendering, establishing a new state-of-the-art in high-fidelity VTR. Our work fills a gap in VTR optimization, providing a foundational step towards reliable and structural faithful visual text generation.', 'score': 0, 'issue_id': 1215, 'pub_date': '2026-02-24', 'pub_date_card': {'ru': '24 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 24', 'zh': '2æœˆ24æ—¥'}, 'hash': '6d6e2f54057be77c', 'authors': ['Hanshen Zhu', 'Yuliang Liu', 'Xuecheng Wu', 'An-Lan Wang', 'Hao Feng', 'Dingkang Yang', 'Chao Feng', 'Can Huang', 'Jingqun Tang', 'Xiang Bai'], 'affiliations': ['ByteDance', 'Huazhong University of Science and Technology'], 'pdf_title_img': 'assets/pdf/title_img/2602.20903.jpg', 'data': {'categories': ['#cv', '#multimodal', '#dataset', '#rl'], 'emoji': 'âœï¸', 'ru': {'title': 'Ğ˜ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ñ… Ğ´ĞµÑ„ĞµĞºÑ‚Ğ¾Ğ² Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'TextPecker â€” ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ° Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ OCR ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ½Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğµ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ñ€Ğ°Ğ·Ğ¼Ñ‹Ñ‚Ğ¸Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ñ… Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ² Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ´Ğ²Ğ¸Ğ¶Ğ¾Ğº Ğ´Ğ»Ñ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ĞºÑ€Ñ‹Ñ‚Ğ¸Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ TextPecker Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'TextPecker: Elevating Text-to-Image Generation with Structural Fidelity', 'desc': 'The paper introduces TextPecker, a reinforcement learning method designed to enhance visual text rendering in text-to-image generation. It addresses common issues like distortion and misalignment that occur in AI-generated text, which are often overlooked by existing models. By creating a specialized dataset with annotations for structural anomalies and developing a synthesis engine, TextPecker improves the performance of various text-to-image generators. The results demonstrate significant improvements in both structural fidelity and semantic alignment, setting a new benchmark for high-quality visual text rendering.'}, 'zh': {'title': 'æå‡è§†è§‰æ–‡æœ¬æ¸²æŸ“çš„ç»“æ„ä¿çœŸåº¦', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºTextPeckerçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œæ—¨åœ¨æ”¹å–„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­çš„è§†è§‰æ–‡æœ¬æ¸²æŸ“è´¨é‡ã€‚å½“å‰çš„ç”Ÿæˆæ¨¡å‹å¸¸å¸¸åœ¨æ–‡æœ¬ç»“æ„ä¸Šå‡ºç°å¤±çœŸã€æ¨¡ç³Šå’Œé”™ä½ç­‰é—®é¢˜ï¼Œè€Œç°æœ‰çš„æœºå™¨å­¦ä¹ æ¨¡å‹å’Œå…‰å­¦å­—ç¬¦è¯†åˆ«ï¼ˆOCRï¼‰æ¨¡å‹éš¾ä»¥è¯†åˆ«è¿™äº›ç»“æ„å¼‚å¸¸ã€‚TextPeckeré€šè¿‡å‡å°‘å™ªå£°å¥–åŠ±ä¿¡å·ï¼Œèƒ½å¤Ÿä¸ä»»ä½•æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå™¨é…åˆä½¿ç”¨ï¼Œä»è€Œæå‡ç”Ÿæˆæ–‡æœ¬çš„ç»“æ„ä¿çœŸåº¦å’Œè¯­ä¹‰å¯¹é½åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTextPeckeråœ¨å¤šä¸ªæ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ä¸Šå‡è¡¨ç°å‡ºæ˜¾è‘—çš„æ”¹è¿›ï¼Œå°¤å…¶åœ¨ä¸­æ–‡æ–‡æœ¬æ¸²æŸ“ä¸­ï¼Œç»“æ„ä¿çœŸåº¦æé«˜äº†4%ï¼Œè¯­ä¹‰å¯¹é½åº¦æé«˜äº†8.7%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.20540', 'title': 'Generative AI and Machine Learning Collaboration for Container Dwell Time Prediction via Data Standardization', 'url': 'https://huggingface.co/papers/2602.20540', 'abstract': 'Abstract A collaborative framework integrating generative artificial intelligence with machine learning improves container dwell time prediction by standardizing unstructured text data, leading to reduced rehandling operations in container terminals.  \t\t\t\t\tAI-generated summary Import container dwell time (ICDT) prediction is a key task for improving productivity in container terminals, as accurate predictions enable the reduction of container re-handling operations by yard cranes. Achieving this objective requires accurately predicting the dwell time of individual containers. However, the primary determinants of dwell time-owner information and cargo information-are recorded as unstructured text, which limits their effective use in machine learning models. This study addresses this limitation by proposing a collaborative framework that integrates generative artificial intelligence (Gen AI) with machine learning. The proposed framework employs Gen AI to standardize unstructured information into standard international codes, with dynamic re-prediction triggered by electronic data interchange state updates, enabling the machine learning model to predict ICDT accurately. Extensive experiments conducted on real container terminal data demonstrate that the proposed methodology achieves a 13.88% improvement in mean absolute error compared to conventional models that do not utilize standardized information. Furthermore, applying the improved predictions to container stacking strategies achieves up to 14.68% reduction in the number of relocations, thereby empirically validating the potential of Gen AI to enhance productivity in container terminal operations. Overall, this study provides both technical and methodological insights into the adoption of Gen AI in port logistics and its effectiveness.', 'score': 0, 'issue_id': 1214, 'pub_date': '2026-02-24', 'pub_date_card': {'ru': '24 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 24', 'zh': '2æœˆ24æ—¥'}, 'hash': 'c4c0f293ff2650e2', 'authors': ['Minseop Kim', 'Takhyeong Kim', 'Taekhyun Park', 'Hanbyeol Park', 'Hyerim Bae'], 'affiliations': ['Graduate School of Data Science, Pusan National University, Republic of Korea', 'Major in Industrial Data Science & Engineering. Department of Industrial Engineering, Pusan National University, Republic of Korea'], 'pdf_title_img': 'assets/pdf/title_img/2602.20540.jpg', 'data': {'categories': ['#optimization'], 'emoji': 'ğŸš¢', 'ru': {'title': 'Generative AI ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞ¹Ğ½ĞµÑ€Ğ¾Ğ² Ğ² Ğ¿Ğ¾Ñ€Ñ‚Ğ°Ñ…', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ½Ğ°Ñ…Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞ¹Ğ½ĞµÑ€Ğ¾Ğ² Ğ² Ñ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ°Ğ»Ğµ, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ AI Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ“Ğ»Ğ°Ğ²Ğ½Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Gen AI Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½ĞµÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… (Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ğ²Ğ»Ğ°Ğ´ĞµĞ»ÑŒÑ†Ğµ Ğ¸ Ğ³Ñ€ÑƒĞ·Ğµ) Ğ² ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğµ Ğ¼ĞµĞ¶Ğ´ÑƒĞ½Ğ°Ñ€Ğ¾Ğ´Ğ½Ñ‹Ğµ ĞºĞ¾Ğ´Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¼Ğ¾Ğ³ÑƒÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ ML-Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑĞµÑ‚ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ² Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ½Ğ° Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ ÑÑ‚Ğ°Ñ‚ÑƒÑĞ° Ğ² ÑĞ»ĞµĞºÑ‚Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¼ Ğ¾Ğ±Ğ¼ĞµĞ½Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 13.88% Ğ¸ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿ĞµÑ€ĞµĞ½Ğ°Ğ»Ğ°Ğ´Ğ¾Ğº ĞºĞ¾Ğ½Ñ‚ĞµĞ¹Ğ½ĞµÑ€Ğ¾Ğ² Ğ½Ğ° 14.68%, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Gen AI Ğ² Ğ»Ğ¾Ğ³Ğ¸ÑÑ‚Ğ¸ĞºĞµ Ğ¿Ğ¾Ñ€Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'Enhancing Container Dwell Time Prediction with Gen AI', 'desc': 'This paper presents a collaborative framework that combines generative artificial intelligence (Gen AI) with machine learning to enhance the prediction of import container dwell time (ICDT) in container terminals. By standardizing unstructured text data related to owner and cargo information into international codes, the framework improves the accuracy of machine learning models. The integration allows for dynamic updates and re-predictions based on electronic data interchange states, leading to a significant reduction in mean absolute error. The results show that this approach not only improves prediction accuracy but also optimizes container stacking strategies, reducing the need for re-handling operations.'}, 'zh': {'title': 'ç”Ÿæˆæ€§äººå·¥æ™ºèƒ½æå‡é›†è£…ç®±åœç•™æ—¶é—´é¢„æµ‹çš„æ•ˆç‡', 'desc': 'æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§å°†ç”Ÿæˆæ€§äººå·¥æ™ºèƒ½ä¸æœºå™¨å­¦ä¹ ç›¸ç»“åˆçš„åä½œæ¡†æ¶ï¼Œä»¥æé«˜é›†è£…ç®±åœç•™æ—¶é—´é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚é€šè¿‡å°†éç»“æ„åŒ–æ–‡æœ¬æ•°æ®æ ‡å‡†åŒ–ä¸ºå›½é™…æ ‡å‡†ä»£ç ï¼Œè§£å†³äº†åœç•™æ—¶é—´é¢„æµ‹ä¸­çš„ä¿¡æ¯åˆ©ç”¨é™åˆ¶ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿæ ¹æ®ç”µå­æ•°æ®äº¤æ¢çŠ¶æ€æ›´æ–°åŠ¨æ€é‡æ–°é¢„æµ‹ï¼Œä»è€Œæé«˜äº†æ¨¡å‹çš„é¢„æµ‹èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¹³å‡ç»å¯¹è¯¯å·®ä¸Šæ¯”ä¼ ç»Ÿæ¨¡å‹æé«˜äº†13.88%ï¼Œå¹¶åœ¨é›†è£…ç®±å †æ”¾ç­–ç•¥ä¸­å‡å°‘äº†14.68%çš„æ¬è¿æ¬¡æ•°ï¼ŒéªŒè¯äº†ç”Ÿæˆæ€§äººå·¥æ™ºèƒ½åœ¨é›†è£…ç®±ç å¤´æ“ä½œä¸­çš„æ½œåŠ›ã€‚'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (2)', '#agents (7)', '#agi', '#alignment (1)', '#architecture (3)', '#audio', '#benchmark (9)', '#cv (5)', '#data (1)', '#dataset (4)', '#diffusion (2)', '#ethics', '#games', '#graphs', '#hallucinations', '#healthcare', '#inference (3)', '#interpretability (2)', '#leakage', '#long_context (3)', '#low_resource (1)', '#machine_translation', '#math', '#multilingual (1)', '#multimodal (6)', '#open_source (7)', '#optimization (8)', '#plp (1)', '#rag (1)', '#reasoning (4)', '#rl (3)', '#rlhf', '#robotics (3)', '#science', '#security', '#small_models (2)', '#story_generation', '#survey', '#synthetic (1)', '#training (9)', '#transfer_learning (1)', '#video (1)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            ğŸ”º ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'ğŸ”„ ' + getTimeDiff('2026-02-25 12:44',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ",
                    pub_date: "Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸",
                    issue_id: "Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "è¯„åˆ†",
                    pub_date: "å‘å¸ƒæ—¥æœŸ",
                    issue_id: "HFä¸Šä¼ æ—¥æœŸ"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2026-02-25 12:44')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2026-02-25 12:44')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    