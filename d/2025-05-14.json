{
    "date": {
        "ru": "14 мая",
        "en": "May 14",
        "zh": "5月14日"
    },
    "time_utc": "2025-05-14 08:15",
    "weekday": 2,
    "issue_id": 3751,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2505.07591",
            "title": "A Multi-Dimensional Constraint Framework for Evaluating and Improving\n  Instruction Following in Large Language Models",
            "url": "https://huggingface.co/papers/2505.07591",
            "abstract": "Instruction following evaluates large language models (LLMs) on their ability to generate outputs that adhere to user-defined constraints. However, existing benchmarks often rely on templated constraint prompts, which lack the diversity of real-world usage and limit fine-grained performance assessment. To fill this gap, we propose a multi-dimensional constraint framework encompassing three constraint patterns, four constraint categories, and four difficulty levels. Building on this framework, we develop an automated instruction generation pipeline that performs constraint expansion, conflict detection, and instruction rewriting, yielding 1,200 code-verifiable instruction-following test samples. We evaluate 19 LLMs across seven model families and uncover substantial variation in performance across constraint forms. For instance, average performance drops from 77.67% at Level I to 32.96% at Level IV. Furthermore, we demonstrate the utility of our approach by using it to generate data for reinforcement learning, achieving substantial gains in instruction following without degrading general performance. In-depth analysis indicates that these gains stem primarily from modifications in the model's attention modules parameters, which enhance constraint recognition and adherence. Code and data are available in https://github.com/Junjie-Ye/MulDimIF.",
            "score": 2,
            "issue_id": 3750,
            "pub_date": "2025-05-12",
            "pub_date_card": {
                "ru": "12 мая",
                "en": "May 12",
                "zh": "5月12日"
            },
            "hash": "ca7c47ccc0066e55",
            "authors": [
                "Junjie Ye",
                "Caishuang Huang",
                "Zhuohan Chen",
                "Wenjie Fu",
                "Chenyuan Yang",
                "Leyi Yang",
                "Yilong Wu",
                "Peng Wang",
                "Meng Zhou",
                "Xiaolong Yang",
                "Tao Gui",
                "Qi Zhang",
                "Zhongchao Shi",
                "Jianping Fan",
                "Xuanjing Huang"
            ],
            "affiliations": [
                "Institute of Modern Languages and Linguistics, Fudan University",
                "Lenovo Research",
                "School of Computer Science, Fudan University",
                "Tencent"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.07591.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#rl",
                    "#alignment",
                    "#optimization",
                    "#benchmark"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Многомерная оценка следования инструкциям для больших языковых моделей",
                    "desc": "Статья представляет новый подход к оценке способности больших языковых моделей (LLM) следовать инструкциям пользователей. Авторы разработали многомерную систему ограничений, включающую три паттерна, четыре категории и четыре уровня сложности. На основе этой системы был создан автоматизированный процесс генерации инструкций, который позволил создать 1200 проверяемых тестовых примеров. Исследование 19 LLM показало значительные различия в производительности в зависимости от типа ограничений, причем средняя производительность снижалась с 77,67% на уровне I до 32,96% на уровне IV."
                },
                "en": {
                    "title": "Enhancing LLMs with Multi-Dimensional Constraints",
                    "desc": "This paper evaluates large language models (LLMs) on their ability to follow user-defined constraints using a new multi-dimensional constraint framework. The framework includes various constraint patterns, categories, and difficulty levels, allowing for a more nuanced assessment of model performance. An automated instruction generation pipeline is developed to create diverse test samples, revealing significant performance variations among different LLMs when faced with increasing constraint complexity. The study also shows that using this framework for reinforcement learning can improve instruction adherence without compromising overall model performance, primarily by adjusting attention module parameters."
                },
                "zh": {
                    "title": "多维约束框架提升语言模型性能",
                    "desc": "本文探讨了如何评估大型语言模型（LLMs）在遵循用户定义约束方面的能力。现有的基准测试通常依赖于模板化的约束提示，缺乏真实世界使用的多样性，限制了细致的性能评估。为了解决这个问题，我们提出了一个多维约束框架，涵盖三种约束模式、四种约束类别和四个难度级别。通过这个框架，我们开发了一个自动化指令生成管道，生成了1200个可验证的指令遵循测试样本，并评估了19个LLM在不同约束形式下的表现。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.07215",
            "title": "Measuring General Intelligence with Generated Games",
            "url": "https://huggingface.co/papers/2505.07215",
            "abstract": "We present gg-bench, a collection of game environments designed to evaluate general reasoning capabilities in language models. Unlike most static benchmarks, gg-bench is a data generating process where new evaluation instances can be generated at will. In particular, gg-bench is synthetically generated by (1) using a large language model (LLM) to generate natural language descriptions of novel games, (2) using the LLM to implement each game in code as a Gym environment, and (3) training reinforcement learning (RL) agents via self-play on the generated games. We evaluate language models by their winrate against these RL agents by prompting models with the game description, current board state, and a list of valid moves, after which models output the moves they wish to take. gg-bench is challenging: state-of-the-art LLMs such as GPT-4o and Claude 3.7 Sonnet achieve winrates of 7-9% on gg-bench using in-context learning, while reasoning models such as o1, o3-mini and DeepSeek-R1 achieve average winrates of 31-36%. We release the generated games, data generation process, and evaluation code in order to support future modeling work and expansion of our benchmark.",
            "score": 2,
            "issue_id": 3751,
            "pub_date": "2025-05-12",
            "pub_date_card": {
                "ru": "12 мая",
                "en": "May 12",
                "zh": "5月12日"
            },
            "hash": "ed99ec3875dd9a95",
            "authors": [
                "Vivek Verma",
                "David Huang",
                "William Chen",
                "Dan Klein",
                "Nicholas Tomlin"
            ],
            "affiliations": [
                "Computer Science Division, University of California, Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.07215.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#reasoning",
                    "#rl",
                    "#games",
                    "#synthetic"
                ],
                "emoji": "🎮",
                "ru": {
                    "title": "gg-bench: Новый подход к оценке способностей языковых моделей через игровые среды",
                    "desc": "В статье представлен gg-bench - набор игровых сред для оценки способностей языковых моделей к общему рассуждению. gg-bench автоматически генерирует новые игры с помощью большой языковой модели (LLM), которая создает описания и код игр. Для оценки языковые модели играют против обученных с помощью обучения с подкреплением агентов. Результаты показывают, что современные LLM достигают низких результатов (7-9% побед), в то время как специализированные модели рассуждений достигают 31-36% побед."
                },
                "en": {
                    "title": "Dynamic Game Environments for Evaluating AI Reasoning",
                    "desc": "The paper introduces gg-bench, a novel benchmark for assessing the reasoning abilities of language models through interactive game environments. Unlike traditional benchmarks, gg-bench allows for the dynamic generation of new game instances using a large language model (LLM) to create game descriptions and implement them as Gym environments. The evaluation involves training reinforcement learning (RL) agents to compete against language models, measuring their performance based on win rates. The results show that while state-of-the-art LLMs achieve low win rates, specialized reasoning models perform significantly better, highlighting the challenges in evaluating general reasoning in AI."
                },
                "zh": {
                    "title": "gg-bench：评估语言模型推理能力的新基准",
                    "desc": "我们提出了gg-bench，这是一个用于评估语言模型一般推理能力的游戏环境集合。与大多数静态基准测试不同，gg-bench是一个数据生成过程，可以随时生成新的评估实例。具体来说，gg-bench通过使用大型语言模型生成新游戏的自然语言描述，并将每个游戏实现为Gym环境，最后通过自我对弈训练强化学习代理。我们通过模型在这些游戏中与强化学习代理的胜率来评估语言模型，发现当前最先进的语言模型在gg-bench上的胜率仅为7-9%。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.08665",
            "title": "SkillFormer: Unified Multi-View Video Understanding for Proficiency\n  Estimation",
            "url": "https://huggingface.co/papers/2505.08665",
            "abstract": "Assessing human skill levels in complex activities is a challenging problem with applications in sports, rehabilitation, and training. In this work, we present SkillFormer, a parameter-efficient architecture for unified multi-view proficiency estimation from egocentric and exocentric videos. Building on the TimeSformer backbone, SkillFormer introduces a CrossViewFusion module that fuses view-specific features using multi-head cross-attention, learnable gating, and adaptive self-calibration. We leverage Low-Rank Adaptation to fine-tune only a small subset of parameters, significantly reducing training costs. In fact, when evaluated on the EgoExo4D dataset, SkillFormer achieves state-of-the-art accuracy in multi-view settings while demonstrating remarkable computational efficiency, using 4.5x fewer parameters and requiring 3.75x fewer training epochs than prior baselines. It excels in multiple structured tasks, confirming the value of multi-view integration for fine-grained skill assessment.",
            "score": 0,
            "issue_id": 3750,
            "pub_date": "2025-05-13",
            "pub_date_card": {
                "ru": "13 мая",
                "en": "May 13",
                "zh": "5月13日"
            },
            "hash": "8cbf16dc2ec90273",
            "authors": [
                "Edoardo Bianchi",
                "Antonio Liotta"
            ],
            "affiliations": [
                "Free University of Bozen-Bolzano Bozen-Bolzano, IT"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.08665.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#dataset",
                    "#training"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "SkillFormer: эффективная оценка мастерства по мультиракурсному видео",
                    "desc": "SkillFormer - это эффективная архитектура для оценки уровня мастерства в сложных действиях по видео с нескольких ракурсов. Она использует модуль CrossViewFusion для объединения признаков из разных ракурсов с помощью кросс-внимания и адаптивной самокалибровки. Благодаря технике Low-Rank Adaptation, SkillFormer дообучает лишь небольшую часть параметров, значительно сокращая вычислительные затраты. На датасете EgoExo4D модель достигает лучшей точности при использовании в 4,5 раза меньше параметров по сравнению с аналогами."
                },
                "en": {
                    "title": "SkillFormer: Efficient Multi-View Skill Assessment",
                    "desc": "This paper introduces SkillFormer, a new machine learning model designed to assess human skill levels in complex activities using videos from different perspectives. It utilizes a CrossViewFusion module that combines features from both egocentric (first-person) and exocentric (third-person) views through advanced techniques like multi-head cross-attention. The model is built on the TimeSformer architecture and employs Low-Rank Adaptation to minimize the number of parameters that need to be trained, making it more efficient. SkillFormer achieves top accuracy on the EgoExo4D dataset while using significantly fewer resources compared to previous models, highlighting the effectiveness of integrating multiple views for skill evaluation."
                },
                "zh": {
                    "title": "SkillFormer：高效的多视角技能评估架构",
                    "desc": "评估人类在复杂活动中的技能水平是一项具有挑战性的任务，广泛应用于体育、康复和培训领域。本文提出了SkillFormer，这是一种高效的架构，能够从自我中心和外部视角的视频中统一估计技能水平。SkillFormer基于TimeSformer骨干网，引入了CrossViewFusion模块，通过多头交叉注意力、可学习的门控和自适应自校准来融合视角特征。通过低秩适应技术，我们仅微调少量参数，显著降低了训练成本，并在EgoExo4D数据集上实现了多视角设置下的最先进准确率。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.07416",
            "title": "ViMRHP: A Vietnamese Benchmark Dataset for Multimodal Review Helpfulness\n  Prediction via Human-AI Collaborative Annotation",
            "url": "https://huggingface.co/papers/2505.07416",
            "abstract": "Multimodal Review Helpfulness Prediction (MRHP) is an essential task in recommender systems, particularly in E-commerce platforms. Determining the helpfulness of user-generated reviews enhances user experience and improves consumer decision-making. However, existing datasets focus predominantly on English and Indonesian, resulting in a lack of linguistic diversity, especially for low-resource languages such as Vietnamese. In this paper, we introduce ViMRHP (Vietnamese Multimodal Review Helpfulness Prediction), a large-scale benchmark dataset for MRHP task in Vietnamese. This dataset covers four domains, including 2K products with 46K reviews. Meanwhile, a large-scale dataset requires considerable time and cost. To optimize the annotation process, we leverage AI to assist annotators in constructing the ViMRHP dataset. With AI assistance, annotation time is reduced (90 to 120 seconds per task down to 20 to 40 seconds per task) while maintaining data quality and lowering overall costs by approximately 65%. However, AI-generated annotations still have limitations in complex annotation tasks, which we further examine through a detailed performance analysis. In our experiment on ViMRHP, we evaluate baseline models on human-verified and AI-generated annotations to assess their quality differences. The ViMRHP dataset is publicly available at https://github.com/trng28/ViMRHP",
            "score": 0,
            "issue_id": 3751,
            "pub_date": "2025-05-12",
            "pub_date_card": {
                "ru": "12 мая",
                "en": "May 12",
                "zh": "5月12日"
            },
            "hash": "4f126d39b5476454",
            "authors": [
                "Truc Mai-Thanh Nguyen",
                "Dat Minh Nguyen",
                "Son T. Luu",
                "Kiet Van Nguyen"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2505.07416.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#dataset",
                    "#data",
                    "#multilingual",
                    "#low_resource",
                    "#open_source"
                ],
                "emoji": "🇻🇳",
                "ru": {
                    "title": "ИИ ускоряет создание датасета для анализа отзывов на вьетнамском языке",
                    "desc": "Статья представляет новый набор данных ViMRHP для прогнозирования полезности отзывов на вьетнамском языке. Авторы использовали искусственный интеллект для помощи аннотаторам, что значительно ускорило процесс разметки и снизило затраты. Датасет охватывает 4 домена, включая 2000 продуктов и 46000 отзывов. Исследователи провели эксперименты с базовыми моделями машинного обучения для оценки качества аннотаций, созданных человеком и ИИ."
                },
                "en": {
                    "title": "Empowering Vietnamese Reviews with AI-Driven Helpfulness Prediction",
                    "desc": "This paper presents the ViMRHP dataset, which is designed for predicting the helpfulness of reviews in Vietnamese, addressing a gap in existing datasets that mainly focus on English and Indonesian. The dataset includes 46,000 reviews across 2,000 products, making it a significant resource for multimodal review helpfulness prediction in low-resource languages. To streamline the annotation process, the authors utilize AI, significantly reducing the time required for each annotation task while also cutting costs by about 65%. The study also evaluates the performance of baseline models using both human-verified and AI-generated annotations, highlighting the strengths and limitations of AI in complex annotation tasks."
                },
                "zh": {
                    "title": "提升越南语评论有用性的智能解决方案",
                    "desc": "多模态评论有用性预测（MRHP）是推荐系统中的一个重要任务，尤其是在电子商务平台上。本文介绍了ViMRHP（越南语多模态评论有用性预测），这是一个针对越南语的MRHP任务的大规模基准数据集，涵盖了四个领域，包括2000个产品和46000条评论。为了优化注释过程，我们利用人工智能辅助注释者构建ViMRHP数据集，从而显著减少了注释时间和成本，同时保持数据质量。尽管AI生成的注释在复杂任务中仍存在局限性，但我们通过详细的性能分析进一步探讨了这些问题。"
                }
            }
        }
    ],
    "link_prev": "2025-05-13.html",
    "link_next": "2025-05-15.html",
    "link_month": "2025-05.html",
    "short_date_prev": {
        "ru": "13.05",
        "en": "05/13",
        "zh": "5月13日"
    },
    "short_date_next": {
        "ru": "15.05",
        "en": "05/15",
        "zh": "5月15日"
    },
    "categories": {
        "#dataset": 3,
        "#data": 1,
        "#benchmark": 2,
        "#agents": 0,
        "#cv": 0,
        "#rl": 2,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 0,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 2,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 2,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 1
    },
    "zh": {
        "text": "我们介绍了 Seed1.5-VL，一个用于推进通用多模态理解和推理的视觉-语言基础模型。它由一个 532M 参数的视觉编码器和一个 20B 活跃参数的混合专家（MoE）LLM 组成。尽管架构相对紧凑，但它在 38 个公共基准测试中表现出色，并在代理任务中优于 OpenAI CUA 和 Claude 3.7。Seed1.5-VL 还展示了强大的推理能力，适用于多模态推理挑战。我们希望这些能力能推动更广泛的应用。",
        "title": "Seed1.5-VL Technical Report",
        "pinyin": "我们介绍了 Seed1.5-VL，一个用于推进通用多模态理解和推理的视觉-语言基础模型。它由一个 532M 参数的视觉编码器和一个 20B 活跃参数的混合专家（MoE）LLM 组成。尽管架构相对紧凑，但它在 38 个公共基准测试中表现出色，并在代理任务中优于 OpenAI CUA 和 Claude 3.7。Seed1.5-VL 还展示了强大的推理能力，适用于多模态推理挑战。我们希望这些能力能推动更广泛的应用。\n\nWǒmen jièshào le Seed1.5-VL, yīgè yòngyú tuījìn tōngyòng duō móshì lǐjiě hé tuīlǐ de shìjué-yǔyán jīchǔ móxíng. Tā yóu yīgè 532M cānshù de shìjué biānmǎqì hé yīgè 20B huóyuè cānshù de hùnhé zhuānjiā (MoE) LLM zǔchéng. Jǐnguǎn jiàgòu xiāngduì jǐncuǒ, dàn tā zài 38 gè gōnggòng jīzhǔn cèshì zhōng biǎoxiàn chūsè, bìng zài dàilǐ rènwù zhōng yōuyú OpenAI CUA hé Claude 3.7. Seed1.5-VL hái zhǎnshì le qiángdà de tuīlǐ nénglì, shìyòng yú duō móshì tuīlǐ tiǎozhàn. Wǒmen xīwàng zhèxiē nénglì néng tuīdòng gèng guǎngfàn de yìngyòng.",
        "vocab": "[\n    {\"word\": \"介绍\", \"pinyin\": \"jiè shào\", \"trans\": \"introduce\"},\n    {\"word\": \"Seed1.5-VL\", \"pinyin\": \"Seed1.5-VL\", \"trans\": \"Seed1.5-VL\"},\n    {\"word\": \"推进\", \"pinyin\": \"tuī jìn\", \"trans\": \"promote\"},\n    {\"word\": \"通用\", \"pinyin\": \"tōng yòng\", \"trans\": \"general\"},\n    {\"word\": \"多模态\", \"pinyin\": \"duō mó shuài\", \"trans\": \"multimodal\"},\n    {\"word\": \"理解\", \"pinyin\": \"lǐ jiě\", \"trans\": \"understanding\"},\n    {\"word\": \"推理\", \"pinyin\": \"tuī lǐ\", \"trans\": \"reasoning\"},\n    {\"word\": \"视觉\", \"pinyin\": \"shì jué\", \"trans\": \"visual\"},\n    {\"word\": \"语言\", \"pinyin\": \"yǔ yán\", \"trans\": \"language\"},\n    {\"word\": \"基础模型\", \"pinyin\": \"jī chǔ mó xíng\", \"trans\": \"foundation model\"},\n    {\"word\": \"由\", \"pinyin\": \"yóu\", \"trans\": \"consist of\"},\n    {\"word\": \"参数\", \"pinyin\": \"cān shǔ\", \"trans\": \"parameters\"},\n    {\"word\": \"视觉编码器\", \"pinyin\": \"shì jué biān mǎ qì\", \"trans\": \"visual encoder\"},\n    {\"word\": \"混合专家\", \"pinyin\": \"hùn hé zhuān jiā\", \"trans\": \"mixture of experts\"},\n    {\"word\": \"LLM\", \"pinyin\": \"LLM\", \"trans\": \"LLM\"},\n    {\"word\": \"组成\", \"pinyin\": \"zǔ chéng\", \"trans\": \"composed of\"},\n    {\"word\": \"尽管\", \"pinyin\": \"jǐn guǎn\", \"trans\": \"although\"},\n    {\"word\": \"架构\", \"pinyin\": \"jià gòu\", \"trans\": \"architecture\"},\n    {\"word\": \"相对\", \"pinyin\": \"xiāng duì\", \"trans\": \"relatively\"},\n    {\"word\": \"紧凑\", \"pinyin\": \"jǐn còu\", \"trans\": \"compact\"},\n    {\"word\": \"但\", \"pinyin\": \"dàn\", \"trans\": \"but\"},\n    {\"word\": \"表现\", \"pinyin\": \"biǎo xiàn\", \"trans\": \"performance\"},\n    {\"word\": \"出色\", \"pinyin\": \"chū sè\", \"trans\": \"outstanding\"},\n    {\"word\": \"公共\", \"pinyin\": \"gōng gòng\", \"trans\": \"public\"},\n    {\"word\": \"基准测试\", \"pinyin\": \"jī zhǔn cè shì\", \"trans\": \"benchmark tests\"},\n    {\"word\": \"代理任务\", \"pinyin\": \"dài lǐ rèn wù\", \"trans\": \"proxy tasks\"},\n    {\"word\": \"优于\", \"pinyin\": \"yōu yú\", \"trans\": \"superior to\"},\n    {\"word\": \"OpenAI\", \"pinyin\": \"OpenAI\", \"trans\": \"OpenAI\"},\n    {\"word\": \"CUA\", \"pinyin\": \"CUA\", \"trans\": \"CUA\"},\n    {\"word\": \"Claude\", \"pinyin\": \"Claude\", \"trans\": \"Claude\"},\n    {\"word\": \"展示\", \"pinyin\": \"zhǎn shì\", \"trans\": \"demonstrate\"},\n    {\"word\": \"强大\", \"pinyin\": \"qiáng dà\", \"trans\": \"powerful\"},\n    {\"word\": \"适用于\", \"pinyin\": \"shì yòng yú\", \"trans\": \"applicable to\"},\n    {\"word\": \"挑战\", \"pinyin\": \"tiǎo zhàn\", \"trans\": \"challenge\"},\n    {\"word\": \"希望\", \"pinyin\": \"xī wàng\", \"trans\": \"hope\"},\n    {\"word\": \"推动\", \"pinyin\": \"tuī dòng\", \"trans\": \"drive\"},\n    {\"word\": \"广泛\", \"pinyin\": \"guǎng fàn\", \"trans\": \"widespread\"},\n    {\"word\": \"应用\", \"pinyin\": \"yìng yòng\", \"trans\": \"application\"}\n]",
        "trans": "We introduce Seed1.5-VL, a vision-language foundation model designed to advance general multimodal understanding and reasoning. It consists of a 532M parameter visual encoder and a 20B active parameter Mixture of Experts (MoE) LLM. Despite its relatively compact architecture, it performs exceptionally well on 38 public benchmarks and outperforms OpenAI CUA and Claude 3.7 in agent tasks. Seed1.5-VL also demonstrates strong reasoning capabilities, making it suitable for multimodal reasoning challenges. We hope these capabilities will drive a broader range of applications.",
        "update_ts": "2025-05-13 09:12"
    }
}