{
    "date": {
        "ru": "14 мая",
        "en": "May 14",
        "zh": "5月14日"
    },
    "time_utc": "2025-05-14 18:14",
    "weekday": 2,
    "issue_id": 3761,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2505.07916",
            "title": "MiniMax-Speech: Intrinsic Zero-Shot Text-to-Speech with a Learnable\n  Speaker Encoder",
            "url": "https://huggingface.co/papers/2505.07916",
            "abstract": "We introduce MiniMax-Speech, an autoregressive Transformer-based Text-to-Speech (TTS) model that generates high-quality speech. A key innovation is our learnable speaker encoder, which extracts timbre features from a reference audio without requiring its transcription. This enables MiniMax-Speech to produce highly expressive speech with timbre consistent with the reference in a zero-shot manner, while also supporting one-shot voice cloning with exceptionally high similarity to the reference voice. In addition, the overall quality of the synthesized audio is enhanced through the proposed Flow-VAE. Our model supports 32 languages and demonstrates excellent performance across multiple objective and subjective evaluations metrics. Notably, it achieves state-of-the-art (SOTA) results on objective voice cloning metrics (Word Error Rate and Speaker Similarity) and has secured the top position on the public TTS Arena leaderboard. Another key strength of MiniMax-Speech, granted by the robust and disentangled representations from the speaker encoder, is its extensibility without modifying the base model, enabling various applications such as: arbitrary voice emotion control via LoRA; text to voice (T2V) by synthesizing timbre features directly from text description; and professional voice cloning (PVC) by fine-tuning timbre features with additional data. We encourage readers to visit https://minimax-ai.github.io/tts_tech_report for more examples.",
            "score": 79,
            "issue_id": 3752,
            "pub_date": "2025-05-12",
            "pub_date_card": {
                "ru": "12 мая",
                "en": "May 12",
                "zh": "5月12日"
            },
            "hash": "30175415a859995c",
            "authors": [
                "Bowen Zhang",
                "Congchao Guo",
                "Geng Yang",
                "Hang Yu",
                "Haozhe Zhang",
                "Heidi Lei",
                "Jialong Mai",
                "Junjie Yan",
                "Kaiyue Yang",
                "Mingqi Yang",
                "Peikai Huang",
                "Ruiyang Jin",
                "Sitan Jiang",
                "Weihua Cheng",
                "Yawei Li",
                "Yichen Xiao",
                "Yiying Zhou",
                "Yongmao Zhang",
                "Yuan Lu",
                "Yucen He"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2505.07916.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#multilingual",
                    "#games",
                    "#audio"
                ],
                "emoji": "🗣️",
                "ru": {
                    "title": "Революция в синтезе речи: MiniMax-Speech - универсальный голосовой клон",
                    "desc": "MiniMax-Speech - это автоматическая модель синтеза речи на основе Трансформера. Ключевой инновацией является обучаемый кодировщик говорящего, который извлекает характеристики тембра из эталонного аудио без необходимости его транскрипции. Модель поддерживает 32 языка и демонстрирует отличные результаты по многим метрикам, достигая лучших показателей в клонировании голоса. MiniMax-Speech также обладает расширяемостью, позволяющей реализовать различные приложения, такие как управление эмоциями голоса и синтез тембра из текстового описания."
                },
                "en": {
                    "title": "Revolutionizing Speech Synthesis with MiniMax-Speech",
                    "desc": "MiniMax-Speech is a cutting-edge Text-to-Speech (TTS) model that utilizes an autoregressive Transformer architecture to generate high-quality speech. A significant feature of this model is its learnable speaker encoder, which captures timbre characteristics from audio samples without needing their text transcriptions. This allows the model to create expressive speech that matches the timbre of the reference audio in a zero-shot manner, and it can also perform one-shot voice cloning with remarkable accuracy. Additionally, MiniMax-Speech enhances audio quality through a Flow-VAE and supports multiple languages, achieving state-of-the-art results in voice cloning metrics and demonstrating versatility for various applications."
                },
                "zh": {
                    "title": "MiniMax-Speech：高质量语音生成的新突破",
                    "desc": "MiniMax-Speech是一种基于自回归Transformer的文本到语音（TTS）模型，能够生成高质量的语音。其创新之处在于可学习的说话人编码器，可以从参考音频中提取音色特征，而无需其转录。该模型支持零样本生成具有参考音色的表达性语音，并且在单样本语音克隆中表现出极高的相似度。此外，MiniMax-Speech在多个客观和主观评估指标上表现优异，支持32种语言，并在语音克隆的客观指标上达到了最先进的水平。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.07591",
            "title": "A Multi-Dimensional Constraint Framework for Evaluating and Improving\n  Instruction Following in Large Language Models",
            "url": "https://huggingface.co/papers/2505.07591",
            "abstract": "Instruction following evaluates large language models (LLMs) on their ability to generate outputs that adhere to user-defined constraints. However, existing benchmarks often rely on templated constraint prompts, which lack the diversity of real-world usage and limit fine-grained performance assessment. To fill this gap, we propose a multi-dimensional constraint framework encompassing three constraint patterns, four constraint categories, and four difficulty levels. Building on this framework, we develop an automated instruction generation pipeline that performs constraint expansion, conflict detection, and instruction rewriting, yielding 1,200 code-verifiable instruction-following test samples. We evaluate 19 LLMs across seven model families and uncover substantial variation in performance across constraint forms. For instance, average performance drops from 77.67% at Level I to 32.96% at Level IV. Furthermore, we demonstrate the utility of our approach by using it to generate data for reinforcement learning, achieving substantial gains in instruction following without degrading general performance. In-depth analysis indicates that these gains stem primarily from modifications in the model's attention modules parameters, which enhance constraint recognition and adherence. Code and data are available in https://github.com/Junjie-Ye/MulDimIF.",
            "score": 6,
            "issue_id": 3750,
            "pub_date": "2025-05-12",
            "pub_date_card": {
                "ru": "12 мая",
                "en": "May 12",
                "zh": "5月12日"
            },
            "hash": "ca7c47ccc0066e55",
            "authors": [
                "Junjie Ye",
                "Caishuang Huang",
                "Zhuohan Chen",
                "Wenjie Fu",
                "Chenyuan Yang",
                "Leyi Yang",
                "Yilong Wu",
                "Peng Wang",
                "Meng Zhou",
                "Xiaolong Yang",
                "Tao Gui",
                "Qi Zhang",
                "Zhongchao Shi",
                "Jianping Fan",
                "Xuanjing Huang"
            ],
            "affiliations": [
                "Institute of Modern Languages and Linguistics, Fudan University",
                "Lenovo Research",
                "School of Computer Science, Fudan University",
                "Tencent"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.07591.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#rl",
                    "#alignment",
                    "#optimization",
                    "#benchmark"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Многомерная оценка следования инструкциям для больших языковых моделей",
                    "desc": "Статья представляет новый подход к оценке способности больших языковых моделей (LLM) следовать инструкциям пользователей. Авторы разработали многомерную систему ограничений, включающую три паттерна, четыре категории и четыре уровня сложности. На основе этой системы был создан автоматизированный процесс генерации инструкций, который позволил создать 1200 проверяемых тестовых примеров. Исследование 19 LLM показало значительные различия в производительности в зависимости от типа ограничений, причем средняя производительность снижалась с 77,67% на уровне I до 32,96% на уровне IV."
                },
                "en": {
                    "title": "Enhancing LLMs with Multi-Dimensional Constraints",
                    "desc": "This paper evaluates large language models (LLMs) on their ability to follow user-defined constraints using a new multi-dimensional constraint framework. The framework includes various constraint patterns, categories, and difficulty levels, allowing for a more nuanced assessment of model performance. An automated instruction generation pipeline is developed to create diverse test samples, revealing significant performance variations among different LLMs when faced with increasing constraint complexity. The study also shows that using this framework for reinforcement learning can improve instruction adherence without compromising overall model performance, primarily by adjusting attention module parameters."
                },
                "zh": {
                    "title": "多维约束框架提升语言模型性能",
                    "desc": "本文探讨了如何评估大型语言模型（LLMs）在遵循用户定义约束方面的能力。现有的基准测试通常依赖于模板化的约束提示，缺乏真实世界使用的多样性，限制了细致的性能评估。为了解决这个问题，我们提出了一个多维约束框架，涵盖三种约束模式、四种约束类别和四个难度级别。通过这个框架，我们开发了一个自动化指令生成管道，生成了1200个可验证的指令遵循测试样本，并评估了19个LLM在不同约束形式下的表现。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.07215",
            "title": "Measuring General Intelligence with Generated Games",
            "url": "https://huggingface.co/papers/2505.07215",
            "abstract": "We present gg-bench, a collection of game environments designed to evaluate general reasoning capabilities in language models. Unlike most static benchmarks, gg-bench is a data generating process where new evaluation instances can be generated at will. In particular, gg-bench is synthetically generated by (1) using a large language model (LLM) to generate natural language descriptions of novel games, (2) using the LLM to implement each game in code as a Gym environment, and (3) training reinforcement learning (RL) agents via self-play on the generated games. We evaluate language models by their winrate against these RL agents by prompting models with the game description, current board state, and a list of valid moves, after which models output the moves they wish to take. gg-bench is challenging: state-of-the-art LLMs such as GPT-4o and Claude 3.7 Sonnet achieve winrates of 7-9% on gg-bench using in-context learning, while reasoning models such as o1, o3-mini and DeepSeek-R1 achieve average winrates of 31-36%. We release the generated games, data generation process, and evaluation code in order to support future modeling work and expansion of our benchmark.",
            "score": 5,
            "issue_id": 3751,
            "pub_date": "2025-05-12",
            "pub_date_card": {
                "ru": "12 мая",
                "en": "May 12",
                "zh": "5月12日"
            },
            "hash": "ed99ec3875dd9a95",
            "authors": [
                "Vivek Verma",
                "David Huang",
                "William Chen",
                "Dan Klein",
                "Nicholas Tomlin"
            ],
            "affiliations": [
                "Computer Science Division, University of California, Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.07215.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#reasoning",
                    "#rl",
                    "#games",
                    "#synthetic"
                ],
                "emoji": "🎮",
                "ru": {
                    "title": "gg-bench: Новый подход к оценке способностей языковых моделей через игровые среды",
                    "desc": "В статье представлен gg-bench - набор игровых сред для оценки способностей языковых моделей к общему рассуждению. gg-bench автоматически генерирует новые игры с помощью большой языковой модели (LLM), которая создает описания и код игр. Для оценки языковые модели играют против обученных с помощью обучения с подкреплением агентов. Результаты показывают, что современные LLM достигают низких результатов (7-9% побед), в то время как специализированные модели рассуждений достигают 31-36% побед."
                },
                "en": {
                    "title": "Dynamic Game Environments for Evaluating AI Reasoning",
                    "desc": "The paper introduces gg-bench, a novel benchmark for assessing the reasoning abilities of language models through interactive game environments. Unlike traditional benchmarks, gg-bench allows for the dynamic generation of new game instances using a large language model (LLM) to create game descriptions and implement them as Gym environments. The evaluation involves training reinforcement learning (RL) agents to compete against language models, measuring their performance based on win rates. The results show that while state-of-the-art LLMs achieve low win rates, specialized reasoning models perform significantly better, highlighting the challenges in evaluating general reasoning in AI."
                },
                "zh": {
                    "title": "gg-bench：评估语言模型推理能力的新基准",
                    "desc": "我们提出了gg-bench，这是一个用于评估语言模型一般推理能力的游戏环境集合。与大多数静态基准测试不同，gg-bench是一个数据生成过程，可以随时生成新的评估实例。具体来说，gg-bench通过使用大型语言模型生成新游戏的自然语言描述，并将每个游戏实现为Gym环境，最后通过自我对弈训练强化学习代理。我们通过模型在这些游戏中与强化学习代理的胜率来评估语言模型，发现当前最先进的语言模型在gg-bench上的胜率仅为7-9%。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.05464",
            "title": "Bring Reason to Vision: Understanding Perception and Reasoning through\n  Model Merging",
            "url": "https://huggingface.co/papers/2505.05464",
            "abstract": "Vision-Language Models (VLMs) combine visual perception with the general capabilities, such as reasoning, of Large Language Models (LLMs). However, the mechanisms by which these two abilities can be combined and contribute remain poorly understood. In this work, we explore to compose perception and reasoning through model merging that connects parameters of different models. Unlike previous works that often focus on merging models of the same kind, we propose merging models across modalities, enabling the incorporation of the reasoning capabilities of LLMs into VLMs. Through extensive experiments, we demonstrate that model merging offers a successful pathway to transfer reasoning abilities from LLMs to VLMs in a training-free manner. Moreover, we utilize the merged models to understand the internal mechanism of perception and reasoning and how merging affects it. We find that perception capabilities are predominantly encoded in the early layers of the model, whereas reasoning is largely facilitated by the middle-to-late layers. After merging, we observe that all layers begin to contribute to reasoning, whereas the distribution of perception abilities across layers remains largely unchanged. These observations shed light on the potential of model merging as a tool for multimodal integration and interpretation.",
            "score": 4,
            "issue_id": 3760,
            "pub_date": "2025-05-08",
            "pub_date_card": {
                "ru": "8 мая",
                "en": "May 8",
                "zh": "5月8日"
            },
            "hash": "d0d13229ec81018d",
            "authors": [
                "Shiqi Chen",
                "Jinghan Zhang",
                "Tongyao Zhu",
                "Wei Liu",
                "Siyang Gao",
                "Miao Xiong",
                "Manling Li",
                "Junxian He"
            ],
            "affiliations": [
                "City University of Hong Kong",
                "Hong Kong University of Science and Technology",
                "National University of Singapore",
                "Northwestern University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.05464.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#transfer_learning",
                    "#architecture",
                    "#reasoning",
                    "#interpretability"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Объединение зрения и мышления: новый подход к мультимодальному ИИ",
                    "desc": "Статья исследует возможность объединения моделей компьютерного зрения и языковых моделей для улучшения восприятия и рассуждения. Авторы предлагают метод слияния параметров моделей разных модальностей, что позволяет передавать способности к рассуждению от языковых моделей к визуально-языковым моделям без дополнительного обучения. Эксперименты показывают, что восприятие в основном кодируется в ранних слоях модели, а рассуждение - в средних и поздних. После слияния все слои начинают участвовать в рассуждении, в то время как распределение способностей восприятия остается неизменным."
                },
                "en": {
                    "title": "Merging Models: Bridging Vision and Reasoning",
                    "desc": "This paper investigates how to combine the visual understanding of Vision-Language Models (VLMs) with the reasoning skills of Large Language Models (LLMs) through a process called model merging. The authors propose a novel approach that merges models from different modalities, allowing VLMs to gain reasoning capabilities without the need for additional training. Their experiments reveal that while perception is mainly captured in the early layers of the model, reasoning is enhanced across all layers after merging. This study highlights the effectiveness of model merging for improving multimodal integration and understanding the interplay between perception and reasoning."
                },
                "zh": {
                    "title": "模型合并：跨模态的推理与感知结合",
                    "desc": "视觉-语言模型（VLMs）将视觉感知与大型语言模型（LLMs）的推理能力结合在一起。然而，这两种能力如何结合并相互贡献仍然不太清楚。我们通过模型合并的方法来探索感知与推理的组合，连接不同模型的参数。实验表明，模型合并能够成功地将LLMs的推理能力转移到VLMs，并且有助于理解感知与推理的内部机制。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.08638",
            "title": "TRAIL: Trace Reasoning and Agentic Issue Localization",
            "url": "https://huggingface.co/papers/2505.08638",
            "abstract": "The increasing adoption of agentic workflows across diverse domains brings a critical need to scalably and systematically evaluate the complex traces these systems generate. Current evaluation methods depend on manual, domain-specific human analysis of lengthy workflow traces - an approach that does not scale with the growing complexity and volume of agentic outputs. Error analysis in these settings is further complicated by the interplay of external tool outputs and language model reasoning, making it more challenging than traditional software debugging. In this work, we (1) articulate the need for robust and dynamic evaluation methods for agentic workflow traces, (2) introduce a formal taxonomy of error types encountered in agentic systems, and (3) present a set of 148 large human-annotated traces (TRAIL) constructed using this taxonomy and grounded in established agentic benchmarks. To ensure ecological validity, we curate traces from both single and multi-agent systems, focusing on real-world applications such as software engineering and open-world information retrieval. Our evaluations reveal that modern long context LLMs perform poorly at trace debugging, with the best Gemini-2.5-pro model scoring a mere 11% on TRAIL. Our dataset and code are made publicly available to support and accelerate future research in scalable evaluation for agentic workflows.",
            "score": 3,
            "issue_id": 3761,
            "pub_date": "2025-05-13",
            "pub_date_card": {
                "ru": "13 мая",
                "en": "May 13",
                "zh": "5月13日"
            },
            "hash": "823a40ec2bb1f793",
            "authors": [
                "Darshan Deshpande",
                "Varun Gangal",
                "Hersh Mehta",
                "Jitin Krishnan",
                "Anand Kannappan",
                "Rebecca Qian"
            ],
            "affiliations": [
                "Patronus AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.08638.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#long_context",
                    "#benchmark",
                    "#agents",
                    "#open_source"
                ],
                "emoji": "🕵️",
                "ru": {
                    "title": "Новый подход к отладке агентных систем: от таксономии ошибок к масштабируемой оценке",
                    "desc": "Статья представляет новый подход к оценке сложных рабочих процессов агентных систем. Авторы разработали таксономию типов ошибок в агентных системах и создали набор данных TRAIL из 148 аннотированных трасс. Исследование показало, что современные языковые модели с длинным контекстом плохо справляются с отладкой трасс, даже лучшая модель Gemini-2.5-pro достигла всего 11% точности на TRAIL. Датасет и код открыты для дальнейших исследований в области масштабируемой оценки агентных рабочих процессов."
                },
                "en": {
                    "title": "Revolutionizing Evaluation for Agentic Workflows",
                    "desc": "This paper addresses the challenges of evaluating agentic workflows, which are increasingly used in various fields. Current methods rely on manual analysis of complex workflow traces, making them inefficient as the volume of data grows. The authors propose a new framework that includes a taxonomy of error types specific to agentic systems and introduce a dataset of 148 annotated traces for evaluation. Their findings indicate that existing large language models struggle with debugging these traces, highlighting the need for improved evaluation techniques in this area."
                },
                "zh": {
                    "title": "智能工作流评估的新方法",
                    "desc": "随着智能工作流在各个领域的广泛应用，系统地评估这些系统生成的复杂轨迹变得至关重要。目前的评估方法依赖于人工、特定领域的分析，这种方法无法适应日益复杂和庞大的智能输出。错误分析在这些环境中变得更加复杂，因为外部工具输出与语言模型推理之间的相互作用，使得调试变得更加困难。本文提出了一种针对智能工作流轨迹的动态评估方法，并引入了一种错误类型的正式分类法，同时构建了148个大型人类标注的轨迹数据集，以支持未来在智能工作流评估方面的研究。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.08727",
            "title": "Memorization-Compression Cycles Improve Generalization",
            "url": "https://huggingface.co/papers/2505.08727",
            "abstract": "We prove theoretically that generalization improves not only through data scaling but also by compressing internal representations. To operationalize this insight, we introduce the Information Bottleneck Language Modeling (IBLM) objective, which reframes language modeling as a constrained optimization problem: minimizing representation entropy subject to optimal prediction performance. Empirically, we observe an emergent memorization-compression cycle during LLM pretraining, evidenced by oscillation positive/negative gradient alignment between cross-entropy and Matrix-Based Entropy (MBE), a measure of representation entropy. This pattern closely mirrors the predictive-compressive trade-off prescribed by IBLM and also parallels the biological alternation between awake learning and sleep consolidation. Motivated by this observation, we propose Gated Phase Transition (GAPT), a training algorithm that adaptively switches between memorization and compression phases. When applied to GPT-2 pretraining on FineWeb dataset, GAPT reduces MBE by 50% and improves cross-entropy by 4.8%. GAPT improves OOD generalizatino by 35% in a pretraining task on arithmetic multiplication. In a setting designed to simulate catastrophic forgetting, GAPT reduces interference by compressing and separating representations, achieving a 97% improvement in separation - paralleling the functional role of sleep consolidation.",
            "score": 2,
            "issue_id": 3760,
            "pub_date": "2025-05-13",
            "pub_date_card": {
                "ru": "13 мая",
                "en": "May 13",
                "zh": "5月13日"
            },
            "hash": "07a8c687bc82dfeb",
            "authors": [
                "Fangyuan Yu"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2505.08727.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#data",
                    "#training",
                    "#math"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Сжатие для обобщения: новый подход к обучению языковых моделей",
                    "desc": "Статья представляет новый подход к обучению языковых моделей, называемый Information Bottleneck Language Modeling (IBLM). Авторы доказывают, что улучшение обобщающей способности модели достигается не только за счет увеличения объема данных, но и путем сжатия внутренних представлений. В работе наблюдается циклическое чередование фаз запоминания и сжатия информации во время предобучения больших языковых моделей. На основе этих наблюдений предлагается алгоритм Gated Phase Transition (GAPT), который адаптивно переключается между фазами запоминания и сжатия, улучшая обобщающую способность модели и снижая эффект катастрофического забывания."
                },
                "en": {
                    "title": "Compressing Knowledge for Better Generalization",
                    "desc": "This paper demonstrates that improving generalization in machine learning models can be achieved not only by increasing the amount of data but also by compressing the internal representations of the model. The authors introduce the Information Bottleneck Language Modeling (IBLM) objective, which focuses on minimizing the complexity of representations while maintaining effective prediction capabilities. They observe a cycle of memorization and compression during the pretraining of large language models, which aligns with the principles of IBLM and reflects biological processes of learning and memory consolidation. To leverage this insight, they propose the Gated Phase Transition (GAPT) algorithm, which alternates between phases of memorization and compression, leading to significant improvements in model performance and generalization."
                },
                "zh": {
                    "title": "压缩与记忆的平衡提升模型泛化能力",
                    "desc": "本文理论上证明，模型的泛化能力不仅通过增加数据量来提高，还可以通过压缩内部表示来实现。我们提出了信息瓶颈语言建模（IBLM）目标，将语言建模重新构建为一个约束优化问题：在最优预测性能的条件下，最小化表示熵。通过实验证明，在大规模语言模型的预训练过程中，出现了记忆与压缩的循环现象，这种现象与IBLM所描述的预测-压缩权衡密切相关。基于这一观察，我们提出了门控相变（GAPT）训练算法，能够自适应地在记忆和压缩阶段之间切换，从而显著提高模型的泛化能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.08665",
            "title": "SkillFormer: Unified Multi-View Video Understanding for Proficiency\n  Estimation",
            "url": "https://huggingface.co/papers/2505.08665",
            "abstract": "Assessing human skill levels in complex activities is a challenging problem with applications in sports, rehabilitation, and training. In this work, we present SkillFormer, a parameter-efficient architecture for unified multi-view proficiency estimation from egocentric and exocentric videos. Building on the TimeSformer backbone, SkillFormer introduces a CrossViewFusion module that fuses view-specific features using multi-head cross-attention, learnable gating, and adaptive self-calibration. We leverage Low-Rank Adaptation to fine-tune only a small subset of parameters, significantly reducing training costs. In fact, when evaluated on the EgoExo4D dataset, SkillFormer achieves state-of-the-art accuracy in multi-view settings while demonstrating remarkable computational efficiency, using 4.5x fewer parameters and requiring 3.75x fewer training epochs than prior baselines. It excels in multiple structured tasks, confirming the value of multi-view integration for fine-grained skill assessment.",
            "score": 2,
            "issue_id": 3750,
            "pub_date": "2025-05-13",
            "pub_date_card": {
                "ru": "13 мая",
                "en": "May 13",
                "zh": "5月13日"
            },
            "hash": "8cbf16dc2ec90273",
            "authors": [
                "Edoardo Bianchi",
                "Antonio Liotta"
            ],
            "affiliations": [
                "Free University of Bozen-Bolzano Bozen-Bolzano, IT"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.08665.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#dataset",
                    "#training"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "SkillFormer: эффективная оценка мастерства по мультиракурсному видео",
                    "desc": "SkillFormer - это эффективная архитектура для оценки уровня мастерства в сложных действиях по видео с нескольких ракурсов. Она использует модуль CrossViewFusion для объединения признаков из разных ракурсов с помощью кросс-внимания и адаптивной самокалибровки. Благодаря технике Low-Rank Adaptation, SkillFormer дообучает лишь небольшую часть параметров, значительно сокращая вычислительные затраты. На датасете EgoExo4D модель достигает лучшей точности при использовании в 4,5 раза меньше параметров по сравнению с аналогами."
                },
                "en": {
                    "title": "SkillFormer: Efficient Multi-View Skill Assessment",
                    "desc": "This paper introduces SkillFormer, a new machine learning model designed to assess human skill levels in complex activities using videos from different perspectives. It utilizes a CrossViewFusion module that combines features from both egocentric (first-person) and exocentric (third-person) views through advanced techniques like multi-head cross-attention. The model is built on the TimeSformer architecture and employs Low-Rank Adaptation to minimize the number of parameters that need to be trained, making it more efficient. SkillFormer achieves top accuracy on the EgoExo4D dataset while using significantly fewer resources compared to previous models, highlighting the effectiveness of integrating multiple views for skill evaluation."
                },
                "zh": {
                    "title": "SkillFormer：高效的多视角技能评估架构",
                    "desc": "评估人类在复杂活动中的技能水平是一项具有挑战性的任务，广泛应用于体育、康复和培训领域。本文提出了SkillFormer，这是一种高效的架构，能够从自我中心和外部视角的视频中统一估计技能水平。SkillFormer基于TimeSformer骨干网，引入了CrossViewFusion模块，通过多头交叉注意力、可学习的门控和自适应自校准来融合视角特征。通过低秩适应技术，我们仅微调少量参数，显著降低了训练成本，并在EgoExo4D数据集上实现了多视角设置下的最先进准确率。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.08311",
            "title": "AM-Thinking-v1: Advancing the Frontier of Reasoning at 32B Scale",
            "url": "https://huggingface.co/papers/2505.08311",
            "abstract": "We present AM-Thinking-v1, a 32B dense language model that advances the frontier of reasoning, embodying the collaborative spirit of open-source innovation. Outperforming DeepSeek-R1 and rivaling leading Mixture-of-Experts (MoE) models like Qwen3-235B-A22B and Seed1.5-Thinking, AM-Thinking-v1 achieves impressive scores of 85.3 on AIME 2024, 74.4 on AIME 2025, and 70.3 on LiveCodeBench, showcasing state-of-the-art mathematical and coding capabilities among open-source models of similar scale.   Built entirely from the open-source Qwen2.5-32B base model and publicly available queries, AM-Thinking-v1 leverages a meticulously crafted post-training pipeline - combining supervised fine-tuning and reinforcement learning - to deliver exceptional reasoning capabilities. This work demonstrates that the open-source community can achieve high performance at the 32B scale, a practical sweet spot for deployment and fine-tuning. By striking a balance between top-tier performance and real-world usability, we hope AM-Thinking-v1 inspires further collaborative efforts to harness mid-scale models, pushing reasoning boundaries while keeping accessibility at the core of innovation. We have open-sourced our model on https://huggingface.co/a-m-team/AM-Thinking-v1{Hugging Face}.",
            "score": 2,
            "issue_id": 3757,
            "pub_date": "2025-05-13",
            "pub_date_card": {
                "ru": "13 мая",
                "en": "May 13",
                "zh": "5月13日"
            },
            "hash": "492d10424bc0d97d",
            "authors": [
                "Yunjie Ji",
                "Xiaoyu Tian",
                "Sitong Zhao",
                "Haotian Wang",
                "Shuaiting Chen",
                "Yiping Peng",
                "Han Zhao",
                "Xiangang Li"
            ],
            "affiliations": [
                "Beike (Ke.com)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.08311.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#dataset",
                    "#training",
                    "#open_source",
                    "#rl"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Открытая 32B-модель устанавливает новую планку в рассуждениях",
                    "desc": "AM-Thinking-v1 - это языковая модель с 32 миллиардами параметров, которая демонстрирует передовые способности к рассуждению. Модель превосходит DeepSeek-R1 и конкурирует с ведущими MoE-моделями, показывая впечатляющие результаты в математических и кодинговых тестах. AM-Thinking-v1 была создана на основе открытой модели Qwen2.5-32B с использованием тщательно разработанного процесса дообучения, включающего supervised fine-tuning и reinforcement learning. Авторы подчеркивают, что модель с 32 миллиардами параметров представляет собой практичный баланс между производительностью и удобством использования."
                },
                "en": {
                    "title": "Unlocking Reasoning with Open-Source Innovation",
                    "desc": "AM-Thinking-v1 is a 32 billion parameter dense language model that enhances reasoning capabilities, showcasing the power of open-source collaboration. It surpasses previous models like DeepSeek-R1 and competes with advanced Mixture-of-Experts models, achieving high scores on various benchmarks. The model is built on the Qwen2.5-32B base and utilizes a combination of supervised fine-tuning and reinforcement learning in its post-training process. This work highlights the potential of mid-scale models for practical applications, aiming to inspire further innovation in the open-source community."
                },
                "zh": {
                    "title": "开源创新，推理新高度",
                    "desc": "我们介绍了AM-Thinking-v1，这是一个32B的密集语言模型，推动了推理的前沿，体现了开源创新的合作精神。它在AIME 2024、AIME 2025和LiveCodeBench等基准测试中表现出色，超越了DeepSeek-R1，并与领先的专家混合模型如Qwen3-235B-A22B和Seed1.5-Thinking相媲美。AM-Thinking-v1完全基于开源的Qwen2.5-32B基础模型，结合监督微调和强化学习，展现了卓越的推理能力。我们的工作证明了开源社区可以在32B规模上实现高性能，平衡顶级性能与实际可用性，激励更多合作努力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.21475",
            "title": "Advancing Arabic Reverse Dictionary Systems: A Transformer-Based\n  Approach with Dataset Construction Guidelines",
            "url": "https://huggingface.co/papers/2504.21475",
            "abstract": "This study addresses the critical gap in Arabic natural language processing by developing an effective Arabic Reverse Dictionary (RD) system that enables users to find words based on their descriptions or meanings. We present a novel transformer-based approach with a semi-encoder neural network architecture featuring geometrically decreasing layers that achieves state-of-the-art results for Arabic RD tasks. Our methodology incorporates a comprehensive dataset construction process and establishes formal quality standards for Arabic lexicographic definitions. Experiments with various pre-trained models demonstrate that Arabic-specific models significantly outperform general multilingual embeddings, with ARBERTv2 achieving the best ranking score (0.0644). Additionally, we provide a formal abstraction of the reverse dictionary task that enhances theoretical understanding and develop a modular, extensible Python library (RDTL) with configurable training pipelines. Our analysis of dataset quality reveals important insights for improving Arabic definition construction, leading to eight specific standards for building high-quality reverse dictionary resources. This work contributes significantly to Arabic computational linguistics and provides valuable tools for language learning, academic writing, and professional communication in Arabic.",
            "score": 2,
            "issue_id": 3756,
            "pub_date": "2025-04-30",
            "pub_date_card": {
                "ru": "30 апреля",
                "en": "April 30",
                "zh": "4月30日"
            },
            "hash": "86e4cc7ad4a84ece",
            "authors": [
                "Serry Sibaee",
                "Samar Ahmed",
                "Abdullah Al Harbi",
                "Omer Nacar",
                "Adel Ammar",
                "Yasser Habashi",
                "Wadii Boulila"
            ],
            "affiliations": [
                "College of Computer & Information Sciences, Prince Sultan University, Riyadh, Saudi Arabia",
                "Faculty of Computing and Information Technology, King Abdulaziz University, Jeddah, Saudi Arabia",
                "Independent Researcher, Riyadh, Saudi Arabia"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.21475.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#data",
                    "#machine_translation",
                    "#dataset",
                    "#architecture",
                    "#multilingual",
                    "#low_resource"
                ],
                "emoji": "📚",
                "ru": {
                    "title": "Революция в арабской лингвистике: трансформеры на службе обратного словаря",
                    "desc": "Исследование посвящено разработке эффективной системы обратного словаря для арабского языка, позволяющей находить слова по их описаниям или значениям. Авторы представляют новый подход на основе трансформеров с полуэнкодерной нейросетевой архитектурой, достигающий наилучших результатов в задачах арабского обратного словаря. Методология включает создание комплексного датасета и установление формальных стандартов качества для арабских лексикографических определений. Эксперименты показали, что арабоязычные предобученные модели значительно превосходят многоязычные эмбеддинги, причем ARBERTv2 достигает наилучшего рейтингового показателя."
                },
                "en": {
                    "title": "Bridging the Gap in Arabic Language Processing with a Reverse Dictionary",
                    "desc": "This paper presents a new system for Arabic natural language processing called the Arabic Reverse Dictionary (RD), which helps users find words based on their meanings. The authors introduce a transformer-based model with a unique semi-encoder architecture that improves performance on Arabic RD tasks. They also create a detailed dataset and establish quality standards for Arabic definitions, showing that specialized Arabic models outperform general multilingual ones. Additionally, the study offers a theoretical framework for reverse dictionaries and introduces a Python library for easy implementation and training."
                },
                "zh": {
                    "title": "阿拉伯语反向词典：提升语言处理的利器",
                    "desc": "本研究解决了阿拉伯语自然语言处理中的关键问题，开发了一种有效的阿拉伯语反向词典系统，用户可以根据描述或含义找到单词。我们提出了一种新颖的基于变换器的半编码神经网络架构，具有几何递减层，达到了阿拉伯语反向词典任务的最先进结果。我们的研究方法包括全面的数据集构建过程，并建立了阿拉伯语词典定义的正式质量标准。实验表明，专门针对阿拉伯语的模型显著优于通用多语言嵌入，ARBERTv2模型获得了最佳排名分数。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.08751",
            "title": "Aya Vision: Advancing the Frontier of Multilingual Multimodality",
            "url": "https://huggingface.co/papers/2505.08751",
            "abstract": "Building multimodal language models is fundamentally challenging: it requires aligning vision and language modalities, curating high-quality instruction data, and avoiding the degradation of existing text-only capabilities once vision is introduced. These difficulties are further magnified in the multilingual setting, where the need for multimodal data in different languages exacerbates existing data scarcity, machine translation often distorts meaning, and catastrophic forgetting is more pronounced. To address the aforementioned challenges, we introduce novel techniques spanning both data and modeling. First, we develop a synthetic annotation framework that curates high-quality, diverse multilingual multimodal instruction data, enabling Aya Vision models to produce natural, human-preferred responses to multimodal inputs across many languages. Complementing this, we propose a cross-modal model merging technique that mitigates catastrophic forgetting, effectively preserving text-only capabilities while simultaneously enhancing multimodal generative performance. Aya-Vision-8B achieves best-in-class performance compared to strong multimodal models such as Qwen-2.5-VL-7B, Pixtral-12B, and even much larger Llama-3.2-90B-Vision. We further scale this approach with Aya-Vision-32B, which outperforms models more than twice its size, such as Molmo-72B and LLaMA-3.2-90B-Vision. Our work advances multilingual progress on the multi-modal frontier, and provides insights into techniques that effectively bend the need for compute while delivering extremely high performance.",
            "score": 1,
            "issue_id": 3757,
            "pub_date": "2025-05-13",
            "pub_date_card": {
                "ru": "13 мая",
                "en": "May 13",
                "zh": "5月13日"
            },
            "hash": "c063c125e504fa88",
            "authors": [
                "Saurabh Dash",
                "Yiyang Nan",
                "John Dang",
                "Arash Ahmadian",
                "Shivalika Singh",
                "Madeline Smith",
                "Bharat Venkitesh",
                "Vlad Shmyhlo",
                "Viraat Aryabumi",
                "Walter Beller-Morales",
                "Jeremy Pekmez",
                "Jason Ozuzu",
                "Pierre Richemond",
                "Acyr Locatelli",
                "Nick Frosst",
                "Phil Blunsom",
                "Aidan Gomez",
                "Ivan Zhang",
                "Marzieh Fadaee",
                "Manoj Govindassamy",
                "Sudip Roy",
                "Matthias Gallé",
                "Beyza Ermis",
                "Ahmet Üstün",
                "Sara Hooker"
            ],
            "affiliations": [
                "Cohere",
                "Cohere Labs"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.08751.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#data",
                    "#low_resource",
                    "#multimodal",
                    "#training",
                    "#multilingual"
                ],
                "emoji": "🌐",
                "ru": {
                    "title": "Прорыв в создании эффективных многоязычных мультимодальных ИИ-моделей",
                    "desc": "Статья представляет новые методы для создания многоязычных мультимодальных языковых моделей. Авторы разработали систему синтетической аннотации для создания качественных многоязычных мультимодальных данных для обучения. Они также предложили технику объединения кросс-модальных моделей для предотвращения катастрофического забывания. Модели Aya-Vision, созданные с использованием этих методов, показывают лучшие результаты по сравнению с более крупными конкурентами в многоязычных мультимодальных задачах."
                },
                "en": {
                    "title": "Advancing Multimodal Language Models for Multilingual Mastery",
                    "desc": "This paper addresses the challenges of building multimodal language models that integrate both vision and language, particularly in a multilingual context. It introduces a synthetic annotation framework to create high-quality, diverse multilingual multimodal instruction data, which helps the Aya Vision models generate human-like responses. Additionally, the authors propose a cross-modal model merging technique to prevent catastrophic forgetting, ensuring that text-only capabilities are maintained while improving multimodal performance. The results show that Aya-Vision models outperform existing multimodal models, demonstrating significant advancements in multilingual multimodal processing."
                },
                "zh": {
                    "title": "多模态语言模型的突破性进展",
                    "desc": "构建多模态语言模型面临许多挑战，包括视觉和语言模态的对齐、高质量指令数据的整理，以及在引入视觉后避免文本能力的退化。在多语言环境中，这些困难更加突出，因为不同语言的多模态数据需求加剧了数据稀缺，机器翻译常常扭曲意义，并且灾难性遗忘现象更加明显。为了解决这些问题，我们提出了新的技术，包括一个合成注释框架，用于整理高质量、多样化的多语言多模态指令数据，使Aya Vision模型能够在多种语言中对多模态输入生成自然、符合人类偏好的响应。此外，我们还提出了一种跨模态模型合并技术，有效减轻灾难性遗忘，同时增强多模态生成性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.08712",
            "title": "NavDP: Learning Sim-to-Real Navigation Diffusion Policy with Privileged\n  Information Guidance",
            "url": "https://huggingface.co/papers/2505.08712",
            "abstract": "Learning navigation in dynamic open-world environments is an important yet challenging skill for robots. Most previous methods rely on precise localization and mapping or learn from expensive real-world demonstrations. In this paper, we propose the Navigation Diffusion Policy (NavDP), an end-to-end framework trained solely in simulation and can zero-shot transfer to different embodiments in diverse real-world environments. The key ingredient of NavDP's network is the combination of diffusion-based trajectory generation and a critic function for trajectory selection, which are conditioned on only local observation tokens encoded from a shared policy transformer. Given the privileged information of the global environment in simulation, we scale up the demonstrations of good quality to train the diffusion policy and formulate the critic value function targets with contrastive negative samples. Our demonstration generation approach achieves about 2,500 trajectories/GPU per day, 20times more efficient than real-world data collection, and results in a large-scale navigation dataset with 363.2km trajectories across 1244 scenes. Trained with this simulation dataset, NavDP achieves state-of-the-art performance and consistently outstanding generalization capability on quadruped, wheeled, and humanoid robots in diverse indoor and outdoor environments. In addition, we present a preliminary attempt at using Gaussian Splatting to make in-domain real-to-sim fine-tuning to further bridge the sim-to-real gap. Experiments show that adding such real-to-sim data can improve the success rate by 30\\% without hurting its generalization capability.",
            "score": 1,
            "issue_id": 3752,
            "pub_date": "2025-05-13",
            "pub_date_card": {
                "ru": "13 мая",
                "en": "May 13",
                "zh": "5月13日"
            },
            "hash": "a68cc40de86ece61",
            "authors": [
                "Wenzhe Cai",
                "Jiaqi Peng",
                "Yuqiang Yang",
                "Yujian Zhang",
                "Meng Wei",
                "Hanqing Wang",
                "Yilun Chen",
                "Tai Wang",
                "Jiangmiao Pang"
            ],
            "affiliations": [
                "Shanghai AI Lab",
                "The University of Hong Kong",
                "Tsinghua University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.08712.jpg",
            "data": {
                "categories": [
                    "#transfer_learning",
                    "#agents",
                    "#diffusion",
                    "#robotics",
                    "#dataset"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "NavDP: Универсальная навигация роботов из симуляции в реальность",
                    "desc": "Статья представляет NavDP - систему навигации роботов в динамичной открытой среде, обученную исключительно на симуляциях. NavDP использует комбинацию генерации траектории на основе диффузии и функции критика для выбора траектории, что позволяет ей успешно работать с разными типами роботов в реальном мире. Система обучается на большом наборе данных, сгенерированном в симуляции, что значительно эффективнее сбора реальных данных. Эксперименты показывают высокую производительность и способность к обобщению для различных типов роботов в разнообразных средах."
                },
                "en": {
                    "title": "Efficient Robot Navigation with Simulation-Driven Learning",
                    "desc": "This paper introduces the Navigation Diffusion Policy (NavDP), a novel framework for robot navigation in dynamic environments. Unlike traditional methods that depend on accurate mapping or costly real-world training, NavDP is trained entirely in simulation and can adapt to various robot types in real-world settings. The framework utilizes diffusion-based trajectory generation combined with a critic function to select optimal paths based on local observations. By generating a large dataset of simulated navigation trajectories efficiently, NavDP demonstrates superior performance and generalization across different robotic embodiments and environments."
                },
                "zh": {
                    "title": "机器人导航的新突破：导航扩散策略",
                    "desc": "本论文提出了一种名为导航扩散策略（NavDP）的新框架，旨在帮助机器人在动态开放世界环境中进行导航。该方法完全在模拟环境中训练，能够零-shot迁移到不同的实际应用中。NavDP结合了基于扩散的轨迹生成和用于轨迹选择的评价函数，利用共享策略变换器编码的局部观察信息进行条件化。通过在模拟中获取全球环境的特权信息，我们生成了高质量的演示数据，并在多种室内外环境中实现了最先进的导航性能和出色的泛化能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.08445",
            "title": "Optimizing Retrieval-Augmented Generation: Analysis of Hyperparameter\n  Impact on Performance and Efficiency",
            "url": "https://huggingface.co/papers/2505.08445",
            "abstract": "Large language models achieve high task performance yet often hallucinate or rely on outdated knowledge. Retrieval-augmented generation (RAG) addresses these gaps by coupling generation with external search. We analyse how hyperparameters influence speed and quality in RAG systems, covering Chroma and Faiss vector stores, chunking policies, cross-encoder re-ranking, and temperature, and we evaluate six metrics: faithfulness, answer correctness, answer relevancy, context precision, context recall, and answer similarity. Chroma processes queries 13% faster, whereas Faiss yields higher retrieval precision, revealing a clear speed-accuracy trade-off. Naive fixed-length chunking with small windows and minimal overlap outperforms semantic segmentation while remaining the quickest option. Re-ranking provides modest gains in retrieval quality yet increases runtime by roughly a factor of 5, so its usefulness depends on latency constraints. These results help practitioners balance computational cost and accuracy when tuning RAG systems for transparent, up-to-date responses. Finally, we re-evaluate the top configurations with a corrective RAG workflow and show that their advantages persist when the model can iteratively request additional evidence. We obtain a near-perfect context precision (99%), which demonstrates that RAG systems can achieve extremely high retrieval accuracy with the right combination of hyperparameters, with significant implications for applications where retrieval quality directly impacts downstream task performance, such as clinical decision support in healthcare.",
            "score": 1,
            "issue_id": 3756,
            "pub_date": "2025-05-13",
            "pub_date_card": {
                "ru": "13 мая",
                "en": "May 13",
                "zh": "5月13日"
            },
            "hash": "7b4d8b3daa6efb6d",
            "authors": [
                "Adel Ammar",
                "Anis Koubaa",
                "Omer Nacar",
                "Wadii Boulila"
            ],
            "affiliations": [
                "Alfaisal University, P.O. Box 50927, Riyadh 11533, Saudi Arabia",
                "Prince Sultan University, Rafha Street, P.O. Box 66833, Riyadh 11586, Saudi Arabia"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.08445.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#data",
                    "#benchmark",
                    "#healthcare",
                    "#hallucinations",
                    "#rag"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Оптимизация RAG: баланс между скоростью и точностью",
                    "desc": "Статья анализирует влияние гиперпараметров на скорость и качество в системах генерации с дополнением извлечением (RAG). Исследуются векторные хранилища Chroma и Faiss, стратегии разбиения текста, переранжирование кросс-энкодером и температура. Оценивается шесть метрик, включая достоверность, корректность и релевантность ответов. Результаты показывают компромисс между скоростью и точностью, а также демонстрируют, что RAG-системы могут достичь очень высокой точности извлечения при правильном подборе гиперпараметров."
                },
                "en": {
                    "title": "Optimizing RAG: Balancing Speed and Accuracy for Better AI Responses",
                    "desc": "This paper explores how hyperparameters affect the performance of Retrieval-Augmented Generation (RAG) systems, which combine language generation with external information retrieval. It evaluates different vector stores, chunking strategies, and re-ranking methods to find the best balance between speed and accuracy. The findings indicate that while Chroma is faster, Faiss offers better retrieval precision, highlighting a trade-off between these two factors. The study also shows that with optimal hyperparameter tuning, RAG systems can achieve very high retrieval accuracy, which is crucial for applications like healthcare decision support."
                },
                "zh": {
                    "title": "优化RAG系统，实现高效准确的检索",
                    "desc": "大型语言模型在任务表现上表现优异，但常常会出现幻觉或依赖过时知识。检索增强生成（RAG）通过将生成与外部搜索结合，解决了这些问题。我们分析了超参数如何影响RAG系统的速度和质量，包括向量存储、分块策略、交叉编码重排序和温度等，并评估了六个指标。研究结果帮助从业者在调整RAG系统时平衡计算成本和准确性，以实现透明且最新的响应。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.08175",
            "title": "Fast Text-to-Audio Generation with Adversarial Post-Training",
            "url": "https://huggingface.co/papers/2505.08175",
            "abstract": "Text-to-audio systems, while increasingly performant, are slow at inference time, thus making their latency unpractical for many creative applications. We present Adversarial Relativistic-Contrastive (ARC) post-training, the first adversarial acceleration algorithm for diffusion/flow models not based on distillation. While past adversarial post-training methods have struggled to compare against their expensive distillation counterparts, ARC post-training is a simple procedure that (1) extends a recent relativistic adversarial formulation to diffusion/flow post-training and (2) combines it with a novel contrastive discriminator objective to encourage better prompt adherence. We pair ARC post-training with a number optimizations to Stable Audio Open and build a model capable of generating approx12s of 44.1kHz stereo audio in approx75ms on an H100, and approx7s on a mobile edge-device, the fastest text-to-audio model to our knowledge.",
            "score": 1,
            "issue_id": 3761,
            "pub_date": "2025-05-13",
            "pub_date_card": {
                "ru": "13 мая",
                "en": "May 13",
                "zh": "5月13日"
            },
            "hash": "845d8b64408c2d62",
            "authors": [
                "Zachary Novack",
                "Zach Evans",
                "Zack Zukowski",
                "Josiah Taylor",
                "CJ Carr",
                "Julian Parker",
                "Adnan Al-Sinan",
                "Gian Marco Iodice",
                "Julian McAuley",
                "Taylor Berg-Kirkpatrick",
                "Jordi Pons"
            ],
            "affiliations": [
                "Arm",
                "Stability AI",
                "UC San Diego"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.08175.jpg",
            "data": {
                "categories": [
                    "#audio",
                    "#inference",
                    "#optimization",
                    "#diffusion",
                    "#open_source"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "Сверхбыстрая генерация аудио по тексту с помощью ARC пост-обучения",
                    "desc": "Статья представляет новый метод ускорения моделей диффузии и потоков для генерации аудио по тексту - Adversarial Relativistic-Contrastive (ARC) пост-обучение. Это первый состязательный алгоритм ускорения, не основанный на дистилляции. ARC сочетает релятивистский состязательный подход с новой контрастивной целевой функцией дискриминатора для улучшения соответствия промпту. В сочетании с оптимизациями Stable Audio Open метод позволяет генерировать 12 секунд стерео аудио 44.1кГц за 75 мс на H100 и 7 секунд на мобильном устройстве."
                },
                "en": {
                    "title": "Accelerating Text-to-Audio: The Fastest Model Yet!",
                    "desc": "This paper introduces Adversarial Relativistic-Contrastive (ARC) post-training, a novel method designed to speed up text-to-audio systems without relying on distillation. ARC post-training enhances diffusion and flow models by applying a relativistic adversarial approach combined with a contrastive discriminator objective, which improves the model's ability to follow prompts accurately. The authors demonstrate that their method significantly reduces inference time, achieving audio generation in approximately 75 milliseconds on high-performance hardware and 7 seconds on mobile devices. This advancement positions ARC post-training as the fastest text-to-audio model currently available, making it more practical for creative applications."
                },
                "zh": {
                    "title": "加速文本到音频生成的革命性方法",
                    "desc": "本文介绍了一种新的后训练方法，称为对抗相对对比（ARC），旨在加速文本到音频系统的推理速度。ARC后训练算法不依赖于蒸馏，能够有效提升扩散/流模型的性能。通过结合相对对抗形式和对比判别器目标，ARC鼓励模型更好地遵循提示。经过优化后，该模型在H100上能在约75毫秒内生成12秒的44.1kHz立体声音频，成为目前已知的最快文本到音频模型。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.07416",
            "title": "ViMRHP: A Vietnamese Benchmark Dataset for Multimodal Review Helpfulness\n  Prediction via Human-AI Collaborative Annotation",
            "url": "https://huggingface.co/papers/2505.07416",
            "abstract": "Multimodal Review Helpfulness Prediction (MRHP) is an essential task in recommender systems, particularly in E-commerce platforms. Determining the helpfulness of user-generated reviews enhances user experience and improves consumer decision-making. However, existing datasets focus predominantly on English and Indonesian, resulting in a lack of linguistic diversity, especially for low-resource languages such as Vietnamese. In this paper, we introduce ViMRHP (Vietnamese Multimodal Review Helpfulness Prediction), a large-scale benchmark dataset for MRHP task in Vietnamese. This dataset covers four domains, including 2K products with 46K reviews. Meanwhile, a large-scale dataset requires considerable time and cost. To optimize the annotation process, we leverage AI to assist annotators in constructing the ViMRHP dataset. With AI assistance, annotation time is reduced (90 to 120 seconds per task down to 20 to 40 seconds per task) while maintaining data quality and lowering overall costs by approximately 65%. However, AI-generated annotations still have limitations in complex annotation tasks, which we further examine through a detailed performance analysis. In our experiment on ViMRHP, we evaluate baseline models on human-verified and AI-generated annotations to assess their quality differences. The ViMRHP dataset is publicly available at https://github.com/trng28/ViMRHP",
            "score": 1,
            "issue_id": 3751,
            "pub_date": "2025-05-12",
            "pub_date_card": {
                "ru": "12 мая",
                "en": "May 12",
                "zh": "5月12日"
            },
            "hash": "4f126d39b5476454",
            "authors": [
                "Truc Mai-Thanh Nguyen",
                "Dat Minh Nguyen",
                "Son T. Luu",
                "Kiet Van Nguyen"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2505.07416.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#dataset",
                    "#data",
                    "#multilingual",
                    "#low_resource",
                    "#open_source"
                ],
                "emoji": "🇻🇳",
                "ru": {
                    "title": "ИИ ускоряет создание датасета для анализа отзывов на вьетнамском языке",
                    "desc": "Статья представляет новый набор данных ViMRHP для прогнозирования полезности отзывов на вьетнамском языке. Авторы использовали искусственный интеллект для помощи аннотаторам, что значительно ускорило процесс разметки и снизило затраты. Датасет охватывает 4 домена, включая 2000 продуктов и 46000 отзывов. Исследователи провели эксперименты с базовыми моделями машинного обучения для оценки качества аннотаций, созданных человеком и ИИ."
                },
                "en": {
                    "title": "Empowering Vietnamese Reviews with AI-Driven Helpfulness Prediction",
                    "desc": "This paper presents the ViMRHP dataset, which is designed for predicting the helpfulness of reviews in Vietnamese, addressing a gap in existing datasets that mainly focus on English and Indonesian. The dataset includes 46,000 reviews across 2,000 products, making it a significant resource for multimodal review helpfulness prediction in low-resource languages. To streamline the annotation process, the authors utilize AI, significantly reducing the time required for each annotation task while also cutting costs by about 65%. The study also evaluates the performance of baseline models using both human-verified and AI-generated annotations, highlighting the strengths and limitations of AI in complex annotation tasks."
                },
                "zh": {
                    "title": "提升越南语评论有用性的智能解决方案",
                    "desc": "多模态评论有用性预测（MRHP）是推荐系统中的一个重要任务，尤其是在电子商务平台上。本文介绍了ViMRHP（越南语多模态评论有用性预测），这是一个针对越南语的MRHP任务的大规模基准数据集，涵盖了四个领域，包括2000个产品和46000条评论。为了优化注释过程，我们利用人工智能辅助注释者构建ViMRHP数据集，从而显著减少了注释时间和成本，同时保持数据质量。尽管AI生成的注释在复杂任务中仍存在局限性，但我们通过详细的性能分析进一步探讨了这些问题。"
                }
            }
        }
    ],
    "link_prev": "2025-05-13.html",
    "link_next": "2025-05-15.html",
    "link_month": "2025-05.html",
    "short_date_prev": {
        "ru": "13.05",
        "en": "05/13",
        "zh": "5月13日"
    },
    "short_date_next": {
        "ru": "15.05",
        "en": "05/15",
        "zh": "5月15日"
    },
    "categories": {
        "#dataset": 7,
        "#data": 5,
        "#benchmark": 4,
        "#agents": 2,
        "#cv": 0,
        "#rl": 3,
        "#rlhf": 0,
        "#rag": 1,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 2,
        "#video": 0,
        "#multimodal": 2,
        "#math": 1,
        "#multilingual": 4,
        "#architecture": 3,
        "#healthcare": 1,
        "#training": 6,
        "#robotics": 1,
        "#agi": 0,
        "#games": 2,
        "#interpretability": 1,
        "#reasoning": 3,
        "#transfer_learning": 2,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 6,
        "#survey": 0,
        "#diffusion": 2,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 1,
        "#synthetic": 2,
        "#machine_translation": 1,
        "#leakage": 0,
        "#open_source": 4,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 3
    },
    "zh": {
        "text": "我们介绍了MiniMax-Speech，一种基于Transformer的文本转语音模型。它能生成高质量的语音。模型的关键创新是可学习的演讲者编码器，从参考音频中提取音色特征，无需转录。这使得MiniMax-Speech能在零样本情况下生成表现力强、音色一致的语音，并支持一样本声音克隆。通过Flow-VAE，合成音频的整体质量得到提升。模型支持32种语言，在多种评估指标上表现出色，达到了最先进的结果。",
        "title": "MiniMax-Speech: Intrinsic Zero-Shot Text-to-Speech with a Learnable\n  Speaker Encoder",
        "pinyin": "我们介绍了MiniMax-Speech，一种基于Transformer的文本转语音模型。它能生成高质量的语音。模型的关键创新是可学习的演讲者编码器，从参考音频中提取音色特征，无需转录。这使得MiniMax-Speech能在零样本情况下生成表现力强、音色一致的语音，并支持一样本声音克隆。通过Flow-VAE，合成音频的整体质量得到提升。模型支持32种语言，在多种评估指标上表现出色，达到了最先进的结果。\n\nWǒmen jièshào le MiniMax-Speech, yī zhǒng jīyú Transformer de wénběn zhuǎn yǔyīn móxíng. Tā néng shēngchéng gāo zhìliàng de yǔyīn. Móxíng de guǎnjiàn chuàngxīn shì kě xuéxí de yǎnjiǎngzhě biānmǎqì, cóng cānkǎo yīnpín zhōng tíquān yīnsè tèzhēng, wúxū zhuǎnlù. Zhè shǐ de MiniMax-Speech néng zài líng yàngběn qíngkuàng xià shēngchéng biǎoxiànlì qiáng, yīnsè yīzhì de yǔyīn, bìng zhīchí yī yàngběn shēngyīn kèlóng. Tōngguò Flow-VAE, héchéng yīnpín de zhěngtǐ zhìliàng dédào tíshēng. Móxíng zhīchí 32 zhǒng yǔyán, zài duō zhǒng pínggū zhǐbiāo shàng biǎoxiàn chūsè, dádào le zuì xiānjìn de jiéguǒ.",
        "vocab": "[\n    {\"word\": \"介绍\", \"pinyin\": \"jiè shào\", \"trans\": \"introduce\"},\n    {\"word\": \"MiniMax-Speech\", \"pinyin\": \"MiniMax-Speech\", \"trans\": \"MiniMax-Speech\"},\n    {\"word\": \"基于\", \"pinyin\": \"jī yú\", \"trans\": \"based on\"},\n    {\"word\": \"Transformer\", \"pinyin\": \"Transformer\", \"trans\": \"Transformer\"},\n    {\"word\": \"文本转语音\", \"pinyin\": \"wén běn zhuǎn yǔ yīn\", \"trans\": \"text-to-speech\"},\n    {\"word\": \"模型\", \"pinyin\": \"mó xíng\", \"trans\": \"model\"},\n    {\"word\": \"生成\", \"pinyin\": \"shēng chéng\", \"trans\": \"generate\"},\n    {\"word\": \"高质量\", \"pinyin\": \"gāo zhì liàng\", \"trans\": \"high quality\"},\n    {\"word\": \"语音\", \"pinyin\": \"yǔ yīn\", \"trans\": \"speech\"},\n    {\"word\": \"关键\", \"pinyin\": \"guǎn jiàn\", \"trans\": \"key\"},\n    {\"word\": \"创新\", \"pinyin\": \"chuàng xīn\", \"trans\": \"innovation\"},\n    {\"word\": \"可学习\", \"pinyin\": \"kě xué xí\", \"trans\": \"learnable\"},\n    {\"word\": \"演讲者\", \"pinyin\": \"yǎn jiǎng zhě\", \"trans\": \"speaker\"},\n    {\"word\": \"编码器\", \"pinyin\": \"biān mǎ qì\", \"trans\": \"encoder\"},\n    {\"word\": \"从\", \"pinyin\": \"cóng\", \"trans\": \"from\"},\n    {\"word\": \"参考\", \"pinyin\": \"cān kǎo\", \"trans\": \"reference\"},\n    {\"word\": \"音频\", \"pinyin\": \"yīn pín\", \"trans\": \"audio\"},\n    {\"word\": \"提取\", \"pinyin\": \"tí qu\", \"trans\": \"extract\"},\n    {\"word\": \"音色\", \"pinyin\": \"yīn sè\", \"trans\": \"timbre\"},\n    {\"word\": \"特征\", \"pinyin\": \"tè zhēng\", \"trans\": \"features\"},\n    {\"word\": \"无需\", \"pinyin\": \"wú xū\", \"trans\": \"no need\"},\n    {\"word\": \"转录\", \"pinyin\": \"zhuǎn lù\", \"trans\": \"transcription\"},\n    {\"word\": \"零样本\", \"pinyin\": \"líng yàng běn\", \"trans\": \"zero-shot\"},\n    {\"word\": \"情况\", \"pinyin\": \"qíng kuàng\", \"trans\": \"situation\"},\n    {\"word\": \"表现力\", \"pinyin\": \"biǎo xiàn lì\", \"trans\": \"expressiveness\"},\n    {\"word\": \"强\", \"pinyin\": \"qiáng\", \"trans\": \"strong\"},\n    {\"word\": \"一致\", \"pinyin\": \"yī zhì\", \"trans\": \"consistent\"},\n    {\"word\": \"支持\", \"pinyin\": \"zhī chí\", \"trans\": \"support\"},\n    {\"word\": \"一样本\", \"pinyin\": \"yī yàng běn\", \"trans\": \"one-shot\"},\n    {\"word\": \"声音\", \"pinyin\": \"shēng yīn\", \"trans\": \"voice\"},\n    {\"word\": \"克隆\", \"pinyin\": \"kè lóng\", \"trans\": \"clone\"},\n    {\"word\": \"通过\", \"pinyin\": \"tōng guò\", \"trans\": \"through\"},\n    {\"word\": \"Flow-VAE\", \"pinyin\": \"Flow-VAE\", \"trans\": \"Flow-VAE\"},\n    {\"word\": \"合成\", \"pinyin\": \"hé chéng\", \"trans\": \"synthesis\"},\n    {\"word\": \"整体\", \"pinyin\": \"zhěng tǐ\", \"trans\": \"overall\"},\n    {\"word\": \"质量\", \"pinyin\": \"zhì liàng\", \"trans\": \"quality\"},\n    {\"word\": \"得到\", \"pinyin\": \"dé dào\", \"trans\": \"obtain\"},\n    {\"word\": \"提升\", \"pinyin\": \"tí shēng\", \"trans\": \"improvement\"},\n    {\"word\": \"多种\", \"pinyin\": \"duō zhǒng\", \"trans\": \"various\"},\n    {\"word\": \"评估\", \"pinyin\": \"píng gū\", \"trans\": \"evaluation\"},\n    {\"word\": \"指标\", \"pinyin\": \"zhǐ biāo\", \"trans\": \"metrics\"},\n    {\"word\": \"表现\", \"pinyin\": \"biǎo xiàn\", \"trans\": \"performance\"},\n    {\"word\": \"出色\", \"pinyin\": \"chū sè\", \"trans\": \"outstanding\"},\n    {\"word\": \"达到\", \"pinyin\": \"dá dào\", \"trans\": \"achieve\"},\n    {\"word\": \"最先进\", \"pinyin\": \"zuì xiān jìn\", \"trans\": \"state-of-the-art\"}\n]",
        "trans": "We introduce MiniMax-Speech, a Transformer-based text-to-speech model capable of generating high-quality speech. The key innovation of the model is a learnable speaker encoder that extracts voice characteristics from reference audio without the need for transcription. This enables MiniMax-Speech to generate expressive and voice-consistent speech in zero-shot scenarios and supports one-shot voice cloning. Through Flow-VAE, the overall quality of the synthesized audio is enhanced. The model supports 32 languages and performs excellently across various evaluation metrics, achieving state-of-the-art results.",
        "update_ts": "2025-05-14 09:12"
    }
}