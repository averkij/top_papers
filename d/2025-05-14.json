{
    "date": {
        "ru": "14 мая",
        "en": "May 14",
        "zh": "5月14日"
    },
    "time_utc": "2025-05-14 10:12",
    "weekday": 2,
    "issue_id": 3753,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2505.07916",
            "title": "MiniMax-Speech: Intrinsic Zero-Shot Text-to-Speech with a Learnable\n  Speaker Encoder",
            "url": "https://huggingface.co/papers/2505.07916",
            "abstract": "We introduce MiniMax-Speech, an autoregressive Transformer-based Text-to-Speech (TTS) model that generates high-quality speech. A key innovation is our learnable speaker encoder, which extracts timbre features from a reference audio without requiring its transcription. This enables MiniMax-Speech to produce highly expressive speech with timbre consistent with the reference in a zero-shot manner, while also supporting one-shot voice cloning with exceptionally high similarity to the reference voice. In addition, the overall quality of the synthesized audio is enhanced through the proposed Flow-VAE. Our model supports 32 languages and demonstrates excellent performance across multiple objective and subjective evaluations metrics. Notably, it achieves state-of-the-art (SOTA) results on objective voice cloning metrics (Word Error Rate and Speaker Similarity) and has secured the top position on the public TTS Arena leaderboard. Another key strength of MiniMax-Speech, granted by the robust and disentangled representations from the speaker encoder, is its extensibility without modifying the base model, enabling various applications such as: arbitrary voice emotion control via LoRA; text to voice (T2V) by synthesizing timbre features directly from text description; and professional voice cloning (PVC) by fine-tuning timbre features with additional data. We encourage readers to visit https://minimax-ai.github.io/tts_tech_report for more examples.",
            "score": 67,
            "issue_id": 3752,
            "pub_date": "2025-05-12",
            "pub_date_card": {
                "ru": "12 мая",
                "en": "May 12",
                "zh": "5月12日"
            },
            "hash": "30175415a859995c",
            "authors": [
                "Bowen Zhang",
                "Congchao Guo",
                "Geng Yang",
                "Hang Yu",
                "Haozhe Zhang",
                "Heidi Lei",
                "Jialong Mai",
                "Junjie Yan",
                "Kaiyue Yang",
                "Mingqi Yang",
                "Peikai Huang",
                "Ruiyang Jin",
                "Sitan Jiang",
                "Weihua Cheng",
                "Yawei Li",
                "Yichen Xiao",
                "Yiying Zhou",
                "Yongmao Zhang",
                "Yuan Lu",
                "Yucen He"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2505.07916.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#multilingual",
                    "#games",
                    "#audio"
                ],
                "emoji": "🗣️",
                "ru": {
                    "title": "Революция в синтезе речи: MiniMax-Speech - универсальный голосовой клон",
                    "desc": "MiniMax-Speech - это автоматическая модель синтеза речи на основе Трансформера. Ключевой инновацией является обучаемый кодировщик говорящего, который извлекает характеристики тембра из эталонного аудио без необходимости его транскрипции. Модель поддерживает 32 языка и демонстрирует отличные результаты по многим метрикам, достигая лучших показателей в клонировании голоса. MiniMax-Speech также обладает расширяемостью, позволяющей реализовать различные приложения, такие как управление эмоциями голоса и синтез тембра из текстового описания."
                },
                "en": {
                    "title": "Revolutionizing Speech Synthesis with MiniMax-Speech",
                    "desc": "MiniMax-Speech is a cutting-edge Text-to-Speech (TTS) model that utilizes an autoregressive Transformer architecture to generate high-quality speech. A significant feature of this model is its learnable speaker encoder, which captures timbre characteristics from audio samples without needing their text transcriptions. This allows the model to create expressive speech that matches the timbre of the reference audio in a zero-shot manner, and it can also perform one-shot voice cloning with remarkable accuracy. Additionally, MiniMax-Speech enhances audio quality through a Flow-VAE and supports multiple languages, achieving state-of-the-art results in voice cloning metrics and demonstrating versatility for various applications."
                },
                "zh": {
                    "title": "MiniMax-Speech：高质量语音生成的新突破",
                    "desc": "MiniMax-Speech是一种基于自回归Transformer的文本到语音（TTS）模型，能够生成高质量的语音。其创新之处在于可学习的说话人编码器，可以从参考音频中提取音色特征，而无需其转录。该模型支持零样本生成具有参考音色的表达性语音，并且在单样本语音克隆中表现出极高的相似度。此外，MiniMax-Speech在多个客观和主观评估指标上表现优异，支持32种语言，并在语音克隆的客观指标上达到了最先进的水平。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.07591",
            "title": "A Multi-Dimensional Constraint Framework for Evaluating and Improving\n  Instruction Following in Large Language Models",
            "url": "https://huggingface.co/papers/2505.07591",
            "abstract": "Instruction following evaluates large language models (LLMs) on their ability to generate outputs that adhere to user-defined constraints. However, existing benchmarks often rely on templated constraint prompts, which lack the diversity of real-world usage and limit fine-grained performance assessment. To fill this gap, we propose a multi-dimensional constraint framework encompassing three constraint patterns, four constraint categories, and four difficulty levels. Building on this framework, we develop an automated instruction generation pipeline that performs constraint expansion, conflict detection, and instruction rewriting, yielding 1,200 code-verifiable instruction-following test samples. We evaluate 19 LLMs across seven model families and uncover substantial variation in performance across constraint forms. For instance, average performance drops from 77.67% at Level I to 32.96% at Level IV. Furthermore, we demonstrate the utility of our approach by using it to generate data for reinforcement learning, achieving substantial gains in instruction following without degrading general performance. In-depth analysis indicates that these gains stem primarily from modifications in the model's attention modules parameters, which enhance constraint recognition and adherence. Code and data are available in https://github.com/Junjie-Ye/MulDimIF.",
            "score": 4,
            "issue_id": 3750,
            "pub_date": "2025-05-12",
            "pub_date_card": {
                "ru": "12 мая",
                "en": "May 12",
                "zh": "5月12日"
            },
            "hash": "ca7c47ccc0066e55",
            "authors": [
                "Junjie Ye",
                "Caishuang Huang",
                "Zhuohan Chen",
                "Wenjie Fu",
                "Chenyuan Yang",
                "Leyi Yang",
                "Yilong Wu",
                "Peng Wang",
                "Meng Zhou",
                "Xiaolong Yang",
                "Tao Gui",
                "Qi Zhang",
                "Zhongchao Shi",
                "Jianping Fan",
                "Xuanjing Huang"
            ],
            "affiliations": [
                "Institute of Modern Languages and Linguistics, Fudan University",
                "Lenovo Research",
                "School of Computer Science, Fudan University",
                "Tencent"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.07591.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#rl",
                    "#alignment",
                    "#optimization",
                    "#benchmark"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Многомерная оценка следования инструкциям для больших языковых моделей",
                    "desc": "Статья представляет новый подход к оценке способности больших языковых моделей (LLM) следовать инструкциям пользователей. Авторы разработали многомерную систему ограничений, включающую три паттерна, четыре категории и четыре уровня сложности. На основе этой системы был создан автоматизированный процесс генерации инструкций, который позволил создать 1200 проверяемых тестовых примеров. Исследование 19 LLM показало значительные различия в производительности в зависимости от типа ограничений, причем средняя производительность снижалась с 77,67% на уровне I до 32,96% на уровне IV."
                },
                "en": {
                    "title": "Enhancing LLMs with Multi-Dimensional Constraints",
                    "desc": "This paper evaluates large language models (LLMs) on their ability to follow user-defined constraints using a new multi-dimensional constraint framework. The framework includes various constraint patterns, categories, and difficulty levels, allowing for a more nuanced assessment of model performance. An automated instruction generation pipeline is developed to create diverse test samples, revealing significant performance variations among different LLMs when faced with increasing constraint complexity. The study also shows that using this framework for reinforcement learning can improve instruction adherence without compromising overall model performance, primarily by adjusting attention module parameters."
                },
                "zh": {
                    "title": "多维约束框架提升语言模型性能",
                    "desc": "本文探讨了如何评估大型语言模型（LLMs）在遵循用户定义约束方面的能力。现有的基准测试通常依赖于模板化的约束提示，缺乏真实世界使用的多样性，限制了细致的性能评估。为了解决这个问题，我们提出了一个多维约束框架，涵盖三种约束模式、四种约束类别和四个难度级别。通过这个框架，我们开发了一个自动化指令生成管道，生成了1200个可验证的指令遵循测试样本，并评估了19个LLM在不同约束形式下的表现。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.07215",
            "title": "Measuring General Intelligence with Generated Games",
            "url": "https://huggingface.co/papers/2505.07215",
            "abstract": "We present gg-bench, a collection of game environments designed to evaluate general reasoning capabilities in language models. Unlike most static benchmarks, gg-bench is a data generating process where new evaluation instances can be generated at will. In particular, gg-bench is synthetically generated by (1) using a large language model (LLM) to generate natural language descriptions of novel games, (2) using the LLM to implement each game in code as a Gym environment, and (3) training reinforcement learning (RL) agents via self-play on the generated games. We evaluate language models by their winrate against these RL agents by prompting models with the game description, current board state, and a list of valid moves, after which models output the moves they wish to take. gg-bench is challenging: state-of-the-art LLMs such as GPT-4o and Claude 3.7 Sonnet achieve winrates of 7-9% on gg-bench using in-context learning, while reasoning models such as o1, o3-mini and DeepSeek-R1 achieve average winrates of 31-36%. We release the generated games, data generation process, and evaluation code in order to support future modeling work and expansion of our benchmark.",
            "score": 4,
            "issue_id": 3751,
            "pub_date": "2025-05-12",
            "pub_date_card": {
                "ru": "12 мая",
                "en": "May 12",
                "zh": "5月12日"
            },
            "hash": "ed99ec3875dd9a95",
            "authors": [
                "Vivek Verma",
                "David Huang",
                "William Chen",
                "Dan Klein",
                "Nicholas Tomlin"
            ],
            "affiliations": [
                "Computer Science Division, University of California, Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.07215.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#reasoning",
                    "#rl",
                    "#games",
                    "#synthetic"
                ],
                "emoji": "🎮",
                "ru": {
                    "title": "gg-bench: Новый подход к оценке способностей языковых моделей через игровые среды",
                    "desc": "В статье представлен gg-bench - набор игровых сред для оценки способностей языковых моделей к общему рассуждению. gg-bench автоматически генерирует новые игры с помощью большой языковой модели (LLM), которая создает описания и код игр. Для оценки языковые модели играют против обученных с помощью обучения с подкреплением агентов. Результаты показывают, что современные LLM достигают низких результатов (7-9% побед), в то время как специализированные модели рассуждений достигают 31-36% побед."
                },
                "en": {
                    "title": "Dynamic Game Environments for Evaluating AI Reasoning",
                    "desc": "The paper introduces gg-bench, a novel benchmark for assessing the reasoning abilities of language models through interactive game environments. Unlike traditional benchmarks, gg-bench allows for the dynamic generation of new game instances using a large language model (LLM) to create game descriptions and implement them as Gym environments. The evaluation involves training reinforcement learning (RL) agents to compete against language models, measuring their performance based on win rates. The results show that while state-of-the-art LLMs achieve low win rates, specialized reasoning models perform significantly better, highlighting the challenges in evaluating general reasoning in AI."
                },
                "zh": {
                    "title": "gg-bench：评估语言模型推理能力的新基准",
                    "desc": "我们提出了gg-bench，这是一个用于评估语言模型一般推理能力的游戏环境集合。与大多数静态基准测试不同，gg-bench是一个数据生成过程，可以随时生成新的评估实例。具体来说，gg-bench通过使用大型语言模型生成新游戏的自然语言描述，并将每个游戏实现为Gym环境，最后通过自我对弈训练强化学习代理。我们通过模型在这些游戏中与强化学习代理的胜率来评估语言模型，发现当前最先进的语言模型在gg-bench上的胜率仅为7-9%。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.08665",
            "title": "SkillFormer: Unified Multi-View Video Understanding for Proficiency\n  Estimation",
            "url": "https://huggingface.co/papers/2505.08665",
            "abstract": "Assessing human skill levels in complex activities is a challenging problem with applications in sports, rehabilitation, and training. In this work, we present SkillFormer, a parameter-efficient architecture for unified multi-view proficiency estimation from egocentric and exocentric videos. Building on the TimeSformer backbone, SkillFormer introduces a CrossViewFusion module that fuses view-specific features using multi-head cross-attention, learnable gating, and adaptive self-calibration. We leverage Low-Rank Adaptation to fine-tune only a small subset of parameters, significantly reducing training costs. In fact, when evaluated on the EgoExo4D dataset, SkillFormer achieves state-of-the-art accuracy in multi-view settings while demonstrating remarkable computational efficiency, using 4.5x fewer parameters and requiring 3.75x fewer training epochs than prior baselines. It excels in multiple structured tasks, confirming the value of multi-view integration for fine-grained skill assessment.",
            "score": 1,
            "issue_id": 3750,
            "pub_date": "2025-05-13",
            "pub_date_card": {
                "ru": "13 мая",
                "en": "May 13",
                "zh": "5月13日"
            },
            "hash": "8cbf16dc2ec90273",
            "authors": [
                "Edoardo Bianchi",
                "Antonio Liotta"
            ],
            "affiliations": [
                "Free University of Bozen-Bolzano Bozen-Bolzano, IT"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.08665.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#dataset",
                    "#training"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "SkillFormer: эффективная оценка мастерства по мультиракурсному видео",
                    "desc": "SkillFormer - это эффективная архитектура для оценки уровня мастерства в сложных действиях по видео с нескольких ракурсов. Она использует модуль CrossViewFusion для объединения признаков из разных ракурсов с помощью кросс-внимания и адаптивной самокалибровки. Благодаря технике Low-Rank Adaptation, SkillFormer дообучает лишь небольшую часть параметров, значительно сокращая вычислительные затраты. На датасете EgoExo4D модель достигает лучшей точности при использовании в 4,5 раза меньше параметров по сравнению с аналогами."
                },
                "en": {
                    "title": "SkillFormer: Efficient Multi-View Skill Assessment",
                    "desc": "This paper introduces SkillFormer, a new machine learning model designed to assess human skill levels in complex activities using videos from different perspectives. It utilizes a CrossViewFusion module that combines features from both egocentric (first-person) and exocentric (third-person) views through advanced techniques like multi-head cross-attention. The model is built on the TimeSformer architecture and employs Low-Rank Adaptation to minimize the number of parameters that need to be trained, making it more efficient. SkillFormer achieves top accuracy on the EgoExo4D dataset while using significantly fewer resources compared to previous models, highlighting the effectiveness of integrating multiple views for skill evaluation."
                },
                "zh": {
                    "title": "SkillFormer：高效的多视角技能评估架构",
                    "desc": "评估人类在复杂活动中的技能水平是一项具有挑战性的任务，广泛应用于体育、康复和培训领域。本文提出了SkillFormer，这是一种高效的架构，能够从自我中心和外部视角的视频中统一估计技能水平。SkillFormer基于TimeSformer骨干网，引入了CrossViewFusion模块，通过多头交叉注意力、可学习的门控和自适应自校准来融合视角特征。通过低秩适应技术，我们仅微调少量参数，显著降低了训练成本，并在EgoExo4D数据集上实现了多视角设置下的最先进准确率。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.08712",
            "title": "NavDP: Learning Sim-to-Real Navigation Diffusion Policy with Privileged\n  Information Guidance",
            "url": "https://huggingface.co/papers/2505.08712",
            "abstract": "Learning navigation in dynamic open-world environments is an important yet challenging skill for robots. Most previous methods rely on precise localization and mapping or learn from expensive real-world demonstrations. In this paper, we propose the Navigation Diffusion Policy (NavDP), an end-to-end framework trained solely in simulation and can zero-shot transfer to different embodiments in diverse real-world environments. The key ingredient of NavDP's network is the combination of diffusion-based trajectory generation and a critic function for trajectory selection, which are conditioned on only local observation tokens encoded from a shared policy transformer. Given the privileged information of the global environment in simulation, we scale up the demonstrations of good quality to train the diffusion policy and formulate the critic value function targets with contrastive negative samples. Our demonstration generation approach achieves about 2,500 trajectories/GPU per day, 20times more efficient than real-world data collection, and results in a large-scale navigation dataset with 363.2km trajectories across 1244 scenes. Trained with this simulation dataset, NavDP achieves state-of-the-art performance and consistently outstanding generalization capability on quadruped, wheeled, and humanoid robots in diverse indoor and outdoor environments. In addition, we present a preliminary attempt at using Gaussian Splatting to make in-domain real-to-sim fine-tuning to further bridge the sim-to-real gap. Experiments show that adding such real-to-sim data can improve the success rate by 30\\% without hurting its generalization capability.",
            "score": 0,
            "issue_id": 3752,
            "pub_date": "2025-05-13",
            "pub_date_card": {
                "ru": "13 мая",
                "en": "May 13",
                "zh": "5月13日"
            },
            "hash": "a68cc40de86ece61",
            "authors": [
                "Wenzhe Cai",
                "Jiaqi Peng",
                "Yuqiang Yang",
                "Yujian Zhang",
                "Meng Wei",
                "Hanqing Wang",
                "Yilun Chen",
                "Tai Wang",
                "Jiangmiao Pang"
            ],
            "affiliations": [
                "Shanghai AI Lab",
                "The University of Hong Kong",
                "Tsinghua University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.08712.jpg",
            "data": {
                "categories": [
                    "#transfer_learning",
                    "#agents",
                    "#diffusion",
                    "#robotics",
                    "#dataset"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "NavDP: Универсальная навигация роботов из симуляции в реальность",
                    "desc": "Статья представляет NavDP - систему навигации роботов в динамичной открытой среде, обученную исключительно на симуляциях. NavDP использует комбинацию генерации траектории на основе диффузии и функции критика для выбора траектории, что позволяет ей успешно работать с разными типами роботов в реальном мире. Система обучается на большом наборе данных, сгенерированном в симуляции, что значительно эффективнее сбора реальных данных. Эксперименты показывают высокую производительность и способность к обобщению для различных типов роботов в разнообразных средах."
                },
                "en": {
                    "title": "Efficient Robot Navigation with Simulation-Driven Learning",
                    "desc": "This paper introduces the Navigation Diffusion Policy (NavDP), a novel framework for robot navigation in dynamic environments. Unlike traditional methods that depend on accurate mapping or costly real-world training, NavDP is trained entirely in simulation and can adapt to various robot types in real-world settings. The framework utilizes diffusion-based trajectory generation combined with a critic function to select optimal paths based on local observations. By generating a large dataset of simulated navigation trajectories efficiently, NavDP demonstrates superior performance and generalization across different robotic embodiments and environments."
                },
                "zh": {
                    "title": "机器人导航的新突破：导航扩散策略",
                    "desc": "本论文提出了一种名为导航扩散策略（NavDP）的新框架，旨在帮助机器人在动态开放世界环境中进行导航。该方法完全在模拟环境中训练，能够零-shot迁移到不同的实际应用中。NavDP结合了基于扩散的轨迹生成和用于轨迹选择的评价函数，利用共享策略变换器编码的局部观察信息进行条件化。通过在模拟中获取全球环境的特权信息，我们生成了高质量的演示数据，并在多种室内外环境中实现了最先进的导航性能和出色的泛化能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.07416",
            "title": "ViMRHP: A Vietnamese Benchmark Dataset for Multimodal Review Helpfulness\n  Prediction via Human-AI Collaborative Annotation",
            "url": "https://huggingface.co/papers/2505.07416",
            "abstract": "Multimodal Review Helpfulness Prediction (MRHP) is an essential task in recommender systems, particularly in E-commerce platforms. Determining the helpfulness of user-generated reviews enhances user experience and improves consumer decision-making. However, existing datasets focus predominantly on English and Indonesian, resulting in a lack of linguistic diversity, especially for low-resource languages such as Vietnamese. In this paper, we introduce ViMRHP (Vietnamese Multimodal Review Helpfulness Prediction), a large-scale benchmark dataset for MRHP task in Vietnamese. This dataset covers four domains, including 2K products with 46K reviews. Meanwhile, a large-scale dataset requires considerable time and cost. To optimize the annotation process, we leverage AI to assist annotators in constructing the ViMRHP dataset. With AI assistance, annotation time is reduced (90 to 120 seconds per task down to 20 to 40 seconds per task) while maintaining data quality and lowering overall costs by approximately 65%. However, AI-generated annotations still have limitations in complex annotation tasks, which we further examine through a detailed performance analysis. In our experiment on ViMRHP, we evaluate baseline models on human-verified and AI-generated annotations to assess their quality differences. The ViMRHP dataset is publicly available at https://github.com/trng28/ViMRHP",
            "score": 0,
            "issue_id": 3751,
            "pub_date": "2025-05-12",
            "pub_date_card": {
                "ru": "12 мая",
                "en": "May 12",
                "zh": "5月12日"
            },
            "hash": "4f126d39b5476454",
            "authors": [
                "Truc Mai-Thanh Nguyen",
                "Dat Minh Nguyen",
                "Son T. Luu",
                "Kiet Van Nguyen"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2505.07416.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#dataset",
                    "#data",
                    "#multilingual",
                    "#low_resource",
                    "#open_source"
                ],
                "emoji": "🇻🇳",
                "ru": {
                    "title": "ИИ ускоряет создание датасета для анализа отзывов на вьетнамском языке",
                    "desc": "Статья представляет новый набор данных ViMRHP для прогнозирования полезности отзывов на вьетнамском языке. Авторы использовали искусственный интеллект для помощи аннотаторам, что значительно ускорило процесс разметки и снизило затраты. Датасет охватывает 4 домена, включая 2000 продуктов и 46000 отзывов. Исследователи провели эксперименты с базовыми моделями машинного обучения для оценки качества аннотаций, созданных человеком и ИИ."
                },
                "en": {
                    "title": "Empowering Vietnamese Reviews with AI-Driven Helpfulness Prediction",
                    "desc": "This paper presents the ViMRHP dataset, which is designed for predicting the helpfulness of reviews in Vietnamese, addressing a gap in existing datasets that mainly focus on English and Indonesian. The dataset includes 46,000 reviews across 2,000 products, making it a significant resource for multimodal review helpfulness prediction in low-resource languages. To streamline the annotation process, the authors utilize AI, significantly reducing the time required for each annotation task while also cutting costs by about 65%. The study also evaluates the performance of baseline models using both human-verified and AI-generated annotations, highlighting the strengths and limitations of AI in complex annotation tasks."
                },
                "zh": {
                    "title": "提升越南语评论有用性的智能解决方案",
                    "desc": "多模态评论有用性预测（MRHP）是推荐系统中的一个重要任务，尤其是在电子商务平台上。本文介绍了ViMRHP（越南语多模态评论有用性预测），这是一个针对越南语的MRHP任务的大规模基准数据集，涵盖了四个领域，包括2000个产品和46000条评论。为了优化注释过程，我们利用人工智能辅助注释者构建ViMRHP数据集，从而显著减少了注释时间和成本，同时保持数据质量。尽管AI生成的注释在复杂任务中仍存在局限性，但我们通过详细的性能分析进一步探讨了这些问题。"
                }
            }
        }
    ],
    "link_prev": "2025-05-13.html",
    "link_next": "2025-05-15.html",
    "link_month": "2025-05.html",
    "short_date_prev": {
        "ru": "13.05",
        "en": "05/13",
        "zh": "5月13日"
    },
    "short_date_next": {
        "ru": "15.05",
        "en": "05/15",
        "zh": "5月15日"
    },
    "categories": {
        "#dataset": 4,
        "#data": 1,
        "#benchmark": 2,
        "#agents": 1,
        "#cv": 0,
        "#rl": 2,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 1,
        "#video": 0,
        "#multimodal": 0,
        "#math": 0,
        "#multilingual": 2,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 2,
        "#robotics": 1,
        "#agi": 0,
        "#games": 2,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 3,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 1
    },
    "zh": {
        "text": "我们介绍了MiniMax-Speech，一种基于Transformer的文本转语音模型。它能生成高质量的语音。模型的关键创新是可学习的演讲者编码器，从参考音频中提取音色特征，无需转录。这使得MiniMax-Speech能在零样本情况下生成表现力强、音色一致的语音，并支持一样本声音克隆。通过Flow-VAE，合成音频的整体质量得到提升。模型支持32种语言，在多种评估指标上表现出色，达到了最先进的结果。",
        "title": "MiniMax-Speech: Intrinsic Zero-Shot Text-to-Speech with a Learnable\n  Speaker Encoder",
        "pinyin": "我们介绍了MiniMax-Speech，一种基于Transformer的文本转语音模型。它能生成高质量的语音。模型的关键创新是可学习的演讲者编码器，从参考音频中提取音色特征，无需转录。这使得MiniMax-Speech能在零样本情况下生成表现力强、音色一致的语音，并支持一样本声音克隆。通过Flow-VAE，合成音频的整体质量得到提升。模型支持32种语言，在多种评估指标上表现出色，达到了最先进的结果。\n\nWǒmen jièshào le MiniMax-Speech, yī zhǒng jīyú Transformer de wénběn zhuǎn yǔyīn móxíng. Tā néng shēngchéng gāo zhìliàng de yǔyīn. Móxíng de guǎnjiàn chuàngxīn shì kě xuéxí de yǎnjiǎngzhě biānmǎqì, cóng cānkǎo yīnpín zhōng tíquān yīnsè tèzhēng, wúxū zhuǎnlù. Zhè shǐ de MiniMax-Speech néng zài líng yàngběn qíngkuàng xià shēngchéng biǎoxiànlì qiáng, yīnsè yīzhì de yǔyīn, bìng zhīchí yī yàngběn shēngyīn kèlóng. Tōngguò Flow-VAE, héchéng yīnpín de zhěngtǐ zhìliàng dédào tíshēng. Móxíng zhīchí 32 zhǒng yǔyán, zài duō zhǒng pínggū zhǐbiāo shàng biǎoxiàn chūsè, dádào le zuì xiānjìn de jiéguǒ.",
        "vocab": "[\n    {\"word\": \"介绍\", \"pinyin\": \"jiè shào\", \"trans\": \"introduce\"},\n    {\"word\": \"MiniMax-Speech\", \"pinyin\": \"MiniMax-Speech\", \"trans\": \"MiniMax-Speech\"},\n    {\"word\": \"基于\", \"pinyin\": \"jī yú\", \"trans\": \"based on\"},\n    {\"word\": \"Transformer\", \"pinyin\": \"Transformer\", \"trans\": \"Transformer\"},\n    {\"word\": \"文本转语音\", \"pinyin\": \"wén běn zhuǎn yǔ yīn\", \"trans\": \"text-to-speech\"},\n    {\"word\": \"模型\", \"pinyin\": \"mó xíng\", \"trans\": \"model\"},\n    {\"word\": \"生成\", \"pinyin\": \"shēng chéng\", \"trans\": \"generate\"},\n    {\"word\": \"高质量\", \"pinyin\": \"gāo zhì liàng\", \"trans\": \"high quality\"},\n    {\"word\": \"语音\", \"pinyin\": \"yǔ yīn\", \"trans\": \"speech\"},\n    {\"word\": \"关键\", \"pinyin\": \"guǎn jiàn\", \"trans\": \"key\"},\n    {\"word\": \"创新\", \"pinyin\": \"chuàng xīn\", \"trans\": \"innovation\"},\n    {\"word\": \"可学习\", \"pinyin\": \"kě xué xí\", \"trans\": \"learnable\"},\n    {\"word\": \"演讲者\", \"pinyin\": \"yǎn jiǎng zhě\", \"trans\": \"speaker\"},\n    {\"word\": \"编码器\", \"pinyin\": \"biān mǎ qì\", \"trans\": \"encoder\"},\n    {\"word\": \"从\", \"pinyin\": \"cóng\", \"trans\": \"from\"},\n    {\"word\": \"参考\", \"pinyin\": \"cān kǎo\", \"trans\": \"reference\"},\n    {\"word\": \"音频\", \"pinyin\": \"yīn pín\", \"trans\": \"audio\"},\n    {\"word\": \"提取\", \"pinyin\": \"tí qu\", \"trans\": \"extract\"},\n    {\"word\": \"音色\", \"pinyin\": \"yīn sè\", \"trans\": \"timbre\"},\n    {\"word\": \"特征\", \"pinyin\": \"tè zhēng\", \"trans\": \"features\"},\n    {\"word\": \"无需\", \"pinyin\": \"wú xū\", \"trans\": \"no need\"},\n    {\"word\": \"转录\", \"pinyin\": \"zhuǎn lù\", \"trans\": \"transcription\"},\n    {\"word\": \"零样本\", \"pinyin\": \"líng yàng běn\", \"trans\": \"zero-shot\"},\n    {\"word\": \"情况\", \"pinyin\": \"qíng kuàng\", \"trans\": \"situation\"},\n    {\"word\": \"表现力\", \"pinyin\": \"biǎo xiàn lì\", \"trans\": \"expressiveness\"},\n    {\"word\": \"强\", \"pinyin\": \"qiáng\", \"trans\": \"strong\"},\n    {\"word\": \"一致\", \"pinyin\": \"yī zhì\", \"trans\": \"consistent\"},\n    {\"word\": \"支持\", \"pinyin\": \"zhī chí\", \"trans\": \"support\"},\n    {\"word\": \"一样本\", \"pinyin\": \"yī yàng běn\", \"trans\": \"one-shot\"},\n    {\"word\": \"声音\", \"pinyin\": \"shēng yīn\", \"trans\": \"voice\"},\n    {\"word\": \"克隆\", \"pinyin\": \"kè lóng\", \"trans\": \"clone\"},\n    {\"word\": \"通过\", \"pinyin\": \"tōng guò\", \"trans\": \"through\"},\n    {\"word\": \"Flow-VAE\", \"pinyin\": \"Flow-VAE\", \"trans\": \"Flow-VAE\"},\n    {\"word\": \"合成\", \"pinyin\": \"hé chéng\", \"trans\": \"synthesis\"},\n    {\"word\": \"整体\", \"pinyin\": \"zhěng tǐ\", \"trans\": \"overall\"},\n    {\"word\": \"质量\", \"pinyin\": \"zhì liàng\", \"trans\": \"quality\"},\n    {\"word\": \"得到\", \"pinyin\": \"dé dào\", \"trans\": \"obtain\"},\n    {\"word\": \"提升\", \"pinyin\": \"tí shēng\", \"trans\": \"improvement\"},\n    {\"word\": \"多种\", \"pinyin\": \"duō zhǒng\", \"trans\": \"various\"},\n    {\"word\": \"评估\", \"pinyin\": \"píng gū\", \"trans\": \"evaluation\"},\n    {\"word\": \"指标\", \"pinyin\": \"zhǐ biāo\", \"trans\": \"metrics\"},\n    {\"word\": \"表现\", \"pinyin\": \"biǎo xiàn\", \"trans\": \"performance\"},\n    {\"word\": \"出色\", \"pinyin\": \"chū sè\", \"trans\": \"outstanding\"},\n    {\"word\": \"达到\", \"pinyin\": \"dá dào\", \"trans\": \"achieve\"},\n    {\"word\": \"最先进\", \"pinyin\": \"zuì xiān jìn\", \"trans\": \"state-of-the-art\"}\n]",
        "trans": "We introduce MiniMax-Speech, a Transformer-based text-to-speech model capable of generating high-quality speech. The key innovation of the model is a learnable speaker encoder that extracts voice characteristics from reference audio without the need for transcription. This enables MiniMax-Speech to generate expressive and voice-consistent speech in zero-shot scenarios and supports one-shot voice cloning. Through Flow-VAE, the overall quality of the synthesized audio is enhanced. The model supports 32 languages and performs excellently across various evaluation metrics, achieving state-of-the-art results.",
        "update_ts": "2025-05-14 09:12"
    }
}