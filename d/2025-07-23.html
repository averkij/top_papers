
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 13 papers. July 23.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ", "Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹", "Ğ¼Ğ¸Ğ½ÑƒÑ‚"],
                hour: ["Ñ‡Ğ°Ñ", "Ñ‡Ğ°ÑĞ°", "Ñ‡Ğ°ÑĞ¾Ğ²"],
                day: ["Ğ´ĞµĞ½ÑŒ", "Ğ´Ğ½Ñ", "Ğ´Ğ½ĞµĞ¹"],
                justNow: "Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ñ‚Ğ¾",
                ago: "Ğ½Ğ°Ğ·Ğ°Ğ´"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["åˆ†é’Ÿ", "åˆ†é’Ÿ", "åˆ†é’Ÿ"],
                hour: ["å°æ—¶", "å°æ—¶", "å°æ—¶"],
                day: ["å¤©", "å¤©", "å¤©"],
                justNow: "åˆšåˆš",
                ago: "å‰"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            } else if (lastDigit === 1) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒÑ";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒĞ¸";
            } else {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ç¯‡è®ºæ–‡"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">ğŸ”º</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">23 Ğ¸ÑĞ»Ñ</span> | <span id="title-articles-count">13 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-07-22.html">â¬…ï¸ <span id="prev-date">22.07</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-07-24.html">â¡ï¸ <span id="next-date">24.07</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-07.html">ğŸ“ˆ <span id='top-month-label'>ĞœĞµÑÑÑ†</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">ğŸ”€ <span id="sort-label-text">Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ</option>
                    <option value="pub_date">Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸</option>
                    <option value="issue_id">Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">ğŸ·ï¸ Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> AâˆªB</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> Aâˆ©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">ğŸ§¹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> âœ–ï¸ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '23 Ğ¸ÑĞ»Ñ', 'en': 'July 23', 'zh': '7æœˆ23æ—¥'};
        let feedDateNext = {'ru': '24.07', 'en': '07/24', 'zh': '7æœˆ24æ—¥'};
        let feedDatePrev = {'ru': '22.07', 'en': '07/22', 'zh': '7æœˆ22æ—¥'};
        let filterLabel = {'ru': 'Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€', 'en': 'Topics', 'zh': 'ä¸»é¢˜ç­›é€‰'}
        let publishedLabel = {'ru': 'ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ñ‚ ', 'en': 'published on ', 'zh': 'å‘è¡¨äº'}
        let sortLabel = {'ru': 'Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾', 'en': 'Sort by', 'zh': 'æ’åºæ–¹å¼'}
        let paperLabel = {'ru': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ', 'en': 'Paper', 'zh': 'è®ºæ–‡'}
        let topMonthLabel = {'ru': 'ĞœĞµÑÑÑ†', 'en': 'Month', 'zh': 'æœˆåº¦è®ºæ–‡'}
        let topDayLabel = {'ru': 'Ğ”ĞµĞ½ÑŒ', 'en': 'Day', 'zh': 'æ—¥åº¦è®ºæ–‡'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2507.16784', 'title': 'Beyond Context Limits: Subconscious Threads for Long-Horizon Reasoning', 'url': 'https://huggingface.co/papers/2507.16784', 'abstract': 'A Thread Inference Model (TIM) and its runtime (TIMRUN) enable long-horizon reasoning in LLMs by using reasoning trees and key-value state retention, overcoming context and memory limitations.  \t\t\t\t\tAI-generated summary \t\t\t\t To break the context limits of large language models (LLMs) that bottleneck reasoning accuracy and efficiency, we propose the Thread Inference Model (TIM), a family of LLMs trained for recursive and decompositional problem solving, and TIMRUN, an inference runtime enabling long-horizon structured reasoning beyond context limits. Together, TIM hosted on TIMRUN supports virtually unlimited working memory and multi-hop tool calls within a single language model inference, overcoming output limits, positional-embedding constraints, and GPU-memory bottlenecks. Performance is achieved by modeling natural language as reasoning trees measured by both length and depth instead of linear sequences. The reasoning trees consist of tasks with thoughts, recursive subtasks, and conclusions based on the concept we proposed in Schroeder et al, 2025. During generation, we maintain a working memory that retains only the key-value states of the most relevant context tokens, selected by a rule-based subtask-pruning mechanism, enabling reuse of positional embeddings and GPU memory pages throughout reasoning. Experimental results show that our system sustains high inference throughput, even when manipulating up to 90% of the KV cache in GPU memory. It also delivers accurate reasoning on mathematical tasks and handles information retrieval challenges that require long-horizon reasoning and multi-hop tool use.', 'score': 36, 'issue_id': 4961, 'pub_date': '2025-07-22', 'pub_date_card': {'ru': '22 Ğ¸ÑĞ»Ñ', 'en': 'July 22', 'zh': '7æœˆ22æ—¥'}, 'hash': '9f1ac89cdfedc134', 'authors': ['Hongyin Luo', 'Nathaniel Morgan', 'Tina Li', 'Derek Zhao', 'Ai Vy Ngo', 'Philip Schroeder', 'Lijie Yang', 'Assaf Ben-Kish', "Jack O'Brien", 'James Glass'], 'affiliations': ['MIT CSAIL', 'Princeton University', 'Subconscious Systems Technologies, Inc.', 'Tel Aviv University'], 'pdf_title_img': 'assets/pdf/title_img/2507.16784.jpg', 'data': {'categories': ['#inference', '#architecture', '#long_context', '#reasoning', '#math'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² LLM Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ñ€ĞµĞ²Ğ¾Ğ²Ğ¸Ğ´Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Thread Inference Model (TIM) Ğ¸ ĞµĞ³Ğ¾ ÑÑ€ĞµĞ´Ñƒ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ TIMRUN, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ (LLM) Ğ¾ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ğ»ÑÑ‚ÑŒ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. TIM Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´ĞµÑ€ĞµĞ²ÑŒÑ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ½ĞµĞ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½ÑƒÑ Ñ€Ğ°Ğ±Ğ¾Ñ‡ÑƒÑ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğµ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ñ‹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ñ€Ğ°Ğ¼ĞºĞ°Ñ… Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ½ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°.'}, 'en': {'title': 'Unlocking Long-Horizon Reasoning in LLMs with TIM', 'desc': 'The Thread Inference Model (TIM) introduces a new approach for large language models (LLMs) to enhance their reasoning capabilities over extended contexts. By utilizing reasoning trees and a key-value state retention mechanism, TIM allows for recursive problem solving and efficient management of memory during inference. This model overcomes traditional limitations of LLMs, such as output constraints and GPU memory bottlenecks, enabling complex multi-hop reasoning tasks. Experimental results demonstrate that TIM can maintain high throughput while accurately performing mathematical reasoning and information retrieval tasks that require long-horizon thinking.'}, 'zh': {'title': 'çªç ´ä¸Šä¸‹æ–‡é™åˆ¶ï¼Œå®ç°é•¿è¿œæ¨ç†', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§çº¿ç¨‹æ¨ç†æ¨¡å‹ï¼ˆTIMï¼‰åŠå…¶è¿è¡Œæ—¶ï¼ˆTIMRUNï¼‰ï¼Œæ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨ç†å‡†ç¡®æ€§å’Œæ•ˆç‡ä¸Šçš„ä¸Šä¸‹æ–‡é™åˆ¶ã€‚TIMé€šè¿‡ä½¿ç”¨æ¨ç†æ ‘å’Œå…³é”®å€¼çŠ¶æ€ä¿ç•™ï¼Œæ”¯æŒé€’å½’å’Œåˆ†è§£é—®é¢˜çš„è§£å†³ï¼Œä»è€Œå®ç°é•¿æ—¶é—´è·¨åº¦çš„æ¨ç†ã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿåœ¨å•æ¬¡æ¨ç†ä¸­æ”¯æŒå‡ ä¹æ— é™çš„å·¥ä½œè®°å¿†å’Œå¤šè·³å·¥å…·è°ƒç”¨ï¼Œå…‹æœäº†è¾“å‡ºé™åˆ¶å’ŒGPUå†…å­˜ç“¶é¢ˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿåœ¨å¤„ç†æ•°å­¦ä»»åŠ¡å’Œä¿¡æ¯æ£€ç´¢æŒ‘æˆ˜æ—¶ï¼Œèƒ½å¤Ÿä¿æŒé«˜æ¨ç†ååé‡ï¼Œå¹¶æä¾›å‡†ç¡®çš„æ¨ç†èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.16632', 'title': 'Step-Audio 2 Technical Report', 'url': 'https://huggingface.co/papers/2507.16632', 'abstract': 'This paper presents Step-Audio~2, an end-to-end multi-modal large language model designed for industry-strength audio understanding and speech conversation. By integrating a latent audio encoder and reasoning-centric reinforcement learning (RL), Step-Audio 2 achieves promising performance in automatic speech recognition (ASR) and audio understanding. To facilitate genuine end-to-end speech conversation, Step-Audio 2 incorporates the generation of discrete audio tokens into language modeling, significantly enhancing its responsiveness to paralinguistic information such as speaking styles and emotions. To effectively leverage the rich textual and acoustic knowledge in real-world data, Step-Audio 2 integrates retrieval-augmented generation (RAG) and is able to call external tools such as web search to mitigate hallucination and audio search to switch timbres. Trained on millions of hours of speech and audio data, Step-Audio 2 delivers intelligence and expressiveness across diverse conversational scenarios. Evaluation results demonstrate that Step-Audio 2 achieves state-of-the-art performance on various audio understanding and conversational benchmarks compared to other open-source and commercial solutions. Please visit https://github.com/stepfun-ai/Step-Audio2 for more information.', 'score': 24, 'issue_id': 4960, 'pub_date': '2025-07-22', 'pub_date_card': {'ru': '22 Ğ¸ÑĞ»Ñ', 'en': 'July 22', 'zh': '7æœˆ22æ—¥'}, 'hash': '09237169b94ecf48', 'authors': ['Boyong Wu', 'Chao Yan', 'Chen Hu', 'Cheng Yi', 'Chengli Feng', 'Fei Tian', 'Feiyu Shen', 'Gang Yu', 'Haoyang Zhang', 'Jingbei Li', 'Mingrui Chen', 'Peng Liu', 'Wang You', 'Xiangyu Tony Zhang', 'Xingyuan Li', 'Xuerui Yang', 'Yayue Deng', 'Yechang Huang', 'Yuxin Li', 'Yuxin Zhang', 'Zhao You', 'Brian Li', 'Changyi Wan', 'Hanpeng Hu', 'Jiangjie Zhen', 'Siyu Chen', 'Song Yuan', 'Xuelin Zhang', 'Yimin Jiang', 'Yu Zhou', 'Yuxiang Yang', 'Bingxin Li', 'Buyun Ma', 'Changhe Song', 'Dongqing Pang', 'Guoqiang Hu', 'Haiyang Sun', 'Kang An', 'Na Wang', 'Shuli Gao', 'Wei Ji', 'Wen Li', 'Wen Sun', 'Xuan Wen', 'Yong Ren', 'Yuankai Ma', 'Yufan Lu', 'Bin Wang', 'Bo Li', 'Changxin Miao', 'Che Liu', 'Chen Xu', 'Dapeng Shi', 'Dingyuan Hu', 'Donghang Wu', 'Enle Liu', 'Guanzhe Huang', 'Gulin Yan', 'Han Zhang', 'Hao Nie', 'Haonan Jia', 'Hongyu Zhou', 'Jianjian Sun', 'Jiaoren Wu', 'Jie Wu', 'Jie Yang', 'Jin Yang', 'Junzhe Lin', 'Kaixiang Li', 'Lei Yang', 'Liying Shi', 'Li Zhou', 'Longlong Gu', 'Ming Li', 'Mingliang Li', 'Mingxiao Li', 'Nan Wu', 'Qi Han', 'Qinyuan Tan', 'Shaoliang Pang', 'Shengjie Fan', 'Siqi Liu', 'Tiancheng Cao', 'Wanying Lu', 'Wenqing He', 'Wuxun Xie', 'Xu Zhao', 'Xueqi Li', 'Yanbo Yu', 'Yang Yang', 'Yi Liu', 'Yifan Lu', 'Yilei Wang', 'Yuanhao Ding', 'Yuanwei Liang', 'Yuanwei Lu', 'Yuchu Luo', 'Yuhe Yin', 'Yumeng Zhan', 'Yuxiang Zhang', 'Zidong Yang', 'Zixin Zhang', 'Binxing Jiao', 'Daxin Jiang', 'Heung-Yeung Shum', 'Jiansheng Chen', 'Jing Li', 'Xiangyu Zhang', 'Yibo Zhu'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2507.16632.jpg', 'data': {'categories': ['#benchmark', '#multimodal', '#agi', '#hallucinations', '#reasoning', '#rl', '#audio', '#open_source', '#rag'], 'emoji': 'ğŸ™ï¸', 'ru': {'title': 'Ğ˜Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ Ñ€ĞµÑ‡Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸', 'desc': 'Step-Audio 2 - ÑÑ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ Ñ€ĞµÑ‡ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ğ°ÑƒĞ´Ğ¸Ğ¾ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸Ğº Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑ‡Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ°ÑƒĞ´Ğ¸Ğ¾Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ. Step-Audio 2 Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ñ‰Ğ°Ñ‚ÑŒÑÑ Ğº Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼ Ğ´Ğ»Ñ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹.'}, 'en': {'title': 'Revolutionizing Audio Understanding with Step-Audio 2', 'desc': 'Step-Audio 2 is a powerful multi-modal large language model that excels in understanding audio and facilitating speech conversations. It uses a latent audio encoder combined with reinforcement learning to improve automatic speech recognition and audio comprehension. The model enhances its ability to respond to different speaking styles and emotions by generating discrete audio tokens within its language framework. Additionally, it employs retrieval-augmented generation to access external information, reducing errors and improving its performance in real-world applications.'}, 'zh': {'title': 'Step-Audio 2ï¼šéŸ³é¢‘ç†è§£ä¸å¯¹è¯çš„æœªæ¥', 'desc': 'æœ¬æ–‡ä»‹ç»äº†Step-Audio 2ï¼Œè¿™æ˜¯ä¸€ç§ç«¯åˆ°ç«¯çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨å®ç°å·¥ä¸šçº§çš„éŸ³é¢‘ç†è§£å’Œè¯­éŸ³å¯¹è¯ã€‚é€šè¿‡é›†æˆæ½œåœ¨éŸ³é¢‘ç¼–ç å™¨å’Œä»¥æ¨ç†ä¸ºä¸­å¿ƒçš„å¼ºåŒ–å­¦ä¹ ï¼ŒStep-Audio 2åœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰å’ŒéŸ³é¢‘ç†è§£æ–¹é¢å–å¾—äº†è‰¯å¥½çš„æ€§èƒ½ã€‚ä¸ºäº†å®ç°çœŸæ­£çš„ç«¯åˆ°ç«¯è¯­éŸ³å¯¹è¯ï¼ŒStep-Audio 2å°†ç¦»æ•£éŸ³é¢‘æ ‡è®°çš„ç”Ÿæˆçº³å…¥è¯­è¨€å»ºæ¨¡ä¸­ï¼Œæ˜¾è‘—å¢å¼ºäº†å¯¹è¯´è¯é£æ ¼å’Œæƒ…æ„Ÿç­‰å‰¯è¯­è¨€ä¿¡æ¯çš„å“åº”èƒ½åŠ›ã€‚ç»è¿‡æ•°ç™¾ä¸‡å°æ—¶çš„è¯­éŸ³å’ŒéŸ³é¢‘æ•°æ®è®­ç»ƒï¼ŒStep-Audio 2åœ¨å„ç§å¯¹è¯åœºæ™¯ä¸­å±•ç°å‡ºæ™ºèƒ½å’Œè¡¨ç°åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.08422', 'title': 'Upsample What Matters: Region-Adaptive Latent Sampling for Accelerated\n  Diffusion Transformers', 'url': 'https://huggingface.co/papers/2507.08422', 'abstract': 'Diffusion transformers have emerged as an alternative to U-net-based diffusion models for high-fidelity image and video generation, offering superior scalability. However, their heavy computation remains a major obstacle to real-world deployment. Existing acceleration methods primarily exploit the temporal dimension such as reusing cached features across diffusion timesteps. Here, we propose Region-Adaptive Latent Upsampling (RALU), a training-free framework that accelerates inference along spatial dimension. RALU performs mixed-resolution sampling across three stages: 1) low-resolution denoising latent diffusion to efficiently capture global semantic structure, 2) region-adaptive upsampling on specific regions prone to artifacts at full-resolution, and 3) all latent upsampling at full-resolution for detail refinement. To stabilize generations across resolution transitions, we leverage noise-timestep rescheduling to adapt the noise level across varying resolutions. Our method significantly reduces computation while preserving image quality by achieving up to 7.0times speed-up on FLUX and 3.0times on Stable Diffusion 3 with minimal degradation. Furthermore, RALU is complementary to existing temporal accelerations such as caching methods, thus can be seamlessly integrated to further reduce inference latency without compromising generation quality.', 'score': 21, 'issue_id': 4964, 'pub_date': '2025-07-11', 'pub_date_card': {'ru': '11 Ğ¸ÑĞ»Ñ', 'en': 'July 11', 'zh': '7æœˆ11æ—¥'}, 'hash': '63043ce148fb06e1', 'authors': ['Wongi Jeong', 'Kyungryeol Lee', 'Hoigi Seo', 'Se Young Chun'], 'affiliations': ['Dept. of Electrical and Computer Engineering, Seoul National University, Republic of Korea', 'IPAI & INMC, Seoul National University, Republic of Korea'], 'pdf_title_img': 'assets/pdf/title_img/2507.08422.jpg', 'data': {'categories': ['#inference', '#diffusion', '#optimization', '#cv'], 'emoji': 'ğŸš€', 'ru': {'title': 'Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² - Region-Adaptive Latent Upsampling (RALU). RALU Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ğ¿Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ¸ ÑÑ‚Ğ°Ğ´Ğ¸Ğ¸: Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ½ÑƒÑ Ğ´ĞµĞ½Ğ¾Ğ¹Ğ·Ğ¸Ğ½Ğ³ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½ÑƒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ñ, Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ… Ğ¸ Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ²ÑĞµĞ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. RALU Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ´Ğ¾ 7 Ñ€Ğ°Ğ· Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ FLUX Ğ¸ Ğ´Ğ¾ 3 Ñ€Ğ°Ğ· Ğ´Ğ»Ñ Stable Diffusion 3 Ğ¿Ñ€Ğ¸ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ¾Ñ‚ĞµÑ€Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°.'}, 'en': {'title': 'Accelerating Diffusion Transformers with RALU for High-Quality Generation', 'desc': 'This paper introduces Region-Adaptive Latent Upsampling (RALU), a novel framework designed to enhance the efficiency of diffusion transformers in image and video generation. RALU accelerates inference by focusing on the spatial dimension, utilizing a three-stage process that includes low-resolution denoising, region-specific upsampling, and full-resolution detail refinement. The method also incorporates noise-timestep rescheduling to maintain stability during resolution transitions, ensuring high-quality outputs. Overall, RALU achieves significant computation speed-ups while preserving image fidelity, making it a valuable addition to existing acceleration techniques.'}, 'zh': {'title': 'åŒºåŸŸè‡ªé€‚åº”æ½œåœ¨ä¸Šé‡‡æ ·ï¼šåŠ é€Ÿå›¾åƒç”Ÿæˆçš„æ–°æ–¹æ³•', 'desc': 'æ‰©æ•£å˜æ¢å™¨ä½œä¸ºé«˜ä¿çœŸå›¾åƒå’Œè§†é¢‘ç”Ÿæˆçš„æ›¿ä»£æ–¹æ¡ˆï¼Œæä¾›äº†æ›´å¥½çš„å¯æ‰©å±•æ€§ï¼Œä½†å…¶è®¡ç®—é‡å¤§ä»ç„¶æ˜¯å®é™…åº”ç”¨çš„éšœç¢ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºåŒºåŸŸè‡ªé€‚åº”æ½œåœ¨ä¸Šé‡‡æ ·ï¼ˆRALUï¼‰çš„æ¡†æ¶ï¼Œæ—¨åœ¨æ²¿ç©ºé—´ç»´åº¦åŠ é€Ÿæ¨ç†ï¼Œè€Œæ— éœ€è®­ç»ƒã€‚RALUé€šè¿‡ä¸‰ä¸ªé˜¶æ®µè¿›è¡Œæ··åˆåˆ†è¾¨ç‡é‡‡æ ·ï¼Œé¦–å…ˆåœ¨ä½åˆ†è¾¨ç‡ä¸‹å»å™ªä»¥æ•æ‰å…¨å±€è¯­ä¹‰ç»“æ„ï¼Œç„¶ååœ¨ç‰¹å®šåŒºåŸŸè¿›è¡Œè‡ªé€‚åº”ä¸Šé‡‡æ ·ï¼Œæœ€ååœ¨å…¨åˆ†è¾¨ç‡ä¸‹è¿›è¡Œç»†èŠ‚ä¼˜åŒ–ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼ŒRALUåœ¨ä¿æŒå›¾åƒè´¨é‡çš„åŒæ—¶ï¼Œæ˜¾è‘—å‡å°‘äº†è®¡ç®—é‡ï¼Œèƒ½å¤Ÿå®ç°é«˜è¾¾7å€çš„åŠ é€Ÿã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.16812', 'title': 'MegaScience: Pushing the Frontiers of Post-Training Datasets for Science\n  Reasoning', 'url': 'https://huggingface.co/papers/2507.16812', 'abstract': 'MegaScience, a large-scale dataset of scientific reasoning questions, enhances the performance and training efficiency of AI models compared to existing datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t Scientific reasoning is critical for developing AI scientists and supporting human researchers in advancing the frontiers of natural science discovery. However, the open-source community has primarily focused on mathematics and coding while neglecting the scientific domain, largely due to the absence of open, large-scale, high-quality, verifiable scientific reasoning datasets. To bridge this gap, we first present TextbookReasoning, an open dataset featuring truthful reference answers extracted from 12k university-level scientific textbooks, comprising 650k reasoning questions spanning 7 scientific disciplines. We further introduce MegaScience, a large-scale mixture of high-quality open-source datasets totaling 1.25 million instances, developed through systematic ablation studies that evaluate various data selection methodologies to identify the optimal subset for each publicly available scientific dataset. Meanwhile, we build a comprehensive evaluation system covering diverse subjects and question types across 15 benchmarks, incorporating comprehensive answer extraction strategies to ensure accurate evaluation metrics. Our experiments demonstrate that our datasets achieve superior performance and training efficiency with more concise response lengths compared to existing open-source scientific datasets. Furthermore, we train Llama3.1, Qwen2.5, and Qwen3 series base models on MegaScience, which significantly outperform the corresponding official instruct models in average performance. In addition, MegaScience exhibits greater effectiveness for larger and stronger models, suggesting a scaling benefit for scientific tuning. We release our data curation pipeline, evaluation system, datasets, and seven trained models to the community to advance scientific reasoning research.', 'score': 19, 'issue_id': 4959, 'pub_date': '2025-07-22', 'pub_date_card': {'ru': '22 Ğ¸ÑĞ»Ñ', 'en': 'July 22', 'zh': '7æœˆ22æ—¥'}, 'hash': 'e6653e3f0a1b904f', 'authors': ['Run-Ze Fan', 'Zengzhi Wang', 'Pengfei Liu'], 'affiliations': ['Shanghai Jiao Tong University, SII, GAIR Lab'], 'pdf_title_img': 'assets/pdf/title_img/2507.16812.jpg', 'data': {'categories': ['#benchmark', '#science', '#data', '#open_source', '#reasoning', '#dataset'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'MegaScience: Ğ¿Ñ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ¼ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğ¸ Ğ˜Ğ˜', 'desc': 'MegaScience - ÑÑ‚Ğ¾ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‰Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ˜Ğ˜ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ 1,25 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¸Ğ· 7 Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ´Ğ¸ÑÑ†Ğ¸Ğ¿Ğ»Ğ¸Ğ½, Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ· ÑƒÑ‡ĞµĞ±Ğ½Ğ¸ĞºĞ¾Ğ² ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ¸Ñ‚ĞµÑ‚ÑĞºĞ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰ÑƒÑ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ñ‹ Ğ¸ Ñ‚Ğ¸Ğ¿Ñ‹ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¿Ğ¾ 15 ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Ñ‚ĞµÑÑ‚Ğ°Ğ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° MegaScience, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ¾Ñ„Ğ¸Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ ÑÑ€ĞµĞ´Ğ½ĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Empowering AI with MegaScience for Superior Scientific Reasoning', 'desc': "This paper introduces MegaScience, a large-scale dataset designed to improve AI models' performance in scientific reasoning tasks. It addresses the lack of high-quality, open-source datasets in the scientific domain by providing 1.25 million instances of reasoning questions derived from university-level textbooks. The authors conducted systematic studies to select the best data subsets, ensuring that the dataset is both comprehensive and effective for training AI models. Their experiments show that models trained on MegaScience outperform existing models, highlighting its potential to enhance scientific reasoning capabilities in AI."}, 'zh': {'title': 'MegaScienceï¼šæ¨åŠ¨ç§‘å­¦æ¨ç†çš„æœªæ¥', 'desc': 'MegaScienceæ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„ç§‘å­¦æ¨ç†é—®é¢˜æ•°æ®é›†ï¼Œæ—¨åœ¨æé«˜äººå·¥æ™ºèƒ½æ¨¡å‹çš„æ€§èƒ½å’Œè®­ç»ƒæ•ˆç‡ã€‚è¯¥æ•°æ®é›†åŒ…å«ä»12000æœ¬å¤§å­¦çº§ç§‘å­¦æ•™ç§‘ä¹¦ä¸­æå–çš„çœŸå®å‚è€ƒç­”æ¡ˆï¼Œæ¶µç›–650000ä¸ªæ¨ç†é—®é¢˜ï¼Œæ¶‰åŠ7ä¸ªç§‘å­¦å­¦ç§‘ã€‚é€šè¿‡ç³»ç»Ÿçš„æ¶ˆèç ”ç©¶ï¼Œæˆ‘ä»¬å¼€å‘äº†1.25ç™¾ä¸‡å®ä¾‹çš„é«˜è´¨é‡å¼€æ”¾æºæ•°æ®é›†ï¼Œå¹¶å»ºç«‹äº†å…¨é¢çš„è¯„ä¼°ç³»ç»Ÿï¼Œä»¥ç¡®ä¿å‡†ç¡®çš„è¯„ä¼°æŒ‡æ ‡ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒMegaScienceåœ¨è®­ç»ƒæ•ˆç‡å’Œå“åº”é•¿åº¦ä¸Šä¼˜äºç°æœ‰çš„å¼€æ”¾æºç§‘å­¦æ•°æ®é›†ï¼Œç‰¹åˆ«é€‚åˆæ›´å¤§å’Œæ›´å¼ºçš„æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.16814', 'title': 'Semi-off-Policy Reinforcement Learning for Vision-Language Slow-thinking\n  Reasoning', 'url': 'https://huggingface.co/papers/2507.16814', 'abstract': 'Enhancing large vision-language models (LVLMs) with visual slow-thinking reasoning is crucial for solving complex multimodal tasks. However, since LVLMs are mainly trained with vision-language alignment, it is difficult to adopt on-policy reinforcement learning (RL) to develop the slow thinking ability because the rollout space is restricted by its initial abilities. Off-policy RL offers a way to go beyond the current policy, but directly distilling trajectories from external models may cause visual hallucinations due to mismatched visual perception abilities across models. To address these issues, this paper proposes SOPHIA, a simple and scalable Semi-Off-Policy RL for vision-language slow-tHInking reAsoning. SOPHIA builds a semi-off-policy behavior model by combining on-policy visual understanding from a trainable LVLM with off-policy slow-thinking reasoning from a language model, assigns outcome-based rewards to reasoning, and propagates visual rewards backward. Then LVLM learns slow-thinking reasoning ability from the obtained reasoning trajectories using propagated rewards via off-policy RL algorithms. Extensive experiments with InternVL2.5 and InternVL3.0 with 8B and 38B sizes show the effectiveness of SOPHIA. Notably, SOPHIA improves InternVL3.0-38B by 8.50% in average, reaching state-of-the-art performance among open-source LVLMs on multiple multimodal reasoning benchmarks, and even outperforms some closed-source models (e.g., GPT-4.1) on the challenging MathVision and OlympiadBench, achieving 49.08% and 49.95% pass@1 accuracy, respectively. Analysis shows SOPHIA outperforms supervised fine-tuning and direct on-policy RL methods, offering a better policy initialization for further on-policy training.', 'score': 16, 'issue_id': 4961, 'pub_date': '2025-07-22', 'pub_date_card': {'ru': '22 Ğ¸ÑĞ»Ñ', 'en': 'July 22', 'zh': '7æœˆ22æ—¥'}, 'hash': 'cbe613b2436dd0f8', 'authors': ['Junhao Shen', 'Haiteng Zhao', 'Yuzhe Gu', 'Songyang Gao', 'Kuikun Liu', 'Haian Huang', 'Jianfei Gao', 'Dahua Lin', 'Wenwei Zhang', 'Kai Chen'], 'affiliations': ['MMLab, The Chinese University of Hong Kong', 'Shanghai AI Laboratory', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2507.16814.jpg', 'data': {'categories': ['#training', '#open_source', '#benchmark', '#multimodal', '#reasoning', '#optimization', '#rl'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'SOPHIA: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğ¸ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ SOPHIA - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. SOPHIA ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ· Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ LVLM Ñ Ğ¼ĞµĞ´Ğ»ĞµĞ½Ğ½Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ· ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¿Ğ¾Ğ»Ñƒ-Ğ¾Ñ„Ñ„-Ğ¿Ğ¾Ğ»Ğ¸ÑĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ» Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾Ğ¹Ğ´Ñ Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° SOPHIA Ğ¿ĞµÑ€ĞµĞ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ Ğ¸ Ğ¿Ñ€ÑĞ¼Ñ‹Ğ¼Ğ¸ Ğ¾Ğ½-Ğ¿Ğ¾Ğ»Ğ¸ÑĞ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼.'}, 'en': {'title': 'SOPHIA: Elevating LVLMs with Semi-Off-Policy Reasoning', 'desc': "This paper introduces SOPHIA, a novel Semi-Off-Policy Reinforcement Learning (RL) approach designed to enhance large vision-language models (LVLMs) by incorporating slow-thinking reasoning capabilities. The challenge with traditional on-policy RL is that it limits the model's ability to explore beyond its initial training, while off-policy RL can lead to visual inconsistencies when using external models. SOPHIA effectively combines on-policy visual understanding from LVLMs with off-policy reasoning from language models, allowing for better learning through outcome-based rewards. Experimental results demonstrate that SOPHIA significantly improves performance on multimodal reasoning tasks, surpassing both open-source and some closed-source models."}, 'zh': {'title': 'SOPHIAï¼šæå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„æ…¢æ€ç»´æ¨ç†èƒ½åŠ›', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºSOPHIAçš„åŠç¦»çº¿å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œç”¨äºå¢å¼ºå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMï¼‰çš„æ…¢æ€ç»´æ¨ç†èƒ½åŠ›ã€‚SOPHIAç»“åˆäº†æ¥è‡ªå¯è®­ç»ƒLVLMçš„åœ¨çº¿è§†è§‰ç†è§£å’Œæ¥è‡ªè¯­è¨€æ¨¡å‹çš„ç¦»çº¿æ…¢æ€ç»´æ¨ç†ï¼Œåˆ©ç”¨ç»“æœå¯¼å‘çš„å¥–åŠ±æ¥ä¿ƒè¿›æ¨ç†è¿‡ç¨‹ã€‚é€šè¿‡è¿™ç§æ–¹æ³•ï¼ŒLVLMèƒ½å¤Ÿä»è·å¾—çš„æ¨ç†è½¨è¿¹ä¸­å­¦ä¹ æ…¢æ€ç»´æ¨ç†èƒ½åŠ›ï¼Œå¹¶é€šè¿‡ç¦»çº¿å¼ºåŒ–å­¦ä¹ ç®—æ³•ä¼ æ’­å¥–åŠ±ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSOPHIAåœ¨å¤šä¸ªå¤šæ¨¡æ€æ¨ç†åŸºå‡†ä¸Šæ˜¾è‘—æé«˜äº†æ¨¡å‹æ€§èƒ½ï¼Œè¶…è¶Šäº†è®¸å¤šç°æœ‰çš„å¼€æºå’Œé—­æºæ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.16815', 'title': 'ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent\n  Planning', 'url': 'https://huggingface.co/papers/2507.16815', 'abstract': 'ThinkAct, a dual-system framework, uses reinforced visual latent planning to enable high-level reasoning and robust action execution in vision-language-action tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-language-action (VLA) reasoning tasks require agents to interpret multimodal instructions, perform long-horizon planning, and act adaptively in dynamic environments. Existing approaches typically train VLA models in an end-to-end fashion, directly mapping inputs to actions without explicit reasoning, which hinders their ability to plan over multiple steps or adapt to complex task variations. In this paper, we propose ThinkAct, a dual-system framework that bridges high-level reasoning with low-level action execution via reinforced visual latent planning. ThinkAct trains a multimodal LLM to generate embodied reasoning plans guided by reinforcing action-aligned visual rewards based on goal completion and trajectory consistency. These reasoning plans are compressed into a visual plan latent that conditions a downstream action model for robust action execution on target environments. Extensive experiments on embodied reasoning and robot manipulation benchmarks demonstrate that ThinkAct enables few-shot adaptation, long-horizon planning, and self-correction behaviors in complex embodied AI tasks.', 'score': 11, 'issue_id': 4959, 'pub_date': '2025-07-22', 'pub_date_card': {'ru': '22 Ğ¸ÑĞ»Ñ', 'en': 'July 22', 'zh': '7æœˆ22æ—¥'}, 'hash': 'a0c0c8cc661a52b5', 'authors': ['Chi-Pin Huang', 'Yueh-Hua Wu', 'Min-Hung Chen', 'Yu-Chiang Frank Wang', 'Fu-En Yang'], 'affiliations': ['NVIDIA', 'National Taiwan University'], 'pdf_title_img': 'assets/pdf/title_img/2507.16815.jpg', 'data': {'categories': ['#agents', '#optimization', '#robotics', '#training', '#multimodal', '#reasoning'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ”ÑƒĞ¼Ğ°Ğ¹ Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²ÑƒĞ¹: Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜', 'desc': 'ThinkAct - ÑÑ‚Ğ¾ Ğ´Ğ²ÑƒÑ…ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. ThinkAct Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ»Ğ°Ğ½Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ¾ÑĞ½Ğ¾Ğ²Ñ‹Ğ²Ğ°ÑÑÑŒ Ğ½Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°Ñ…, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑĞ¼Ğ¸. Ğ­Ñ‚Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ñ‹ ÑĞ¶Ğ¸Ğ¼Ğ°ÑÑ‚ÑÑ Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ğ¿Ğ»Ğ°Ğ½, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ² Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ ÑÑ€ĞµĞ´Ğµ.'}, 'en': {'title': 'ThinkAct: Bridging Reasoning and Action in AI Tasks', 'desc': 'ThinkAct is a novel framework designed for vision-language-action tasks that combines high-level reasoning with low-level action execution. It utilizes reinforced visual latent planning to create effective plans that guide agents in dynamic environments. By training a multimodal large language model (LLM), ThinkAct generates reasoning plans that are optimized through visual rewards, ensuring actions align with goals. The framework shows significant improvements in few-shot adaptation and long-horizon planning, making it suitable for complex AI tasks like robot manipulation.'}, 'zh': {'title': 'ThinkActï¼šé«˜æ•ˆæ¨ç†ä¸åŠ¨ä½œæ‰§è¡Œçš„åŒç³»ç»Ÿæ¡†æ¶', 'desc': 'ThinkActæ˜¯ä¸€ä¸ªåŒç³»ç»Ÿæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å¼ºåŒ–è§†è§‰æ½œåœ¨è§„åˆ’å®ç°é«˜æ°´å¹³æ¨ç†å’Œç¨³å¥çš„åŠ¨ä½œæ‰§è¡Œã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿå¤„ç†è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰ä»»åŠ¡ï¼Œå¸®åŠ©æ™ºèƒ½ä½“ç†è§£å¤šæ¨¡æ€æŒ‡ä»¤å¹¶è¿›è¡Œé•¿è¿œè§„åˆ’ã€‚ä¸ä¼ ç»Ÿçš„ç«¯åˆ°ç«¯è®­ç»ƒæ–¹æ³•ä¸åŒï¼ŒThinkActé€šè¿‡ç”Ÿæˆä¸åŠ¨ä½œå¯¹é½çš„è§†è§‰å¥–åŠ±æ¥æŒ‡å¯¼æ¨ç†è®¡åˆ’ï¼Œä»è€Œæé«˜äº†æ™ºèƒ½ä½“åœ¨å¤æ‚ç¯å¢ƒä¸­çš„é€‚åº”èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒThinkActåœ¨å°‘é‡æ ·æœ¬é€‚åº”ã€é•¿è¿œè§„åˆ’å’Œè‡ªæˆ‘ä¿®æ­£è¡Œä¸ºæ–¹é¢è¡¨ç°å‡ºè‰²ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.16813', 'title': 'HOComp: Interaction-Aware Human-Object Composition', 'url': 'https://huggingface.co/papers/2507.16813', 'abstract': 'HOComp uses MLLMs and attention mechanisms to achieve seamless human-object interactions with consistent appearances in image compositing.  \t\t\t\t\tAI-generated summary \t\t\t\t While existing image-guided composition methods may help insert a foreground object onto a user-specified region of a background image, achieving natural blending inside the region with the rest of the image unchanged, we observe that these existing methods often struggle in synthesizing seamless interaction-aware compositions when the task involves human-object interactions. In this paper, we first propose HOComp, a novel approach for compositing a foreground object onto a human-centric background image, while ensuring harmonious interactions between the foreground object and the background person and their consistent appearances. Our approach includes two key designs: (1) MLLMs-driven Region-based Pose Guidance (MRPG), which utilizes MLLMs to identify the interaction region as well as the interaction type (e.g., holding and lefting) to provide coarse-to-fine constraints to the generated pose for the interaction while incorporating human pose landmarks to track action variations and enforcing fine-grained pose constraints; and (2) Detail-Consistent Appearance Preservation (DCAP), which unifies a shape-aware attention modulation mechanism, a multi-view appearance loss, and a background consistency loss to ensure consistent shapes/textures of the foreground and faithful reproduction of the background human. We then propose the first dataset, named Interaction-aware Human-Object Composition (IHOC), for the task. Experimental results on our dataset show that HOComp effectively generates harmonious human-object interactions with consistent appearances, and outperforms relevant methods qualitatively and quantitatively.', 'score': 9, 'issue_id': 4959, 'pub_date': '2025-07-22', 'pub_date_card': {'ru': '22 Ğ¸ÑĞ»Ñ', 'en': 'July 22', 'zh': '7æœˆ22æ—¥'}, 'hash': '5a513d80af038052', 'authors': ['Dong Liang', 'Jinyuan Jia', 'Yuhao Liu', 'Rynson W. H. Lau'], 'affiliations': ['CityUHK', 'HKUST(GZ)', 'Tongji University'], 'pdf_title_img': 'assets/pdf/title_img/2507.16813.jpg', 'data': {'categories': ['#cv', '#synthetic', '#games', '#dataset'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'Ğ“Ğ°Ñ€Ğ¼Ğ¾Ğ½Ğ¸Ñ‡Ğ½Ğ°Ñ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°', 'desc': 'HOComp - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ³Ğ°Ñ€Ğ¼Ğ¾Ğ½Ğ¸Ñ‡Ğ½Ğ¾Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ¼ Ğ¿ĞµÑ€ĞµĞ´Ğ½ĞµĞ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ° Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¼ Ğ½Ğ° Ñ„Ğ¾Ğ½Ğ¾Ğ²Ğ¾Ğ¼ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¸. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (MLLM) Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ° Ğ¸ Ñ‚Ğ¸Ğ¿Ğ° Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ³Ğ¾ Ğ²Ğ¸Ğ´Ğ°. HOComp Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¹ Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ĞµĞ¼ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… IHOC Ğ´Ğ»Ñ ÑÑ‚Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸.'}, 'en': {'title': 'Seamless Human-Object Interaction in Image Compositing', 'desc': 'HOComp is a new method that enhances how foreground objects interact with people in images, ensuring they blend naturally into the scene. It uses Multi-Layered Language Models (MLLMs) to guide the placement and pose of objects based on the type of interaction, like holding or lifting. Additionally, it employs a technique called Detail-Consistent Appearance Preservation (DCAP) to maintain the visual consistency of shapes and textures between the foreground and background. The paper also introduces a new dataset, Interaction-aware Human-Object Composition (IHOC), to evaluate the effectiveness of HOComp, which shows significant improvements over existing methods.'}, 'zh': {'title': 'æ— ç¼äººæœºäº’åŠ¨çš„å›¾åƒåˆæˆæ–°æ–¹æ³•', 'desc': 'HOCompæ˜¯ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œæ—¨åœ¨å°†å‰æ™¯ç‰©ä½“æ— ç¼åœ°åˆæˆåˆ°ä»¥äººä¸ºä¸­å¿ƒçš„èƒŒæ™¯å›¾åƒä¸­ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å¤šè¯­è¨€å¤§æ¨¡å‹ï¼ˆMLLMsï¼‰å’Œæ³¨æ„åŠ›æœºåˆ¶ï¼Œç¡®ä¿å‰æ™¯ç‰©ä½“ä¸èƒŒæ™¯äººç‰©ä¹‹é—´çš„å’Œè°äº’åŠ¨åŠä¸€è‡´çš„å¤–è§‚ã€‚å…¶æ ¸å¿ƒè®¾è®¡åŒ…æ‹¬åŸºäºåŒºåŸŸçš„å§¿æ€å¼•å¯¼å’Œç»†èŠ‚ä¸€è‡´çš„å¤–è§‚ä¿ç•™ï¼Œå‰è€…å¸®åŠ©è¯†åˆ«äº’åŠ¨åŒºåŸŸå’Œç±»å‹ï¼Œåè€…ç¡®ä¿å‰æ™¯å’ŒèƒŒæ™¯çš„ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒHOCompåœ¨ç”Ÿæˆè‡ªç„¶çš„äººç‰©ä¸ç‰©ä½“äº’åŠ¨æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†ç›¸å…³æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.16746', 'title': 'Zebra-CoT: A Dataset for Interleaved Vision Language Reasoning', 'url': 'https://huggingface.co/papers/2507.16746', 'abstract': "Humans often use visual aids, for example diagrams or sketches, when solving complex problems. Training multimodal models to do the same, known as Visual Chain of Thought (Visual CoT), is challenging due to: (1) poor off-the-shelf visual CoT performance, which hinders reinforcement learning, and (2) the lack of high-quality visual CoT training data. We introduce Zebra-CoT, a diverse large-scale dataset with 182,384 samples, containing logically coherent interleaved text-image reasoning traces. We focus on four categories of tasks where sketching or visual reasoning is especially natural, spanning scientific questions such as geometry, physics, and algorithms; 2D visual reasoning tasks like visual search and jigsaw puzzles; 3D reasoning tasks including 3D multi-hop inference, embodied and robot planning; visual logic problems and strategic games like chess. Fine-tuning the Anole-7B model on the Zebra-CoT training corpus results in an improvement of +12% in our test-set accuracy and yields up to +13% performance gain on standard VLM benchmark evaluations. Fine-tuning Bagel-7B yields a model that generates high-quality interleaved visual reasoning chains, underscoring Zebra-CoT's effectiveness for developing multimodal reasoning abilities. We open-source our dataset and models to support development and evaluation of visual CoT.", 'score': 8, 'issue_id': 4959, 'pub_date': '2025-07-22', 'pub_date_card': {'ru': '22 Ğ¸ÑĞ»Ñ', 'en': 'July 22', 'zh': '7æœˆ22æ—¥'}, 'hash': '195867d3f8c130bf', 'authors': ['Ang Li', 'Charles Wang', 'Kaiyu Yue', 'Zikui Cai', 'Ollie Liu', 'Deqing Fu', 'Peng Guo', 'Wang Bill Zhu', 'Vatsal Sharan', 'Robin Jia', 'Willie Neiswanger', 'Furong Huang', 'Tom Goldstein', 'Micah Goldblum'], 'affiliations': ['Columbia University', 'New York University', 'University of Maryland', 'University of Southern California'], 'pdf_title_img': 'assets/pdf/title_img/2507.16746.jpg', 'data': {'categories': ['#rl', '#benchmark', '#games', '#optimization', '#multimodal', '#open_source', '#cv', '#dataset'], 'emoji': 'ğŸ¦“', 'ru': {'title': 'Zebra-CoT: ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ˜Ğ˜ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Zebra-CoT - ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ°Ğ±Ğ¾Ñ€ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 182,384 Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ° Ñ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾-Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ°Ğ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹, 2D Ğ¸ 3D Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ, Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸Ğ³Ñ€Ñ‹. Ğ¤Ğ°Ğ¹Ğ½Ñ‚ÑĞ½Ğ¸Ğ½Ğ³ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Anole-7B Ğ½Ğ° ÑÑ‚Ğ¾Ğ¼ ĞºĞ¾Ñ€Ğ¿ÑƒÑĞµ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ğ» Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° 12% Ğ² Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ¸ Ğ´Ğ¾ 13% Ğ² ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… VLM. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ»Ğ¸ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ Ğº Ğ½Ğ°Ğ±Ğ¾Ñ€Ñƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Zebra-CoT: Enhancing Visual Reasoning with a Rich Dataset', 'desc': "This paper presents Zebra-CoT, a large dataset designed to enhance multimodal models' ability to perform visual reasoning tasks. It addresses the challenges of poor performance in existing visual chain of thought (CoT) models and the scarcity of quality training data. The dataset includes 182,384 samples that combine text and images for various reasoning tasks, such as geometry and strategic games. Fine-tuning models like Anole-7B and Bagel-7B on this dataset significantly improves their accuracy and ability to generate coherent visual reasoning chains."}, 'zh': {'title': 'Zebra-CoTï¼šæå‡å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›çš„å…³é”®æ•°æ®é›†', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºZebra-CoTçš„å¤§è§„æ¨¡æ•°æ®é›†ï¼ŒåŒ…å«182,384ä¸ªæ ·æœ¬ï¼Œæ—¨åœ¨å¸®åŠ©å¤šæ¨¡æ€æ¨¡å‹è¿›è¡Œè§†è§‰æ¨ç†ã€‚è¯¥æ•°æ®é›†æä¾›äº†é€»è¾‘è¿è´¯çš„æ–‡æœ¬-å›¾åƒæ¨ç†é“¾ï¼Œé€‚ç”¨äºç§‘å­¦é—®é¢˜ã€2Då’Œ3Dæ¨ç†ä»»åŠ¡ä»¥åŠè§†è§‰é€»è¾‘é—®é¢˜ç­‰å¤šç§ä»»åŠ¡ã€‚é€šè¿‡å¯¹Anole-7Bæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œæµ‹è¯•é›†å‡†ç¡®ç‡æé«˜äº†12%ï¼Œå¹¶åœ¨æ ‡å‡†VLMåŸºå‡†è¯„ä¼°ä¸­è·å¾—äº†é«˜è¾¾13%çš„æ€§èƒ½æå‡ã€‚æˆ‘ä»¬å¼€æºäº†è¯¥æ•°æ®é›†å’Œæ¨¡å‹ï¼Œä»¥æ”¯æŒè§†è§‰æ¨ç†èƒ½åŠ›çš„å¼€å‘å’Œè¯„ä¼°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.15024', 'title': 'RefCritic: Training Long Chain-of-Thought Critic Models with Refinement\n  Feedback', 'url': 'https://huggingface.co/papers/2507.15024', 'abstract': "With the rapid advancement of Large Language Models (LLMs), developing effective critic modules for precise guidance has become crucial yet challenging. In this paper, we initially demonstrate that supervised fine-tuning for building critic modules (which is widely adopted in current solutions) fails to genuinely enhance models' critique abilities, producing superficial critiques with insufficient reflections and verifications. To unlock the unprecedented critique capabilities, we propose RefCritic, a long-chain-of-thought critic module based on reinforcement learning with dual rule-based rewards: (1) instance-level correctness of solution judgments and (2) refinement accuracies of the policy model based on critiques, aiming to generate high-quality evaluations with actionable feedback that effectively guides model refinement. We evaluate RefCritic on Qwen2.5-14B-Instruct and DeepSeek-R1-Distill-Qwen-14B across five benchmarks. On critique and refinement settings, RefCritic demonstrates consistent advantages across all benchmarks, e.g., 6.8\\% and 7.2\\% gains on AIME25 for the respective base models. Notably, under majority voting, policy models filtered by RefCritic show superior scaling with increased voting numbers. Moreover, despite training on solution-level supervision, RefCritic outperforms step-level supervised approaches on ProcessBench, a benchmark to identify erroneous steps in mathematical reasoning.", 'score': 5, 'issue_id': 4960, 'pub_date': '2025-07-20', 'pub_date_card': {'ru': '20 Ğ¸ÑĞ»Ñ', 'en': 'July 20', 'zh': '7æœˆ20æ—¥'}, 'hash': 'd7fe33ada300b4f0', 'authors': ['Qiaoyu Tang', 'Hao Xiang', 'Le Yu', 'Bowen Yu', 'Hongyu Lin', 'Yaojie Lu', 'Xianpei Han', 'Le Sun', 'Junyang Lin'], 'affiliations': ['Alibaba Group', 'Chinese Information Processing Laboratory, Institute of Software, Chinese Academy of Sciences', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2507.15024.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#training', '#reasoning', '#rl', '#rlhf'], 'emoji': 'ğŸ”', 'ru': {'title': 'RefCritic: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞµ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ÑƒĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) - RefCritic. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼, RefCritic Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ´Ğ²Ğ¾Ğ¹Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·ÑŒÑ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ RefCritic Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ¸, ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Unlocking True Critique Power with RefCritic', 'desc': 'This paper addresses the limitations of current critic modules in Large Language Models (LLMs) that rely on supervised fine-tuning, which often leads to shallow critiques. The authors introduce RefCritic, a novel critic module that utilizes reinforcement learning and dual rule-based rewards to enhance critique quality. By focusing on both the correctness of solution judgments and the refinement of the policy model, RefCritic aims to provide actionable feedback for model improvement. Evaluation results show that RefCritic consistently outperforms existing methods across multiple benchmarks, demonstrating its effectiveness in generating high-quality evaluations.'}, 'zh': {'title': 'æå‡æ‰¹è¯„èƒ½åŠ›ï¼Œé‡å¡‘æ¨¡å‹æŒ‡å¯¼ï¼', 'desc': 'éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œå¼€å‘æœ‰æ•ˆçš„æ‰¹è¯„æ¨¡å—ä»¥æä¾›ç²¾ç¡®æŒ‡å¯¼å˜å¾—è‡³å…³é‡è¦ä¸”å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æœ¬æ–‡é¦–å…ˆè¡¨æ˜ï¼Œå½“å‰å¹¿æ³›é‡‡ç”¨çš„ç›‘ç£å¾®è°ƒæ–¹æ³•å¹¶æœªçœŸæ­£æå‡æ¨¡å‹çš„æ‰¹è¯„èƒ½åŠ›ï¼Œäº§ç”Ÿçš„æ‰¹è¯„å¾€å¾€è¡¨é¢åŒ–ï¼Œç¼ºä¹æ·±å…¥çš„åæ€å’ŒéªŒè¯ã€‚ä¸ºäº†è§£é”å‰æ‰€æœªæœ‰çš„æ‰¹è¯„èƒ½åŠ›ï¼Œæˆ‘ä»¬æå‡ºäº†RefCriticï¼Œè¿™æ˜¯ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„é•¿é“¾æ€ç»´æ‰¹è¯„æ¨¡å—ï¼Œé‡‡ç”¨åŒé‡è§„åˆ™å¥–åŠ±æœºåˆ¶ï¼Œæ—¨åœ¨ç”Ÿæˆé«˜è´¨é‡çš„è¯„ä¼°å’Œå¯æ“ä½œçš„åé¦ˆï¼Œä»è€Œæœ‰æ•ˆæŒ‡å¯¼æ¨¡å‹çš„æ”¹è¿›ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¯„ä¼°äº†RefCriticï¼Œç»“æœæ˜¾ç¤ºå…¶åœ¨æ‰¹è¯„å’Œæ”¹è¿›è®¾ç½®ä¸Šå‡è¡¨ç°å‡ºä¸€è‡´çš„ä¼˜åŠ¿ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.15245', 'title': 'SPAR: Scholar Paper Retrieval with LLM-based Agents for Enhanced\n  Academic Search', 'url': 'https://huggingface.co/papers/2507.15245', 'abstract': 'Recent advances in large language models (LLMs) have opened new opportunities for academic literature retrieval. However, existing systems often rely on rigid pipelines and exhibit limited reasoning capabilities. We introduce SPAR, a multi-agent framework that incorporates RefChain-based query decomposition and query evolution to enable more flexible and effective search. To facilitate systematic evaluation, we also construct SPARBench, a challenging benchmark with expert-annotated relevance labels. Experimental results demonstrate that SPAR substantially outperforms strong baselines, achieving up to +56% F1 on AutoScholar and +23% F1 on SPARBench over the best-performing baseline. Together, SPAR and SPARBench provide a scalable, interpretable, and high-performing foundation for advancing research in scholarly retrieval. Code and data will be available at: https://github.com/xiaofengShi/SPAR', 'score': 4, 'issue_id': 4959, 'pub_date': '2025-07-21', 'pub_date_card': {'ru': '21 Ğ¸ÑĞ»Ñ', 'en': 'July 21', 'zh': '7æœˆ21æ—¥'}, 'hash': 'a01817799751346b', 'authors': ['Xiaofeng Shi', 'Yuduo Li', 'Qian Kou', 'Longbin Yu', 'Jinxin Xie', 'Hua Zhou'], 'affiliations': ['Beijing Academy of Artificial Intelligence (BAAI)', 'Beijing Jiaotong University (BJTU)'], 'pdf_title_img': 'assets/pdf/title_img/2507.15245.jpg', 'data': {'categories': ['#agents', '#benchmark', '#survey', '#interpretability', '#reasoning'], 'emoji': 'ğŸ”', 'ru': {'title': 'SPAR: Ğ£Ğ¼Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ğ»Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ñ‹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜', 'desc': 'SPAR - ÑÑ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ğ»Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ñ‹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ½Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ Ğ¸ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ RefChain Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ³Ğ¸Ğ±ĞºĞ¾Ğ³Ğ¾ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº SPARBench Ñ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¾Ğ¹ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ‚Ğ°ĞºĞ¸Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ SPAR Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ F1-Ğ¼ĞµÑ€Ñ‹ Ğ´Ğ¾ 56% Ğ½Ğ° AutoScholar Ğ¸ 23% Ğ½Ğ° SPARBench.'}, 'en': {'title': 'Revolutionizing Academic Search with SPAR', 'desc': 'This paper presents SPAR, a novel multi-agent framework designed to enhance academic literature retrieval using large language models. SPAR utilizes RefChain-based query decomposition and evolution, allowing for more adaptable and effective search strategies compared to traditional rigid systems. The authors also introduce SPARBench, a benchmark with expert-annotated relevance labels to systematically evaluate retrieval performance. Experimental results show that SPAR significantly improves retrieval accuracy, outperforming existing methods by notable margins on both AutoScholar and SPARBench datasets.'}, 'zh': {'title': 'SPARï¼šæå‡å­¦æœ¯æ£€ç´¢çš„çµæ´»æ€§ä¸æ•ˆæœ', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºSPARçš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å­¦æœ¯æ–‡çŒ®æ£€ç´¢çš„çµæ´»æ€§å’Œæœ‰æ•ˆæ€§ã€‚SPARé€šè¿‡åŸºäºRefChainçš„æŸ¥è¯¢åˆ†è§£å’ŒæŸ¥è¯¢æ¼”å˜æŠ€æœ¯ï¼Œå…‹æœäº†ç°æœ‰ç³»ç»Ÿçš„å±€é™æ€§ã€‚ä¸ºäº†ç³»ç»Ÿè¯„ä¼°ï¼Œæˆ‘ä»¬æ„å»ºäº†SPARBenchï¼Œè¿™æ˜¯ä¸€ä¸ªå…·æœ‰ä¸“å®¶æ ‡æ³¨ç›¸å…³æ€§æ ‡ç­¾çš„æŒ‘æˆ˜æ€§åŸºå‡†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSPARåœ¨AutoScholarå’ŒSPARBenchä¸Šåˆ†åˆ«æ¯”æœ€ä½³åŸºçº¿æé«˜äº†56%å’Œ23%çš„F1åˆ†æ•°ï¼Œå±•ç¤ºäº†å…¶ä¼˜è¶Šçš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.15974', 'title': 'Does More Inference-Time Compute Really Help Robustness?', 'url': 'https://huggingface.co/papers/2507.15974', 'abstract': 'Recently, Zaremba et al. demonstrated that increasing inference-time computation improves robustness in large proprietary reasoning LLMs. In this paper, we first show that smaller-scale, open-source models (e.g., DeepSeek R1, Qwen3, Phi-reasoning) can also benefit from inference-time scaling using a simple budget forcing strategy. More importantly, we reveal and critically examine an implicit assumption in prior work: intermediate reasoning steps are hidden from adversaries. By relaxing this assumption, we identify an important security risk, intuitively motivated and empirically verified as an inverse scaling law: if intermediate reasoning steps become explicitly accessible, increased inference-time computation consistently reduces model robustness. Finally, we discuss practical scenarios where models with hidden reasoning chains are still vulnerable to attacks, such as models with tool-integrated reasoning and advanced reasoning extraction attacks. Our findings collectively demonstrate that the robustness benefits of inference-time scaling depend heavily on the adversarial setting and deployment context. We urge practitioners to carefully weigh these subtle trade-offs before applying inference-time scaling in security-sensitive, real-world applications.', 'score': 2, 'issue_id': 4961, 'pub_date': '2025-07-21', 'pub_date_card': {'ru': '21 Ğ¸ÑĞ»Ñ', 'en': 'July 21', 'zh': '7æœˆ21æ—¥'}, 'hash': 'e1f3efb4b484fe5f', 'authors': ['Tong Wu', 'Chong Xiang', 'Jiachen T. Wang', 'Weichen Yu', 'Chawin Sitawarin', 'Vikash Sehwag', 'Prateek Mittal'], 'affiliations': ['Carnegie Mellon University', 'Google DeepMind', 'NVIDIA', 'Princeton University'], 'pdf_title_img': 'assets/pdf/title_img/2507.15974.jpg', 'data': {'categories': ['#security', '#inference', '#reasoning'], 'emoji': 'ğŸ›¡ï¸', 'ru': {'title': 'ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°: Ğ¿Ğ°Ğ»ĞºĞ° Ğ¾ Ğ´Ğ²ÑƒÑ… ĞºĞ¾Ğ½Ñ†Ğ°Ñ… Ğ´Ğ»Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ˜Ğ˜', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ½Ğ¾ Ğ¸ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼. ĞĞ´Ğ½Ğ°ĞºĞ¾ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ĞµÑĞ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ ÑˆĞ°Ğ³Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ÑÑ‚ÑÑ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ğ¼Ğ¸, ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­Ñ‚Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ñ€Ğ¸ÑĞºĞ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€Ğ¸ Ğ°Ñ‚Ğ°ĞºĞ°Ñ… Ğ½Ğ° Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑÑ‹ Ğ¿ĞµÑ€ĞµĞ´ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ² Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğº Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ÑÑ….'}, 'en': {'title': 'Inference-Time Scaling: A Double-Edged Sword for Model Robustness', 'desc': 'This paper explores how increasing computation during inference can enhance the robustness of smaller, open-source language models, similar to findings in larger proprietary models. The authors challenge the assumption that intermediate reasoning steps in these models are hidden from adversaries, revealing a security risk when these steps are made accessible. They introduce the concept of an inverse scaling law, showing that more computation can actually decrease robustness if adversaries can see the reasoning process. The paper emphasizes the need for careful consideration of these trade-offs in security-sensitive applications, especially when models are integrated with tools or face advanced attack methods.'}, 'zh': {'title': 'æ¨ç†æ—¶é—´æ‰©å±•çš„é²æ£’æ€§ä¸å®‰å…¨é£é™©', 'desc': 'æœ€è¿‘ï¼ŒZarembaç­‰äººå±•ç¤ºäº†å¢åŠ æ¨ç†æ—¶é—´è®¡ç®—å¯ä»¥æé«˜å¤§å‹ä¸“æœ‰æ¨ç†å¤§è¯­è¨€æ¨¡å‹çš„é²æ£’æ€§ã€‚æœ¬æ–‡é¦–å…ˆè¡¨æ˜ï¼Œå°è§„æ¨¡çš„å¼€æºæ¨¡å‹ï¼ˆå¦‚DeepSeek R1ã€Qwen3ã€Phi-reasoningï¼‰ä¹Ÿå¯ä»¥é€šè¿‡ç®€å•çš„é¢„ç®—å¼ºåˆ¶ç­–ç•¥å—ç›Šäºæ¨ç†æ—¶é—´çš„æ‰©å±•ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬æ­ç¤ºå¹¶æ‰¹åˆ¤æ€§åœ°å®¡è§†äº†å…ˆå‰å·¥ä½œçš„ä¸€ä¸ªéšå«å‡è®¾ï¼šä¸­é—´æ¨ç†æ­¥éª¤å¯¹å¯¹æ‰‹æ˜¯éšè—çš„ã€‚é€šè¿‡æ”¾å®½è¿™ä¸€å‡è®¾ï¼Œæˆ‘ä»¬è¯†åˆ«å‡ºä¸€ä¸ªé‡è¦çš„å®‰å…¨é£é™©ï¼Œå³åå‘ç¼©æ”¾æ³•åˆ™ï¼šå¦‚æœä¸­é—´æ¨ç†æ­¥éª¤å˜å¾—æ˜¾å¼å¯è®¿é—®ï¼Œå¢åŠ çš„æ¨ç†æ—¶é—´è®¡ç®—ä¼šæŒç»­é™ä½æ¨¡å‹çš„é²æ£’æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.16795', 'title': 'Steering Out-of-Distribution Generalization with Concept Ablation\n  Fine-Tuning', 'url': 'https://huggingface.co/papers/2507.16795', 'abstract': "Concept Ablation Fine-Tuning (CAFT) uses interpretability tools to steer LLM generalization away from unintended concepts without altering training data.  \t\t\t\t\tAI-generated summary \t\t\t\t Fine-tuning large language models (LLMs) can lead to unintended out-of-distribution generalization. Standard approaches to this problem rely on modifying training data, for example by adding data that better specify the intended generalization. However, this is not always practical. We introduce Concept Ablation Fine-Tuning (CAFT), a technique that leverages interpretability tools to control how LLMs generalize from fine-tuning, without needing to modify the training data or otherwise use data from the target distribution. Given a set of directions in an LLM's latent space corresponding to undesired concepts, CAFT works by ablating these concepts with linear projections during fine-tuning, steering the model away from unintended generalizations. We successfully apply CAFT to three fine-tuning tasks, including emergent misalignment, a phenomenon where LLMs fine-tuned on a narrow task generalize to give egregiously misaligned responses to general questions. Without any changes to the fine-tuning data, CAFT reduces misaligned responses by 10x without degrading performance on the training distribution. Overall, CAFT represents a novel approach for steering LLM generalization without modifying training data.", 'score': 1, 'issue_id': 4962, 'pub_date': '2025-07-22', 'pub_date_card': {'ru': '22 Ğ¸ÑĞ»Ñ', 'en': 'July 22', 'zh': '7æœˆ22æ—¥'}, 'hash': '18824f37cd717c2f', 'authors': ['Helena Casademunt', 'Caden Juang', 'Adam Karvonen', 'Samuel Marks', 'Senthooran Rajamanoharan', 'Neel Nanda'], 'affiliations': ['Anthropic', 'Harvard University', 'Independent', 'Northeastern University'], 'pdf_title_img': 'assets/pdf/title_img/2507.16795.jpg', 'data': {'categories': ['#training', '#interpretability', '#alignment'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ£Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸ĞµĞ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…', 'desc': 'ĞœĞµÑ‚Ğ¾Ğ´ Concept Ablation Fine-Tuning (CAFT) Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸ĞµĞ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ±ĞµĞ· Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. CAFT Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾ĞµĞºÑ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¶ĞµĞ»Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹ Ğ² Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½ Ğº Ñ‚Ñ€ĞµĞ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ÑÑ‰ĞµĞ³Ğ¾ Ñ€Ğ°ÑÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ñ. CAFT Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ½ĞµĞ¶ĞµĞ»Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ±ĞµĞ· ÑƒÑ…ÑƒĞ´ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰ĞµĞ¼ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¸.'}, 'en': {'title': 'Steering LLMs Away from Unwanted Concepts Without Data Changes', 'desc': "Concept Ablation Fine-Tuning (CAFT) is a method designed to improve how large language models (LLMs) generalize after fine-tuning. Instead of changing the training data, CAFT uses interpretability tools to identify and remove unwanted concepts from the model's understanding. By applying linear projections in the model's latent space, CAFT effectively steers the model away from making unintended generalizations. This technique has been shown to significantly reduce misaligned responses in LLMs while maintaining their performance on the original training tasks."}, 'zh': {'title': 'æ¦‚å¿µæ¶ˆèå¾®è°ƒï¼šå¼•å¯¼æ¨¡å‹é¿å…é”™è¯¯æ³›åŒ–', 'desc': 'æ¦‚å¿µæ¶ˆèå¾®è°ƒï¼ˆCAFTï¼‰æ˜¯ä¸€ç§æ–°æŠ€æœ¯ï¼Œæ—¨åœ¨æ§åˆ¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ³›åŒ–èƒ½åŠ›ï¼Œé¿å…å…¶äº§ç”Ÿä¸å¿…è¦çš„æ¦‚å¿µã€‚ä¼ ç»Ÿæ–¹æ³•é€šå¸¸éœ€è¦ä¿®æ”¹è®­ç»ƒæ•°æ®ï¼Œä½†è¿™å¹¶ä¸æ€»æ˜¯å¯è¡Œã€‚CAFTåˆ©ç”¨å¯è§£é‡Šæ€§å·¥å…·ï¼Œé€šè¿‡åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­å¯¹ä¸å¸Œæœ›çš„æ¦‚å¿µè¿›è¡Œæ¶ˆèï¼Œæ¥å¼•å¯¼æ¨¡å‹è¿œç¦»é”™è¯¯çš„æ³›åŒ–ã€‚å®éªŒè¡¨æ˜ï¼ŒCAFTåœ¨ä¸æ”¹å˜è®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹ï¼Œèƒ½å¤Ÿæ˜¾è‘—å‡å°‘æ¨¡å‹çš„é”™è¯¯å“åº”ï¼ŒåŒæ—¶ä¿æŒåœ¨è®­ç»ƒæ•°æ®ä¸Šçš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.15454', 'title': 'ObjectGS: Object-aware Scene Reconstruction and Scene Understanding via\n  Gaussian Splatting', 'url': 'https://huggingface.co/papers/2507.15454', 'abstract': 'ObjectGS combines 3D scene reconstruction with semantic understanding by modeling individual objects as neural Gaussians, achieving superior performance in segmentation and integration with applications like mesh extraction and scene editing.  \t\t\t\t\tAI-generated summary \t\t\t\t 3D Gaussian Splatting is renowned for its high-fidelity reconstructions and real-time novel view synthesis, yet its lack of semantic understanding limits object-level perception. In this work, we propose ObjectGS, an object-aware framework that unifies 3D scene reconstruction with semantic understanding. Instead of treating the scene as a unified whole, ObjectGS models individual objects as local anchors that generate neural Gaussians and share object IDs, enabling precise object-level reconstruction. During training, we dynamically grow or prune these anchors and optimize their features, while a one-hot ID encoding with a classification loss enforces clear semantic constraints. We show through extensive experiments that ObjectGS not only outperforms state-of-the-art methods on open-vocabulary and panoptic segmentation tasks, but also integrates seamlessly with applications like mesh extraction and scene editing. Project page: https://ruijiezhu94.github.io/ObjectGS_page', 'score': 1, 'issue_id': 4960, 'pub_date': '2025-07-21', 'pub_date_card': {'ru': '21 Ğ¸ÑĞ»Ñ', 'en': 'July 21', 'zh': '7æœˆ21æ—¥'}, 'hash': '98f5b463073c9876', 'authors': ['Ruijie Zhu', 'Mulin Yu', 'Linning Xu', 'Lihan Jiang', 'Yixuan Li', 'Tianzhu Zhang', 'Jiangmiao Pang', 'Bo Dai'], 'affiliations': ['Shanghai Artificial Intelligence Laboratory', 'The Chinese University of Hong Kong', 'The University of Hong Kong', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2507.15454.jpg', 'data': {'categories': ['#3d', '#games', '#optimization', '#cv'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞĞ±ÑŠĞµĞºÑ‚Ğ½Ğ¾-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ 3D-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼', 'desc': 'ObjectGS - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº 3D-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ ÑÑ†ĞµĞ½, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğ¹ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ³Ğ°ÑƒÑÑĞ¸Ğ°Ğ½Ñ‹ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸. ObjectGS Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ ÑĞ»Ğ¾Ğ²Ğ°Ñ€ĞµĞ¼ Ğ¸ Ğ¿Ğ°Ğ½Ğ¾Ñ€Ğ°Ğ¼Ğ½Ğ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸. Ğ¢ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ñ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ĞµÑˆ-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑÑ†ĞµĞ½.'}, 'en': {'title': 'Revolutionizing 3D Reconstruction with Object-Level Semantic Understanding', 'desc': 'ObjectGS is a novel framework that enhances 3D scene reconstruction by incorporating semantic understanding of individual objects. It models each object as a neural Gaussian, allowing for precise segmentation and reconstruction at the object level. The framework dynamically adjusts object anchors during training, optimizing their features while enforcing semantic clarity through a classification loss. Extensive experiments demonstrate that ObjectGS surpasses existing methods in segmentation tasks and integrates effectively with applications like mesh extraction and scene editing.'}, 'zh': {'title': 'å¯¹è±¡æ„ŸçŸ¥çš„3Dåœºæ™¯é‡å»ºæ–°æ–¹æ³•', 'desc': 'ObjectGS æ˜¯ä¸€ä¸ªç»“åˆäº† 3D åœºæ™¯é‡å»ºå’Œè¯­ä¹‰ç†è§£çš„æ¡†æ¶ã€‚å®ƒé€šè¿‡å°†æ¯ä¸ªå¯¹è±¡å»ºæ¨¡ä¸ºå±€éƒ¨é”šç‚¹ï¼Œç”Ÿæˆç¥ç»é«˜æ–¯åˆ†å¸ƒï¼Œä»è€Œå®ç°ç²¾ç¡®çš„å¯¹è±¡çº§é‡å»ºã€‚è®­ç»ƒè¿‡ç¨‹ä¸­ï¼ŒObjectGS åŠ¨æ€è°ƒæ•´è¿™äº›é”šç‚¹çš„æ•°é‡å’Œç‰¹å¾ï¼Œå¹¶é€šè¿‡ä¸€çƒ­ç¼–ç å’Œåˆ†ç±»æŸå¤±æ¥å¼ºåŒ–è¯­ä¹‰çº¦æŸã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒObjectGS åœ¨å¼€æ”¾è¯æ±‡å’Œå…¨æ™¯åˆ†å‰²ä»»åŠ¡ä¸Šè¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œå¹¶èƒ½ä¸ç½‘æ ¼æå–å’Œåœºæ™¯ç¼–è¾‘ç­‰åº”ç”¨æ— ç¼é›†æˆã€‚'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (1)', '#agents (2)', '#agi (1)', '#alignment (1)', '#architecture (1)', '#audio (1)', '#benchmark (6)', '#cv (4)', '#data (1)', '#dataset (3)', '#diffusion (1)', '#ethics', '#games (3)', '#graphs', '#hallucinations (1)', '#healthcare', '#inference (3)', '#interpretability (2)', '#leakage', '#long_context (1)', '#low_resource', '#machine_translation', '#math (1)', '#multilingual', '#multimodal (4)', '#open_source (4)', '#optimization (6)', '#plp', '#rag (1)', '#reasoning (8)', '#rl (4)', '#rlhf (1)', '#robotics (1)', '#science (1)', '#security (1)', '#small_models', '#story_generation', '#survey (1)', '#synthetic (1)', '#training (4)', '#transfer_learning', '#video'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            ğŸ”º ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'ğŸ”„ ' + getTimeDiff('2025-07-23 09:18',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ",
                    pub_date: "Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸",
                    issue_id: "Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "è¯„åˆ†",
                    pub_date: "å‘å¸ƒæ—¥æœŸ",
                    issue_id: "HFä¸Šä¼ æ—¥æœŸ"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-07-23 09:18')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-07-23 09:18')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    