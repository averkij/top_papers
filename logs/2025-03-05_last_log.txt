[05.03.2025 03:21] Read previous papers.
[05.03.2025 03:21] Generating top page (month).
[05.03.2025 03:21] Writing top page (month).
[05.03.2025 04:13] Read previous papers.
[05.03.2025 04:13] Get feed.
[05.03.2025 04:13] Extract page data from URL. URL: https://huggingface.co/papers/2503.02846
[05.03.2025 04:13] Extract page data from URL. URL: https://huggingface.co/papers/2503.02682
[05.03.2025 04:13] Extract page data from URL. URL: https://huggingface.co/papers/2503.02879
[05.03.2025 04:13] Get page data from previous paper. URL: https://huggingface.co/papers/2503.01935
[05.03.2025 04:13] Get page data from previous paper. URL: https://huggingface.co/papers/2503.01328
[05.03.2025 04:13] Get page data from previous paper. URL: https://huggingface.co/papers/2503.02197
[05.03.2025 04:13] Get page data from previous paper. URL: https://huggingface.co/papers/2503.02878
[05.03.2025 04:13] Get page data from previous paper. URL: https://huggingface.co/papers/2503.02876
[05.03.2025 04:13] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[05.03.2025 04:13] No deleted papers detected.
[05.03.2025 04:13] Downloading and parsing papers (pdf, html). Total: 8.
[05.03.2025 04:13] Downloading and parsing paper https://huggingface.co/papers/2503.02846.
[05.03.2025 04:13] Downloading paper 2503.02846 from http://arxiv.org/pdf/2503.02846v1...
[05.03.2025 04:13] Extracting affiliations from text.
[05.03.2025 04:13] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 4 ] . [ 1 6 4 8 2 0 . 3 0 5 2 : r Published as conference paper at ICLR MASK-DPO: GENERALIZABLE FINE-GRAINED FACTUALITY ALIGNMENT OF LLMS Yuzhe Gu1,2 Wenwei Zhang2 Chengqi Lyu2 Dahua Lin2,3 Kai Chen2 1Shanghai Jiao Tong University 2Shanghai AI Laboratory 3MMLab, The Chinese University of Hong Kong {guyuzhe,zhangwenwei,lvchengqi,chenkai}@pjlab.org.cn "
[05.03.2025 04:13] Response: ```python
["Shanghai Jiao Tong University", "Shanghai AI Laboratory", "MMLab, The Chinese University of Hong Kong"]
```
[05.03.2025 04:13] Deleting PDF ./assets/pdf/2503.02846.pdf.
[05.03.2025 04:13] Success.
[05.03.2025 04:13] Downloading and parsing paper https://huggingface.co/papers/2503.02682.
[05.03.2025 04:13] Downloading paper 2503.02682 from http://arxiv.org/pdf/2503.02682v1...
[05.03.2025 04:13] Extracting affiliations from text.
[05.03.2025 04:13] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"MPO: Boosting LLM Agents with Meta Plan Optimization Weimin Xiong, Yifan Song, Qingxiu Dong, Bingchan Zhao Feifan Song, Xun Wang, Sujian Li* National Key Laboratory for Multimedia Information Processing, School of Computer Science, Peking University University of Washington Peking University {wmxiong, lisujian}@pku.edu.cn https://github.com/WeiminXiong/MPO 5 2 0 2 4 ] . [ 1 2 8 6 2 0 . 3 0 5 2 : r a "
[05.03.2025 04:13] Response: ```python
["National Key Laboratory for Multimedia Information Processing, School of Computer Science, Peking University", "University of Washington", "Peking University"]
```
[05.03.2025 04:13] Deleting PDF ./assets/pdf/2503.02682.pdf.
[05.03.2025 04:13] Success.
[05.03.2025 04:13] Downloading and parsing paper https://huggingface.co/papers/2503.02879.
[05.03.2025 04:13] Downloading paper 2503.02879 from http://arxiv.org/pdf/2503.02879v1...
[05.03.2025 04:13] Extracting affiliations from text.
[05.03.2025 04:13] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Wikipedia in the Era of LLMs: Evolution and Risks Siming Huang1, Yuliang Xu1, Mingmeng Geng2*, Yao Wan1*, Dongping Chen1 1 Huazhong University of Science and Technology 2 International School for Advanced Studies (SISSA) mgeng@sissa.it, wanyao@hust.edu.cn 5 2 0 2 4 ] . [ 1 9 7 8 2 0 . 3 0 5 2 : r a "
[05.03.2025 04:13] Response: ```python
["Huazhong University of Science and Technology", "International School for Advanced Studies (SISSA)"]
```
[05.03.2025 04:13] Deleting PDF ./assets/pdf/2503.02879.pdf.
[05.03.2025 04:13] Success.
[05.03.2025 04:13] Downloading and parsing paper https://huggingface.co/papers/2503.01935.
[05.03.2025 04:13] Extra JSON file exists (./assets/json/2503.01935.json), skip PDF parsing.
[05.03.2025 04:13] Paper image links file exists (./assets/img_data/2503.01935.json), skip HTML parsing.
[05.03.2025 04:13] Success.
[05.03.2025 04:13] Downloading and parsing paper https://huggingface.co/papers/2503.01328.
[05.03.2025 04:13] Extra JSON file exists (./assets/json/2503.01328.json), skip PDF parsing.
[05.03.2025 04:13] Paper image links file exists (./assets/img_data/2503.01328.json), skip HTML parsing.
[05.03.2025 04:13] Success.
[05.03.2025 04:13] Downloading and parsing paper https://huggingface.co/papers/2503.02197.
[05.03.2025 04:13] Extra JSON file exists (./assets/json/2503.02197.json), skip PDF parsing.
[05.03.2025 04:13] Paper image links file exists (./assets/img_data/2503.02197.json), skip HTML parsing.
[05.03.2025 04:13] Success.
[05.03.2025 04:13] Downloading and parsing paper https://huggingface.co/papers/2503.02878.
[05.03.2025 04:13] Extra JSON file exists (./assets/json/2503.02878.json), skip PDF parsing.
[05.03.2025 04:13] Paper image links file exists (./assets/img_data/2503.02878.json), skip HTML parsing.
[05.03.2025 04:13] Success.
[05.03.2025 04:13] Downloading and parsing paper https://huggingface.co/papers/2503.02876.
[05.03.2025 04:13] Extra JSON file exists (./assets/json/2503.02876.json), skip PDF parsing.
[05.03.2025 04:13] Paper image links file exists (./assets/img_data/2503.02876.json), skip HTML parsing.
[05.03.2025 04:13] Success.
[05.03.2025 04:13] Enriching papers with extra data.
[05.03.2025 04:13] ********************************************************************************
[05.03.2025 04:13] Abstract 0. Large language models (LLMs) exhibit hallucinations (i.e., unfaithful or nonsensical information) when serving as AI assistants in various domains. Since hallucinations always come with truthful content in the LLM responses, previous factuality alignment methods that conduct response-level preferenc...
[05.03.2025 04:13] ********************************************************************************
[05.03.2025 04:13] Abstract 1. Recent advancements in large language models (LLMs) have enabled LLM-based agents to successfully tackle interactive planning tasks. However, despite their successes, existing approaches often suffer from planning hallucinations and require retraining for each new agent. To address these challenges,...
[05.03.2025 04:13] ********************************************************************************
[05.03.2025 04:13] Abstract 2. In this paper, we present a thorough analysis of the impact of Large Language Models (LLMs) on Wikipedia, examining the evolution of Wikipedia through existing data and using simulations to explore potential risks. We begin by analyzing page views and article content to study Wikipedia's recent chan...
[05.03.2025 04:13] ********************************************************************************
[05.03.2025 04:13] Abstract 3. Large Language Models (LLMs) have shown remarkable capabilities as autonomous agents, yet existing benchmarks either focus on single-agent tasks or are confined to narrow domains, failing to capture the dynamics of multi-agent coordination and competition. In this paper, we introduce MultiAgentBench...
[05.03.2025 04:13] ********************************************************************************
[05.03.2025 04:13] Abstract 4. Pipeline parallelism (PP) is widely used for training large language models (LLMs), yet its scalability is often constrained by high activation memory consumption as the number of in-flight microbatches grows with the degree of PP. In this paper, we focus on addressing this challenge by leveraging t...
[05.03.2025 04:13] ********************************************************************************
[05.03.2025 04:13] Abstract 5. Large Language Model (LLM) agents have demonstrated remarkable generalization capabilities across multi-domain tasks. Existing agent tuning approaches typically employ supervised finetuning on entire expert trajectories. However, behavior-cloning of full trajectories can introduce expert bias and we...
[05.03.2025 04:13] ********************************************************************************
[05.03.2025 04:13] Abstract 6. Collecting ground truth task completion rewards or human demonstrations for multi-step reasoning tasks is often cost-prohibitive and time-consuming, especially in interactive domains like web tasks. To address this bottleneck, we present self-taught lookahead, a self-supervised method that leverages...
[05.03.2025 04:13] ********************************************************************************
[05.03.2025 04:13] Abstract 7. Advancing AI in computational pathology requires large, high-quality, and diverse datasets, yet existing public datasets are often limited in organ diversity, class coverage, or annotation quality. To bridge this gap, we introduce SPIDER (Supervised Pathology Image-DEscription Repository), the large...
[05.03.2025 04:13] Read previous papers.
[05.03.2025 04:13] Generating reviews via LLM API.
[05.03.2025 04:13] Querying the API.
[05.03.2025 04:13] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Large language models (LLMs) exhibit hallucinations (i.e., unfaithful or nonsensical information) when serving as AI assistants in various domains. Since hallucinations always come with truthful content in the LLM responses, previous factuality alignment methods that conduct response-level preference learning inevitably introduced noises during training. Therefore, this paper proposes a fine-grained factuality alignment method based on Direct Preference Optimization (DPO), called Mask-DPO. Incorporating sentence-level factuality as mask signals, Mask-DPO only learns from factually correct sentences in the preferred samples and prevents the penalty on factual contents in the not preferred samples, which resolves the ambiguity in the preference learning. Extensive experimental results demonstrate that Mask-DPO can significantly improve the factuality of LLMs responses to questions from both in-domain and out-of-domain datasets, although these questions and their corresponding topics are unseen during training. Only trained on the ANAH train set, the score of Llama3.1-8B-Instruct on the ANAH test set is improved from 49.19% to 77.53%, even surpassing the score of Llama3.1-70B-Instruct (53.44%), while its FactScore on the out-of-domain Biography dataset is also improved from 30.29% to 39.39%. We further study the generalization property of Mask-DPO using different training sample scaling strategies and find that scaling the number of topics in the dataset is more effective than the number of questions. We provide a hypothesis of what factual alignment is doing with LLMs, on the implication of this phenomenon, and conduct proof-of-concept experiments to verify it. We hope the method and the findings pave the way for future research on scaling factuality alignment.
[05.03.2025 04:13] Error getting data: Error code: 529 - {'type': 'error', 'error': {'type': 'overloaded_error', 'message': 'Overloaded'}}
[05.03.2025 04:13] Querying the API.
[05.03.2025 04:13] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Recent advancements in large language models (LLMs) have enabled LLM-based agents to successfully tackle interactive planning tasks. However, despite their successes, existing approaches often suffer from planning hallucinations and require retraining for each new agent. To address these challenges, we propose the Meta Plan Optimization (MPO) framework, which enhances agent planning capabilities by directly incorporating explicit guidance. Unlike previous methods that rely on complex knowledge, which either require significant human effort or lack quality assurance, MPO leverages high-level general guidance through meta plans to assist agent planning and enables continuous optimization of the meta plans based on feedback from the agent's task execution. Our experiments conducted on two representative tasks demonstrate that MPO significantly outperforms existing baselines. Moreover, our analysis indicates that MPO provides a plug-and-play solution that enhances both task completion efficiency and generalization capabilities in previous unseen scenarios.
[05.03.2025 04:13] Error getting data: Error code: 529 - {'type': 'error', 'error': {'type': 'overloaded_error', 'message': 'Overloaded'}}
[05.03.2025 04:13] Querying the API.
[05.03.2025 04:13] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

In this paper, we present a thorough analysis of the impact of Large Language Models (LLMs) on Wikipedia, examining the evolution of Wikipedia through existing data and using simulations to explore potential risks. We begin by analyzing page views and article content to study Wikipedia's recent changes and assess the impact of LLMs. Subsequently, we evaluate how LLMs affect various Natural Language Processing (NLP) tasks related to Wikipedia, including machine translation and retrieval-augmented generation (RAG). Our findings and simulation results reveal that Wikipedia articles have been influenced by LLMs, with an impact of approximately 1%-2% in certain categories. If the machine translation benchmark based on Wikipedia is influenced by LLMs, the scores of the models may become inflated, and the comparative results among models might shift as well. Moreover, the effectiveness of RAG might decrease if the knowledge base becomes polluted by LLM-generated content. While LLMs have not yet fully changed Wikipedia's language and knowledge structures, we believe that our empirical findings signal the need for careful consideration of potential future risks.
[05.03.2025 04:13] Error getting data: Error code: 529 - {'type': 'error', 'error': {'type': 'overloaded_error', 'message': 'Overloaded'}}
[05.03.2025 04:13] Using data from previous issue: {"categories": ["#games", "#optimization", "#open_source", "#benchmark", "#agents"], "emoji": "🤖", "ru": {"title": "MultiAgentBench: новый стандарт для оценки мультиагентных LLM-систем", "desc": "Статья представляет MultiAgentBench - комплексный бенчмарк для оценки мультиагентных систем на основе бо
[05.03.2025 04:13] Using data from previous issue: {"categories": ["#open_source", "#inference", "#optimization", "#training"], "emoji": "🚀", "ru": {"title": "Оптимизация памяти для масштабирования больших языковых моделей", "desc": "Статья посвящена улучшению масштабируемости конвейерного параллелизма при обучении больших языковых моделей. Авторы п
[05.03.2025 04:13] Using data from previous issue: {"categories": ["#optimization", "#reasoning", "#training", "#agents", "#agi"], "emoji": "🎯", "ru": {"title": "ATLaS: точечная настройка LLM-агентов для улучшения обобщения", "desc": "ATLaS - это новый метод настройки агентов на основе больших языковых моделей (LLM). Он фокусируется на обучении толь
[05.03.2025 04:13] Using data from previous issue: {"categories": ["#reasoning", "#rl", "#optimization", "#training"], "emoji": "🧠", "ru": {"title": "Самообучение для эффективного поиска без дорогостоящих демонстраций", "desc": "Статья представляет метод 'self-taught lookahead' для обучения модели оценки, способной эффективно направлять поиск, управ
[05.03.2025 04:13] Using data from previous issue: {"categories": ["#open_source", "#dataset", "#multimodal", "#science", "#benchmark"], "emoji": "🕷️", "ru": {"title": "SPIDER: Новый стандарт данных для ИИ в патологии", "desc": "Статья представляет SPIDER - крупнейший общедоступный набор данных для вычислительной патологии, охватывающий множество ти
[05.03.2025 04:13] Loading Chinese text from previous data.
[05.03.2025 04:13] Renaming data file.
[05.03.2025 04:13] Renaming previous data. hf_papers.json to ./d/2025-03-05.json
[05.03.2025 04:13] Saving new data file.
[05.03.2025 04:13] Generating page.
[05.03.2025 04:13] Renaming previous page.
[05.03.2025 04:13] Renaming previous data. index.html to ./d/2025-03-05.html
[05.03.2025 04:13] [Experimental] Generating Chinese page for reading.
[05.03.2025 04:13] Chinese vocab [{'word': '视觉强化微调', 'pinyin': 'shìjué qiángzhù wēitiáo', 'trans': 'visual reinforcement fine-tuning'}, {'word': '大型视觉-语言模型', 'pinyin': 'dàxíng shìjué-yǔyán móxíng', 'trans': 'large vision-language models'}, {'word': '表现', 'pinyin': 'biǎoxiàn', 'trans': 'performance'}, {'word': '生成', 'pinyin': 'shēngchéng', 'trans': 'generate'}, {'word': '可验证的', 'pinyin': 'kě yànzhèng de', 'trans': 'verifiable'}, {'word': '奖励函数', 'pinyin': 'jiǎnglì hánshù', 'trans': 'reward function'}, {'word': '更新', 'pinyin': 'gēngxīn', 'trans': 'update'}, {'word': '实验结果', 'pinyin': 'shíyàn jiéguǒ', 'trans': 'experimental results'}, {'word': '出色', 'pinyin': 'chūsè', 'trans': 'outstanding'}, {'word': '优于', 'pinyin': 'yōu yú', 'trans': 'superior to'}, {'word': '监督微调', 'pinyin': 'jiàndū wēitiáo', 'trans': 'supervised fine-tuning'}, {'word': '细粒度', 'pinyin': 'xì lìdù', 'trans': 'fine-grained'}, {'word': '图像分类', 'pinyin': 'túxiàng fēnlèi', 'trans': 'image classification'}, {'word': '准确率', 'pinyin': 'zhǔnquèlǜ', 'trans': 'accuracy'}]
[05.03.2025 04:13] Renaming previous Chinese page.
[05.03.2025 04:13] Renaming previous data. zh.html to ./d/2025-03-04_zh_reading_task.html
[05.03.2025 04:13] Writing Chinese reading task.
[05.03.2025 04:13] Writing result.
[05.03.2025 04:13] Renaming log file.
[05.03.2025 04:13] Renaming previous data. log.txt to ./logs/2025-03-05_last_log.txt
