[05.03.2025 00:48] Read previous papers.
[05.03.2025 00:48] Generating top page (month).
[05.03.2025 00:48] Writing top page (month).
[05.03.2025 02:16] Read previous papers.
[05.03.2025 02:16] Get feed.
[05.03.2025 02:16] Get page data from previous paper. URL: https://huggingface.co/papers/2503.01785
[05.03.2025 02:16] Get page data from previous paper. URL: https://huggingface.co/papers/2503.01743
[05.03.2025 02:16] Get page data from previous paper. URL: https://huggingface.co/papers/2503.01774
[05.03.2025 02:16] Get page data from previous paper. URL: https://huggingface.co/papers/2503.01183
[05.03.2025 02:16] Get page data from previous paper. URL: https://huggingface.co/papers/2502.18965
[05.03.2025 02:16] Get page data from previous paper. URL: https://huggingface.co/papers/2503.01688
[05.03.2025 02:16] Get page data from previous paper. URL: https://huggingface.co/papers/2502.18890
[05.03.2025 02:16] Get page data from previous paper. URL: https://huggingface.co/papers/2503.01307
[05.03.2025 02:16] Get page data from previous paper. URL: https://huggingface.co/papers/2503.01496
[05.03.2025 02:16] Get page data from previous paper. URL: https://huggingface.co/papers/2503.00714
[05.03.2025 02:16] Get page data from previous paper. URL: https://huggingface.co/papers/2503.00501
[05.03.2025 02:16] Get page data from previous paper. URL: https://huggingface.co/papers/2503.00031
[05.03.2025 02:16] Get page data from previous paper. URL: https://huggingface.co/papers/2503.00784
[05.03.2025 02:16] Get page data from previous paper. URL: https://huggingface.co/papers/2503.01506
[05.03.2025 02:16] Get page data from previous paper. URL: https://huggingface.co/papers/2503.01370
[05.03.2025 02:16] Get page data from previous paper. URL: https://huggingface.co/papers/2503.00455
[05.03.2025 02:16] Get page data from previous paper. URL: https://huggingface.co/papers/2503.01714
[05.03.2025 02:16] Get page data from previous paper. URL: https://huggingface.co/papers/2503.01295
[05.03.2025 02:16] Get page data from previous paper. URL: https://huggingface.co/papers/2503.01807
[05.03.2025 02:16] Get page data from previous paper. URL: https://huggingface.co/papers/2502.19402
[05.03.2025 02:16] Get page data from previous paper. URL: https://huggingface.co/papers/2503.01063
[05.03.2025 02:16] Get page data from previous paper. URL: https://huggingface.co/papers/2503.01739
[05.03.2025 02:16] Extract page data from URL. URL: https://huggingface.co/papers/2503.01820
[05.03.2025 02:16] Get page data from previous paper. URL: https://huggingface.co/papers/2503.01103
[05.03.2025 02:16] Get page data from previous paper. URL: https://huggingface.co/papers/2502.16779
[05.03.2025 02:16] Get page data from previous paper. URL: https://huggingface.co/papers/2503.00729
[05.03.2025 02:16] Get page data from previous paper. URL: https://huggingface.co/papers/2502.20383
[05.03.2025 02:16] Extract page data from URL. URL: https://huggingface.co/papers/2503.02379
[05.03.2025 02:16] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[05.03.2025 02:16] No deleted papers detected.
[05.03.2025 02:16] Downloading and parsing papers (pdf, html). Total: 28.
[05.03.2025 02:16] Downloading and parsing paper https://huggingface.co/papers/2503.01785.
[05.03.2025 02:16] Extra JSON file exists (./assets/json/2503.01785.json), skip PDF parsing.
[05.03.2025 02:16] Paper image links file exists (./assets/img_data/2503.01785.json), skip HTML parsing.
[05.03.2025 02:16] Success.
[05.03.2025 02:16] Downloading and parsing paper https://huggingface.co/papers/2503.01743.
[05.03.2025 02:16] Extra JSON file exists (./assets/json/2503.01743.json), skip PDF parsing.
[05.03.2025 02:16] Paper image links file exists (./assets/img_data/2503.01743.json), skip HTML parsing.
[05.03.2025 02:16] Success.
[05.03.2025 02:16] Downloading and parsing paper https://huggingface.co/papers/2503.01774.
[05.03.2025 02:16] Extra JSON file exists (./assets/json/2503.01774.json), skip PDF parsing.
[05.03.2025 02:16] Paper image links file exists (./assets/img_data/2503.01774.json), skip HTML parsing.
[05.03.2025 02:16] Success.
[05.03.2025 02:16] Downloading and parsing paper https://huggingface.co/papers/2503.01183.
[05.03.2025 02:16] Extra JSON file exists (./assets/json/2503.01183.json), skip PDF parsing.
[05.03.2025 02:16] Paper image links file exists (./assets/img_data/2503.01183.json), skip HTML parsing.
[05.03.2025 02:16] Success.
[05.03.2025 02:16] Downloading and parsing paper https://huggingface.co/papers/2502.18965.
[05.03.2025 02:16] Extra JSON file exists (./assets/json/2502.18965.json), skip PDF parsing.
[05.03.2025 02:16] Paper image links file exists (./assets/img_data/2502.18965.json), skip HTML parsing.
[05.03.2025 02:16] Success.
[05.03.2025 02:16] Downloading and parsing paper https://huggingface.co/papers/2503.01688.
[05.03.2025 02:16] Extra JSON file exists (./assets/json/2503.01688.json), skip PDF parsing.
[05.03.2025 02:16] Paper image links file exists (./assets/img_data/2503.01688.json), skip HTML parsing.
[05.03.2025 02:16] Success.
[05.03.2025 02:16] Downloading and parsing paper https://huggingface.co/papers/2502.18890.
[05.03.2025 02:16] Extra JSON file exists (./assets/json/2502.18890.json), skip PDF parsing.
[05.03.2025 02:16] Paper image links file exists (./assets/img_data/2502.18890.json), skip HTML parsing.
[05.03.2025 02:16] Success.
[05.03.2025 02:16] Downloading and parsing paper https://huggingface.co/papers/2503.01307.
[05.03.2025 02:16] Extra JSON file exists (./assets/json/2503.01307.json), skip PDF parsing.
[05.03.2025 02:16] Paper image links file exists (./assets/img_data/2503.01307.json), skip HTML parsing.
[05.03.2025 02:16] Success.
[05.03.2025 02:16] Downloading and parsing paper https://huggingface.co/papers/2503.01496.
[05.03.2025 02:16] Extra JSON file exists (./assets/json/2503.01496.json), skip PDF parsing.
[05.03.2025 02:16] Paper image links file exists (./assets/img_data/2503.01496.json), skip HTML parsing.
[05.03.2025 02:16] Success.
[05.03.2025 02:16] Downloading and parsing paper https://huggingface.co/papers/2503.00714.
[05.03.2025 02:16] Extra JSON file exists (./assets/json/2503.00714.json), skip PDF parsing.
[05.03.2025 02:16] Paper image links file exists (./assets/img_data/2503.00714.json), skip HTML parsing.
[05.03.2025 02:16] Success.
[05.03.2025 02:16] Downloading and parsing paper https://huggingface.co/papers/2503.00501.
[05.03.2025 02:16] Extra JSON file exists (./assets/json/2503.00501.json), skip PDF parsing.
[05.03.2025 02:16] Paper image links file exists (./assets/img_data/2503.00501.json), skip HTML parsing.
[05.03.2025 02:16] Success.
[05.03.2025 02:16] Downloading and parsing paper https://huggingface.co/papers/2503.00031.
[05.03.2025 02:16] Extra JSON file exists (./assets/json/2503.00031.json), skip PDF parsing.
[05.03.2025 02:16] Paper image links file exists (./assets/img_data/2503.00031.json), skip HTML parsing.
[05.03.2025 02:16] Success.
[05.03.2025 02:16] Downloading and parsing paper https://huggingface.co/papers/2503.00784.
[05.03.2025 02:16] Extra JSON file exists (./assets/json/2503.00784.json), skip PDF parsing.
[05.03.2025 02:16] Paper image links file exists (./assets/img_data/2503.00784.json), skip HTML parsing.
[05.03.2025 02:16] Success.
[05.03.2025 02:16] Downloading and parsing paper https://huggingface.co/papers/2503.01506.
[05.03.2025 02:16] Extra JSON file exists (./assets/json/2503.01506.json), skip PDF parsing.
[05.03.2025 02:16] Paper image links file exists (./assets/img_data/2503.01506.json), skip HTML parsing.
[05.03.2025 02:16] Success.
[05.03.2025 02:16] Downloading and parsing paper https://huggingface.co/papers/2503.01370.
[05.03.2025 02:16] Downloading paper 2503.01370 from http://arxiv.org/pdf/2503.01370v1...
[05.03.2025 02:16] Failed to download and parse paper https://huggingface.co/papers/2503.01370: max() arg is an empty sequence
[05.03.2025 02:16] Downloading and parsing paper https://huggingface.co/papers/2503.00455.
[05.03.2025 02:16] Extra JSON file exists (./assets/json/2503.00455.json), skip PDF parsing.
[05.03.2025 02:16] Paper image links file exists (./assets/img_data/2503.00455.json), skip HTML parsing.
[05.03.2025 02:16] Success.
[05.03.2025 02:16] Downloading and parsing paper https://huggingface.co/papers/2503.01714.
[05.03.2025 02:16] Extra JSON file exists (./assets/json/2503.01714.json), skip PDF parsing.
[05.03.2025 02:16] Paper image links file exists (./assets/img_data/2503.01714.json), skip HTML parsing.
[05.03.2025 02:16] Success.
[05.03.2025 02:16] Downloading and parsing paper https://huggingface.co/papers/2503.01295.
[05.03.2025 02:16] Extra JSON file exists (./assets/json/2503.01295.json), skip PDF parsing.
[05.03.2025 02:16] Paper image links file exists (./assets/img_data/2503.01295.json), skip HTML parsing.
[05.03.2025 02:16] Success.
[05.03.2025 02:16] Downloading and parsing paper https://huggingface.co/papers/2503.01807.
[05.03.2025 02:16] Extra JSON file exists (./assets/json/2503.01807.json), skip PDF parsing.
[05.03.2025 02:16] Paper image links file exists (./assets/img_data/2503.01807.json), skip HTML parsing.
[05.03.2025 02:16] Success.
[05.03.2025 02:16] Downloading and parsing paper https://huggingface.co/papers/2502.19402.
[05.03.2025 02:16] Extra JSON file exists (./assets/json/2502.19402.json), skip PDF parsing.
[05.03.2025 02:16] Paper image links file exists (./assets/img_data/2502.19402.json), skip HTML parsing.
[05.03.2025 02:16] Success.
[05.03.2025 02:16] Downloading and parsing paper https://huggingface.co/papers/2503.01063.
[05.03.2025 02:16] Extra JSON file exists (./assets/json/2503.01063.json), skip PDF parsing.
[05.03.2025 02:16] Paper image links file exists (./assets/img_data/2503.01063.json), skip HTML parsing.
[05.03.2025 02:16] Success.
[05.03.2025 02:16] Downloading and parsing paper https://huggingface.co/papers/2503.01739.
[05.03.2025 02:16] Extra JSON file exists (./assets/json/2503.01739.json), skip PDF parsing.
[05.03.2025 02:16] Paper image links file exists (./assets/img_data/2503.01739.json), skip HTML parsing.
[05.03.2025 02:16] Success.
[05.03.2025 02:16] Downloading and parsing paper https://huggingface.co/papers/2503.01820.
[05.03.2025 02:16] Downloading paper 2503.01820 from http://arxiv.org/pdf/2503.01820v1...
[05.03.2025 02:16] Extracting affiliations from text.
[05.03.2025 02:16] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"RSQ: Learning from Important Tokens Leads to Better Quantized LLMs Yi-Lin Sung 1 Prateek Yadav 1 Jialu Li 1 Jaehong Yoon 1 Mohit Bansal 1 5 2 0 2 3 ] . [ 1 0 2 8 1 0 . 3 0 5 2 : r a "
[05.03.2025 02:16] Response: []
[05.03.2025 02:16] Extracting affiliations from text.
[05.03.2025 02:16] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"RSQ: Learning from Important Tokens Leads to Better Quantized LLMs Yi-Lin Sung 1 Prateek Yadav 1 Jialu Li 1 Jaehong Yoon 1 Mohit Bansal 1 5 2 0 2 3 ] . [ 1 0 2 8 1 0 . 3 0 5 2 : r aLayer-wise quantization is key technique for efficiently compressing large models without expensive retraining. Previous methods typically quantize the weights of each layer by uniformly optimizing the layer reconstruction loss across all output tokens. However, in this paper, we demonstrate that better-quantized models can be obtained by prioritizing learning from important tokens (e.g. which have large attention scores). Building on this finding, we propose RSQ (Rotate, Scale, then Quantize), which (1) applies rotations (orthogonal transformation) to the model to mitigate outliers (those with exceptionally large magnitude), (2) scales the token feature based on its importance, and (3) quantizes the model using the GPTQ framework with the second-order statistics computed by scaled tokens. To compute token importance, we explore both heuristic and dynamic strategies. Based on thorough analysis of all approaches, we adopt attention concentration, which uses attention scores of each token as its importance, as the best approach. We demonstrate that RSQ consistently outperforms baseline methods across multiple downstream tasks and three model families: LLaMA3, Mistral, and Qwen2.5. Additionally, models quantized with RSQ achieve superior performance on long-context tasks, further highlighting its effectiveness. Lastly, RSQ demonstrates generalizability across various setups, including different model sizes, calibration datasets, bit precisions, and quantization methods. Our code is available at https://github.com/ylsung/rsq. 1. Introduction Large language models (LLMs) (Georgiev et al., 2024; Achiam et al., 2023) have recently achieved great success and have transformed the landscape of artificial intelligence. 1Department of Computer Science, UNC at Chapel Hill. Correspondence to: Yi-Lin Sung <ylsung@cs.unc.edu>. However, the substantial computational demands associated with these models pose significant challenges to their usage and deployment, especially in resource-constrained scenarios. Weight quantization (Han et al., 2016; Wu et al., 2016) is widely used technique for reducing the computational costs of LLMs by representing weight values with fewer bits. Among various approaches, post-training quantization (PTQ) (Frantar & Alistarh, 2022; Liu et al., 2021) is particularly favored by practitioners, as it enables the quantization of pre-trained LLMs using only small calibration dataset, eliminating the need for expensive retraining. We focus on the layer-wise post-training quantization scheme (Hubara et al., 2020; Li et al., 2021; Frantar et al., 2023) that has been demonstrated to be both effective and efficient for quantizing large models. Layer-wise quantization methods quantize an LLMs weights one layer at time by minimizing the token-level feature distance between the outputs of the original and quantized weights (i.e., the layer reconstruction loss, WX WX2 2). Several advancements have been made to improve layer-wise quantization techniques in the past few years. For example, GPTQ (Frantar et al., 2023) improves the efficiency and stability to compute second-order statistics and their inverses. QuIP# (Tseng et al., 2024) and AQLM (Egiazarian et al., 2024) represent quantized weights with vectors rather than fixed scalars. Additionally, QuIP (Chee et al., 2023) and QuaRot (Ashkboos et al., 2024b) demonstrate through empirical studies that weight outliersparameters with unusually large magnitudescan be effectively mitigated by applying orthogonal transformations. Previous methods commonly perform layer-wise weight quantization by optimizing the layer reconstruction loss across all input tokens uniformly. However, research has shown that LLMs do not treat all tokens equally: (1) StreamingLLM (Xiao et al., 2024) shows that initial tokens often have strong attention scores, (2) H2O (Zhang et al., 2023) reveals that some tokens in KV cache contribute most of the attention values while decoding, and (3) RHO-1 (Lin et al., 2024c) demonstrates not all tokens are equal in training LLMs. Since quantized models inherently lose information due to the reduced capacity of lower bit representations, we argue that it should be particularly crucial RSQ: Learning from Important Tokens Leads to Better Quantized LLMs for them to focus on learning and preserving the most critical information during the quantization process to maximize their performance. Inspired by these insights, we reconsider the conventional approach in quantization methods by optimizing the layer reconstruction loss over only subset of important input tokens (i.e., using only the first 1/4 of the tokens). Our findings reveal that this strategy improves the quantized models accuracy across ten downstream tasks by up to 2.2%. Building on our findings and previous approaches, we propose RSQ to quantize the model in three steps: (1) rotate (orthogonally transform) the model to mitigate weight outliers, (2) scale the token feature based on its importance, and (3) quantize the weights using the GPTQ mechanism while leveraging token importance. We note that the token importance integrates seamlessly into the GPTQ framework in the third step, ensuring both compatibility and efficiency. Fig. 1 illustrates the three steps in RSQ. In this paper, we explore two categories of approaches for obtaining token importance: (1) heuristic approaches and (2) dynamic approaches. Within the heuristic category, we investigate methods such as First-N and First&Last-N, which prioritize initial tokens and combination of initial and final tokens for quantization, respectively. These approaches outperform the conventional quantization method of optimizing across all tokens, achieving peak performance when is roughly 510% of the total tokens. It is important to note that the initial or final tokens do not inherently contain more meaningful semantic information. Instead, their importance likely stems from their positional characteristics and their tendency to receive stronger attention scores (Xiao et al., 2024; Sun et al., 2024a). In this aforementioned approach, token importance was determined solely based on heuristics (e.g., token positions). To further improve performance beyond heuristic methods, we also explore several dynamic approaches for computing token importanc"
[05.03.2025 02:16] Mistral response. {"id": "cb789d0105954f899eb4aec5426d77df", "object": "chat.completion", "created": 1741141006, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\"Department of Computer Science, UNC at Chapel Hill\"]\n```"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1617, "total_tokens": 1637, "completion_tokens": 20}}
[05.03.2025 02:16] Response: ```python
["Department of Computer Science, UNC at Chapel Hill"]
```
[05.03.2025 02:16] Deleting PDF ./assets/pdf/2503.01820.pdf.
[05.03.2025 02:16] Success.
[05.03.2025 02:16] Downloading and parsing paper https://huggingface.co/papers/2503.01103.
[05.03.2025 02:16] Extra JSON file exists (./assets/json/2503.01103.json), skip PDF parsing.
[05.03.2025 02:16] Paper image links file exists (./assets/img_data/2503.01103.json), skip HTML parsing.
[05.03.2025 02:16] Success.
[05.03.2025 02:16] Downloading and parsing paper https://huggingface.co/papers/2502.16779.
[05.03.2025 02:16] Extra JSON file exists (./assets/json/2502.16779.json), skip PDF parsing.
[05.03.2025 02:16] Paper image links file exists (./assets/img_data/2502.16779.json), skip HTML parsing.
[05.03.2025 02:16] Success.
[05.03.2025 02:16] Downloading and parsing paper https://huggingface.co/papers/2503.00729.
[05.03.2025 02:16] Extra JSON file exists (./assets/json/2503.00729.json), skip PDF parsing.
[05.03.2025 02:16] Paper image links file exists (./assets/img_data/2503.00729.json), skip HTML parsing.
[05.03.2025 02:16] Success.
[05.03.2025 02:16] Downloading and parsing paper https://huggingface.co/papers/2502.20383.
[05.03.2025 02:16] Extra JSON file exists (./assets/json/2502.20383.json), skip PDF parsing.
[05.03.2025 02:16] Paper image links file exists (./assets/img_data/2502.20383.json), skip HTML parsing.
[05.03.2025 02:16] Success.
[05.03.2025 02:16] Downloading and parsing paper https://huggingface.co/papers/2503.02379.
[05.03.2025 02:16] Downloading paper 2503.02379 from http://arxiv.org/pdf/2503.02379v1...
[05.03.2025 02:16] Extracting affiliations from text.
[05.03.2025 02:16] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Jiwan Chung Saejin Kim Yongrae Jo Jaewoo Park Dongjun Min Youngjae Yu Yonsei University LG AI Research jiwan.chung.research@gmail.com 5 2 0 2 M 4 ] . [ 1 9 7 3 2 0 . 3 0 5 2 : r a "
[05.03.2025 02:16] Response: ```python
["Yonsei University", "LG AI Research"]
```
[05.03.2025 02:16] Deleting PDF ./assets/pdf/2503.02379.pdf.
[05.03.2025 02:17] Success.
[05.03.2025 02:17] Enriching papers with extra data.
[05.03.2025 02:17] ********************************************************************************
[05.03.2025 02:17] Abstract 0. Reinforcement Fine-Tuning (RFT) in Large Reasoning Models like OpenAI o1 learns from feedback on its answers, which is especially useful in applications when fine-tuning data is scarce. Recent open-source work like DeepSeek-R1 demonstrates that reinforcement learning with verifiable reward is one ke...
[05.03.2025 02:17] ********************************************************************************
[05.03.2025 02:17] Abstract 1. We introduce Phi-4-Mini and Phi-4-Multimodal, compact yet highly capable language and multimodal models. Phi-4-Mini is a 3.8-billion-parameter language model trained on high-quality web and synthetic data, significantly outperforming recent open-source models of similar size and matching the perform...
[05.03.2025 02:17] ********************************************************************************
[05.03.2025 02:17] Abstract 2. Neural Radiance Fields and 3D Gaussian Splatting have revolutionized 3D reconstruction and novel-view synthesis task. However, achieving photorealistic rendering from extreme novel viewpoints remains challenging, as artifacts persist across representations. In this work, we introduce Difix3D+, a nov...
[05.03.2025 02:17] ********************************************************************************
[05.03.2025 02:17] Abstract 3. Recent advancements in music generation have garnered significant attention, yet existing approaches face critical limitations. Some current generative models can only synthesize either the vocal track or the accompaniment track. While some models can generate combined vocal and accompaniment, they ...
[05.03.2025 02:17] ********************************************************************************
[05.03.2025 02:17] Abstract 4. Recently, generative retrieval-based recommendation systems have emerged as a promising paradigm. However, most modern recommender systems adopt a retrieve-and-rank strategy, where the generative model functions only as a selector during the retrieval stage. In this paper, we propose OneRec, which r...
[05.03.2025 02:17] ********************************************************************************
[05.03.2025 02:17] Abstract 5. Uncertainty estimation is crucial for evaluating Large Language Models (LLMs), particularly in high-stakes domains where incorrect answers result in significant consequences. Numerous approaches consider this problem, while focusing on a specific type of uncertainty, ignoring others. We investigate ...
[05.03.2025 02:17] ********************************************************************************
[05.03.2025 02:17] Abstract 6. Generating ultra-long sequences with large language models (LLMs) has become increasingly crucial but remains a highly time-intensive task, particularly for sequences up to 100K tokens. While traditional speculative decoding methods exist, simply extending their generation limits fails to accelerate...
[05.03.2025 02:17] ********************************************************************************
[05.03.2025 02:17] Abstract 7. Test-time inference has emerged as a powerful paradigm for enabling language models to ``think'' longer and more carefully about complex challenges, much like skilled human experts. While reinforcement learning (RL) can drive self-improvement in language models on verifiable tasks, some models exhib...
[05.03.2025 02:17] ********************************************************************************
[05.03.2025 02:17] Abstract 8. Transformers with linear recurrent modeling offer linear-time training and constant-memory inference. Despite their demonstrated efficiency and performance, pretraining such non-standard architectures from scratch remains costly and risky. The linearization of large language models (LLMs) transforms...
[05.03.2025 02:17] ********************************************************************************
[05.03.2025 02:17] Abstract 9. Analyzing large datasets requires responsive query execution, but executing SQL queries on massive datasets can be slow. This paper explores whether query execution can begin even before the user has finished typing, allowing results to appear almost instantly. We propose SpeQL, a system that levera...
[05.03.2025 02:17] ********************************************************************************
[05.03.2025 02:17] Abstract 10. User-generated content (UGC) communities, especially those featuring multimodal content, improve user experiences by integrating visual and textual information into results (or items). The challenge of improving user experiences in complex systems with search and recommendation (S\&R) services has d...
[05.03.2025 02:17] ********************************************************************************
[05.03.2025 02:17] Abstract 11. Increasing test-time computation is a straightforward approach to enhancing the quality of responses in Large Language Models (LLMs). While Best-of-N sampling and Self-Consistency with majority voting are simple and effective, they require a fixed number of sampling responses for each query, regardl...
[05.03.2025 02:17] ********************************************************************************
[05.03.2025 02:17] Abstract 12. Large language models (LLMs) exhibit exceptional performance across a wide range of tasks; however, their token-by-token autoregressive generation process significantly hinders inference speed. Speculative decoding presents a promising draft-then-verify framework that reduces generation latency whil...
[05.03.2025 02:17] ********************************************************************************
[05.03.2025 02:17] Abstract 13. Existing pretraining data mixing methods for large language models (LLMs) typically follow a domain-wise methodology, a top-down process that first determines domain weights and then performs uniform data sampling across each domain. However, these approaches neglect significant inter-domain overlap...
[05.03.2025 02:17] ********************************************************************************
[05.03.2025 02:17] Abstract 14. Diffusion models have achieved great success in generating 2D images. However, the quality and generalizability of 3D content generation remain limited. State-of-the-art methods often require large-scale 3D assets for training, which are challenging to collect. In this work, we introduce Kiss3DGen (...
[05.03.2025 02:17] ********************************************************************************
[05.03.2025 02:17] Abstract 15. Existing Existing automatic audio generation methods struggle to generate podcast-like audio programs effectively. The key challenges lie in in-depth content generation, appropriate and expressive voice production. This paper proposed PodAgent, a comprehensive framework for creating audio programs. ...
[05.03.2025 02:17] ********************************************************************************
[05.03.2025 02:17] Abstract 16. Human readers can efficiently comprehend scrambled words, a phenomenon known as Typoglycemia, primarily by relying on word form; if word form alone is insufficient, they further utilize contextual cues for interpretation. While advanced large language models (LLMs) exhibit similar abilities, the und...
[05.03.2025 02:17] ********************************************************************************
[05.03.2025 02:17] Abstract 17. Large Language Models (LLMs) have reshaped code generation by synergizing their exceptional comprehension of natural language and programming syntax, thereby substantially boosting developer productivity. These advancements have prompted numerous efforts to quantitatively evaluate their coding capab...
[05.03.2025 02:17] ********************************************************************************
[05.03.2025 02:17] Abstract 18. Selecting high-quality training data from a larger pool is a crucial step when instruction-tuning language models, as carefully curated datasets often produce models that outperform those trained on much larger, noisier datasets. Automated data selection approaches for instruction-tuning are typical...
[05.03.2025 02:17] ********************************************************************************
[05.03.2025 02:17] Abstract 19. Large Language Models (LLMs) have demonstrated impressive real-world utility, exemplifying artificial useful intelligence (AUI). However, their ability to reason adaptively and robustly -- the hallmarks of artificial general intelligence (AGI) -- remains fragile. While LLMs seemingly succeed in comm...
[05.03.2025 02:17] ********************************************************************************
[05.03.2025 02:17] Abstract 20. This paper investigates the potential for large language models (LLMs) to develop private tonal languages for machine-to-machine (M2M) communication. Inspired by cryptophasia in human twins (affecting up to 50% of twin births) and natural tonal languages like Mandarin and Vietnamese, we implement a ...
[05.03.2025 02:17] ********************************************************************************
[05.03.2025 02:17] Abstract 21. Text-to-video generative models convert textual prompts into dynamic visual content, offering wide-ranging applications in film production, gaming, and education. However, their real-world performance often falls short of user expectations. One key reason is that these models have not been trained o...
[05.03.2025 02:17] ********************************************************************************
[05.03.2025 02:17] Abstract 22. Layer-wise quantization is a key technique for efficiently compressing large models without expensive retraining. Previous methods typically quantize the weights of each layer by "uniformly" optimizing the layer reconstruction loss across all output tokens. However, in this paper, we demonstrate tha...
[05.03.2025 02:17] ********************************************************************************
[05.03.2025 02:17] Abstract 23. While likelihood-based generative models, particularly diffusion and autoregressive models, have achieved remarkable fidelity in visual generation, the maximum likelihood estimation (MLE) objective inherently suffers from a mode-covering tendency that limits the generation quality under limited mode...
[05.03.2025 02:17] ********************************************************************************
[05.03.2025 02:17] Abstract 24. Room layout estimation from multiple-perspective images is poorly investigated due to the complexities that emerge from multi-view geometry, which requires muti-step solutions such as camera intrinsic and extrinsic estimation, image matching, and triangulation. However, in 3D reconstruction, the adv...
[05.03.2025 02:17] ********************************************************************************
[05.03.2025 02:17] Abstract 25. Large Language Models (LLMs) exhibit remarkable capabilities in the hierarchical decomposition of complex tasks through semantic reasoning. However, their application in embodied systems faces challenges in ensuring reliable execution of subtask sequences and achieving one-shot success in long-term ...
[05.03.2025 02:17] ********************************************************************************
[05.03.2025 02:17] Abstract 26. Recent advancements in Web AI agents have demonstrated remarkable capabilities in addressing complex web navigation tasks. However, emerging research shows that these agents exhibit greater vulnerability compared to standalone Large Language Models (LLMs), despite both being built upon the same safe...
[05.03.2025 02:17] ********************************************************************************
[05.03.2025 02:17] Abstract 27. As large language models expand beyond natural language to domains such as mathematics, multimodal understanding, and embodied agents, tokens increasingly reflect metric relationships rather than purely linguistic meaning. We introduce DIST2Loss, a distance-aware framework designed to train autoregr...
[05.03.2025 02:17] Read previous papers.
[05.03.2025 02:17] Generating reviews via LLM API.
[05.03.2025 02:17] Using data from previous issue: {"categories": ["#multimodal", "#open_source", "#cv", "#optimization", "#rlhf", "#reasoning", "#training", "#rl"], "emoji": "üî¨", "ru": {"title": "Visual-RFT: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–µ –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Visual Reinforcement Fine-Tuning (Visual-RFT) - –º–µ—Ç–æ–¥, 
[05.03.2025 02:17] Using data from previous issue: {"categories": ["#multimodal", "#small_models", "#data", "#agi", "#synthetic", "#long_context", "#optimization", "#dataset", "#training"], "emoji": "üß†", "ru": {"title": "–ö–æ–º–ø–∞–∫—Ç–Ω—ã–µ –º–æ–¥–µ–ª–∏ —Å –±–æ–ª—å—à–∏–º–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è–º–∏: –ø—Ä–æ—Ä—ã–≤ –≤ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –ò–ò", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã –¥–≤–µ –Ω–æ–≤—ã–µ –º–æ–¥–µ–ª–∏: Phi-4-Mini –∏ Phi-4
[05.03.2025 02:17] Using data from previous issue: {"categories": ["#3d", "#diffusion"], "emoji": "üñºÔ∏è", "ru": {"title": "–û–¥–Ω–æ—à–∞–≥–æ–≤–∞—è –¥–∏—Ñ—Ñ—É–∑–∏—è –¥–ª—è —Ñ–æ—Ç–æ—Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ–π 3D-—Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏", "desc": "Difix3D+ - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é 3D-—Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –∏ —Å–∏–Ω—Ç–µ–∑–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –Ω–æ–≤—ã—Ö —Ä–∞–∫—É—Ä—Å–æ–≤. –í –µ–≥–æ –æ—Å–Ω–æ–≤–µ –ª–µ–∂–∏—Ç Difix - –æ–¥–Ω–æ—à–∞–≥–æ–≤–∞—è –º–æ–¥–µ–ª—å –¥–∏—Ñ—Ñ—É–∑–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ
[05.03.2025 02:17] Using data from previous issue: {"categories": ["#diffusion", "#inference", "#dataset", "#open_source", "#audio"], "emoji": "üéµ", "ru": {"title": "DiffRhythm: –ë—ã—Å—Ç—Ä–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ–ª–Ω—ã—Ö –ø–µ—Å–µ–Ω —Å –ø–æ–º–æ—â—å—é –ª–∞—Ç–µ–Ω—Ç–Ω–æ–π –¥–∏—Ñ—Ñ—É–∑–∏–∏", "desc": "DiffRhythm - —ç—Ç–æ –ø–µ—Ä–≤–∞—è –º–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–µ—Å–µ–Ω –Ω–∞ –æ—Å–Ω–æ–≤–µ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–π –¥–∏—Ñ—Ñ—É–∑–∏–∏, —Å–ø–æ—Å–æ–±–Ω–∞—è —Å–∏–Ω—Ç–µ–∑–∏—Ä–æ–≤–∞—Ç—å –ø–æ
[05.03.2025 02:17] Using data from previous issue: {"categories": ["#alignment", "#rlhf", "#rag", "#games", "#training", "#optimization"], "emoji": "üé•", "ru": {"title": "OneRec: –ï–¥–∏–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Ä–µ–≤–æ–ª—é—Ü–∏–∏ –≤ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ç–µ–ª—å–Ω—ã—Ö —Å–∏—Å—Ç–µ–º–∞—Ö", "desc": "OneRec - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è –µ–¥–∏–Ω—É—é –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—É—é –º–æ–¥–µ–ª—å –≤–º–µ—Å—Ç–æ –∫–∞—Å–∫–∞
[05.03.2025 02:17] Using data from previous issue: {"categories": ["#ethics", "#hallucinations", "#benchmark", "#reasoning", "#data", "#multilingual"], "emoji": "ü§ñ", "ru": {"title": "–≠–Ω—Ç—Ä–æ–ø–∏—è –æ—Ç–≤–µ—Ç–æ–≤ –∫–∞–∫ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏ LLM –≤ –∑–∞–¥–∞—á–∞—Ö —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º –≤—ã–±–æ—Ä–æ–º", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ –æ—Ü–µ–Ω–∫–µ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏ –≤ –∫—Ä—É–ø–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª
[05.03.2025 02:17] Using data from previous issue: {"categories": ["#training", "#architecture", "#long_context", "#inference", "#optimization"], "emoji": "‚ö°", "ru": {"title": "TOKENSWIFT: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ —Å–∫–æ—Ä–æ—Å—Ç–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–≤–µ—Ä—Ö–¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ TOKENSWIFT - –Ω–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–≤–µ—Ä—Ö–¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞
[05.03.2025 02:17] Using data from previous issue: {"categories": ["#training", "#optimization", "#rl", "#reasoning"], "emoji": "üß†", "ru": {"title": "–ö–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã–µ –Ω–∞–≤—ã–∫–∏ - –∫–ª—é—á –∫ —Å–∞–º–æ—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–∏—é —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∫ —Å–∞–º–æ—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–∏—é –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –Ω–∞–ª–∏—á–∏—è —É –Ω–∏—Ö –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö –∫–æ–≥
[05.03.2025 02:17] Using data from previous issue: {"categories": ["#architecture", "#training", "#optimization", "#benchmark"], "emoji": "üî¢", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –ª–∏–Ω–µ–∞—Ä–∏–∑–∞—Ü–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–î–∞–Ω–Ω–∞—è —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ Liger –¥–ª—è –ª–∏–Ω–µ–∞—Ä–∏–∑–∞—Ü–∏–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≤ –≥–µ–π—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ª–∏–Ω–µ–π–Ω–æ-—Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω—ã–µ 
[05.03.2025 02:17] Using data from previous issue: {"categories": ["#dataset", "#data", "#benchmark"], "emoji": "‚ö°", "ru": {"title": "–ú–æ–ª–Ω–∏–µ–Ω–æ—Å–Ω—ã–µ SQL-–∑–∞–ø—Ä–æ—Å—ã —Å –ø–æ–º–æ—â—å—é –ø—Ä–µ–¥–∏–∫—Ç–∏–≤–Ω–æ–π –∞–Ω–∞–ª–∏—Ç–∏–∫–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–∏—Å—Ç–µ–º—É SpeQL, –∏—Å–ø–æ–ª—å–∑—É—é—â—É—é –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è SQL-–∑–∞–ø—Ä–æ—Å–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è. SpeQL –ø—Ä–µ–¥—É–≥–∞–¥—ã–≤–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∑–∞
[05.03.2025 02:17] Using data from previous issue: {"categories": ["#multimodal", "#dataset", "#rag"], "emoji": "üîç", "ru": {"title": "Qilin: –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–∏—Å–∫–∞ –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö Qilin –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞, —Å–æ–±—Ä–∞–Ω–Ω—ã–π –Ω–∞ –ø–ª–∞—Ç—Ñ–æ—Ä–º–µ Xiaohongshu. –î–∞—Ç–∞—Å–µ—Ç –≤–∫–ª—é—á–∞–µ—Ç –ø–æ–ª—å
[05.03.2025 02:17] Using data from previous issue: {"categories": ["#optimization", "#training", "#inference"], "emoji": "üéØ", "ru": {"title": "–ü–æ–≤—ã—à–µ–Ω–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ LLM —á–µ—Ä–µ–∑ –∫–∞–ª–∏–±—Ä–æ–≤–∫—É —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏", "desc": "–≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –º–µ—Ç–æ–¥ Self-Calibration –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∫—Ä—É–ø–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –ø—Ä–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏. –ú–µ—Ç–æ–¥ –æ—Å–Ω–æ–≤–∞
[05.03.2025 02:17] Using data from previous issue: {"categories": ["#inference", "#training", "#optimization"], "emoji": "üöÄ", "ru": {"title": "DuoDecoding: –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ —É—Å–∫–æ—Ä–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —É—Å–∫–æ—Ä–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ –±–æ–ª—å—à–∏–º–∏ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ (LLM) –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º DuoDecoding. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø
[05.03.2025 02:17] Using data from previous issue: {"categories": ["#transfer_learning", "#training", "#optimization", "#data"], "emoji": "üîÄ", "ru": {"title": "SampleMix: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ —Å–º–µ—à–∏–≤–∞–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è LLM", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–º–µ—à–∏–≤–∞–Ω–∏—é –ø—Ä–µ–¥–æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM), –Ω–∞–∑–≤–∞–Ω–Ω—ã–π SampleMix. –í –æ—Ç–ª–∏
[05.03.2025 02:17] Using data from previous issue: {"categories": ["#cv", "#diffusion", "#3d"], "emoji": "üé®", "ru": {"title": "–ü—Ä–æ—Å—Ç–∞—è –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è 3D-–≥–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ 2D-–¥–∏—Ñ—Ñ—É–∑–∏–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Kiss3DGen - —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏ —É–ª—É—á—à–µ–Ω–∏—è 3D-–æ–±—ä–µ–∫—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –¥–∏—Ñ—Ñ—É–∑–∏–∏ –¥
[05.03.2025 02:17] Using data from previous issue: {"categories": ["#games", "#audio", "#interpretability", "#benchmark", "#optimization", "#multimodal"], "emoji": "üéôÔ∏è", "ru": {"title": "PodAgent: –ò–ò-–≤–µ–¥—É—â–∏–π –¥–ª—è –ø–æ–¥–∫–∞—Å—Ç–æ–≤ –Ω–æ–≤–æ–≥–æ –ø–æ–∫–æ–ª–µ–Ω–∏—è", "desc": "PodAgent - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–æ–∑–¥–∞–Ω–∏—è –∞—É–¥–∏–æ–ø—Ä–æ–≥—Ä–∞–º–º –≤ —Å—Ç–∏–ª–µ –ø–æ–¥–∫–∞—Å—Ç–æ–≤. –û–Ω–∞ –∏—Å–ø–æ–ª—å
[05.03.2025 02:17] Using data from previous issue: {"categories": ["#training", "#interpretability", "#data", "#multimodal", "#alignment"], "emoji": "üîÄ", "ru": {"title": "–§–æ—Ä–º–∞ —Å–ª–æ–≤–∞ - –∫–ª—é—á –∫ –ø–æ–Ω–∏–º–∞–Ω–∏—é –ø–µ—Ä–µ–º–µ—à–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –ò–ò", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –ø–æ–Ω–∏–º–∞—Ç—å –ø–µ—Ä–µ–º–µ—à–∞–Ω–Ω—ã–µ —Å–ª–æ–≤–∞, –ø–æ–¥–æ–±–Ω–æ –ª—é–¥—è–º. 
[05.03.2025 02:17] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#leakage", "#open_source"], "emoji": "üèüÔ∏è", "ru": {"title": "CodeArena: –°–ø—Ä–∞–≤–µ–¥–ª–∏–≤–∞—è –∞—Ä–µ–Ω–∞ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ LLM –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞", "desc": "CodeArena - —ç—Ç–æ –Ω–æ–≤–∞—è –æ–Ω–ª–∞–π–Ω-–ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞ –±–æ–ª—å—à–∏–º–∏ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ (LLM). –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∫–æ–ª–ª–µ–∫—Ç–∏–≤
[05.03.2025 02:17] Using data from previous issue: {"categories": ["#data", "#open_source", "#optimization", "#dataset", "#training"], "emoji": "üîç", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –æ—Ç–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π: –º–µ–Ω—å—à–µ –∑–Ω–∞—á–∏—Ç –±–æ–ª—å—à–µ", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –º–µ—Ç–æ–¥—ã –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ—Ç–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∏–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ
[05.03.2025 02:17] Using data from previous issue: {"categories": ["#agi", "#transfer_learning", "#architecture", "#rl", "#synthetic", "#training", "#reasoning"], "emoji": "üß†", "ru": {"title": "–û—Ç–¥–µ–ª—è—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –æ—Ç –∑–Ω–∞–Ω–∏–π: –ø—É—Ç—å –∫ AGI", "desc": "–°—Ç–∞—Ç—å—è —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≤ –æ–±–ª–∞—Å—Ç–∏ –æ–±–æ–±—â–µ–Ω–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, –Ω–µ—Å–º–æ
[05.03.2025 02:17] Using data from previous issue: {"categories": ["#security", "#ethics", "#audio", "#multimodal"], "emoji": "üéµ", "ru": {"title": "–¢–æ–Ω–∞–ª—å–Ω—ã–µ —è–∑—ã–∫–∏: —Å–µ–∫—Ä–µ—Ç–Ω—ã–π –∫–æ–¥ –º–∞—à–∏–Ω –±—É–¥—É—â–µ–≥–æ", "desc": "–≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∏–∑—É—á–∞–µ—Ç –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –ø—Ä–∏–≤–∞—Ç–Ω—ã—Ö —Ç–æ–Ω–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤ –¥–ª—è –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏ –º–µ–∂–¥—É –º–∞—à–∏–Ω–∞–º–∏. –ê–≤—Ç–æ—Ä
[05.03.2025 02:17] Using data from previous issue: {"categories": ["#video", "#dataset", "#data", "#games", "#open_source"], "emoji": "üé¨", "ru": {"title": "VideoUFO: –ù–æ–≤—ã–π —ç—Ç–∞–ª–æ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –ø–æ –∑–∞–ø—Ä–æ—Å—É", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç VideoUFO - –Ω–æ–≤—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –ø–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–º—É –æ–ø–∏—Å–∞–Ω–∏—é. –≠—Ç–æ—Ç –¥–∞—Ç
[05.03.2025 02:17] Querying the API.
[05.03.2025 02:17] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Layer-wise quantization is a key technique for efficiently compressing large models without expensive retraining. Previous methods typically quantize the weights of each layer by "uniformly" optimizing the layer reconstruction loss across all output tokens. However, in this paper, we demonstrate that better-quantized models can be obtained by prioritizing learning from important tokens (e.g. which have large attention scores). Building on this finding, we propose RSQ (Rotate, Scale, then Quantize), which (1) applies rotations (orthogonal transformation) to the model to mitigate outliers (those with exceptionally large magnitude), (2) scales the token feature based on its importance, and (3) quantizes the model using the GPTQ framework with the second-order statistics computed by scaled tokens. To compute token importance, we explore both heuristic and dynamic strategies. Based on a thorough analysis of all approaches, we adopt attention concentration, which uses attention scores of each token as its importance, as the best approach. We demonstrate that RSQ consistently outperforms baseline methods across multiple downstream tasks and three model families: LLaMA3, Mistral, and Qwen2.5. Additionally, models quantized with RSQ achieve superior performance on long-context tasks, further highlighting its effectiveness. Lastly, RSQ demonstrates generalizability across various setups, including different model sizes, calibration datasets, bit precisions, and quantization methods.
[05.03.2025 02:17] Response: {
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º RSQ (Rotate, Scale, then Quantize). –ú–µ—Ç–æ–¥ –ø—Ä–∏–º–µ–Ω—è–µ—Ç –æ—Ä—Ç–æ–≥–æ–Ω–∞–ª—å–Ω—ã–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è –≤—ã–±—Ä–æ—Å–æ–≤, –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏ —Ç–æ–∫–µ–Ω–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Ö –≤–∞–∂–Ω–æ—Å—Ç–∏ –∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ GPTQ –¥–ª—è –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è. RSQ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –±–∞–∑–æ–≤—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö –∏ –º–æ–¥–µ–ª—è—Ö, –≤–∫–ª—é—á–∞—è LLaMA3, Mistral –∏ Qwen2.5. –ú–µ—Ç–æ–¥ —Ç–∞–∫–∂–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –Ω–∞ –∑–∞–¥–∞—á–∞—Ö —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –∏ –æ–±–æ–±—â–∞–µ–º–æ—Å—Ç—å –Ω–∞ —Ä–∞–∑–Ω—ã–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏.",
  "emoji": "üî¢",
  "title": "RSQ: –£–º–Ω–æ–µ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Å–∂–∞—Ç–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π"
}
[05.03.2025 02:17] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Layer-wise quantization is a key technique for efficiently compressing large models without expensive retraining. Previous methods typically quantize the weights of each layer by "uniformly" optimizing the layer reconstruction loss across all output tokens. However, in this paper, we demonstrate that better-quantized models can be obtained by prioritizing learning from important tokens (e.g. which have large attention scores). Building on this finding, we propose RSQ (Rotate, Scale, then Quantize), which (1) applies rotations (orthogonal transformation) to the model to mitigate outliers (those with exceptionally large magnitude), (2) scales the token feature based on its importance, and (3) quantizes the model using the GPTQ framework with the second-order statistics computed by scaled tokens. To compute token importance, we explore both heuristic and dynamic strategies. Based on a thorough analysis of all approaches, we adopt attention concentration, which uses attention scores of each token as its importance, as the best approach. We demonstrate that RSQ consistently outperforms baseline methods across multiple downstream tasks and three model families: LLaMA3, Mistral, and Qwen2.5. Additionally, models quantized with RSQ achieve superior performance on long-context tasks, further highlighting its effectiveness. Lastly, RSQ demonstrates generalizability across various setups, including different model sizes, calibration datasets, bit precisions, and quantization methods."

[05.03.2025 02:17] Response: ```python
["INFERENCE", "TRAINING"]
```
[05.03.2025 02:17] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Layer-wise quantization is a key technique for efficiently compressing large models without expensive retraining. Previous methods typically quantize the weights of each layer by "uniformly" optimizing the layer reconstruction loss across all output tokens. However, in this paper, we demonstrate that better-quantized models can be obtained by prioritizing learning from important tokens (e.g. which have large attention scores). Building on this finding, we propose RSQ (Rotate, Scale, then Quantize), which (1) applies rotations (orthogonal transformation) to the model to mitigate outliers (those with exceptionally large magnitude), (2) scales the token feature based on its importance, and (3) quantizes the model using the GPTQ framework with the second-order statistics computed by scaled tokens. To compute token importance, we explore both heuristic and dynamic strategies. Based on a thorough analysis of all approaches, we adopt attention concentration, which uses attention scores of each token as its importance, as the best approach. We demonstrate that RSQ consistently outperforms baseline methods across multiple downstream tasks and three model families: LLaMA3, Mistral, and Qwen2.5. Additionally, models quantized with RSQ achieve superior performance on long-context tasks, further highlighting its effectiveness. Lastly, RSQ demonstrates generalizability across various setups, including different model sizes, calibration datasets, bit precisions, and quantization methods."

[05.03.2025 02:17] Response: ```python
["OPTIMIZATION", "LONG_CONTEXT"]
```
[05.03.2025 02:17] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces a novel method called RSQ for quantizing large machine learning models efficiently. Unlike traditional methods that optimize layer reconstruction loss uniformly, RSQ focuses on important tokens identified by their attention scores to improve model performance. The process involves rotating the model to handle outliers, scaling token features based on their importance, and applying quantization using the GPTQ framework. The results show that RSQ outperforms existing methods across various tasks and model families, especially in long-context scenarios, demonstrating its versatility and effectiveness.","title":"Prioritizing Important Tokens for Better Model Quantization"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper introduces a novel method called RSQ for quantizing large machine learning models efficiently. Unlike traditional methods that optimize layer reconstruction loss uniformly, RSQ focuses on important tokens identified by their attention scores to improve model performance. The process involves rotating the model to handle outliers, scaling token features based on their importance, and applying quantization using the GPTQ framework. The results show that RSQ outperforms existing methods across various tasks and model families, especially in long-context scenarios, demonstrating its versatility and effectiveness.', title='Prioritizing Important Tokens for Better Model Quantization'))
[05.03.2025 02:17] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Â±ÇÁ∫ßÈáèÂåñÊòØ‰∏ÄÁßçÊúâÊïàÂéãÁº©Â§ßÂûãÊ®°ÂûãÁöÑÊäÄÊúØÔºåÊó†ÈúÄÊòÇË¥µÁöÑÈáçÊñ∞ËÆ≠ÁªÉ„ÄÇ‰ª•ÂæÄÁöÑÊñπÊ≥ïÈÄöÂ∏∏ÈÄöËøáÂùáÂåÄ‰ºòÂåñÂ±ÇÈáçÂª∫ÊçüÂ§±Êù•ÈáèÂåñÊØèÂ±ÇÁöÑÊùÉÈáçÔºå‰ΩÜÊàë‰ª¨ÂèëÁé∞‰ºòÂÖàÂ≠¶‰π†ÈáçË¶ÅÁöÑÊ†áËÆ∞Ôºà‰æãÂ¶ÇÔºåÂÖ∑ÊúâËæÉÂ§ßÊ≥®ÊÑèÂäõÂàÜÊï∞ÁöÑÊ†áËÆ∞ÔºâÂèØ‰ª•Ëé∑ÂæóÊõ¥Â•ΩÁöÑÈáèÂåñÊ®°Âûã„ÄÇÊàë‰ª¨ÊèêÂá∫ÁöÑRSQÊñπÊ≥ïÈÄöËøáÊóãËΩ¨„ÄÅÁº©ÊîæÂíåÈáèÂåñÁöÑÊ≠•È™§ÔºåÊîπÂñÑ‰∫ÜÊ®°ÂûãÁöÑÈáèÂåñÊïàÊûúÔºåÂπ∂Âú®Â§ö‰∏™‰∏ãÊ∏∏‰ªªÂä°‰∏≠Ë∂ÖË∂ä‰∫ÜÂü∫Á∫øÊñπÊ≥ï„ÄÇRSQÂú®Èïø‰∏ä‰∏ãÊñá‰ªªÂä°‰∏≠Ë°®Áé∞‰ºòÂºÇÔºå‰∏îÂú®‰∏çÂêåÊ®°ÂûãËßÑÊ®°ÂíåÈáèÂåñÊñπÊ≥ï‰∏≠ÂÖ∑ÊúâËâØÂ•ΩÁöÑÈÄöÁî®ÊÄß„ÄÇ","title":"‰ºòÂÖàÂ≠¶‰π†ÈáçË¶ÅÊ†áËÆ∞ÔºåÂÆûÁé∞È´òÊïàÈáèÂåñ"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Â±ÇÁ∫ßÈáèÂåñÊòØ‰∏ÄÁßçÊúâÊïàÂéãÁº©Â§ßÂûãÊ®°ÂûãÁöÑÊäÄÊúØÔºåÊó†ÈúÄÊòÇË¥µÁöÑÈáçÊñ∞ËÆ≠ÁªÉ„ÄÇ‰ª•ÂæÄÁöÑÊñπÊ≥ïÈÄöÂ∏∏ÈÄöËøáÂùáÂåÄ‰ºòÂåñÂ±ÇÈáçÂª∫ÊçüÂ§±Êù•ÈáèÂåñÊØèÂ±ÇÁöÑÊùÉÈáçÔºå‰ΩÜÊàë‰ª¨ÂèëÁé∞‰ºòÂÖàÂ≠¶‰π†ÈáçË¶ÅÁöÑÊ†áËÆ∞Ôºà‰æãÂ¶ÇÔºåÂÖ∑ÊúâËæÉÂ§ßÊ≥®ÊÑèÂäõÂàÜÊï∞ÁöÑÊ†áËÆ∞ÔºâÂèØ‰ª•Ëé∑ÂæóÊõ¥Â•ΩÁöÑÈáèÂåñÊ®°Âûã„ÄÇÊàë‰ª¨ÊèêÂá∫ÁöÑRSQÊñπÊ≥ïÈÄöËøáÊóãËΩ¨„ÄÅÁº©ÊîæÂíåÈáèÂåñÁöÑÊ≠•È™§ÔºåÊîπÂñÑ‰∫ÜÊ®°ÂûãÁöÑÈáèÂåñÊïàÊûúÔºåÂπ∂Âú®Â§ö‰∏™‰∏ãÊ∏∏‰ªªÂä°‰∏≠Ë∂ÖË∂ä‰∫ÜÂü∫Á∫øÊñπÊ≥ï„ÄÇRSQÂú®Èïø‰∏ä‰∏ãÊñá‰ªªÂä°‰∏≠Ë°®Áé∞‰ºòÂºÇÔºå‰∏îÂú®‰∏çÂêåÊ®°ÂûãËßÑÊ®°ÂíåÈáèÂåñÊñπÊ≥ï‰∏≠ÂÖ∑ÊúâËâØÂ•ΩÁöÑÈÄöÁî®ÊÄß„ÄÇ', title='‰ºòÂÖàÂ≠¶‰π†ÈáçË¶ÅÊ†áËÆ∞ÔºåÂÆûÁé∞È´òÊïàÈáèÂåñ'))
[05.03.2025 02:17] Using data from previous issue: {"categories": ["#training", "#diffusion", "#cv", "#optimization"], "emoji": "üöÄ", "ru": {"title": "DDO: –ü—Ä–æ—Ä—ã–≤ –≤ –æ–±—É—á–µ–Ω–∏–∏ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –±–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π MLE", "desc": "–ê–≤—Ç–æ—Ä—ã —Å—Ç–∞—Ç—å–∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Direct Discriminative Optimization (DDO). –≠—Ç–æ
[05.03.2025 02:17] Using data from previous issue: {"categories": ["#3d", "#optimization", "#cv", "#synthetic"], "emoji": "üè†", "ru": {"title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –æ—Ü–µ–Ω–∫–µ –ø–ª–∞–Ω–∏—Ä–æ–≤–∫–∏ –ø–æ–º–µ—â–µ–Ω–∏–π: –æ—Ç –º–Ω–æ–∂–µ—Å—Ç–≤–∞ —à–∞–≥–æ–≤ –∫ –µ–¥–∏–Ω–æ–º—É —Ä–µ—à–µ–Ω–∏—é", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Plane-DUSt3R - –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø–ª–∞–Ω–∏—Ä–æ–≤–∫–∏ –ø–æ–º–µ—â–µ–Ω–∏–π –ø–æ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º —Ä–∞–∫—É—Ä—Å–∞–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. 
[05.03.2025 02:17] Using data from previous issue: {"categories": ["#architecture", "#reasoning", "#open_source", "#robotics", "#agents", "#optimization"], "emoji": "ü§ñ", "ru": {"title": "CLEA: –ü–æ–≤—ã—à–µ–Ω–∏–µ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á —Ä–æ–±–æ—Ç–∞–º–∏ —Å –ø–æ–º–æ—â—å—é —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º CLEA (Closed-Loop Emb
[05.03.2025 02:17] Using data from previous issue: {"categories": ["#benchmark", "#security", "#agents"], "emoji": "üï∏Ô∏è", "ru": {"title": "–†–∞—Å–∫—Ä—ã—Ç–∏–µ —É—è–∑–≤–∏–º–æ—Å—Ç–µ–π –≤–µ–±-–∞–≥–µ–Ω—Ç–æ–≤ –ò–ò: –ø—É—Ç—å –∫ –±–æ–ª–µ–µ –±–µ–∑–æ–ø–∞—Å–Ω—ã–º —Å–∏—Å—Ç–µ–º–∞–º", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –≤–µ–±-–∞–≥–µ–Ω—Ç—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ –±–æ–ª–µ–µ —É—è–∑–≤–∏–º—ã, —á–µ–º –∞–≤—Ç–æ–Ω–æ–º–Ω—ã–µ –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (
[05.03.2025 02:17] Querying the API.
[05.03.2025 02:17] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

As large language models expand beyond natural language to domains such as mathematics, multimodal understanding, and embodied agents, tokens increasingly reflect metric relationships rather than purely linguistic meaning. We introduce DIST2Loss, a distance-aware framework designed to train autoregressive discrete models by leveraging predefined distance relationships among output tokens. At its core, DIST2Loss transforms continuous exponential family distributions derived from inherent distance metrics into discrete, categorical optimization targets compatible with the models' architectures. This approach enables the models to learn and preserve meaningful distance relationships during token generation while maintaining compatibility with existing architectures. Empirical evaluations show consistent performance gains in diverse multimodal applications, including visual grounding, robotic manipulation, generative reward modeling, and image generation using vector-quantized features. These improvements are pronounced in cases of limited training data, highlighting DIST2Loss's effectiveness in resource-constrained settings.
[05.03.2025 02:17] Response: {
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç DIST2Loss - –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã—Ö –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, —É—á–∏—Ç—ã–≤–∞—é—â–∏–π –º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–µ –æ—Ç–Ω–æ—à–µ–Ω–∏—è –º–µ–∂–¥—É —Ç–æ–∫–µ–Ω–∞–º–∏. DIST2Loss –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–≥–æ —Å–µ–º–µ–π—Å—Ç–≤–∞ –≤ –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ü–µ–ª–∏, —Å–æ–≤–º–µ—Å—Ç–∏–º—ã–µ —Å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞–º–∏ –º–æ–¥–µ–ª–µ–π. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª—è–º —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –∑–Ω–∞—á–∏–º—ã–µ –º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–µ –æ—Ç–Ω–æ—à–µ–Ω–∏—è –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤. –≠–º–ø–∏—Ä–∏—á–µ—Å–∫–∏–µ –æ—Ü–µ–Ω–∫–∏ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö, –æ—Å–æ–±–µ–Ω–Ω–æ –ø—Ä–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã—Ö –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö.",
  "emoji": "üìè",
  "title": "DIST2Loss: –°–æ—Ö—Ä–∞–Ω—è—è –º–µ—Ç—Ä–∏–∫—É –≤ –¥–∏—Å–∫—Ä–µ—Ç–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ"
}
[05.03.2025 02:17] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"As large language models expand beyond natural language to domains such as mathematics, multimodal understanding, and embodied agents, tokens increasingly reflect metric relationships rather than purely linguistic meaning. We introduce DIST2Loss, a distance-aware framework designed to train autoregressive discrete models by leveraging predefined distance relationships among output tokens. At its core, DIST2Loss transforms continuous exponential family distributions derived from inherent distance metrics into discrete, categorical optimization targets compatible with the models' architectures. This approach enables the models to learn and preserve meaningful distance relationships during token generation while maintaining compatibility with existing architectures. Empirical evaluations show consistent performance gains in diverse multimodal applications, including visual grounding, robotic manipulation, generative reward modeling, and image generation using vector-quantized features. These improvements are pronounced in cases of limited training data, highlighting DIST2Loss's effectiveness in resource-constrained settings."

[05.03.2025 02:17] Response: ```python
["MULTIMODAL", "AGENTS", "CV", "ROBOTICS", "TRAINING"]
```
[05.03.2025 02:17] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"As large language models expand beyond natural language to domains such as mathematics, multimodal understanding, and embodied agents, tokens increasingly reflect metric relationships rather than purely linguistic meaning. We introduce DIST2Loss, a distance-aware framework designed to train autoregressive discrete models by leveraging predefined distance relationships among output tokens. At its core, DIST2Loss transforms continuous exponential family distributions derived from inherent distance metrics into discrete, categorical optimization targets compatible with the models' architectures. This approach enables the models to learn and preserve meaningful distance relationships during token generation while maintaining compatibility with existing architectures. Empirical evaluations show consistent performance gains in diverse multimodal applications, including visual grounding, robotic manipulation, generative reward modeling, and image generation using vector-quantized features. These improvements are pronounced in cases of limited training data, highlighting DIST2Loss's effectiveness in resource-constrained settings."

[05.03.2025 02:17] Response: ```python
["OPTIMIZATION", "LOW_RESOURCE"]
```
[05.03.2025 02:17] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents DIST2Loss, a novel framework that enhances the training of autoregressive discrete models by incorporating distance relationships among output tokens. By transforming continuous distributions based on these distances into discrete targets, the framework allows models to generate tokens that maintain meaningful metric relationships. This method is particularly beneficial in multimodal applications, such as visual grounding and robotic manipulation, where understanding the spatial or contextual distance between elements is crucial. The results demonstrate that DIST2Loss significantly improves model performance, especially when training data is limited, making it a valuable tool in resource-constrained environments.","title":"Enhancing Token Generation with Distance Awareness"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper presents DIST2Loss, a novel framework that enhances the training of autoregressive discrete models by incorporating distance relationships among output tokens. By transforming continuous distributions based on these distances into discrete targets, the framework allows models to generate tokens that maintain meaningful metric relationships. This method is particularly beneficial in multimodal applications, such as visual grounding and robotic manipulation, where understanding the spatial or contextual distance between elements is crucial. The results demonstrate that DIST2Loss significantly improves model performance, especially when training data is limited, making it a valuable tool in resource-constrained environments.', title='Enhancing Token Generation with Distance Awareness'))
[05.03.2025 02:17] Response: ParsedChatCompletionMessage[Article](content='{"desc":"ÈöèÁùÄÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÂ∫îÁî®Êâ©Â±ïÂà∞Êï∞Â≠¶„ÄÅÂ§öÊ®°ÊÄÅÁêÜËß£ÂíåÂÖ∑Ë∫´Êô∫ËÉΩ‰ΩìÁ≠âÈ¢ÜÂüüÔºåÊ®°Âûã‰∏≠ÁöÑÊ†áËÆ∞Ë∂äÊù•Ë∂äÂèçÊò†Â∫¶ÈáèÂÖ≥Á≥ªÔºåËÄå‰∏ç‰ªÖ‰ªÖÊòØËØ≠Ë®ÄÊÑè‰πâ„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜDIST2LossÔºåËøôÊòØ‰∏ÄÁßçË∑ùÁ¶ªÊÑüÁü•Ê°ÜÊû∂ÔºåÊó®Âú®ÈÄöËøáÂà©Áî®ËæìÂá∫Ê†áËÆ∞‰πãÈó¥ÁöÑÈ¢ÑÂÆö‰πâË∑ùÁ¶ªÂÖ≥Á≥ªÊù•ËÆ≠ÁªÉËá™ÂõûÂΩíÁ¶ªÊï£Ê®°Âûã„ÄÇDIST2LossÁöÑÊ†∏ÂøÉÊòØÂ∞ÜÂü∫‰∫éÂõ∫ÊúâË∑ùÁ¶ªÂ∫¶ÈáèÁöÑËøûÁª≠ÊåáÊï∞ÊóèÂàÜÂ∏ÉËΩ¨Âåñ‰∏∫‰∏éÊ®°ÂûãÊû∂ÊûÑÂÖºÂÆπÁöÑÁ¶ªÊï£ÂàÜÁ±ª‰ºòÂåñÁõÆÊ†á„ÄÇÂÆûËØÅËØÑ‰º∞ÊòæÁ§∫ÔºåÂú®ËßÜËßâÂÆö‰Ωç„ÄÅÊú∫Âô®‰∫∫Êìç‰Ωú„ÄÅÁîüÊàêÂ•ñÂä±Âª∫Ê®°Âíå‰ΩøÁî®ÂêëÈáèÈáèÂåñÁâπÂæÅÁöÑÂõæÂÉèÁîüÊàêÁ≠âÂ§öÁßçÂ§öÊ®°ÊÄÅÂ∫îÁî®‰∏≠ÔºåDIST2LossÈÉΩËÉΩÂ∏¶Êù•‰∏ÄËá¥ÁöÑÊÄßËÉΩÊèêÂçáÔºåÂ∞§ÂÖ∂Âú®ËÆ≠ÁªÉÊï∞ÊçÆÊúâÈôêÁöÑÊÉÖÂÜµ‰∏ãÊïàÊûúÊõ¥‰∏∫ÊòæËëó„ÄÇ","title":"Âà©Áî®Ë∑ùÁ¶ªÂÖ≥Á≥ªÊèêÂçáÊ®°ÂûãÊÄßËÉΩÁöÑÂàõÊñ∞ÊñπÊ≥ï"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, parsed=Article(desc='ÈöèÁùÄÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÂ∫îÁî®Êâ©Â±ïÂà∞Êï∞Â≠¶„ÄÅÂ§öÊ®°ÊÄÅÁêÜËß£ÂíåÂÖ∑Ë∫´Êô∫ËÉΩ‰ΩìÁ≠âÈ¢ÜÂüüÔºåÊ®°Âûã‰∏≠ÁöÑÊ†áËÆ∞Ë∂äÊù•Ë∂äÂèçÊò†Â∫¶ÈáèÂÖ≥Á≥ªÔºåËÄå‰∏ç‰ªÖ‰ªÖÊòØËØ≠Ë®ÄÊÑè‰πâ„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜDIST2LossÔºåËøôÊòØ‰∏ÄÁßçË∑ùÁ¶ªÊÑüÁü•Ê°ÜÊû∂ÔºåÊó®Âú®ÈÄöËøáÂà©Áî®ËæìÂá∫Ê†áËÆ∞‰πãÈó¥ÁöÑÈ¢ÑÂÆö‰πâË∑ùÁ¶ªÂÖ≥Á≥ªÊù•ËÆ≠ÁªÉËá™ÂõûÂΩíÁ¶ªÊï£Ê®°Âûã„ÄÇDIST2LossÁöÑÊ†∏ÂøÉÊòØÂ∞ÜÂü∫‰∫éÂõ∫ÊúâË∑ùÁ¶ªÂ∫¶ÈáèÁöÑËøûÁª≠ÊåáÊï∞ÊóèÂàÜÂ∏ÉËΩ¨Âåñ‰∏∫‰∏éÊ®°ÂûãÊû∂ÊûÑÂÖºÂÆπÁöÑÁ¶ªÊï£ÂàÜÁ±ª‰ºòÂåñÁõÆÊ†á„ÄÇÂÆûËØÅËØÑ‰º∞ÊòæÁ§∫ÔºåÂú®ËßÜËßâÂÆö‰Ωç„ÄÅÊú∫Âô®‰∫∫Êìç‰Ωú„ÄÅÁîüÊàêÂ•ñÂä±Âª∫Ê®°Âíå‰ΩøÁî®ÂêëÈáèÈáèÂåñÁâπÂæÅÁöÑÂõæÂÉèÁîüÊàêÁ≠âÂ§öÁßçÂ§öÊ®°ÊÄÅÂ∫îÁî®‰∏≠ÔºåDIST2LossÈÉΩËÉΩÂ∏¶Êù•‰∏ÄËá¥ÁöÑÊÄßËÉΩÊèêÂçáÔºåÂ∞§ÂÖ∂Âú®ËÆ≠ÁªÉÊï∞ÊçÆÊúâÈôêÁöÑÊÉÖÂÜµ‰∏ãÊïàÊûúÊõ¥‰∏∫ÊòæËëó„ÄÇ', title='Âà©Áî®Ë∑ùÁ¶ªÂÖ≥Á≥ªÊèêÂçáÊ®°ÂûãÊÄßËÉΩÁöÑÂàõÊñ∞ÊñπÊ≥ï'))
[05.03.2025 02:17] Loading Chinese text from previous data.
[05.03.2025 02:17] Renaming data file.
[05.03.2025 02:17] Renaming previous data. hf_papers.json to ./d/2025-03-05.json
[05.03.2025 02:17] Saving new data file.
[05.03.2025 02:17] Generating page.
[05.03.2025 02:17] Renaming previous page.
[05.03.2025 02:17] Renaming previous data. index.html to ./d/2025-03-05.html
[05.03.2025 02:17] [Experimental] Generating Chinese page for reading.
[05.03.2025 02:17] Chinese vocab [{'word': 'ËßÜËßâÂº∫ÂåñÂæÆË∞É', 'pinyin': 'sh√¨ju√© qi√°ngzh√π wƒìiti√°o', 'trans': 'visual reinforcement fine-tuning'}, {'word': 'Â§ßÂûãËßÜËßâ-ËØ≠Ë®ÄÊ®°Âûã', 'pinyin': 'd√†x√≠ng sh√¨ju√©-y«îy√°n m√≥x√≠ng', 'trans': 'large vision-language models'}, {'word': 'Ë°®Áé∞', 'pinyin': 'bi«éoxi√†n', 'trans': 'performance'}, {'word': 'ÁîüÊàê', 'pinyin': 'shƒìngch√©ng', 'trans': 'generate'}, {'word': 'ÂèØÈ™åËØÅÁöÑ', 'pinyin': 'kƒõ y√†nzh√®ng de', 'trans': 'verifiable'}, {'word': 'Â•ñÂä±ÂáΩÊï∞', 'pinyin': 'ji«éngl√¨ h√°nsh√π', 'trans': 'reward function'}, {'word': 'Êõ¥Êñ∞', 'pinyin': 'gƒìngxƒ´n', 'trans': 'update'}, {'word': 'ÂÆûÈ™åÁªìÊûú', 'pinyin': 'sh√≠y√†n ji√©gu«í', 'trans': 'experimental results'}, {'word': 'Âá∫Ëâ≤', 'pinyin': 'ch≈´s√®', 'trans': 'outstanding'}, {'word': '‰ºò‰∫é', 'pinyin': 'y≈çu y√∫', 'trans': 'superior to'}, {'word': 'ÁõëÁù£ÂæÆË∞É', 'pinyin': 'ji√†nd≈´ wƒìiti√°o', 'trans': 'supervised fine-tuning'}, {'word': 'ÁªÜÁ≤íÂ∫¶', 'pinyin': 'x√¨ l√¨d√π', 'trans': 'fine-grained'}, {'word': 'ÂõæÂÉèÂàÜÁ±ª', 'pinyin': 't√∫xi√†ng fƒìnl√®i', 'trans': 'image classification'}, {'word': 'ÂáÜÁ°ÆÁéá', 'pinyin': 'zh«înqu√®l«ú', 'trans': 'accuracy'}]
[05.03.2025 02:17] Renaming previous Chinese page.
[05.03.2025 02:17] Renaming previous data. zh.html to ./d/2025-03-04_zh_reading_task.html
[05.03.2025 02:17] Writing Chinese reading task.
[05.03.2025 02:17] Writing result.
[05.03.2025 02:17] Renaming log file.
[05.03.2025 02:17] Renaming previous data. log.txt to ./logs/2025-03-05_last_log.txt
