[05.03.2025 08:14] Read previous papers.
[05.03.2025 08:14] Generating top page (month).
[05.03.2025 08:14] Writing top page (month).
[05.03.2025 09:11] Read previous papers.
[05.03.2025 09:11] Get feed.
[05.03.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.02682
[05.03.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.02846
[05.03.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.02879
[05.03.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.01935
[05.03.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.01328
[05.03.2025 09:11] Extract page data from URL. URL: https://huggingface.co/papers/2503.02368
[05.03.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.14856
[05.03.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.00955
[05.03.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.02197
[05.03.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.02878
[05.03.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.01342
[05.03.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.02876
[05.03.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.02268
[05.03.2025 09:11] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[05.03.2025 09:11] No deleted papers detected.
[05.03.2025 09:11] Downloading and parsing papers (pdf, html). Total: 13.
[05.03.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2503.02682.
[05.03.2025 09:11] Extra JSON file exists (./assets/json/2503.02682.json), skip PDF parsing.
[05.03.2025 09:11] Paper image links file exists (./assets/img_data/2503.02682.json), skip HTML parsing.
[05.03.2025 09:11] Success.
[05.03.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2503.02846.
[05.03.2025 09:11] Extra JSON file exists (./assets/json/2503.02846.json), skip PDF parsing.
[05.03.2025 09:11] Paper image links file exists (./assets/img_data/2503.02846.json), skip HTML parsing.
[05.03.2025 09:11] Success.
[05.03.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2503.02879.
[05.03.2025 09:11] Extra JSON file exists (./assets/json/2503.02879.json), skip PDF parsing.
[05.03.2025 09:11] Paper image links file exists (./assets/img_data/2503.02879.json), skip HTML parsing.
[05.03.2025 09:11] Success.
[05.03.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2503.01935.
[05.03.2025 09:11] Extra JSON file exists (./assets/json/2503.01935.json), skip PDF parsing.
[05.03.2025 09:11] Paper image links file exists (./assets/img_data/2503.01935.json), skip HTML parsing.
[05.03.2025 09:11] Success.
[05.03.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2503.01328.
[05.03.2025 09:11] Extra JSON file exists (./assets/json/2503.01328.json), skip PDF parsing.
[05.03.2025 09:11] Paper image links file exists (./assets/img_data/2503.01328.json), skip HTML parsing.
[05.03.2025 09:11] Success.
[05.03.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2503.02368.
[05.03.2025 09:11] Downloading paper 2503.02368 from http://arxiv.org/pdf/2503.02368v1...
[05.03.2025 09:11] Extracting affiliations from text.
[05.03.2025 09:11] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Zhenhua Liu * 1 2 Lijun Li * 1 Ruizhe Chen * 3 Yuxian Jiang 4 Tong Zhu 2 Zhaochen Su 2 Wenliang Chen 2 Jing Shao "
[05.03.2025 09:11] Response: []
[05.03.2025 09:11] Extracting affiliations from text.
[05.03.2025 09:11] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Zhenhua Liu * 1 2 Lijun Li * 1 Ruizhe Chen * 3 Yuxian Jiang 4 Tong Zhu 2 Zhaochen Su 2 Wenliang Chen 2 Jing ShaoWhile Reinforcement Learning from Human Feedback (RLHF) has become the predominant method for controlling language model outputs, it suffers from high computational costs and training instability. Guided decoding, especially valueguided methods, offers cost-effective alternative by controlling outputs without re-training models. However, the accuracy of the value function is crucial for value-guided decoding, as inaccuracies can lead to suboptimal decision-making and degraded performance. Existing methods struggle with accurately estimating the optimal value function, leading to less effective control. We propose Iterative Value Function Optimization, novel framework that addresses these limitations through two key components: Monte Carlo Value Estimation, which reduces estimation variance by exploring diverse trajectories, and Iterative On-Policy Optimization, which progressively improves value estimation through collecting trajectories from value-guided policies. Extensive experiments on text summarization, multi-turn dialogue, and instruction following demonstrate the effectiveness of value-guided decoding approaches in aligning language models. These approaches not only achieve alignment but also significantly reduce computational costs by leveraging principled value function optimization for efficient and effective control. 5 2 0 2 4 ] . [ 1 8 6 3 2 0 . 3 0 5 2 : r 1. Introduction Reinforcement Learning from Human Feedback (RLHF) (Christiano et al., 2017; Bai et al., 2022; Ouyang et al., 2022) has emerged as widely adopted approach to align advanced language models with human values and task requirements (Wei et al., 2022; Achiam et al., 2023; Chao *Equal contribution 1Shanghai Artificial Intelligence Laboratory 2Soochow University 3Zhejiang University 4Fudan University. Correspondence to: Jing Shao <shaojing@pjlab.org.cn>. Figure 1. Visualization of different decoding strategies in the output space. Given query, the base policy generates outputs with suboptimal rewards (lighter regions). Guided decoding with an estimated value function shifts the distribution towards higherreward regions, while the optimal value function would guide the policy to achieve maximum rewards (darkest regions). et al., 2024; Su et al., 2024a). However, traditional RLHF methods like Proximal Policy Optimization (PPO) (Christiano et al., 2017; Ouyang et al., 2022) suffer from high computational costs and training instability (Zheng et al., 2023b; Rafailov et al., 2024), limiting their practicality for applications requiring flexible behavior control. Among various alternatives, guided decoding methods have gained increasing attention as they can control model outputs without expensive model re-training (Snell et al., 2022; Mudgal et al., 2023; Han et al., 2024; Chakraborty et al., 2024). Within this framework, value-guided approaches, which train value function Vθ to evaluate partial outputs and steer the language model towards high-reward trajectories, have emerged as particularly promising (Yang & Klein, 2021; Qin et al., 2022; Mudgal et al., 2023; Han et al., 2024). Under the KL-regularized Reinforcement Learning framework, given an optimal value function , we can derive policy that maximizes expected rewards while maintaining bounded KL-divergence from the base policy πbase. 1 Iterative Value Function Optimization As visualized in Figure 1, while guided decoding with an estimated value function can shift the base policys output distribution towards higher-reward regions, this improvement remains suboptimal compared to the theoretical maximum achievable through the optimal value function . This gap stems from two fundamental challenges in estimating (st), the maximum expected reward attainable when following the optimal policy π from state st until generation completion. First, existing methods (Khanov et al., 2024; Mudgal et al., 2023) rely on sampling only single trajectory from the base policy πbase per prompt, resulting in high-variance value estimates due to insufficient exploration of the trajectory space. Second, the inherent inaccessibility of the optimal policy π prevents direct acquisition of high-reward trajectories for training. These limitations lead to substantial suboptimality in value function estimation, ultimately hindering decoding effectiveness. To address these challenges, we propose Iterative Value Function Optimization (IVO). This novel framework introduces two synergistic components for better value function training: (1) Monte Carlo Value Estimation employs stochastic sampling to reduce variance through comprehensive trajectory space exploration. (2) Iterative On-Policy Optimization creates self-improving cycle where valueguided policies generate higher-quality trajectories for subsequent value function training. This dual mechanism enables IVO to progressively bridge the base-optimal policy gap, achieving more accurate value estimation than previous ones (Yang & Klein, 2021; Han et al., 2024). Unlike traditional online RLHF methods that require repeatedly collecting preference data and retraining the policy model, IVO achieves policy improvement by optimizing only the value function, substantially reducing computational costs while maintaining the benefits of iterative refinement. Our main contributions are summarized as following: We introduce IVO, novel framework that combines Monte Carlo Value Estimation and Iterative On-Policy Optimization to significantly reduce variance in value estimation and enhance the exploration of high-reward trajectories. We demonstrate the generalizability and effectiveness of IVO by conducting extensive experiments across variety of challenging tasks, including text summarization, multi-turn dialogue, and instruction following, showing consistent improvement in performance over existing approaches. Our method achieves 77.52% GPT-4 win rates on the Multi-turn Dialogue against the base policy and outperforms baseline methods in terms of reward scores across all evaluated tasks. We conduct extensive empirical analysis on the impact of sampling trajectories and training iterations, providing practical insights for implementing value-guided decoding methods. 2. Related Work Reinforcement Learning for Language Models. Large Language Models (LLMs) commonly leverage Reinforcement Learning from Human Feedback (RLHF) to enhance model pe"
[05.03.2025 09:11] Mistral response. {"id": "caae661d9fb9415db69fd5c43d14bbb2", "object": "chat.completion", "created": 1741165912, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\"Shanghai Artificial Intelligence Laboratory\", \"Soochow University\", \"Zhejiang University\", \"Fudan University\"]\n```"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1546, "total_tokens": 1582, "completion_tokens": 36}}
[05.03.2025 09:11] Response: ```python
["Shanghai Artificial Intelligence Laboratory", "Soochow University", "Zhejiang University", "Fudan University"]
```
[05.03.2025 09:11] Deleting PDF ./assets/pdf/2503.02368.pdf.
[05.03.2025 09:11] Success.
[05.03.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2502.14856.
[05.03.2025 09:11] Extra JSON file exists (./assets/json/2502.14856.json), skip PDF parsing.
[05.03.2025 09:11] Paper image links file exists (./assets/img_data/2502.14856.json), skip HTML parsing.
[05.03.2025 09:11] Success.
[05.03.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2503.00955.
[05.03.2025 09:11] Extra JSON file exists (./assets/json/2503.00955.json), skip PDF parsing.
[05.03.2025 09:11] Paper image links file exists (./assets/img_data/2503.00955.json), skip HTML parsing.
[05.03.2025 09:11] Success.
[05.03.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2503.02197.
[05.03.2025 09:11] Extra JSON file exists (./assets/json/2503.02197.json), skip PDF parsing.
[05.03.2025 09:11] Paper image links file exists (./assets/img_data/2503.02197.json), skip HTML parsing.
[05.03.2025 09:11] Success.
[05.03.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2503.02878.
[05.03.2025 09:11] Extra JSON file exists (./assets/json/2503.02878.json), skip PDF parsing.
[05.03.2025 09:11] Paper image links file exists (./assets/img_data/2503.02878.json), skip HTML parsing.
[05.03.2025 09:11] Success.
[05.03.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2503.01342.
[05.03.2025 09:11] Extra JSON file exists (./assets/json/2503.01342.json), skip PDF parsing.
[05.03.2025 09:11] Paper image links file exists (./assets/img_data/2503.01342.json), skip HTML parsing.
[05.03.2025 09:11] Success.
[05.03.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2503.02876.
[05.03.2025 09:11] Extra JSON file exists (./assets/json/2503.02876.json), skip PDF parsing.
[05.03.2025 09:11] Paper image links file exists (./assets/img_data/2503.02876.json), skip HTML parsing.
[05.03.2025 09:11] Success.
[05.03.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2503.02268.
[05.03.2025 09:11] Extra JSON file exists (./assets/json/2503.02268.json), skip PDF parsing.
[05.03.2025 09:11] Paper image links file exists (./assets/img_data/2503.02268.json), skip HTML parsing.
[05.03.2025 09:11] Success.
[05.03.2025 09:11] Enriching papers with extra data.
[05.03.2025 09:11] ********************************************************************************
[05.03.2025 09:11] Abstract 0. Recent advancements in large language models (LLMs) have enabled LLM-based agents to successfully tackle interactive planning tasks. However, despite their successes, existing approaches often suffer from planning hallucinations and require retraining for each new agent. To address these challenges,...
[05.03.2025 09:11] ********************************************************************************
[05.03.2025 09:11] Abstract 1. Large language models (LLMs) exhibit hallucinations (i.e., unfaithful or nonsensical information) when serving as AI assistants in various domains. Since hallucinations always come with truthful content in the LLM responses, previous factuality alignment methods that conduct response-level preferenc...
[05.03.2025 09:11] ********************************************************************************
[05.03.2025 09:11] Abstract 2. In this paper, we present a thorough analysis of the impact of Large Language Models (LLMs) on Wikipedia, examining the evolution of Wikipedia through existing data and using simulations to explore potential risks. We begin by analyzing page views and article content to study Wikipedia's recent chan...
[05.03.2025 09:11] ********************************************************************************
[05.03.2025 09:11] Abstract 3. Large Language Models (LLMs) have shown remarkable capabilities as autonomous agents, yet existing benchmarks either focus on single-agent tasks or are confined to narrow domains, failing to capture the dynamics of multi-agent coordination and competition. In this paper, we introduce MultiAgentBench...
[05.03.2025 09:11] ********************************************************************************
[05.03.2025 09:11] Abstract 4. Pipeline parallelism (PP) is widely used for training large language models (LLMs), yet its scalability is often constrained by high activation memory consumption as the number of in-flight microbatches grows with the degree of PP. In this paper, we focus on addressing this challenge by leveraging t...
[05.03.2025 09:11] ********************************************************************************
[05.03.2025 09:11] Abstract 5. While Reinforcement Learning from Human Feedback (RLHF) has become the predominant method for controlling language model outputs, it suffers from high computational costs and training instability. Guided decoding, especially value-guided methods, offers a cost-effective alternative by controlling ou...
[05.03.2025 09:11] ********************************************************************************
[05.03.2025 09:11] Abstract 6. Speculative sampling has emerged as an important technique for accelerating the auto-regressive generation process of large language models (LLMs) by utilizing a draft-then-verify mechanism to produce multiple tokens per forward pass. While state-of-the-art speculative sampling methods use only a si...
[05.03.2025 09:11] ********************************************************************************
[05.03.2025 09:11] Abstract 7. The rise of misinformation, exacerbated by Large Language Models (LLMs) like GPT and Gemini, demands robust fact-checking solutions, especially for low-resource languages like Vietnamese. Existing methods struggle with semantic ambiguity, homonyms, and complex linguistic structures, often trading ac...
[05.03.2025 09:11] ********************************************************************************
[05.03.2025 09:11] Abstract 8. Large Language Model (LLM) agents have demonstrated remarkable generalization capabilities across multi-domain tasks. Existing agent tuning approaches typically employ supervised finetuning on entire expert trajectories. However, behavior-cloning of full trajectories can introduce expert bias and we...
[05.03.2025 09:11] ********************************************************************************
[05.03.2025 09:11] Abstract 9. Collecting ground truth task completion rewards or human demonstrations for multi-step reasoning tasks is often cost-prohibitive and time-consuming, especially in interactive domains like web tasks. To address this bottleneck, we present self-taught lookahead, a self-supervised method that leverages...
[05.03.2025 09:11] ********************************************************************************
[05.03.2025 09:11] Abstract 10. Generalist models have achieved remarkable success in both language and vision-language tasks, showcasing the potential of unified modeling. However, effectively integrating fine-grained perception tasks like detection and segmentation into these models remains a significant challenge. This is prima...
[05.03.2025 09:11] ********************************************************************************
[05.03.2025 09:11] Abstract 11. Advancing AI in computational pathology requires large, high-quality, and diverse datasets, yet existing public datasets are often limited in organ diversity, class coverage, or annotation quality. To bridge this gap, we introduce SPIDER (Supervised Pathology Image-DEscription Repository), the large...
[05.03.2025 09:11] ********************************************************************************
[05.03.2025 09:11] Abstract 12. Recent advancements in Large Language Models (LLMs) have led to the development of intelligent LLM-based agents capable of interacting with graphical user interfaces (GUIs). These agents demonstrate strong reasoning and adaptability, enabling them to perform complex tasks that traditionally required...
[05.03.2025 09:11] Read previous papers.
[05.03.2025 09:11] Generating reviews via LLM API.
[05.03.2025 09:11] Using data from previous issue: {"categories": ["#rl", "#training", "#optimization", "#hallucinations", "#agents"], "emoji": "🧠", "ru": {"title": "Метапланы для умных агентов: эффективнее, универсальнее, без галлюцинаций", "desc": "Статья представляет новый подход к улучшению планирования задач агентами на основе больших языковых 
[05.03.2025 09:11] Using data from previous issue: {"categories": ["#hallucinations", "#training", "#rlhf", "#alignment"], "emoji": "🎭", "ru": {"title": "Mask-DPO: точная настройка фактов в ответах языковых моделей", "desc": "Эта статья представляет метод Mask-DPO для улучшения фактической точности больших языковых моделей (LLM). Метод основан на оп
[05.03.2025 09:11] Using data from previous issue: {"categories": ["#rag", "#benchmark", "#machine_translation", "#dataset", "#multimodal", "#science", "#data"], "emoji": "🧠", "ru": {"title": "Большие языковые модели меняют лицо Википедии: анализ влияния и потенциальных рисков", "desc": "Эта статья представляет анализ влияния больших языковых моделе
[05.03.2025 09:11] Using data from previous issue: {"categories": ["#games", "#optimization", "#open_source", "#benchmark", "#agents"], "emoji": "🤖", "ru": {"title": "MultiAgentBench: новый стандарт для оценки мультиагентных LLM-систем", "desc": "Статья представляет MultiAgentBench - комплексный бенчмарк для оценки мультиагентных систем на основе бо
[05.03.2025 09:11] Using data from previous issue: {"categories": ["#open_source", "#inference", "#optimization", "#training"], "emoji": "🚀", "ru": {"title": "Оптимизация памяти для масштабирования больших языковых моделей", "desc": "Статья посвящена улучшению масштабируемости конвейерного параллелизма при обучении больших языковых моделей. Авторы п
[05.03.2025 09:11] Querying the API.
[05.03.2025 09:11] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

While Reinforcement Learning from Human Feedback (RLHF) has become the predominant method for controlling language model outputs, it suffers from high computational costs and training instability. Guided decoding, especially value-guided methods, offers a cost-effective alternative by controlling outputs without re-training models. However, the accuracy of the value function is crucial for value-guided decoding, as inaccuracies can lead to suboptimal decision-making and degraded performance. Existing methods struggle with accurately estimating the optimal value function, leading to less effective control. We propose Iterative Value Function Optimization, a novel framework that addresses these limitations through two key components: Monte Carlo Value Estimation, which reduces estimation variance by exploring diverse trajectories, and Iterative On-Policy Optimization, which progressively improves value estimation through collecting trajectories from value-guided policies. Extensive experiments on text summarization, multi-turn dialogue, and instruction following demonstrate the effectiveness of value-guided decoding approaches in aligning language models. These approaches not only achieve alignment but also significantly reduce computational costs by leveraging principled value function optimization for efficient and effective control.
[05.03.2025 09:11] Response: {
  "desc": "Статья представляет новый метод управления выходными данными языковых моделей, называемый 'Итеративная оптимизация функции ценности'. Этот подход предлагает альтернативу традиционному обучению с подкреплением на основе обратной связи от человека (RLHF), снижая вычислительные затраты и повышая стабильность обучения. Метод использует управляемое декодирование на основе функции ценности, улучшая её точность с помощью оценки Монте-Карло и итеративной оптимизации. Эксперименты показали эффективность предложенного подхода в задачах суммаризации текста, многоэтапного диалога и выполнения инструкций.",
  "emoji": "🧠",
  "title": "Эффективное управление языковыми моделями без переобучения"
}
[05.03.2025 09:11] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"While Reinforcement Learning from Human Feedback (RLHF) has become the predominant method for controlling language model outputs, it suffers from high computational costs and training instability. Guided decoding, especially value-guided methods, offers a cost-effective alternative by controlling outputs without re-training models. However, the accuracy of the value function is crucial for value-guided decoding, as inaccuracies can lead to suboptimal decision-making and degraded performance. Existing methods struggle with accurately estimating the optimal value function, leading to less effective control. We propose Iterative Value Function Optimization, a novel framework that addresses these limitations through two key components: Monte Carlo Value Estimation, which reduces estimation variance by exploring diverse trajectories, and Iterative On-Policy Optimization, which progressively improves value estimation through collecting trajectories from value-guided policies. Extensive experiments on text summarization, multi-turn dialogue, and instruction following demonstrate the effectiveness of value-guided decoding approaches in aligning language models. These approaches not only achieve alignment but also significantly reduce computational costs by leveraging principled value function optimization for efficient and effective control."

[05.03.2025 09:12] Response: ```python
["RLHF", "TRAINING"]
```
[05.03.2025 09:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"While Reinforcement Learning from Human Feedback (RLHF) has become the predominant method for controlling language model outputs, it suffers from high computational costs and training instability. Guided decoding, especially value-guided methods, offers a cost-effective alternative by controlling outputs without re-training models. However, the accuracy of the value function is crucial for value-guided decoding, as inaccuracies can lead to suboptimal decision-making and degraded performance. Existing methods struggle with accurately estimating the optimal value function, leading to less effective control. We propose Iterative Value Function Optimization, a novel framework that addresses these limitations through two key components: Monte Carlo Value Estimation, which reduces estimation variance by exploring diverse trajectories, and Iterative On-Policy Optimization, which progressively improves value estimation through collecting trajectories from value-guided policies. Extensive experiments on text summarization, multi-turn dialogue, and instruction following demonstrate the effectiveness of value-guided decoding approaches in aligning language models. These approaches not only achieve alignment but also significantly reduce computational costs by leveraging principled value function optimization for efficient and effective control."

[05.03.2025 09:12] Response: ```python
["ALIGNMENT", "OPTIMIZATION"]
```
[05.03.2025 09:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper addresses the challenges of Reinforcement Learning from Human Feedback (RLHF) in controlling language models, particularly its high computational costs and instability. It introduces a new framework called Iterative Value Function Optimization, which enhances value-guided decoding methods by improving the accuracy of the value function. The framework employs Monte Carlo Value Estimation to minimize estimation variance and Iterative On-Policy Optimization to refine value estimation through trajectory collection. Experimental results show that this approach not only aligns language models effectively but also reduces computational expenses significantly.","title":"Optimizing Value Functions for Efficient Language Model Control"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper addresses the challenges of Reinforcement Learning from Human Feedback (RLHF) in controlling language models, particularly its high computational costs and instability. It introduces a new framework called Iterative Value Function Optimization, which enhances value-guided decoding methods by improving the accuracy of the value function. The framework employs Monte Carlo Value Estimation to minimize estimation variance and Iterative On-Policy Optimization to refine value estimation through trajectory collection. Experimental results show that this approach not only aligns language models effectively but also reduces computational expenses significantly.', title='Optimizing Value Functions for Efficient Language Model Control'))
[05.03.2025 09:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本论文探讨了人类反馈强化学习（RLHF）在控制语言模型输出时的高计算成本和训练不稳定性问题。我们提出了一种新的框架——迭代值函数优化，通过蒙特卡洛值估计和迭代在线优化来提高值函数的准确性。该方法通过探索多样化的轨迹来减少估计方差，并逐步改进值估计。实验结果表明，基于值引导的解码方法在文本摘要、多轮对话和指令跟随任务中表现出色，显著降低了计算成本。","title":"高效控制语言模型的新方法"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本论文探讨了人类反馈强化学习（RLHF）在控制语言模型输出时的高计算成本和训练不稳定性问题。我们提出了一种新的框架——迭代值函数优化，通过蒙特卡洛值估计和迭代在线优化来提高值函数的准确性。该方法通过探索多样化的轨迹来减少估计方差，并逐步改进值估计。实验结果表明，基于值引导的解码方法在文本摘要、多轮对话和指令跟随任务中表现出色，显著降低了计算成本。', title='高效控制语言模型的新方法'))
[05.03.2025 09:12] Using data from previous issue: {"categories": ["#training", "#inference", "#optimization"], "emoji": "🚀", "ru": {"title": "Быстрее и эффективнее: оптимизация спекулятивной выборки для LLM", "desc": "Статья представляет FR-Spec - новый метод спекулятивной выборки для ускорения работы больших языковых моделей (LLM). Метод оптимизир
[05.03.2025 09:12] Using data from previous issue: {"categories": ["#multilingual", "#inference", "#benchmark", "#low_resource", "#dataset", "#science", "#data"], "emoji": "🕵️", "ru": {"title": "SemViQA: Передовая система проверки фактов для борьбы с дезинформацией на вьетнамском языке", "desc": "SemViQA - это новая система проверки фактов на вьетна
[05.03.2025 09:12] Using data from previous issue: {"categories": ["#optimization", "#reasoning", "#training", "#agents", "#agi"], "emoji": "🎯", "ru": {"title": "ATLaS: точечная настройка LLM-агентов для улучшения обобщения", "desc": "ATLaS - это новый метод настройки агентов на основе больших языковых моделей (LLM). Он фокусируется на обучении толь
[05.03.2025 09:12] Using data from previous issue: {"categories": ["#reasoning", "#rl", "#optimization", "#training"], "emoji": "🧠", "ru": {"title": "Самообучение для эффективного поиска без дорогостоящих демонстраций", "desc": "Статья представляет метод 'self-taught lookahead' для обучения модели оценки, способной эффективно направлять поиск, управ
[05.03.2025 09:12] Using data from previous issue: {"categories": ["#cv", "#training", "#optimization", "#agi", "#open_source", "#multimodal", "#architecture"], "emoji": "🔬", "ru": {"title": "Унификация задач компьютерного зрения через языковой интерфейс", "desc": "Статья представляет новый фреймворк для унификации задач тонкой визуальной перцепции 
[05.03.2025 09:12] Using data from previous issue: {"categories": ["#open_source", "#dataset", "#multimodal", "#science", "#benchmark"], "emoji": "🕷️", "ru": {"title": "SPIDER: Новый стандарт данных для ИИ в патологии", "desc": "Статья представляет SPIDER - крупнейший общедоступный набор данных для вычислительной патологии, охватывающий множество ти
[05.03.2025 09:12] Using data from previous issue: {"categories": ["#optimization", "#agents", "#benchmark", "#reasoning", "#open_source", "#training"], "emoji": "🧠", "ru": {"title": "Эволюционное обучение агентов GUI: баланс эффективности и гибкости", "desc": "Эта статья представляет новый эволюционный подход для агентов, работающих с графическим и
[05.03.2025 09:12] Trying to get texts in Chinese.
[05.03.2025 09:12] Mistral request. Model: mistral-large-latest. Prompt: Write simple and brief explanation (4-5 sentences) of an article in Chinese. Use short sentences. Text:

Recent advancements in large language models (LLMs) have enabled LLM-based agents to successfully tackle interactive planning tasks. However, despite their successes, existing approaches often suffer from planning hallucinations and require retraining for each new agent. To address these challenges, we propose the Meta Plan Optimization (MPO) framework, which enhances agent planning capabilities by directly incorporating explicit guidance. Unlike previous methods that rely on complex knowledge, which either require significant human effort or lack quality assurance, MPO leverages high-level general guidance through meta plans to assist agent planning and enables continuous optimization of the meta plans based on feedback from the agent's task execution. Our experiments conducted on two representative tasks demonstrate that MPO significantly outperforms existing baselines. Moreover, our analysis indicates that MPO provides a plug-and-play solution that enhances both task completion efficiency and generalization capabilities in previous unseen scenarios.
[05.03.2025 09:12] Mistral response. {"id": "3b294819c968460886167a013de52fd8", "object": "chat.completion", "created": 1741165929, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "\u6700\u8fd1\u7684\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u5c55\u4f7f\u5f97\u57fa\u4e8eLLM\u7684\u4ee3\u7406\u80fd\u591f\u6210\u529f\u5904\u7406\u4e92\u52a8\u5f0f\u89c4\u5212\u4efb\u52a1\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u5e38\u5e38\u53d7\u5230\u89c4\u5212\u5e7b\u89c9\u7684\u56f0\u6270\uff0c\u5e76\u4e14\u6bcf\u4e2a\u65b0\u4ee3\u7406\u90fd\u9700\u8981\u91cd\u65b0\u8bad\u7ec3\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u5143\u89c4\u5212\u4f18\u5316\uff08MPO\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u76f4\u63a5\u5f15\u5165\u663e\u5f0f\u6307\u5bfc\u6765\u589e\u5f3a\u4ee3\u7406\u7684\u89c4\u5212\u80fd\u529b\u3002MPO\u5229\u7528\u5143\u89c4\u5212\u63d0\u4f9b\u9ad8\u5c42\u6b21\u7684\u901a\u7528\u6307\u5bfc\uff0c\u5e2e\u52a9\u4ee3\u7406\u89c4\u5212\uff0c\u5e76\u6839\u636e\u4efb\u52a1\u6267\u884c\u53cd\u9988\u6301\u7eed\u4f18\u5316\u5143\u89c4\u5212\u3002\u5b9e\u9a8c\u8868\u660e\uff0cMPO\u5728\u4e24\u9879\u4ee3\u8868\u6027\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u51c6\uff0c\u5e76\u63d0\u9ad8\u4e86\u4efb\u52a1\u5b8c\u6210\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\u3002"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 218, "total_tokens": 427, "completion_tokens": 209}}
[05.03.2025 09:12] Response: 最近的大语言模型进展使得基于LLM的代理能够成功处理互动式规划任务。然而，现有方法常常受到规划幻觉的困扰，并且每个新代理都需要重新训练。为解决这些挑战，我们提出了元规划优化（MPO）框架，通过直接引入显式指导来增强代理的规划能力。MPO利用元规划提供高层次的通用指导，帮助代理规划，并根据任务执行反馈持续优化元规划。实验表明，MPO在两项代表性任务中显著优于现有基准，并提高了任务完成效率和泛化能力。
[05.03.2025 09:12] Mistral request. Model: mistral-large-latest. Prompt: Write pinyin transcription for text. Text:

最近的大语言模型进展使得基于LLM的代理能够成功处理互动式规划任务。然而，现有方法常常受到规划幻觉的困扰，并且每个新代理都需要重新训练。为解决这些挑战，我们提出了元规划优化（MPO）框架，通过直接引入显式指导来增强代理的规划能力。MPO利用元规划提供高层次的通用指导，帮助代理规划，并根据任务执行反馈持续优化元规划。实验表明，MPO在两项代表性任务中显著优于现有基准，并提高了任务完成效率和泛化能力。
[05.03.2025 09:12] Mistral response. {"id": "9af093a425834b439207518b50b33a73", "object": "chat.completion", "created": 1741165933, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "\u6700\u8fd1\u7684\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u5c55\u4f7f\u5f97\u57fa\u4e8eLLM\u7684\u4ee3\u7406\u80fd\u591f\u6210\u529f\u5904\u7406\u4e92\u52a8\u5f0f\u89c4\u5212\u4efb\u52a1\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u5e38\u5e38\u53d7\u5230\u89c4\u5212\u5e7b\u89c9\u7684\u56f0\u6270\uff0c\u5e76\u4e14\u6bcf\u4e2a\u65b0\u4ee3\u7406\u90fd\u9700\u8981\u91cd\u65b0\u8bad\u7ec3\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u5143\u89c4\u5212\u4f18\u5316\uff08MPO\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u76f4\u63a5\u5f15\u5165\u663e\u5f0f\u6307\u5bfc\u6765\u589e\u5f3a\u4ee3\u7406\u7684\u89c4\u5212\u80fd\u529b\u3002MPO\u5229\u7528\u5143\u89c4\u5212\u63d0\u4f9b\u9ad8\u5c42\u6b21\u7684\u901a\u7528\u6307\u5bfc\uff0c\u5e2e\u52a9\u4ee3\u7406\u89c4\u5212\uff0c\u5e76\u6839\u636e\u4efb\u52a1\u6267\u884c\u53cd\u9988\u6301\u7eed\u4f18\u5316\u5143\u89c4\u5212\u3002\u5b9e\u9a8c\u8868\u660e\uff0cMPO\u5728\u4e24\u9879\u4ee3\u8868\u6027\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u51c6\uff0c\u5e76\u63d0\u9ad8\u4e86\u4efb\u52a1\u5b8c\u6210\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\u3002\n\nZu\u00ecj\u00ecn de d\u00e0 y\u01d4y\u00e1n m\u00f3x\u00edng j\u00ecnzh\u01cen sh\u01d0d\u00e9 j\u012by\u00fa LLM de d\u00e0il\u01d0 n\u00e9ngg\u00f2u ch\u00e9ngg\u014dng ch\u01d4l\u01d0 h\u00f9d\u00f2ngsh\u00ec gu\u012bhu\u00e0 r\u00e8nw\u00f9. R\u00e1n'\u00e9r, xi\u00e0ny\u01d2u f\u0101ngf\u01ce ch\u00e1ngch\u00e1ng sh\u00f2ud\u00e0o gu\u012bhu\u00e0 hu\u00e0nju\u00e9 de k\u00f9nhu\u00f2, b\u00ecngqi\u011b m\u011bi g\u00e8 x\u012bn d\u00e0il\u01d0 d\u014du x\u016by\u00e0o ch\u00f3ngx\u012bn x\u00f9nli\u00e0n. W\u00e8i ji\u011bju\u00e9 zh\u00e8xi\u0113 ti\u01ceozh\u00e0n, w\u01d2men t\u00edch\u016ble yu\u00e1n gu\u012bhu\u00e0 y\u014duhu\u00e0 (MPO) ku\u00e0ngji\u00e0, t\u014dnggu\u00f2 zh\u00edji\u0113 y\u01d0nr\u00f9 xi\u01censh\u00ec zh\u01d0d\u01ceo l\u00e1i z\u0113ngqi\u00e1ng d\u00e0il\u01d0 de gu\u012bhu\u00e0 n\u00e9ngl\u00ec. MPO l\u00ecy\u00f2ng yu\u00e1n gu\u012bhu\u00e0 t\u00edg\u014dng g\u0101o c\u00e9ngc\u00ec de t\u014dngy\u00f2ng zh\u01d0d\u01ceo, b\u0101ngzh\u00f9 d\u00e0il\u01d0 gu\u012bhu\u00e0, b\u00ecngg\u0113nj\u00f9 r\u00e8nw\u00f9 zh\u00edx\u00edng f\u01cenku\u00ec ch\u00edx\u00f9 y\u014duhu\u00e0 yu\u00e1n gu\u012bhu\u00e0. Sh\u00edy\u00e0n bi\u01ceom\u00edng, MPO z\u00e0i li\u01ceng xi\u00e0ng d\u00e0ibi\u01ceox\u00ecng r\u00e8nw\u00f9 zh\u014dng xi\u01cenzh\u00f9 y\u014duhu\u00e0n xi\u00e0nzh\u00f9n b\u01d0zh\u01d4n, b\u00ecng t\u00edg\u0101ole r\u00e8nw\u00f9 w\u00e1nch\u00e9ng xi\u00e0ol\u01dc h\u00e9 f\u00e0nhu\u00e0 n\u00e9ngl\u00ec."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 224, "total_tokens": 884, "completion_tokens": 660}}
[05.03.2025 09:12] Response: 最近的大语言模型进展使得基于LLM的代理能够成功处理互动式规划任务。然而，现有方法常常受到规划幻觉的困扰，并且每个新代理都需要重新训练。为解决这些挑战，我们提出了元规划优化（MPO）框架，通过直接引入显式指导来增强代理的规划能力。MPO利用元规划提供高层次的通用指导，帮助代理规划，并根据任务执行反馈持续优化元规划。实验表明，MPO在两项代表性任务中显著优于现有基准，并提高了任务完成效率和泛化能力。

Zuìjìn de dà yǔyán móxíng jìnzhǎn shǐdé jīyú LLM de dàilǐ nénggòu chénggōng chǔlǐ hùdòngshì guīhuà rènwù. Rán'ér, xiànyǒu fāngfǎ chángcháng shòudào guīhuà huànjué de kùnhuò, bìngqiě měi gè xīn dàilǐ dōu xūyào chóngxīn xùnliàn. Wèi jiějué zhèxiē tiǎozhàn, wǒmen tíchūle yuán guīhuà yōuhuà (MPO) kuàngjià, tōngguò zhíjiē yǐnrù xiǎnshì zhǐdǎo lái zēngqiáng dàilǐ de guīhuà nénglì. MPO lìyòng yuán guīhuà tígōng gāo céngcì de tōngyòng zhǐdǎo, bāngzhù dàilǐ guīhuà, bìnggēnjù rènwù zhíxíng fǎnkuì chíxù yōuhuà yuán guīhuà. Shíyàn biǎomíng, MPO zài liǎng xiàng dàibiǎoxìng rènwù zhōng xiǎnzhù yōuhuàn xiànzhùn bǐzhǔn, bìng tígāole rènwù wánchéng xiàolǜ hé fànhuà nénglì.
[05.03.2025 09:12] Mistral request. Model: mistral-large-latest. Prompt: Write vocab of difficult words for this text as an array of objects with fields 'word', 'pinyin', 'trans'. Return as python list without formatting. Return list and nothing else. Text:

最近的大语言模型进展使得基于LLM的代理能够成功处理互动式规划任务。然而，现有方法常常受到规划幻觉的困扰，并且每个新代理都需要重新训练。为解决这些挑战，我们提出了元规划优化（MPO）框架，通过直接引入显式指导来增强代理的规划能力。MPO利用元规划提供高层次的通用指导，帮助代理规划，并根据任务执行反馈持续优化元规划。实验表明，MPO在两项代表性任务中显著优于现有基准，并提高了任务完成效率和泛化能力。
[05.03.2025 09:12] Mistral response. {"id": "5908b55c8f0b4648ace0406ad7fd67ba", "object": "chat.completion", "created": 1741165946, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "[{'word': '\u5927\u8bed\u8a00\u6a21\u578b', 'pinyin': 'd\u00e0 y\u01d4y\u00e1n m\u00f3x\u00edng', 'trans': 'large language model'},\n{'word': '\u57fa\u4e8e', 'pinyin': 'j\u012by\u00fa', 'trans': 'based on'},\n{'word': '\u4ee3\u7406', 'pinyin': 'd\u00e0il\u01d0', 'trans': 'agent'},\n{'word': '\u4e92\u52a8\u5f0f', 'pinyin': 'h\u00f9d\u00f2ngsh\u00ec', 'trans': 'interactive'},\n{'word': '\u89c4\u5212', 'pinyin': 'gu\u012bhu\u00e0', 'trans': 'planning'},\n{'word': '\u4efb\u52a1', 'pinyin': 'r\u00e8nw\u00f9', 'trans': 'task'},\n{'word': '\u5e7b\u89c9', 'pinyin': 'hu\u00e0nju\u00e9', 'trans': 'hallucination'},\n{'word': '\u56f0\u6270', 'pinyin': 'k\u00f9nr\u01ceo', 'trans': 'trouble'},\n{'word': '\u91cd\u65b0', 'pinyin': 'ch\u00f3ngx\u012bn', 'trans': 'renew'},\n{'word': '\u8bad\u7ec3', 'pinyin': 'x\u00f9nli\u00e0n', 'trans': 'training'},\n{'word': '\u6311\u6218', 'pinyin': 'ti\u01ceozh\u00e0n', 'trans': 'challenge'},\n{'word': '\u63d0\u51fa', 'pinyin': 't\u00edch\u016b', 'trans': 'propose'},\n{'word': '\u6846\u67b6', 'pinyin': 'ku\u00e0ngji\u00e0', 'trans': 'framework'},\n{'word': '\u663e\u5f0f', 'pinyin': 'xi\u01censh\u00ec', 'trans': 'explicit'},\n{'word': '\u6307\u5bfc', 'pinyin': 'zh\u01d0d\u01ceo', 'trans': 'guidance'},\n{'word': '\u589e\u5f3a', 'pinyin': 'z\u0113ngqi\u00e1ng', 'trans': 'enhance'},\n{'word': '\u80fd\u529b', 'pinyin': 'n\u00e9ngl\u00ec', 'trans': 'ability'},\n{'word': '\u5229\u7528', 'pinyin': 'l\u00ecy\u00f2ng', 'trans': 'utilize'},\n{'word': '\u63d0\u4f9b', 'pinyin': 't\u00edg\u014dng', 'trans': 'provide'},\n{'word': '\u9ad8\u5c42\u6b21', 'pinyin': 'g\u0101o c\u00e9ngc\u00ec', 'trans': 'high-level'},\n{'word': '\u901a\u7528', 'pinyin': 't\u014dngy\u00f2ng', 'trans': 'general'},\n{'word': '\u5e2e\u52a9', 'pinyin': 'b\u0101ngzh\u00f9', 'trans': 'help'},\n{'word': '\u6267\u884c', 'pinyin': 'zh\u00edx\u00edng', 'trans': 'execute'},\n{'word': '\u53cd\u9988', 'pinyin': 'f\u01cenku\u00ec', 'trans': 'feedback'},\n{'word': '\u6301\u7eed', 'pinyin': 'ch\u00edx\u00f9', 'trans': 'continuous'},\n{'word': '\u4f18\u5316', 'pinyin': 'y\u014duhu\u00e0', 'trans': 'optimize'},\n{'word': '\u5b9e\u9a8c', 'pinyin': 'sh\u00edy\u00e0n', 'trans': 'experiment'},\n{'word': '\u8868\u660e', 'pinyin': 'bi\u01ceom\u00edng', 'trans': 'indicate'},\n{'word': '\u4ee3\u8868\u6027', 'pinyin': 'd\u00e0ibi\u01ceox\u00ecng', 'trans': 'representative'},\n{'word': '\u663e\u8457', 'pinyin': 'xi\u01cenzh\u00f9', 'trans': 'significant'},\n{'word': '\u4f18\u4e8e', 'pinyin': 'y\u014duy\u00fa', 'trans': 'superior to'},\n{'word': '\u73b0\u6709', 'pinyin': 'xi\u00e0ny\u01d2u', 'trans': 'existing'},\n{'word': '\u57fa\u51c6', 'pinyin': 'j\u012bzh\u01d4n', 'trans': 'benchmark'},\n{'word': '\u63d0\u9ad8', 'pinyin': 't\u00edg\u0101o', 'trans': 'improve'},\n{'word': '\u5b8c\u6210', 'pinyin': 'w\u00e1nch\u00e9ng', 'trans': 'complete'},\n{'word': '\u6548\u7387', 'pinyin': 'xi\u00e0ol\u01dc', 'trans': 'efficiency'},\n{'word': '\u6cdb\u5316', 'pinyin': 'f\u00e0nhu\u00e0', 'trans': 'generalize'}]"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 256, "total_tokens": 1284, "completion_tokens": 1028}}
[05.03.2025 09:12] Response: [{'word': '大语言模型', 'pinyin': 'dà yǔyán móxíng', 'trans': 'large language model'},
{'word': '基于', 'pinyin': 'jīyú', 'trans': 'based on'},
{'word': '代理', 'pinyin': 'dàilǐ', 'trans': 'agent'},
{'word': '互动式', 'pinyin': 'hùdòngshì', 'trans': 'interactive'},
{'word': '规划', 'pinyin': 'guīhuà', 'trans': 'planning'},
{'word': '任务', 'pinyin': 'rènwù', 'trans': 'task'},
{'word': '幻觉', 'pinyin': 'huànjué', 'trans': 'hallucination'},
{'word': '困扰', 'pinyin': 'kùnrǎo', 'trans': 'trouble'},
{'word': '重新', 'pinyin': 'chóngxīn', 'trans': 'renew'},
{'word': '训练', 'pinyin': 'xùnliàn', 'trans': 'training'},
{'word': '挑战', 'pinyin': 'tiǎozhàn', 'trans': 'challenge'},
{'word': '提出', 'pinyin': 'tíchū', 'trans': 'propose'},
{'word': '框架', 'pinyin': 'kuàngjià', 'trans': 'framework'},
{'word': '显式', 'pinyin': 'xiǎnshì', 'trans': 'explicit'},
{'word': '指导', 'pinyin': 'zhǐdǎo', 'trans': 'guidance'},
{'word': '增强', 'pinyin': 'zēngqiáng', 'trans': 'enhance'},
{'word': '能力', 'pinyin': 'nénglì', 'trans': 'ability'},
{'word': '利用', 'pinyin': 'lìyòng', 'trans': 'utilize'},
{'word': '提供', 'pinyin': 'tígōng', 'trans': 'provide'},
{'word': '高层次', 'pinyin': 'gāo céngcì', 'trans': 'high-level'},
{'word': '通用', 'pinyin': 'tōngyòng', 'trans': 'general'},
{'word': '帮助', 'pinyin': 'bāngzhù', 'trans': 'help'},
{'word': '执行', 'pinyin': 'zhíxíng', 'trans': 'execute'},
{'word': '反馈', 'pinyin': 'fǎnkuì', 'trans': 'feedback'},
{'word': '持续', 'pinyin': 'chíxù', 'trans': 'continuous'},
{'word': '优化', 'pinyin': 'yōuhuà', 'trans': 'optimize'},
{'word': '实验', 'pinyin': 'shíyàn', 'trans': 'experiment'},
{'word': '表明', 'pinyin': 'biǎomíng', 'trans': 'indicate'},
{'word': '代表性', 'pinyin': 'dàibiǎoxìng', 'trans': 'representative'},
{'word': '显著', 'pinyin': 'xiǎnzhù', 'trans': 'significant'},
{'word': '优于', 'pinyin': 'yōuyú', 'trans': 'superior to'},
{'word': '现有', 'pinyin': 'xiànyǒu', 'trans': 'existing'},
{'word': '基准', 'pinyin': 'jīzhǔn', 'trans': 'benchmark'},
{'word': '提高', 'pinyin': 'tígāo', 'trans': 'improve'},
{'word': '完成', 'pinyin': 'wánchéng', 'trans': 'complete'},
{'word': '效率', 'pinyin': 'xiàolǜ', 'trans': 'efficiency'},
{'word': '泛化', 'pinyin': 'fànhuà', 'trans': 'generalize'}]
[05.03.2025 09:12] Mistral request. Model: mistral-large-latest. Prompt: Translate this text in English. Text:

最近的大语言模型进展使得基于LLM的代理能够成功处理互动式规划任务。然而，现有方法常常受到规划幻觉的困扰，并且每个新代理都需要重新训练。为解决这些挑战，我们提出了元规划优化（MPO）框架，通过直接引入显式指导来增强代理的规划能力。MPO利用元规划提供高层次的通用指导，帮助代理规划，并根据任务执行反馈持续优化元规划。实验表明，MPO在两项代表性任务中显著优于现有基准，并提高了任务完成效率和泛化能力。
[05.03.2025 09:12] Mistral response. {"id": "9f4762db155d4efd85d2e9fd12a891d4", "object": "chat.completion", "created": 1741165966, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "Recent advancements in large language models have enabled LLM-based agents to successfully handle interactive planning tasks. However, existing methods often suffer from planning hallucinations, and each new agent requires retraining. To address these challenges, we propose the Meta-Planning Optimization (MPO) framework, which enhances the agent's planning capability by directly introducing explicit guidance. MPO leverages meta-planning to provide high-level, general guidance to assist the agent in planning and continuously optimizes the meta-planning based on task execution feedback. Experiments demonstrate that MPO significantly outperforms existing benchmarks in two representative tasks and improves task completion efficiency and generalization ability."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 222, "total_tokens": 363, "completion_tokens": 141}}
[05.03.2025 09:12] Response: Recent advancements in large language models have enabled LLM-based agents to successfully handle interactive planning tasks. However, existing methods often suffer from planning hallucinations, and each new agent requires retraining. To address these challenges, we propose the Meta-Planning Optimization (MPO) framework, which enhances the agent's planning capability by directly introducing explicit guidance. MPO leverages meta-planning to provide high-level, general guidance to assist the agent in planning and continuously optimizes the meta-planning based on task execution feedback. Experiments demonstrate that MPO significantly outperforms existing benchmarks in two representative tasks and improves task completion efficiency and generalization ability.
[05.03.2025 09:12] Renaming data file.
[05.03.2025 09:12] Renaming previous data. hf_papers.json to ./d/2025-03-05.json
[05.03.2025 09:12] Saving new data file.
[05.03.2025 09:12] Generating page.
[05.03.2025 09:12] Renaming previous page.
[05.03.2025 09:12] Renaming previous data. index.html to ./d/2025-03-05.html
[05.03.2025 09:12] [Experimental] Generating Chinese page for reading.
[05.03.2025 09:12] Chinese vocab [{'word': '大语言模型', 'pinyin': 'dà yǔyán móxíng', 'trans': 'large language model'}, {'word': '基于', 'pinyin': 'jīyú', 'trans': 'based on'}, {'word': '代理', 'pinyin': 'dàilǐ', 'trans': 'agent'}, {'word': '互动式', 'pinyin': 'hùdòngshì', 'trans': 'interactive'}, {'word': '规划', 'pinyin': 'guīhuà', 'trans': 'planning'}, {'word': '任务', 'pinyin': 'rènwù', 'trans': 'task'}, {'word': '幻觉', 'pinyin': 'huànjué', 'trans': 'hallucination'}, {'word': '困扰', 'pinyin': 'kùnrǎo', 'trans': 'trouble'}, {'word': '重新', 'pinyin': 'chóngxīn', 'trans': 'renew'}, {'word': '训练', 'pinyin': 'xùnliàn', 'trans': 'training'}, {'word': '挑战', 'pinyin': 'tiǎozhàn', 'trans': 'challenge'}, {'word': '提出', 'pinyin': 'tíchū', 'trans': 'propose'}, {'word': '框架', 'pinyin': 'kuàngjià', 'trans': 'framework'}, {'word': '显式', 'pinyin': 'xiǎnshì', 'trans': 'explicit'}, {'word': '指导', 'pinyin': 'zhǐdǎo', 'trans': 'guidance'}, {'word': '增强', 'pinyin': 'zēngqiáng', 'trans': 'enhance'}, {'word': '能力', 'pinyin': 'nénglì', 'trans': 'ability'}, {'word': '利用', 'pinyin': 'lìyòng', 'trans': 'utilize'}, {'word': '提供', 'pinyin': 'tígōng', 'trans': 'provide'}, {'word': '高层次', 'pinyin': 'gāo céngcì', 'trans': 'high-level'}, {'word': '通用', 'pinyin': 'tōngyòng', 'trans': 'general'}, {'word': '帮助', 'pinyin': 'bāngzhù', 'trans': 'help'}, {'word': '执行', 'pinyin': 'zhíxíng', 'trans': 'execute'}, {'word': '反馈', 'pinyin': 'fǎnkuì', 'trans': 'feedback'}, {'word': '持续', 'pinyin': 'chíxù', 'trans': 'continuous'}, {'word': '优化', 'pinyin': 'yōuhuà', 'trans': 'optimize'}, {'word': '实验', 'pinyin': 'shíyàn', 'trans': 'experiment'}, {'word': '表明', 'pinyin': 'biǎomíng', 'trans': 'indicate'}, {'word': '代表性', 'pinyin': 'dàibiǎoxìng', 'trans': 'representative'}, {'word': '显著', 'pinyin': 'xiǎnzhù', 'trans': 'significant'}, {'word': '优于', 'pinyin': 'yōuyú', 'trans': 'superior to'}, {'word': '现有', 'pinyin': 'xiànyǒu', 'trans': 'existing'}, {'word': '基准', 'pinyin': 'jīzhǔn', 'trans': 'benchmark'}, {'word': '提高', 'pinyin': 'tígāo', 'trans': 'improve'}, {'word': '完成', 'pinyin': 'wánchéng', 'trans': 'complete'}, {'word': '效率', 'pinyin': 'xiàolǜ', 'trans': 'efficiency'}, {'word': '泛化', 'pinyin': 'fànhuà', 'trans': 'generalize'}]
[05.03.2025 09:12] Renaming previous Chinese page.
[05.03.2025 09:12] Renaming previous data. zh.html to ./d/2025-03-04_zh_reading_task.html
[05.03.2025 09:12] Writing Chinese reading task.
[05.03.2025 09:12] Writing result.
[05.03.2025 09:12] Renaming log file.
[05.03.2025 09:12] Renaming previous data. log.txt to ./logs/2025-03-05_last_log.txt
