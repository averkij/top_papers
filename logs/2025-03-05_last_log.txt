[05.03.2025 20:12] Read previous papers.
[05.03.2025 20:12] Generating top page (month).
[05.03.2025 20:12] Writing top page (month).
[05.03.2025 21:10] Read previous papers.
[05.03.2025 21:10] Get feed.
[05.03.2025 21:10] Get page data from previous paper. URL: https://huggingface.co/papers/2503.02682
[05.03.2025 21:10] Get page data from previous paper. URL: https://huggingface.co/papers/2503.02846
[05.03.2025 21:10] Get page data from previous paper. URL: https://huggingface.co/papers/2503.02879
[05.03.2025 21:10] Get page data from previous paper. URL: https://huggingface.co/papers/2503.01935
[05.03.2025 21:10] Get page data from previous paper. URL: https://huggingface.co/papers/2503.01328
[05.03.2025 21:10] Get page data from previous paper. URL: https://huggingface.co/papers/2503.00735
[05.03.2025 21:10] Get page data from previous paper. URL: https://huggingface.co/papers/2503.02368
[05.03.2025 21:10] Get page data from previous paper. URL: https://huggingface.co/papers/2503.00955
[05.03.2025 21:10] Get page data from previous paper. URL: https://huggingface.co/papers/2502.14856
[05.03.2025 21:10] Get page data from previous paper. URL: https://huggingface.co/papers/2503.02197
[05.03.2025 21:10] Get page data from previous paper. URL: https://huggingface.co/papers/2503.02878
[05.03.2025 21:10] Get page data from previous paper. URL: https://huggingface.co/papers/2503.02537
[05.03.2025 21:10] Get page data from previous paper. URL: https://huggingface.co/papers/2503.01342
[05.03.2025 21:10] Get page data from previous paper. URL: https://huggingface.co/papers/2503.02876
[05.03.2025 21:10] Get page data from previous paper. URL: https://huggingface.co/papers/2503.00876
[05.03.2025 21:10] Get page data from previous paper. URL: https://huggingface.co/papers/2503.02357
[05.03.2025 21:10] Get page data from previous paper. URL: https://huggingface.co/papers/2503.02783
[05.03.2025 21:10] Get page data from previous paper. URL: https://huggingface.co/papers/2503.02268
[05.03.2025 21:10] Get page data from previous paper. URL: https://huggingface.co/papers/2503.02812
[05.03.2025 21:10] Get page data from previous paper. URL: https://huggingface.co/papers/2503.02823
[05.03.2025 21:10] Get page data from previous paper. URL: https://huggingface.co/papers/2503.02152
[05.03.2025 21:10] Get page data from previous paper. URL: https://huggingface.co/papers/2503.02304
[05.03.2025 21:10] Get page data from previous paper. URL: https://huggingface.co/papers/2503.01842
[05.03.2025 21:10] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[05.03.2025 21:10] No deleted papers detected.
[05.03.2025 21:10] Downloading and parsing papers (pdf, html). Total: 23.
[05.03.2025 21:10] Downloading and parsing paper https://huggingface.co/papers/2503.02682.
[05.03.2025 21:10] Extra JSON file exists (./assets/json/2503.02682.json), skip PDF parsing.
[05.03.2025 21:10] Paper image links file exists (./assets/img_data/2503.02682.json), skip HTML parsing.
[05.03.2025 21:10] Success.
[05.03.2025 21:10] Downloading and parsing paper https://huggingface.co/papers/2503.02846.
[05.03.2025 21:10] Extra JSON file exists (./assets/json/2503.02846.json), skip PDF parsing.
[05.03.2025 21:10] Paper image links file exists (./assets/img_data/2503.02846.json), skip HTML parsing.
[05.03.2025 21:10] Success.
[05.03.2025 21:10] Downloading and parsing paper https://huggingface.co/papers/2503.02879.
[05.03.2025 21:10] Extra JSON file exists (./assets/json/2503.02879.json), skip PDF parsing.
[05.03.2025 21:10] Paper image links file exists (./assets/img_data/2503.02879.json), skip HTML parsing.
[05.03.2025 21:10] Success.
[05.03.2025 21:10] Downloading and parsing paper https://huggingface.co/papers/2503.01935.
[05.03.2025 21:10] Extra JSON file exists (./assets/json/2503.01935.json), skip PDF parsing.
[05.03.2025 21:10] Paper image links file exists (./assets/img_data/2503.01935.json), skip HTML parsing.
[05.03.2025 21:10] Success.
[05.03.2025 21:10] Downloading and parsing paper https://huggingface.co/papers/2503.01328.
[05.03.2025 21:10] Extra JSON file exists (./assets/json/2503.01328.json), skip PDF parsing.
[05.03.2025 21:10] Paper image links file exists (./assets/img_data/2503.01328.json), skip HTML parsing.
[05.03.2025 21:10] Success.
[05.03.2025 21:10] Downloading and parsing paper https://huggingface.co/papers/2503.00735.
[05.03.2025 21:10] Extra JSON file exists (./assets/json/2503.00735.json), skip PDF parsing.
[05.03.2025 21:10] Paper image links file exists (./assets/img_data/2503.00735.json), skip HTML parsing.
[05.03.2025 21:10] Success.
[05.03.2025 21:10] Downloading and parsing paper https://huggingface.co/papers/2503.02368.
[05.03.2025 21:10] Extra JSON file exists (./assets/json/2503.02368.json), skip PDF parsing.
[05.03.2025 21:10] Paper image links file exists (./assets/img_data/2503.02368.json), skip HTML parsing.
[05.03.2025 21:10] Success.
[05.03.2025 21:10] Downloading and parsing paper https://huggingface.co/papers/2503.00955.
[05.03.2025 21:10] Extra JSON file exists (./assets/json/2503.00955.json), skip PDF parsing.
[05.03.2025 21:10] Paper image links file exists (./assets/img_data/2503.00955.json), skip HTML parsing.
[05.03.2025 21:10] Success.
[05.03.2025 21:10] Downloading and parsing paper https://huggingface.co/papers/2502.14856.
[05.03.2025 21:10] Extra JSON file exists (./assets/json/2502.14856.json), skip PDF parsing.
[05.03.2025 21:10] Paper image links file exists (./assets/img_data/2502.14856.json), skip HTML parsing.
[05.03.2025 21:10] Success.
[05.03.2025 21:10] Downloading and parsing paper https://huggingface.co/papers/2503.02197.
[05.03.2025 21:10] Extra JSON file exists (./assets/json/2503.02197.json), skip PDF parsing.
[05.03.2025 21:10] Paper image links file exists (./assets/img_data/2503.02197.json), skip HTML parsing.
[05.03.2025 21:10] Success.
[05.03.2025 21:10] Downloading and parsing paper https://huggingface.co/papers/2503.02878.
[05.03.2025 21:10] Extra JSON file exists (./assets/json/2503.02878.json), skip PDF parsing.
[05.03.2025 21:10] Paper image links file exists (./assets/img_data/2503.02878.json), skip HTML parsing.
[05.03.2025 21:10] Success.
[05.03.2025 21:10] Downloading and parsing paper https://huggingface.co/papers/2503.02537.
[05.03.2025 21:10] Extra JSON file exists (./assets/json/2503.02537.json), skip PDF parsing.
[05.03.2025 21:10] Paper image links file exists (./assets/img_data/2503.02537.json), skip HTML parsing.
[05.03.2025 21:10] Success.
[05.03.2025 21:10] Downloading and parsing paper https://huggingface.co/papers/2503.01342.
[05.03.2025 21:10] Extra JSON file exists (./assets/json/2503.01342.json), skip PDF parsing.
[05.03.2025 21:10] Paper image links file exists (./assets/img_data/2503.01342.json), skip HTML parsing.
[05.03.2025 21:10] Success.
[05.03.2025 21:10] Downloading and parsing paper https://huggingface.co/papers/2503.02876.
[05.03.2025 21:10] Extra JSON file exists (./assets/json/2503.02876.json), skip PDF parsing.
[05.03.2025 21:10] Paper image links file exists (./assets/img_data/2503.02876.json), skip HTML parsing.
[05.03.2025 21:10] Success.
[05.03.2025 21:10] Downloading and parsing paper https://huggingface.co/papers/2503.00876.
[05.03.2025 21:10] Extra JSON file exists (./assets/json/2503.00876.json), skip PDF parsing.
[05.03.2025 21:10] Paper image links file exists (./assets/img_data/2503.00876.json), skip HTML parsing.
[05.03.2025 21:10] Success.
[05.03.2025 21:10] Downloading and parsing paper https://huggingface.co/papers/2503.02357.
[05.03.2025 21:10] Extra JSON file exists (./assets/json/2503.02357.json), skip PDF parsing.
[05.03.2025 21:10] Paper image links file exists (./assets/img_data/2503.02357.json), skip HTML parsing.
[05.03.2025 21:10] Success.
[05.03.2025 21:10] Downloading and parsing paper https://huggingface.co/papers/2503.02783.
[05.03.2025 21:10] Extra JSON file exists (./assets/json/2503.02783.json), skip PDF parsing.
[05.03.2025 21:10] Paper image links file exists (./assets/img_data/2503.02783.json), skip HTML parsing.
[05.03.2025 21:10] Success.
[05.03.2025 21:10] Downloading and parsing paper https://huggingface.co/papers/2503.02268.
[05.03.2025 21:10] Extra JSON file exists (./assets/json/2503.02268.json), skip PDF parsing.
[05.03.2025 21:10] Paper image links file exists (./assets/img_data/2503.02268.json), skip HTML parsing.
[05.03.2025 21:10] Success.
[05.03.2025 21:10] Downloading and parsing paper https://huggingface.co/papers/2503.02812.
[05.03.2025 21:10] Extra JSON file exists (./assets/json/2503.02812.json), skip PDF parsing.
[05.03.2025 21:10] Paper image links file exists (./assets/img_data/2503.02812.json), skip HTML parsing.
[05.03.2025 21:10] Success.
[05.03.2025 21:10] Downloading and parsing paper https://huggingface.co/papers/2503.02823.
[05.03.2025 21:10] Extra JSON file exists (./assets/json/2503.02823.json), skip PDF parsing.
[05.03.2025 21:10] Paper image links file exists (./assets/img_data/2503.02823.json), skip HTML parsing.
[05.03.2025 21:10] Success.
[05.03.2025 21:10] Downloading and parsing paper https://huggingface.co/papers/2503.02152.
[05.03.2025 21:10] Extra JSON file exists (./assets/json/2503.02152.json), skip PDF parsing.
[05.03.2025 21:10] Paper image links file exists (./assets/img_data/2503.02152.json), skip HTML parsing.
[05.03.2025 21:10] Success.
[05.03.2025 21:10] Downloading and parsing paper https://huggingface.co/papers/2503.02304.
[05.03.2025 21:10] Extra JSON file exists (./assets/json/2503.02304.json), skip PDF parsing.
[05.03.2025 21:10] Paper image links file exists (./assets/img_data/2503.02304.json), skip HTML parsing.
[05.03.2025 21:10] Success.
[05.03.2025 21:10] Downloading and parsing paper https://huggingface.co/papers/2503.01842.
[05.03.2025 21:10] Extra JSON file exists (./assets/json/2503.01842.json), skip PDF parsing.
[05.03.2025 21:10] Paper image links file exists (./assets/img_data/2503.01842.json), skip HTML parsing.
[05.03.2025 21:10] Success.
[05.03.2025 21:10] Enriching papers with extra data.
[05.03.2025 21:10] ********************************************************************************
[05.03.2025 21:10] Abstract 0. Recent advancements in large language models (LLMs) have enabled LLM-based agents to successfully tackle interactive planning tasks. However, despite their successes, existing approaches often suffer from planning hallucinations and require retraining for each new agent. To address these challenges,...
[05.03.2025 21:10] ********************************************************************************
[05.03.2025 21:10] Abstract 1. Large language models (LLMs) exhibit hallucinations (i.e., unfaithful or nonsensical information) when serving as AI assistants in various domains. Since hallucinations always come with truthful content in the LLM responses, previous factuality alignment methods that conduct response-level preferenc...
[05.03.2025 21:10] ********************************************************************************
[05.03.2025 21:10] Abstract 2. In this paper, we present a thorough analysis of the impact of Large Language Models (LLMs) on Wikipedia, examining the evolution of Wikipedia through existing data and using simulations to explore potential risks. We begin by analyzing page views and article content to study Wikipedia's recent chan...
[05.03.2025 21:10] ********************************************************************************
[05.03.2025 21:10] Abstract 3. Large Language Models (LLMs) have shown remarkable capabilities as autonomous agents, yet existing benchmarks either focus on single-agent tasks or are confined to narrow domains, failing to capture the dynamics of multi-agent coordination and competition. In this paper, we introduce MultiAgentBench...
[05.03.2025 21:10] ********************************************************************************
[05.03.2025 21:10] Abstract 4. Pipeline parallelism (PP) is widely used for training large language models (LLMs), yet its scalability is often constrained by high activation memory consumption as the number of in-flight microbatches grows with the degree of PP. In this paper, we focus on addressing this challenge by leveraging t...
[05.03.2025 21:10] ********************************************************************************
[05.03.2025 21:10] Abstract 5. We introduce LADDER (Learning through Autonomous Difficulty-Driven Example Recursion), a framework which enables Large Language Models to autonomously improve their problem-solving capabilities through self-guided learning by recursively generating and solving progressively simpler variants of compl...
[05.03.2025 21:10] ********************************************************************************
[05.03.2025 21:10] Abstract 6. While Reinforcement Learning from Human Feedback (RLHF) has become the predominant method for controlling language model outputs, it suffers from high computational costs and training instability. Guided decoding, especially value-guided methods, offers a cost-effective alternative by controlling ou...
[05.03.2025 21:10] ********************************************************************************
[05.03.2025 21:10] Abstract 7. The rise of misinformation, exacerbated by Large Language Models (LLMs) like GPT and Gemini, demands robust fact-checking solutions, especially for low-resource languages like Vietnamese. Existing methods struggle with semantic ambiguity, homonyms, and complex linguistic structures, often trading ac...
[05.03.2025 21:10] ********************************************************************************
[05.03.2025 21:10] Abstract 8. Speculative sampling has emerged as an important technique for accelerating the auto-regressive generation process of large language models (LLMs) by utilizing a draft-then-verify mechanism to produce multiple tokens per forward pass. While state-of-the-art speculative sampling methods use only a si...
[05.03.2025 21:10] ********************************************************************************
[05.03.2025 21:10] Abstract 9. Large Language Model (LLM) agents have demonstrated remarkable generalization capabilities across multi-domain tasks. Existing agent tuning approaches typically employ supervised finetuning on entire expert trajectories. However, behavior-cloning of full trajectories can introduce expert bias and we...
[05.03.2025 21:10] ********************************************************************************
[05.03.2025 21:10] Abstract 10. Collecting ground truth task completion rewards or human demonstrations for multi-step reasoning tasks is often cost-prohibitive and time-consuming, especially in interactive domains like web tasks. To address this bottleneck, we present self-taught lookahead, a self-supervised method that leverages...
[05.03.2025 21:10] ********************************************************************************
[05.03.2025 21:10] Abstract 11. Diffusion models have achieved remarkable advances in various image generation tasks. However, their performance notably declines when generating images at resolutions higher than those used during the training period. Despite the existence of numerous methods for producing high-resolution images, t...
[05.03.2025 21:10] ********************************************************************************
[05.03.2025 21:10] Abstract 12. Generalist models have achieved remarkable success in both language and vision-language tasks, showcasing the potential of unified modeling. However, effectively integrating fine-grained perception tasks like detection and segmentation into these models remains a significant challenge. This is prima...
[05.03.2025 21:10] ********************************************************************************
[05.03.2025 21:10] Abstract 13. Advancing AI in computational pathology requires large, high-quality, and diverse datasets, yet existing public datasets are often limited in organ diversity, class coverage, or annotation quality. To bridge this gap, we introduce SPIDER (Supervised Pathology Image-DEscription Repository), the large...
[05.03.2025 21:10] ********************************************************************************
[05.03.2025 21:10] Abstract 14. In representation learning, uniformity refers to the uniform feature distribution in the latent space (i.e., unit hypersphere). Previous work has shown that improving uniformity contributes to the learning of under-represented classes. However, most of the previous work focused on classification; th...
[05.03.2025 21:10] ********************************************************************************
[05.03.2025 21:10] Abstract 15. Evaluating text-to-vision content hinges on two crucial aspects: visual quality and alignment. While significant progress has been made in developing objective models to assess these dimensions, the performance of such models heavily relies on the scale and quality of human annotations. According to...
[05.03.2025 21:10] ********************************************************************************
[05.03.2025 21:10] Abstract 16. Preference learning enhances Code LLMs beyond supervised fine-tuning by leveraging relative quality comparisons. Existing methods construct preference pairs from   candidates based on test case success, treating the higher pass rate sample as positive and the lower as negative. However, this approac...
[05.03.2025 21:10] ********************************************************************************
[05.03.2025 21:10] Abstract 17. Recent advancements in Large Language Models (LLMs) have led to the development of intelligent LLM-based agents capable of interacting with graphical user interfaces (GUIs). These agents demonstrate strong reasoning and adaptability, enabling them to perform complex tasks that traditionally required...
[05.03.2025 21:10] ********************************************************************************
[05.03.2025 21:10] Abstract 18. Autoregressive language models rely on a Key-Value (KV) Cache, which avoids re-computing past hidden states during generation, making it faster. As model sizes and context lengths grow, the KV Cache becomes a significant memory bottleneck, which calls for compression methods that limit its size duri...
[05.03.2025 21:10] ********************************************************************************
[05.03.2025 21:10] Abstract 19. In recent decades, neuroscientific and psychological research has traced direct relationships between taste and auditory perceptions. This article explores multimodal generative models capable of converting taste information into music, building on this foundational research. We provide a brief revi...
[05.03.2025 21:10] ********************************************************************************
[05.03.2025 21:10] Abstract 20. While advances in large language models (LLMs) have greatly improved the quality of synthetic text data in recent years, synthesizing tabular data has received relatively less attention. We address this disparity with Tabby, a simple but powerful post-training modification to the standard Transforme...
[05.03.2025 21:10] ********************************************************************************
[05.03.2025 21:10] Abstract 21. In recent years, general visual foundation models (VFMs) have witnessed increasing adoption, particularly as image encoders for popular multi-modal large language models (MLLMs). However, without semantically fine-grained supervision, these models still encounter fundamental prediction errors in the...
[05.03.2025 21:10] ********************************************************************************
[05.03.2025 21:10] Abstract 22. This paper introduces Discrete-time Hybrid Automata Learning (DHAL), a framework using on-policy Reinforcement Learning to identify and execute mode-switching without trajectory segmentation or event function learning. Hybrid dynamical systems, which include continuous flow and discrete mode switchi...
[05.03.2025 21:10] Read previous papers.
[05.03.2025 21:10] Generating reviews via LLM API.
[05.03.2025 21:10] Using data from previous issue: {"categories": ["#rl", "#training", "#optimization", "#hallucinations", "#agents"], "emoji": "üß†", "ru": {"title": "–ú–µ—Ç–∞–ø–ª–∞–Ω—ã –¥–ª—è —É–º–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ, —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–µ–µ, –±–µ–∑ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –∑–∞–¥–∞—á –∞–≥–µ–Ω—Ç–∞–º–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö 
[05.03.2025 21:10] Using data from previous issue: {"categories": ["#hallucinations", "#training", "#rlhf", "#alignment"], "emoji": "üé≠", "ru": {"title": "Mask-DPO: —Ç–æ—á–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ —Ñ–∞–∫—Ç–æ–≤ –≤ –æ—Ç–≤–µ—Ç–∞—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ Mask-DPO –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –ú–µ—Ç–æ–¥ –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ –æ–ø
[05.03.2025 21:10] Using data from previous issue: {"categories": ["#rag", "#benchmark", "#machine_translation", "#dataset", "#multimodal", "#science", "#data"], "emoji": "üß†", "ru": {"title": "–ë–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –º–µ–Ω—è—é—Ç –ª–∏—Ü–æ –í–∏–∫–∏–ø–µ–¥–∏–∏: –∞–Ω–∞–ª–∏–∑ –≤–ª–∏—è–Ω–∏—è –∏ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö —Ä–∏—Å–∫–æ–≤", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∞–Ω–∞–ª–∏–∑ –≤–ª–∏—è–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ
[05.03.2025 21:10] Using data from previous issue: {"categories": ["#games", "#optimization", "#open_source", "#benchmark", "#agents"], "emoji": "ü§ñ", "ru": {"title": "MultiAgentBench: –Ω–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω—ã—Ö LLM-—Å–∏—Å—Ç–µ–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç MultiAgentBench - –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ
[05.03.2025 21:10] Using data from previous issue: {"categories": ["#open_source", "#inference", "#optimization", "#training"], "emoji": "üöÄ", "ru": {"title": "–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞–º—è—Ç–∏ –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ —É–ª—É—á—à–µ–Ω–∏—é –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç–∏ –∫–æ–Ω–≤–µ–π–µ—Ä–Ω–æ–≥–æ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã –ø
[05.03.2025 21:10] Using data from previous issue: {"categories": ["#math", "#reasoning", "#optimization", "#training", "#rl"], "emoji": "üßÆ", "ru": {"title": "–°–∞–º–æ–æ–±—É—á–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é —É–ø—Ä–æ—â–µ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –º–µ—Ç–æ–¥ LADDER, –ø–æ–∑–≤–æ–ª—è—é—â–∏–π —è–∑—ã–∫–æ–≤—ã–º –º–æ–¥–µ–ª—è–º —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞—Ç—å –Ω–∞–≤—ã–∫–∏ —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á –ø—É—Ç–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏ 
[05.03.2025 21:10] Using data from previous issue: {"categories": ["#optimization", "#alignment", "#training", "#rlhf"], "emoji": "üß†", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –≤—ã—Ö–æ–¥–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –Ω–∞–∑—ã–≤–∞–µ–º—ã–π '–ò—Ç–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Ñ—É–Ω–∫—Ü–∏–∏ —Ü–µ–Ω–Ω–æ—Å
[05.03.2025 21:10] Using data from previous issue: {"categories": ["#multilingual", "#inference", "#benchmark", "#low_resource", "#dataset", "#science", "#data"], "emoji": "üïµÔ∏è", "ru": {"title": "SemViQA: –ü–µ—Ä–µ–¥–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ñ–∞–∫—Ç–æ–≤ –¥–ª—è –±–æ—Ä—å–±—ã —Å –¥–µ–∑–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –Ω–∞ –≤—å–µ—Ç–Ω–∞–º—Å–∫–æ–º —è–∑—ã–∫–µ", "desc": "SemViQA - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ñ–∞–∫—Ç–æ–≤ –Ω–∞ –≤—å–µ—Ç–Ω–∞
[05.03.2025 21:10] Using data from previous issue: {"categories": ["#training", "#inference", "#optimization"], "emoji": "üöÄ", "ru": {"title": "–ë—ã—Å—Ç—Ä–µ–µ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ: –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Å–ø–µ–∫—É–ª—è—Ç–∏–≤–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏ –¥–ª—è LLM", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç FR-Spec - –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —Å–ø–µ–∫—É–ª—è—Ç–∏–≤–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è —Ä–∞–±–æ—Ç—ã –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –ú–µ—Ç–æ–¥ –æ–ø—Ç–∏–º–∏–∑–∏—Ä
[05.03.2025 21:10] Using data from previous issue: {"categories": ["#optimization", "#reasoning", "#training", "#agents", "#agi"], "emoji": "üéØ", "ru": {"title": "ATLaS: —Ç–æ—á–µ—á–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ LLM-–∞–≥–µ–Ω—Ç–æ–≤ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±–æ–±—â–µ–Ω–∏—è", "desc": "ATLaS - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –û–Ω —Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è –Ω–∞ –æ–±—É—á–µ–Ω–∏–∏ —Ç–æ–ª—å
[05.03.2025 21:10] Using data from previous issue: {"categories": ["#reasoning", "#rl", "#optimization", "#training"], "emoji": "üß†", "ru": {"title": "–°–∞–º–æ–æ–±—É—á–µ–Ω–∏–µ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –±–µ–∑ –¥–æ—Ä–æ–≥–æ—Å—Ç–æ—è—â–∏—Ö –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ 'self-taught lookahead' –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –æ—Ü–µ–Ω–∫–∏, —Å–ø–æ—Å–æ–±–Ω–æ–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –Ω–∞–ø—Ä–∞–≤–ª—è—Ç—å –ø–æ–∏—Å–∫, —É–ø—Ä–∞–≤
[05.03.2025 21:10] Using data from previous issue: {"categories": ["#cv", "#optimization", "#diffusion", "#training"], "emoji": "üñºÔ∏è", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤—ã—Å–æ–∫–æ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ RectifiedHR –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤—ã—Å–æ–∫–æ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é –¥–∏—Ñ—Ñ—É–∑
[05.03.2025 21:10] Using data from previous issue: {"categories": ["#cv", "#training", "#optimization", "#agi", "#open_source", "#multimodal", "#architecture"], "emoji": "üî¨", "ru": {"title": "–£–Ω–∏—Ñ–∏–∫–∞—Ü–∏—è –∑–∞–¥–∞—á –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è —á–µ—Ä–µ–∑ —è–∑—ã–∫–æ–≤–æ–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É–Ω–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∑–∞–¥–∞—á —Ç–æ–Ω–∫–æ–π –≤–∏–∑—É–∞–ª—å–Ω–æ–π –ø–µ—Ä—Ü–µ–ø—Ü–∏–∏ 
[05.03.2025 21:10] Using data from previous issue: {"categories": ["#open_source", "#dataset", "#multimodal", "#science", "#benchmark"], "emoji": "üï∑Ô∏è", "ru": {"title": "SPIDER: –ù–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ò–ò –≤ –ø–∞—Ç–æ–ª–æ–≥–∏–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç SPIDER - –∫—Ä—É–ø–Ω–µ–π—à–∏–π –æ–±—â–µ–¥–æ—Å—Ç—É–ø–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–π –ø–∞—Ç–æ–ª–æ–≥–∏–∏, –æ—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏–π –º–Ω–æ–∂–µ—Å—Ç–≤–æ —Ç–∏
[05.03.2025 21:10] Using data from previous issue: {"categories": ["#data", "#optimization", "#training"], "emoji": "üåê", "ru": {"title": "–ì–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–π –ø–æ–¥—Ö–æ–¥ –∫ —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–º—É –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—é –≤ –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –ø—Ä–æ–±–ª–µ–º—É —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –≤ –∑–∞–¥–∞—á–∞—Ö —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ —Å –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –≤—ã–±–æ—Ä–∫–∞–º–∏. –ê–≤
[05.03.2025 21:10] Using data from previous issue: {"categories": ["#benchmark", "#alignment", "#long_context", "#dataset", "#multimodal", "#cv", "#video"], "emoji": "üîç", "ru": {"title": "–ë–æ–ª—å—à–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ª—É—á—à–µ–π –æ—Ü–µ–Ω–∫–∏ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç —Å–æ–∑–¥–∞–Ω–∏–µ –±–æ–ª—å—à–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö Q-EVAL-100K –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –∫–æ
[05.03.2025 21:10] Using data from previous issue: {"categories": ["#alignment", "#rlhf", "#open_source", "#dataset", "#optimization", "#training"], "emoji": "üîß", "ru": {"title": "–ò—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º –¥–ª—è —É—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∫–æ–¥–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ IterPref –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∫–æ–¥–∞ (Co
[05.03.2025 21:10] Using data from previous issue: {"categories": ["#optimization", "#agents", "#benchmark", "#reasoning", "#open_source", "#training"], "emoji": "üß†", "ru": {"title": "–≠–≤–æ–ª—é—Ü–∏–æ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–æ–≤ GUI: –±–∞–ª–∞–Ω—Å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∏ –≥–∏–±–∫–æ—Å—Ç–∏", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π —ç–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –¥–ª—è –∞–≥–µ–Ω—Ç–æ–≤, —Ä–∞–±–æ—Ç–∞—é—â–∏—Ö —Å –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–º –∏
[05.03.2025 21:10] Using data from previous issue: {"categories": ["#inference", "#long_context", "#training", "#optimization"], "emoji": "üîç", "ru": {"title": "Q-Filters: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Å–∂–∞—Ç–∏–µ KV-–∫—ç—à–∞ –±–µ–∑ –∫–æ–º–ø—Ä–æ–º–∏—Å—Å–æ–≤ –≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —Å–∂–∞—Ç–∏—è KV-–∫—ç—à–∞ –¥–ª—è –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Q-Fil
[05.03.2025 21:10] Using data from previous issue: {"categories": ["#multimodal", "#open_source", "#games", "#dataset"], "emoji": "üéµ", "ru": {"title": "–û—Ç –≤–∫—É—Å–∞ –∫ –º–µ–ª–æ–¥–∏–∏: –ò–ò –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –≥–∞—Å—Ç—Ä–æ–Ω–æ–º–∏—á–µ—Å–∫–∏–µ –æ—â—É—â–µ–Ω–∏—è –≤ –º—É–∑—ã–∫—É", "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏, —Å–ø–æ—Å–æ–±–Ω—ã–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤—ã–≤–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≤–∫—É—Å–µ –≤ –º—É–∑—ã–∫—É. –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ–≤–µ–ª–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω
[05.03.2025 21:10] Using data from previous issue: {"categories": ["#dataset", "#synthetic", "#architecture", "#data"], "emoji": "üìä", "ru": {"title": "Tabby: –ø—Ä–æ—Ä—ã–≤ –≤ —Å–∏–Ω—Ç–µ–∑–µ —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å –ø–æ–º–æ—â—å—é —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ Tabby - –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. Tabby –∏—Å–ø–æ–ª—å–∑—É–µ—Ç Gat
[05.03.2025 21:10] Using data from previous issue: {"categories": ["#dataset", "#cv", "#agi", "#reasoning", "#optimization", "#multimodal", "#games", "#data", "#open_source"], "emoji": "üîç", "ru": {"title": "TokenOCR: –ù–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ TokenOCR - –ø–µ—Ä–≤—É—é –≤–∏–∑—É–∞–ª—å–Ω—É—é —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—É—é –º
[05.03.2025 21:10] Using data from previous issue: {"categories": ["#games", "#rl", "#robotics", "#optimization"], "emoji": "ü§ñ", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ –≥–∏–±—Ä–∏–¥–Ω—ã–º —Å–∏—Å—Ç–µ–º–∞–º –±–µ–∑ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç DHAL - —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –≥–∏–±—Ä–∏–¥–Ω—ã–º –∞–≤—Ç–æ–º–∞—Ç–∞–º —Å –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã–º –≤—Ä–µ–º–µ–Ω–µ–º, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –∏–¥–µ–Ω
[05.03.2025 21:10] Loading Chinese text from previous data.
[05.03.2025 21:10] Renaming data file.
[05.03.2025 21:10] Renaming previous data. hf_papers.json to ./d/2025-03-05.json
[05.03.2025 21:10] Saving new data file.
[05.03.2025 21:10] Generating page.
[05.03.2025 21:10] Renaming previous page.
[05.03.2025 21:10] Renaming previous data. index.html to ./d/2025-03-05.html
[05.03.2025 21:10] [Experimental] Generating Chinese page for reading.
[05.03.2025 21:10] Chinese vocab [{'word': 'Â§ßËØ≠Ë®ÄÊ®°Âûã', 'pinyin': 'd√† y«îy√°n m√≥x√≠ng', 'trans': 'large language model'}, {'word': 'Âü∫‰∫é', 'pinyin': 'jƒ´y√∫', 'trans': 'based on'}, {'word': '‰ª£ÁêÜ', 'pinyin': 'd√†il«ê', 'trans': 'agent'}, {'word': '‰∫íÂä®Âºè', 'pinyin': 'h√πd√≤ngsh√¨', 'trans': 'interactive'}, {'word': 'ËßÑÂàí', 'pinyin': 'guƒ´hu√†', 'trans': 'planning'}, {'word': '‰ªªÂä°', 'pinyin': 'r√®nw√π', 'trans': 'task'}, {'word': 'ÂπªËßâ', 'pinyin': 'hu√†nju√©', 'trans': 'hallucination'}, {'word': 'Âõ∞Êâ∞', 'pinyin': 'k√πnr«éo', 'trans': 'trouble'}, {'word': 'ÈáçÊñ∞', 'pinyin': 'ch√≥ngxƒ´n', 'trans': 'renew'}, {'word': 'ËÆ≠ÁªÉ', 'pinyin': 'x√πnli√†n', 'trans': 'training'}, {'word': 'ÊåëÊàò', 'pinyin': 'ti«éozh√†n', 'trans': 'challenge'}, {'word': 'ÊèêÂá∫', 'pinyin': 't√≠ch≈´', 'trans': 'propose'}, {'word': 'Ê°ÜÊû∂', 'pinyin': 'ku√†ngji√†', 'trans': 'framework'}, {'word': 'ÊòæÂºè', 'pinyin': 'xi«énsh√¨', 'trans': 'explicit'}, {'word': 'ÊåáÂØº', 'pinyin': 'zh«êd«éo', 'trans': 'guidance'}, {'word': 'Â¢ûÂº∫', 'pinyin': 'zƒìngqi√°ng', 'trans': 'enhance'}, {'word': 'ËÉΩÂäõ', 'pinyin': 'n√©ngl√¨', 'trans': 'ability'}, {'word': 'Âà©Áî®', 'pinyin': 'l√¨y√≤ng', 'trans': 'utilize'}, {'word': 'Êèê‰æõ', 'pinyin': 't√≠g≈çng', 'trans': 'provide'}, {'word': 'È´òÂ±ÇÊ¨°', 'pinyin': 'gƒÅo c√©ngc√¨', 'trans': 'high-level'}, {'word': 'ÈÄöÁî®', 'pinyin': 't≈çngy√≤ng', 'trans': 'general'}, {'word': 'Â∏ÆÂä©', 'pinyin': 'bƒÅngzh√π', 'trans': 'help'}, {'word': 'ÊâßË°å', 'pinyin': 'zh√≠x√≠ng', 'trans': 'execute'}, {'word': 'ÂèçÈ¶à', 'pinyin': 'f«énku√¨', 'trans': 'feedback'}, {'word': 'ÊåÅÁª≠', 'pinyin': 'ch√≠x√π', 'trans': 'continuous'}, {'word': '‰ºòÂåñ', 'pinyin': 'y≈çuhu√†', 'trans': 'optimize'}, {'word': 'ÂÆûÈ™å', 'pinyin': 'sh√≠y√†n', 'trans': 'experiment'}, {'word': 'Ë°®Êòé', 'pinyin': 'bi«éom√≠ng', 'trans': 'indicate'}, {'word': '‰ª£Ë°®ÊÄß', 'pinyin': 'd√†ibi«éox√¨ng', 'trans': 'representative'}, {'word': 'ÊòæËëó', 'pinyin': 'xi«énzh√π', 'trans': 'significant'}, {'word': '‰ºò‰∫é', 'pinyin': 'y≈çuy√∫', 'trans': 'superior to'}, {'word': 'Áé∞Êúâ', 'pinyin': 'xi√†ny«íu', 'trans': 'existing'}, {'word': 'Âü∫ÂáÜ', 'pinyin': 'jƒ´zh«în', 'trans': 'benchmark'}, {'word': 'ÊèêÈ´ò', 'pinyin': 't√≠gƒÅo', 'trans': 'improve'}, {'word': 'ÂÆåÊàê', 'pinyin': 'w√°nch√©ng', 'trans': 'complete'}, {'word': 'ÊïàÁéá', 'pinyin': 'xi√†ol«ú', 'trans': 'efficiency'}, {'word': 'Ê≥õÂåñ', 'pinyin': 'f√†nhu√†', 'trans': 'generalize'}]
[05.03.2025 21:10] Renaming previous Chinese page.
[05.03.2025 21:10] Renaming previous data. zh.html to ./d/2025-03-04_zh_reading_task.html
[05.03.2025 21:10] Writing Chinese reading task.
[05.03.2025 21:10] Writing result.
[05.03.2025 21:10] Renaming log file.
[05.03.2025 21:10] Renaming previous data. log.txt to ./logs/2025-03-05_last_log.txt
