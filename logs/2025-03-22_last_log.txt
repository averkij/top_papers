[22.03.2025 06:29] Read previous papers.
[22.03.2025 06:29] Generating top page (month).
[22.03.2025 06:29] Writing top page (month).
[22.03.2025 12:40] Read previous papers.
[22.03.2025 12:40] Get feed.
[22.03.2025 12:40] Get page data from previous paper. URL: https://huggingface.co/papers/2503.13358
[22.03.2025 12:40] Get page data from previous paper. URL: https://huggingface.co/papers/2503.16419
[22.03.2025 12:40] Get page data from previous paper. URL: https://huggingface.co/papers/2503.16416
[22.03.2025 12:40] Get page data from previous paper. URL: https://huggingface.co/papers/2503.16302
[22.03.2025 12:40] Get page data from previous paper. URL: https://huggingface.co/papers/2503.16397
[22.03.2025 12:40] Get page data from previous paper. URL: https://huggingface.co/papers/2503.15558
[22.03.2025 12:40] Get page data from previous paper. URL: https://huggingface.co/papers/2503.16365
[22.03.2025 12:40] Get page data from previous paper. URL: https://huggingface.co/papers/2503.14487
[22.03.2025 12:40] Get page data from previous paper. URL: https://huggingface.co/papers/2503.16418
[22.03.2025 12:40] Get page data from previous paper. URL: https://huggingface.co/papers/2503.13657
[22.03.2025 12:40] Get page data from previous paper. URL: https://huggingface.co/papers/2503.16257
[22.03.2025 12:40] Get page data from previous paper. URL: https://huggingface.co/papers/2503.16212
[22.03.2025 12:40] Get page data from previous paper. URL: https://huggingface.co/papers/2503.16420
[22.03.2025 12:40] Get page data from previous paper. URL: https://huggingface.co/papers/2503.15299
[22.03.2025 12:40] Get page data from previous paper. URL: https://huggingface.co/papers/2503.16252
[22.03.2025 12:40] Get page data from previous paper. URL: https://huggingface.co/papers/2503.16413
[22.03.2025 12:40] Get page data from previous paper. URL: https://huggingface.co/papers/2503.16219
[22.03.2025 12:40] Get page data from previous paper. URL: https://huggingface.co/papers/2503.10625
[22.03.2025 12:40] Get page data from previous paper. URL: https://huggingface.co/papers/2503.16422
[22.03.2025 12:40] Get page data from previous paper. URL: https://huggingface.co/papers/2503.16356
[22.03.2025 12:40] Get page data from previous paper. URL: https://huggingface.co/papers/2503.16322
[22.03.2025 12:40] Get page data from previous paper. URL: https://huggingface.co/papers/2503.16057
[22.03.2025 12:40] Get page data from previous paper. URL: https://huggingface.co/papers/2503.16425
[22.03.2025 12:40] Get page data from previous paper. URL: https://huggingface.co/papers/2503.16428
[22.03.2025 12:40] Get page data from previous paper. URL: https://huggingface.co/papers/2503.16421
[22.03.2025 12:40] Get page data from previous paper. URL: https://huggingface.co/papers/2503.16375
[22.03.2025 12:40] Get page data from previous paper. URL: https://huggingface.co/papers/2503.16055
[22.03.2025 12:40] Get page data from previous paper. URL: https://huggingface.co/papers/2503.15242
[22.03.2025 12:40] Get page data from previous paper. URL: https://huggingface.co/papers/2503.13356
[22.03.2025 12:40] Get page data from previous paper. URL: https://huggingface.co/papers/2503.16188
[22.03.2025 12:40] Get page data from previous paper. URL: https://huggingface.co/papers/2503.15851
[22.03.2025 12:40] Get page data from previous paper. URL: https://huggingface.co/papers/2503.15567
[22.03.2025 12:40] Get page data from previous paper. URL: https://huggingface.co/papers/2503.16278
[22.03.2025 12:40] Get page data from previous paper. URL: https://huggingface.co/papers/2503.14237
[22.03.2025 12:40] Get page data from previous paper. URL: https://huggingface.co/papers/2503.16429
[22.03.2025 12:40] Get page data from previous paper. URL: https://huggingface.co/papers/2503.12689
[22.03.2025 12:40] Get page data from previous paper. URL: https://huggingface.co/papers/2503.09949
[22.03.2025 12:40] Get page data from previous paper. URL: https://huggingface.co/papers/2503.16194
[22.03.2025 12:40] Get page data from previous paper. URL: https://huggingface.co/papers/2503.16031
[22.03.2025 12:40] Get page data from previous paper. URL: https://huggingface.co/papers/2503.15855
[22.03.2025 12:40] Get page data from previous paper. URL: https://huggingface.co/papers/2503.15451
[22.03.2025 12:40] Get page data from previous paper. URL: https://huggingface.co/papers/2503.13834
[22.03.2025 12:40] Get page data from previous paper. URL: https://huggingface.co/papers/2503.07906
[22.03.2025 12:40] Get page data from previous paper. URL: https://huggingface.co/papers/2503.16091
[22.03.2025 12:40] Get page data from previous paper. URL: https://huggingface.co/papers/2503.14201
[22.03.2025 12:40] Get page data from previous paper. URL: https://huggingface.co/papers/2503.13891
[22.03.2025 12:40] Get page data from previous paper. URL: https://huggingface.co/papers/2503.11509
[22.03.2025 12:40] Get page data from previous paper. URL: https://huggingface.co/papers/2503.15672
[22.03.2025 12:40] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[22.03.2025 12:40] No deleted papers detected.
[22.03.2025 12:40] Downloading and parsing papers (pdf, html). Total: 48.
[22.03.2025 12:40] Downloading and parsing paper https://huggingface.co/papers/2503.13358.
[22.03.2025 12:40] Extra JSON file exists (./assets/json/2503.13358.json), skip PDF parsing.
[22.03.2025 12:40] Paper image links file exists (./assets/img_data/2503.13358.json), skip HTML parsing.
[22.03.2025 12:40] Success.
[22.03.2025 12:40] Downloading and parsing paper https://huggingface.co/papers/2503.16419.
[22.03.2025 12:40] Extra JSON file exists (./assets/json/2503.16419.json), skip PDF parsing.
[22.03.2025 12:40] Paper image links file exists (./assets/img_data/2503.16419.json), skip HTML parsing.
[22.03.2025 12:40] Success.
[22.03.2025 12:40] Downloading and parsing paper https://huggingface.co/papers/2503.16416.
[22.03.2025 12:40] Extra JSON file exists (./assets/json/2503.16416.json), skip PDF parsing.
[22.03.2025 12:40] Paper image links file exists (./assets/img_data/2503.16416.json), skip HTML parsing.
[22.03.2025 12:40] Success.
[22.03.2025 12:40] Downloading and parsing paper https://huggingface.co/papers/2503.16302.
[22.03.2025 12:40] Extra JSON file exists (./assets/json/2503.16302.json), skip PDF parsing.
[22.03.2025 12:40] Paper image links file exists (./assets/img_data/2503.16302.json), skip HTML parsing.
[22.03.2025 12:40] Success.
[22.03.2025 12:40] Downloading and parsing paper https://huggingface.co/papers/2503.16397.
[22.03.2025 12:40] Extra JSON file exists (./assets/json/2503.16397.json), skip PDF parsing.
[22.03.2025 12:40] Paper image links file exists (./assets/img_data/2503.16397.json), skip HTML parsing.
[22.03.2025 12:40] Success.
[22.03.2025 12:40] Downloading and parsing paper https://huggingface.co/papers/2503.15558.
[22.03.2025 12:40] Extra JSON file exists (./assets/json/2503.15558.json), skip PDF parsing.
[22.03.2025 12:40] Paper image links file exists (./assets/img_data/2503.15558.json), skip HTML parsing.
[22.03.2025 12:40] Success.
[22.03.2025 12:40] Downloading and parsing paper https://huggingface.co/papers/2503.16365.
[22.03.2025 12:40] Extra JSON file exists (./assets/json/2503.16365.json), skip PDF parsing.
[22.03.2025 12:40] Paper image links file exists (./assets/img_data/2503.16365.json), skip HTML parsing.
[22.03.2025 12:40] Success.
[22.03.2025 12:40] Downloading and parsing paper https://huggingface.co/papers/2503.14487.
[22.03.2025 12:40] Extra JSON file exists (./assets/json/2503.14487.json), skip PDF parsing.
[22.03.2025 12:40] Paper image links file exists (./assets/img_data/2503.14487.json), skip HTML parsing.
[22.03.2025 12:40] Success.
[22.03.2025 12:40] Downloading and parsing paper https://huggingface.co/papers/2503.16418.
[22.03.2025 12:40] Extra JSON file exists (./assets/json/2503.16418.json), skip PDF parsing.
[22.03.2025 12:40] Paper image links file exists (./assets/img_data/2503.16418.json), skip HTML parsing.
[22.03.2025 12:40] Success.
[22.03.2025 12:40] Downloading and parsing paper https://huggingface.co/papers/2503.13657.
[22.03.2025 12:40] Extra JSON file exists (./assets/json/2503.13657.json), skip PDF parsing.
[22.03.2025 12:40] Paper image links file exists (./assets/img_data/2503.13657.json), skip HTML parsing.
[22.03.2025 12:40] Success.
[22.03.2025 12:40] Downloading and parsing paper https://huggingface.co/papers/2503.16257.
[22.03.2025 12:40] Extra JSON file exists (./assets/json/2503.16257.json), skip PDF parsing.
[22.03.2025 12:40] Paper image links file exists (./assets/img_data/2503.16257.json), skip HTML parsing.
[22.03.2025 12:40] Success.
[22.03.2025 12:40] Downloading and parsing paper https://huggingface.co/papers/2503.16212.
[22.03.2025 12:40] Extra JSON file exists (./assets/json/2503.16212.json), skip PDF parsing.
[22.03.2025 12:40] Paper image links file exists (./assets/img_data/2503.16212.json), skip HTML parsing.
[22.03.2025 12:40] Success.
[22.03.2025 12:40] Downloading and parsing paper https://huggingface.co/papers/2503.16420.
[22.03.2025 12:40] Extra JSON file exists (./assets/json/2503.16420.json), skip PDF parsing.
[22.03.2025 12:40] Paper image links file exists (./assets/img_data/2503.16420.json), skip HTML parsing.
[22.03.2025 12:40] Success.
[22.03.2025 12:40] Downloading and parsing paper https://huggingface.co/papers/2503.15299.
[22.03.2025 12:40] Extra JSON file exists (./assets/json/2503.15299.json), skip PDF parsing.
[22.03.2025 12:40] Paper image links file exists (./assets/img_data/2503.15299.json), skip HTML parsing.
[22.03.2025 12:40] Success.
[22.03.2025 12:40] Downloading and parsing paper https://huggingface.co/papers/2503.16252.
[22.03.2025 12:40] Extra JSON file exists (./assets/json/2503.16252.json), skip PDF parsing.
[22.03.2025 12:40] Paper image links file exists (./assets/img_data/2503.16252.json), skip HTML parsing.
[22.03.2025 12:40] Success.
[22.03.2025 12:40] Downloading and parsing paper https://huggingface.co/papers/2503.16413.
[22.03.2025 12:40] Extra JSON file exists (./assets/json/2503.16413.json), skip PDF parsing.
[22.03.2025 12:40] Paper image links file exists (./assets/img_data/2503.16413.json), skip HTML parsing.
[22.03.2025 12:40] Success.
[22.03.2025 12:40] Downloading and parsing paper https://huggingface.co/papers/2503.16219.
[22.03.2025 12:40] Extra JSON file exists (./assets/json/2503.16219.json), skip PDF parsing.
[22.03.2025 12:40] Paper image links file exists (./assets/img_data/2503.16219.json), skip HTML parsing.
[22.03.2025 12:40] Success.
[22.03.2025 12:40] Downloading and parsing paper https://huggingface.co/papers/2503.10625.
[22.03.2025 12:40] Extra JSON file exists (./assets/json/2503.10625.json), skip PDF parsing.
[22.03.2025 12:40] Paper image links file exists (./assets/img_data/2503.10625.json), skip HTML parsing.
[22.03.2025 12:40] Success.
[22.03.2025 12:40] Downloading and parsing paper https://huggingface.co/papers/2503.16422.
[22.03.2025 12:40] Extra JSON file exists (./assets/json/2503.16422.json), skip PDF parsing.
[22.03.2025 12:40] Paper image links file exists (./assets/img_data/2503.16422.json), skip HTML parsing.
[22.03.2025 12:40] Success.
[22.03.2025 12:40] Downloading and parsing paper https://huggingface.co/papers/2503.16356.
[22.03.2025 12:40] Extra JSON file exists (./assets/json/2503.16356.json), skip PDF parsing.
[22.03.2025 12:40] Paper image links file exists (./assets/img_data/2503.16356.json), skip HTML parsing.
[22.03.2025 12:40] Success.
[22.03.2025 12:40] Downloading and parsing paper https://huggingface.co/papers/2503.16322.
[22.03.2025 12:40] Extra JSON file exists (./assets/json/2503.16322.json), skip PDF parsing.
[22.03.2025 12:40] Paper image links file exists (./assets/img_data/2503.16322.json), skip HTML parsing.
[22.03.2025 12:40] Success.
[22.03.2025 12:40] Downloading and parsing paper https://huggingface.co/papers/2503.16057.
[22.03.2025 12:40] Extra JSON file exists (./assets/json/2503.16057.json), skip PDF parsing.
[22.03.2025 12:40] Paper image links file exists (./assets/img_data/2503.16057.json), skip HTML parsing.
[22.03.2025 12:40] Success.
[22.03.2025 12:40] Downloading and parsing paper https://huggingface.co/papers/2503.16425.
[22.03.2025 12:40] Extra JSON file exists (./assets/json/2503.16425.json), skip PDF parsing.
[22.03.2025 12:40] Paper image links file exists (./assets/img_data/2503.16425.json), skip HTML parsing.
[22.03.2025 12:40] Success.
[22.03.2025 12:40] Downloading and parsing paper https://huggingface.co/papers/2503.16428.
[22.03.2025 12:40] Extra JSON file exists (./assets/json/2503.16428.json), skip PDF parsing.
[22.03.2025 12:40] Paper image links file exists (./assets/img_data/2503.16428.json), skip HTML parsing.
[22.03.2025 12:40] Success.
[22.03.2025 12:40] Downloading and parsing paper https://huggingface.co/papers/2503.16421.
[22.03.2025 12:40] Extra JSON file exists (./assets/json/2503.16421.json), skip PDF parsing.
[22.03.2025 12:40] Paper image links file exists (./assets/img_data/2503.16421.json), skip HTML parsing.
[22.03.2025 12:40] Success.
[22.03.2025 12:40] Downloading and parsing paper https://huggingface.co/papers/2503.16375.
[22.03.2025 12:40] Extra JSON file exists (./assets/json/2503.16375.json), skip PDF parsing.
[22.03.2025 12:40] Paper image links file exists (./assets/img_data/2503.16375.json), skip HTML parsing.
[22.03.2025 12:40] Success.
[22.03.2025 12:40] Downloading and parsing paper https://huggingface.co/papers/2503.16055.
[22.03.2025 12:40] Extra JSON file exists (./assets/json/2503.16055.json), skip PDF parsing.
[22.03.2025 12:40] Paper image links file exists (./assets/img_data/2503.16055.json), skip HTML parsing.
[22.03.2025 12:40] Success.
[22.03.2025 12:40] Downloading and parsing paper https://huggingface.co/papers/2503.15242.
[22.03.2025 12:40] Extra JSON file exists (./assets/json/2503.15242.json), skip PDF parsing.
[22.03.2025 12:40] Paper image links file exists (./assets/img_data/2503.15242.json), skip HTML parsing.
[22.03.2025 12:40] Success.
[22.03.2025 12:40] Downloading and parsing paper https://huggingface.co/papers/2503.13356.
[22.03.2025 12:40] Extra JSON file exists (./assets/json/2503.13356.json), skip PDF parsing.
[22.03.2025 12:40] Paper image links file exists (./assets/img_data/2503.13356.json), skip HTML parsing.
[22.03.2025 12:40] Success.
[22.03.2025 12:40] Downloading and parsing paper https://huggingface.co/papers/2503.16188.
[22.03.2025 12:40] Extra JSON file exists (./assets/json/2503.16188.json), skip PDF parsing.
[22.03.2025 12:40] Paper image links file exists (./assets/img_data/2503.16188.json), skip HTML parsing.
[22.03.2025 12:40] Success.
[22.03.2025 12:40] Downloading and parsing paper https://huggingface.co/papers/2503.15851.
[22.03.2025 12:40] Extra JSON file exists (./assets/json/2503.15851.json), skip PDF parsing.
[22.03.2025 12:40] Paper image links file exists (./assets/img_data/2503.15851.json), skip HTML parsing.
[22.03.2025 12:40] Success.
[22.03.2025 12:40] Downloading and parsing paper https://huggingface.co/papers/2503.15567.
[22.03.2025 12:40] Extra JSON file exists (./assets/json/2503.15567.json), skip PDF parsing.
[22.03.2025 12:40] Paper image links file exists (./assets/img_data/2503.15567.json), skip HTML parsing.
[22.03.2025 12:40] Success.
[22.03.2025 12:40] Downloading and parsing paper https://huggingface.co/papers/2503.16278.
[22.03.2025 12:40] Extra JSON file exists (./assets/json/2503.16278.json), skip PDF parsing.
[22.03.2025 12:40] Paper image links file exists (./assets/img_data/2503.16278.json), skip HTML parsing.
[22.03.2025 12:40] Success.
[22.03.2025 12:40] Downloading and parsing paper https://huggingface.co/papers/2503.14237.
[22.03.2025 12:40] Extra JSON file exists (./assets/json/2503.14237.json), skip PDF parsing.
[22.03.2025 12:40] Paper image links file exists (./assets/img_data/2503.14237.json), skip HTML parsing.
[22.03.2025 12:40] Success.
[22.03.2025 12:40] Downloading and parsing paper https://huggingface.co/papers/2503.16429.
[22.03.2025 12:40] Extra JSON file exists (./assets/json/2503.16429.json), skip PDF parsing.
[22.03.2025 12:40] Paper image links file exists (./assets/img_data/2503.16429.json), skip HTML parsing.
[22.03.2025 12:40] Success.
[22.03.2025 12:40] Downloading and parsing paper https://huggingface.co/papers/2503.12689.
[22.03.2025 12:40] Extra JSON file exists (./assets/json/2503.12689.json), skip PDF parsing.
[22.03.2025 12:40] Paper image links file exists (./assets/img_data/2503.12689.json), skip HTML parsing.
[22.03.2025 12:40] Success.
[22.03.2025 12:40] Downloading and parsing paper https://huggingface.co/papers/2503.09949.
[22.03.2025 12:40] Extra JSON file exists (./assets/json/2503.09949.json), skip PDF parsing.
[22.03.2025 12:40] Paper image links file exists (./assets/img_data/2503.09949.json), skip HTML parsing.
[22.03.2025 12:40] Success.
[22.03.2025 12:40] Downloading and parsing paper https://huggingface.co/papers/2503.16194.
[22.03.2025 12:40] Extra JSON file exists (./assets/json/2503.16194.json), skip PDF parsing.
[22.03.2025 12:40] Paper image links file exists (./assets/img_data/2503.16194.json), skip HTML parsing.
[22.03.2025 12:40] Success.
[22.03.2025 12:40] Downloading and parsing paper https://huggingface.co/papers/2503.16031.
[22.03.2025 12:40] Extra JSON file exists (./assets/json/2503.16031.json), skip PDF parsing.
[22.03.2025 12:40] Paper image links file exists (./assets/img_data/2503.16031.json), skip HTML parsing.
[22.03.2025 12:40] Success.
[22.03.2025 12:40] Downloading and parsing paper https://huggingface.co/papers/2503.15855.
[22.03.2025 12:40] Extra JSON file exists (./assets/json/2503.15855.json), skip PDF parsing.
[22.03.2025 12:40] Paper image links file exists (./assets/img_data/2503.15855.json), skip HTML parsing.
[22.03.2025 12:40] Success.
[22.03.2025 12:40] Downloading and parsing paper https://huggingface.co/papers/2503.15451.
[22.03.2025 12:40] Extra JSON file exists (./assets/json/2503.15451.json), skip PDF parsing.
[22.03.2025 12:40] Paper image links file exists (./assets/img_data/2503.15451.json), skip HTML parsing.
[22.03.2025 12:40] Success.
[22.03.2025 12:40] Downloading and parsing paper https://huggingface.co/papers/2503.13834.
[22.03.2025 12:40] Extra JSON file exists (./assets/json/2503.13834.json), skip PDF parsing.
[22.03.2025 12:40] Paper image links file exists (./assets/img_data/2503.13834.json), skip HTML parsing.
[22.03.2025 12:40] Success.
[22.03.2025 12:40] Downloading and parsing paper https://huggingface.co/papers/2503.07906.
[22.03.2025 12:40] Extra JSON file exists (./assets/json/2503.07906.json), skip PDF parsing.
[22.03.2025 12:40] Paper image links file exists (./assets/img_data/2503.07906.json), skip HTML parsing.
[22.03.2025 12:40] Success.
[22.03.2025 12:40] Downloading and parsing paper https://huggingface.co/papers/2503.16091.
[22.03.2025 12:40] Extra JSON file exists (./assets/json/2503.16091.json), skip PDF parsing.
[22.03.2025 12:40] Paper image links file exists (./assets/img_data/2503.16091.json), skip HTML parsing.
[22.03.2025 12:40] Success.
[22.03.2025 12:40] Downloading and parsing paper https://huggingface.co/papers/2503.14201.
[22.03.2025 12:40] Extra JSON file exists (./assets/json/2503.14201.json), skip PDF parsing.
[22.03.2025 12:40] Paper image links file exists (./assets/img_data/2503.14201.json), skip HTML parsing.
[22.03.2025 12:40] Success.
[22.03.2025 12:40] Downloading and parsing paper https://huggingface.co/papers/2503.13891.
[22.03.2025 12:40] Extra JSON file exists (./assets/json/2503.13891.json), skip PDF parsing.
[22.03.2025 12:40] Paper image links file exists (./assets/img_data/2503.13891.json), skip HTML parsing.
[22.03.2025 12:40] Success.
[22.03.2025 12:40] Downloading and parsing paper https://huggingface.co/papers/2503.11509.
[22.03.2025 12:40] Extra JSON file exists (./assets/json/2503.11509.json), skip PDF parsing.
[22.03.2025 12:40] Paper image links file exists (./assets/img_data/2503.11509.json), skip HTML parsing.
[22.03.2025 12:40] Success.
[22.03.2025 12:40] Downloading and parsing paper https://huggingface.co/papers/2503.15672.
[22.03.2025 12:40] Extra JSON file exists (./assets/json/2503.15672.json), skip PDF parsing.
[22.03.2025 12:40] Paper image links file exists (./assets/img_data/2503.15672.json), skip HTML parsing.
[22.03.2025 12:40] Success.
[22.03.2025 12:40] Enriching papers with extra data.
[22.03.2025 12:40] ********************************************************************************
[22.03.2025 12:40] Abstract 0. Diffusion models for super-resolution (SR) produce high-quality visual results but require expensive computational costs. Despite the development of several methods to accelerate diffusion-based SR models, some (e.g., SinSR) fail to produce realistic perceptual details, while others (e.g., OSEDiff) ...
[22.03.2025 12:40] ********************************************************************************
[22.03.2025 12:40] Abstract 1. Large Language Models (LLMs) have demonstrated remarkable capabilities in complex tasks. Recent advancements in Large Reasoning Models (LRMs), such as OpenAI o1 and DeepSeek-R1, have further improved performance in System-2 reasoning domains like mathematics and programming by harnessing supervised ...
[22.03.2025 12:40] ********************************************************************************
[22.03.2025 12:40] Abstract 2. The emergence of LLM-based agents represents a paradigm shift in AI, enabling autonomous systems to plan, reason, use tools, and maintain memory while interacting with dynamic environments. This paper provides the first comprehensive survey of evaluation methodologies for these increasingly capable ...
[22.03.2025 12:40] ********************************************************************************
[22.03.2025 12:40] Abstract 3. 3D shape generation has greatly flourished through the development of so-called "native" 3D diffusion, particularly through the Vecset Diffusion Model (VDM). While recent advancements have shown promising results in generating high-resolution 3D shapes, VDM still struggles with high-speed generation...
[22.03.2025 12:40] ********************************************************************************
[22.03.2025 12:40] Abstract 4. We present SwD, a scale-wise distillation framework for diffusion models (DMs), which effectively employs next-scale prediction ideas for diffusion-based few-step generators. In more detail, SwD is inspired by the recent insights relating diffusion processes to the implicit spectral autoregression. ...
[22.03.2025 12:40] ********************************************************************************
[22.03.2025 12:40] Abstract 5. Physical AI systems need to perceive, understand, and perform complex actions in the physical world. In this paper, we present the Cosmos-Reason1 models that can understand the physical world and generate appropriate embodied decisions (e.g., next step action) in natural language through long chain-...
[22.03.2025 12:40] ********************************************************************************
[22.03.2025 12:40] Abstract 6. Recently, action-based decision-making in open-world environments has gained significant attention. Visual Language Action (VLA) models, pretrained on large-scale web datasets, have shown promise in decision-making tasks. However, previous work has primarily focused on action post-training, often ne...
[22.03.2025 12:40] ********************************************************************************
[22.03.2025 12:40] Abstract 7. Diffusion models have demonstrated remarkable success in various image generation tasks, but their performance is often limited by the uniform processing of inputs across varying conditions and noise levels. To address this limitation, we propose a novel approach that leverages the inherent heteroge...
[22.03.2025 12:40] ********************************************************************************
[22.03.2025 12:40] Abstract 8. Achieving flexible and high-fidelity identity-preserved image generation remains formidable, particularly with advanced Diffusion Transformers (DiTs) like FLUX. We introduce InfiniteYou (InfU), one of the earliest robust frameworks leveraging DiTs for this task. InfU addresses significant issues of ...
[22.03.2025 12:40] ********************************************************************************
[22.03.2025 12:40] Abstract 9. Despite growing enthusiasm for Multi-Agent Systems (MAS), where multiple LLM agents collaborate to accomplish tasks, their performance gains across popular benchmarks remain minimal compared to single-agent frameworks. This gap highlights the need to analyze the challenges hindering MAS effectivenes...
[22.03.2025 12:40] ********************************************************************************
[22.03.2025 12:40] Abstract 10. Video large language models (VideoLLMs) have demonstrated the capability to process longer video inputs and enable complex reasoning and analysis. However, due to the thousands of visual tokens from the video frames, key-value (KV) cache can significantly increase memory requirements, becoming a bot...
[22.03.2025 12:40] ********************************************************************************
[22.03.2025 12:40] Abstract 11. Large Language Models (LLMs) have shown impressive progress in mathematical reasoning. While data augmentation is promising to enhance mathematical problem-solving ability, current approaches are predominantly limited to instance-level modifications-such as rephrasing or generating syntactic variati...
[22.03.2025 12:40] ********************************************************************************
[22.03.2025 12:40] Abstract 12. We address the challenge of generating 3D worlds from textual descriptions. We propose SynCity, a training- and optimization-free approach, which leverages the geometric precision of pre-trained 3D generative models and the artistic versatility of 2D image generators to create large, high-quality 3D...
[22.03.2025 12:40] ********************************************************************************
[22.03.2025 12:40] Abstract 13. This work presents a framework for assessing whether large language models (LLMs) encode more factual knowledge in their parameters than what they express in their outputs. While a few studies hint at this possibility, none has clearly defined or demonstrated this phenomenon. We first propose a form...
[22.03.2025 12:40] ********************************************************************************
[22.03.2025 12:40] Abstract 14. Reasoning large language models are rapidly evolving across various domains. However, their capabilities in handling complex financial tasks still require in-depth exploration. In this paper, we introduce Fin-R1, a reasoning large language model specifically designed for the financial sector. Fin-R1...
[22.03.2025 12:40] ********************************************************************************
[22.03.2025 12:40] Abstract 15. We present 3D Spatial MultiModal Memory (M3), a multimodal memory system designed to retain information about medium-sized static scenes through video sources for visual perception. By integrating 3D Gaussian Splatting techniques with foundation models, M3 builds a multimodal memory capable of rende...
[22.03.2025 12:40] ********************************************************************************
[22.03.2025 12:40] Abstract 16. Enhancing the reasoning capabilities of large language models (LLMs) typically relies on massive computational resources and extensive datasets, limiting accessibility for resource-constrained settings. Our study investigates the potential of reinforcement learning (RL) to improve reasoning in small...
[22.03.2025 12:40] ********************************************************************************
[22.03.2025 12:40] Abstract 17. Animatable 3D human reconstruction from a single image is a challenging problem due to the ambiguity in decoupling geometry, appearance, and deformation. Recent advances in 3D human reconstruction mainly focus on static human modeling, and the reliance of using synthetic 3D scans for training limits...
[22.03.2025 12:40] ********************************************************************************
[22.03.2025 12:40] Abstract 18. 4D Gaussian Splatting (4DGS) has recently gained considerable attention as a method for reconstructing dynamic scenes. Despite achieving superior quality, 4DGS typically requires substantial storage and suffers from slow rendering speed. In this work, we delve into these issues and identify two key ...
[22.03.2025 12:40] ********************************************************************************
[22.03.2025 12:40] Abstract 19. Knowledge Editing (KE) enables the modification of outdated or incorrect information in large language models (LLMs). While existing KE methods can update isolated facts, they struggle to generalize these updates to multi-hop reasoning tasks that depend on the modified knowledge. Through an analysis...
[22.03.2025 12:40] ********************************************************************************
[22.03.2025 12:40] Abstract 20. Text-to-image diffusion models have achieved remarkable progress in recent years. However, training models for high-resolution image generation remains challenging, particularly when training data and computational resources are limited. In this paper, we explore this practical problem from two key ...
[22.03.2025 12:40] ********************************************************************************
[22.03.2025 12:40] Abstract 21. Diffusion models have emerged as mainstream framework in visual generation. Building upon this success, the integration of Mixture of Experts (MoE) methods has shown promise in enhancing model scalability and performance. In this paper, we introduce Race-DiT, a novel MoE model for diffusion transfor...
[22.03.2025 12:40] ********************************************************************************
[22.03.2025 12:40] Abstract 22. This paper proposes a fundamentally new paradigm for image generation through set-based tokenization and distribution modeling. Unlike conventional methods that serialize images into fixed-position latent codes with a uniform compression ratio, we introduce an unordered token set representation to d...
[22.03.2025 12:40] ********************************************************************************
[22.03.2025 12:40] Abstract 23. Long-Context Transformer Models (LCTMs) are vital for real-world applications but suffer high computational costs due to attention's quadratic complexity. Block-sparse attention mitigates this by focusing computation on critical regions, yet existing methods struggle with balancing accuracy and effi...
[22.03.2025 12:40] ********************************************************************************
[22.03.2025 12:40] Abstract 24. Recent advances in video generation have led to remarkable improvements in visual quality and temporal coherence. Upon this, trajectory-controllable video generation has emerged to enable precise object motion control through explicitly defined spatial paths. However, existing methods struggle with ...
[22.03.2025 12:40] ********************************************************************************
[22.03.2025 12:40] Abstract 25. In this paper, we explore the task of generating expansive outdoor scenes, ranging from castles to high-rises. Unlike indoor scene generation, which has been a primary focus of prior work, outdoor scene generation presents unique challenges, including wide variations in scene heights and the need fo...
[22.03.2025 12:40] ********************************************************************************
[22.03.2025 12:40] Abstract 26. The complex nature of medical image segmentation calls for models that are specifically designed to capture detailed, domain-specific features. Large foundation models offer considerable flexibility, yet the cost of fine-tuning these models remains a significant barrier. Parameter-Efficient Fine-Tun...
[22.03.2025 12:40] ********************************************************************************
[22.03.2025 12:40] Abstract 27. We introduce BigO(Bench), a novel coding benchmark designed to evaluate the capabilities of generative language models in understanding and generating code with specified time and space complexities. This benchmark addresses the gap in current evaluations that often overlook the ability of models to...
[22.03.2025 12:40] ********************************************************************************
[22.03.2025 12:40] Abstract 28. We present PORTAL, a novel framework for developing artificial intelligence agents capable of playing thousands of 3D video games through language-guided policy generation. By transforming decision-making problems into language modeling tasks, our approach leverages large language models (LLMs) to g...
[22.03.2025 12:40] ********************************************************************************
[22.03.2025 12:40] Abstract 29. Classification is a core task in machine learning. Recent research has shown that although Multimodal Large Language Models (MLLMs) are initially poor at image classification, fine-tuning them with an adequate amount of data can significantly enhance their performance, making them comparable to SOTA...
[22.03.2025 12:40] ********************************************************************************
[22.03.2025 12:40] Abstract 30. Animatable head avatar generation typically requires extensive data for training. To reduce the data requirements, a natural solution is to leverage existing data-free static avatar generation methods, such as pre-trained diffusion models with score distillation sampling (SDS), which align avatars w...
[22.03.2025 12:40] ********************************************************************************
[22.03.2025 12:40] Abstract 31. 3D molecule generation is crucial for drug discovery and material science, requiring models to process complex multi-modalities, including atom types, chemical bonds, and 3D coordinates. A key challenge is integrating these modalities of different shapes while maintaining SE(3) equivariance for 3D c...
[22.03.2025 12:40] ********************************************************************************
[22.03.2025 12:40] Abstract 32. Recent advancements in large language models and their multi-modal extensions have demonstrated the effectiveness of unifying generation and understanding through autoregressive next-token prediction. However, despite the critical role of 3D structural generation and understanding ({3D GU}) in AI fo...
[22.03.2025 12:40] ********************************************************************************
[22.03.2025 12:40] Abstract 33. Popular video training methods mainly operate on a fixed number of tokens sampled from a predetermined spatiotemporal grid, resulting in sub-optimal accuracy-computation trade-offs due to inherent video redundancy. They also lack adaptability to varying computational budgets for downstream tasks, hi...
[22.03.2025 12:40] ********************************************************************************
[22.03.2025 12:40] Abstract 34. In this paper, we question whether we have a reliable self-supervised point cloud model that can be used for diverse 3D tasks via simple linear probing, even with limited data and minimal computation. We find that existing 3D self-supervised learning approaches fall short when evaluated on represent...
[22.03.2025 12:40] ********************************************************************************
[22.03.2025 12:40] Abstract 35. Video identity customization seeks to produce high-fidelity videos that maintain consistent identity and exhibit significant dynamics based on users' reference images. However, existing approaches face two key challenges: identity degradation over extended video length and reduced dynamics during tr...
[22.03.2025 12:40] ********************************************************************************
[22.03.2025 12:40] Abstract 36. With the rapid growth of video generative models (VGMs), it is essential to develop reliable and comprehensive automatic metrics for AI-generated videos (AIGVs). Existing methods either use off-the-shelf models optimized for other tasks or rely on human assessment data to train specialized evaluator...
[22.03.2025 12:40] ********************************************************************************
[22.03.2025 12:40] Abstract 37. Autoregressive models have shown remarkable success in image generation by adapting sequential prediction techniques from language modeling. However, applying these approaches to images requires discretizing continuous pixel data through vector quantization methods like VQ-VAE. To alleviate the quan...
[22.03.2025 12:40] ********************************************************************************
[22.03.2025 12:40] Abstract 38. This paper presents the Deceptive Humor Dataset (DHD), a novel resource for studying humor derived from fabricated claims and misinformation. In an era of rampant misinformation, understanding how humor intertwines with deception is essential. DHD consists of humor-infused comments generated from fa...
[22.03.2025 12:40] ********************************************************************************
[22.03.2025 12:40] Abstract 39. We propose VideoRFSplat, a direct text-to-3D model leveraging a video generation model to generate realistic 3D Gaussian Splatting (3DGS) for unbounded real-world scenes. To generate diverse camera poses and unbounded spatial extent of real-world scenes, while ensuring generalization to arbitrary te...
[22.03.2025 12:40] ********************************************************************************
[22.03.2025 12:40] Abstract 40. This paper addresses the challenge of text-conditioned streaming motion generation, which requires us to predict the next-step human pose based on variable-length historical motions and incoming texts. Existing methods struggle to achieve streaming motion generation, e.g., diffusion models are const...
[22.03.2025 12:40] ********************************************************************************
[22.03.2025 12:40] Abstract 41. Vision-language (VL) models have demonstrated strong performance across various tasks. However, these models often rely on a specific modality for predictions, leading to "dominant modality bias.'' This bias significantly hurts performance, especially when one modality is impaired. In this study, we...
[22.03.2025 12:40] ********************************************************************************
[22.03.2025 12:40] Abstract 42. Image captioning has long been a pivotal task in visual understanding, with recent advancements in vision-language models (VLMs) significantly enhancing the ability to generate detailed image captions. However, the evaluation of detailed image captioning remains underexplored due to outdated evaluat...
[22.03.2025 12:40] ********************************************************************************
[22.03.2025 12:40] Abstract 43. Adherence to prescribed treatments is crucial for individuals with chronic conditions to avoid costly or adverse health outcomes. For certain patient groups, intensive lifestyle interventions are vital for enhancing medication adherence. Accurate forecasting of treatment adherence can open pathways ...
[22.03.2025 12:40] ********************************************************************************
[22.03.2025 12:40] Abstract 44. Deep learning (DL)-based code completion tools have transformed software development by enabling advanced code generation. These tools leverage models trained on vast amounts of code from numerous repositories, capturing general coding patterns. However, the impact of fine-tuning these models for sp...
[22.03.2025 12:40] ********************************************************************************
[22.03.2025 12:40] Abstract 45. Large Vision-Language Models (LVLMs) have shown promising performance in vision-language understanding and reasoning tasks. However, their visual understanding behaviors remain underexplored. A fundamental question arises: to what extent do LVLMs rely on visual input, and which image regions contrib...
[22.03.2025 12:40] ********************************************************************************
[22.03.2025 12:40] Abstract 46. With the rise of generative AI, synthesizing figures from text captions becomes a compelling application. However, achieving high geometric precision and editability requires representing figures as graphics programs in languages like TikZ, and aligned training data (i.e., graphics programs with cap...
[22.03.2025 12:40] ********************************************************************************
[22.03.2025 12:40] Abstract 47. Self-supervised pre-training based on next-token prediction has enabled large language models to capture the underlying structure of text, and has led to unprecedented performance on a large array of tasks when applied at scale. Similarly, autonomous driving generates vast amounts of spatiotemporal ...
[22.03.2025 12:40] Read previous papers.
[22.03.2025 12:40] Generating reviews via LLM API.
[22.03.2025 12:40] Using data from previous issue: {"categories": ["#synthetic", "#hallucinations", "#training", "#diffusion", "#cv", "#optimization"], "emoji": "üîç", "ru": {"title": "RSD: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è —Å–≤–µ—Ä—Ö—Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –¥–ª—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Å–≤–µ—Ä—Ö—Ä
[22.03.2025 12:40] Using data from previous issue: {"categories": ["#small_models", "#optimization", "#reasoning", "#rl", "#dataset", "#survey", "#training", "#benchmark"], "emoji": "üß†", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –≤ LLM: –ø—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ –∏–∑–±—ã—Ç–æ—á–Ω–æ—Å—Ç–∏ –¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –æ–±–∑–æ—Ä –º–µ—Ç–æ–¥–æ–≤ –ø
[22.03.2025 12:40] Using data from previous issue: {"categories": ["#benchmark", "#survey", "#reasoning", "#agents"], "emoji": "ü§ñ", "ru": {"title": "–ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –º–µ—Ç–æ–¥–æ–≤ –æ—Ü–µ–Ω–∫–∏ AI-–∞–≥–µ–Ω—Ç–æ–≤ –Ω–æ–≤–æ–≥–æ –ø–æ–∫–æ–ª–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –ø–µ—Ä–≤—ã–π –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –æ–±–∑–æ—Ä –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏–π –æ—Ü–µ–Ω–∫–∏ –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –ê–≤—Ç–æ—Ä—ã –∞–Ω
[22.03.2025 12:40] Using data from previous issue: {"categories": ["#open_source", "#3d", "#diffusion", "#optimization", "#inference"], "emoji": "üöÄ", "ru": {"title": "–ú–æ–ª–Ω–∏–µ–Ω–æ—Å–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è 3D-—Ñ–æ—Ä–º —Å FlashVDM", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç FlashVDM - —Å–∏—Å—Ç–µ–º—É –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ 3D-—Ñ–æ—Ä–º —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π (VDM). –ê–≤—Ç–æ—Ä
[22.03.2025 12:40] Using data from previous issue: {"categories": ["#inference", "#training", "#diffusion", "#cv", "#optimization"], "emoji": "üîç", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ–º", "desc": "SwD - —ç—Ç–æ –Ω–æ–≤–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–ª—è –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è –∏–¥–µ—é –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å–ª–µ–¥—É—é—â–µ–≥–æ –º–∞—Å—à—Ç–∞
[22.03.2025 12:40] Using data from previous issue: {"categories": ["#training", "#rl", "#benchmark", "#open_source", "#agents", "#reasoning", "#multimodal"], "emoji": "ü§ñ", "ru": {"title": "–§–∏–∑–∏—á–µ—Å–∫–∏–π –ò–ò: –æ—Ç –ø–æ–Ω–∏–º–∞–Ω–∏—è –º–∏—Ä–∞ –∫ –≤–æ–ø–ª–æ—â–µ–Ω–Ω—ã–º –¥–µ–π—Å—Ç–≤–∏—è–º", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã –º–æ–¥–µ–ª–∏ Cosmos-Reason1, —Å–ø–æ—Å–æ–±–Ω—ã–µ –ø–æ–Ω–∏–º–∞—Ç—å —Ñ–∏–∑–∏—á–µ—Å–∫–∏–π –º–∏—Ä –∏ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å 
[22.03.2025 12:40] Using data from previous issue: {"categories": ["#open_source", "#games", "#cv", "#agents", "#training"], "emoji": "ü§ñ", "ru": {"title": "–£–ª—É—á—à–µ–Ω–∏–µ VLA –º–æ–¥–µ–ª–µ–π –¥–ª—è –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π –≤ –æ—Ç–∫—Ä—ã—Ç—ã—Ö —Å—Ä–µ–¥–∞—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –º–æ–¥–µ–ª–µ–π –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ —è–∑—ã–∫–∞ –∏ –¥–µ–π—Å—Ç–≤–∏–π (VLA) –¥–ª—è –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π –≤ –æ—Ç–∫—Ä—ã—Ç—ã—Ö —Å—Ä–µ–¥
[22.03.2025 12:40] Using data from previous issue: {"categories": ["#benchmark", "#cv", "#diffusion", "#architecture"], "emoji": "üñºÔ∏è", "ru": {"title": "DiffMoE: –£–º–Ω–∞—è —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "DiffMoE - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—É–ª –≥–ª–æ–±–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤
[22.03.2025 12:40] Using data from previous issue: {"categories": ["#cv", "#diffusion", "#architecture", "#synthetic", "#training"], "emoji": "üñºÔ∏è", "ru": {"title": "InfiniteYou: –ù–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç–∏", "desc": "InfiniteYou (InfU) - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç–∏, –∏—Å–ø–æ–ª—å
[22.03.2025 12:40] Using data from previous issue: {"categories": ["#alignment", "#benchmark", "#dataset", "#agi", "#agents", "#open_source"], "emoji": "ü§ñ", "ru": {"title": "–†–∞—Å–∫—Ä—ã–≤–∞—è –ø—Ä–æ–±–ª–µ–º—ã –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º: –ø—É—Ç—å –∫ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–º—É —Å–æ—Ç—Ä—É–¥–Ω–∏—á–µ—Å—Ç–≤—É –ò–ò", "desc": "–≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º (–ú–ê–°) —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –±–æ–ª
[22.03.2025 12:40] Using data from previous issue: {"categories": ["#video", "#long_context", "#inference", "#optimization"], "emoji": "üé•", "ru": {"title": "VidKV: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Å–∂–∞—Ç–∏–µ –∫—ç—à–∞ –¥–ª—è –≤–∏–¥–µ–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç VidKV - –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ –∫—ç—à–∞ –∫–ª—é—á-–∑–Ω–∞—á–µ–Ω–∏–µ (KV) –¥–ª—è –≤–∏–¥–µ–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (VideoLLMs). –ê–≤—Ç–æ—Ä
[22.03.2025 12:40] Using data from previous issue: {"categories": ["#training", "#synthetic", "#open_source", "#math", "#reasoning", "#dataset", "#benchmark", "#data"], "emoji": "üßÆ", "ru": {"title": "–°–ª–∏—è–Ω–∏–µ –∑–∞–¥–∞—á –¥–ª—è –ø—Ä–æ–∫–∞—á–∫–∏ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –ò–ò", "desc": "MathFusion - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è —É –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ
[22.03.2025 12:40] Using data from previous issue: {"categories": ["#synthetic", "#3d", "#games"], "emoji": "üèôÔ∏è", "ru": {"title": "SynCity: –°–æ–∑–¥–∞–Ω–∏–µ –±–µ—Å–∫–æ–Ω–µ—á–Ω—ã—Ö 3D-–º–∏—Ä–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç SynCity - –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç—Ä–µ—Ö–º–µ—Ä–Ω—ã—Ö –º–∏—Ä–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –æ–ø–∏—Å–∞–Ω–∏–π. –ú–µ—Ç–æ–¥ –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫—É—é —Ç–æ—á–Ω–æ—Å—Ç—å –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö 3D-–≥–µ–Ω–µ—Ä–∞
[22.03.2025 12:40] Using data from previous issue: {"categories": ["#hallucinations", "#multimodal", "#data", "#benchmark", "#interpretability"], "emoji": "üß†", "ru": {"title": "–°–∫—Ä—ã—Ç—ã–µ –∑–Ω–∞–Ω–∏—è –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö: –±–æ–ª—å—à–µ, —á–µ–º –∫–∞–∂–µ—Ç—Å—è –Ω–∞ –ø–µ—Ä–≤—ã–π –≤–∑–≥–ª—è–¥", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—é –æ—Ü–µ–Ω–∫–∏ —Å–∫—Ä—ã—Ç—ã—Ö –∑–Ω–∞–Ω–∏–π –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM). –ê–≤—Ç
[22.03.2025 12:40] Using data from previous issue: {"categories": ["#training", "#rl", "#architecture", "#reasoning", "#dataset", "#healthcare"], "emoji": "üíπ", "ru": {"title": "Fin-R1: –ú–æ—â–Ω–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç Fin-R1 - —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å, —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –¥–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–≥–æ —Å–µ–∫—Ç–æ—Ä
[22.03.2025 12:40] Using data from previous issue: {"categories": ["#robotics", "#architecture", "#optimization", "#games", "#3d", "#multimodal"], "emoji": "üß†", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è 3D –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –ø–∞–º—è—Ç—å –¥–ª—è –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç 3D –ü—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—É—é –ú—É–ª—å—Ç–∏–ú–æ–¥–∞–ª—å–Ω—É—é –ü–∞–º—è—Ç—å (M3) - —Å–∏—Å—Ç–µ–º—É, –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–Ω—É—é –¥–ª
[22.03.2025 12:40] Using data from previous issue: {"categories": ["#rl", "#dataset", "#small_models", "#reasoning", "#open_source", "#training", "#optimization"], "emoji": "üß†", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –º–∞–ª—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö —Å –ø–æ–º–æ—â—å—é RL", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω
[22.03.2025 12:40] Using data from previous issue: {"categories": ["#multimodal", "#3d", "#architecture"], "emoji": "üßë‚Äçü¶∞", "ru": {"title": "–†–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –∞–Ω–∏–º–∏—Ä—É–µ–º—ã—Ö 3D-–∞–≤–∞—Ç–∞—Ä–æ–≤ —á–µ–ª–æ–≤–µ–∫–∞ –ø–æ –æ–¥–Ω–æ–º—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é —Å –ø–æ–º–æ—â—å—é —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç LHM (Large Animatable Human Reconstruction Model) - –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å –¥–ª—è —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –∞–Ω–∏–º
[22.03.2025 12:40] Using data from previous issue: {"categories": ["#3d", "#inference", "#optimization"], "emoji": "üöÄ", "ru": {"title": "4DGS-1K: –°–≤–µ—Ä—Ö–±—ã—Å—Ç—Ä–∞—è —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö —Å—Ü–µ–Ω", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ 4DGS-1K –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ 4D –≥–∞—É—Å—Å–æ–≤–æ–≥–æ —Å–ø–ª–∞—Ç—Ç–∏–Ω–≥–∞ –≤ —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö —Å—Ü–µ–Ω. –ê–≤—Ç–æ—Ä—ã —Ä–µ—à–∞—é—Ç –ø—Ä–æ–±–ª–µ–º—ã –∏–∑–±—ã—Ç–æ—á–Ω–æ–≥–æ —Ö—Ä–∞–Ω
[22.03.2025 12:40] Using data from previous issue: {"categories": ["#training", "#open_source", "#reasoning", "#dataset", "#data"], "emoji": "üß†", "ru": {"title": "CaKE: —É–ª—É—á—à–µ–Ω–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –ò–ò —á–µ—Ä–µ–∑ —É–º–Ω–æ–µ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–Ω–∞–Ω–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∑–Ω–∞–Ω–∏–π –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º CaKE. –≠—Ç–æ—Ç –º–µ—Ç–æ
[22.03.2025 12:40] Using data from previous issue: {"categories": ["#optimization", "#data", "#diffusion", "#cv", "#synthetic", "#training", "#benchmark"], "emoji": "üñºÔ∏è", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è —Å–≤–µ—Ä—Ö–≤—ã—Å–æ–∫–æ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–∞–±–æ—Ä —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º URAE –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –¥–∏—Ñ—Ñ—É–∑–∏–æ
[22.03.2025 12:40] Using data from previous issue: {"categories": ["#architecture", "#diffusion", "#cv", "#training", "#optimization"], "emoji": "üèéÔ∏è", "ru": {"title": "Race-DiT: –ì–æ–Ω–∫–∏ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Race-DiT - –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å Mixture of Experts (MoE) –¥–ª—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ —Å –≥–∏–±–∫–æ–π —Å—Ç
[22.03.2025 12:40] Using data from previous issue: {"categories": ["#cv", "#open_source", "#diffusion", "#video"], "emoji": "üß©", "ru": {"title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: –æ—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –∫ –º–Ω–æ–∂–µ—Å—Ç–≤–∞–º", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –ø–∞—Ä–∞–¥–∏–≥–º—É –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∏—Å–ø–æ–ª—å–∑—É—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–Ω–æ–∂–µ—Å—Ç–≤ –∏ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–∞—Å
[22.03.2025 12:40] Using data from previous issue: {"categories": ["#inference", "#architecture", "#long_context", "#optimization", "#video", "#benchmark"], "emoji": "üöÄ", "ru": {"title": "XAttention: –£—Å–∫–æ—Ä–µ–Ω–∏–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ —Ç–æ—á–Ω–æ—Å—Ç–∏", "desc": "XAttention - —ç—Ç–æ –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª–∏–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤ –≤ –º–æ–¥–µ–ª—è—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ
[22.03.2025 12:40] Using data from previous issue: {"categories": ["#open_source", "#optimization", "#video", "#games", "#dataset", "#benchmark"], "emoji": "üé•", "ru": {"title": "MagicMotion: —Ç–æ—á–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–≤–∏–∂–µ–Ω–∏–µ–º –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º–æ–º –≤–∏–¥–µ–æ", "desc": "MagicMotion - –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –∫–æ–Ω—Ç—Ä–æ–ª–µ–º —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –¥–≤–∏–∂–µ–Ω–∏—è 
[22.03.2025 12:40] Using data from previous issue: {"categories": ["#dataset", "#diffusion", "#synthetic", "#3d"], "emoji": "üèôÔ∏è", "ru": {"title": "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –º–∞—Å—à—Ç–∞–±–Ω—ã—Ö –æ—Ç–∫—Ä—ã—Ç—ã—Ö —Å—Ü–µ–Ω: –æ—Ç –∑–∞–º–∫–æ–≤ –¥–æ –Ω–µ–±–æ—Å–∫—Ä–µ–±–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –æ–±—à–∏—Ä–Ω—ã—Ö –æ—Ç–∫—Ä—ã—Ç—ã—Ö —Å—Ü–µ–Ω, –≤–∫–ª—é—á–∞—è –∑–∞–º–∫–∏ –∏ –Ω–µ–±–æ—Å–∫—Ä–µ–±—ã. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –ø–æ–¥—Ö–æ–¥, –∫–æ–¥–∏—Ä—É—é—â–∏–π —Ñ—Ä–∞–≥
[22.03.2025 12:40] Using data from previous issue: {"categories": ["#open_source", "#low_resource", "#healthcare", "#training", "#optimization"], "emoji": "üß†", "ru": {"title": "SALT: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –º–æ–¥–µ–ª–µ–π –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ SALT –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω
[22.03.2025 12:40] Using data from previous issue: {"categories": ["#synthetic", "#benchmark", "#dataset", "#plp", "#reasoning"], "emoji": "‚è±Ô∏è", "ru": {"title": "BigO(Bench): –û—Ü–µ–Ω–∫–∞ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∞–ª–≥–æ—Ä–∏—Ç–º–∏—á–µ—Å–∫–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏", "desc": "BigO(Bench) - —ç—Ç–æ –Ω–æ–≤—ã–π benchmark –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø–æ–Ω–∏–º–∞—Ç—å –∏ —Å–æ–∑–¥–∞
[22.03.2025 12:40] Using data from previous issue: {"categories": ["#rl", "#interpretability", "#video", "#3d", "#games", "#reasoning", "#agi", "#agents"], "emoji": "üéÆ", "ru": {"title": "–Ø–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –ø–æ–∫–æ—Ä—è—é—Ç –º–∏—Ä 3D-–∏–≥—Ä", "desc": "PORTAL - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤, —Å–ø–æ—Å–æ–±–Ω—ã—Ö –∏–≥—Ä–∞—Ç—å –≤ —Ç—ã—Å—è—á–∏ 3D-–≤–∏–¥–µ–æ–∏–≥—Ä —Å –ø–æ–º–æ—â—å—é –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ–ª–∏—Ç–∏–∫
[22.03.2025 12:40] Using data from previous issue: {"categories": ["#rl", "#transfer_learning", "#optimization", "#cv", "#multimodal", "#training"], "emoji": "ü§ñ", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç –Ω–æ–≤—ã–µ –≥–æ—Ä–∏–∑–æ–Ω—Ç—ã –≤ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –ò–ò", "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —É
[22.03.2025 12:40] Using data from previous issue: {"categories": ["#data", "#diffusion", "#dataset", "#synthetic", "#video", "#open_source"], "emoji": "üé≠", "ru": {"title": "Zero-1-to-A: –°–æ–∑–¥–∞–Ω–∏–µ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö 3D-–∞–≤–∞—Ç–∞—Ä–æ–≤ –±–µ–∑ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ Zero-1-to-A –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∞–Ω–∏–º–∏—Ä—É–µ–º—ã—Ö 3D-–∞–≤–∞—Ç–∞—Ä–æ–≤ –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ä–µ–∞–ª—å
[22.03.2025 12:40] Using data from previous issue: {"categories": ["#multimodal", "#diffusion", "#benchmark", "#3d", "#optimization", "#dataset"], "emoji": "üß™", "ru": {"title": "–ï–¥–∏–Ω–æ–µ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ 3D –º–æ–ª–µ–∫—É–ª", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç—Ä–µ—Ö–º–µ—Ä–Ω—ã—Ö –º–æ–ª–µ–∫—É–ª –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –ª–µ–∫–∞—Ä—Å—Ç–≤ –∏ –º–∞—Ç–µ—Ä
[22.03.2025 12:40] Using data from previous issue: {"categories": ["#multimodal", "#science", "#training", "#architecture", "#3d", "#optimization", "#inference"], "emoji": "üß¨", "ru": {"title": "Uni-3DAR: –ï–¥–∏–Ω–∞—è –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è 3D –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Uni-3DAR - —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è –∑–∞–¥–∞—á 3D –≥–µ–Ω–µ—Ä–∞—Ü
[22.03.2025 12:40] Using data from previous issue: {"categories": ["#benchmark", "#video", "#open_source", "#training", "#optimization"], "emoji": "üé•", "ru": {"title": "Flux: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –≤–∏–¥–µ–æ–º–æ–¥–µ–ª–µ–π —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π —Ç–æ–∫–µ–Ω–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è –≤–∏–¥–µ–æ–º–æ–¥–µ–ª–µ–π –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Flux. –û–Ω –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç –≤—ã–±–æ—Ä–∫—É —Ç–æ–∫–µ–Ω–æ–≤ –∏–∑
[22.03.2025 12:40] Using data from previous issue: {"categories": ["#3d", "#optimization", "#transfer_learning"], "emoji": "üåê", "ru": {"title": "Sonata: –Ω–æ–≤–∞—è —ç—Ä–∞ —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏—è –¥–ª—è 3D –æ–±–ª–∞–∫–æ–≤ —Ç–æ—á–µ–∫", "desc": "–í —Å—Ç–∞—Ç—å–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ —Å–æ–∑–¥–∞–Ω–∏—è –Ω–∞–¥–µ–∂–Ω–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –æ–±–ª–∞–∫–∞–º–∏ —Ç–æ—á–µ–∫ –≤ 3D, –∫–æ—Ç–æ—Ä–∞—è –º–æ–∂–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á —Å
[22.03.2025 12:40] Using data from previous issue: {"categories": ["#video", "#multimodal"], "emoji": "üé≠", "ru": {"title": "MagicID: –ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤–∏–¥–µ–æ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç–∏ –∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–π –¥–∏–Ω–∞–º–∏–∫–æ–π", "desc": "MagicID - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≤–∏–¥–µ–æ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç–∏ –∏ –¥–∏–Ω–∞–º–∏–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –û–Ω–∞ 
[22.03.2025 12:40] Using data from previous issue: {"categories": ["#interpretability", "#video", "#games", "#optimization", "#benchmark", "#multimodal"], "emoji": "üé•", "ru": {"title": "MLLM –∫–∞–∫ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –æ—Ü–µ–Ω—â–∏–∫ AI-–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ—Ü–µ–Ω–∫–µ –≤–∏–¥–µ–æ, —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–æ–º (AIGV), 
[22.03.2025 12:40] Using data from previous issue: {"categories": ["#cv", "#training", "#optimization", "#architecture"], "emoji": "üñºÔ∏è", "ru": {"title": "–û—Ç –≥—Ä—É–±–æ–≥–æ –∫ —Ç–æ—á–Ω–æ–º—É: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥ –ø—Ä
[22.03.2025 12:40] Using data from previous issue: {"categories": ["#ethics", "#low_resource", "#benchmark", "#dataset", "#multilingual"], "emoji": "ü§°", "ru": {"title": "–°–º–µ—Ö —Å–∫–≤–æ–∑—å –ª–æ–∂—å: –Ω–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –æ–±–º–∞–Ω—á–∏–≤–æ–≥–æ —é–º–æ—Ä–∞", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö DHD (Deceptive Humor Dataset) –¥–ª—è –∏–∑—É—á–µ–Ω–∏—è —é–º–æ—Ä–∞, –æ—Å–Ω–æ–≤–∞–Ω–Ω–æ–≥–æ –Ω–∞ 
[22.03.2025 12:40] Using data from previous issue: {"categories": ["#architecture", "#video", "#3d"], "emoji": "üé•", "ru": {"title": "–†–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ 3D-—Å—Ü–µ–Ω—ã –∏–∑ —Ç–µ–∫—Å—Ç–∞ –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏", "desc": "VideoRFSplat - —ç—Ç–æ –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö 3D-—Å—Ü–µ–Ω –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –æ–ø–∏—Å–∞–Ω–∏–π. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –º–Ω
[22.03.2025 12:40] Using data from previous issue: {"categories": ["#multimodal", "#architecture", "#games", "#video", "#long_context"], "emoji": "ü§ñ", "ru": {"title": "–ü–ª–∞–≤–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–≤–∏–∂–µ–Ω–∏–π –ø–æ —Ç–µ–∫—Å—Ç—É –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç MotionStreamer - –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–≤–∏–∂–µ–Ω–∏–π —á–µ–ª–æ–≤–µ–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –æ–ø–∏—Å–∞–Ω–∏–π –≤ 
[22.03.2025 12:40] Using data from previous issue: {"categories": ["#interpretability", "#multimodal", "#optimization", "#training"], "emoji": "‚öñÔ∏è", "ru": {"title": "–ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ –ø—Ä–æ–±–ª–µ–º–µ –¥–æ–º–∏–Ω–∏—Ä—É—é—â–µ–π –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–∏ –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã 
[22.03.2025 12:40] Using data from previous issue: {"categories": ["#optimization", "#hallucinations", "#cv", "#benchmark", "#multimodal", "#training"], "emoji": "üñºÔ∏è", "ru": {"title": "–ù–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ—Ü–µ–Ω–∫–µ –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –æ–ø–∏—Å–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ—Ü–µ–Ω–∫–∏ –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –æ–ø–∏—Å–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º DCScore. –≠—Ç–æ—Ç –º–µ
[22.03.2025 12:40] Using data from previous issue: {"categories": ["#science", "#architecture", "#healthcare", "#optimization", "#open_source", "#training"], "emoji": "üíä", "ru": {"title": "–ò–ò –Ω–∞ —Å—Ç—Ä–∞–∂–µ –≤–∞—à–µ–≥–æ –∑–¥–æ—Ä–æ–≤—å—è: —Ç–æ—á–Ω–æ–µ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–µ–º–∞ –ª–µ–∫–∞—Ä—Å—Ç–≤", "desc": "–î–∞–Ω–Ω–∞—è —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–∏—Å—Ç–µ–º—É AIMI (Adherence Forecasting and Intervention wi
[22.03.2025 12:40] Using data from previous issue: {"categories": ["#transfer_learning", "#dataset", "#open_source", "#small_models", "#training", "#architecture"], "emoji": "üöÄ", "ru": {"title": "–î–æ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –∫–æ–¥–∞: –º–µ–Ω—å—à–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –≤—ã—à–µ —Ç–æ—á–Ω–æ—Å—Ç—å", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –¥–æ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –¥–ª—è –∞–≤—Ç–æ–¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è –∫–æ–¥–∞ –Ω–∞ –¥–∞–Ω–Ω—ã—Ö –∫–æ–Ω–∫
[22.03.2025 12:40] Using data from previous issue: {"categories": ["#cv", "#benchmark", "#multimodal", "#interpretability", "#games"], "emoji": "üîç", "ru": {"title": "–ó–∞–≥–ª—è–¥—ã–≤–∞—è –≤ –≥–ª–∞–∑–∞ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–º—É –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É: –ø–æ–Ω–∏–º–∞–Ω–∏–µ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è LVLM", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –ø–æ–≤–µ–¥–µ–Ω–∏–µ –±–æ–ª—å—à–∏—Ö –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LVLM) –≤ –∑–∞–¥–∞—á–∞—Ö –ø–æ–Ω–∏–º–∞–Ω
[22.03.2025 12:40] Using data from previous issue: {"categories": ["#training", "#synthetic", "#open_source", "#multimodal", "#data", "#dataset", "#inference"], "emoji": "üé®", "ru": {"title": "TikZero: –º–æ—Å—Ç –º–µ–∂–¥—É —Ç–µ–∫—Å—Ç–æ–º –∏ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –≥—Ä–∞—Ñ–∏–∫–æ–π —á–µ—Ä–µ–∑ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è", "desc": "TikZero - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ–≥—Ä–∞–º–º –Ω–∞ –æ—Å–Ω–æ
[22.03.2025 12:40] Using data from previous issue: {"categories": ["#optimization", "#dataset", "#benchmark", "#3d", "#architecture", "#survey", "#multimodal"], "emoji": "üöó", "ru": {"title": "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ 4D-–∑–∞–Ω—è—Ç–æ—Å—Ç–∏ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è –≤ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–º –≤–æ–∂–¥–µ–Ω–∏–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ GASP - –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–æ–µ –∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å–∞–º–æ–∫–æ
[22.03.2025 12:40] Trying to get texts in Chinese.
[22.03.2025 12:40] Mistral request. Model: mistral-large-latest. Prompt: Write simple and brief explanation (4-5 sentences) of an article in Chinese. Use short sentences. Text:

Diffusion models for super-resolution (SR) produce high-quality visual results but require expensive computational costs. Despite the development of several methods to accelerate diffusion-based SR models, some (e.g., SinSR) fail to produce realistic perceptual details, while others (e.g., OSEDiff) may hallucinate non-existent structures. To overcome these issues, we present RSD, a new distillation method for ResShift, one of the top diffusion-based SR models. Our method is based on training the student network to produce such images that a new fake ResShift model trained on them will coincide with the teacher model. RSD achieves single-step restoration and outperforms the teacher by a large margin. We show that our distillation method can surpass the other distillation-based method for ResShift - SinSR - making it on par with state-of-the-art diffusion-based SR distillation methods. Compared to SR methods based on pre-trained text-to-image models, RSD produces competitive perceptual quality, provides images with better alignment to degraded input images, and requires fewer parameters and GPU memory. We provide experimental results on various real-world and synthetic datasets, including RealSR, RealSet65, DRealSR, ImageNet, and DIV2K.
[22.03.2025 12:40] Mistral response. {"id": "7908f989e93e4a76b971e485f7e23248", "object": "chat.completion", "created": 1742647240, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "\u8fd9\u7bc7\u6587\u7ae0\u8ba8\u8bba\u4e86\u8d85\u5206\u8fa8\u7387\uff08SR\uff09\u6269\u6563\u6a21\u578b\u7684\u8ba1\u7b97\u6210\u672c\u95ee\u9898\u3002\u867d\u7136\u6709\u4e9b\u65b9\u6cd5\u52a0\u901f\u4e86SR\u6a21\u578b\uff0c\u4f46\u5b83\u4eec\u8981\u4e48\u5931\u53bb\u4e86\u771f\u5b9e\u7684\u611f\u77e5\u7ec6\u8282\uff0c\u8981\u4e48\u4ea7\u751f\u4e86\u4e0d\u5b58\u5728\u7684\u7ed3\u6784\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86RSD\uff0c\u4e00\u79cd\u65b0\u7684\u84b8\u998f\u65b9\u6cd5\u3002RSD\u901a\u8fc7\u8bad\u7ec3\u5b66\u751f\u7f51\u7edc\u4ea7\u751f\u4e0e\u6559\u5e08\u6a21\u578b\u4e00\u81f4\u7684\u56fe\u50cf\uff0c\u5b9e\u73b0\u4e86\u5355\u6b65\u6062\u590d\uff0c\u5e76\u8d85\u8d8a\u4e86\u6559\u5e08\u6a21\u578b\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cRSD\u5728\u591a\u4e2a\u771f\u5b9e\u548c\u5408\u6210\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\u3002"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 313, "total_tokens": 484, "completion_tokens": 171}}
[22.03.2025 12:40] Response: ËøôÁØáÊñáÁ´†ËÆ®ËÆ∫‰∫ÜË∂ÖÂàÜËæ®ÁéáÔºàSRÔºâÊâ©Êï£Ê®°ÂûãÁöÑËÆ°ÁÆóÊàêÊú¨ÈóÆÈ¢ò„ÄÇËôΩÁÑ∂Êúâ‰∫õÊñπÊ≥ïÂä†ÈÄü‰∫ÜSRÊ®°ÂûãÔºå‰ΩÜÂÆÉ‰ª¨Ë¶Å‰πàÂ§±Âéª‰∫ÜÁúüÂÆûÁöÑÊÑüÁü•ÁªÜËäÇÔºåË¶Å‰πà‰∫ßÁîü‰∫Ü‰∏çÂ≠òÂú®ÁöÑÁªìÊûÑ„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºå‰ΩúËÄÖÊèêÂá∫‰∫ÜRSDÔºå‰∏ÄÁßçÊñ∞ÁöÑËí∏È¶èÊñπÊ≥ï„ÄÇRSDÈÄöËøáËÆ≠ÁªÉÂ≠¶ÁîüÁΩëÁªú‰∫ßÁîü‰∏éÊïôÂ∏àÊ®°Âûã‰∏ÄËá¥ÁöÑÂõæÂÉèÔºåÂÆûÁé∞‰∫ÜÂçïÊ≠•ÊÅ¢Â§çÔºåÂπ∂Ë∂ÖË∂ä‰∫ÜÊïôÂ∏àÊ®°Âûã„ÄÇÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåRSDÂú®Â§ö‰∏™ÁúüÂÆûÂíåÂêàÊàêÊï∞ÊçÆÈõÜ‰∏äË°®Áé∞Âá∫Ëâ≤„ÄÇ
[22.03.2025 12:40] Mistral request. Model: mistral-large-latest. Prompt: Write pinyin transcription for text. Text:

ËøôÁØáÊñáÁ´†ËÆ®ËÆ∫‰∫ÜË∂ÖÂàÜËæ®ÁéáÔºàSRÔºâÊâ©Êï£Ê®°ÂûãÁöÑËÆ°ÁÆóÊàêÊú¨ÈóÆÈ¢ò„ÄÇËôΩÁÑ∂Êúâ‰∫õÊñπÊ≥ïÂä†ÈÄü‰∫ÜSRÊ®°ÂûãÔºå‰ΩÜÂÆÉ‰ª¨Ë¶Å‰πàÂ§±Âéª‰∫ÜÁúüÂÆûÁöÑÊÑüÁü•ÁªÜËäÇÔºåË¶Å‰πà‰∫ßÁîü‰∫Ü‰∏çÂ≠òÂú®ÁöÑÁªìÊûÑ„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºå‰ΩúËÄÖÊèêÂá∫‰∫ÜRSDÔºå‰∏ÄÁßçÊñ∞ÁöÑËí∏È¶èÊñπÊ≥ï„ÄÇRSDÈÄöËøáËÆ≠ÁªÉÂ≠¶ÁîüÁΩëÁªú‰∫ßÁîü‰∏éÊïôÂ∏àÊ®°Âûã‰∏ÄËá¥ÁöÑÂõæÂÉèÔºåÂÆûÁé∞‰∫ÜÂçïÊ≠•ÊÅ¢Â§çÔºåÂπ∂Ë∂ÖË∂ä‰∫ÜÊïôÂ∏àÊ®°Âûã„ÄÇÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåRSDÂú®Â§ö‰∏™ÁúüÂÆûÂíåÂêàÊàêÊï∞ÊçÆÈõÜ‰∏äË°®Áé∞Âá∫Ëâ≤„ÄÇ
[22.03.2025 12:40] Mistral response. {"id": "2f5ac1ea0fb14bdfa72c7c086996ddbc", "object": "chat.completion", "created": 1742647245, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "\u8fd9\u7bc7\u6587\u7ae0\u8ba8\u8bba\u4e86\u8d85\u5206\u8fa8\u7387\uff08SR\uff09\u6269\u6563\u6a21\u578b\u7684\u8ba1\u7b97\u6210\u672c\u95ee\u9898\u3002\u867d\u7136\u6709\u4e9b\u65b9\u6cd5\u52a0\u901f\u4e86SR\u6a21\u578b\uff0c\u4f46\u5b83\u4eec\u8981\u4e48\u5931\u53bb\u4e86\u771f\u5b9e\u7684\u611f\u77e5\u7ec6\u8282\uff0c\u8981\u4e48\u4ea7\u751f\u4e86\u4e0d\u5b58\u5728\u7684\u7ed3\u6784\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86RSD\uff0c\u4e00\u79cd\u65b0\u7684\u84b8\u998f\u65b9\u6cd5\u3002RSD\u901a\u8fc7\u8bad\u7ec3\u5b66\u751f\u7f51\u7edc\u4ea7\u751f\u4e0e\u6559\u5e08\u6a21\u578b\u4e00\u81f4\u7684\u56fe\u50cf\uff0c\u5b9e\u73b0\u4e86\u5355\u6b65\u6062\u590d\uff0c\u5e76\u8d85\u8d8a\u4e86\u6559\u5e08\u6a21\u578b\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cRSD\u5728\u591a\u4e2a\u771f\u5b9e\u548c\u5408\u6210\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\u3002\n\nZh\u00e8 pi\u0101n w\u00e9nzh\u0101ng t\u01ceol\u00f9n le ch\u0101of\u0113n b\u01d0l\u01dc (SR) ku\u00f2s\u00e0n m\u00f3x\u00edng de j\u00ecsu\u00e0n ch\u00e9ngb\u011bn w\u00e8nt\u00ed. Su\u012br\u00e1n y\u01d2uxi\u0113 f\u0101ngf\u01ce ji\u0101s\u00f9 le SR m\u00f3x\u00edng, d\u00e0n t\u0101men y\u00e0om\u00e9i sh\u012bq\u00f9 le zh\u0113nsh\u00ed de g\u01cenju\u00e9 x\u00ecji\u00e9, y\u00e0om\u00e9i ch\u01censh\u0113ng le b\u00f9 c\u00fanz\u00e0i de ji\u00e9g\u00f2u. W\u00e8ile ji\u011bju\u00e9 zh\u00e8xi\u0113 w\u00e8nt\u00ed, zu\u00f2zh\u011b t\u00edch\u016b le RSD, y\u012bzh\u01d2ng x\u012bn de zh\u0113ngli\u00fa f\u0101ngf\u01ce. RSD t\u014dnggu\u00f2 x\u00f9nli\u00e0n xu\u00e9sh\u0113ng w\u01cenglu\u00f2 ch\u01censh\u0113ng y\u01d4 ji\u00e0osh\u012b m\u00f3x\u00edng y\u012bzh\u00ec de t\u00faxi\u00e0ng, sh\u00edxi\u00e0n le d\u0101nb\u00f9 hu\u012bf\u00f9, b\u00ecng ch\u0101oyu\u00e8 le ji\u00e0osh\u012b m\u00f3x\u00edng. Sh\u00edy\u00e0n ji\u00e9gu\u01d2 xi\u01censh\u00ec, RSD z\u00e0i du\u014dg\u00e8 zh\u0113nsh\u00ed h\u00e9 h\u00e9ch\u00e9ng sh\u00f9j\u00f9j\u00ed sh\u00e0ng bi\u01ceoxi\u00e0n ch\u016bs\u00e8."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 186, "total_tokens": 695, "completion_tokens": 509}}
[22.03.2025 12:40] Response: ËøôÁØáÊñáÁ´†ËÆ®ËÆ∫‰∫ÜË∂ÖÂàÜËæ®ÁéáÔºàSRÔºâÊâ©Êï£Ê®°ÂûãÁöÑËÆ°ÁÆóÊàêÊú¨ÈóÆÈ¢ò„ÄÇËôΩÁÑ∂Êúâ‰∫õÊñπÊ≥ïÂä†ÈÄü‰∫ÜSRÊ®°ÂûãÔºå‰ΩÜÂÆÉ‰ª¨Ë¶Å‰πàÂ§±Âéª‰∫ÜÁúüÂÆûÁöÑÊÑüÁü•ÁªÜËäÇÔºåË¶Å‰πà‰∫ßÁîü‰∫Ü‰∏çÂ≠òÂú®ÁöÑÁªìÊûÑ„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºå‰ΩúËÄÖÊèêÂá∫‰∫ÜRSDÔºå‰∏ÄÁßçÊñ∞ÁöÑËí∏È¶èÊñπÊ≥ï„ÄÇRSDÈÄöËøáËÆ≠ÁªÉÂ≠¶ÁîüÁΩëÁªú‰∫ßÁîü‰∏éÊïôÂ∏àÊ®°Âûã‰∏ÄËá¥ÁöÑÂõæÂÉèÔºåÂÆûÁé∞‰∫ÜÂçïÊ≠•ÊÅ¢Â§çÔºåÂπ∂Ë∂ÖË∂ä‰∫ÜÊïôÂ∏àÊ®°Âûã„ÄÇÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåRSDÂú®Â§ö‰∏™ÁúüÂÆûÂíåÂêàÊàêÊï∞ÊçÆÈõÜ‰∏äË°®Áé∞Âá∫Ëâ≤„ÄÇ

Zh√® piƒÅn w√©nzhƒÅng t«éol√πn le chƒÅofƒìn b«êl«ú (SR) ku√≤s√†n m√≥x√≠ng de j√¨su√†n ch√©ngbƒõn w√®nt√≠. Suƒ´r√°n y«íuxiƒì fƒÅngf«é jiƒÅs√π le SR m√≥x√≠ng, d√†n tƒÅmen y√†om√©i shƒ´q√π le zhƒìnsh√≠ de g«énju√© x√¨ji√©, y√†om√©i ch«énshƒìng le b√π c√∫nz√†i de ji√©g√≤u. W√®ile jiƒõju√© zh√®xiƒì w√®nt√≠, zu√≤zhƒõ t√≠ch≈´ le RSD, yƒ´zh«íng xƒ´n de zhƒìngli√∫ fƒÅngf«é. RSD t≈çnggu√≤ x√πnli√†n xu√©shƒìng w«énglu√≤ ch«énshƒìng y«î ji√†oshƒ´ m√≥x√≠ng yƒ´zh√¨ de t√∫xi√†ng, sh√≠xi√†n le dƒÅnb√π huƒ´f√π, b√¨ng chƒÅoyu√® le ji√†oshƒ´ m√≥x√≠ng. Sh√≠y√†n ji√©gu«í xi«énsh√¨, RSD z√†i du≈çg√® zhƒìnsh√≠ h√© h√©ch√©ng sh√πj√πj√≠ sh√†ng bi«éoxi√†n ch≈´s√®.
[22.03.2025 12:40] Mistral request. Model: mistral-large-latest. Prompt: Write vocab of difficult words for this text as an array of objects with fields 'word', 'pinyin', 'trans'. Return as python list without formatting. Return list and nothing else. Text:

ËøôÁØáÊñáÁ´†ËÆ®ËÆ∫‰∫ÜË∂ÖÂàÜËæ®ÁéáÔºàSRÔºâÊâ©Êï£Ê®°ÂûãÁöÑËÆ°ÁÆóÊàêÊú¨ÈóÆÈ¢ò„ÄÇËôΩÁÑ∂Êúâ‰∫õÊñπÊ≥ïÂä†ÈÄü‰∫ÜSRÊ®°ÂûãÔºå‰ΩÜÂÆÉ‰ª¨Ë¶Å‰πàÂ§±Âéª‰∫ÜÁúüÂÆûÁöÑÊÑüÁü•ÁªÜËäÇÔºåË¶Å‰πà‰∫ßÁîü‰∫Ü‰∏çÂ≠òÂú®ÁöÑÁªìÊûÑ„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºå‰ΩúËÄÖÊèêÂá∫‰∫ÜRSDÔºå‰∏ÄÁßçÊñ∞ÁöÑËí∏È¶èÊñπÊ≥ï„ÄÇRSDÈÄöËøáËÆ≠ÁªÉÂ≠¶ÁîüÁΩëÁªú‰∫ßÁîü‰∏éÊïôÂ∏àÊ®°Âûã‰∏ÄËá¥ÁöÑÂõæÂÉèÔºåÂÆûÁé∞‰∫ÜÂçïÊ≠•ÊÅ¢Â§çÔºåÂπ∂Ë∂ÖË∂ä‰∫ÜÊïôÂ∏àÊ®°Âûã„ÄÇÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåRSDÂú®Â§ö‰∏™ÁúüÂÆûÂíåÂêàÊàêÊï∞ÊçÆÈõÜ‰∏äË°®Áé∞Âá∫Ëâ≤„ÄÇ
[22.03.2025 12:41] Mistral response. {"id": "72ef81cc079a42c6bb3b28e850a26b46", "object": "chat.completion", "created": 1742647258, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "[\n    {\"word\": \"\u8d85\u5206\u8fa8\u7387\", \"pinyin\": \"ch\u0101o f\u0113n bi\u00e0o l\u01dc\", \"trans\": \"super-resolution\"},\n    {\"word\": \"\u6269\u6563\", \"pinyin\": \"ku\u00f2 s\u00e0n\", \"trans\": \"diffusion\"},\n    {\"word\": \"\u6a21\u578b\", \"pinyin\": \"m\u00f3 x\u00edng\", \"trans\": \"model\"},\n    {\"word\": \"\u8ba1\u7b97\", \"pinyin\": \"j\u00ec su\u00e0n\", \"trans\": \"computation\"},\n    {\"word\": \"\u6210\u672c\", \"pinyin\": \"ch\u00e9ng b\u011bn\", \"trans\": \"cost\"},\n    {\"word\": \"\u8ba8\u8bba\", \"pinyin\": \"t\u01ceo l\u00f9n\", \"trans\": \"discuss\"},\n    {\"word\": \"\u52a0\u901f\", \"pinyin\": \"ji\u0101 s\u00f9\", \"trans\": \"accelerate\"},\n    {\"word\": \"\u771f\u5b9e\", \"pinyin\": \"zh\u0113n sh\u00ed\", \"trans\": \"real\"},\n    {\"word\": \"\u611f\u77e5\", \"pinyin\": \"g\u01cen zh\u012b\", \"trans\": \"perception\"},\n    {\"word\": \"\u7ec6\u8282\", \"pinyin\": \"x\u00ec ji\u011b\", \"trans\": \"detail\"},\n    {\"word\": \"\u7ed3\u6784\", \"pinyin\": \"ji\u00e9 g\u00f2u\", \"trans\": \"structure\"},\n    {\"word\": \"\u63d0\u51fa\", \"pinyin\": \"t\u00ed ch\u016b\", \"trans\": \"propose\"},\n    {\"word\": \"\u84b8\u998f\", \"pinyin\": \"zh\u0113ng li\u00fa\", \"trans\": \"distillation\"},\n    {\"word\": \"\u5b66\u751f\", \"pinyin\": \"xu\u00e9 sh\u0113ng\", \"trans\": \"student\"},\n    {\"word\": \"\u7f51\u7edc\", \"pinyin\": \"w\u01ceng lu\u00f2\", \"trans\": \"network\"},\n    {\"word\": \"\u6559\u5e08\", \"pinyin\": \"ji\u00e0o sh\u012b\", \"trans\": \"teacher\"},\n    {\"word\": \"\u4e00\u81f4\", \"pinyin\": \"y\u012b zh\u00ec\", \"trans\": \"consistent\"},\n    {\"word\": \"\u56fe\u50cf\", \"pinyin\": \"t\u00fa xi\u00e0ng\", \"trans\": \"image\"},\n    {\"word\": \"\u5b9e\u73b0\", \"pinyin\": \"sh\u00ed xi\u00e0n\", \"trans\": \"achieve\"},\n    {\"word\": \"\u6062\u590d\", \"pinyin\": \"hu\u012b f\u00f9\", \"trans\": \"recover\"},\n    {\"word\": \"\u8d85\u8d8a\", \"pinyin\": \"ch\u0101o yu\u00e8\", \"trans\": \"surpass\"},\n    {\"word\": \"\u5b9e\u9a8c\", \"pinyin\": \"sh\u00ed y\u00e0n\", \"trans\": \"experiment\"},\n    {\"word\": \"\u7ed3\u679c\", \"pinyin\": \"ji\u00e9 gu\u01d2\", \"trans\": \"result\"},\n    {\"word\": \"\u8868\u73b0\", \"pinyin\": \"bi\u01ceo xi\u00e0n\", \"trans\": \"performance\"},\n    {\"word\": \"\u51fa\u8272\", \"pinyin\": \"ch\u016b s\u00e8\", \"trans\": \"outstanding\"},\n    {\"word\": \"\u6570\u636e\u96c6\", \"pinyin\": \"sh\u00f9 j\u00f9 j\u00ed\", \"trans\": \"dataset\"}\n]"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 218, "total_tokens": 964, "completion_tokens": 746}}
[22.03.2025 12:41] Response: [
    {"word": "Ë∂ÖÂàÜËæ®Áéá", "pinyin": "chƒÅo fƒìn bi√†o l«ú", "trans": "super-resolution"},
    {"word": "Êâ©Êï£", "pinyin": "ku√≤ s√†n", "trans": "diffusion"},
    {"word": "Ê®°Âûã", "pinyin": "m√≥ x√≠ng", "trans": "model"},
    {"word": "ËÆ°ÁÆó", "pinyin": "j√¨ su√†n", "trans": "computation"},
    {"word": "ÊàêÊú¨", "pinyin": "ch√©ng bƒõn", "trans": "cost"},
    {"word": "ËÆ®ËÆ∫", "pinyin": "t«éo l√πn", "trans": "discuss"},
    {"word": "Âä†ÈÄü", "pinyin": "jiƒÅ s√π", "trans": "accelerate"},
    {"word": "ÁúüÂÆû", "pinyin": "zhƒìn sh√≠", "trans": "real"},
    {"word": "ÊÑüÁü•", "pinyin": "g«én zhƒ´", "trans": "perception"},
    {"word": "ÁªÜËäÇ", "pinyin": "x√¨ jiƒõ", "trans": "detail"},
    {"word": "ÁªìÊûÑ", "pinyin": "ji√© g√≤u", "trans": "structure"},
    {"word": "ÊèêÂá∫", "pinyin": "t√≠ ch≈´", "trans": "propose"},
    {"word": "Ëí∏È¶è", "pinyin": "zhƒìng li√∫", "trans": "distillation"},
    {"word": "Â≠¶Áîü", "pinyin": "xu√© shƒìng", "trans": "student"},
    {"word": "ÁΩëÁªú", "pinyin": "w«éng lu√≤", "trans": "network"},
    {"word": "ÊïôÂ∏à", "pinyin": "ji√†o shƒ´", "trans": "teacher"},
    {"word": "‰∏ÄËá¥", "pinyin": "yƒ´ zh√¨", "trans": "consistent"},
    {"word": "ÂõæÂÉè", "pinyin": "t√∫ xi√†ng", "trans": "image"},
    {"word": "ÂÆûÁé∞", "pinyin": "sh√≠ xi√†n", "trans": "achieve"},
    {"word": "ÊÅ¢Â§ç", "pinyin": "huƒ´ f√π", "trans": "recover"},
    {"word": "Ë∂ÖË∂ä", "pinyin": "chƒÅo yu√®", "trans": "surpass"},
    {"word": "ÂÆûÈ™å", "pinyin": "sh√≠ y√†n", "trans": "experiment"},
    {"word": "ÁªìÊûú", "pinyin": "ji√© gu«í", "trans": "result"},
    {"word": "Ë°®Áé∞", "pinyin": "bi«éo xi√†n", "trans": "performance"},
    {"word": "Âá∫Ëâ≤", "pinyin": "ch≈´ s√®", "trans": "outstanding"},
    {"word": "Êï∞ÊçÆÈõÜ", "pinyin": "sh√π j√π j√≠", "trans": "dataset"}
]
[22.03.2025 12:41] Mistral request. Model: mistral-large-latest. Prompt: Translate this text in English. Text:

ËøôÁØáÊñáÁ´†ËÆ®ËÆ∫‰∫ÜË∂ÖÂàÜËæ®ÁéáÔºàSRÔºâÊâ©Êï£Ê®°ÂûãÁöÑËÆ°ÁÆóÊàêÊú¨ÈóÆÈ¢ò„ÄÇËôΩÁÑ∂Êúâ‰∫õÊñπÊ≥ïÂä†ÈÄü‰∫ÜSRÊ®°ÂûãÔºå‰ΩÜÂÆÉ‰ª¨Ë¶Å‰πàÂ§±Âéª‰∫ÜÁúüÂÆûÁöÑÊÑüÁü•ÁªÜËäÇÔºåË¶Å‰πà‰∫ßÁîü‰∫Ü‰∏çÂ≠òÂú®ÁöÑÁªìÊûÑ„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºå‰ΩúËÄÖÊèêÂá∫‰∫ÜRSDÔºå‰∏ÄÁßçÊñ∞ÁöÑËí∏È¶èÊñπÊ≥ï„ÄÇRSDÈÄöËøáËÆ≠ÁªÉÂ≠¶ÁîüÁΩëÁªú‰∫ßÁîü‰∏éÊïôÂ∏àÊ®°Âûã‰∏ÄËá¥ÁöÑÂõæÂÉèÔºåÂÆûÁé∞‰∫ÜÂçïÊ≠•ÊÅ¢Â§çÔºåÂπ∂Ë∂ÖË∂ä‰∫ÜÊïôÂ∏àÊ®°Âûã„ÄÇÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåRSDÂú®Â§ö‰∏™ÁúüÂÆûÂíåÂêàÊàêÊï∞ÊçÆÈõÜ‰∏äË°®Áé∞Âá∫Ëâ≤„ÄÇ
[22.03.2025 12:41] Mistral response. {"id": "425af16b876a4b22a4929c33b1fc3b5d", "object": "chat.completion", "created": 1742647276, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "This article discusses the computational cost issue of super-resolution (SR) diffusion models. Although some methods have accelerated SR models, they either lose authentic perceptual details or generate non-existent structures. To address these problems, the authors propose RSD, a new distillation method. RSD achieves single-step recovery by training a student network to produce images consistent with the teacher model and surpasses the teacher model. Experimental results demonstrate that RSD performs excellently on multiple real and synthetic datasets."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 184, "total_tokens": 293, "completion_tokens": 109}}
[22.03.2025 12:41] Response: This article discusses the computational cost issue of super-resolution (SR) diffusion models. Although some methods have accelerated SR models, they either lose authentic perceptual details or generate non-existent structures. To address these problems, the authors propose RSD, a new distillation method. RSD achieves single-step recovery by training a student network to produce images consistent with the teacher model and surpasses the teacher model. Experimental results demonstrate that RSD performs excellently on multiple real and synthetic datasets.
[22.03.2025 12:41] Renaming data file.
[22.03.2025 12:41] Renaming previous data. hf_papers.json to ./d/2025-03-21.json
[22.03.2025 12:41] Saving new data file.
[22.03.2025 12:41] Generating page.
[22.03.2025 12:41] Renaming previous page.
[22.03.2025 12:41] Renaming previous data. index.html to ./d/2025-03-21.html
[22.03.2025 12:41] [Experimental] Generating Chinese page for reading.
[22.03.2025 12:41] Chinese vocab [{'word': 'Ë∂ÖÂàÜËæ®Áéá', 'pinyin': 'chƒÅo fƒìn bi√†o l«ú', 'trans': 'super-resolution'}, {'word': 'Êâ©Êï£', 'pinyin': 'ku√≤ s√†n', 'trans': 'diffusion'}, {'word': 'Ê®°Âûã', 'pinyin': 'm√≥ x√≠ng', 'trans': 'model'}, {'word': 'ËÆ°ÁÆó', 'pinyin': 'j√¨ su√†n', 'trans': 'computation'}, {'word': 'ÊàêÊú¨', 'pinyin': 'ch√©ng bƒõn', 'trans': 'cost'}, {'word': 'ËÆ®ËÆ∫', 'pinyin': 't«éo l√πn', 'trans': 'discuss'}, {'word': 'Âä†ÈÄü', 'pinyin': 'jiƒÅ s√π', 'trans': 'accelerate'}, {'word': 'ÁúüÂÆû', 'pinyin': 'zhƒìn sh√≠', 'trans': 'real'}, {'word': 'ÊÑüÁü•', 'pinyin': 'g«én zhƒ´', 'trans': 'perception'}, {'word': 'ÁªÜËäÇ', 'pinyin': 'x√¨ jiƒõ', 'trans': 'detail'}, {'word': 'ÁªìÊûÑ', 'pinyin': 'ji√© g√≤u', 'trans': 'structure'}, {'word': 'ÊèêÂá∫', 'pinyin': 't√≠ ch≈´', 'trans': 'propose'}, {'word': 'Ëí∏È¶è', 'pinyin': 'zhƒìng li√∫', 'trans': 'distillation'}, {'word': 'Â≠¶Áîü', 'pinyin': 'xu√© shƒìng', 'trans': 'student'}, {'word': 'ÁΩëÁªú', 'pinyin': 'w«éng lu√≤', 'trans': 'network'}, {'word': 'ÊïôÂ∏à', 'pinyin': 'ji√†o shƒ´', 'trans': 'teacher'}, {'word': '‰∏ÄËá¥', 'pinyin': 'yƒ´ zh√¨', 'trans': 'consistent'}, {'word': 'ÂõæÂÉè', 'pinyin': 't√∫ xi√†ng', 'trans': 'image'}, {'word': 'ÂÆûÁé∞', 'pinyin': 'sh√≠ xi√†n', 'trans': 'achieve'}, {'word': 'ÊÅ¢Â§ç', 'pinyin': 'huƒ´ f√π', 'trans': 'recover'}, {'word': 'Ë∂ÖË∂ä', 'pinyin': 'chƒÅo yu√®', 'trans': 'surpass'}, {'word': 'ÂÆûÈ™å', 'pinyin': 'sh√≠ y√†n', 'trans': 'experiment'}, {'word': 'ÁªìÊûú', 'pinyin': 'ji√© gu«í', 'trans': 'result'}, {'word': 'Ë°®Áé∞', 'pinyin': 'bi«éo xi√†n', 'trans': 'performance'}, {'word': 'Âá∫Ëâ≤', 'pinyin': 'ch≈´ s√®', 'trans': 'outstanding'}, {'word': 'Êï∞ÊçÆÈõÜ', 'pinyin': 'sh√π j√π j√≠', 'trans': 'dataset'}]
[22.03.2025 12:41] Renaming previous Chinese page.
[22.03.2025 12:41] Renaming previous data. zh.html to ./d/2025-03-21_zh_reading_task.html
[22.03.2025 12:41] Writing Chinese reading task.
[22.03.2025 12:41] Writing result.
[22.03.2025 12:41] Renaming log file.
[22.03.2025 12:41] Renaming previous data. log.txt to ./logs/2025-03-22_last_log.txt
