[27.12.2024 08:13] Read previous papers.
[27.12.2024 08:13] Generating top page (month).
[27.12.2024 08:13] Writing top page (month).
[27.12.2024 09:10] Read previous papers.
[27.12.2024 09:10] Get feed.
[27.12.2024 09:10] Get page data from previous paper. URL: https://huggingface.co/papers/2412.17743
[27.12.2024 09:10] Get page data from previous paper. URL: https://huggingface.co/papers/2412.17483
[27.12.2024 09:10] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[27.12.2024 09:10] No deleted papers detected.
[27.12.2024 09:10] Downloading and parsing papers (pdf, html). Total: 2.
[27.12.2024 09:10] Downloading and parsing paper https://huggingface.co/papers/2412.17743.
[27.12.2024 09:10] Extra JSON file exists (./assets/json/2412.17743.json), skip PDF parsing.
[27.12.2024 09:10] Paper image links file exists (./assets/img_data/2412.17743.json), skip HTML parsing.
[27.12.2024 09:10] Success.
[27.12.2024 09:10] Downloading and parsing paper https://huggingface.co/papers/2412.17483.
[27.12.2024 09:10] Extra JSON file exists (./assets/json/2412.17483.json), skip PDF parsing.
[27.12.2024 09:10] Paper image links file exists (./assets/img_data/2412.17483.json), skip HTML parsing.
[27.12.2024 09:10] Success.
[27.12.2024 09:10] Enriching papers with extra data.
[27.12.2024 09:10] ********************************************************************************
[27.12.2024 09:10] Abstract 0. Effective pre-training of large language models (LLMs) has been challenging due to the immense resource demands and the complexity of the technical processes involved. This paper presents a detailed technical report on YuLan-Mini, a highly capable base model with 2.42B parameters that achieves top-t...
[27.12.2024 09:10] ********************************************************************************
[27.12.2024 09:10] Abstract 1. In this work, we provide a thorough investigation of gist-based context compression methods to improve long-context processing in large language models. We focus on two key questions: (1) How well can these methods replace full attention models? and (2) What potential failure patterns arise due to c...
[27.12.2024 09:10] Read previous papers.
[27.12.2024 09:10] Generating reviews via LLM API.
[27.12.2024 09:10] Using data from previous issue: {"categories": ["#long_context", "#open_source", "#optimization", "#small_models", "#data", "#training"], "emoji": "🚀", "ru": {"title": "Эффективное предобучение больших языковых моделей с меньшими ресурсами", "desc": "Статья представляет YuLan-Mini - языковую модель с 2.42 миллиардами параметров, д
[27.12.2024 09:10] Using data from previous issue: {"categories": ["#training", "#rag", "#optimization", "#long_context", "#data"], "emoji": "🗜️", "ru": {"title": "Сжатие без потерь: оптимизация обработки длинного контекста в больших языковых моделях", "desc": "Это исследование посвящено методам сжатия контекста на основе гист-токенов для улучшения 
[27.12.2024 09:10] Trying to get texts in Chinese.
[27.12.2024 09:10] Mistral request. Model: mistral-large-latest. Prompt: Write simple and brief explanation (4-5 sentences) of an article in Chinese. Use short sentences. Text:

Effective pre-training of large language models (LLMs) has been challenging due to the immense resource demands and the complexity of the technical processes involved. This paper presents a detailed technical report on YuLan-Mini, a highly capable base model with 2.42B parameters that achieves top-tier performance among models of similar parameter scale. Our pre-training approach focuses on enhancing training efficacy through three key technical contributions: an elaborate data pipeline combines data cleaning with data schedule strategies, a robust optimization method to mitigate training instability, and an effective annealing approach that incorporates targeted data selection and long context training. Remarkably, YuLan-Mini, trained on 1.08T tokens, achieves performance comparable to industry-leading models that require significantly more data. To facilitate reproduction, we release the full details of the data composition for each training phase. Project details can be accessed at the following link: https://github.com/RUC-GSAI/YuLan-Mini.
[27.12.2024 09:10] Mistral response. {"id": "3e84b562fe5046b4a6421e09ffc9935e", "object": "chat.completion", "created": 1735290610, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "\u8fd9\u7bc7\u6587\u7ae0\u4ecb\u7ecd\u4e86YuLan-Mini\uff0c\u4e00\u4e2a\u5177\u670924.2\u4ebf\u53c2\u6570\u7684\u57fa\u7840\u6a21\u578b\u3002\u8be5\u6a21\u578b\u5728\u9884\u8bad\u7ec3\u4e2d\u901a\u8fc7\u4e09\u4e2a\u5173\u952e\u6280\u672f\u8d21\u732e\u63d0\u9ad8\u4e86\u8bad\u7ec3\u6548\u7387\uff1a\u6570\u636e\u7ba1\u9053\u3001\u4f18\u5316\u65b9\u6cd5\u548c\u9000\u706b\u65b9\u6cd5\u3002YuLan-Mini\u57281.08\u4e07\u4ebf\u4e2a\u8bb0\u53f7\u4e0a\u8bad\u7ec3\uff0c\u6027\u80fd\u5ab2\u7f8e\u9700\u8981\u66f4\u591a\u6570\u636e\u7684\u884c\u4e1a\u9886\u5148\u6a21\u578b\u3002\u8be6\u7ec6\u4fe1\u606f\u53ef\u5728GitHub\u4e0a\u627e\u5230\u3002"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 249, "total_tokens": 384, "completion_tokens": 135}}
[27.12.2024 09:10] Response: 这篇文章介绍了YuLan-Mini，一个具有24.2亿参数的基础模型。该模型在预训练中通过三个关键技术贡献提高了训练效率：数据管道、优化方法和退火方法。YuLan-Mini在1.08万亿个记号上训练，性能媲美需要更多数据的行业领先模型。详细信息可在GitHub上找到。
[27.12.2024 09:10] Mistral request. Model: mistral-large-latest. Prompt: Write pinyin transcription for text. Text:

这篇文章介绍了YuLan-Mini，一个具有24.2亿参数的基础模型。该模型在预训练中通过三个关键技术贡献提高了训练效率：数据管道、优化方法和退火方法。YuLan-Mini在1.08万亿个记号上训练，性能媲美需要更多数据的行业领先模型。详细信息可在GitHub上找到。
[27.12.2024 09:10] Mistral response. {"id": "c9831cbbc45f4e5e952a5f46870f85ea", "object": "chat.completion", "created": 1735290614, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "\u8fd9\u7bc7\u6587\u7ae0\u4ecb\u7ecd\u4e86YuLan-Mini\uff0c\u4e00\u4e2a\u5177\u670924.2\u4ebf\u53c2\u6570\u7684\u57fa\u7840\u6a21\u578b\u3002\u8be5\u6a21\u578b\u5728\u9884\u8bad\u7ec3\u4e2d\u901a\u8fc7\u4e09\u4e2a\u5173\u952e\u6280\u672f\u8d21\u732e\u63d0\u9ad8\u4e86\u8bad\u7ec3\u6548\u7387\uff1a\u6570\u636e\u7ba1\u9053\u3001\u4f18\u5316\u65b9\u6cd5\u548c\u9000\u706b\u65b9\u6cd5\u3002YuLan-Mini\u57281.08\u4e07\u4ebf\u4e2a\u8bb0\u53f7\u4e0a\u8bad\u7ec3\uff0c\u6027\u80fd\u5ab2\u7f8e\u9700\u8981\u66f4\u591a\u6570\u636e\u7684\u884c\u4e1a\u9886\u5148\u6a21\u578b\u3002\u8be6\u7ec6\u4fe1\u606f\u53ef\u5728GitHub\u4e0a\u627e\u5230\u3002\n\nZh\u00e8 pi\u0101n w\u00e9nzh\u0101ng ji\u00e8sh\u00e0o le YuLan-Mini, y\u012bg\u00e8 j\u00f9y\u01d2u 24.2 y\u00ec c\u0101nsh\u00f9 de j\u012bch\u01d4 m\u00f3x\u00edng. G\u01cei m\u00f3x\u00edng z\u00e0i y\u00f9x\u00f9nli\u00e0n zh\u014dng t\u014dnggu\u00f2 s\u0101n g\u00e8 gu\u01cenji\u00e0n j\u00ecsh\u00f9 g\u00f2ngxi\u00e0n t\u00edg\u0101o le x\u00f9nli\u00e0n xi\u00e0ol\u01dc: sh\u00f9j\u00f9 gu\u01cend\u00e0o, y\u014duhu\u00e0 f\u0101ngf\u01ce h\u00e9 tu\u00echu\u01d2 f\u0101ngf\u01ce. YuLan-Mini z\u00e0i 1.08 w\u00e0n y\u00ec g\u00e8 j\u00ech\u00e0o sh\u00e0ng x\u00f9nli\u00e0n, x\u00ecngn\u00e9ng j\u012bm\u011bi x\u016by\u00e0o g\u00e8ng du\u014d sh\u00f9j\u00f9 de h\u00e1ngy\u00e8 l\u01d0ngxi\u0101n m\u00f3x\u00edng. Xi\u00e1ngx\u00ec x\u00ecnx\u012b k\u011b z\u00e0i GitHub sh\u00e0ng zh\u01ceo d\u00e0o."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 150, "total_tokens": 548, "completion_tokens": 398}}
[27.12.2024 09:10] Response: 这篇文章介绍了YuLan-Mini，一个具有24.2亿参数的基础模型。该模型在预训练中通过三个关键技术贡献提高了训练效率：数据管道、优化方法和退火方法。YuLan-Mini在1.08万亿个记号上训练，性能媲美需要更多数据的行业领先模型。详细信息可在GitHub上找到。

Zhè piān wénzhāng jièshào le YuLan-Mini, yīgè jùyǒu 24.2 yì cānshù de jīchǔ móxíng. Gǎi móxíng zài yùxùnliàn zhōng tōngguò sān gè guǎnjiàn jìshù gòngxiàn tígāo le xùnliàn xiàolǜ: shùjù guǎndào, yōuhuà fāngfǎ hé tuìhuǒ fāngfǎ. YuLan-Mini zài 1.08 wàn yì gè jìhào shàng xùnliàn, xìngnéng jīměi xūyào gèng duō shùjù de hángyè lǐngxiān móxíng. Xiángxì xìnxī kě zài GitHub shàng zhǎo dào.
[27.12.2024 09:10] Mistral request. Model: mistral-large-latest. Prompt: Write vocab of difficult words for this text as an array of objects with fields 'word', 'pinyin', 'trans'. Return as python list without formatting. Return list and nothing else. Text:

这篇文章介绍了YuLan-Mini，一个具有24.2亿参数的基础模型。该模型在预训练中通过三个关键技术贡献提高了训练效率：数据管道、优化方法和退火方法。YuLan-Mini在1.08万亿个记号上训练，性能媲美需要更多数据的行业领先模型。详细信息可在GitHub上找到。
[27.12.2024 09:10] Mistral response. {"id": "8506dda50cc24d0b84fa8d1a81e73967", "object": "chat.completion", "created": 1735290626, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "[\n    {\"word\": \"\u7bc7\", \"pinyin\": \"pi\u0101n\", \"trans\": \"article, piece of writing\"},\n    {\"word\": \"\u4ecb\u7ecd\", \"pinyin\": \"ji\u00e8sh\u00e0o\", \"trans\": \"introduce, present\"},\n    {\"word\": \"\u5177\u6709\", \"pinyin\": \"j\u00f9y\u01d2u\", \"trans\": \"possess, have\"},\n    {\"word\": \"\u4ebf\", \"pinyin\": \"y\u00ec\", \"trans\": \"hundred million\"},\n    {\"word\": \"\u53c2\u6570\", \"pinyin\": \"c\u0101nsh\u00f9\", \"trans\": \"parameter\"},\n    {\"word\": \"\u57fa\u7840\", \"pinyin\": \"j\u012bch\u01d4\", \"trans\": \"basic, foundation\"},\n    {\"word\": \"\u6a21\u578b\", \"pinyin\": \"m\u00f3x\u00edng\", \"trans\": \"model\"},\n    {\"word\": \"\u9884\u8bad\u7ec3\", \"pinyin\": \"y\u00f9x\u00f9nli\u00e0n\", \"trans\": \"pre-training\"},\n    {\"word\": \"\u901a\u8fc7\", \"pinyin\": \"t\u014dnggu\u00f2\", \"trans\": \"through, by means of\"},\n    {\"word\": \"\u5173\u952e\", \"pinyin\": \"gu\u01cenji\u00e0n\", \"trans\": \"key, crucial\"},\n    {\"word\": \"\u6280\u672f\", \"pinyin\": \"j\u00ecsh\u00f9\", \"trans\": \"technology, skill\"},\n    {\"word\": \"\u8d21\u732e\", \"pinyin\": \"g\u00f2ngxi\u00e0n\", \"trans\": \"contribution\"},\n    {\"word\": \"\u63d0\u9ad8\", \"pinyin\": \"t\u00edg\u0101o\", \"trans\": \"improve, raise\"},\n    {\"word\": \"\u6548\u7387\", \"pinyin\": \"xi\u00e0ol\u01dc\", \"trans\": \"efficiency\"},\n    {\"word\": \"\u6570\u636e\", \"pinyin\": \"sh\u00f9j\u00f9\", \"trans\": \"data\"},\n    {\"word\": \"\u7ba1\u9053\", \"pinyin\": \"gu\u01cend\u00e0o\", \"trans\": \"pipeline\"},\n    {\"word\": \"\u4f18\u5316\", \"pinyin\": \"y\u014duhu\u00e0\", \"trans\": \"optimization\"},\n    {\"word\": \"\u65b9\u6cd5\", \"pinyin\": \"f\u0101ngf\u01ce\", \"trans\": \"method\"},\n    {\"word\": \"\u9000\u706b\", \"pinyin\": \"tu\u00echu\u01d2\", \"trans\": \"annealing\"},\n    {\"word\": \"\u8bb0\u53f7\", \"pinyin\": \"j\u00ech\u00e0o\", \"trans\": \"symbol, token\"},\n    {\"word\": \"\u6027\u80fd\", \"pinyin\": \"x\u00ecngn\u00e9ng\", \"trans\": \"performance\"},\n    {\"word\": \"\u5ab2\u7f8e\", \"pinyin\": \"p\u00ecm\u011bi\", \"trans\": \"rival, match\"},\n    {\"word\": \"\u884c\u4e1a\", \"pinyin\": \"h\u00e1ngy\u00e8\", \"trans\": \"industry\"},\n    {\"word\": \"\u9886\u5148\", \"pinyin\": \"l\u01d0ngxi\u0101n\", \"trans\": \"leading, ahead\"},\n    {\"word\": \"\u8be6\u7ec6\", \"pinyin\": \"xi\u00e1ngx\u00ec\", \"trans\": \"detailed\"},\n    {\"word\": \"\u4fe1\u606f\", \"pinyin\": \"x\u00ecnx\u012b\", \"trans\": \"information\"},\n    {\"word\": \"GitHub\", \"pinyin\": \"GitHub\", \"trans\": \"GitHub\"}\n]"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 182, "total_tokens": 964, "completion_tokens": 782}}
[27.12.2024 09:10] Response: [
    {"word": "篇", "pinyin": "piān", "trans": "article, piece of writing"},
    {"word": "介绍", "pinyin": "jièshào", "trans": "introduce, present"},
    {"word": "具有", "pinyin": "jùyǒu", "trans": "possess, have"},
    {"word": "亿", "pinyin": "yì", "trans": "hundred million"},
    {"word": "参数", "pinyin": "cānshù", "trans": "parameter"},
    {"word": "基础", "pinyin": "jīchǔ", "trans": "basic, foundation"},
    {"word": "模型", "pinyin": "móxíng", "trans": "model"},
    {"word": "预训练", "pinyin": "yùxùnliàn", "trans": "pre-training"},
    {"word": "通过", "pinyin": "tōngguò", "trans": "through, by means of"},
    {"word": "关键", "pinyin": "guǎnjiàn", "trans": "key, crucial"},
    {"word": "技术", "pinyin": "jìshù", "trans": "technology, skill"},
    {"word": "贡献", "pinyin": "gòngxiàn", "trans": "contribution"},
    {"word": "提高", "pinyin": "tígāo", "trans": "improve, raise"},
    {"word": "效率", "pinyin": "xiàolǜ", "trans": "efficiency"},
    {"word": "数据", "pinyin": "shùjù", "trans": "data"},
    {"word": "管道", "pinyin": "guǎndào", "trans": "pipeline"},
    {"word": "优化", "pinyin": "yōuhuà", "trans": "optimization"},
    {"word": "方法", "pinyin": "fāngfǎ", "trans": "method"},
    {"word": "退火", "pinyin": "tuìhuǒ", "trans": "annealing"},
    {"word": "记号", "pinyin": "jìhào", "trans": "symbol, token"},
    {"word": "性能", "pinyin": "xìngnéng", "trans": "performance"},
    {"word": "媲美", "pinyin": "pìměi", "trans": "rival, match"},
    {"word": "行业", "pinyin": "hángyè", "trans": "industry"},
    {"word": "领先", "pinyin": "lǐngxiān", "trans": "leading, ahead"},
    {"word": "详细", "pinyin": "xiángxì", "trans": "detailed"},
    {"word": "信息", "pinyin": "xìnxī", "trans": "information"},
    {"word": "GitHub", "pinyin": "GitHub", "trans": "GitHub"}
]
[27.12.2024 09:10] Mistral request. Model: mistral-large-latest. Prompt: Translate this text in English. Text:

这篇文章介绍了YuLan-Mini，一个具有24.2亿参数的基础模型。该模型在预训练中通过三个关键技术贡献提高了训练效率：数据管道、优化方法和退火方法。YuLan-Mini在1.08万亿个记号上训练，性能媲美需要更多数据的行业领先模型。详细信息可在GitHub上找到。
[27.12.2024 09:10] Mistral response. {"id": "41a90de8e2a3459c812624693d486377", "object": "chat.completion", "created": 1735290643, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "This article introduces YuLan-Mini, a foundational model with 2.42 billion parameters. During pre-training, the model's training efficiency was enhanced through three key technological contributions: the data pipeline, optimization methods, and annealing techniques. YuLan-Mini was trained on 1.08 trillion tokens and performs comparably to industry-leading models that require more data. Detailed information can be found on GitHub."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 148, "total_tokens": 245, "completion_tokens": 97}}
[27.12.2024 09:10] Response: This article introduces YuLan-Mini, a foundational model with 2.42 billion parameters. During pre-training, the model's training efficiency was enhanced through three key technological contributions: the data pipeline, optimization methods, and annealing techniques. YuLan-Mini was trained on 1.08 trillion tokens and performs comparably to industry-leading models that require more data. Detailed information can be found on GitHub.
[27.12.2024 09:10] Renaming data file.
[27.12.2024 09:10] Renaming previous data. hf_papers.json to ./d/2024-12-27.json
[27.12.2024 09:10] Saving new data file.
[27.12.2024 09:10] Generating page.
[27.12.2024 09:10] Renaming previous page.
[27.12.2024 09:10] Renaming previous data. index.html to ./d/2024-12-27.html
[27.12.2024 09:10] [Experimental] Generating Chinese page for reading.
[27.12.2024 09:10] Chinese vocab [{'word': '篇', 'pinyin': 'piān', 'trans': 'article, piece of writing'}, {'word': '介绍', 'pinyin': 'jièshào', 'trans': 'introduce, present'}, {'word': '具有', 'pinyin': 'jùyǒu', 'trans': 'possess, have'}, {'word': '亿', 'pinyin': 'yì', 'trans': 'hundred million'}, {'word': '参数', 'pinyin': 'cānshù', 'trans': 'parameter'}, {'word': '基础', 'pinyin': 'jīchǔ', 'trans': 'basic, foundation'}, {'word': '模型', 'pinyin': 'móxíng', 'trans': 'model'}, {'word': '预训练', 'pinyin': 'yùxùnliàn', 'trans': 'pre-training'}, {'word': '通过', 'pinyin': 'tōngguò', 'trans': 'through, by means of'}, {'word': '关键', 'pinyin': 'guǎnjiàn', 'trans': 'key, crucial'}, {'word': '技术', 'pinyin': 'jìshù', 'trans': 'technology, skill'}, {'word': '贡献', 'pinyin': 'gòngxiàn', 'trans': 'contribution'}, {'word': '提高', 'pinyin': 'tígāo', 'trans': 'improve, raise'}, {'word': '效率', 'pinyin': 'xiàolǜ', 'trans': 'efficiency'}, {'word': '数据', 'pinyin': 'shùjù', 'trans': 'data'}, {'word': '管道', 'pinyin': 'guǎndào', 'trans': 'pipeline'}, {'word': '优化', 'pinyin': 'yōuhuà', 'trans': 'optimization'}, {'word': '方法', 'pinyin': 'fāngfǎ', 'trans': 'method'}, {'word': '退火', 'pinyin': 'tuìhuǒ', 'trans': 'annealing'}, {'word': '记号', 'pinyin': 'jìhào', 'trans': 'symbol, token'}, {'word': '性能', 'pinyin': 'xìngnéng', 'trans': 'performance'}, {'word': '媲美', 'pinyin': 'pìměi', 'trans': 'rival, match'}, {'word': '行业', 'pinyin': 'hángyè', 'trans': 'industry'}, {'word': '领先', 'pinyin': 'lǐngxiān', 'trans': 'leading, ahead'}, {'word': '详细', 'pinyin': 'xiángxì', 'trans': 'detailed'}, {'word': '信息', 'pinyin': 'xìnxī', 'trans': 'information'}, {'word': 'GitHub', 'pinyin': 'GitHub', 'trans': 'GitHub'}]
[27.12.2024 09:10] Renaming previous Chinese page.
[27.12.2024 09:10] Renaming previous data. zh.html to ./d/2024-12-26_zh_reading_task.html
[27.12.2024 09:10] Writing Chinese reading task.
[27.12.2024 09:10] Writing result.
[27.12.2024 09:10] Renaming log file.
[27.12.2024 09:10] Renaming previous data. log.txt to ./logs/2024-12-27_last_log.txt
