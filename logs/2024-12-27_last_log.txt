[27.12.2024 10:10] Read previous papers.
[27.12.2024 10:10] Generating top page (month).
[27.12.2024 10:10] Writing top page (month).
[27.12.2024 11:08] Read previous papers.
[27.12.2024 11:08] Get feed.
[27.12.2024 11:08] Get page data from previous paper. URL: https://huggingface.co/papers/2412.17743
[27.12.2024 11:08] Get page data from previous paper. URL: https://huggingface.co/papers/2412.17483
[27.12.2024 11:08] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[27.12.2024 11:08] No deleted papers detected.
[27.12.2024 11:08] Downloading and parsing papers (pdf, html). Total: 2.
[27.12.2024 11:08] Downloading and parsing paper https://huggingface.co/papers/2412.17743.
[27.12.2024 11:08] Extra JSON file exists (./assets/json/2412.17743.json), skip PDF parsing.
[27.12.2024 11:08] Paper image links file exists (./assets/img_data/2412.17743.json), skip HTML parsing.
[27.12.2024 11:08] Success.
[27.12.2024 11:08] Downloading and parsing paper https://huggingface.co/papers/2412.17483.
[27.12.2024 11:08] Extra JSON file exists (./assets/json/2412.17483.json), skip PDF parsing.
[27.12.2024 11:08] Paper image links file exists (./assets/img_data/2412.17483.json), skip HTML parsing.
[27.12.2024 11:08] Success.
[27.12.2024 11:08] Enriching papers with extra data.
[27.12.2024 11:08] ********************************************************************************
[27.12.2024 11:08] Abstract 0. Effective pre-training of large language models (LLMs) has been challenging due to the immense resource demands and the complexity of the technical processes involved. This paper presents a detailed technical report on YuLan-Mini, a highly capable base model with 2.42B parameters that achieves top-t...
[27.12.2024 11:08] ********************************************************************************
[27.12.2024 11:08] Abstract 1. In this work, we provide a thorough investigation of gist-based context compression methods to improve long-context processing in large language models. We focus on two key questions: (1) How well can these methods replace full attention models? and (2) What potential failure patterns arise due to c...
[27.12.2024 11:08] Read previous papers.
[27.12.2024 11:08] Generating reviews via LLM API.
[27.12.2024 11:08] Using data from previous issue: {"categories": ["#long_context", "#open_source", "#optimization", "#small_models", "#data", "#training"], "emoji": "ğŸš€", "ru": {"title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğ¼Ğ¸ Ñ€ĞµÑÑƒÑ€ÑĞ°Ğ¼Ğ¸", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ YuLan-Mini - ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ 2.42 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ´
[27.12.2024 11:08] Using data from previous issue: {"categories": ["#training", "#rag", "#optimization", "#long_context", "#data"], "emoji": "ğŸ—œï¸", "ru": {"title": "Ğ¡Ğ¶Ğ°Ñ‚Ğ¸Ğµ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ: Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…", "desc": "Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ³Ğ¸ÑÑ‚-Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ 
[27.12.2024 11:08] Loading Chinese text from previous data.
[27.12.2024 11:08] Renaming data file.
[27.12.2024 11:08] Renaming previous data. hf_papers.json to ./d/2024-12-27.json
[27.12.2024 11:08] Saving new data file.
[27.12.2024 11:08] Generating page.
[27.12.2024 11:08] Renaming previous page.
[27.12.2024 11:08] Renaming previous data. index.html to ./d/2024-12-27.html
[27.12.2024 11:08] [Experimental] Generating Chinese page for reading.
[27.12.2024 11:08] Chinese vocab [{'word': 'ç¯‡', 'pinyin': 'piÄn', 'trans': 'article, piece of writing'}, {'word': 'ä»‹ç»', 'pinyin': 'jiÃ¨shÃ o', 'trans': 'introduce, present'}, {'word': 'å…·æœ‰', 'pinyin': 'jÃ¹yÇ’u', 'trans': 'possess, have'}, {'word': 'äº¿', 'pinyin': 'yÃ¬', 'trans': 'hundred million'}, {'word': 'å‚æ•°', 'pinyin': 'cÄnshÃ¹', 'trans': 'parameter'}, {'word': 'åŸºç¡€', 'pinyin': 'jÄ«chÇ”', 'trans': 'basic, foundation'}, {'word': 'æ¨¡å‹', 'pinyin': 'mÃ³xÃ­ng', 'trans': 'model'}, {'word': 'é¢„è®­ç»ƒ', 'pinyin': 'yÃ¹xÃ¹nliÃ n', 'trans': 'pre-training'}, {'word': 'é€šè¿‡', 'pinyin': 'tÅngguÃ²', 'trans': 'through, by means of'}, {'word': 'å…³é”®', 'pinyin': 'guÇnjiÃ n', 'trans': 'key, crucial'}, {'word': 'æŠ€æœ¯', 'pinyin': 'jÃ¬shÃ¹', 'trans': 'technology, skill'}, {'word': 'è´¡çŒ®', 'pinyin': 'gÃ²ngxiÃ n', 'trans': 'contribution'}, {'word': 'æé«˜', 'pinyin': 'tÃ­gÄo', 'trans': 'improve, raise'}, {'word': 'æ•ˆç‡', 'pinyin': 'xiÃ olÇœ', 'trans': 'efficiency'}, {'word': 'æ•°æ®', 'pinyin': 'shÃ¹jÃ¹', 'trans': 'data'}, {'word': 'ç®¡é“', 'pinyin': 'guÇndÃ o', 'trans': 'pipeline'}, {'word': 'ä¼˜åŒ–', 'pinyin': 'yÅuhuÃ ', 'trans': 'optimization'}, {'word': 'æ–¹æ³•', 'pinyin': 'fÄngfÇ', 'trans': 'method'}, {'word': 'é€€ç«', 'pinyin': 'tuÃ¬huÇ’', 'trans': 'annealing'}, {'word': 'è®°å·', 'pinyin': 'jÃ¬hÃ o', 'trans': 'symbol, token'}, {'word': 'æ€§èƒ½', 'pinyin': 'xÃ¬ngnÃ©ng', 'trans': 'performance'}, {'word': 'åª²ç¾', 'pinyin': 'pÃ¬mÄ›i', 'trans': 'rival, match'}, {'word': 'è¡Œä¸š', 'pinyin': 'hÃ¡ngyÃ¨', 'trans': 'industry'}, {'word': 'é¢†å…ˆ', 'pinyin': 'lÇngxiÄn', 'trans': 'leading, ahead'}, {'word': 'è¯¦ç»†', 'pinyin': 'xiÃ¡ngxÃ¬', 'trans': 'detailed'}, {'word': 'ä¿¡æ¯', 'pinyin': 'xÃ¬nxÄ«', 'trans': 'information'}, {'word': 'GitHub', 'pinyin': 'GitHub', 'trans': 'GitHub'}]
[27.12.2024 11:08] Renaming previous Chinese page.
[27.12.2024 11:08] Renaming previous data. zh.html to ./d/2024-12-26_zh_reading_task.html
[27.12.2024 11:08] Writing Chinese reading task.
[27.12.2024 11:08] Writing result.
[27.12.2024 11:08] Renaming log file.
[27.12.2024 11:08] Renaming previous data. log.txt to ./logs/2024-12-27_last_log.txt
