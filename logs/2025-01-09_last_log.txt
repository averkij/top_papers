[09.01.2025 13:18] Read previous papers.
[09.01.2025 13:18] Generating top page (month).
[09.01.2025 13:18] Writing top page (month).
[09.01.2025 14:10] Read previous papers.
[09.01.2025 14:10] Get feed.
[09.01.2025 14:10] Get page data from previous paper. URL: https://huggingface.co/papers/2501.04519
[09.01.2025 14:10] Get page data from previous paper. URL: https://huggingface.co/papers/2501.04686
[09.01.2025 14:10] Get page data from previous paper. URL: https://huggingface.co/papers/2501.04227
[09.01.2025 14:10] Get page data from previous paper. URL: https://huggingface.co/papers/2501.04682
[09.01.2025 14:10] Get page data from previous paper. URL: https://huggingface.co/papers/2501.04306
[09.01.2025 14:10] Get page data from previous paper. URL: https://huggingface.co/papers/2501.04575
[09.01.2025 14:10] Get page data from previous paper. URL: https://huggingface.co/papers/2501.02772
[09.01.2025 14:10] Get page data from previous paper. URL: https://huggingface.co/papers/2501.04144
[09.01.2025 14:10] Get page data from previous paper. URL: https://huggingface.co/papers/2501.04689
[09.01.2025 14:10] Get page data from previous paper. URL: https://huggingface.co/papers/2501.04694
[09.01.2025 14:10] Get page data from previous paper. URL: https://huggingface.co/papers/2501.03271
[09.01.2025 14:10] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[09.01.2025 14:10] No deleted papers detected.
[09.01.2025 14:10] Downloading and parsing papers (pdf, html). Total: 11.
[09.01.2025 14:10] Downloading and parsing paper https://huggingface.co/papers/2501.04519.
[09.01.2025 14:10] Extra JSON file exists (./assets/json/2501.04519.json), skip PDF parsing.
[09.01.2025 14:10] Paper image links file exists (./assets/img_data/2501.04519.json), skip HTML parsing.
[09.01.2025 14:10] Success.
[09.01.2025 14:10] Downloading and parsing paper https://huggingface.co/papers/2501.04686.
[09.01.2025 14:10] Extra JSON file exists (./assets/json/2501.04686.json), skip PDF parsing.
[09.01.2025 14:10] Paper image links file exists (./assets/img_data/2501.04686.json), skip HTML parsing.
[09.01.2025 14:10] Success.
[09.01.2025 14:10] Downloading and parsing paper https://huggingface.co/papers/2501.04227.
[09.01.2025 14:10] Extra JSON file exists (./assets/json/2501.04227.json), skip PDF parsing.
[09.01.2025 14:10] Paper image links file exists (./assets/img_data/2501.04227.json), skip HTML parsing.
[09.01.2025 14:10] Success.
[09.01.2025 14:10] Downloading and parsing paper https://huggingface.co/papers/2501.04682.
[09.01.2025 14:10] Extra JSON file exists (./assets/json/2501.04682.json), skip PDF parsing.
[09.01.2025 14:10] Paper image links file exists (./assets/img_data/2501.04682.json), skip HTML parsing.
[09.01.2025 14:10] Success.
[09.01.2025 14:10] Downloading and parsing paper https://huggingface.co/papers/2501.04306.
[09.01.2025 14:10] Extra JSON file exists (./assets/json/2501.04306.json), skip PDF parsing.
[09.01.2025 14:10] Paper image links file exists (./assets/img_data/2501.04306.json), skip HTML parsing.
[09.01.2025 14:10] Success.
[09.01.2025 14:10] Downloading and parsing paper https://huggingface.co/papers/2501.04575.
[09.01.2025 14:10] Extra JSON file exists (./assets/json/2501.04575.json), skip PDF parsing.
[09.01.2025 14:10] Paper image links file exists (./assets/img_data/2501.04575.json), skip HTML parsing.
[09.01.2025 14:10] Success.
[09.01.2025 14:10] Downloading and parsing paper https://huggingface.co/papers/2501.02772.
[09.01.2025 14:10] Extra JSON file exists (./assets/json/2501.02772.json), skip PDF parsing.
[09.01.2025 14:10] Paper image links file exists (./assets/img_data/2501.02772.json), skip HTML parsing.
[09.01.2025 14:10] Success.
[09.01.2025 14:10] Downloading and parsing paper https://huggingface.co/papers/2501.04144.
[09.01.2025 14:10] Extra JSON file exists (./assets/json/2501.04144.json), skip PDF parsing.
[09.01.2025 14:10] Paper image links file exists (./assets/img_data/2501.04144.json), skip HTML parsing.
[09.01.2025 14:10] Success.
[09.01.2025 14:10] Downloading and parsing paper https://huggingface.co/papers/2501.04689.
[09.01.2025 14:10] Extra JSON file exists (./assets/json/2501.04689.json), skip PDF parsing.
[09.01.2025 14:10] Paper image links file exists (./assets/img_data/2501.04689.json), skip HTML parsing.
[09.01.2025 14:10] Success.
[09.01.2025 14:10] Downloading and parsing paper https://huggingface.co/papers/2501.04694.
[09.01.2025 14:10] Extra JSON file exists (./assets/json/2501.04694.json), skip PDF parsing.
[09.01.2025 14:10] Paper image links file exists (./assets/img_data/2501.04694.json), skip HTML parsing.
[09.01.2025 14:10] Success.
[09.01.2025 14:10] Downloading and parsing paper https://huggingface.co/papers/2501.03271.
[09.01.2025 14:10] Extra JSON file exists (./assets/json/2501.03271.json), skip PDF parsing.
[09.01.2025 14:10] Paper image links file exists (./assets/img_data/2501.03271.json), skip HTML parsing.
[09.01.2025 14:10] Success.
[09.01.2025 14:10] Enriching papers with extra data.
[09.01.2025 14:10] ********************************************************************************
[09.01.2025 14:10] Abstract 0. We present rStar-Math to demonstrate that small language models (SLMs) can rival or even surpass the math reasoning capability of OpenAI o1, without distillation from superior models. rStar-Math achieves this by exercising "deep thinking" through Monte Carlo Tree Search (MCTS), where a math policy S...
[09.01.2025 14:10] ********************************************************************************
[09.01.2025 14:10] Abstract 1. Chain-of-thought (CoT) reasoning has been widely applied in the mathematical reasoning of Large Language Models (LLMs). Recently, the introduction of derivative process supervision on CoT trajectories has sparked discussions on enhancing scaling capabilities during test time, thereby boosting the po...
[09.01.2025 14:10] ********************************************************************************
[09.01.2025 14:10] Abstract 2. Historically, scientific discovery has been a lengthy and costly process, demanding substantial time and resources from initial conception to final results. To accelerate scientific discovery, reduce research costs, and improve research quality, we introduce Agent Laboratory, an autonomous LLM-based...
[09.01.2025 14:10] ********************************************************************************
[09.01.2025 14:10] Abstract 3. We propose a novel framework, Meta Chain-of-Thought (Meta-CoT), which extends traditional Chain-of-Thought (CoT) by explicitly modeling the underlying reasoning required to arrive at a particular CoT. We present empirical evidence from state-of-the-art models exhibiting behaviors consistent with in-...
[09.01.2025 14:10] ********************************************************************************
[09.01.2025 14:10] Abstract 4. In recent years, the rapid advancement of Large Language Models (LLMs) has transformed the landscape of scientific research, offering unprecedented support across various stages of the research cycle. This paper presents the first systematic survey dedicated to exploring how LLMs are revolutionizing...
[09.01.2025 14:10] ********************************************************************************
[09.01.2025 14:10] Abstract 5. Graphical User Interface (GUI) Agents, powered by multimodal large language models (MLLMs), have shown great potential for task automation on computing devices such as computers and mobile phones. However, existing agents face challenges in multi-step reasoning and reliance on textual annotations, l...
[09.01.2025 14:10] ********************************************************************************
[09.01.2025 14:10] Abstract 6. Document retrieval techniques form the foundation for the development of large-scale information systems. The prevailing methodology is to construct a bi-encoder and compute the semantic similarity. However, such scalar similarity is difficult to reflect enough information and impedes our comprehens...
[09.01.2025 14:10] ********************************************************************************
[09.01.2025 14:10] Abstract 7. In this paper, we push the boundaries of fine-grained 3D generation into truly creative territory. Current methods either lack intricate details or simply mimic existing objects -- we enable both. By lifting 2D fine-grained understanding into 3D through multi-view diffusion and modeling part latents...
[09.01.2025 14:10] ********************************************************************************
[09.01.2025 14:10] Abstract 8. We study the problem of single-image 3D object reconstruction. Recent works have diverged into two directions: regression-based modeling and generative modeling. Regression methods efficiently infer visible surfaces, but struggle with occluded regions. Generative methods handle uncertain regions bet...
[09.01.2025 14:10] ********************************************************************************
[09.01.2025 14:10] Abstract 9. Effective instruction tuning is indispensable for optimizing code LLMs, aligning model behavior with user expectations and enhancing model performance in real-world applications. However, most existing methods focus on code snippets, which are limited to specific functionalities and rigid structures...
[09.01.2025 14:10] ********************************************************************************
[09.01.2025 14:10] Abstract 10. The rapid rise of large language models (LLMs) has unlocked many applications but also underscores the challenge of aligning them with diverse values and preferences. Direct Preference Optimization (DPO) is central to alignment but constrained by fixed divergences and limited feature transformations...
[09.01.2025 14:10] Read previous papers.
[09.01.2025 14:10] Generating reviews via LLM API.
[09.01.2025 14:10] Using data from previous issue: {"categories": ["#training", "#reasoning", "#optimization", "#benchmark", "#small_models", "#dataset"], "emoji": "üßÆ", "ru": {"title": "–ú–∞–ª—ã–µ –º–æ–¥–µ–ª–∏ —Ä–µ—à–∞—é—Ç –±–æ–ª—å—à–∏–µ –∑–∞–¥–∞—á–∏: rStar-Math –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –≥–∏–≥–∞–Ω—Ç–æ–≤ –≤ –º–∞—Ç–µ–º–∞—Ç–∏–∫–µ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç rStar-Math - –ø–æ–¥—Ö–æ–¥, –ø–æ–∑–≤–æ–ª—è—é—â–∏–π –º–∞–ª—ã–º —è–∑—ã–∫–æ–≤—ã–º –º–æ–¥–µ–ª
[09.01.2025 14:10] Using data from previous issue: {"categories": ["#dataset", "#training", "#multimodal", "#data", "#open_source", "#reasoning", "#math", "#architecture", "#benchmark"], "emoji": "üß†", "ru": {"title": "–£—Å–∏–ª–µ–Ω–∏–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —á–µ—Ä–µ–∑ —Å–∏–Ω—Ç–µ–∑ –¥–∞–Ω–Ω—ã—Ö –∏ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—é", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É
[09.01.2025 14:10] Using data from previous issue: {"categories": ["#science", "#training", "#agents", "#rlhf", "#survey"], "emoji": "üß™", "ru": {"title": "–ê–≤—Ç–æ–Ω–æ–º–Ω–∞—è –ª–∞–±–æ—Ä–∞—Ç–æ—Ä–∏—è –ò–ò: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ –Ω–∞—É—á–Ω—ã—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Agent Laboratory - –∞–≤—Ç–æ–Ω–æ–º–Ω—É—é —Å–∏—Å—Ç–µ–º—É –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–æ–¥–µ–ª–µ–π LLM, —Å–ø–æ—Å–æ–±–Ω—É—é –≤—ã–ø–æ–ª–Ω—è—Ç—å –ø–æ–ª–Ω—ã–π —Ü–∏–∫–ª –Ω–∞—É—á–Ω–æ–≥–æ –∏—Å
[09.01.2025 14:10] Using data from previous issue: {"categories": ["#synthetic", "#training", "#rlhf", "#rl", "#multimodal", "#optimization", "#reasoning"], "emoji": "üß†", "ru": {"title": "Meta-CoT: –Ω–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –¥–ª—è –ò–ò", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –Ω–æ–≤—É—é –∫–æ–Ω—Ü–µ–ø—Ü–∏—é –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Meta Chain-of-Thought (Meta-CoT), –∫–æ—Ç–æ—Ä–∞—è —Ä–∞—Å—à–∏—Ä—è–µ—Ç —Ç—Ä
[09.01.2025 14:10] Using data from previous issue: {"categories": ["#science", "#survey", "#multimodal", "#benchmark"], "emoji": "üß†", "ru": {"title": "LLM –∫–∞–∫ —Ä–µ–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –≤ –Ω–∞—É—á–Ω—ã—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è—Ö", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –ø–µ—Ä–≤—ã–π —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –æ–±–∑–æ—Ä —Ä–æ–ª–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≤ –Ω–∞—É—á–Ω—ã—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è—Ö. –ê–≤—Ç–æ—Ä—ã –∞–Ω–∞–ª–∏
[09.01.2025 14:10] Using data from previous issue: {"categories": ["#benchmark", "#synthetic", "#training", "#agents", "#multimodal", "#reasoning"], "emoji": "ü§ñ", "ru": {"title": "–£–º–Ω—ã–π –∞–≥–µ–Ω—Ç GUI: –Ω–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–≤", "desc": "InfiGUIAgent - —ç—Ç–æ –∞–≥–µ–Ω—Ç –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à
[09.01.2025 14:10] Using data from previous issue: {"categories": ["#interpretability", "#data", "#rag", "#synthetic", "#dataset"], "emoji": "üîç", "ru": {"title": "GeAR: –ù–æ–≤—ã–π –≤–∑–≥–ª—è–¥ –Ω–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Generation Augmented Retrieval (GeAR). –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç
[09.01.2025 14:10] Using data from previous issue: {"categories": ["#diffusion", "#open_source", "#3d"], "emoji": "üê¶", "ru": {"title": "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫—Ä–µ–∞—Ç–∏–≤–Ω—ã—Ö 3D-–º–æ–¥–µ–ª–µ–π —Å –±–µ—Å–ø—Ä–µ—Ü–µ–¥–µ–Ω—Ç–Ω–æ–π –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–µ–π", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–µ—Ç–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö 3D-–æ–±—ä–µ–∫—Ç–æ–≤, –≤—ã—Ö–æ–¥—è—â–∏–π –∑–∞ —Ä–∞–º–∫–∏ –ø—Ä–æ—Å—Ç–æ–≥–æ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤. –ê–≤
[09.01.2025 14:10] Using data from previous issue: {"categories": ["#3d"], "emoji": "üßä", "ru": {"title": "SPAR3D: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—è 3D-–æ–±—ä–µ–∫—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –æ–±–ª–∞–∫–æ–≤ —Ç–æ—á–µ–∫", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–π –ø–æ–¥—Ö–æ–¥ SPAR3D –¥–ª—è —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ 3D-–æ–±—ä–µ–∫—Ç–æ–≤ –ø–æ –æ–¥–Ω–æ–º—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é. –ù–∞ –ø–µ—Ä–≤–æ–º —ç—Ç–∞–ø–µ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç—Å—è —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–µ –æ–±–ª–∞–∫–æ —Ç–æ—á
[09.01.2025 14:10] Using data from previous issue: {"categories": ["#dataset", "#data", "#synthetic", "#training", "#optimization", "#alignment", "#architecture"], "emoji": "üå≥", "ru": {"title": "–î–µ—Ä–µ–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: –Ω–æ–≤—ã–π –ø—É—Ç—å –∫ —É–ª—É—á—à–µ–Ω–∏—é —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –∫–æ–¥–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤
[09.01.2025 14:10] Using data from previous issue: {"categories": ["#rlhf", "#alignment", "#reasoning", "#dataset", "#training"], "emoji": "üß†", "ru": {"title": "DPO-Kernels: –ù–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—é —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º DPO-Kernels –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) —Å —Ä–∞–∑
[09.01.2025 14:10] Loading Chinese text from previous data.
[09.01.2025 14:10] Renaming data file.
[09.01.2025 14:10] Renaming previous data. hf_papers.json to ./d/2025-01-09.json
[09.01.2025 14:10] Saving new data file.
[09.01.2025 14:10] Generating page.
[09.01.2025 14:10] Renaming previous page.
[09.01.2025 14:10] Renaming previous data. index.html to ./d/2025-01-09.html
[09.01.2025 14:10] [Experimental] Generating Chinese page for reading.
[09.01.2025 14:10] Chinese vocab [{'word': 'Â±ïÁ§∫', 'pinyin': 'zh«énsh√¨', 'trans': 'display, show'}, {'word': 'Â∞èÂûã', 'pinyin': 'xi«éox√≠ng', 'trans': 'small, mini'}, {'word': 'ËØ≠Ë®ÄÊ®°Âûã', 'pinyin': 'y«îy√°n m√≥x√≠ng', 'trans': 'language model'}, {'word': 'ËíôÁâπÂç°ÁΩóÊ†ëÊêúÁ¥¢', 'pinyin': 'M√©ngt√®k«élu√≥ sh√π s≈çusu«í', 'trans': 'Monte Carlo Tree Search'}, {'word': 'Ê∑±Â∫¶ÊÄùËÄÉ', 'pinyin': 'shƒìnd√π sƒ´k«éo', 'trans': 'deep thinking'}, {'word': 'Â™≤Áæé', 'pinyin': 'p√¨mƒõi', 'trans': 'rival, match'}, {'word': 'Ë∂ÖË∂ä', 'pinyin': 'chƒÅoyu√®', 'trans': 'surpass, exceed'}, {'word': 'Êé®ÁêÜ', 'pinyin': 'tuƒ´l«ê', 'trans': 'reasoning'}, {'word': 'ËÉΩÂäõ', 'pinyin': 'n√©ngl√¨', 'trans': 'ability, capability'}, {'word': 'ÂàõÊñ∞', 'pinyin': 'chu√†ngxƒ´n', 'trans': 'innovation'}, {'word': 'ËÆ≠ÁªÉ', 'pinyin': 'x√πnli√†n', 'trans': 'train, training'}, {'word': '‰ª£Á†Å', 'pinyin': 'd√†im«é', 'trans': 'code'}, {'word': 'Â¢ûÂº∫', 'pinyin': 'zƒìngqi√°ng', 'trans': 'enhance, strengthen'}, {'word': 'ÊÄùÁª¥Èìæ', 'pinyin': 'sƒ´w√©i li√°n', 'trans': 'chain of thought'}, {'word': 'Êï∞ÊçÆÂêàÊàê', 'pinyin': 'sh√πj√π h√©ch√©ng', 'trans': 'data synthesis'}, {'word': 'ÊñπÊ≥ï', 'pinyin': 'fƒÅngf«é', 'trans': 'method'}, {'word': 'ËøáÁ®ã', 'pinyin': 'gu√≤ch√©ng', 'trans': 'process'}, {'word': 'ÂÅèÂ•Ω', 'pinyin': 'piƒÅnh√†o', 'trans': 'preference'}, {'word': 'Ê®°Âûã', 'pinyin': 'm√≥x√≠ng', 'trans': 'model'}, {'word': 'Ëá™ÊàëËøõÂåñ', 'pinyin': 'z√¨w«í j√¨nhu√†', 'trans': 'self-evolution'}, {'word': 'ÈÖçÊñπ', 'pinyin': 'p√®ifƒÅng', 'trans': 'formula, recipe'}, {'word': 'ÂõõËΩÆ', 'pinyin': 's√¨ l√∫n', 'trans': 'four rounds'}, {'word': 'ÊúÄÂÖàËøõ', 'pinyin': 'zu√¨ xiƒÅnj√¨n', 'trans': 'most advanced'}]
[09.01.2025 14:10] Renaming previous Chinese page.
[09.01.2025 14:10] Renaming previous data. zh.html to ./d/2025-01-08_zh_reading_task.html
[09.01.2025 14:10] Writing Chinese reading task.
[09.01.2025 14:10] Writing result.
[09.01.2025 14:10] Renaming log file.
[09.01.2025 14:10] Renaming previous data. log.txt to ./logs/2025-01-09_last_log.txt
