[20.05.2025 15:13] Read previous papers.
[20.05.2025 15:13] Generating top page (month).
[20.05.2025 15:13] Writing top page (month).
[20.05.2025 16:14] Read previous papers.
[20.05.2025 16:14] Get feed.
[20.05.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2505.11820
[20.05.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2505.13417
[20.05.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2505.11896
[20.05.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2505.11254
[20.05.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2505.13227
[20.05.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2505.13379
[20.05.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2505.13308
[20.05.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2505.13427
[20.05.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2505.13215
[20.05.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2505.12805
[20.05.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2505.12504
[20.05.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2505.12082
[20.05.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2505.12992
[20.05.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2505.13389
[20.05.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2505.11932
[20.05.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2505.12346
[20.05.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2505.12081
[20.05.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2505.13444
[20.05.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2505.13180
[20.05.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2505.11855
[20.05.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2505.12849
[20.05.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2505.12058
[20.05.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2505.13437
[20.05.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2505.10238
[20.05.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2505.12996
[20.05.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2505.12120
[20.05.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2505.11497
[20.05.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2505.11484
[20.05.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2505.12872
[20.05.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2505.11475
[20.05.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2505.12257
[20.05.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2505.11988
[20.05.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2505.10420
[20.05.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2505.03332
[20.05.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2505.12973
[20.05.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2505.10831
[20.05.2025 16:14] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[20.05.2025 16:14] No deleted papers detected.
[20.05.2025 16:14] Downloading and parsing papers (pdf, html). Total: 36.
[20.05.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2505.11820.
[20.05.2025 16:14] Extra JSON file exists (./assets/json/2505.11820.json), skip PDF parsing.
[20.05.2025 16:14] Paper image links file exists (./assets/img_data/2505.11820.json), skip HTML parsing.
[20.05.2025 16:14] Success.
[20.05.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2505.13417.
[20.05.2025 16:14] Extra JSON file exists (./assets/json/2505.13417.json), skip PDF parsing.
[20.05.2025 16:14] Paper image links file exists (./assets/img_data/2505.13417.json), skip HTML parsing.
[20.05.2025 16:14] Success.
[20.05.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2505.11896.
[20.05.2025 16:14] Extra JSON file exists (./assets/json/2505.11896.json), skip PDF parsing.
[20.05.2025 16:14] Paper image links file exists (./assets/img_data/2505.11896.json), skip HTML parsing.
[20.05.2025 16:14] Success.
[20.05.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2505.11254.
[20.05.2025 16:14] Extra JSON file exists (./assets/json/2505.11254.json), skip PDF parsing.
[20.05.2025 16:14] Paper image links file exists (./assets/img_data/2505.11254.json), skip HTML parsing.
[20.05.2025 16:14] Success.
[20.05.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2505.13227.
[20.05.2025 16:14] Extra JSON file exists (./assets/json/2505.13227.json), skip PDF parsing.
[20.05.2025 16:14] Paper image links file exists (./assets/img_data/2505.13227.json), skip HTML parsing.
[20.05.2025 16:14] Success.
[20.05.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2505.13379.
[20.05.2025 16:14] Extra JSON file exists (./assets/json/2505.13379.json), skip PDF parsing.
[20.05.2025 16:14] Paper image links file exists (./assets/img_data/2505.13379.json), skip HTML parsing.
[20.05.2025 16:14] Success.
[20.05.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2505.13308.
[20.05.2025 16:14] Extra JSON file exists (./assets/json/2505.13308.json), skip PDF parsing.
[20.05.2025 16:14] Paper image links file exists (./assets/img_data/2505.13308.json), skip HTML parsing.
[20.05.2025 16:14] Success.
[20.05.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2505.13427.
[20.05.2025 16:14] Extra JSON file exists (./assets/json/2505.13427.json), skip PDF parsing.
[20.05.2025 16:14] Paper image links file exists (./assets/img_data/2505.13427.json), skip HTML parsing.
[20.05.2025 16:14] Success.
[20.05.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2505.13215.
[20.05.2025 16:14] Extra JSON file exists (./assets/json/2505.13215.json), skip PDF parsing.
[20.05.2025 16:14] Paper image links file exists (./assets/img_data/2505.13215.json), skip HTML parsing.
[20.05.2025 16:14] Success.
[20.05.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2505.12805.
[20.05.2025 16:14] Extra JSON file exists (./assets/json/2505.12805.json), skip PDF parsing.
[20.05.2025 16:14] Paper image links file exists (./assets/img_data/2505.12805.json), skip HTML parsing.
[20.05.2025 16:14] Success.
[20.05.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2505.12504.
[20.05.2025 16:14] Extra JSON file exists (./assets/json/2505.12504.json), skip PDF parsing.
[20.05.2025 16:14] Paper image links file exists (./assets/img_data/2505.12504.json), skip HTML parsing.
[20.05.2025 16:14] Success.
[20.05.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2505.12082.
[20.05.2025 16:14] Extra JSON file exists (./assets/json/2505.12082.json), skip PDF parsing.
[20.05.2025 16:14] Paper image links file exists (./assets/img_data/2505.12082.json), skip HTML parsing.
[20.05.2025 16:14] Success.
[20.05.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2505.12992.
[20.05.2025 16:14] Extra JSON file exists (./assets/json/2505.12992.json), skip PDF parsing.
[20.05.2025 16:14] Paper image links file exists (./assets/img_data/2505.12992.json), skip HTML parsing.
[20.05.2025 16:14] Success.
[20.05.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2505.13389.
[20.05.2025 16:14] Extra JSON file exists (./assets/json/2505.13389.json), skip PDF parsing.
[20.05.2025 16:14] Paper image links file exists (./assets/img_data/2505.13389.json), skip HTML parsing.
[20.05.2025 16:14] Success.
[20.05.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2505.11932.
[20.05.2025 16:14] Extra JSON file exists (./assets/json/2505.11932.json), skip PDF parsing.
[20.05.2025 16:14] Paper image links file exists (./assets/img_data/2505.11932.json), skip HTML parsing.
[20.05.2025 16:14] Success.
[20.05.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2505.12346.
[20.05.2025 16:14] Extra JSON file exists (./assets/json/2505.12346.json), skip PDF parsing.
[20.05.2025 16:14] Paper image links file exists (./assets/img_data/2505.12346.json), skip HTML parsing.
[20.05.2025 16:14] Success.
[20.05.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2505.12081.
[20.05.2025 16:14] Extra JSON file exists (./assets/json/2505.12081.json), skip PDF parsing.
[20.05.2025 16:14] Paper image links file exists (./assets/img_data/2505.12081.json), skip HTML parsing.
[20.05.2025 16:14] Success.
[20.05.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2505.13444.
[20.05.2025 16:14] Extra JSON file exists (./assets/json/2505.13444.json), skip PDF parsing.
[20.05.2025 16:14] Paper image links file exists (./assets/img_data/2505.13444.json), skip HTML parsing.
[20.05.2025 16:14] Success.
[20.05.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2505.13180.
[20.05.2025 16:14] Extra JSON file exists (./assets/json/2505.13180.json), skip PDF parsing.
[20.05.2025 16:14] Paper image links file exists (./assets/img_data/2505.13180.json), skip HTML parsing.
[20.05.2025 16:14] Success.
[20.05.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2505.11855.
[20.05.2025 16:14] Extra JSON file exists (./assets/json/2505.11855.json), skip PDF parsing.
[20.05.2025 16:14] Paper image links file exists (./assets/img_data/2505.11855.json), skip HTML parsing.
[20.05.2025 16:14] Success.
[20.05.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2505.12849.
[20.05.2025 16:14] Extra JSON file exists (./assets/json/2505.12849.json), skip PDF parsing.
[20.05.2025 16:14] Paper image links file exists (./assets/img_data/2505.12849.json), skip HTML parsing.
[20.05.2025 16:14] Success.
[20.05.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2505.12058.
[20.05.2025 16:14] Extra JSON file exists (./assets/json/2505.12058.json), skip PDF parsing.
[20.05.2025 16:14] Paper image links file exists (./assets/img_data/2505.12058.json), skip HTML parsing.
[20.05.2025 16:14] Success.
[20.05.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2505.13437.
[20.05.2025 16:14] Extra JSON file exists (./assets/json/2505.13437.json), skip PDF parsing.
[20.05.2025 16:14] Paper image links file exists (./assets/img_data/2505.13437.json), skip HTML parsing.
[20.05.2025 16:14] Success.
[20.05.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2505.10238.
[20.05.2025 16:14] Extra JSON file exists (./assets/json/2505.10238.json), skip PDF parsing.
[20.05.2025 16:14] Paper image links file exists (./assets/img_data/2505.10238.json), skip HTML parsing.
[20.05.2025 16:14] Success.
[20.05.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2505.12996.
[20.05.2025 16:14] Extra JSON file exists (./assets/json/2505.12996.json), skip PDF parsing.
[20.05.2025 16:14] Paper image links file exists (./assets/img_data/2505.12996.json), skip HTML parsing.
[20.05.2025 16:14] Success.
[20.05.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2505.12120.
[20.05.2025 16:14] Extra JSON file exists (./assets/json/2505.12120.json), skip PDF parsing.
[20.05.2025 16:14] Paper image links file exists (./assets/img_data/2505.12120.json), skip HTML parsing.
[20.05.2025 16:14] Success.
[20.05.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2505.11497.
[20.05.2025 16:14] Extra JSON file exists (./assets/json/2505.11497.json), skip PDF parsing.
[20.05.2025 16:14] Paper image links file exists (./assets/img_data/2505.11497.json), skip HTML parsing.
[20.05.2025 16:14] Success.
[20.05.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2505.11484.
[20.05.2025 16:14] Extra JSON file exists (./assets/json/2505.11484.json), skip PDF parsing.
[20.05.2025 16:14] Paper image links file exists (./assets/img_data/2505.11484.json), skip HTML parsing.
[20.05.2025 16:14] Success.
[20.05.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2505.12872.
[20.05.2025 16:14] Extra JSON file exists (./assets/json/2505.12872.json), skip PDF parsing.
[20.05.2025 16:14] Paper image links file exists (./assets/img_data/2505.12872.json), skip HTML parsing.
[20.05.2025 16:14] Success.
[20.05.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2505.11475.
[20.05.2025 16:14] Extra JSON file exists (./assets/json/2505.11475.json), skip PDF parsing.
[20.05.2025 16:14] Paper image links file exists (./assets/img_data/2505.11475.json), skip HTML parsing.
[20.05.2025 16:14] Success.
[20.05.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2505.12257.
[20.05.2025 16:14] Extra JSON file exists (./assets/json/2505.12257.json), skip PDF parsing.
[20.05.2025 16:14] Paper image links file exists (./assets/img_data/2505.12257.json), skip HTML parsing.
[20.05.2025 16:14] Success.
[20.05.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2505.11988.
[20.05.2025 16:14] Extra JSON file exists (./assets/json/2505.11988.json), skip PDF parsing.
[20.05.2025 16:14] Paper image links file exists (./assets/img_data/2505.11988.json), skip HTML parsing.
[20.05.2025 16:14] Success.
[20.05.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2505.10420.
[20.05.2025 16:14] Extra JSON file exists (./assets/json/2505.10420.json), skip PDF parsing.
[20.05.2025 16:14] Paper image links file exists (./assets/img_data/2505.10420.json), skip HTML parsing.
[20.05.2025 16:14] Success.
[20.05.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2505.03332.
[20.05.2025 16:14] Extra JSON file exists (./assets/json/2505.03332.json), skip PDF parsing.
[20.05.2025 16:14] Paper image links file exists (./assets/img_data/2505.03332.json), skip HTML parsing.
[20.05.2025 16:14] Success.
[20.05.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2505.12973.
[20.05.2025 16:14] Extra JSON file exists (./assets/json/2505.12973.json), skip PDF parsing.
[20.05.2025 16:14] Paper image links file exists (./assets/img_data/2505.12973.json), skip HTML parsing.
[20.05.2025 16:14] Success.
[20.05.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2505.10831.
[20.05.2025 16:14] Extra JSON file exists (./assets/json/2505.10831.json), skip PDF parsing.
[20.05.2025 16:14] Paper image links file exists (./assets/img_data/2505.10831.json), skip HTML parsing.
[20.05.2025 16:14] Success.
[20.05.2025 16:14] Enriching papers with extra data.
[20.05.2025 16:14] ********************************************************************************
[20.05.2025 16:14] Abstract 0. In this paper, we propose a novel learning paradigm, termed Chain-of-Model (CoM), which incorporates the causal relationship into the hidden states of each layer as a chain style, thereby introducing great scaling efficiency in model training and inference flexibility in deployment. We introduce the...
[20.05.2025 16:14] ********************************************************************************
[20.05.2025 16:14] Abstract 1. Recently, large reasoning models have achieved impressive performance on various tasks by employing human-like deep thinking. However, the lengthy thinking process substantially increases inference overhead, making efficiency a critical bottleneck. In this work, we first demonstrate that NoThinking,...
[20.05.2025 16:14] ********************************************************************************
[20.05.2025 16:14] Abstract 2. Large Language Models (LLMs) have demonstrated remarkable capabilities but often face challenges with tasks requiring sophisticated reasoning. While Chain-of-Thought (CoT) prompting significantly enhances reasoning, it indiscriminately generates lengthy reasoning steps for all queries, leading to su...
[20.05.2025 16:14] ********************************************************************************
[20.05.2025 16:14] Abstract 3. The attention mechanism of a transformer has a quadratic complexity, leading to high inference costs and latency for long sequences. However, attention matrices are mostly sparse, which implies that many entries may be omitted from computation for efficient inference. Sparse attention inference meth...
[20.05.2025 16:14] ********************************************************************************
[20.05.2025 16:14] Abstract 4. Graphical user interface (GUI) grounding, the ability to map natural language instructions to specific actions on graphical user interfaces, remains a critical bottleneck in computer use agent development. Current benchmarks oversimplify grounding tasks as short referring expressions, failing to cap...
[20.05.2025 16:14] ********************************************************************************
[20.05.2025 16:14] Abstract 5. Reasoning Language Models, capable of extended chain-of-thought reasoning, have demonstrated remarkable performance on tasks requiring complex logical inference. However, applying elaborate reasoning for all queries often results in substantial computational inefficiencies, particularly when many pr...
[20.05.2025 16:14] ********************************************************************************
[20.05.2025 16:14] Abstract 6. Reasoning ability, a core component of human intelligence, continues to pose a significant challenge for Large Language Models (LLMs) in the pursuit of AGI. Although model performance has improved under the training scaling law, significant challenges remain, particularly with respect to training al...
[20.05.2025 16:14] ********************************************************************************
[20.05.2025 16:14] Abstract 7. While Multimodal Large Language Models (MLLMs) have achieved impressive progress in vision-language understanding, they still struggle with complex multi-step reasoning, often producing logically inconsistent or partially correct solutions. A key limitation lies in the lack of fine-grained supervisi...
[20.05.2025 16:14] ********************************************************************************
[20.05.2025 16:14] Abstract 8. Recent advancements in dynamic 3D scene reconstruction have shown promising results, enabling high-fidelity 3D novel view synthesis with improved temporal consistency. Among these, 4D Gaussian Splatting (4DGS) has emerged as an appealing approach due to its ability to model high-fidelity spatial and...
[20.05.2025 16:14] ********************************************************************************
[20.05.2025 16:14] Abstract 9. Low-Rank Adaptation (LoRA), which introduces a product of two trainable low-rank matrices into frozen pre-trained weights, is widely used for efficient fine-tuning of language models in federated learning (FL). However, when combined with differentially private stochastic gradient descent (DP-SGD), ...
[20.05.2025 16:14] ********************************************************************************
[20.05.2025 16:14] Abstract 10. Recent advances in rule-based reinforcement learning (RL) have significantly improved the reasoning capability of language models (LMs) with rule-based rewards. However, existing RL methods -- such as GRPO, REINFORCE++, and RLOO -- often suffer from training instability, where large policy updates a...
[20.05.2025 16:14] ********************************************************************************
[20.05.2025 16:14] Abstract 11. Model merging has emerged as a promising technique for enhancing large language models, though its application in large-scale pre-training remains relatively unexplored. In this paper, we present a comprehensive investigation of model merging techniques during the pre-training process. Through exten...
[20.05.2025 16:14] ********************************************************************************
[20.05.2025 16:14] Abstract 12. Inference-time scaling techniques have significantly bolstered the reasoning capabilities of large language models (LLMs) by harnessing additional computational effort at inference without retraining. Similarly, Chain-of-Thought (CoT) prompting and its extension, Long CoT, improve accuracy by genera...
[20.05.2025 16:14] ********************************************************************************
[20.05.2025 16:14] Abstract 13. Scaling video diffusion transformers (DiTs) is limited by their quadratic 3D attention, even though most of the attention mass concentrates on a small subset of positions. We turn this observation into VSA, a trainable, hardware-efficient sparse attention that replaces full attention at both trainin...
[20.05.2025 16:14] ********************************************************************************
[20.05.2025 16:14] Abstract 14. Precise recognition of search intent in Retrieval-Augmented Generation (RAG) systems remains a challenging goal, especially under resource constraints and for complex queries with nested structures and dependencies. This paper presents QCompiler, a neuro-symbolic framework inspired by linguistic gra...
[20.05.2025 16:14] ********************************************************************************
[20.05.2025 16:14] Abstract 15. Large language models (LLMs) exhibit varying levels of confidence across input prompts (questions): some lead to consistent, semantically similar answers, while others yield diverse or contradictory outputs. This variation reflects LLM's uncertainty about the input prompt, a signal of how confidentl...
[20.05.2025 16:14] ********************************************************************************
[20.05.2025 16:14] Abstract 16. Large vision-language models exhibit inherent capabilities to handle diverse visual perception tasks. In this paper, we introduce VisionReasoner, a unified framework capable of reasoning and solving multiple visual perception tasks within a shared model. Specifically, by designing novel multi-object...
[20.05.2025 16:14] ********************************************************************************
[20.05.2025 16:14] Abstract 17. Chart understanding presents a unique challenge for large vision-language models (LVLMs), as it requires the integration of sophisticated textual and visual reasoning capabilities. However, current LVLMs exhibit a notable imbalance between these skills, falling short on visual reasoning that is diff...
[20.05.2025 16:14] ********************************************************************************
[20.05.2025 16:14] Abstract 18. Integrating Large Language Models with symbolic planners is a promising direction for obtaining verifiable and grounded plans compared to planning in natural language, with recent works extending this idea to visual domains using Vision-Language Models (VLMs). However, rigorous comparison between VL...
[20.05.2025 16:14] ********************************************************************************
[20.05.2025 16:14] Abstract 19. Recent advances in large language models (LLMs) have fueled the vision of automated scientific discovery, often called AI Co-Scientists. To date, prior work casts these systems as generative co-authors responsible for crafting hypotheses, synthesizing code, or drafting manuscripts. In this work, we ...
[20.05.2025 16:14] ********************************************************************************
[20.05.2025 16:14] Abstract 20. Image generation models have achieved widespread applications. As an instance, the TarFlow model combines the transformer architecture with Normalizing Flow models, achieving state-of-the-art results on multiple benchmarks. However, due to the causal form of attention requiring sequential computatio...
[20.05.2025 16:14] ********************************************************************************
[20.05.2025 16:14] Abstract 21. Tiny QA Benchmark++ (TQB++) presents an ultra-lightweight, multilingual smoke-test suite designed to give large-language-model (LLM) pipelines a unit-test style safety net dataset that runs in seconds with minimal cost. Born out of the tight feedback-loop demands building the Comet Opik prompt-optim...
[20.05.2025 16:14] ********************************************************************************
[20.05.2025 16:14] Abstract 22. Despite significant advances in video generation, synthesizing physically plausible human actions remains a persistent challenge, particularly in modeling fine-grained semantics and complex temporal dynamics. For instance, generating gymnastics routines such as "switch leap with 0.5 turn" poses subs...
[20.05.2025 16:14] ********************************************************************************
[20.05.2025 16:14] Abstract 23. Human image animation has gained increasing attention and developed rapidly due to its broad applications in digital humans. However, existing methods rely largely on 2D-rendered pose images for motion guidance, which limits generalization and discards essential 3D information for open-world animati...
[20.05.2025 16:14] ********************************************************************************
[20.05.2025 16:14] Abstract 24. In recent years, the emergence of large reasoning models (LRMs), such as OpenAI-o1 and DeepSeek-R1, has shown impressive capabilities in complex problems, e.g., mathematics and coding. Some pioneering studies attempt to bring the success of LRMs in neural machine translation (MT). They try to build ...
[20.05.2025 16:14] ********************************************************************************
[20.05.2025 16:14] Abstract 25. Recent advancements in Digital Pathology (DP), particularly through artificial intelligence and Foundation Models, have underscored the importance of large-scale, diverse, and richly annotated datasets. Despite their critical role, publicly available Whole Slide Image (WSI) datasets often lack suffi...
[20.05.2025 16:14] ********************************************************************************
[20.05.2025 16:14] Abstract 26. Video diffusion models (DMs) have enabled high-quality video synthesis. Yet, their substantial computational and memory demands pose serious challenges to real-world deployment, even on high-end GPUs. As a commonly adopted solution, quantization has proven notable success in reducing cost for image ...
[20.05.2025 16:14] ********************************************************************************
[20.05.2025 16:14] Abstract 27. Test-Time Scaling (TTS) refers to approaches that improve reasoning performance by allocating extra computation during inference, without altering the model's parameters. While existing TTS methods operate in a discrete token space by generating more intermediate steps, recent studies in Coconut and...
[20.05.2025 16:14] ********************************************************************************
[20.05.2025 16:14] Abstract 28. Early cavemen relied on gestures, vocalizations, and simple signals to coordinate, plan, avoid predators, and share resources. Today, humans collaborate using complex languages to achieve remarkable results. What drives this evolution in communication? How does language emerge, adapt, and become vit...
[20.05.2025 16:14] ********************************************************************************
[20.05.2025 16:14] Abstract 29. Preference datasets are essential for training general-domain, instruction-following language models with Reinforcement Learning from Human Feedback (RLHF). Each subsequent data release raises expectations for future data collection, meaning there is a constant need to advance the quality and divers...
[20.05.2025 16:14] ********************************************************************************
[20.05.2025 16:14] Abstract 30. Identifying subtle technical errors within complex scientific and technical documents, especially those requiring multimodal interpretation (e.g., formulas in images), presents a significant hurdle for Large Language Models (LLMs) whose inherent error-correction tendencies can mask inaccuracies. Thi...
[20.05.2025 16:14] ********************************************************************************
[20.05.2025 16:14] Abstract 31. Accurately identifying adversarial techniques in security texts is critical for effective cyber defense. However, existing methods face a fundamental trade-off: they either rely on generic models with limited domain precision or require resource-intensive pipelines that depend on large labeled datas...
[20.05.2025 16:14] ********************************************************************************
[20.05.2025 16:14] Abstract 32. The Image Signal Processor (ISP) is a fundamental component in modern smartphone cameras responsible for conversion of RAW sensor image data to RGB images with a strong focus on perceptual quality. Recent work highlights the potential of deep learning approaches and their ability to capture details ...
[20.05.2025 16:14] ********************************************************************************
[20.05.2025 16:14] Abstract 33. Critical peer review of scientific manuscripts presents a significant challenge for Large Language Models (LLMs), partly due to data limitations and the complexity of expert reasoning. This report introduces Persistent Workflow Prompting (PWP), a potentially broadly applicable prompt engineering met...
[20.05.2025 16:14] ********************************************************************************
[20.05.2025 16:14] Abstract 34. Homograph disambiguation remains a significant challenge in grapheme-to-phoneme (G2P) conversion, especially for low-resource languages. This challenge is twofold: (1) creating balanced and comprehensive homograph datasets is labor-intensive and costly, and (2) specific disambiguation strategies int...
[20.05.2025 16:14] ********************************************************************************
[20.05.2025 16:14] Abstract 35. Human-computer interaction has long imagined technology that understands us-from our preferences and habits, to the timing and purpose of our everyday actions. Yet current user models remain fragmented, narrowly tailored to specific apps, and incapable of the flexible reasoning required to fulfill t...
[20.05.2025 16:14] Read previous papers.
[20.05.2025 16:14] Generating reviews via LLM API.
[20.05.2025 16:14] Using data from previous issue: {"categories": ["#training", "#open_source", "#inference", "#agi", "#architecture", "#optimization"], "emoji": "üîó", "ru": {"title": "–¶–µ–ø–Ω–∞—è —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö: –≥–∏–±–∫–æ—Å—Ç—å –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å", "desc": "–í —ç—Ç–æ–π —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–∞—è –ø–∞—Ä–∞–¥–∏–≥–º–∞ –æ–±—É—á–µ–Ω–∏—è –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Chain-of-Model (CoM), –∫–æ—Ç–æ—Ä–∞
[20.05.2025 16:14] Using data from previous issue: {"categories": ["#math", "#reasoning", "#inference", "#training", "#optimization", "#rl"], "emoji": "üß†", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –ò–ò", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ –Ω–æ–≤—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º AdaptThink, –∫–æ—Ç–æ—Ä—ã–π —É—á–∏—Ç –º–æ–¥–µ–ª–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∞–¥–∞–ø—Ç–∏–≤–Ω–æ –≤—ã–±–∏—Ä–∞—Ç—å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π 
[20.05.2025 16:14] Using data from previous issue: {"categories": ["#optimization", "#reasoning", "#rlhf", "#rl", "#training"], "emoji": "üß†", "ru": {"title": "AdaCoT: –£–º–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –¥–ª—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "AdaCoT - —ç—Ç–æ –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –ø–æ–∑–≤–æ–ª—è—é—â–∏–π –∫—Ä—É–ø–Ω—ã–º —è–∑—ã–∫–æ–≤—ã–º –º–æ–¥–µ–ª—è–º (LLM) –∞–¥–∞–ø—Ç–∏–≤–Ω–æ —Ä–µ—à–∞—Ç—å, –∫–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–µ—Ç–æ–¥ —Ü–µ–ø–æ—á–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏
[20.05.2025 16:14] Using data from previous issue: {"categories": ["#optimization", "#long_context", "#architecture", "#inference", "#benchmark"], "emoji": "üîç", "ru": {"title": "–ö–æ—Ä—Ä–µ–∫—Ü–∏—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è –≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞—Ö. –ê–≤
[20.05.2025 16:14] Using data from previous issue: {"categories": ["#data", "#dataset", "#graphs", "#agents", "#benchmark", "#open_source"], "emoji": "üñ•Ô∏è", "ru": {"title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –æ–±—É—á–µ–Ω–∏–∏ –ò–ò —Ä–∞–±–æ—Ç–µ —Å –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω—ã–º–∏ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞–º–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ OSWorld-G –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∫ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞
[20.05.2025 16:14] Using data from previous issue: {"categories": ["#optimization", "#reasoning", "#rl", "#benchmark", "#training"], "emoji": "üß†", "ru": {"title": "–£–º–Ω–æ–µ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –º–µ–∂–¥—É –∫—Ä–∞—Ç–∫–∏–º –∏ —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç—ã–º –º—ã—à–ª–µ–Ω–∏–µ–º –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Thinkless - –æ–±—É—á–∞–µ–º—É—é —Å–∏—Å—Ç–µ–º—É, –ø–æ–∑–≤–æ–ª—è—é—â—É—é —è–∑—ã–∫–æ–≤—ã–º –º–æ–¥–µ–ª—è–º –∞–¥–∞–ø—Ç–∏–≤–Ω–æ –≤—ã–±–∏—Ä–∞—Ç—å –º
[20.05.2025 16:14] Using data from previous issue: {"categories": ["#optimization", "#benchmark", "#architecture", "#reasoning", "#training", "#agi"], "emoji": "üß†", "ru": {"title": "LatentSeek: –ü–æ–≤—ã—à–µ–Ω–∏–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —Ä–∞—Å—Å—É–∂–¥–∞—Ç—å —É –ò–ò —á–µ—Ä–µ–∑ –∞–¥–∞–ø—Ç–∞—Ü–∏—é –≤ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç LatentSeek - –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å
[20.05.2025 16:14] Using data from previous issue: {"categories": ["#optimization", "#reasoning", "#multimodal", "#math", "#training", "#open_source", "#benchmark", "#data", "#dataset"], "emoji": "üß†", "ru": {"title": "–ê–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –ø–æ—à–∞–≥–æ–≤—ã–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –º–æ–¥–µ–ª—å MM-PRM, –æ–±—É—á–∞—é—â–∞—è—Å
[20.05.2025 16:14] Using data from previous issue: {"categories": ["#3d"], "emoji": "üé•", "ru": {"title": "–ì–∏–±—Ä–∏–¥–Ω—ã–π 3D-4D –ø–æ–¥—Ö–æ–¥ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö —Å—Ü–µ–Ω", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ 3D-4DGS –¥–ª—è —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö 3D-—Å—Ü–µ–Ω. –û–Ω –∫–æ–º–±–∏–Ω–∏—Ä—É–µ—Ç 3D –≥–∞—É—Å—Å–∏–∞–Ω—ã –¥–ª—è —Å—Ç–∞—Ç–∏—á–Ω—ã—Ö –æ–±–ª–∞—Å—Ç–µ–π –∏ 4D –≥–∞—É—Å—Å–∏–∞–Ω—ã –¥–ª—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö 
[20.05.2025 16:14] Using data from previous issue: {"categories": ["#training", "#data", "#benchmark", "#security", "#optimization"], "emoji": "üîí", "ru": {"title": "FedSVD: –ó–∞—â–∏—â–µ–Ω–Ω–æ–µ —Ñ–µ–¥–µ—Ä–∞—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ FedSVD –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Ñ–µ–¥–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º
[20.05.2025 16:14] Using data from previous issue: {"categories": ["#training", "#optimization", "#rl", "#reasoning"], "emoji": "üß†", "ru": {"title": "–°—Ç–∞–±–∏–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º CPGD –¥–ª—è —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏–∏ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. CPGD –≤–≤–æ–¥–∏—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –Ω–∞ –¥—Ä–µ–π—Ñ 
[20.05.2025 16:14] Using data from previous issue: {"categories": ["#optimization", "#open_source", "#architecture", "#training"], "emoji": "üîÄ", "ru": {"title": "–°–ª–∏—è–Ω–∏–µ –º–æ–¥–µ–ª–µ–π: –ø—É—Ç—å –∫ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–º—É –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—é –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ç–µ—Ö–Ω–∏–∫–∏ —Å–ª–∏—è–Ω–∏—è –º–æ–¥–µ–ª–µ–π –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤
[20.05.2025 16:14] Using data from previous issue: {"categories": ["#training", "#reasoning", "#optimization", "#benchmark", "#inference"], "emoji": "üß†", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Fractured Sampling –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π 
[20.05.2025 16:14] Using data from previous issue: {"categories": ["#optimization", "#architecture", "#diffusion", "#training", "#open_source", "#video"], "emoji": "üé•", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –≤–∏–¥–µ–æ-–¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è (VSA) –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä–æ
[20.05.2025 16:14] Using data from previous issue: {"categories": ["#optimization", "#reasoning", "#rag", "#multimodal"], "emoji": "üß†", "ru": {"title": "–ö–æ–º–ø–∏–ª—è—Ü–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –≤ RAG-—Å–∏—Å—Ç–µ–º–∞—Ö", "desc": "QCompiler - —ç—Ç–æ –Ω–µ–π—Ä–æ-—Å–∏–º–≤–æ–ª–∏—á–µ—Å–∫–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–Ω–∏–º–∞–Ω–∏—è —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ RAG-—Å–∏—Å—Ç–µ–º–∞—Ö. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–∏–Ω–∏–º–∞–ª—å–Ω—É—é –≥—Ä–∞–º–º
[20.05.2025 16:14] Using data from previous issue: {"categories": ["#reasoning", "#training", "#optimization", "#math", "#benchmark", "#rl", "#hallucinations"], "emoji": "üß†", "ru": {"title": "–£–º–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –ò–ò: —É—á–∏—Ç—ã–≤–∞–µ–º –Ω–µ—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º SEED-GRPO. –≠—Ç–æ—Ç –ø–æ
[20.05.2025 16:14] Using data from previous issue: {"categories": ["#multimodal", "#cv", "#benchmark", "#reasoning"], "emoji": "üß†", "ru": {"title": "–ï–¥–∏–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –º–Ω–æ–≥–æ–∑–∞–¥–∞—á–Ω–æ–≥–æ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω VisionReasoner - —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Ä–µ—à–µ–Ω–∏—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è. –ú–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–æ–≤—ã
[20.05.2025 16:14] Using data from previous issue: {"categories": ["#open_source", "#interpretability", "#cv", "#benchmark", "#synthetic", "#reasoning", "#dataset"], "emoji": "üìä", "ru": {"title": "–†–∞—Å–∫—Ä—ã–≤–∞—è –ø—Ä–æ–±–µ–ª—ã –≤ –≤–∏–∑—É–∞–ª—å–Ω–æ–º –º—ã—à–ª–µ–Ω–∏–∏ –ò–ò –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –¥–∏–∞–≥—Ä–∞–º–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π —Ç–µ—Å—Ç–æ–≤—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö ChartMuseum –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø–æ–Ω–∏–º–∞–Ω–∏
[20.05.2025 16:14] Using data from previous issue: {"categories": ["#benchmark", "#video", "#cv", "#games", "#reasoning", "#agents", "#open_source"], "emoji": "ü§ñ", "ru": {"title": "ViPlan: –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Å–∏–º–≤–æ–ª—å–Ω–æ–≥–æ –∏ –ø—Ä—è–º–æ–≥–æ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç ViPlan - –ø–µ—Ä–≤—ã–π –æ—Ç–∫—Ä—ã—Ç—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –ø–ª–∞–Ω–∏
[20.05.2025 16:14] Using data from previous issue: {"categories": ["#data", "#science", "#dataset", "#benchmark"], "emoji": "üîç", "ru": {"title": "–ë–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –ø–æ–∫–∞ –Ω–µ –≥–æ—Ç–æ–≤—ã –±—ã—Ç—å –Ω–∞—É—á–Ω—ã–º–∏ —Ä–µ—Ü–µ–Ω–∑–µ–Ω—Ç–∞–º–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç SPOT - –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∏–∑ 83 –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–Ω—ã—Ö –Ω–∞—É—á–Ω—ã—Ö —Ä–∞–±–æ—Ç —Å 91 –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–π –æ—à–∏–±–∫–æ–π, –ø—Ä–∏–≤–µ–¥—à–µ–π –∫ –æ–ø–µ—á–∞—Ç–∫–∞–º –∏–ª–∏ 
[20.05.2025 16:14] Using data from previous issue: {"categories": ["#optimization", "#architecture", "#open_source", "#cv", "#training"], "emoji": "üöÄ", "ru": {"title": "–£—Å–∫–æ—Ä–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ TarFlow —Å –ø–æ–º–æ—â—å—é –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∏—Ç–µ—Ä–∞—Ü–∏–π", "desc": "–î–∞–Ω–Ω–∞—è —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ —É—Å–∫–æ—Ä–µ–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞ —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏—è –≤ –º–æ–¥–µ–ª–∏ TarFlow –¥–ª—è –≥–µ–Ω–µ—Ä–∞
[20.05.2025 16:14] Using data from previous issue: {"categories": ["#multilingual", "#dataset", "#synthetic", "#open_source", "#benchmark"], "emoji": "üöÄ", "ru": {"title": "–ú–æ–ª–Ω–∏–µ–Ω–æ—Å–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –≤—Å–µ—Ö", "desc": "TQB++ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –ª–µ–≥–∫–æ–≤–µ—Å–Ω—ã–π –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–π –Ω–∞–±–æ—Ä —Ç–µ—Å—Ç–æ–≤ –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –û–Ω –≤–∫–ª—é—á–∞–µ—Ç –≤ 
[20.05.2025 16:14] Using data from previous issue: {"categories": ["#3d", "#optimization", "#diffusion", "#video"], "emoji": "ü§∏", "ru": {"title": "–¢–æ—á–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–≤–∏–∂–µ–Ω–∏–π —á–µ–ª–æ–≤–µ–∫–∞ —Å –ø–æ–º–æ—â—å—é —Ñ–∏–∑–∏—á–µ—Å–∫–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç FinePhys - —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–æ—á–Ω—ã—Ö –¥–≤–∏–∂–µ–Ω–∏–π —á–µ–ª–æ–≤–µ–∫–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ñ–∏–∑–∏—á–µ—Å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π. –°–∏—Å—Ç–µ
[20.05.2025 16:14] Using data from previous issue: {"categories": ["#3d", "#video"], "emoji": "üï∫", "ru": {"title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –∞–Ω–∏–º–∞—Ü–∏–∏ —á–µ–ª–æ–≤–µ–∫–∞: –æ—Ç 2D –∫ 4D –¥–≤–∏–∂–µ–Ω–∏—é", "desc": "MTVCrafter - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∞–Ω–∏–º–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —á–µ–ª–æ–≤–µ–∫–∞, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é 4D –¥–≤–∏–∂–µ–Ω–∏—è –≤–º–µ—Å—Ç–æ 2D-–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ–∑. –ú–µ—Ç–æ–¥ –≤–≤–æ–¥–∏—Ç 4DMoT –¥–ª—è –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è 3D –ø–æ—Å–ª–µ–¥–æ–≤
[20.05.2025 16:14] Using data from previous issue: {"categories": ["#reasoning", "#multilingual", "#rl", "#low_resource", "#machine_translation"], "emoji": "üåê", "ru": {"title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –º–∞—à–∏–Ω–Ω–æ–º –ø–µ—Ä–µ–≤–æ–¥–µ: –æ—Ç –æ–¥–Ω–æ—è–∑—ã—á–Ω–æ–≥–æ –∫ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–º—É —Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤—É", "desc": "–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º
[20.05.2025 16:14] Using data from previous issue: {"categories": ["#multimodal", "#healthcare", "#dataset", "#data"], "emoji": "üî¨", "ru": {"title": "HISTAI: –ö—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–æ—Ä—ã–≤–∞ –≤ —Ü–∏—Ñ—Ä–æ–≤–æ–π –ø–∞—Ç–æ–ª–æ–≥–∏–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö HISTAI –¥–ª—è —Ü–∏—Ñ—Ä–æ–≤–æ–π –ø–∞—Ç–æ–ª–æ–≥–∏–∏. –û–Ω —Å–æ–¥–µ—Ä–∂–∏—Ç –±–æ–ª–µ–µ 60 000 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å—Ä–µ–∑–æ–≤ —Ç–∫–∞–Ω–µ
[20.05.2025 16:14] Using data from previous issue: {"categories": ["#optimization", "#diffusion", "#inference", "#video"], "emoji": "üé¨", "ru": {"title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–∏ –≤–∏–¥–µ–æ-–¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "QVGen - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è —Å —É—á–µ—Ç–æ–º –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è –¥–ª—è –≤—ã—Å–æ–∫–æ–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω—ã—Ö –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö –ø—Ä–∏ –≤—ã–≤–æ–¥–µ –≤–∏–¥–µ–æ-–¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π 
[20.05.2025 16:14] Using data from previous issue: {"categories": ["#optimization", "#inference", "#benchmark", "#reasoning", "#training"], "emoji": "üß†", "ru": {"title": "–£–ª—É—á—à–µ–Ω–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –ò–ò —á–µ—Ä–µ–∑ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –ª–∞—Ç–µ–Ω—Ç–Ω—ã–µ –º—ã—Å–ª–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ SoftCoT++, —É–ª—É—á—à–∞—é—â–∏–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–º –ª–∞—Ç–µ–Ω—Ç–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞
[20.05.2025 16:14] Using data from previous issue: {"categories": ["#rl", "#rlhf", "#agents", "#reasoning", "#open_source", "#multimodal", "#games"], "emoji": "üó£Ô∏è", "ru": {"title": "–≠–≤–æ–ª—é—Ü–∏—è —è–∑—ã–∫–∞ —á–µ—Ä–µ–∑ –ø—Ä–∏–∑–º—É –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞", "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç —ç–≤–æ–ª—é—Ü–∏—é —è–∑—ã–∫–∞ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã—Ö –∏–≥—Ä –ø–æ –¥–æ–±—ã—á–µ —Ä–µ—Å—É—Ä—Å–æ–≤, –∏—Å–ø–æ–ª—å–∑—É—è –≥–ª—É–±–æ–∫–æ–µ –æ–±—É
[20.05.2025 16:14] Using data from previous issue: {"categories": ["#alignment", "#rlhf", "#multilingual", "#open_source", "#dataset"], "emoji": "ü§ñ", "ru": {"title": "HelpSteer3-Preference: –Ω–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è RLHF", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç HelpSteer3-Preference - –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π 
[20.05.2025 16:14] Using data from previous issue: {"categories": ["#science", "#multimodal", "#interpretability", "#inference"], "emoji": "üîç", "ru": {"title": "–ü–æ–≤—ã—à–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ LLM –≤ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–∏ –æ—à–∏–±–æ–∫ —á–µ—Ä–µ–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é –Ω–∞—Å—Ç—Ä–æ–π–∫—É –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞", "desc": "–≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∏–∑—É—á–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –Ω–∞—Å—Ç—Ä–æ–π–∫–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ
[20.05.2025 16:14] Using data from previous issue: {"categories": ["#security", "#data", "#hallucinations", "#rag", "#benchmark"], "emoji": "üõ°Ô∏è", "ru": {"title": "–¢–æ—á–Ω–æ–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ç–µ—Ö–Ω–∏–∫ –∑–ª–æ—É–º—ã—à–ª–µ–Ω–Ω–∏–∫–æ–≤ —Å –º–∏–Ω–∏–º—É–º–æ–º –¥–∞–Ω–Ω—ã—Ö", "desc": "TechniqueRAG - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–µ—Ö–Ω–∏–∫ –ø—Ä–æ—Ç–∏–≤–Ω–∏–∫–æ–≤ –≤ —Ç–µ–∫—Å—Ç–∞—Ö –ø–æ –∫–∏–±–µ—Ä–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É
[20.05.2025 16:14] Using data from previous issue: {"categories": ["#architecture", "#cv", "#training", "#dataset"], "emoji": "üì±", "ru": {"title": "–£–º–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–æ—Ç–æ –±–µ–∑ –ø–∞—Ä–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö", "desc": "–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è Image Signal Processor (ISP) –¥–ª—è —Å–º–∞—Ä—Ç—Ñ–æ–Ω–æ–≤ –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞—Ä–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –ø–æ–¥—Ö–æ–¥ —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏
[20.05.2025 16:14] Using data from previous issue: {"categories": ["#reasoning", "#data", "#science", "#multimodal", "#interpretability", "#architecture"], "emoji": "üî¨", "ru": {"title": "PWP: –ù–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–æ–º—É –∞–Ω–∞–ª–∏–∑—É –Ω–∞—É—á–Ω—ã—Ö —Ä–∞–±–æ—Ç —Å –ø–æ–º–æ—â—å—é LLM", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∏–Ω–∂–µ–Ω–µ—Ä–∏–∏ –ø—Ä–æ–º–ø—Ç–æ–≤ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Persistent Workflow
[20.05.2025 16:14] Using data from previous issue: {"categories": ["#data", "#healthcare", "#low_resource", "#dataset"], "emoji": "üó£Ô∏è", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ –æ–º–æ–≥—Ä–∞—Ñ–æ–≤ –¥–ª—è G2P –∫–æ–Ω–≤–µ—Ä—Å–∏–∏ –≤ –º–∞–ª–æ—Ä–µ—Å—É—Ä—Å–Ω—ã—Ö —è–∑—ã–∫–∞—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –ø—Ä–æ–±–ª–µ–º–µ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –æ–º–æ–≥—Ä–∞—Ñ–æ–≤ –≤ –∑–∞–¥–∞—á–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –≥—Ä–∞—Ñ–µ–º –≤ —Ñ–æ–Ω–µ–º—ã (G2P) –¥–ª—è –º–∞–ª–æ—Ä–µ—Å—É—Ä—Å–Ω—ã—Ö —è–∑
[20.05.2025 16:14] Using data from previous issue: {"categories": ["#interpretability", "#agents", "#multimodal", "#agi", "#architecture", "#reasoning"], "emoji": "üß†", "ru": {"title": "GUM: —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∏–Ω—Ç—É–∏—Ç–∏–≤–Ω—ã—Ö –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–≤ –±—É–¥—É—â–µ–≥–æ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –æ–±—â–µ–π –º–æ–¥–µ–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (GUM), –∫–æ—Ç–æ—Ä
[20.05.2025 16:14] Loading Chinese text from previous data.
[20.05.2025 16:14] Renaming data file.
[20.05.2025 16:14] Renaming previous data. hf_papers.json to ./d/2025-05-20.json
[20.05.2025 16:14] Saving new data file.
[20.05.2025 16:14] Generating page.
[20.05.2025 16:14] Renaming previous page.
[20.05.2025 16:14] Renaming previous data. index.html to ./d/2025-05-20.html
[20.05.2025 16:14] [Experimental] Generating Chinese page for reading.
[20.05.2025 16:14] Chinese vocab [{'word': 'ËåÉÂºè', 'pinyin': 'f√†n sh√¨', 'trans': 'paradigm'}, {'word': 'Chain-of-Model', 'pinyin': 'Ch√®in-√≤f-M√≥del', 'trans': 'Chain-of-Model'}, {'word': 'Âõ†ÊûúÂÖ≥Á≥ª', 'pinyin': 'yƒ´n gu«í guƒÅn x√¨', 'trans': 'causal relationship'}, {'word': 'ÈöêËóèÁä∂ÊÄÅ', 'pinyin': 'y«ên c√°ng zhu√†ng t√†i', 'trans': 'hidden state'}, {'word': 'ÈìæÂºèÁªìÊûÑ', 'pinyin': 'li√†n sh√¨ ji√©g√≤u', 'trans': 'chain structure'}, {'word': 'Êâ©Â±ïÊïàÁéá', 'pinyin': 'ku√≤ zh«én xi√†o l«ú', 'trans': 'scalability'}, {'word': 'ÈÉ®ÁΩ≤', 'pinyin': 'b√π sh«î', 'trans': 'deployment'}, {'word': 'ÁÅµÊ¥ªÊÄß', 'pinyin': 'l√≠ng hu√≥ x√¨ng', 'trans': 'flexibility'}, {'word': 'Chain-of-Representation', 'pinyin': 'Ch√®in-√≤f-Rƒõprizen t√©i shƒìn', 'trans': 'Chain-of-Representation'}, {'word': 'Â≠êË°®Á§∫', 'pinyin': 'z«ê bi«éo sh√¨', 'trans': 'sub-representation'}, {'word': 'ÁªÑÂêà', 'pinyin': 'z«î h√©', 'trans': 'combination'}, {'word': 'ÂâçÂ∫èÈìæ', 'pinyin': 'qi√°n x√π li√†n', 'trans': 'preceding chain'}, {'word': 'ÂºπÊÄßÊé®ÁêÜ', 'pinyin': 't√°n x√¨ng tuƒ´ l«ê', 'trans': 'elastic inference'}, {'word': 'Chain-of-Language-Model', 'pinyin': 'Ch√®in-√≤f-L√°ngg√π M√≥del', 'trans': 'Chain-of-Language-Model'}, {'word': 'KVÂÖ±‰∫´Êú∫Âà∂', 'pinyin': 'KV g√≤ng xi«éng jƒ´ zh√¨', 'trans': 'KV sharing mechanism'}, {'word': 'CoLM-Air', 'pinyin': 'CoLM-√âir', 'trans': 'CoLM-Air'}, {'word': 'Êâ©Â±ïÂäüËÉΩ', 'pinyin': 'ku√≤ zh«én g≈çng n√©ng', 'trans': 'extended functionality'}, {'word': 'Transformer', 'pinyin': 'T√®insh√®in f≈çmƒõi', 'trans': 'Transformer'}]
[20.05.2025 16:14] Renaming previous Chinese page.
[20.05.2025 16:14] Renaming previous data. zh.html to ./d/2025-05-19_zh_reading_task.html
[20.05.2025 16:14] Writing Chinese reading task.
[20.05.2025 16:14] Writing result.
[20.05.2025 16:14] Renaming log file.
[20.05.2025 16:14] Renaming previous data. log.txt to ./logs/2025-05-20_last_log.txt
