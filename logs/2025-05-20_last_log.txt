[20.05.2025 08:18] Read previous papers.
[20.05.2025 08:18] Generating top page (month).
[20.05.2025 08:18] Writing top page (month).
[20.05.2025 09:13] Read previous papers.
[20.05.2025 09:13] Get feed.
[20.05.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.11820
[20.05.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.13417
[20.05.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.11896
[20.05.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.11254
[20.05.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.13227
[20.05.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.13379
[20.05.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.13427
[20.05.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.13215
[20.05.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.13308
[20.05.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.12805
[20.05.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.12504
[20.05.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.12992
[20.05.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.12081
[20.05.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.13389
[20.05.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.11932
[20.05.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.13180
[20.05.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.12849
[20.05.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.11855
[20.05.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.13444
[20.05.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.13437
[20.05.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.10238
[20.05.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.12996
[20.05.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.11484
[20.05.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.12257
[20.05.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.11988
[20.05.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.11497
[20.05.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.03332
[20.05.2025 09:13] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[20.05.2025 09:13] No deleted papers detected.
[20.05.2025 09:13] Downloading and parsing papers (pdf, html). Total: 27.
[20.05.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2505.11820.
[20.05.2025 09:13] Extra JSON file exists (./assets/json/2505.11820.json), skip PDF parsing.
[20.05.2025 09:13] Paper image links file exists (./assets/img_data/2505.11820.json), skip HTML parsing.
[20.05.2025 09:13] Success.
[20.05.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2505.13417.
[20.05.2025 09:13] Extra JSON file exists (./assets/json/2505.13417.json), skip PDF parsing.
[20.05.2025 09:13] Paper image links file exists (./assets/img_data/2505.13417.json), skip HTML parsing.
[20.05.2025 09:13] Success.
[20.05.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2505.11896.
[20.05.2025 09:13] Extra JSON file exists (./assets/json/2505.11896.json), skip PDF parsing.
[20.05.2025 09:13] Paper image links file exists (./assets/img_data/2505.11896.json), skip HTML parsing.
[20.05.2025 09:13] Success.
[20.05.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2505.11254.
[20.05.2025 09:13] Extra JSON file exists (./assets/json/2505.11254.json), skip PDF parsing.
[20.05.2025 09:13] Paper image links file exists (./assets/img_data/2505.11254.json), skip HTML parsing.
[20.05.2025 09:13] Success.
[20.05.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2505.13227.
[20.05.2025 09:13] Extra JSON file exists (./assets/json/2505.13227.json), skip PDF parsing.
[20.05.2025 09:13] Paper image links file exists (./assets/img_data/2505.13227.json), skip HTML parsing.
[20.05.2025 09:13] Success.
[20.05.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2505.13379.
[20.05.2025 09:13] Extra JSON file exists (./assets/json/2505.13379.json), skip PDF parsing.
[20.05.2025 09:13] Paper image links file exists (./assets/img_data/2505.13379.json), skip HTML parsing.
[20.05.2025 09:13] Success.
[20.05.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2505.13427.
[20.05.2025 09:13] Extra JSON file exists (./assets/json/2505.13427.json), skip PDF parsing.
[20.05.2025 09:13] Paper image links file exists (./assets/img_data/2505.13427.json), skip HTML parsing.
[20.05.2025 09:13] Success.
[20.05.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2505.13215.
[20.05.2025 09:13] Extra JSON file exists (./assets/json/2505.13215.json), skip PDF parsing.
[20.05.2025 09:13] Paper image links file exists (./assets/img_data/2505.13215.json), skip HTML parsing.
[20.05.2025 09:13] Success.
[20.05.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2505.13308.
[20.05.2025 09:13] Extra JSON file exists (./assets/json/2505.13308.json), skip PDF parsing.
[20.05.2025 09:13] Paper image links file exists (./assets/img_data/2505.13308.json), skip HTML parsing.
[20.05.2025 09:13] Success.
[20.05.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2505.12805.
[20.05.2025 09:13] Extra JSON file exists (./assets/json/2505.12805.json), skip PDF parsing.
[20.05.2025 09:13] Paper image links file exists (./assets/img_data/2505.12805.json), skip HTML parsing.
[20.05.2025 09:13] Success.
[20.05.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2505.12504.
[20.05.2025 09:13] Extra JSON file exists (./assets/json/2505.12504.json), skip PDF parsing.
[20.05.2025 09:13] Paper image links file exists (./assets/img_data/2505.12504.json), skip HTML parsing.
[20.05.2025 09:13] Success.
[20.05.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2505.12992.
[20.05.2025 09:13] Extra JSON file exists (./assets/json/2505.12992.json), skip PDF parsing.
[20.05.2025 09:13] Paper image links file exists (./assets/img_data/2505.12992.json), skip HTML parsing.
[20.05.2025 09:13] Success.
[20.05.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2505.12081.
[20.05.2025 09:13] Extra JSON file exists (./assets/json/2505.12081.json), skip PDF parsing.
[20.05.2025 09:13] Paper image links file exists (./assets/img_data/2505.12081.json), skip HTML parsing.
[20.05.2025 09:13] Success.
[20.05.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2505.13389.
[20.05.2025 09:13] Extra JSON file exists (./assets/json/2505.13389.json), skip PDF parsing.
[20.05.2025 09:13] Paper image links file exists (./assets/img_data/2505.13389.json), skip HTML parsing.
[20.05.2025 09:13] Success.
[20.05.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2505.11932.
[20.05.2025 09:13] Extra JSON file exists (./assets/json/2505.11932.json), skip PDF parsing.
[20.05.2025 09:13] Paper image links file exists (./assets/img_data/2505.11932.json), skip HTML parsing.
[20.05.2025 09:13] Success.
[20.05.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2505.13180.
[20.05.2025 09:13] Extra JSON file exists (./assets/json/2505.13180.json), skip PDF parsing.
[20.05.2025 09:13] Paper image links file exists (./assets/img_data/2505.13180.json), skip HTML parsing.
[20.05.2025 09:13] Success.
[20.05.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2505.12849.
[20.05.2025 09:13] Extra JSON file exists (./assets/json/2505.12849.json), skip PDF parsing.
[20.05.2025 09:13] Paper image links file exists (./assets/img_data/2505.12849.json), skip HTML parsing.
[20.05.2025 09:13] Success.
[20.05.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2505.11855.
[20.05.2025 09:13] Extra JSON file exists (./assets/json/2505.11855.json), skip PDF parsing.
[20.05.2025 09:13] Paper image links file exists (./assets/img_data/2505.11855.json), skip HTML parsing.
[20.05.2025 09:13] Success.
[20.05.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2505.13444.
[20.05.2025 09:13] Extra JSON file exists (./assets/json/2505.13444.json), skip PDF parsing.
[20.05.2025 09:13] Paper image links file exists (./assets/img_data/2505.13444.json), skip HTML parsing.
[20.05.2025 09:13] Success.
[20.05.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2505.13437.
[20.05.2025 09:13] Extra JSON file exists (./assets/json/2505.13437.json), skip PDF parsing.
[20.05.2025 09:13] Paper image links file exists (./assets/img_data/2505.13437.json), skip HTML parsing.
[20.05.2025 09:13] Success.
[20.05.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2505.10238.
[20.05.2025 09:13] Extra JSON file exists (./assets/json/2505.10238.json), skip PDF parsing.
[20.05.2025 09:13] Paper image links file exists (./assets/img_data/2505.10238.json), skip HTML parsing.
[20.05.2025 09:13] Success.
[20.05.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2505.12996.
[20.05.2025 09:13] Extra JSON file exists (./assets/json/2505.12996.json), skip PDF parsing.
[20.05.2025 09:13] Paper image links file exists (./assets/img_data/2505.12996.json), skip HTML parsing.
[20.05.2025 09:13] Success.
[20.05.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2505.11484.
[20.05.2025 09:13] Extra JSON file exists (./assets/json/2505.11484.json), skip PDF parsing.
[20.05.2025 09:13] Paper image links file exists (./assets/img_data/2505.11484.json), skip HTML parsing.
[20.05.2025 09:13] Success.
[20.05.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2505.12257.
[20.05.2025 09:13] Extra JSON file exists (./assets/json/2505.12257.json), skip PDF parsing.
[20.05.2025 09:13] Paper image links file exists (./assets/img_data/2505.12257.json), skip HTML parsing.
[20.05.2025 09:13] Success.
[20.05.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2505.11988.
[20.05.2025 09:13] Extra JSON file exists (./assets/json/2505.11988.json), skip PDF parsing.
[20.05.2025 09:13] Paper image links file exists (./assets/img_data/2505.11988.json), skip HTML parsing.
[20.05.2025 09:13] Success.
[20.05.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2505.11497.
[20.05.2025 09:13] Extra JSON file exists (./assets/json/2505.11497.json), skip PDF parsing.
[20.05.2025 09:13] Paper image links file exists (./assets/img_data/2505.11497.json), skip HTML parsing.
[20.05.2025 09:13] Success.
[20.05.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2505.03332.
[20.05.2025 09:13] Extra JSON file exists (./assets/json/2505.03332.json), skip PDF parsing.
[20.05.2025 09:13] Paper image links file exists (./assets/img_data/2505.03332.json), skip HTML parsing.
[20.05.2025 09:13] Success.
[20.05.2025 09:13] Enriching papers with extra data.
[20.05.2025 09:13] ********************************************************************************
[20.05.2025 09:13] Abstract 0. In this paper, we propose a novel learning paradigm, termed Chain-of-Model (CoM), which incorporates the causal relationship into the hidden states of each layer as a chain style, thereby introducing great scaling efficiency in model training and inference flexibility in deployment. We introduce the...
[20.05.2025 09:13] ********************************************************************************
[20.05.2025 09:13] Abstract 1. Recently, large reasoning models have achieved impressive performance on various tasks by employing human-like deep thinking. However, the lengthy thinking process substantially increases inference overhead, making efficiency a critical bottleneck. In this work, we first demonstrate that NoThinking,...
[20.05.2025 09:13] ********************************************************************************
[20.05.2025 09:13] Abstract 2. Large Language Models (LLMs) have demonstrated remarkable capabilities but often face challenges with tasks requiring sophisticated reasoning. While Chain-of-Thought (CoT) prompting significantly enhances reasoning, it indiscriminately generates lengthy reasoning steps for all queries, leading to su...
[20.05.2025 09:13] ********************************************************************************
[20.05.2025 09:13] Abstract 3. The attention mechanism of a transformer has a quadratic complexity, leading to high inference costs and latency for long sequences. However, attention matrices are mostly sparse, which implies that many entries may be omitted from computation for efficient inference. Sparse attention inference meth...
[20.05.2025 09:13] ********************************************************************************
[20.05.2025 09:13] Abstract 4. Graphical user interface (GUI) grounding, the ability to map natural language instructions to specific actions on graphical user interfaces, remains a critical bottleneck in computer use agent development. Current benchmarks oversimplify grounding tasks as short referring expressions, failing to cap...
[20.05.2025 09:13] ********************************************************************************
[20.05.2025 09:13] Abstract 5. Reasoning Language Models, capable of extended chain-of-thought reasoning, have demonstrated remarkable performance on tasks requiring complex logical inference. However, applying elaborate reasoning for all queries often results in substantial computational inefficiencies, particularly when many pr...
[20.05.2025 09:13] ********************************************************************************
[20.05.2025 09:13] Abstract 6. While Multimodal Large Language Models (MLLMs) have achieved impressive progress in vision-language understanding, they still struggle with complex multi-step reasoning, often producing logically inconsistent or partially correct solutions. A key limitation lies in the lack of fine-grained supervisi...
[20.05.2025 09:13] ********************************************************************************
[20.05.2025 09:13] Abstract 7. Recent advancements in dynamic 3D scene reconstruction have shown promising results, enabling high-fidelity 3D novel view synthesis with improved temporal consistency. Among these, 4D Gaussian Splatting (4DGS) has emerged as an appealing approach due to its ability to model high-fidelity spatial and...
[20.05.2025 09:13] ********************************************************************************
[20.05.2025 09:13] Abstract 8. Reasoning ability, a core component of human intelligence, continues to pose a significant challenge for Large Language Models (LLMs) in the pursuit of AGI. Although model performance has improved under the training scaling law, significant challenges remain, particularly with respect to training al...
[20.05.2025 09:13] ********************************************************************************
[20.05.2025 09:13] Abstract 9. Low-Rank Adaptation (LoRA), which introduces a product of two trainable low-rank matrices into frozen pre-trained weights, is widely used for efficient fine-tuning of language models in federated learning (FL). However, when combined with differentially private stochastic gradient descent (DP-SGD), ...
[20.05.2025 09:13] ********************************************************************************
[20.05.2025 09:13] Abstract 10. Recent advances in rule-based reinforcement learning (RL) have significantly improved the reasoning capability of language models (LMs) with rule-based rewards. However, existing RL methods -- such as GRPO, REINFORCE++, and RLOO -- often suffer from training instability, where large policy updates a...
[20.05.2025 09:13] ********************************************************************************
[20.05.2025 09:13] Abstract 11. Inference-time scaling techniques have significantly bolstered the reasoning capabilities of large language models (LLMs) by harnessing additional computational effort at inference without retraining. Similarly, Chain-of-Thought (CoT) prompting and its extension, Long CoT, improve accuracy by genera...
[20.05.2025 09:13] ********************************************************************************
[20.05.2025 09:13] Abstract 12. Large vision-language models exhibit inherent capabilities to handle diverse visual perception tasks. In this paper, we introduce VisionReasoner, a unified framework capable of reasoning and solving multiple visual perception tasks within a shared model. Specifically, by designing novel multi-object...
[20.05.2025 09:13] ********************************************************************************
[20.05.2025 09:13] Abstract 13. Scaling video diffusion transformers (DiTs) is limited by their quadratic 3D attention, even though most of the attention mass concentrates on a small subset of positions. We turn this observation into VSA, a trainable, hardware-efficient sparse attention that replaces full attention at both trainin...
[20.05.2025 09:13] ********************************************************************************
[20.05.2025 09:13] Abstract 14. Precise recognition of search intent in Retrieval-Augmented Generation (RAG) systems remains a challenging goal, especially under resource constraints and for complex queries with nested structures and dependencies. This paper presents QCompiler, a neuro-symbolic framework inspired by linguistic gra...
[20.05.2025 09:13] ********************************************************************************
[20.05.2025 09:13] Abstract 15. Integrating Large Language Models with symbolic planners is a promising direction for obtaining verifiable and grounded plans compared to planning in natural language, with recent works extending this idea to visual domains using Vision-Language Models (VLMs). However, rigorous comparison between VL...
[20.05.2025 09:13] ********************************************************************************
[20.05.2025 09:13] Abstract 16. Image generation models have achieved widespread applications. As an instance, the TarFlow model combines the transformer architecture with Normalizing Flow models, achieving state-of-the-art results on multiple benchmarks. However, due to the causal form of attention requiring sequential computatio...
[20.05.2025 09:13] ********************************************************************************
[20.05.2025 09:13] Abstract 17. Recent advances in large language models (LLMs) have fueled the vision of automated scientific discovery, often called AI Co-Scientists. To date, prior work casts these systems as generative co-authors responsible for crafting hypotheses, synthesizing code, or drafting manuscripts. In this work, we ...
[20.05.2025 09:13] ********************************************************************************
[20.05.2025 09:13] Abstract 18. Chart understanding presents a unique challenge for large vision-language models (LVLMs), as it requires the integration of sophisticated textual and visual reasoning capabilities. However, current LVLMs exhibit a notable imbalance between these skills, falling short on visual reasoning that is diff...
[20.05.2025 09:13] ********************************************************************************
[20.05.2025 09:13] Abstract 19. Despite significant advances in video generation, synthesizing physically plausible human actions remains a persistent challenge, particularly in modeling fine-grained semantics and complex temporal dynamics. For instance, generating gymnastics routines such as "switch leap with 0.5 turn" poses subs...
[20.05.2025 09:13] ********************************************************************************
[20.05.2025 09:13] Abstract 20. Human image animation has gained increasing attention and developed rapidly due to its broad applications in digital humans. However, existing methods rely largely on 2D-rendered pose images for motion guidance, which limits generalization and discards essential 3D information for open-world animati...
[20.05.2025 09:13] ********************************************************************************
[20.05.2025 09:13] Abstract 21. In recent years, the emergence of large reasoning models (LRMs), such as OpenAI-o1 and DeepSeek-R1, has shown impressive capabilities in complex problems, e.g., mathematics and coding. Some pioneering studies attempt to bring the success of LRMs in neural machine translation (MT). They try to build ...
[20.05.2025 09:13] ********************************************************************************
[20.05.2025 09:13] Abstract 22. Test-Time Scaling (TTS) refers to approaches that improve reasoning performance by allocating extra computation during inference, without altering the model's parameters. While existing TTS methods operate in a discrete token space by generating more intermediate steps, recent studies in Coconut and...
[20.05.2025 09:13] ********************************************************************************
[20.05.2025 09:13] Abstract 23. Identifying subtle technical errors within complex scientific and technical documents, especially those requiring multimodal interpretation (e.g., formulas in images), presents a significant hurdle for Large Language Models (LLMs) whose inherent error-correction tendencies can mask inaccuracies. Thi...
[20.05.2025 09:13] ********************************************************************************
[20.05.2025 09:13] Abstract 24. Accurately identifying adversarial techniques in security texts is critical for effective cyber defense. However, existing methods face a fundamental trade-off: they either rely on generic models with limited domain precision or require resource-intensive pipelines that depend on large labeled datas...
[20.05.2025 09:13] ********************************************************************************
[20.05.2025 09:13] Abstract 25. Video diffusion models (DMs) have enabled high-quality video synthesis. Yet, their substantial computational and memory demands pose serious challenges to real-world deployment, even on high-end GPUs. As a commonly adopted solution, quantization has proven notable success in reducing cost for image ...
[20.05.2025 09:13] ********************************************************************************
[20.05.2025 09:13] Abstract 26. Critical peer review of scientific manuscripts presents a significant challenge for Large Language Models (LLMs), partly due to data limitations and the complexity of expert reasoning. This report introduces Persistent Workflow Prompting (PWP), a potentially broadly applicable prompt engineering met...
[20.05.2025 09:13] Read previous papers.
[20.05.2025 09:13] Generating reviews via LLM API.
[20.05.2025 09:13] Using data from previous issue: {"categories": ["#training", "#open_source", "#inference", "#agi", "#architecture", "#optimization"], "emoji": "ğŸ”—", "ru": {"title": "Ğ¦ĞµĞ¿Ğ½Ğ°Ñ Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…: Ğ³Ğ¸Ğ±ĞºĞ¾ÑÑ‚ÑŒ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ", "desc": "Ğ’ ÑÑ‚Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Chain-of-Model (CoM), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°
[20.05.2025 09:13] Using data from previous issue: {"categories": ["#math", "#reasoning", "#inference", "#training", "#optimization", "#rl"], "emoji": "ğŸ§ ", "ru": {"title": "ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ˜Ğ˜", "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ AdaptThink, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒÑ‡Ğ¸Ñ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°Ñ‚ÑŒ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ 
[20.05.2025 09:13] Using data from previous issue: {"categories": ["#optimization", "#reasoning", "#rlhf", "#rl", "#training"], "emoji": "ğŸ§ ", "ru": {"title": "AdaCoT: Ğ£Ğ¼Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹", "desc": "AdaCoT - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ (LLM) Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ, ĞºĞ¾Ğ³Ğ´Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸
[20.05.2025 09:13] Using data from previous issue: {"categories": ["#optimization", "#long_context", "#architecture", "#inference", "#benchmark"], "emoji": "ğŸ”", "ru": {"title": "ĞšĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ñ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°Ñ…. ĞĞ²
[20.05.2025 09:13] Using data from previous issue: {"categories": ["#data", "#dataset", "#graphs", "#agents", "#benchmark", "#open_source"], "emoji": "ğŸ–¥ï¸", "ru": {"title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ˜Ğ˜ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ°Ğ¼Ğ¸", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº OSWorld-G Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğº Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°
[20.05.2025 09:13] Using data from previous issue: {"categories": ["#optimization", "#reasoning", "#rl", "#benchmark", "#training"], "emoji": "ğŸ§ ", "ru": {"title": "Ğ£Ğ¼Ğ½Ğ¾Ğµ Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºÑ€Ğ°Ñ‚ĞºĞ¸Ğ¼ Ğ¸ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ğ½ÑƒÑ‚Ñ‹Ğ¼ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Thinkless - Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰ÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°Ñ‚ÑŒ Ğ¼
[20.05.2025 09:13] Using data from previous issue: {"categories": ["#optimization", "#reasoning", "#multimodal", "#math", "#training", "#open_source", "#benchmark", "#data", "#dataset"], "emoji": "ğŸ§ ", "ru": {"title": "ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼", "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ MM-PRM, Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ°ÑÑ
[20.05.2025 09:13] Using data from previous issue: {"categories": ["#3d"], "emoji": "ğŸ¥", "ru": {"title": "Ğ“Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¹ 3D-4D Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ†ĞµĞ½", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ 3D-4DGS Ğ´Ğ»Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… 3D-ÑÑ†ĞµĞ½. ĞĞ½ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ 3D Ğ³Ğ°ÑƒÑÑĞ¸Ğ°Ğ½Ñ‹ Ğ´Ğ»Ñ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ Ğ¸ 4D Ğ³Ğ°ÑƒÑÑĞ¸Ğ°Ğ½Ñ‹ Ğ´Ğ»Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… 
[20.05.2025 09:13] Using data from previous issue: {"categories": ["#optimization", "#benchmark", "#architecture", "#reasoning", "#training", "#agi"], "emoji": "ğŸ§ ", "ru": {"title": "LatentSeek: ĞŸĞ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ Ñƒ Ğ˜Ğ˜ Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ² Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ LatentSeek - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾Ñ
[20.05.2025 09:13] Using data from previous issue: {"categories": ["#training", "#data", "#benchmark", "#security", "#optimization"], "emoji": "ğŸ”’", "ru": {"title": "FedSVD: Ğ—Ğ°Ñ‰Ğ¸Ñ‰ĞµĞ½Ğ½Ğ¾Ğµ Ñ„ĞµĞ´ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ FedSVD Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ„ĞµĞ´ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼
[20.05.2025 09:13] Using data from previous issue: {"categories": ["#training", "#optimization", "#rl", "#reasoning"], "emoji": "ğŸ§ ", "ru": {"title": "Ğ¡Ñ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ CPGD Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. CPGD Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ´Ñ€ĞµĞ¹Ñ„ 
[20.05.2025 09:13] Using data from previous issue: {"categories": ["#training", "#reasoning", "#optimization", "#benchmark", "#inference"], "emoji": "ğŸ§ ", "ru": {"title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ", "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Fractured Sampling Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ 
[20.05.2025 09:13] Using data from previous issue: {"categories": ["#multimodal", "#cv", "#benchmark", "#reasoning"], "emoji": "ğŸ§ ", "ru": {"title": "Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ", "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ VisionReasoner - ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹
[20.05.2025 09:13] Using data from previous issue: {"categories": ["#optimization", "#architecture", "#diffusion", "#training", "#open_source", "#video"], "emoji": "ğŸ¥", "ru": {"title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ (VSA) Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾
[20.05.2025 09:13] Using data from previous issue: {"categories": ["#optimization", "#reasoning", "#rag", "#multimodal"], "emoji": "ğŸ§ ", "ru": {"title": "ĞšĞ¾Ğ¼Ğ¿Ğ¸Ğ»ÑÑ†Ğ¸Ñ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ² RAG-ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…", "desc": "QCompiler - ÑÑ‚Ğ¾ Ğ½ĞµĞ¹Ñ€Ğ¾-ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ² RAG-ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ³Ñ€Ğ°Ğ¼Ğ¼
[20.05.2025 09:13] Using data from previous issue: {"categories": ["#benchmark", "#video", "#cv", "#games", "#reasoning", "#agents", "#open_source"], "emoji": "ğŸ¤–", "ru": {"title": "ViPlan: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¸ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ViPlan - Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ¸
[20.05.2025 09:13] Using data from previous issue: {"categories": ["#optimization", "#architecture", "#open_source", "#cv", "#training"], "emoji": "ğŸš€", "ru": {"title": "Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² TarFlow Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹", "desc": "Ğ”Ğ°Ğ½Ğ½Ğ°Ñ ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ TarFlow Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°
[20.05.2025 09:13] Using data from previous issue: {"categories": ["#data", "#science", "#dataset", "#benchmark"], "emoji": "ğŸ”", "ru": {"title": "Ğ‘Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ĞºĞ° Ğ½Ğµ Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ñ‹ Ğ±Ñ‹Ñ‚ÑŒ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµÑ†ĞµĞ½Ğ·ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ SPOT - Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· 83 Ğ¾Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‚ Ñ 91 Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¾Ğ¹, Ğ¿Ñ€Ğ¸Ğ²ĞµĞ´ÑˆĞµĞ¹ Ğº Ğ¾Ğ¿ĞµÑ‡Ğ°Ñ‚ĞºĞ°Ğ¼ Ğ¸Ğ»Ğ¸ 
[20.05.2025 09:13] Using data from previous issue: {"categories": ["#open_source", "#interpretability", "#cv", "#benchmark", "#synthetic", "#reasoning", "#dataset"], "emoji": "ğŸ“Š", "ru": {"title": "Ğ Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ñ‹ Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğ¸ Ğ˜Ğ˜ Ğ¿Ñ€Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğµ Ğ´Ğ¸Ğ°Ğ³Ñ€Ğ°Ğ¼Ğ¼", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ChartMuseum Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸
[20.05.2025 09:13] Using data from previous issue: {"categories": ["#3d", "#optimization", "#diffusion", "#video"], "emoji": "ğŸ¤¸", "ru": {"title": "Ğ¢Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ FinePhys - Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ¡Ğ¸ÑÑ‚Ğµ
[20.05.2025 09:13] Using data from previous issue: {"categories": ["#3d", "#video"], "emoji": "ğŸ•º", "ru": {"title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°: Ğ¾Ñ‚ 2D Ğº 4D Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ", "desc": "MTVCrafter - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ 4D Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ 2D-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾Ğ·. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ 4DMoT Ğ´Ğ»Ñ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ 3D Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²
[20.05.2025 09:13] Using data from previous issue: {"categories": ["#reasoning", "#multilingual", "#rl", "#low_resource", "#machine_translation"], "emoji": "ğŸŒ", "ru": {"title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ¼ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğµ: Ğ¾Ñ‚ Ğ¾Ğ´Ğ½Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğº Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ¼Ñƒ ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ñƒ", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼
[20.05.2025 09:13] Using data from previous issue: {"categories": ["#optimization", "#inference", "#benchmark", "#reasoning", "#training"], "emoji": "ğŸ§ ", "ru": {"title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ˜Ğ˜ Ñ‡ĞµÑ€ĞµĞ· Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ¼Ñ‹ÑĞ»Ğ¸", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ SoftCoT++, ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‰Ğ¸Ğ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ¼ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°
[20.05.2025 09:13] Using data from previous issue: {"categories": ["#science", "#multimodal", "#interpretability", "#inference"], "emoji": "ğŸ”", "ru": {"title": "ĞŸĞ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ LLM Ğ² Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¸ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ñ‡ĞµÑ€ĞµĞ· ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°", "desc": "Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·ÑƒÑ‡Ğ°ĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾
[20.05.2025 09:13] Using data from previous issue: {"categories": ["#security", "#data", "#hallucinations", "#rag", "#benchmark"], "emoji": "ğŸ›¡ï¸", "ru": {"title": "Ğ¢Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸Ğº Ğ·Ğ»Ğ¾ÑƒĞ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ½Ğ¸ĞºĞ¾Ğ² Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼ÑƒĞ¼Ğ¾Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…", "desc": "TechniqueRAG - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµÑ…Ğ½Ğ¸Ğº Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ½Ğ¸ĞºĞ¾Ğ² Ğ² Ñ‚ĞµĞºÑÑ‚Ğ°Ñ… Ğ¿Ğ¾ ĞºĞ¸Ğ±ĞµÑ€Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ñƒ
[20.05.2025 09:13] Using data from previous issue: {"categories": ["#optimization", "#diffusion", "#inference", "#video"], "emoji": "ğŸ¬", "ru": {"title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹", "desc": "QVGen - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ 
[20.05.2025 09:13] Using data from previous issue: {"categories": ["#reasoning", "#data", "#science", "#multimodal", "#interpretability", "#architecture"], "emoji": "ğŸ”¬", "ru": {"title": "PWP: ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‚ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ LLM", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Persistent Workflow
[20.05.2025 09:13] Trying to get texts in Chinese.
[20.05.2025 09:13] Mistral request. Model: mistral-large-latest. Prompt: Write simple and brief explanation (4-5 sentences) of an article in Chinese. Use short sentences. Text:

In this paper, we propose a novel learning paradigm, termed Chain-of-Model (CoM), which incorporates the causal relationship into the hidden states of each layer as a chain style, thereby introducing great scaling efficiency in model training and inference flexibility in deployment. We introduce the concept of Chain-of-Representation (CoR), which formulates the hidden states at each layer as a combination of multiple sub-representations (i.e., chains) at the hidden dimension level. In each layer, each chain from the output representations can only view all of its preceding chains in the input representations. Consequently, the model built upon CoM framework can progressively scale up the model size by increasing the chains based on the previous models (i.e., chains), and offer multiple sub-models at varying sizes for elastic inference by using different chain numbers. Based on this principle, we devise Chain-of-Language-Model (CoLM), which incorporates the idea of CoM into each layer of Transformer architecture. Based on CoLM, we further introduce CoLM-Air by introducing a KV sharing mechanism, that computes all keys and values within the first chain and then shares across all chains. This design demonstrates additional extensibility, such as enabling seamless LM switching, prefilling acceleration and so on. Experimental results demonstrate our CoLM family can achieve comparable performance to the standard Transformer, while simultaneously enabling greater flexiblity, such as progressive scaling to improve training efficiency and offer multiple varying model sizes for elastic inference, paving a a new way toward building language models. Our code will be released in the future at: https://github.com/microsoft/CoLM.
[20.05.2025 09:13] Mistral response. {"id": "e6e16fc657234a94b7d254cb47f8dc5a", "object": "chat.completion", "created": 1747732395, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "\u8fd9\u7bc7\u6587\u7ae0\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5b66\u4e60\u8303\u5f0f\uff0c\u79f0\u4e3aChain-of-Model (CoM)\u3002\u5b83\u5c06\u56e0\u679c\u5173\u7cfb\u5f15\u5165\u6bcf\u5c42\u7684\u9690\u85cf\u72b6\u6001\uff0c\u5f62\u6210\u94fe\u5f0f\u7ed3\u6784\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u8bad\u7ec3\u7684\u6269\u5c55\u6548\u7387\u548c\u90e8\u7f72\u7684\u7075\u6d3b\u6027\u3002\u4f5c\u8005\u5f15\u5165\u4e86Chain-of-Representation (CoR)\u7684\u6982\u5ff5\uff0c\u5c06\u6bcf\u5c42\u7684\u9690\u85cf\u72b6\u6001\u8868\u793a\u4e3a\u591a\u4e2a\u5b50\u8868\u793a\uff08\u5373\u94fe\uff09\u7684\u7ec4\u5408\u3002\u6bcf\u5c42\u4e2d\uff0c\u6bcf\u4e2a\u94fe\u53ea\u80fd\u67e5\u770b\u8f93\u5165\u8868\u793a\u4e2d\u7684\u6240\u6709\u524d\u5e8f\u94fe\u3002\u56e0\u6b64\uff0c\u57fa\u4e8eCoM\u6846\u67b6\u7684\u6a21\u578b\u53ef\u4ee5\u901a\u8fc7\u589e\u52a0\u94fe\u6765\u9010\u6b65\u6269\u5c55\u6a21\u578b\u5927\u5c0f\uff0c\u5e76\u901a\u8fc7\u4f7f\u7528\u4e0d\u540c\u7684\u94fe\u6570\u91cf\u63d0\u4f9b\u591a\u4e2a\u4e0d\u540c\u5927\u5c0f\u7684\u5b50\u6a21\u578b\u8fdb\u884c\u5f39\u6027\u63a8\u7406\u3002\u57fa\u4e8e\u8fd9\u4e00\u539f\u5219\uff0c\u4f5c\u8005\u8bbe\u8ba1\u4e86Chain-of-Language-Model (CoLM)\uff0c\u5e76\u8fdb\u4e00\u6b65\u5f15\u5165\u4e86KV\u5171\u4eab\u673a\u5236\u7684CoLM-Air\uff0c\u4ee5\u5b9e\u73b0\u66f4\u591a\u7684\u6269\u5c55\u529f\u80fd\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cCoLM\u7cfb\u5217\u6a21\u578b\u5728\u6027\u80fd\u4e0a\u4e0e\u6807\u51c6Transformer\u76f8\u5f53\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u66f4\u5927\u7684\u7075\u6d3b\u6027\u3002"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 390, "total_tokens": 683, "completion_tokens": 293}}
[20.05.2025 09:13] Response: è¿™ç¯‡æ–‡ç« æå‡ºäº†ä¸€ç§æ–°çš„å­¦ä¹ èŒƒå¼ï¼Œç§°ä¸ºChain-of-Model (CoM)ã€‚å®ƒå°†å› æœå…³ç³»å¼•å…¥æ¯å±‚çš„éšè—çŠ¶æ€ï¼Œå½¢æˆé“¾å¼ç»“æ„ï¼Œæé«˜äº†æ¨¡å‹è®­ç»ƒçš„æ‰©å±•æ•ˆç‡å’Œéƒ¨ç½²çš„çµæ´»æ€§ã€‚ä½œè€…å¼•å…¥äº†Chain-of-Representation (CoR)çš„æ¦‚å¿µï¼Œå°†æ¯å±‚çš„éšè—çŠ¶æ€è¡¨ç¤ºä¸ºå¤šä¸ªå­è¡¨ç¤ºï¼ˆå³é“¾ï¼‰çš„ç»„åˆã€‚æ¯å±‚ä¸­ï¼Œæ¯ä¸ªé“¾åªèƒ½æŸ¥çœ‹è¾“å…¥è¡¨ç¤ºä¸­çš„æ‰€æœ‰å‰åºé“¾ã€‚å› æ­¤ï¼ŒåŸºäºCoMæ¡†æ¶çš„æ¨¡å‹å¯ä»¥é€šè¿‡å¢åŠ é“¾æ¥é€æ­¥æ‰©å±•æ¨¡å‹å¤§å°ï¼Œå¹¶é€šè¿‡ä½¿ç”¨ä¸åŒçš„é“¾æ•°é‡æä¾›å¤šä¸ªä¸åŒå¤§å°çš„å­æ¨¡å‹è¿›è¡Œå¼¹æ€§æ¨ç†ã€‚åŸºäºè¿™ä¸€åŸåˆ™ï¼Œä½œè€…è®¾è®¡äº†Chain-of-Language-Model (CoLM)ï¼Œå¹¶è¿›ä¸€æ­¥å¼•å…¥äº†KVå…±äº«æœºåˆ¶çš„CoLM-Airï¼Œä»¥å®ç°æ›´å¤šçš„æ‰©å±•åŠŸèƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCoLMç³»åˆ—æ¨¡å‹åœ¨æ€§èƒ½ä¸Šä¸æ ‡å‡†Transformerç›¸å½“ï¼ŒåŒæ—¶æä¾›äº†æ›´å¤§çš„çµæ´»æ€§ã€‚
[20.05.2025 09:13] Mistral request. Model: mistral-large-latest. Prompt: Write pinyin transcription for text. Text:

è¿™ç¯‡æ–‡ç« æå‡ºäº†ä¸€ç§æ–°çš„å­¦ä¹ èŒƒå¼ï¼Œç§°ä¸ºChain-of-Model (CoM)ã€‚å®ƒå°†å› æœå…³ç³»å¼•å…¥æ¯å±‚çš„éšè—çŠ¶æ€ï¼Œå½¢æˆé“¾å¼ç»“æ„ï¼Œæé«˜äº†æ¨¡å‹è®­ç»ƒçš„æ‰©å±•æ•ˆç‡å’Œéƒ¨ç½²çš„çµæ´»æ€§ã€‚ä½œè€…å¼•å…¥äº†Chain-of-Representation (CoR)çš„æ¦‚å¿µï¼Œå°†æ¯å±‚çš„éšè—çŠ¶æ€è¡¨ç¤ºä¸ºå¤šä¸ªå­è¡¨ç¤ºï¼ˆå³é“¾ï¼‰çš„ç»„åˆã€‚æ¯å±‚ä¸­ï¼Œæ¯ä¸ªé“¾åªèƒ½æŸ¥çœ‹è¾“å…¥è¡¨ç¤ºä¸­çš„æ‰€æœ‰å‰åºé“¾ã€‚å› æ­¤ï¼ŒåŸºäºCoMæ¡†æ¶çš„æ¨¡å‹å¯ä»¥é€šè¿‡å¢åŠ é“¾æ¥é€æ­¥æ‰©å±•æ¨¡å‹å¤§å°ï¼Œå¹¶é€šè¿‡ä½¿ç”¨ä¸åŒçš„é“¾æ•°é‡æä¾›å¤šä¸ªä¸åŒå¤§å°çš„å­æ¨¡å‹è¿›è¡Œå¼¹æ€§æ¨ç†ã€‚åŸºäºè¿™ä¸€åŸåˆ™ï¼Œä½œè€…è®¾è®¡äº†Chain-of-Language-Model (CoLM)ï¼Œå¹¶è¿›ä¸€æ­¥å¼•å…¥äº†KVå…±äº«æœºåˆ¶çš„CoLM-Airï¼Œä»¥å®ç°æ›´å¤šçš„æ‰©å±•åŠŸèƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCoLMç³»åˆ—æ¨¡å‹åœ¨æ€§èƒ½ä¸Šä¸æ ‡å‡†Transformerç›¸å½“ï¼ŒåŒæ—¶æä¾›äº†æ›´å¤§çš„çµæ´»æ€§ã€‚
[20.05.2025 09:14] Mistral response. {"id": "2e8b42e3b89b4989926867d72c416d27", "object": "chat.completion", "created": 1747732420, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "Zh\u00e8 pi\u0101n w\u00e9nzh\u0101ng t\u00edch\u016b le y\u012bzh\u01d2ng x\u012bn de xu\u00e9x\u00ed f\u00e0nsh\u00ec, ch\u0113ngw\u00e9i Chain-of-Model (CoM). T\u0101 ji\u0101ng y\u012bngu\u01d2 gu\u0101nx\u00ec y\u01d0nr\u00f9 m\u011bi c\u00e9ng de y\u01d0nc\u00e1ng zhu\u00e0ngt\u00e0i, x\u00edngch\u00e9ng li\u00e0nsh\u00ec ji\u00e9g\u00f2u, t\u012bg\u0101o le m\u00f3x\u00edng x\u00f9nli\u00e0n de ku\u00f2zh\u01cen xi\u00e0ol\u01dc h\u00e9 b\u00f9sh\u01d4 de l\u00ednghu\u00f3x\u00ecng. Zu\u00f2zh\u011b y\u01d0nr\u00f9 le Chain-of-Representation (CoR) de g\u00e0ini\u00e0n, ji\u0101ng m\u011bi c\u00e9ng de y\u01d0nc\u00e1ng zhu\u00e0ngt\u00e0i bi\u01ceosh\u00ec w\u00e9i du\u014dg\u00e8 z\u01d0 bi\u01ceosh\u00ec (ji\u0113 li\u00e0n) de z\u01d4h\u00e9. M\u011bi c\u00e9ng zh\u014dng, m\u011bi g\u00e8 li\u00e0n zh\u01d0n\u00e9ng ch\u00e1 k\u00e0n sh\u016br\u00f9 bi\u01ceosh\u00ec zh\u014dng de su\u01d2y\u01d2u qi\u00e1nx\u00f9 li\u00e0n. Y\u012bnc\u01d0, j\u012by\u00fa CoM ku\u00e0ngji\u00e0 de m\u00f3x\u00edng k\u011by\u01d0 t\u014dnggu\u00f2 z\u0113ngji\u0101 li\u00e0n l\u00e1i zh\u00fab\u00f9 ku\u00f2zh\u01cen m\u00f3x\u00edng d\u00e0x\u00ecng, b\u00ecng t\u014dnggu\u00f2 sh\u01d0y\u00f2ng b\u00f9t\u00f3ng de li\u00e0n sh\u00f9li\u00e0ng t\u00edg\u014dng du\u014dg\u00e8 b\u00f9t\u00f3ng d\u00e0x\u00ecng de z\u01d0 m\u00f3x\u00edng j\u00ecnx\u00edng t\u00e1nx\u00ecng tu\u012bl\u01d0. J\u012by\u00fa zh\u00e8 y\u012b yu\u00e1nz\u00e9, zu\u00f2zh\u011b sh\u00e8j\u00ec le Chain-of-Language-Model (CoLM), b\u00ecng j\u00ecn y\u012bb\u00f9 y\u01d0nr\u00f9 le KV g\u00f2ngxi\u01ceng j\u012bzh\u00ec de CoLM-Air, y\u01d0 sh\u00edxi\u00e0n g\u00e8ng du\u014d de ku\u00f2zh\u01cen g\u014dngn\u00e9ng. Sh\u00edy\u00e0n ji\u00e9gu\u01d2 bi\u01ceom\u00edng, CoLM x\u00ecli\u00e8 m\u00f3x\u00edng z\u00e0i x\u00ecngn\u00e9ng sh\u00e0ng y\u01d4 bi\u0101ozh\u01d4n Transformer xi\u0101ngd\u0101ng, t\u00f3ngsh\u00ed t\u00edg\u014dng le g\u00e8ng d\u00e0 de l\u00ednghu\u00f3x\u00ecng."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 307, "total_tokens": 893, "completion_tokens": 586}}
[20.05.2025 09:14] Response: ZhÃ¨ piÄn wÃ©nzhÄng tÃ­chÅ« le yÄ«zhÇ’ng xÄ«n de xuÃ©xÃ­ fÃ nshÃ¬, chÄ“ngwÃ©i Chain-of-Model (CoM). TÄ jiÄng yÄ«nguÇ’ guÄnxÃ¬ yÇnrÃ¹ mÄ›i cÃ©ng de yÇncÃ¡ng zhuÃ ngtÃ i, xÃ­ngchÃ©ng liÃ nshÃ¬ jiÃ©gÃ²u, tÄ«gÄo le mÃ³xÃ­ng xÃ¹nliÃ n de kuÃ²zhÇn xiÃ olÇœ hÃ© bÃ¹shÇ” de lÃ­nghuÃ³xÃ¬ng. ZuÃ²zhÄ› yÇnrÃ¹ le Chain-of-Representation (CoR) de gÃ iniÃ n, jiÄng mÄ›i cÃ©ng de yÇncÃ¡ng zhuÃ ngtÃ i biÇoshÃ¬ wÃ©i duÅgÃ¨ zÇ biÇoshÃ¬ (jiÄ“ liÃ n) de zÇ”hÃ©. MÄ›i cÃ©ng zhÅng, mÄ›i gÃ¨ liÃ n zhÇnÃ©ng chÃ¡ kÃ n shÅ«rÃ¹ biÇoshÃ¬ zhÅng de suÇ’yÇ’u qiÃ¡nxÃ¹ liÃ n. YÄ«ncÇ, jÄ«yÃº CoM kuÃ ngjiÃ  de mÃ³xÃ­ng kÄ›yÇ tÅngguÃ² zÄ“ngjiÄ liÃ n lÃ¡i zhÃºbÃ¹ kuÃ²zhÇn mÃ³xÃ­ng dÃ xÃ¬ng, bÃ¬ng tÅngguÃ² shÇyÃ²ng bÃ¹tÃ³ng de liÃ n shÃ¹liÃ ng tÃ­gÅng duÅgÃ¨ bÃ¹tÃ³ng dÃ xÃ¬ng de zÇ mÃ³xÃ­ng jÃ¬nxÃ­ng tÃ¡nxÃ¬ng tuÄ«lÇ. JÄ«yÃº zhÃ¨ yÄ« yuÃ¡nzÃ©, zuÃ²zhÄ› shÃ¨jÃ¬ le Chain-of-Language-Model (CoLM), bÃ¬ng jÃ¬n yÄ«bÃ¹ yÇnrÃ¹ le KV gÃ²ngxiÇng jÄ«zhÃ¬ de CoLM-Air, yÇ shÃ­xiÃ n gÃ¨ng duÅ de kuÃ²zhÇn gÅngnÃ©ng. ShÃ­yÃ n jiÃ©guÇ’ biÇomÃ­ng, CoLM xÃ¬liÃ¨ mÃ³xÃ­ng zÃ i xÃ¬ngnÃ©ng shÃ ng yÇ” biÄozhÇ”n Transformer xiÄngdÄng, tÃ³ngshÃ­ tÃ­gÅng le gÃ¨ng dÃ  de lÃ­nghuÃ³xÃ¬ng.
[20.05.2025 09:14] Mistral request. Model: mistral-large-latest. Prompt: Write vocab of difficult words for this text as an array of objects with fields 'word', 'pinyin', 'trans'. Return as python list without formatting. Return list and nothing else. Text:

è¿™ç¯‡æ–‡ç« æå‡ºäº†ä¸€ç§æ–°çš„å­¦ä¹ èŒƒå¼ï¼Œç§°ä¸ºChain-of-Model (CoM)ã€‚å®ƒå°†å› æœå…³ç³»å¼•å…¥æ¯å±‚çš„éšè—çŠ¶æ€ï¼Œå½¢æˆé“¾å¼ç»“æ„ï¼Œæé«˜äº†æ¨¡å‹è®­ç»ƒçš„æ‰©å±•æ•ˆç‡å’Œéƒ¨ç½²çš„çµæ´»æ€§ã€‚ä½œè€…å¼•å…¥äº†Chain-of-Representation (CoR)çš„æ¦‚å¿µï¼Œå°†æ¯å±‚çš„éšè—çŠ¶æ€è¡¨ç¤ºä¸ºå¤šä¸ªå­è¡¨ç¤ºï¼ˆå³é“¾ï¼‰çš„ç»„åˆã€‚æ¯å±‚ä¸­ï¼Œæ¯ä¸ªé“¾åªèƒ½æŸ¥çœ‹è¾“å…¥è¡¨ç¤ºä¸­çš„æ‰€æœ‰å‰åºé“¾ã€‚å› æ­¤ï¼ŒåŸºäºCoMæ¡†æ¶çš„æ¨¡å‹å¯ä»¥é€šè¿‡å¢åŠ é“¾æ¥é€æ­¥æ‰©å±•æ¨¡å‹å¤§å°ï¼Œå¹¶é€šè¿‡ä½¿ç”¨ä¸åŒçš„é“¾æ•°é‡æä¾›å¤šä¸ªä¸åŒå¤§å°çš„å­æ¨¡å‹è¿›è¡Œå¼¹æ€§æ¨ç†ã€‚åŸºäºè¿™ä¸€åŸåˆ™ï¼Œä½œè€…è®¾è®¡äº†Chain-of-Language-Model (CoLM)ï¼Œå¹¶è¿›ä¸€æ­¥å¼•å…¥äº†KVå…±äº«æœºåˆ¶çš„CoLM-Airï¼Œä»¥å®ç°æ›´å¤šçš„æ‰©å±•åŠŸèƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCoLMç³»åˆ—æ¨¡å‹åœ¨æ€§èƒ½ä¸Šä¸æ ‡å‡†Transformerç›¸å½“ï¼ŒåŒæ—¶æä¾›äº†æ›´å¤§çš„çµæ´»æ€§ã€‚
[20.05.2025 09:14] Mistral response. {"id": "6ba1afc0db4d456b804e2d79a280c2f8", "object": "chat.completion", "created": 1747732440, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "[\n    {\"word\": \"\u8303\u5f0f\", \"pinyin\": \"f\u00e0n sh\u00ec\", \"trans\": \"paradigm\"},\n    {\"word\": \"Chain-of-Model\", \"pinyin\": \"Ch\u00e8in-\u00f2f-M\u00f3del\", \"trans\": \"Chain-of-Model\"},\n    {\"word\": \"\u56e0\u679c\u5173\u7cfb\", \"pinyin\": \"y\u012bn gu\u01d2 gu\u0101n x\u00ec\", \"trans\": \"causal relationship\"},\n    {\"word\": \"\u9690\u85cf\u72b6\u6001\", \"pinyin\": \"y\u01d0n c\u00e1ng zhu\u00e0ng t\u00e0i\", \"trans\": \"hidden state\"},\n    {\"word\": \"\u94fe\u5f0f\u7ed3\u6784\", \"pinyin\": \"li\u00e0n sh\u00ec ji\u00e9g\u00f2u\", \"trans\": \"chain structure\"},\n    {\"word\": \"\u6269\u5c55\u6548\u7387\", \"pinyin\": \"ku\u00f2 zh\u01cen xi\u00e0o l\u01dc\", \"trans\": \"scalability\"},\n    {\"word\": \"\u90e8\u7f72\", \"pinyin\": \"b\u00f9 sh\u01d4\", \"trans\": \"deployment\"},\n    {\"word\": \"\u7075\u6d3b\u6027\", \"pinyin\": \"l\u00edng hu\u00f3 x\u00ecng\", \"trans\": \"flexibility\"},\n    {\"word\": \"Chain-of-Representation\", \"pinyin\": \"Ch\u00e8in-\u00f2f-R\u011bprizen t\u00e9i sh\u0113n\", \"trans\": \"Chain-of-Representation\"},\n    {\"word\": \"\u5b50\u8868\u793a\", \"pinyin\": \"z\u01d0 bi\u01ceo sh\u00ec\", \"trans\": \"sub-representation\"},\n    {\"word\": \"\u7ec4\u5408\", \"pinyin\": \"z\u01d4 h\u00e9\", \"trans\": \"combination\"},\n    {\"word\": \"\u524d\u5e8f\u94fe\", \"pinyin\": \"qi\u00e1n x\u00f9 li\u00e0n\", \"trans\": \"preceding chain\"},\n    {\"word\": \"\u5f39\u6027\u63a8\u7406\", \"pinyin\": \"t\u00e1n x\u00ecng tu\u012b l\u01d0\", \"trans\": \"elastic inference\"},\n    {\"word\": \"Chain-of-Language-Model\", \"pinyin\": \"Ch\u00e8in-\u00f2f-L\u00e1ngg\u00f9 M\u00f3del\", \"trans\": \"Chain-of-Language-Model\"},\n    {\"word\": \"KV\u5171\u4eab\u673a\u5236\", \"pinyin\": \"KV g\u00f2ng xi\u01ceng j\u012b zh\u00ec\", \"trans\": \"KV sharing mechanism\"},\n    {\"word\": \"CoLM-Air\", \"pinyin\": \"CoLM-\u00c9ir\", \"trans\": \"CoLM-Air\"},\n    {\"word\": \"\u6269\u5c55\u529f\u80fd\", \"pinyin\": \"ku\u00f2 zh\u01cen g\u014dng n\u00e9ng\", \"trans\": \"extended functionality\"},\n    {\"word\": \"Transformer\", \"pinyin\": \"T\u00e8insh\u00e8in f\u014dm\u011bi\", \"trans\": \"Transformer\"}\n]"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 339, "total_tokens": 991, "completion_tokens": 652}}
[20.05.2025 09:14] Response: [
    {"word": "èŒƒå¼", "pinyin": "fÃ n shÃ¬", "trans": "paradigm"},
    {"word": "Chain-of-Model", "pinyin": "ChÃ¨in-Ã²f-MÃ³del", "trans": "Chain-of-Model"},
    {"word": "å› æœå…³ç³»", "pinyin": "yÄ«n guÇ’ guÄn xÃ¬", "trans": "causal relationship"},
    {"word": "éšè—çŠ¶æ€", "pinyin": "yÇn cÃ¡ng zhuÃ ng tÃ i", "trans": "hidden state"},
    {"word": "é“¾å¼ç»“æ„", "pinyin": "liÃ n shÃ¬ jiÃ©gÃ²u", "trans": "chain structure"},
    {"word": "æ‰©å±•æ•ˆç‡", "pinyin": "kuÃ² zhÇn xiÃ o lÇœ", "trans": "scalability"},
    {"word": "éƒ¨ç½²", "pinyin": "bÃ¹ shÇ”", "trans": "deployment"},
    {"word": "çµæ´»æ€§", "pinyin": "lÃ­ng huÃ³ xÃ¬ng", "trans": "flexibility"},
    {"word": "Chain-of-Representation", "pinyin": "ChÃ¨in-Ã²f-RÄ›prizen tÃ©i shÄ“n", "trans": "Chain-of-Representation"},
    {"word": "å­è¡¨ç¤º", "pinyin": "zÇ biÇo shÃ¬", "trans": "sub-representation"},
    {"word": "ç»„åˆ", "pinyin": "zÇ” hÃ©", "trans": "combination"},
    {"word": "å‰åºé“¾", "pinyin": "qiÃ¡n xÃ¹ liÃ n", "trans": "preceding chain"},
    {"word": "å¼¹æ€§æ¨ç†", "pinyin": "tÃ¡n xÃ¬ng tuÄ« lÇ", "trans": "elastic inference"},
    {"word": "Chain-of-Language-Model", "pinyin": "ChÃ¨in-Ã²f-LÃ¡nggÃ¹ MÃ³del", "trans": "Chain-of-Language-Model"},
    {"word": "KVå…±äº«æœºåˆ¶", "pinyin": "KV gÃ²ng xiÇng jÄ« zhÃ¬", "trans": "KV sharing mechanism"},
    {"word": "CoLM-Air", "pinyin": "CoLM-Ã‰ir", "trans": "CoLM-Air"},
    {"word": "æ‰©å±•åŠŸèƒ½", "pinyin": "kuÃ² zhÇn gÅng nÃ©ng", "trans": "extended functionality"},
    {"word": "Transformer", "pinyin": "TÃ¨inshÃ¨in fÅmÄ›i", "trans": "Transformer"}
]
[20.05.2025 09:14] Mistral request. Model: mistral-large-latest. Prompt: Translate this text in English. Text:

è¿™ç¯‡æ–‡ç« æå‡ºäº†ä¸€ç§æ–°çš„å­¦ä¹ èŒƒå¼ï¼Œç§°ä¸ºChain-of-Model (CoM)ã€‚å®ƒå°†å› æœå…³ç³»å¼•å…¥æ¯å±‚çš„éšè—çŠ¶æ€ï¼Œå½¢æˆé“¾å¼ç»“æ„ï¼Œæé«˜äº†æ¨¡å‹è®­ç»ƒçš„æ‰©å±•æ•ˆç‡å’Œéƒ¨ç½²çš„çµæ´»æ€§ã€‚ä½œè€…å¼•å…¥äº†Chain-of-Representation (CoR)çš„æ¦‚å¿µï¼Œå°†æ¯å±‚çš„éšè—çŠ¶æ€è¡¨ç¤ºä¸ºå¤šä¸ªå­è¡¨ç¤ºï¼ˆå³é“¾ï¼‰çš„ç»„åˆã€‚æ¯å±‚ä¸­ï¼Œæ¯ä¸ªé“¾åªèƒ½æŸ¥çœ‹è¾“å…¥è¡¨ç¤ºä¸­çš„æ‰€æœ‰å‰åºé“¾ã€‚å› æ­¤ï¼ŒåŸºäºCoMæ¡†æ¶çš„æ¨¡å‹å¯ä»¥é€šè¿‡å¢åŠ é“¾æ¥é€æ­¥æ‰©å±•æ¨¡å‹å¤§å°ï¼Œå¹¶é€šè¿‡ä½¿ç”¨ä¸åŒçš„é“¾æ•°é‡æä¾›å¤šä¸ªä¸åŒå¤§å°çš„å­æ¨¡å‹è¿›è¡Œå¼¹æ€§æ¨ç†ã€‚åŸºäºè¿™ä¸€åŸåˆ™ï¼Œä½œè€…è®¾è®¡äº†Chain-of-Language-Model (CoLM)ï¼Œå¹¶è¿›ä¸€æ­¥å¼•å…¥äº†KVå…±äº«æœºåˆ¶çš„CoLM-Airï¼Œä»¥å®ç°æ›´å¤šçš„æ‰©å±•åŠŸèƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCoLMç³»åˆ—æ¨¡å‹åœ¨æ€§èƒ½ä¸Šä¸æ ‡å‡†Transformerç›¸å½“ï¼ŒåŒæ—¶æä¾›äº†æ›´å¤§çš„çµæ´»æ€§ã€‚
[20.05.2025 09:14] Mistral response. {"id": "26669f72ac734b58b58660479ddf0772", "object": "chat.completion", "created": 1747732446, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "This article proposes a new learning paradigm called Chain-of-Model (CoM). It introduces causality into the hidden states of each layer, forming a chain-like structure that enhances the scalability of model training and the flexibility of deployment. The authors introduce the concept of Chain-of-Representation (CoR), representing the hidden states of each layer as a combination of multiple sub-representations (i.e., chains). Within each layer, each chain can only view all preceding chains in the input representation. Therefore, models based on the CoM framework can incrementally scale the model size by adding chains and provide multiple sub-models of different sizes for elastic inference by using different numbers of chains. Based on this principle, the authors designed Chain-of-Language-Model (CoLM) and further introduced CoLM-Air with a KV sharing mechanism to achieve more scalable functionalities. Experimental results show that the CoLM series models perform comparably to standard Transformers while offering greater flexibility."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 305, "total_tokens": 521, "completion_tokens": 216}}
[20.05.2025 09:14] Response: This article proposes a new learning paradigm called Chain-of-Model (CoM). It introduces causality into the hidden states of each layer, forming a chain-like structure that enhances the scalability of model training and the flexibility of deployment. The authors introduce the concept of Chain-of-Representation (CoR), representing the hidden states of each layer as a combination of multiple sub-representations (i.e., chains). Within each layer, each chain can only view all preceding chains in the input representation. Therefore, models based on the CoM framework can incrementally scale the model size by adding chains and provide multiple sub-models of different sizes for elastic inference by using different numbers of chains. Based on this principle, the authors designed Chain-of-Language-Model (CoLM) and further introduced CoLM-Air with a KV sharing mechanism to achieve more scalable functionalities. Experimental results show that the CoLM series models perform comparably to standard Transformers while offering greater flexibility.
[20.05.2025 09:14] Renaming data file.
[20.05.2025 09:14] Renaming previous data. hf_papers.json to ./d/2025-05-20.json
[20.05.2025 09:14] Saving new data file.
[20.05.2025 09:14] Generating page.
[20.05.2025 09:14] Renaming previous page.
[20.05.2025 09:14] Renaming previous data. index.html to ./d/2025-05-20.html
[20.05.2025 09:14] [Experimental] Generating Chinese page for reading.
[20.05.2025 09:14] Chinese vocab [{'word': 'èŒƒå¼', 'pinyin': 'fÃ n shÃ¬', 'trans': 'paradigm'}, {'word': 'Chain-of-Model', 'pinyin': 'ChÃ¨in-Ã²f-MÃ³del', 'trans': 'Chain-of-Model'}, {'word': 'å› æœå…³ç³»', 'pinyin': 'yÄ«n guÇ’ guÄn xÃ¬', 'trans': 'causal relationship'}, {'word': 'éšè—çŠ¶æ€', 'pinyin': 'yÇn cÃ¡ng zhuÃ ng tÃ i', 'trans': 'hidden state'}, {'word': 'é“¾å¼ç»“æ„', 'pinyin': 'liÃ n shÃ¬ jiÃ©gÃ²u', 'trans': 'chain structure'}, {'word': 'æ‰©å±•æ•ˆç‡', 'pinyin': 'kuÃ² zhÇn xiÃ o lÇœ', 'trans': 'scalability'}, {'word': 'éƒ¨ç½²', 'pinyin': 'bÃ¹ shÇ”', 'trans': 'deployment'}, {'word': 'çµæ´»æ€§', 'pinyin': 'lÃ­ng huÃ³ xÃ¬ng', 'trans': 'flexibility'}, {'word': 'Chain-of-Representation', 'pinyin': 'ChÃ¨in-Ã²f-RÄ›prizen tÃ©i shÄ“n', 'trans': 'Chain-of-Representation'}, {'word': 'å­è¡¨ç¤º', 'pinyin': 'zÇ biÇo shÃ¬', 'trans': 'sub-representation'}, {'word': 'ç»„åˆ', 'pinyin': 'zÇ” hÃ©', 'trans': 'combination'}, {'word': 'å‰åºé“¾', 'pinyin': 'qiÃ¡n xÃ¹ liÃ n', 'trans': 'preceding chain'}, {'word': 'å¼¹æ€§æ¨ç†', 'pinyin': 'tÃ¡n xÃ¬ng tuÄ« lÇ', 'trans': 'elastic inference'}, {'word': 'Chain-of-Language-Model', 'pinyin': 'ChÃ¨in-Ã²f-LÃ¡nggÃ¹ MÃ³del', 'trans': 'Chain-of-Language-Model'}, {'word': 'KVå…±äº«æœºåˆ¶', 'pinyin': 'KV gÃ²ng xiÇng jÄ« zhÃ¬', 'trans': 'KV sharing mechanism'}, {'word': 'CoLM-Air', 'pinyin': 'CoLM-Ã‰ir', 'trans': 'CoLM-Air'}, {'word': 'æ‰©å±•åŠŸèƒ½', 'pinyin': 'kuÃ² zhÇn gÅng nÃ©ng', 'trans': 'extended functionality'}, {'word': 'Transformer', 'pinyin': 'TÃ¨inshÃ¨in fÅmÄ›i', 'trans': 'Transformer'}]
[20.05.2025 09:14] Renaming previous Chinese page.
[20.05.2025 09:14] Renaming previous data. zh.html to ./d/2025-05-19_zh_reading_task.html
[20.05.2025 09:14] Writing Chinese reading task.
[20.05.2025 09:14] Writing result.
[20.05.2025 09:14] Renaming log file.
[20.05.2025 09:14] Renaming previous data. log.txt to ./logs/2025-05-20_last_log.txt
