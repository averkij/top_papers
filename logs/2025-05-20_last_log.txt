[20.05.2025 07:13] Read previous papers.
[20.05.2025 07:13] Generating top page (month).
[20.05.2025 07:13] Writing top page (month).
[20.05.2025 08:16] Read previous papers.
[20.05.2025 08:16] Get feed.
[20.05.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.11820
[20.05.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.13417
[20.05.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.11896
[20.05.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.11254
[20.05.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.13227
[20.05.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.13379
[20.05.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.13427
[20.05.2025 08:16] Extract page data from URL. URL: https://huggingface.co/papers/2505.13308
[20.05.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.13215
[20.05.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.12805
[20.05.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.12504
[20.05.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.12992
[20.05.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.12081
[20.05.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.11932
[20.05.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.13389
[20.05.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.12849
[20.05.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.11855
[20.05.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.13444
[20.05.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.13437
[20.05.2025 08:16] Extract page data from URL. URL: https://huggingface.co/papers/2505.13180
[20.05.2025 08:16] Extract page data from URL. URL: https://huggingface.co/papers/2505.10238
[20.05.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.12996
[20.05.2025 08:16] Extract page data from URL. URL: https://huggingface.co/papers/2505.11484
[20.05.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.12257
[20.05.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.11988
[20.05.2025 08:16] Extract page data from URL. URL: https://huggingface.co/papers/2505.11497
[20.05.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.03332
[20.05.2025 08:16] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[20.05.2025 08:16] No deleted papers detected.
[20.05.2025 08:16] Downloading and parsing papers (pdf, html). Total: 27.
[20.05.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2505.11820.
[20.05.2025 08:16] Extra JSON file exists (./assets/json/2505.11820.json), skip PDF parsing.
[20.05.2025 08:16] Paper image links file exists (./assets/img_data/2505.11820.json), skip HTML parsing.
[20.05.2025 08:16] Success.
[20.05.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2505.13417.
[20.05.2025 08:16] Extra JSON file exists (./assets/json/2505.13417.json), skip PDF parsing.
[20.05.2025 08:16] Paper image links file exists (./assets/img_data/2505.13417.json), skip HTML parsing.
[20.05.2025 08:16] Success.
[20.05.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2505.11896.
[20.05.2025 08:16] Extra JSON file exists (./assets/json/2505.11896.json), skip PDF parsing.
[20.05.2025 08:16] Paper image links file exists (./assets/img_data/2505.11896.json), skip HTML parsing.
[20.05.2025 08:16] Success.
[20.05.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2505.11254.
[20.05.2025 08:16] Extra JSON file exists (./assets/json/2505.11254.json), skip PDF parsing.
[20.05.2025 08:16] Paper image links file exists (./assets/img_data/2505.11254.json), skip HTML parsing.
[20.05.2025 08:16] Success.
[20.05.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2505.13227.
[20.05.2025 08:16] Extra JSON file exists (./assets/json/2505.13227.json), skip PDF parsing.
[20.05.2025 08:16] Paper image links file exists (./assets/img_data/2505.13227.json), skip HTML parsing.
[20.05.2025 08:16] Success.
[20.05.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2505.13379.
[20.05.2025 08:16] Extra JSON file exists (./assets/json/2505.13379.json), skip PDF parsing.
[20.05.2025 08:16] Paper image links file exists (./assets/img_data/2505.13379.json), skip HTML parsing.
[20.05.2025 08:16] Success.
[20.05.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2505.13427.
[20.05.2025 08:16] Extra JSON file exists (./assets/json/2505.13427.json), skip PDF parsing.
[20.05.2025 08:16] Paper image links file exists (./assets/img_data/2505.13427.json), skip HTML parsing.
[20.05.2025 08:16] Success.
[20.05.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2505.13308.
[20.05.2025 08:16] Downloading paper 2505.13308 from http://arxiv.org/pdf/2505.13308v1...
[20.05.2025 08:16] Extracting affiliations from text.
[20.05.2025 08:16] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 9 1 ] . [ 1 8 0 3 3 1 . 5 0 5 2 : r Seek in the Dark: Reasoning via Test-Time Instance-Level Policy Gradient in Latent Space May 20, 2025 Hengli Li 1,2 , Chenxi Li 2,3 , Tong Wu 2, Xuekai Zhu 2,4, Yuxuan Wang 2, Zhaoxin Yu 5, Eric Hanchen Jiang 6, Song-Chun Zhu 1,2,3, Zixia Jia 2, Ying Nian Wu 6 (cid:0) and Zilong Zheng 2 (cid:0) 1 Institute for Artificial Intelligence, Peking University 2 NLCo Lab, Beijing Institute for General Artificial Intelligence 3 Department of Automation, Tsinghua University 5 Institute of Automation, Chinese Academy of Sciences 6 University of California, Los Angeles 4 Shanghai Jiao Tong University lihengli@stu.pku.edu.cn, lichenxi23@mails.tsinghua.edu.cn, ywu@stat.ucla.edu, zlzheng@bigai.ai Reasoning ability, core component of human intelligence, continues to pose significant challenge for Large Language Models (LLMs) in the pursuit of AGI. Although model performance has improved under the training scaling law, significant challenges remain, particularly with respect to training algorithmssuch as catastrophic forgettingand the limited availability of novel training data. As an alternative, test-time scaling enhances reasoning performance by increasing test-time computation without parameter updating. Unlike prior methods in this paradigm focused on token space, we propose leveraging latent space for more effective reasoning and better adherence to the test-time scaling law. We introduce LATENTSEEK, novel framework that enhances LLM reasoning through Test-Time Instance-level Adaptation (TTIA) within the models latent space. Specifically, LATENTSEEK leverages policy gradient to iteratively update latent representations, guided by self-generated reward signals. LATENTSEEK is evaluated on range of reasoning benchmarks, including GSM8K, MATH-500, and AIME2024, across multiple LLM architectures. Results show that LATENTSEEK consistently outperforms strong baselines, such as Chain-of-Thought prompting and fine-tuning-based methods. Furthe"
[20.05.2025 08:16] Response: ```python
[
    "Institute for Artificial Intelligence, Peking University",
    "NLCo Lab, Beijing Institute for General Artificial Intelligence",
    "Department of Automation, Tsinghua University",
    "Institute of Automation, Chinese Academy of Sciences",
    "University of California, Los Angeles",
    "Shanghai Jiao Tong University"
]
```
[20.05.2025 08:16] Deleting PDF ./assets/pdf/2505.13308.pdf.
[20.05.2025 08:16] Success.
[20.05.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2505.13215.
[20.05.2025 08:16] Extra JSON file exists (./assets/json/2505.13215.json), skip PDF parsing.
[20.05.2025 08:16] Paper image links file exists (./assets/img_data/2505.13215.json), skip HTML parsing.
[20.05.2025 08:16] Success.
[20.05.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2505.12805.
[20.05.2025 08:16] Extra JSON file exists (./assets/json/2505.12805.json), skip PDF parsing.
[20.05.2025 08:16] Paper image links file exists (./assets/img_data/2505.12805.json), skip HTML parsing.
[20.05.2025 08:16] Success.
[20.05.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2505.12504.
[20.05.2025 08:16] Extra JSON file exists (./assets/json/2505.12504.json), skip PDF parsing.
[20.05.2025 08:16] Paper image links file exists (./assets/img_data/2505.12504.json), skip HTML parsing.
[20.05.2025 08:16] Success.
[20.05.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2505.12992.
[20.05.2025 08:16] Extra JSON file exists (./assets/json/2505.12992.json), skip PDF parsing.
[20.05.2025 08:16] Paper image links file exists (./assets/img_data/2505.12992.json), skip HTML parsing.
[20.05.2025 08:16] Success.
[20.05.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2505.12081.
[20.05.2025 08:16] Extra JSON file exists (./assets/json/2505.12081.json), skip PDF parsing.
[20.05.2025 08:16] Paper image links file exists (./assets/img_data/2505.12081.json), skip HTML parsing.
[20.05.2025 08:16] Success.
[20.05.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2505.11932.
[20.05.2025 08:16] Extra JSON file exists (./assets/json/2505.11932.json), skip PDF parsing.
[20.05.2025 08:16] Paper image links file exists (./assets/img_data/2505.11932.json), skip HTML parsing.
[20.05.2025 08:16] Success.
[20.05.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2505.13389.
[20.05.2025 08:16] Extra JSON file exists (./assets/json/2505.13389.json), skip PDF parsing.
[20.05.2025 08:16] Paper image links file exists (./assets/img_data/2505.13389.json), skip HTML parsing.
[20.05.2025 08:16] Success.
[20.05.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2505.12849.
[20.05.2025 08:16] Extra JSON file exists (./assets/json/2505.12849.json), skip PDF parsing.
[20.05.2025 08:16] Paper image links file exists (./assets/img_data/2505.12849.json), skip HTML parsing.
[20.05.2025 08:16] Success.
[20.05.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2505.11855.
[20.05.2025 08:16] Extra JSON file exists (./assets/json/2505.11855.json), skip PDF parsing.
[20.05.2025 08:16] Paper image links file exists (./assets/img_data/2505.11855.json), skip HTML parsing.
[20.05.2025 08:16] Success.
[20.05.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2505.13444.
[20.05.2025 08:16] Extra JSON file exists (./assets/json/2505.13444.json), skip PDF parsing.
[20.05.2025 08:16] Paper image links file exists (./assets/img_data/2505.13444.json), skip HTML parsing.
[20.05.2025 08:16] Success.
[20.05.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2505.13437.
[20.05.2025 08:16] Extra JSON file exists (./assets/json/2505.13437.json), skip PDF parsing.
[20.05.2025 08:16] Paper image links file exists (./assets/img_data/2505.13437.json), skip HTML parsing.
[20.05.2025 08:16] Success.
[20.05.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2505.13180.
[20.05.2025 08:16] Downloading paper 2505.13180 from http://arxiv.org/pdf/2505.13180v1...
[20.05.2025 08:16] Extracting affiliations from text.
[20.05.2025 08:16] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"ViPlan: Benchmark for Visual Planning with Symbolic Predicates and Vision-Language Models Matteo Merler1,2 , Nicola Dainese1 , Minttu Alakuijala1, Giovanni Bonetta2, 5 2 0 2 9 ] . [ 1 0 8 1 3 1 . 5 0 5 2 : r Pietro Ferrazzi2,3, Yu Tian1, Bernardo Magnini2 1Department of Computer Science, Aalto University , Pekka Marttinen1 2Fondazione Bruno Kessler 3Department of Mathematics, Università degli Studi di Padova {mmerler, gbonetta, pferrazzi, magnini}@fbk.eu {nicola.dainese, minttu.alakuijala, yu.tian, pekka.marttinen}@aalto.fi "
[20.05.2025 08:16] Response: ```python
[
    "Department of Computer Science, Aalto University",
    "Fondazione Bruno Kessler",
    "Department of Mathematics, Università degli Studi di Padova"
]
```
[20.05.2025 08:16] Deleting PDF ./assets/pdf/2505.13180.pdf.
[20.05.2025 08:16] Success.
[20.05.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2505.10238.
[20.05.2025 08:16] Downloading paper 2505.10238 from http://arxiv.org/pdf/2505.10238v2...
[20.05.2025 08:16] Extracting affiliations from text.
[20.05.2025 08:16] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 6 1 ] . [ 2 8 3 2 0 1 . 5 0 5 2 : r MTVCrafter: 4D Motion Tokenization for Open-World Human Image Animation Yanbo Ding, Xirui Hu , Zhizhi Guo , Yali Wang {yb.ding, yl.wang}@siat.ac.cn 2392027275@stu.xjtu.edu.cn guozz2@chinatelecom.cn Figure 1: We propose MTVCrafter, which can effectively transfer pose sequences from driven video to diverse, unseen single or multiple characters in either full-body or half-body settings across various styles such as anime, pixel art, ink drawings, and photorealism, outperforming existing state-of-the-art methods in generation robustness and generalizability to open-world scenarios. "
[20.05.2025 08:16] Response: ```python
["siat.ac.cn", "xjtu.edu.cn", "chinatelecom.cn"]
```
[20.05.2025 08:16] Deleting PDF ./assets/pdf/2505.10238.pdf.
[20.05.2025 08:17] Success.
[20.05.2025 08:17] Downloading and parsing paper https://huggingface.co/papers/2505.12996.
[20.05.2025 08:17] Extra JSON file exists (./assets/json/2505.12996.json), skip PDF parsing.
[20.05.2025 08:17] Paper image links file exists (./assets/img_data/2505.12996.json), skip HTML parsing.
[20.05.2025 08:17] Success.
[20.05.2025 08:17] Downloading and parsing paper https://huggingface.co/papers/2505.11484.
[20.05.2025 08:17] Downloading paper 2505.11484 from http://arxiv.org/pdf/2505.11484v1...
[20.05.2025 08:17] Extracting affiliations from text.
[20.05.2025 08:17] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 6 1 ] . [ 1 4 8 4 1 1 . 5 0 5 2 : r SoftCoT++: Test-Time Scaling with Soft Chain-of-Thought Reasoning Yige Xu1,3, Xu Guo2,4, Zhiwei Zeng2, Chunyan Miao1,2,3 1Joint NTU-UBC Research Centre of Excellence in Active Living for the Elderly 2Alibaba-NTU Global e-Sustainability CorpLab (ANGEL) 3College of Computing and Data Science, Nanyang Technological University, Singapore 4KTH Royal Institute of Technology, Sweden {yige002,xu008}@e.ntu.edu.sg, {zhiwei.zeng,ascymiao}@ntu.edu.sg "
[20.05.2025 08:17] Response: ```python
[
    "Joint NTU-UBC Research Centre of Excellence in Active Living for the Elderly",
    "Alibaba-NTU Global e-Sustainability CorpLab (ANGEL)",
    "College of Computing and Data Science, Nanyang Technological University, Singapore",
    "KTH Royal Institute of Technology, Sweden"
]
```
[20.05.2025 08:17] Deleting PDF ./assets/pdf/2505.11484.pdf.
[20.05.2025 08:17] Success.
[20.05.2025 08:17] Downloading and parsing paper https://huggingface.co/papers/2505.12257.
[20.05.2025 08:17] Extra JSON file exists (./assets/json/2505.12257.json), skip PDF parsing.
[20.05.2025 08:17] Paper image links file exists (./assets/img_data/2505.12257.json), skip HTML parsing.
[20.05.2025 08:17] Success.
[20.05.2025 08:17] Downloading and parsing paper https://huggingface.co/papers/2505.11988.
[20.05.2025 08:17] Extra JSON file exists (./assets/json/2505.11988.json), skip PDF parsing.
[20.05.2025 08:17] Paper image links file exists (./assets/img_data/2505.11988.json), skip HTML parsing.
[20.05.2025 08:17] Success.
[20.05.2025 08:17] Downloading and parsing paper https://huggingface.co/papers/2505.11497.
[20.05.2025 08:17] Downloading paper 2505.11497 from http://arxiv.org/pdf/2505.11497v1...
[20.05.2025 08:17] Extracting affiliations from text.
[20.05.2025 08:17] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 6 1 ] . [ 1 7 9 4 1 1 . 5 0 5 2 : r QVGen: Pushing the Limit of Quantized Video Generative Models Yushi Huang1, 2 Ruihao Gong2, 3 Jing Liu4 Yifu Ding3 Chengtao Lv2 Haotong Qin5 Jun Zhang1 1Hong Kong University of Science and Technology 2SenseTime Research 3Beihang University 4Monash University 5ETH Zürich {huangyushi1, gongruihao, lvchengtao}@sensetime.com liujing_95@outlook.com haotong.qin@pbl.ee.ethz.ch yifuding@buaa.edu.cn eejzhang@ust.hk "
[20.05.2025 08:17] Response: ```python
[
    "Hong Kong University of Science and Technology",
    "SenseTime Research",
    "Beihang University",
    "Monash University",
    "ETH Zürich"
]
```
[20.05.2025 08:17] Deleting PDF ./assets/pdf/2505.11497.pdf.
[20.05.2025 08:17] Success.
[20.05.2025 08:17] Downloading and parsing paper https://huggingface.co/papers/2505.03332.
[20.05.2025 08:17] Extra JSON file exists (./assets/json/2505.03332.json), skip PDF parsing.
[20.05.2025 08:17] Paper image links file exists (./assets/img_data/2505.03332.json), skip HTML parsing.
[20.05.2025 08:17] Success.
[20.05.2025 08:17] Enriching papers with extra data.
[20.05.2025 08:17] ********************************************************************************
[20.05.2025 08:17] Abstract 0. In this paper, we propose a novel learning paradigm, termed Chain-of-Model (CoM), which incorporates the causal relationship into the hidden states of each layer as a chain style, thereby introducing great scaling efficiency in model training and inference flexibility in deployment. We introduce the...
[20.05.2025 08:17] ********************************************************************************
[20.05.2025 08:17] Abstract 1. Recently, large reasoning models have achieved impressive performance on various tasks by employing human-like deep thinking. However, the lengthy thinking process substantially increases inference overhead, making efficiency a critical bottleneck. In this work, we first demonstrate that NoThinking,...
[20.05.2025 08:17] ********************************************************************************
[20.05.2025 08:17] Abstract 2. Large Language Models (LLMs) have demonstrated remarkable capabilities but often face challenges with tasks requiring sophisticated reasoning. While Chain-of-Thought (CoT) prompting significantly enhances reasoning, it indiscriminately generates lengthy reasoning steps for all queries, leading to su...
[20.05.2025 08:17] ********************************************************************************
[20.05.2025 08:17] Abstract 3. The attention mechanism of a transformer has a quadratic complexity, leading to high inference costs and latency for long sequences. However, attention matrices are mostly sparse, which implies that many entries may be omitted from computation for efficient inference. Sparse attention inference meth...
[20.05.2025 08:17] ********************************************************************************
[20.05.2025 08:17] Abstract 4. Graphical user interface (GUI) grounding, the ability to map natural language instructions to specific actions on graphical user interfaces, remains a critical bottleneck in computer use agent development. Current benchmarks oversimplify grounding tasks as short referring expressions, failing to cap...
[20.05.2025 08:17] ********************************************************************************
[20.05.2025 08:17] Abstract 5. Reasoning Language Models, capable of extended chain-of-thought reasoning, have demonstrated remarkable performance on tasks requiring complex logical inference. However, applying elaborate reasoning for all queries often results in substantial computational inefficiencies, particularly when many pr...
[20.05.2025 08:17] ********************************************************************************
[20.05.2025 08:17] Abstract 6. While Multimodal Large Language Models (MLLMs) have achieved impressive progress in vision-language understanding, they still struggle with complex multi-step reasoning, often producing logically inconsistent or partially correct solutions. A key limitation lies in the lack of fine-grained supervisi...
[20.05.2025 08:17] ********************************************************************************
[20.05.2025 08:17] Abstract 7. Reasoning ability, a core component of human intelligence, continues to pose a significant challenge for Large Language Models (LLMs) in the pursuit of AGI. Although model performance has improved under the training scaling law, significant challenges remain, particularly with respect to training al...
[20.05.2025 08:17] ********************************************************************************
[20.05.2025 08:17] Abstract 8. Recent advancements in dynamic 3D scene reconstruction have shown promising results, enabling high-fidelity 3D novel view synthesis with improved temporal consistency. Among these, 4D Gaussian Splatting (4DGS) has emerged as an appealing approach due to its ability to model high-fidelity spatial and...
[20.05.2025 08:17] ********************************************************************************
[20.05.2025 08:17] Abstract 9. Low-Rank Adaptation (LoRA), which introduces a product of two trainable low-rank matrices into frozen pre-trained weights, is widely used for efficient fine-tuning of language models in federated learning (FL). However, when combined with differentially private stochastic gradient descent (DP-SGD), ...
[20.05.2025 08:17] ********************************************************************************
[20.05.2025 08:17] Abstract 10. Recent advances in rule-based reinforcement learning (RL) have significantly improved the reasoning capability of language models (LMs) with rule-based rewards. However, existing RL methods -- such as GRPO, REINFORCE++, and RLOO -- often suffer from training instability, where large policy updates a...
[20.05.2025 08:17] ********************************************************************************
[20.05.2025 08:17] Abstract 11. Inference-time scaling techniques have significantly bolstered the reasoning capabilities of large language models (LLMs) by harnessing additional computational effort at inference without retraining. Similarly, Chain-of-Thought (CoT) prompting and its extension, Long CoT, improve accuracy by genera...
[20.05.2025 08:17] ********************************************************************************
[20.05.2025 08:17] Abstract 12. Large vision-language models exhibit inherent capabilities to handle diverse visual perception tasks. In this paper, we introduce VisionReasoner, a unified framework capable of reasoning and solving multiple visual perception tasks within a shared model. Specifically, by designing novel multi-object...
[20.05.2025 08:17] ********************************************************************************
[20.05.2025 08:17] Abstract 13. Precise recognition of search intent in Retrieval-Augmented Generation (RAG) systems remains a challenging goal, especially under resource constraints and for complex queries with nested structures and dependencies. This paper presents QCompiler, a neuro-symbolic framework inspired by linguistic gra...
[20.05.2025 08:17] ********************************************************************************
[20.05.2025 08:17] Abstract 14. Scaling video diffusion transformers (DiTs) is limited by their quadratic 3D attention, even though most of the attention mass concentrates on a small subset of positions. We turn this observation into VSA, a trainable, hardware-efficient sparse attention that replaces full attention at both trainin...
[20.05.2025 08:17] ********************************************************************************
[20.05.2025 08:17] Abstract 15. Image generation models have achieved widespread applications. As an instance, the TarFlow model combines the transformer architecture with Normalizing Flow models, achieving state-of-the-art results on multiple benchmarks. However, due to the causal form of attention requiring sequential computatio...
[20.05.2025 08:17] ********************************************************************************
[20.05.2025 08:17] Abstract 16. Recent advances in large language models (LLMs) have fueled the vision of automated scientific discovery, often called AI Co-Scientists. To date, prior work casts these systems as generative co-authors responsible for crafting hypotheses, synthesizing code, or drafting manuscripts. In this work, we ...
[20.05.2025 08:17] ********************************************************************************
[20.05.2025 08:17] Abstract 17. Chart understanding presents a unique challenge for large vision-language models (LVLMs), as it requires the integration of sophisticated textual and visual reasoning capabilities. However, current LVLMs exhibit a notable imbalance between these skills, falling short on visual reasoning that is diff...
[20.05.2025 08:17] ********************************************************************************
[20.05.2025 08:17] Abstract 18. Despite significant advances in video generation, synthesizing physically plausible human actions remains a persistent challenge, particularly in modeling fine-grained semantics and complex temporal dynamics. For instance, generating gymnastics routines such as "switch leap with 0.5 turn" poses subs...
[20.05.2025 08:17] ********************************************************************************
[20.05.2025 08:17] Abstract 19. Integrating Large Language Models with symbolic planners is a promising direction for obtaining verifiable and grounded plans compared to planning in natural language, with recent works extending this idea to visual domains using Vision-Language Models (VLMs). However, rigorous comparison between VL...
[20.05.2025 08:17] ********************************************************************************
[20.05.2025 08:17] Abstract 20. Human image animation has gained increasing attention and developed rapidly due to its broad applications in digital humans. However, existing methods rely largely on 2D-rendered pose images for motion guidance, which limits generalization and discards essential 3D information for open-world animati...
[20.05.2025 08:17] ********************************************************************************
[20.05.2025 08:17] Abstract 21. In recent years, the emergence of large reasoning models (LRMs), such as OpenAI-o1 and DeepSeek-R1, has shown impressive capabilities in complex problems, e.g., mathematics and coding. Some pioneering studies attempt to bring the success of LRMs in neural machine translation (MT). They try to build ...
[20.05.2025 08:17] ********************************************************************************
[20.05.2025 08:17] Abstract 22. Test-Time Scaling (TTS) refers to approaches that improve reasoning performance by allocating extra computation during inference, without altering the model's parameters. While existing TTS methods operate in a discrete token space by generating more intermediate steps, recent studies in Coconut and...
[20.05.2025 08:17] ********************************************************************************
[20.05.2025 08:17] Abstract 23. Identifying subtle technical errors within complex scientific and technical documents, especially those requiring multimodal interpretation (e.g., formulas in images), presents a significant hurdle for Large Language Models (LLMs) whose inherent error-correction tendencies can mask inaccuracies. Thi...
[20.05.2025 08:17] ********************************************************************************
[20.05.2025 08:17] Abstract 24. Accurately identifying adversarial techniques in security texts is critical for effective cyber defense. However, existing methods face a fundamental trade-off: they either rely on generic models with limited domain precision or require resource-intensive pipelines that depend on large labeled datas...
[20.05.2025 08:17] ********************************************************************************
[20.05.2025 08:17] Abstract 25. Video diffusion models (DMs) have enabled high-quality video synthesis. Yet, their substantial computational and memory demands pose serious challenges to real-world deployment, even on high-end GPUs. As a commonly adopted solution, quantization has proven notable success in reducing cost for image ...
[20.05.2025 08:17] ********************************************************************************
[20.05.2025 08:17] Abstract 26. Critical peer review of scientific manuscripts presents a significant challenge for Large Language Models (LLMs), partly due to data limitations and the complexity of expert reasoning. This report introduces Persistent Workflow Prompting (PWP), a potentially broadly applicable prompt engineering met...
[20.05.2025 08:17] Read previous papers.
[20.05.2025 08:17] Generating reviews via LLM API.
[20.05.2025 08:17] Using data from previous issue: {"categories": ["#training", "#open_source", "#inference", "#agi", "#architecture", "#optimization"], "emoji": "🔗", "ru": {"title": "Цепная революция в языковых моделях: гибкость и эффективность", "desc": "В этой статье представлена новая парадигма обучения под названием Chain-of-Model (CoM), котора
[20.05.2025 08:17] Using data from previous issue: {"categories": ["#math", "#reasoning", "#inference", "#training", "#optimization", "#rl"], "emoji": "🧠", "ru": {"title": "Адаптивное мышление для оптимизации рассуждений ИИ", "desc": "Исследователи представили новый алгоритм AdaptThink, который учит модели рассуждения адаптивно выбирать оптимальный 
[20.05.2025 08:17] Using data from previous issue: {"categories": ["#optimization", "#reasoning", "#rlhf", "#rl", "#training"], "emoji": "🧠", "ru": {"title": "AdaCoT: Умное рассуждение для языковых моделей", "desc": "AdaCoT - это новый фреймворк, позволяющий крупным языковым моделям (LLM) адаптивно решать, когда использовать метод цепочки рассуждени
[20.05.2025 08:17] Using data from previous issue: {"categories": ["#optimization", "#long_context", "#architecture", "#inference", "#benchmark"], "emoji": "🔍", "ru": {"title": "Коррекция распределения для эффективного разреженного внимания", "desc": "Статья предлагает новый метод для повышения эффективности разреженного внимания в трансформерах. Ав
[20.05.2025 08:17] Using data from previous issue: {"categories": ["#data", "#dataset", "#graphs", "#agents", "#benchmark", "#open_source"], "emoji": "🖥️", "ru": {"title": "Революция в обучении ИИ работе с компьютерными интерфейсами", "desc": "Статья представляет новый бенчмарк OSWorld-G для оценки способности моделей машинного обучения к интерпрета
[20.05.2025 08:17] Using data from previous issue: {"categories": ["#optimization", "#reasoning", "#rl", "#benchmark", "#training"], "emoji": "🧠", "ru": {"title": "Умное переключение между кратким и развернутым мышлением в языковых моделях", "desc": "Статья представляет Thinkless - обучаемую систему, позволяющую языковым моделям адаптивно выбирать м
[20.05.2025 08:17] Using data from previous issue: {"categories": ["#optimization", "#reasoning", "#multimodal", "#math", "#training", "#open_source", "#benchmark", "#data", "#dataset"], "emoji": "🧠", "ru": {"title": "Автоматизированное обучение мультимодальных моделей пошаговым рассуждениям", "desc": "В статье представлена модель MM-PRM, обучающаяс
[20.05.2025 08:17] Querying the API.
[20.05.2025 08:17] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Reasoning ability, a core component of human intelligence, continues to pose a significant challenge for Large Language Models (LLMs) in the pursuit of AGI. Although model performance has improved under the training scaling law, significant challenges remain, particularly with respect to training algorithms, such as catastrophic forgetting, and the limited availability of novel training data. As an alternative, test-time scaling enhances reasoning performance by increasing test-time computation without parameter updating. Unlike prior methods in this paradigm focused on token space, we propose leveraging latent space for more effective reasoning and better adherence to the test-time scaling law. We introduce LatentSeek, a novel framework that enhances LLM reasoning through Test-Time Instance-level Adaptation (TTIA) within the model's latent space. Specifically, LatentSeek leverages policy gradient to iteratively update latent representations, guided by self-generated reward signals. LatentSeek is evaluated on a range of reasoning benchmarks, including GSM8K, MATH-500, and AIME2024, across multiple LLM architectures. Results show that LatentSeek consistently outperforms strong baselines, such as Chain-of-Thought prompting and fine-tuning-based methods. Furthermore, our analysis demonstrates that LatentSeek is highly efficient, typically converging within a few iterations for problems of average complexity, while also benefiting from additional iterations, thereby highlighting the potential of test-time scaling in the latent space. These findings position LatentSeek as a lightweight, scalable, and effective solution for enhancing the reasoning capabilities of LLMs.
[20.05.2025 08:17] Response: {
  "desc": "Статья представляет LatentSeek - новый фреймворк для улучшения способностей рассуждения больших языковых моделей (LLM) с помощью адаптации на уровне экземпляров во время тестирования в латентном пространстве модели. LatentSeek использует градиент политики для итеративного обновления латентных представлений, руководствуясь самогенерируемыми сигналами вознаграждения. Метод превосходит сильные базовые линии на различных тестах рассуждений и демонстрирует высокую эффективность, обычно сходясь за несколько итераций. LatentSeek позиционируется как легковесное, масштабируемое и эффективное решение для улучшения способностей рассуждения LLM.",

  "emoji": "🧠",

  "title": "LatentSeek: Повышение способности рассуждать у ИИ через адаптацию в латентном пространстве"
}
[20.05.2025 08:17] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Reasoning ability, a core component of human intelligence, continues to pose a significant challenge for Large Language Models (LLMs) in the pursuit of AGI. Although model performance has improved under the training scaling law, significant challenges remain, particularly with respect to training algorithms, such as catastrophic forgetting, and the limited availability of novel training data. As an alternative, test-time scaling enhances reasoning performance by increasing test-time computation without parameter updating. Unlike prior methods in this paradigm focused on token space, we propose leveraging latent space for more effective reasoning and better adherence to the test-time scaling law. We introduce LatentSeek, a novel framework that enhances LLM reasoning through Test-Time Instance-level Adaptation (TTIA) within the model's latent space. Specifically, LatentSeek leverages policy gradient to iteratively update latent representations, guided by self-generated reward signals. LatentSeek is evaluated on a range of reasoning benchmarks, including GSM8K, MATH-500, and AIME2024, across multiple LLM architectures. Results show that LatentSeek consistently outperforms strong baselines, such as Chain-of-Thought prompting and fine-tuning-based methods. Furthermore, our analysis demonstrates that LatentSeek is highly efficient, typically converging within a few iterations for problems of average complexity, while also benefiting from additional iterations, thereby highlighting the potential of test-time scaling in the latent space. These findings position LatentSeek as a lightweight, scalable, and effective solution for enhancing the reasoning capabilities of LLMs."

[20.05.2025 08:17] Response: ```python
['TRAINING', 'BENCHMARK', 'ARCHITECTURE']
```
[20.05.2025 08:17] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Reasoning ability, a core component of human intelligence, continues to pose a significant challenge for Large Language Models (LLMs) in the pursuit of AGI. Although model performance has improved under the training scaling law, significant challenges remain, particularly with respect to training algorithms, such as catastrophic forgetting, and the limited availability of novel training data. As an alternative, test-time scaling enhances reasoning performance by increasing test-time computation without parameter updating. Unlike prior methods in this paradigm focused on token space, we propose leveraging latent space for more effective reasoning and better adherence to the test-time scaling law. We introduce LatentSeek, a novel framework that enhances LLM reasoning through Test-Time Instance-level Adaptation (TTIA) within the model's latent space. Specifically, LatentSeek leverages policy gradient to iteratively update latent representations, guided by self-generated reward signals. LatentSeek is evaluated on a range of reasoning benchmarks, including GSM8K, MATH-500, and AIME2024, across multiple LLM architectures. Results show that LatentSeek consistently outperforms strong baselines, such as Chain-of-Thought prompting and fine-tuning-based methods. Furthermore, our analysis demonstrates that LatentSeek is highly efficient, typically converging within a few iterations for problems of average complexity, while also benefiting from additional iterations, thereby highlighting the potential of test-time scaling in the latent space. These findings position LatentSeek as a lightweight, scalable, and effective solution for enhancing the reasoning capabilities of LLMs."

[20.05.2025 08:17] Response: ```python
["AGI", "REASONING", "OPTIMIZATION"]
```
[20.05.2025 08:17] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper addresses the challenges of reasoning in Large Language Models (LLMs) as they strive for Artificial General Intelligence (AGI). It introduces LatentSeek, a framework that improves reasoning by adapting the model\'s latent space during test time, rather than updating parameters. By using policy gradient methods, LatentSeek iteratively refines latent representations based on self-generated rewards, leading to enhanced performance on reasoning tasks. The results show that LatentSeek outperforms existing methods and is efficient, converging quickly while still benefiting from additional iterations.","title":"Enhancing LLM Reasoning with LatentSeek"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="This paper addresses the challenges of reasoning in Large Language Models (LLMs) as they strive for Artificial General Intelligence (AGI). It introduces LatentSeek, a framework that improves reasoning by adapting the model's latent space during test time, rather than updating parameters. By using policy gradient methods, LatentSeek iteratively refines latent representations based on self-generated rewards, leading to enhanced performance on reasoning tasks. The results show that LatentSeek outperforms existing methods and is efficient, converging quickly while still benefiting from additional iterations.", title='Enhancing LLM Reasoning with LatentSeek'))
[20.05.2025 08:17] Response: ParsedChatCompletionMessage[Article](content='{"desc":"这篇论文探讨了大型语言模型（LLMs）在推理能力方面的挑战，尤其是在追求通用人工智能（AGI）时。作者提出了一种新框架LatentSeek，通过在模型的潜在空间中进行测试时实例级适应（TTIA），来增强LLMs的推理能力。与以往方法不同，LatentSeek利用策略梯度迭代更新潜在表示，并通过自生成的奖励信号进行指导。实验结果表明，LatentSeek在多个推理基准测试中表现优异，显示出其在潜在空间中进行测试时扩展的潜力。","title":"LatentSeek：提升LLM推理能力的新方法"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='这篇论文探讨了大型语言模型（LLMs）在推理能力方面的挑战，尤其是在追求通用人工智能（AGI）时。作者提出了一种新框架LatentSeek，通过在模型的潜在空间中进行测试时实例级适应（TTIA），来增强LLMs的推理能力。与以往方法不同，LatentSeek利用策略梯度迭代更新潜在表示，并通过自生成的奖励信号进行指导。实验结果表明，LatentSeek在多个推理基准测试中表现优异，显示出其在潜在空间中进行测试时扩展的潜力。', title='LatentSeek：提升LLM推理能力的新方法'))
[20.05.2025 08:17] Using data from previous issue: {"categories": ["#3d"], "emoji": "🎥", "ru": {"title": "Гибридный 3D-4D подход для эффективной реконструкции динамических сцен", "desc": "Статья представляет новый метод 3D-4DGS для реконструкции динамических 3D-сцен. Он комбинирует 3D гауссианы для статичных областей и 4D гауссианы для динамических 
[20.05.2025 08:17] Using data from previous issue: {"categories": ["#training", "#data", "#benchmark", "#security", "#optimization"], "emoji": "🔒", "ru": {"title": "FedSVD: Защищенное федеративное обучение языковых моделей с сохранением эффективности", "desc": "Статья представляет новый метод FedSVD для эффективного федеративного обучения языковых м
[20.05.2025 08:17] Using data from previous issue: {"categories": ["#training", "#optimization", "#rl", "#reasoning"], "emoji": "🧠", "ru": {"title": "Стабильное обучение с подкреплением для языковых моделей", "desc": "Статья представляет новый алгоритм CPGD для стабилизации обучения с подкреплением языковых моделей. CPGD вводит ограничение на дрейф 
[20.05.2025 08:17] Using data from previous issue: {"categories": ["#training", "#reasoning", "#optimization", "#benchmark", "#inference"], "emoji": "🧠", "ru": {"title": "Эффективное масштабирование рассуждений языковых моделей без переобучения", "desc": "Эта статья представляет новый метод под названием Fractured Sampling для улучшения рассуждений 
[20.05.2025 08:17] Using data from previous issue: {"categories": ["#multimodal", "#cv", "#benchmark", "#reasoning"], "emoji": "🧠", "ru": {"title": "Единая модель для многозадачного визуального восприятия", "desc": "В статье представлен VisionReasoner - унифицированная модель для решения различных задач визуального восприятия. Модель использует новы
[20.05.2025 08:17] Using data from previous issue: {"categories": ["#optimization", "#reasoning", "#rag", "#multimodal"], "emoji": "🧠", "ru": {"title": "Компиляция запросов для точного поиска в RAG-системах", "desc": "QCompiler - это нейро-символическая система для улучшения понимания сложных запросов в RAG-системах. Она использует минимальную грамм
[20.05.2025 08:17] Using data from previous issue: {"categories": ["#optimization", "#architecture", "#diffusion", "#training", "#open_source", "#video"], "emoji": "🎥", "ru": {"title": "Эффективное разреженное внимание для масштабирования видео-диффузионных моделей", "desc": "Статья представляет новый метод разреженного внимания (VSA) для масштабиро
[20.05.2025 08:17] Using data from previous issue: {"categories": ["#optimization", "#architecture", "#open_source", "#cv", "#training"], "emoji": "🚀", "ru": {"title": "Ускорение генерации изображений в TarFlow с помощью оптимизированных итераций", "desc": "Данная статья представляет метод ускорения процесса сэмплирования в модели TarFlow для генера
[20.05.2025 08:17] Using data from previous issue: {"categories": ["#data", "#science", "#dataset", "#benchmark"], "emoji": "🔍", "ru": {"title": "Большие языковые модели пока не готовы быть научными рецензентами", "desc": "Статья представляет SPOT - набор данных из 83 опубликованных научных работ с 91 значительной ошибкой, приведшей к опечаткам или 
[20.05.2025 08:17] Using data from previous issue: {"categories": ["#open_source", "#interpretability", "#cv", "#benchmark", "#synthetic", "#reasoning", "#dataset"], "emoji": "📊", "ru": {"title": "Раскрывая пробелы в визуальном мышлении ИИ при анализе диаграмм", "desc": "Статья представляет новый тестовый набор данных ChartMuseum для оценки понимани
[20.05.2025 08:17] Using data from previous issue: {"categories": ["#3d", "#optimization", "#diffusion", "#video"], "emoji": "🤸", "ru": {"title": "Точная генерация движений человека с помощью физического моделирования", "desc": "Статья представляет FinePhys - фреймворк для генерации точных движений человека с использованием физических моделей. Систе
[20.05.2025 08:17] Querying the API.
[20.05.2025 08:17] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Integrating Large Language Models with symbolic planners is a promising direction for obtaining verifiable and grounded plans compared to planning in natural language, with recent works extending this idea to visual domains using Vision-Language Models (VLMs). However, rigorous comparison between VLM-grounded symbolic approaches and methods that plan directly with a VLM has been hindered by a lack of common environments, evaluation protocols and model coverage. We introduce ViPlan, the first open-source benchmark for Visual Planning with symbolic predicates and VLMs. ViPlan features a series of increasingly challenging tasks in two domains: a visual variant of the classic Blocksworld planning problem and a simulated household robotics environment. We benchmark nine open-source VLM families across multiple sizes, along with selected closed models, evaluating both VLM-grounded symbolic planning and using the models directly to propose actions. We find symbolic planning to outperform direct VLM planning in Blocksworld, where accurate image grounding is crucial, whereas the opposite is true in the household robotics tasks, where commonsense knowledge and the ability to recover from errors are beneficial. Finally, we show that across most models and methods, there is no significant benefit to using Chain-of-Thought prompting, suggesting that current VLMs still struggle with visual reasoning.
[20.05.2025 08:17] Response: {
  "desc": "Статья представляет ViPlan - первый открытый бенчмарк для визуального планирования с использованием символьных предикатов и моделей визуально-языкового восприятия (VLM). Авторы сравнивают эффективность символьного планирования на основе VLM и прямого планирования с помощью VLM в двух доменах: визуальном варианте классической задачи Blocksworld и симулированной среде домашней робототехники. Исследование показывает, что символьное планирование превосходит прямое планирование VLM в Blocksworld, где критически важна точная привязка к изображению, в то время как в задачах домашней робототехники наблюдается обратная ситуация. Авторы также обнаружили, что использование метода Chain-of-Thought не дает значительных преимуществ для большинства моделей и методов.",
  "emoji": "🤖",
  "title": "ViPlan: новый бенчмарк для сравнения символьного и прямого визуального планирования"
}
[20.05.2025 08:17] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Integrating Large Language Models with symbolic planners is a promising direction for obtaining verifiable and grounded plans compared to planning in natural language, with recent works extending this idea to visual domains using Vision-Language Models (VLMs). However, rigorous comparison between VLM-grounded symbolic approaches and methods that plan directly with a VLM has been hindered by a lack of common environments, evaluation protocols and model coverage. We introduce ViPlan, the first open-source benchmark for Visual Planning with symbolic predicates and VLMs. ViPlan features a series of increasingly challenging tasks in two domains: a visual variant of the classic Blocksworld planning problem and a simulated household robotics environment. We benchmark nine open-source VLM families across multiple sizes, along with selected closed models, evaluating both VLM-grounded symbolic planning and using the models directly to propose actions. We find symbolic planning to outperform direct VLM planning in Blocksworld, where accurate image grounding is crucial, whereas the opposite is true in the household robotics tasks, where commonsense knowledge and the ability to recover from errors are beneficial. Finally, we show that across most models and methods, there is no significant benefit to using Chain-of-Thought prompting, suggesting that current VLMs still struggle with visual reasoning."

[20.05.2025 08:17] Response: ```python
['BENCHMARK', 'CV', 'AGENTS', 'VIDEO']
```
[20.05.2025 08:17] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Integrating Large Language Models with symbolic planners is a promising direction for obtaining verifiable and grounded plans compared to planning in natural language, with recent works extending this idea to visual domains using Vision-Language Models (VLMs). However, rigorous comparison between VLM-grounded symbolic approaches and methods that plan directly with a VLM has been hindered by a lack of common environments, evaluation protocols and model coverage. We introduce ViPlan, the first open-source benchmark for Visual Planning with symbolic predicates and VLMs. ViPlan features a series of increasingly challenging tasks in two domains: a visual variant of the classic Blocksworld planning problem and a simulated household robotics environment. We benchmark nine open-source VLM families across multiple sizes, along with selected closed models, evaluating both VLM-grounded symbolic planning and using the models directly to propose actions. We find symbolic planning to outperform direct VLM planning in Blocksworld, where accurate image grounding is crucial, whereas the opposite is true in the household robotics tasks, where commonsense knowledge and the ability to recover from errors are beneficial. Finally, we show that across most models and methods, there is no significant benefit to using Chain-of-Thought prompting, suggesting that current VLMs still struggle with visual reasoning."

[20.05.2025 08:17] Response: ```python
['GAMES', 'OPEN_SOURCE', 'REASONING']
```
[20.05.2025 08:17] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper discusses the integration of Large Language Models (LLMs) with symbolic planners to create more reliable and understandable plans in visual tasks. It introduces ViPlan, an open-source benchmark designed to evaluate various planning methods using Vision-Language Models (VLMs) in two distinct environments: Blocksworld and household robotics. The study compares the performance of symbolic planning against direct VLM planning, revealing that symbolic methods excel in scenarios requiring precise image grounding, while VLMs perform better in tasks that demand commonsense reasoning. Additionally, the findings indicate that current VLMs do not significantly benefit from Chain-of-Thought prompting, highlighting ongoing challenges in visual reasoning capabilities.","title":"ViPlan: Bridging Symbolic Planning and Vision-Language Models for Better Visual Reasoning"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper discusses the integration of Large Language Models (LLMs) with symbolic planners to create more reliable and understandable plans in visual tasks. It introduces ViPlan, an open-source benchmark designed to evaluate various planning methods using Vision-Language Models (VLMs) in two distinct environments: Blocksworld and household robotics. The study compares the performance of symbolic planning against direct VLM planning, revealing that symbolic methods excel in scenarios requiring precise image grounding, while VLMs perform better in tasks that demand commonsense reasoning. Additionally, the findings indicate that current VLMs do not significantly benefit from Chain-of-Thought prompting, highlighting ongoing challenges in visual reasoning capabilities.', title='ViPlan: Bridging Symbolic Planning and Vision-Language Models for Better Visual Reasoning'))
[20.05.2025 08:17] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本论文介绍了ViPlan，这是第一个用于视觉规划的开源基准，结合了符号规划和视觉语言模型（VLM）。我们设计了一系列逐渐增加难度的任务，包括经典的Blocksworld规划问题和模拟家庭机器人环境。通过对九个开源VLM模型进行基准测试，我们发现符号规划在Blocksworld中表现优于直接使用VLM进行规划，而在家庭机器人任务中则相反。最后，我们的研究表明，当前的VLM在视觉推理方面仍然存在困难，使用Chain-of-Thought提示并没有显著提高性能。","title":"视觉规划的新基准：ViPlan"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本论文介绍了ViPlan，这是第一个用于视觉规划的开源基准，结合了符号规划和视觉语言模型（VLM）。我们设计了一系列逐渐增加难度的任务，包括经典的Blocksworld规划问题和模拟家庭机器人环境。通过对九个开源VLM模型进行基准测试，我们发现符号规划在Blocksworld中表现优于直接使用VLM进行规划，而在家庭机器人任务中则相反。最后，我们的研究表明，当前的VLM在视觉推理方面仍然存在困难，使用Chain-of-Thought提示并没有显著提高性能。', title='视觉规划的新基准：ViPlan'))
[20.05.2025 08:17] Querying the API.
[20.05.2025 08:17] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Human image animation has gained increasing attention and developed rapidly due to its broad applications in digital humans. However, existing methods rely largely on 2D-rendered pose images for motion guidance, which limits generalization and discards essential 3D information for open-world animation. To tackle this problem, we propose MTVCrafter (Motion Tokenization Video Crafter), the first framework that directly models raw 3D motion sequences (i.e., 4D motion) for human image animation. Specifically, we introduce 4DMoT (4D motion tokenizer) to quantize 3D motion sequences into 4D motion tokens. Compared to 2D-rendered pose images, 4D motion tokens offer more robust spatio-temporal cues and avoid strict pixel-level alignment between pose image and character, enabling more flexible and disentangled control. Then, we introduce MV-DiT (Motion-aware Video DiT). By designing unique motion attention with 4D positional encodings, MV-DiT can effectively leverage motion tokens as 4D compact yet expressive context for human image animation in the complex 3D world. Hence, it marks a significant step forward in this field and opens a new direction for pose-guided human video generation. Experiments show that our MTVCrafter achieves state-of-the-art results with an FID-VID of 6.98, surpassing the second-best by 65%. Powered by robust motion tokens, MTVCrafter also generalizes well to diverse open-world characters (single/multiple, full/half-body) across various styles and scenarios. Our video demos and code are on: https://github.com/DINGYANB/MTVCrafter.
[20.05.2025 08:17] Response: {
  "desc": "MTVCrafter - это новый подход к анимации изображений человека, использующий токенизацию 4D движения вместо 2D-изображений поз. Метод вводит 4DMoT для квантования 3D последовательностей движения в токены и MV-DiT для эффективного использования этих токенов в анимации. MTVCrafter превосходит существующие методы по метрике FID-VID на 65% и хорошо обобщается на разнообразных персонажей в открытом мире. Этот подход открывает новое направление в генерации видео с управлением позой человека.",

  "emoji": "🕺",

  "title": "Революция в анимации человека: от 2D к 4D движению"
}
[20.05.2025 08:17] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Human image animation has gained increasing attention and developed rapidly due to its broad applications in digital humans. However, existing methods rely largely on 2D-rendered pose images for motion guidance, which limits generalization and discards essential 3D information for open-world animation. To tackle this problem, we propose MTVCrafter (Motion Tokenization Video Crafter), the first framework that directly models raw 3D motion sequences (i.e., 4D motion) for human image animation. Specifically, we introduce 4DMoT (4D motion tokenizer) to quantize 3D motion sequences into 4D motion tokens. Compared to 2D-rendered pose images, 4D motion tokens offer more robust spatio-temporal cues and avoid strict pixel-level alignment between pose image and character, enabling more flexible and disentangled control. Then, we introduce MV-DiT (Motion-aware Video DiT). By designing unique motion attention with 4D positional encodings, MV-DiT can effectively leverage motion tokens as 4D compact yet expressive context for human image animation in the complex 3D world. Hence, it marks a significant step forward in this field and opens a new direction for pose-guided human video generation. Experiments show that our MTVCrafter achieves state-of-the-art results with an FID-VID of 6.98, surpassing the second-best by 65%. Powered by robust motion tokens, MTVCrafter also generalizes well to diverse open-world characters (single/multiple, full/half-body) across various styles and scenarios. Our video demos and code are on: https://github.com/DINGYANB/MTVCrafter."

[20.05.2025 08:17] Response: ```python
["3D", "VIDEO"]
```
[20.05.2025 08:17] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Human image animation has gained increasing attention and developed rapidly due to its broad applications in digital humans. However, existing methods rely largely on 2D-rendered pose images for motion guidance, which limits generalization and discards essential 3D information for open-world animation. To tackle this problem, we propose MTVCrafter (Motion Tokenization Video Crafter), the first framework that directly models raw 3D motion sequences (i.e., 4D motion) for human image animation. Specifically, we introduce 4DMoT (4D motion tokenizer) to quantize 3D motion sequences into 4D motion tokens. Compared to 2D-rendered pose images, 4D motion tokens offer more robust spatio-temporal cues and avoid strict pixel-level alignment between pose image and character, enabling more flexible and disentangled control. Then, we introduce MV-DiT (Motion-aware Video DiT). By designing unique motion attention with 4D positional encodings, MV-DiT can effectively leverage motion tokens as 4D compact yet expressive context for human image animation in the complex 3D world. Hence, it marks a significant step forward in this field and opens a new direction for pose-guided human video generation. Experiments show that our MTVCrafter achieves state-of-the-art results with an FID-VID of 6.98, surpassing the second-best by 65%. Powered by robust motion tokens, MTVCrafter also generalizes well to diverse open-world characters (single/multiple, full/half-body) across various styles and scenarios. Our video demos and code are on: https://github.com/DINGYANB/MTVCrafter."

[20.05.2025 08:17] Response: ```python
[]
```
[20.05.2025 08:17] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents MTVCrafter, a novel framework for human image animation that utilizes raw 3D motion sequences instead of traditional 2D-rendered pose images. By introducing 4DMoT, the framework quantizes 3D motion into 4D motion tokens, which provide enhanced spatio-temporal cues and greater flexibility in animation control. Additionally, MV-DiT employs motion-aware attention mechanisms to effectively utilize these motion tokens, allowing for more expressive human image generation in complex 3D environments. The results demonstrate that MTVCrafter achieves state-of-the-art performance, significantly improving generalization across various character types and styles.","title":"Revolutionizing Human Animation with 4D Motion Tokens"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper presents MTVCrafter, a novel framework for human image animation that utilizes raw 3D motion sequences instead of traditional 2D-rendered pose images. By introducing 4DMoT, the framework quantizes 3D motion into 4D motion tokens, which provide enhanced spatio-temporal cues and greater flexibility in animation control. Additionally, MV-DiT employs motion-aware attention mechanisms to effectively utilize these motion tokens, allowing for more expressive human image generation in complex 3D environments. The results demonstrate that MTVCrafter achieves state-of-the-art performance, significantly improving generalization across various character types and styles.', title='Revolutionizing Human Animation with 4D Motion Tokens'))
[20.05.2025 08:17] Response: ParsedChatCompletionMessage[Article](content='{"desc":"人像动画技术在数字人类应用中越来越受到关注，但现有方法主要依赖于2D渲染的姿态图像，限制了其泛化能力并丢失了重要的3D信息。为了解决这个问题，我们提出了MTVCrafter框架，它直接建模原始的3D运动序列（即4D运动），并引入了4DMoT（4D运动标记器）将3D运动序列量化为4D运动标记。与2D渲染的姿态图像相比，4D运动标记提供了更强的时空线索，避免了姿态图像与角色之间严格的像素级对齐，从而实现了更灵活和解耦的控制。我们的实验表明，MTVCrafter在多种风格和场景下对不同的开放世界角色具有良好的泛化能力，取得了最先进的结果。","title":"开创4D运动标记的人像动画新方向"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='人像动画技术在数字人类应用中越来越受到关注，但现有方法主要依赖于2D渲染的姿态图像，限制了其泛化能力并丢失了重要的3D信息。为了解决这个问题，我们提出了MTVCrafter框架，它直接建模原始的3D运动序列（即4D运动），并引入了4DMoT（4D运动标记器）将3D运动序列量化为4D运动标记。与2D渲染的姿态图像相比，4D运动标记提供了更强的时空线索，避免了姿态图像与角色之间严格的像素级对齐，从而实现了更灵活和解耦的控制。我们的实验表明，MTVCrafter在多种风格和场景下对不同的开放世界角色具有良好的泛化能力，取得了最先进的结果。', title='开创4D运动标记的人像动画新方向'))
[20.05.2025 08:17] Using data from previous issue: {"categories": ["#reasoning", "#multilingual", "#rl", "#low_resource", "#translation"], "emoji": "🌐", "ru": {"title": "Революция в машинном переводе: от одноязычного к многоязычному совершенству", "desc": "Статья описывает новый метод моделирования вознаграждения для обучения с подкреплением в нейро
[20.05.2025 08:17] Querying the API.
[20.05.2025 08:17] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Test-Time Scaling (TTS) refers to approaches that improve reasoning performance by allocating extra computation during inference, without altering the model's parameters. While existing TTS methods operate in a discrete token space by generating more intermediate steps, recent studies in Coconut and SoftCoT have demonstrated that thinking in the continuous latent space can further enhance the reasoning performance. Such latent thoughts encode informative thinking without the information loss associated with autoregressive token generation, sparking increased interest in continuous-space reasoning. Unlike discrete decoding, where repeated sampling enables exploring diverse reasoning paths, latent representations in continuous space are fixed for a given input, which limits diverse exploration, as all decoded paths originate from the same latent thought. To overcome this limitation, we introduce SoftCoT++ to extend SoftCoT to the Test-Time Scaling paradigm by enabling diverse exploration of thinking paths. Specifically, we perturb latent thoughts via multiple specialized initial tokens and apply contrastive learning to promote diversity among soft thought representations. Experiments across five reasoning benchmarks and two distinct LLM architectures demonstrate that SoftCoT++ significantly boosts SoftCoT and also outperforms SoftCoT with self-consistency scaling. Moreover, it shows strong compatibility with conventional scaling techniques such as self-consistency. Source code is available at https://github.com/xuyige/SoftCoT.
[20.05.2025 08:18] Response: {
  "desc": "Статья представляет метод SoftCoT++, улучшающий рассуждения языковых моделей в непрерывном латентном пространстве. В отличие от дискретных методов, SoftCoT++ позволяет исследовать разнообразные пути рассуждений путем возмущения латентных представлений. Метод применяет контрастивное обучение для повышения разнообразия мягких представлений мыслей. Эксперименты показывают, что SoftCoT++ превосходит базовый SoftCoT и хорошо сочетается с другими методами масштабирования.",
  "emoji": "🧠",
  "title": "Улучшение рассуждений ИИ через разнообразные латентные мысли"
}
[20.05.2025 08:18] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Test-Time Scaling (TTS) refers to approaches that improve reasoning performance by allocating extra computation during inference, without altering the model's parameters. While existing TTS methods operate in a discrete token space by generating more intermediate steps, recent studies in Coconut and SoftCoT have demonstrated that thinking in the continuous latent space can further enhance the reasoning performance. Such latent thoughts encode informative thinking without the information loss associated with autoregressive token generation, sparking increased interest in continuous-space reasoning. Unlike discrete decoding, where repeated sampling enables exploring diverse reasoning paths, latent representations in continuous space are fixed for a given input, which limits diverse exploration, as all decoded paths originate from the same latent thought. To overcome this limitation, we introduce SoftCoT++ to extend SoftCoT to the Test-Time Scaling paradigm by enabling diverse exploration of thinking paths. Specifically, we perturb latent thoughts via multiple specialized initial tokens and apply contrastive learning to promote diversity among soft thought representations. Experiments across five reasoning benchmarks and two distinct LLM architectures demonstrate that SoftCoT++ significantly boosts SoftCoT and also outperforms SoftCoT with self-consistency scaling. Moreover, it shows strong compatibility with conventional scaling techniques such as self-consistency. Source code is available at https://github.com/xuyige/SoftCoT."

[20.05.2025 08:18] Response: ```python
["INFERENCE", "BENCHMARK", "TRAINING"]
```
[20.05.2025 08:18] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Test-Time Scaling (TTS) refers to approaches that improve reasoning performance by allocating extra computation during inference, without altering the model's parameters. While existing TTS methods operate in a discrete token space by generating more intermediate steps, recent studies in Coconut and SoftCoT have demonstrated that thinking in the continuous latent space can further enhance the reasoning performance. Such latent thoughts encode informative thinking without the information loss associated with autoregressive token generation, sparking increased interest in continuous-space reasoning. Unlike discrete decoding, where repeated sampling enables exploring diverse reasoning paths, latent representations in continuous space are fixed for a given input, which limits diverse exploration, as all decoded paths originate from the same latent thought. To overcome this limitation, we introduce SoftCoT++ to extend SoftCoT to the Test-Time Scaling paradigm by enabling diverse exploration of thinking paths. Specifically, we perturb latent thoughts via multiple specialized initial tokens and apply contrastive learning to promote diversity among soft thought representations. Experiments across five reasoning benchmarks and two distinct LLM architectures demonstrate that SoftCoT++ significantly boosts SoftCoT and also outperforms SoftCoT with self-consistency scaling. Moreover, it shows strong compatibility with conventional scaling techniques such as self-consistency. Source code is available at https://github.com/xuyige/SoftCoT."

[20.05.2025 08:18] Response: ```python
["REASONING", "OPTIMIZATION"]
```
[20.05.2025 08:18] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces SoftCoT++, an advanced method for Test-Time Scaling (TTS) that enhances reasoning performance in machine learning models. Unlike traditional TTS methods that work with discrete tokens, SoftCoT++ leverages continuous latent space to improve the quality of reasoning by allowing for diverse exploration of thought paths. By perturbing latent thoughts with specialized initial tokens and using contrastive learning, the method promotes diversity in soft thought representations. Experimental results show that SoftCoT++ significantly outperforms previous methods, demonstrating its effectiveness across various reasoning benchmarks and compatibility with existing scaling techniques.","title":"Enhancing Reasoning with Diverse Latent Thoughts"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper introduces SoftCoT++, an advanced method for Test-Time Scaling (TTS) that enhances reasoning performance in machine learning models. Unlike traditional TTS methods that work with discrete tokens, SoftCoT++ leverages continuous latent space to improve the quality of reasoning by allowing for diverse exploration of thought paths. By perturbing latent thoughts with specialized initial tokens and using contrastive learning, the method promotes diversity in soft thought representations. Experimental results show that SoftCoT++ significantly outperforms previous methods, demonstrating its effectiveness across various reasoning benchmarks and compatibility with existing scaling techniques.', title='Enhancing Reasoning with Diverse Latent Thoughts'))
[20.05.2025 08:18] Response: ParsedChatCompletionMessage[Article](content='{"desc":"测试时扩展（TTS）是一种在推理过程中通过分配额外计算来提高推理性能的方法，而不改变模型参数。最近的研究表明，在连续潜在空间中进行思考可以进一步提升推理性能。与离散解码不同，连续空间中的潜在表示是固定的，这限制了多样化的探索。为了解决这个问题，我们提出了SoftCoT++，通过扰动潜在思维并应用对比学习来促进思维路径的多样性，从而扩展了SoftCoT在测试时扩展范式中的应用。","title":"多样化思维路径的探索新方法"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='测试时扩展（TTS）是一种在推理过程中通过分配额外计算来提高推理性能的方法，而不改变模型参数。最近的研究表明，在连续潜在空间中进行思考可以进一步提升推理性能。与离散解码不同，连续空间中的潜在表示是固定的，这限制了多样化的探索。为了解决这个问题，我们提出了SoftCoT++，通过扰动潜在思维并应用对比学习来促进思维路径的多样性，从而扩展了SoftCoT在测试时扩展范式中的应用。', title='多样化思维路径的探索新方法'))
[20.05.2025 08:18] Using data from previous issue: {"categories": ["#science", "#multimodal", "#interpretability", "#inference"], "emoji": "🔍", "ru": {"title": "Повышение точности LLM в обнаружении ошибок через структурированную настройку контекста", "desc": "Это исследование изучает структурированный подход к настройке контекста больших языковых мо
[20.05.2025 08:18] Using data from previous issue: {"categories": ["#security", "#data", "#hallucinations", "#rag", "#benchmark"], "emoji": "🛡️", "ru": {"title": "Точное распознавание техник злоумышленников с минимумом данных", "desc": "TechniqueRAG - это новая система для идентификации техник противников в текстах по кибербезопасности. Она использу
[20.05.2025 08:18] Querying the API.
[20.05.2025 08:18] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Video diffusion models (DMs) have enabled high-quality video synthesis. Yet, their substantial computational and memory demands pose serious challenges to real-world deployment, even on high-end GPUs. As a commonly adopted solution, quantization has proven notable success in reducing cost for image DMs, while its direct application to video DMs remains ineffective. In this paper, we present QVGen, a novel quantization-aware training (QAT) framework tailored for high-performance and inference-efficient video DMs under extremely low-bit quantization (e.g., 4-bit or below). We begin with a theoretical analysis demonstrating that reducing the gradient norm is essential to facilitate convergence for QAT. To this end, we introduce auxiliary modules (Phi) to mitigate large quantization errors, leading to significantly enhanced convergence. To eliminate the inference overhead of Phi, we propose a rank-decay strategy that progressively eliminates Phi. Specifically, we repeatedly employ singular value decomposition (SVD) and a proposed rank-based regularization gamma to identify and decay low-contributing components. This strategy retains performance while zeroing out inference overhead. Extensive experiments across 4 state-of-the-art (SOTA) video DMs, with parameter sizes ranging from 1.3B sim14B, show that QVGen is the first to reach full-precision comparable quality under 4-bit settings. Moreover, it significantly outperforms existing methods. For instance, our 3-bit CogVideoX-2B achieves improvements of +25.28 in Dynamic Degree and +8.43 in Scene Consistency on VBench.
[20.05.2025 08:18] Response: {
  "desc": "QVGen - это новая система обучения с учетом квантования для высокопроизводительных и эффективных при выводе видео-диффузионных моделей при экстремально низкобитном квантовании. Система вводит вспомогательные модули для уменьшения ошибок квантования и улучшения сходимости. Используется стратегия уменьшения ранга для устранения накладных расходов при выводе. Эксперименты показывают, что QVGen достигает качества полной точности при 4-битных настройках и значительно превосходит существующие методы.",
  "emoji": "🎬",
  "title": "Революция в квантовании видео-диффузионных моделей"
}
[20.05.2025 08:18] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Video diffusion models (DMs) have enabled high-quality video synthesis. Yet, their substantial computational and memory demands pose serious challenges to real-world deployment, even on high-end GPUs. As a commonly adopted solution, quantization has proven notable success in reducing cost for image DMs, while its direct application to video DMs remains ineffective. In this paper, we present QVGen, a novel quantization-aware training (QAT) framework tailored for high-performance and inference-efficient video DMs under extremely low-bit quantization (e.g., 4-bit or below). We begin with a theoretical analysis demonstrating that reducing the gradient norm is essential to facilitate convergence for QAT. To this end, we introduce auxiliary modules (Phi) to mitigate large quantization errors, leading to significantly enhanced convergence. To eliminate the inference overhead of Phi, we propose a rank-decay strategy that progressively eliminates Phi. Specifically, we repeatedly employ singular value decomposition (SVD) and a proposed rank-based regularization gamma to identify and decay low-contributing components. This strategy retains performance while zeroing out inference overhead. Extensive experiments across 4 state-of-the-art (SOTA) video DMs, with parameter sizes ranging from 1.3B sim14B, show that QVGen is the first to reach full-precision comparable quality under 4-bit settings. Moreover, it significantly outperforms existing methods. For instance, our 3-bit CogVideoX-2B achieves improvements of +25.28 in Dynamic Degree and +8.43 in Scene Consistency on VBench."

[20.05.2025 08:18] Response: ```python
["INFERENCE", "VIDEO"]
```
[20.05.2025 08:18] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Video diffusion models (DMs) have enabled high-quality video synthesis. Yet, their substantial computational and memory demands pose serious challenges to real-world deployment, even on high-end GPUs. As a commonly adopted solution, quantization has proven notable success in reducing cost for image DMs, while its direct application to video DMs remains ineffective. In this paper, we present QVGen, a novel quantization-aware training (QAT) framework tailored for high-performance and inference-efficient video DMs under extremely low-bit quantization (e.g., 4-bit or below). We begin with a theoretical analysis demonstrating that reducing the gradient norm is essential to facilitate convergence for QAT. To this end, we introduce auxiliary modules (Phi) to mitigate large quantization errors, leading to significantly enhanced convergence. To eliminate the inference overhead of Phi, we propose a rank-decay strategy that progressively eliminates Phi. Specifically, we repeatedly employ singular value decomposition (SVD) and a proposed rank-based regularization gamma to identify and decay low-contributing components. This strategy retains performance while zeroing out inference overhead. Extensive experiments across 4 state-of-the-art (SOTA) video DMs, with parameter sizes ranging from 1.3B sim14B, show that QVGen is the first to reach full-precision comparable quality under 4-bit settings. Moreover, it significantly outperforms existing methods. For instance, our 3-bit CogVideoX-2B achieves improvements of +25.28 in Dynamic Degree and +8.43 in Scene Consistency on VBench."

[20.05.2025 08:18] Response: ```python
["OPTIMIZATION", "DIFFUSION"]
```
[20.05.2025 08:18] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces QVGen, a new framework for quantization-aware training (QAT) specifically designed for video diffusion models (DMs). It addresses the challenges of high computational and memory requirements by enabling efficient video synthesis at extremely low-bit quantization levels, such as 4-bit. The authors demonstrate that reducing the gradient norm is crucial for effective convergence in QAT and propose auxiliary modules to minimize quantization errors. Additionally, they implement a rank-decay strategy to eliminate inference overhead while maintaining performance, achieving significant improvements over existing methods in video quality metrics.","title":"Efficient Video Synthesis with Low-Bit Quantization"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper introduces QVGen, a new framework for quantization-aware training (QAT) specifically designed for video diffusion models (DMs). It addresses the challenges of high computational and memory requirements by enabling efficient video synthesis at extremely low-bit quantization levels, such as 4-bit. The authors demonstrate that reducing the gradient norm is crucial for effective convergence in QAT and propose auxiliary modules to minimize quantization errors. Additionally, they implement a rank-decay strategy to eliminate inference overhead while maintaining performance, achieving significant improvements over existing methods in video quality metrics.', title='Efficient Video Synthesis with Low-Bit Quantization'))
[20.05.2025 08:18] Response: ParsedChatCompletionMessage[Article](content='{"desc":"视频扩散模型（DMs）在高质量视频合成方面取得了显著进展，但其计算和内存需求高，限制了实际应用。本文提出了一种新颖的量化感知训练（QAT）框架QVGen，旨在在极低位量化（如4位或更低）下实现高性能和高效推理。我们通过理论分析表明，降低梯度范数对QAT的收敛至关重要，并引入辅助模块（Phi）来减小量化误差，从而显著提高收敛性。通过逐步消除Phi的推理开销，我们的实验表明QVGen在4位设置下首次实现了与全精度相当的质量，并显著超越了现有方法。","title":"量化感知训练，提升视频合成效率！"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='视频扩散模型（DMs）在高质量视频合成方面取得了显著进展，但其计算和内存需求高，限制了实际应用。本文提出了一种新颖的量化感知训练（QAT）框架QVGen，旨在在极低位量化（如4位或更低）下实现高性能和高效推理。我们通过理论分析表明，降低梯度范数对QAT的收敛至关重要，并引入辅助模块（Phi）来减小量化误差，从而显著提高收敛性。通过逐步消除Phi的推理开销，我们的实验表明QVGen在4位设置下首次实现了与全精度相当的质量，并显著超越了现有方法。', title='量化感知训练，提升视频合成效率！'))
[20.05.2025 08:18] Using data from previous issue: {"categories": ["#reasoning", "#data", "#science", "#multimodal", "#interpretability", "#architecture"], "emoji": "🔬", "ru": {"title": "PWP: Новый подход к критическому анализу научных работ с помощью LLM", "desc": "Статья представляет новый метод инженерии промптов под названием Persistent Workflow
[20.05.2025 08:18] Loading Chinese text from previous data.
[20.05.2025 08:18] Renaming data file.
[20.05.2025 08:18] Renaming previous data. hf_papers.json to ./d/2025-05-20.json
[20.05.2025 08:18] Saving new data file.
[20.05.2025 08:18] Generating page.
[20.05.2025 08:18] Renaming previous page.
[20.05.2025 08:18] Renaming previous data. index.html to ./d/2025-05-20.html
[20.05.2025 08:18] [Experimental] Generating Chinese page for reading.
[20.05.2025 08:18] Chinese vocab [{'word': '系列', 'pinyin': 'xìliè', 'trans': 'series'}, {'word': '版本', 'pinyin': 'bǎnběn', 'trans': 'version'}, {'word': '旨在', 'pinyin': 'zhǐzài', 'trans': 'aim to'}, {'word': '效率', 'pinyin': 'xiàolǜ', 'trans': 'efficiency'}, {'word': '多语言', 'pinyin': 'duōyǔyán', 'trans': 'multilingual'}, {'word': '能力', 'pinyin': 'nénglì', 'trans': 'ability'}, {'word': '包含', 'pinyin': 'bāohán', 'trans': 'contain'}, {'word': '密集', 'pinyin': 'mìjí', 'trans': 'dense'}, {'word': '混合', 'pinyin': 'hùnhé', 'trans': 'hybrid'}, {'word': '专家', 'pinyin': 'zhuānjiā', 'trans': 'expert'}, {'word': '架构', 'pinyin': 'jiàgòu', 'trans': 'architecture'}, {'word': '参数', 'pinyin': 'cānshǔ', 'trans': 'parameter'}, {'word': '规模', 'pinyin': 'guīmó', 'trans': 'scale'}, {'word': '创新', 'pinyin': 'chuàngxīn', 'trans': 'innovation'}, {'word': '之处', 'pinyin': 'zhīchù', 'trans': 'place'}, {'word': '思考', 'pinyin': 'sīkǎo', 'trans': 'think'}, {'word': '模式', 'pinyin': 'móshì', 'trans': 'mode'}, {'word': '结合', 'pinyin': 'jiéhé', 'trans': 'combine'}, {'word': '框架', 'pinyin': 'kuàngjià', 'trans': 'framework'}, {'word': '消除', 'pinyin': 'xiāochú', 'trans': 'eliminate'}, {'word': '切换', 'pinyin': 'qiēhuàn', 'trans': 'switch'}, {'word': '需要', 'pinyin': 'xūyào', 'trans': 'need'}, {'word': '引入', 'pinyin': 'yǐnrù', 'trans': 'introduce'}, {'word': '预算', 'pinyin': 'yùsuàn', 'trans': 'budget'}, {'word': '机制', 'pinyin': 'jīzhì', 'trans': 'mechanism'}, {'word': '允许', 'pinyin': 'yǔnxǔ', 'trans': 'allow'}, {'word': '根据', 'pinyin': 'gēnjù', 'trans': 'according to'}, {'word': '任务', 'pinyin': 'rènwù', 'trans': 'task'}, {'word': '复杂性', 'pinyin': 'fùzáxìng', 'trans': 'complexity'}, {'word': '动态', 'pinyin': 'dòngtài', 'trans': 'dynamic'}, {'word': '分配', 'pinyin': 'fēnpèi', 'trans': 'allocate'}, {'word': '计算', 'pinyin': 'jìsuàn', 'trans': 'compute'}, {'word': '资源', 'pinyin': 'zīyuán', 'trans': 'resources'}]
[20.05.2025 08:18] Renaming previous Chinese page.
[20.05.2025 08:18] Renaming previous data. zh.html to ./d/2025-05-19_zh_reading_task.html
[20.05.2025 08:18] Writing Chinese reading task.
[20.05.2025 08:18] Writing result.
[20.05.2025 08:18] Renaming log file.
[20.05.2025 08:18] Renaming previous data. log.txt to ./logs/2025-05-20_last_log.txt
