[31.10.2024 22:11] [Experimental] Generating an image for paper CORAL: Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.
[31.10.2024 22:11] [Experimental] Image for paper CORAL: Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation already exists.
[31.10.2024 22:11] [Experimental] Generating an image for paper ReferEverything: Towards Segmenting Everything We Can Speak of in Videos.
[31.10.2024 22:11] [Experimental] Image for paper ReferEverything: Towards Segmenting Everything We Can Speak of in Videos already exists.
[31.10.2024 22:11] [Experimental] Generating an image for paper A Large Recurrent Action Model: xLSTM enables Fast Inference for Robotics Tasks.
[31.10.2024 22:11] [Experimental] Image for paper A Large Recurrent Action Model: xLSTM enables Fast Inference for Robotics Tasks already exists.
[31.10.2024 22:11] [Experimental] Generating an image for paper Decoding Reading Goals from Eye Movements.
[31.10.2024 22:11] [Experimental] Image for paper Decoding Reading Goals from Eye Movements already exists.
[01.11.2024 01:03] Read previous papers.
[01.11.2024 01:03] Get feed.
[01.11.2024 01:03] Get page data from previous paper. URL: https://huggingface.co/papers/2410.23090
[01.11.2024 01:03] Get page data from previous paper. URL: https://huggingface.co/papers/2410.20779
[01.11.2024 01:03] Get page data from previous paper. URL: https://huggingface.co/papers/2410.23287
[01.11.2024 01:03] Get page data from previous paper. URL: https://huggingface.co/papers/2410.22391
[01.11.2024 01:03] Get page data from previous paper. URL: https://huggingface.co/papers/2410.23168
[01.11.2024 01:03] Get page data from previous paper. URL: https://huggingface.co/papers/2410.22884
[01.11.2024 01:03] Get page data from previous paper. URL: https://huggingface.co/papers/2410.20050
[01.11.2024 01:03] Get page data from previous paper. URL: https://huggingface.co/papers/2410.23123
[01.11.2024 01:03] Get page data from previous paper. URL: https://huggingface.co/papers/2410.23277
[01.11.2024 01:03] Get page data from previous paper. URL: https://huggingface.co/papers/2410.22587
[01.11.2024 01:03] ********************************************************************************
[01.11.2024 01:03] Abstract 0. Retrieval-Augmented Generation (RAG) has become a powerful paradigm for enhancing large language models (LLMs) through external knowledge retrieval. Despite its widespread attention, existing academic research predominantly focuses on single-turn RAG, leaving a significant gap in addressing the comp...
[01.11.2024 01:03] ********************************************************************************
[01.11.2024 01:03] Abstract 1. Readers can have different goals with respect to the text they are reading. Can these goals be decoded from the pattern of their eye movements over the text? In this work, we examine for the first time whether it is possible to decode two types of reading goals that are common in daily life: informa...
[01.11.2024 01:03] ********************************************************************************
[01.11.2024 01:03] Abstract 2. We present REM, a framework for segmenting a wide range of concepts in video that can be described through natural language. Our method capitalizes on visual-language representations learned by video diffusion models on Internet-scale datasets. A key insight of our approach is preserving as much of ...
[01.11.2024 01:03] ********************************************************************************
[01.11.2024 01:03] Abstract 3. In recent years, there has been a trend in the field of Reinforcement Learning (RL) towards large action models trained offline on large-scale datasets via sequence modeling. Existing models are primarily based on the Transformer architecture, which result in powerful agents. However, due to slow in...
[01.11.2024 01:03] ********************************************************************************
[01.11.2024 01:03] Abstract 4. Transformers have become the predominant architecture in foundation models due to their excellent performance across various domains. However, the substantial cost of scaling these models remains a significant concern. This problem arises primarily from their dependence on a fixed number of paramete...
[01.11.2024 01:03] ********************************************************************************
[01.11.2024 01:03] Abstract 5. Mixture-of-Experts (MoE) models improve the efficiency and scalability of dense language models by routing each token to a small number of experts in each layer. In this paper, we show how an adversary that can arrange for their queries to appear in the same batch of examples as a victim's queries c...
[01.11.2024 01:03] ********************************************************************************
[01.11.2024 01:03] Abstract 6. Medical information retrieval (MIR) is essential for retrieving relevant medical knowledge from diverse sources, including electronic health records, scientific literature, and medical databases. However, achieving effective zero-shot dense retrieval in the medical domain poses substantial challenge...
[01.11.2024 01:03] ********************************************************************************
[01.11.2024 01:03] Abstract 7. Large language models (LLMs) achieve good performance on challenging reasoning benchmarks, yet could also make basic reasoning mistakes. This contrasting behavior is puzzling when it comes to understanding the mechanisms behind LLMs' reasoning capabilities. One hypothesis is that the increasingly hi...
[01.11.2024 01:03] ********************************************************************************
[01.11.2024 01:03] Abstract 8. Human beings are endowed with a complementary learning system, which bridges the slow learning of general world dynamics with fast storage of episodic memory from a new experience. Previous video generation models, however, primarily focus on slow learning by pre-training on vast amounts of data, ov...
[01.11.2024 01:03] ********************************************************************************
[01.11.2024 01:03] Abstract 9. Open-source large language models are becoming increasingly available and popular among researchers and practitioners. While significant progress has been made on open-weight models, open training data is a practice yet to be adopted by the leading open-weight models creators. At the same time, ther...
[01.11.2024 01:03] Read previous papers.
[01.11.2024 01:03] Generating reviews via LLM API.
[01.11.2024 01:03] Using data from previous issue: {"categories": ["#rag", "#benchmark"], "emoji": "üó£Ô∏è", "ru": {"title": "CORAL: –ù–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–Ω–æ–≥–æ—Ö–æ–¥–æ–≤—ã—Ö –¥–∏–∞–ª–æ–≥–æ–≤—ã—Ö —Å–∏—Å—Ç–µ–º —Å RAG", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ CORAL –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–∏—Å—Ç–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π (RAG) –≤ –º–Ω–æ–≥–æ—Ö–æ–¥–æ–≤—ã—Ö –¥–∏–∞–ª–æ–≥–∞—Ö. CORAL –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ
[01.11.2024 01:03] Using data from previous issue: {"categories": ["#cv", "#interpretability", "#dataset", "#benchmark"], "emoji": "üëÅÔ∏è", "ru": {"title": "–†–∞–∑–≥–∞–¥–∫–∞ —Ü–µ–ª–µ–π —á—Ç–µ–Ω–∏—è –ø–æ –¥–≤–∏–∂–µ–Ω–∏—è–º –≥–ª–∞–∑", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—é —Ü–µ–ª–µ–π —á—Ç–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–≤–∏–∂–µ–Ω–∏–π –≥–ª–∞–∑. –ê–≤—Ç–æ—Ä—ã –ø—Ä–∏–º–µ–Ω—è—é—Ç —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫—Ä—É–ø
[01.11.2024 01:03] Using data from previous issue: {"categories": ["#video", "#benchmark", "#multimodal", "#cv"], "emoji": "üé•", "ru": {"title": "REM: —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞", "desc": "REM - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∫–æ–Ω—Ü–µ–ø—Ü–∏–π –≤ –≤–∏–¥–µ–æ, –æ–ø–∏—Å—ã–≤–∞–µ–º—ã—Ö —Å –ø–æ–º–æ—â—å—é –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≤–∏–∑—É–∞–ª
[01.11.2024 01:03] Using data from previous issue: {"categories": ["#rl", "#agents", "#architecture"], "emoji": "ü§ñ", "ru": {"title": "LRAM: –ë—ã—Å—Ç—Ä–µ–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤, —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å LRAM (Large Recurrent Action Model) –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –æ—Å–Ω–æ–≤–∞–Ω–Ω–∞—è –Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ xLSTM. LRAM –ø—Ä–µ–¥–ª–∞–≥
[01.11.2024 01:03] Using data from previous issue: {"categories": ["#architecture", "#optimization"], "emoji": "üîÑ", "ru": {"title": "–ì–∏–±–∫–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π –±–µ–∑ –ø–æ–ª–Ω–æ–≥–æ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è", "desc": "TokenFormer - —ç—Ç–æ –Ω–æ–≤–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª–∏, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–∑–≤–æ–ª—è–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞—Ç—å –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ –±–µ–∑ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –ø–æ–ª–Ω–æ–≥–æ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è. –í –æ—Ç–ª
[01.11.2024 01:03] Using data from previous issue: {"categories": ["#security", "#architecture"], "emoji": "üïµÔ∏è", "ru": {"title": "–£—è–∑–≤–∏–º–æ—Å—Ç—å –≤ MoE –º–æ–¥–µ–ª—è—Ö: –∫–∞–∫ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ –º–æ–≥—É—Ç —Ä–∞—Å–∫—Ä—ã—Ç—å –≤–∞—à –ø—Ä–æ–º–ø—Ç", "desc": "–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç —É—è–∑–≤–∏–º–æ—Å—Ç—å –≤ –º–æ–¥–µ–ª—è—Ö Mixture-of-Experts (MoE), –∏—Å–ø–æ–ª—å–∑—É—é—â–∏—Ö –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—é Expert-Choice-Routing. –ê–≤—Ç–æ—Ä—ã –¥–µ–º–æ–Ω—Å—Ç
[01.11.2024 01:03] Using data from previous issue: {"categories": ["#dataset", "#data", "#benchmark", "#medicine"], "emoji": "ü©∫", "ru": {"title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–º –ø–æ–∏—Å–∫–µ: SL-HyDE –∏ CMIRB –æ—Ç–∫—Ä—ã–≤–∞—é—Ç –Ω–æ–≤—ã–µ –≥–æ—Ä–∏–∑–æ–Ω—Ç—ã", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–º—É –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–º—É –ø–æ–∏—Å–∫—É –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º SL-HyDE. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –±–æ–ª
[01.11.2024 01:03] Using data from previous issue: {"categories": ["#reasoning", "#interpretability"], "emoji": "üß†", "ru": {"title": "–ó–∞–ø–æ–º–∏–Ω–∞–Ω–∏–µ vs –†–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ: –°–ª–æ–∂–Ω–æ–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–í —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –≤–∑–∞–∏–º–æ—Å–≤—è–∑—å –º–µ–∂–¥—É —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å—é –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –∫ –∑–∞–ø–æ–º–∏–Ω–∞–Ω–∏—é –∏ –∏—Ö –Ω–∞–≤—ã–∫–∞–º–∏ –ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –º—ã—à–ª–µ
[01.11.2024 01:03] Using data from previous issue: {"categories": ["#video", "#dataset", "#benchmark"], "emoji": "üé¨", "ru": {"title": "–î–≤—É—Ö—Å–∫–æ—Ä–æ—Å—Ç–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç SlowFast-VGen - –Ω–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É –¥–≤—É—Ö—Å–∫–æ—Ä–æ—Å—Ç–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–µ–π—Å—Ç–≤–∏–π. –ú–æ–¥–µ–ª—å —Å–æ—á–µ—Ç–∞–µ—Ç 
[01.11.2024 01:03] Using data from previous issue: {"categories": ["#dataset", "#data", "#interpretability"], "emoji": "üõ°Ô∏è", "ru": {"title": "–ë–µ–∑–æ–ø–∞—Å–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –¥–∞–Ω–Ω—ã—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ —Ç–æ–∫—Å–∏—á–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –≤ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã —Å–æ–∑–¥–∞–ª–∏ –¥–∞—Ç–∞—Å–µ—Ç ToxicCo
[01.11.2024 01:03] Loading Chinese text from previous data.
[01.11.2024 01:03] Renaming data file.
[01.11.2024 01:03] Renaming previous data. hf_papers.json to ./d/2024-11-01.json
[01.11.2024 01:03] Saving new data file.
[01.11.2024 01:03] Generating page.
[01.11.2024 01:03] Renaming previous page.
[01.11.2024 01:03] Renaming previous data. index.html to ./d/2024-11-01.html
[01.11.2024 01:03] [Experimental] Generating Chinese page for reading.
[01.11.2024 01:03] Chinese vocab [{'word': 'Ê£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàê', 'pinyin': 'ji«énsu«í zƒìngqi√°ng shƒìngch√©ng', 'trans': 'Retrieval-Augmented Generation'}, {'word': 'ËåÉÂºè', 'pinyin': 'f√†nsh√¨', 'trans': 'paradigm'}, {'word': 'ÂπøÊ≥õ', 'pinyin': 'gu«éngf√†n', 'trans': 'extensive'}, {'word': 'ÂçïËΩÆ', 'pinyin': 'dƒÅnl√∫n', 'trans': 'single-turn'}, {'word': 'ÂØπËØù', 'pinyin': 'du√¨hu√†', 'trans': 'dialogue'}, {'word': 'Â§çÊùÇ', 'pinyin': 'f√πz√°', 'trans': 'complex'}, {'word': 'Â§öËΩÆ', 'pinyin': 'du≈çl√∫n', 'trans': 'multi-turn'}, {'word': 'Â°´Ë°•', 'pinyin': 'ti√°nb«î', 'trans': 'fill'}, {'word': 'Á©∫ÁôΩ', 'pinyin': 'k√≤ngb√°i', 'trans': 'gap'}, {'word': 'Âü∫ÂáÜ', 'pinyin': 'jƒ´zh«în', 'trans': 'benchmark'}, {'word': 'Êï∞ÊçÆÈõÜ', 'pinyin': 'sh√πj√πj√≠', 'trans': 'dataset'}, {'word': 'ËØÑ‰º∞', 'pinyin': 'p√≠ngg≈´', 'trans': 'evaluate'}, {'word': '‰ø°ÊÅØÂØªÊ±Ç', 'pinyin': 'x√¨nxƒ´ x√∫nqi√∫', 'trans': 'information-seeking'}, {'word': 'ÂºÄÊîæÂüü', 'pinyin': 'kƒÅif√†ng y√π', 'trans': 'open-domain'}, {'word': 'Áü•ËØÜÂØÜÈõÜÂ∫¶', 'pinyin': 'zhƒ´shi m√¨jƒ´d√π', 'trans': 'knowledge intensity'}, {'word': 'Ëá™Áî±ÂΩ¢Âºè', 'pinyin': 'z√¨y√≥u x√≠ngsh√¨', 'trans': 'free-form'}, {'word': 'ÂõûÂ§ç', 'pinyin': 'hu√≠f√π', 'trans': 'response'}, {'word': 'ËØùÈ¢òËΩ¨Êç¢', 'pinyin': 'hu√†t√≠ zhu«énhu√†n', 'trans': 'topic switching'}, {'word': 'ÊÆµËêΩÊ£ÄÁ¥¢', 'pinyin': 'du√†nlu√≤ ji«énsu«í', 'trans': 'paragraph retrieval'}, {'word': 'ÂºïÁî®Ê†áËÆ∞', 'pinyin': 'y«êny√≤ng biƒÅoj√¨', 'trans': 'citation marking'}, {'word': 'Ê°ÜÊû∂', 'pinyin': 'ku√†ngji√†', 'trans': 'framework'}, {'word': 'ÁªºÂêà', 'pinyin': 'z≈çngh√©', 'trans': 'comprehensive'}, {'word': 'ÊΩúÂäõ', 'pinyin': 'qi√°nl√¨', 'trans': 'potential'}]
[01.11.2024 01:03] Renaming previous Chinese page.
[01.11.2024 01:03] Renaming previous data. zh.html to ./d/2024-10-31_zh_reading_task.html
[01.11.2024 01:03] Writing result.
[01.11.2024 01:03] Writing Chinese reading task.
[01.11.2024 01:03] Renaming log file.
[01.11.2024 01:03] Renaming previous data. log.txt to ./logs/2024-11-01_last_log.txt
