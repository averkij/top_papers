[01.11.2024 06:07] [Experimental] Generating an image for paper Constraint Back-translation Improves Complex Instruction Following of Large Language Models.
[01.11.2024 06:07] OpenAI request. Model: gpt-4o-mini. Prompt: Write a text with image prompt in style of surrealism and modern art based on the following paper. Use key themes and elements from it. Add instruction to write a text that reads as brief paper title as a label on some object on an image. Style: linear art on white background. Return only prompt and nothing else. Title: 'Constraint Back-translation Improves Complex Instruction Following of Large Language Models' Text: 'Large language models (LLMs) struggle to follow instructions with complex constraints in format, length, etc. Following the conventional instruction-tuning practice, previous works conduct post-training on complex instruction-response pairs generated by feeding complex instructions to advanced LLMs. However, even advanced LLMs cannot follow complex instructions well, thus limiting the quality of generated data. In this work, we find that existing datasets inherently contain implicit complex constraints and propose a novel data generation technique, constraint back-translation. Specifically, we take the high-quality instruction-response pairs in existing datasets and only adopt advanced LLMs to add complex constraints already met by the responses to the instructions, which naturally reduces costs and data noise. In the experiments, we adopt Llama3-70B-Instruct to back-translate constraints and create a high-quality complex instruction-response dataset, named CRAB. We present that post-training on CRAB improves multiple backbone LLMs' complex instruction-following ability, evaluated on extensive instruction-following benchmarks. We further find that constraint back-translation also serves as a useful auxiliary training objective in post-training. Our code, data, and models will be released to facilitate future research.'
[01.11.2024 06:07] Response: **Prompt:** Create a linear art illustration on a white background that visually represents the themes of 'Constraint Back-translation Improves Complex Instruction Following of Large Language Models.' Imagine a surreal landscape where abstract shapes symbolize complex instructions intertwined with flowing lines and patterns representing constraints. Add elements such as fragmented text blocks and geometric figures to depict the idea of advanced LLMs struggling and evolving through layers of complex information. In the foreground, place an oversized paper label with the title 'Constraint Back-translation Improves Complex Instruction Following of Large Language Models' inscribed in a bold, modern font, serving as a central object of focus in the artwork.
[01.11.2024 06:07] Generating image by prompt: **Prompt:** Create a linear art illustration on a white background that visually represents the themes of 'Constraint Back-translation Improves Complex Instruction Following of Large Language Models.' Imagine a surreal landscape where abstract shapes symbolize complex instructions intertwined with flowing lines and patterns representing constraints. Add elements such as fragmented text blocks and geometric figures to depict the idea of advanced LLMs struggling and evolving through layers of complex information. In the foreground, place an oversized paper label with the title 'Constraint Back-translation Improves Complex Instruction Following of Large Language Models' inscribed in a bold, modern font, serving as a central object of focus in the artwork..
[01.11.2024 06:07] Saving generated image from https://fal.media/files/monkey/7ZBHbKkm2LBZDJHsdIa_r.png to 6550f79d46b1945c.jpg.
[01.11.2024 06:18] Read previous papers.
[01.11.2024 06:18] Get feed.
[01.11.2024 06:18] Get page data from previous paper. URL: https://huggingface.co/papers/2410.24175
[01.11.2024 06:18] Get page data from previous paper. URL: https://huggingface.co/papers/2410.23743
[01.11.2024 06:18] Get page data from previous paper. URL: https://huggingface.co/papers/2410.21969
[01.11.2024 06:18] Get page data from previous paper. URL: https://huggingface.co/papers/2410.23933
[01.11.2024 06:18] Extract page data from URL. URL: https://huggingface.co/papers/2410.24211
[01.11.2024 06:18] Get page data from previous paper. URL: https://huggingface.co/papers/2410.24213
[01.11.2024 06:18] ********************************************************************************
[01.11.2024 06:18] Abstract 0. Large language models (LLMs) struggle to follow instructions with complex constraints in format, length, etc. Following the conventional instruction-tuning practice, previous works conduct post-training on complex instruction-response pairs generated by feeding complex instructions to advanced LLMs....
[01.11.2024 06:18] ********************************************************************************
[01.11.2024 06:18] Abstract 1. What makes a difference in the post-training of LLMs? We investigate the training patterns of different layers in large language models (LLMs), through the lens of gradient, when training with different responses and initial models. We are specifically interested in how fast vs. slow thinking affect...
[01.11.2024 06:18] ********************************************************************************
[01.11.2024 06:18] Abstract 2. Medical Vision-Language Pretraining (MedVLP) shows promise in learning generalizable and transferable visual representations from paired and unpaired medical images and reports. MedVLP can provide useful features to downstream tasks and facilitate adapting task-specific models to new setups using fe...
[01.11.2024 06:18] ********************************************************************************
[01.11.2024 06:18] Abstract 3. Recent advancements in Large Language Models (LLMs) have significantly enhanced their ability to process long contexts, yet a notable gap remains in generating long, aligned outputs. This limitation stems from a training gap where pre-training lacks effective instructions for long-text generation, a...
[01.11.2024 06:18] ********************************************************************************
[01.11.2024 06:18] Abstract 4. Tracking dense 3D motion from monocular videos remains challenging, particularly when aiming for pixel-level precision over long sequences. We introduce \Approach, a novel method that efficiently tracks every pixel in 3D space, enabling accurate motion estimation across entire videos. Our approach l...
[01.11.2024 06:18] ********************************************************************************
[01.11.2024 06:18] Abstract 5. In this paper, we show that useful video representations can be learned from synthetic videos and natural images, without incorporating natural videos in the training. We propose a progression of video datasets synthesized by simple generative processes, that model a growing set of natural video pro...
[01.11.2024 06:18] Read previous papers.
[01.11.2024 06:18] Generating reviews via LLM API.
[01.11.2024 06:18] Using data from previous issue: {"categories": ["#dataset", "#data", "#benchmark"], "emoji": "ü¶Ä", "ru": {"title": "–£–ª—É—á—à–µ–Ω–∏–µ —Å–ª–µ–¥–æ–≤–∞–Ω–∏—è —Å–ª–æ–∂–Ω—ã–º –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º –≤ –ë–Ø–ú —á–µ—Ä–µ–∑ –æ–±—Ä–∞—Ç–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (–ë–Ø–ú) —Å–ª–µ–¥–æ–≤–∞—Ç—å —Å–ª–æ–∂–Ω—ã–º –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º. –ê–≤—Ç–æ
[01.11.2024 06:18] Using data from previous issue: {"categories": ["#training", "#agents", "#reasoning"], "emoji": "üß†", "ru": {"title": "–ì—Ä–∞–¥–∏–µ–Ω—Ç—ã —Ä–∞—Å–∫—Ä—ã–≤–∞—é—Ç —Ç–∞–π–Ω—ã –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø–∞—Ç—Ç–µ—Ä–Ω—ã –æ–±—É—á–µ–Ω–∏—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å–ª–æ–µ–≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) —á–µ—Ä–µ–∑ –ø—Ä–∏–∑–º—É –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤. –ê–≤—Ç–æ—Ä—ã –∏–∑—É—á–∞—é—Ç –≤–ª–∏—è–Ω–∏–µ –±—ã—Å—Ç—Ä–æ–≥–æ –∏ –º
[01.11.2024 06:18] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#medicine"], "emoji": "ü©ª", "ru": {"title": "–ï–¥–∏–Ω—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π –∞–Ω–∞–ª–∏–∑–∞ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç BenchX - —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å–∏—Å—Ç–µ–º—É –æ—Ü–µ–Ω–∫–∏ –º–µ—Ç–æ–¥–æ–≤ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ —Ç–µ–∫—Å—Ç–æ–≤ (MedV
[01.11.2024 06:18] Using data from previous issue: {"categories": ["#long_context", "#benchmark"], "emoji": "üìè", "ru": {"title": "–°–∞–º–æ—É–¥–ª–∏–Ω–µ–Ω–∏–µ: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è LLM", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤, –Ω–∞–∑—ã–≤–∞–µ–º—ã–π Self-Lengthen. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –∏—Å–ø–æ–ª—å–∑—É
[01.11.2024 06:18] Querying the API.
[01.11.2024 06:18] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Tracking dense 3D motion from monocular videos remains challenging, particularly when aiming for pixel-level precision over long sequences. We introduce \Approach, a novel method that efficiently tracks every pixel in 3D space, enabling accurate motion estimation across entire videos. Our approach leverages a joint global-local attention mechanism for reduced-resolution tracking, followed by a transformer-based upsampler to achieve high-resolution predictions. Unlike existing methods, which are limited by computational inefficiency or sparse tracking, \Approach delivers dense 3D tracking at scale, running over 8x faster than previous methods while achieving state-of-the-art accuracy. Furthermore, we explore the impact of depth representation on tracking performance and identify log-depth as the optimal choice. Extensive experiments demonstrate the superiority of \Approach on multiple benchmarks, achieving new state-of-the-art results in both 2D and 3D dense tracking tasks. Our method provides a robust solution for applications requiring fine-grained, long-term motion tracking in 3D space.
[01.11.2024 06:18] Response: {
  "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ \Approach –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –∫–∞–∂–¥–æ–≥–æ –ø–∏–∫—Å–µ–ª—è –≤ 3D-–ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –Ω–∞ –ø—Ä–æ—Ç—è–∂–µ–Ω–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–µ—Ö–∞–Ω–∏–∑–º —Å–æ–≤–º–µ—Å—Ç–Ω–æ–≥–æ –≥–ª–æ–±–∞–ª—å–Ω–æ-–ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –≤ –ø–æ–Ω–∏–∂–µ–Ω–Ω–æ–º —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–∏, –∞ –∑–∞—Ç–µ–º –ø—Ä–∏–º–µ–Ω—è–µ—Ç –∞–ø—Å–µ–º–ø–ª–µ—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –≤—ã—Å–æ–∫–æ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è. \Approach —Ä–∞–±–æ—Ç–∞–µ—Ç –≤ 8 —Ä–∞–∑ –±—ã—Å—Ç—Ä–µ–µ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –º–µ—Ç–æ–¥–æ–≤, –¥–æ—Å—Ç–∏–≥–∞—è –ø—Ä–∏ —ç—Ç–æ–º –Ω–∞–∏–ª—É—á—à–µ–π —Ç–æ—á–Ω–æ—Å—Ç–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ –º–µ—Ç–æ–¥–∞ –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö –≤ –∑–∞–¥–∞—á–∞—Ö –ø–ª–æ—Ç–Ω–æ–≥–æ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –≤ 2D –∏ 3D.",

  "emoji": "üé•",

  "title": "–¢–æ—á–Ω–æ–µ –∏ –±—ã—Å—Ç—Ä–æ–µ 3D-–æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –¥–≤–∏–∂–µ–Ω–∏—è –≤ –≤–∏–¥–µ–æ –Ω–∞ —É—Ä–æ–≤–Ω–µ –ø–∏–∫—Å–µ–ª–µ–π"
}
[01.11.2024 06:18] Error. Failed to parse JSON from LLM. {
  "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ \Approach –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –∫–∞–∂–¥–æ–≥–æ –ø–∏–∫—Å–µ–ª—è –≤ 3D-–ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –Ω–∞ –ø—Ä–æ—Ç—è–∂–µ–Ω–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–µ—Ö–∞–Ω–∏–∑–º —Å–æ–≤–º–µ—Å—Ç–Ω–æ–≥–æ –≥–ª–æ–±–∞–ª—å–Ω–æ-–ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –≤ –ø–æ–Ω–∏–∂–µ–Ω–Ω–æ–º —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–∏, –∞ –∑–∞—Ç–µ–º –ø—Ä–∏–º–µ–Ω—è–µ—Ç –∞–ø—Å–µ–º–ø–ª–µ—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –≤—ã—Å–æ–∫–æ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è. \Approach —Ä–∞–±–æ—Ç–∞–µ—Ç –≤ 8 —Ä–∞–∑ –±—ã—Å—Ç—Ä–µ–µ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –º–µ—Ç–æ–¥–æ–≤, –¥–æ—Å—Ç–∏–≥–∞—è –ø—Ä–∏ —ç—Ç–æ–º –Ω–∞–∏–ª—É—á—à–µ–π —Ç–æ—á–Ω–æ—Å—Ç–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ –º–µ—Ç–æ–¥–∞ –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö –≤ –∑–∞–¥–∞—á–∞—Ö –ø–ª–æ—Ç–Ω–æ–≥–æ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –≤ 2D –∏ 3D.",

  "emoji": "üé•",

  "title": "–¢–æ—á–Ω–æ–µ –∏ –±—ã—Å—Ç—Ä–æ–µ 3D-–æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –¥–≤–∏–∂–µ–Ω–∏—è –≤ –≤–∏–¥–µ–æ –Ω–∞ —É—Ä–æ–≤–Ω–µ –ø–∏–∫—Å–µ–ª–µ–π"
}
[01.11.2024 06:18] Using data from previous issue: {"categories": ["#dataset", "#video", "#synthetic"], "emoji": "üéûÔ∏è", "ru": {"title": "–°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –≤–∏–¥–µ–æ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è –≤–∏–¥–µ–æ–º–æ–¥–µ–ª–µ–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –ø–æ–ª–µ–∑–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –≤–∏–¥–µ–æ –º–æ–∂–Ω–æ –æ–±—É—á–∏—Ç—å –Ω–∞ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –≤–∏–¥–µ–æ –∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö, –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è 
[01.11.2024 06:18] Loading Chinese text from previous data.
[01.11.2024 06:18] Renaming data file.
[01.11.2024 06:18] Renaming previous data. hf_papers.json to ./d/2024-11-01.json
[01.11.2024 06:18] Saving new data file.
[01.11.2024 06:18] Generating page.
[01.11.2024 06:18] Renaming previous page.
[01.11.2024 06:18] Renaming previous data. index.html to ./d/2024-11-01.html
[01.11.2024 06:18] [Experimental] Generating Chinese page for reading.
[01.11.2024 06:18] Chinese vocab [{'word': 'Ê£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàê', 'pinyin': 'ji«énsu«í zƒìngqi√°ng shƒìngch√©ng', 'trans': 'Retrieval-Augmented Generation'}, {'word': 'ËåÉÂºè', 'pinyin': 'f√†nsh√¨', 'trans': 'paradigm'}, {'word': 'ÂπøÊ≥õ', 'pinyin': 'gu«éngf√†n', 'trans': 'extensive'}, {'word': 'ÂçïËΩÆ', 'pinyin': 'dƒÅnl√∫n', 'trans': 'single-turn'}, {'word': 'ÂØπËØù', 'pinyin': 'du√¨hu√†', 'trans': 'dialogue'}, {'word': 'Â§çÊùÇ', 'pinyin': 'f√πz√°', 'trans': 'complex'}, {'word': 'Â§öËΩÆ', 'pinyin': 'du≈çl√∫n', 'trans': 'multi-turn'}, {'word': 'Â°´Ë°•', 'pinyin': 'ti√°nb«î', 'trans': 'fill'}, {'word': 'Á©∫ÁôΩ', 'pinyin': 'k√≤ngb√°i', 'trans': 'gap'}, {'word': 'Âü∫ÂáÜ', 'pinyin': 'jƒ´zh«în', 'trans': 'benchmark'}, {'word': 'Êï∞ÊçÆÈõÜ', 'pinyin': 'sh√πj√πj√≠', 'trans': 'dataset'}, {'word': 'ËØÑ‰º∞', 'pinyin': 'p√≠ngg≈´', 'trans': 'evaluate'}, {'word': '‰ø°ÊÅØÂØªÊ±Ç', 'pinyin': 'x√¨nxƒ´ x√∫nqi√∫', 'trans': 'information-seeking'}, {'word': 'ÂºÄÊîæÂüü', 'pinyin': 'kƒÅif√†ng y√π', 'trans': 'open-domain'}, {'word': 'Áü•ËØÜÂØÜÈõÜÂ∫¶', 'pinyin': 'zhƒ´shi m√¨jƒ´d√π', 'trans': 'knowledge intensity'}, {'word': 'Ëá™Áî±ÂΩ¢Âºè', 'pinyin': 'z√¨y√≥u x√≠ngsh√¨', 'trans': 'free-form'}, {'word': 'ÂõûÂ§ç', 'pinyin': 'hu√≠f√π', 'trans': 'response'}, {'word': 'ËØùÈ¢òËΩ¨Êç¢', 'pinyin': 'hu√†t√≠ zhu«énhu√†n', 'trans': 'topic switching'}, {'word': 'ÊÆµËêΩÊ£ÄÁ¥¢', 'pinyin': 'du√†nlu√≤ ji«énsu«í', 'trans': 'paragraph retrieval'}, {'word': 'ÂºïÁî®Ê†áËÆ∞', 'pinyin': 'y«êny√≤ng biƒÅoj√¨', 'trans': 'citation marking'}, {'word': 'Ê°ÜÊû∂', 'pinyin': 'ku√†ngji√†', 'trans': 'framework'}, {'word': 'ÁªºÂêà', 'pinyin': 'z≈çngh√©', 'trans': 'comprehensive'}, {'word': 'ÊΩúÂäõ', 'pinyin': 'qi√°nl√¨', 'trans': 'potential'}]
[01.11.2024 06:18] Renaming previous Chinese page.
[01.11.2024 06:18] Renaming previous data. zh.html to ./d/2024-10-31_zh_reading_task.html
[01.11.2024 06:18] Writing result.
[01.11.2024 06:18] Writing Chinese reading task.
[01.11.2024 06:18] Renaming log file.
[01.11.2024 06:18] Renaming previous data. log.txt to ./logs/2024-11-01_last_log.txt
