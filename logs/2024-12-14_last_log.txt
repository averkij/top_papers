[14.12.2024 06:30] Read previous papers.
[14.12.2024 06:30] Generating top page (month).
[14.12.2024 06:30] Writing top page (month).
[14.12.2024 12:41] Read previous papers.
[14.12.2024 12:41] Get feed.
[14.12.2024 12:41] Get page data from previous paper. URL: https://huggingface.co/papers/2412.09596
[14.12.2024 12:41] Get page data from previous paper. URL: https://huggingface.co/papers/2412.08905
[14.12.2024 12:41] Get page data from previous paper. URL: https://huggingface.co/papers/2412.08737
[14.12.2024 12:41] Get page data from previous paper. URL: https://huggingface.co/papers/2412.08635
[14.12.2024 12:41] Get page data from previous paper. URL: https://huggingface.co/papers/2412.09618
[14.12.2024 12:41] Get page data from previous paper. URL: https://huggingface.co/papers/2412.09605
[14.12.2024 12:41] Get page data from previous paper. URL: https://huggingface.co/papers/2412.09619
[14.12.2024 12:41] Get page data from previous paper. URL: https://huggingface.co/papers/2412.09593
[14.12.2024 12:41] Get page data from previous paper. URL: https://huggingface.co/papers/2412.09501
[14.12.2024 12:41] Get page data from previous paper. URL: https://huggingface.co/papers/2412.05994
[14.12.2024 12:41] Get page data from previous paper. URL: https://huggingface.co/papers/2412.09569
[14.12.2024 12:41] Get page data from previous paper. URL: https://huggingface.co/papers/2412.09405
[14.12.2024 12:41] Get page data from previous paper. URL: https://huggingface.co/papers/2412.09585
[14.12.2024 12:41] Get page data from previous paper. URL: https://huggingface.co/papers/2412.08972
[14.12.2024 12:41] Get page data from previous paper. URL: https://huggingface.co/papers/2412.08687
[14.12.2024 12:41] Get page data from previous paper. URL: https://huggingface.co/papers/2412.09622
[14.12.2024 12:41] Get page data from previous paper. URL: https://huggingface.co/papers/2412.09013
[14.12.2024 12:41] Get page data from previous paper. URL: https://huggingface.co/papers/2412.09573
[14.12.2024 12:41] Get page data from previous paper. URL: https://huggingface.co/papers/2412.06745
[14.12.2024 12:41] Get page data from previous paper. URL: https://huggingface.co/papers/2412.09349
[14.12.2024 12:41] Get page data from previous paper. URL: https://huggingface.co/papers/2412.09370
[14.12.2024 12:41] Get page data from previous paper. URL: https://huggingface.co/papers/2412.09586
[14.12.2024 12:41] Get page data from previous paper. URL: https://huggingface.co/papers/2412.06329
[14.12.2024 12:41] Get page data from previous paper. URL: https://huggingface.co/papers/2412.09025
[14.12.2024 12:41] Get page data from previous paper. URL: https://huggingface.co/papers/2412.09460
[14.12.2024 12:41] Get page data from previous paper. URL: https://huggingface.co/papers/2412.05552
[14.12.2024 12:41] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[14.12.2024 12:41] No deleted papers detected.
[14.12.2024 12:41] Downloading and parsing papers (pdf, html). Total: 26.
[14.12.2024 12:41] Downloading and parsing paper https://huggingface.co/papers/2412.09596.
[14.12.2024 12:41] Extra JSON file exists (./assets/json/2412.09596.json), skip PDF parsing.
[14.12.2024 12:41] Paper image links file exists (./assets/img_data/2412.09596.json), skip HTML parsing.
[14.12.2024 12:41] Success.
[14.12.2024 12:41] Downloading and parsing paper https://huggingface.co/papers/2412.08905.
[14.12.2024 12:41] Extra JSON file exists (./assets/json/2412.08905.json), skip PDF parsing.
[14.12.2024 12:41] Paper image links file exists (./assets/img_data/2412.08905.json), skip HTML parsing.
[14.12.2024 12:41] Success.
[14.12.2024 12:41] Downloading and parsing paper https://huggingface.co/papers/2412.08737.
[14.12.2024 12:41] Extra JSON file exists (./assets/json/2412.08737.json), skip PDF parsing.
[14.12.2024 12:41] Paper image links file exists (./assets/img_data/2412.08737.json), skip HTML parsing.
[14.12.2024 12:41] Success.
[14.12.2024 12:41] Downloading and parsing paper https://huggingface.co/papers/2412.08635.
[14.12.2024 12:41] Extra JSON file exists (./assets/json/2412.08635.json), skip PDF parsing.
[14.12.2024 12:41] Paper image links file exists (./assets/img_data/2412.08635.json), skip HTML parsing.
[14.12.2024 12:41] Success.
[14.12.2024 12:41] Downloading and parsing paper https://huggingface.co/papers/2412.09618.
[14.12.2024 12:41] Extra JSON file exists (./assets/json/2412.09618.json), skip PDF parsing.
[14.12.2024 12:41] Paper image links file exists (./assets/img_data/2412.09618.json), skip HTML parsing.
[14.12.2024 12:41] Success.
[14.12.2024 12:41] Downloading and parsing paper https://huggingface.co/papers/2412.09605.
[14.12.2024 12:41] Extra JSON file exists (./assets/json/2412.09605.json), skip PDF parsing.
[14.12.2024 12:41] Paper image links file exists (./assets/img_data/2412.09605.json), skip HTML parsing.
[14.12.2024 12:41] Success.
[14.12.2024 12:41] Downloading and parsing paper https://huggingface.co/papers/2412.09619.
[14.12.2024 12:41] Extra JSON file exists (./assets/json/2412.09619.json), skip PDF parsing.
[14.12.2024 12:41] Paper image links file exists (./assets/img_data/2412.09619.json), skip HTML parsing.
[14.12.2024 12:41] Success.
[14.12.2024 12:41] Downloading and parsing paper https://huggingface.co/papers/2412.09593.
[14.12.2024 12:41] Extra JSON file exists (./assets/json/2412.09593.json), skip PDF parsing.
[14.12.2024 12:41] Paper image links file exists (./assets/img_data/2412.09593.json), skip HTML parsing.
[14.12.2024 12:41] Success.
[14.12.2024 12:41] Downloading and parsing paper https://huggingface.co/papers/2412.09501.
[14.12.2024 12:41] Extra JSON file exists (./assets/json/2412.09501.json), skip PDF parsing.
[14.12.2024 12:41] Paper image links file exists (./assets/img_data/2412.09501.json), skip HTML parsing.
[14.12.2024 12:41] Success.
[14.12.2024 12:41] Downloading and parsing paper https://huggingface.co/papers/2412.05994.
[14.12.2024 12:41] Extra JSON file exists (./assets/json/2412.05994.json), skip PDF parsing.
[14.12.2024 12:41] Paper image links file exists (./assets/img_data/2412.05994.json), skip HTML parsing.
[14.12.2024 12:41] Success.
[14.12.2024 12:41] Downloading and parsing paper https://huggingface.co/papers/2412.09569.
[14.12.2024 12:41] Extra JSON file exists (./assets/json/2412.09569.json), skip PDF parsing.
[14.12.2024 12:41] Paper image links file exists (./assets/img_data/2412.09569.json), skip HTML parsing.
[14.12.2024 12:41] Success.
[14.12.2024 12:41] Downloading and parsing paper https://huggingface.co/papers/2412.09405.
[14.12.2024 12:41] Extra JSON file exists (./assets/json/2412.09405.json), skip PDF parsing.
[14.12.2024 12:41] Paper image links file exists (./assets/img_data/2412.09405.json), skip HTML parsing.
[14.12.2024 12:41] Success.
[14.12.2024 12:41] Downloading and parsing paper https://huggingface.co/papers/2412.09585.
[14.12.2024 12:41] Extra JSON file exists (./assets/json/2412.09585.json), skip PDF parsing.
[14.12.2024 12:41] Paper image links file exists (./assets/img_data/2412.09585.json), skip HTML parsing.
[14.12.2024 12:41] Success.
[14.12.2024 12:41] Downloading and parsing paper https://huggingface.co/papers/2412.08972.
[14.12.2024 12:41] Extra JSON file exists (./assets/json/2412.08972.json), skip PDF parsing.
[14.12.2024 12:41] Paper image links file exists (./assets/img_data/2412.08972.json), skip HTML parsing.
[14.12.2024 12:41] Success.
[14.12.2024 12:41] Downloading and parsing paper https://huggingface.co/papers/2412.08687.
[14.12.2024 12:41] Extra JSON file exists (./assets/json/2412.08687.json), skip PDF parsing.
[14.12.2024 12:41] Paper image links file exists (./assets/img_data/2412.08687.json), skip HTML parsing.
[14.12.2024 12:41] Success.
[14.12.2024 12:41] Downloading and parsing paper https://huggingface.co/papers/2412.09622.
[14.12.2024 12:41] Extra JSON file exists (./assets/json/2412.09622.json), skip PDF parsing.
[14.12.2024 12:41] Paper image links file exists (./assets/img_data/2412.09622.json), skip HTML parsing.
[14.12.2024 12:41] Success.
[14.12.2024 12:41] Downloading and parsing paper https://huggingface.co/papers/2412.09013.
[14.12.2024 12:41] Extra JSON file exists (./assets/json/2412.09013.json), skip PDF parsing.
[14.12.2024 12:41] Paper image links file exists (./assets/img_data/2412.09013.json), skip HTML parsing.
[14.12.2024 12:41] Success.
[14.12.2024 12:41] Downloading and parsing paper https://huggingface.co/papers/2412.09573.
[14.12.2024 12:41] Extra JSON file exists (./assets/json/2412.09573.json), skip PDF parsing.
[14.12.2024 12:41] Paper image links file exists (./assets/img_data/2412.09573.json), skip HTML parsing.
[14.12.2024 12:41] Success.
[14.12.2024 12:41] Downloading and parsing paper https://huggingface.co/papers/2412.06745.
[14.12.2024 12:41] Extra JSON file exists (./assets/json/2412.06745.json), skip PDF parsing.
[14.12.2024 12:41] Paper image links file exists (./assets/img_data/2412.06745.json), skip HTML parsing.
[14.12.2024 12:41] Success.
[14.12.2024 12:41] Downloading and parsing paper https://huggingface.co/papers/2412.09349.
[14.12.2024 12:41] Extra JSON file exists (./assets/json/2412.09349.json), skip PDF parsing.
[14.12.2024 12:41] Paper image links file exists (./assets/img_data/2412.09349.json), skip HTML parsing.
[14.12.2024 12:41] Success.
[14.12.2024 12:41] Downloading and parsing paper https://huggingface.co/papers/2412.09370.
[14.12.2024 12:41] Extra JSON file exists (./assets/json/2412.09370.json), skip PDF parsing.
[14.12.2024 12:41] Paper image links file exists (./assets/img_data/2412.09370.json), skip HTML parsing.
[14.12.2024 12:41] Success.
[14.12.2024 12:41] Downloading and parsing paper https://huggingface.co/papers/2412.09586.
[14.12.2024 12:41] Extra JSON file exists (./assets/json/2412.09586.json), skip PDF parsing.
[14.12.2024 12:41] Paper image links file exists (./assets/img_data/2412.09586.json), skip HTML parsing.
[14.12.2024 12:41] Success.
[14.12.2024 12:41] Downloading and parsing paper https://huggingface.co/papers/2412.06329.
[14.12.2024 12:41] Extra JSON file exists (./assets/json/2412.06329.json), skip PDF parsing.
[14.12.2024 12:41] Paper image links file exists (./assets/img_data/2412.06329.json), skip HTML parsing.
[14.12.2024 12:41] Success.
[14.12.2024 12:41] Downloading and parsing paper https://huggingface.co/papers/2412.09025.
[14.12.2024 12:41] Extra JSON file exists (./assets/json/2412.09025.json), skip PDF parsing.
[14.12.2024 12:41] Paper image links file exists (./assets/img_data/2412.09025.json), skip HTML parsing.
[14.12.2024 12:41] Success.
[14.12.2024 12:41] Downloading and parsing paper https://huggingface.co/papers/2412.09460.
[14.12.2024 12:41] Extra JSON file exists (./assets/json/2412.09460.json), skip PDF parsing.
[14.12.2024 12:41] Paper image links file exists (./assets/img_data/2412.09460.json), skip HTML parsing.
[14.12.2024 12:41] Success.
[14.12.2024 12:41] Downloading and parsing paper https://huggingface.co/papers/2412.05552.
[14.12.2024 12:41] Extra JSON file exists (./assets/json/2412.05552.json), skip PDF parsing.
[14.12.2024 12:41] Paper image links file exists (./assets/img_data/2412.05552.json), skip HTML parsing.
[14.12.2024 12:41] Success.
[14.12.2024 12:41] Enriching papers with extra data.
[14.12.2024 12:41] ********************************************************************************
[14.12.2024 12:41] Abstract 0. Creating AI systems that can interact with environments over long periods, similar to human cognition, has been a longstanding research goal. Recent advancements in multimodal large language models (MLLMs) have made significant strides in open-world understanding. However, the challenge of continuou...
[14.12.2024 12:41] ********************************************************************************
[14.12.2024 12:41] Abstract 1. We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data ...
[14.12.2024 12:41] ********************************************************************************
[14.12.2024 12:41] Abstract 2. Multimodal large language models (MLLMs) have made rapid progress in recent years, yet continue to struggle with low-level visual perception (LLVP) -- particularly the ability to accurately describe the geometric details of an image. This capability is crucial for applications in areas such as robot...
[14.12.2024 12:41] ********************************************************************************
[14.12.2024 12:41] Abstract 3. Multimodal generative models require a unified approach to handle both discrete data (e.g., text and code) and continuous data (e.g., image, audio, video). In this work, we propose Latent Language Modeling (LatentLM), which seamlessly integrates continuous and discrete data using causal Transformers...
[14.12.2024 12:41] ********************************************************************************
[14.12.2024 12:41] Abstract 4. Significant achievements in personalization of diffusion models have been witnessed. Conventional tuning-free methods mostly encode multiple reference images by averaging their image embeddings as the injection condition, but such an image-independent operation cannot perform interaction among image...
[14.12.2024 12:41] ********************************************************************************
[14.12.2024 12:41] Abstract 5. Graphical User Interface (GUI) agents hold great potential for automating complex tasks across diverse digital environments, from web applications to desktop software. However, the development of such agents is hindered by the lack of high-quality, multi-step trajectory data required for effective t...
[14.12.2024 12:41] ********************************************************************************
[14.12.2024 12:41] Abstract 6. Existing text-to-image (T2I) diffusion models face several limitations, including large model sizes, slow runtime, and low-quality generation on mobile devices. This paper aims to address all of these challenges by developing an extremely small and fast T2I model that generates high-resolution and h...
[14.12.2024 12:41] ********************************************************************************
[14.12.2024 12:41] Abstract 7. Recovering the geometry and materials of objects from a single image is challenging due to its under-constrained nature. In this paper, we present Neural LightRig, a novel framework that boosts intrinsic estimation by leveraging auxiliary multi-lighting conditions from 2D diffusion priors. Specifica...
[14.12.2024 12:41] ********************************************************************************
[14.12.2024 12:41] Abstract 8. As Multi-modal Large Language Models (MLLMs) evolve, expanding beyond single-domain capabilities is essential to meet the demands for more versatile and efficient AI. However, previous omni-models have insufficiently explored speech, neglecting its integration with multi-modality. We introduce Lyra,...
[14.12.2024 12:41] ********************************************************************************
[14.12.2024 12:41] Abstract 9. The approximation of Partial Differential Equations (PDEs) using neural networks has seen significant advancements through Physics-Informed Neural Networks (PINNs). Despite their straightforward optimization framework and flexibility in implementing various PDEs, PINNs often suffer from limited accu...
[14.12.2024 12:41] ********************************************************************************
[14.12.2024 12:41] Abstract 10. Given the rapid progress of generative AI, there is a pressing need to systematically compare and choose between the numerous models and configurations available. The scale and versatility of such evaluations make the use of LLM-based judges a compelling solution for this challenge. Crucially, this ...
[14.12.2024 12:41] ********************************************************************************
[14.12.2024 12:41] Abstract 11. Modern sensors produce increasingly rich streams of high-resolution data. Due to resource constraints, machine learning systems discard the vast majority of this information via resolution reduction. Compressed-domain learning allows models to operate on compact latent representations, allowing high...
[14.12.2024 12:41] ********************************************************************************
[14.12.2024 12:41] Abstract 12. The standard practice for developing contemporary MLLMs is to feed features from vision encoder(s) into the LLM and train with natural language supervision. In this work, we posit an overlooked opportunity to optimize the intermediate LLM representations through a vision perspective (objective), i.e...
[14.12.2024 12:41] ********************************************************************************
[14.12.2024 12:41] Abstract 13. This paper introduces RuleArena, a novel and challenging benchmark designed to evaluate the ability of large language models (LLMs) to follow complex, real-world rules in reasoning. Covering three practical domains -- airline baggage fees, NBA transactions, and tax regulations -- RuleArena assesses ...
[14.12.2024 12:41] ********************************************************************************
[14.12.2024 12:41] Abstract 14. With the growing adoption and capabilities of vision-language models (VLMs) comes the need for benchmarks that capture authentic user-VLM interactions. In response, we create VisionArena, a dataset of 230K real-world conversations between users and VLMs. Collected from Chatbot Arena - an open-source...
[14.12.2024 12:41] ********************************************************************************
[14.12.2024 12:41] Abstract 15. Recent advances in text-to-image customization have enabled high-fidelity, context-rich generation of personalized images, allowing specific concepts to appear in a variety of scenarios. However, current methods struggle with combining multiple personalized models, often leading to attribute entangl...
[14.12.2024 12:41] ********************************************************************************
[14.12.2024 12:41] Abstract 16. This study presents a new image super-resolution (SR) technique based on diffusion inversion, aiming at harnessing the rich image priors encapsulated in large pre-trained diffusion models to improve SR performance. We design a Partial noise Prediction strategy to construct an intermediate state of t...
[14.12.2024 12:41] ********************************************************************************
[14.12.2024 12:41] Abstract 17. Existing sparse-view reconstruction models heavily rely on accurate known camera poses. However, deriving camera extrinsics and intrinsics from sparse-view images presents significant challenges. In this work, we present FreeSplatter, a highly scalable, feed-forward reconstruction framework capable ...
[14.12.2024 12:41] ********************************************************************************
[14.12.2024 12:41] Abstract 18. Traditional fixed test sets fall short in evaluating open-ended capabilities of foundation models. To address this, we propose ONEBench(OpeN-Ended Benchmarking), a new testing paradigm that consolidates individual evaluation datasets into a unified, ever-expanding sample pool. ONEBench allows users ...
[14.12.2024 12:41] ********************************************************************************
[14.12.2024 12:41] Abstract 19. Controllable human image animation aims to generate videos from reference images using driving videos. Due to the limited control signals provided by sparse guidance (e.g., skeleton pose), recent works have attempted to introduce additional dense conditions (e.g., depth map) to ensure motion alignme...
[14.12.2024 12:41] ********************************************************************************
[14.12.2024 12:41] Abstract 20. Word Sense Disambiguation (WSD) is the task of associating a word in a given context with its most suitable meaning among a set of possible candidates. While the task has recently witnessed renewed interest, with systems achieving performances above the estimated inter-annotator agreement, at the ti...
[14.12.2024 12:41] ********************************************************************************
[14.12.2024 12:41] Abstract 21. We address the problem of gaze target estimation, which aims to predict where a person is looking in a scene. Predicting a person's gaze target requires reasoning both about the person's appearance and the contents of the scene. Prior works have developed increasingly complex, hand-crafted pipelines...
[14.12.2024 12:41] ********************************************************************************
[14.12.2024 12:41] Abstract 22. Normalizing Flows (NFs) are likelihood-based models for continuous inputs. They have demonstrated promising results on both density estimation and generative modeling tasks, but have received relatively little attention in recent years. In this work, we demonstrate that NFs are more powerful than pr...
[14.12.2024 12:41] ********************************************************************************
[14.12.2024 12:41] Abstract 23. Neural Machine Translation (NMT) models are typically trained on datasets with limited exposure to Scientific, Technical and Educational domains. Translation models thus, in general, struggle with tasks that involve scientific understanding or technical jargon. Their performance is found to be even ...
[14.12.2024 12:41] ********************************************************************************
[14.12.2024 12:41] Abstract 24. The use of copyrighted materials in training generative language models raises critical legal and ethical questions. This paper presents a framework for and the results of empirically assessing the impact of copyrighted materials on the performance of large language models (LLMs) for Norwegian. We f...
[14.12.2024 12:41] ********************************************************************************
[14.12.2024 12:41] Abstract 25. The academic field of learning instruction-guided visual navigation can be generally categorized into high-level category-specific search and low-level language-guided navigation, depending on the granularity of language instruction, in which the former emphasizes the exploration process, while the ...
[14.12.2024 12:41] Read previous papers.
[14.12.2024 12:41] Generating reviews via LLM API.
[14.12.2024 12:41] Using data from previous issue: {"categories": ["#long_context", "#audio", "#cv", "#multimodal", "#reasoning"], "emoji": "ğŸ§ ", "ru": {"title": "ĞĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑˆĞ°Ğ³ Ğº Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾Ğ¼Ñƒ Ğ˜Ğ˜", "desc": "Ğ”Ğ°Ğ½Ğ½Ğ°Ñ ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… Ğº Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼
[14.12.2024 12:41] Using data from previous issue: {"categories": ["#data", "#reasoning", "#synthetic", "#training", "#benchmark", "#architecture"], "emoji": "ğŸ§ ", "ru": {"title": "ĞšĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… - ĞºĞ»ÑÑ‡ Ğº Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ñƒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ phi-4 - ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ 14 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½ÑƒÑ Ñ Ğ°ĞºÑ†ĞµĞ½Ñ‚Ğ¾Ğ¼ Ğ½Ğ° ĞºĞ°Ñ‡Ğµ
[14.12.2024 12:41] Using data from previous issue: {"categories": ["#benchmark", "#architecture", "#low_resource", "#synthetic", "#multimodal", "#optimization", "#training", "#data"], "emoji": "ğŸ“", "ru": {"title": "ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Geoperception Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸
[14.12.2024 12:41] Using data from previous issue: {"categories": ["#diffusion", "#optimization", "#audio", "#video", "#cv", "#multimodal", "#training"], "emoji": "ğŸ§ ", "ru": {"title": "LatentLM: ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğµ Ğ¸ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğµ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Latent Language Modeling (LatentLM) - ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾
[14.12.2024 12:41] Using data from previous issue: {"categories": ["#benchmark", "#dataset", "#multimodal", "#diffusion", "#optimization", "#training"], "emoji": "ğŸ–¼ï¸", "ru": {"title": "EasyRef: ÑƒĞ¼Ğ½Ğ°Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼ Ñ€ĞµÑ„ĞµÑ€ĞµĞ½ÑĞ°Ğ¼", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ EasyRef - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ñ
[14.12.2024 12:41] Using data from previous issue: {"categories": ["#data", "#dataset", "#training", "#optimization", "#agents", "#synthetic"], "emoji": "ğŸ¤–", "ru": {"title": "AgentTrek: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ GUI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ²ĞµĞ±-Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ AgentTrek - Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²
[14.12.2024 12:41] Using data from previous issue: {"categories": ["#architecture", "#small_models", "#training", "#optimization", "#diffusion", "#benchmark"], "emoji": "ğŸ“±", "ru": {"title": "ĞšĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ°Ñ Ğ¸ Ğ±Ñ‹ÑÑ‚Ñ€Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ…", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ, SnapGen, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±
[14.12.2024 12:41] Using data from previous issue: {"categories": ["#diffusion", "#synthetic", "#open_source", "#cv", "#3d", "#dataset"], "emoji": "ğŸ’¡", "ru": {"title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ 3D-Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ñ", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Neural LightRig - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ñ… ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ² Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¸. ĞœĞµ
[14.12.2024 12:41] Using data from previous issue: {"categories": ["#open_source", "#dataset", "#long_context", "#multimodal", "#benchmark"], "emoji": "ğŸ­", "ru": {"title": "Lyra: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑ‡Ğ¸ Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹", "desc": "Lyra - ÑÑ‚Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ (MLLM), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñƒ
[14.12.2024 12:41] Using data from previous issue: {"categories": ["#optimization", "#math", "#architecture"], "emoji": "ğŸ“Š", "ru": {"title": "ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ñ‹ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¹", "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ°Ğ¿Ğ¿Ñ€Ğ¾ĞºÑĞ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¹ Ğ² Ñ‡Ğ°ÑÑ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ½Ñ‹Ñ… (Ğ”Ğ£Ğ§ĞŸ) Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·
[14.12.2024 12:41] Using data from previous issue: {"categories": ["#interpretability", "#alignment", "#benchmark", "#rlhf"], "emoji": "âš–ï¸", "ru": {"title": "LLM ĞºĞ°Ğº Ğ±ĞµÑĞ¿Ñ€Ğ¸ÑÑ‚Ñ€Ğ°ÑÑ‚Ğ½Ñ‹Ğµ ÑÑƒĞ´ÑŒĞ¸ Ğ˜Ğ˜-ÑĞ¸ÑÑ‚ĞµĞ¼", "desc": "Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ¾Ñ†ĞµĞ½ĞºĞµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ñ€Ğ¾Ğ»Ğ¸ ÑÑƒĞ´ĞµĞ¹ Ğ´Ğ»Ñ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ˜Ğ˜-ÑĞ¸ÑÑ‚ĞµĞ¼
[14.12.2024 12:41] Using data from previous issue: {"categories": ["#architecture", "#inference", "#synthetic", "#dataset", "#optimization", "#multimodal"], "emoji": "ğŸ—œï¸", "ru": {"title": "WaLLoC: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ WaLLoC - Ğ½Ğ¾Ğ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´ĞµĞºĞ° Ğ´Ğ»Ñ ÑĞ¶
[14.12.2024 12:41] Using data from previous issue: {"categories": ["#optimization", "#benchmark", "#architecture", "#open_source", "#cv", "#multimodal"], "emoji": "ğŸ‘ï¸", "ru": {"title": "ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¸Ğ·Ğ¼Ñƒ Ğ·Ñ€ĞµĞ½Ğ¸Ñ", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM)
[14.12.2024 12:41] Using data from previous issue: {"categories": ["#long_context", "#math", "#benchmark", "#reasoning"], "emoji": "ğŸ§ ", "ru": {"title": "Ğ‘Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ¿Ğ¾Ñ‚Ñ‹ĞºĞ°ÑÑ‚ÑÑ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ°Ñ… Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ°", "desc": "RuleArena - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ°Ğ¼ Ğ² 
[14.12.2024 12:41] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#open_source", "#games", "#training", "#reasoning", "#interpretability", "#dataset"], "emoji": "ğŸ–¼ï¸", "ru": {"title": "VisionArena: Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¸ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ·Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ VisionArena - Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· 230 Ñ‚Ñ‹Ñ
[14.12.2024 12:41] Using data from previous issue: {"categories": ["#training", "#multimodal", "#cv"], "emoji": "ğŸ¨", "ru": {"title": "LoRACLR: Ğ“Ğ°Ñ€Ğ¼Ğ¾Ğ½Ğ¸Ñ‡Ğ½Ğ¾Ğµ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹", "desc": "LoRACLR - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸ÑĞ¼Ğ¸, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğ¹ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ LoRA Ğ±ĞµĞ· 
[14.12.2024 12:41] Using data from previous issue: {"categories": ["#cv", "#open_source", "#dataset", "#architecture", "#diffusion"], "emoji": "ğŸ”", "ru": {"title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑĞ²ĞµÑ€Ñ…Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¸Ğ½Ğ²ĞµÑ€ÑĞ¸Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸", "desc": "Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ ÑĞ²ĞµÑ€Ñ…Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ½Ğ° Ğ¸Ğ½Ğ²ĞµÑ€ÑĞ¸Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸. ĞœĞµÑ‚
[14.12.2024 12:41] Using data from previous issue: {"categories": ["#architecture", "#3d"], "emoji": "ğŸ”", "ru": {"title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² 3D-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸: Ğ¾Ñ‚ Ğ½ĞµĞ¾Ñ‚ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğº Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼", "desc": "FreeSplatter - ÑÑ‚Ğ¾ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‰Ğ°Ñ Ñ Ğ½ĞµĞ¾Ñ‚ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ². ĞĞ½Ğ° Ğ¸
[14.12.2024 12:41] Using data from previous issue: {"categories": ["#survey", "#optimization", "#benchmark", "#dataset"], "emoji": "ğŸ§ª", "ru": {"title": "Ğ•Ğ´Ğ¸Ğ½Ñ‹Ğ¹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹", "desc": "ONEBench - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ°Ñ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ² ĞµĞ´Ğ¸Ğ½Ñ‹Ğ¹ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµĞ¼Ñ‹Ğ¹ Ğ¿
[14.12.2024 12:41] Using data from previous issue: {"categories": ["#cv", "#multimodal", "#video"], "emoji": "ğŸ­", "ru": {"title": "DisPose: Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ DisPose - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ€Ğ°Ğ·Ğ´ĞµĞ»Ğ¸Ñ‚ÑŒ ÑĞºĞµĞ»ĞµÑ‚Ğ½Ñƒ
[14.12.2024 12:41] Using data from previous issue: {"categories": ["#multilingual", "#reasoning", "#dataset", "#architecture"], "emoji": "ğŸ”—", "ru": {"title": "ĞÑ‚ WSD Ğº WSL: ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑĞ¼Ñ‹ÑĞ»Ğ° ÑĞ»Ğ¾Ğ² Ğ² Ñ‚ĞµĞºÑÑ‚Ğµ", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Word Sense Linking (WSL), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Word Sense
[14.12.2024 12:41] Using data from previous issue: {"categories": ["#optimization", "#benchmark", "#architecture", "#cv", "#reasoning"], "emoji": "ğŸ‘€", "ru": {"title": "ĞŸÑ€Ğ¾ÑÑ‚Ğ¾Ğ¹ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ğ·Ğ³Ğ»ÑĞ´Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ²", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Gaze-LLE - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ğ·Ğ³Ğ»ÑĞ´Ğ° Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚Ñ€Ğ°Ğ½
[14.12.2024 12:41] Using data from previous issue: {"categories": ["#architecture", "#diffusion", "#optimization", "#training", "#open_source"], "emoji": "ğŸŒŠ", "ru": {"title": "TarFlow: Ğ’Ğ¾Ğ·Ñ€Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ² Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹", "desc": "TarFlow - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ², Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾
[14.12.2024 12:41] Using data from previous issue: {"categories": ["#benchmark", "#multilingual", "#low_resource", "#machine_translation", "#dataset", "#open_source", "#training"], "emoji": "ğŸŒ", "ru": {"title": "ĞĞ¾Ğ²Ñ‹Ğ¹ ĞºĞ¾Ñ€Ğ¿ÑƒÑ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ° Ğ¸Ğ½Ğ´Ğ¸Ğ¹ÑĞºĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²", "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ñ€Ğ¿ÑƒÑ Ğ´Ğ»
[14.12.2024 12:41] Using data from previous issue: {"categories": ["#low_resource", "#multilingual", "#dataset", "#benchmark", "#ethics"], "emoji": "ğŸ“š", "ru": {"title": "ĞĞ²Ñ‚Ğ¾Ñ€ÑĞºĞ¾Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¾ Ğ² ÑĞ¿Ğ¾Ñ…Ñƒ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°: Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ñ Ğ½Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ·Ğ°Ñ‰Ğ¸Ñ‰ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾Ñ€ÑĞºĞ¸Ğ¼ Ğ¿Ñ€Ğ°Ğ²Ğ¾Ğ¼ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ² Ğ½Ğ° ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒ
[14.12.2024 12:41] Using data from previous issue: {"categories": ["#reasoning", "#multimodal", "#agents", "#optimization"], "emoji": "ğŸ§­", "ru": {"title": "Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸ Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼Ğ¸", "desc": "Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸ Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼Ğ¸
[14.12.2024 12:41] Trying to get texts in Chinese.
[14.12.2024 12:41] Mistral request. Model: mistral-large-latest. Prompt: Write simple and brief explanation (4-5 sentences) of an article in Chinese. Use short sentences. Text:

Creating AI systems that can interact with environments over long periods, similar to human cognition, has been a longstanding research goal. Recent advancements in multimodal large language models (MLLMs) have made significant strides in open-world understanding. However, the challenge of continuous and simultaneous streaming perception, memory, and reasoning remains largely unexplored. Current MLLMs are constrained by their sequence-to-sequence architecture, which limits their ability to process inputs and generate responses simultaneously, akin to being unable to think while perceiving. Furthermore, relying on long contexts to store historical data is impractical for long-term interactions, as retaining all information becomes costly and inefficient. Therefore, rather than relying on a single foundation model to perform all functions, this project draws inspiration from the concept of the Specialized Generalist AI and introduces disentangled streaming perception, reasoning, and memory mechanisms, enabling real-time interaction with streaming video and audio input. The proposed framework InternLM-XComposer2.5-OmniLive (IXC2.5-OL) consists of three key modules: (1) Streaming Perception Module: Processes multimodal information in real-time, storing key details in memory and triggering reasoning in response to user queries. (2) Multi-modal Long Memory Module: Integrates short-term and long-term memory, compressing short-term memories into long-term ones for efficient retrieval and improved accuracy. (3) Reasoning Module: Responds to queries and executes reasoning tasks, coordinating with the perception and memory modules. This project simulates human-like cognition, enabling multimodal large language models to provide continuous and adaptive service over time.
[14.12.2024 12:41] Mistral response. {"id": "63760e11a8d24da5ad8e848a985a0ade", "object": "chat.completion", "created": 1734180100, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "\u8fd9\u7bc7\u6587\u7ae0\u8ba8\u8bba\u4e86\u521b\u5efa\u80fd\u591f\u957f\u65f6\u95f4\u4e0e\u73af\u5883\u4e92\u52a8\u7684\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u7684\u7814\u7a76\u76ee\u6807\u3002\u8fd1\u671f\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u5f00\u653e\u4e16\u754c\u7406\u89e3\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u4ecd\u9762\u4e34\u540c\u65f6\u5904\u7406\u611f\u77e5\u3001\u8bb0\u5fc6\u548c\u63a8\u7406\u7684\u6311\u6218\u3002\u73b0\u6709\u7684MLLMs\u7531\u4e8e\u5176\u5e8f\u5217\u5230\u5e8f\u5217\u7684\u67b6\u6784\uff0c\u65e0\u6cd5\u540c\u65f6\u5904\u7406\u8f93\u5165\u548c\u751f\u6210\u54cd\u5e94\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u8be5\u9879\u76ee\u5f15\u5165\u4e86\u5206\u79bb\u7684\u611f\u77e5\u3001\u63a8\u7406\u548c\u8bb0\u5fc6\u673a\u5236\uff0c\u4f7f\u5176\u80fd\u591f\u5b9e\u65f6\u5904\u7406\u89c6\u9891\u548c\u97f3\u9891\u8f93\u5165\u3002\u63d0\u51fa\u7684\u6846\u67b6IXC2.5-OL\u5305\u62ec\u5b9e\u65f6\u5904\u7406\u591a\u6a21\u6001\u4fe1\u606f\u7684\u611f\u77e5\u6a21\u5757\u3001\u6574\u5408\u77ed\u671f\u548c\u957f\u671f\u8bb0\u5fc6\u7684\u8bb0\u5fc6\u6a21\u5757\u4ee5\u53ca\u54cd\u5e94\u67e5\u8be2\u548c\u6267\u884c\u63a8\u7406\u4efb\u52a1\u7684\u63a8\u7406\u6a21\u5757\u3002"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 400, "total_tokens": 638, "completion_tokens": 238}}
[14.12.2024 12:41] Response: è¿™ç¯‡æ–‡ç« è®¨è®ºäº†åˆ›å»ºèƒ½å¤Ÿé•¿æ—¶é—´ä¸ç¯å¢ƒäº’åŠ¨çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„ç ”ç©¶ç›®æ ‡ã€‚è¿‘æœŸå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¼€æ”¾ä¸–ç•Œç†è§£æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ä»é¢ä¸´åŒæ—¶å¤„ç†æ„ŸçŸ¥ã€è®°å¿†å’Œæ¨ç†çš„æŒ‘æˆ˜ã€‚ç°æœ‰çš„MLLMsç”±äºå…¶åºåˆ—åˆ°åºåˆ—çš„æ¶æ„ï¼Œæ— æ³•åŒæ—¶å¤„ç†è¾“å…¥å’Œç”Ÿæˆå“åº”ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œè¯¥é¡¹ç›®å¼•å…¥äº†åˆ†ç¦»çš„æ„ŸçŸ¥ã€æ¨ç†å’Œè®°å¿†æœºåˆ¶ï¼Œä½¿å…¶èƒ½å¤Ÿå®æ—¶å¤„ç†è§†é¢‘å’ŒéŸ³é¢‘è¾“å…¥ã€‚æå‡ºçš„æ¡†æ¶IXC2.5-OLåŒ…æ‹¬å®æ—¶å¤„ç†å¤šæ¨¡æ€ä¿¡æ¯çš„æ„ŸçŸ¥æ¨¡å—ã€æ•´åˆçŸ­æœŸå’Œé•¿æœŸè®°å¿†çš„è®°å¿†æ¨¡å—ä»¥åŠå“åº”æŸ¥è¯¢å’Œæ‰§è¡Œæ¨ç†ä»»åŠ¡çš„æ¨ç†æ¨¡å—ã€‚
[14.12.2024 12:41] Mistral request. Model: mistral-large-latest. Prompt: Write pinyin transcription for text. Text:

è¿™ç¯‡æ–‡ç« è®¨è®ºäº†åˆ›å»ºèƒ½å¤Ÿé•¿æ—¶é—´ä¸ç¯å¢ƒäº’åŠ¨çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„ç ”ç©¶ç›®æ ‡ã€‚è¿‘æœŸå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¼€æ”¾ä¸–ç•Œç†è§£æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ä»é¢ä¸´åŒæ—¶å¤„ç†æ„ŸçŸ¥ã€è®°å¿†å’Œæ¨ç†çš„æŒ‘æˆ˜ã€‚ç°æœ‰çš„MLLMsç”±äºå…¶åºåˆ—åˆ°åºåˆ—çš„æ¶æ„ï¼Œæ— æ³•åŒæ—¶å¤„ç†è¾“å…¥å’Œç”Ÿæˆå“åº”ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œè¯¥é¡¹ç›®å¼•å…¥äº†åˆ†ç¦»çš„æ„ŸçŸ¥ã€æ¨ç†å’Œè®°å¿†æœºåˆ¶ï¼Œä½¿å…¶èƒ½å¤Ÿå®æ—¶å¤„ç†è§†é¢‘å’ŒéŸ³é¢‘è¾“å…¥ã€‚æå‡ºçš„æ¡†æ¶IXC2.5-OLåŒ…æ‹¬å®æ—¶å¤„ç†å¤šæ¨¡æ€ä¿¡æ¯çš„æ„ŸçŸ¥æ¨¡å—ã€æ•´åˆçŸ­æœŸå’Œé•¿æœŸè®°å¿†çš„è®°å¿†æ¨¡å—ä»¥åŠå“åº”æŸ¥è¯¢å’Œæ‰§è¡Œæ¨ç†ä»»åŠ¡çš„æ¨ç†æ¨¡å—ã€‚
[14.12.2024 12:41] Mistral response. {"id": "8e89c75d2e484a2ca533583c1a898191", "object": "chat.completion", "created": 1734180105, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "\u8fd9\u7bc7\u6587\u7ae0\u8ba8\u8bba\u4e86\u521b\u5efa\u80fd\u591f\u957f\u65f6\u95f4\u4e0e\u73af\u5883\u4e92\u52a8\u7684\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u7684\u7814\u7a76\u76ee\u6807\u3002\u8fd1\u671f\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u5f00\u653e\u4e16\u754c\u7406\u89e3\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u4ecd\u9762\u4e34\u540c\u65f6\u5904\u7406\u611f\u77e5\u3001\u8bb0\u5fc6\u548c\u63a8\u7406\u7684\u6311\u6218\u3002\u73b0\u6709\u7684MLLMs\u7531\u4e8e\u5176\u5e8f\u5217\u5230\u5e8f\u5217\u7684\u67b6\u6784\uff0c\u65e0\u6cd5\u540c\u65f6\u5904\u7406\u8f93\u5165\u548c\u751f\u6210\u54cd\u5e94\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u8be5\u9879\u76ee\u5f15\u5165\u4e86\u5206\u79bb\u7684\u611f\u77e5\u3001\u63a8\u7406\u548c\u8bb0\u5fc6\u673a\u5236\uff0c\u4f7f\u5176\u80fd\u591f\u5b9e\u65f6\u5904\u7406\u89c6\u9891\u548c\u97f3\u9891\u8f93\u5165\u3002\u63d0\u51fa\u7684\u6846\u67b6IXC2.5-OL\u5305\u62ec\u5b9e\u65f6\u5904\u7406\u591a\u6a21\u6001\u4fe1\u606f\u7684\u611f\u77e5\u6a21\u5757\u3001\u6574\u5408\u77ed\u671f\u548c\u957f\u671f\u8bb0\u5fc6\u7684\u8bb0\u5fc6\u6a21\u5757\u4ee5\u53ca\u54cd\u5e94\u67e5\u8be2\u548c\u6267\u884c\u63a8\u7406\u4efb\u52a1\u7684\u63a8\u7406\u6a21\u5757\u3002\n\nZh\u00e8 pi\u0101n w\u00e9nzh\u0101ng t\u01ceol\u00f9n le chu\u00e0ngji\u00e0n n\u00e9ngg\u00f2u ch\u00e1ng sh\u00edji\u0101n y\u01d4 hu\u00e1nj\u00ecng h\u00f9d\u00f2ng de r\u00e9ng\u014dng zh\u00ecn\u00e9ng x\u00ect\u01d2ng de y\u00e1nji\u016b m\u00f9bi\u0101o. J\u00ecnq\u012b du\u014d m\u00f3shu\u00e0i d\u00e0 y\u01d4y\u00e1n m\u00f3x\u00edng (MLLMs) z\u00e0i k\u0101if\u00e0ng sh\u00ecji\u00e8 l\u01d0ji\u011b f\u0101ngmi\u00e0n q\u01d4d\u00e9 le xi\u01cenzh\u00f9 j\u00ecnb\u00f9, d\u00e0n r\u00e9ng mi\u00e0nmi\u00e0n t\u00f3ngsh\u00ed ch\u01d4l\u01d0 g\u01cenju\u00e9, j\u00ecy\u00ec h\u00e9 tu\u00edl\u01d0 de ti\u01ceozh\u00e0n. Xi\u00e0ny\u01d2u de MLLMs y\u00f3uy\u00fa q\u00ed x\u00f9li\u00e8 d\u00e0o x\u00f9li\u00e8 de ji\u00e0g\u00f2u, w\u00faf\u01ce t\u00f3ngsh\u00ed ch\u01d4l\u01d0 sh\u016br\u00f9 h\u00e9 sh\u0113ngch\u00e9ng xi\u01cengy\u00ecng. W\u00e8ile ji\u011bju\u00e9 zh\u00e8g\u00e8 w\u00e8nt\u00ed, g\u01cei xi\u00e0ngm\u00f9 y\u01d0nr\u00f9 le f\u0113nl\u00ed de g\u01cenju\u00e9, tu\u00edl\u01d0 h\u00e9 j\u00ecy\u00ec j\u012bzh\u00ec, sh\u01d0 q\u00ed n\u00e9ngg\u00f2u sh\u00edsh\u00ed ch\u01d4l\u01d0 sh\u00ecp\u00edn h\u00e9 y\u012bnp\u00edn sh\u016br\u00f9. T\u00edch\u016b de ku\u00e0ngji\u00e0 IXC2.5-OL b\u0101oku\u00f2 sh\u00edsh\u00ed ch\u01d4l\u01d0 du\u014d m\u00f3shu\u00e0i x\u00ecnx\u012b de g\u01cenju\u00e9 m\u00f3ku\u00e0i, zh\u011bngh\u00e9 du\u01cenq\u012b h\u00e9 ch\u00e1ngq\u012b j\u00ecy\u00ec de j\u00ecy\u00ec m\u00f3ku\u00e0i y\u01d0ji xi\u01cengy\u00ecng ch\u00e1x\u00fan h\u00e9 zh\u00edx\u00edng tu\u00edl\u01d0 r\u00e8nw\u00f9 de tu\u00edl\u01d0 m\u00f3ku\u00e0i."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 253, "total_tokens": 959, "completion_tokens": 706}}
[14.12.2024 12:41] Response: è¿™ç¯‡æ–‡ç« è®¨è®ºäº†åˆ›å»ºèƒ½å¤Ÿé•¿æ—¶é—´ä¸ç¯å¢ƒäº’åŠ¨çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„ç ”ç©¶ç›®æ ‡ã€‚è¿‘æœŸå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¼€æ”¾ä¸–ç•Œç†è§£æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ä»é¢ä¸´åŒæ—¶å¤„ç†æ„ŸçŸ¥ã€è®°å¿†å’Œæ¨ç†çš„æŒ‘æˆ˜ã€‚ç°æœ‰çš„MLLMsç”±äºå…¶åºåˆ—åˆ°åºåˆ—çš„æ¶æ„ï¼Œæ— æ³•åŒæ—¶å¤„ç†è¾“å…¥å’Œç”Ÿæˆå“åº”ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œè¯¥é¡¹ç›®å¼•å…¥äº†åˆ†ç¦»çš„æ„ŸçŸ¥ã€æ¨ç†å’Œè®°å¿†æœºåˆ¶ï¼Œä½¿å…¶èƒ½å¤Ÿå®æ—¶å¤„ç†è§†é¢‘å’ŒéŸ³é¢‘è¾“å…¥ã€‚æå‡ºçš„æ¡†æ¶IXC2.5-OLåŒ…æ‹¬å®æ—¶å¤„ç†å¤šæ¨¡æ€ä¿¡æ¯çš„æ„ŸçŸ¥æ¨¡å—ã€æ•´åˆçŸ­æœŸå’Œé•¿æœŸè®°å¿†çš„è®°å¿†æ¨¡å—ä»¥åŠå“åº”æŸ¥è¯¢å’Œæ‰§è¡Œæ¨ç†ä»»åŠ¡çš„æ¨ç†æ¨¡å—ã€‚

ZhÃ¨ piÄn wÃ©nzhÄng tÇolÃ¹n le chuÃ ngjiÃ n nÃ©nggÃ²u chÃ¡ng shÃ­jiÄn yÇ” huÃ¡njÃ¬ng hÃ¹dÃ²ng de rÃ©ngÅng zhÃ¬nÃ©ng xÃ¬tÇ’ng de yÃ¡njiÅ« mÃ¹biÄo. JÃ¬nqÄ« duÅ mÃ³shuÃ i dÃ  yÇ”yÃ¡n mÃ³xÃ­ng (MLLMs) zÃ i kÄifÃ ng shÃ¬jiÃ¨ lÇjiÄ› fÄngmiÃ n qÇ”dÃ© le xiÇnzhÃ¹ jÃ¬nbÃ¹, dÃ n rÃ©ng miÃ nmiÃ n tÃ³ngshÃ­ chÇ”lÇ gÇnjuÃ©, jÃ¬yÃ¬ hÃ© tuÃ­lÇ de tiÇozhÃ n. XiÃ nyÇ’u de MLLMs yÃ³uyÃº qÃ­ xÃ¹liÃ¨ dÃ o xÃ¹liÃ¨ de jiÃ gÃ²u, wÃºfÇ tÃ³ngshÃ­ chÇ”lÇ shÅ«rÃ¹ hÃ© shÄ“ngchÃ©ng xiÇngyÃ¬ng. WÃ¨ile jiÄ›juÃ© zhÃ¨gÃ¨ wÃ¨ntÃ­, gÇi xiÃ ngmÃ¹ yÇnrÃ¹ le fÄ“nlÃ­ de gÇnjuÃ©, tuÃ­lÇ hÃ© jÃ¬yÃ¬ jÄ«zhÃ¬, shÇ qÃ­ nÃ©nggÃ²u shÃ­shÃ­ chÇ”lÇ shÃ¬pÃ­n hÃ© yÄ«npÃ­n shÅ«rÃ¹. TÃ­chÅ« de kuÃ ngjiÃ  IXC2.5-OL bÄokuÃ² shÃ­shÃ­ chÇ”lÇ duÅ mÃ³shuÃ i xÃ¬nxÄ« de gÇnjuÃ© mÃ³kuÃ i, zhÄ›nghÃ© duÇnqÄ« hÃ© chÃ¡ngqÄ« jÃ¬yÃ¬ de jÃ¬yÃ¬ mÃ³kuÃ i yÇji xiÇngyÃ¬ng chÃ¡xÃºn hÃ© zhÃ­xÃ­ng tuÃ­lÇ rÃ¨nwÃ¹ de tuÃ­lÇ mÃ³kuÃ i.
[14.12.2024 12:41] Mistral request. Model: mistral-large-latest. Prompt: Write vocab of difficult words for this text as an array of objects with fields 'word', 'pinyin', 'trans'. Return as python list without formatting. Return list and nothing else. Text:

è¿™ç¯‡æ–‡ç« è®¨è®ºäº†åˆ›å»ºèƒ½å¤Ÿé•¿æ—¶é—´ä¸ç¯å¢ƒäº’åŠ¨çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„ç ”ç©¶ç›®æ ‡ã€‚è¿‘æœŸå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¼€æ”¾ä¸–ç•Œç†è§£æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ä»é¢ä¸´åŒæ—¶å¤„ç†æ„ŸçŸ¥ã€è®°å¿†å’Œæ¨ç†çš„æŒ‘æˆ˜ã€‚ç°æœ‰çš„MLLMsç”±äºå…¶åºåˆ—åˆ°åºåˆ—çš„æ¶æ„ï¼Œæ— æ³•åŒæ—¶å¤„ç†è¾“å…¥å’Œç”Ÿæˆå“åº”ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œè¯¥é¡¹ç›®å¼•å…¥äº†åˆ†ç¦»çš„æ„ŸçŸ¥ã€æ¨ç†å’Œè®°å¿†æœºåˆ¶ï¼Œä½¿å…¶èƒ½å¤Ÿå®æ—¶å¤„ç†è§†é¢‘å’ŒéŸ³é¢‘è¾“å…¥ã€‚æå‡ºçš„æ¡†æ¶IXC2.5-OLåŒ…æ‹¬å®æ—¶å¤„ç†å¤šæ¨¡æ€ä¿¡æ¯çš„æ„ŸçŸ¥æ¨¡å—ã€æ•´åˆçŸ­æœŸå’Œé•¿æœŸè®°å¿†çš„è®°å¿†æ¨¡å—ä»¥åŠå“åº”æŸ¥è¯¢å’Œæ‰§è¡Œæ¨ç†ä»»åŠ¡çš„æ¨ç†æ¨¡å—ã€‚
[14.12.2024 12:42] Mistral response. {"id": "41183af61ff34661b5b701355b9752d4", "object": "chat.completion", "created": 1734180119, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "[{'word': '\u8ba8\u8bba', 'pinyin': 't\u01ceo l\u00f9n', 'trans': 'discuss'},\n{'word': '\u521b\u5efa', 'pinyin': 'chu\u00e0ng ji\u00e0n', 'trans': 'create'},\n{'word': '\u957f\u65f6\u95f4', 'pinyin': 'ch\u00e1ng sh\u00ed ji\u0101n', 'trans': 'long period of time'},\n{'word': '\u4e92\u52a8', 'pinyin': 'h\u00f9 d\u00f2ng', 'trans': 'interact'},\n{'word': '\u4eba\u5de5\u667a\u80fd', 'pinyin': 'r\u00e9n g\u014dng zh\u00ec n\u00e9ng', 'trans': 'artificial intelligence'},\n{'word': '\u7cfb\u7edf', 'pinyin': 'x\u00ec t\u01d2ng', 'trans': 'system'},\n{'word': '\u7814\u7a76', 'pinyin': 'y\u00e1n ji\u016b', 'trans': 'research'},\n{'word': '\u76ee\u6807', 'pinyin': 'm\u00f9 bi\u0101o', 'trans': 'goal'},\n{'word': '\u591a\u6a21\u6001', 'pinyin': 'du\u014d m\u00f3 t\u00e0i', 'trans': 'multimodal'},\n{'word': '\u5927\u8bed\u8a00\u6a21\u578b', 'pinyin': 'd\u00e0 y\u01d4 y\u00e1n m\u00f3 x\u00edng', 'trans': 'large language model'},\n{'word': '\u5f00\u653e', 'pinyin': 'k\u0101i f\u00e0ng', 'trans': 'open'},\n{'word': '\u4e16\u754c', 'pinyin': 'sh\u00ec ji\u00e8', 'trans': 'world'},\n{'word': '\u7406\u89e3', 'pinyin': 'l\u01d0 ji\u011b', 'trans': 'understand'},\n{'word': '\u65b9\u9762', 'pinyin': 'f\u0101ng mi\u00e0n', 'trans': 'aspect'},\n{'word': '\u53d6\u5f97', 'pinyin': 'q\u01d4 d\u00e9', 'trans': 'achieve'},\n{'word': '\u663e\u8457', 'pinyin': 'xi\u01cen zh\u00f9', 'trans': 'significant'},\n{'word': '\u8fdb\u5c55', 'pinyin': 'j\u00ecn zh\u01cen', 'trans': 'progress'},\n{'word': '\u9762\u4e34', 'pinyin': 'mi\u00e0n l\u00edn', 'trans': 'face'},\n{'word': '\u540c\u65f6', 'pinyin': 't\u00f3ng sh\u00ed', 'trans': 'simultaneously'},\n{'word': '\u5904\u7406', 'pinyin': 'ch\u01d4 l\u01d0', 'trans': 'process'},\n{'word': '\u611f\u77e5', 'pinyin': 'g\u01cen zh\u012b', 'trans': 'perception'},\n{'word': '\u8bb0\u5fc6', 'pinyin': 'j\u00ec y\u00ec', 'trans': 'memory'},\n{'word': '\u63a8\u7406', 'pinyin': 'tu\u012b l\u01d0', 'trans': 'reasoning'},\n{'word': '\u6311\u6218', 'pinyin': 'ti\u01ceo zh\u00e0n', 'trans': 'challenge'},\n{'word': '\u73b0\u6709', 'pinyin': 'xi\u00e0n y\u01d2u', 'trans': 'existing'},\n{'word': '\u7531\u4e8e', 'pinyin': 'y\u00f3u y\u00fa', 'trans': 'due to'},\n{'word': '\u5e8f\u5217', 'pinyin': 'x\u00f9 li\u00e8', 'trans': 'sequence'},\n{'word': '\u67b6\u6784', 'pinyin': 'ji\u00e0 g\u00f2u', 'trans': 'architecture'},\n{'word': '\u65e0\u6cd5', 'pinyin': 'w\u00fa f\u01ce', 'trans': 'unable'},\n{'word': '\u8f93\u5165', 'pinyin': 'sh\u016b r\u00f9', 'trans': 'input'},\n{'word': '\u751f\u6210', 'pinyin': 'sh\u0113ng ch\u00e9ng', 'trans': 'generate'},\n{'word': '\u54cd\u5e94', 'pinyin': 'xi\u01ceng y\u00ecng', 'trans': 'response'},\n{'word': '\u89e3\u51b3', 'pinyin': 'ji\u011b ju\u00e9', 'trans': 'solve'},\n{'word': '\u95ee\u9898', 'pinyin': 'w\u00e8n t\u00ed', 'trans': 'problem'},\n{'word': '\u9879\u76ee', 'pinyin': 'xi\u00e0ng m\u00f9', 'trans': 'project'},\n{'word': '\u5f15\u5165', 'pinyin': 'y\u01d0n r\u00f9', 'trans': 'introduce'},\n{'word': '\u5206\u79bb', 'pinyin': 'f\u0113n l\u00ed', 'trans': 'separate'},\n{'word': '\u673a\u5236', 'pinyin': 'j\u012b zh\u00ec', 'trans': 'mechanism'},\n{'word': '\u5b9e\u65f6', 'pinyin': 'sh\u00ed sh\u00ed', 'trans': 'real-time'},\n{'word': '\u89c6\u9891', 'pinyin': 'sh\u00ec p\u00edn', 'trans': 'video'},\n{'word': '\u97f3\u9891', 'pinyin': 'y\u012bn p\u00edn', 'trans': 'audio'},\n{'word': '\u63d0\u51fa', 'pinyin': 't\u00ed ch\u016b', 'trans': 'propose'},\n{'word': '\u6846\u67b6', 'pinyin': 'ku\u00e0ng ji\u00e0', 'trans': 'framework'},\n{'word': '\u6574\u5408', 'pinyin': 'zh\u011bng h\u00e9', 'trans': 'integrate'},\n{'word': '\u77ed\u671f', 'pinyin': 'du\u01cen q\u012b', 'trans': 'short-term'},\n{'word': '\u957f\u671f', 'pinyin': 'ch\u00e1ng q\u012b', 'trans': 'long-term'},\n{'word': '\u67e5\u8be2', 'pinyin': 'ch\u00e1 x\u00fan', 'trans': 'query'},\n{'word': '\u6267\u884c', 'pinyin': 'zh\u00ed x\u00edng', 'trans': 'execute'},\n{'word': '\u4efb\u52a1', 'pinyin': 'r\u00e8n w\u00f9', 'trans': 'task'}]"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 285, "total_tokens": 1629, "completion_tokens": 1344}}
[14.12.2024 12:42] Response: [{'word': 'è®¨è®º', 'pinyin': 'tÇo lÃ¹n', 'trans': 'discuss'},
{'word': 'åˆ›å»º', 'pinyin': 'chuÃ ng jiÃ n', 'trans': 'create'},
{'word': 'é•¿æ—¶é—´', 'pinyin': 'chÃ¡ng shÃ­ jiÄn', 'trans': 'long period of time'},
{'word': 'äº’åŠ¨', 'pinyin': 'hÃ¹ dÃ²ng', 'trans': 'interact'},
{'word': 'äººå·¥æ™ºèƒ½', 'pinyin': 'rÃ©n gÅng zhÃ¬ nÃ©ng', 'trans': 'artificial intelligence'},
{'word': 'ç³»ç»Ÿ', 'pinyin': 'xÃ¬ tÇ’ng', 'trans': 'system'},
{'word': 'ç ”ç©¶', 'pinyin': 'yÃ¡n jiÅ«', 'trans': 'research'},
{'word': 'ç›®æ ‡', 'pinyin': 'mÃ¹ biÄo', 'trans': 'goal'},
{'word': 'å¤šæ¨¡æ€', 'pinyin': 'duÅ mÃ³ tÃ i', 'trans': 'multimodal'},
{'word': 'å¤§è¯­è¨€æ¨¡å‹', 'pinyin': 'dÃ  yÇ” yÃ¡n mÃ³ xÃ­ng', 'trans': 'large language model'},
{'word': 'å¼€æ”¾', 'pinyin': 'kÄi fÃ ng', 'trans': 'open'},
{'word': 'ä¸–ç•Œ', 'pinyin': 'shÃ¬ jiÃ¨', 'trans': 'world'},
{'word': 'ç†è§£', 'pinyin': 'lÇ jiÄ›', 'trans': 'understand'},
{'word': 'æ–¹é¢', 'pinyin': 'fÄng miÃ n', 'trans': 'aspect'},
{'word': 'å–å¾—', 'pinyin': 'qÇ” dÃ©', 'trans': 'achieve'},
{'word': 'æ˜¾è‘—', 'pinyin': 'xiÇn zhÃ¹', 'trans': 'significant'},
{'word': 'è¿›å±•', 'pinyin': 'jÃ¬n zhÇn', 'trans': 'progress'},
{'word': 'é¢ä¸´', 'pinyin': 'miÃ n lÃ­n', 'trans': 'face'},
{'word': 'åŒæ—¶', 'pinyin': 'tÃ³ng shÃ­', 'trans': 'simultaneously'},
{'word': 'å¤„ç†', 'pinyin': 'chÇ” lÇ', 'trans': 'process'},
{'word': 'æ„ŸçŸ¥', 'pinyin': 'gÇn zhÄ«', 'trans': 'perception'},
{'word': 'è®°å¿†', 'pinyin': 'jÃ¬ yÃ¬', 'trans': 'memory'},
{'word': 'æ¨ç†', 'pinyin': 'tuÄ« lÇ', 'trans': 'reasoning'},
{'word': 'æŒ‘æˆ˜', 'pinyin': 'tiÇo zhÃ n', 'trans': 'challenge'},
{'word': 'ç°æœ‰', 'pinyin': 'xiÃ n yÇ’u', 'trans': 'existing'},
{'word': 'ç”±äº', 'pinyin': 'yÃ³u yÃº', 'trans': 'due to'},
{'word': 'åºåˆ—', 'pinyin': 'xÃ¹ liÃ¨', 'trans': 'sequence'},
{'word': 'æ¶æ„', 'pinyin': 'jiÃ  gÃ²u', 'trans': 'architecture'},
{'word': 'æ— æ³•', 'pinyin': 'wÃº fÇ', 'trans': 'unable'},
{'word': 'è¾“å…¥', 'pinyin': 'shÅ« rÃ¹', 'trans': 'input'},
{'word': 'ç”Ÿæˆ', 'pinyin': 'shÄ“ng chÃ©ng', 'trans': 'generate'},
{'word': 'å“åº”', 'pinyin': 'xiÇng yÃ¬ng', 'trans': 'response'},
{'word': 'è§£å†³', 'pinyin': 'jiÄ› juÃ©', 'trans': 'solve'},
{'word': 'é—®é¢˜', 'pinyin': 'wÃ¨n tÃ­', 'trans': 'problem'},
{'word': 'é¡¹ç›®', 'pinyin': 'xiÃ ng mÃ¹', 'trans': 'project'},
{'word': 'å¼•å…¥', 'pinyin': 'yÇn rÃ¹', 'trans': 'introduce'},
{'word': 'åˆ†ç¦»', 'pinyin': 'fÄ“n lÃ­', 'trans': 'separate'},
{'word': 'æœºåˆ¶', 'pinyin': 'jÄ« zhÃ¬', 'trans': 'mechanism'},
{'word': 'å®æ—¶', 'pinyin': 'shÃ­ shÃ­', 'trans': 'real-time'},
{'word': 'è§†é¢‘', 'pinyin': 'shÃ¬ pÃ­n', 'trans': 'video'},
{'word': 'éŸ³é¢‘', 'pinyin': 'yÄ«n pÃ­n', 'trans': 'audio'},
{'word': 'æå‡º', 'pinyin': 'tÃ­ chÅ«', 'trans': 'propose'},
{'word': 'æ¡†æ¶', 'pinyin': 'kuÃ ng jiÃ ', 'trans': 'framework'},
{'word': 'æ•´åˆ', 'pinyin': 'zhÄ›ng hÃ©', 'trans': 'integrate'},
{'word': 'çŸ­æœŸ', 'pinyin': 'duÇn qÄ«', 'trans': 'short-term'},
{'word': 'é•¿æœŸ', 'pinyin': 'chÃ¡ng qÄ«', 'trans': 'long-term'},
{'word': 'æŸ¥è¯¢', 'pinyin': 'chÃ¡ xÃºn', 'trans': 'query'},
{'word': 'æ‰§è¡Œ', 'pinyin': 'zhÃ­ xÃ­ng', 'trans': 'execute'},
{'word': 'ä»»åŠ¡', 'pinyin': 'rÃ¨n wÃ¹', 'trans': 'task'}]
[14.12.2024 12:42] Mistral request. Model: mistral-large-latest. Prompt: Translate this text in English. Text:

è¿™ç¯‡æ–‡ç« è®¨è®ºäº†åˆ›å»ºèƒ½å¤Ÿé•¿æ—¶é—´ä¸ç¯å¢ƒäº’åŠ¨çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„ç ”ç©¶ç›®æ ‡ã€‚è¿‘æœŸå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¼€æ”¾ä¸–ç•Œç†è§£æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ä»é¢ä¸´åŒæ—¶å¤„ç†æ„ŸçŸ¥ã€è®°å¿†å’Œæ¨ç†çš„æŒ‘æˆ˜ã€‚ç°æœ‰çš„MLLMsç”±äºå…¶åºåˆ—åˆ°åºåˆ—çš„æ¶æ„ï¼Œæ— æ³•åŒæ—¶å¤„ç†è¾“å…¥å’Œç”Ÿæˆå“åº”ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œè¯¥é¡¹ç›®å¼•å…¥äº†åˆ†ç¦»çš„æ„ŸçŸ¥ã€æ¨ç†å’Œè®°å¿†æœºåˆ¶ï¼Œä½¿å…¶èƒ½å¤Ÿå®æ—¶å¤„ç†è§†é¢‘å’ŒéŸ³é¢‘è¾“å…¥ã€‚æå‡ºçš„æ¡†æ¶IXC2.5-OLåŒ…æ‹¬å®æ—¶å¤„ç†å¤šæ¨¡æ€ä¿¡æ¯çš„æ„ŸçŸ¥æ¨¡å—ã€æ•´åˆçŸ­æœŸå’Œé•¿æœŸè®°å¿†çš„è®°å¿†æ¨¡å—ä»¥åŠå“åº”æŸ¥è¯¢å’Œæ‰§è¡Œæ¨ç†ä»»åŠ¡çš„æ¨ç†æ¨¡å—ã€‚
[14.12.2024 12:42] Mistral response. {"id": "842489b3b60946eabb8d8eedb2e08cbe", "object": "chat.completion", "created": 1734180137, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "This article discusses the research goal of creating artificial intelligence systems capable of long-term interaction with the environment. Recent multimodal large language models (MLLMs) have made significant progress in understanding the open world but still face challenges in simultaneously handling perception, memory, and reasoning. Existing MLLMs, due to their sequence-to-sequence architecture, cannot process inputs and generate responses concurrently. To address this issue, the project introduces separate mechanisms for perception, reasoning, and memory, enabling real-time processing of video and audio inputs. The proposed framework, IXC2.5-OL, includes a perception module for real-time processing of multimodal information, a memory module for integrating short-term and long-term memory, and a reasoning module for responding to queries and performing reasoning tasks."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 251, "total_tokens": 421, "completion_tokens": 170}}
[14.12.2024 12:42] Response: This article discusses the research goal of creating artificial intelligence systems capable of long-term interaction with the environment. Recent multimodal large language models (MLLMs) have made significant progress in understanding the open world but still face challenges in simultaneously handling perception, memory, and reasoning. Existing MLLMs, due to their sequence-to-sequence architecture, cannot process inputs and generate responses concurrently. To address this issue, the project introduces separate mechanisms for perception, reasoning, and memory, enabling real-time processing of video and audio inputs. The proposed framework, IXC2.5-OL, includes a perception module for real-time processing of multimodal information, a memory module for integrating short-term and long-term memory, and a reasoning module for responding to queries and performing reasoning tasks.
[14.12.2024 12:42] Renaming data file.
[14.12.2024 12:42] Renaming previous data. hf_papers.json to ./d/2024-12-13.json
[14.12.2024 12:42] Saving new data file.
[14.12.2024 12:42] Generating page.
[14.12.2024 12:42] Renaming previous page.
[14.12.2024 12:42] Renaming previous data. index.html to ./d/2024-12-13.html
[14.12.2024 12:42] [Experimental] Generating Chinese page for reading.
[14.12.2024 12:42] Chinese vocab [{'word': 'è®¨è®º', 'pinyin': 'tÇo lÃ¹n', 'trans': 'discuss'}, {'word': 'åˆ›å»º', 'pinyin': 'chuÃ ng jiÃ n', 'trans': 'create'}, {'word': 'é•¿æ—¶é—´', 'pinyin': 'chÃ¡ng shÃ­ jiÄn', 'trans': 'long period of time'}, {'word': 'äº’åŠ¨', 'pinyin': 'hÃ¹ dÃ²ng', 'trans': 'interact'}, {'word': 'äººå·¥æ™ºèƒ½', 'pinyin': 'rÃ©n gÅng zhÃ¬ nÃ©ng', 'trans': 'artificial intelligence'}, {'word': 'ç³»ç»Ÿ', 'pinyin': 'xÃ¬ tÇ’ng', 'trans': 'system'}, {'word': 'ç ”ç©¶', 'pinyin': 'yÃ¡n jiÅ«', 'trans': 'research'}, {'word': 'ç›®æ ‡', 'pinyin': 'mÃ¹ biÄo', 'trans': 'goal'}, {'word': 'å¤šæ¨¡æ€', 'pinyin': 'duÅ mÃ³ tÃ i', 'trans': 'multimodal'}, {'word': 'å¤§è¯­è¨€æ¨¡å‹', 'pinyin': 'dÃ  yÇ” yÃ¡n mÃ³ xÃ­ng', 'trans': 'large language model'}, {'word': 'å¼€æ”¾', 'pinyin': 'kÄi fÃ ng', 'trans': 'open'}, {'word': 'ä¸–ç•Œ', 'pinyin': 'shÃ¬ jiÃ¨', 'trans': 'world'}, {'word': 'ç†è§£', 'pinyin': 'lÇ jiÄ›', 'trans': 'understand'}, {'word': 'æ–¹é¢', 'pinyin': 'fÄng miÃ n', 'trans': 'aspect'}, {'word': 'å–å¾—', 'pinyin': 'qÇ” dÃ©', 'trans': 'achieve'}, {'word': 'æ˜¾è‘—', 'pinyin': 'xiÇn zhÃ¹', 'trans': 'significant'}, {'word': 'è¿›å±•', 'pinyin': 'jÃ¬n zhÇn', 'trans': 'progress'}, {'word': 'é¢ä¸´', 'pinyin': 'miÃ n lÃ­n', 'trans': 'face'}, {'word': 'åŒæ—¶', 'pinyin': 'tÃ³ng shÃ­', 'trans': 'simultaneously'}, {'word': 'å¤„ç†', 'pinyin': 'chÇ” lÇ', 'trans': 'process'}, {'word': 'æ„ŸçŸ¥', 'pinyin': 'gÇn zhÄ«', 'trans': 'perception'}, {'word': 'è®°å¿†', 'pinyin': 'jÃ¬ yÃ¬', 'trans': 'memory'}, {'word': 'æ¨ç†', 'pinyin': 'tuÄ« lÇ', 'trans': 'reasoning'}, {'word': 'æŒ‘æˆ˜', 'pinyin': 'tiÇo zhÃ n', 'trans': 'challenge'}, {'word': 'ç°æœ‰', 'pinyin': 'xiÃ n yÇ’u', 'trans': 'existing'}, {'word': 'ç”±äº', 'pinyin': 'yÃ³u yÃº', 'trans': 'due to'}, {'word': 'åºåˆ—', 'pinyin': 'xÃ¹ liÃ¨', 'trans': 'sequence'}, {'word': 'æ¶æ„', 'pinyin': 'jiÃ  gÃ²u', 'trans': 'architecture'}, {'word': 'æ— æ³•', 'pinyin': 'wÃº fÇ', 'trans': 'unable'}, {'word': 'è¾“å…¥', 'pinyin': 'shÅ« rÃ¹', 'trans': 'input'}, {'word': 'ç”Ÿæˆ', 'pinyin': 'shÄ“ng chÃ©ng', 'trans': 'generate'}, {'word': 'å“åº”', 'pinyin': 'xiÇng yÃ¬ng', 'trans': 'response'}, {'word': 'è§£å†³', 'pinyin': 'jiÄ› juÃ©', 'trans': 'solve'}, {'word': 'é—®é¢˜', 'pinyin': 'wÃ¨n tÃ­', 'trans': 'problem'}, {'word': 'é¡¹ç›®', 'pinyin': 'xiÃ ng mÃ¹', 'trans': 'project'}, {'word': 'å¼•å…¥', 'pinyin': 'yÇn rÃ¹', 'trans': 'introduce'}, {'word': 'åˆ†ç¦»', 'pinyin': 'fÄ“n lÃ­', 'trans': 'separate'}, {'word': 'æœºåˆ¶', 'pinyin': 'jÄ« zhÃ¬', 'trans': 'mechanism'}, {'word': 'å®æ—¶', 'pinyin': 'shÃ­ shÃ­', 'trans': 'real-time'}, {'word': 'è§†é¢‘', 'pinyin': 'shÃ¬ pÃ­n', 'trans': 'video'}, {'word': 'éŸ³é¢‘', 'pinyin': 'yÄ«n pÃ­n', 'trans': 'audio'}, {'word': 'æå‡º', 'pinyin': 'tÃ­ chÅ«', 'trans': 'propose'}, {'word': 'æ¡†æ¶', 'pinyin': 'kuÃ ng jiÃ ', 'trans': 'framework'}, {'word': 'æ•´åˆ', 'pinyin': 'zhÄ›ng hÃ©', 'trans': 'integrate'}, {'word': 'çŸ­æœŸ', 'pinyin': 'duÇn qÄ«', 'trans': 'short-term'}, {'word': 'é•¿æœŸ', 'pinyin': 'chÃ¡ng qÄ«', 'trans': 'long-term'}, {'word': 'æŸ¥è¯¢', 'pinyin': 'chÃ¡ xÃºn', 'trans': 'query'}, {'word': 'æ‰§è¡Œ', 'pinyin': 'zhÃ­ xÃ­ng', 'trans': 'execute'}, {'word': 'ä»»åŠ¡', 'pinyin': 'rÃ¨n wÃ¹', 'trans': 'task'}]
[14.12.2024 12:42] Renaming previous Chinese page.
[14.12.2024 12:42] Renaming previous data. zh.html to ./d/2024-12-13_zh_reading_task.html
[14.12.2024 12:42] Writing Chinese reading task.
[14.12.2024 12:42] Writing result.
[14.12.2024 12:42] Renaming log file.
[14.12.2024 12:42] Renaming previous data. log.txt to ./logs/2024-12-14_last_log.txt
