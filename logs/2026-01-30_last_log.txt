[30.01.2026 12:58] Read previous papers.
[30.01.2026 12:58] Generating top page (month).
[30.01.2026 12:58] Writing top page (month).
[30.01.2026 13:59] Read previous papers.
[30.01.2026 13:59] Get feed.
[30.01.2026 13:59] Get page data from previous paper. URL: https://huggingface.co/papers/2601.20354
[30.01.2026 13:59] Get page data from previous paper. URL: https://huggingface.co/papers/2601.20833
[30.01.2026 13:59] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21204
[30.01.2026 13:59] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21639
[30.01.2026 13:59] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22153
[30.01.2026 13:59] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21821
[30.01.2026 13:59] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21420
[30.01.2026 13:59] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22046
[30.01.2026 13:59] Get page data from previous paper. URL: https://huggingface.co/papers/2601.20730
[30.01.2026 13:59] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21337
[30.01.2026 13:59] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22154
[30.01.2026 13:59] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21754
[30.01.2026 13:59] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22157
[30.01.2026 13:59] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21590
[30.01.2026 13:59] Get page data from previous paper. URL: https://huggingface.co/papers/2601.18129
[30.01.2026 13:59] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21181
[30.01.2026 13:59] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22069
[30.01.2026 13:59] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21051
[30.01.2026 13:59] Get page data from previous paper. URL: https://huggingface.co/papers/2601.20975
[30.01.2026 13:59] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22054
[30.01.2026 13:59] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21996
[30.01.2026 13:59] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21598
[30.01.2026 13:59] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21343
[30.01.2026 13:59] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22158
[30.01.2026 13:59] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22156
[30.01.2026 13:59] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22146
[30.01.2026 13:59] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21406
[30.01.2026 13:59] Get page data from previous paper. URL: https://huggingface.co/papers/2601.19001
[30.01.2026 13:59] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22101
[30.01.2026 13:59] Get page data from previous paper. URL: https://huggingface.co/papers/2601.20465
[30.01.2026 13:59] Get page data from previous paper. URL: https://huggingface.co/papers/2601.17690
[30.01.2026 13:59] Get page data from previous paper. URL: https://huggingface.co/papers/2601.11747
[30.01.2026 13:59] Extract page data from URL. URL: https://huggingface.co/papers/2601.22143
[30.01.2026 13:59] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21872
[30.01.2026 13:59] Extract page data from URL. URL: https://huggingface.co/papers/2601.21579
[30.01.2026 13:59] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21416
[30.01.2026 13:59] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21282
[30.01.2026 13:59] Get page data from previous paper. URL: https://huggingface.co/papers/2601.20381
[30.01.2026 13:59] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[30.01.2026 13:59] No deleted papers detected.
[30.01.2026 13:59] Downloading and parsing papers (pdf, html). Total: 38.
[30.01.2026 13:59] Downloading and parsing paper https://huggingface.co/papers/2601.20354.
[30.01.2026 13:59] Extra JSON file exists (./assets/json/2601.20354.json), skip PDF parsing.
[30.01.2026 13:59] Paper image links file exists (./assets/img_data/2601.20354.json), skip HTML parsing.
[30.01.2026 13:59] Success.
[30.01.2026 13:59] Downloading and parsing paper https://huggingface.co/papers/2601.20833.
[30.01.2026 13:59] Extra JSON file exists (./assets/json/2601.20833.json), skip PDF parsing.
[30.01.2026 13:59] Paper image links file exists (./assets/img_data/2601.20833.json), skip HTML parsing.
[30.01.2026 13:59] Success.
[30.01.2026 13:59] Downloading and parsing paper https://huggingface.co/papers/2601.21204.
[30.01.2026 13:59] Extra JSON file exists (./assets/json/2601.21204.json), skip PDF parsing.
[30.01.2026 13:59] Paper image links file exists (./assets/img_data/2601.21204.json), skip HTML parsing.
[30.01.2026 13:59] Success.
[30.01.2026 13:59] Downloading and parsing paper https://huggingface.co/papers/2601.21639.
[30.01.2026 13:59] Extra JSON file exists (./assets/json/2601.21639.json), skip PDF parsing.
[30.01.2026 13:59] Paper image links file exists (./assets/img_data/2601.21639.json), skip HTML parsing.
[30.01.2026 13:59] Success.
[30.01.2026 13:59] Downloading and parsing paper https://huggingface.co/papers/2601.22153.
[30.01.2026 13:59] Extra JSON file exists (./assets/json/2601.22153.json), skip PDF parsing.
[30.01.2026 13:59] Paper image links file exists (./assets/img_data/2601.22153.json), skip HTML parsing.
[30.01.2026 13:59] Success.
[30.01.2026 13:59] Downloading and parsing paper https://huggingface.co/papers/2601.21821.
[30.01.2026 13:59] Extra JSON file exists (./assets/json/2601.21821.json), skip PDF parsing.
[30.01.2026 13:59] Paper image links file exists (./assets/img_data/2601.21821.json), skip HTML parsing.
[30.01.2026 13:59] Success.
[30.01.2026 13:59] Downloading and parsing paper https://huggingface.co/papers/2601.21420.
[30.01.2026 13:59] Extra JSON file exists (./assets/json/2601.21420.json), skip PDF parsing.
[30.01.2026 13:59] Paper image links file exists (./assets/img_data/2601.21420.json), skip HTML parsing.
[30.01.2026 13:59] Success.
[30.01.2026 13:59] Downloading and parsing paper https://huggingface.co/papers/2601.22046.
[30.01.2026 13:59] Extra JSON file exists (./assets/json/2601.22046.json), skip PDF parsing.
[30.01.2026 13:59] Paper image links file exists (./assets/img_data/2601.22046.json), skip HTML parsing.
[30.01.2026 13:59] Success.
[30.01.2026 13:59] Downloading and parsing paper https://huggingface.co/papers/2601.20730.
[30.01.2026 13:59] Extra JSON file exists (./assets/json/2601.20730.json), skip PDF parsing.
[30.01.2026 13:59] Paper image links file exists (./assets/img_data/2601.20730.json), skip HTML parsing.
[30.01.2026 13:59] Success.
[30.01.2026 13:59] Downloading and parsing paper https://huggingface.co/papers/2601.21337.
[30.01.2026 13:59] Extra JSON file exists (./assets/json/2601.21337.json), skip PDF parsing.
[30.01.2026 13:59] Paper image links file exists (./assets/img_data/2601.21337.json), skip HTML parsing.
[30.01.2026 13:59] Success.
[30.01.2026 13:59] Downloading and parsing paper https://huggingface.co/papers/2601.22154.
[30.01.2026 13:59] Extra JSON file exists (./assets/json/2601.22154.json), skip PDF parsing.
[30.01.2026 13:59] Paper image links file exists (./assets/img_data/2601.22154.json), skip HTML parsing.
[30.01.2026 13:59] Success.
[30.01.2026 13:59] Downloading and parsing paper https://huggingface.co/papers/2601.21754.
[30.01.2026 13:59] Extra JSON file exists (./assets/json/2601.21754.json), skip PDF parsing.
[30.01.2026 13:59] Paper image links file exists (./assets/img_data/2601.21754.json), skip HTML parsing.
[30.01.2026 13:59] Success.
[30.01.2026 13:59] Downloading and parsing paper https://huggingface.co/papers/2601.22157.
[30.01.2026 13:59] Extra JSON file exists (./assets/json/2601.22157.json), skip PDF parsing.
[30.01.2026 13:59] Paper image links file exists (./assets/img_data/2601.22157.json), skip HTML parsing.
[30.01.2026 13:59] Success.
[30.01.2026 13:59] Downloading and parsing paper https://huggingface.co/papers/2601.21590.
[30.01.2026 13:59] Extra JSON file exists (./assets/json/2601.21590.json), skip PDF parsing.
[30.01.2026 13:59] Paper image links file exists (./assets/img_data/2601.21590.json), skip HTML parsing.
[30.01.2026 13:59] Success.
[30.01.2026 13:59] Downloading and parsing paper https://huggingface.co/papers/2601.18129.
[30.01.2026 13:59] Extra JSON file exists (./assets/json/2601.18129.json), skip PDF parsing.
[30.01.2026 13:59] Paper image links file exists (./assets/img_data/2601.18129.json), skip HTML parsing.
[30.01.2026 13:59] Success.
[30.01.2026 13:59] Downloading and parsing paper https://huggingface.co/papers/2601.21181.
[30.01.2026 13:59] Extra JSON file exists (./assets/json/2601.21181.json), skip PDF parsing.
[30.01.2026 13:59] Paper image links file exists (./assets/img_data/2601.21181.json), skip HTML parsing.
[30.01.2026 13:59] Success.
[30.01.2026 13:59] Downloading and parsing paper https://huggingface.co/papers/2601.22069.
[30.01.2026 13:59] Extra JSON file exists (./assets/json/2601.22069.json), skip PDF parsing.
[30.01.2026 13:59] Paper image links file exists (./assets/img_data/2601.22069.json), skip HTML parsing.
[30.01.2026 13:59] Success.
[30.01.2026 13:59] Downloading and parsing paper https://huggingface.co/papers/2601.21051.
[30.01.2026 13:59] Extra JSON file exists (./assets/json/2601.21051.json), skip PDF parsing.
[30.01.2026 13:59] Paper image links file exists (./assets/img_data/2601.21051.json), skip HTML parsing.
[30.01.2026 13:59] Success.
[30.01.2026 13:59] Downloading and parsing paper https://huggingface.co/papers/2601.20975.
[30.01.2026 13:59] Extra JSON file exists (./assets/json/2601.20975.json), skip PDF parsing.
[30.01.2026 13:59] Paper image links file exists (./assets/img_data/2601.20975.json), skip HTML parsing.
[30.01.2026 13:59] Success.
[30.01.2026 13:59] Downloading and parsing paper https://huggingface.co/papers/2601.22054.
[30.01.2026 13:59] Extra JSON file exists (./assets/json/2601.22054.json), skip PDF parsing.
[30.01.2026 13:59] Paper image links file exists (./assets/img_data/2601.22054.json), skip HTML parsing.
[30.01.2026 13:59] Success.
[30.01.2026 13:59] Downloading and parsing paper https://huggingface.co/papers/2601.21996.
[30.01.2026 13:59] Extra JSON file exists (./assets/json/2601.21996.json), skip PDF parsing.
[30.01.2026 13:59] Paper image links file exists (./assets/img_data/2601.21996.json), skip HTML parsing.
[30.01.2026 13:59] Success.
[30.01.2026 13:59] Downloading and parsing paper https://huggingface.co/papers/2601.21598.
[30.01.2026 13:59] Extra JSON file exists (./assets/json/2601.21598.json), skip PDF parsing.
[30.01.2026 13:59] Paper image links file exists (./assets/img_data/2601.21598.json), skip HTML parsing.
[30.01.2026 13:59] Success.
[30.01.2026 13:59] Downloading and parsing paper https://huggingface.co/papers/2601.21343.
[30.01.2026 13:59] Extra JSON file exists (./assets/json/2601.21343.json), skip PDF parsing.
[30.01.2026 13:59] Paper image links file exists (./assets/img_data/2601.21343.json), skip HTML parsing.
[30.01.2026 13:59] Success.
[30.01.2026 13:59] Downloading and parsing paper https://huggingface.co/papers/2601.22158.
[30.01.2026 13:59] Extra JSON file exists (./assets/json/2601.22158.json), skip PDF parsing.
[30.01.2026 13:59] Paper image links file exists (./assets/img_data/2601.22158.json), skip HTML parsing.
[30.01.2026 13:59] Success.
[30.01.2026 13:59] Downloading and parsing paper https://huggingface.co/papers/2601.22156.
[30.01.2026 13:59] Extra JSON file exists (./assets/json/2601.22156.json), skip PDF parsing.
[30.01.2026 13:59] Paper image links file exists (./assets/img_data/2601.22156.json), skip HTML parsing.
[30.01.2026 13:59] Success.
[30.01.2026 13:59] Downloading and parsing paper https://huggingface.co/papers/2601.22146.
[30.01.2026 13:59] Extra JSON file exists (./assets/json/2601.22146.json), skip PDF parsing.
[30.01.2026 13:59] Paper image links file exists (./assets/img_data/2601.22146.json), skip HTML parsing.
[30.01.2026 13:59] Success.
[30.01.2026 13:59] Downloading and parsing paper https://huggingface.co/papers/2601.21406.
[30.01.2026 13:59] Extra JSON file exists (./assets/json/2601.21406.json), skip PDF parsing.
[30.01.2026 13:59] Paper image links file exists (./assets/img_data/2601.21406.json), skip HTML parsing.
[30.01.2026 13:59] Success.
[30.01.2026 13:59] Downloading and parsing paper https://huggingface.co/papers/2601.19001.
[30.01.2026 13:59] Extra JSON file exists (./assets/json/2601.19001.json), skip PDF parsing.
[30.01.2026 13:59] Paper image links file exists (./assets/img_data/2601.19001.json), skip HTML parsing.
[30.01.2026 13:59] Success.
[30.01.2026 13:59] Downloading and parsing paper https://huggingface.co/papers/2601.22101.
[30.01.2026 13:59] Extra JSON file exists (./assets/json/2601.22101.json), skip PDF parsing.
[30.01.2026 13:59] Paper image links file exists (./assets/img_data/2601.22101.json), skip HTML parsing.
[30.01.2026 13:59] Success.
[30.01.2026 13:59] Downloading and parsing paper https://huggingface.co/papers/2601.20465.
[30.01.2026 13:59] Extra JSON file exists (./assets/json/2601.20465.json), skip PDF parsing.
[30.01.2026 13:59] Paper image links file exists (./assets/img_data/2601.20465.json), skip HTML parsing.
[30.01.2026 13:59] Success.
[30.01.2026 13:59] Downloading and parsing paper https://huggingface.co/papers/2601.17690.
[30.01.2026 13:59] Extra JSON file exists (./assets/json/2601.17690.json), skip PDF parsing.
[30.01.2026 13:59] Paper image links file exists (./assets/img_data/2601.17690.json), skip HTML parsing.
[30.01.2026 13:59] Success.
[30.01.2026 13:59] Downloading and parsing paper https://huggingface.co/papers/2601.11747.
[30.01.2026 13:59] Extra JSON file exists (./assets/json/2601.11747.json), skip PDF parsing.
[30.01.2026 13:59] Paper image links file exists (./assets/img_data/2601.11747.json), skip HTML parsing.
[30.01.2026 13:59] Success.
[30.01.2026 13:59] Downloading and parsing paper https://huggingface.co/papers/2601.22143.
[30.01.2026 13:59] Downloading paper 2601.22143 from https://arxiv.org/pdf/2601.22143v1...
[30.01.2026 13:59] Extracting affiliations from text.
[30.01.2026 13:59] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"JUST-DUB-IT : Video Dubbing via Joint Audio-Visual Diffusion ANTHONY CHEN, Tel Aviv University, Israel and Lightricks, Israel NAOMI KEN KOREM, Lightricks, Israel TAVI HALPERIN, Lightricks, Israel MATAN BEN YOSEF, Lightricks, Israel URSKA JELERCIC, Lightricks, Israel OFIR BIBI, Lightricks, Israel OR PATASHNIK, Tel Aviv University, Israel DANIEL COHEN-OR, Tel Aviv University, Israel 6 2 0 2 9 2 ] . [ 1 3 4 1 2 2 . 1 0 6 2 : r Audio-Visual Foundation Models, which are pretrained to jointly generate sound and visual content, have recently shown an unprecedented ability to model multi-modal generation and editing, opening new opportunities for downstream tasks. Among these tasks, video dubbing could greatly benefit from such priors, yet most existing solutions still rely on complex, task-specific pipelines that struggle in real-world settings. In this work, we introduce single-model approach that adapts foundational audio-video diffusion model for video-to-video dubbing via lightweight LoRA. The LoRA enables the model to condition on an input audio-video while jointly generating translated audio and synchronized facial motion. To train this LoRA, we leverage the generative model itself to synthesize paired multilingual videos of the same speaker. Specifically, we generate multilingual videos with language switches within single clip, and then inpaint the face and audio in each half to match the language of the other half. By leveraging the rich generative prior of the audio-visual model, our approach preserves speaker identity and lip synchronization while remaining robust to complex motion and real-world dynamics. We demonstrate that our approach produces high-quality dubbed videos with improved visual fidelity, lip synchronization, and robustness compared to existing dubbing pipelines. Webpage available at https://justdubit.github.io. Additional Key Words and Phrases: Audio-Visual generation, Video dubbing Recent years have seen significant progress in generative model"
[30.01.2026 13:59] Response: ```python
[
    "Tel Aviv University",
    "Lightricks"
]
```
[30.01.2026 13:59] Deleting PDF ./assets/pdf/2601.22143.pdf.
[30.01.2026 13:59] Success.
[30.01.2026 13:59] Downloading and parsing paper https://huggingface.co/papers/2601.21872.
[30.01.2026 13:59] Extra JSON file exists (./assets/json/2601.21872.json), skip PDF parsing.
[30.01.2026 13:59] Paper image links file exists (./assets/img_data/2601.21872.json), skip HTML parsing.
[30.01.2026 13:59] Success.
[30.01.2026 13:59] Downloading and parsing paper https://huggingface.co/papers/2601.21579.
[30.01.2026 13:59] Downloading paper 2601.21579 from https://arxiv.org/pdf/2601.21579v1...
[30.01.2026 14:00] Extracting affiliations from text.
[30.01.2026 14:00] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"KromHC: Manifold-Constrained Hyper-Connections with Kronecker-Product Residual Matrices Wuyang Zhou 1 Yuxuan Gu 1 Giorgos Iacovides 1 Danilo Mandic "
[30.01.2026 14:00] Response: ```python
[]
```
[30.01.2026 14:00] Extracting affiliations from text.
[30.01.2026 14:00] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"KromHC: Manifold-Constrained Hyper-Connections with Kronecker-Product Residual Matrices Wuyang Zhou 1 Yuxuan Gu 1 Giorgos Iacovides 1 Danilo MandicThe success of Hyper-Connections (HC) in neural networks (NN) has also highlighted issues related to its training instability and restricted scalability. The Manifold-Constrained HyperConnections (mHC) mitigate these challenges by projecting the residual connection space onto Birkhoff polytope, however, it faces two issues: 1) its iterative Sinkhorn-Knopp (SK) algorithm does not always yield exact doubly stochastic residual matrices; 2) mHC incurs prohibitive O(n3C) parameter complexity with as the width of the residual stream and as the feature dimension. The recently proposed mHClite reparametrizes the residual matrix via the Birkhoff-von-Neumann theorem to guarantee double stochasticity, but also faces factorial explosion in its parameter complexity, (nC n!). To address both challenges, we propose KromHC, which uses the Kronecker products of smaller doubly stochastic matrices to parametrize the residual matrix in mHC. By enforcing manifold constraints across the factor residual matrices along each mode of the tensorized residual stream, KromHC guarantees exact double stochasticity of the residual matrices while reducing parameter complexity to O(n2C). Comprehensive experiments demonstrate that KromHC matches or even outperforms state-of-the-art (SOTA) mHC variants, while requiring significantly fewer trainable parameters. The code is available at https: //github.com/wz1119/KromHC. 6 2 0 2 9 2 ] . [ 1 9 7 5 1 2 . 1 0 6 2 : r 1. Introduction Hyper-Connections (HC) (Zhu et al., 2025) have emerged as powerful alternative to the ubiquitous residual connections (He et al., 2016). This is achieved by expanding the residual 1Department of Electrical and Electronic Engineering, Imperial College London, London, United Kingdom. Preprint. January 30, 2026. Table 1. Comparisons between SOTA mHC variants. Our proposed KromHC is the only method that simultaneously achieves exact doubly stochastic residual matrices, parameter efficiency, and requires no specialized kernel optimization. Criterion mHC mHC-lite Doubly Stochastic Parameter Efficient PyTorch Native . . KromHC (Ours) stream width to enhance topological complexity. Unlike the standard residual mapping xl+1 = xl + F(xl) (He et al., 2016), HC increases the stream width by an expansion rate, n, without incurring computational overhead regarding FLOPs. By introducing learnable mixing across the multiple residual streams, HC allows for more expressive feature propagation. More specifically, single layer of HC is defined as Xl+1 = Hres Xl + Hpost (cid:0)Hpre Xl (cid:1) , (1) R1n, and Hpost where Xl RnC and Xl+1 RnC are the expanded Rnn, input and output at the l-th HC layer; Hres Hpre R1n are learnable mappings that, respectively, mix the residual streams, aggregate features from streams into one, and map the layer output back onto the streams; F() is learned residual function such as the attention mechanism (Vaswani et al., 2017). }L Recent work (Xie et al., 2025) has suggested that the unconstrained residual matrices ({Hres l=1) in HC can lead to numerical instabilities when training large-scale neural networks (NNs) such as large language models (LLMs). In particular, HC cannot preserve the identity mapping property of the standard residual connections (He et al., 2016) when stacked across multiple layers, as (cid:81)Ll Li fails to preserve the global mean of the features in i=1 Hres (cid:32)Ll (cid:89) (cid:33) Hres Li Xl XL = i=1 L1 (cid:88) + L1i (cid:89) Hpost Hres Lj F(Hpre Xi) , (2) i=l j=1 1 KromHC: Manifold-Constrained Hyper-Connections with Kronecker-Product Residual Matrices Figure 1. Illustration of variants of manifold-constrained hyper-connections with residual stream width = 8. (a) mHC: utilizes iterative Sinkhorn-Knopp (SK) algorithm to approximate doubly stochastic residual matrix; (b) mHC-lite: builds the residual matrix as convex combinations of n! permutation matrices, but becomes infeasible for large n; (c) KromHC (Ours): constructs the residual matrix as the Kronecker products of smaller (e.g., 2 2) doubly stochastic matrices, thus guaranteeing double stochasticity while remaining parameter efficient. where and represent deeper and shallower layer, respectively (Xie et al., 2025). To address the training instability issue of HC, authors in Xie et al. (2025) have proposed the Manifold-Constrained Hyper-Connections, which applies the Sinkhorn-Knopp algorithm (Sinkhorn & Knopp, 1967) to iteratively project the residual matrices, {Hres l=1, onto the Birkhoff polytope (i.e. the set of doubly stochastic matrices). Since the sum of the individual rows and columns of doubly stochastic matrices is always equal to 1, the residual mixing mapping, Hres Xl, becomes convex combination of the input features, preserving feature mean across layers and regularizing the norm of the residual matrices. }L However, the Sinkhorn-Knopp algorithm (Sinkhorn & Knopp, 1967) in mHC can fail to achieve double stochasticity when employed over finite number of iterations (e.g., 20 iterations in mHC). This leads to error accumulation across layers and undermines training stability (Xie et al., 2025; Yang & Gao, 2026) (See Figure 2). To this end, Yang & Gao (2026) proposed mHC-lite, which guarantees exact double stochasticity by using the Birkhoff-von-Neumann theorem (Birkhoff, 1946) to parametrize the residual matrices as the convex combinations of permutation matrices. Despite achieving exact doubly stochastic residual matrices, mHC-lite suffers from an explosion in parameter complexity, as it requires n! unique permutation matrices of size nn to be stored. Furthermore, the generic mHC (Xie et al., 2025) has parameter complexity of O(n3C), thus preventing effective scaling of the residual stream width (See Figure 3). Therefore, the following question naturally arises: Can we achieve exact double stochasticity of the residual matrices without incurring an explosion in parameter count as the width of the residual stream, n, increases? 2 KromHC: Manifold-Constrained Hyper-Connections with Kronecker-Product Residual Matrices In summary, the contributions of this paper are as follows: Based on the Kronecker product, we propose KromHC, novel manifold-constrained hyper-connection framework, providing link to tensorized residual mixing. We resolve the conflict between exact double stochasticity and parameter"
[30.01.2026 14:00] Mistral response. {"id": "c6e3c9fda2c14aaf871fb79b0b005bdb", "created": 1769781607, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1661, "total_tokens": 1683, "completion_tokens": 22, "num_cached_tokens": 1660}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\"Department of Electrical and Electronic Engineering, Imperial College London, London, United Kingdom\"]\n```"}}]}
[30.01.2026 14:00] Response: ```python
["Department of Electrical and Electronic Engineering, Imperial College London, London, United Kingdom"]
```
[30.01.2026 14:00] Deleting PDF ./assets/pdf/2601.21579.pdf.
[30.01.2026 14:00] Success.
[30.01.2026 14:00] Downloading and parsing paper https://huggingface.co/papers/2601.21416.
[30.01.2026 14:00] Extra JSON file exists (./assets/json/2601.21416.json), skip PDF parsing.
[30.01.2026 14:00] Paper image links file exists (./assets/img_data/2601.21416.json), skip HTML parsing.
[30.01.2026 14:00] Success.
[30.01.2026 14:00] Downloading and parsing paper https://huggingface.co/papers/2601.21282.
[30.01.2026 14:00] Extra JSON file exists (./assets/json/2601.21282.json), skip PDF parsing.
[30.01.2026 14:00] Paper image links file exists (./assets/img_data/2601.21282.json), skip HTML parsing.
[30.01.2026 14:00] Success.
[30.01.2026 14:00] Downloading and parsing paper https://huggingface.co/papers/2601.20381.
[30.01.2026 14:00] Extra JSON file exists (./assets/json/2601.20381.json), skip PDF parsing.
[30.01.2026 14:00] Paper image links file exists (./assets/img_data/2601.20381.json), skip HTML parsing.
[30.01.2026 14:00] Success.
[30.01.2026 14:00] Enriching papers with extra data.
[30.01.2026 14:00] ********************************************************************************
[30.01.2026 14:00] Abstract 0. A new benchmark and dataset are introduced to evaluate and improve spatial reasoning capabilities in text-to-image models through information-dense prompts and fine-tuning.  					AI-generated summary 				 Text-to-image (T2I) models have achieved remarkable success in generating high-fidelity images,...
[30.01.2026 14:00] ********************************************************************************
[30.01.2026 14:00] Abstract 1. Offline knowledge construction through structured methodological graphs enables more reliable and scalable autonomous scientific discovery by reducing reliance on real-time literature processing.  					AI-generated summary 				 Autonomous scientific discovery with large language model (LLM)-based ag...
[30.01.2026 14:00] ********************************************************************************
[30.01.2026 14:00] Abstract 2. Embedding scaling offers superior sparsity scaling compared to expert scaling in large language models, enabling efficient inference through system optimizations and speculative decoding.  					AI-generated summary 				 While Mixture-of-Experts (MoE) architectures have become the standard for sparsi...
[30.01.2026 14:00] ********************************************************************************
[30.01.2026 14:00] Abstract 3. OCRVerse is a novel end-to-end OCR method that unifies text-centric and vision-centric approaches through comprehensive data engineering and a two-stage SFT-RL training framework with domain-specific reward strategies.  					AI-generated summary 				 The development of large vision language models d...
[30.01.2026 14:00] ********************************************************************************
[30.01.2026 14:00] Abstract 4. DynamicVLA addresses dynamic object manipulation challenges through a compact vision-language-action model with temporal reasoning and closed-loop adaptation, supported by a new benchmark for dynamic manipulation tasks.  					AI-generated summary 				 Manipulating dynamic objects remains an open cha...
[30.01.2026 14:00] ********************************************************************************
[30.01.2026 14:00] Abstract 5. A large-scale multimodal reasoning dataset called MMFineReason is introduced to improve vision language models' performance through high-quality reasoning annotations and demonstrates superior parameter efficiency in fine-tuned models.  					AI-generated summary 				 Recent advances in Vision Langua...
[30.01.2026 14:00] ********************************************************************************
[30.01.2026 14:00] Abstract 6. ConceptMoE dynamically allocates computation by merging similar tokens into concept representations, improving both performance and efficiency in large language models through adaptive processing and reduced attention computation.  					AI-generated summary 				 Large language models allocate unifor...
[30.01.2026 14:00] ********************************************************************************
[30.01.2026 14:00] Abstract 7. PLANING presents an efficient streaming reconstruction framework that combines explicit geometric primitives with neural Gaussians to achieve high-quality rendering and accurate geometry simultaneously through decoupled optimization.  					AI-generated summary 				 Streaming reconstruction from mono...
[30.01.2026 14:00] ********************************************************************************
[30.01.2026 14:00] Abstract 8. AgentLongBench evaluates large language models as autonomous agents through dynamic environment interactions, revealing challenges in handling high-information-density tool responses compared to memory fragmentation in long conversations.  					AI-generated summary 				 The evolution of Large Langua...
[30.01.2026 14:00] ********************************************************************************
[30.01.2026 14:00] Abstract 9. The Qwen3-ASR family introduces speech recognition models with language identification capabilities and a non-autoregressive forced alignment model, achieving state-of-the-art performance and efficient processing.  					AI-generated summary 				 In this report, we introduce Qwen3-ASR family, which i...
[30.01.2026 14:00] ********************************************************************************
[30.01.2026 14:00] Abstract 10. Agent-RRM, a multi-faceted reward model, provides structured feedback for agentic trajectories through reasoning traces, critiques, and performance scores, with unified feedback integration showing superior performance across diverse benchmarks.  					AI-generated summary 				 Agentic Reinforcement ...
[30.01.2026 14:00] ********************************************************************************
[30.01.2026 14:00] Abstract 11. A novel framework called SCOUT is introduced that uses lightweight scouts to reduce exploration costs for large language models in nonlinguistic environments, enabling improved performance through supervised fine-tuning and reinforcement learning.  					AI-generated summary 				 While Large Language...
[30.01.2026 14:00] ********************************************************************************
[30.01.2026 14:00] Abstract 12. Hidden superior models exist in public repositories but are overlooked due to inefficient discovery methods; a multi-armed bandit approach using shared query sets and aggressive elimination significantly accelerates identification of top-performing models.  					AI-generated summary 				 Public repo...
[30.01.2026 14:00] ********************************************************************************
[30.01.2026 14:00] Abstract 13. A theoretically grounded method for improving large language model reasoning performance through distribution sharpening without iterative sampling or external rewards, achieving comparable results to reinforcement learning post-training with significantly reduced computational costs.  					AI-gener...
[30.01.2026 14:00] ********************************************************************************
[30.01.2026 14:00] Abstract 14. A minimal post-training approach using supervised fine-tuning, on-policy distillation, and small-scale reinforcement fine-tuning enables the development of high-quality sovereign language models with reduced resource requirements.  					AI-generated summary 				 Large language models (LLMs) have pro...
[30.01.2026 14:00] ********************************************************************************
[30.01.2026 14:00] Abstract 15. Multimodal Large Language Models suffer from cross-modal hallucinations where one modality incorrectly influences generation from another, leading to fabricated outputs; this exposes a fundamental deficiency in modality-interaction control. To address this, a training-free method called Modality-Ada...
[30.01.2026 14:00] ********************************************************************************
[30.01.2026 14:00] Abstract 16. VTC-R1 enables efficient long-context reasoning by compressing textual traces into compact images and iteratively feeding them back into vision-language models as optical memory, achieving significant speedup without sacrificing performance.  					AI-generated summary 				 Long-context reasoning has...
[30.01.2026 14:00] ********************************************************************************
[30.01.2026 14:00] Abstract 17. A two-stage trained cybersecurity reasoning model achieves competitive performance on specialized tasks while maintaining general capabilities through supervised fine-tuning and reinforcement learning from verifiable rewards.  					AI-generated summary 				 We present Foundation-Sec-8B-Reasoning, th...
[30.01.2026 14:00] ********************************************************************************
[30.01.2026 14:00] Abstract 18. DeepSearchQA presents a 900-prompt benchmark evaluating agents on complex multi-step information-seeking tasks requiring systematic information collation, deduplication, and reasoning about stopping criteria across 17 fields.  					AI-generated summary 				 We introduce DeepSearchQA, a 900-prompt be...
[30.01.2026 14:00] ********************************************************************************
[30.01.2026 14:00] Abstract 19. Metric Anything presents a scalable pretraining framework for metric depth estimation that leverages diverse 3D data and sparse metric prompts to achieve superior performance across multiple vision tasks.  					AI-generated summary 				 Scaling has powered recent advances in vision foundation models...
[30.01.2026 14:00] ********************************************************************************
[30.01.2026 14:00] Abstract 20. Mechnistic Data Attribution framework traces interpretable units to specific training samples using influence functions, demonstrating causal relationships between data structure and neural circuit formation in language models.  					AI-generated summary 				 While Mechanistic Interpretability has i...
[30.01.2026 14:00] ********************************************************************************
[30.01.2026 14:00] Abstract 21. Active latent planning method improves reasoning accuracy and efficiency by modeling latent token supervision as conditional VAE and using reinforcement learning with coherence rewards.  					AI-generated summary 				 Aiming at efficient and dense chain-of-thought (CoT) reasoning, latent reasoning m...
[30.01.2026 14:00] ********************************************************************************
[30.01.2026 14:00] Abstract 22. A reinforcement learning-based pretraining method improves language model safety, factuality, and quality by evaluating generations through a combination of model rollouts, original suffixes, and rewritten suffixes.  					AI-generated summary 				 Ensuring safety, factuality and overall quality in t...
[30.01.2026 14:00] ********************************************************************************
[30.01.2026 14:00] Abstract 23. Pixel MeanFlow introduces a one-step latent-free image generation method by separating network output space from loss space, achieving strong performance on ImageNet at multiple resolutions.  					AI-generated summary 				 Modern diffusion/flow-based models for image generation typically exhibit two...
[30.01.2026 14:00] ********************************************************************************
[30.01.2026 14:00] Abstract 24. HALO enables efficient conversion of Transformer models to RNN-attention hybrid architectures with improved long-context performance using minimal training data.  					AI-generated summary 				 Hybrid Transformer architectures, which combine softmax attention blocks and recurrent neural networks (RN...
[30.01.2026 14:00] ********************************************************************************
[30.01.2026 14:00] Abstract 25. Large language models can be pre-trained from scratch using synthetic instruction-response pairs generated from unstructured text corpora, outperforming traditional methods on benchmarks measuring response quality.  					AI-generated summary 				 Due to limited supervised training data, large langua...
[30.01.2026 14:00] ********************************************************************************
[30.01.2026 14:00] Abstract 26. UniMRG enhances unified multimodal models by training them to generate multiple visual representations, improving both understanding and generation capabilities through complementary information capture.  					AI-generated summary 				 Unified Multimodal Models (UMMs) integrate both visual understan...
[30.01.2026 14:00] ********************************************************************************
[30.01.2026 14:00] Abstract 27. FROST is an attention-aware method that improves reasoning efficiency by pruning uncritical paths and removing reasoning outliers, leading to reduced token usage and improved accuracy.  					AI-generated summary 				 We propose FROST, an attention-aware method for efficient reasoning. Unlike traditi...
[30.01.2026 14:00] ********************************************************************************
[30.01.2026 14:00] Abstract 28. Error-compensating optimizer eliminates memory overhead from master weights in quantized LLM training while maintaining near-lossless accuracy.  					AI-generated summary 				 Quantization has significantly improved the compute and memory efficiency of Large Language Model (LLM) training. However, e...
[30.01.2026 14:00] ********************************************************************************
[30.01.2026 14:00] Abstract 29. BMAM presents a brain-inspired multi-agent memory architecture that decomposes memory into specialized subsystems to address long-term reasoning challenges in language-model-based agents.  					AI-generated summary 				 Language-model-based agents operating over extended interaction horizons face pe...
[30.01.2026 14:00] ********************************************************************************
[30.01.2026 14:00] Abstract 30. Neural audio fingerprinting performance varies with segment length, with short segments (0.5-second) generally providing better retrieval accuracy, and large language models showing promise in recommending optimal segment durations.  					AI-generated summary 				 Audio fingerprinting provides an id...
[30.01.2026 14:00] ********************************************************************************
[30.01.2026 14:00] Abstract 31. PRISM leverages design data to create a knowledge base for improving graphic designs based on natural language instructions, achieving superior style alignment compared to existing methods.  					AI-generated summary 				 Graphic design often involves exploring different stylistic directions, which ...
[30.01.2026 14:00] ********************************************************************************
[30.01.2026 14:00] Abstract 32. A lightweight LoRA adaptation of an audio-video diffusion model enables high-quality video dubbing with preserved speaker identity and improved lip synchronization through synthetic multilingual video training.  					AI-generated summary 				 Audio-Visual Foundation Models, which are pretrained to j...
[30.01.2026 14:00] ********************************************************************************
[30.01.2026 14:00] Abstract 33. WebArbiter introduces a reasoning-first WebPRM that formulates reward modeling as text generation to improve web navigation through structured justifications and preference verdicts, outperforming existing baselines in complex web environments.  					AI-generated summary 				 Web agents hold great p...
[30.01.2026 14:00] ********************************************************************************
[30.01.2026 14:00] Abstract 34. KromHC addresses training instability and scalability issues in hyper-connections by using Kronecker products to parametrize residual matrices with reduced parameter complexity.  					AI-generated summary 				 The success of Hyper-Connections (HC) in neural networks (NN) has also highlighted issues ...
[30.01.2026 14:00] ********************************************************************************
[30.01.2026 14:00] Abstract 35. Slot-Based Object-Centric Representations outperform global and dense feature representations in robotic manipulation tasks by providing better generalization under visual distribution shifts.  					AI-generated summary 				 The generalization capabilities of robotic manipulation policies are heavil...
[30.01.2026 14:00] ********************************************************************************
[30.01.2026 14:00] Abstract 36. WorldBench is introduced as a video-based benchmark for disentangled evaluation of physical reasoning in generative models, revealing specific failure patterns in current state-of-the-art video world models.  					AI-generated summary 				 Recent advances in generative foundational models, often ter...
[30.01.2026 14:00] ********************************************************************************
[30.01.2026 14:00] Abstract 37. STORM enhances robotic manipulation by adapting visual foundation models with semantic-aware slots through multi-phase training, improving generalization and control performance.  					AI-generated summary 				 Visual foundation models provide strong perceptual features for robotics, but their dense...
[30.01.2026 14:00] Read previous papers.
[30.01.2026 14:00] Generating reviews via LLM API.
[30.01.2026 14:00] Using data from previous issue: {"categories": ["#cv", "#benchmark", "#synthetic", "#reasoning", "#dataset", "#training", "#multimodal"], "emoji": "üß©", "ru": {"title": "–ü—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –≤ text-to-image –º–æ–¥–µ–ª—è—Ö —á–µ—Ä–µ–∑ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ SpatialGenEval –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å
[30.01.2026 14:00] Using data from previous issue: {"categories": ["#graphs", "#hallucinations", "#long_context", "#science"], "emoji": "üß¨", "ru": {"title": "–ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–µ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∑–Ω–∞–Ω–∏–π –¥–ª—è –Ω–∞–¥—ë–∂–Ω–æ–≥–æ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –Ω–∞—É—á–Ω–æ–≥–æ –æ—Ç–∫—Ä—ã—Ç–∏—è", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è Idea2Story ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –Ω–∞—É—á–Ω–æ–≥–æ –æ—Ç–∫—Ä—ã—Ç–∏—è, –∫–æ—Ç–æ—Ä—ã–π –ø–µ—Ä–µ–Ω–æ—Å–∏—Ç –æ–±—Ä–∞–±–æ
[30.01.2026 14:00] Using data from previous issue: {"categories": ["#architecture", "#inference", "#training", "#optimization", "#open_source"], "emoji": "‚ö°", "ru": {"title": "–≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –≤–º–µ—Å—Ç–æ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤: –Ω–æ–≤—ã–π –ø—É—Ç—å –∫ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–º—É –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—é", "desc": "–í —ç—Ç–æ–π —Ä–∞–±–æ—Ç–µ –∞–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∫–∞–∫ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—É —ç–∫—Å
[30.01.2026 14:00] Using data from previous issue: {"categories": ["#data", "#cv", "#open_source", "#dataset", "#science", "#training", "#optimization", "#rl", "#multimodal"], "emoji": "üìÑ", "ru": {"title": "–£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è–º–∏", "desc": "OCRVerse –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π 
[30.01.2026 14:00] Using data from previous issue: {"categories": ["#multimodal", "#dataset", "#benchmark", "#robotics", "#inference", "#architecture", "#transfer_learning", "#small_models", "#synthetic"], "emoji": "ü§ñ", "ru": {"title": "–ë—ã—Å—Ç—Ä–∞—è –∏ –∞–¥–∞–ø—Ç–∏–≤–Ω–∞—è –º–∞–Ω–∏–ø—É–ª—è—Ü–∏—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–º–∏ –æ–±—ä–µ–∫—Ç–∞–º–∏ —á–µ—Ä–µ–∑ –∫–æ–º–ø–∞–∫—Ç–Ω—ã–µ vision-language –º–æ–¥–µ–ª–∏", "desc": "Dynami
[30.01.2026 14:00] Using data from previous issue: {"categories": ["#dataset", "#reasoning", "#multimodal", "#open_source", "#training", "#small_models", "#synthetic"], "emoji": "üß†", "ru": {"title": "–í—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –¥–µ–ª–∞—é—Ç –º–∞–ª–µ–Ω—å–∫–∏–µ –º–æ–¥–µ–ª–∏ —É–º–Ω–µ–µ –±–æ–ª—å—à–∏—Ö", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ MMFineReason ‚Äî –±–æ–ª—å—à–æ–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –¥–∞—Ç
[30.01.2026 14:00] Using data from previous issue: {"categories": ["#inference", "#multimodal", "#architecture", "#training"], "emoji": "‚ö°", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–æ–∫–µ–Ω–æ–≤ —á–µ—Ä–µ–∑ –∫–æ–Ω—Ü–µ–ø—Ç—É–∞–ª—å–Ω–æ–µ —Å–ª–∏—è–Ω–∏–µ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π", "desc": "ConceptMoE ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö, –∫–æ
[30.01.2026 14:00] Using data from previous issue: {"categories": [], "emoji": "üé¨", "ru": {"title": "–†–∞–∑–¥–µ–ª—ë–Ω–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–µ–æ–º–µ—Ç—Ä–∏–∏ –∏ –≤–Ω–µ—à–Ω–µ–≥–æ –≤–∏–¥–∞ –¥–ª—è –±—ã—Å—Ç—Ä–æ–π —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ —Å—Ü–µ–Ω", "desc": "PLANING ‚Äî —ç—Ç–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –ø–æ—Ç–æ–∫–æ–≤–æ–π —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ —Å—Ü–µ–Ω –∏–∑ –º–æ–Ω–æ–∫—É–ª—è—Ä–Ω—ã—Ö –≤–∏–¥–µ–æ–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π, –∫–æ—Ç–æ—Ä–∞—è –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç —è–≤–Ω—ã–µ –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–∏—Ç–∏–≤—ã —Å –Ω
[30.01.2026 14:00] Using data from previous issue: {"categories": ["#benchmark", "#agents", "#long_context", "#dataset", "#reasoning"], "emoji": "üîÑ", "ru": {"title": "–ê–≥–µ–Ω—Ç—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM: –æ—Ç —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Å—Ç–æ–≤ –∫ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–º—É —Å–∏–Ω—Ç–µ–∑—É –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏", "desc": "AgentLongBench ‚Äî —ç—Ç–æ –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ —Ä–æ–ª–∏ –∞–≤—Ç–æ–Ω–æ–º–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ —á–µ
[30.01.2026 14:00] Using data from previous issue: {"categories": ["#benchmark", "#multilingual", "#inference", "#low_resource", "#audio", "#open_source"], "emoji": "üé§", "ru": {"title": "–ú–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏ —Å –Ω–µ–∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–º –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ–º", "desc": "–°–µ–º—å—è –º–æ–¥–µ–ª–µ–π Qwen3-ASR –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –Ω–∞–±–æ—Ä —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ—á
[30.01.2026 14:00] Using data from previous issue: {"categories": ["#dataset", "#reasoning", "#benchmark", "#open_source", "#rl", "#optimization", "#agents"], "emoji": "üéØ", "ru": {"title": "–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ–±—Ä–∞—Ç–Ω–∞—è —Å–≤—è–∑—å –¥–ª—è —É–º–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Agent-RRM, –º–Ω–æ–≥–æ–∞—Å–ø–µ–∫—Ç–Ω—É—é –º–æ–¥–µ–ª—å –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä–∞—è –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å
[30.01.2026 14:00] Using data from previous issue: {"categories": ["#reasoning", "#training", "#rl", "#optimization", "#transfer_learning", "#agents", "#small_models"], "emoji": "üîç", "ru": {"title": "–†–∞–∑–≤–µ–¥—á–∏–∫–∏ –¥–ª—è —Ä–∞–∑—É–º–Ω–æ–π —Ä–∞–∑–≤–µ–¥–∫–∏: –∫–∞–∫ –º–∞–ª–µ–Ω—å–∫–∏–µ –º–æ–¥–µ–ª–∏ —É—á–∞—Ç –±–æ–ª—å—à–∏—Ö", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ SCOUT, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ª—ë–≥–∫–∏—Ö –∞–≥–µ–Ω—Ç–æ–≤-
[30.01.2026 14:00] Using data from previous issue: {"categories": ["#small_models", "#benchmark", "#inference"], "emoji": "üíé", "ru": {"title": "–ü–æ–∏—Å–∫ —Å–∫—Ä—ã—Ç—ã—Ö –∂–µ–º—á—É–∂–∏–Ω: –±—ã—Å—Ç—Ä–æ–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –º–Ω–æ–≥–æ—Ä—É–∫–∏–π –±–∞–Ω–¥–∏—Ç", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø–æ–∫–∞–∑–∞–Ω–æ, —á—Ç–æ –≤ –ø—É–±–ª–∏—á–Ω—ã—Ö —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è—Ö –Ω–∞—Ö–æ–¥—è—Ç—Å—è —Å–∫—Ä—ã—Ç—ã–µ –≤—ã—Å–æ–∫–æ–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏, –∫–æ—Ç–æ—Ä—ã–µ –æ—Å—Ç
[30.01.2026 14:00] Using data from previous issue: {"categories": ["#rl", "#plp", "#reasoning", "#optimization", "#training", "#math"], "emoji": "üî•", "ru": {"title": "–ó–∞–æ—Å—Ç—Ä–µ–Ω–∏–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤–º–µ—Å—Ç–æ –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏—è: —ç–∫–æ–Ω–æ–º–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π LLM", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –∑–∞
[30.01.2026 14:00] Using data from previous issue: {"categories": ["#rlhf", "#reasoning", "#open_source", "#training", "#multilingual", "#optimization", "#low_resource", "#small_models"], "emoji": "üèõÔ∏è", "ru": {"title": "–°—É–≤–µ—Ä–µ–Ω–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –¥–æ–æ–±—É—á–µ–Ω–∏—é —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π,
[30.01.2026 14:00] Using data from previous issue: {"categories": ["#multimodal", "#video", "#audio", "#inference"], "emoji": "üéØ", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –ø–æ–¥–∞–≤–ª–µ–Ω–∏—è –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–í —Å—Ç–∞—Ç—å–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ –∫—Ä–æ—Å—Å-–º–æ–¥–∞–ª—å–Ω—ã—Ö –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö, –∫–æ–≥–¥–∞ 
[30.01.2026 14:00] Using data from previous issue: {"categories": ["#multimodal", "#reasoning", "#benchmark", "#open_source", "#training", "#long_context", "#inference"], "emoji": "üß†", "ru": {"title": "–û–ø—Ç–∏—á–µ—Å–∫–∞—è –ø–∞–º—è—Ç—å –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –≤—ã–≤–æ–¥–∞: —Å–∂–∞—Ç–∏–µ —á–µ—Ä–µ–∑ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é", "desc": "VTC-R1 ‚Äî –Ω–æ–≤–∞—è –ø–∞—Ä–∞–¥–∏–≥–º–∞ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä–∞
[30.01.2026 14:00] Using data from previous issue: {"categories": ["#rlhf", "#dataset", "#reasoning", "#benchmark", "#open_source", "#training", "#science", "#small_models"], "emoji": "üîê", "ru": {"title": "–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ –≤ –∫–æ–º–ø–∞–∫—Ç–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ: —Ä–∞—Å—Å—É–∂–¥–∞—é—â–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –∫–∏–±–µ—Ä–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ Foundation-Sec-8B-Reasoning ‚Äî
[30.01.2026 14:00] Using data from previous issue: {"categories": ["#dataset", "#agents", "#benchmark"], "emoji": "üîç", "ru": {"title": "–ì–ª—É–±–æ–∫–∏–π –ø–æ–∏—Å–∫ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏: –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –º–Ω–æ–≥–æ—à–∞–≥–æ–≤–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤", "desc": "–†–∞–±–æ—Ç–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –±–µ–Ω—á–º–∞—Ä–∫ DeepSearchQA —Å 900 –∑–∞–¥–∞—á–∞–º–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ —Å–ª–æ–∂–Ω—ã—Ö –º–Ω–æ–≥–æ—à–∞–≥–æ–≤—ã—Ö –∑–∞–¥–∞—á–∞—Ö –ø–æ–∏—Å–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏. –ë–µ–Ω
[30.01.2026 14:00] Using data from previous issue: {"categories": ["#3d", "#cv", "#architecture", "#transfer_learning", "#open_source", "#dataset", "#optimization", "#multimodal"], "emoji": "üìè", "ru": {"title": "–ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞–∫ –ø—É—Ç—å –∫ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–º—É –≤–æ—Å–ø—Ä–∏—è—Ç–∏—é –º–µ—Ç—Ä–∏—á–µ—Å–∫–æ–π –≥–ª—É–±–∏–Ω—ã", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Metric Anything ‚Äî –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—É—é —Å–∏
[30.01.2026 14:00] Using data from previous issue: {"categories": ["#synthetic", "#architecture", "#data", "#training", "#interpretability"], "emoji": "üîç", "ru": {"title": "–û—Ç –¥–∞–Ω–Ω—ã—Ö –∫ —Å—Ö–µ–º–∞–º: –ø—Ä–∏—á–∏–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏—Å—Ö–æ–∂–¥–µ–Ω–∏—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–∞—è –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è Mechanistic Data Attribution
[30.01.2026 14:00] Using data from previous issue: {"categories": ["#reasoning", "#training", "#rl", "#optimization", "#small_models"], "emoji": "üß†", "ru": {"title": "–ê–∫—Ç–∏–≤–Ω–æ–µ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ —Å–∫—Ä—ã—Ç–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –º–µ—Ç–æ–¥ –∞–∫—Ç–∏–≤–Ω–æ–≥–æ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –≤ —Å–∫—Ä—ã—Ç–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è —É–ª—É
[30.01.2026 14:00] Using data from previous issue: {"categories": ["#rl", "#training", "#rlhf"], "emoji": "üõ°Ô∏è", "ru": {"title": "–ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –∏ —Ñ–∞–∫–æ—Ç–∏—á–Ω–æ—Å—Ç—å –≤–æ –≤—Ä–µ–º—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —á–µ—Ä–µ–∑ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –æ–±—É—á–µ–Ω–∏–∏ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –∫–æ—Ç
[30.01.2026 14:00] Using data from previous issue: {"categories": ["#diffusion", "#cv", "#architecture", "#optimization", "#training"], "emoji": "‚ö°", "ru": {"title": "–û–¥–Ω–æ—à–∞–≥–æ–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –±–µ–∑ —Å–∫—Ä—ã—Ç–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ —á–µ—Ä–µ–∑ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –≤—ã—Ö–æ–¥–æ–≤ –∏ –ø–æ—Ç–µ—Ä—å", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –º–µ—Ç–æ–¥ Pixel MeanFlow –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∑–∞ –æ–¥–∏–Ω —à–∞–≥
[30.01.2026 14:00] Using data from previous issue: {"categories": ["#long_context", "#transfer_learning", "#architecture", "#optimization", "#training", "#inference"], "emoji": "‚ö°", "ru": {"title": "–ì–∏–±—Ä–∏–¥–Ω—ã–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã: —É—Å–∫–æ—Ä–µ–Ω–∏–µ –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ —á–µ—Ä–µ–∑ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—é –≤ RNN —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω HALO ‚Äî –º–µ—Ç–æ–¥ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –¥
[30.01.2026 14:00] Using data from previous issue: {"categories": ["#synthetic", "#open_source"], "emoji": "üîÑ", "ru": {"title": "–°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –≤–º–µ—Å—Ç–æ —Ç–µ–∫—Å—Ç–æ–≤: –Ω–æ–≤—ã–π –ø—É—Ç—å –∫ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–º—É –æ–±—É—á–µ–Ω–∏—é —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ —Å–æ–∑–¥–∞–Ω–∏—è –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –ø–∞—Ä –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è-–æ—Ç–≤–µ—Ç –∏–∑ –Ω–µ—Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∫–æ—Ä–ø—É
[30.01.2026 14:00] Using data from previous issue: {"categories": ["#multimodal", "#cv", "#training"], "emoji": "üîÑ", "ru": {"title": "–í–∑–∞–∏–º–Ω–æ–µ —É—Å–∏–ª–µ–Ω–∏–µ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è", "desc": "UniMRG ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥ –ø–æ—Å—Ç–æ–±—É—á–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –ø—É—Ç—ë–º –æ–±—É—á–µ–Ω–∏—è –∏—Ö –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤—Å
[30.01.2026 14:00] Using data from previous issue: {"categories": ["#optimization", "#open_source", "#reasoning"], "emoji": "‚úÇÔ∏è", "ru": {"title": "–£–º–Ω–æ–µ –æ–±—Ä–µ–∑–∞–Ω–∏–µ —Ü–µ–ø–æ—á–µ–∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —á–µ—Ä–µ–∑ –∞–Ω–∞–ª–∏–∑ –≤–Ω–∏–º–∞–Ω–∏—è", "desc": "FROST ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –∞–Ω–∞–ª–∏–∑–µ –≤–µ—Å–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è (attention weights) –≤ –Ω–µ–π—Ä–æ—Å–µ—Ç—è—Ö. –ü–æ–¥—Ö–æ–¥ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ 
[30.01.2026 14:00] Using data from previous issue: {"categories": ["#inference", "#optimization", "#training", "#architecture"], "emoji": "‚ö°", "ru": {"title": "–û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä —Å –∫–æ–º–ø–µ–Ω—Å–∞—Ü–∏–µ–π –æ—à–∏–±–æ–∫: –æ–±—É—á–µ–Ω–∏–µ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω—ã—Ö LLM –±–µ–∑ –ø–æ—Ç–µ—Ä—å –ø–∞–º—è—Ç–∏", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è Error-Compensating Optimizer (ECO) ‚Äî –Ω–æ–≤—ã–π –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä, –∫–æ—Ç–æ—Ä—ã–π —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É
[30.01.2026 14:00] Using data from previous issue: {"categories": ["#architecture", "#benchmark", "#reasoning", "#long_context", "#agents"], "emoji": "üß†", "ru": {"title": "–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –ø–∞–º—è—Ç—å –∫–∞–∫ –∫–ª—é—á –∫ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –ø–∞–º—è—Ç–∏ BMAM, –≤–¥–æ—Ö–Ω–æ–≤–ª–µ–Ω–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π –º–æ–∑–≥–∞, –∫–æ—Ç–æ—Ä–∞—è —Ä–∞–∑–¥–µ–ª—è–µ—Ç –ø–∞–º
[30.01.2026 14:00] Using data from previous issue: {"categories": ["#audio", "#architecture", "#benchmark"], "emoji": "üîç", "ru": {"title": "–û–ø—Ç–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Å–µ–≥–º–µ–Ω—Ç–∞: –∫–ª—é—á –∫ —Ç–æ—á–Ω–æ—Å—Ç–∏ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤—ã—Ö –∞—É–¥–∏–æ—Ñ–∏–Ω–≥–µ—Ä–ø—Ä–∏–Ω—Ç–æ–≤", "desc": "–í —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –≤–ª–∏—è–Ω–∏–µ –¥–ª–∏–Ω—ã –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –æ–∫–Ω–∞ –Ω–∞ –∫–∞—á–µ—Å—Ç–≤–æ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤—ã—Ö –∞—É–¥–∏–æ—Ñ–∏–Ω–≥–µ—Ä–ø—Ä–∏–Ω—Ç–æ–≤ ‚Äî –∫–æ–º–ø–∞–∫—Ç–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –∑–≤
[30.01.2026 14:00] Using data from previous issue: {"categories": [], "emoji": "üé®", "ru": {"title": "–ó–Ω–∞–Ω–∏—è –∏–∑ –¥–∞–Ω–Ω—ã—Ö –¥–∏–∑–∞–π–Ω–∞ –¥–ª—è —Å—Ç–∏–ª–∏—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ —É–ª—É—á—à–µ–Ω–∏—è –ø–æ —Ç–µ–∫—Å—Ç—É", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ PRISM –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏—Ö –¥–∏–∑–∞–π–Ω–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞, –∫–æ—Ç–æ—Ä—ã–π —Å–æ–∑–¥–∞—ë—Ç –±–∞–∑—É –∑–Ω–∞–Ω–∏–π –∏–∑ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∏–∑–∞–π–Ω-–¥–∞–Ω–Ω—ã—Ö. –ê–≤—Ç–æ—Ä—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ
[30.01.2026 14:00] Querying the API.
[30.01.2026 14:00] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A lightweight LoRA adaptation of an audio-video diffusion model enables high-quality video dubbing with preserved speaker identity and improved lip synchronization through synthetic multilingual video training.  					AI-generated summary 				 Audio-Visual Foundation Models, which are pretrained to jointly generate sound and visual content, have recently shown an unprecedented ability to model multi-modal generation and editing, opening new opportunities for downstream tasks. Among these tasks, video dubbing could greatly benefit from such priors, yet most existing solutions still rely on complex, task-specific pipelines that struggle in real-world settings. In this work, we introduce a single-model approach that adapts a foundational audio-video diffusion model for video-to-video dubbing via a lightweight LoRA. The LoRA enables the model to condition on an input audio-video while jointly generating translated audio and synchronized facial motion. To train this LoRA, we leverage the generative model itself to synthesize paired multilingual videos of the same speaker. Specifically, we generate multilingual videos with language switches within a single clip, and then inpaint the face and audio in each half to match the language of the other half. By leveraging the rich generative prior of the audio-visual model, our approach preserves speaker identity and lip synchronization while remaining robust to complex motion and real-world dynamics. We demonstrate that our approach produces high-quality dubbed videos with improved visual fidelity, lip synchronization, and robustness compared to existing dubbing pipelines.
[30.01.2026 14:00] Response: ```json
{
  "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –º–µ—Ç–æ–¥ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –∞—É–¥–∏–æ-–≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é lightweight LoRA –¥–ª—è –∑–∞–¥–∞—á–∏ –¥—É–±–ª—è–∂–∞ –≤–∏–¥–µ–æ. –ú–æ–¥–µ–ª—å –æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö –≤–∏–¥–µ–æ, –∫–æ—Ç–æ—Ä—ã–µ –≥–µ–Ω–µ—Ä–∏—Ä—É—é—Ç—Å—è —Å–∞–º–æ–π –º–æ–¥–µ–ª—å—é –ø—É—Ç—ë–º –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤ –∏ –∏–Ω–ø–µ–π–Ω—Ç–∏–Ω–≥–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö –æ–±–ª–∞—Å—Ç–µ–π –ª–∏—Ü–∞ –∏ –∑–≤—É–∫–∞. –ü–æ–¥—Ö–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç—å –≥–æ–≤–æ—Ä—è—â–µ–≥–æ –∏ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—é –≥—É–± –±–ª–∞–≥–æ–¥–∞—Ä—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –±–æ–≥–∞—Ç–æ–≥–æ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –ø—Ä–∏–æ—Ä–∞ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏. –ü–æ–ª—É—á–µ–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç —É–ª—É—á—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –¥—É–±–ª—è–∂–∞, —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏ –≥—É–± –∏ —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏ –∫ —Å–ª–æ–∂–Ω—ã–º –¥–≤–∏–∂–µ–Ω–∏—è–º –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏.",
  "emoji": "üé¨",
  "title": "–õ—ë–≥–∫–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è –∞—É–¥–∏–æ-–≤–∏–¥–µ–æ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–≥–æ –¥—É–±–ª—è–∂–∞"
}
```
[30.01.2026 14:00] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A lightweight LoRA adaptation of an audio-video diffusion model enables high-quality video dubbing with preserved speaker identity and improved lip synchronization through synthetic multilingual video training.  					AI-generated summary 				 Audio-Visual Foundation Models, which are pretrained to jointly generate sound and visual content, have recently shown an unprecedented ability to model multi-modal generation and editing, opening new opportunities for downstream tasks. Among these tasks, video dubbing could greatly benefit from such priors, yet most existing solutions still rely on complex, task-specific pipelines that struggle in real-world settings. In this work, we introduce a single-model approach that adapts a foundational audio-video diffusion model for video-to-video dubbing via a lightweight LoRA. The LoRA enables the model to condition on an input audio-video while jointly generating translated audio and synchronized facial motion. To train this LoRA, we leverage the generative model itself to synthesize paired multilingual videos of the same speaker. Specifically, we generate multilingual videos with language switches within a single clip, and then inpaint the face and audio in each half to match the language of the other half. By leveraging the rich generative prior of the audio-visual model, our approach preserves speaker identity and lip synchronization while remaining robust to complex motion and real-world dynamics. We demonstrate that our approach produces high-quality dubbed videos with improved visual fidelity, lip synchronization, and robustness compared to existing dubbing pipelines."

[30.01.2026 14:00] Response: ```python
['VIDEO', 'AUDIO', 'MULTIMODAL', 'TRAINING']
```
[30.01.2026 14:00] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A lightweight LoRA adaptation of an audio-video diffusion model enables high-quality video dubbing with preserved speaker identity and improved lip synchronization through synthetic multilingual video training.  					AI-generated summary 				 Audio-Visual Foundation Models, which are pretrained to jointly generate sound and visual content, have recently shown an unprecedented ability to model multi-modal generation and editing, opening new opportunities for downstream tasks. Among these tasks, video dubbing could greatly benefit from such priors, yet most existing solutions still rely on complex, task-specific pipelines that struggle in real-world settings. In this work, we introduce a single-model approach that adapts a foundational audio-video diffusion model for video-to-video dubbing via a lightweight LoRA. The LoRA enables the model to condition on an input audio-video while jointly generating translated audio and synchronized facial motion. To train this LoRA, we leverage the generative model itself to synthesize paired multilingual videos of the same speaker. Specifically, we generate multilingual videos with language switches within a single clip, and then inpaint the face and audio in each half to match the language of the other half. By leveraging the rich generative prior of the audio-visual model, our approach preserves speaker identity and lip synchronization while remaining robust to complex motion and real-world dynamics. We demonstrate that our approach produces high-quality dubbed videos with improved visual fidelity, lip synchronization, and robustness compared to existing dubbing pipelines."

[30.01.2026 14:00] Response: ```python
['DIFFUSION', 'TRANSLATION', 'SYNTHETIC']
```
[30.01.2026 14:00] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents a novel approach to video dubbing using a lightweight LoRA adaptation of an audio-video diffusion model. The method allows for high-quality dubbing while maintaining the speaker\'s identity and ensuring accurate lip synchronization. By synthesizing multilingual video training data, the model can generate translated audio and facial movements that align with the original video. The results show that this approach outperforms traditional dubbing methods in terms of visual quality and synchronization accuracy.","title":"Effortless Video Dubbing with Speaker Identity Preservation"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="This paper presents a novel approach to video dubbing using a lightweight LoRA adaptation of an audio-video diffusion model. The method allows for high-quality dubbing while maintaining the speaker's identity and ensuring accurate lip synchronization. By synthesizing multilingual video training data, the model can generate translated audio and facial movements that align with the original video. The results show that this approach outperforms traditional dubbing methods in terms of visual quality and synchronization accuracy.", title='Effortless Video Dubbing with Speaker Identity Preservation'))
[30.01.2026 14:00] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçËΩªÈáèÁ∫ßÁöÑLoRAÈÄÇÂ∫îÈü≥ËßÜÈ¢ëÊâ©Êï£Ê®°ÂûãÁöÑÊñπÊ≥ïÔºåÁî®‰∫éÈ´òË¥®ÈáèÁöÑËßÜÈ¢ëÈÖçÈü≥„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÂêàÊàêÂ§öËØ≠Ë®ÄËßÜÈ¢ëËÆ≠ÁªÉÔºåËÉΩÂ§ü‰øùÊåÅËØ¥ËØùËÄÖË∫´‰ªΩÂíåÊîπÂñÑÂò¥ÂîáÂêåÊ≠•„ÄÇÊàë‰ª¨Âà©Áî®ÁîüÊàêÊ®°ÂûãÂêàÊàêÈÖçÂØπÁöÑÂ§öËØ≠Ë®ÄËßÜÈ¢ëÔºåÂπ∂Âú®Âêå‰∏ÄÁâáÊÆµÂÜÖËøõË°åËØ≠Ë®ÄÂàáÊç¢Ôºå‰ª•ÂÆûÁé∞Èü≥È¢ëÂíåÈù¢ÈÉ®Âä®‰ΩúÁöÑÂêåÊ≠•ÁîüÊàê„ÄÇÂÆûÈ™åË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®ËßÜËßâ‰øùÁúüÂ∫¶„ÄÅÂò¥ÂîáÂêåÊ≠•ÂíåÂØπÂ§çÊùÇËøêÂä®ÁöÑÈ≤ÅÊ£íÊÄßÊñπÈù¢‰ºò‰∫éÁé∞ÊúâÁöÑÈÖçÈü≥ÊñπÊ°à„ÄÇ","title":"ËΩªÈáèÁ∫ßLoRAÂÆûÁé∞È´òË¥®ÈáèËßÜÈ¢ëÈÖçÈü≥"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçËΩªÈáèÁ∫ßÁöÑLoRAÈÄÇÂ∫îÈü≥ËßÜÈ¢ëÊâ©Êï£Ê®°ÂûãÁöÑÊñπÊ≥ïÔºåÁî®‰∫éÈ´òË¥®ÈáèÁöÑËßÜÈ¢ëÈÖçÈü≥„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÂêàÊàêÂ§öËØ≠Ë®ÄËßÜÈ¢ëËÆ≠ÁªÉÔºåËÉΩÂ§ü‰øùÊåÅËØ¥ËØùËÄÖË∫´‰ªΩÂíåÊîπÂñÑÂò¥ÂîáÂêåÊ≠•„ÄÇÊàë‰ª¨Âà©Áî®ÁîüÊàêÊ®°ÂûãÂêàÊàêÈÖçÂØπÁöÑÂ§öËØ≠Ë®ÄËßÜÈ¢ëÔºåÂπ∂Âú®Âêå‰∏ÄÁâáÊÆµÂÜÖËøõË°åËØ≠Ë®ÄÂàáÊç¢Ôºå‰ª•ÂÆûÁé∞Èü≥È¢ëÂíåÈù¢ÈÉ®Âä®‰ΩúÁöÑÂêåÊ≠•ÁîüÊàê„ÄÇÂÆûÈ™åË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®ËßÜËßâ‰øùÁúüÂ∫¶„ÄÅÂò¥ÂîáÂêåÊ≠•ÂíåÂØπÂ§çÊùÇËøêÂä®ÁöÑÈ≤ÅÊ£íÊÄßÊñπÈù¢‰ºò‰∫éÁé∞ÊúâÁöÑÈÖçÈü≥ÊñπÊ°à„ÄÇ', title='ËΩªÈáèÁ∫ßLoRAÂÆûÁé∞È´òË¥®ÈáèËßÜÈ¢ëÈÖçÈü≥'))
[30.01.2026 14:00] Using data from previous issue: {"categories": ["#dataset", "#reasoning", "#rlhf", "#benchmark", "#open_source", "#interpretability", "#agents"], "emoji": "üï∑Ô∏è", "ru": {"title": "–†–∞—Å—Å—É–∂–¥–∞—é—â–∞—è –º–æ–¥–µ–ª—å –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–π –¥–ª—è –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–π –≤–µ–±-–Ω–∞–≤–∏–≥–∞—Ü–∏–∏", "desc": "WebArbiter –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—é –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–π –¥–ª—è –≤
[30.01.2026 14:00] Querying the API.
[30.01.2026 14:00] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

KromHC addresses training instability and scalability issues in hyper-connections by using Kronecker products to parametrize residual matrices with reduced parameter complexity.  					AI-generated summary 				 The success of Hyper-Connections (HC) in neural networks (NN) has also highlighted issues related to its training instability and restricted scalability. The Manifold-Constrained Hyper-Connections (mHC) mitigate these challenges by projecting the residual connection space onto a Birkhoff polytope, however, it faces two issues: 1) its iterative Sinkhorn-Knopp (SK) algorithm does not always yield exact doubly stochastic residual matrices; 2) mHC incurs a prohibitive O(n^3C) parameter complexity with n as the width of the residual stream and C as the feature dimension. The recently proposed mHC-lite reparametrizes the residual matrix via the Birkhoff-von-Neumann theorem to guarantee double stochasticity, but also faces a factorial explosion in its parameter complexity, O left( nC cdot n! right). To address both challenges, we propose KromHC, which uses the Kronecker products of smaller doubly stochastic matrices to parametrize the residual matrix in mHC. By enforcing manifold constraints across the factor residual matrices along each mode of the tensorized residual stream, KromHC guarantees exact double stochasticity of the residual matrices while reducing parameter complexity to O(n^2C). Comprehensive experiments demonstrate that KromHC matches or even outperforms state-of-the-art (SOTA) mHC variants, while requiring significantly fewer trainable parameters. The code is available at https://github.com/wz1119/KromHC.
[30.01.2026 14:00] Response: ```json
{
  "desc": "–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ —É–ª—É—á—à–µ–Ω–∏—é –≥–∏–ø–µ—Ä-—Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π –≤ –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç—è—Ö, –∫–æ—Ç–æ—Ä—ã–µ —Å—Ç—Ä–∞–¥–∞—é—Ç –æ—Ç –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–π –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç–∏. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç KromHC ‚Äî –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è –ö—Ä–æ–Ω–µ–∫–µ—Ä–∞ –¥–ª—è –ø–∞—Ä–∞–º–µ—Ç—Ä–∏–∑–∞—Ü–∏–∏ –æ—Å—Ç–∞—Ç–æ—á–Ω—ã—Ö –º–∞—Ç—Ä–∏—Ü —Å –≥–∞—Ä–∞–Ω—Ç–∏–µ–π –¥–≤–æ–π–Ω–æ–π —Å—Ç–æ—Ö–∞—Å—Ç–∏—á–Ω–æ—Å—Ç–∏. –≠—Ç–æ —Ä–µ—à–µ–Ω–∏–µ —Å–Ω–∏–∂–∞–µ—Ç —Å–ª–æ–∂–Ω–æ—Å—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å –∫—É–±–∏—á–µ—Å–∫–æ–π O(n¬≥C) –¥–æ –∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–æ–π O(n¬≤C), –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –∏–∑–±–µ–≥–∞—è —Ñ–∞–∫—Ç–æ—Ä–∏–∞–ª—å–Ω–æ–≥–æ –≤–∑—Ä—ã–≤–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ KromHC –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –∏–ª–∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã –ø—Ä–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –º–µ–Ω—å—à–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ –æ–±—É—á–∞–µ–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.",
  "emoji": "üìâ",
  "title": "–ö—Ä–æ–Ω–µ–∫–µ—Ä–æ–≤—ã –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –ø–∞—Ä–∞–º–µ—Ç—Ä–∏–∑–∞—Ü–∏–∏ –≥–∏–ø–µ—Ä-—Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π"
}
```
[30.01.2026 14:00] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"KromHC addresses training instability and scalability issues in hyper-connections by using Kronecker products to parametrize residual matrices with reduced parameter complexity.  					AI-generated summary 				 The success of Hyper-Connections (HC) in neural networks (NN) has also highlighted issues related to its training instability and restricted scalability. The Manifold-Constrained Hyper-Connections (mHC) mitigate these challenges by projecting the residual connection space onto a Birkhoff polytope, however, it faces two issues: 1) its iterative Sinkhorn-Knopp (SK) algorithm does not always yield exact doubly stochastic residual matrices; 2) mHC incurs a prohibitive O(n^3C) parameter complexity with n as the width of the residual stream and C as the feature dimension. The recently proposed mHC-lite reparametrizes the residual matrix via the Birkhoff-von-Neumann theorem to guarantee double stochasticity, but also faces a factorial explosion in its parameter complexity, O left( nC cdot n! right). To address both challenges, we propose KromHC, which uses the Kronecker products of smaller doubly stochastic matrices to parametrize the residual matrix in mHC. By enforcing manifold constraints across the factor residual matrices along each mode of the tensorized residual stream, KromHC guarantees exact double stochasticity of the residual matrices while reducing parameter complexity to O(n^2C). Comprehensive experiments demonstrate that KromHC matches or even outperforms state-of-the-art (SOTA) mHC variants, while requiring significantly fewer trainable parameters. The code is available at https://github.com/wz1119/KromHC."

[30.01.2026 14:00] Response: ```python
["ARCHITECTURE", "TRAINING"]
```
[30.01.2026 14:00] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"KromHC addresses training instability and scalability issues in hyper-connections by using Kronecker products to parametrize residual matrices with reduced parameter complexity.  					AI-generated summary 				 The success of Hyper-Connections (HC) in neural networks (NN) has also highlighted issues related to its training instability and restricted scalability. The Manifold-Constrained Hyper-Connections (mHC) mitigate these challenges by projecting the residual connection space onto a Birkhoff polytope, however, it faces two issues: 1) its iterative Sinkhorn-Knopp (SK) algorithm does not always yield exact doubly stochastic residual matrices; 2) mHC incurs a prohibitive O(n^3C) parameter complexity with n as the width of the residual stream and C as the feature dimension. The recently proposed mHC-lite reparametrizes the residual matrix via the Birkhoff-von-Neumann theorem to guarantee double stochasticity, but also faces a factorial explosion in its parameter complexity, O left( nC cdot n! right). To address both challenges, we propose KromHC, which uses the Kronecker products of smaller doubly stochastic matrices to parametrize the residual matrix in mHC. By enforcing manifold constraints across the factor residual matrices along each mode of the tensorized residual stream, KromHC guarantees exact double stochasticity of the residual matrices while reducing parameter complexity to O(n^2C). Comprehensive experiments demonstrate that KromHC matches or even outperforms state-of-the-art (SOTA) mHC variants, while requiring significantly fewer trainable parameters. The code is available at https://github.com/wz1119/KromHC."

[30.01.2026 14:00] Response: ```python
["OPTIMIZATION", "OPEN_SOURCE"]
```

**Justification:**

- **OPTIMIZATION**: The paper directly addresses training optimization by tackling "training instability and scalability issues" in neural networks through improved parametrization methods (Kronecker products) that reduce parameter complexity from O(n¬≥C) to O(n¬≤C).

- **OPEN_SOURCE**: The paper explicitly states "The code is available at https://github.com/wz1119/KromHC," indicating the authors are releasing their code publicly.
[30.01.2026 14:00] Error. Failed to parse JSON from LLM. ["OPTIMIZATION", "OPEN_SOURCE"]


**Justification:**

- **OPTIMIZATION**: The paper directly addresses training optimization by tackling "training instability and scalability issues" in neural networks through improved parametrization methods (Kronecker products) that reduce parameter complexity from O(n¬≥C) to O(n¬≤C).

- **OPEN_SOURCE**: The paper explicitly states "The code is available at https://github.com/wz1119/KromHC," indicating the authors are releasing their code publicly.
[30.01.2026 14:00] Response: ParsedChatCompletionMessage[Article](content='{"desc":"KromHC is a novel approach that addresses the training instability and scalability issues found in hyper-connections within neural networks. It utilizes Kronecker products to effectively parametrize residual matrices, significantly reducing the complexity of parameters needed for training. By enforcing manifold constraints on the factor residual matrices, KromHC ensures that the residual matrices remain doubly stochastic while lowering the parameter complexity to O(n^2C). Experimental results show that KromHC not only matches but can also surpass the performance of existing state-of-the-art methods, all while using fewer trainable parameters.","title":"KromHC: Simplifying Hyper-Connections with Kronecker Products"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='KromHC is a novel approach that addresses the training instability and scalability issues found in hyper-connections within neural networks. It utilizes Kronecker products to effectively parametrize residual matrices, significantly reducing the complexity of parameters needed for training. By enforcing manifold constraints on the factor residual matrices, KromHC ensures that the residual matrices remain doubly stochastic while lowering the parameter complexity to O(n^2C). Experimental results show that KromHC not only matches but can also surpass the performance of existing state-of-the-art methods, all while using fewer trainable parameters.', title='KromHC: Simplifying Hyper-Connections with Kronecker Products'))
[30.01.2026 14:00] Response: ParsedChatCompletionMessage[Article](content='{"desc":"KromHCÊòØ‰∏ÄÁßçÊñ∞ÊñπÊ≥ïÔºåÊó®Âú®Ëß£ÂÜ≥Ë∂ÖËøûÊé•ÔºàHCÔºâÂú®Á•ûÁªèÁΩëÁªúËÆ≠ÁªÉ‰∏≠ÁöÑ‰∏çÁ®≥ÂÆöÊÄßÂíåÂèØÊâ©Â±ïÊÄßÈóÆÈ¢ò„ÄÇÂÆÉÈÄöËøá‰ΩøÁî®ÂÖãÁΩóÂÜÖÂÖãÁßØÂØπÊÆãÂ∑ÆÁü©ÈòµËøõË°åÂèÇÊï∞ÂåñÔºå‰ªéËÄåÈôç‰ΩéÂèÇÊï∞Â§çÊùÇÂ∫¶„ÄÇ‰∏é‰πãÂâçÁöÑÊñπÊ≥ïÁõ∏ÊØîÔºåKromHCÁ°Æ‰øù‰∫ÜÊÆãÂ∑ÆÁü©ÈòµÁöÑÂèåÈöèÊú∫ÊÄßÔºåÂπ∂Â∞ÜÂèÇÊï∞Â§çÊùÇÂ∫¶ÂáèÂ∞ëÂà∞O(n^2C)„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåKromHCÂú®ÊÄßËÉΩ‰∏ä‰∏éÊúÄÂÖàËøõÁöÑmHCÂèò‰ΩìÁõ∏ÂΩìÔºåÁîöËá≥Êõ¥‰ºòÔºåÂêåÊó∂ÈúÄË¶ÅÁöÑÂèØËÆ≠ÁªÉÂèÇÊï∞ÊòæËëóÂáèÂ∞ë„ÄÇ","title":"KromHCÔºöÈôç‰ΩéË∂ÖËøûÊé•ÁöÑËÆ≠ÁªÉÂ§çÊùÇÂ∫¶‰∏é‰∏çÁ®≥ÂÆöÊÄß"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='KromHCÊòØ‰∏ÄÁßçÊñ∞ÊñπÊ≥ïÔºåÊó®Âú®Ëß£ÂÜ≥Ë∂ÖËøûÊé•ÔºàHCÔºâÂú®Á•ûÁªèÁΩëÁªúËÆ≠ÁªÉ‰∏≠ÁöÑ‰∏çÁ®≥ÂÆöÊÄßÂíåÂèØÊâ©Â±ïÊÄßÈóÆÈ¢ò„ÄÇÂÆÉÈÄöËøá‰ΩøÁî®ÂÖãÁΩóÂÜÖÂÖãÁßØÂØπÊÆãÂ∑ÆÁü©ÈòµËøõË°åÂèÇÊï∞ÂåñÔºå‰ªéËÄåÈôç‰ΩéÂèÇÊï∞Â§çÊùÇÂ∫¶„ÄÇ‰∏é‰πãÂâçÁöÑÊñπÊ≥ïÁõ∏ÊØîÔºåKromHCÁ°Æ‰øù‰∫ÜÊÆãÂ∑ÆÁü©ÈòµÁöÑÂèåÈöèÊú∫ÊÄßÔºåÂπ∂Â∞ÜÂèÇÊï∞Â§çÊùÇÂ∫¶ÂáèÂ∞ëÂà∞O(n^2C)„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåKromHCÂú®ÊÄßËÉΩ‰∏ä‰∏éÊúÄÂÖàËøõÁöÑmHCÂèò‰ΩìÁõ∏ÂΩìÔºåÁîöËá≥Êõ¥‰ºòÔºåÂêåÊó∂ÈúÄË¶ÅÁöÑÂèØËÆ≠ÁªÉÂèÇÊï∞ÊòæËëóÂáèÂ∞ë„ÄÇ', title='KromHCÔºöÈôç‰ΩéË∂ÖËøûÊé•ÁöÑËÆ≠ÁªÉÂ§çÊùÇÂ∫¶‰∏é‰∏çÁ®≥ÂÆöÊÄß'))
[30.01.2026 14:00] Using data from previous issue: {"categories": ["#benchmark", "#cv", "#robotics"], "emoji": "ü§ñ", "ru": {"title": "–û–±—ä–µ–∫—Ç–Ω–æ-—Ü–µ–Ω—Ç—Ä–∏—á–µ—Å–∫–∏–µ —Å–ª–æ—Ç—ã ‚Äî –∫–ª—é—á –∫ –Ω–∞–¥—ë–∂–Ω–æ–π –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–∏ —Ä–æ–±–æ—Ç–æ–≤ –ø—Ä–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–∏ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —É—Å–ª–æ–≤–∏–π", "desc": "–í —ç—Ç–æ–π —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ –≤—ã–±–æ—Ä–∞ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –ø–æ–ª–∏—Ç–∏–∫ –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–∏ —Ä–æ–±–æ—Ç–æ–≤.
[30.01.2026 14:00] Using data from previous issue: {"categories": ["#benchmark", "#video", "#robotics"], "emoji": "üé¨", "ru": {"title": "–†–∞–∑–¥–µ–ª—ë–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —Ñ–∏–∑–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –≤–∏–¥–µ–æ-–≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω WorldBench ‚Äî –≤–∏–¥–µ–æ-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è —Ä–∞–∑–¥–µ–ª—ë–Ω–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ —Ñ–∏–∑–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª
[30.01.2026 14:00] Using data from previous issue: {"categories": ["#training", "#cv", "#robotics", "#architecture"], "emoji": "ü§ñ", "ru": {"title": "–û–±—ä–µ–∫—Ç–Ω–æ-—Ü–µ–Ω—Ç—Ä–∏—á–Ω–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∏ —á–µ—Ä–µ–∑ –º–Ω–æ–≥–æ—Ñ–∞–∑–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è STORM ‚Äî –ª—ë–≥–∫–∏–π –∞–¥–∞–ø—Ç–∞—Ü–∏–æ–Ω–Ω—ã–π –º–æ–¥—É–ª—å, –∫–æ—Ç–æ—Ä—ã–π –¥–æ–ø–æ–ª–Ω—è–µ—Ç –∑–∞–º–æ—Ä–æ–∂–µ–Ω–Ω—ã–µ –≤–∏–∑—É–∞–ª—å
[30.01.2026 14:00] Renaming data file.
[30.01.2026 14:00] Renaming previous data. hf_papers.json to ./d/2026-01-30.json
[30.01.2026 14:00] Saving new data file.
[30.01.2026 14:00] Generating page.
[30.01.2026 14:00] Renaming previous page.
[30.01.2026 14:00] Renaming previous data. index.html to ./d/2026-01-30.html
[30.01.2026 14:00] Writing result.
[30.01.2026 14:00] Renaming log file.
[30.01.2026 14:00] Renaming previous data. log.txt to ./logs/2026-01-30_last_log.txt
