[30.01.2026 21:29] Read previous papers.
[30.01.2026 21:29] Generating top page (month).
[30.01.2026 21:29] Writing top page (month).
[30.01.2026 22:28] Read previous papers.
[30.01.2026 22:28] Get feed.
[30.01.2026 22:28] Get page data from previous paper. URL: https://huggingface.co/papers/2601.20833
[30.01.2026 22:28] Get page data from previous paper. URL: https://huggingface.co/papers/2601.20354
[30.01.2026 22:28] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21204
[30.01.2026 22:28] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22153
[30.01.2026 22:28] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21639
[30.01.2026 22:28] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21821
[30.01.2026 22:28] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21420
[30.01.2026 22:28] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22046
[30.01.2026 22:28] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21337
[30.01.2026 22:28] Get page data from previous paper. URL: https://huggingface.co/papers/2601.20730
[30.01.2026 22:28] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22154
[30.01.2026 22:28] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21754
[30.01.2026 22:28] Get page data from previous paper. URL: https://huggingface.co/papers/2601.16914
[30.01.2026 22:28] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22157
[30.01.2026 22:28] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22083
[30.01.2026 22:28] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21590
[30.01.2026 22:28] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21571
[30.01.2026 22:28] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21051
[30.01.2026 22:28] Get page data from previous paper. URL: https://huggingface.co/papers/2601.18129
[30.01.2026 22:28] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22069
[30.01.2026 22:28] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21181
[30.01.2026 22:28] Get page data from previous paper. URL: https://huggingface.co/papers/2601.20975
[30.01.2026 22:28] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21598
[30.01.2026 22:28] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22158
[30.01.2026 22:28] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22156
[30.01.2026 22:28] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22146
[30.01.2026 22:28] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21579
[30.01.2026 22:28] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21343
[30.01.2026 22:28] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22101
[30.01.2026 22:28] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22054
[30.01.2026 22:28] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21996
[30.01.2026 22:28] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21406
[30.01.2026 22:28] Get page data from previous paper. URL: https://huggingface.co/papers/2601.20465
[30.01.2026 22:28] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22143
[30.01.2026 22:28] Get page data from previous paper. URL: https://huggingface.co/papers/2601.19001
[30.01.2026 22:28] Get page data from previous paper. URL: https://huggingface.co/papers/2601.20103
[30.01.2026 22:28] Get page data from previous paper. URL: https://huggingface.co/papers/2601.17690
[30.01.2026 22:28] Get page data from previous paper. URL: https://huggingface.co/papers/2601.11747
[30.01.2026 22:28] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21872
[30.01.2026 22:28] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21416
[30.01.2026 22:28] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21282
[30.01.2026 22:28] Get page data from previous paper. URL: https://huggingface.co/papers/2601.20381
[30.01.2026 22:28] Get page data from previous paper. URL: https://huggingface.co/papers/2601.18005
[30.01.2026 22:28] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[30.01.2026 22:28] No deleted papers detected.
[30.01.2026 22:28] Downloading and parsing papers (pdf, html). Total: 43.
[30.01.2026 22:28] Downloading and parsing paper https://huggingface.co/papers/2601.20833.
[30.01.2026 22:28] Extra JSON file exists (./assets/json/2601.20833.json), skip PDF parsing.
[30.01.2026 22:28] Paper image links file exists (./assets/img_data/2601.20833.json), skip HTML parsing.
[30.01.2026 22:28] Success.
[30.01.2026 22:28] Downloading and parsing paper https://huggingface.co/papers/2601.20354.
[30.01.2026 22:28] Extra JSON file exists (./assets/json/2601.20354.json), skip PDF parsing.
[30.01.2026 22:28] Paper image links file exists (./assets/img_data/2601.20354.json), skip HTML parsing.
[30.01.2026 22:28] Success.
[30.01.2026 22:28] Downloading and parsing paper https://huggingface.co/papers/2601.21204.
[30.01.2026 22:28] Extra JSON file exists (./assets/json/2601.21204.json), skip PDF parsing.
[30.01.2026 22:28] Paper image links file exists (./assets/img_data/2601.21204.json), skip HTML parsing.
[30.01.2026 22:28] Success.
[30.01.2026 22:28] Downloading and parsing paper https://huggingface.co/papers/2601.22153.
[30.01.2026 22:28] Extra JSON file exists (./assets/json/2601.22153.json), skip PDF parsing.
[30.01.2026 22:28] Paper image links file exists (./assets/img_data/2601.22153.json), skip HTML parsing.
[30.01.2026 22:28] Success.
[30.01.2026 22:28] Downloading and parsing paper https://huggingface.co/papers/2601.21639.
[30.01.2026 22:28] Extra JSON file exists (./assets/json/2601.21639.json), skip PDF parsing.
[30.01.2026 22:28] Paper image links file exists (./assets/img_data/2601.21639.json), skip HTML parsing.
[30.01.2026 22:28] Success.
[30.01.2026 22:28] Downloading and parsing paper https://huggingface.co/papers/2601.21821.
[30.01.2026 22:28] Extra JSON file exists (./assets/json/2601.21821.json), skip PDF parsing.
[30.01.2026 22:28] Paper image links file exists (./assets/img_data/2601.21821.json), skip HTML parsing.
[30.01.2026 22:28] Success.
[30.01.2026 22:28] Downloading and parsing paper https://huggingface.co/papers/2601.21420.
[30.01.2026 22:28] Extra JSON file exists (./assets/json/2601.21420.json), skip PDF parsing.
[30.01.2026 22:28] Paper image links file exists (./assets/img_data/2601.21420.json), skip HTML parsing.
[30.01.2026 22:28] Success.
[30.01.2026 22:28] Downloading and parsing paper https://huggingface.co/papers/2601.22046.
[30.01.2026 22:28] Extra JSON file exists (./assets/json/2601.22046.json), skip PDF parsing.
[30.01.2026 22:28] Paper image links file exists (./assets/img_data/2601.22046.json), skip HTML parsing.
[30.01.2026 22:28] Success.
[30.01.2026 22:28] Downloading and parsing paper https://huggingface.co/papers/2601.21337.
[30.01.2026 22:28] Extra JSON file exists (./assets/json/2601.21337.json), skip PDF parsing.
[30.01.2026 22:28] Paper image links file exists (./assets/img_data/2601.21337.json), skip HTML parsing.
[30.01.2026 22:28] Success.
[30.01.2026 22:28] Downloading and parsing paper https://huggingface.co/papers/2601.20730.
[30.01.2026 22:28] Extra JSON file exists (./assets/json/2601.20730.json), skip PDF parsing.
[30.01.2026 22:28] Paper image links file exists (./assets/img_data/2601.20730.json), skip HTML parsing.
[30.01.2026 22:28] Success.
[30.01.2026 22:28] Downloading and parsing paper https://huggingface.co/papers/2601.22154.
[30.01.2026 22:28] Extra JSON file exists (./assets/json/2601.22154.json), skip PDF parsing.
[30.01.2026 22:28] Paper image links file exists (./assets/img_data/2601.22154.json), skip HTML parsing.
[30.01.2026 22:28] Success.
[30.01.2026 22:28] Downloading and parsing paper https://huggingface.co/papers/2601.21754.
[30.01.2026 22:28] Extra JSON file exists (./assets/json/2601.21754.json), skip PDF parsing.
[30.01.2026 22:28] Paper image links file exists (./assets/img_data/2601.21754.json), skip HTML parsing.
[30.01.2026 22:28] Success.
[30.01.2026 22:28] Downloading and parsing paper https://huggingface.co/papers/2601.16914.
[30.01.2026 22:28] Extra JSON file exists (./assets/json/2601.16914.json), skip PDF parsing.
[30.01.2026 22:28] Paper image links file exists (./assets/img_data/2601.16914.json), skip HTML parsing.
[30.01.2026 22:28] Success.
[30.01.2026 22:28] Downloading and parsing paper https://huggingface.co/papers/2601.22157.
[30.01.2026 22:28] Extra JSON file exists (./assets/json/2601.22157.json), skip PDF parsing.
[30.01.2026 22:28] Paper image links file exists (./assets/img_data/2601.22157.json), skip HTML parsing.
[30.01.2026 22:28] Success.
[30.01.2026 22:28] Downloading and parsing paper https://huggingface.co/papers/2601.22083.
[30.01.2026 22:28] Extra JSON file exists (./assets/json/2601.22083.json), skip PDF parsing.
[30.01.2026 22:28] Paper image links file exists (./assets/img_data/2601.22083.json), skip HTML parsing.
[30.01.2026 22:28] Success.
[30.01.2026 22:28] Downloading and parsing paper https://huggingface.co/papers/2601.21590.
[30.01.2026 22:28] Extra JSON file exists (./assets/json/2601.21590.json), skip PDF parsing.
[30.01.2026 22:28] Paper image links file exists (./assets/img_data/2601.21590.json), skip HTML parsing.
[30.01.2026 22:28] Success.
[30.01.2026 22:28] Downloading and parsing paper https://huggingface.co/papers/2601.21571.
[30.01.2026 22:28] Extra JSON file exists (./assets/json/2601.21571.json), skip PDF parsing.
[30.01.2026 22:28] Paper image links file exists (./assets/img_data/2601.21571.json), skip HTML parsing.
[30.01.2026 22:28] Success.
[30.01.2026 22:28] Downloading and parsing paper https://huggingface.co/papers/2601.21051.
[30.01.2026 22:28] Extra JSON file exists (./assets/json/2601.21051.json), skip PDF parsing.
[30.01.2026 22:28] Paper image links file exists (./assets/img_data/2601.21051.json), skip HTML parsing.
[30.01.2026 22:28] Success.
[30.01.2026 22:28] Downloading and parsing paper https://huggingface.co/papers/2601.18129.
[30.01.2026 22:28] Extra JSON file exists (./assets/json/2601.18129.json), skip PDF parsing.
[30.01.2026 22:28] Paper image links file exists (./assets/img_data/2601.18129.json), skip HTML parsing.
[30.01.2026 22:28] Success.
[30.01.2026 22:28] Downloading and parsing paper https://huggingface.co/papers/2601.22069.
[30.01.2026 22:28] Extra JSON file exists (./assets/json/2601.22069.json), skip PDF parsing.
[30.01.2026 22:28] Paper image links file exists (./assets/img_data/2601.22069.json), skip HTML parsing.
[30.01.2026 22:28] Success.
[30.01.2026 22:28] Downloading and parsing paper https://huggingface.co/papers/2601.21181.
[30.01.2026 22:28] Extra JSON file exists (./assets/json/2601.21181.json), skip PDF parsing.
[30.01.2026 22:28] Paper image links file exists (./assets/img_data/2601.21181.json), skip HTML parsing.
[30.01.2026 22:28] Success.
[30.01.2026 22:28] Downloading and parsing paper https://huggingface.co/papers/2601.20975.
[30.01.2026 22:28] Extra JSON file exists (./assets/json/2601.20975.json), skip PDF parsing.
[30.01.2026 22:28] Paper image links file exists (./assets/img_data/2601.20975.json), skip HTML parsing.
[30.01.2026 22:28] Success.
[30.01.2026 22:28] Downloading and parsing paper https://huggingface.co/papers/2601.21598.
[30.01.2026 22:28] Extra JSON file exists (./assets/json/2601.21598.json), skip PDF parsing.
[30.01.2026 22:28] Paper image links file exists (./assets/img_data/2601.21598.json), skip HTML parsing.
[30.01.2026 22:28] Success.
[30.01.2026 22:28] Downloading and parsing paper https://huggingface.co/papers/2601.22158.
[30.01.2026 22:28] Extra JSON file exists (./assets/json/2601.22158.json), skip PDF parsing.
[30.01.2026 22:28] Paper image links file exists (./assets/img_data/2601.22158.json), skip HTML parsing.
[30.01.2026 22:28] Success.
[30.01.2026 22:28] Downloading and parsing paper https://huggingface.co/papers/2601.22156.
[30.01.2026 22:28] Extra JSON file exists (./assets/json/2601.22156.json), skip PDF parsing.
[30.01.2026 22:28] Paper image links file exists (./assets/img_data/2601.22156.json), skip HTML parsing.
[30.01.2026 22:28] Success.
[30.01.2026 22:28] Downloading and parsing paper https://huggingface.co/papers/2601.22146.
[30.01.2026 22:28] Extra JSON file exists (./assets/json/2601.22146.json), skip PDF parsing.
[30.01.2026 22:28] Paper image links file exists (./assets/img_data/2601.22146.json), skip HTML parsing.
[30.01.2026 22:28] Success.
[30.01.2026 22:28] Downloading and parsing paper https://huggingface.co/papers/2601.21579.
[30.01.2026 22:28] Extra JSON file exists (./assets/json/2601.21579.json), skip PDF parsing.
[30.01.2026 22:28] Paper image links file exists (./assets/img_data/2601.21579.json), skip HTML parsing.
[30.01.2026 22:28] Success.
[30.01.2026 22:28] Downloading and parsing paper https://huggingface.co/papers/2601.21343.
[30.01.2026 22:28] Extra JSON file exists (./assets/json/2601.21343.json), skip PDF parsing.
[30.01.2026 22:28] Paper image links file exists (./assets/img_data/2601.21343.json), skip HTML parsing.
[30.01.2026 22:28] Success.
[30.01.2026 22:28] Downloading and parsing paper https://huggingface.co/papers/2601.22101.
[30.01.2026 22:28] Extra JSON file exists (./assets/json/2601.22101.json), skip PDF parsing.
[30.01.2026 22:28] Paper image links file exists (./assets/img_data/2601.22101.json), skip HTML parsing.
[30.01.2026 22:28] Success.
[30.01.2026 22:28] Downloading and parsing paper https://huggingface.co/papers/2601.22054.
[30.01.2026 22:28] Extra JSON file exists (./assets/json/2601.22054.json), skip PDF parsing.
[30.01.2026 22:28] Paper image links file exists (./assets/img_data/2601.22054.json), skip HTML parsing.
[30.01.2026 22:28] Success.
[30.01.2026 22:28] Downloading and parsing paper https://huggingface.co/papers/2601.21996.
[30.01.2026 22:28] Extra JSON file exists (./assets/json/2601.21996.json), skip PDF parsing.
[30.01.2026 22:28] Paper image links file exists (./assets/img_data/2601.21996.json), skip HTML parsing.
[30.01.2026 22:28] Success.
[30.01.2026 22:28] Downloading and parsing paper https://huggingface.co/papers/2601.21406.
[30.01.2026 22:28] Extra JSON file exists (./assets/json/2601.21406.json), skip PDF parsing.
[30.01.2026 22:28] Paper image links file exists (./assets/img_data/2601.21406.json), skip HTML parsing.
[30.01.2026 22:28] Success.
[30.01.2026 22:28] Downloading and parsing paper https://huggingface.co/papers/2601.20465.
[30.01.2026 22:28] Extra JSON file exists (./assets/json/2601.20465.json), skip PDF parsing.
[30.01.2026 22:28] Paper image links file exists (./assets/img_data/2601.20465.json), skip HTML parsing.
[30.01.2026 22:28] Success.
[30.01.2026 22:28] Downloading and parsing paper https://huggingface.co/papers/2601.22143.
[30.01.2026 22:28] Extra JSON file exists (./assets/json/2601.22143.json), skip PDF parsing.
[30.01.2026 22:28] Paper image links file exists (./assets/img_data/2601.22143.json), skip HTML parsing.
[30.01.2026 22:28] Success.
[30.01.2026 22:28] Downloading and parsing paper https://huggingface.co/papers/2601.19001.
[30.01.2026 22:28] Extra JSON file exists (./assets/json/2601.19001.json), skip PDF parsing.
[30.01.2026 22:28] Paper image links file exists (./assets/img_data/2601.19001.json), skip HTML parsing.
[30.01.2026 22:28] Success.
[30.01.2026 22:28] Downloading and parsing paper https://huggingface.co/papers/2601.20103.
[30.01.2026 22:28] Extra JSON file exists (./assets/json/2601.20103.json), skip PDF parsing.
[30.01.2026 22:28] Paper image links file exists (./assets/img_data/2601.20103.json), skip HTML parsing.
[30.01.2026 22:28] Success.
[30.01.2026 22:28] Downloading and parsing paper https://huggingface.co/papers/2601.17690.
[30.01.2026 22:28] Extra JSON file exists (./assets/json/2601.17690.json), skip PDF parsing.
[30.01.2026 22:28] Paper image links file exists (./assets/img_data/2601.17690.json), skip HTML parsing.
[30.01.2026 22:28] Success.
[30.01.2026 22:28] Downloading and parsing paper https://huggingface.co/papers/2601.11747.
[30.01.2026 22:28] Extra JSON file exists (./assets/json/2601.11747.json), skip PDF parsing.
[30.01.2026 22:28] Paper image links file exists (./assets/img_data/2601.11747.json), skip HTML parsing.
[30.01.2026 22:28] Success.
[30.01.2026 22:28] Downloading and parsing paper https://huggingface.co/papers/2601.21872.
[30.01.2026 22:28] Extra JSON file exists (./assets/json/2601.21872.json), skip PDF parsing.
[30.01.2026 22:28] Paper image links file exists (./assets/img_data/2601.21872.json), skip HTML parsing.
[30.01.2026 22:28] Success.
[30.01.2026 22:28] Downloading and parsing paper https://huggingface.co/papers/2601.21416.
[30.01.2026 22:28] Extra JSON file exists (./assets/json/2601.21416.json), skip PDF parsing.
[30.01.2026 22:28] Paper image links file exists (./assets/img_data/2601.21416.json), skip HTML parsing.
[30.01.2026 22:28] Success.
[30.01.2026 22:28] Downloading and parsing paper https://huggingface.co/papers/2601.21282.
[30.01.2026 22:28] Extra JSON file exists (./assets/json/2601.21282.json), skip PDF parsing.
[30.01.2026 22:28] Paper image links file exists (./assets/img_data/2601.21282.json), skip HTML parsing.
[30.01.2026 22:28] Success.
[30.01.2026 22:28] Downloading and parsing paper https://huggingface.co/papers/2601.20381.
[30.01.2026 22:28] Extra JSON file exists (./assets/json/2601.20381.json), skip PDF parsing.
[30.01.2026 22:28] Paper image links file exists (./assets/img_data/2601.20381.json), skip HTML parsing.
[30.01.2026 22:28] Success.
[30.01.2026 22:28] Downloading and parsing paper https://huggingface.co/papers/2601.18005.
[30.01.2026 22:28] Extra JSON file exists (./assets/json/2601.18005.json), skip PDF parsing.
[30.01.2026 22:28] Paper image links file exists (./assets/img_data/2601.18005.json), skip HTML parsing.
[30.01.2026 22:28] Success.
[30.01.2026 22:28] Enriching papers with extra data.
[30.01.2026 22:28] ********************************************************************************
[30.01.2026 22:28] Abstract 0. Offline knowledge construction through structured methodological graphs enables more reliable and scalable autonomous scientific discovery by reducing reliance on real-time literature processing.  					AI-generated summary 				 Autonomous scientific discovery with large language model (LLM)-based ag...
[30.01.2026 22:28] ********************************************************************************
[30.01.2026 22:28] Abstract 1. A new benchmark and dataset are introduced to evaluate and improve spatial reasoning capabilities in text-to-image models through information-dense prompts and fine-tuning.  					AI-generated summary 				 Text-to-image (T2I) models have achieved remarkable success in generating high-fidelity images,...
[30.01.2026 22:28] ********************************************************************************
[30.01.2026 22:28] Abstract 2. Embedding scaling offers superior sparsity scaling compared to expert scaling in large language models, enabling efficient inference through system optimizations and speculative decoding.  					AI-generated summary 				 While Mixture-of-Experts (MoE) architectures have become the standard for sparsi...
[30.01.2026 22:28] ********************************************************************************
[30.01.2026 22:28] Abstract 3. DynamicVLA addresses dynamic object manipulation challenges through a compact vision-language-action model with temporal reasoning and closed-loop adaptation, supported by a new benchmark for dynamic manipulation tasks.  					AI-generated summary 				 Manipulating dynamic objects remains an open cha...
[30.01.2026 22:28] ********************************************************************************
[30.01.2026 22:28] Abstract 4. OCRVerse is a novel end-to-end OCR method that unifies text-centric and vision-centric approaches through comprehensive data engineering and a two-stage SFT-RL training framework with domain-specific reward strategies.  					AI-generated summary 				 The development of large vision language models d...
[30.01.2026 22:28] ********************************************************************************
[30.01.2026 22:28] Abstract 5. A large-scale multimodal reasoning dataset called MMFineReason is introduced to improve vision language models' performance through high-quality reasoning annotations and demonstrates superior parameter efficiency in fine-tuned models.  					AI-generated summary 				 Recent advances in Vision Langua...
[30.01.2026 22:28] ********************************************************************************
[30.01.2026 22:28] Abstract 6. ConceptMoE dynamically allocates computation by merging similar tokens into concept representations, improving both performance and efficiency in large language models through adaptive processing and reduced attention computation.  					AI-generated summary 				 Large language models allocate unifor...
[30.01.2026 22:28] ********************************************************************************
[30.01.2026 22:28] Abstract 7. PLANING presents an efficient streaming reconstruction framework that combines explicit geometric primitives with neural Gaussians to achieve high-quality rendering and accurate geometry simultaneously through decoupled optimization.  					AI-generated summary 				 Streaming reconstruction from mono...
[30.01.2026 22:28] ********************************************************************************
[30.01.2026 22:28] Abstract 8. The Qwen3-ASR family introduces speech recognition models with language identification capabilities and a non-autoregressive forced alignment model, achieving state-of-the-art performance and efficient processing.  					AI-generated summary 				 In this report, we introduce Qwen3-ASR family, which i...
[30.01.2026 22:28] ********************************************************************************
[30.01.2026 22:28] Abstract 9. AgentLongBench evaluates large language models as autonomous agents through dynamic environment interactions, revealing challenges in handling high-information-density tool responses compared to memory fragmentation in long conversations.  					AI-generated summary 				 The evolution of Large Langua...
[30.01.2026 22:28] ********************************************************************************
[30.01.2026 22:28] Abstract 10. Agent-RRM, a multi-faceted reward model, provides structured feedback for agentic trajectories through reasoning traces, critiques, and performance scores, with unified feedback integration showing superior performance across diverse benchmarks.  					AI-generated summary 				 Agentic Reinforcement ...
[30.01.2026 22:28] ********************************************************************************
[30.01.2026 22:28] Abstract 11. A novel framework called SCOUT is introduced that uses lightweight scouts to reduce exploration costs for large language models in nonlinguistic environments, enabling improved performance through supervised fine-tuning and reinforcement learning.  					AI-generated summary 				 While Large Language...
[30.01.2026 22:28] ********************************************************************************
[30.01.2026 22:28] Abstract 12. Researchers developed a method to overcome sink-collapse in autoregressive video generation by addressing the conflict between Rotary Position Embedding and multi-head attention mechanisms, enabling real-time streaming of videos up to 12 hours long.  					AI-generated summary 				 Recent research in...
[30.01.2026 22:28] ********************************************************************************
[30.01.2026 22:28] Abstract 13. Hidden superior models exist in public repositories but are overlooked due to inefficient discovery methods; a multi-armed bandit approach using shared query sets and aggressive elimination significantly accelerates identification of top-performing models.  					AI-generated summary 				 Public repo...
[30.01.2026 22:28] ********************************************************************************
[30.01.2026 22:28] Abstract 14. GANPO uses latent-space regularization through adversarial divergence minimization to improve language model preference optimization, offering more robust structural feedback than token-level methods.  					AI-generated summary 				 Learning from human feedback typically relies on preference optimiz...
[30.01.2026 22:28] ********************************************************************************
[30.01.2026 22:28] Abstract 15. A theoretically grounded method for improving large language model reasoning performance through distribution sharpening without iterative sampling or external rewards, achieving comparable results to reinforcement learning post-training with significantly reduced computational costs.  					AI-gener...
[30.01.2026 22:28] ********************************************************************************
[30.01.2026 22:28] Abstract 16. Token filtering during pretraining effectively reduces unwanted language model capabilities while maintaining alignment, becoming more effective at larger scales and tolerating noisy labels with sufficient compute.  					AI-generated summary 				 Current approaches to reducing undesired capabilities...
[30.01.2026 22:28] ********************************************************************************
[30.01.2026 22:28] Abstract 17. A two-stage trained cybersecurity reasoning model achieves competitive performance on specialized tasks while maintaining general capabilities through supervised fine-tuning and reinforcement learning from verifiable rewards.  					AI-generated summary 				 We present Foundation-Sec-8B-Reasoning, th...
[30.01.2026 22:28] ********************************************************************************
[30.01.2026 22:28] Abstract 18. A minimal post-training approach using supervised fine-tuning, on-policy distillation, and small-scale reinforcement fine-tuning enables the development of high-quality sovereign language models with reduced resource requirements.  					AI-generated summary 				 Large language models (LLMs) have pro...
[30.01.2026 22:28] ********************************************************************************
[30.01.2026 22:28] Abstract 19. VTC-R1 enables efficient long-context reasoning by compressing textual traces into compact images and iteratively feeding them back into vision-language models as optical memory, achieving significant speedup without sacrificing performance.  					AI-generated summary 				 Long-context reasoning has...
[30.01.2026 22:28] ********************************************************************************
[30.01.2026 22:28] Abstract 20. Multimodal Large Language Models suffer from cross-modal hallucinations where one modality incorrectly influences generation from another, leading to fabricated outputs; this exposes a fundamental deficiency in modality-interaction control. To address this, a training-free method called Modality-Ada...
[30.01.2026 22:28] ********************************************************************************
[30.01.2026 22:28] Abstract 21. DeepSearchQA presents a 900-prompt benchmark evaluating agents on complex multi-step information-seeking tasks requiring systematic information collation, deduplication, and reasoning about stopping criteria across 17 fields.  					AI-generated summary 				 We introduce DeepSearchQA, a 900-prompt be...
[30.01.2026 22:28] ********************************************************************************
[30.01.2026 22:28] Abstract 22. Active latent planning method improves reasoning accuracy and efficiency by modeling latent token supervision as conditional VAE and using reinforcement learning with coherence rewards.  					AI-generated summary 				 Aiming at efficient and dense chain-of-thought (CoT) reasoning, latent reasoning m...
[30.01.2026 22:28] ********************************************************************************
[30.01.2026 22:28] Abstract 23. Pixel MeanFlow introduces a one-step latent-free image generation method by separating network output space from loss space, achieving strong performance on ImageNet at multiple resolutions.  					AI-generated summary 				 Modern diffusion/flow-based models for image generation typically exhibit two...
[30.01.2026 22:28] ********************************************************************************
[30.01.2026 22:28] Abstract 24. HALO enables efficient conversion of Transformer models to RNN-attention hybrid architectures with improved long-context performance using minimal training data.  					AI-generated summary 				 Hybrid Transformer architectures, which combine softmax attention blocks and recurrent neural networks (RN...
[30.01.2026 22:28] ********************************************************************************
[30.01.2026 22:28] Abstract 25. Large language models can be pre-trained from scratch using synthetic instruction-response pairs generated from unstructured text corpora, outperforming traditional methods on benchmarks measuring response quality.  					AI-generated summary 				 Due to limited supervised training data, large langua...
[30.01.2026 22:28] ********************************************************************************
[30.01.2026 22:28] Abstract 26. KromHC addresses training instability and scalability issues in hyper-connections by using Kronecker products to parametrize residual matrices with reduced parameter complexity.  					AI-generated summary 				 The success of Hyper-Connections (HC) in neural networks (NN) has also highlighted issues ...
[30.01.2026 22:28] ********************************************************************************
[30.01.2026 22:28] Abstract 27. A reinforcement learning-based pretraining method improves language model safety, factuality, and quality by evaluating generations through a combination of model rollouts, original suffixes, and rewritten suffixes.  					AI-generated summary 				 Ensuring safety, factuality and overall quality in t...
[30.01.2026 22:28] ********************************************************************************
[30.01.2026 22:28] Abstract 28. Error-compensating optimizer eliminates memory overhead from master weights in quantized LLM training while maintaining near-lossless accuracy.  					AI-generated summary 				 Quantization has significantly improved the compute and memory efficiency of Large Language Model (LLM) training. However, e...
[30.01.2026 22:28] ********************************************************************************
[30.01.2026 22:28] Abstract 29. Metric Anything presents a scalable pretraining framework for metric depth estimation that leverages diverse 3D data and sparse metric prompts to achieve superior performance across multiple vision tasks.  					AI-generated summary 				 Scaling has powered recent advances in vision foundation models...
[30.01.2026 22:28] ********************************************************************************
[30.01.2026 22:28] Abstract 30. Mechnistic Data Attribution framework traces interpretable units to specific training samples using influence functions, demonstrating causal relationships between data structure and neural circuit formation in language models.  					AI-generated summary 				 While Mechanistic Interpretability has i...
[30.01.2026 22:28] ********************************************************************************
[30.01.2026 22:28] Abstract 31. UniMRG enhances unified multimodal models by training them to generate multiple visual representations, improving both understanding and generation capabilities through complementary information capture.  					AI-generated summary 				 Unified Multimodal Models (UMMs) integrate both visual understan...
[30.01.2026 22:28] ********************************************************************************
[30.01.2026 22:28] Abstract 32. BMAM presents a brain-inspired multi-agent memory architecture that decomposes memory into specialized subsystems to address long-term reasoning challenges in language-model-based agents.  					AI-generated summary 				 Language-model-based agents operating over extended interaction horizons face pe...
[30.01.2026 22:28] ********************************************************************************
[30.01.2026 22:28] Abstract 33. A lightweight LoRA adaptation of an audio-video diffusion model enables high-quality video dubbing with preserved speaker identity and improved lip synchronization through synthetic multilingual video training.  					AI-generated summary 				 Audio-Visual Foundation Models, which are pretrained to j...
[30.01.2026 22:28] ********************************************************************************
[30.01.2026 22:28] Abstract 34. FROST is an attention-aware method that improves reasoning efficiency by pruning uncritical paths and removing reasoning outliers, leading to reduced token usage and improved accuracy.  					AI-generated summary 				 We propose FROST, an attention-aware method for efficient reasoning. Unlike traditi...
[30.01.2026 22:28] ********************************************************************************
[30.01.2026 22:28] Abstract 35. Researchers developed a comprehensive benchmark for detecting reward hacking in code generation environments, demonstrating that contrastive anomaly detection outperforms isolated classification approaches and revealing challenges with semantically contextualized reward hacks.  					AI-generated sum...
[30.01.2026 22:28] ********************************************************************************
[30.01.2026 22:28] Abstract 36. Neural audio fingerprinting performance varies with segment length, with short segments (0.5-second) generally providing better retrieval accuracy, and large language models showing promise in recommending optimal segment durations.  					AI-generated summary 				 Audio fingerprinting provides an id...
[30.01.2026 22:28] ********************************************************************************
[30.01.2026 22:28] Abstract 37. PRISM leverages design data to create a knowledge base for improving graphic designs based on natural language instructions, achieving superior style alignment compared to existing methods.  					AI-generated summary 				 Graphic design often involves exploring different stylistic directions, which ...
[30.01.2026 22:28] ********************************************************************************
[30.01.2026 22:28] Abstract 38. WebArbiter introduces a reasoning-first WebPRM that formulates reward modeling as text generation to improve web navigation through structured justifications and preference verdicts, outperforming existing baselines in complex web environments.  					AI-generated summary 				 Web agents hold great p...
[30.01.2026 22:28] ********************************************************************************
[30.01.2026 22:28] Abstract 39. Slot-Based Object-Centric Representations outperform global and dense feature representations in robotic manipulation tasks by providing better generalization under visual distribution shifts.  					AI-generated summary 				 The generalization capabilities of robotic manipulation policies are heavil...
[30.01.2026 22:28] ********************************************************************************
[30.01.2026 22:28] Abstract 40. WorldBench is introduced as a video-based benchmark for disentangled evaluation of physical reasoning in generative models, revealing specific failure patterns in current state-of-the-art video world models.  					AI-generated summary 				 Recent advances in generative foundational models, often ter...
[30.01.2026 22:28] ********************************************************************************
[30.01.2026 22:28] Abstract 41. STORM enhances robotic manipulation by adapting visual foundation models with semantic-aware slots through multi-phase training, improving generalization and control performance.  					AI-generated summary 				 Visual foundation models provide strong perceptual features for robotics, but their dense...
[30.01.2026 22:28] ********************************************************************************
[30.01.2026 22:28] Abstract 42. FlowBoost is a closed-loop generative framework that combines geometry-aware flow-matching, reward-guided policy optimization, and stochastic local search to efficiently discover extremal geometric structures with improved results over existing methods.  					AI-generated summary 				 The discovery ...
[30.01.2026 22:28] Read previous papers.
[30.01.2026 22:28] Generating reviews via LLM API.
[30.01.2026 22:28] Using data from previous issue: {"categories": ["#graphs", "#hallucinations", "#long_context", "#science"], "emoji": "üß¨", "ru": {"title": "–ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–µ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∑–Ω–∞–Ω–∏–π –¥–ª—è –Ω–∞–¥—ë–∂–Ω–æ–≥–æ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –Ω–∞—É—á–Ω–æ–≥–æ –æ—Ç–∫—Ä—ã—Ç–∏—è", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è Idea2Story ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –Ω–∞—É—á–Ω–æ–≥–æ –æ—Ç–∫—Ä—ã—Ç–∏—è, –∫–æ—Ç–æ—Ä—ã–π –ø–µ—Ä–µ–Ω–æ—Å–∏—Ç –æ–±—Ä–∞–±–æ
[30.01.2026 22:28] Using data from previous issue: {"categories": ["#cv", "#benchmark", "#synthetic", "#reasoning", "#dataset", "#training", "#multimodal"], "emoji": "üß©", "ru": {"title": "–ü—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –≤ text-to-image –º–æ–¥–µ–ª—è—Ö —á–µ—Ä–µ–∑ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ SpatialGenEval –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å
[30.01.2026 22:28] Using data from previous issue: {"categories": ["#architecture", "#inference", "#training", "#optimization", "#open_source"], "emoji": "‚ö°", "ru": {"title": "–≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –≤–º–µ—Å—Ç–æ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤: –Ω–æ–≤—ã–π –ø—É—Ç—å –∫ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–º—É –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—é", "desc": "–í —ç—Ç–æ–π —Ä–∞–±–æ—Ç–µ –∞–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∫–∞–∫ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—É —ç–∫—Å
[30.01.2026 22:28] Using data from previous issue: {"categories": ["#multimodal", "#dataset", "#benchmark", "#robotics", "#inference", "#architecture", "#transfer_learning", "#small_models", "#synthetic"], "emoji": "ü§ñ", "ru": {"title": "–ë—ã—Å—Ç—Ä–∞—è –∏ –∞–¥–∞–ø—Ç–∏–≤–Ω–∞—è –º–∞–Ω–∏–ø—É–ª—è—Ü–∏—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–º–∏ –æ–±—ä–µ–∫—Ç–∞–º–∏ —á–µ—Ä–µ–∑ –∫–æ–º–ø–∞–∫—Ç–Ω—ã–µ vision-language –º–æ–¥–µ–ª–∏", "desc": "Dynami
[30.01.2026 22:28] Using data from previous issue: {"categories": ["#data", "#cv", "#open_source", "#dataset", "#science", "#training", "#optimization", "#rl", "#multimodal"], "emoji": "üìÑ", "ru": {"title": "–£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è–º–∏", "desc": "OCRVerse –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π 
[30.01.2026 22:28] Using data from previous issue: {"categories": ["#dataset", "#reasoning", "#multimodal", "#open_source", "#training", "#small_models", "#synthetic"], "emoji": "üß†", "ru": {"title": "–í—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –¥–µ–ª–∞—é—Ç –º–∞–ª–µ–Ω—å–∫–∏–µ –º–æ–¥–µ–ª–∏ —É–º–Ω–µ–µ –±–æ–ª—å—à–∏—Ö", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ MMFineReason ‚Äî –±–æ–ª—å—à–æ–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –¥–∞—Ç
[30.01.2026 22:28] Using data from previous issue: {"categories": ["#inference", "#multimodal", "#architecture", "#training"], "emoji": "‚ö°", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–æ–∫–µ–Ω–æ–≤ —á–µ—Ä–µ–∑ –∫–æ–Ω—Ü–µ–ø—Ç—É–∞–ª—å–Ω–æ–µ —Å–ª–∏—è–Ω–∏–µ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π", "desc": "ConceptMoE ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö, –∫–æ
[30.01.2026 22:28] Using data from previous issue: {"categories": [], "emoji": "üé¨", "ru": {"title": "–†–∞–∑–¥–µ–ª—ë–Ω–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–µ–æ–º–µ—Ç—Ä–∏–∏ –∏ –≤–Ω–µ—à–Ω–µ–≥–æ –≤–∏–¥–∞ –¥–ª—è –±—ã—Å—Ç—Ä–æ–π —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ —Å—Ü–µ–Ω", "desc": "PLANING ‚Äî —ç—Ç–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –ø–æ—Ç–æ–∫–æ–≤–æ–π —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ —Å—Ü–µ–Ω –∏–∑ –º–æ–Ω–æ–∫—É–ª—è—Ä–Ω—ã—Ö –≤–∏–¥–µ–æ–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π, –∫–æ—Ç–æ—Ä–∞—è –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç —è–≤–Ω—ã–µ –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–∏—Ç–∏–≤—ã —Å –Ω
[30.01.2026 22:28] Using data from previous issue: {"categories": ["#benchmark", "#multilingual", "#inference", "#low_resource", "#audio", "#open_source"], "emoji": "üé§", "ru": {"title": "–ú–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏ —Å –Ω–µ–∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–º –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ–º", "desc": "–°–µ–º—å—è –º–æ–¥–µ–ª–µ–π Qwen3-ASR –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –Ω–∞–±–æ—Ä —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ—á
[30.01.2026 22:28] Using data from previous issue: {"categories": ["#benchmark", "#agents", "#long_context", "#dataset", "#reasoning"], "emoji": "üîÑ", "ru": {"title": "–ê–≥–µ–Ω—Ç—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM: –æ—Ç —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Å—Ç–æ–≤ –∫ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–º—É —Å–∏–Ω—Ç–µ–∑—É –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏", "desc": "AgentLongBench ‚Äî —ç—Ç–æ –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ —Ä–æ–ª–∏ –∞–≤—Ç–æ–Ω–æ–º–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ —á–µ
[30.01.2026 22:28] Using data from previous issue: {"categories": ["#dataset", "#reasoning", "#benchmark", "#open_source", "#rl", "#optimization", "#agents"], "emoji": "üéØ", "ru": {"title": "–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ–±—Ä–∞—Ç–Ω–∞—è —Å–≤—è–∑—å –¥–ª—è —É–º–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Agent-RRM, –º–Ω–æ–≥–æ–∞—Å–ø–µ–∫—Ç–Ω—É—é –º–æ–¥–µ–ª—å –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä–∞—è –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å
[30.01.2026 22:28] Using data from previous issue: {"categories": ["#reasoning", "#training", "#rl", "#optimization", "#transfer_learning", "#agents", "#small_models"], "emoji": "üîç", "ru": {"title": "–†–∞–∑–≤–µ–¥—á–∏–∫–∏ –¥–ª—è —Ä–∞–∑—É–º–Ω–æ–π —Ä–∞–∑–≤–µ–¥–∫–∏: –∫–∞–∫ –º–∞–ª–µ–Ω—å–∫–∏–µ –º–æ–¥–µ–ª–∏ —É—á–∞—Ç –±–æ–ª—å—à–∏—Ö", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ SCOUT, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ª—ë–≥–∫–∏—Ö –∞–≥–µ–Ω—Ç–æ–≤-
[30.01.2026 22:28] Using data from previous issue: {"categories": ["#long_context", "#inference", "#architecture", "#video", "#optimization"], "emoji": "üé¨", "ru": {"title": "–ë–µ—Å–∫–æ–Ω–µ—á–Ω–æ–µ –≤–∏–¥–µ–æ –±–µ–∑ –∫–æ–ª–ª–∞–ø—Å–∞: –ø–æ—Ç–æ–∫–æ–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –º–µ—Ç–æ–¥ –¥–ª—è —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º—ã sink-collapse –≤ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–π –≥–µ–Ω–µ—Ä–∞
[30.01.2026 22:28] Using data from previous issue: {"categories": ["#small_models", "#benchmark", "#inference"], "emoji": "üíé", "ru": {"title": "–ü–æ–∏—Å–∫ —Å–∫—Ä—ã—Ç—ã—Ö –∂–µ–º—á—É–∂–∏–Ω: –±—ã—Å—Ç—Ä–æ–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –º–Ω–æ–≥–æ—Ä—É–∫–∏–π –±–∞–Ω–¥–∏—Ç", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø–æ–∫–∞–∑–∞–Ω–æ, —á—Ç–æ –≤ –ø—É–±–ª–∏—á–Ω—ã—Ö —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è—Ö –Ω–∞—Ö–æ–¥—è—Ç—Å—è —Å–∫—Ä—ã—Ç—ã–µ –≤—ã—Å–æ–∫–æ–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏, –∫–æ—Ç–æ—Ä—ã–µ –æ—Å—Ç
[30.01.2026 22:28] Using data from previous issue: {"categories": [], "emoji": "üé≠", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ —Å–∫—Ä—ã—Ç—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –≤–º–µ—Å—Ç–æ —Ç–æ–∫–µ–Ω–æ–≤", "desc": "GANPO ‚Äî —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π —á–µ–ª–æ–≤–µ–∫–∞, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—é –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ —Å–∫—Ä—ã—Ç—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –≤–º–µ—Å—Ç–æ —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ –ø–æ
[30.01.2026 22:28] Using data from previous issue: {"categories": ["#rl", "#plp", "#reasoning", "#optimization", "#training", "#math"], "emoji": "üî•", "ru": {"title": "–ó–∞–æ—Å—Ç—Ä–µ–Ω–∏–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤–º–µ—Å—Ç–æ –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏—è: —ç–∫–æ–Ω–æ–º–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π LLM", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –∑–∞
[30.01.2026 22:28] Using data from previous issue: {"categories": ["#architecture", "#training", "#alignment", "#data", "#security"], "emoji": "üîç", "ru": {"title": "–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ç–æ–∫–µ–Ω–æ–≤: —É–¥–∞–ª–µ–Ω–∏–µ –Ω–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –≤–æ –≤—Ä–µ–º—è –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è", "desc": "–í —ç—Ç–æ–π —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –º–µ—Ç–æ–¥ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤ –Ω–∞ —ç—Ç–∞–ø–µ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è —É–¥–∞
[30.01.2026 22:28] Using data from previous issue: {"categories": ["#rlhf", "#dataset", "#reasoning", "#benchmark", "#open_source", "#training", "#science", "#small_models"], "emoji": "üîê", "ru": {"title": "–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ –≤ –∫–æ–º–ø–∞–∫—Ç–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ: —Ä–∞—Å—Å—É–∂–¥–∞—é—â–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –∫–∏–±–µ—Ä–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ Foundation-Sec-8B-Reasoning ‚Äî
[30.01.2026 22:28] Using data from previous issue: {"categories": ["#rlhf", "#reasoning", "#open_source", "#training", "#multilingual", "#optimization", "#low_resource", "#small_models"], "emoji": "üèõÔ∏è", "ru": {"title": "–°—É–≤–µ—Ä–µ–Ω–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –¥–æ–æ–±—É—á–µ–Ω–∏—é —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π,
[30.01.2026 22:28] Using data from previous issue: {"categories": ["#multimodal", "#reasoning", "#benchmark", "#open_source", "#training", "#long_context", "#inference"], "emoji": "üß†", "ru": {"title": "–û–ø—Ç–∏—á–µ—Å–∫–∞—è –ø–∞–º—è—Ç—å –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –≤—ã–≤–æ–¥–∞: —Å–∂–∞—Ç–∏–µ —á–µ—Ä–µ–∑ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é", "desc": "VTC-R1 ‚Äî –Ω–æ–≤–∞—è –ø–∞—Ä–∞–¥–∏–≥–º–∞ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä–∞
[30.01.2026 22:28] Using data from previous issue: {"categories": ["#multimodal", "#video", "#audio", "#inference"], "emoji": "üéØ", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –ø–æ–¥–∞–≤–ª–µ–Ω–∏—è –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–í —Å—Ç–∞—Ç—å–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ –∫—Ä–æ—Å—Å-–º–æ–¥–∞–ª—å–Ω—ã—Ö –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö, –∫–æ–≥–¥–∞ 
[30.01.2026 22:28] Using data from previous issue: {"categories": ["#dataset", "#agents", "#benchmark"], "emoji": "üîç", "ru": {"title": "–ì–ª—É–±–æ–∫–∏–π –ø–æ–∏—Å–∫ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏: –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –º–Ω–æ–≥–æ—à–∞–≥–æ–≤–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤", "desc": "–†–∞–±–æ—Ç–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –±–µ–Ω—á–º–∞—Ä–∫ DeepSearchQA —Å 900 –∑–∞–¥–∞—á–∞–º–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ —Å–ª–æ–∂–Ω—ã—Ö –º–Ω–æ–≥–æ—à–∞–≥–æ–≤—ã—Ö –∑–∞–¥–∞—á–∞—Ö –ø–æ–∏—Å–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏. –ë–µ–Ω
[30.01.2026 22:28] Using data from previous issue: {"categories": ["#reasoning", "#training", "#rl", "#optimization", "#small_models"], "emoji": "üß†", "ru": {"title": "–ê–∫—Ç–∏–≤–Ω–æ–µ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ —Å–∫—Ä—ã—Ç–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –º–µ—Ç–æ–¥ –∞–∫—Ç–∏–≤–Ω–æ–≥–æ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –≤ —Å–∫—Ä—ã—Ç–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è —É–ª—É
[30.01.2026 22:28] Using data from previous issue: {"categories": ["#diffusion", "#cv", "#architecture", "#optimization", "#training"], "emoji": "‚ö°", "ru": {"title": "–û–¥–Ω–æ—à–∞–≥–æ–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –±–µ–∑ —Å–∫—Ä—ã—Ç–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ —á–µ—Ä–µ–∑ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –≤—ã—Ö–æ–¥–æ–≤ –∏ –ø–æ—Ç–µ—Ä—å", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –º–µ—Ç–æ–¥ Pixel MeanFlow –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∑–∞ –æ–¥–∏–Ω —à–∞–≥
[30.01.2026 22:28] Using data from previous issue: {"categories": ["#long_context", "#transfer_learning", "#architecture", "#optimization", "#training", "#inference"], "emoji": "‚ö°", "ru": {"title": "–ì–∏–±—Ä–∏–¥–Ω—ã–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã: —É—Å–∫–æ—Ä–µ–Ω–∏–µ –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ —á–µ—Ä–µ–∑ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—é –≤ RNN —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω HALO ‚Äî –º–µ—Ç–æ–¥ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –¥
[30.01.2026 22:28] Using data from previous issue: {"categories": ["#synthetic", "#open_source"], "emoji": "üîÑ", "ru": {"title": "–°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –≤–º–µ—Å—Ç–æ —Ç–µ–∫—Å—Ç–æ–≤: –Ω–æ–≤—ã–π –ø—É—Ç—å –∫ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–º—É –æ–±—É—á–µ–Ω–∏—é —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ —Å–æ–∑–¥–∞–Ω–∏—è –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –ø–∞—Ä –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è-–æ—Ç–≤–µ—Ç –∏–∑ –Ω–µ—Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∫–æ—Ä–ø—É
[30.01.2026 22:28] Using data from previous issue: {"categories": ["#training", "#architecture"], "emoji": "üìâ", "ru": {"title": "–ö—Ä–æ–Ω–µ–∫–µ—Ä–æ–≤—ã –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –ø–∞—Ä–∞–º–µ—Ç—Ä–∏–∑–∞—Ü–∏–∏ –≥–∏–ø–µ—Ä-—Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ —É–ª—É—á—à–µ–Ω–∏—é –≥–∏–ø–µ—Ä-—Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π –≤ –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç—è—Ö, –∫–æ—Ç–æ—Ä—ã–µ —Å—Ç—Ä–∞–¥–∞—é—Ç –æ—Ç –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–π –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç–∏
[30.01.2026 22:28] Using data from previous issue: {"categories": ["#rl", "#training", "#rlhf"], "emoji": "üõ°Ô∏è", "ru": {"title": "–ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –∏ —Ñ–∞–∫–æ—Ç–∏—á–Ω–æ—Å—Ç—å –≤–æ –≤—Ä–µ–º—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —á–µ—Ä–µ–∑ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –æ–±—É—á–µ–Ω–∏–∏ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –∫–æ—Ç
[30.01.2026 22:28] Using data from previous issue: {"categories": ["#inference", "#optimization", "#training", "#architecture"], "emoji": "‚ö°", "ru": {"title": "–û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä —Å –∫–æ–º–ø–µ–Ω—Å–∞—Ü–∏–µ–π –æ—à–∏–±–æ–∫: –æ–±—É—á–µ–Ω–∏–µ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω—ã—Ö LLM –±–µ–∑ –ø–æ—Ç–µ—Ä—å –ø–∞–º—è—Ç–∏", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è Error-Compensating Optimizer (ECO) ‚Äî –Ω–æ–≤—ã–π –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä, –∫–æ—Ç–æ—Ä—ã–π —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É
[30.01.2026 22:28] Using data from previous issue: {"categories": ["#3d", "#cv", "#architecture", "#transfer_learning", "#open_source", "#dataset", "#optimization", "#multimodal"], "emoji": "üìè", "ru": {"title": "–ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞–∫ –ø—É—Ç—å –∫ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–º—É –≤–æ—Å–ø—Ä–∏—è—Ç–∏—é –º–µ—Ç—Ä–∏—á–µ—Å–∫–æ–π –≥–ª—É–±–∏–Ω—ã", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Metric Anything ‚Äî –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—É—é —Å–∏
[30.01.2026 22:28] Using data from previous issue: {"categories": ["#synthetic", "#architecture", "#data", "#training", "#interpretability"], "emoji": "üîç", "ru": {"title": "–û—Ç –¥–∞–Ω–Ω—ã—Ö –∫ —Å—Ö–µ–º–∞–º: –ø—Ä–∏—á–∏–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏—Å—Ö–æ–∂–¥–µ–Ω–∏—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–∞—è –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è Mechanistic Data Attribution
[30.01.2026 22:28] Using data from previous issue: {"categories": ["#multimodal", "#cv", "#training"], "emoji": "üîÑ", "ru": {"title": "–í–∑–∞–∏–º–Ω–æ–µ —É—Å–∏–ª–µ–Ω–∏–µ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è", "desc": "UniMRG ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥ –ø–æ—Å—Ç–æ–±—É—á–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –ø—É—Ç—ë–º –æ–±—É—á–µ–Ω–∏—è –∏—Ö –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤—Å
[30.01.2026 22:28] Using data from previous issue: {"categories": ["#architecture", "#benchmark", "#reasoning", "#long_context", "#agents"], "emoji": "üß†", "ru": {"title": "–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –ø–∞–º—è—Ç—å –∫–∞–∫ –∫–ª—é—á –∫ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –ø–∞–º—è—Ç–∏ BMAM, –≤–¥–æ—Ö–Ω–æ–≤–ª–µ–Ω–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π –º–æ–∑–≥–∞, –∫–æ—Ç–æ—Ä–∞—è —Ä–∞–∑–¥–µ–ª—è–µ—Ç –ø–∞–º
[30.01.2026 22:28] Using data from previous issue: {"categories": ["#synthetic", "#training", "#multimodal", "#audio", "#machine_translation", "#video", "#diffusion"], "emoji": "üé¨", "ru": {"title": "–õ—ë–≥–∫–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è –∞—É–¥–∏–æ-–≤–∏–¥–µ–æ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–≥–æ –¥—É–±–ª—è–∂–∞", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –º–µ—Ç–æ–¥ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–æ–π –º
[30.01.2026 22:28] Using data from previous issue: {"categories": ["#optimization", "#open_source", "#reasoning"], "emoji": "‚úÇÔ∏è", "ru": {"title": "–£–º–Ω–æ–µ –æ–±—Ä–µ–∑–∞–Ω–∏–µ —Ü–µ–ø–æ—á–µ–∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —á–µ—Ä–µ–∑ –∞–Ω–∞–ª–∏–∑ –≤–Ω–∏–º–∞–Ω–∏—è", "desc": "FROST ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –∞–Ω–∞–ª–∏–∑–µ –≤–µ—Å–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è (attention weights) –≤ –Ω–µ–π—Ä–æ—Å–µ—Ç—è—Ö. –ü–æ–¥—Ö–æ–¥ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ 
[30.01.2026 22:28] Using data from previous issue: {"categories": ["#dataset", "#plp", "#rl", "#benchmark"], "emoji": "üîç", "ru": {"title": "–û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è —á–µ—Ä–µ–∑ –∫–æ–Ω—Ç—Ä–∞—Å—Ç–∏–≤–Ω—ã–µ –∞–Ω–æ–º–∞–ª–∏–∏ –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ TRACE –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–π —Å —Ñ—É–Ω–∫—Ü–∏–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –≤ —Å—Ä–µ–¥–µ –≥–µ
[30.01.2026 22:28] Using data from previous issue: {"categories": ["#audio", "#architecture", "#benchmark"], "emoji": "üîç", "ru": {"title": "–û–ø—Ç–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Å–µ–≥–º–µ–Ω—Ç–∞: –∫–ª—é—á –∫ —Ç–æ—á–Ω–æ—Å—Ç–∏ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤—ã—Ö –∞—É–¥–∏–æ—Ñ–∏–Ω–≥–µ—Ä–ø—Ä–∏–Ω—Ç–æ–≤", "desc": "–í —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –≤–ª–∏—è–Ω–∏–µ –¥–ª–∏–Ω—ã –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –æ–∫–Ω–∞ –Ω–∞ –∫–∞—á–µ—Å—Ç–≤–æ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤—ã—Ö –∞—É–¥–∏–æ—Ñ–∏–Ω–≥–µ—Ä–ø—Ä–∏–Ω—Ç–æ–≤ ‚Äî –∫–æ–º–ø–∞–∫—Ç–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –∑–≤
[30.01.2026 22:28] Using data from previous issue: {"categories": [], "emoji": "üé®", "ru": {"title": "–ó–Ω–∞–Ω–∏—è –∏–∑ –¥–∞–Ω–Ω—ã—Ö –¥–∏–∑–∞–π–Ω–∞ –¥–ª—è —Å—Ç–∏–ª–∏—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ —É–ª—É—á—à–µ–Ω–∏—è –ø–æ —Ç–µ–∫—Å—Ç—É", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ PRISM –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏—Ö –¥–∏–∑–∞–π–Ω–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞, –∫–æ—Ç–æ—Ä—ã–π —Å–æ–∑–¥–∞—ë—Ç –±–∞–∑—É –∑–Ω–∞–Ω–∏–π –∏–∑ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∏–∑–∞–π–Ω-–¥–∞–Ω–Ω—ã—Ö. –ê–≤—Ç–æ—Ä—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ
[30.01.2026 22:28] Using data from previous issue: {"categories": ["#dataset", "#reasoning", "#rlhf", "#benchmark", "#open_source", "#interpretability", "#agents"], "emoji": "üï∑Ô∏è", "ru": {"title": "–†–∞—Å—Å—É–∂–¥–∞—é—â–∞—è –º–æ–¥–µ–ª—å –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–π –¥–ª—è –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–π –≤–µ–±-–Ω–∞–≤–∏–≥–∞—Ü–∏–∏", "desc": "WebArbiter –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—é –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–π –¥–ª—è –≤
[30.01.2026 22:28] Using data from previous issue: {"categories": ["#benchmark", "#cv", "#robotics"], "emoji": "ü§ñ", "ru": {"title": "–û–±—ä–µ–∫—Ç–Ω–æ-—Ü–µ–Ω—Ç—Ä–∏—á–µ—Å–∫–∏–µ —Å–ª–æ—Ç—ã ‚Äî –∫–ª—é—á –∫ –Ω–∞–¥—ë–∂–Ω–æ–π –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–∏ —Ä–æ–±–æ—Ç–æ–≤ –ø—Ä–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–∏ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —É—Å–ª–æ–≤–∏–π", "desc": "–í —ç—Ç–æ–π —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ –≤—ã–±–æ—Ä–∞ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –ø–æ–ª–∏—Ç–∏–∫ –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–∏ —Ä–æ–±–æ—Ç–æ–≤.
[30.01.2026 22:28] Using data from previous issue: {"categories": ["#benchmark", "#video", "#robotics"], "emoji": "üé¨", "ru": {"title": "–†–∞–∑–¥–µ–ª—ë–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —Ñ–∏–∑–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –≤–∏–¥–µ–æ-–≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω WorldBench ‚Äî –≤–∏–¥–µ–æ-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è —Ä–∞–∑–¥–µ–ª—ë–Ω–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ —Ñ–∏–∑–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª
[30.01.2026 22:28] Using data from previous issue: {"categories": ["#training", "#cv", "#robotics", "#architecture"], "emoji": "ü§ñ", "ru": {"title": "–û–±—ä–µ–∫—Ç–Ω–æ-—Ü–µ–Ω—Ç—Ä–∏—á–Ω–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∏ —á–µ—Ä–µ–∑ –º–Ω–æ–≥–æ—Ñ–∞–∑–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è STORM ‚Äî –ª—ë–≥–∫–∏–π –∞–¥–∞–ø—Ç–∞—Ü–∏–æ–Ω–Ω—ã–π –º–æ–¥—É–ª—å, –∫–æ—Ç–æ—Ä—ã–π –¥–æ–ø–æ–ª–Ω—è–µ—Ç –∑–∞–º–æ—Ä–æ–∂–µ–Ω–Ω—ã–µ –≤–∏–∑—É–∞–ª—å
[30.01.2026 22:28] Using data from previous issue: {"categories": ["#rl", "#architecture", "#math", "#training"], "emoji": "üåü", "ru": {"title": "–ó–∞–º–∫–Ω—É—Ç–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã—Ö —Å—Ç—Ä—É–∫—Ç—É—Ä —á–µ—Ä–µ–∑ –ø–æ—Ç–æ–∫–æ–≤–æ–µ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ", "desc": "FlowBoost –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –∑–∞–º–∫–Ω—É—Ç—É—é —Å–∏—Å—Ç–µ–º—É –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –ø–æ–∏—Å–∫–∞ —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã—Ö –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏—Ö —Å—Ç—Ä—É–∫—Ç—É
[30.01.2026 22:28] Renaming data file.
[30.01.2026 22:28] Renaming previous data. hf_papers.json to ./d/2026-01-30.json
[30.01.2026 22:28] Saving new data file.
[30.01.2026 22:28] Generating page.
[30.01.2026 22:28] Renaming previous page.
[30.01.2026 22:28] Renaming previous data. index.html to ./d/2026-01-30.html
[30.01.2026 22:28] Writing result.
[30.01.2026 22:28] Renaming log file.
[30.01.2026 22:28] Renaming previous data. log.txt to ./logs/2026-01-30_last_log.txt
