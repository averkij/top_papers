[30.01.2026 06:53] Read previous papers.
[30.01.2026 06:53] Generating top page (month).
[30.01.2026 06:53] Writing top page (month).
[30.01.2026 07:41] Read previous papers.
[30.01.2026 07:41] Get feed.
[30.01.2026 07:41] Extract page data from URL. URL: https://huggingface.co/papers/2601.20354
[30.01.2026 07:41] Get page data from previous paper. URL: https://huggingface.co/papers/2601.20833
[30.01.2026 07:41] Extract page data from URL. URL: https://huggingface.co/papers/2601.21639
[30.01.2026 07:41] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22153
[30.01.2026 07:41] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21821
[30.01.2026 07:41] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21420
[30.01.2026 07:41] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21754
[30.01.2026 07:41] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21337
[30.01.2026 07:41] Get page data from previous paper. URL: https://huggingface.co/papers/2601.18129
[30.01.2026 07:41] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22154
[30.01.2026 07:41] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21181
[30.01.2026 07:41] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22069
[30.01.2026 07:41] Extract page data from URL. URL: https://huggingface.co/papers/2601.22054
[30.01.2026 07:41] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21598
[30.01.2026 07:41] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21406
[30.01.2026 07:41] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21343
[30.01.2026 07:41] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21204
[30.01.2026 07:41] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21051
[30.01.2026 07:41] Get page data from previous paper. URL: https://huggingface.co/papers/2601.20975
[30.01.2026 07:41] Extract page data from URL. URL: https://huggingface.co/papers/2601.20465
[30.01.2026 07:41] Get page data from previous paper. URL: https://huggingface.co/papers/2601.17690
[30.01.2026 07:41] Get page data from previous paper. URL: https://huggingface.co/papers/2601.11747
[30.01.2026 07:41] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21872
[30.01.2026 07:41] Extract page data from URL. URL: https://huggingface.co/papers/2601.21416
[30.01.2026 07:41] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21282
[30.01.2026 07:41] Extract page data from URL. URL: https://huggingface.co/papers/2601.20381
[30.01.2026 07:41] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[30.01.2026 07:41] No deleted papers detected.
[30.01.2026 07:41] Downloading and parsing papers (pdf, html). Total: 26.
[30.01.2026 07:41] Downloading and parsing paper https://huggingface.co/papers/2601.20354.
[30.01.2026 07:41] Downloading paper 2601.20354 from https://arxiv.org/pdf/2601.20354v2...
[30.01.2026 07:41] Extracting affiliations from text.
[30.01.2026 07:41] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 9 2 ] . [ 2 4 5 3 0 2 . 1 0 6 2 : r Published as conference paper at ICLR 2026 EVERYTHING IN ITS PLACE: BENCHMARKING SPATIAL INTELLIGENCE OF TEXT-TO-IMAGE MODELS Zengbin Wang1,2, Xuecai Hu1 , Yong Wang1 , Feng Xiong1, Man Zhang2, Xiangxiang Chu1 1AMAP, Alibaba Group, 2Beijing University of Posts and Telecommunications (cid:135) Project Page: https://github.com/AMAP-ML/SpatialGenEval "
[30.01.2026 07:41] Response: ```python
[
    "AMAP, Alibaba Group",
    "Beijing University of Posts and Telecommunications"
]
```
[30.01.2026 07:41] Deleting PDF ./assets/pdf/2601.20354.pdf.
[30.01.2026 07:41] Success.
[30.01.2026 07:41] Downloading and parsing paper https://huggingface.co/papers/2601.20833.
[30.01.2026 07:41] Extra JSON file exists (./assets/json/2601.20833.json), skip PDF parsing.
[30.01.2026 07:41] Paper image links file exists (./assets/img_data/2601.20833.json), skip HTML parsing.
[30.01.2026 07:41] Success.
[30.01.2026 07:41] Downloading and parsing paper https://huggingface.co/papers/2601.21639.
[30.01.2026 07:41] Downloading paper 2601.21639 from https://arxiv.org/pdf/2601.21639v1...
[30.01.2026 07:41] Extracting affiliations from text.
[30.01.2026 07:41] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 9 2 ] . [ 1 9 3 6 1 2 . 1 0 6 2 : r OCRVerse: Towards Holistic OCR in End-to-End Vision-Language Models Yufeng Zhong1, Lei Chen1, Xuanle Zhao1, Wenkang Han1, Liming Zheng1, Jing Huang1, Deyang Jiang1, Yilin Cao1, Lin Ma1, Zhixiong Zeng1 1Meituan, forest.linma@gmail.com, zengzhixiong@meituan.com Figure 1: Performance comparison of OCRVerse on text-centric OCR tasks (top row) and visioncentric OCR tasks (bottom row). Since existing OCR methods primarily focus on text-centric scenarios, we compare against both specialized OCR models and general-purpose models for text-centric benchmarks, while comparing only against general-purpose models for vision-centric benchmarks. "
[30.01.2026 07:41] Response: ```python
["Meituan"]
```
[30.01.2026 07:41] Deleting PDF ./assets/pdf/2601.21639.pdf.
[30.01.2026 07:41] Success.
[30.01.2026 07:41] Downloading and parsing paper https://huggingface.co/papers/2601.22153.
[30.01.2026 07:41] Extra JSON file exists (./assets/json/2601.22153.json), skip PDF parsing.
[30.01.2026 07:41] Paper image links file exists (./assets/img_data/2601.22153.json), skip HTML parsing.
[30.01.2026 07:41] Success.
[30.01.2026 07:41] Downloading and parsing paper https://huggingface.co/papers/2601.21821.
[30.01.2026 07:41] Extra JSON file exists (./assets/json/2601.21821.json), skip PDF parsing.
[30.01.2026 07:41] Paper image links file exists (./assets/img_data/2601.21821.json), skip HTML parsing.
[30.01.2026 07:41] Success.
[30.01.2026 07:41] Downloading and parsing paper https://huggingface.co/papers/2601.21420.
[30.01.2026 07:41] Extra JSON file exists (./assets/json/2601.21420.json), skip PDF parsing.
[30.01.2026 07:41] Paper image links file exists (./assets/img_data/2601.21420.json), skip HTML parsing.
[30.01.2026 07:41] Success.
[30.01.2026 07:41] Downloading and parsing paper https://huggingface.co/papers/2601.21754.
[30.01.2026 07:41] Extra JSON file exists (./assets/json/2601.21754.json), skip PDF parsing.
[30.01.2026 07:41] Paper image links file exists (./assets/img_data/2601.21754.json), skip HTML parsing.
[30.01.2026 07:41] Success.
[30.01.2026 07:41] Downloading and parsing paper https://huggingface.co/papers/2601.21337.
[30.01.2026 07:41] Extra JSON file exists (./assets/json/2601.21337.json), skip PDF parsing.
[30.01.2026 07:41] Paper image links file exists (./assets/img_data/2601.21337.json), skip HTML parsing.
[30.01.2026 07:41] Success.
[30.01.2026 07:41] Downloading and parsing paper https://huggingface.co/papers/2601.18129.
[30.01.2026 07:41] Extra JSON file exists (./assets/json/2601.18129.json), skip PDF parsing.
[30.01.2026 07:41] Paper image links file exists (./assets/img_data/2601.18129.json), skip HTML parsing.
[30.01.2026 07:41] Success.
[30.01.2026 07:41] Downloading and parsing paper https://huggingface.co/papers/2601.22154.
[30.01.2026 07:41] Extra JSON file exists (./assets/json/2601.22154.json), skip PDF parsing.
[30.01.2026 07:41] Paper image links file exists (./assets/img_data/2601.22154.json), skip HTML parsing.
[30.01.2026 07:41] Success.
[30.01.2026 07:41] Downloading and parsing paper https://huggingface.co/papers/2601.21181.
[30.01.2026 07:41] Extra JSON file exists (./assets/json/2601.21181.json), skip PDF parsing.
[30.01.2026 07:41] Paper image links file exists (./assets/img_data/2601.21181.json), skip HTML parsing.
[30.01.2026 07:41] Success.
[30.01.2026 07:41] Downloading and parsing paper https://huggingface.co/papers/2601.22069.
[30.01.2026 07:41] Extra JSON file exists (./assets/json/2601.22069.json), skip PDF parsing.
[30.01.2026 07:41] Paper image links file exists (./assets/img_data/2601.22069.json), skip HTML parsing.
[30.01.2026 07:41] Success.
[30.01.2026 07:41] Downloading and parsing paper https://huggingface.co/papers/2601.22054.
[30.01.2026 07:41] Downloading paper 2601.22054 from https://arxiv.org/pdf/2601.22054v1...
[30.01.2026 07:41] Extracting affiliations from text.
[30.01.2026 07:41] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"MetricAnything: Scaling Metric Depth Pretraining with Noisy Heterogeneous Sources Baorui Ma, Jiahui Yang, Donglin Di, Xuancheng Zhang, Jianxun Cui, Hao Li, Xie Yan, Wei Chen Li Auto Inc Abstract Scaling has powered recent advances in vision foundation models, yet extending this paradigm to metric depth estimation remains challenging due to heterogeneous sensor noise, cameradependent biases, and metric ambiguity in noisy cross-source 3D data. We introduce Metric Anything, simple and scalable pretraining framework that learns metric depth from noisy, diverse 3D sources without manually engineered prompts, camera-specific modeling, or taskspecific architectures. Central to our approach is the Sparse Metric Prompt, created by randomly masking depth maps, which serves as universal interface that decouples spatial reasoning from sensor and camera biases. Using 20M imagedepth pairs spanning reconstructed, captured, and rendered 3D data across 10,000+ camera models, we demonstratefor the first timea clear scaling trend in the metric depth track. The pretrained model excels at promptdriven tasks such as depth completion, super-resolution and Radar-camera fusion, while its distilled prompt-free student achieves state-of-the-art results on monocular depth estimation, camera intrinsics recovery, single/multi-view metric 3D reconstruction, and VLA planning. We also show that using pretrained ViT of Metric Anything as visual encoder significantly boosts Multimodal Large Language Model capabilities in spatial intelligence. These results show that metric depth estimation can benefit from the same scaling laws that drive modern foundation models, establishing new path toward scalable and efficient real-world metric perception. We opensource MetricAnything at https://metric-anything.github.io/metric-anything-io/ to support community research. 6 2 0 2 9 ] . [ 1 4 5 0 2 2 . 1 0 6 2 : r Figure 1: Overview of Metric Anything. (I) We aggregate diverse open-source 3D data into per-pixel me"
[30.01.2026 07:41] Response: ```python
["Li Auto Inc"]
```
[30.01.2026 07:41] Deleting PDF ./assets/pdf/2601.22054.pdf.
[30.01.2026 07:41] Success.
[30.01.2026 07:41] Downloading and parsing paper https://huggingface.co/papers/2601.21598.
[30.01.2026 07:41] Extra JSON file exists (./assets/json/2601.21598.json), skip PDF parsing.
[30.01.2026 07:41] Paper image links file exists (./assets/img_data/2601.21598.json), skip HTML parsing.
[30.01.2026 07:41] Success.
[30.01.2026 07:41] Downloading and parsing paper https://huggingface.co/papers/2601.21406.
[30.01.2026 07:41] Extra JSON file exists (./assets/json/2601.21406.json), skip PDF parsing.
[30.01.2026 07:41] Paper image links file exists (./assets/img_data/2601.21406.json), skip HTML parsing.
[30.01.2026 07:41] Success.
[30.01.2026 07:41] Downloading and parsing paper https://huggingface.co/papers/2601.21343.
[30.01.2026 07:41] Extra JSON file exists (./assets/json/2601.21343.json), skip PDF parsing.
[30.01.2026 07:41] Paper image links file exists (./assets/img_data/2601.21343.json), skip HTML parsing.
[30.01.2026 07:41] Success.
[30.01.2026 07:41] Downloading and parsing paper https://huggingface.co/papers/2601.21204.
[30.01.2026 07:41] Extra JSON file exists (./assets/json/2601.21204.json), skip PDF parsing.
[30.01.2026 07:41] Paper image links file exists (./assets/img_data/2601.21204.json), skip HTML parsing.
[30.01.2026 07:41] Success.
[30.01.2026 07:41] Downloading and parsing paper https://huggingface.co/papers/2601.21051.
[30.01.2026 07:41] Extra JSON file exists (./assets/json/2601.21051.json), skip PDF parsing.
[30.01.2026 07:41] Paper image links file exists (./assets/img_data/2601.21051.json), skip HTML parsing.
[30.01.2026 07:41] Success.
[30.01.2026 07:41] Downloading and parsing paper https://huggingface.co/papers/2601.20975.
[30.01.2026 07:41] Extra JSON file exists (./assets/json/2601.20975.json), skip PDF parsing.
[30.01.2026 07:41] Paper image links file exists (./assets/img_data/2601.20975.json), skip HTML parsing.
[30.01.2026 07:41] Success.
[30.01.2026 07:41] Downloading and parsing paper https://huggingface.co/papers/2601.20465.
[30.01.2026 07:41] Downloading paper 2601.20465 from https://arxiv.org/pdf/2601.20465v1...
[30.01.2026 07:41] Extracting affiliations from text.
[30.01.2026 07:41] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"BMAM: Brain-inspired Multi-Agent Memory Framework Jiaxiang Liu1 Yusong Wang2 Yujie Wu3 Mingkun Xu1* Yang Li1 1 Guangdong Institute of Intelligence Science and Technology, Zhuhai, China 2 Institute of Science Tokyo 3 The Hong Kong Polytechnic University 6 2 0 2 8 2 ] . [ 1 5 6 4 0 2 . 1 0 6 2 : r a "
[30.01.2026 07:41] Response: ```python
[
    "Guangdong Institute of Intelligence Science and Technology, Zhuhai, China",
    "Institute of Science Tokyo",
    "The Hong Kong Polytechnic University"
]
```
[30.01.2026 07:41] Deleting PDF ./assets/pdf/2601.20465.pdf.
[30.01.2026 07:41] Success.
[30.01.2026 07:41] Downloading and parsing paper https://huggingface.co/papers/2601.17690.
[30.01.2026 07:41] Extra JSON file exists (./assets/json/2601.17690.json), skip PDF parsing.
[30.01.2026 07:41] Paper image links file exists (./assets/img_data/2601.17690.json), skip HTML parsing.
[30.01.2026 07:41] Success.
[30.01.2026 07:41] Downloading and parsing paper https://huggingface.co/papers/2601.11747.
[30.01.2026 07:41] Extra JSON file exists (./assets/json/2601.11747.json), skip PDF parsing.
[30.01.2026 07:41] Paper image links file exists (./assets/img_data/2601.11747.json), skip HTML parsing.
[30.01.2026 07:41] Success.
[30.01.2026 07:41] Downloading and parsing paper https://huggingface.co/papers/2601.21872.
[30.01.2026 07:41] Extra JSON file exists (./assets/json/2601.21872.json), skip PDF parsing.
[30.01.2026 07:41] Paper image links file exists (./assets/img_data/2601.21872.json), skip HTML parsing.
[30.01.2026 07:41] Success.
[30.01.2026 07:41] Downloading and parsing paper https://huggingface.co/papers/2601.21416.
[30.01.2026 07:41] Downloading paper 2601.21416 from https://arxiv.org/pdf/2601.21416v1...
[30.01.2026 07:42] Extracting affiliations from text.
[30.01.2026 07:42] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 9 2 ] . [ 1 6 1 4 1 2 . 1 0 6 2 : r Spotlighting Task-Relevant Features: Object-Centric Representations for Better Generalization in Robotic Manipulation Alexandre Chapin1, Bruno Machado1, Emmanuel Dellandrea1, Liming Chen1 Abstract The ability of visuomotor policies to generalize across tasks and environments critically depends on the structure of the underlying visual representations. While most state-of-the-art robot policies rely on either global or dense these approaches features from pre-trained vision models, often entangle task-relevant and irrelevant signals, limiting their robustness to visual distribution shifts. In this paper, we investigate Slot-based Object-Centric Representations (SOCRs) as structured alternative that decomposes scenes into finite set of object-like entities. We present the first large-scale, systematic comparison of global, dense, and object-centric representations across diverse simulated (METAWORLD, LIBERO) and realworld robotic manipulation tasks. Our results show that SOCRs enable superior policy performance and significantly improve generalization under shifts in lighting, texture, and clutter even without task-specific tuning. We further demonstrate that pretraining SOCRs on robotic video datasets amplifies these benefits. Our findings highlight the importance of structured visual abstraction in robotic perception and suggest that SOCRs offer promising pathway toward more robust and generalizable manipulation policies. I. INTRODUCTION visual-motor policy (or visuomotor policy) tells robot what to do based on what it sees given goal, mapping raw visual inputstypically camera imagesto motor actions in robotic system. State-of-the-art approaches for visuomotor policy learning are data-driven based on deep learning, e.g. imitation learning (IL), in leveraging few expert demonstrations without the need for explicit programming. As such, the visual representation of the robot visual input is key to the robots generalization c"
[30.01.2026 07:42] Response: ```python
["Ã‰cole Centrale de Lyon"]
```
[30.01.2026 07:42] Deleting PDF ./assets/pdf/2601.21416.pdf.
[30.01.2026 07:42] Success.
[30.01.2026 07:42] Downloading and parsing paper https://huggingface.co/papers/2601.21282.
[30.01.2026 07:42] Extra JSON file exists (./assets/json/2601.21282.json), skip PDF parsing.
[30.01.2026 07:42] Paper image links file exists (./assets/img_data/2601.21282.json), skip HTML parsing.
[30.01.2026 07:42] Success.
[30.01.2026 07:42] Downloading and parsing paper https://huggingface.co/papers/2601.20381.
[30.01.2026 07:42] Downloading paper 2601.20381 from https://arxiv.org/pdf/2601.20381v1...
[30.01.2026 07:42] Extracting affiliations from text.
[30.01.2026 07:42] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 8 2 ] . [ 1 1 8 3 0 2 . 1 0 6 2 : r STORM: Slot-based Task-aware Object-centric Representation for robotic Manipulation Alexandre Chapin1[0009000037799387], Emmanuel Dellandrea1[000000017346228X], and Liming Chen1[0000000236549498] Ecole Centrale de Lyon {first-name.surname}@ec-lyon.fr Abstract. Visual foundation models provide strong perceptual features for robotics, but their dense representations lack explicit object-level structure, limiting robustness and controllability in manipulation tasks. We propose STORM (Slot-based Task-aware Object-centric Representation for robotic Manipulation), lightweight object-centric adaptation module that augments frozen visual foundation models with small set of semantic-aware slots for robotic manipulation. Rather than retraining large backbones, STORM employs multi-phase training strategy: object-centric slots are first stabilized through visualsemantic pretraining using language embeddings, then jointly adapted with downstream manipulation policy. This staged learning prevents degenerate slot formation and preserves semantic consistency while aligning perception with task objectives. Experiments on object discovery benchmarks and simulated manipulation tasks show that STORM improves generalization to visual distractors, and control performance compared to directly using frozen foundation model features or training object-centric representations end-to-end. Our results highlight multi-phase adaptation as an efficient mechanism for transforming generic foundation model features into task-aware object-centric representations for robotic control. Keywords: Object-centric representation Robotics Semantic aware. Robotic manipulation requires perceptual representations that capture fine-grained spatial structure while remaining interpretable and controllable by high-level task specifications. Recent visual foundation models such as DINOv2 [26] provide powerful dense feature representations that generalize across domains, bu"
[30.01.2026 07:42] Response: ```python
["Ecole Centrale de Lyon"]
```
[30.01.2026 07:42] Deleting PDF ./assets/pdf/2601.20381.pdf.
[30.01.2026 07:42] Success.
[30.01.2026 07:42] Enriching papers with extra data.
[30.01.2026 07:42] ********************************************************************************
[30.01.2026 07:42] Abstract 0. A new benchmark and dataset are introduced to evaluate and improve spatial reasoning capabilities in text-to-image models through information-dense prompts and fine-tuning.  					AI-generated summary 				 Text-to-image (T2I) models have achieved remarkable success in generating high-fidelity images,...
[30.01.2026 07:42] ********************************************************************************
[30.01.2026 07:42] Abstract 1. Offline knowledge construction through structured methodological graphs enables more reliable and scalable autonomous scientific discovery by reducing reliance on real-time literature processing.  					AI-generated summary 				 Autonomous scientific discovery with large language model (LLM)-based ag...
[30.01.2026 07:42] ********************************************************************************
[30.01.2026 07:42] Abstract 2. OCRVerse is a novel end-to-end OCR method that unifies text-centric and vision-centric approaches through comprehensive data engineering and a two-stage SFT-RL training framework with domain-specific reward strategies.  					AI-generated summary 				 The development of large vision language models d...
[30.01.2026 07:42] ********************************************************************************
[30.01.2026 07:42] Abstract 3. DynamicVLA addresses dynamic object manipulation challenges through a compact vision-language-action model with temporal reasoning and closed-loop adaptation, supported by a new benchmark for dynamic manipulation tasks.  					AI-generated summary 				 Manipulating dynamic objects remains an open cha...
[30.01.2026 07:42] ********************************************************************************
[30.01.2026 07:42] Abstract 4. A large-scale multimodal reasoning dataset called MMFineReason is introduced to improve vision language models' performance through high-quality reasoning annotations and demonstrates superior parameter efficiency in fine-tuned models.  					AI-generated summary 				 Recent advances in Vision Langua...
[30.01.2026 07:42] ********************************************************************************
[30.01.2026 07:42] Abstract 5. ConceptMoE dynamically allocates computation by merging similar tokens into concept representations, improving both performance and efficiency in large language models through adaptive processing and reduced attention computation.  					AI-generated summary 				 Large language models allocate unifor...
[30.01.2026 07:42] ********************************************************************************
[30.01.2026 07:42] Abstract 6. A novel framework called SCOUT is introduced that uses lightweight scouts to reduce exploration costs for large language models in nonlinguistic environments, enabling improved performance through supervised fine-tuning and reinforcement learning.  					AI-generated summary 				 While Large Language...
[30.01.2026 07:42] ********************************************************************************
[30.01.2026 07:42] Abstract 7. The Qwen3-ASR family introduces speech recognition models with language identification capabilities and a non-autoregressive forced alignment model, achieving state-of-the-art performance and efficient processing.  					AI-generated summary 				 In this report, we introduce Qwen3-ASR family, which i...
[30.01.2026 07:42] ********************************************************************************
[30.01.2026 07:42] Abstract 8. A minimal post-training approach using supervised fine-tuning, on-policy distillation, and small-scale reinforcement fine-tuning enables the development of high-quality sovereign language models with reduced resource requirements.  					AI-generated summary 				 Large language models (LLMs) have pro...
[30.01.2026 07:42] ********************************************************************************
[30.01.2026 07:42] Abstract 9. Agent-RRM, a multi-faceted reward model, provides structured feedback for agentic trajectories through reasoning traces, critiques, and performance scores, with unified feedback integration showing superior performance across diverse benchmarks.  					AI-generated summary 				 Agentic Reinforcement ...
[30.01.2026 07:42] ********************************************************************************
[30.01.2026 07:42] Abstract 10. Multimodal Large Language Models suffer from cross-modal hallucinations where one modality incorrectly influences generation from another, leading to fabricated outputs; this exposes a fundamental deficiency in modality-interaction control. To address this, a training-free method called Modality-Ada...
[30.01.2026 07:42] ********************************************************************************
[30.01.2026 07:42] Abstract 11. VTC-R1 enables efficient long-context reasoning by compressing textual traces into compact images and iteratively feeding them back into vision-language models as optical memory, achieving significant speedup without sacrificing performance.  					AI-generated summary 				 Long-context reasoning has...
[30.01.2026 07:42] ********************************************************************************
[30.01.2026 07:42] Abstract 12. Metric Anything presents a scalable pretraining framework for metric depth estimation that leverages diverse 3D data and sparse metric prompts to achieve superior performance across multiple vision tasks.  					AI-generated summary 				 Scaling has powered recent advances in vision foundation models...
[30.01.2026 07:42] ********************************************************************************
[30.01.2026 07:42] Abstract 13. Active latent planning method improves reasoning accuracy and efficiency by modeling latent token supervision as conditional VAE and using reinforcement learning with coherence rewards.  					AI-generated summary 				 Aiming at efficient and dense chain-of-thought (CoT) reasoning, latent reasoning m...
[30.01.2026 07:42] ********************************************************************************
[30.01.2026 07:42] Abstract 14. UniMRG enhances unified multimodal models by training them to generate multiple visual representations, improving both understanding and generation capabilities through complementary information capture.  					AI-generated summary 				 Unified Multimodal Models (UMMs) integrate both visual understan...
[30.01.2026 07:42] ********************************************************************************
[30.01.2026 07:42] Abstract 15. A reinforcement learning-based pretraining method improves language model safety, factuality, and quality by evaluating generations through a combination of model rollouts, original suffixes, and rewritten suffixes.  					AI-generated summary 				 Ensuring safety, factuality and overall quality in t...
[30.01.2026 07:42] ********************************************************************************
[30.01.2026 07:42] Abstract 16. Embedding scaling offers superior sparsity scaling compared to expert scaling in large language models, enabling efficient inference through system optimizations and speculative decoding.  					AI-generated summary 				 While Mixture-of-Experts (MoE) architectures have become the standard for sparsi...
[30.01.2026 07:42] ********************************************************************************
[30.01.2026 07:42] Abstract 17. A two-stage trained cybersecurity reasoning model achieves competitive performance on specialized tasks while maintaining general capabilities through supervised fine-tuning and reinforcement learning from verifiable rewards.  					AI-generated summary 				 We present Foundation-Sec-8B-Reasoning, th...
[30.01.2026 07:42] ********************************************************************************
[30.01.2026 07:42] Abstract 18. DeepSearchQA presents a 900-prompt benchmark evaluating agents on complex multi-step information-seeking tasks requiring systematic information collation, deduplication, and reasoning about stopping criteria across 17 fields.  					AI-generated summary 				 We introduce DeepSearchQA, a 900-prompt be...
[30.01.2026 07:42] ********************************************************************************
[30.01.2026 07:42] Abstract 19. BMAM presents a brain-inspired multi-agent memory architecture that decomposes memory into specialized subsystems to address long-term reasoning challenges in language-model-based agents.  					AI-generated summary 				 Language-model-based agents operating over extended interaction horizons face pe...
[30.01.2026 07:42] ********************************************************************************
[30.01.2026 07:42] Abstract 20. Neural audio fingerprinting performance varies with segment length, with short segments (0.5-second) generally providing better retrieval accuracy, and large language models showing promise in recommending optimal segment durations.  					AI-generated summary 				 Audio fingerprinting provides an id...
[30.01.2026 07:42] ********************************************************************************
[30.01.2026 07:42] Abstract 21. PRISM leverages design data to create a knowledge base for improving graphic designs based on natural language instructions, achieving superior style alignment compared to existing methods.  					AI-generated summary 				 Graphic design often involves exploring different stylistic directions, which ...
[30.01.2026 07:42] ********************************************************************************
[30.01.2026 07:42] Abstract 22. WebArbiter introduces a reasoning-first WebPRM that formulates reward modeling as text generation to improve web navigation through structured justifications and preference verdicts, outperforming existing baselines in complex web environments.  					AI-generated summary 				 Web agents hold great p...
[30.01.2026 07:42] ********************************************************************************
[30.01.2026 07:42] Abstract 23. Slot-Based Object-Centric Representations outperform global and dense feature representations in robotic manipulation tasks by providing better generalization under visual distribution shifts.  					AI-generated summary 				 The generalization capabilities of robotic manipulation policies are heavil...
[30.01.2026 07:42] ********************************************************************************
[30.01.2026 07:42] Abstract 24. WorldBench is introduced as a video-based benchmark for disentangled evaluation of physical reasoning in generative models, revealing specific failure patterns in current state-of-the-art video world models.  					AI-generated summary 				 Recent advances in generative foundational models, often ter...
[30.01.2026 07:42] ********************************************************************************
[30.01.2026 07:42] Abstract 25. STORM enhances robotic manipulation by adapting visual foundation models with semantic-aware slots through multi-phase training, improving generalization and control performance.  					AI-generated summary 				 Visual foundation models provide strong perceptual features for robotics, but their dense...
[30.01.2026 07:42] Read previous papers.
[30.01.2026 07:42] Generating reviews via LLM API.
[30.01.2026 07:42] Querying the API.
[30.01.2026 07:42] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A new benchmark and dataset are introduced to evaluate and improve spatial reasoning capabilities in text-to-image models through information-dense prompts and fine-tuning.  					AI-generated summary 				 Text-to-image (T2I) models have achieved remarkable success in generating high-fidelity images, but they often fail in handling complex spatial relationships, e.g., spatial perception, reasoning, or interaction. These critical aspects are largely overlooked by current benchmarks due to their short or information-sparse prompt design. In this paper, we introduce SpatialGenEval, a new benchmark designed to systematically evaluate the spatial intelligence of T2I models, covering two key aspects: (1) SpatialGenEval involves 1,230 long, information-dense prompts across 25 real-world scenes. Each prompt integrates 10 spatial sub-domains and corresponding 10 multi-choice question-answer pairs, ranging from object position and layout to occlusion and causality. Our extensive evaluation of 21 state-of-the-art models reveals that higher-order spatial reasoning remains a primary bottleneck. (2) To demonstrate that the utility of our information-dense design goes beyond simple evaluation, we also construct the SpatialT2I dataset. It contains 15,400 text-image pairs with rewritten prompts to ensure image consistency while preserving information density. Fine-tuned results on current foundation models (i.e., Stable Diffusion-XL, Uniworld-V1, OmniGen2) yield consistent performance gains (+4.2%, +5.7%, +4.4%) and more realistic effects in spatial relations, highlighting a data-centric paradigm to achieve spatial intelligence in T2I models.
[30.01.2026 07:42] Response: ```json
{
  "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº SpatialGenEval Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ text-to-image Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 1,230 Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ², Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… 25 Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ² Ñ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸ Ğ¾ Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ğ¾ĞºĞºĞ»ÑĞ·Ğ¸Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-ÑĞ»ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ²ÑĞ·ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ SpatialT2I Ğ¸Ğ· 15,400 Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€ Ğ´Ğ»Ñ fine-tuning Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ°Ñ… ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° 4-6% Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¹ Ğ² ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… foundation models.",
  "emoji": "ğŸ§©",
  "title": "ĞŸÑ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ² text-to-image Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹"
}
```
[30.01.2026 07:42] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A new benchmark and dataset are introduced to evaluate and improve spatial reasoning capabilities in text-to-image models through information-dense prompts and fine-tuning.  					AI-generated summary 				 Text-to-image (T2I) models have achieved remarkable success in generating high-fidelity images, but they often fail in handling complex spatial relationships, e.g., spatial perception, reasoning, or interaction. These critical aspects are largely overlooked by current benchmarks due to their short or information-sparse prompt design. In this paper, we introduce SpatialGenEval, a new benchmark designed to systematically evaluate the spatial intelligence of T2I models, covering two key aspects: (1) SpatialGenEval involves 1,230 long, information-dense prompts across 25 real-world scenes. Each prompt integrates 10 spatial sub-domains and corresponding 10 multi-choice question-answer pairs, ranging from object position and layout to occlusion and causality. Our extensive evaluation of 21 state-of-the-art models reveals that higher-order spatial reasoning remains a primary bottleneck. (2) To demonstrate that the utility of our information-dense design goes beyond simple evaluation, we also construct the SpatialT2I dataset. It contains 15,400 text-image pairs with rewritten prompts to ensure image consistency while preserving information density. Fine-tuned results on current foundation models (i.e., Stable Diffusion-XL, Uniworld-V1, OmniGen2) yield consistent performance gains (+4.2%, +5.7%, +4.4%) and more realistic effects in spatial relations, highlighting a data-centric paradigm to achieve spatial intelligence in T2I models."

[30.01.2026 07:42] Response: ```python
["DATASET", "BENCHMARK", "CV", "MULTIMODAL", "TRAINING"]
```
[30.01.2026 07:42] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A new benchmark and dataset are introduced to evaluate and improve spatial reasoning capabilities in text-to-image models through information-dense prompts and fine-tuning.  					AI-generated summary 				 Text-to-image (T2I) models have achieved remarkable success in generating high-fidelity images, but they often fail in handling complex spatial relationships, e.g., spatial perception, reasoning, or interaction. These critical aspects are largely overlooked by current benchmarks due to their short or information-sparse prompt design. In this paper, we introduce SpatialGenEval, a new benchmark designed to systematically evaluate the spatial intelligence of T2I models, covering two key aspects: (1) SpatialGenEval involves 1,230 long, information-dense prompts across 25 real-world scenes. Each prompt integrates 10 spatial sub-domains and corresponding 10 multi-choice question-answer pairs, ranging from object position and layout to occlusion and causality. Our extensive evaluation of 21 state-of-the-art models reveals that higher-order spatial reasoning remains a primary bottleneck. (2) To demonstrate that the utility of our information-dense design goes beyond simple evaluation, we also construct the SpatialT2I dataset. It contains 15,400 text-image pairs with rewritten prompts to ensure image consistency while preserving information density. Fine-tuned results on current foundation models (i.e., Stable Diffusion-XL, Uniworld-V1, OmniGen2) yield consistent performance gains (+4.2%, +5.7%, +4.4%) and more realistic effects in spatial relations, highlighting a data-centric paradigm to achieve spatial intelligence in T2I models."

[30.01.2026 07:42] Response: ```python
['REASONING', 'SYNTHETIC']
```
[30.01.2026 07:42] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents a new benchmark called SpatialGenEval to assess and enhance the spatial reasoning abilities of text-to-image (T2I) models. It introduces 1,230 detailed prompts that cover various spatial aspects, such as object positioning and interactions, to evaluate how well these models understand complex spatial relationships. The study also creates the SpatialT2I dataset, which includes 15,400 text-image pairs designed to maintain information density while ensuring image consistency. Fine-tuning existing models with this dataset shows significant improvements in their ability to generate realistic spatial relations, indicating the importance of data quality in developing spatial intelligence in T2I systems.","title":"Enhancing Spatial Reasoning in Text-to-Image Models"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper presents a new benchmark called SpatialGenEval to assess and enhance the spatial reasoning abilities of text-to-image (T2I) models. It introduces 1,230 detailed prompts that cover various spatial aspects, such as object positioning and interactions, to evaluate how well these models understand complex spatial relationships. The study also creates the SpatialT2I dataset, which includes 15,400 text-image pairs designed to maintain information density while ensuring image consistency. Fine-tuning existing models with this dataset shows significant improvements in their ability to generate realistic spatial relations, indicating the importance of data quality in developing spatial intelligence in T2I systems.', title='Enhancing Spatial Reasoning in Text-to-Image Models'))
[30.01.2026 07:42] Response: ParsedChatCompletionMessage[Article](content='{"desc":"æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªæ–°çš„åŸºå‡†å’Œæ•°æ®é›†ï¼Œæ—¨åœ¨é€šè¿‡ä¿¡æ¯å¯†é›†çš„æç¤ºå’Œå¾®è°ƒæ¥è¯„ä¼°å’Œæå‡æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚ç°æœ‰çš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹åœ¨ç”Ÿæˆé«˜è´¨é‡å›¾åƒæ–¹é¢å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†åœ¨å¤„ç†å¤æ‚çš„ç©ºé—´å…³ç³»æ—¶å¸¸å¸¸è¡¨ç°ä¸ä½³ã€‚æˆ‘ä»¬æå‡ºçš„SpatialGenEvalåŸºå‡†åŒ…å«1230ä¸ªé•¿çš„ã€ä¿¡æ¯å¯†é›†çš„æç¤ºï¼Œæ¶µç›–25ä¸ªçœŸå®åœºæ™¯ï¼Œç³»ç»Ÿè¯„ä¼°æ¨¡å‹çš„ç©ºé—´æ™ºèƒ½ã€‚é€šè¿‡å¯¹21ä¸ªæœ€å…ˆè¿›æ¨¡å‹çš„è¯„ä¼°ï¼Œæˆ‘ä»¬å‘ç°é«˜é˜¶ç©ºé—´æ¨ç†ä»ç„¶æ˜¯ä¸€ä¸ªä¸»è¦ç“¶é¢ˆï¼Œå¹¶é€šè¿‡æ„å»ºSpatialT2Iæ•°æ®é›†æ¥å±•ç¤ºä¿¡æ¯å¯†é›†è®¾è®¡çš„å®ç”¨æ€§ã€‚","title":"æå‡æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„ç©ºé—´æ¨ç†èƒ½åŠ›"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªæ–°çš„åŸºå‡†å’Œæ•°æ®é›†ï¼Œæ—¨åœ¨é€šè¿‡ä¿¡æ¯å¯†é›†çš„æç¤ºå’Œå¾®è°ƒæ¥è¯„ä¼°å’Œæå‡æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚ç°æœ‰çš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹åœ¨ç”Ÿæˆé«˜è´¨é‡å›¾åƒæ–¹é¢å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†åœ¨å¤„ç†å¤æ‚çš„ç©ºé—´å…³ç³»æ—¶å¸¸å¸¸è¡¨ç°ä¸ä½³ã€‚æˆ‘ä»¬æå‡ºçš„SpatialGenEvalåŸºå‡†åŒ…å«1230ä¸ªé•¿çš„ã€ä¿¡æ¯å¯†é›†çš„æç¤ºï¼Œæ¶µç›–25ä¸ªçœŸå®åœºæ™¯ï¼Œç³»ç»Ÿè¯„ä¼°æ¨¡å‹çš„ç©ºé—´æ™ºèƒ½ã€‚é€šè¿‡å¯¹21ä¸ªæœ€å…ˆè¿›æ¨¡å‹çš„è¯„ä¼°ï¼Œæˆ‘ä»¬å‘ç°é«˜é˜¶ç©ºé—´æ¨ç†ä»ç„¶æ˜¯ä¸€ä¸ªä¸»è¦ç“¶é¢ˆï¼Œå¹¶é€šè¿‡æ„å»ºSpatialT2Iæ•°æ®é›†æ¥å±•ç¤ºä¿¡æ¯å¯†é›†è®¾è®¡çš„å®ç”¨æ€§ã€‚', title='æå‡æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„ç©ºé—´æ¨ç†èƒ½åŠ›'))
[30.01.2026 07:42] Using data from previous issue: {"categories": ["#graphs", "#hallucinations", "#long_context", "#science"], "emoji": "ğŸ§¬", "ru": {"title": "ĞŸÑ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ñ", "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Idea2Story â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ¸Ñ‚ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾
[30.01.2026 07:42] Querying the API.
[30.01.2026 07:42] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

OCRVerse is a novel end-to-end OCR method that unifies text-centric and vision-centric approaches through comprehensive data engineering and a two-stage SFT-RL training framework with domain-specific reward strategies.  					AI-generated summary 				 The development of large vision language models drives the demand for managing, and applying massive amounts of multimodal data, making OCR technology, which extracts information from visual images, increasingly popular. However, existing OCR methods primarily focus on recognizing text elements from images or scanned documents (Text-centric OCR), neglecting the identification of visual elements from visually information-dense image sources (Vision-centric OCR), such as charts, web pages and science plots. In reality, these visually information-dense images are widespread on the internet and have significant real-world application value, such as data visualization and web page analysis. In this technical report, we propose OCRVerse, the first holistic OCR method in end-to-end manner that enables unified text-centric OCR and vision-centric OCR. To this end, we constructe comprehensive data engineering to cover a wide range of text-centric documents, such as newspapers, magazines and books, as well as vision-centric rendered composites, including charts, web pages and scientific plots. Moreover, we propose a two-stage SFT-RL multi-domain training method for OCRVerse. SFT directly mixes cross-domain data to train and establish initial domain knowledge, while RL focuses on designing personalized reward strategies for the characteristics of each domain. Specifically, since different domains require various output formats and expected outputs, we provide sufficient flexibility in the RL stage to customize flexible reward signals for each domain, thereby improving cross-domain fusion and avoiding data conflicts. Experimental results demonstrate the effectiveness of OCRVerse, achieving competitive results across text-centric and vision-centric data types, even comparable to large-scale open-source and closed-source models.
[30.01.2026 07:42] Response: ```json
{
  "desc": "OCRVerse Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹, Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ñ‚ĞµĞºÑÑ‚ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ğµ. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ supervised fine-tuning (SFT) Ğ¸ reinforcement learning (RL), Ğ³Ğ´Ğµ SFT ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ° RL Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºÑƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ¸Ğ² Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° (Ğ³Ğ°Ğ·ĞµÑ‚Ñ‹, Ğ¶ÑƒÑ€Ğ½Ğ°Ğ»Ñ‹, ĞºĞ½Ğ¸Ğ³Ğ¸) Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾ Ğ½Ğ°ÑÑ‹Ñ‰ĞµĞ½Ğ½Ñ‹Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ (Ğ´Ğ¸Ğ°Ğ³Ñ€Ğ°Ğ¼Ğ¼Ñ‹, Ğ²ĞµĞ±-ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹, Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğµ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¸). ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ¾Ğ±Ğ¾Ğ¸Ñ… Ñ‚Ğ¸Ğ¿Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ñ‹Ğµ Ñ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸.",
  "emoji": "ğŸ“„",
  "title": "Ğ£Ğ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸"
}
```
[30.01.2026 07:42] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"OCRVerse is a novel end-to-end OCR method that unifies text-centric and vision-centric approaches through comprehensive data engineering and a two-stage SFT-RL training framework with domain-specific reward strategies.  					AI-generated summary 				 The development of large vision language models drives the demand for managing, and applying massive amounts of multimodal data, making OCR technology, which extracts information from visual images, increasingly popular. However, existing OCR methods primarily focus on recognizing text elements from images or scanned documents (Text-centric OCR), neglecting the identification of visual elements from visually information-dense image sources (Vision-centric OCR), such as charts, web pages and science plots. In reality, these visually information-dense images are widespread on the internet and have significant real-world application value, such as data visualization and web page analysis. In this technical report, we propose OCRVerse, the first holistic OCR method in end-to-end manner that enables unified text-centric OCR and vision-centric OCR. To this end, we constructe comprehensive data engineering to cover a wide range of text-centric documents, such as newspapers, magazines and books, as well as vision-centric rendered composites, including charts, web pages and scientific plots. Moreover, we propose a two-stage SFT-RL multi-domain training method for OCRVerse. SFT directly mixes cross-domain data to train and establish initial domain knowledge, while RL focuses on designing personalized reward strategies for the characteristics of each domain. Specifically, since different domains require various output formats and expected outputs, we provide sufficient flexibility in the RL stage to customize flexible reward signals for each domain, thereby improving cross-domain fusion and avoiding data conflicts. Experimental results demonstrate the effectiveness of OCRVerse, achieving competitive results across text-centric and vision-centric data types, even comparable to large-scale open-source and closed-source models."

[30.01.2026 07:42] Response: ```python
["CV", "MULTIMODAL", "DATASET", "DATA", "TRAINING", "RL"]
```
[30.01.2026 07:42] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"OCRVerse is a novel end-to-end OCR method that unifies text-centric and vision-centric approaches through comprehensive data engineering and a two-stage SFT-RL training framework with domain-specific reward strategies.  					AI-generated summary 				 The development of large vision language models drives the demand for managing, and applying massive amounts of multimodal data, making OCR technology, which extracts information from visual images, increasingly popular. However, existing OCR methods primarily focus on recognizing text elements from images or scanned documents (Text-centric OCR), neglecting the identification of visual elements from visually information-dense image sources (Vision-centric OCR), such as charts, web pages and science plots. In reality, these visually information-dense images are widespread on the internet and have significant real-world application value, such as data visualization and web page analysis. In this technical report, we propose OCRVerse, the first holistic OCR method in end-to-end manner that enables unified text-centric OCR and vision-centric OCR. To this end, we constructe comprehensive data engineering to cover a wide range of text-centric documents, such as newspapers, magazines and books, as well as vision-centric rendered composites, including charts, web pages and scientific plots. Moreover, we propose a two-stage SFT-RL multi-domain training method for OCRVerse. SFT directly mixes cross-domain data to train and establish initial domain knowledge, while RL focuses on designing personalized reward strategies for the characteristics of each domain. Specifically, since different domains require various output formats and expected outputs, we provide sufficient flexibility in the RL stage to customize flexible reward signals for each domain, thereby improving cross-domain fusion and avoiding data conflicts. Experimental results demonstrate the effectiveness of OCRVerse, achieving competitive results across text-centric and vision-centric data types, even comparable to large-scale open-source and closed-source models."

[30.01.2026 07:42] Response: ```python
['OPTIMIZATION', 'OPEN_SOURCE', 'SCIENCE']
```
[30.01.2026 07:42] Response: ParsedChatCompletionMessage[Article](content='{"desc":"OCRVerse is an innovative end-to-end Optical Character Recognition (OCR) method that integrates both text-centric and vision-centric approaches. It employs a two-stage training framework combining Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) with tailored reward strategies for different domains. This method allows for effective extraction of information from both traditional text documents and complex visual data like charts and web pages. Experimental results show that OCRVerse performs competitively against existing models, demonstrating its versatility in handling diverse multimodal data.","title":"OCRVerse: Unifying Text and Vision for Enhanced OCR"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='OCRVerse is an innovative end-to-end Optical Character Recognition (OCR) method that integrates both text-centric and vision-centric approaches. It employs a two-stage training framework combining Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) with tailored reward strategies for different domains. This method allows for effective extraction of information from both traditional text documents and complex visual data like charts and web pages. Experimental results show that OCRVerse performs competitively against existing models, demonstrating its versatility in handling diverse multimodal data.', title='OCRVerse: Unifying Text and Vision for Enhanced OCR'))
[30.01.2026 07:42] Response: ParsedChatCompletionMessage[Article](content='{"desc":"OCRVerseæ˜¯ä¸€ç§æ–°é¢–çš„ç«¯åˆ°ç«¯å…‰å­¦å­—ç¬¦è¯†åˆ«ï¼ˆOCRï¼‰æ–¹æ³•ï¼Œå®ƒé€šè¿‡å…¨é¢çš„æ•°æ®å·¥ç¨‹å’Œä¸¤é˜¶æ®µçš„SFT-RLè®­ç»ƒæ¡†æ¶ï¼Œå°†ä»¥æ–‡æœ¬ä¸ºä¸­å¿ƒå’Œä»¥è§†è§‰ä¸ºä¸­å¿ƒçš„æ–¹æ³•ç»Ÿä¸€èµ·æ¥ã€‚ç°æœ‰çš„OCRæ–¹æ³•ä¸»è¦å…³æ³¨ä»å›¾åƒæˆ–æ‰«ææ–‡æ¡£ä¸­è¯†åˆ«æ–‡æœ¬å…ƒç´ ï¼Œè€Œå¿½è§†äº†ä»ä¿¡æ¯å¯†é›†çš„è§†è§‰å›¾åƒæºï¼ˆå¦‚å›¾è¡¨å’Œç½‘é¡µï¼‰ä¸­è¯†åˆ«è§†è§‰å…ƒç´ ã€‚OCRVerseé€šè¿‡æ„å»ºå…¨é¢çš„æ•°æ®å·¥ç¨‹ï¼Œæ¶µç›–äº†å¹¿æ³›çš„æ–‡æœ¬å’Œè§†è§‰æ–‡æ¡£ï¼Œæä¾›äº†è·¨é¢†åŸŸçš„è®­ç»ƒæ–¹æ³•ï¼Œä»¥é€‚åº”ä¸åŒé¢†åŸŸçš„è¾“å‡ºæ ¼å¼å’ŒæœŸæœ›ç»“æœã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOCRVerseåœ¨æ–‡æœ¬å’Œè§†è§‰æ•°æ®ç±»å‹ä¸Šéƒ½å–å¾—äº†ç«äº‰åŠ›çš„æ•ˆæœï¼Œç”šè‡³ä¸å¤§å‹å¼€æºå’Œé—­æºæ¨¡å‹ç›¸åª²ç¾ã€‚","title":"OCRVerseï¼šç»Ÿä¸€æ–‡æœ¬ä¸è§†è§‰çš„OCRæ–°æ–¹æ³•"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='OCRVerseæ˜¯ä¸€ç§æ–°é¢–çš„ç«¯åˆ°ç«¯å…‰å­¦å­—ç¬¦è¯†åˆ«ï¼ˆOCRï¼‰æ–¹æ³•ï¼Œå®ƒé€šè¿‡å…¨é¢çš„æ•°æ®å·¥ç¨‹å’Œä¸¤é˜¶æ®µçš„SFT-RLè®­ç»ƒæ¡†æ¶ï¼Œå°†ä»¥æ–‡æœ¬ä¸ºä¸­å¿ƒå’Œä»¥è§†è§‰ä¸ºä¸­å¿ƒçš„æ–¹æ³•ç»Ÿä¸€èµ·æ¥ã€‚ç°æœ‰çš„OCRæ–¹æ³•ä¸»è¦å…³æ³¨ä»å›¾åƒæˆ–æ‰«ææ–‡æ¡£ä¸­è¯†åˆ«æ–‡æœ¬å…ƒç´ ï¼Œè€Œå¿½è§†äº†ä»ä¿¡æ¯å¯†é›†çš„è§†è§‰å›¾åƒæºï¼ˆå¦‚å›¾è¡¨å’Œç½‘é¡µï¼‰ä¸­è¯†åˆ«è§†è§‰å…ƒç´ ã€‚OCRVerseé€šè¿‡æ„å»ºå…¨é¢çš„æ•°æ®å·¥ç¨‹ï¼Œæ¶µç›–äº†å¹¿æ³›çš„æ–‡æœ¬å’Œè§†è§‰æ–‡æ¡£ï¼Œæä¾›äº†è·¨é¢†åŸŸçš„è®­ç»ƒæ–¹æ³•ï¼Œä»¥é€‚åº”ä¸åŒé¢†åŸŸçš„è¾“å‡ºæ ¼å¼å’ŒæœŸæœ›ç»“æœã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOCRVerseåœ¨æ–‡æœ¬å’Œè§†è§‰æ•°æ®ç±»å‹ä¸Šéƒ½å–å¾—äº†ç«äº‰åŠ›çš„æ•ˆæœï¼Œç”šè‡³ä¸å¤§å‹å¼€æºå’Œé—­æºæ¨¡å‹ç›¸åª²ç¾ã€‚', title='OCRVerseï¼šç»Ÿä¸€æ–‡æœ¬ä¸è§†è§‰çš„OCRæ–°æ–¹æ³•'))
[30.01.2026 07:42] Using data from previous issue: {"categories": ["#multimodal", "#dataset", "#benchmark", "#robotics", "#inference", "#architecture", "#transfer_learning", "#small_models", "#synthetic"], "emoji": "ğŸ¤–", "ru": {"title": "Ğ‘Ñ‹ÑÑ‚Ñ€Ğ°Ñ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸ Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğµ vision-language Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸", "desc": "Dynami
[30.01.2026 07:42] Using data from previous issue: {"categories": ["#dataset", "#reasoning", "#multimodal", "#open_source", "#training", "#small_models", "#synthetic"], "emoji": "ğŸ§ ", "ru": {"title": "Ğ’Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´ĞµĞ»Ğ°ÑÑ‚ Ğ¼Ğ°Ğ»ĞµĞ½ÑŒĞºĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑƒĞ¼Ğ½ĞµĞµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ…", "desc": "ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ MMFineReason â€” Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚
[30.01.2026 07:42] Using data from previous issue: {"categories": ["#inference", "#multimodal", "#architecture", "#training"], "emoji": "âš¡", "ru": {"title": "ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹", "desc": "ConceptMoE â€” ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, ĞºĞ¾
[30.01.2026 07:42] Using data from previous issue: {"categories": ["#reasoning", "#training", "#rl", "#optimization", "#transfer_learning", "#agents", "#small_models"], "emoji": "ğŸ”", "ru": {"title": "Ğ Ğ°Ğ·Ğ²ĞµĞ´Ñ‡Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·ÑƒĞ¼Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ²ĞµĞ´ĞºĞ¸: ĞºĞ°Ğº Ğ¼Ğ°Ğ»ĞµĞ½ÑŒĞºĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑƒÑ‡Ğ°Ñ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ…", "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº SCOUT, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ»Ñ‘Ğ³ĞºĞ¸Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²-
[30.01.2026 07:42] Using data from previous issue: {"categories": ["#benchmark", "#multilingual", "#inference", "#low_resource", "#audio", "#open_source"], "emoji": "ğŸ¤", "ru": {"title": "ĞœĞ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€ĞµÑ‡Ğ¸ Ñ Ğ½ĞµĞ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼", "desc": "Ğ¡ĞµĞ¼ÑŒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Qwen3-ASR Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑ‡
[30.01.2026 07:42] Using data from previous issue: {"categories": ["#rlhf", "#reasoning", "#open_source", "#training", "#multilingual", "#optimization", "#low_resource", "#small_models"], "emoji": "ğŸ›ï¸", "ru": {"title": "Ğ¡ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµÑÑƒÑ€ÑĞ°Ğ¼Ğ¸", "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹,
[30.01.2026 07:42] Using data from previous issue: {"categories": ["#dataset", "#reasoning", "#benchmark", "#open_source", "#rl", "#optimization", "#agents"], "emoji": "ğŸ¯", "ru": {"title": "Ğ¡Ñ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ°Ñ ÑĞ²ÑĞ·ÑŒ Ğ´Ğ»Ñ ÑƒĞ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Agent-RRM, Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°ÑĞ¿ĞµĞºÑ‚Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ
[30.01.2026 07:42] Using data from previous issue: {"categories": ["#multimodal", "#video", "#audio", "#inference"], "emoji": "ğŸ¯", "ru": {"title": "ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…", "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, ĞºĞ¾Ğ³Ğ´Ğ° 
[30.01.2026 07:42] Using data from previous issue: {"categories": ["#multimodal", "#reasoning", "#benchmark", "#open_source", "#training", "#long_context", "#inference"], "emoji": "ğŸ§ ", "ru": {"title": "ĞĞ¿Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°: ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ", "desc": "VTC-R1 â€” Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°
[30.01.2026 07:42] Querying the API.
[30.01.2026 07:42] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Metric Anything presents a scalable pretraining framework for metric depth estimation that leverages diverse 3D data and sparse metric prompts to achieve superior performance across multiple vision tasks.  					AI-generated summary 				 Scaling has powered recent advances in vision foundation models, yet extending this paradigm to metric depth estimation remains challenging due to heterogeneous sensor noise, camera-dependent biases, and metric ambiguity in noisy cross-source 3D data. We introduce Metric Anything, a simple and scalable pretraining framework that learns metric depth from noisy, diverse 3D sources without manually engineered prompts, camera-specific modeling, or task-specific architectures. Central to our approach is the Sparse Metric Prompt, created by randomly masking depth maps, which serves as a universal interface that decouples spatial reasoning from sensor and camera biases. Using about 20M image-depth pairs spanning reconstructed, captured, and rendered 3D data across 10000 camera models, we demonstrate-for the first time-a clear scaling trend in the metric depth track. The pretrained model excels at prompt-driven tasks such as depth completion, super-resolution and Radar-camera fusion, while its distilled prompt-free student achieves state-of-the-art results on monocular depth estimation, camera intrinsics recovery, single/multi-view metric 3D reconstruction, and VLA planning. We also show that using pretrained ViT of Metric Anything as a visual encoder significantly boosts Multimodal Large Language Model capabilities in spatial intelligence. These results show that metric depth estimation can benefit from the same scaling laws that drive modern foundation models, establishing a new path toward scalable and efficient real-world metric perception. We open-source MetricAnything at http://metric-anything.github.io/metric-anything-io/ to support community research.
[30.01.2026 07:42] Response: ```json
{
  "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Metric Anything â€” Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ 3D Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ñ â€” Sparse Metric Prompt (Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ°Ñ Ğ¼Ğ°ÑĞºĞ° Ğ½Ğ° ĞºĞ°Ñ€Ñ‚Ğ°Ñ… Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ñ‚Ğ´ĞµĞ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¾Ñ‚ ÑˆÑƒĞ¼Ğ¾Ğ² Ğ´Ğ°Ñ‚Ñ‡Ğ¸ĞºĞ¾Ğ² Ğ¸ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ğ¹ ĞºĞ°Ğ¼ĞµÑ€Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±ÑƒÑ‡Ğ¸Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° 20 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ñ… Ğ¿Ğ°Ñ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ-Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğ° Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ² Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ²Ğ¿ĞµÑ€Ğ²Ñ‹Ğµ Ñ‡ĞµÑ‚ĞºÑƒÑ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹. ĞŸÑ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…: Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹, ÑÑƒĞ¿ĞµÑ€Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ, Ñ„ÑŒÑĞ¶ĞµĞ½ Radar-ĞºĞ°Ğ¼ĞµÑ€Ñ‹, Ğ¼Ğ¾Ğ½Ğ¾ĞºÑƒĞ»ÑÑ€Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ¸ 3D Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ.",
  "emoji": "ğŸ“",
  "title": "ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ĞºĞ°Ğº Ğ¿ÑƒÑ‚ÑŒ Ğº ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹"
}
```
[30.01.2026 07:42] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Metric Anything presents a scalable pretraining framework for metric depth estimation that leverages diverse 3D data and sparse metric prompts to achieve superior performance across multiple vision tasks.  					AI-generated summary 				 Scaling has powered recent advances in vision foundation models, yet extending this paradigm to metric depth estimation remains challenging due to heterogeneous sensor noise, camera-dependent biases, and metric ambiguity in noisy cross-source 3D data. We introduce Metric Anything, a simple and scalable pretraining framework that learns metric depth from noisy, diverse 3D sources without manually engineered prompts, camera-specific modeling, or task-specific architectures. Central to our approach is the Sparse Metric Prompt, created by randomly masking depth maps, which serves as a universal interface that decouples spatial reasoning from sensor and camera biases. Using about 20M image-depth pairs spanning reconstructed, captured, and rendered 3D data across 10000 camera models, we demonstrate-for the first time-a clear scaling trend in the metric depth track. The pretrained model excels at prompt-driven tasks such as depth completion, super-resolution and Radar-camera fusion, while its distilled prompt-free student achieves state-of-the-art results on monocular depth estimation, camera intrinsics recovery, single/multi-view metric 3D reconstruction, and VLA planning. We also show that using pretrained ViT of Metric Anything as a visual encoder significantly boosts Multimodal Large Language Model capabilities in spatial intelligence. These results show that metric depth estimation can benefit from the same scaling laws that drive modern foundation models, establishing a new path toward scalable and efficient real-world metric perception. We open-source MetricAnything at http://metric-anything.github.io/metric-anything-io/ to support community research."

[30.01.2026 07:42] Response: ```python
['CV', '3D', 'DATASET', 'MULTIMODAL', 'ARCHITECTURE']
```
[30.01.2026 07:42] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Metric Anything presents a scalable pretraining framework for metric depth estimation that leverages diverse 3D data and sparse metric prompts to achieve superior performance across multiple vision tasks.  					AI-generated summary 				 Scaling has powered recent advances in vision foundation models, yet extending this paradigm to metric depth estimation remains challenging due to heterogeneous sensor noise, camera-dependent biases, and metric ambiguity in noisy cross-source 3D data. We introduce Metric Anything, a simple and scalable pretraining framework that learns metric depth from noisy, diverse 3D sources without manually engineered prompts, camera-specific modeling, or task-specific architectures. Central to our approach is the Sparse Metric Prompt, created by randomly masking depth maps, which serves as a universal interface that decouples spatial reasoning from sensor and camera biases. Using about 20M image-depth pairs spanning reconstructed, captured, and rendered 3D data across 10000 camera models, we demonstrate-for the first time-a clear scaling trend in the metric depth track. The pretrained model excels at prompt-driven tasks such as depth completion, super-resolution and Radar-camera fusion, while its distilled prompt-free student achieves state-of-the-art results on monocular depth estimation, camera intrinsics recovery, single/multi-view metric 3D reconstruction, and VLA planning. We also show that using pretrained ViT of Metric Anything as a visual encoder significantly boosts Multimodal Large Language Model capabilities in spatial intelligence. These results show that metric depth estimation can benefit from the same scaling laws that drive modern foundation models, establishing a new path toward scalable and efficient real-world metric perception. We open-source MetricAnything at http://metric-anything.github.io/metric-anything-io/ to support community research."

[30.01.2026 07:42] Response: ```python
['OPTIMIZATION', 'TRANSFER_LEARNING', 'OPEN_SOURCE']
```
[30.01.2026 07:42] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Metric Anything introduces a scalable framework for metric depth estimation that utilizes diverse 3D data and sparse metric prompts. This approach addresses challenges like sensor noise and camera biases by employing a Sparse Metric Prompt, which allows the model to learn depth without needing specific prompts or architectures. The framework is trained on a large dataset of image-depth pairs, demonstrating significant improvements in various vision tasks such as depth completion and monocular depth estimation. By leveraging scaling laws similar to those in foundation models, this work paves the way for more efficient metric perception in real-world applications.","title":"Revolutionizing Metric Depth Estimation with Scalable Learning"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Metric Anything introduces a scalable framework for metric depth estimation that utilizes diverse 3D data and sparse metric prompts. This approach addresses challenges like sensor noise and camera biases by employing a Sparse Metric Prompt, which allows the model to learn depth without needing specific prompts or architectures. The framework is trained on a large dataset of image-depth pairs, demonstrating significant improvements in various vision tasks such as depth completion and monocular depth estimation. By leveraging scaling laws similar to those in foundation models, this work paves the way for more efficient metric perception in real-world applications.', title='Revolutionizing Metric Depth Estimation with Scalable Learning'))
[30.01.2026 07:42] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Metric Anything æ˜¯ä¸€ä¸ªå¯æ‰©å±•çš„é¢„è®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡åˆ©ç”¨å¤šæ ·çš„ä¸‰ç»´æ•°æ®å’Œç¨€ç–çš„åº¦é‡æç¤ºæ¥æé«˜åº¦é‡æ·±åº¦ä¼°è®¡çš„æ€§èƒ½ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿä»å˜ˆæ‚çš„ä¸‰ç»´æ•°æ®ä¸­å­¦ä¹ åº¦é‡æ·±åº¦ï¼Œè€Œæ— éœ€æ‰‹åŠ¨è®¾è®¡æç¤ºæˆ–ç‰¹å®šäºç›¸æœºçš„å»ºæ¨¡ã€‚æ ¸å¿ƒåˆ›æ–°æ˜¯ç¨€ç–åº¦é‡æç¤ºï¼Œé€šè¿‡éšæœºé®ç½©æ·±åº¦å›¾æ¥åˆ›å»ºï¼Œèƒ½å¤Ÿå°†ç©ºé—´æ¨ç†ä¸ä¼ æ„Ÿå™¨å’Œç›¸æœºåå·®è§£è€¦ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œé¢„è®­ç»ƒæ¨¡å‹åœ¨å¤šä¸ªè§†è§‰ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå±•ç¤ºäº†åº¦é‡æ·±åº¦ä¼°è®¡çš„æ–°è·¯å¾„ã€‚","title":"åº¦é‡æ·±åº¦ä¼°è®¡çš„æ–°è·¯å¾„"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Metric Anything æ˜¯ä¸€ä¸ªå¯æ‰©å±•çš„é¢„è®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡åˆ©ç”¨å¤šæ ·çš„ä¸‰ç»´æ•°æ®å’Œç¨€ç–çš„åº¦é‡æç¤ºæ¥æé«˜åº¦é‡æ·±åº¦ä¼°è®¡çš„æ€§èƒ½ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿä»å˜ˆæ‚çš„ä¸‰ç»´æ•°æ®ä¸­å­¦ä¹ åº¦é‡æ·±åº¦ï¼Œè€Œæ— éœ€æ‰‹åŠ¨è®¾è®¡æç¤ºæˆ–ç‰¹å®šäºç›¸æœºçš„å»ºæ¨¡ã€‚æ ¸å¿ƒåˆ›æ–°æ˜¯ç¨€ç–åº¦é‡æç¤ºï¼Œé€šè¿‡éšæœºé®ç½©æ·±åº¦å›¾æ¥åˆ›å»ºï¼Œèƒ½å¤Ÿå°†ç©ºé—´æ¨ç†ä¸ä¼ æ„Ÿå™¨å’Œç›¸æœºåå·®è§£è€¦ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œé¢„è®­ç»ƒæ¨¡å‹åœ¨å¤šä¸ªè§†è§‰ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå±•ç¤ºäº†åº¦é‡æ·±åº¦ä¼°è®¡çš„æ–°è·¯å¾„ã€‚', title='åº¦é‡æ·±åº¦ä¼°è®¡çš„æ–°è·¯å¾„'))
[30.01.2026 07:42] Using data from previous issue: {"categories": ["#reasoning", "#training", "#rl", "#optimization", "#small_models"], "emoji": "ğŸ§ ", "ru": {"title": "ĞĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ² ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´Ğ»Ñ ÑƒĞ»Ñƒ
[30.01.2026 07:42] Using data from previous issue: {"categories": ["#multimodal", "#cv", "#training"], "emoji": "ğŸ”„", "ru": {"title": "Ğ’Ğ·Ğ°Ğ¸Ğ¼Ğ½Ğ¾Ğµ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ", "desc": "UniMRG â€” ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ²Ñ
[30.01.2026 07:42] Using data from previous issue: {"categories": ["#rl", "#training", "#rlhf"], "emoji": "ğŸ›¡ï¸", "ru": {"title": "Ğ‘ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ñ„Ğ°ĞºĞ¾Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼", "desc": "ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, ĞºĞ¾Ñ‚
[30.01.2026 07:42] Using data from previous issue: {"categories": ["#architecture", "#inference", "#training", "#optimization", "#open_source"], "emoji": "âš¡", "ru": {"title": "Ğ­Ğ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ²: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿ÑƒÑ‚ÑŒ Ğº ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ", "desc": "Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² ĞºĞ°Ğº Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ñƒ ÑĞºÑ
[30.01.2026 07:42] Using data from previous issue: {"categories": ["#rlhf", "#dataset", "#reasoning", "#benchmark", "#open_source", "#training", "#science", "#small_models"], "emoji": "ğŸ”", "ru": {"title": "Ğ¡Ğ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ² ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾Ğ¼ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ: Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°ÑÑ‰Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ĞºĞ¸Ğ±ĞµÑ€Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸", "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Foundation-Sec-8B-Reasoning â€”
[30.01.2026 07:42] Using data from previous issue: {"categories": ["#dataset", "#agents", "#benchmark"], "emoji": "ğŸ”", "ru": {"title": "Ğ“Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸: Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²", "desc": "Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº DeepSearchQA Ñ 900 Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. Ğ‘ĞµĞ½
[30.01.2026 07:42] Querying the API.
[30.01.2026 07:42] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

BMAM presents a brain-inspired multi-agent memory architecture that decomposes memory into specialized subsystems to address long-term reasoning challenges in language-model-based agents.  					AI-generated summary 				 Language-model-based agents operating over extended interaction horizons face persistent challenges in preserving temporally grounded information and maintaining behavioral consistency across sessions, a failure mode we term soul erosion. We present BMAM (Brain-inspired Multi-Agent Memory), a general-purpose memory architecture that models agent memory as a set of functionally specialized subsystems rather than a single unstructured store. Inspired by cognitive memory systems, BMAM decomposes memory into episodic, semantic, salience-aware, and control-oriented components that operate at complementary time scales. To support long-horizon reasoning, BMAM organizes episodic memories along explicit timelines and retrieves evidence by fusing multiple complementary signals. Experiments on the LoCoMo benchmark show that BMAM achieves 78.45 percent accuracy under the standard long-horizon evaluation setting, and ablation analyses confirm that the hippocampus-inspired episodic memory subsystem plays a critical role in temporal reasoning.
[30.01.2026 07:42] Response: ```json
{
  "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ BMAM, Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ğ°Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¾Ğ¹ Ğ¼Ğ¾Ğ·Ğ³Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ½Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹: ÑĞ¿Ğ¸Ğ·Ğ¾Ğ´Ğ¸Ñ‡ĞµÑĞºÑƒÑ, ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ, ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹, Ğ¾Ñ‚Ğ²ĞµÑ‡Ğ°ÑÑ‰Ğ¸Ğµ Ğ·Ğ° Ğ²Ñ‹Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ²Ğ°Ğ¶Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸, Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‰Ğ¸Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹. Ğ­Ñ‚Ğ¾ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Â«ÑÑ‚Ğ¸Ñ€Ğ°Ğ½Ğ¸Ñ Ğ»Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸Â» â€” ĞºĞ¾Ğ³Ğ´Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‚ĞµÑ€ÑÑÑ‚ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¸ Ğ¸ Ğ½Ğµ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑÑ‚ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºĞ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·ÑƒĞµÑ‚ ÑĞ¿Ğ¸Ğ·Ğ¾Ğ´Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ²Ğ¾ÑĞ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ñ Ğ²Ğ´Ğ¾Ğ»ÑŒ ÑĞ²Ğ½Ğ¾Ğ¹ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑˆĞºĞ°Ğ»Ñ‹ Ğ¸ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ÑÑÑ‰Ğ¸Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ LoCoMo Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ 78,45% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ñ€Ğ¾Ğ»ÑŒ Ğ¿Ğ¾Ğ´ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ ÑĞ¿Ğ¸Ğ·Ğ¾Ğ´Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ… Ğ½Ğ° Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ğ°Ñ….",
  "emoji": "ğŸ§ ",
  "title": "Ğ¡Ñ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ ĞºĞ°Ğº ĞºĞ»ÑÑ‡ Ğº ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼"
}
```
[30.01.2026 07:42] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"BMAM presents a brain-inspired multi-agent memory architecture that decomposes memory into specialized subsystems to address long-term reasoning challenges in language-model-based agents.  					AI-generated summary 				 Language-model-based agents operating over extended interaction horizons face persistent challenges in preserving temporally grounded information and maintaining behavioral consistency across sessions, a failure mode we term soul erosion. We present BMAM (Brain-inspired Multi-Agent Memory), a general-purpose memory architecture that models agent memory as a set of functionally specialized subsystems rather than a single unstructured store. Inspired by cognitive memory systems, BMAM decomposes memory into episodic, semantic, salience-aware, and control-oriented components that operate at complementary time scales. To support long-horizon reasoning, BMAM organizes episodic memories along explicit timelines and retrieves evidence by fusing multiple complementary signals. Experiments on the LoCoMo benchmark show that BMAM achieves 78.45 percent accuracy under the standard long-horizon evaluation setting, and ablation analyses confirm that the hippocampus-inspired episodic memory subsystem plays a critical role in temporal reasoning."

[30.01.2026 07:42] Response: ```python
["AGENTS", "BENCHMARK", "ARCHITECTURE"]
```
[30.01.2026 07:42] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"BMAM presents a brain-inspired multi-agent memory architecture that decomposes memory into specialized subsystems to address long-term reasoning challenges in language-model-based agents.  					AI-generated summary 				 Language-model-based agents operating over extended interaction horizons face persistent challenges in preserving temporally grounded information and maintaining behavioral consistency across sessions, a failure mode we term soul erosion. We present BMAM (Brain-inspired Multi-Agent Memory), a general-purpose memory architecture that models agent memory as a set of functionally specialized subsystems rather than a single unstructured store. Inspired by cognitive memory systems, BMAM decomposes memory into episodic, semantic, salience-aware, and control-oriented components that operate at complementary time scales. To support long-horizon reasoning, BMAM organizes episodic memories along explicit timelines and retrieves evidence by fusing multiple complementary signals. Experiments on the LoCoMo benchmark show that BMAM achieves 78.45 percent accuracy under the standard long-horizon evaluation setting, and ablation analyses confirm that the hippocampus-inspired episodic memory subsystem plays a critical role in temporal reasoning."

[30.01.2026 07:42] Response: ```python
['REASONING', 'LONG_CONTEXT']
```
[30.01.2026 07:42] Response: ParsedChatCompletionMessage[Article](content='{"desc":"BMAM introduces a novel memory architecture for language-model-based agents, inspired by how the human brain organizes memory. It divides memory into specialized subsystems, including episodic, semantic, salience-aware, and control-oriented components, to enhance long-term reasoning. This structure helps agents maintain consistent behavior and retain relevant information over extended interactions, addressing the issue of \'soul erosion.\' Experimental results demonstrate that BMAM significantly improves accuracy in long-horizon reasoning tasks, highlighting the importance of its episodic memory subsystem.","title":"Revolutionizing Memory for Long-Term Reasoning in AI Agents"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="BMAM introduces a novel memory architecture for language-model-based agents, inspired by how the human brain organizes memory. It divides memory into specialized subsystems, including episodic, semantic, salience-aware, and control-oriented components, to enhance long-term reasoning. This structure helps agents maintain consistent behavior and retain relevant information over extended interactions, addressing the issue of 'soul erosion.' Experimental results demonstrate that BMAM significantly improves accuracy in long-horizon reasoning tasks, highlighting the importance of its episodic memory subsystem.", title='Revolutionizing Memory for Long-Term Reasoning in AI Agents'))
[30.01.2026 07:42] Response: ParsedChatCompletionMessage[Article](content='{"desc":"BMAMï¼ˆè„‘å¯å‘çš„å¤šæ™ºèƒ½ä½“è®°å¿†æ¶æ„ï¼‰æ˜¯ä¸€ç§æ–°å‹çš„è®°å¿†æ¶æ„ï¼Œæ—¨åœ¨è§£å†³åŸºäºè¯­è¨€æ¨¡å‹çš„æ™ºèƒ½ä½“åœ¨é•¿æœŸæ¨ç†ä¸­çš„æŒ‘æˆ˜ã€‚è¯¥æ¶æ„å°†è®°å¿†åˆ†è§£ä¸ºå¤šä¸ªåŠŸèƒ½ä¸“é—¨çš„å­ç³»ç»Ÿï¼ŒåŒ…æ‹¬æƒ…èŠ‚è®°å¿†ã€è¯­ä¹‰è®°å¿†ã€æ˜¾è‘—æ€§æ„ŸçŸ¥å’Œæ§åˆ¶å¯¼å‘ç»„ä»¶ï¼Œä»¥ä¾¿åœ¨ä¸åŒæ—¶é—´å°ºåº¦ä¸ŠååŒå·¥ä½œã€‚BMAMé€šè¿‡æ˜ç¡®çš„æ—¶é—´çº¿ç»„ç»‡æƒ…èŠ‚è®°å¿†ï¼Œå¹¶é€šè¿‡èåˆå¤šä¸ªäº’è¡¥ä¿¡å·æ¥æ£€ç´¢è¯æ®ï¼Œä»è€Œæ”¯æŒé•¿æœŸæ¨ç†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒBMAMåœ¨LoCoMoåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†78.45%çš„å‡†ç¡®ç‡ï¼Œè¯æ˜äº†å…¶åœ¨æ—¶é—´æ¨ç†ä¸­çš„æœ‰æ•ˆæ€§ã€‚","title":"è„‘å¯å‘çš„å¤šæ™ºèƒ½ä½“è®°å¿†æ¶æ„ï¼šè§£å†³é•¿æœŸæ¨ç†æŒ‘æˆ˜"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='BMAMï¼ˆè„‘å¯å‘çš„å¤šæ™ºèƒ½ä½“è®°å¿†æ¶æ„ï¼‰æ˜¯ä¸€ç§æ–°å‹çš„è®°å¿†æ¶æ„ï¼Œæ—¨åœ¨è§£å†³åŸºäºè¯­è¨€æ¨¡å‹çš„æ™ºèƒ½ä½“åœ¨é•¿æœŸæ¨ç†ä¸­çš„æŒ‘æˆ˜ã€‚è¯¥æ¶æ„å°†è®°å¿†åˆ†è§£ä¸ºå¤šä¸ªåŠŸèƒ½ä¸“é—¨çš„å­ç³»ç»Ÿï¼ŒåŒ…æ‹¬æƒ…èŠ‚è®°å¿†ã€è¯­ä¹‰è®°å¿†ã€æ˜¾è‘—æ€§æ„ŸçŸ¥å’Œæ§åˆ¶å¯¼å‘ç»„ä»¶ï¼Œä»¥ä¾¿åœ¨ä¸åŒæ—¶é—´å°ºåº¦ä¸ŠååŒå·¥ä½œã€‚BMAMé€šè¿‡æ˜ç¡®çš„æ—¶é—´çº¿ç»„ç»‡æƒ…èŠ‚è®°å¿†ï¼Œå¹¶é€šè¿‡èåˆå¤šä¸ªäº’è¡¥ä¿¡å·æ¥æ£€ç´¢è¯æ®ï¼Œä»è€Œæ”¯æŒé•¿æœŸæ¨ç†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒBMAMåœ¨LoCoMoåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†78.45%çš„å‡†ç¡®ç‡ï¼Œè¯æ˜äº†å…¶åœ¨æ—¶é—´æ¨ç†ä¸­çš„æœ‰æ•ˆæ€§ã€‚', title='è„‘å¯å‘çš„å¤šæ™ºèƒ½ä½“è®°å¿†æ¶æ„ï¼šè§£å†³é•¿æœŸæ¨ç†æŒ‘æˆ˜'))
[30.01.2026 07:42] Using data from previous issue: {"categories": ["#audio", "#architecture", "#benchmark"], "emoji": "ğŸ”", "ru": {"title": "ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ´Ğ»Ğ¸Ğ½Ğ° ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°: ĞºĞ»ÑÑ‡ Ğº Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ²Ñ‹Ñ… Ğ°ÑƒĞ´Ğ¸Ğ¾Ñ„Ğ¸Ğ½Ğ³ĞµÑ€Ğ¿Ñ€Ğ¸Ğ½Ñ‚Ğ¾Ğ²", "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾ĞºĞ½Ğ° Ğ½Ğ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ²Ñ‹Ñ… Ğ°ÑƒĞ´Ğ¸Ğ¾Ñ„Ğ¸Ğ½Ğ³ĞµÑ€Ğ¿Ñ€Ğ¸Ğ½Ñ‚Ğ¾Ğ² â€” ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ·Ğ²
[30.01.2026 07:42] Using data from previous issue: {"categories": [], "emoji": "ğŸ¨", "ru": {"title": "Ğ—Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ· Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğ° Ğ´Ğ»Ñ ÑÑ‚Ğ¸Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ", "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ PRISM Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ Ğ±Ğ°Ğ·Ñƒ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸Ğ· Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½-Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾
[30.01.2026 07:42] Using data from previous issue: {"categories": ["#dataset", "#reasoning", "#rlhf", "#benchmark", "#open_source", "#interpretability", "#agents"], "emoji": "ğŸ•·ï¸", "ru": {"title": "Ğ Ğ°ÑÑÑƒĞ¶Ğ´Ğ°ÑÑ‰Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ²ĞµĞ±-Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸", "desc": "WebArbiter Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ²
[30.01.2026 07:42] Querying the API.
[30.01.2026 07:42] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Slot-Based Object-Centric Representations outperform global and dense feature representations in robotic manipulation tasks by providing better generalization under visual distribution shifts.  					AI-generated summary 				 The generalization capabilities of robotic manipulation policies are heavily influenced by the choice of visual representations. Existing approaches typically rely on representations extracted from pre-trained encoders, using two dominant types of features: global features, which summarize an entire image via a single pooled vector, and dense features, which preserve a patch-wise embedding from the final encoder layer. While widely used, both feature types mix task-relevant and irrelevant information, leading to poor generalization under distribution shifts, such as changes in lighting, textures, or the presence of distractors. In this work, we explore an intermediate structured alternative: Slot-Based Object-Centric Representations (SBOCR), which group dense features into a finite set of object-like entities. This representation permits to naturally reduce the noise provided to the robotic manipulation policy while keeping enough information to efficiently perform the task. We benchmark a range of global and dense representations against intermediate slot-based representations, across a suite of simulated and real-world manipulation tasks ranging from simple to complex. We evaluate their generalization under diverse visual conditions, including changes in lighting, texture, and the presence of distractors. Our findings reveal that SBOCR-based policies outperform dense and global representation-based policies in generalization settings, even without task-specific pretraining. These insights suggest that SBOCR is a promising direction for designing visual systems that generalize effectively in dynamic, real-world robotic environments.
[30.01.2026 07:42] Response: ```json
{
  "desc": "Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ½Ğ¾-Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ»Ğ¾Ñ‚Ğ¾Ğ² (SBOCR), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ² Ğ½Ğ°Ğ±Ğ¾Ñ€ ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚ĞµĞ¹, Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸Ñ… Ğ½Ğ° Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹, Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ»Ğ¸ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ SBOCR-Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ»ÑƒÑ‡ÑˆĞµ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‚ÑÑ Ğ¿Ñ€Ğ¸ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑĞ´Ğ²Ğ¸Ğ³Ğ°Ñ…, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ñ, Ñ‚ĞµĞºÑÑ‚ÑƒÑ€ Ğ¸ Ğ¿Ğ¾ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾Ñ‚Ğ²Ğ»ĞµĞºĞ°ÑÑ‰Ğ¸Ñ… ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ ĞºĞ°Ğº Ğ² ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ¸.",
  "emoji": "ğŸ¤–",
  "title": "ĞĞ±ÑŠĞµĞºÑ‚Ğ½Ğ¾-Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ»Ğ¾Ñ‚Ñ‹ â€” ĞºĞ»ÑÑ‡ Ğº Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾Ğ¹ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ğ¹"
}
```
[30.01.2026 07:42] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Slot-Based Object-Centric Representations outperform global and dense feature representations in robotic manipulation tasks by providing better generalization under visual distribution shifts.  					AI-generated summary 				 The generalization capabilities of robotic manipulation policies are heavily influenced by the choice of visual representations. Existing approaches typically rely on representations extracted from pre-trained encoders, using two dominant types of features: global features, which summarize an entire image via a single pooled vector, and dense features, which preserve a patch-wise embedding from the final encoder layer. While widely used, both feature types mix task-relevant and irrelevant information, leading to poor generalization under distribution shifts, such as changes in lighting, textures, or the presence of distractors. In this work, we explore an intermediate structured alternative: Slot-Based Object-Centric Representations (SBOCR), which group dense features into a finite set of object-like entities. This representation permits to naturally reduce the noise provided to the robotic manipulation policy while keeping enough information to efficiently perform the task. We benchmark a range of global and dense representations against intermediate slot-based representations, across a suite of simulated and real-world manipulation tasks ranging from simple to complex. We evaluate their generalization under diverse visual conditions, including changes in lighting, texture, and the presence of distractors. Our findings reveal that SBOCR-based policies outperform dense and global representation-based policies in generalization settings, even without task-specific pretraining. These insights suggest that SBOCR is a promising direction for designing visual systems that generalize effectively in dynamic, real-world robotic environments."

[30.01.2026 07:42] Response: ```python
["ROBOTICS", "CV", "BENCHMARK"]
```
[30.01.2026 07:42] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Slot-Based Object-Centric Representations outperform global and dense feature representations in robotic manipulation tasks by providing better generalization under visual distribution shifts.  					AI-generated summary 				 The generalization capabilities of robotic manipulation policies are heavily influenced by the choice of visual representations. Existing approaches typically rely on representations extracted from pre-trained encoders, using two dominant types of features: global features, which summarize an entire image via a single pooled vector, and dense features, which preserve a patch-wise embedding from the final encoder layer. While widely used, both feature types mix task-relevant and irrelevant information, leading to poor generalization under distribution shifts, such as changes in lighting, textures, or the presence of distractors. In this work, we explore an intermediate structured alternative: Slot-Based Object-Centric Representations (SBOCR), which group dense features into a finite set of object-like entities. This representation permits to naturally reduce the noise provided to the robotic manipulation policy while keeping enough information to efficiently perform the task. We benchmark a range of global and dense representations against intermediate slot-based representations, across a suite of simulated and real-world manipulation tasks ranging from simple to complex. We evaluate their generalization under diverse visual conditions, including changes in lighting, texture, and the presence of distractors. Our findings reveal that SBOCR-based policies outperform dense and global representation-based policies in generalization settings, even without task-specific pretraining. These insights suggest that SBOCR is a promising direction for designing visual systems that generalize effectively in dynamic, real-world robotic environments."

[30.01.2026 07:43] Response: ```python
["TRANSFER_LEARNING"]
```

The paper discusses how visual representations transfer across different visual distribution shifts (lighting, textures, distractors) in robotic manipulation tasks, which is directly related to transfer learning concepts. The focus on generalization under distribution shifts and the comparison of different representation types for improved generalization across visual conditions aligns with transfer learning principles.
[30.01.2026 07:43] Error. Failed to parse JSON from LLM. ["TRANSFER_LEARNING"]


The paper discusses how visual representations transfer across different visual distribution shifts (lighting, textures, distractors) in robotic manipulation tasks, which is directly related to transfer learning concepts. The focus on generalization under distribution shifts and the comparison of different representation types for improved generalization across visual conditions aligns with transfer learning principles.
[30.01.2026 07:43] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces Slot-Based Object-Centric Representations (SBOCR) as a new approach for visual representation in robotic manipulation tasks. Unlike traditional global and dense features that mix relevant and irrelevant information, SBOCR organizes dense features into distinct object-like slots, reducing noise and enhancing task performance. The authors demonstrate that SBOCR significantly improves generalization under varying visual conditions, such as changes in lighting and textures. Their experiments show that policies using SBOCR outperform those based on global and dense representations, highlighting its potential for effective visual systems in robotics.","title":"Enhancing Robotic Manipulation with Slot-Based Object-Centric Representations"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper introduces Slot-Based Object-Centric Representations (SBOCR) as a new approach for visual representation in robotic manipulation tasks. Unlike traditional global and dense features that mix relevant and irrelevant information, SBOCR organizes dense features into distinct object-like slots, reducing noise and enhancing task performance. The authors demonstrate that SBOCR significantly improves generalization under varying visual conditions, such as changes in lighting and textures. Their experiments show that policies using SBOCR outperform those based on global and dense representations, highlighting its potential for effective visual systems in robotics.', title='Enhancing Robotic Manipulation with Slot-Based Object-Centric Representations'))
[30.01.2026 07:43] Response: ParsedChatCompletionMessage[Article](content='{"desc":"æœ¬ç ”ç©¶æ¢è®¨äº†åœ¨æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­ï¼ŒåŸºäºæ§½çš„ç‰©ä½“ä¸­å¿ƒè¡¨ç¤ºï¼ˆSBOCRï¼‰ç›¸è¾ƒäºå…¨å±€å’Œå¯†é›†ç‰¹å¾è¡¨ç¤ºçš„ä¼˜åŠ¿ã€‚SBOCRé€šè¿‡å°†å¯†é›†ç‰¹å¾åˆ†ç»„ä¸ºæœ‰é™çš„ç‰©ä½“æ ·æœ¬ï¼Œå‡å°‘äº†ä»»åŠ¡ç›¸å…³å’Œæ— å…³ä¿¡æ¯çš„æ··åˆï¼Œä»è€Œæé«˜äº†åœ¨è§†è§‰åˆ†å¸ƒå˜åŒ–ä¸‹çš„æ³›åŒ–èƒ½åŠ›ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒSBOCRåœ¨ä¸åŒçš„è§†è§‰æ¡ä»¶ä¸‹è¡¨ç°ä¼˜äºä¼ ç»Ÿçš„å…¨å±€å’Œå¯†é›†ç‰¹å¾è¡¨ç¤ºï¼Œå°¤å…¶æ˜¯åœ¨å…‰ç…§ã€çº¹ç†å˜åŒ–å’Œå¹²æ‰°ç‰©å­˜åœ¨çš„æƒ…å†µä¸‹ã€‚è¯¥æ–¹æ³•ä¸ºè®¾è®¡èƒ½å¤Ÿåœ¨åŠ¨æ€çœŸå®ç¯å¢ƒä¸­æœ‰æ•ˆæ³›åŒ–çš„è§†è§‰ç³»ç»Ÿæä¾›äº†æ–°çš„æ€è·¯ã€‚","title":"æ§½åŸºç‰©ä½“ä¸­å¿ƒè¡¨ç¤ºæå‡æœºå™¨äººæ“ä½œçš„æ³›åŒ–èƒ½åŠ›"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='æœ¬ç ”ç©¶æ¢è®¨äº†åœ¨æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­ï¼ŒåŸºäºæ§½çš„ç‰©ä½“ä¸­å¿ƒè¡¨ç¤ºï¼ˆSBOCRï¼‰ç›¸è¾ƒäºå…¨å±€å’Œå¯†é›†ç‰¹å¾è¡¨ç¤ºçš„ä¼˜åŠ¿ã€‚SBOCRé€šè¿‡å°†å¯†é›†ç‰¹å¾åˆ†ç»„ä¸ºæœ‰é™çš„ç‰©ä½“æ ·æœ¬ï¼Œå‡å°‘äº†ä»»åŠ¡ç›¸å…³å’Œæ— å…³ä¿¡æ¯çš„æ··åˆï¼Œä»è€Œæé«˜äº†åœ¨è§†è§‰åˆ†å¸ƒå˜åŒ–ä¸‹çš„æ³›åŒ–èƒ½åŠ›ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒSBOCRåœ¨ä¸åŒçš„è§†è§‰æ¡ä»¶ä¸‹è¡¨ç°ä¼˜äºä¼ ç»Ÿçš„å…¨å±€å’Œå¯†é›†ç‰¹å¾è¡¨ç¤ºï¼Œå°¤å…¶æ˜¯åœ¨å…‰ç…§ã€çº¹ç†å˜åŒ–å’Œå¹²æ‰°ç‰©å­˜åœ¨çš„æƒ…å†µä¸‹ã€‚è¯¥æ–¹æ³•ä¸ºè®¾è®¡èƒ½å¤Ÿåœ¨åŠ¨æ€çœŸå®ç¯å¢ƒä¸­æœ‰æ•ˆæ³›åŒ–çš„è§†è§‰ç³»ç»Ÿæä¾›äº†æ–°çš„æ€è·¯ã€‚', title='æ§½åŸºç‰©ä½“ä¸­å¿ƒè¡¨ç¤ºæå‡æœºå™¨äººæ“ä½œçš„æ³›åŒ–èƒ½åŠ›'))
[30.01.2026 07:43] Using data from previous issue: {"categories": ["#benchmark", "#video", "#robotics"], "emoji": "ğŸ¬", "ru": {"title": "Ğ Ğ°Ğ·Ğ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…", "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ WorldBench â€” Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»
[30.01.2026 07:43] Querying the API.
[30.01.2026 07:43] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

STORM enhances robotic manipulation by adapting visual foundation models with semantic-aware slots through multi-phase training, improving generalization and control performance.  					AI-generated summary 				 Visual foundation models provide strong perceptual features for robotics, but their dense representations lack explicit object-level structure, limiting robustness and contractility in manipulation tasks. We propose STORM (Slot-based Task-aware Object-centric Representation for robotic Manipulation), a lightweight object-centric adaptation module that augments frozen visual foundation models with a small set of semantic-aware slots for robotic manipulation. Rather than retraining large backbones, STORM employs a multi-phase training strategy: object-centric slots are first stabilized through visual--semantic pretraining using language embeddings, then jointly adapted with a downstream manipulation policy. This staged learning prevents degenerate slot formation and preserves semantic consistency while aligning perception with task objectives. Experiments on object discovery benchmarks and simulated manipulation tasks show that STORM improves generalization to visual distractors, and control performance compared to directly using frozen foundation model features or training object-centric representations end-to-end. Our results highlight multi-phase adaptation as an efficient mechanism for transforming generic foundation model features into task-aware object-centric representations for robotic control.
[30.01.2026 07:43] Response: ```json
{
  "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ STORM â€” Ğ»Ñ‘Ğ³ĞºĞ¸Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ Ğ·Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ foundation models ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸-Ğ¾ÑĞ²ĞµĞ´Ğ¾Ğ¼Ğ»ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ»Ğ¾Ñ‚Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ². Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹-Ğ±ÑĞºĞ±Ğ¾Ğ½Ğ¾Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ„Ğ°Ğ·Ğ½Ğ°Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° ÑĞ»Ğ¾Ñ‚Ñ‹ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ², Ğ·Ğ°Ñ‚ĞµĞ¼ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¾Ğ¹ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸. Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ²Ñ‹Ñ€Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ğµ ÑĞ»Ğ¾Ñ‚Ğ¾Ğ² Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ STORM ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ¾Ğ¼ĞµÑ…Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ÑĞ¼Ñ‹Ğ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ·Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² foundation models.",
  "emoji": "ğŸ¤–",
  "title": "ĞĞ±ÑŠĞµĞºÑ‚Ğ½Ğ¾-Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡Ğ½Ğ°Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ„Ğ°Ğ·Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ"
}
```
[30.01.2026 07:43] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"STORM enhances robotic manipulation by adapting visual foundation models with semantic-aware slots through multi-phase training, improving generalization and control performance.  					AI-generated summary 				 Visual foundation models provide strong perceptual features for robotics, but their dense representations lack explicit object-level structure, limiting robustness and contractility in manipulation tasks. We propose STORM (Slot-based Task-aware Object-centric Representation for robotic Manipulation), a lightweight object-centric adaptation module that augments frozen visual foundation models with a small set of semantic-aware slots for robotic manipulation. Rather than retraining large backbones, STORM employs a multi-phase training strategy: object-centric slots are first stabilized through visual--semantic pretraining using language embeddings, then jointly adapted with a downstream manipulation policy. This staged learning prevents degenerate slot formation and preserves semantic consistency while aligning perception with task objectives. Experiments on object discovery benchmarks and simulated manipulation tasks show that STORM improves generalization to visual distractors, and control performance compared to directly using frozen foundation model features or training object-centric representations end-to-end. Our results highlight multi-phase adaptation as an efficient mechanism for transforming generic foundation model features into task-aware object-centric representations for robotic control."

[30.01.2026 07:43] Response: ```python
["ROBOTICS", "CV", "TRAINING", "ARCHITECTURE"]
```
[30.01.2026 07:43] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"STORM enhances robotic manipulation by adapting visual foundation models with semantic-aware slots through multi-phase training, improving generalization and control performance.  					AI-generated summary 				 Visual foundation models provide strong perceptual features for robotics, but their dense representations lack explicit object-level structure, limiting robustness and contractility in manipulation tasks. We propose STORM (Slot-based Task-aware Object-centric Representation for robotic Manipulation), a lightweight object-centric adaptation module that augments frozen visual foundation models with a small set of semantic-aware slots for robotic manipulation. Rather than retraining large backbones, STORM employs a multi-phase training strategy: object-centric slots are first stabilized through visual--semantic pretraining using language embeddings, then jointly adapted with a downstream manipulation policy. This staged learning prevents degenerate slot formation and preserves semantic consistency while aligning perception with task objectives. Experiments on object discovery benchmarks and simulated manipulation tasks show that STORM improves generalization to visual distractors, and control performance compared to directly using frozen foundation model features or training object-centric representations end-to-end. Our results highlight multi-phase adaptation as an efficient mechanism for transforming generic foundation model features into task-aware object-centric representations for robotic control."

[30.01.2026 07:43] Response: ```python
['TRANSFER_LEARNING', 'OPTIMIZATION']
```

**Justification:**

- **TRANSFER_LEARNING**: The paper explicitly discusses adapting pre-trained visual foundation models (frozen backbones) to a downstream robotic manipulation task, which is a core transfer learning approach. The method leverages knowledge from foundation models and transfers it to a new domain (robotic control).

- **OPTIMIZATION**: The paper presents a multi-phase training strategy designed to optimize the adaptation process, preventing degenerate slot formation and improving training efficiency by avoiding retraining of large backbones.
[30.01.2026 07:43] Error. Failed to parse JSON from LLM. ["TRANSFER_LEARNING", "OPTIMIZATION"]


**Justification:**

- **TRANSFER_LEARNING**: The paper explicitly discusses adapting pre-trained visual foundation models (frozen backbones) to a downstream robotic manipulation task, which is a core transfer learning approach. The method leverages knowledge from foundation models and transfers it to a new domain (robotic control).

- **OPTIMIZATION**: The paper presents a multi-phase training strategy designed to optimize the adaptation process, preventing degenerate slot formation and improving training efficiency by avoiding retraining of large backbones.
[30.01.2026 07:43] Response: ParsedChatCompletionMessage[Article](content='{"desc":"STORM is a method that enhances robotic manipulation by integrating visual foundation models with semantic-aware slots. It addresses the limitations of dense representations in these models, which often lack clear object-level structure. By using a multi-phase training approach, STORM stabilizes object-centric slots through visual-semantic pretraining and then adapts them alongside a manipulation policy. This process improves the robot\'s ability to generalize to new visual scenarios and enhances its control performance in manipulation tasks.","title":"Transforming Visual Models for Smarter Robotic Manipulation"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="STORM is a method that enhances robotic manipulation by integrating visual foundation models with semantic-aware slots. It addresses the limitations of dense representations in these models, which often lack clear object-level structure. By using a multi-phase training approach, STORM stabilizes object-centric slots through visual-semantic pretraining and then adapts them alongside a manipulation policy. This process improves the robot's ability to generalize to new visual scenarios and enhances its control performance in manipulation tasks.", title='Transforming Visual Models for Smarter Robotic Manipulation'))
[30.01.2026 07:43] Response: ParsedChatCompletionMessage[Article](content='{"desc":"STORMæ˜¯ä¸€ç§å¢å¼ºæœºå™¨äººæ“ä½œèƒ½åŠ›çš„æ–¹æ³•ï¼Œé€šè¿‡å¤šé˜¶æ®µè®­ç»ƒå°†è§†è§‰åŸºç¡€æ¨¡å‹ä¸è¯­ä¹‰æ„ŸçŸ¥æ§½ç»“åˆèµ·æ¥ï¼Œä»è€Œæé«˜äº†æ³›åŒ–èƒ½åŠ›å’Œæ§åˆ¶æ€§èƒ½ã€‚ä¼ ç»Ÿçš„è§†è§‰åŸºç¡€æ¨¡å‹è™½ç„¶æä¾›äº†å¼ºå¤§çš„æ„ŸçŸ¥ç‰¹å¾ï¼Œä½†å…¶å¯†é›†è¡¨ç¤ºç¼ºä¹æ˜ç¡®çš„å¯¹è±¡çº§ç»“æ„ï¼Œé™åˆ¶äº†åœ¨æ“ä½œä»»åŠ¡ä¸­çš„é²æ£’æ€§å’Œå¯æ§æ€§ã€‚STORMé€šè¿‡è½»é‡çº§çš„å¯¹è±¡ä¸­å¿ƒé€‚åº”æ¨¡å—ï¼Œåˆ©ç”¨å°è§„æ¨¡çš„è¯­ä¹‰æ„ŸçŸ¥æ§½æ¥å¢å¼ºå†»ç»“çš„è§†è§‰åŸºç¡€æ¨¡å‹ï¼Œè€Œä¸æ˜¯é‡æ–°è®­ç»ƒå¤§å‹æ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSTORMåœ¨å¤„ç†è§†è§‰å¹²æ‰°ç‰©æ—¶çš„æ³›åŒ–èƒ½åŠ›å’Œæ§åˆ¶æ€§èƒ½ä¼˜äºç›´æ¥ä½¿ç”¨å†»ç»“åŸºç¡€æ¨¡å‹ç‰¹å¾æˆ–ç«¯åˆ°ç«¯è®­ç»ƒå¯¹è±¡ä¸­å¿ƒè¡¨ç¤ºã€‚","title":"STORMï¼šæå‡æœºå™¨äººæ“ä½œçš„æ™ºèƒ½è§£å†³æ–¹æ¡ˆ"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='STORMæ˜¯ä¸€ç§å¢å¼ºæœºå™¨äººæ“ä½œèƒ½åŠ›çš„æ–¹æ³•ï¼Œé€šè¿‡å¤šé˜¶æ®µè®­ç»ƒå°†è§†è§‰åŸºç¡€æ¨¡å‹ä¸è¯­ä¹‰æ„ŸçŸ¥æ§½ç»“åˆèµ·æ¥ï¼Œä»è€Œæé«˜äº†æ³›åŒ–èƒ½åŠ›å’Œæ§åˆ¶æ€§èƒ½ã€‚ä¼ ç»Ÿçš„è§†è§‰åŸºç¡€æ¨¡å‹è™½ç„¶æä¾›äº†å¼ºå¤§çš„æ„ŸçŸ¥ç‰¹å¾ï¼Œä½†å…¶å¯†é›†è¡¨ç¤ºç¼ºä¹æ˜ç¡®çš„å¯¹è±¡çº§ç»“æ„ï¼Œé™åˆ¶äº†åœ¨æ“ä½œä»»åŠ¡ä¸­çš„é²æ£’æ€§å’Œå¯æ§æ€§ã€‚STORMé€šè¿‡è½»é‡çº§çš„å¯¹è±¡ä¸­å¿ƒé€‚åº”æ¨¡å—ï¼Œåˆ©ç”¨å°è§„æ¨¡çš„è¯­ä¹‰æ„ŸçŸ¥æ§½æ¥å¢å¼ºå†»ç»“çš„è§†è§‰åŸºç¡€æ¨¡å‹ï¼Œè€Œä¸æ˜¯é‡æ–°è®­ç»ƒå¤§å‹æ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSTORMåœ¨å¤„ç†è§†è§‰å¹²æ‰°ç‰©æ—¶çš„æ³›åŒ–èƒ½åŠ›å’Œæ§åˆ¶æ€§èƒ½ä¼˜äºç›´æ¥ä½¿ç”¨å†»ç»“åŸºç¡€æ¨¡å‹ç‰¹å¾æˆ–ç«¯åˆ°ç«¯è®­ç»ƒå¯¹è±¡ä¸­å¿ƒè¡¨ç¤ºã€‚', title='STORMï¼šæå‡æœºå™¨äººæ“ä½œçš„æ™ºèƒ½è§£å†³æ–¹æ¡ˆ'))
[30.01.2026 07:43] Renaming data file.
[30.01.2026 07:43] Renaming previous data. hf_papers.json to ./d/2026-01-30.json
[30.01.2026 07:43] Saving new data file.
[30.01.2026 07:43] Generating page.
[30.01.2026 07:43] Renaming previous page.
[30.01.2026 07:43] Renaming previous data. index.html to ./d/2026-01-30.html
[30.01.2026 07:43] Writing result.
[30.01.2026 07:43] Renaming log file.
[30.01.2026 07:43] Renaming previous data. log.txt to ./logs/2026-01-30_last_log.txt
