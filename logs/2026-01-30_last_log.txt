[30.01.2026 08:43] Read previous papers.
[30.01.2026 08:43] Generating top page (month).
[30.01.2026 08:43] Writing top page (month).
[30.01.2026 09:40] Read previous papers.
[30.01.2026 09:40] Get feed.
[30.01.2026 09:40] Get page data from previous paper. URL: https://huggingface.co/papers/2601.20354
[30.01.2026 09:40] Get page data from previous paper. URL: https://huggingface.co/papers/2601.20833
[30.01.2026 09:40] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21639
[30.01.2026 09:40] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22153
[30.01.2026 09:40] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22046
[30.01.2026 09:40] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21821
[30.01.2026 09:40] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21204
[30.01.2026 09:40] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21420
[30.01.2026 09:40] Extract page data from URL. URL: https://huggingface.co/papers/2601.20730
[30.01.2026 09:40] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22154
[30.01.2026 09:40] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21754
[30.01.2026 09:40] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21337
[30.01.2026 09:40] Get page data from previous paper. URL: https://huggingface.co/papers/2601.18129
[30.01.2026 09:40] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21181
[30.01.2026 09:40] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22069
[30.01.2026 09:40] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22054
[30.01.2026 09:40] Get page data from previous paper. URL: https://huggingface.co/papers/2601.20975
[30.01.2026 09:40] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22158
[30.01.2026 09:40] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21598
[30.01.2026 09:40] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21406
[30.01.2026 09:40] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21343
[30.01.2026 09:40] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21051
[30.01.2026 09:40] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22156
[30.01.2026 09:40] Get page data from previous paper. URL: https://huggingface.co/papers/2601.20465
[30.01.2026 09:40] Get page data from previous paper. URL: https://huggingface.co/papers/2601.19001
[30.01.2026 09:40] Get page data from previous paper. URL: https://huggingface.co/papers/2601.17690
[30.01.2026 09:40] Get page data from previous paper. URL: https://huggingface.co/papers/2601.11747
[30.01.2026 09:40] Extract page data from URL. URL: https://huggingface.co/papers/2601.22101
[30.01.2026 09:40] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21872
[30.01.2026 09:40] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21416
[30.01.2026 09:40] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21282
[30.01.2026 09:40] Get page data from previous paper. URL: https://huggingface.co/papers/2601.20381
[30.01.2026 09:40] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[30.01.2026 09:40] No deleted papers detected.
[30.01.2026 09:40] Downloading and parsing papers (pdf, html). Total: 32.
[30.01.2026 09:40] Downloading and parsing paper https://huggingface.co/papers/2601.20354.
[30.01.2026 09:40] Extra JSON file exists (./assets/json/2601.20354.json), skip PDF parsing.
[30.01.2026 09:40] Paper image links file exists (./assets/img_data/2601.20354.json), skip HTML parsing.
[30.01.2026 09:40] Success.
[30.01.2026 09:40] Downloading and parsing paper https://huggingface.co/papers/2601.20833.
[30.01.2026 09:40] Extra JSON file exists (./assets/json/2601.20833.json), skip PDF parsing.
[30.01.2026 09:40] Paper image links file exists (./assets/img_data/2601.20833.json), skip HTML parsing.
[30.01.2026 09:40] Success.
[30.01.2026 09:40] Downloading and parsing paper https://huggingface.co/papers/2601.21639.
[30.01.2026 09:40] Extra JSON file exists (./assets/json/2601.21639.json), skip PDF parsing.
[30.01.2026 09:40] Paper image links file exists (./assets/img_data/2601.21639.json), skip HTML parsing.
[30.01.2026 09:40] Success.
[30.01.2026 09:40] Downloading and parsing paper https://huggingface.co/papers/2601.22153.
[30.01.2026 09:40] Extra JSON file exists (./assets/json/2601.22153.json), skip PDF parsing.
[30.01.2026 09:40] Paper image links file exists (./assets/img_data/2601.22153.json), skip HTML parsing.
[30.01.2026 09:40] Success.
[30.01.2026 09:40] Downloading and parsing paper https://huggingface.co/papers/2601.22046.
[30.01.2026 09:40] Extra JSON file exists (./assets/json/2601.22046.json), skip PDF parsing.
[30.01.2026 09:40] Paper image links file exists (./assets/img_data/2601.22046.json), skip HTML parsing.
[30.01.2026 09:40] Success.
[30.01.2026 09:40] Downloading and parsing paper https://huggingface.co/papers/2601.21821.
[30.01.2026 09:40] Extra JSON file exists (./assets/json/2601.21821.json), skip PDF parsing.
[30.01.2026 09:40] Paper image links file exists (./assets/img_data/2601.21821.json), skip HTML parsing.
[30.01.2026 09:40] Success.
[30.01.2026 09:40] Downloading and parsing paper https://huggingface.co/papers/2601.21204.
[30.01.2026 09:40] Extra JSON file exists (./assets/json/2601.21204.json), skip PDF parsing.
[30.01.2026 09:40] Paper image links file exists (./assets/img_data/2601.21204.json), skip HTML parsing.
[30.01.2026 09:40] Success.
[30.01.2026 09:40] Downloading and parsing paper https://huggingface.co/papers/2601.21420.
[30.01.2026 09:40] Extra JSON file exists (./assets/json/2601.21420.json), skip PDF parsing.
[30.01.2026 09:40] Paper image links file exists (./assets/img_data/2601.21420.json), skip HTML parsing.
[30.01.2026 09:40] Success.
[30.01.2026 09:40] Downloading and parsing paper https://huggingface.co/papers/2601.20730.
[30.01.2026 09:40] Downloading paper 2601.20730 from https://arxiv.org/pdf/2601.20730v2...
[30.01.2026 09:40] Extracting affiliations from text.
[30.01.2026 09:40] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 9 2 ] . [ 2 0 3 7 0 2 . 1 0 6 2 : r AgentLongBench: Controllable Long Benchmark For Long-Contexts Agents via Environment Rollouts Shicheng Fang1,2 Yuxin Wang1 XiaoRan Liu1,2 Jiahao Lu1,2 Chuanyuan Tan3 Xinchi Chen1 Yining Zheng1, Xuanjing Huang1 Xipeng Qiu1,2, 1Fudan University 2Shanghai Innovation Institute 3Soochow University "
[30.01.2026 09:40] Response: ```python
[
    "Fudan University",
    "Shanghai Innovation Institute",
    "Soochow University"
]
```
[30.01.2026 09:40] Deleting PDF ./assets/pdf/2601.20730.pdf.
[30.01.2026 09:40] Success.
[30.01.2026 09:40] Downloading and parsing paper https://huggingface.co/papers/2601.22154.
[30.01.2026 09:40] Extra JSON file exists (./assets/json/2601.22154.json), skip PDF parsing.
[30.01.2026 09:40] Paper image links file exists (./assets/img_data/2601.22154.json), skip HTML parsing.
[30.01.2026 09:40] Success.
[30.01.2026 09:40] Downloading and parsing paper https://huggingface.co/papers/2601.21754.
[30.01.2026 09:40] Extra JSON file exists (./assets/json/2601.21754.json), skip PDF parsing.
[30.01.2026 09:40] Paper image links file exists (./assets/img_data/2601.21754.json), skip HTML parsing.
[30.01.2026 09:40] Success.
[30.01.2026 09:40] Downloading and parsing paper https://huggingface.co/papers/2601.21337.
[30.01.2026 09:40] Extra JSON file exists (./assets/json/2601.21337.json), skip PDF parsing.
[30.01.2026 09:40] Paper image links file exists (./assets/img_data/2601.21337.json), skip HTML parsing.
[30.01.2026 09:40] Success.
[30.01.2026 09:40] Downloading and parsing paper https://huggingface.co/papers/2601.18129.
[30.01.2026 09:40] Extra JSON file exists (./assets/json/2601.18129.json), skip PDF parsing.
[30.01.2026 09:40] Paper image links file exists (./assets/img_data/2601.18129.json), skip HTML parsing.
[30.01.2026 09:40] Success.
[30.01.2026 09:40] Downloading and parsing paper https://huggingface.co/papers/2601.21181.
[30.01.2026 09:40] Extra JSON file exists (./assets/json/2601.21181.json), skip PDF parsing.
[30.01.2026 09:40] Paper image links file exists (./assets/img_data/2601.21181.json), skip HTML parsing.
[30.01.2026 09:40] Success.
[30.01.2026 09:40] Downloading and parsing paper https://huggingface.co/papers/2601.22069.
[30.01.2026 09:40] Extra JSON file exists (./assets/json/2601.22069.json), skip PDF parsing.
[30.01.2026 09:40] Paper image links file exists (./assets/img_data/2601.22069.json), skip HTML parsing.
[30.01.2026 09:40] Success.
[30.01.2026 09:40] Downloading and parsing paper https://huggingface.co/papers/2601.22054.
[30.01.2026 09:40] Extra JSON file exists (./assets/json/2601.22054.json), skip PDF parsing.
[30.01.2026 09:40] Paper image links file exists (./assets/img_data/2601.22054.json), skip HTML parsing.
[30.01.2026 09:40] Success.
[30.01.2026 09:40] Downloading and parsing paper https://huggingface.co/papers/2601.20975.
[30.01.2026 09:40] Extra JSON file exists (./assets/json/2601.20975.json), skip PDF parsing.
[30.01.2026 09:40] Paper image links file exists (./assets/img_data/2601.20975.json), skip HTML parsing.
[30.01.2026 09:40] Success.
[30.01.2026 09:40] Downloading and parsing paper https://huggingface.co/papers/2601.22158.
[30.01.2026 09:40] Extra JSON file exists (./assets/json/2601.22158.json), skip PDF parsing.
[30.01.2026 09:40] Paper image links file exists (./assets/img_data/2601.22158.json), skip HTML parsing.
[30.01.2026 09:40] Success.
[30.01.2026 09:40] Downloading and parsing paper https://huggingface.co/papers/2601.21598.
[30.01.2026 09:40] Extra JSON file exists (./assets/json/2601.21598.json), skip PDF parsing.
[30.01.2026 09:40] Paper image links file exists (./assets/img_data/2601.21598.json), skip HTML parsing.
[30.01.2026 09:40] Success.
[30.01.2026 09:40] Downloading and parsing paper https://huggingface.co/papers/2601.21406.
[30.01.2026 09:40] Extra JSON file exists (./assets/json/2601.21406.json), skip PDF parsing.
[30.01.2026 09:40] Paper image links file exists (./assets/img_data/2601.21406.json), skip HTML parsing.
[30.01.2026 09:40] Success.
[30.01.2026 09:40] Downloading and parsing paper https://huggingface.co/papers/2601.21343.
[30.01.2026 09:40] Extra JSON file exists (./assets/json/2601.21343.json), skip PDF parsing.
[30.01.2026 09:40] Paper image links file exists (./assets/img_data/2601.21343.json), skip HTML parsing.
[30.01.2026 09:40] Success.
[30.01.2026 09:40] Downloading and parsing paper https://huggingface.co/papers/2601.21051.
[30.01.2026 09:40] Extra JSON file exists (./assets/json/2601.21051.json), skip PDF parsing.
[30.01.2026 09:40] Paper image links file exists (./assets/img_data/2601.21051.json), skip HTML parsing.
[30.01.2026 09:40] Success.
[30.01.2026 09:40] Downloading and parsing paper https://huggingface.co/papers/2601.22156.
[30.01.2026 09:40] Extra JSON file exists (./assets/json/2601.22156.json), skip PDF parsing.
[30.01.2026 09:40] Paper image links file exists (./assets/img_data/2601.22156.json), skip HTML parsing.
[30.01.2026 09:40] Success.
[30.01.2026 09:40] Downloading and parsing paper https://huggingface.co/papers/2601.20465.
[30.01.2026 09:40] Extra JSON file exists (./assets/json/2601.20465.json), skip PDF parsing.
[30.01.2026 09:40] Paper image links file exists (./assets/img_data/2601.20465.json), skip HTML parsing.
[30.01.2026 09:40] Success.
[30.01.2026 09:40] Downloading and parsing paper https://huggingface.co/papers/2601.19001.
[30.01.2026 09:40] Extra JSON file exists (./assets/json/2601.19001.json), skip PDF parsing.
[30.01.2026 09:40] Paper image links file exists (./assets/img_data/2601.19001.json), skip HTML parsing.
[30.01.2026 09:40] Success.
[30.01.2026 09:40] Downloading and parsing paper https://huggingface.co/papers/2601.17690.
[30.01.2026 09:40] Extra JSON file exists (./assets/json/2601.17690.json), skip PDF parsing.
[30.01.2026 09:40] Paper image links file exists (./assets/img_data/2601.17690.json), skip HTML parsing.
[30.01.2026 09:40] Success.
[30.01.2026 09:40] Downloading and parsing paper https://huggingface.co/papers/2601.11747.
[30.01.2026 09:40] Extra JSON file exists (./assets/json/2601.11747.json), skip PDF parsing.
[30.01.2026 09:40] Paper image links file exists (./assets/img_data/2601.11747.json), skip HTML parsing.
[30.01.2026 09:40] Success.
[30.01.2026 09:40] Downloading and parsing paper https://huggingface.co/papers/2601.22101.
[30.01.2026 09:40] Downloading paper 2601.22101 from https://arxiv.org/pdf/2601.22101v1...
[30.01.2026 09:40] Extracting affiliations from text.
[30.01.2026 09:40] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 9 2 ] . [ 1 1 0 1 2 2 . 1 0 6 2 : r ECO: Quantized Training without Full-Precision Master Weights Mahdi Nikdan1,2, Amir Zandieh1, Dan Alistarh2 and Vahab Mirrokni1 1Google Research, 2ISTA Quantization has significantly improved the compute and memory efficiency of Large Language Model (LLM) training. However, existing approaches still rely on accumulating their updates in high-precision: concretely, gradient updates must be applied to high-precision weight buffer, known as master weights. This buffer introduces substantial memory overhead, particularly for Sparse Mixture of Experts (SMoE) models, where model parameters and optimizer states dominate memory usage. To address this, we introduce the Error-Compensating Optimizer (ECO), which eliminates master weights by applying updates directly to quantized parameters. ECO quantizes weights after each step and carefully injects the resulting quantization error into the optimizer momentum, forming an error-feedback loop with no additional memory. We prove that, under standard assumptions and decaying learning rate, ECO converges to constant-radius neighborhood of the optimum, while naive master-weight removal can incur an error that is inversely proportional to the learning rate. We show empirical results for pretraining small Transformers (30800M), Gemma-3 1B model, and 2.1B parameter Sparse MoE model with FP8 quantization, and fine-tuning DeepSeek-MoE-16B in INT4 precision. Throughout, ECO matches baselines with master weights up to near-lossless accuracy, significantly shifting the static memory vs validation loss Pareto frontier. 1. Introduction Scaling Large Language Model (LLM) training comes with substantial computational and memory costs. As models have grown from billions to trillions of parameters, training memory has become central bottleneck. Low-precision training has therefore emerged as practical direction: recent FP8 (Liu et al., 2024a; Peng et al., 2023), and even lower precision (Panferov et al."
[30.01.2026 09:40] Response: ```python
["Google Research", "ISTA"]
```
[30.01.2026 09:40] Deleting PDF ./assets/pdf/2601.22101.pdf.
[30.01.2026 09:40] Success.
[30.01.2026 09:40] Downloading and parsing paper https://huggingface.co/papers/2601.21872.
[30.01.2026 09:40] Extra JSON file exists (./assets/json/2601.21872.json), skip PDF parsing.
[30.01.2026 09:40] Paper image links file exists (./assets/img_data/2601.21872.json), skip HTML parsing.
[30.01.2026 09:40] Success.
[30.01.2026 09:40] Downloading and parsing paper https://huggingface.co/papers/2601.21416.
[30.01.2026 09:40] Extra JSON file exists (./assets/json/2601.21416.json), skip PDF parsing.
[30.01.2026 09:40] Paper image links file exists (./assets/img_data/2601.21416.json), skip HTML parsing.
[30.01.2026 09:40] Success.
[30.01.2026 09:40] Downloading and parsing paper https://huggingface.co/papers/2601.21282.
[30.01.2026 09:40] Extra JSON file exists (./assets/json/2601.21282.json), skip PDF parsing.
[30.01.2026 09:40] Paper image links file exists (./assets/img_data/2601.21282.json), skip HTML parsing.
[30.01.2026 09:40] Success.
[30.01.2026 09:40] Downloading and parsing paper https://huggingface.co/papers/2601.20381.
[30.01.2026 09:40] Extra JSON file exists (./assets/json/2601.20381.json), skip PDF parsing.
[30.01.2026 09:40] Paper image links file exists (./assets/img_data/2601.20381.json), skip HTML parsing.
[30.01.2026 09:40] Success.
[30.01.2026 09:40] Enriching papers with extra data.
[30.01.2026 09:40] ********************************************************************************
[30.01.2026 09:40] Abstract 0. A new benchmark and dataset are introduced to evaluate and improve spatial reasoning capabilities in text-to-image models through information-dense prompts and fine-tuning.  					AI-generated summary 				 Text-to-image (T2I) models have achieved remarkable success in generating high-fidelity images,...
[30.01.2026 09:40] ********************************************************************************
[30.01.2026 09:40] Abstract 1. Offline knowledge construction through structured methodological graphs enables more reliable and scalable autonomous scientific discovery by reducing reliance on real-time literature processing.  					AI-generated summary 				 Autonomous scientific discovery with large language model (LLM)-based ag...
[30.01.2026 09:40] ********************************************************************************
[30.01.2026 09:40] Abstract 2. OCRVerse is a novel end-to-end OCR method that unifies text-centric and vision-centric approaches through comprehensive data engineering and a two-stage SFT-RL training framework with domain-specific reward strategies.  					AI-generated summary 				 The development of large vision language models d...
[30.01.2026 09:40] ********************************************************************************
[30.01.2026 09:40] Abstract 3. DynamicVLA addresses dynamic object manipulation challenges through a compact vision-language-action model with temporal reasoning and closed-loop adaptation, supported by a new benchmark for dynamic manipulation tasks.  					AI-generated summary 				 Manipulating dynamic objects remains an open cha...
[30.01.2026 09:40] ********************************************************************************
[30.01.2026 09:40] Abstract 4. PLANING presents an efficient streaming reconstruction framework that combines explicit geometric primitives with neural Gaussians to achieve high-quality rendering and accurate geometry simultaneously through decoupled optimization.  					AI-generated summary 				 Streaming reconstruction from mono...
[30.01.2026 09:40] ********************************************************************************
[30.01.2026 09:40] Abstract 5. A large-scale multimodal reasoning dataset called MMFineReason is introduced to improve vision language models' performance through high-quality reasoning annotations and demonstrates superior parameter efficiency in fine-tuned models.  					AI-generated summary 				 Recent advances in Vision Langua...
[30.01.2026 09:40] ********************************************************************************
[30.01.2026 09:40] Abstract 6. Embedding scaling offers superior sparsity scaling compared to expert scaling in large language models, enabling efficient inference through system optimizations and speculative decoding.  					AI-generated summary 				 While Mixture-of-Experts (MoE) architectures have become the standard for sparsi...
[30.01.2026 09:40] ********************************************************************************
[30.01.2026 09:40] Abstract 7. ConceptMoE dynamically allocates computation by merging similar tokens into concept representations, improving both performance and efficiency in large language models through adaptive processing and reduced attention computation.  					AI-generated summary 				 Large language models allocate unifor...
[30.01.2026 09:40] ********************************************************************************
[30.01.2026 09:40] Abstract 8. AgentLongBench evaluates large language models as autonomous agents through dynamic environment interactions, revealing challenges in handling high-information-density tool responses compared to memory fragmentation in long conversations.  					AI-generated summary 				 The evolution of Large Langua...
[30.01.2026 09:40] ********************************************************************************
[30.01.2026 09:40] Abstract 9. Agent-RRM, a multi-faceted reward model, provides structured feedback for agentic trajectories through reasoning traces, critiques, and performance scores, with unified feedback integration showing superior performance across diverse benchmarks.  					AI-generated summary 				 Agentic Reinforcement ...
[30.01.2026 09:40] ********************************************************************************
[30.01.2026 09:40] Abstract 10. A novel framework called SCOUT is introduced that uses lightweight scouts to reduce exploration costs for large language models in nonlinguistic environments, enabling improved performance through supervised fine-tuning and reinforcement learning.  					AI-generated summary 				 While Large Language...
[30.01.2026 09:40] ********************************************************************************
[30.01.2026 09:40] Abstract 11. The Qwen3-ASR family introduces speech recognition models with language identification capabilities and a non-autoregressive forced alignment model, achieving state-of-the-art performance and efficient processing.  					AI-generated summary 				 In this report, we introduce Qwen3-ASR family, which i...
[30.01.2026 09:40] ********************************************************************************
[30.01.2026 09:40] Abstract 12. A minimal post-training approach using supervised fine-tuning, on-policy distillation, and small-scale reinforcement fine-tuning enables the development of high-quality sovereign language models with reduced resource requirements.  					AI-generated summary 				 Large language models (LLMs) have pro...
[30.01.2026 09:40] ********************************************************************************
[30.01.2026 09:40] Abstract 13. Multimodal Large Language Models suffer from cross-modal hallucinations where one modality incorrectly influences generation from another, leading to fabricated outputs; this exposes a fundamental deficiency in modality-interaction control. To address this, a training-free method called Modality-Ada...
[30.01.2026 09:40] ********************************************************************************
[30.01.2026 09:40] Abstract 14. VTC-R1 enables efficient long-context reasoning by compressing textual traces into compact images and iteratively feeding them back into vision-language models as optical memory, achieving significant speedup without sacrificing performance.  					AI-generated summary 				 Long-context reasoning has...
[30.01.2026 09:40] ********************************************************************************
[30.01.2026 09:40] Abstract 15. Metric Anything presents a scalable pretraining framework for metric depth estimation that leverages diverse 3D data and sparse metric prompts to achieve superior performance across multiple vision tasks.  					AI-generated summary 				 Scaling has powered recent advances in vision foundation models...
[30.01.2026 09:40] ********************************************************************************
[30.01.2026 09:40] Abstract 16. DeepSearchQA presents a 900-prompt benchmark evaluating agents on complex multi-step information-seeking tasks requiring systematic information collation, deduplication, and reasoning about stopping criteria across 17 fields.  					AI-generated summary 				 We introduce DeepSearchQA, a 900-prompt be...
[30.01.2026 09:40] ********************************************************************************
[30.01.2026 09:40] Abstract 17. Pixel MeanFlow introduces a one-step latent-free image generation method by separating network output space from loss space, achieving strong performance on ImageNet at multiple resolutions.  					AI-generated summary 				 Modern diffusion/flow-based models for image generation typically exhibit two...
[30.01.2026 09:40] ********************************************************************************
[30.01.2026 09:40] Abstract 18. Active latent planning method improves reasoning accuracy and efficiency by modeling latent token supervision as conditional VAE and using reinforcement learning with coherence rewards.  					AI-generated summary 				 Aiming at efficient and dense chain-of-thought (CoT) reasoning, latent reasoning m...
[30.01.2026 09:40] ********************************************************************************
[30.01.2026 09:40] Abstract 19. UniMRG enhances unified multimodal models by training them to generate multiple visual representations, improving both understanding and generation capabilities through complementary information capture.  					AI-generated summary 				 Unified Multimodal Models (UMMs) integrate both visual understan...
[30.01.2026 09:40] ********************************************************************************
[30.01.2026 09:40] Abstract 20. A reinforcement learning-based pretraining method improves language model safety, factuality, and quality by evaluating generations through a combination of model rollouts, original suffixes, and rewritten suffixes.  					AI-generated summary 				 Ensuring safety, factuality and overall quality in t...
[30.01.2026 09:40] ********************************************************************************
[30.01.2026 09:40] Abstract 21. A two-stage trained cybersecurity reasoning model achieves competitive performance on specialized tasks while maintaining general capabilities through supervised fine-tuning and reinforcement learning from verifiable rewards.  					AI-generated summary 				 We present Foundation-Sec-8B-Reasoning, th...
[30.01.2026 09:40] ********************************************************************************
[30.01.2026 09:40] Abstract 22. HALO enables efficient conversion of Transformer models to RNN-attention hybrid architectures with improved long-context performance using minimal training data.  					AI-generated summary 				 Hybrid Transformer architectures, which combine softmax attention blocks and recurrent neural networks (RN...
[30.01.2026 09:40] ********************************************************************************
[30.01.2026 09:40] Abstract 23. BMAM presents a brain-inspired multi-agent memory architecture that decomposes memory into specialized subsystems to address long-term reasoning challenges in language-model-based agents.  					AI-generated summary 				 Language-model-based agents operating over extended interaction horizons face pe...
[30.01.2026 09:40] ********************************************************************************
[30.01.2026 09:40] Abstract 24. FROST is an attention-aware method that improves reasoning efficiency by pruning uncritical paths and removing reasoning outliers, leading to reduced token usage and improved accuracy.  					AI-generated summary 				 We propose FROST, an attention-aware method for efficient reasoning. Unlike traditi...
[30.01.2026 09:40] ********************************************************************************
[30.01.2026 09:40] Abstract 25. Neural audio fingerprinting performance varies with segment length, with short segments (0.5-second) generally providing better retrieval accuracy, and large language models showing promise in recommending optimal segment durations.  					AI-generated summary 				 Audio fingerprinting provides an id...
[30.01.2026 09:40] ********************************************************************************
[30.01.2026 09:40] Abstract 26. PRISM leverages design data to create a knowledge base for improving graphic designs based on natural language instructions, achieving superior style alignment compared to existing methods.  					AI-generated summary 				 Graphic design often involves exploring different stylistic directions, which ...
[30.01.2026 09:40] ********************************************************************************
[30.01.2026 09:40] Abstract 27. Error-compensating optimizer eliminates memory overhead from master weights in quantized LLM training while maintaining near-lossless accuracy.  					AI-generated summary 				 Quantization has significantly improved the compute and memory efficiency of Large Language Model (LLM) training. However, e...
[30.01.2026 09:40] ********************************************************************************
[30.01.2026 09:40] Abstract 28. WebArbiter introduces a reasoning-first WebPRM that formulates reward modeling as text generation to improve web navigation through structured justifications and preference verdicts, outperforming existing baselines in complex web environments.  					AI-generated summary 				 Web agents hold great p...
[30.01.2026 09:40] ********************************************************************************
[30.01.2026 09:40] Abstract 29. Slot-Based Object-Centric Representations outperform global and dense feature representations in robotic manipulation tasks by providing better generalization under visual distribution shifts.  					AI-generated summary 				 The generalization capabilities of robotic manipulation policies are heavil...
[30.01.2026 09:40] ********************************************************************************
[30.01.2026 09:40] Abstract 30. WorldBench is introduced as a video-based benchmark for disentangled evaluation of physical reasoning in generative models, revealing specific failure patterns in current state-of-the-art video world models.  					AI-generated summary 				 Recent advances in generative foundational models, often ter...
[30.01.2026 09:40] ********************************************************************************
[30.01.2026 09:40] Abstract 31. STORM enhances robotic manipulation by adapting visual foundation models with semantic-aware slots through multi-phase training, improving generalization and control performance.  					AI-generated summary 				 Visual foundation models provide strong perceptual features for robotics, but their dense...
[30.01.2026 09:40] Read previous papers.
[30.01.2026 09:40] Generating reviews via LLM API.
[30.01.2026 09:40] Using data from previous issue: {"categories": ["#cv", "#benchmark", "#synthetic", "#reasoning", "#dataset", "#training", "#multimodal"], "emoji": "üß©", "ru": {"title": "–ü—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –≤ text-to-image –º–æ–¥–µ–ª—è—Ö —á–µ—Ä–µ–∑ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ SpatialGenEval –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å
[30.01.2026 09:40] Using data from previous issue: {"categories": ["#graphs", "#hallucinations", "#long_context", "#science"], "emoji": "üß¨", "ru": {"title": "–ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–µ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∑–Ω–∞–Ω–∏–π –¥–ª—è –Ω–∞–¥—ë–∂–Ω–æ–≥–æ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –Ω–∞—É—á–Ω–æ–≥–æ –æ—Ç–∫—Ä—ã—Ç–∏—è", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è Idea2Story ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –Ω–∞—É—á–Ω–æ–≥–æ –æ—Ç–∫—Ä—ã—Ç–∏—è, –∫–æ—Ç–æ—Ä—ã–π –ø–µ—Ä–µ–Ω–æ—Å–∏—Ç –æ–±—Ä–∞–±–æ
[30.01.2026 09:40] Using data from previous issue: {"categories": ["#data", "#cv", "#open_source", "#dataset", "#science", "#training", "#optimization", "#rl", "#multimodal"], "emoji": "üìÑ", "ru": {"title": "–£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è–º–∏", "desc": "OCRVerse –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π 
[30.01.2026 09:40] Using data from previous issue: {"categories": ["#multimodal", "#dataset", "#benchmark", "#robotics", "#inference", "#architecture", "#transfer_learning", "#small_models", "#synthetic"], "emoji": "ü§ñ", "ru": {"title": "–ë—ã—Å—Ç—Ä–∞—è –∏ –∞–¥–∞–ø—Ç–∏–≤–Ω–∞—è –º–∞–Ω–∏–ø—É–ª—è—Ü–∏—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–º–∏ –æ–±—ä–µ–∫—Ç–∞–º–∏ —á–µ—Ä–µ–∑ –∫–æ–º–ø–∞–∫—Ç–Ω—ã–µ vision-language –º–æ–¥–µ–ª–∏", "desc": "Dynami
[30.01.2026 09:40] Using data from previous issue: {"categories": [], "emoji": "üé¨", "ru": {"title": "–†–∞–∑–¥–µ–ª—ë–Ω–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–µ–æ–º–µ—Ç—Ä–∏–∏ –∏ –≤–Ω–µ—à–Ω–µ–≥–æ –≤–∏–¥–∞ –¥–ª—è –±—ã—Å—Ç—Ä–æ–π —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ —Å—Ü–µ–Ω", "desc": "PLANING ‚Äî —ç—Ç–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –ø–æ—Ç–æ–∫–æ–≤–æ–π —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ —Å—Ü–µ–Ω –∏–∑ –º–æ–Ω–æ–∫—É–ª—è—Ä–Ω—ã—Ö –≤–∏–¥–µ–æ–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π, –∫–æ—Ç–æ—Ä–∞—è –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç —è–≤–Ω—ã–µ –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–∏—Ç–∏–≤—ã —Å –Ω
[30.01.2026 09:40] Using data from previous issue: {"categories": ["#dataset", "#reasoning", "#multimodal", "#open_source", "#training", "#small_models", "#synthetic"], "emoji": "üß†", "ru": {"title": "–í—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –¥–µ–ª–∞—é—Ç –º–∞–ª–µ–Ω—å–∫–∏–µ –º–æ–¥–µ–ª–∏ —É–º–Ω–µ–µ –±–æ–ª—å—à–∏—Ö", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ MMFineReason ‚Äî –±–æ–ª—å—à–æ–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –¥–∞—Ç
[30.01.2026 09:40] Using data from previous issue: {"categories": ["#architecture", "#inference", "#training", "#optimization", "#open_source"], "emoji": "‚ö°", "ru": {"title": "–≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –≤–º–µ—Å—Ç–æ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤: –Ω–æ–≤—ã–π –ø—É—Ç—å –∫ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–º—É –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—é", "desc": "–í —ç—Ç–æ–π —Ä–∞–±–æ—Ç–µ –∞–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∫–∞–∫ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—É —ç–∫—Å
[30.01.2026 09:40] Using data from previous issue: {"categories": ["#inference", "#multimodal", "#architecture", "#training"], "emoji": "‚ö°", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–æ–∫–µ–Ω–æ–≤ —á–µ—Ä–µ–∑ –∫–æ–Ω—Ü–µ–ø—Ç—É–∞–ª—å–Ω–æ–µ —Å–ª–∏—è–Ω–∏–µ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π", "desc": "ConceptMoE ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö, –∫–æ
[30.01.2026 09:40] Querying the API.
[30.01.2026 09:40] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

AgentLongBench evaluates large language models as autonomous agents through dynamic environment interactions, revealing challenges in handling high-information-density tool responses compared to memory fragmentation in long conversations.  					AI-generated summary 				 The evolution of Large Language Models (LLMs) into autonomous agents necessitates the management of extensive, dynamic contexts. Current benchmarks, however, remain largely static, relying on passive retrieval tasks that fail to simulate the complexities of agent-environment interaction, such as non-linear reasoning and iterative feedback. To address this, we introduce AgentLongBench, which evaluates agents through simulated environment rollouts based on Lateral Thinking Puzzles. This framework generates rigorous interaction trajectories across knowledge-intensive and knowledge-free scenarios. Experiments with state-of-the-art models and memory systems (32K to 4M tokens) expose a critical weakness: while adept at static retrieval, agents struggle with the dynamic information synthesis essential for workflows. Our analysis indicates that this degradation is driven by the minimum number of tokens required to resolve a query. This factor explains why the high information density inherent in massive tool responses poses a significantly greater challenge than the memory fragmentation typical of long-turn dialogues.
[30.01.2026 09:40] Response: ```json
{
  "desc": "AgentLongBench ‚Äî —ç—Ç–æ –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ —Ä–æ–ª–∏ –∞–≤—Ç–æ–Ω–æ–º–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ —Å –æ–∫—Ä—É–∂–µ–Ω–∏–µ–º. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö, —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –º–æ–¥–µ–ª–∏—Ä—É–µ—Ç —Å–ª–æ–∂–Ω—ã–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –≥–æ–ª–æ–≤–æ–ª–æ–º–æ–∫, –≥–µ–Ω–µ—Ä–∏—Ä—É—è —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –≤ –∑–Ω–∞–Ω–∏–µ—ë–º–∫–∏—Ö –∏ —Å–≤–æ–±–æ–¥–Ω—ã—Ö –æ—Ç –∑–Ω–∞–Ω–∏–π –∑–∞–¥–∞—á–∞—Ö. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –≤—ã—è–≤–∏–ª–æ –∫—Ä–∏—Ç–∏—á–µ—Å–∫—É—é —Å–ª–∞–±–æ—Å—Ç—å —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∏ —Å–∏—Å—Ç–µ–º –ø–∞–º—è—Ç–∏: —Ö–æ—Ç—è –æ–Ω–∏ —Ö–æ—Ä–æ—à–æ —Å–ø—Ä–∞–≤–ª—è—é—Ç—Å—è —Å –ø–∞—Å—Å–∏–≤–Ω—ã–º –ø–æ–∏—Å–∫–æ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, –æ–Ω–∏ –∏—Å–ø—ã—Ç—ã–≤–∞—é—Ç —Ç—Ä—É–¥–Ω–æ—Å—Ç–∏ —Å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–º —Å–∏–Ω—Ç–µ–∑–æ–º –¥–∞–Ω–Ω—ã—Ö, –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–º –¥–ª—è –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞–±–æ—á–∏—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤. –ê–Ω–∞–ª–∏–∑ –ø–æ–∫–∞–∑–∞–ª, —á—Ç–æ –æ—Å–Ω–æ–≤–Ω–æ–π –ø—Ä–∏—á–∏–Ω–æ–π –¥–µ–≥—Ä–∞–¥–∞—Ü–∏–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —è–≤–ª—è–µ—Ç—Å—è –≤—ã—Å–æ–∫–∞—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –æ—Ç–≤–µ—Ç–∞—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤, —á—Ç–æ —Å–ª–æ–∂–Ω–µ–µ, —á–µ–º —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞—Ü–∏—è –ø–∞–º—è—Ç–∏ –≤ –¥–ª–∏–Ω–Ω—ã—Ö –¥–∏–∞–ª–æ–≥–∞—Ö.",
  "emoji": "üîÑ",
  "title": "–ê–≥–µ–Ω—Ç—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM: –æ—Ç —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Å—Ç–æ–≤ –∫ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–º—É —Å–∏–Ω—Ç–µ–∑—É –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏"
}
```
[30.01.2026 09:40] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"AgentLongBench evaluates large language models as autonomous agents through dynamic environment interactions, revealing challenges in handling high-information-density tool responses compared to memory fragmentation in long conversations.  					AI-generated summary 				 The evolution of Large Language Models (LLMs) into autonomous agents necessitates the management of extensive, dynamic contexts. Current benchmarks, however, remain largely static, relying on passive retrieval tasks that fail to simulate the complexities of agent-environment interaction, such as non-linear reasoning and iterative feedback. To address this, we introduce AgentLongBench, which evaluates agents through simulated environment rollouts based on Lateral Thinking Puzzles. This framework generates rigorous interaction trajectories across knowledge-intensive and knowledge-free scenarios. Experiments with state-of-the-art models and memory systems (32K to 4M tokens) expose a critical weakness: while adept at static retrieval, agents struggle with the dynamic information synthesis essential for workflows. Our analysis indicates that this degradation is driven by the minimum number of tokens required to resolve a query. This factor explains why the high information density inherent in massive tool responses poses a significantly greater challenge than the memory fragmentation typical of long-turn dialogues."

[30.01.2026 09:40] Response: ```python
["BENCHMARK", "AGENTS", "DATASET"]
```
[30.01.2026 09:40] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"AgentLongBench evaluates large language models as autonomous agents through dynamic environment interactions, revealing challenges in handling high-information-density tool responses compared to memory fragmentation in long conversations.  					AI-generated summary 				 The evolution of Large Language Models (LLMs) into autonomous agents necessitates the management of extensive, dynamic contexts. Current benchmarks, however, remain largely static, relying on passive retrieval tasks that fail to simulate the complexities of agent-environment interaction, such as non-linear reasoning and iterative feedback. To address this, we introduce AgentLongBench, which evaluates agents through simulated environment rollouts based on Lateral Thinking Puzzles. This framework generates rigorous interaction trajectories across knowledge-intensive and knowledge-free scenarios. Experiments with state-of-the-art models and memory systems (32K to 4M tokens) expose a critical weakness: while adept at static retrieval, agents struggle with the dynamic information synthesis essential for workflows. Our analysis indicates that this degradation is driven by the minimum number of tokens required to resolve a query. This factor explains why the high information density inherent in massive tool responses poses a significantly greater challenge than the memory fragmentation typical of long-turn dialogues."

[30.01.2026 09:40] Response: ```python
['LONG_CONTEXT', 'REASONING']
```
[30.01.2026 09:40] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The paper introduces AgentLongBench, a new framework for evaluating large language models (LLMs) as autonomous agents in dynamic environments. It highlights the limitations of current benchmarks that focus on static tasks, which do not adequately reflect the complexities of real-world interactions. Through the use of Lateral Thinking Puzzles, AgentLongBench assesses how well agents can manage information in both knowledge-rich and knowledge-poor contexts. The findings reveal that while LLMs excel at retrieving static information, they struggle with synthesizing dynamic information, particularly when faced with high-density tool responses.","title":"Evaluating LLMs: From Static Retrieval to Dynamic Interaction Challenges"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='The paper introduces AgentLongBench, a new framework for evaluating large language models (LLMs) as autonomous agents in dynamic environments. It highlights the limitations of current benchmarks that focus on static tasks, which do not adequately reflect the complexities of real-world interactions. Through the use of Lateral Thinking Puzzles, AgentLongBench assesses how well agents can manage information in both knowledge-rich and knowledge-poor contexts. The findings reveal that while LLMs excel at retrieving static information, they struggle with synthesizing dynamic information, particularly when faced with high-density tool responses.', title='Evaluating LLMs: From Static Retrieval to Dynamic Interaction Challenges'))
[30.01.2026 09:40] Response: ParsedChatCompletionMessage[Article](content='{"desc":"AgentLongBench ÊòØ‰∏Ä‰∏™ËØÑ‰º∞Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã‰Ωú‰∏∫Ëá™‰∏ªÊô∫ËÉΩ‰ΩìÁöÑÂ∑•ÂÖ∑ÔºåÈáçÁÇπÂú®‰∫éÂä®ÊÄÅÁéØÂ¢É‰∫§‰∫í‰∏≠ÁöÑË°®Áé∞„ÄÇËØ•Á†îÁ©∂ÊåáÂá∫ÔºåÁé∞ÊúâÁöÑÂü∫ÂáÜÊµãËØï‰∏ªË¶ÅÊòØÈùôÊÄÅÁöÑÔºåÊó†Ê≥ïÊúâÊïàÊ®°ÊãüÊô∫ËÉΩ‰Ωì‰∏éÁéØÂ¢É‰πãÈó¥Â§çÊùÇÁöÑ‰∫íÂä®„ÄÇÈÄöËøáÂºïÂÖ•Âü∫‰∫é‰æßÂêëÊÄùÁª¥ÈöæÈ¢òÁöÑÊ®°ÊãüÁéØÂ¢ÉÔºåAgentLongBench ËÉΩÂ§üÁîüÊàê‰∏•Ê†ºÁöÑ‰∫§‰∫íËΩ®ËøπÔºåÊè≠Á§∫‰∫ÜÊô∫ËÉΩ‰ΩìÂú®Â§ÑÁêÜÈ´ò‰ø°ÊÅØÂØÜÂ∫¶ÁöÑÂ∑•ÂÖ∑ÂìçÂ∫îÊó∂ÁöÑÊåëÊàò„ÄÇÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåÂ∞ΩÁÆ°Êô∫ËÉΩ‰ΩìÂú®ÈùôÊÄÅÊ£ÄÁ¥¢ÊñπÈù¢Ë°®Áé∞ËâØÂ•ΩÔºå‰ΩÜÂú®Âä®ÊÄÅ‰ø°ÊÅØÁªºÂêàÊñπÈù¢Âç¥Â≠òÂú®ÊòæËëóÁöÑÂº±ÁÇπ„ÄÇ","title":"ËØÑ‰º∞Ëá™‰∏ªÊô∫ËÉΩ‰ΩìÁöÑÂä®ÊÄÅÁéØÂ¢É‰∫§‰∫íËÉΩÂäõ"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='AgentLongBench ÊòØ‰∏Ä‰∏™ËØÑ‰º∞Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã‰Ωú‰∏∫Ëá™‰∏ªÊô∫ËÉΩ‰ΩìÁöÑÂ∑•ÂÖ∑ÔºåÈáçÁÇπÂú®‰∫éÂä®ÊÄÅÁéØÂ¢É‰∫§‰∫í‰∏≠ÁöÑË°®Áé∞„ÄÇËØ•Á†îÁ©∂ÊåáÂá∫ÔºåÁé∞ÊúâÁöÑÂü∫ÂáÜÊµãËØï‰∏ªË¶ÅÊòØÈùôÊÄÅÁöÑÔºåÊó†Ê≥ïÊúâÊïàÊ®°ÊãüÊô∫ËÉΩ‰Ωì‰∏éÁéØÂ¢É‰πãÈó¥Â§çÊùÇÁöÑ‰∫íÂä®„ÄÇÈÄöËøáÂºïÂÖ•Âü∫‰∫é‰æßÂêëÊÄùÁª¥ÈöæÈ¢òÁöÑÊ®°ÊãüÁéØÂ¢ÉÔºåAgentLongBench ËÉΩÂ§üÁîüÊàê‰∏•Ê†ºÁöÑ‰∫§‰∫íËΩ®ËøπÔºåÊè≠Á§∫‰∫ÜÊô∫ËÉΩ‰ΩìÂú®Â§ÑÁêÜÈ´ò‰ø°ÊÅØÂØÜÂ∫¶ÁöÑÂ∑•ÂÖ∑ÂìçÂ∫îÊó∂ÁöÑÊåëÊàò„ÄÇÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåÂ∞ΩÁÆ°Êô∫ËÉΩ‰ΩìÂú®ÈùôÊÄÅÊ£ÄÁ¥¢ÊñπÈù¢Ë°®Áé∞ËâØÂ•ΩÔºå‰ΩÜÂú®Âä®ÊÄÅ‰ø°ÊÅØÁªºÂêàÊñπÈù¢Âç¥Â≠òÂú®ÊòæËëóÁöÑÂº±ÁÇπ„ÄÇ', title='ËØÑ‰º∞Ëá™‰∏ªÊô∫ËÉΩ‰ΩìÁöÑÂä®ÊÄÅÁéØÂ¢É‰∫§‰∫íËÉΩÂäõ'))
[30.01.2026 09:40] Using data from previous issue: {"categories": ["#dataset", "#reasoning", "#benchmark", "#open_source", "#rl", "#optimization", "#agents"], "emoji": "üéØ", "ru": {"title": "–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ–±—Ä–∞—Ç–Ω–∞—è —Å–≤—è–∑—å –¥–ª—è —É–º–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Agent-RRM, –º–Ω–æ–≥–æ–∞—Å–ø–µ–∫—Ç–Ω—É—é –º–æ–¥–µ–ª—å –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä–∞—è –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å
[30.01.2026 09:40] Using data from previous issue: {"categories": ["#reasoning", "#training", "#rl", "#optimization", "#transfer_learning", "#agents", "#small_models"], "emoji": "üîç", "ru": {"title": "–†–∞–∑–≤–µ–¥—á–∏–∫–∏ –¥–ª—è —Ä–∞–∑—É–º–Ω–æ–π —Ä–∞–∑–≤–µ–¥–∫–∏: –∫–∞–∫ –º–∞–ª–µ–Ω—å–∫–∏–µ –º–æ–¥–µ–ª–∏ —É—á–∞—Ç –±–æ–ª—å—à–∏—Ö", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ SCOUT, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ª—ë–≥–∫–∏—Ö –∞–≥–µ–Ω—Ç–æ–≤-
[30.01.2026 09:40] Using data from previous issue: {"categories": ["#benchmark", "#multilingual", "#inference", "#low_resource", "#audio", "#open_source"], "emoji": "üé§", "ru": {"title": "–ú–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏ —Å –Ω–µ–∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–º –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ–º", "desc": "–°–µ–º—å—è –º–æ–¥–µ–ª–µ–π Qwen3-ASR –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –Ω–∞–±–æ—Ä —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ—á
[30.01.2026 09:40] Using data from previous issue: {"categories": ["#rlhf", "#reasoning", "#open_source", "#training", "#multilingual", "#optimization", "#low_resource", "#small_models"], "emoji": "üèõÔ∏è", "ru": {"title": "–°—É–≤–µ—Ä–µ–Ω–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –¥–æ–æ–±—É—á–µ–Ω–∏—é —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π,
[30.01.2026 09:40] Using data from previous issue: {"categories": ["#multimodal", "#video", "#audio", "#inference"], "emoji": "üéØ", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –ø–æ–¥–∞–≤–ª–µ–Ω–∏—è –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–í —Å—Ç–∞—Ç—å–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ –∫—Ä–æ—Å—Å-–º–æ–¥–∞–ª—å–Ω—ã—Ö –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö, –∫–æ–≥–¥–∞ 
[30.01.2026 09:40] Using data from previous issue: {"categories": ["#multimodal", "#reasoning", "#benchmark", "#open_source", "#training", "#long_context", "#inference"], "emoji": "üß†", "ru": {"title": "–û–ø—Ç–∏—á–µ—Å–∫–∞—è –ø–∞–º—è—Ç—å –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –≤—ã–≤–æ–¥–∞: —Å–∂–∞—Ç–∏–µ —á–µ—Ä–µ–∑ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é", "desc": "VTC-R1 ‚Äî –Ω–æ–≤–∞—è –ø–∞—Ä–∞–¥–∏–≥–º–∞ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä–∞
[30.01.2026 09:40] Using data from previous issue: {"categories": ["#3d", "#cv", "#architecture", "#transfer_learning", "#open_source", "#dataset", "#optimization", "#multimodal"], "emoji": "üìè", "ru": {"title": "–ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞–∫ –ø—É—Ç—å –∫ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–º—É –≤–æ—Å–ø—Ä–∏—è—Ç–∏—é –º–µ—Ç—Ä–∏—á–µ—Å–∫–æ–π –≥–ª—É–±–∏–Ω—ã", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Metric Anything ‚Äî –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—É—é —Å–∏
[30.01.2026 09:40] Using data from previous issue: {"categories": ["#dataset", "#agents", "#benchmark"], "emoji": "üîç", "ru": {"title": "–ì–ª—É–±–æ–∫–∏–π –ø–æ–∏—Å–∫ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏: –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –º–Ω–æ–≥–æ—à–∞–≥–æ–≤–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤", "desc": "–†–∞–±–æ—Ç–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –±–µ–Ω—á–º–∞—Ä–∫ DeepSearchQA —Å 900 –∑–∞–¥–∞—á–∞–º–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ —Å–ª–æ–∂–Ω—ã—Ö –º–Ω–æ–≥–æ—à–∞–≥–æ–≤—ã—Ö –∑–∞–¥–∞—á–∞—Ö –ø–æ–∏—Å–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏. –ë–µ–Ω
[30.01.2026 09:40] Using data from previous issue: {"categories": ["#diffusion", "#cv", "#architecture", "#optimization", "#training"], "emoji": "‚ö°", "ru": {"title": "–û–¥–Ω–æ—à–∞–≥–æ–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –±–µ–∑ —Å–∫—Ä—ã—Ç–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ —á–µ—Ä–µ–∑ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –≤—ã—Ö–æ–¥–æ–≤ –∏ –ø–æ—Ç–µ—Ä—å", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –º–µ—Ç–æ–¥ Pixel MeanFlow –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∑–∞ –æ–¥–∏–Ω —à–∞–≥
[30.01.2026 09:40] Using data from previous issue: {"categories": ["#reasoning", "#training", "#rl", "#optimization", "#small_models"], "emoji": "üß†", "ru": {"title": "–ê–∫—Ç–∏–≤–Ω–æ–µ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ —Å–∫—Ä—ã—Ç–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –º–µ—Ç–æ–¥ –∞–∫—Ç–∏–≤–Ω–æ–≥–æ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –≤ —Å–∫—Ä—ã—Ç–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è —É–ª—É
[30.01.2026 09:40] Using data from previous issue: {"categories": ["#multimodal", "#cv", "#training"], "emoji": "üîÑ", "ru": {"title": "–í–∑–∞–∏–º–Ω–æ–µ —É—Å–∏–ª–µ–Ω–∏–µ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è", "desc": "UniMRG ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥ –ø–æ—Å—Ç–æ–±—É—á–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –ø—É—Ç—ë–º –æ–±—É—á–µ–Ω–∏—è –∏—Ö –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤—Å
[30.01.2026 09:40] Using data from previous issue: {"categories": ["#rl", "#training", "#rlhf"], "emoji": "üõ°Ô∏è", "ru": {"title": "–ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –∏ —Ñ–∞–∫–æ—Ç–∏—á–Ω–æ—Å—Ç—å –≤–æ –≤—Ä–µ–º—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —á–µ—Ä–µ–∑ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –æ–±—É—á–µ–Ω–∏–∏ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –∫–æ—Ç
[30.01.2026 09:40] Using data from previous issue: {"categories": ["#rlhf", "#dataset", "#reasoning", "#benchmark", "#open_source", "#training", "#science", "#small_models"], "emoji": "üîê", "ru": {"title": "–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ –≤ –∫–æ–º–ø–∞–∫—Ç–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ: —Ä–∞—Å—Å—É–∂–¥–∞—é—â–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –∫–∏–±–µ—Ä–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ Foundation-Sec-8B-Reasoning ‚Äî
[30.01.2026 09:40] Using data from previous issue: {"categories": ["#long_context", "#transfer_learning", "#architecture", "#optimization", "#training", "#inference"], "emoji": "‚ö°", "ru": {"title": "–ì–∏–±—Ä–∏–¥–Ω—ã–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã: —É—Å–∫–æ—Ä–µ–Ω–∏–µ –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ —á–µ—Ä–µ–∑ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—é –≤ RNN —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω HALO ‚Äî –º–µ—Ç–æ–¥ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –¥
[30.01.2026 09:40] Using data from previous issue: {"categories": ["#architecture", "#benchmark", "#reasoning", "#long_context", "#agents"], "emoji": "üß†", "ru": {"title": "–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –ø–∞–º—è—Ç—å –∫–∞–∫ –∫–ª—é—á –∫ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –ø–∞–º—è—Ç–∏ BMAM, –≤–¥–æ—Ö–Ω–æ–≤–ª–µ–Ω–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π –º–æ–∑–≥–∞, –∫–æ—Ç–æ—Ä–∞—è —Ä–∞–∑–¥–µ–ª—è–µ—Ç –ø–∞–º
[30.01.2026 09:40] Using data from previous issue: {"categories": ["#optimization", "#open_source", "#reasoning"], "emoji": "‚úÇÔ∏è", "ru": {"title": "–£–º–Ω–æ–µ –æ–±—Ä–µ–∑–∞–Ω–∏–µ —Ü–µ–ø–æ—á–µ–∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —á–µ—Ä–µ–∑ –∞–Ω–∞–ª–∏–∑ –≤–Ω–∏–º–∞–Ω–∏—è", "desc": "FROST ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –∞–Ω–∞–ª–∏–∑–µ –≤–µ—Å–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è (attention weights) –≤ –Ω–µ–π—Ä–æ—Å–µ—Ç—è—Ö. –ü–æ–¥—Ö–æ–¥ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ 
[30.01.2026 09:40] Using data from previous issue: {"categories": ["#audio", "#architecture", "#benchmark"], "emoji": "üîç", "ru": {"title": "–û–ø—Ç–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Å–µ–≥–º–µ–Ω—Ç–∞: –∫–ª—é—á –∫ —Ç–æ—á–Ω–æ—Å—Ç–∏ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤—ã—Ö –∞—É–¥–∏–æ—Ñ–∏–Ω–≥–µ—Ä–ø—Ä–∏–Ω—Ç–æ–≤", "desc": "–í —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –≤–ª–∏—è–Ω–∏–µ –¥–ª–∏–Ω—ã –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –æ–∫–Ω–∞ –Ω–∞ –∫–∞—á–µ—Å—Ç–≤–æ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤—ã—Ö –∞—É–¥–∏–æ—Ñ–∏–Ω–≥–µ—Ä–ø—Ä–∏–Ω—Ç–æ–≤ ‚Äî –∫–æ–º–ø–∞–∫—Ç–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –∑–≤
[30.01.2026 09:40] Using data from previous issue: {"categories": [], "emoji": "üé®", "ru": {"title": "–ó–Ω–∞–Ω–∏—è –∏–∑ –¥–∞–Ω–Ω—ã—Ö –¥–∏–∑–∞–π–Ω–∞ –¥–ª—è —Å—Ç–∏–ª–∏—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ —É–ª—É—á—à–µ–Ω–∏—è –ø–æ —Ç–µ–∫—Å—Ç—É", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ PRISM –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏—Ö –¥–∏–∑–∞–π–Ω–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞, –∫–æ—Ç–æ—Ä—ã–π —Å–æ–∑–¥–∞—ë—Ç –±–∞–∑—É –∑–Ω–∞–Ω–∏–π –∏–∑ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∏–∑–∞–π–Ω-–¥–∞–Ω–Ω—ã—Ö. –ê–≤—Ç–æ—Ä—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ
[30.01.2026 09:40] Querying the API.
[30.01.2026 09:40] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Error-compensating optimizer eliminates memory overhead from master weights in quantized LLM training while maintaining near-lossless accuracy.  					AI-generated summary 				 Quantization has significantly improved the compute and memory efficiency of Large Language Model (LLM) training. However, existing approaches still rely on accumulating their updates in high-precision: concretely, gradient updates must be applied to a high-precision weight buffer, known as master weights. This buffer introduces substantial memory overhead, particularly for Sparse Mixture of Experts (SMoE) models, where model parameters and optimizer states dominate memory usage. To address this, we introduce the Error-Compensating Optimizer (ECO), which eliminates master weights by applying updates directly to quantized parameters. ECO quantizes weights after each step and carefully injects the resulting quantization error into the optimizer momentum, forming an error-feedback loop with no additional memory. We prove that, under standard assumptions and a decaying learning rate, ECO converges to a constant-radius neighborhood of the optimum, while naive master-weight removal can incur an error that is inversely proportional to the learning rate. We show empirical results for pretraining small Transformers (30-800M), a Gemma-3 1B model, and a 2.1B parameter Sparse MoE model with FP8 quantization, and fine-tuning DeepSeek-MoE-16B in INT4 precision. Throughout, ECO matches baselines with master weights up to near-lossless accuracy, significantly shifting the static memory vs validation loss Pareto frontier.
[30.01.2026 09:40] Response: ```json
{
  "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è Error-Compensating Optimizer (ECO) ‚Äî –Ω–æ–≤—ã–π –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä, –∫–æ—Ç–æ—Ä—ã–π —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –≤—ã—Å–æ–∫–æ–≥–æ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏—è –ø–∞–º—è—Ç–∏ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ö–ª—é—á–µ–≤–æ–µ –Ω–æ–≤–æ–≤–≤–µ–¥–µ–Ω–∏–µ –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —Ç–æ–º, —á—Ç–æ ECO –∏–∑–±–∞–≤–ª—è–µ—Ç—Å—è –æ—Ç –±—É—Ñ–µ—Ä–∞ –≤—ã—Å–æ–∫–æ—Ç–æ—á–Ω—ã—Ö –≤–µ—Å–æ–≤ (master weights) –∏ –ø—Ä–∏–º–µ–Ω—è–µ—Ç –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –Ω–µ–ø–æ—Å—Ä–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ –∫ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω—ã–º –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º. –ê–ª–≥–æ—Ä–∏—Ç–º –∫–æ–º–ø–µ–Ω—Å–∏—Ä—É–µ—Ç –æ—à–∏–±–∫–∏ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏, –≤–Ω–µ–¥—Ä—è—è –∏—Ö –≤ –∏–º–ø—É–ª—å—Å –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞, —Å–æ–∑–¥–∞–≤–∞—è –∑–∞–º–∫–Ω—É—Ç—ã–π —Ü–∏–∫–ª –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–∞—Ç—Ä–∞—Ç –ø–∞–º—è—Ç–∏. –ê–≤—Ç–æ—Ä—ã –¥–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ ECO —Å—Ö–æ–¥–∏—Ç—Å—è –∫ –æ–ø—Ç–∏–º—É–º—É —Å –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é –∏ –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞–ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ –º–æ–¥–µ–ª—è—Ö —Ä–∞–∑–º–µ—Ä–æ–º –æ—Ç 30–ú –¥–æ 16B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–µ–π FP8 –∏ INT4.",
  "emoji": "‚ö°",
  "title": "–û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä —Å –∫–æ–º–ø–µ–Ω—Å–∞—Ü–∏–µ–π –æ—à–∏–±–æ–∫: –æ–±—É—á–µ–Ω–∏–µ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω—ã—Ö LLM –±–µ–∑ –ø–æ—Ç–µ—Ä—å –ø–∞–º—è—Ç–∏"
}
```
[30.01.2026 09:40] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Error-compensating optimizer eliminates memory overhead from master weights in quantized LLM training while maintaining near-lossless accuracy.  					AI-generated summary 				 Quantization has significantly improved the compute and memory efficiency of Large Language Model (LLM) training. However, existing approaches still rely on accumulating their updates in high-precision: concretely, gradient updates must be applied to a high-precision weight buffer, known as master weights. This buffer introduces substantial memory overhead, particularly for Sparse Mixture of Experts (SMoE) models, where model parameters and optimizer states dominate memory usage. To address this, we introduce the Error-Compensating Optimizer (ECO), which eliminates master weights by applying updates directly to quantized parameters. ECO quantizes weights after each step and carefully injects the resulting quantization error into the optimizer momentum, forming an error-feedback loop with no additional memory. We prove that, under standard assumptions and a decaying learning rate, ECO converges to a constant-radius neighborhood of the optimum, while naive master-weight removal can incur an error that is inversely proportional to the learning rate. We show empirical results for pretraining small Transformers (30-800M), a Gemma-3 1B model, and a 2.1B parameter Sparse MoE model with FP8 quantization, and fine-tuning DeepSeek-MoE-16B in INT4 precision. Throughout, ECO matches baselines with master weights up to near-lossless accuracy, significantly shifting the static memory vs validation loss Pareto frontier."

[30.01.2026 09:41] Response: ```python
["INFERENCE", "TRAINING", "ARCHITECTURE"]
```
[30.01.2026 09:41] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Error-compensating optimizer eliminates memory overhead from master weights in quantized LLM training while maintaining near-lossless accuracy.  					AI-generated summary 				 Quantization has significantly improved the compute and memory efficiency of Large Language Model (LLM) training. However, existing approaches still rely on accumulating their updates in high-precision: concretely, gradient updates must be applied to a high-precision weight buffer, known as master weights. This buffer introduces substantial memory overhead, particularly for Sparse Mixture of Experts (SMoE) models, where model parameters and optimizer states dominate memory usage. To address this, we introduce the Error-Compensating Optimizer (ECO), which eliminates master weights by applying updates directly to quantized parameters. ECO quantizes weights after each step and carefully injects the resulting quantization error into the optimizer momentum, forming an error-feedback loop with no additional memory. We prove that, under standard assumptions and a decaying learning rate, ECO converges to a constant-radius neighborhood of the optimum, while naive master-weight removal can incur an error that is inversely proportional to the learning rate. We show empirical results for pretraining small Transformers (30-800M), a Gemma-3 1B model, and a 2.1B parameter Sparse MoE model with FP8 quantization, and fine-tuning DeepSeek-MoE-16B in INT4 precision. Throughout, ECO matches baselines with master weights up to near-lossless accuracy, significantly shifting the static memory vs validation loss Pareto frontier."

[30.01.2026 09:41] Response: ```python
["OPTIMIZATION"]
```
[30.01.2026 09:41] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents the Error-Compensating Optimizer (ECO), which enhances the training of quantized Large Language Models (LLMs) by removing the need for high-precision master weights. Traditional methods require maintaining a master weight buffer, leading to significant memory overhead, especially in Sparse Mixture of Experts (SMoE) models. ECO directly updates quantized parameters and incorporates quantization errors into the optimizer\'s momentum, creating an efficient error-feedback loop without additional memory costs. The results demonstrate that ECO achieves near-lossless accuracy while reducing memory usage, thus improving the efficiency of LLM training.","title":"ECO: Optimizing Memory Efficiency in Quantized LLM Training"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="This paper presents the Error-Compensating Optimizer (ECO), which enhances the training of quantized Large Language Models (LLMs) by removing the need for high-precision master weights. Traditional methods require maintaining a master weight buffer, leading to significant memory overhead, especially in Sparse Mixture of Experts (SMoE) models. ECO directly updates quantized parameters and incorporates quantization errors into the optimizer's momentum, creating an efficient error-feedback loop without additional memory costs. The results demonstrate that ECO achieves near-lossless accuracy while reducing memory usage, thus improving the efficiency of LLM training.", title='ECO: Optimizing Memory Efficiency in Quantized LLM Training'))
[30.01.2026 09:41] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÈîôËØØË°•ÂÅø‰ºòÂåñÂô®ÔºàECOÔºâÔºåÊó®Âú®Ê∂àÈô§ÈáèÂåñÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâËÆ≠ÁªÉ‰∏≠ÁöÑ‰∏ªÊùÉÈáçÂÜÖÂ≠òÂºÄÈîÄÔºåÂêåÊó∂‰øùÊåÅÊé•ËøëÊó†ÊçüÁöÑÂáÜÁ°ÆÊÄß„ÄÇ‰º†ÁªüÊñπÊ≥ï‰æùËµñ‰∫éÈ´òÁ≤æÂ∫¶ÁöÑ‰∏ªÊùÉÈáçÁºìÂÜ≤Âå∫Êù•Á¥ØÁßØÊ¢ØÂ∫¶Êõ¥Êñ∞ÔºåËøôÂú®Á®ÄÁñè‰∏ìÂÆ∂Ê®°Âûã‰∏≠‰ºöÂØºËá¥ÊòæËëóÁöÑÂÜÖÂ≠òÂç†Áî®„ÄÇECOÈÄöËøáÁõ¥Êé•ÂØπÈáèÂåñÂèÇÊï∞Â∫îÁî®Êõ¥Êñ∞ÔºåÈÅøÂÖç‰∫Ü‰∏ªÊùÉÈáçÁöÑ‰ΩøÁî®ÔºåÂπ∂Âú®‰ºòÂåñÂô®Âä®Èáè‰∏≠Ê≥®ÂÖ•ÈáèÂåñËØØÂ∑ÆÔºåÂΩ¢Êàê‰∏Ä‰∏™Êó†È¢ùÂ§ñÂÜÖÂ≠òÁöÑÂèçÈ¶àÂæ™ÁéØ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåECOÂú®Â∞èÂûãTransformerÂíåÁ®ÄÁñèMoEÊ®°ÂûãÁöÑÈ¢ÑËÆ≠ÁªÉ‰∏≠ÔºåËÉΩÂ§üÂú®‰∏çÂ¢ûÂä†ÂÜÖÂ≠òÁöÑÊÉÖÂÜµ‰∏ãÔºåËææÂà∞‰∏é‰º†ÁªüÊñπÊ≥ïÁõ∏‰ººÁöÑÂáÜÁ°ÆÊÄß„ÄÇ","title":"ÈîôËØØË°•ÂÅø‰ºòÂåñÂô®ÔºöÈáèÂåñËÆ≠ÁªÉÁöÑÊñ∞Á™ÅÁ†¥"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÈîôËØØË°•ÂÅø‰ºòÂåñÂô®ÔºàECOÔºâÔºåÊó®Âú®Ê∂àÈô§ÈáèÂåñÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâËÆ≠ÁªÉ‰∏≠ÁöÑ‰∏ªÊùÉÈáçÂÜÖÂ≠òÂºÄÈîÄÔºåÂêåÊó∂‰øùÊåÅÊé•ËøëÊó†ÊçüÁöÑÂáÜÁ°ÆÊÄß„ÄÇ‰º†ÁªüÊñπÊ≥ï‰æùËµñ‰∫éÈ´òÁ≤æÂ∫¶ÁöÑ‰∏ªÊùÉÈáçÁºìÂÜ≤Âå∫Êù•Á¥ØÁßØÊ¢ØÂ∫¶Êõ¥Êñ∞ÔºåËøôÂú®Á®ÄÁñè‰∏ìÂÆ∂Ê®°Âûã‰∏≠‰ºöÂØºËá¥ÊòæËëóÁöÑÂÜÖÂ≠òÂç†Áî®„ÄÇECOÈÄöËøáÁõ¥Êé•ÂØπÈáèÂåñÂèÇÊï∞Â∫îÁî®Êõ¥Êñ∞ÔºåÈÅøÂÖç‰∫Ü‰∏ªÊùÉÈáçÁöÑ‰ΩøÁî®ÔºåÂπ∂Âú®‰ºòÂåñÂô®Âä®Èáè‰∏≠Ê≥®ÂÖ•ÈáèÂåñËØØÂ∑ÆÔºåÂΩ¢Êàê‰∏Ä‰∏™Êó†È¢ùÂ§ñÂÜÖÂ≠òÁöÑÂèçÈ¶àÂæ™ÁéØ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåECOÂú®Â∞èÂûãTransformerÂíåÁ®ÄÁñèMoEÊ®°ÂûãÁöÑÈ¢ÑËÆ≠ÁªÉ‰∏≠ÔºåËÉΩÂ§üÂú®‰∏çÂ¢ûÂä†ÂÜÖÂ≠òÁöÑÊÉÖÂÜµ‰∏ãÔºåËææÂà∞‰∏é‰º†ÁªüÊñπÊ≥ïÁõ∏‰ººÁöÑÂáÜÁ°ÆÊÄß„ÄÇ', title='ÈîôËØØË°•ÂÅø‰ºòÂåñÂô®ÔºöÈáèÂåñËÆ≠ÁªÉÁöÑÊñ∞Á™ÅÁ†¥'))
[30.01.2026 09:41] Using data from previous issue: {"categories": ["#dataset", "#reasoning", "#rlhf", "#benchmark", "#open_source", "#interpretability", "#agents"], "emoji": "üï∑Ô∏è", "ru": {"title": "–†–∞—Å—Å—É–∂–¥–∞—é—â–∞—è –º–æ–¥–µ–ª—å –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–π –¥–ª—è –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–π –≤–µ–±-–Ω–∞–≤–∏–≥–∞—Ü–∏–∏", "desc": "WebArbiter –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—é –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–π –¥–ª—è –≤
[30.01.2026 09:41] Using data from previous issue: {"categories": ["#benchmark", "#cv", "#robotics"], "emoji": "ü§ñ", "ru": {"title": "–û–±—ä–µ–∫—Ç–Ω–æ-—Ü–µ–Ω—Ç—Ä–∏—á–µ—Å–∫–∏–µ —Å–ª–æ—Ç—ã ‚Äî –∫–ª—é—á –∫ –Ω–∞–¥—ë–∂–Ω–æ–π –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–∏ —Ä–æ–±–æ—Ç–æ–≤ –ø—Ä–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–∏ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —É—Å–ª–æ–≤–∏–π", "desc": "–í —ç—Ç–æ–π —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ –≤—ã–±–æ—Ä–∞ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –ø–æ–ª–∏—Ç–∏–∫ –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–∏ —Ä–æ–±–æ—Ç–æ–≤.
[30.01.2026 09:41] Using data from previous issue: {"categories": ["#benchmark", "#video", "#robotics"], "emoji": "üé¨", "ru": {"title": "–†–∞–∑–¥–µ–ª—ë–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —Ñ–∏–∑–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –≤–∏–¥–µ–æ-–≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω WorldBench ‚Äî –≤–∏–¥–µ–æ-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è —Ä–∞–∑–¥–µ–ª—ë–Ω–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ —Ñ–∏–∑–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª
[30.01.2026 09:41] Using data from previous issue: {"categories": ["#training", "#cv", "#robotics", "#architecture"], "emoji": "ü§ñ", "ru": {"title": "–û–±—ä–µ–∫—Ç–Ω–æ-—Ü–µ–Ω—Ç—Ä–∏—á–Ω–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∏ —á–µ—Ä–µ–∑ –º–Ω–æ–≥–æ—Ñ–∞–∑–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è STORM ‚Äî –ª—ë–≥–∫–∏–π –∞–¥–∞–ø—Ç–∞—Ü–∏–æ–Ω–Ω—ã–π –º–æ–¥—É–ª—å, –∫–æ—Ç–æ—Ä—ã–π –¥–æ–ø–æ–ª–Ω—è–µ—Ç –∑–∞–º–æ—Ä–æ–∂–µ–Ω–Ω—ã–µ –≤–∏–∑—É–∞–ª—å
[30.01.2026 09:41] Renaming data file.
[30.01.2026 09:41] Renaming previous data. hf_papers.json to ./d/2026-01-30.json
[30.01.2026 09:41] Saving new data file.
[30.01.2026 09:41] Generating page.
[30.01.2026 09:41] Renaming previous page.
[30.01.2026 09:41] Renaming previous data. index.html to ./d/2026-01-30.html
[30.01.2026 09:41] Writing result.
[30.01.2026 09:41] Renaming log file.
[30.01.2026 09:41] Renaming previous data. log.txt to ./logs/2026-01-30_last_log.txt
