[30.01.2026 17:36] Read previous papers.
[30.01.2026 17:36] Generating top page (month).
[30.01.2026 17:36] Writing top page (month).
[30.01.2026 18:44] Read previous papers.
[30.01.2026 18:44] Get feed.
[30.01.2026 18:44] Get page data from previous paper. URL: https://huggingface.co/papers/2601.20354
[30.01.2026 18:44] Get page data from previous paper. URL: https://huggingface.co/papers/2601.20833
[30.01.2026 18:44] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21204
[30.01.2026 18:44] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22153
[30.01.2026 18:44] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21639
[30.01.2026 18:44] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21821
[30.01.2026 18:44] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21420
[30.01.2026 18:44] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22046
[30.01.2026 18:44] Get page data from previous paper. URL: https://huggingface.co/papers/2601.20730
[30.01.2026 18:44] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21337
[30.01.2026 18:44] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22154
[30.01.2026 18:44] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21754
[30.01.2026 18:44] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22157
[30.01.2026 18:44] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21590
[30.01.2026 18:44] Get page data from previous paper. URL: https://huggingface.co/papers/2601.18129
[30.01.2026 18:44] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21181
[30.01.2026 18:44] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21051
[30.01.2026 18:44] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21571
[30.01.2026 18:44] Extract page data from URL. URL: https://huggingface.co/papers/2601.16914
[30.01.2026 18:44] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22069
[30.01.2026 18:44] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21598
[30.01.2026 18:44] Get page data from previous paper. URL: https://huggingface.co/papers/2601.20975
[30.01.2026 18:44] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22158
[30.01.2026 18:44] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21579
[30.01.2026 18:44] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22146
[30.01.2026 18:44] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22054
[30.01.2026 18:44] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21996
[30.01.2026 18:44] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21406
[30.01.2026 18:44] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21343
[30.01.2026 18:44] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22156
[30.01.2026 18:44] Get page data from previous paper. URL: https://huggingface.co/papers/2601.19001
[30.01.2026 18:44] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22143
[30.01.2026 18:44] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22101
[30.01.2026 18:44] Extract page data from URL. URL: https://huggingface.co/papers/2601.22083
[30.01.2026 18:44] Get page data from previous paper. URL: https://huggingface.co/papers/2601.20465
[30.01.2026 18:44] Get page data from previous paper. URL: https://huggingface.co/papers/2601.20103
[30.01.2026 18:44] Get page data from previous paper. URL: https://huggingface.co/papers/2601.17690
[30.01.2026 18:44] Get page data from previous paper. URL: https://huggingface.co/papers/2601.11747
[30.01.2026 18:44] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21872
[30.01.2026 18:44] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21416
[30.01.2026 18:44] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21282
[30.01.2026 18:44] Get page data from previous paper. URL: https://huggingface.co/papers/2601.20381
[30.01.2026 18:44] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[30.01.2026 18:44] No deleted papers detected.
[30.01.2026 18:44] Downloading and parsing papers (pdf, html). Total: 42.
[30.01.2026 18:44] Downloading and parsing paper https://huggingface.co/papers/2601.20354.
[30.01.2026 18:44] Extra JSON file exists (./assets/json/2601.20354.json), skip PDF parsing.
[30.01.2026 18:44] Paper image links file exists (./assets/img_data/2601.20354.json), skip HTML parsing.
[30.01.2026 18:44] Success.
[30.01.2026 18:44] Downloading and parsing paper https://huggingface.co/papers/2601.20833.
[30.01.2026 18:44] Extra JSON file exists (./assets/json/2601.20833.json), skip PDF parsing.
[30.01.2026 18:44] Paper image links file exists (./assets/img_data/2601.20833.json), skip HTML parsing.
[30.01.2026 18:44] Success.
[30.01.2026 18:44] Downloading and parsing paper https://huggingface.co/papers/2601.21204.
[30.01.2026 18:44] Extra JSON file exists (./assets/json/2601.21204.json), skip PDF parsing.
[30.01.2026 18:44] Paper image links file exists (./assets/img_data/2601.21204.json), skip HTML parsing.
[30.01.2026 18:44] Success.
[30.01.2026 18:44] Downloading and parsing paper https://huggingface.co/papers/2601.22153.
[30.01.2026 18:44] Extra JSON file exists (./assets/json/2601.22153.json), skip PDF parsing.
[30.01.2026 18:44] Paper image links file exists (./assets/img_data/2601.22153.json), skip HTML parsing.
[30.01.2026 18:44] Success.
[30.01.2026 18:44] Downloading and parsing paper https://huggingface.co/papers/2601.21639.
[30.01.2026 18:44] Extra JSON file exists (./assets/json/2601.21639.json), skip PDF parsing.
[30.01.2026 18:44] Paper image links file exists (./assets/img_data/2601.21639.json), skip HTML parsing.
[30.01.2026 18:44] Success.
[30.01.2026 18:44] Downloading and parsing paper https://huggingface.co/papers/2601.21821.
[30.01.2026 18:44] Extra JSON file exists (./assets/json/2601.21821.json), skip PDF parsing.
[30.01.2026 18:44] Paper image links file exists (./assets/img_data/2601.21821.json), skip HTML parsing.
[30.01.2026 18:44] Success.
[30.01.2026 18:44] Downloading and parsing paper https://huggingface.co/papers/2601.21420.
[30.01.2026 18:44] Extra JSON file exists (./assets/json/2601.21420.json), skip PDF parsing.
[30.01.2026 18:44] Paper image links file exists (./assets/img_data/2601.21420.json), skip HTML parsing.
[30.01.2026 18:44] Success.
[30.01.2026 18:44] Downloading and parsing paper https://huggingface.co/papers/2601.22046.
[30.01.2026 18:44] Extra JSON file exists (./assets/json/2601.22046.json), skip PDF parsing.
[30.01.2026 18:44] Paper image links file exists (./assets/img_data/2601.22046.json), skip HTML parsing.
[30.01.2026 18:44] Success.
[30.01.2026 18:44] Downloading and parsing paper https://huggingface.co/papers/2601.20730.
[30.01.2026 18:44] Extra JSON file exists (./assets/json/2601.20730.json), skip PDF parsing.
[30.01.2026 18:44] Paper image links file exists (./assets/img_data/2601.20730.json), skip HTML parsing.
[30.01.2026 18:44] Success.
[30.01.2026 18:44] Downloading and parsing paper https://huggingface.co/papers/2601.21337.
[30.01.2026 18:44] Extra JSON file exists (./assets/json/2601.21337.json), skip PDF parsing.
[30.01.2026 18:44] Paper image links file exists (./assets/img_data/2601.21337.json), skip HTML parsing.
[30.01.2026 18:44] Success.
[30.01.2026 18:44] Downloading and parsing paper https://huggingface.co/papers/2601.22154.
[30.01.2026 18:44] Extra JSON file exists (./assets/json/2601.22154.json), skip PDF parsing.
[30.01.2026 18:44] Paper image links file exists (./assets/img_data/2601.22154.json), skip HTML parsing.
[30.01.2026 18:44] Success.
[30.01.2026 18:44] Downloading and parsing paper https://huggingface.co/papers/2601.21754.
[30.01.2026 18:44] Extra JSON file exists (./assets/json/2601.21754.json), skip PDF parsing.
[30.01.2026 18:44] Paper image links file exists (./assets/img_data/2601.21754.json), skip HTML parsing.
[30.01.2026 18:44] Success.
[30.01.2026 18:44] Downloading and parsing paper https://huggingface.co/papers/2601.22157.
[30.01.2026 18:44] Extra JSON file exists (./assets/json/2601.22157.json), skip PDF parsing.
[30.01.2026 18:44] Paper image links file exists (./assets/img_data/2601.22157.json), skip HTML parsing.
[30.01.2026 18:44] Success.
[30.01.2026 18:44] Downloading and parsing paper https://huggingface.co/papers/2601.21590.
[30.01.2026 18:44] Extra JSON file exists (./assets/json/2601.21590.json), skip PDF parsing.
[30.01.2026 18:44] Paper image links file exists (./assets/img_data/2601.21590.json), skip HTML parsing.
[30.01.2026 18:44] Success.
[30.01.2026 18:44] Downloading and parsing paper https://huggingface.co/papers/2601.18129.
[30.01.2026 18:44] Extra JSON file exists (./assets/json/2601.18129.json), skip PDF parsing.
[30.01.2026 18:44] Paper image links file exists (./assets/img_data/2601.18129.json), skip HTML parsing.
[30.01.2026 18:44] Success.
[30.01.2026 18:44] Downloading and parsing paper https://huggingface.co/papers/2601.21181.
[30.01.2026 18:44] Extra JSON file exists (./assets/json/2601.21181.json), skip PDF parsing.
[30.01.2026 18:44] Paper image links file exists (./assets/img_data/2601.21181.json), skip HTML parsing.
[30.01.2026 18:44] Success.
[30.01.2026 18:44] Downloading and parsing paper https://huggingface.co/papers/2601.21051.
[30.01.2026 18:44] Extra JSON file exists (./assets/json/2601.21051.json), skip PDF parsing.
[30.01.2026 18:44] Paper image links file exists (./assets/img_data/2601.21051.json), skip HTML parsing.
[30.01.2026 18:44] Success.
[30.01.2026 18:44] Downloading and parsing paper https://huggingface.co/papers/2601.21571.
[30.01.2026 18:44] Extra JSON file exists (./assets/json/2601.21571.json), skip PDF parsing.
[30.01.2026 18:44] Paper image links file exists (./assets/img_data/2601.21571.json), skip HTML parsing.
[30.01.2026 18:44] Success.
[30.01.2026 18:44] Downloading and parsing paper https://huggingface.co/papers/2601.16914.
[30.01.2026 18:44] Downloading paper 2601.16914 from https://arxiv.org/pdf/2601.16914v1...
[30.01.2026 18:45] Extracting affiliations from text.
[30.01.2026 18:45] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 3 2 ] . [ 1 4 1 9 6 1 . 1 0 6 2 : r LoL: Longer than Longer, Scaling Video Generation to Justin Cui1,2 Jie Wu2, Ming Li2,3 Tao Yang2 Xiaojie Li2 Rui Wang2 Andrew Bai1 Yuanhao Ban1 Cho-Jui Hsieh1, 1UCLA, 2ByteDance Seed, 3University of Central Florida Corresponding author, Project lead "
[30.01.2026 18:45] Response: ```python
["UCLA", "ByteDance Seed", "University of Central Florida"]
```
[30.01.2026 18:45] Deleting PDF ./assets/pdf/2601.16914.pdf.
[30.01.2026 18:45] Success.
[30.01.2026 18:45] Downloading and parsing paper https://huggingface.co/papers/2601.22069.
[30.01.2026 18:45] Extra JSON file exists (./assets/json/2601.22069.json), skip PDF parsing.
[30.01.2026 18:45] Paper image links file exists (./assets/img_data/2601.22069.json), skip HTML parsing.
[30.01.2026 18:45] Success.
[30.01.2026 18:45] Downloading and parsing paper https://huggingface.co/papers/2601.21598.
[30.01.2026 18:45] Extra JSON file exists (./assets/json/2601.21598.json), skip PDF parsing.
[30.01.2026 18:45] Paper image links file exists (./assets/img_data/2601.21598.json), skip HTML parsing.
[30.01.2026 18:45] Success.
[30.01.2026 18:45] Downloading and parsing paper https://huggingface.co/papers/2601.20975.
[30.01.2026 18:45] Extra JSON file exists (./assets/json/2601.20975.json), skip PDF parsing.
[30.01.2026 18:45] Paper image links file exists (./assets/img_data/2601.20975.json), skip HTML parsing.
[30.01.2026 18:45] Success.
[30.01.2026 18:45] Downloading and parsing paper https://huggingface.co/papers/2601.22158.
[30.01.2026 18:45] Extra JSON file exists (./assets/json/2601.22158.json), skip PDF parsing.
[30.01.2026 18:45] Paper image links file exists (./assets/img_data/2601.22158.json), skip HTML parsing.
[30.01.2026 18:45] Success.
[30.01.2026 18:45] Downloading and parsing paper https://huggingface.co/papers/2601.21579.
[30.01.2026 18:45] Extra JSON file exists (./assets/json/2601.21579.json), skip PDF parsing.
[30.01.2026 18:45] Paper image links file exists (./assets/img_data/2601.21579.json), skip HTML parsing.
[30.01.2026 18:45] Success.
[30.01.2026 18:45] Downloading and parsing paper https://huggingface.co/papers/2601.22146.
[30.01.2026 18:45] Extra JSON file exists (./assets/json/2601.22146.json), skip PDF parsing.
[30.01.2026 18:45] Paper image links file exists (./assets/img_data/2601.22146.json), skip HTML parsing.
[30.01.2026 18:45] Success.
[30.01.2026 18:45] Downloading and parsing paper https://huggingface.co/papers/2601.22054.
[30.01.2026 18:45] Extra JSON file exists (./assets/json/2601.22054.json), skip PDF parsing.
[30.01.2026 18:45] Paper image links file exists (./assets/img_data/2601.22054.json), skip HTML parsing.
[30.01.2026 18:45] Success.
[30.01.2026 18:45] Downloading and parsing paper https://huggingface.co/papers/2601.21996.
[30.01.2026 18:45] Extra JSON file exists (./assets/json/2601.21996.json), skip PDF parsing.
[30.01.2026 18:45] Paper image links file exists (./assets/img_data/2601.21996.json), skip HTML parsing.
[30.01.2026 18:45] Success.
[30.01.2026 18:45] Downloading and parsing paper https://huggingface.co/papers/2601.21406.
[30.01.2026 18:45] Extra JSON file exists (./assets/json/2601.21406.json), skip PDF parsing.
[30.01.2026 18:45] Paper image links file exists (./assets/img_data/2601.21406.json), skip HTML parsing.
[30.01.2026 18:45] Success.
[30.01.2026 18:45] Downloading and parsing paper https://huggingface.co/papers/2601.21343.
[30.01.2026 18:45] Extra JSON file exists (./assets/json/2601.21343.json), skip PDF parsing.
[30.01.2026 18:45] Paper image links file exists (./assets/img_data/2601.21343.json), skip HTML parsing.
[30.01.2026 18:45] Success.
[30.01.2026 18:45] Downloading and parsing paper https://huggingface.co/papers/2601.22156.
[30.01.2026 18:45] Extra JSON file exists (./assets/json/2601.22156.json), skip PDF parsing.
[30.01.2026 18:45] Paper image links file exists (./assets/img_data/2601.22156.json), skip HTML parsing.
[30.01.2026 18:45] Success.
[30.01.2026 18:45] Downloading and parsing paper https://huggingface.co/papers/2601.19001.
[30.01.2026 18:45] Extra JSON file exists (./assets/json/2601.19001.json), skip PDF parsing.
[30.01.2026 18:45] Paper image links file exists (./assets/img_data/2601.19001.json), skip HTML parsing.
[30.01.2026 18:45] Success.
[30.01.2026 18:45] Downloading and parsing paper https://huggingface.co/papers/2601.22143.
[30.01.2026 18:45] Extra JSON file exists (./assets/json/2601.22143.json), skip PDF parsing.
[30.01.2026 18:45] Paper image links file exists (./assets/img_data/2601.22143.json), skip HTML parsing.
[30.01.2026 18:45] Success.
[30.01.2026 18:45] Downloading and parsing paper https://huggingface.co/papers/2601.22101.
[30.01.2026 18:45] Extra JSON file exists (./assets/json/2601.22101.json), skip PDF parsing.
[30.01.2026 18:45] Paper image links file exists (./assets/img_data/2601.22101.json), skip HTML parsing.
[30.01.2026 18:45] Success.
[30.01.2026 18:45] Downloading and parsing paper https://huggingface.co/papers/2601.22083.
[30.01.2026 18:45] Downloading paper 2601.22083 from https://arxiv.org/pdf/2601.22083v1...
[30.01.2026 18:45] Extracting affiliations from text.
[30.01.2026 18:45] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 9 2 ] . [ 1 3 8 0 2 2 . 1 0 6 2 : r a Enyi Jiang 1 2 Yibo Jacky Zhang 1 Yinglun Xu 2 Andreas Haupt 1 Nancy Amato 2 Sanmi Koyejo "
[30.01.2026 18:45] Response: ```python
[]
```
[30.01.2026 18:45] Extracting affiliations from text.
[30.01.2026 18:45] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 9 2 ] . [ 1 3 8 0 2 2 . 1 0 6 2 : r aEnyi Jiang 1 2 Yibo Jacky Zhang 1 Yinglun Xu 2 Andreas Haupt 1 Nancy Amato 2 Sanmi KoyejoLearning from human feedback typically relies on preference optimization that constrains policy updates through token-level regularization. However, preference optimization for language models is particularly challenging because tokenspace similarity does not imply semantic or behavioral similarity. To address this challenge, we leverage latent-space regularization for language model preference optimization. We introduce GANPO, which achieves latent-space regularization by penalizing divergence between the internal representations of policy model and reference model. Given that latent representations are not associated with explicit probability densities, we adopt an adversarial approach inspired by GANs to minimize latent-space divergence. We integrate GANPO as regularizer into existing offline preference optimization objectives. Experiments across multiple model architectures and tasks show consistent improvements from latent-space regularization. Further, by comparing GANPO-induced inferential biases with those from token-level regularization, we find that GANPO provides more robust structural feedback under distributional shift and noise while maintaining comparable downstream performance with minor computational overhead. 1. Introduction Learning from human feedback is essential for aligning large language models (LLMs) with human preferences (Leike et al., 2018; Ouyang et al., 2022). Most modern methods are done via pairwise preference optimization (PO): given pairwise preference data, we update policy model œÄŒ∏ while constraining it to remain close to reference model œÄref. This regularization constraint is crucial for improving 1Department of Computer Science, Stanford University 2Siebel School of Computing and Data Science, University of Illinois at Urbana-Champaign. Correspondence to: Enyi Jiang <enyij2@illinois.edu, enyij@stanford.edu>. Preprint. January 30, 2026. generalization and reducing reward hacking, and it is most commonly implemented via KL regularizer (Ouyang et al., 2022; Rafailov et al., 2023b; Xiong et al., 2024). Recent work has also explored alternative divergence measures, e.g., œá2-divergence and other -divergences (Wang et al., 2023a; Huang et al., 2025). However, limitation of these regularizers is that they operate purely in token space. Two sentences, for example, may be distant under token-level divergences while remaining semantically very similar (Hi there vs Good morning to you) and vice versa (Hi there vs Hit there), as illustrated in Figure 2. As result, token-space divergences can be coarse proxy for the actual behavioral similarity. Motivated by the limitation of token space optimization, recent work begun to explore the use of latent spaces instead of token spaces. For example, studies on continuous latent reasoning suggest that optimization in latent representations can lead to improved reasoning capabilities (Hao et al., 2025; Zhu et al., 2025) compared to methods that purely operate in token space. This paper investigates regularization performed in the latent space: we show that latent regularization can provide structural alignment feedback, which is absent from tokenlevel constraints. Concretely, given preference dataset, we consider the distributions of internal representations produced by the policy model and the reference model, and penalize their divergence. Latent representations are lowerdimensional than token distributions and often encode dense, structured information about semantics and reasoning state. This makes latent regularization potentially better way for semantic alignment than explicit token-level constraints. The immediate challenge is that, unlike token probabilities, latent representations do not admit an explicit probability density, making standard divergence measures computational challenge. To address this, we adopt technique inspired by GANs (Goodfellow et al., 2014): we introduce discriminators that distinguish representations generated by the policy from those generated by the reference model. We show that optimizing the policy against such discriminators is equivalent to minimizing latent-space divergence, yielding an efficient adversarial regularizer that can be added to existing preference optimization objectives. To fully leverage paired preference signals, we move beyond the standard Latent Adversarial Regularization for Offline Preference Optimization Figure 1. Comparison between DPO and GANPO. Offline preference optimization methods (e.g., DPO) optimize an implicit reward defined by preference data. GANPO augments this objective with latent-space discriminator, whose adversarial interaction induces regularization between the latent representation distributions of the policy model and the reference model. models internal representation space, yielding improved robustness to stochastic sampling noise and distributional shifts. Moreover, GANPO maintains comparable performance on downstream tasks with modest additional computational overhead. 2. Preliminaries 2.1. Preference Optimization In modern language-model alignment, preference learning is commonly instantiated through reinforcement learning from human feedback (RLHF) (Christiano et al., 2017; Ouyang et al., 2022) by optimizing KL-regularized reward objective. For parameter Œ≤ > 0, this objective is given by max œÄŒ∏ r(œÄŒ∏) Œ≤ DKL(œÄŒ∏ œÄref ), (1) where r(œÄŒ∏) := ExD,yœÄŒ∏(x)[r(x, y)] denotes the expected reward of the policy over the data distribution D, and DKL(œÄŒ∏ œÄref ) := ExD[DKL(œÄŒ∏(x) œÄref (x))] is the expected KL between the policy and reference distribution. In many practical settings, the available supervision is offline and pairwise: dataset consisting of preference triplets (x, yw, yl), representing prompt x, chosen response yw, and rejected response yl. Preference optimization methods working with such offline datasets are referred to as offline preference optimization (OPO). standard approach of OPO is Direct Preference Optimization (DPO) (Rafailov et al., 2023b). DPO uses the analytical solution to the KL-regularized RLHF objective (equation 1) to remove the explicit reward function. Instead, the reward r(x, y) is implicitly reparameterized in terms of the optimal policy œÄŒ∏ and the reference policy œÄref. To model human preferences, DPO incorporates this formulation into the Bradley-Terry model (Bradley & Terry, 19"
[30.01.2026 18:45] Mistral response. {"id": "a7aa4676b0454bf1922f5ab8052f40d7", "created": 1769798711, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1416, "total_tokens": 1455, "completion_tokens": 39, "num_cached_tokens": 1415}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\n    \"Department of Computer Science, Stanford University\",\n    \"Siebel School of Computing and Data Science, University of Illinois at Urbana-Champaign\"\n]\n```"}}]}
[30.01.2026 18:45] Response: ```python
[
    "Department of Computer Science, Stanford University",
    "Siebel School of Computing and Data Science, University of Illinois at Urbana-Champaign"
]
```
[30.01.2026 18:45] Deleting PDF ./assets/pdf/2601.22083.pdf.
[30.01.2026 18:45] Success.
[30.01.2026 18:45] Downloading and parsing paper https://huggingface.co/papers/2601.20465.
[30.01.2026 18:45] Extra JSON file exists (./assets/json/2601.20465.json), skip PDF parsing.
[30.01.2026 18:45] Paper image links file exists (./assets/img_data/2601.20465.json), skip HTML parsing.
[30.01.2026 18:45] Success.
[30.01.2026 18:45] Downloading and parsing paper https://huggingface.co/papers/2601.20103.
[30.01.2026 18:45] Extra JSON file exists (./assets/json/2601.20103.json), skip PDF parsing.
[30.01.2026 18:45] Paper image links file exists (./assets/img_data/2601.20103.json), skip HTML parsing.
[30.01.2026 18:45] Success.
[30.01.2026 18:45] Downloading and parsing paper https://huggingface.co/papers/2601.17690.
[30.01.2026 18:45] Extra JSON file exists (./assets/json/2601.17690.json), skip PDF parsing.
[30.01.2026 18:45] Paper image links file exists (./assets/img_data/2601.17690.json), skip HTML parsing.
[30.01.2026 18:45] Success.
[30.01.2026 18:45] Downloading and parsing paper https://huggingface.co/papers/2601.11747.
[30.01.2026 18:45] Extra JSON file exists (./assets/json/2601.11747.json), skip PDF parsing.
[30.01.2026 18:45] Paper image links file exists (./assets/img_data/2601.11747.json), skip HTML parsing.
[30.01.2026 18:45] Success.
[30.01.2026 18:45] Downloading and parsing paper https://huggingface.co/papers/2601.21872.
[30.01.2026 18:45] Extra JSON file exists (./assets/json/2601.21872.json), skip PDF parsing.
[30.01.2026 18:45] Paper image links file exists (./assets/img_data/2601.21872.json), skip HTML parsing.
[30.01.2026 18:45] Success.
[30.01.2026 18:45] Downloading and parsing paper https://huggingface.co/papers/2601.21416.
[30.01.2026 18:45] Extra JSON file exists (./assets/json/2601.21416.json), skip PDF parsing.
[30.01.2026 18:45] Paper image links file exists (./assets/img_data/2601.21416.json), skip HTML parsing.
[30.01.2026 18:45] Success.
[30.01.2026 18:45] Downloading and parsing paper https://huggingface.co/papers/2601.21282.
[30.01.2026 18:45] Extra JSON file exists (./assets/json/2601.21282.json), skip PDF parsing.
[30.01.2026 18:45] Paper image links file exists (./assets/img_data/2601.21282.json), skip HTML parsing.
[30.01.2026 18:45] Success.
[30.01.2026 18:45] Downloading and parsing paper https://huggingface.co/papers/2601.20381.
[30.01.2026 18:45] Extra JSON file exists (./assets/json/2601.20381.json), skip PDF parsing.
[30.01.2026 18:45] Paper image links file exists (./assets/img_data/2601.20381.json), skip HTML parsing.
[30.01.2026 18:45] Success.
[30.01.2026 18:45] Enriching papers with extra data.
[30.01.2026 18:45] ********************************************************************************
[30.01.2026 18:45] Abstract 0. A new benchmark and dataset are introduced to evaluate and improve spatial reasoning capabilities in text-to-image models through information-dense prompts and fine-tuning.  					AI-generated summary 				 Text-to-image (T2I) models have achieved remarkable success in generating high-fidelity images,...
[30.01.2026 18:45] ********************************************************************************
[30.01.2026 18:45] Abstract 1. Offline knowledge construction through structured methodological graphs enables more reliable and scalable autonomous scientific discovery by reducing reliance on real-time literature processing.  					AI-generated summary 				 Autonomous scientific discovery with large language model (LLM)-based ag...
[30.01.2026 18:45] ********************************************************************************
[30.01.2026 18:45] Abstract 2. Embedding scaling offers superior sparsity scaling compared to expert scaling in large language models, enabling efficient inference through system optimizations and speculative decoding.  					AI-generated summary 				 While Mixture-of-Experts (MoE) architectures have become the standard for sparsi...
[30.01.2026 18:45] ********************************************************************************
[30.01.2026 18:45] Abstract 3. DynamicVLA addresses dynamic object manipulation challenges through a compact vision-language-action model with temporal reasoning and closed-loop adaptation, supported by a new benchmark for dynamic manipulation tasks.  					AI-generated summary 				 Manipulating dynamic objects remains an open cha...
[30.01.2026 18:45] ********************************************************************************
[30.01.2026 18:45] Abstract 4. OCRVerse is a novel end-to-end OCR method that unifies text-centric and vision-centric approaches through comprehensive data engineering and a two-stage SFT-RL training framework with domain-specific reward strategies.  					AI-generated summary 				 The development of large vision language models d...
[30.01.2026 18:45] ********************************************************************************
[30.01.2026 18:45] Abstract 5. A large-scale multimodal reasoning dataset called MMFineReason is introduced to improve vision language models' performance through high-quality reasoning annotations and demonstrates superior parameter efficiency in fine-tuned models.  					AI-generated summary 				 Recent advances in Vision Langua...
[30.01.2026 18:45] ********************************************************************************
[30.01.2026 18:45] Abstract 6. ConceptMoE dynamically allocates computation by merging similar tokens into concept representations, improving both performance and efficiency in large language models through adaptive processing and reduced attention computation.  					AI-generated summary 				 Large language models allocate unifor...
[30.01.2026 18:45] ********************************************************************************
[30.01.2026 18:45] Abstract 7. PLANING presents an efficient streaming reconstruction framework that combines explicit geometric primitives with neural Gaussians to achieve high-quality rendering and accurate geometry simultaneously through decoupled optimization.  					AI-generated summary 				 Streaming reconstruction from mono...
[30.01.2026 18:45] ********************************************************************************
[30.01.2026 18:45] Abstract 8. AgentLongBench evaluates large language models as autonomous agents through dynamic environment interactions, revealing challenges in handling high-information-density tool responses compared to memory fragmentation in long conversations.  					AI-generated summary 				 The evolution of Large Langua...
[30.01.2026 18:45] ********************************************************************************
[30.01.2026 18:45] Abstract 9. The Qwen3-ASR family introduces speech recognition models with language identification capabilities and a non-autoregressive forced alignment model, achieving state-of-the-art performance and efficient processing.  					AI-generated summary 				 In this report, we introduce Qwen3-ASR family, which i...
[30.01.2026 18:45] ********************************************************************************
[30.01.2026 18:45] Abstract 10. Agent-RRM, a multi-faceted reward model, provides structured feedback for agentic trajectories through reasoning traces, critiques, and performance scores, with unified feedback integration showing superior performance across diverse benchmarks.  					AI-generated summary 				 Agentic Reinforcement ...
[30.01.2026 18:45] ********************************************************************************
[30.01.2026 18:45] Abstract 11. A novel framework called SCOUT is introduced that uses lightweight scouts to reduce exploration costs for large language models in nonlinguistic environments, enabling improved performance through supervised fine-tuning and reinforcement learning.  					AI-generated summary 				 While Large Language...
[30.01.2026 18:45] ********************************************************************************
[30.01.2026 18:45] Abstract 12. Hidden superior models exist in public repositories but are overlooked due to inefficient discovery methods; a multi-armed bandit approach using shared query sets and aggressive elimination significantly accelerates identification of top-performing models.  					AI-generated summary 				 Public repo...
[30.01.2026 18:45] ********************************************************************************
[30.01.2026 18:45] Abstract 13. A theoretically grounded method for improving large language model reasoning performance through distribution sharpening without iterative sampling or external rewards, achieving comparable results to reinforcement learning post-training with significantly reduced computational costs.  					AI-gener...
[30.01.2026 18:45] ********************************************************************************
[30.01.2026 18:45] Abstract 14. A minimal post-training approach using supervised fine-tuning, on-policy distillation, and small-scale reinforcement fine-tuning enables the development of high-quality sovereign language models with reduced resource requirements.  					AI-generated summary 				 Large language models (LLMs) have pro...
[30.01.2026 18:45] ********************************************************************************
[30.01.2026 18:45] Abstract 15. Multimodal Large Language Models suffer from cross-modal hallucinations where one modality incorrectly influences generation from another, leading to fabricated outputs; this exposes a fundamental deficiency in modality-interaction control. To address this, a training-free method called Modality-Ada...
[30.01.2026 18:45] ********************************************************************************
[30.01.2026 18:45] Abstract 16. A two-stage trained cybersecurity reasoning model achieves competitive performance on specialized tasks while maintaining general capabilities through supervised fine-tuning and reinforcement learning from verifiable rewards.  					AI-generated summary 				 We present Foundation-Sec-8B-Reasoning, th...
[30.01.2026 18:45] ********************************************************************************
[30.01.2026 18:45] Abstract 17. Token filtering during pretraining effectively reduces unwanted language model capabilities while maintaining alignment, becoming more effective at larger scales and tolerating noisy labels with sufficient compute.  					AI-generated summary 				 Current approaches to reducing undesired capabilities...
[30.01.2026 18:45] ********************************************************************************
[30.01.2026 18:45] Abstract 18. Researchers developed a method to overcome sink-collapse in autoregressive video generation by addressing the conflict between Rotary Position Embedding and multi-head attention mechanisms, enabling real-time streaming of videos up to 12 hours long.  					AI-generated summary 				 Recent research in...
[30.01.2026 18:45] ********************************************************************************
[30.01.2026 18:45] Abstract 19. VTC-R1 enables efficient long-context reasoning by compressing textual traces into compact images and iteratively feeding them back into vision-language models as optical memory, achieving significant speedup without sacrificing performance.  					AI-generated summary 				 Long-context reasoning has...
[30.01.2026 18:45] ********************************************************************************
[30.01.2026 18:45] Abstract 20. Active latent planning method improves reasoning accuracy and efficiency by modeling latent token supervision as conditional VAE and using reinforcement learning with coherence rewards.  					AI-generated summary 				 Aiming at efficient and dense chain-of-thought (CoT) reasoning, latent reasoning m...
[30.01.2026 18:45] ********************************************************************************
[30.01.2026 18:45] Abstract 21. DeepSearchQA presents a 900-prompt benchmark evaluating agents on complex multi-step information-seeking tasks requiring systematic information collation, deduplication, and reasoning about stopping criteria across 17 fields.  					AI-generated summary 				 We introduce DeepSearchQA, a 900-prompt be...
[30.01.2026 18:45] ********************************************************************************
[30.01.2026 18:45] Abstract 22. Pixel MeanFlow introduces a one-step latent-free image generation method by separating network output space from loss space, achieving strong performance on ImageNet at multiple resolutions.  					AI-generated summary 				 Modern diffusion/flow-based models for image generation typically exhibit two...
[30.01.2026 18:45] ********************************************************************************
[30.01.2026 18:45] Abstract 23. KromHC addresses training instability and scalability issues in hyper-connections by using Kronecker products to parametrize residual matrices with reduced parameter complexity.  					AI-generated summary 				 The success of Hyper-Connections (HC) in neural networks (NN) has also highlighted issues ...
[30.01.2026 18:45] ********************************************************************************
[30.01.2026 18:45] Abstract 24. Large language models can be pre-trained from scratch using synthetic instruction-response pairs generated from unstructured text corpora, outperforming traditional methods on benchmarks measuring response quality.  					AI-generated summary 				 Due to limited supervised training data, large langua...
[30.01.2026 18:45] ********************************************************************************
[30.01.2026 18:45] Abstract 25. Metric Anything presents a scalable pretraining framework for metric depth estimation that leverages diverse 3D data and sparse metric prompts to achieve superior performance across multiple vision tasks.  					AI-generated summary 				 Scaling has powered recent advances in vision foundation models...
[30.01.2026 18:45] ********************************************************************************
[30.01.2026 18:45] Abstract 26. Mechnistic Data Attribution framework traces interpretable units to specific training samples using influence functions, demonstrating causal relationships between data structure and neural circuit formation in language models.  					AI-generated summary 				 While Mechanistic Interpretability has i...
[30.01.2026 18:45] ********************************************************************************
[30.01.2026 18:45] Abstract 27. UniMRG enhances unified multimodal models by training them to generate multiple visual representations, improving both understanding and generation capabilities through complementary information capture.  					AI-generated summary 				 Unified Multimodal Models (UMMs) integrate both visual understan...
[30.01.2026 18:45] ********************************************************************************
[30.01.2026 18:45] Abstract 28. A reinforcement learning-based pretraining method improves language model safety, factuality, and quality by evaluating generations through a combination of model rollouts, original suffixes, and rewritten suffixes.  					AI-generated summary 				 Ensuring safety, factuality and overall quality in t...
[30.01.2026 18:45] ********************************************************************************
[30.01.2026 18:45] Abstract 29. HALO enables efficient conversion of Transformer models to RNN-attention hybrid architectures with improved long-context performance using minimal training data.  					AI-generated summary 				 Hybrid Transformer architectures, which combine softmax attention blocks and recurrent neural networks (RN...
[30.01.2026 18:45] ********************************************************************************
[30.01.2026 18:45] Abstract 30. FROST is an attention-aware method that improves reasoning efficiency by pruning uncritical paths and removing reasoning outliers, leading to reduced token usage and improved accuracy.  					AI-generated summary 				 We propose FROST, an attention-aware method for efficient reasoning. Unlike traditi...
[30.01.2026 18:45] ********************************************************************************
[30.01.2026 18:45] Abstract 31. A lightweight LoRA adaptation of an audio-video diffusion model enables high-quality video dubbing with preserved speaker identity and improved lip synchronization through synthetic multilingual video training.  					AI-generated summary 				 Audio-Visual Foundation Models, which are pretrained to j...
[30.01.2026 18:45] ********************************************************************************
[30.01.2026 18:45] Abstract 32. Error-compensating optimizer eliminates memory overhead from master weights in quantized LLM training while maintaining near-lossless accuracy.  					AI-generated summary 				 Quantization has significantly improved the compute and memory efficiency of Large Language Model (LLM) training. However, e...
[30.01.2026 18:45] ********************************************************************************
[30.01.2026 18:45] Abstract 33. GANPO uses latent-space regularization through adversarial divergence minimization to improve language model preference optimization, offering more robust structural feedback than token-level methods.  					AI-generated summary 				 Learning from human feedback typically relies on preference optimiz...
[30.01.2026 18:45] ********************************************************************************
[30.01.2026 18:45] Abstract 34. BMAM presents a brain-inspired multi-agent memory architecture that decomposes memory into specialized subsystems to address long-term reasoning challenges in language-model-based agents.  					AI-generated summary 				 Language-model-based agents operating over extended interaction horizons face pe...
[30.01.2026 18:45] ********************************************************************************
[30.01.2026 18:45] Abstract 35. Researchers developed a comprehensive benchmark for detecting reward hacking in code generation environments, demonstrating that contrastive anomaly detection outperforms isolated classification approaches and revealing challenges with semantically contextualized reward hacks.  					AI-generated sum...
[30.01.2026 18:45] ********************************************************************************
[30.01.2026 18:45] Abstract 36. Neural audio fingerprinting performance varies with segment length, with short segments (0.5-second) generally providing better retrieval accuracy, and large language models showing promise in recommending optimal segment durations.  					AI-generated summary 				 Audio fingerprinting provides an id...
[30.01.2026 18:45] ********************************************************************************
[30.01.2026 18:45] Abstract 37. PRISM leverages design data to create a knowledge base for improving graphic designs based on natural language instructions, achieving superior style alignment compared to existing methods.  					AI-generated summary 				 Graphic design often involves exploring different stylistic directions, which ...
[30.01.2026 18:45] ********************************************************************************
[30.01.2026 18:45] Abstract 38. WebArbiter introduces a reasoning-first WebPRM that formulates reward modeling as text generation to improve web navigation through structured justifications and preference verdicts, outperforming existing baselines in complex web environments.  					AI-generated summary 				 Web agents hold great p...
[30.01.2026 18:45] ********************************************************************************
[30.01.2026 18:45] Abstract 39. Slot-Based Object-Centric Representations outperform global and dense feature representations in robotic manipulation tasks by providing better generalization under visual distribution shifts.  					AI-generated summary 				 The generalization capabilities of robotic manipulation policies are heavil...
[30.01.2026 18:45] ********************************************************************************
[30.01.2026 18:45] Abstract 40. WorldBench is introduced as a video-based benchmark for disentangled evaluation of physical reasoning in generative models, revealing specific failure patterns in current state-of-the-art video world models.  					AI-generated summary 				 Recent advances in generative foundational models, often ter...
[30.01.2026 18:45] ********************************************************************************
[30.01.2026 18:45] Abstract 41. STORM enhances robotic manipulation by adapting visual foundation models with semantic-aware slots through multi-phase training, improving generalization and control performance.  					AI-generated summary 				 Visual foundation models provide strong perceptual features for robotics, but their dense...
[30.01.2026 18:45] Read previous papers.
[30.01.2026 18:45] Generating reviews via LLM API.
[30.01.2026 18:45] Using data from previous issue: {"categories": ["#cv", "#benchmark", "#synthetic", "#reasoning", "#dataset", "#training", "#multimodal"], "emoji": "üß©", "ru": {"title": "–ü—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –≤ text-to-image –º–æ–¥–µ–ª—è—Ö —á–µ—Ä–µ–∑ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ SpatialGenEval –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å
[30.01.2026 18:45] Using data from previous issue: {"categories": ["#graphs", "#hallucinations", "#long_context", "#science"], "emoji": "üß¨", "ru": {"title": "–ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–µ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∑–Ω–∞–Ω–∏–π –¥–ª—è –Ω–∞–¥—ë–∂–Ω–æ–≥–æ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –Ω–∞—É—á–Ω–æ–≥–æ –æ—Ç–∫—Ä—ã—Ç–∏—è", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è Idea2Story ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –Ω–∞—É—á–Ω–æ–≥–æ –æ—Ç–∫—Ä—ã—Ç–∏—è, –∫–æ—Ç–æ—Ä—ã–π –ø–µ—Ä–µ–Ω–æ—Å–∏—Ç –æ–±—Ä–∞–±–æ
[30.01.2026 18:45] Using data from previous issue: {"categories": ["#architecture", "#inference", "#training", "#optimization", "#open_source"], "emoji": "‚ö°", "ru": {"title": "–≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –≤–º–µ—Å—Ç–æ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤: –Ω–æ–≤—ã–π –ø—É—Ç—å –∫ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–º—É –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—é", "desc": "–í —ç—Ç–æ–π —Ä–∞–±–æ—Ç–µ –∞–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∫–∞–∫ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—É —ç–∫—Å
[30.01.2026 18:45] Using data from previous issue: {"categories": ["#multimodal", "#dataset", "#benchmark", "#robotics", "#inference", "#architecture", "#transfer_learning", "#small_models", "#synthetic"], "emoji": "ü§ñ", "ru": {"title": "–ë—ã—Å—Ç—Ä–∞—è –∏ –∞–¥–∞–ø—Ç–∏–≤–Ω–∞—è –º–∞–Ω–∏–ø—É–ª—è—Ü–∏—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–º–∏ –æ–±—ä–µ–∫—Ç–∞–º–∏ —á–µ—Ä–µ–∑ –∫–æ–º–ø–∞–∫—Ç–Ω—ã–µ vision-language –º–æ–¥–µ–ª–∏", "desc": "Dynami
[30.01.2026 18:45] Using data from previous issue: {"categories": ["#data", "#cv", "#open_source", "#dataset", "#science", "#training", "#optimization", "#rl", "#multimodal"], "emoji": "üìÑ", "ru": {"title": "–£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è–º–∏", "desc": "OCRVerse –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π 
[30.01.2026 18:45] Using data from previous issue: {"categories": ["#dataset", "#reasoning", "#multimodal", "#open_source", "#training", "#small_models", "#synthetic"], "emoji": "üß†", "ru": {"title": "–í—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –¥–µ–ª–∞—é—Ç –º–∞–ª–µ–Ω—å–∫–∏–µ –º–æ–¥–µ–ª–∏ —É–º–Ω–µ–µ –±–æ–ª—å—à–∏—Ö", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ MMFineReason ‚Äî –±–æ–ª—å—à–æ–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –¥–∞—Ç
[30.01.2026 18:45] Using data from previous issue: {"categories": ["#inference", "#multimodal", "#architecture", "#training"], "emoji": "‚ö°", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–æ–∫–µ–Ω–æ–≤ —á–µ—Ä–µ–∑ –∫–æ–Ω—Ü–µ–ø—Ç—É–∞–ª—å–Ω–æ–µ —Å–ª–∏—è–Ω–∏–µ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π", "desc": "ConceptMoE ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö, –∫–æ
[30.01.2026 18:45] Using data from previous issue: {"categories": [], "emoji": "üé¨", "ru": {"title": "–†–∞–∑–¥–µ–ª—ë–Ω–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–µ–æ–º–µ—Ç—Ä–∏–∏ –∏ –≤–Ω–µ—à–Ω–µ–≥–æ –≤–∏–¥–∞ –¥–ª—è –±—ã—Å—Ç—Ä–æ–π —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ —Å—Ü–µ–Ω", "desc": "PLANING ‚Äî —ç—Ç–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –ø–æ—Ç–æ–∫–æ–≤–æ–π —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ —Å—Ü–µ–Ω –∏–∑ –º–æ–Ω–æ–∫—É–ª—è—Ä–Ω—ã—Ö –≤–∏–¥–µ–æ–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π, –∫–æ—Ç–æ—Ä–∞—è –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç —è–≤–Ω—ã–µ –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–∏—Ç–∏–≤—ã —Å –Ω
[30.01.2026 18:45] Using data from previous issue: {"categories": ["#benchmark", "#agents", "#long_context", "#dataset", "#reasoning"], "emoji": "üîÑ", "ru": {"title": "–ê–≥–µ–Ω—Ç—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM: –æ—Ç —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Å—Ç–æ–≤ –∫ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–º—É —Å–∏–Ω—Ç–µ–∑—É –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏", "desc": "AgentLongBench ‚Äî —ç—Ç–æ –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ —Ä–æ–ª–∏ –∞–≤—Ç–æ–Ω–æ–º–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ —á–µ
[30.01.2026 18:45] Using data from previous issue: {"categories": ["#benchmark", "#multilingual", "#inference", "#low_resource", "#audio", "#open_source"], "emoji": "üé§", "ru": {"title": "–ú–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏ —Å –Ω–µ–∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–º –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ–º", "desc": "–°–µ–º—å—è –º–æ–¥–µ–ª–µ–π Qwen3-ASR –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –Ω–∞–±–æ—Ä —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ—á
[30.01.2026 18:45] Using data from previous issue: {"categories": ["#dataset", "#reasoning", "#benchmark", "#open_source", "#rl", "#optimization", "#agents"], "emoji": "üéØ", "ru": {"title": "–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ–±—Ä–∞—Ç–Ω–∞—è —Å–≤—è–∑—å –¥–ª—è —É–º–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Agent-RRM, –º–Ω–æ–≥–æ–∞—Å–ø–µ–∫—Ç–Ω—É—é –º–æ–¥–µ–ª—å –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä–∞—è –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å
[30.01.2026 18:45] Using data from previous issue: {"categories": ["#reasoning", "#training", "#rl", "#optimization", "#transfer_learning", "#agents", "#small_models"], "emoji": "üîç", "ru": {"title": "–†–∞–∑–≤–µ–¥—á–∏–∫–∏ –¥–ª—è —Ä–∞–∑—É–º–Ω–æ–π —Ä–∞–∑–≤–µ–¥–∫–∏: –∫–∞–∫ –º–∞–ª–µ–Ω—å–∫–∏–µ –º–æ–¥–µ–ª–∏ —É—á–∞—Ç –±–æ–ª—å—à–∏—Ö", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ SCOUT, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ª—ë–≥–∫–∏—Ö –∞–≥–µ–Ω—Ç–æ–≤-
[30.01.2026 18:45] Using data from previous issue: {"categories": ["#small_models", "#benchmark", "#inference"], "emoji": "üíé", "ru": {"title": "–ü–æ–∏—Å–∫ —Å–∫—Ä—ã—Ç—ã—Ö –∂–µ–º—á—É–∂–∏–Ω: –±—ã—Å—Ç—Ä–æ–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –º–Ω–æ–≥–æ—Ä—É–∫–∏–π –±–∞–Ω–¥–∏—Ç", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø–æ–∫–∞–∑–∞–Ω–æ, —á—Ç–æ –≤ –ø—É–±–ª–∏—á–Ω—ã—Ö —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è—Ö –Ω–∞—Ö–æ–¥—è—Ç—Å—è —Å–∫—Ä—ã—Ç—ã–µ –≤—ã—Å–æ–∫–æ–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏, –∫–æ—Ç–æ—Ä—ã–µ –æ—Å—Ç
[30.01.2026 18:45] Using data from previous issue: {"categories": ["#rl", "#plp", "#reasoning", "#optimization", "#training", "#math"], "emoji": "üî•", "ru": {"title": "–ó–∞–æ—Å—Ç—Ä–µ–Ω–∏–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤–º–µ—Å—Ç–æ –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏—è: —ç–∫–æ–Ω–æ–º–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π LLM", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –∑–∞
[30.01.2026 18:45] Using data from previous issue: {"categories": ["#rlhf", "#reasoning", "#open_source", "#training", "#multilingual", "#optimization", "#low_resource", "#small_models"], "emoji": "üèõÔ∏è", "ru": {"title": "–°—É–≤–µ—Ä–µ–Ω–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –¥–æ–æ–±—É—á–µ–Ω–∏—é —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π,
[30.01.2026 18:45] Using data from previous issue: {"categories": ["#multimodal", "#video", "#audio", "#inference"], "emoji": "üéØ", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –ø–æ–¥–∞–≤–ª–µ–Ω–∏—è –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–í —Å—Ç–∞—Ç—å–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ –∫—Ä–æ—Å—Å-–º–æ–¥–∞–ª—å–Ω—ã—Ö –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö, –∫–æ–≥–¥–∞ 
[30.01.2026 18:45] Using data from previous issue: {"categories": ["#rlhf", "#dataset", "#reasoning", "#benchmark", "#open_source", "#training", "#science", "#small_models"], "emoji": "üîê", "ru": {"title": "–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ –≤ –∫–æ–º–ø–∞–∫—Ç–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ: —Ä–∞—Å—Å—É–∂–¥–∞—é—â–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –∫–∏–±–µ—Ä–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ Foundation-Sec-8B-Reasoning ‚Äî
[30.01.2026 18:45] Using data from previous issue: {"categories": ["#architecture", "#training", "#alignment", "#data", "#security"], "emoji": "üîç", "ru": {"title": "–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ç–æ–∫–µ–Ω–æ–≤: —É–¥–∞–ª–µ–Ω–∏–µ –Ω–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –≤–æ –≤—Ä–µ–º—è –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è", "desc": "–í —ç—Ç–æ–π —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –º–µ—Ç–æ–¥ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤ –Ω–∞ —ç—Ç–∞–ø–µ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è —É–¥–∞
[30.01.2026 18:45] Querying the API.
[30.01.2026 18:45] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Researchers developed a method to overcome sink-collapse in autoregressive video generation by addressing the conflict between Rotary Position Embedding and multi-head attention mechanisms, enabling real-time streaming of videos up to 12 hours long.  					AI-generated summary 				 Recent research in long-form video generation has shifted from bidirectional to autoregressive models, yet these methods commonly suffer from error accumulation and a loss of long-term coherence. While attention sink frames have been introduced to mitigate this performance decay, they often induce a critical failure mode we term sink-collapse: the generated content repeatedly reverts to the sink frame, resulting in abrupt scene resets and cyclic motion patterns. Our analysis reveals that sink-collapse originates from an inherent conflict between the periodic structure of Rotary Position Embedding (RoPE) and the multi-head attention mechanisms prevalent in current generative models. To address it, we propose a lightweight, training-free approach that effectively suppresses this behavior by introducing multi-head RoPE jitter that breaks inter-head attention homogenization and mitigates long-horizon collapse. Extensive experiments show that our method successfully alleviates sink-collapse while preserving generation quality. To the best of our knowledge, this work achieves the first demonstration of real-time, streaming, and infinite-length video generation with little quality decay. As an illustration of this robustness, we generate continuous videos up to 12 hours in length, which, to our knowledge, is among the longest publicly demonstrated results in streaming video generation.
[30.01.2026 18:45] Response: ```json
{
  "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –º–µ—Ç–æ–¥ –¥–ª—è —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º—ã sink-collapse –≤ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ, –∫–æ—Ç–æ—Ä–∞—è –≤–æ–∑–Ω–∏–∫–∞–µ—Ç –∏–∑-–∑–∞ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–∞ –º–µ–∂–¥—É –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π Rotary Position Embedding –∏ –º–µ—Ö–∞–Ω–∏–∑–º–æ–º multi-head attention. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç multi-head RoPE jitter –¥–ª—è –ø–æ–¥–∞–≤–ª–µ–Ω–∏—è –Ω–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω–æ–≥–æ –ø–æ–≤–µ–¥–µ–Ω–∏—è, –∫–æ–≥–¥–∞ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –ø–æ—Å—Ç–æ—è–Ω–Ω–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç—Å—è –∫ –æ–ø–æ—Ä–Ω–æ–º—É –∫–∞–¥—Ä—É. –ú–µ—Ç–æ–¥ —è–≤–ª—è–µ—Ç—Å—è –ª—ë–≥–∫–∏–º –∏ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏, —Å–æ—Ö—Ä–∞–Ω—è—è –ø—Ä–∏ —ç—Ç–æ–º –∫–∞—á–µ—Å—Ç–≤–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ. –†–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –≤–ø–µ—Ä–≤—ã–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏ —Å –ø–æ—Ç–æ–∫–æ–≤–æ–π –ø–µ—Ä–µ–¥–∞—á–µ–π –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é –¥–æ 12 —á–∞—Å–æ–≤ –±–µ–∑ —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ–π –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞.",
  "emoji": "üé¨",
  "title": "–ë–µ—Å–∫–æ–Ω–µ—á–Ω–æ–µ –≤–∏–¥–µ–æ –±–µ–∑ –∫–æ–ª–ª–∞–ø—Å–∞: –ø–æ—Ç–æ–∫–æ–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏"
}
```
[30.01.2026 18:45] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Researchers developed a method to overcome sink-collapse in autoregressive video generation by addressing the conflict between Rotary Position Embedding and multi-head attention mechanisms, enabling real-time streaming of videos up to 12 hours long.  					AI-generated summary 				 Recent research in long-form video generation has shifted from bidirectional to autoregressive models, yet these methods commonly suffer from error accumulation and a loss of long-term coherence. While attention sink frames have been introduced to mitigate this performance decay, they often induce a critical failure mode we term sink-collapse: the generated content repeatedly reverts to the sink frame, resulting in abrupt scene resets and cyclic motion patterns. Our analysis reveals that sink-collapse originates from an inherent conflict between the periodic structure of Rotary Position Embedding (RoPE) and the multi-head attention mechanisms prevalent in current generative models. To address it, we propose a lightweight, training-free approach that effectively suppresses this behavior by introducing multi-head RoPE jitter that breaks inter-head attention homogenization and mitigates long-horizon collapse. Extensive experiments show that our method successfully alleviates sink-collapse while preserving generation quality. To the best of our knowledge, this work achieves the first demonstration of real-time, streaming, and infinite-length video generation with little quality decay. As an illustration of this robustness, we generate continuous videos up to 12 hours in length, which, to our knowledge, is among the longest publicly demonstrated results in streaming video generation."

[30.01.2026 18:45] Response: ```python
["VIDEO", "ARCHITECTURE", "INFERENCE"]
```
[30.01.2026 18:45] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Researchers developed a method to overcome sink-collapse in autoregressive video generation by addressing the conflict between Rotary Position Embedding and multi-head attention mechanisms, enabling real-time streaming of videos up to 12 hours long.  					AI-generated summary 				 Recent research in long-form video generation has shifted from bidirectional to autoregressive models, yet these methods commonly suffer from error accumulation and a loss of long-term coherence. While attention sink frames have been introduced to mitigate this performance decay, they often induce a critical failure mode we term sink-collapse: the generated content repeatedly reverts to the sink frame, resulting in abrupt scene resets and cyclic motion patterns. Our analysis reveals that sink-collapse originates from an inherent conflict between the periodic structure of Rotary Position Embedding (RoPE) and the multi-head attention mechanisms prevalent in current generative models. To address it, we propose a lightweight, training-free approach that effectively suppresses this behavior by introducing multi-head RoPE jitter that breaks inter-head attention homogenization and mitigates long-horizon collapse. Extensive experiments show that our method successfully alleviates sink-collapse while preserving generation quality. To the best of our knowledge, this work achieves the first demonstration of real-time, streaming, and infinite-length video generation with little quality decay. As an illustration of this robustness, we generate continuous videos up to 12 hours in length, which, to our knowledge, is among the longest publicly demonstrated results in streaming video generation."

[30.01.2026 18:45] Response: ```python
['LONG_CONTEXT', 'OPTIMIZATION']
```
[30.01.2026 18:45] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents a novel method to tackle the issue of sink-collapse in autoregressive video generation, which is a problem where generated content repeatedly returns to a specific frame, causing unnatural motion. The authors identify that this issue arises from a conflict between Rotary Position Embedding (RoPE) and multi-head attention mechanisms used in generative models. To resolve this, they introduce a technique called multi-head RoPE jitter, which disrupts the uniformity of attention across heads and helps maintain coherence over long video sequences. Their approach enables real-time streaming of videos, achieving impressive lengths of up to 12 hours with minimal quality loss, marking a significant advancement in the field of video generation.","title":"Breaking the Cycle: Real-Time Long-Form Video Generation Without Sink-Collapse"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper presents a novel method to tackle the issue of sink-collapse in autoregressive video generation, which is a problem where generated content repeatedly returns to a specific frame, causing unnatural motion. The authors identify that this issue arises from a conflict between Rotary Position Embedding (RoPE) and multi-head attention mechanisms used in generative models. To resolve this, they introduce a technique called multi-head RoPE jitter, which disrupts the uniformity of attention across heads and helps maintain coherence over long video sequences. Their approach enables real-time streaming of videos, achieving impressive lengths of up to 12 hours with minimal quality loss, marking a significant advancement in the field of video generation.', title='Breaking the Cycle: Real-Time Long-Form Video Generation Without Sink-Collapse'))
[30.01.2026 18:45] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Á†îÁ©∂‰∫∫ÂëòÂºÄÂèë‰∫Ü‰∏ÄÁßçÊñπÊ≥ïÊù•ÂÖãÊúçËá™ÂõûÂΩíËßÜÈ¢ëÁîüÊàê‰∏≠ÁöÑÊ≤âÊ≤°Â¥©Ê∫ÉÈóÆÈ¢òÔºåËß£ÂÜ≥‰∫ÜÊóãËΩ¨‰ΩçÁΩÆÂµåÂÖ•‰∏éÂ§öÂ§¥Ê≥®ÊÑèÂäõÊú∫Âà∂‰πãÈó¥ÁöÑÂÜ≤Á™Å„ÄÇËøôÁßçÊñπÊ≥ïËÉΩÂ§üÂÆûÁé∞ÂÆûÊó∂ÊµÅÂ™í‰ΩìËßÜÈ¢ëÁîüÊàêÔºåÈïøÂ∫¶ÂèØËææ12Â∞èÊó∂„ÄÇÈÄöËøáÂºïÂÖ•Â§öÂ§¥ÊóãËΩ¨‰ΩçÁΩÆÂµåÂÖ•ÊäñÂä®ÔºåÁ†îÁ©∂ËÄÖÊúâÊïàÊäëÂà∂‰∫ÜÊ≤âÊ≤°Â¥©Ê∫ÉÁé∞Ë±°ÔºåÂêåÊó∂‰øùÊåÅ‰∫ÜÁîüÊàêË¥®Èáè„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®ÁîüÊàêÈïøËßÜÈ¢ëÊó∂Ë°®Áé∞Âá∫Ëâ≤ÔºåÊàêÂäüÈÅøÂÖç‰∫ÜÂÜÖÂÆπÈáçÂ§çÂíåÂú∫ÊôØÈáçÁΩÆÁöÑÈóÆÈ¢ò„ÄÇ","title":"Á™ÅÁ†¥Ëá™ÂõûÂΩíËßÜÈ¢ëÁîüÊàêÁöÑÊ≤âÊ≤°Â¥©Ê∫É"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Á†îÁ©∂‰∫∫ÂëòÂºÄÂèë‰∫Ü‰∏ÄÁßçÊñπÊ≥ïÊù•ÂÖãÊúçËá™ÂõûÂΩíËßÜÈ¢ëÁîüÊàê‰∏≠ÁöÑÊ≤âÊ≤°Â¥©Ê∫ÉÈóÆÈ¢òÔºåËß£ÂÜ≥‰∫ÜÊóãËΩ¨‰ΩçÁΩÆÂµåÂÖ•‰∏éÂ§öÂ§¥Ê≥®ÊÑèÂäõÊú∫Âà∂‰πãÈó¥ÁöÑÂÜ≤Á™Å„ÄÇËøôÁßçÊñπÊ≥ïËÉΩÂ§üÂÆûÁé∞ÂÆûÊó∂ÊµÅÂ™í‰ΩìËßÜÈ¢ëÁîüÊàêÔºåÈïøÂ∫¶ÂèØËææ12Â∞èÊó∂„ÄÇÈÄöËøáÂºïÂÖ•Â§öÂ§¥ÊóãËΩ¨‰ΩçÁΩÆÂµåÂÖ•ÊäñÂä®ÔºåÁ†îÁ©∂ËÄÖÊúâÊïàÊäëÂà∂‰∫ÜÊ≤âÊ≤°Â¥©Ê∫ÉÁé∞Ë±°ÔºåÂêåÊó∂‰øùÊåÅ‰∫ÜÁîüÊàêË¥®Èáè„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®ÁîüÊàêÈïøËßÜÈ¢ëÊó∂Ë°®Áé∞Âá∫Ëâ≤ÔºåÊàêÂäüÈÅøÂÖç‰∫ÜÂÜÖÂÆπÈáçÂ§çÂíåÂú∫ÊôØÈáçÁΩÆÁöÑÈóÆÈ¢ò„ÄÇ', title='Á™ÅÁ†¥Ëá™ÂõûÂΩíËßÜÈ¢ëÁîüÊàêÁöÑÊ≤âÊ≤°Â¥©Ê∫É'))
[30.01.2026 18:45] Using data from previous issue: {"categories": ["#multimodal", "#reasoning", "#benchmark", "#open_source", "#training", "#long_context", "#inference"], "emoji": "üß†", "ru": {"title": "–û–ø—Ç–∏—á–µ—Å–∫–∞—è –ø–∞–º—è—Ç—å –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –≤—ã–≤–æ–¥–∞: —Å–∂–∞—Ç–∏–µ —á–µ—Ä–µ–∑ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é", "desc": "VTC-R1 ‚Äî –Ω–æ–≤–∞—è –ø–∞—Ä–∞–¥–∏–≥–º–∞ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä–∞
[30.01.2026 18:45] Using data from previous issue: {"categories": ["#reasoning", "#training", "#rl", "#optimization", "#small_models"], "emoji": "üß†", "ru": {"title": "–ê–∫—Ç–∏–≤–Ω–æ–µ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ —Å–∫—Ä—ã—Ç–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –º–µ—Ç–æ–¥ –∞–∫—Ç–∏–≤–Ω–æ–≥–æ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –≤ —Å–∫—Ä—ã—Ç–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è —É–ª—É
[30.01.2026 18:45] Using data from previous issue: {"categories": ["#dataset", "#agents", "#benchmark"], "emoji": "üîç", "ru": {"title": "–ì–ª—É–±–æ–∫–∏–π –ø–æ–∏—Å–∫ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏: –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –º–Ω–æ–≥–æ—à–∞–≥–æ–≤–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤", "desc": "–†–∞–±–æ—Ç–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –±–µ–Ω—á–º–∞—Ä–∫ DeepSearchQA —Å 900 –∑–∞–¥–∞—á–∞–º–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ —Å–ª–æ–∂–Ω—ã—Ö –º–Ω–æ–≥–æ—à–∞–≥–æ–≤—ã—Ö –∑–∞–¥–∞—á–∞—Ö –ø–æ–∏—Å–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏. –ë–µ–Ω
[30.01.2026 18:45] Using data from previous issue: {"categories": ["#diffusion", "#cv", "#architecture", "#optimization", "#training"], "emoji": "‚ö°", "ru": {"title": "–û–¥–Ω–æ—à–∞–≥–æ–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –±–µ–∑ —Å–∫—Ä—ã—Ç–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ —á–µ—Ä–µ–∑ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –≤—ã—Ö–æ–¥–æ–≤ –∏ –ø–æ—Ç–µ—Ä—å", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –º–µ—Ç–æ–¥ Pixel MeanFlow –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∑–∞ –æ–¥–∏–Ω —à–∞–≥
[30.01.2026 18:45] Using data from previous issue: {"categories": ["#training", "#architecture"], "emoji": "üìâ", "ru": {"title": "–ö—Ä–æ–Ω–µ–∫–µ—Ä–æ–≤—ã –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –ø–∞—Ä–∞–º–µ—Ç—Ä–∏–∑–∞—Ü–∏–∏ –≥–∏–ø–µ—Ä-—Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ —É–ª—É—á—à–µ–Ω–∏—é –≥–∏–ø–µ—Ä-—Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π –≤ –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç—è—Ö, –∫–æ—Ç–æ—Ä—ã–µ —Å—Ç—Ä–∞–¥–∞—é—Ç –æ—Ç –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–π –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç–∏
[30.01.2026 18:45] Using data from previous issue: {"categories": ["#synthetic", "#open_source"], "emoji": "üîÑ", "ru": {"title": "–°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –≤–º–µ—Å—Ç–æ —Ç–µ–∫—Å—Ç–æ–≤: –Ω–æ–≤—ã–π –ø—É—Ç—å –∫ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–º—É –æ–±—É—á–µ–Ω–∏—é —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ —Å–æ–∑–¥–∞–Ω–∏—è –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –ø–∞—Ä –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è-–æ—Ç–≤–µ—Ç –∏–∑ –Ω–µ—Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∫–æ—Ä–ø—É
[30.01.2026 18:45] Using data from previous issue: {"categories": ["#3d", "#cv", "#architecture", "#transfer_learning", "#open_source", "#dataset", "#optimization", "#multimodal"], "emoji": "üìè", "ru": {"title": "–ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞–∫ –ø—É—Ç—å –∫ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–º—É –≤–æ—Å–ø—Ä–∏—è—Ç–∏—é –º–µ—Ç—Ä–∏—á–µ—Å–∫–æ–π –≥–ª—É–±–∏–Ω—ã", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Metric Anything ‚Äî –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—É—é —Å–∏
[30.01.2026 18:45] Using data from previous issue: {"categories": ["#synthetic", "#architecture", "#data", "#training", "#interpretability"], "emoji": "üîç", "ru": {"title": "–û—Ç –¥–∞–Ω–Ω—ã—Ö –∫ —Å—Ö–µ–º–∞–º: –ø—Ä–∏—á–∏–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏—Å—Ö–æ–∂–¥–µ–Ω–∏—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–∞—è –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è Mechanistic Data Attribution
[30.01.2026 18:45] Using data from previous issue: {"categories": ["#multimodal", "#cv", "#training"], "emoji": "üîÑ", "ru": {"title": "–í–∑–∞–∏–º–Ω–æ–µ —É—Å–∏–ª–µ–Ω–∏–µ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è", "desc": "UniMRG ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥ –ø–æ—Å—Ç–æ–±—É—á–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –ø—É—Ç—ë–º –æ–±—É—á–µ–Ω–∏—è –∏—Ö –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤—Å
[30.01.2026 18:45] Using data from previous issue: {"categories": ["#rl", "#training", "#rlhf"], "emoji": "üõ°Ô∏è", "ru": {"title": "–ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –∏ —Ñ–∞–∫–æ—Ç–∏—á–Ω–æ—Å—Ç—å –≤–æ –≤—Ä–µ–º—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —á–µ—Ä–µ–∑ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –æ–±—É—á–µ–Ω–∏–∏ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –∫–æ—Ç
[30.01.2026 18:45] Using data from previous issue: {"categories": ["#long_context", "#transfer_learning", "#architecture", "#optimization", "#training", "#inference"], "emoji": "‚ö°", "ru": {"title": "–ì–∏–±—Ä–∏–¥–Ω—ã–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã: —É—Å–∫–æ—Ä–µ–Ω–∏–µ –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ —á–µ—Ä–µ–∑ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—é –≤ RNN —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω HALO ‚Äî –º–µ—Ç–æ–¥ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –¥
[30.01.2026 18:45] Using data from previous issue: {"categories": ["#optimization", "#open_source", "#reasoning"], "emoji": "‚úÇÔ∏è", "ru": {"title": "–£–º–Ω–æ–µ –æ–±—Ä–µ–∑–∞–Ω–∏–µ —Ü–µ–ø–æ—á–µ–∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —á–µ—Ä–µ–∑ –∞–Ω–∞–ª–∏–∑ –≤–Ω–∏–º–∞–Ω–∏—è", "desc": "FROST ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –∞–Ω–∞–ª–∏–∑–µ –≤–µ—Å–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è (attention weights) –≤ –Ω–µ–π—Ä–æ—Å–µ—Ç—è—Ö. –ü–æ–¥—Ö–æ–¥ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ 
[30.01.2026 18:45] Using data from previous issue: {"categories": ["#synthetic", "#training", "#multimodal", "#audio", "#machine_translation", "#video", "#diffusion"], "emoji": "üé¨", "ru": {"title": "–õ—ë–≥–∫–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è –∞—É–¥–∏–æ-–≤–∏–¥–µ–æ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–≥–æ –¥—É–±–ª—è–∂–∞", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –º–µ—Ç–æ–¥ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–æ–π –º
[30.01.2026 18:45] Using data from previous issue: {"categories": ["#inference", "#optimization", "#training", "#architecture"], "emoji": "‚ö°", "ru": {"title": "–û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä —Å –∫–æ–º–ø–µ–Ω—Å–∞—Ü–∏–µ–π –æ—à–∏–±–æ–∫: –æ–±—É—á–µ–Ω–∏–µ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω—ã—Ö LLM –±–µ–∑ –ø–æ—Ç–µ—Ä—å –ø–∞–º—è—Ç–∏", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è Error-Compensating Optimizer (ECO) ‚Äî –Ω–æ–≤—ã–π –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä, –∫–æ—Ç–æ—Ä—ã–π —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É
[30.01.2026 18:45] Querying the API.
[30.01.2026 18:45] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

GANPO uses latent-space regularization through adversarial divergence minimization to improve language model preference optimization, offering more robust structural feedback than token-level methods.  					AI-generated summary 				 Learning from human feedback typically relies on preference optimization that constrains policy updates through token-level regularization. However, preference optimization for language models is particularly challenging because token-space similarity does not imply semantic or behavioral similarity. To address this challenge, we leverage latent-space regularization for language model preference optimization. We introduce GANPO, which achieves latent-space regularization by penalizing divergence between the internal representations of a policy model and a reference model. Given that latent representations are not associated with explicit probability densities, we adopt an adversarial approach inspired by GANs to minimize latent-space divergence. We integrate GANPO as a regularizer into existing offline preference optimization objectives. Experiments across multiple model architectures and tasks show consistent improvements from latent-space regularization. Further, by comparing GANPO-induced inferential biases with those from token-level regularization, we find that GANPO provides more robust structural feedback under distributional shift and noise while maintaining comparable downstream performance with minor computational overhead.
[30.01.2026 18:45] Response: ```json
{
  "desc": "GANPO ‚Äî —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π —á–µ–ª–æ–≤–µ–∫–∞, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—é –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ —Å–∫—Ä—ã—Ç—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –≤–º–µ—Å—Ç–æ —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞ –Ω–∞ —É—Ä–æ–≤–Ω–µ —Ç–æ–∫–µ–Ω–æ–≤. –ê–≤—Ç–æ—Ä—ã –ø—Ä–∏–º–µ–Ω—è—é—Ç –∞–¥–≤–µ—Ä—Å–∞—Ä–∏–≤–Ω—ã–π –ø–æ–¥—Ö–æ–¥, –≤–¥–æ—Ö–Ω–æ–≤–ª–µ–Ω–Ω—ã–π GAN, –¥–ª—è –º–∏–Ω–∏–º–∏–∑–∞—Ü–∏–∏ —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏—è –º–µ–∂–¥—É –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–º–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è–º–∏ –æ–±—É—á–∞–µ–º–æ–π –º–æ–¥–µ–ª–∏ –∏ —ç—Ç–∞–ª–æ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –ª—É—á—à–µ –æ—Ç—Ä–∞–∂–∞–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ –∏ –ø–æ–≤–µ–¥–µ–Ω—á–µ—Å–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ, —á–µ–º —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è –Ω–∞ —É—Ä–æ–≤–Ω–µ —Ç–æ–∫–µ–Ω–æ–≤, –ø–æ—Å–∫–æ–ª—å–∫—É —Å—Ö–æ–¥—Å—Ç–≤–æ –≤ —Å–∫—Ä—ã—Ç–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –ª—É—á—à–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Å—Ö–æ–¥—Å—Ç–≤—É –∑–Ω–∞—á–µ–Ω–∏–π. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ GANPO –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –±–æ–ª–µ–µ –Ω–∞–¥–µ–∂–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—É—é –æ–±—Ä–∞—Ç–Ω—É—é —Å–≤—è–∑—å –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ —à—É–º–∞ –∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö —Å–¥–≤–∏–≥–æ–≤, —Å–æ—Ö—Ä–∞–Ω—è—è –ø—Ä–∏ —ç—Ç–æ–º —Å—Ä–∞–≤–Ω–∏–º–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–º–∏ –∑–∞—Ç—Ä–∞—Ç–∞–º–∏.",
  "emoji": "üé≠",
  "title": "–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ —Å–∫—Ä—ã—Ç—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –≤–º–µ—Å—Ç–æ —Ç–æ–∫–µ–Ω–æ–≤"
}
```
[30.01.2026 18:45] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"GANPO uses latent-space regularization through adversarial divergence minimization to improve language model preference optimization, offering more robust structural feedback than token-level methods.  					AI-generated summary 				 Learning from human feedback typically relies on preference optimization that constrains policy updates through token-level regularization. However, preference optimization for language models is particularly challenging because token-space similarity does not imply semantic or behavioral similarity. To address this challenge, we leverage latent-space regularization for language model preference optimization. We introduce GANPO, which achieves latent-space regularization by penalizing divergence between the internal representations of a policy model and a reference model. Given that latent representations are not associated with explicit probability densities, we adopt an adversarial approach inspired by GANs to minimize latent-space divergence. We integrate GANPO as a regularizer into existing offline preference optimization objectives. Experiments across multiple model architectures and tasks show consistent improvements from latent-space regularization. Further, by comparing GANPO-induced inferential biases with those from token-level regularization, we find that GANPO provides more robust structural feedback under distributional shift and noise while maintaining comparable downstream performance with minor computational overhead."

[30.01.2026 18:45] Response: ```python
["RLHF", "TRAINING", "ARCHITECTURE"]
```

**Justification:**

- **RLHF**: The paper explicitly discusses "learning from human feedback" and "preference optimization," which are core components of RLHF methods. The paper proposes GANPO as an improvement to preference optimization techniques.

- **TRAINING**: The paper focuses on improving model training and fine-tuning methods through better regularization techniques for preference optimization, which directly relates to training methodology improvements.

- **ARCHITECTURE**: The paper proposes GANPO, a novel architectural component/approach that uses adversarial divergence minimization and latent-space regularization, representing a novel method for structuring the preference optimization process.
[30.01.2026 18:45] Error. Failed to parse JSON from LLM. ["RLHF", "TRAINING", "ARCHITECTURE"]


**Justification:**

- **RLHF**: The paper explicitly discusses "learning from human feedback" and "preference optimization," which are core components of RLHF methods. The paper proposes GANPO as an improvement to preference optimization techniques.

- **TRAINING**: The paper focuses on improving model training and fine-tuning methods through better regularization techniques for preference optimization, which directly relates to training methodology improvements.

- **ARCHITECTURE**: The paper proposes GANPO, a novel architectural component/approach that uses adversarial divergence minimization and latent-space regularization, representing a novel method for structuring the preference optimization process.
[30.01.2026 18:45] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"GANPO uses latent-space regularization through adversarial divergence minimization to improve language model preference optimization, offering more robust structural feedback than token-level methods.  					AI-generated summary 				 Learning from human feedback typically relies on preference optimization that constrains policy updates through token-level regularization. However, preference optimization for language models is particularly challenging because token-space similarity does not imply semantic or behavioral similarity. To address this challenge, we leverage latent-space regularization for language model preference optimization. We introduce GANPO, which achieves latent-space regularization by penalizing divergence between the internal representations of a policy model and a reference model. Given that latent representations are not associated with explicit probability densities, we adopt an adversarial approach inspired by GANs to minimize latent-space divergence. We integrate GANPO as a regularizer into existing offline preference optimization objectives. Experiments across multiple model architectures and tasks show consistent improvements from latent-space regularization. Further, by comparing GANPO-induced inferential biases with those from token-level regularization, we find that GANPO provides more robust structural feedback under distributional shift and noise while maintaining comparable downstream performance with minor computational overhead."

[30.01.2026 18:45] Response: ```python
['ALIGNMENT', 'OPTIMIZATION']
```

**Justification:**

1. **ALIGNMENT**: The paper discusses preference optimization for language models, which is fundamentally about aligning model behavior with human preferences through learning from human feedback. This is a core alignment topic.

2. **OPTIMIZATION**: The paper presents GANPO, a method for improving the optimization process of language model preference optimization through latent-space regularization and adversarial divergence minimization. It explicitly addresses optimization challenges in training.
[30.01.2026 18:45] Error. Failed to parse JSON from LLM. ["ALIGNMENT", "OPTIMIZATION"]


**Justification:**

1. **ALIGNMENT**: The paper discusses preference optimization for language models, which is fundamentally about aligning model behavior with human preferences through learning from human feedback. This is a core alignment topic.

2. **OPTIMIZATION**: The paper presents GANPO, a method for improving the optimization process of language model preference optimization through latent-space regularization and adversarial divergence minimization. It explicitly addresses optimization challenges in training.
[30.01.2026 18:45] Response: ParsedChatCompletionMessage[Article](content='{"desc":"GANPO is a method that enhances language model preference optimization by using latent-space regularization instead of traditional token-level methods. It focuses on minimizing the divergence between the internal representations of a policy model and a reference model, which helps in capturing semantic similarities better. By employing an adversarial approach similar to Generative Adversarial Networks (GANs), GANPO effectively penalizes discrepancies in latent representations. Experiments demonstrate that this approach yields more reliable feedback in challenging conditions while keeping computational costs low and maintaining performance.","title":"Enhancing Language Models with Latent-Space Regularization"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='GANPO is a method that enhances language model preference optimization by using latent-space regularization instead of traditional token-level methods. It focuses on minimizing the divergence between the internal representations of a policy model and a reference model, which helps in capturing semantic similarities better. By employing an adversarial approach similar to Generative Adversarial Networks (GANs), GANPO effectively penalizes discrepancies in latent representations. Experiments demonstrate that this approach yields more reliable feedback in challenging conditions while keeping computational costs low and maintaining performance.', title='Enhancing Language Models with Latent-Space Regularization'))
[30.01.2026 18:45] Response: ParsedChatCompletionMessage[Article](content='{"desc":"GANPOÊòØ‰∏ÄÁßçÈÄöËøáÂØπÊäóÊÄßÊï£Â∫¶ÊúÄÂ∞èÂåñÊù•ÂÆûÁé∞ÊΩúÂú®Á©∫Èó¥Ê≠£ÂàôÂåñÁöÑÊñπÊ≥ïÔºåÊó®Âú®ÊîπÂñÑËØ≠Ë®ÄÊ®°ÂûãÁöÑÂÅèÂ•Ω‰ºòÂåñ„ÄÇ‰∏éÂü∫‰∫éÊ†áËÆ∞ÁöÑÊ≠£ÂàôÂåñÊñπÊ≥ïÁõ∏ÊØîÔºåGANPOÊèê‰æõ‰∫ÜÊõ¥Âº∫ÁöÑÁªìÊûÑÂèçÈ¶àÔºåÁâπÂà´ÊòØÂú®Â§ÑÁêÜËØ≠Ë®ÄÊ®°ÂûãÊó∂„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÊÉ©ÁΩöÁ≠ñÁï•Ê®°ÂûãÂíåÂèÇËÄÉÊ®°Âûã‰πãÈó¥ÁöÑÂÜÖÈÉ®Ë°®Á§∫Êï£Â∫¶Êù•ÂÆûÁé∞ÊΩúÂú®Á©∫Èó¥Ê≠£ÂàôÂåñ„ÄÇÂÆûÈ™åË°®ÊòéÔºåGANPOÂú®Â§ö‰∏™Ê®°ÂûãÊû∂ÊûÑÂíå‰ªªÂä°‰∏≠ÈÉΩËÉΩÂ∏¶Êù•‰∏ÄËá¥ÁöÑÊÄßËÉΩÊèêÂçáÔºåÂ∞§ÂÖ∂Âú®ÂàÜÂ∏ÉÂèòÂåñÂíåÂô™Â£∞ÊÉÖÂÜµ‰∏ãË°®Áé∞Êõ¥‰∏∫Á®≥ÂÅ•„ÄÇ","title":"GANPOÔºöÊèêÂçáËØ≠Ë®ÄÊ®°ÂûãÁöÑÊΩúÂú®Á©∫Èó¥Ê≠£ÂàôÂåñ"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='GANPOÊòØ‰∏ÄÁßçÈÄöËøáÂØπÊäóÊÄßÊï£Â∫¶ÊúÄÂ∞èÂåñÊù•ÂÆûÁé∞ÊΩúÂú®Á©∫Èó¥Ê≠£ÂàôÂåñÁöÑÊñπÊ≥ïÔºåÊó®Âú®ÊîπÂñÑËØ≠Ë®ÄÊ®°ÂûãÁöÑÂÅèÂ•Ω‰ºòÂåñ„ÄÇ‰∏éÂü∫‰∫éÊ†áËÆ∞ÁöÑÊ≠£ÂàôÂåñÊñπÊ≥ïÁõ∏ÊØîÔºåGANPOÊèê‰æõ‰∫ÜÊõ¥Âº∫ÁöÑÁªìÊûÑÂèçÈ¶àÔºåÁâπÂà´ÊòØÂú®Â§ÑÁêÜËØ≠Ë®ÄÊ®°ÂûãÊó∂„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÊÉ©ÁΩöÁ≠ñÁï•Ê®°ÂûãÂíåÂèÇËÄÉÊ®°Âûã‰πãÈó¥ÁöÑÂÜÖÈÉ®Ë°®Á§∫Êï£Â∫¶Êù•ÂÆûÁé∞ÊΩúÂú®Á©∫Èó¥Ê≠£ÂàôÂåñ„ÄÇÂÆûÈ™åË°®ÊòéÔºåGANPOÂú®Â§ö‰∏™Ê®°ÂûãÊû∂ÊûÑÂíå‰ªªÂä°‰∏≠ÈÉΩËÉΩÂ∏¶Êù•‰∏ÄËá¥ÁöÑÊÄßËÉΩÊèêÂçáÔºåÂ∞§ÂÖ∂Âú®ÂàÜÂ∏ÉÂèòÂåñÂíåÂô™Â£∞ÊÉÖÂÜµ‰∏ãË°®Áé∞Êõ¥‰∏∫Á®≥ÂÅ•„ÄÇ', title='GANPOÔºöÊèêÂçáËØ≠Ë®ÄÊ®°ÂûãÁöÑÊΩúÂú®Á©∫Èó¥Ê≠£ÂàôÂåñ'))
[30.01.2026 18:45] Using data from previous issue: {"categories": ["#architecture", "#benchmark", "#reasoning", "#long_context", "#agents"], "emoji": "üß†", "ru": {"title": "–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –ø–∞–º—è—Ç—å –∫–∞–∫ –∫–ª—é—á –∫ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –ø–∞–º—è—Ç–∏ BMAM, –≤–¥–æ—Ö–Ω–æ–≤–ª–µ–Ω–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π –º–æ–∑–≥–∞, –∫–æ—Ç–æ—Ä–∞—è —Ä–∞–∑–¥–µ–ª—è–µ—Ç –ø–∞–º
[30.01.2026 18:45] Using data from previous issue: {"categories": ["#dataset", "#plp", "#rl", "#benchmark"], "emoji": "üîç", "ru": {"title": "–û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è —á–µ—Ä–µ–∑ –∫–æ–Ω—Ç—Ä–∞—Å—Ç–∏–≤–Ω—ã–µ –∞–Ω–æ–º–∞–ª–∏–∏ –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ TRACE –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–π —Å —Ñ—É–Ω–∫—Ü–∏–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –≤ —Å—Ä–µ–¥–µ –≥–µ
[30.01.2026 18:45] Using data from previous issue: {"categories": ["#audio", "#architecture", "#benchmark"], "emoji": "üîç", "ru": {"title": "–û–ø—Ç–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Å–µ–≥–º–µ–Ω—Ç–∞: –∫–ª—é—á –∫ —Ç–æ—á–Ω–æ—Å—Ç–∏ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤—ã—Ö –∞—É–¥–∏–æ—Ñ–∏–Ω–≥–µ—Ä–ø—Ä–∏–Ω—Ç–æ–≤", "desc": "–í —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –≤–ª–∏—è–Ω–∏–µ –¥–ª–∏–Ω—ã –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –æ–∫–Ω–∞ –Ω–∞ –∫–∞—á–µ—Å—Ç–≤–æ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤—ã—Ö –∞—É–¥–∏–æ—Ñ–∏–Ω–≥–µ—Ä–ø—Ä–∏–Ω—Ç–æ–≤ ‚Äî –∫–æ–º–ø–∞–∫—Ç–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –∑–≤
[30.01.2026 18:45] Using data from previous issue: {"categories": [], "emoji": "üé®", "ru": {"title": "–ó–Ω–∞–Ω–∏—è –∏–∑ –¥–∞–Ω–Ω—ã—Ö –¥–∏–∑–∞–π–Ω–∞ –¥–ª—è —Å—Ç–∏–ª–∏—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ —É–ª—É—á—à–µ–Ω–∏—è –ø–æ —Ç–µ–∫—Å—Ç—É", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ PRISM –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏—Ö –¥–∏–∑–∞–π–Ω–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞, –∫–æ—Ç–æ—Ä—ã–π —Å–æ–∑–¥–∞—ë—Ç –±–∞–∑—É –∑–Ω–∞–Ω–∏–π –∏–∑ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∏–∑–∞–π–Ω-–¥–∞–Ω–Ω—ã—Ö. –ê–≤—Ç–æ—Ä—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ
[30.01.2026 18:45] Using data from previous issue: {"categories": ["#dataset", "#reasoning", "#rlhf", "#benchmark", "#open_source", "#interpretability", "#agents"], "emoji": "üï∑Ô∏è", "ru": {"title": "–†–∞—Å—Å—É–∂–¥–∞—é—â–∞—è –º–æ–¥–µ–ª—å –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–π –¥–ª—è –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–π –≤–µ–±-–Ω–∞–≤–∏–≥–∞—Ü–∏–∏", "desc": "WebArbiter –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—é –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–π –¥–ª—è –≤
[30.01.2026 18:45] Using data from previous issue: {"categories": ["#benchmark", "#cv", "#robotics"], "emoji": "ü§ñ", "ru": {"title": "–û–±—ä–µ–∫—Ç–Ω–æ-—Ü–µ–Ω—Ç—Ä–∏—á–µ—Å–∫–∏–µ —Å–ª–æ—Ç—ã ‚Äî –∫–ª—é—á –∫ –Ω–∞–¥—ë–∂–Ω–æ–π –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–∏ —Ä–æ–±–æ—Ç–æ–≤ –ø—Ä–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–∏ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —É—Å–ª–æ–≤–∏–π", "desc": "–í —ç—Ç–æ–π —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ –≤—ã–±–æ—Ä–∞ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –ø–æ–ª–∏—Ç–∏–∫ –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–∏ —Ä–æ–±–æ—Ç–æ–≤.
[30.01.2026 18:45] Using data from previous issue: {"categories": ["#benchmark", "#video", "#robotics"], "emoji": "üé¨", "ru": {"title": "–†–∞–∑–¥–µ–ª—ë–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —Ñ–∏–∑–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –≤–∏–¥–µ–æ-–≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω WorldBench ‚Äî –≤–∏–¥–µ–æ-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è —Ä–∞–∑–¥–µ–ª—ë–Ω–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ —Ñ–∏–∑–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª
[30.01.2026 18:45] Using data from previous issue: {"categories": ["#training", "#cv", "#robotics", "#architecture"], "emoji": "ü§ñ", "ru": {"title": "–û–±—ä–µ–∫—Ç–Ω–æ-—Ü–µ–Ω—Ç—Ä–∏—á–Ω–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∏ —á–µ—Ä–µ–∑ –º–Ω–æ–≥–æ—Ñ–∞–∑–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è STORM ‚Äî –ª—ë–≥–∫–∏–π –∞–¥–∞–ø—Ç–∞—Ü–∏–æ–Ω–Ω—ã–π –º–æ–¥—É–ª—å, –∫–æ—Ç–æ—Ä—ã–π –¥–æ–ø–æ–ª–Ω—è–µ—Ç –∑–∞–º–æ—Ä–æ–∂–µ–Ω–Ω—ã–µ –≤–∏–∑—É–∞–ª—å
[30.01.2026 18:45] Renaming data file.
[30.01.2026 18:45] Renaming previous data. hf_papers.json to ./d/2026-01-30.json
[30.01.2026 18:45] Saving new data file.
[30.01.2026 18:45] Generating page.
[30.01.2026 18:45] Renaming previous page.
[30.01.2026 18:45] Renaming previous data. index.html to ./d/2026-01-30.html
[30.01.2026 18:45] Writing result.
[30.01.2026 18:45] Renaming log file.
[30.01.2026 18:45] Renaming previous data. log.txt to ./logs/2026-01-30_last_log.txt
