[30.01.2026 07:43] Read previous papers.
[30.01.2026 07:43] Generating top page (month).
[30.01.2026 07:43] Writing top page (month).
[30.01.2026 08:42] Read previous papers.
[30.01.2026 08:42] Get feed.
[30.01.2026 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2601.20354
[30.01.2026 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2601.20833
[30.01.2026 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21639
[30.01.2026 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22153
[30.01.2026 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21821
[30.01.2026 08:42] Extract page data from URL. URL: https://huggingface.co/papers/2601.22046
[30.01.2026 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21420
[30.01.2026 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22154
[30.01.2026 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21754
[30.01.2026 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21337
[30.01.2026 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21204
[30.01.2026 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2601.18129
[30.01.2026 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21181
[30.01.2026 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22069
[30.01.2026 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2601.20975
[30.01.2026 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22054
[30.01.2026 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21598
[30.01.2026 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21406
[30.01.2026 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21343
[30.01.2026 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21051
[30.01.2026 08:42] Extract page data from URL. URL: https://huggingface.co/papers/2601.22158
[30.01.2026 08:42] Extract page data from URL. URL: https://huggingface.co/papers/2601.22156
[30.01.2026 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2601.20465
[30.01.2026 08:42] Extract page data from URL. URL: https://huggingface.co/papers/2601.19001
[30.01.2026 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2601.17690
[30.01.2026 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2601.11747
[30.01.2026 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21872
[30.01.2026 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21416
[30.01.2026 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21282
[30.01.2026 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2601.20381
[30.01.2026 08:42] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[30.01.2026 08:42] No deleted papers detected.
[30.01.2026 08:42] Downloading and parsing papers (pdf, html). Total: 30.
[30.01.2026 08:42] Downloading and parsing paper https://huggingface.co/papers/2601.20354.
[30.01.2026 08:42] Extra JSON file exists (./assets/json/2601.20354.json), skip PDF parsing.
[30.01.2026 08:42] Paper image links file exists (./assets/img_data/2601.20354.json), skip HTML parsing.
[30.01.2026 08:42] Success.
[30.01.2026 08:42] Downloading and parsing paper https://huggingface.co/papers/2601.20833.
[30.01.2026 08:42] Extra JSON file exists (./assets/json/2601.20833.json), skip PDF parsing.
[30.01.2026 08:42] Paper image links file exists (./assets/img_data/2601.20833.json), skip HTML parsing.
[30.01.2026 08:42] Success.
[30.01.2026 08:42] Downloading and parsing paper https://huggingface.co/papers/2601.21639.
[30.01.2026 08:42] Extra JSON file exists (./assets/json/2601.21639.json), skip PDF parsing.
[30.01.2026 08:42] Paper image links file exists (./assets/img_data/2601.21639.json), skip HTML parsing.
[30.01.2026 08:42] Success.
[30.01.2026 08:42] Downloading and parsing paper https://huggingface.co/papers/2601.22153.
[30.01.2026 08:42] Extra JSON file exists (./assets/json/2601.22153.json), skip PDF parsing.
[30.01.2026 08:42] Paper image links file exists (./assets/img_data/2601.22153.json), skip HTML parsing.
[30.01.2026 08:42] Success.
[30.01.2026 08:42] Downloading and parsing paper https://huggingface.co/papers/2601.21821.
[30.01.2026 08:42] Extra JSON file exists (./assets/json/2601.21821.json), skip PDF parsing.
[30.01.2026 08:42] Paper image links file exists (./assets/img_data/2601.21821.json), skip HTML parsing.
[30.01.2026 08:42] Success.
[30.01.2026 08:42] Downloading and parsing paper https://huggingface.co/papers/2601.22046.
[30.01.2026 08:42] Downloading paper 2601.22046 from https://arxiv.org/pdf/2601.22046v1...
[30.01.2026 08:42] Extracting affiliations from text.
[30.01.2026 08:42] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 9 2 ] . [ 1 6 4 0 2 2 . 1 0 6 2 : r PLANING: Loosely Coupled Triangle-Gaussian Framework for Streaming 3D Reconstruction Changjian Jiang1,2* Kerui Ren3,2* Xudong Li2 Kaiwen Song4,2 Linning Xu5,2 Tao Lu2 Junting Dong2 Yu Zhang1 Bo Dai6 Mulin Yu2 1Zhejiang University 2Shanghai Artificial Intelligence Laboratory 3Shanghai Jiao Tong University 4The University of Science and Technology of China 5The Chinese University of Hong Kong 6The University of Hong Kong Figure 1. PLANING introduces loosely coupled triangle-Gaussian representation for streaming 3D reconstruction, balancing geometric accuracy, high-fidelity rendering, and computational efficiency. Building upon this hybrid representation, we further adapt it to an efficient streaming reconstruction framework for monocular image sequences, enabling effective modeling of both scene geometry and appearance in streaming setting. Leveraging the inherent edge-preserving property of triangle primitives, our method allows for the explicit extraction of compact planar structures, which can serve as high-performance simulation environment for locomotion training in embodied AI. "
[30.01.2026 08:42] Response: ```python
[
    "Zhejiang University",
    "Shanghai Artificial Intelligence Laboratory",
    "Shanghai Jiao Tong University",
    "The University of Science and Technology of China",
    "The Chinese University of Hong Kong",
    "The University of Hong Kong"
]
```
[30.01.2026 08:42] Deleting PDF ./assets/pdf/2601.22046.pdf.
[30.01.2026 08:42] Success.
[30.01.2026 08:42] Downloading and parsing paper https://huggingface.co/papers/2601.21420.
[30.01.2026 08:42] Extra JSON file exists (./assets/json/2601.21420.json), skip PDF parsing.
[30.01.2026 08:42] Paper image links file exists (./assets/img_data/2601.21420.json), skip HTML parsing.
[30.01.2026 08:42] Success.
[30.01.2026 08:42] Downloading and parsing paper https://huggingface.co/papers/2601.22154.
[30.01.2026 08:42] Extra JSON file exists (./assets/json/2601.22154.json), skip PDF parsing.
[30.01.2026 08:42] Paper image links file exists (./assets/img_data/2601.22154.json), skip HTML parsing.
[30.01.2026 08:42] Success.
[30.01.2026 08:42] Downloading and parsing paper https://huggingface.co/papers/2601.21754.
[30.01.2026 08:42] Extra JSON file exists (./assets/json/2601.21754.json), skip PDF parsing.
[30.01.2026 08:42] Paper image links file exists (./assets/img_data/2601.21754.json), skip HTML parsing.
[30.01.2026 08:42] Success.
[30.01.2026 08:42] Downloading and parsing paper https://huggingface.co/papers/2601.21337.
[30.01.2026 08:42] Extra JSON file exists (./assets/json/2601.21337.json), skip PDF parsing.
[30.01.2026 08:42] Paper image links file exists (./assets/img_data/2601.21337.json), skip HTML parsing.
[30.01.2026 08:42] Success.
[30.01.2026 08:42] Downloading and parsing paper https://huggingface.co/papers/2601.21204.
[30.01.2026 08:42] Extra JSON file exists (./assets/json/2601.21204.json), skip PDF parsing.
[30.01.2026 08:42] Paper image links file exists (./assets/img_data/2601.21204.json), skip HTML parsing.
[30.01.2026 08:42] Success.
[30.01.2026 08:42] Downloading and parsing paper https://huggingface.co/papers/2601.18129.
[30.01.2026 08:42] Extra JSON file exists (./assets/json/2601.18129.json), skip PDF parsing.
[30.01.2026 08:42] Paper image links file exists (./assets/img_data/2601.18129.json), skip HTML parsing.
[30.01.2026 08:42] Success.
[30.01.2026 08:42] Downloading and parsing paper https://huggingface.co/papers/2601.21181.
[30.01.2026 08:42] Extra JSON file exists (./assets/json/2601.21181.json), skip PDF parsing.
[30.01.2026 08:42] Paper image links file exists (./assets/img_data/2601.21181.json), skip HTML parsing.
[30.01.2026 08:42] Success.
[30.01.2026 08:42] Downloading and parsing paper https://huggingface.co/papers/2601.22069.
[30.01.2026 08:42] Extra JSON file exists (./assets/json/2601.22069.json), skip PDF parsing.
[30.01.2026 08:42] Paper image links file exists (./assets/img_data/2601.22069.json), skip HTML parsing.
[30.01.2026 08:42] Success.
[30.01.2026 08:42] Downloading and parsing paper https://huggingface.co/papers/2601.20975.
[30.01.2026 08:42] Extra JSON file exists (./assets/json/2601.20975.json), skip PDF parsing.
[30.01.2026 08:42] Paper image links file exists (./assets/img_data/2601.20975.json), skip HTML parsing.
[30.01.2026 08:42] Success.
[30.01.2026 08:42] Downloading and parsing paper https://huggingface.co/papers/2601.22054.
[30.01.2026 08:42] Extra JSON file exists (./assets/json/2601.22054.json), skip PDF parsing.
[30.01.2026 08:42] Paper image links file exists (./assets/img_data/2601.22054.json), skip HTML parsing.
[30.01.2026 08:42] Success.
[30.01.2026 08:42] Downloading and parsing paper https://huggingface.co/papers/2601.21598.
[30.01.2026 08:42] Extra JSON file exists (./assets/json/2601.21598.json), skip PDF parsing.
[30.01.2026 08:42] Paper image links file exists (./assets/img_data/2601.21598.json), skip HTML parsing.
[30.01.2026 08:42] Success.
[30.01.2026 08:42] Downloading and parsing paper https://huggingface.co/papers/2601.21406.
[30.01.2026 08:42] Extra JSON file exists (./assets/json/2601.21406.json), skip PDF parsing.
[30.01.2026 08:42] Paper image links file exists (./assets/img_data/2601.21406.json), skip HTML parsing.
[30.01.2026 08:42] Success.
[30.01.2026 08:42] Downloading and parsing paper https://huggingface.co/papers/2601.21343.
[30.01.2026 08:42] Extra JSON file exists (./assets/json/2601.21343.json), skip PDF parsing.
[30.01.2026 08:42] Paper image links file exists (./assets/img_data/2601.21343.json), skip HTML parsing.
[30.01.2026 08:42] Success.
[30.01.2026 08:42] Downloading and parsing paper https://huggingface.co/papers/2601.21051.
[30.01.2026 08:42] Extra JSON file exists (./assets/json/2601.21051.json), skip PDF parsing.
[30.01.2026 08:42] Paper image links file exists (./assets/img_data/2601.21051.json), skip HTML parsing.
[30.01.2026 08:42] Success.
[30.01.2026 08:42] Downloading and parsing paper https://huggingface.co/papers/2601.22158.
[30.01.2026 08:42] Downloading paper 2601.22158 from https://arxiv.org/pdf/2601.22158v1...
[30.01.2026 08:42] Extracting affiliations from text.
[30.01.2026 08:42] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"One-step Latent-free Image Generation with Pixel Mean Flows Yiyang Lu * 1 Susie Lu * 1 Qiao Sun * 1 Hanhong Zhao * 1 Zhicheng Jiang 1 Xianbang Wang 1 Tianhong Li 1 Zhengyang Geng 2 Kaiming He 1 6 2 0 2 9 2 ] . [ 1 8 5 1 2 2 . 1 0 6 2 : r Figure 1. The pixel MeanFlow (pMF) formulation, driven by the manifold hypothesis. (Left): Following MeanFlow (Geng et al., 2025a), pMF aims to approximate the average velocity field u(zt, r, t) induced by the underlying ODE trajectory (black). We define new field x(zt, r, t) zt u(zt, r, t), which behaves like denoised images. We hypothesize that approximately lies on low-dimensional data manifold (orange curve) and can therefore be more accurately approximated by neural network. (Right): Visualization of the quantities zt, u, obtained by tracking an ODE trajectory via simulation. The average velocity field corresponds to noisy images and is inevitably higher-dimensional; the induced field corresponds to approximately clean or blurred images, which can be easier to model by neural network. Abstract Modern diffusion/flow-based models for image generation typically exhibit two core characteristics: (i) using multi-step sampling, and (ii) operating in latent space. Recent advances have made encouraging progress on each aspect individually, paving the way toward one-step diffusion/flow without latents. In this work, we take further step towards this goal and propose pixel MeanFlow (pMF). Our core guideline is to formulate the network output space and the loss space separately. The network target is designed to be on presumed low-dimensional image manifold (i.e., x-prediction), while the loss is defined via MeanFlow in the velocity space. We introduce simple transformation between the image manifold and the average velocity field. In experiments, pMF achieves strong results for one-step latent-free generation on ImageNet at 256256 resolution (2.22 FID) and 512512 resolution (2.48 FID), filling key missing piece in this regime. We hope th"
[30.01.2026 08:42] Response: ```python
["Meta"]
```
[30.01.2026 08:42] Deleting PDF ./assets/pdf/2601.22158.pdf.
[30.01.2026 08:42] Success.
[30.01.2026 08:42] Downloading and parsing paper https://huggingface.co/papers/2601.22156.
[30.01.2026 08:42] Downloading paper 2601.22156 from https://arxiv.org/pdf/2601.22156v1...
[30.01.2026 08:42] Extracting affiliations from text.
[30.01.2026 08:42] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Hybrid Linear Attention Done Right: Efficient Distillation and Effective Architectures for Extremely Long Contexts 6 2 0 2 9 2 ] . [ 1 6 5 1 2 2 . 1 0 6 2 : r Yingfa Chen * 1 Zhen Leng Thai * 1 Zihan Zhou 2 Zhu Zhang 2 Xingyu Shen 1 Shuo Wang 1 Chaojun Xiao 1 Xu Han 1 Zhiyuan Liu 1 1NLP Group, DCST, IAI, BNRIST, Tsinghua University, Beijing, China 2OpenBMB {chenyingfa1999, thaizhenleng123}@gmail.com wangshuo.thu@gmail.com, han-xu@tsinghua.edu.cn "
[30.01.2026 08:42] Response: ```python
[
    "NLP Group, DCST, IAI, BNRIST, Tsinghua University, Beijing, China",
    "OpenBMB"
]
```
[30.01.2026 08:42] Deleting PDF ./assets/pdf/2601.22156.pdf.
[30.01.2026 08:42] Success.
[30.01.2026 08:42] Downloading and parsing paper https://huggingface.co/papers/2601.20465.
[30.01.2026 08:42] Extra JSON file exists (./assets/json/2601.20465.json), skip PDF parsing.
[30.01.2026 08:42] Paper image links file exists (./assets/img_data/2601.20465.json), skip HTML parsing.
[30.01.2026 08:42] Success.
[30.01.2026 08:42] Downloading and parsing paper https://huggingface.co/papers/2601.19001.
[30.01.2026 08:42] Downloading paper 2601.19001 from https://arxiv.org/pdf/2601.19001v1...
[30.01.2026 08:42] Extracting affiliations from text.
[30.01.2026 08:42] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 6 2 ] . [ 1 1 0 0 9 1 . 1 0 6 2 : r Published as conference paper at ICLR FROST: FILTERING REASONING OUTLIERS WITH ATTENTION FOR EFFICIENT REASONING Haozheng Luo Zhuolin Jiang Md Zahid Hasan Yan Chen Soumalya Sarkar Department of Computer Science, Northwestern University, Evanston, IL 60208 USA RTX Technology Research Center (RTRC), East Hartford, CT 06118 USA Department of Electrical and Computer Engineering, Iowa State University, Ames, IA 50011 USA hluo@u.northwestern.edu {zhuolin.jiang,soumalya.sarkar}@rtx.com zahid@iastate.edu ychen@northwestern.edu "
[30.01.2026 08:42] Response: ```python
[
    "Department of Computer Science, Northwestern University, Evanston, IL 60208 USA",
    "RTX Technology Research Center (RTRC), East Hartford, CT 06118 USA",
    "Department of Electrical and Computer Engineering, Iowa State University, Ames, IA 50011 USA"
]
```
[30.01.2026 08:42] Deleting PDF ./assets/pdf/2601.19001.pdf.
[30.01.2026 08:42] Success.
[30.01.2026 08:42] Downloading and parsing paper https://huggingface.co/papers/2601.17690.
[30.01.2026 08:42] Extra JSON file exists (./assets/json/2601.17690.json), skip PDF parsing.
[30.01.2026 08:42] Paper image links file exists (./assets/img_data/2601.17690.json), skip HTML parsing.
[30.01.2026 08:42] Success.
[30.01.2026 08:42] Downloading and parsing paper https://huggingface.co/papers/2601.11747.
[30.01.2026 08:42] Extra JSON file exists (./assets/json/2601.11747.json), skip PDF parsing.
[30.01.2026 08:42] Paper image links file exists (./assets/img_data/2601.11747.json), skip HTML parsing.
[30.01.2026 08:42] Success.
[30.01.2026 08:42] Downloading and parsing paper https://huggingface.co/papers/2601.21872.
[30.01.2026 08:42] Extra JSON file exists (./assets/json/2601.21872.json), skip PDF parsing.
[30.01.2026 08:42] Paper image links file exists (./assets/img_data/2601.21872.json), skip HTML parsing.
[30.01.2026 08:42] Success.
[30.01.2026 08:42] Downloading and parsing paper https://huggingface.co/papers/2601.21416.
[30.01.2026 08:42] Extra JSON file exists (./assets/json/2601.21416.json), skip PDF parsing.
[30.01.2026 08:42] Paper image links file exists (./assets/img_data/2601.21416.json), skip HTML parsing.
[30.01.2026 08:42] Success.
[30.01.2026 08:42] Downloading and parsing paper https://huggingface.co/papers/2601.21282.
[30.01.2026 08:42] Extra JSON file exists (./assets/json/2601.21282.json), skip PDF parsing.
[30.01.2026 08:42] Paper image links file exists (./assets/img_data/2601.21282.json), skip HTML parsing.
[30.01.2026 08:42] Success.
[30.01.2026 08:42] Downloading and parsing paper https://huggingface.co/papers/2601.20381.
[30.01.2026 08:42] Extra JSON file exists (./assets/json/2601.20381.json), skip PDF parsing.
[30.01.2026 08:42] Paper image links file exists (./assets/img_data/2601.20381.json), skip HTML parsing.
[30.01.2026 08:42] Success.
[30.01.2026 08:42] Enriching papers with extra data.
[30.01.2026 08:42] ********************************************************************************
[30.01.2026 08:42] Abstract 0. A new benchmark and dataset are introduced to evaluate and improve spatial reasoning capabilities in text-to-image models through information-dense prompts and fine-tuning.  					AI-generated summary 				 Text-to-image (T2I) models have achieved remarkable success in generating high-fidelity images,...
[30.01.2026 08:42] ********************************************************************************
[30.01.2026 08:42] Abstract 1. Offline knowledge construction through structured methodological graphs enables more reliable and scalable autonomous scientific discovery by reducing reliance on real-time literature processing.  					AI-generated summary 				 Autonomous scientific discovery with large language model (LLM)-based ag...
[30.01.2026 08:42] ********************************************************************************
[30.01.2026 08:42] Abstract 2. OCRVerse is a novel end-to-end OCR method that unifies text-centric and vision-centric approaches through comprehensive data engineering and a two-stage SFT-RL training framework with domain-specific reward strategies.  					AI-generated summary 				 The development of large vision language models d...
[30.01.2026 08:42] ********************************************************************************
[30.01.2026 08:42] Abstract 3. DynamicVLA addresses dynamic object manipulation challenges through a compact vision-language-action model with temporal reasoning and closed-loop adaptation, supported by a new benchmark for dynamic manipulation tasks.  					AI-generated summary 				 Manipulating dynamic objects remains an open cha...
[30.01.2026 08:42] ********************************************************************************
[30.01.2026 08:42] Abstract 4. A large-scale multimodal reasoning dataset called MMFineReason is introduced to improve vision language models' performance through high-quality reasoning annotations and demonstrates superior parameter efficiency in fine-tuned models.  					AI-generated summary 				 Recent advances in Vision Langua...
[30.01.2026 08:42] ********************************************************************************
[30.01.2026 08:42] Abstract 5. PLANING presents an efficient streaming reconstruction framework that combines explicit geometric primitives with neural Gaussians to achieve high-quality rendering and accurate geometry simultaneously through decoupled optimization.  					AI-generated summary 				 Streaming reconstruction from mono...
[30.01.2026 08:42] ********************************************************************************
[30.01.2026 08:42] Abstract 6. ConceptMoE dynamically allocates computation by merging similar tokens into concept representations, improving both performance and efficiency in large language models through adaptive processing and reduced attention computation.  					AI-generated summary 				 Large language models allocate unifor...
[30.01.2026 08:42] ********************************************************************************
[30.01.2026 08:42] Abstract 7. Agent-RRM, a multi-faceted reward model, provides structured feedback for agentic trajectories through reasoning traces, critiques, and performance scores, with unified feedback integration showing superior performance across diverse benchmarks.  					AI-generated summary 				 Agentic Reinforcement ...
[30.01.2026 08:42] ********************************************************************************
[30.01.2026 08:42] Abstract 8. A novel framework called SCOUT is introduced that uses lightweight scouts to reduce exploration costs for large language models in nonlinguistic environments, enabling improved performance through supervised fine-tuning and reinforcement learning.  					AI-generated summary 				 While Large Language...
[30.01.2026 08:42] ********************************************************************************
[30.01.2026 08:42] Abstract 9. The Qwen3-ASR family introduces speech recognition models with language identification capabilities and a non-autoregressive forced alignment model, achieving state-of-the-art performance and efficient processing.  					AI-generated summary 				 In this report, we introduce Qwen3-ASR family, which i...
[30.01.2026 08:42] ********************************************************************************
[30.01.2026 08:42] Abstract 10. Embedding scaling offers superior sparsity scaling compared to expert scaling in large language models, enabling efficient inference through system optimizations and speculative decoding.  					AI-generated summary 				 While Mixture-of-Experts (MoE) architectures have become the standard for sparsi...
[30.01.2026 08:42] ********************************************************************************
[30.01.2026 08:42] Abstract 11. A minimal post-training approach using supervised fine-tuning, on-policy distillation, and small-scale reinforcement fine-tuning enables the development of high-quality sovereign language models with reduced resource requirements.  					AI-generated summary 				 Large language models (LLMs) have pro...
[30.01.2026 08:42] ********************************************************************************
[30.01.2026 08:42] Abstract 12. Multimodal Large Language Models suffer from cross-modal hallucinations where one modality incorrectly influences generation from another, leading to fabricated outputs; this exposes a fundamental deficiency in modality-interaction control. To address this, a training-free method called Modality-Ada...
[30.01.2026 08:42] ********************************************************************************
[30.01.2026 08:42] Abstract 13. VTC-R1 enables efficient long-context reasoning by compressing textual traces into compact images and iteratively feeding them back into vision-language models as optical memory, achieving significant speedup without sacrificing performance.  					AI-generated summary 				 Long-context reasoning has...
[30.01.2026 08:42] ********************************************************************************
[30.01.2026 08:42] Abstract 14. DeepSearchQA presents a 900-prompt benchmark evaluating agents on complex multi-step information-seeking tasks requiring systematic information collation, deduplication, and reasoning about stopping criteria across 17 fields.  					AI-generated summary 				 We introduce DeepSearchQA, a 900-prompt be...
[30.01.2026 08:42] ********************************************************************************
[30.01.2026 08:42] Abstract 15. Metric Anything presents a scalable pretraining framework for metric depth estimation that leverages diverse 3D data and sparse metric prompts to achieve superior performance across multiple vision tasks.  					AI-generated summary 				 Scaling has powered recent advances in vision foundation models...
[30.01.2026 08:42] ********************************************************************************
[30.01.2026 08:42] Abstract 16. Active latent planning method improves reasoning accuracy and efficiency by modeling latent token supervision as conditional VAE and using reinforcement learning with coherence rewards.  					AI-generated summary 				 Aiming at efficient and dense chain-of-thought (CoT) reasoning, latent reasoning m...
[30.01.2026 08:42] ********************************************************************************
[30.01.2026 08:42] Abstract 17. UniMRG enhances unified multimodal models by training them to generate multiple visual representations, improving both understanding and generation capabilities through complementary information capture.  					AI-generated summary 				 Unified Multimodal Models (UMMs) integrate both visual understan...
[30.01.2026 08:42] ********************************************************************************
[30.01.2026 08:42] Abstract 18. A reinforcement learning-based pretraining method improves language model safety, factuality, and quality by evaluating generations through a combination of model rollouts, original suffixes, and rewritten suffixes.  					AI-generated summary 				 Ensuring safety, factuality and overall quality in t...
[30.01.2026 08:42] ********************************************************************************
[30.01.2026 08:42] Abstract 19. A two-stage trained cybersecurity reasoning model achieves competitive performance on specialized tasks while maintaining general capabilities through supervised fine-tuning and reinforcement learning from verifiable rewards.  					AI-generated summary 				 We present Foundation-Sec-8B-Reasoning, th...
[30.01.2026 08:42] ********************************************************************************
[30.01.2026 08:42] Abstract 20. Pixel MeanFlow introduces a one-step latent-free image generation method by separating network output space from loss space, achieving strong performance on ImageNet at multiple resolutions.  					AI-generated summary 				 Modern diffusion/flow-based models for image generation typically exhibit two...
[30.01.2026 08:42] ********************************************************************************
[30.01.2026 08:42] Abstract 21. HALO enables efficient conversion of Transformer models to RNN-attention hybrid architectures with improved long-context performance using minimal training data.  					AI-generated summary 				 Hybrid Transformer architectures, which combine softmax attention blocks and recurrent neural networks (RN...
[30.01.2026 08:42] ********************************************************************************
[30.01.2026 08:42] Abstract 22. BMAM presents a brain-inspired multi-agent memory architecture that decomposes memory into specialized subsystems to address long-term reasoning challenges in language-model-based agents.  					AI-generated summary 				 Language-model-based agents operating over extended interaction horizons face pe...
[30.01.2026 08:42] ********************************************************************************
[30.01.2026 08:42] Abstract 23. FROST is an attention-aware method that improves reasoning efficiency by pruning uncritical paths and removing reasoning outliers, leading to reduced token usage and improved accuracy.  					AI-generated summary 				 We propose FROST, an attention-aware method for efficient reasoning. Unlike traditi...
[30.01.2026 08:42] ********************************************************************************
[30.01.2026 08:42] Abstract 24. Neural audio fingerprinting performance varies with segment length, with short segments (0.5-second) generally providing better retrieval accuracy, and large language models showing promise in recommending optimal segment durations.  					AI-generated summary 				 Audio fingerprinting provides an id...
[30.01.2026 08:42] ********************************************************************************
[30.01.2026 08:42] Abstract 25. PRISM leverages design data to create a knowledge base for improving graphic designs based on natural language instructions, achieving superior style alignment compared to existing methods.  					AI-generated summary 				 Graphic design often involves exploring different stylistic directions, which ...
[30.01.2026 08:42] ********************************************************************************
[30.01.2026 08:42] Abstract 26. WebArbiter introduces a reasoning-first WebPRM that formulates reward modeling as text generation to improve web navigation through structured justifications and preference verdicts, outperforming existing baselines in complex web environments.  					AI-generated summary 				 Web agents hold great p...
[30.01.2026 08:42] ********************************************************************************
[30.01.2026 08:42] Abstract 27. Slot-Based Object-Centric Representations outperform global and dense feature representations in robotic manipulation tasks by providing better generalization under visual distribution shifts.  					AI-generated summary 				 The generalization capabilities of robotic manipulation policies are heavil...
[30.01.2026 08:42] ********************************************************************************
[30.01.2026 08:42] Abstract 28. WorldBench is introduced as a video-based benchmark for disentangled evaluation of physical reasoning in generative models, revealing specific failure patterns in current state-of-the-art video world models.  					AI-generated summary 				 Recent advances in generative foundational models, often ter...
[30.01.2026 08:42] ********************************************************************************
[30.01.2026 08:42] Abstract 29. STORM enhances robotic manipulation by adapting visual foundation models with semantic-aware slots through multi-phase training, improving generalization and control performance.  					AI-generated summary 				 Visual foundation models provide strong perceptual features for robotics, but their dense...
[30.01.2026 08:42] Read previous papers.
[30.01.2026 08:42] Generating reviews via LLM API.
[30.01.2026 08:42] Using data from previous issue: {"categories": ["#cv", "#benchmark", "#synthetic", "#reasoning", "#dataset", "#training", "#multimodal"], "emoji": "üß©", "ru": {"title": "–ü—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –≤ text-to-image –º–æ–¥–µ–ª—è—Ö —á–µ—Ä–µ–∑ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ SpatialGenEval –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å
[30.01.2026 08:42] Using data from previous issue: {"categories": ["#graphs", "#hallucinations", "#long_context", "#science"], "emoji": "üß¨", "ru": {"title": "–ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–µ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∑–Ω–∞–Ω–∏–π –¥–ª—è –Ω–∞–¥—ë–∂–Ω–æ–≥–æ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –Ω–∞—É—á–Ω–æ–≥–æ –æ—Ç–∫—Ä—ã—Ç–∏—è", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è Idea2Story ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –Ω–∞—É—á–Ω–æ–≥–æ –æ—Ç–∫—Ä—ã—Ç–∏—è, –∫–æ—Ç–æ—Ä—ã–π –ø–µ—Ä–µ–Ω–æ—Å–∏—Ç –æ–±—Ä–∞–±–æ
[30.01.2026 08:42] Using data from previous issue: {"categories": ["#data", "#cv", "#open_source", "#dataset", "#science", "#training", "#optimization", "#rl", "#multimodal"], "emoji": "üìÑ", "ru": {"title": "–£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è–º–∏", "desc": "OCRVerse –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π 
[30.01.2026 08:42] Using data from previous issue: {"categories": ["#multimodal", "#dataset", "#benchmark", "#robotics", "#inference", "#architecture", "#transfer_learning", "#small_models", "#synthetic"], "emoji": "ü§ñ", "ru": {"title": "–ë—ã—Å—Ç—Ä–∞—è –∏ –∞–¥–∞–ø—Ç–∏–≤–Ω–∞—è –º–∞–Ω–∏–ø—É–ª—è—Ü–∏—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–º–∏ –æ–±—ä–µ–∫—Ç–∞–º–∏ —á–µ—Ä–µ–∑ –∫–æ–º–ø–∞–∫—Ç–Ω—ã–µ vision-language –º–æ–¥–µ–ª–∏", "desc": "Dynami
[30.01.2026 08:42] Using data from previous issue: {"categories": ["#dataset", "#reasoning", "#multimodal", "#open_source", "#training", "#small_models", "#synthetic"], "emoji": "üß†", "ru": {"title": "–í—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –¥–µ–ª–∞—é—Ç –º–∞–ª–µ–Ω—å–∫–∏–µ –º–æ–¥–µ–ª–∏ —É–º–Ω–µ–µ –±–æ–ª—å—à–∏—Ö", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ MMFineReason ‚Äî –±–æ–ª—å—à–æ–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –¥–∞—Ç
[30.01.2026 08:42] Querying the API.
[30.01.2026 08:42] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

PLANING presents an efficient streaming reconstruction framework that combines explicit geometric primitives with neural Gaussians to achieve high-quality rendering and accurate geometry simultaneously through decoupled optimization.  					AI-generated summary 				 Streaming reconstruction from monocular image sequences remains challenging, as existing methods typically favor either high-quality rendering or accurate geometry, but rarely both. We present PLANING, an efficient on-the-fly reconstruction framework built on a hybrid representation that loosely couples explicit geometric primitives with neural Gaussians, enabling geometry and appearance to be modeled in a decoupled manner. This decoupling supports an online initialization and optimization strategy that separates geometry and appearance updates, yielding stable streaming reconstruction with substantially reduced structural redundancy. PLANING improves dense mesh Chamfer-L2 by 18.52% over PGSR, surpasses ARTDECO by 1.31 dB PSNR, and reconstructs ScanNetV2 scenes in under 100 seconds, over 5x faster than 2D Gaussian Splatting, while matching the quality of offline per-scene optimization. Beyond reconstruction quality, the structural clarity and computational efficiency of \modelname~make it well suited for a broad range of downstream applications, such as enabling large-scale scene modeling and simulation-ready environments for embodied AI. Project page: https://city-super.github.io/PLANING/ .
[30.01.2026 08:42] Response: ```json
{
  "desc": "PLANING ‚Äî —ç—Ç–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –ø–æ—Ç–æ–∫–æ–≤–æ–π —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ —Å—Ü–µ–Ω –∏–∑ –º–æ–Ω–æ–∫—É–ª—è—Ä–Ω—ã—Ö –≤–∏–¥–µ–æ–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π, –∫–æ—Ç–æ—Ä–∞—è –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç —è–≤–Ω—ã–µ –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–∏—Ç–∏–≤—ã —Å –Ω–µ–π—Ä–æ–Ω–Ω—ã–º–∏ –≥–∞—É—Å—Å–∏–∞–Ω–∞–º–∏. –ì–∏–±—Ä–∏–¥–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –ø–æ–∑–≤–æ–ª—è–µ—Ç –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≥–µ–æ–º–µ—Ç—Ä–∏—é –∏ –≤–Ω–µ—à–Ω–∏–π –≤–∏–¥, —á—Ç–æ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –≤—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–∞ –∏ —Ç–æ—á–Ω—É—é –≥–µ–æ–º–µ—Ç—Ä–∏—é. –°–∏—Å—Ç–µ–º–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–Ω–ª–∞–π–Ω-—Å—Ç—Ä–∞—Ç–µ–≥–∏—é –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏, –∫–æ—Ç–æ—Ä–∞—è —Ä–∞–∑–¥–µ–ª—è–µ—Ç –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –≥–µ–æ–º–µ—Ç—Ä–∏–∏ –∏ –≤–Ω–µ—à–Ω–µ–≥–æ –≤–∏–¥–∞, –¥–æ—Å—Ç–∏–≥–∞—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–π —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ —Å –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π –∏–∑–±—ã—Ç–æ—á–Ω–æ—Å—Ç—å—é. PLANING –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã –ø–æ –∫–∞—á–µ—Å—Ç–≤—É –∏ —Å–∫–æ—Ä–æ—Å—Ç–∏, —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∏—Ä—É—è —Å—Ü–µ–Ω—ã –≤ 5 —Ä–∞–∑ –±—ã—Å—Ç—Ä–µ–µ –∞–Ω–∞–ª–æ–≥–æ–≤, –ø—Ä–∏ —ç—Ç–æ–º –æ—Å—Ç–∞–≤–∞—è—Å—å –ø—Ä–∏–º–µ–Ω–∏–º–æ–π –¥–ª—è –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è —Å—Ü–µ–Ω.",
  "emoji": "üé¨",
  "title": "–†–∞–∑–¥–µ–ª—ë–Ω–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–µ–æ–º–µ—Ç—Ä–∏–∏ –∏ –≤–Ω–µ—à–Ω–µ–≥–æ –≤–∏–¥–∞ –¥–ª—è –±—ã—Å—Ç—Ä–æ–π —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ —Å—Ü–µ–Ω"
}
```
[30.01.2026 08:42] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"PLANING presents an efficient streaming reconstruction framework that combines explicit geometric primitives with neural Gaussians to achieve high-quality rendering and accurate geometry simultaneously through decoupled optimization.  					AI-generated summary 				 Streaming reconstruction from monocular image sequences remains challenging, as existing methods typically favor either high-quality rendering or accurate geometry, but rarely both. We present PLANING, an efficient on-the-fly reconstruction framework built on a hybrid representation that loosely couples explicit geometric primitives with neural Gaussians, enabling geometry and appearance to be modeled in a decoupled manner. This decoupling supports an online initialization and optimization strategy that separates geometry and appearance updates, yielding stable streaming reconstruction with substantially reduced structural redundancy. PLANING improves dense mesh Chamfer-L2 by 18.52% over PGSR, surpasses ARTDECO by 1.31 dB PSNR, and reconstructs ScanNetV2 scenes in under 100 seconds, over 5x faster than 2D Gaussian Splatting, while matching the quality of offline per-scene optimization. Beyond reconstruction quality, the structural clarity and computational efficiency of \modelname~make it well suited for a broad range of downstream applications, such as enabling large-scale scene modeling and simulation-ready environments for embodied AI. Project page: https://city-super.github.io/PLANING/ ."

[30.01.2026 08:42] Response: ```python
['3D', 'ROBOTICS']
```

**Justification:**
- **3D**: The paper focuses on 3D reconstruction from monocular image sequences, mesh generation, and 3D scene modeling with geometric primitives and neural representations.
- **ROBOTICS**: The paper explicitly mentions applications for "embodied AI" and creating "simulation-ready environments," which are key components of robotic systems and embodied AI applications.
[30.01.2026 08:42] Error. Failed to parse JSON from LLM. ["3D", "ROBOTICS"]


**Justification:**
- **3D**: The paper focuses on 3D reconstruction from monocular image sequences, mesh generation, and 3D scene modeling with geometric primitives and neural representations.
- **ROBOTICS**: The paper explicitly mentions applications for "embodied AI" and creating "simulation-ready environments," which are key components of robotic systems and embodied AI applications.
[30.01.2026 08:42] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"PLANING presents an efficient streaming reconstruction framework that combines explicit geometric primitives with neural Gaussians to achieve high-quality rendering and accurate geometry simultaneously through decoupled optimization.  					AI-generated summary 				 Streaming reconstruction from monocular image sequences remains challenging, as existing methods typically favor either high-quality rendering or accurate geometry, but rarely both. We present PLANING, an efficient on-the-fly reconstruction framework built on a hybrid representation that loosely couples explicit geometric primitives with neural Gaussians, enabling geometry and appearance to be modeled in a decoupled manner. This decoupling supports an online initialization and optimization strategy that separates geometry and appearance updates, yielding stable streaming reconstruction with substantially reduced structural redundancy. PLANING improves dense mesh Chamfer-L2 by 18.52% over PGSR, surpasses ARTDECO by 1.31 dB PSNR, and reconstructs ScanNetV2 scenes in under 100 seconds, over 5x faster than 2D Gaussian Splatting, while matching the quality of offline per-scene optimization. Beyond reconstruction quality, the structural clarity and computational efficiency of \modelname~make it well suited for a broad range of downstream applications, such as enabling large-scale scene modeling and simulation-ready environments for embodied AI. Project page: https://city-super.github.io/PLANING/ ."

[30.01.2026 08:42] Response: ```python
[]
```

The paper is about 3D scene reconstruction from monocular images using a hybrid representation combining geometric primitives and neural Gaussians. While it mentions applications for "embodied AI," the paper itself is fundamentally a computer vision/graphics paper focused on reconstruction and rendering techniques. It does not directly address any of the specified topics (AGI, GAMES, INTERPRETABILITY, REASONING, TRANSFER_LEARNING, GRAPHS, ETHICS, SECURITY, OPTIMIZATION, SURVEY, DIFFUSION, ALIGNMENT, STORY_GENERATION, HALLUCINATIONS, LONG_CONTEXT, SYNTHETIC, TRANSLATION, LEAKAGE, OPEN_SOURCE, SCIENCE, or LOW_RESOURCE).
[30.01.2026 08:42] Error. Failed to parse JSON from LLM. []


The paper is about 3D scene reconstruction from monocular images using a hybrid representation combining geometric primitives and neural Gaussians. While it mentions applications for "embodied AI," the paper itself is fundamentally a computer vision/graphics paper focused on reconstruction and rendering techniques. It does not directly address any of the specified topics (AGI, GAMES, INTERPRETABILITY, REASONING, TRANSFER_LEARNING, GRAPHS, ETHICS, SECURITY, OPTIMIZATION, SURVEY, DIFFUSION, ALIGNMENT, STORY_GENERATION, HALLUCINATIONS, LONG_CONTEXT, SYNTHETIC, TRANSLATION, LEAKAGE, OPEN_SOURCE, SCIENCE, or LOW_RESOURCE).
[30.01.2026 08:42] Response: ParsedChatCompletionMessage[Article](content='{"desc":"PLANING is a novel streaming reconstruction framework that effectively combines explicit geometric shapes with neural Gaussian representations. This approach allows for high-quality rendering and precise geometry to be achieved simultaneously through a process of decoupled optimization. By separating the updates for geometry and appearance, PLANING enhances the stability of streaming reconstruction while minimizing structural redundancy. The framework demonstrates significant improvements in reconstruction metrics and speed, making it suitable for various applications in scene modeling and AI simulations.","title":"Efficient Streaming Reconstruction with PLANING: Quality Meets Speed!"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='PLANING is a novel streaming reconstruction framework that effectively combines explicit geometric shapes with neural Gaussian representations. This approach allows for high-quality rendering and precise geometry to be achieved simultaneously through a process of decoupled optimization. By separating the updates for geometry and appearance, PLANING enhances the stability of streaming reconstruction while minimizing structural redundancy. The framework demonstrates significant improvements in reconstruction metrics and speed, making it suitable for various applications in scene modeling and AI simulations.', title='Efficient Streaming Reconstruction with PLANING: Quality Meets Speed!'))
[30.01.2026 08:42] Response: ParsedChatCompletionMessage[Article](content='{"desc":"PLANINGÊòØ‰∏ÄÁßçÈ´òÊïàÁöÑÊµÅÂºèÈáçÂª∫Ê°ÜÊû∂ÔºåÂÆÉÁªìÂêà‰∫ÜÊòæÂºèÂá†‰ΩïÂéüËØ≠ÂíåÁ•ûÁªèÈ´òÊñØÊ®°ÂûãÔºåËÉΩÂ§üÂêåÊó∂ÂÆûÁé∞È´òË¥®ÈáèÊ∏≤ÊüìÂíåÂáÜÁ°ÆÂá†‰ΩïÁöÑÈáçÂª∫„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáËß£ËÄ¶‰ºòÂåñÔºå‰ΩøÂá†‰ΩïÂíåÂ§ñËßÇÁöÑÂª∫Ê®°ËøáÁ®ãÁõ∏‰∫íÁã¨Á´ãÔºå‰ªéËÄåÊîØÊåÅÂú®Á∫øÂàùÂßãÂåñÂíå‰ºòÂåñÁ≠ñÁï•„ÄÇPLANINGÂú®ÈáçÂª∫Ë¥®Èáè‰∏äÊòæËëó‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåËÉΩÂ§üÂú®100ÁßíÂÜÖÈáçÂª∫ScanNetV2Âú∫ÊôØÔºåÈÄüÂ∫¶ÊØî2DÈ´òÊñØÁÇπ‰∫ëÂø´5ÂÄç‰ª•‰∏ä„ÄÇÂÖ∂ÁªìÊûÑÊ∏ÖÊô∞ÂíåËÆ°ÁÆóÊïàÁéá‰ΩøÂÖ∂ÈÄÇÁî®‰∫éÂ§ßËßÑÊ®°Âú∫ÊôØÂª∫Ê®°ÂíåÈÄÇÂêàÊ®°ÊãüÁöÑÁéØÂ¢ÉÔºå‰∏∫ÂµåÂÖ•Âºè‰∫∫Â∑•Êô∫ËÉΩÊèê‰æõ‰∫ÜËâØÂ•ΩÁöÑÂü∫Á°Ä„ÄÇ","title":"È´òÊïàÊµÅÂºèÈáçÂª∫ÔºåËß£ËÄ¶‰ºòÂåñÊñ∞ÊñπÊ≥ï"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='PLANINGÊòØ‰∏ÄÁßçÈ´òÊïàÁöÑÊµÅÂºèÈáçÂª∫Ê°ÜÊû∂ÔºåÂÆÉÁªìÂêà‰∫ÜÊòæÂºèÂá†‰ΩïÂéüËØ≠ÂíåÁ•ûÁªèÈ´òÊñØÊ®°ÂûãÔºåËÉΩÂ§üÂêåÊó∂ÂÆûÁé∞È´òË¥®ÈáèÊ∏≤ÊüìÂíåÂáÜÁ°ÆÂá†‰ΩïÁöÑÈáçÂª∫„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáËß£ËÄ¶‰ºòÂåñÔºå‰ΩøÂá†‰ΩïÂíåÂ§ñËßÇÁöÑÂª∫Ê®°ËøáÁ®ãÁõ∏‰∫íÁã¨Á´ãÔºå‰ªéËÄåÊîØÊåÅÂú®Á∫øÂàùÂßãÂåñÂíå‰ºòÂåñÁ≠ñÁï•„ÄÇPLANINGÂú®ÈáçÂª∫Ë¥®Èáè‰∏äÊòæËëó‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåËÉΩÂ§üÂú®100ÁßíÂÜÖÈáçÂª∫ScanNetV2Âú∫ÊôØÔºåÈÄüÂ∫¶ÊØî2DÈ´òÊñØÁÇπ‰∫ëÂø´5ÂÄç‰ª•‰∏ä„ÄÇÂÖ∂ÁªìÊûÑÊ∏ÖÊô∞ÂíåËÆ°ÁÆóÊïàÁéá‰ΩøÂÖ∂ÈÄÇÁî®‰∫éÂ§ßËßÑÊ®°Âú∫ÊôØÂª∫Ê®°ÂíåÈÄÇÂêàÊ®°ÊãüÁöÑÁéØÂ¢ÉÔºå‰∏∫ÂµåÂÖ•Âºè‰∫∫Â∑•Êô∫ËÉΩÊèê‰æõ‰∫ÜËâØÂ•ΩÁöÑÂü∫Á°Ä„ÄÇ', title='È´òÊïàÊµÅÂºèÈáçÂª∫ÔºåËß£ËÄ¶‰ºòÂåñÊñ∞ÊñπÊ≥ï'))
[30.01.2026 08:42] Using data from previous issue: {"categories": ["#inference", "#multimodal", "#architecture", "#training"], "emoji": "‚ö°", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–æ–∫–µ–Ω–æ–≤ —á–µ—Ä–µ–∑ –∫–æ–Ω—Ü–µ–ø—Ç—É–∞–ª—å–Ω–æ–µ —Å–ª–∏—è–Ω–∏–µ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π", "desc": "ConceptMoE ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö, –∫–æ
[30.01.2026 08:42] Using data from previous issue: {"categories": ["#dataset", "#reasoning", "#benchmark", "#open_source", "#rl", "#optimization", "#agents"], "emoji": "üéØ", "ru": {"title": "–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ–±—Ä–∞—Ç–Ω–∞—è —Å–≤—è–∑—å –¥–ª—è —É–º–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Agent-RRM, –º–Ω–æ–≥–æ–∞—Å–ø–µ–∫—Ç–Ω—É—é –º–æ–¥–µ–ª—å –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä–∞—è –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å
[30.01.2026 08:42] Using data from previous issue: {"categories": ["#reasoning", "#training", "#rl", "#optimization", "#transfer_learning", "#agents", "#small_models"], "emoji": "üîç", "ru": {"title": "–†–∞–∑–≤–µ–¥—á–∏–∫–∏ –¥–ª—è —Ä–∞–∑—É–º–Ω–æ–π —Ä–∞–∑–≤–µ–¥–∫–∏: –∫–∞–∫ –º–∞–ª–µ–Ω—å–∫–∏–µ –º–æ–¥–µ–ª–∏ —É—á–∞—Ç –±–æ–ª—å—à–∏—Ö", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ SCOUT, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ª—ë–≥–∫–∏—Ö –∞–≥–µ–Ω—Ç–æ–≤-
[30.01.2026 08:42] Using data from previous issue: {"categories": ["#benchmark", "#multilingual", "#inference", "#low_resource", "#audio", "#open_source"], "emoji": "üé§", "ru": {"title": "–ú–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏ —Å –Ω–µ–∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–º –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ–º", "desc": "–°–µ–º—å—è –º–æ–¥–µ–ª–µ–π Qwen3-ASR –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –Ω–∞–±–æ—Ä —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ—á
[30.01.2026 08:42] Using data from previous issue: {"categories": ["#architecture", "#inference", "#training", "#optimization", "#open_source"], "emoji": "‚ö°", "ru": {"title": "–≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –≤–º–µ—Å—Ç–æ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤: –Ω–æ–≤—ã–π –ø—É—Ç—å –∫ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–º—É –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—é", "desc": "–í —ç—Ç–æ–π —Ä–∞–±–æ—Ç–µ –∞–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∫–∞–∫ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—É —ç–∫—Å
[30.01.2026 08:42] Using data from previous issue: {"categories": ["#rlhf", "#reasoning", "#open_source", "#training", "#multilingual", "#optimization", "#low_resource", "#small_models"], "emoji": "üèõÔ∏è", "ru": {"title": "–°—É–≤–µ—Ä–µ–Ω–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –¥–æ–æ–±—É—á–µ–Ω–∏—é —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π,
[30.01.2026 08:42] Using data from previous issue: {"categories": ["#multimodal", "#video", "#audio", "#inference"], "emoji": "üéØ", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –ø–æ–¥–∞–≤–ª–µ–Ω–∏—è –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–í —Å—Ç–∞—Ç—å–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ –∫—Ä–æ—Å—Å-–º–æ–¥–∞–ª—å–Ω—ã—Ö –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö, –∫–æ–≥–¥–∞ 
[30.01.2026 08:42] Using data from previous issue: {"categories": ["#multimodal", "#reasoning", "#benchmark", "#open_source", "#training", "#long_context", "#inference"], "emoji": "üß†", "ru": {"title": "–û–ø—Ç–∏—á–µ—Å–∫–∞—è –ø–∞–º—è—Ç—å –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –≤—ã–≤–æ–¥–∞: —Å–∂–∞—Ç–∏–µ —á–µ—Ä–µ–∑ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é", "desc": "VTC-R1 ‚Äî –Ω–æ–≤–∞—è –ø–∞—Ä–∞–¥–∏–≥–º–∞ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä–∞
[30.01.2026 08:42] Using data from previous issue: {"categories": ["#dataset", "#agents", "#benchmark"], "emoji": "üîç", "ru": {"title": "–ì–ª—É–±–æ–∫–∏–π –ø–æ–∏—Å–∫ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏: –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –º–Ω–æ–≥–æ—à–∞–≥–æ–≤–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤", "desc": "–†–∞–±–æ—Ç–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –±–µ–Ω—á–º–∞—Ä–∫ DeepSearchQA —Å 900 –∑–∞–¥–∞—á–∞–º–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ —Å–ª–æ–∂–Ω—ã—Ö –º–Ω–æ–≥–æ—à–∞–≥–æ–≤—ã—Ö –∑–∞–¥–∞—á–∞—Ö –ø–æ–∏—Å–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏. –ë–µ–Ω
[30.01.2026 08:42] Using data from previous issue: {"categories": ["#3d", "#cv", "#architecture", "#transfer_learning", "#open_source", "#dataset", "#optimization", "#multimodal"], "emoji": "üìè", "ru": {"title": "–ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞–∫ –ø—É—Ç—å –∫ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–º—É –≤–æ—Å–ø—Ä–∏—è—Ç–∏—é –º–µ—Ç—Ä–∏—á–µ—Å–∫–æ–π –≥–ª—É–±–∏–Ω—ã", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Metric Anything ‚Äî –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—É—é —Å–∏
[30.01.2026 08:42] Using data from previous issue: {"categories": ["#reasoning", "#training", "#rl", "#optimization", "#small_models"], "emoji": "üß†", "ru": {"title": "–ê–∫—Ç–∏–≤–Ω–æ–µ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ —Å–∫—Ä—ã—Ç–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –º–µ—Ç–æ–¥ –∞–∫—Ç–∏–≤–Ω–æ–≥–æ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –≤ —Å–∫—Ä—ã—Ç–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è —É–ª—É
[30.01.2026 08:42] Using data from previous issue: {"categories": ["#multimodal", "#cv", "#training"], "emoji": "üîÑ", "ru": {"title": "–í–∑–∞–∏–º–Ω–æ–µ —É—Å–∏–ª–µ–Ω–∏–µ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è", "desc": "UniMRG ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥ –ø–æ—Å—Ç–æ–±—É—á–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –ø—É—Ç—ë–º –æ–±—É—á–µ–Ω–∏—è –∏—Ö –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤—Å
[30.01.2026 08:42] Using data from previous issue: {"categories": ["#rl", "#training", "#rlhf"], "emoji": "üõ°Ô∏è", "ru": {"title": "–ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –∏ —Ñ–∞–∫–æ—Ç–∏—á–Ω–æ—Å—Ç—å –≤–æ –≤—Ä–µ–º—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —á–µ—Ä–µ–∑ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –æ–±—É—á–µ–Ω–∏–∏ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –∫–æ—Ç
[30.01.2026 08:42] Using data from previous issue: {"categories": ["#rlhf", "#dataset", "#reasoning", "#benchmark", "#open_source", "#training", "#science", "#small_models"], "emoji": "üîê", "ru": {"title": "–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ –≤ –∫–æ–º–ø–∞–∫—Ç–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ: —Ä–∞—Å—Å—É–∂–¥–∞—é—â–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –∫–∏–±–µ—Ä–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ Foundation-Sec-8B-Reasoning ‚Äî
[30.01.2026 08:42] Querying the API.
[30.01.2026 08:42] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Pixel MeanFlow introduces a one-step latent-free image generation method by separating network output space from loss space, achieving strong performance on ImageNet at multiple resolutions.  					AI-generated summary 				 Modern diffusion/flow-based models for image generation typically exhibit two core characteristics: (i) using multi-step sampling, and (ii) operating in a latent space. Recent advances have made encouraging progress on each aspect individually, paving the way toward one-step diffusion/flow without latents. In this work, we take a further step towards this goal and propose "pixel MeanFlow" (pMF). Our core guideline is to formulate the network output space and the loss space separately. The network target is designed to be on a presumed low-dimensional image manifold (i.e., x-prediction), while the loss is defined via MeanFlow in the velocity space. We introduce a simple transformation between the image manifold and the average velocity field. In experiments, pMF achieves strong results for one-step latent-free generation on ImageNet at 256x256 resolution (2.22 FID) and 512x512 resolution (2.48 FID), filling a key missing piece in this regime. We hope that our study will further advance the boundaries of diffusion/flow-based generative models.
[30.01.2026 08:42] Response: ```json
{
  "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –º–µ—Ç–æ–¥ Pixel MeanFlow –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∑–∞ –æ–¥–∏–Ω —à–∞–≥ –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Å–∫—Ä—ã—Ç–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞. –ö–ª—é—á–µ–≤–∞—è –∏–¥–µ—è –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –≤—ã—Ö–æ–¥–æ–≤ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ –∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å: —Å–µ—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–∞ –Ω–∏–∑–∫–æ—Ä–∞–∑–º–µ—Ä–Ω–æ–º –º–Ω–æ–≥–æ–æ–±—Ä–∞–∑–∏–∏, –∞ –ø–æ—Ç–µ—Ä–∏ –≤—ã—á–∏—Å–ª—è—é—Ç—Å—è –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –ø–æ–ª–µ–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º MeanFlow. –ê–≤—Ç–æ—Ä—ã –≤–≤–æ–¥—è—Ç –ø—Ä–æ—Å—Ç–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –º–µ–∂–¥—É –º–Ω–æ–≥–æ–æ–±—Ä–∞–∑–∏–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –ø–æ–ª–µ–º —Å—Ä–µ–¥–Ω–∏—Ö —Å–∫–æ—Ä–æ—Å—Ç–µ–π –¥–ª—è —Å–≤—è–∑–∏ —ç—Ç–∏—Ö –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤. –ú–µ—Ç–æ–¥ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —Å–∏–ª—å–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ ImageNet: 2.22 FID –¥–ª—è —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è 256x256 –∏ 2.48 FID –¥–ª—è 512x512.",
  "emoji": "‚ö°",
  "title": "–û–¥–Ω–æ—à–∞–≥–æ–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –±–µ–∑ —Å–∫—Ä—ã—Ç–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ —á–µ—Ä–µ–∑ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –≤—ã—Ö–æ–¥–æ–≤ –∏ –ø–æ—Ç–µ—Ä—å"
}
```
[30.01.2026 08:42] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Pixel MeanFlow introduces a one-step latent-free image generation method by separating network output space from loss space, achieving strong performance on ImageNet at multiple resolutions.  					AI-generated summary 				 Modern diffusion/flow-based models for image generation typically exhibit two core characteristics: (i) using multi-step sampling, and (ii) operating in a latent space. Recent advances have made encouraging progress on each aspect individually, paving the way toward one-step diffusion/flow without latents. In this work, we take a further step towards this goal and propose "pixel MeanFlow" (pMF). Our core guideline is to formulate the network output space and the loss space separately. The network target is designed to be on a presumed low-dimensional image manifold (i.e., x-prediction), while the loss is defined via MeanFlow in the velocity space. We introduce a simple transformation between the image manifold and the average velocity field. In experiments, pMF achieves strong results for one-step latent-free generation on ImageNet at 256x256 resolution (2.22 FID) and 512x512 resolution (2.48 FID), filling a key missing piece in this regime. We hope that our study will further advance the boundaries of diffusion/flow-based generative models."

[30.01.2026 08:42] Response: ```python
['CV', 'ARCHITECTURE', 'TRAINING']
```
[30.01.2026 08:42] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Pixel MeanFlow introduces a one-step latent-free image generation method by separating network output space from loss space, achieving strong performance on ImageNet at multiple resolutions.  					AI-generated summary 				 Modern diffusion/flow-based models for image generation typically exhibit two core characteristics: (i) using multi-step sampling, and (ii) operating in a latent space. Recent advances have made encouraging progress on each aspect individually, paving the way toward one-step diffusion/flow without latents. In this work, we take a further step towards this goal and propose "pixel MeanFlow" (pMF). Our core guideline is to formulate the network output space and the loss space separately. The network target is designed to be on a presumed low-dimensional image manifold (i.e., x-prediction), while the loss is defined via MeanFlow in the velocity space. We introduce a simple transformation between the image manifold and the average velocity field. In experiments, pMF achieves strong results for one-step latent-free generation on ImageNet at 256x256 resolution (2.22 FID) and 512x512 resolution (2.48 FID), filling a key missing piece in this regime. We hope that our study will further advance the boundaries of diffusion/flow-based generative models."

[30.01.2026 08:42] Response: ```python
["DIFFUSION", "OPTIMIZATION"]
```
[30.01.2026 08:42] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Pixel MeanFlow (pMF) presents a novel approach to image generation that eliminates the need for latent spaces and multi-step sampling. By separating the network output space from the loss space, pMF effectively targets a low-dimensional image manifold while utilizing MeanFlow in the velocity space for loss calculation. This method allows for one-step generation of images, achieving impressive results on ImageNet at various resolutions. The findings suggest that pMF could significantly enhance the capabilities of diffusion and flow-based generative models.","title":"One-Step Image Generation Without Latents!"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Pixel MeanFlow (pMF) presents a novel approach to image generation that eliminates the need for latent spaces and multi-step sampling. By separating the network output space from the loss space, pMF effectively targets a low-dimensional image manifold while utilizing MeanFlow in the velocity space for loss calculation. This method allows for one-step generation of images, achieving impressive results on ImageNet at various resolutions. The findings suggest that pMF could significantly enhance the capabilities of diffusion and flow-based generative models.', title='One-Step Image Generation Without Latents!'))
[30.01.2026 08:42] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Pixel MeanFlowÔºàpMFÔºâÊèêÂá∫‰∫Ü‰∏ÄÁßç‰∏ÄÊ≠•ÁîüÊàêÂõæÂÉèÁöÑÊñπÊ≥ïÔºåÈÅøÂÖç‰∫Ü‰ΩøÁî®ÊΩúÂú®Á©∫Èó¥„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÂ∞ÜÁΩëÁªúËæìÂá∫Á©∫Èó¥‰∏éÊçüÂ§±Á©∫Èó¥ÂàÜÂºÄÊù•ÂÆûÁé∞ÔºåÁΩëÁªúÁõÆÊ†á‰Ωç‰∫éÂÅáÂÆöÁöÑ‰ΩéÁª¥ÂõæÂÉèÊµÅÂΩ¢‰∏äÔºåËÄåÊçüÂ§±ÂàôÈÄöËøáÈÄüÂ∫¶Á©∫Èó¥‰∏≠ÁöÑMeanFlowÂÆö‰πâ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåpMFÂú®256x256Âíå512x512ÂàÜËæ®Áéá‰∏ãÁöÑImageNet‰∏äË°®Áé∞Âá∫Ëâ≤ÔºåÂàÜÂà´ËææÂà∞‰∫Ü2.22Âíå2.48ÁöÑFIDÂÄº„ÄÇËøôÈ°πÁ†îÁ©∂‰∏∫Êó†ÊΩúÂú®Á©∫Èó¥ÁöÑÊâ©Êï£/ÊµÅÁîüÊàêÊ®°ÂûãÁöÑËøõ‰∏ÄÊ≠•ÂèëÂ±ïÂ•†ÂÆö‰∫ÜÂü∫Á°Ä„ÄÇ","title":"‰∏ÄÊ≠•ÁîüÊàêÔºåÊó†ÈúÄÊΩúÂú®Á©∫Èó¥ÁöÑÂõæÂÉèÁîüÊàê"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Pixel MeanFlowÔºàpMFÔºâÊèêÂá∫‰∫Ü‰∏ÄÁßç‰∏ÄÊ≠•ÁîüÊàêÂõæÂÉèÁöÑÊñπÊ≥ïÔºåÈÅøÂÖç‰∫Ü‰ΩøÁî®ÊΩúÂú®Á©∫Èó¥„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÂ∞ÜÁΩëÁªúËæìÂá∫Á©∫Èó¥‰∏éÊçüÂ§±Á©∫Èó¥ÂàÜÂºÄÊù•ÂÆûÁé∞ÔºåÁΩëÁªúÁõÆÊ†á‰Ωç‰∫éÂÅáÂÆöÁöÑ‰ΩéÁª¥ÂõæÂÉèÊµÅÂΩ¢‰∏äÔºåËÄåÊçüÂ§±ÂàôÈÄöËøáÈÄüÂ∫¶Á©∫Èó¥‰∏≠ÁöÑMeanFlowÂÆö‰πâ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåpMFÂú®256x256Âíå512x512ÂàÜËæ®Áéá‰∏ãÁöÑImageNet‰∏äË°®Áé∞Âá∫Ëâ≤ÔºåÂàÜÂà´ËææÂà∞‰∫Ü2.22Âíå2.48ÁöÑFIDÂÄº„ÄÇËøôÈ°πÁ†îÁ©∂‰∏∫Êó†ÊΩúÂú®Á©∫Èó¥ÁöÑÊâ©Êï£/ÊµÅÁîüÊàêÊ®°ÂûãÁöÑËøõ‰∏ÄÊ≠•ÂèëÂ±ïÂ•†ÂÆö‰∫ÜÂü∫Á°Ä„ÄÇ', title='‰∏ÄÊ≠•ÁîüÊàêÔºåÊó†ÈúÄÊΩúÂú®Á©∫Èó¥ÁöÑÂõæÂÉèÁîüÊàê'))
[30.01.2026 08:42] Querying the API.
[30.01.2026 08:42] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

HALO enables efficient conversion of Transformer models to RNN-attention hybrid architectures with improved long-context performance using minimal training data.  					AI-generated summary 				 Hybrid Transformer architectures, which combine softmax attention blocks and recurrent neural networks (RNNs), have shown a desirable performance-throughput tradeoff for long-context modeling, but their adoption and studies are hindered by the prohibitive cost of large-scale pre-training from scratch. Some recent studies have shown that pre-trained softmax attention blocks can be converted into RNN blocks through parameter transfer and knowledge distillation. However, these transfer methods require substantial amounts of training data (more than 10B tokens), and the resulting hybrid models also exhibit poor long-context performance, which is the scenario where hybrid models enjoy significant inference speedups over Transformer-based models. In this paper, we present HALO (Hybrid Attention via Layer Optimization), a pipeline for distilling Transformer models into RNN-attention hybrid models. We then present HypeNet, a hybrid architecture with superior length generalization enabled by a novel position encoding scheme (named HyPE) and various architectural modifications. We convert the Qwen3 series into HypeNet using HALO, achieving performance comparable to the original Transformer models while enjoying superior long-context performance and efficiency. The conversion requires just 2.3B tokens, less than 0.01% of their pre-training data
[30.01.2026 08:43] Response: ```json
{
  "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω HALO ‚Äî –º–µ—Ç–æ–¥ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä-–º–æ–¥–µ–ª–µ–π –≤ –≥–∏–±—Ä–∏–¥–Ω—ã–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∏–µ –±–ª–æ–∫–∏ –≤–Ω–∏–º–∞–Ω–∏—è –∏ —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω—ã–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ HypeNet ‚Äî –Ω–æ–≤—É—é –≥–∏–±—Ä–∏–¥–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞–ª–∏–∑–∞—Ü–∏–µ–π –Ω–∞ –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è—Ö –±–ª–∞–≥–æ–¥–∞—Ä—è –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω–æ–π —Å—Ö–µ–º–µ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è HyPE. –ö–ª—é—á–µ–≤–æ–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ –ø–æ–¥—Ö–æ–¥–∞ –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —Ç–æ–º, —á—Ç–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç—Ä–µ–±—É–µ—Ç –≤—Å–µ–≥–æ 2.3 –º–ª—Ä–¥ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è, —á—Ç–æ —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ–Ω–µ–µ 0.01% –æ—Ç –æ–±—ä—ë–º–∞ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏—Å—Ö–æ–¥–Ω–æ–π –º–æ–¥–µ–ª–∏. –ì–∏–±—Ä–∏–¥–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å, —Å–æ–ø–æ—Å—Ç–∞–≤–∏–º—É—é —Å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–º–∏ –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞–º–∏, –Ω–æ –ø—Ä–∏ —ç—Ç–æ–º –æ–±–µ—Å–ø–µ—á–∏–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ª—É—á—à—É—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∏ —Å–∫–æ—Ä–æ—Å—Ç—å –≤—ã–≤–æ–¥–∞ –Ω–∞ –¥–ª–∏–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞—Ö.",
  "emoji": "‚ö°",
  "title": "–ì–∏–±—Ä–∏–¥–Ω—ã–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã: —É—Å–∫–æ—Ä–µ–Ω–∏–µ –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ —á–µ—Ä–µ–∑ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—é –≤ RNN —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏"
}
```
[30.01.2026 08:43] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"HALO enables efficient conversion of Transformer models to RNN-attention hybrid architectures with improved long-context performance using minimal training data.  					AI-generated summary 				 Hybrid Transformer architectures, which combine softmax attention blocks and recurrent neural networks (RNNs), have shown a desirable performance-throughput tradeoff for long-context modeling, but their adoption and studies are hindered by the prohibitive cost of large-scale pre-training from scratch. Some recent studies have shown that pre-trained softmax attention blocks can be converted into RNN blocks through parameter transfer and knowledge distillation. However, these transfer methods require substantial amounts of training data (more than 10B tokens), and the resulting hybrid models also exhibit poor long-context performance, which is the scenario where hybrid models enjoy significant inference speedups over Transformer-based models. In this paper, we present HALO (Hybrid Attention via Layer Optimization), a pipeline for distilling Transformer models into RNN-attention hybrid models. We then present HypeNet, a hybrid architecture with superior length generalization enabled by a novel position encoding scheme (named HyPE) and various architectural modifications. We convert the Qwen3 series into HypeNet using HALO, achieving performance comparable to the original Transformer models while enjoying superior long-context performance and efficiency. The conversion requires just 2.3B tokens, less than 0.01% of their pre-training data"

[30.01.2026 08:43] Response: ```python
["ARCHITECTURE", "TRAINING", "INFERENCE"]
```
[30.01.2026 08:43] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"HALO enables efficient conversion of Transformer models to RNN-attention hybrid architectures with improved long-context performance using minimal training data.  					AI-generated summary 				 Hybrid Transformer architectures, which combine softmax attention blocks and recurrent neural networks (RNNs), have shown a desirable performance-throughput tradeoff for long-context modeling, but their adoption and studies are hindered by the prohibitive cost of large-scale pre-training from scratch. Some recent studies have shown that pre-trained softmax attention blocks can be converted into RNN blocks through parameter transfer and knowledge distillation. However, these transfer methods require substantial amounts of training data (more than 10B tokens), and the resulting hybrid models also exhibit poor long-context performance, which is the scenario where hybrid models enjoy significant inference speedups over Transformer-based models. In this paper, we present HALO (Hybrid Attention via Layer Optimization), a pipeline for distilling Transformer models into RNN-attention hybrid models. We then present HypeNet, a hybrid architecture with superior length generalization enabled by a novel position encoding scheme (named HyPE) and various architectural modifications. We convert the Qwen3 series into HypeNet using HALO, achieving performance comparable to the original Transformer models while enjoying superior long-context performance and efficiency. The conversion requires just 2.3B tokens, less than 0.01% of their pre-training data"

[30.01.2026 08:43] Response: ```python
["LONG_CONTEXT", "TRANSFER_LEARNING", "OPTIMIZATION"]
```
[30.01.2026 08:43] Response: ParsedChatCompletionMessage[Article](content='{"desc":"HALO is a method that helps change Transformer models into hybrid architectures that use both RNNs and attention mechanisms, making them better at handling long sequences of data. This approach reduces the need for extensive training data, requiring only 2.3 billion tokens instead of the usual 10 billion. The new hybrid model, called HypeNet, benefits from a unique position encoding method that enhances its ability to generalize over longer contexts. As a result, HypeNet performs similarly to the original Transformer models while being more efficient and faster for long-context tasks.","title":"HALO: Transforming Transformers for Efficient Long-Context Learning"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='HALO is a method that helps change Transformer models into hybrid architectures that use both RNNs and attention mechanisms, making them better at handling long sequences of data. This approach reduces the need for extensive training data, requiring only 2.3 billion tokens instead of the usual 10 billion. The new hybrid model, called HypeNet, benefits from a unique position encoding method that enhances its ability to generalize over longer contexts. As a result, HypeNet performs similarly to the original Transformer models while being more efficient and faster for long-context tasks.', title='HALO: Transforming Transformers for Efficient Long-Context Learning'))
[30.01.2026 08:43] Response: ParsedChatCompletionMessage[Article](content='{"desc":"HALOÊòØ‰∏ÄÁßçÂ∞ÜTransformerÊ®°ÂûãÈ´òÊïàËΩ¨Êç¢‰∏∫RNN-Ê≥®ÊÑèÂäõÊ∑∑ÂêàÊû∂ÊûÑÁöÑÊñπÊ≥ïÔºåËÉΩÂ§üÂú®‰ΩøÁî®ÊûÅÂ∞ëËÆ≠ÁªÉÊï∞ÊçÆÁöÑÊÉÖÂÜµ‰∏ãÊèêÈ´òÈïø‰∏ä‰∏ãÊñáÊÄßËÉΩ„ÄÇ‰º†ÁªüÁöÑÊ∑∑ÂêàÊû∂ÊûÑÂú®Èïø‰∏ä‰∏ãÊñáÂª∫Ê®°‰∏≠Ë°®Áé∞ËâØÂ•ΩÔºå‰ΩÜÁî±‰∫éÂ§ßËßÑÊ®°È¢ÑËÆ≠ÁªÉÁöÑÈ´òÊàêÊú¨ÔºåÂØºËá¥ÂÖ∂Â∫îÁî®ÂèóÂà∞ÈôêÂà∂„ÄÇHALOÈÄöËøáÂèÇÊï∞ËΩ¨ÁßªÂíåÁü•ËØÜËí∏È¶èÔºåËÉΩÂ§ü‰ª•‰ªÖÈúÄ2.3BÊ†áËÆ∞ÁöÑÊï∞ÊçÆÂ∞ÜÈ¢ÑËÆ≠ÁªÉÁöÑsoftmaxÊ≥®ÊÑèÂäõÂùóËΩ¨Êç¢‰∏∫RNNÂùóÔºå‰ªéËÄåÂÆûÁé∞È´òÊïàÁöÑÊ®°ÂûãËΩ¨Êç¢„ÄÇÊúÄÁªàÔºåHypeNetÊû∂ÊûÑÂú®Èïø‰∏ä‰∏ãÊñáÊÄßËÉΩÂíåÊïàÁéá‰∏äË∂ÖË∂ä‰∫ÜÂéüÂßãÁöÑTransformerÊ®°Âûã„ÄÇ","title":"HALOÔºöÈ´òÊïàËΩ¨Êç¢TransformerÊ®°ÂûãÁöÑÂà©Âô®"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='HALOÊòØ‰∏ÄÁßçÂ∞ÜTransformerÊ®°ÂûãÈ´òÊïàËΩ¨Êç¢‰∏∫RNN-Ê≥®ÊÑèÂäõÊ∑∑ÂêàÊû∂ÊûÑÁöÑÊñπÊ≥ïÔºåËÉΩÂ§üÂú®‰ΩøÁî®ÊûÅÂ∞ëËÆ≠ÁªÉÊï∞ÊçÆÁöÑÊÉÖÂÜµ‰∏ãÊèêÈ´òÈïø‰∏ä‰∏ãÊñáÊÄßËÉΩ„ÄÇ‰º†ÁªüÁöÑÊ∑∑ÂêàÊû∂ÊûÑÂú®Èïø‰∏ä‰∏ãÊñáÂª∫Ê®°‰∏≠Ë°®Áé∞ËâØÂ•ΩÔºå‰ΩÜÁî±‰∫éÂ§ßËßÑÊ®°È¢ÑËÆ≠ÁªÉÁöÑÈ´òÊàêÊú¨ÔºåÂØºËá¥ÂÖ∂Â∫îÁî®ÂèóÂà∞ÈôêÂà∂„ÄÇHALOÈÄöËøáÂèÇÊï∞ËΩ¨ÁßªÂíåÁü•ËØÜËí∏È¶èÔºåËÉΩÂ§ü‰ª•‰ªÖÈúÄ2.3BÊ†áËÆ∞ÁöÑÊï∞ÊçÆÂ∞ÜÈ¢ÑËÆ≠ÁªÉÁöÑsoftmaxÊ≥®ÊÑèÂäõÂùóËΩ¨Êç¢‰∏∫RNNÂùóÔºå‰ªéËÄåÂÆûÁé∞È´òÊïàÁöÑÊ®°ÂûãËΩ¨Êç¢„ÄÇÊúÄÁªàÔºåHypeNetÊû∂ÊûÑÂú®Èïø‰∏ä‰∏ãÊñáÊÄßËÉΩÂíåÊïàÁéá‰∏äË∂ÖË∂ä‰∫ÜÂéüÂßãÁöÑTransformerÊ®°Âûã„ÄÇ', title='HALOÔºöÈ´òÊïàËΩ¨Êç¢TransformerÊ®°ÂûãÁöÑÂà©Âô®'))
[30.01.2026 08:43] Using data from previous issue: {"categories": ["#architecture", "#benchmark", "#reasoning", "#long_context", "#agents"], "emoji": "üß†", "ru": {"title": "–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –ø–∞–º—è—Ç—å –∫–∞–∫ –∫–ª—é—á –∫ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –ø–∞–º—è—Ç–∏ BMAM, –≤–¥–æ—Ö–Ω–æ–≤–ª–µ–Ω–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π –º–æ–∑–≥–∞, –∫–æ—Ç–æ—Ä–∞—è —Ä–∞–∑–¥–µ–ª—è–µ—Ç –ø–∞–º
[30.01.2026 08:43] Querying the API.
[30.01.2026 08:43] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

FROST is an attention-aware method that improves reasoning efficiency by pruning uncritical paths and removing reasoning outliers, leading to reduced token usage and improved accuracy.  					AI-generated summary 				 We propose FROST, an attention-aware method for efficient reasoning. Unlike traditional approaches, FROST leverages attention weights to prune uncritical reasoning paths, yielding shorter and more reliable reasoning trajectories. Methodologically, we introduce the concept of reasoning outliers and design an attention-based mechanism to remove them. Theoretically, FROST preserves and enhances the model's reasoning capacity while eliminating outliers at the sentence level. Empirically, we validate FROST on four benchmarks using two strong reasoning models (Phi-4-Reasoning and GPT-OSS-20B), outperforming state-of-the-art methods such as TALE and ThinkLess. Notably, FROST achieves an average 69.68% reduction in token usage and a 26.70% improvement in accuracy over the base model. Furthermore, in evaluations of attention outlier metrics, FROST reduces the maximum infinity norm by 15.97% and the average kurtosis by 91.09% compared to the base model. Code is available at https://github.com/robinzixuan/FROST
[30.01.2026 08:43] Response: ```json
{
  "desc": "FROST ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –∞–Ω–∞–ª–∏–∑–µ –≤–µ—Å–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è (attention weights) –≤ –Ω–µ–π—Ä–æ—Å–µ—Ç—è—Ö. –ü–æ–¥—Ö–æ–¥ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —É–¥–∞–ª—è–µ—Ç –Ω–µ–∫—Ä–∏—Ç–∏—á–Ω—ã–µ –ø—É—Ç–∏ –≤ —Ü–µ–ø–æ—á–∫–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∏ –≤—ã—è–≤–ª—è–µ—Ç –∞–Ω–æ–º–∞–ª—å–Ω—ã–µ –≤—ã—Ö–æ–¥—ã –Ω–∞ —É—Ä–æ–≤–Ω–µ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–æ–∫—Ä–∞—Ç–∏—Ç—å –¥–ª–∏–Ω—É –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º–æ–≥–æ —Ç–µ–∫—Å—Ç–∞. –ú–µ—Ç–æ–¥ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω –Ω–∞ –¥–≤—É—Ö –º–æ—â–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å–Ω–∏–∂–µ–Ω–∏–µ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤ –Ω–∞ 69.68% –ø—Ä–∏ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–º —É–ª—É—á—à–µ–Ω–∏–∏ —Ç–æ—á–Ω–æ—Å—Ç–∏ –Ω–∞ 26.70%. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç, —á—Ç–æ FROST —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏ –¥–∞–∂–µ —É–ª—É—á—à–∞–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é, –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –¥–µ–ª–∞—è –ø—Ä–æ—Ü–µ—Å—Å –≤—ã–≤–æ–¥–∞ –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–º –∏ –Ω–∞–¥—ë–∂–Ω—ã–º.",
  "emoji": "‚úÇÔ∏è",
  "title": "–£–º–Ω–æ–µ –æ–±—Ä–µ–∑–∞–Ω–∏–µ —Ü–µ–ø–æ—á–µ–∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —á–µ—Ä–µ–∑ –∞–Ω–∞–ª–∏–∑ –≤–Ω–∏–º–∞–Ω–∏—è"
}
```
[30.01.2026 08:43] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"FROST is an attention-aware method that improves reasoning efficiency by pruning uncritical paths and removing reasoning outliers, leading to reduced token usage and improved accuracy.  					AI-generated summary 				 We propose FROST, an attention-aware method for efficient reasoning. Unlike traditional approaches, FROST leverages attention weights to prune uncritical reasoning paths, yielding shorter and more reliable reasoning trajectories. Methodologically, we introduce the concept of reasoning outliers and design an attention-based mechanism to remove them. Theoretically, FROST preserves and enhances the model's reasoning capacity while eliminating outliers at the sentence level. Empirically, we validate FROST on four benchmarks using two strong reasoning models (Phi-4-Reasoning and GPT-OSS-20B), outperforming state-of-the-art methods such as TALE and ThinkLess. Notably, FROST achieves an average 69.68% reduction in token usage and a 26.70% improvement in accuracy over the base model. Furthermore, in evaluations of attention outlier metrics, FROST reduces the maximum infinity norm by 15.97% and the average kurtosis by 91.09% compared to the base model. Code is available at https://github.com/robinzixuan/FROST"

[30.01.2026 08:43] Response: ```python
['INFERENCE', 'TRAINING']
```

**Justification:**

- **INFERENCE**: FROST focuses on optimizing model deployment by reducing token usage (69.68% reduction) through pruning and efficient reasoning paths, which directly relates to inference optimization.

- **TRAINING**: The paper proposes an attention-aware method that improves model training/fine-tuning by removing reasoning outliers and enhancing the model's reasoning capacity, which relates to improving training methodologies.
[30.01.2026 08:43] Error. Failed to parse JSON from LLM. ["INFERENCE", "TRAINING"]


**Justification:**

- **INFERENCE**: FROST focuses on optimizing model deployment by reducing token usage (69.68% reduction) through pruning and efficient reasoning paths, which directly relates to inference optimization.

- **TRAINING**: The paper proposes an attention-aware method that improves model training/fine-tuning by removing reasoning outliers and enhancing the model"s reasoning capacity, which relates to improving training methodologies.
[30.01.2026 08:43] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"FROST is an attention-aware method that improves reasoning efficiency by pruning uncritical paths and removing reasoning outliers, leading to reduced token usage and improved accuracy.  					AI-generated summary 				 We propose FROST, an attention-aware method for efficient reasoning. Unlike traditional approaches, FROST leverages attention weights to prune uncritical reasoning paths, yielding shorter and more reliable reasoning trajectories. Methodologically, we introduce the concept of reasoning outliers and design an attention-based mechanism to remove them. Theoretically, FROST preserves and enhances the model's reasoning capacity while eliminating outliers at the sentence level. Empirically, we validate FROST on four benchmarks using two strong reasoning models (Phi-4-Reasoning and GPT-OSS-20B), outperforming state-of-the-art methods such as TALE and ThinkLess. Notably, FROST achieves an average 69.68% reduction in token usage and a 26.70% improvement in accuracy over the base model. Furthermore, in evaluations of attention outlier metrics, FROST reduces the maximum infinity norm by 15.97% and the average kurtosis by 91.09% compared to the base model. Code is available at https://github.com/robinzixuan/FROST"

[30.01.2026 08:43] Response: ```python
["REASONING", "OPTIMIZATION", "OPEN_SOURCE"]
```
[30.01.2026 08:43] Response: ParsedChatCompletionMessage[Article](content='{"desc":"FROST is a novel attention-aware method designed to enhance reasoning efficiency in machine learning models. It intelligently prunes uncritical reasoning paths by utilizing attention weights, which leads to shorter and more reliable reasoning processes. The method also introduces a mechanism to identify and eliminate reasoning outliers, thereby preserving the model\'s reasoning capacity while improving accuracy. Empirical results demonstrate that FROST significantly reduces token usage and enhances performance compared to existing state-of-the-art methods.","title":"FROST: Pruning for Efficient and Accurate Reasoning"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="FROST is a novel attention-aware method designed to enhance reasoning efficiency in machine learning models. It intelligently prunes uncritical reasoning paths by utilizing attention weights, which leads to shorter and more reliable reasoning processes. The method also introduces a mechanism to identify and eliminate reasoning outliers, thereby preserving the model's reasoning capacity while improving accuracy. Empirical results demonstrate that FROST significantly reduces token usage and enhances performance compared to existing state-of-the-art methods.", title='FROST: Pruning for Efficient and Accurate Reasoning'))
[30.01.2026 08:43] Response: ParsedChatCompletionMessage[Article](content='{"desc":"FROSTÊòØ‰∏ÄÁßçÂÖ≥Ê≥®Ê≥®ÊÑèÂäõÁöÑÊé®ÁêÜÊñπÊ≥ïÔºåÈÄöËøá‰øÆÂâ™‰∏çÈáçË¶ÅÁöÑÊé®ÁêÜË∑ØÂæÑÂíåÂéªÈô§Êé®ÁêÜÂºÇÂ∏∏ÂÄºÔºåÊèêÈ´òÊé®ÁêÜÊïàÁéá„ÄÇ‰∏é‰º†ÁªüÊñπÊ≥ï‰∏çÂêåÔºåFROSTÂà©Áî®Ê≥®ÊÑèÂäõÊùÉÈáçÊù•‰ºòÂåñÊé®ÁêÜËøáÁ®ãÔºå‰ΩøÂæóÊé®ÁêÜËΩ®ËøπÊõ¥Áü≠‰∏îÊõ¥ÂèØÈù†„ÄÇËØ•ÊñπÊ≥ïÂºïÂÖ•‰∫ÜÊé®ÁêÜÂºÇÂ∏∏ÂÄºÁöÑÊ¶ÇÂøµÔºåÂπ∂ËÆæËÆ°‰∫Ü‰∏ÄÁßçÂü∫‰∫éÊ≥®ÊÑèÂäõÁöÑÊú∫Âà∂Êù•ÂéªÈô§Ëøô‰∫õÂºÇÂ∏∏ÂÄº„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåFROSTÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞‰ºò‰∫éÁé∞ÊúâÁöÑÊúÄÂÖàËøõÊñπÊ≥ïÔºåÊòæËëóÂáèÂ∞ë‰∫Ü‰ª§Áâå‰ΩøÁî®ÈáèÂπ∂ÊèêÈ´ò‰∫ÜÂáÜÁ°ÆÊÄß„ÄÇ","title":"FROSTÔºöÈ´òÊïàÊé®ÁêÜÁöÑÊñ∞ÊñπÊ≥ï"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='FROSTÊòØ‰∏ÄÁßçÂÖ≥Ê≥®Ê≥®ÊÑèÂäõÁöÑÊé®ÁêÜÊñπÊ≥ïÔºåÈÄöËøá‰øÆÂâ™‰∏çÈáçË¶ÅÁöÑÊé®ÁêÜË∑ØÂæÑÂíåÂéªÈô§Êé®ÁêÜÂºÇÂ∏∏ÂÄºÔºåÊèêÈ´òÊé®ÁêÜÊïàÁéá„ÄÇ‰∏é‰º†ÁªüÊñπÊ≥ï‰∏çÂêåÔºåFROSTÂà©Áî®Ê≥®ÊÑèÂäõÊùÉÈáçÊù•‰ºòÂåñÊé®ÁêÜËøáÁ®ãÔºå‰ΩøÂæóÊé®ÁêÜËΩ®ËøπÊõ¥Áü≠‰∏îÊõ¥ÂèØÈù†„ÄÇËØ•ÊñπÊ≥ïÂºïÂÖ•‰∫ÜÊé®ÁêÜÂºÇÂ∏∏ÂÄºÁöÑÊ¶ÇÂøµÔºåÂπ∂ËÆæËÆ°‰∫Ü‰∏ÄÁßçÂü∫‰∫éÊ≥®ÊÑèÂäõÁöÑÊú∫Âà∂Êù•ÂéªÈô§Ëøô‰∫õÂºÇÂ∏∏ÂÄº„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåFROSTÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞‰ºò‰∫éÁé∞ÊúâÁöÑÊúÄÂÖàËøõÊñπÊ≥ïÔºåÊòæËëóÂáèÂ∞ë‰∫Ü‰ª§Áâå‰ΩøÁî®ÈáèÂπ∂ÊèêÈ´ò‰∫ÜÂáÜÁ°ÆÊÄß„ÄÇ', title='FROSTÔºöÈ´òÊïàÊé®ÁêÜÁöÑÊñ∞ÊñπÊ≥ï'))
[30.01.2026 08:43] Using data from previous issue: {"categories": ["#audio", "#architecture", "#benchmark"], "emoji": "üîç", "ru": {"title": "–û–ø—Ç–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Å–µ–≥–º–µ–Ω—Ç–∞: –∫–ª—é—á –∫ —Ç–æ—á–Ω–æ—Å—Ç–∏ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤—ã—Ö –∞—É–¥–∏–æ—Ñ–∏–Ω–≥–µ—Ä–ø—Ä–∏–Ω—Ç–æ–≤", "desc": "–í —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –≤–ª–∏—è–Ω–∏–µ –¥–ª–∏–Ω—ã –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –æ–∫–Ω–∞ –Ω–∞ –∫–∞—á–µ—Å—Ç–≤–æ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤—ã—Ö –∞—É–¥–∏–æ—Ñ–∏–Ω–≥–µ—Ä–ø—Ä–∏–Ω—Ç–æ–≤ ‚Äî –∫–æ–º–ø–∞–∫—Ç–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –∑–≤
[30.01.2026 08:43] Using data from previous issue: {"categories": [], "emoji": "üé®", "ru": {"title": "–ó–Ω–∞–Ω–∏—è –∏–∑ –¥–∞–Ω–Ω—ã—Ö –¥–∏–∑–∞–π–Ω–∞ –¥–ª—è —Å—Ç–∏–ª–∏—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ —É–ª—É—á—à–µ–Ω–∏—è –ø–æ —Ç–µ–∫—Å—Ç—É", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ PRISM –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏—Ö –¥–∏–∑–∞–π–Ω–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞, –∫–æ—Ç–æ—Ä—ã–π —Å–æ–∑–¥–∞—ë—Ç –±–∞–∑—É –∑–Ω–∞–Ω–∏–π –∏–∑ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∏–∑–∞–π–Ω-–¥–∞–Ω–Ω—ã—Ö. –ê–≤—Ç–æ—Ä—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ
[30.01.2026 08:43] Using data from previous issue: {"categories": ["#dataset", "#reasoning", "#rlhf", "#benchmark", "#open_source", "#interpretability", "#agents"], "emoji": "üï∑Ô∏è", "ru": {"title": "–†–∞—Å—Å—É–∂–¥–∞—é—â–∞—è –º–æ–¥–µ–ª—å –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–π –¥–ª—è –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–π –≤–µ–±-–Ω–∞–≤–∏–≥–∞—Ü–∏–∏", "desc": "WebArbiter –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—é –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–π –¥–ª—è –≤
[30.01.2026 08:43] Using data from previous issue: {"categories": ["#benchmark", "#cv", "#robotics"], "emoji": "ü§ñ", "ru": {"title": "–û–±—ä–µ–∫—Ç–Ω–æ-—Ü–µ–Ω—Ç—Ä–∏—á–µ—Å–∫–∏–µ —Å–ª–æ—Ç—ã ‚Äî –∫–ª—é—á –∫ –Ω–∞–¥—ë–∂–Ω–æ–π –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–∏ —Ä–æ–±–æ—Ç–æ–≤ –ø—Ä–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–∏ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —É—Å–ª–æ–≤–∏–π", "desc": "–í —ç—Ç–æ–π —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ –≤—ã–±–æ—Ä–∞ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –ø–æ–ª–∏—Ç–∏–∫ –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–∏ —Ä–æ–±–æ—Ç–æ–≤.
[30.01.2026 08:43] Using data from previous issue: {"categories": ["#benchmark", "#video", "#robotics"], "emoji": "üé¨", "ru": {"title": "–†–∞–∑–¥–µ–ª—ë–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —Ñ–∏–∑–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –≤–∏–¥–µ–æ-–≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω WorldBench ‚Äî –≤–∏–¥–µ–æ-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è —Ä–∞–∑–¥–µ–ª—ë–Ω–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ —Ñ–∏–∑–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª
[30.01.2026 08:43] Using data from previous issue: {"categories": ["#training", "#cv", "#robotics", "#architecture"], "emoji": "ü§ñ", "ru": {"title": "–û–±—ä–µ–∫—Ç–Ω–æ-—Ü–µ–Ω—Ç—Ä–∏—á–Ω–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∏ —á–µ—Ä–µ–∑ –º–Ω–æ–≥–æ—Ñ–∞–∑–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è STORM ‚Äî –ª—ë–≥–∫–∏–π –∞–¥–∞–ø—Ç–∞—Ü–∏–æ–Ω–Ω—ã–π –º–æ–¥—É–ª—å, –∫–æ—Ç–æ—Ä—ã–π –¥–æ–ø–æ–ª–Ω—è–µ—Ç –∑–∞–º–æ—Ä–æ–∂–µ–Ω–Ω—ã–µ –≤–∏–∑—É–∞–ª—å
[30.01.2026 08:43] Renaming data file.
[30.01.2026 08:43] Renaming previous data. hf_papers.json to ./d/2026-01-30.json
[30.01.2026 08:43] Saving new data file.
[30.01.2026 08:43] Generating page.
[30.01.2026 08:43] Renaming previous page.
[30.01.2026 08:43] Renaming previous data. index.html to ./d/2026-01-30.html
[30.01.2026 08:43] Writing result.
[30.01.2026 08:43] Renaming log file.
[30.01.2026 08:43] Renaming previous data. log.txt to ./logs/2026-01-30_last_log.txt
