[08.07.2025 09:14] Read previous papers.
[08.07.2025 09:14] Generating top page (month).
[08.07.2025 09:14] Writing top page (month).
[08.07.2025 10:13] Read previous papers.
[08.07.2025 10:13] Get feed.
[08.07.2025 10:13] Get page data from previous paper. URL: https://huggingface.co/papers/2507.03724
[08.07.2025 10:13] Get page data from previous paper. URL: https://huggingface.co/papers/2507.05163
[08.07.2025 10:13] Get page data from previous paper. URL: https://huggingface.co/papers/2507.04447
[08.07.2025 10:13] Get page data from previous paper. URL: https://huggingface.co/papers/2507.05197
[08.07.2025 10:13] Extract page data from URL. URL: https://huggingface.co/papers/2507.00994
[08.07.2025 10:13] Get page data from previous paper. URL: https://huggingface.co/papers/2507.03483
[08.07.2025 10:13] Get page data from previous paper. URL: https://huggingface.co/papers/2507.02029
[08.07.2025 10:13] Get page data from previous paper. URL: https://huggingface.co/papers/2507.04009
[08.07.2025 10:13] Get page data from previous paper. URL: https://huggingface.co/papers/2507.03253
[08.07.2025 10:13] Get page data from previous paper. URL: https://huggingface.co/papers/2507.05108
[08.07.2025 10:13] Get page data from previous paper. URL: https://huggingface.co/papers/2507.03745
[08.07.2025 10:13] Get page data from previous paper. URL: https://huggingface.co/papers/2507.04952
[08.07.2025 10:13] Get page data from previous paper. URL: https://huggingface.co/papers/2507.04590
[08.07.2025 10:13] Get page data from previous paper. URL: https://huggingface.co/papers/2507.03607
[08.07.2025 10:13] Get page data from previous paper. URL: https://huggingface.co/papers/2507.04036
[08.07.2025 10:13] Get page data from previous paper. URL: https://huggingface.co/papers/2507.05259
[08.07.2025 10:13] Get page data from previous paper. URL: https://huggingface.co/papers/2507.04562
[08.07.2025 10:13] Get page data from previous paper. URL: https://huggingface.co/papers/2507.04376
[08.07.2025 10:13] Get page data from previous paper. URL: https://huggingface.co/papers/2507.03336
[08.07.2025 10:13] Get page data from previous paper. URL: https://huggingface.co/papers/2507.02659
[08.07.2025 10:13] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[08.07.2025 10:13] No deleted papers detected.
[08.07.2025 10:13] Downloading and parsing papers (pdf, html). Total: 20.
[08.07.2025 10:13] Downloading and parsing paper https://huggingface.co/papers/2507.03724.
[08.07.2025 10:13] Extra JSON file exists (./assets/json/2507.03724.json), skip PDF parsing.
[08.07.2025 10:13] Paper image links file exists (./assets/img_data/2507.03724.json), skip HTML parsing.
[08.07.2025 10:13] Success.
[08.07.2025 10:13] Downloading and parsing paper https://huggingface.co/papers/2507.05163.
[08.07.2025 10:13] Extra JSON file exists (./assets/json/2507.05163.json), skip PDF parsing.
[08.07.2025 10:13] Paper image links file exists (./assets/img_data/2507.05163.json), skip HTML parsing.
[08.07.2025 10:13] Success.
[08.07.2025 10:13] Downloading and parsing paper https://huggingface.co/papers/2507.04447.
[08.07.2025 10:13] Extra JSON file exists (./assets/json/2507.04447.json), skip PDF parsing.
[08.07.2025 10:13] Paper image links file exists (./assets/img_data/2507.04447.json), skip HTML parsing.
[08.07.2025 10:13] Success.
[08.07.2025 10:13] Downloading and parsing paper https://huggingface.co/papers/2507.05197.
[08.07.2025 10:13] Extra JSON file exists (./assets/json/2507.05197.json), skip PDF parsing.
[08.07.2025 10:13] Paper image links file exists (./assets/img_data/2507.05197.json), skip HTML parsing.
[08.07.2025 10:13] Success.
[08.07.2025 10:13] Downloading and parsing paper https://huggingface.co/papers/2507.00994.
[08.07.2025 10:13] Downloading paper 2507.00994 from http://arxiv.org/pdf/2507.00994v2...
[08.07.2025 10:13] Extracting affiliations from text.
[08.07.2025 10:13] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Should We Still Pretrain Encoders with Masked Language Modeling? Hippolyte Gisserot-Boukhlef1,6,* Nicolas Boizard2,6,* Manuel Faysse3,6 Duarte M. Alves7,8 Andr√© F. T. Martins5,7,8 C√©line Hudelot6 1Artefact Research Center 2Diabolocom 3Illuin Technology Emmanuel Malherbe1 Pierre Colombo4,6 4Equall 5Unbabel 5 2 0 2 4 ] . [ 2 4 9 9 0 0 . 7 0 5 2 : r 6MICS, CentraleSup√©lec, Universit√© Paris-Saclay 7Instituto Superior T√©cnico & Universidade de Lisboa (Lisbon ELLIS Unit) 8Instituto de Telecomunica√ß√µes hippolyte.gisserot-boukhlef@centralesupelec.fr "
[08.07.2025 10:13] Response: ```python
[
    "Artefact Research Center",
    "Diabolocom",
    "Illuin Technology",
    "Equall",
    "Unbabel",
    "MICS, CentraleSup√©lec, Universit√© Paris-Saclay",
    "Instituto Superior T√©cnico & Universidade de Lisboa (Lisbon ELLIS Unit)",
    "Instituto de Telecomunica√ß√µes"
]
```
[08.07.2025 10:13] Deleting PDF ./assets/pdf/2507.00994.pdf.
[08.07.2025 10:13] Success.
[08.07.2025 10:13] Downloading and parsing paper https://huggingface.co/papers/2507.03483.
[08.07.2025 10:13] Extra JSON file exists (./assets/json/2507.03483.json), skip PDF parsing.
[08.07.2025 10:13] Paper image links file exists (./assets/img_data/2507.03483.json), skip HTML parsing.
[08.07.2025 10:13] Success.
[08.07.2025 10:13] Downloading and parsing paper https://huggingface.co/papers/2507.02029.
[08.07.2025 10:13] Downloading paper 2507.02029 from http://arxiv.org/pdf/2507.02029v2...
[08.07.2025 10:13] Extracting affiliations from text.
[08.07.2025 10:13] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"RoboBrain 2.0 Technical Report Please see Contributions and Author List for more author details. "
[08.07.2025 10:13] Response: []
[08.07.2025 10:13] Extracting affiliations from text.
[08.07.2025 10:13] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"RoboBrain 2.0 Technical ReportPlease see Contributions and Author List for more author details.We introduce RoboBrain 2.0, our latest generation of embodied vision-language foundation models, designed to unify perception, reasoning, and planning for complex embodied tasks in physical environments. It comes in two variants: lightweight 7B model and full-scale 32B model, featuring heterogeneous architecture with vision encoder and language model. Despite its compact size, RoboBrain 2.0 achieves strong performance across wide spectrum of embodied reasoning tasks. On both spatial and temporal benchmarks, the 32B variant achieves leading results, surpassing prior open-source and proprietary models. In particular, it supports key real-world embodied AI capabilities, including spatial understanding (e.g., affordance prediction, spatial referring, trajectory forecasting) and temporal decision-making (e.g., closed-loop interaction, multi-agent longhorizon planning, and scene graph updating). This report details the model architecture, data construction, multi-stage training strategies, infrastructure and practical applications. We hope RoboBrain 2.0 advances embodied AI research and serves as practical step toward building generalist embodied agents. The code, checkpoint and benchmark are available at https://superrobobrain.github.io. 5 2 0 2 5 ] . [ 2 9 2 0 2 0 . 7 0 5 2 : r Figure 1 Benchmark comparison across spatial and temporal reasoning. RoboBrain2.0-32B achieves best performance on both spatial and temporal reasoning benchmarks across BLINK-Spatial, RoboSpatial, RefSpatial-Bench, Where2Place, EgoPlan2 and Multi-Robot-Plan, outperforming prior open-source models and proprietary models.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.1 Input Modalities and Tokenization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.2 Vision Encoder and Projection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.3 LLM Decoder and Output Representations 3 Training Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.1 General MLLM VQA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2 Spatial Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.3 Temporal Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 Training Strategy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.1 Stage 1: Foundational Spatiotemporal Learning . . . . . . . . . . . . . . . . . . . . . . . . . . 4.2 Stage 2: Embodied Spatiotemporal Enhancement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.3 Stage 3: Chain-of-Thought Reasoning in Embodied Contexts 5 Infrastructures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.1 Large-Scale Training Infrastructure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.1.1 Multi-Dimensional Hybrid Parallelism . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.1.2 Pre-Allocate Memory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.1.3 Data Pre-Processing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.1.4 Distributed Data Loading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.1.5 Fault Tolerance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.2 Reinforcement Fine-Tuning Infrastructure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Inference Infrastructure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5. 6 Evaluation Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.1 Spatial Reasoning Capability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.2 Temporal Reasoning Capability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 Conclusion and Future Works . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 Contributions and Author List . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Qualitative examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.1 Examples for Pointing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.2 Examples for Affordance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.3 Examples for Trajectory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.4 Examples for EgoPlan2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.5 Examples for Close-Loop Interaction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.6 Examples for Multi-Robot Planning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.7 Examples for Synthetic Benchmarks Prompts Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.1 Spatial Understanding: Coordinates Pointing . . . . . . . . . . . . . . . . . . . . . . . . . . B.2 Spatial Understanding: Coordinates Trajectory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.3 Spatial Understanding: Bounding Box Affordance B.4 Spatial Understanding: Freeform Q&A General Spatial Analysis . . . . . . . . . . . . . . . B.5 Temporal Understanding: Long-horizon Planning . . . . . . . . . . . . . . . . . . . . . . . . . B.6 Temporal Understanding: Closed Loop Conversation . . . . . . . . . . . . . . . . . . . . . . . B.7 Temporal Understanding: Multi-Robot Planning . . . . . . . . . . . . . . . . . . . . . . . . . 4 5 5 6 6 6 7 8 9 9 10 10 11 11 11 11 11 12 12 12 12 13 13 15 22 23 23 40 42 44 47 51 52 54 54 54 54 55 55 55 55In recent years, large language models (LLMs) and vision-language models (VLMs) have emerged as key driving forces in the advancement of general artificial intelligence (AGI). Within digital environments, these models have demonstrated "
[08.07.2025 10:13] Mistral response. {"object": "error", "message": "Service tier capacity exceeded for this model.", "type": "invalid_request_error", "param": null, "code": null}
[08.07.2025 10:13] Failed to download and parse paper https://huggingface.co/papers/2507.02029: 'choices'
[08.07.2025 10:13] Downloading and parsing paper https://huggingface.co/papers/2507.04009.
[08.07.2025 10:13] Extra JSON file exists (./assets/json/2507.04009.json), skip PDF parsing.
[08.07.2025 10:13] Paper image links file exists (./assets/img_data/2507.04009.json), skip HTML parsing.
[08.07.2025 10:13] Success.
[08.07.2025 10:13] Downloading and parsing paper https://huggingface.co/papers/2507.03253.
[08.07.2025 10:13] Extra JSON file exists (./assets/json/2507.03253.json), skip PDF parsing.
[08.07.2025 10:13] Paper image links file exists (./assets/img_data/2507.03253.json), skip HTML parsing.
[08.07.2025 10:13] Success.
[08.07.2025 10:13] Downloading and parsing paper https://huggingface.co/papers/2507.05108.
[08.07.2025 10:13] Extra JSON file exists (./assets/json/2507.05108.json), skip PDF parsing.
[08.07.2025 10:13] Paper image links file exists (./assets/img_data/2507.05108.json), skip HTML parsing.
[08.07.2025 10:13] Success.
[08.07.2025 10:13] Downloading and parsing paper https://huggingface.co/papers/2507.03745.
[08.07.2025 10:13] Extra JSON file exists (./assets/json/2507.03745.json), skip PDF parsing.
[08.07.2025 10:13] Paper image links file exists (./assets/img_data/2507.03745.json), skip HTML parsing.
[08.07.2025 10:13] Success.
[08.07.2025 10:13] Downloading and parsing paper https://huggingface.co/papers/2507.04952.
[08.07.2025 10:13] Extra JSON file exists (./assets/json/2507.04952.json), skip PDF parsing.
[08.07.2025 10:13] Paper image links file exists (./assets/img_data/2507.04952.json), skip HTML parsing.
[08.07.2025 10:13] Success.
[08.07.2025 10:13] Downloading and parsing paper https://huggingface.co/papers/2507.04590.
[08.07.2025 10:13] Extra JSON file exists (./assets/json/2507.04590.json), skip PDF parsing.
[08.07.2025 10:13] Paper image links file exists (./assets/img_data/2507.04590.json), skip HTML parsing.
[08.07.2025 10:13] Success.
[08.07.2025 10:13] Downloading and parsing paper https://huggingface.co/papers/2507.03607.
[08.07.2025 10:13] Extra JSON file exists (./assets/json/2507.03607.json), skip PDF parsing.
[08.07.2025 10:13] Paper image links file exists (./assets/img_data/2507.03607.json), skip HTML parsing.
[08.07.2025 10:13] Success.
[08.07.2025 10:13] Downloading and parsing paper https://huggingface.co/papers/2507.04036.
[08.07.2025 10:13] Extra JSON file exists (./assets/json/2507.04036.json), skip PDF parsing.
[08.07.2025 10:13] Paper image links file exists (./assets/img_data/2507.04036.json), skip HTML parsing.
[08.07.2025 10:13] Success.
[08.07.2025 10:13] Downloading and parsing paper https://huggingface.co/papers/2507.05259.
[08.07.2025 10:13] Extra JSON file exists (./assets/json/2507.05259.json), skip PDF parsing.
[08.07.2025 10:13] Paper image links file exists (./assets/img_data/2507.05259.json), skip HTML parsing.
[08.07.2025 10:13] Success.
[08.07.2025 10:13] Downloading and parsing paper https://huggingface.co/papers/2507.04562.
[08.07.2025 10:13] Extra JSON file exists (./assets/json/2507.04562.json), skip PDF parsing.
[08.07.2025 10:13] Paper image links file exists (./assets/img_data/2507.04562.json), skip HTML parsing.
[08.07.2025 10:13] Success.
[08.07.2025 10:13] Downloading and parsing paper https://huggingface.co/papers/2507.04376.
[08.07.2025 10:13] Extra JSON file exists (./assets/json/2507.04376.json), skip PDF parsing.
[08.07.2025 10:13] Paper image links file exists (./assets/img_data/2507.04376.json), skip HTML parsing.
[08.07.2025 10:13] Success.
[08.07.2025 10:13] Downloading and parsing paper https://huggingface.co/papers/2507.03336.
[08.07.2025 10:13] Extra JSON file exists (./assets/json/2507.03336.json), skip PDF parsing.
[08.07.2025 10:13] Paper image links file exists (./assets/img_data/2507.03336.json), skip HTML parsing.
[08.07.2025 10:13] Success.
[08.07.2025 10:13] Downloading and parsing paper https://huggingface.co/papers/2507.02659.
[08.07.2025 10:13] Extra JSON file exists (./assets/json/2507.02659.json), skip PDF parsing.
[08.07.2025 10:13] Paper image links file exists (./assets/img_data/2507.02659.json), skip HTML parsing.
[08.07.2025 10:13] Success.
[08.07.2025 10:13] Enriching papers with extra data.
[08.07.2025 10:13] ********************************************************************************
[08.07.2025 10:13] Abstract 0. MemOS is proposed as a memory operating system for Large Language Models to enhance memory management, enabling efficient storage and retrieval, and facilitating continual learning and personalized modeling.  					AI-generated summary 				 Large Language Models (LLMs) have become an essential infras...
[08.07.2025 10:13] ********************************************************************************
[08.07.2025 10:13] Abstract 1. A high-speed 4D capturing system using low FPS cameras with asynchronous capture and video-diffusion-based artifact correction enhances reconstruction quality.  					AI-generated summary 				 Reconstructing fast-dynamic scenes from multi-view videos is crucial for high-speed motion analysis and real...
[08.07.2025 10:13] ********************************************************************************
[08.07.2025 10:13] Abstract 2. DreamVLA improves robot manipulation through a VLA framework that incorporates world knowledge, dynamic-region guidance, and a diffusion-based transformer to ensure clear, disentangled representations for action planning.  					AI-generated summary 				 Recent advances in vision-language-action (VLA...
[08.07.2025 10:13] ********************************************************************************
[08.07.2025 10:13] Abstract 3. A scalable reward modeling method, Policy Discriminative Learning (POLAR), enhances reward model performance and generalizes robustly in reinforcement learning through policy comparison.  					AI-generated summary 				 We offer a novel perspective on reward modeling by formulating it as a policy dis...
[08.07.2025 10:13] ********************************************************************************
[08.07.2025 10:13] Abstract 4. Learning high-quality text representations is fundamental to a wide range of NLP tasks. While encoder pretraining has traditionally relied on Masked Language Modeling (MLM), recent evidence suggests that decoder models pretrained with Causal Language Modeling (CLM) can be effectively repurposed as e...
[08.07.2025 10:13] ********************************************************************************
[08.07.2025 10:13] Abstract 5. A large-scale dataset and verification tool are introduced for assessing and improving cross-disciplinary reasoning capabilities in multimodal models.  					AI-generated summary 				 In this paper, we introduce BMMR, a large-scale bilingual, multimodal, multi-disciplinary reasoning dataset for the c...
[08.07.2025 10:13] ********************************************************************************
[08.07.2025 10:13] Abstract 6. RoboBrain 2.0, a vision-language foundation model, achieves top performance in embodied tasks through its heterogeneous architecture and multi-stage training strategies.  					AI-generated summary 				 We introduce RoboBrain 2.0, our latest generation of embodied vision-language foundation models, d...
[08.07.2025 10:13] ********************************************************************************
[08.07.2025 10:13] Abstract 7. A unified framework called Easy Dataset synthesizes fine-tuning data from unstructured documents using a GUI and LLMs, improving domain-specific performance of LLMs while maintaining general knowledge.  					AI-generated summary 				 Large language models (LLMs) have shown impressive performance on ...
[08.07.2025 10:13] ********************************************************************************
[08.07.2025 10:13] Abstract 8. RefineX is a scalable framework for improving the quality of large language model pre-training data through programmatic editing, yielding better performance than alternative methods across various downstream tasks.  					AI-generated summary 				 The foundational capabilities of large language mode...
[08.07.2025 10:13] ********************************************************************************
[08.07.2025 10:13] Abstract 9. Historical documents represent an invaluable cultural heritage, yet have undergone significant degradation over time through tears, water erosion, and oxidation. Existing Historical Document Restoration (HDR) methods primarily focus on single modality or limited-size restoration, failing to meet pra...
[08.07.2025 10:13] ********************************************************************************
[08.07.2025 10:13] Abstract 10. A streaming video generation model named StreamDiT, based on transformer-based diffusion models, enables real-time video generation with high content consistency and visual quality.  					AI-generated summary 				 Recently, great progress has been achieved in text-to-video (T2V) generation by scalin...
[08.07.2025 10:13] ********************************************************************************
[08.07.2025 10:13] Abstract 11. ArtifactsBench, a novel benchmark and evaluation framework, automates the assessment of visual code generation quality using temporal screenshots and a multimodal language model judge.  					AI-generated summary 				 The generative capabilities of Large Language Models (LLMs) are rapidly expanding f...
[08.07.2025 10:13] ********************************************************************************
[08.07.2025 10:13] Abstract 12. A unified framework VLM2Vec-V2 is proposed for learning embeddings across diverse visual forms such as videos and documents, demonstrating strong performance on new tasks and improving upon existing benchmarks for images.  					AI-generated summary 				 Multimodal embedding models have been crucial ...
[08.07.2025 10:13] ********************************************************************************
[08.07.2025 10:13] Abstract 13. A transformer-based model predicts software vulnerability severity levels directly from text, enhancing triage efficiency and consistency.  					AI-generated summary 				 This paper presents VLAI, a transformer-based model that predicts software vulnerability severity levels directly from text descr...
[08.07.2025 10:13] ********************************************************************************
[08.07.2025 10:13] Abstract 14. A multimodal agent transforms documents into detailed presentation videos with audio, evaluated using a comprehensive framework involving vision-language models.  					AI-generated summary 				 We present PresentAgent, a multimodal agent that transforms long-form documents into narrated presentation...
[08.07.2025 10:13] ********************************************************************************
[08.07.2025 10:13] Abstract 15. X-Planner, a planning system utilizing a multimodal large language model, decomposes complex text-guided image editing instructions into precise sub-instructions, ensuring localized, identity-preserving edits and achieving top performance on established benchmarks.  					AI-generated summary 				 Re...
[08.07.2025 10:13] ********************************************************************************
[08.07.2025 10:13] Abstract 16. State-of-the-art large language models are evaluated on forecasting questions and show lower accuracy compared to human superforecasters.  					AI-generated summary 				 Large language models (LLMs) have demonstrated remarkable capabilities across diverse tasks, but their ability to forecast future ...
[08.07.2025 10:13] ********************************************************************************
[08.07.2025 10:13] Abstract 17. As Artificial Intelligence systems evolve from monolithic models to ecosystems of specialized agents, the need for standardized communication protocols becomes increasingly critical. This paper introduces MOD-X (Modular Open Decentralized eXchange), a novel architectural framework proposal for agent...
[08.07.2025 10:13] ********************************************************************************
[08.07.2025 10:13] Abstract 18. DiaFORGE is a disambiguation framework that enhances large language models' ability to invoke enterprise APIs accurately through dialogue synthesis, supervised fine-tuning, and real-world evaluation.  					AI-generated summary 				 Large language models (LLMs) are increasingly tasked with invoking e...
[08.07.2025 10:13] ********************************************************************************
[08.07.2025 10:13] Abstract 19. OmniDraft, a unified framework, addresses cross-vocabulary mismatch and improves decoding speed by allowing a single draft model to interact dynamically with diverse target models in online settings.  					AI-generated summary 				 Speculative decoding generally dictates having a small, efficient dr...
[08.07.2025 10:13] Read previous papers.
[08.07.2025 10:13] Generating reviews via LLM API.
[08.07.2025 10:13] Using data from previous issue: {"categories": ["#training", "#agi", "#rag", "#long_context", "#optimization", "#data"], "emoji": "üß†", "ru": {"title": "MemOS: –æ–ø–µ—Ä–∞—Ü–∏–æ–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –ø–∞–º—è—Ç–∏ –¥–ª—è –±–æ–ª–µ–µ —É–º–Ω—ã—Ö –∏ –∞–¥–∞–ø—Ç–∏–≤–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "MemOS - —ç—Ç–æ –æ–ø–µ—Ä–∞—Ü–∏–æ–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –ø–∞–º—è—Ç–∏ –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM), –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–∞—è
[08.07.2025 10:13] Using data from previous issue: {"categories": ["#optimization", "#diffusion", "#3d", "#video"], "emoji": "üé•", "ru": {"title": "–í—ã—Å–æ–∫–æ—Å–∫–æ—Ä–æ—Å—Ç–Ω–∞—è 4D-—Å—ä–µ–º–∫–∞ –æ–±—ã—á–Ω—ã–º–∏ –∫–∞–º–µ—Ä–∞–º–∏", "desc": "–ü—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è —Å–∏—Å—Ç–µ–º–∞ –≤—ã—Å–æ–∫–æ—Å–∫–æ—Ä–æ—Å—Ç–Ω–æ–π 4D-—Å—ä–µ–º–∫–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∫–∞–º–µ—Ä —Å –Ω–∏–∑–∫–æ–π —á–∞—Å—Ç–æ—Ç–æ–π –∫–∞–¥—Ä–æ–≤ –∏ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–º –∑–∞—Ö–≤–∞—Ç–æ–º. –°–∏—Å—Ç–µ–º–∞ –ø–æ–≤—ã—à–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—É—é
[08.07.2025 10:13] Using data from previous issue: {"categories": ["#robotics", "#reasoning", "#training", "#optimization", "#multimodal", "#diffusion", "#agents", "#games"], "emoji": "ü§ñ", "ru": {"title": "DreamVLA: –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–µ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–µ–π—Å—Ç–≤–∏–π —Ä–æ–±–æ—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –æ–∫—Ä—É–∂–∞—é—â–µ–≥–æ –º–∏—Ä–∞", "desc": "DreamVLA - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ —É
[08.07.2025 10:13] Using data from previous issue: {"categories": ["#optimization", "#rlhf", "#training", "#rl"], "emoji": "üéØ", "ru": {"title": "POLAR: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–π –≤ –æ–±—É—á–µ–Ω–∏–∏ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –Ω–∞–∑–≤–∞–Ω–Ω—ã–π Policy Discriminat
[08.07.2025 10:13] Querying the API.
[08.07.2025 10:13] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Learning high-quality text representations is fundamental to a wide range of NLP tasks. While encoder pretraining has traditionally relied on Masked Language Modeling (MLM), recent evidence suggests that decoder models pretrained with Causal Language Modeling (CLM) can be effectively repurposed as encoders, often surpassing traditional encoders on text representation benchmarks. However, it remains unclear whether these gains reflect an inherent advantage of the CLM objective or arise from confounding factors such as model and data scale. In this paper, we address this question through a series of large-scale, carefully controlled pretraining ablations, training a total of 30 models ranging from 210 million to 1 billion parameters, and conducting over 15,000 fine-tuning and evaluation runs. We find that while training with MLM generally yields better performance across text representation tasks, CLM-trained models are more data-efficient and demonstrate improved fine-tuning stability. Building on these findings, we experimentally show that a biphasic training strategy that sequentially applies CLM and then MLM, achieves optimal performance under a fixed computational training budget. Moreover, we demonstrate that this strategy becomes more appealing when initializing from readily available pretrained CLM models (from the existing LLM ecosystem), reducing the computational burden needed to train best-in-class encoder models. We release all project artifacts at https://hf.co/MLMvsCLM to foster further research.
[08.07.2025 10:13] Response: {
  "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Å—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –º–µ—Ç–æ–¥–æ–≤ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è —ç–Ω–∫–æ–¥–µ—Ä–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —è–∑—ã–∫–æ–≤–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è (MLM) –∏ –∫–∞—É–∑–∞–ª—å–Ω–æ–≥–æ —è–∑—ã–∫–æ–≤–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è (CLM). –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ–≤–æ–¥—è—Ç –º–∞—Å—à—Ç–∞–±–Ω—ã–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã —Å –º–æ–¥–µ–ª—è–º–∏ —Ä–∞–∑–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞, –æ—Ü–µ–Ω–∏–≤–∞—è –∏—Ö –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ –∑–∞–¥–∞—á–∞—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ MLM –≤ —Ü–µ–ª–æ–º –¥–∞–µ—Ç –ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, –Ω–æ CLM –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–µ–Ω —Å —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –∏ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –¥–æ–æ–±—É—á–µ–Ω–∏—è. –ü—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –¥–≤—É—Ö—Ñ–∞–∑–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –æ–±—É—á–µ–Ω–∏—è, —Å–æ—á–µ—Ç–∞—é—â–∞—è CLM –∏ MLM, –∫–æ—Ç–æ—Ä–∞—è –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–∏ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–º –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–º –±—é–¥–∂–µ—Ç–µ.",
  "emoji": "üß†",
  "title": "–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è —ç–Ω–∫–æ–¥–µ—Ä–æ–≤: —Å–∏–Ω–µ—Ä–≥–∏—è CLM –∏ MLM"
}
[08.07.2025 10:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Learning high-quality text representations is fundamental to a wide range of NLP tasks. While encoder pretraining has traditionally relied on Masked Language Modeling (MLM), recent evidence suggests that decoder models pretrained with Causal Language Modeling (CLM) can be effectively repurposed as encoders, often surpassing traditional encoders on text representation benchmarks. However, it remains unclear whether these gains reflect an inherent advantage of the CLM objective or arise from confounding factors such as model and data scale. In this paper, we address this question through a series of large-scale, carefully controlled pretraining ablations, training a total of 30 models ranging from 210 million to 1 billion parameters, and conducting over 15,000 fine-tuning and evaluation runs. We find that while training with MLM generally yields better performance across text representation tasks, CLM-trained models are more data-efficient and demonstrate improved fine-tuning stability. Building on these findings, we experimentally show that a biphasic training strategy that sequentially applies CLM and then MLM, achieves optimal performance under a fixed computational training budget. Moreover, we demonstrate that this strategy becomes more appealing when initializing from readily available pretrained CLM models (from the existing LLM ecosystem), reducing the computational burden needed to train best-in-class encoder models. We release all project artifacts at https://hf.co/MLMvsCLM to foster further research."

[08.07.2025 10:14] Response: ```python
["DATASET", "TRAINING"]
```
[08.07.2025 10:14] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Learning high-quality text representations is fundamental to a wide range of NLP tasks. While encoder pretraining has traditionally relied on Masked Language Modeling (MLM), recent evidence suggests that decoder models pretrained with Causal Language Modeling (CLM) can be effectively repurposed as encoders, often surpassing traditional encoders on text representation benchmarks. However, it remains unclear whether these gains reflect an inherent advantage of the CLM objective or arise from confounding factors such as model and data scale. In this paper, we address this question through a series of large-scale, carefully controlled pretraining ablations, training a total of 30 models ranging from 210 million to 1 billion parameters, and conducting over 15,000 fine-tuning and evaluation runs. We find that while training with MLM generally yields better performance across text representation tasks, CLM-trained models are more data-efficient and demonstrate improved fine-tuning stability. Building on these findings, we experimentally show that a biphasic training strategy that sequentially applies CLM and then MLM, achieves optimal performance under a fixed computational training budget. Moreover, we demonstrate that this strategy becomes more appealing when initializing from readily available pretrained CLM models (from the existing LLM ecosystem), reducing the computational burden needed to train best-in-class encoder models. We release all project artifacts at https://hf.co/MLMvsCLM to foster further research."

[08.07.2025 10:14] Response: ```python
["OPTIMIZATION", "SURVEY"]
```
[08.07.2025 10:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper investigates the effectiveness of Causal Language Modeling (CLM) compared to traditional Masked Language Modeling (MLM) for training text representations in natural language processing (NLP). The authors conduct extensive experiments with various model sizes and training strategies, revealing that while MLM generally performs better, CLM models are more efficient with data and offer greater stability during fine-tuning. They propose a biphasic training approach that first uses CLM and then MLM, which optimizes performance within a limited computational budget. Additionally, leveraging existing pretrained CLM models can significantly reduce the resources needed to achieve high-quality encoder models.","title":"Unlocking Text Representation: CLM Meets MLM for Optimal Performance"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper investigates the effectiveness of Causal Language Modeling (CLM) compared to traditional Masked Language Modeling (MLM) for training text representations in natural language processing (NLP). The authors conduct extensive experiments with various model sizes and training strategies, revealing that while MLM generally performs better, CLM models are more efficient with data and offer greater stability during fine-tuning. They propose a biphasic training approach that first uses CLM and then MLM, which optimizes performance within a limited computational budget. Additionally, leveraging existing pretrained CLM models can significantly reduce the resources needed to achieve high-quality encoder models.', title='Unlocking Text Representation: CLM Meets MLM for Optimal Performance'))
[08.07.2025 10:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨ÊñáÊé¢ËÆ®‰∫ÜÊñáÊú¨Ë°®Á§∫Â≠¶‰π†Âú®Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ‰∏≠ÁöÑÈáçË¶ÅÊÄß„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºå‰ΩøÁî®Âõ†ÊûúËØ≠Ë®ÄÊ®°ÂûãÔºàCLMÔºâÈ¢ÑËÆ≠ÁªÉÁöÑËß£Á†ÅÂô®Ê®°ÂûãÂèØ‰ª•ÊúâÊïàÂú∞‰Ωú‰∏∫ÁºñÁ†ÅÂô®Ôºå‰∏îÂú®ÊñáÊú¨Ë°®Á§∫Âü∫ÂáÜÊµãËØï‰∏≠Â∏∏Â∏∏Ë∂ÖË∂ä‰º†ÁªüÁöÑÁºñÁ†ÅÂô®„ÄÇÂ∞ΩÁÆ°‰ΩøÁî®Êé©Á†ÅËØ≠Ë®ÄÊ®°ÂûãÔºàMLMÔºâËÆ≠ÁªÉÈÄöÂ∏∏Âú®ÊñáÊú¨Ë°®Á§∫‰ªªÂä°‰∏≠Ë°®Áé∞Êõ¥Â•ΩÔºå‰ΩÜCLMËÆ≠ÁªÉÁöÑÊ®°ÂûãÂú®Êï∞ÊçÆÊïàÁéáÂíåÂæÆË∞ÉÁ®≥ÂÆöÊÄßÊñπÈù¢Ë°®Áé∞Êõ¥‰Ω≥„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂèåÁõ∏ËÆ≠ÁªÉÁ≠ñÁï•ÔºåÂÖàÂ∫îÁî®CLMÂÜçÂ∫îÁî®MLMÔºåÂú®Âõ∫ÂÆöÁöÑËÆ°ÁÆóÈ¢ÑÁÆó‰∏ãÂÆûÁé∞ÊúÄ‰Ω≥ÊÄßËÉΩÔºåÂπ∂‰∏îÂú®‰ΩøÁî®Áé∞ÊúâÁöÑÈ¢ÑËÆ≠ÁªÉCLMÊ®°ÂûãÂàùÂßãÂåñÊó∂ÔºåËøõ‰∏ÄÊ≠•Èôç‰Ωé‰∫ÜËÆ≠ÁªÉÈ°∂Á∫ßÁºñÁ†ÅÂô®Ê®°ÂûãÁöÑËÆ°ÁÆóË¥üÊãÖ„ÄÇ","title":"ÂèåÁõ∏ËÆ≠ÁªÉÁ≠ñÁï•ÔºöCLM‰∏éMLMÁöÑÊúÄ‰Ω≥ÁªìÂêà"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨ÊñáÊé¢ËÆ®‰∫ÜÊñáÊú¨Ë°®Á§∫Â≠¶‰π†Âú®Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ‰∏≠ÁöÑÈáçË¶ÅÊÄß„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºå‰ΩøÁî®Âõ†ÊûúËØ≠Ë®ÄÊ®°ÂûãÔºàCLMÔºâÈ¢ÑËÆ≠ÁªÉÁöÑËß£Á†ÅÂô®Ê®°ÂûãÂèØ‰ª•ÊúâÊïàÂú∞‰Ωú‰∏∫ÁºñÁ†ÅÂô®Ôºå‰∏îÂú®ÊñáÊú¨Ë°®Á§∫Âü∫ÂáÜÊµãËØï‰∏≠Â∏∏Â∏∏Ë∂ÖË∂ä‰º†ÁªüÁöÑÁºñÁ†ÅÂô®„ÄÇÂ∞ΩÁÆ°‰ΩøÁî®Êé©Á†ÅËØ≠Ë®ÄÊ®°ÂûãÔºàMLMÔºâËÆ≠ÁªÉÈÄöÂ∏∏Âú®ÊñáÊú¨Ë°®Á§∫‰ªªÂä°‰∏≠Ë°®Áé∞Êõ¥Â•ΩÔºå‰ΩÜCLMËÆ≠ÁªÉÁöÑÊ®°ÂûãÂú®Êï∞ÊçÆÊïàÁéáÂíåÂæÆË∞ÉÁ®≥ÂÆöÊÄßÊñπÈù¢Ë°®Áé∞Êõ¥‰Ω≥„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂèåÁõ∏ËÆ≠ÁªÉÁ≠ñÁï•ÔºåÂÖàÂ∫îÁî®CLMÂÜçÂ∫îÁî®MLMÔºåÂú®Âõ∫ÂÆöÁöÑËÆ°ÁÆóÈ¢ÑÁÆó‰∏ãÂÆûÁé∞ÊúÄ‰Ω≥ÊÄßËÉΩÔºåÂπ∂‰∏îÂú®‰ΩøÁî®Áé∞ÊúâÁöÑÈ¢ÑËÆ≠ÁªÉCLMÊ®°ÂûãÂàùÂßãÂåñÊó∂ÔºåËøõ‰∏ÄÊ≠•Èôç‰Ωé‰∫ÜËÆ≠ÁªÉÈ°∂Á∫ßÁºñÁ†ÅÂô®Ê®°ÂûãÁöÑËÆ°ÁÆóË¥üÊãÖ„ÄÇ', title='ÂèåÁõ∏ËÆ≠ÁªÉÁ≠ñÁï•ÔºöCLM‰∏éMLMÁöÑÊúÄ‰Ω≥ÁªìÂêà'))
[08.07.2025 10:14] Using data from previous issue: {"categories": ["#multimodal", "#reasoning", "#dataset", "#benchmark", "#open_source", "#data"], "emoji": "üß†", "ru": {"title": "–ù–æ–≤—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º—É–ª—å—Ç–∏–¥–∏—Å—Ü–∏–ø–ª–∏–Ω–∞—Ä–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –ò–ò", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç BMMR - –º–∞—Å—à—Ç–∞–±–Ω—ã–π –¥–≤—É—è–∑—ã—á–Ω—ã–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤
[08.07.2025 10:14] Using data from previous issue: {"categories": ["#agi", "#reasoning", "#architecture", "#benchmark", "#training", "#agents", "#open_source"], "emoji": "ü§ñ", "ru": {"title": "RoboBrain 2.0: –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ò–ò –¥–ª—è –≤–æ–ø–ª–æ—â–µ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á", "desc": "RoboBrain 2.0 - —ç—Ç–æ –Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ –¥–ª—è –≤–æ–ø–ª–æ—â–µ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∞—è
[08.07.2025 10:14] Using data from previous issue: {"categories": ["#synthetic", "#data", "#dataset", "#open_source"], "emoji": "üß†", "ru": {"title": "–°–∏–Ω—Ç–µ–∑ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è LLM —Å –ø–æ–º–æ—â—å—é Easy Dataset", "desc": "Easy Dataset - —ç—Ç–æ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –∏–∑ –Ω–µ—Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥
[08.07.2025 10:14] Using data from previous issue: {"categories": ["#training", "#optimization", "#data"], "emoji": "‚úÇÔ∏è", "ru": {"title": "RefineX: —Ö–∏—Ä—É—Ä–≥–∏—á–µ—Å–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –≤ —É–ª—É—á—à–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ò–ò", "desc": "RefineX - —ç—Ç–æ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø—É—Ç–µ–º –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–≥–æ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è. –û
[08.07.2025 10:14] Using data from previous issue: {"categories": ["#architecture", "#cv", "#multimodal", "#dataset", "#data"], "emoji": "üìú", "ru": {"title": "AutoHDR: –í–æ–∑—Ä–æ–∂–¥–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ —á–µ—Ä–µ–∑ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Ä–µ—Å—Ç–∞–≤—Ä–∞—Ü–∏–∏ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º AutoHDR. –ê–≤—Ç–æ—Ä
[08.07.2025 10:14] Using data from previous issue: {"categories": ["#diffusion", "#video", "#games"], "emoji": "üé¨", "ru": {"title": "StreamDiT: –†–µ–∞–ª—å–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é –ò–ò", "desc": "StreamDiT - —ç—Ç–æ –º–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ—Ç–æ–∫–æ–≤–æ–≥–æ –≤–∏–¥–µ–æ, –æ—Å–Ω–æ–≤–∞–Ω–Ω–∞—è –Ω–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–Ω—ã—Ö –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö. –û–Ω–∞ –ø–æ–∑–≤–æ–ª—è–µ—Ç –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –≤–∏–¥–µ–æ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º
[08.07.2025 10:14] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#games", "#open_source"], "emoji": "üñ•Ô∏è", "ru": {"title": "–ê–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –∫–æ–¥–∞ —Å –ø–æ–º–æ—â—å—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "ArtifactsBench - —ç—Ç–æ –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –∫
[08.07.2025 10:14] Using data from previous issue: {"categories": ["#games", "#rag", "#survey", "#benchmark", "#transfer_learning", "#multimodal"], "emoji": "üé•", "ru": {"title": "–ï–¥–∏–Ω–∞—è –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è –≤—Å–µ—Ö –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤", "desc": "VLM2Vec-V2 - —ç—Ç–æ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤, –≤–∫–ª—é—á–∞—è –≤
[08.07.2025 10:14] Using data from previous issue: {"categories": ["#security", "#architecture", "#dataset", "#data", "#training", "#open_source"], "emoji": "üõ°Ô∏è", "ru": {"title": "–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç –Ω–∞ —Å—Ç—Ä–∞–∂–µ –∫–∏–±–µ—Ä–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏: –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ—Ü–µ–Ω–∫–∞ —É—è–∑–≤–∏–º–æ—Å—Ç–µ–π –ü–û", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –º–æ–¥–µ–ª—å VLAI –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç
[08.07.2025 10:14] Using data from previous issue: {"categories": ["#cv", "#multimodal", "#agents", "#optimization", "#dataset", "#benchmark", "#games", "#interpretability"], "emoji": "üé•", "ru": {"title": "–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç —Å–æ–∑–¥–∞–µ—Ç –ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏–∏ –Ω–∞ —É—Ä–æ–≤–Ω–µ —á–µ–ª–æ–≤–µ–∫–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç PresentAgent - –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –∞–≥–µ–Ω—Ç–∞, –ø—Ä–µ–æ–±—Ä–∞–∑—É—é—â
[08.07.2025 10:14] Using data from previous issue: {"categories": ["#dataset", "#reasoning", "#benchmark", "#multimodal", "#diffusion"], "emoji": "üé®", "ru": {"title": "X-Planner: —É–º–Ω–æ–µ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "X-Planner - —ç—Ç–æ —Å–∏—Å—Ç–µ–º–∞ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–π –±–æ–ª—å—à–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω
[08.07.2025 10:14] Using data from previous issue: {"categories": ["#benchmark", "#reasoning"], "emoji": "üîÆ", "ru": {"title": "–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç vs —Å—É–ø–µ—Ä–ø—Ä–æ–≥–Ω–æ–∑–∏—Å—Ç—ã: –±–∏—Ç–≤–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç–µ–ª–µ–π –±—É–¥—É—â–µ–≥–æ", "desc": "–°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (LLM) –±—ã–ª–∏ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω—ã –Ω–∞ –∑–∞–¥–∞—á–∞—Ö –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –±—É–¥—É—â–∏—Ö —Å–æ–±—ã—Ç–∏–π. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ LLM –¥–æ—Å
[08.07.2025 10:14] Using data from previous issue: {"categories": ["#optimization", "#interpretability", "#agi", "#agents", "#architecture"], "emoji": "üîÄ", "ru": {"title": "MOD-X: –ù–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç MOD-X - –Ω–æ–≤—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—É—é –∫–æ–Ω—Ü–µ–ø—Ü–∏—é –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –º–µ–∂–¥—É –ò–ò-–∞–≥–µ–Ω—Ç–∞–º–∏. MOD-X –ø
[08.07.2025 10:14] Using data from previous issue: {"categories": ["#optimization", "#open_source", "#dataset", "#training", "#data", "#alignment", "#benchmark", "#agents"], "emoji": "üîß", "ru": {"title": "DiaFORGE: —Ç–æ—á–Ω—ã–µ –≤—ã–∑–æ–≤—ã API —á–µ—Ä–µ–∑ —Å–∏–Ω—Ç–µ–∑ –¥–∏–∞–ª–æ–≥–æ–≤ –∏ –¥–æ–æ–±—É—á–µ–Ω–∏–µ LLM", "desc": "DiaFORGE - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö 
[08.07.2025 10:14] Using data from previous issue: {"categories": ["#optimization", "#games", "#inference", "#training", "#reasoning", "#multimodal"], "emoji": "üöÄ", "ru": {"title": "–û–¥–∏–Ω —á–µ—Ä–Ω–æ–≤–∏–∫ –¥–ª—è –≤—Å–µ—Ö: —É—Å–∫–æ—Ä–µ–Ω–∏–µ –∏ –∞–¥–∞–ø—Ç–∞—Ü–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "OmniDraft - —ç—Ç–æ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è —Ä–∞–±–æ—Ç—ã —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –û–Ω–∞ —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª
[08.07.2025 10:14] Renaming data file.
[08.07.2025 10:14] Renaming previous data. hf_papers.json to ./d/2025-07-08.json
[08.07.2025 10:14] Saving new data file.
[08.07.2025 10:14] Generating page.
[08.07.2025 10:14] Renaming previous page.
[08.07.2025 10:14] Renaming previous data. index.html to ./d/2025-07-08.html
[08.07.2025 10:14] Writing result.
[08.07.2025 10:14] Renaming log file.
[08.07.2025 10:14] Renaming previous data. log.txt to ./logs/2025-07-08_last_log.txt
