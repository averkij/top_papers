[08.07.2025 21:12] Read previous papers.
[08.07.2025 21:12] Generating top page (month).
[08.07.2025 21:12] Writing top page (month).
[08.07.2025 22:11] Read previous papers.
[08.07.2025 22:11] Get feed.
[08.07.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2507.03724
[08.07.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2507.00994
[08.07.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2507.05163
[08.07.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2507.04447
[08.07.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2507.05197
[08.07.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2507.03483
[08.07.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2507.02029
[08.07.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2507.03253
[08.07.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2507.04009
[08.07.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2507.03745
[08.07.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2507.05108
[08.07.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2507.02659
[08.07.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2507.03683
[08.07.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2507.04952
[08.07.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.21884
[08.07.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2507.05257
[08.07.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2507.04590
[08.07.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2507.03607
[08.07.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2507.04036
[08.07.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2507.03033
[08.07.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2507.05259
[08.07.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2507.03336
[08.07.2025 22:11] Extract page data from URL. URL: https://huggingface.co/papers/2507.04642
[08.07.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2507.04562
[08.07.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2507.04376
[08.07.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2507.04285
[08.07.2025 22:11] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[08.07.2025 22:11] No deleted papers detected.
[08.07.2025 22:11] Downloading and parsing papers (pdf, html). Total: 26.
[08.07.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2507.03724.
[08.07.2025 22:11] Extra JSON file exists (./assets/json/2507.03724.json), skip PDF parsing.
[08.07.2025 22:11] Paper image links file exists (./assets/img_data/2507.03724.json), skip HTML parsing.
[08.07.2025 22:11] Success.
[08.07.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2507.00994.
[08.07.2025 22:11] Extra JSON file exists (./assets/json/2507.00994.json), skip PDF parsing.
[08.07.2025 22:11] Paper image links file exists (./assets/img_data/2507.00994.json), skip HTML parsing.
[08.07.2025 22:11] Success.
[08.07.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2507.05163.
[08.07.2025 22:11] Extra JSON file exists (./assets/json/2507.05163.json), skip PDF parsing.
[08.07.2025 22:11] Paper image links file exists (./assets/img_data/2507.05163.json), skip HTML parsing.
[08.07.2025 22:11] Success.
[08.07.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2507.04447.
[08.07.2025 22:11] Extra JSON file exists (./assets/json/2507.04447.json), skip PDF parsing.
[08.07.2025 22:11] Paper image links file exists (./assets/img_data/2507.04447.json), skip HTML parsing.
[08.07.2025 22:11] Success.
[08.07.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2507.05197.
[08.07.2025 22:11] Extra JSON file exists (./assets/json/2507.05197.json), skip PDF parsing.
[08.07.2025 22:11] Paper image links file exists (./assets/img_data/2507.05197.json), skip HTML parsing.
[08.07.2025 22:11] Success.
[08.07.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2507.03483.
[08.07.2025 22:11] Extra JSON file exists (./assets/json/2507.03483.json), skip PDF parsing.
[08.07.2025 22:11] Paper image links file exists (./assets/img_data/2507.03483.json), skip HTML parsing.
[08.07.2025 22:11] Success.
[08.07.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2507.02029.
[08.07.2025 22:11] Extra JSON file exists (./assets/json/2507.02029.json), skip PDF parsing.
[08.07.2025 22:11] Paper image links file exists (./assets/img_data/2507.02029.json), skip HTML parsing.
[08.07.2025 22:11] Success.
[08.07.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2507.03253.
[08.07.2025 22:11] Extra JSON file exists (./assets/json/2507.03253.json), skip PDF parsing.
[08.07.2025 22:11] Paper image links file exists (./assets/img_data/2507.03253.json), skip HTML parsing.
[08.07.2025 22:11] Success.
[08.07.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2507.04009.
[08.07.2025 22:11] Extra JSON file exists (./assets/json/2507.04009.json), skip PDF parsing.
[08.07.2025 22:11] Paper image links file exists (./assets/img_data/2507.04009.json), skip HTML parsing.
[08.07.2025 22:11] Success.
[08.07.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2507.03745.
[08.07.2025 22:11] Extra JSON file exists (./assets/json/2507.03745.json), skip PDF parsing.
[08.07.2025 22:11] Paper image links file exists (./assets/img_data/2507.03745.json), skip HTML parsing.
[08.07.2025 22:11] Success.
[08.07.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2507.05108.
[08.07.2025 22:11] Extra JSON file exists (./assets/json/2507.05108.json), skip PDF parsing.
[08.07.2025 22:11] Paper image links file exists (./assets/img_data/2507.05108.json), skip HTML parsing.
[08.07.2025 22:11] Success.
[08.07.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2507.02659.
[08.07.2025 22:11] Extra JSON file exists (./assets/json/2507.02659.json), skip PDF parsing.
[08.07.2025 22:11] Paper image links file exists (./assets/img_data/2507.02659.json), skip HTML parsing.
[08.07.2025 22:11] Success.
[08.07.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2507.03683.
[08.07.2025 22:11] Extra JSON file exists (./assets/json/2507.03683.json), skip PDF parsing.
[08.07.2025 22:11] Paper image links file exists (./assets/img_data/2507.03683.json), skip HTML parsing.
[08.07.2025 22:11] Success.
[08.07.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2507.04952.
[08.07.2025 22:11] Extra JSON file exists (./assets/json/2507.04952.json), skip PDF parsing.
[08.07.2025 22:11] Paper image links file exists (./assets/img_data/2507.04952.json), skip HTML parsing.
[08.07.2025 22:11] Success.
[08.07.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2506.21884.
[08.07.2025 22:11] Extra JSON file exists (./assets/json/2506.21884.json), skip PDF parsing.
[08.07.2025 22:11] Paper image links file exists (./assets/img_data/2506.21884.json), skip HTML parsing.
[08.07.2025 22:11] Success.
[08.07.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2507.05257.
[08.07.2025 22:11] Extra JSON file exists (./assets/json/2507.05257.json), skip PDF parsing.
[08.07.2025 22:11] Paper image links file exists (./assets/img_data/2507.05257.json), skip HTML parsing.
[08.07.2025 22:11] Success.
[08.07.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2507.04590.
[08.07.2025 22:11] Extra JSON file exists (./assets/json/2507.04590.json), skip PDF parsing.
[08.07.2025 22:11] Paper image links file exists (./assets/img_data/2507.04590.json), skip HTML parsing.
[08.07.2025 22:11] Success.
[08.07.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2507.03607.
[08.07.2025 22:11] Extra JSON file exists (./assets/json/2507.03607.json), skip PDF parsing.
[08.07.2025 22:11] Paper image links file exists (./assets/img_data/2507.03607.json), skip HTML parsing.
[08.07.2025 22:11] Success.
[08.07.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2507.04036.
[08.07.2025 22:11] Extra JSON file exists (./assets/json/2507.04036.json), skip PDF parsing.
[08.07.2025 22:11] Paper image links file exists (./assets/img_data/2507.04036.json), skip HTML parsing.
[08.07.2025 22:11] Success.
[08.07.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2507.03033.
[08.07.2025 22:11] Extra JSON file exists (./assets/json/2507.03033.json), skip PDF parsing.
[08.07.2025 22:11] Paper image links file exists (./assets/img_data/2507.03033.json), skip HTML parsing.
[08.07.2025 22:11] Success.
[08.07.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2507.05259.
[08.07.2025 22:11] Extra JSON file exists (./assets/json/2507.05259.json), skip PDF parsing.
[08.07.2025 22:11] Paper image links file exists (./assets/img_data/2507.05259.json), skip HTML parsing.
[08.07.2025 22:11] Success.
[08.07.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2507.03336.
[08.07.2025 22:11] Extra JSON file exists (./assets/json/2507.03336.json), skip PDF parsing.
[08.07.2025 22:11] Paper image links file exists (./assets/img_data/2507.03336.json), skip HTML parsing.
[08.07.2025 22:11] Success.
[08.07.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2507.04642.
[08.07.2025 22:11] Downloading paper 2507.04642 from http://arxiv.org/pdf/2507.04642v1...
[08.07.2025 22:11] Extracting affiliations from text.
[08.07.2025 22:11] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"R1-RE: Cross-Domain Relationship Extraction with RLVR Runpeng Dai1 Tong Zheng2 Run Yang3 Hongtu Zhu1: 1University of North Carolina at Chapel Hill 2University of Maryland, College Park 3BiliBili {runpeng, htzhu}@email.unc.edu tzheng24@umd.edu yangrun@bilibili.com 5 2 0 J 7 ] . [ 1 2 4 6 4 0 . 7 0 5 2 : r a "
[08.07.2025 22:11] Response: ```python
["University of North Carolina at Chapel Hill", "University of Maryland, College Park", "BiliBili"]
```
[08.07.2025 22:11] Deleting PDF ./assets/pdf/2507.04642.pdf.
[08.07.2025 22:11] Success.
[08.07.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2507.04562.
[08.07.2025 22:11] Extra JSON file exists (./assets/json/2507.04562.json), skip PDF parsing.
[08.07.2025 22:11] Paper image links file exists (./assets/img_data/2507.04562.json), skip HTML parsing.
[08.07.2025 22:11] Success.
[08.07.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2507.04376.
[08.07.2025 22:11] Extra JSON file exists (./assets/json/2507.04376.json), skip PDF parsing.
[08.07.2025 22:11] Paper image links file exists (./assets/img_data/2507.04376.json), skip HTML parsing.
[08.07.2025 22:11] Success.
[08.07.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2507.04285.
[08.07.2025 22:11] Extra JSON file exists (./assets/json/2507.04285.json), skip PDF parsing.
[08.07.2025 22:11] Paper image links file exists (./assets/img_data/2507.04285.json), skip HTML parsing.
[08.07.2025 22:11] Success.
[08.07.2025 22:11] Enriching papers with extra data.
[08.07.2025 22:11] ********************************************************************************
[08.07.2025 22:11] Abstract 0. MemOS is proposed as a memory operating system for Large Language Models to enhance memory management, enabling efficient storage and retrieval, and facilitating continual learning and personalized modeling.  					AI-generated summary 				 Large Language Models (LLMs) have become an essential infras...
[08.07.2025 22:11] ********************************************************************************
[08.07.2025 22:11] Abstract 1. Learning high-quality text representations is fundamental to a wide range of NLP tasks. While encoder pretraining has traditionally relied on Masked Language Modeling (MLM), recent evidence suggests that decoder models pretrained with Causal Language Modeling (CLM) can be effectively repurposed as e...
[08.07.2025 22:11] ********************************************************************************
[08.07.2025 22:11] Abstract 2. A high-speed 4D capturing system using low FPS cameras with asynchronous capture and video-diffusion-based artifact correction enhances reconstruction quality.  					AI-generated summary 				 Reconstructing fast-dynamic scenes from multi-view videos is crucial for high-speed motion analysis and real...
[08.07.2025 22:11] ********************************************************************************
[08.07.2025 22:11] Abstract 3. DreamVLA improves robot manipulation through a VLA framework that incorporates world knowledge, dynamic-region guidance, and a diffusion-based transformer to ensure clear, disentangled representations for action planning.  					AI-generated summary 				 Recent advances in vision-language-action (VLA...
[08.07.2025 22:11] ********************************************************************************
[08.07.2025 22:11] Abstract 4. A scalable reward modeling method, Policy Discriminative Learning (POLAR), enhances reward model performance and generalizes robustly in reinforcement learning through policy comparison.  					AI-generated summary 				 We offer a novel perspective on reward modeling by formulating it as a policy dis...
[08.07.2025 22:11] ********************************************************************************
[08.07.2025 22:11] Abstract 5. A large-scale dataset and verification tool are introduced for assessing and improving cross-disciplinary reasoning capabilities in multimodal models.  					AI-generated summary 				 In this paper, we introduce BMMR, a large-scale bilingual, multimodal, multi-disciplinary reasoning dataset for the c...
[08.07.2025 22:11] ********************************************************************************
[08.07.2025 22:11] Abstract 6. RoboBrain 2.0, a vision-language foundation model, achieves top performance in embodied tasks through its heterogeneous architecture and multi-stage training strategies.  					AI-generated summary 				 We introduce RoboBrain 2.0, our latest generation of embodied vision-language foundation models, d...
[08.07.2025 22:11] ********************************************************************************
[08.07.2025 22:11] Abstract 7. RefineX is a scalable framework for improving the quality of large language model pre-training data through programmatic editing, yielding better performance than alternative methods across various downstream tasks.  					AI-generated summary 				 The foundational capabilities of large language mode...
[08.07.2025 22:11] ********************************************************************************
[08.07.2025 22:11] Abstract 8. A unified framework called Easy Dataset synthesizes fine-tuning data from unstructured documents using a GUI and LLMs, improving domain-specific performance of LLMs while maintaining general knowledge.  					AI-generated summary 				 Large language models (LLMs) have shown impressive performance on ...
[08.07.2025 22:11] ********************************************************************************
[08.07.2025 22:11] Abstract 9. A streaming video generation model named StreamDiT, based on transformer-based diffusion models, enables real-time video generation with high content consistency and visual quality.  					AI-generated summary 				 Recently, great progress has been achieved in text-to-video (T2V) generation by scalin...
[08.07.2025 22:11] ********************************************************************************
[08.07.2025 22:11] Abstract 10. Historical documents represent an invaluable cultural heritage, yet have undergone significant degradation over time through tears, water erosion, and oxidation. Existing Historical Document Restoration (HDR) methods primarily focus on single modality or limited-size restoration, failing to meet pra...
[08.07.2025 22:11] ********************************************************************************
[08.07.2025 22:11] Abstract 11. OmniDraft, a unified framework, addresses cross-vocabulary mismatch and improves decoding speed by allowing a single draft model to interact dynamically with diverse target models in online settings.  					AI-generated summary 				 Speculative decoding generally dictates having a small, efficient dr...
[08.07.2025 22:11] ********************************************************************************
[08.07.2025 22:11] Abstract 12. Visual embedding models often capture continuous, ordinal attributes along specific axes, enabling effective image ranking with minimal supervision.  					AI-generated summary 				 We study whether visual embedding models capture continuous, ordinal attributes along linear directions, which we term ...
[08.07.2025 22:11] ********************************************************************************
[08.07.2025 22:11] Abstract 13. ArtifactsBench, a novel benchmark and evaluation framework, automates the assessment of visual code generation quality using temporal screenshots and a multimodal language model judge.  					AI-generated summary 				 The generative capabilities of Large Language Models (LLMs) are rapidly expanding f...
[08.07.2025 22:11] ********************************************************************************
[08.07.2025 22:11] Abstract 14. A framework combining NeRF with spectral unmixing yields accurate material segmentation and editing through hyperspectral synthesis.  					AI-generated summary 				 Neural Radiance Field (NeRF)-based segmentation methods focus on object semantics and rely solely on RGB data, lacking intrinsic materi...
[08.07.2025 22:11] ********************************************************************************
[08.07.2025 22:11] Abstract 15. Recent benchmarks for Large Language Model (LLM) agents primarily focus on evaluating reasoning, planning, and execution capabilities, while another critical component-memory, encompassing how agents memorize, update, and retrieve long-term information-is under-evaluated due to the lack of benchmark...
[08.07.2025 22:11] ********************************************************************************
[08.07.2025 22:11] Abstract 16. A unified framework VLM2Vec-V2 is proposed for learning embeddings across diverse visual forms such as videos and documents, demonstrating strong performance on new tasks and improving upon existing benchmarks for images.  					AI-generated summary 				 Multimodal embedding models have been crucial ...
[08.07.2025 22:11] ********************************************************************************
[08.07.2025 22:11] Abstract 17. A transformer-based model predicts software vulnerability severity levels directly from text, enhancing triage efficiency and consistency.  					AI-generated summary 				 This paper presents VLAI, a transformer-based model that predicts software vulnerability severity levels directly from text descr...
[08.07.2025 22:11] ********************************************************************************
[08.07.2025 22:11] Abstract 18. A multimodal agent transforms documents into detailed presentation videos with audio, evaluated using a comprehensive framework involving vision-language models.  					AI-generated summary 				 We present PresentAgent, a multimodal agent that transforms long-form documents into narrated presentation...
[08.07.2025 22:11] ********************************************************************************
[08.07.2025 22:11] Abstract 19. A fine-tuned Llama 3.2 1B model using PEFT with LoRA in the browser improves medical transcription accuracy and reduces privacy and computational concerns.  					AI-generated summary 				 Background: Clinical documentation represents a significant burden for healthcare providers, with physicians spe...
[08.07.2025 22:11] ********************************************************************************
[08.07.2025 22:11] Abstract 20. X-Planner, a planning system utilizing a multimodal large language model, decomposes complex text-guided image editing instructions into precise sub-instructions, ensuring localized, identity-preserving edits and achieving top performance on established benchmarks.  					AI-generated summary 				 Re...
[08.07.2025 22:11] ********************************************************************************
[08.07.2025 22:11] Abstract 21. DiaFORGE is a disambiguation framework that enhances large language models' ability to invoke enterprise APIs accurately through dialogue synthesis, supervised fine-tuning, and real-world evaluation.  					AI-generated summary 				 Large language models (LLMs) are increasingly tasked with invoking e...
[08.07.2025 22:11] ********************************************************************************
[08.07.2025 22:11] Abstract 22. R1-RE, a reinforcement learning with verifiable reward framework, enhances out-of-domain robustness in relationship extraction by leveraging small language models' reasoning abilities.  					AI-generated summary 				 Relationship extraction (RE) is a core task in natural language processing. Traditi...
[08.07.2025 22:11] ********************************************************************************
[08.07.2025 22:11] Abstract 23. State-of-the-art large language models are evaluated on forecasting questions and show lower accuracy compared to human superforecasters.  					AI-generated summary 				 Large language models (LLMs) have demonstrated remarkable capabilities across diverse tasks, but their ability to forecast future ...
[08.07.2025 22:11] ********************************************************************************
[08.07.2025 22:11] Abstract 24. As Artificial Intelligence systems evolve from monolithic models to ecosystems of specialized agents, the need for standardized communication protocols becomes increasingly critical. This paper introduces MOD-X (Modular Open Decentralized eXchange), a novel architectural framework proposal for agent...
[08.07.2025 22:11] ********************************************************************************
[08.07.2025 22:11] Abstract 25. SeqTex leverages pretrained video foundation models to directly generate high-fidelity UV texture maps through a sequence generation approach, enhancing 3D texture generation with superior consistency and alignment.  					AI-generated summary 				 Training native 3D texture generative models remains...
[08.07.2025 22:11] Read previous papers.
[08.07.2025 22:11] Generating reviews via LLM API.
[08.07.2025 22:11] Using data from previous issue: {"categories": ["#training", "#agi", "#rag", "#long_context", "#optimization", "#data"], "emoji": "🧠", "ru": {"title": "MemOS: операционная система памяти для более умных и адаптивных языковых моделей", "desc": "MemOS - это операционная система памяти для больших языковых моделей (LLM), предложенная
[08.07.2025 22:11] Using data from previous issue: {"categories": ["#survey", "#optimization", "#training", "#dataset"], "emoji": "🧠", "ru": {"title": "Оптимизация предобучения энкодеров: синергия CLM и MLM", "desc": "Исследование сравнивает эффективность методов предобучения энкодеров на основе маскированного языкового моделирования (MLM) и каузаль
[08.07.2025 22:11] Using data from previous issue: {"categories": ["#optimization", "#diffusion", "#3d", "#video"], "emoji": "🎥", "ru": {"title": "Высокоскоростная 4D-съемка обычными камерами", "desc": "Предлагается система высокоскоростной 4D-съемки с использованием камер с низкой частотой кадров и асинхронным захватом. Система повышает эффективную
[08.07.2025 22:11] Using data from previous issue: {"categories": ["#robotics", "#reasoning", "#training", "#optimization", "#multimodal", "#diffusion", "#agents", "#games"], "emoji": "🤖", "ru": {"title": "DreamVLA: Интеллектуальное планирование действий робота на основе комплексного анализа окружающего мира", "desc": "DreamVLA - это новая система у
[08.07.2025 22:11] Using data from previous issue: {"categories": ["#optimization", "#rlhf", "#training", "#rl"], "emoji": "🎯", "ru": {"title": "POLAR: Революция в моделировании вознаграждений для обучения с подкреплением", "desc": "В статье представлен новый метод моделирования вознаграждений в обучении с подкреплением, названный Policy Discriminat
[08.07.2025 22:11] Using data from previous issue: {"categories": ["#multimodal", "#reasoning", "#dataset", "#benchmark", "#open_source", "#data"], "emoji": "🧠", "ru": {"title": "Новый инструмент для оценки мультидисциплинарных рассуждений ИИ", "desc": "Статья представляет BMMR - масштабный двуязычный мультимодальный датасет для оценки рассуждений в
[08.07.2025 22:11] Using data from previous issue: {"categories": ["#agi", "#reasoning", "#architecture", "#benchmark", "#training", "#agents", "#open_source"], "emoji": "🤖", "ru": {"title": "RoboBrain 2.0: Универсальный ИИ для воплощенных задач", "desc": "RoboBrain 2.0 - это новая модель искусственного интеллекта для воплощенных задач, объединяющая
[08.07.2025 22:11] Using data from previous issue: {"categories": ["#training", "#optimization", "#data"], "emoji": "✂️", "ru": {"title": "RefineX: хирургическая точность в улучшении данных для ИИ", "desc": "RefineX - это масштабируемый фреймворк для улучшения качества данных предобучения больших языковых моделей путем программного редактирования. О
[08.07.2025 22:11] Using data from previous issue: {"categories": ["#synthetic", "#data", "#dataset", "#open_source"], "emoji": "🧠", "ru": {"title": "Синтез данных для дообучения LLM с помощью Easy Dataset", "desc": "Easy Dataset - это унифицированный фреймворк для синтеза данных для дообучения больших языковых моделей (LLM) из неструктурированных д
[08.07.2025 22:11] Using data from previous issue: {"categories": ["#diffusion", "#video", "#games"], "emoji": "🎬", "ru": {"title": "StreamDiT: Реальновременная генерация видео с помощью ИИ", "desc": "StreamDiT - это модель генерации потокового видео, основанная на трансформерных диффузионных моделях. Она позволяет генерировать видео в реальном врем
[08.07.2025 22:11] Using data from previous issue: {"categories": ["#architecture", "#cv", "#multimodal", "#dataset", "#data"], "emoji": "📜", "ru": {"title": "AutoHDR: Возрождение истории через искусственный интеллект", "desc": "Эта статья представляет новый подход к автоматизированной реставрации исторических документов под названием AutoHDR. Автор
[08.07.2025 22:11] Using data from previous issue: {"categories": ["#optimization", "#games", "#inference", "#training", "#reasoning", "#multimodal"], "emoji": "🚀", "ru": {"title": "Один черновик для всех: ускорение и адаптация языковых моделей", "desc": "OmniDraft - это унифицированная система для ускорения работы языковых моделей. Она решает пробл
[08.07.2025 22:11] Using data from previous issue: {"categories": ["#cv", "#dataset"], "emoji": "📊", "ru": {"title": "Ранжирование изображений с минимальным надзором благодаря скрытой структуре встраиваний", "desc": "Исследование показывает, что визуальные модели встраивания часто захватывают непрерывные, порядковые атрибуты вдоль линейных направлен
[08.07.2025 22:11] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#games", "#open_source"], "emoji": "🖥️", "ru": {"title": "Автоматизированная оценка визуального кода с помощью мультимодальных языковых моделей", "desc": "ArtifactsBench - это новый фреймворк для автоматизированной оценки качества генерации визуального к
[08.07.2025 22:11] Using data from previous issue: {"categories": ["#robotics", "#cv", "#3d"], "emoji": "🌈", "ru": {"title": "Гиперспектральный NeRF для точной сегментации и редактирования материалов", "desc": "Статья представляет UnMix-NeRF - фреймворк, объединяющий нейронные радиальные поля (NeRF) со спектральным разложением для синтеза гиперспект
[08.07.2025 22:11] Using data from previous issue: {"categories": ["#long_context", "#reasoning", "#agents", "#benchmark", "#rag", "#dataset"], "emoji": "🧠", "ru": {"title": "Новый бенчмарк для оценки памяти LLM-агентов", "desc": "Статья представляет новый бенчмарк MemoryAgentBench для оценки агентов с механизмами памяти на основе больших языковых м
[08.07.2025 22:11] Using data from previous issue: {"categories": ["#games", "#rag", "#survey", "#benchmark", "#transfer_learning", "#multimodal"], "emoji": "🎥", "ru": {"title": "Единая модель эмбеддингов для всех визуальных форматов", "desc": "VLM2Vec-V2 - это унифицированная система для создания эмбеддингов различных визуальных форматов, включая в
[08.07.2025 22:11] Using data from previous issue: {"categories": ["#security", "#architecture", "#dataset", "#data", "#training", "#open_source"], "emoji": "🛡️", "ru": {"title": "Искусственный интеллект на страже кибербезопасности: автоматическая оценка уязвимостей ПО", "desc": "Представлена модель VLAI на основе трансформера, которая предсказывает
[08.07.2025 22:11] Using data from previous issue: {"categories": ["#cv", "#multimodal", "#agents", "#optimization", "#dataset", "#benchmark", "#games", "#interpretability"], "emoji": "🎥", "ru": {"title": "Искусственный интеллект создает презентации на уровне человека", "desc": "Статья представляет PresentAgent - мультимодального агента, преобразующ
[08.07.2025 22:11] Using data from previous issue: {"categories": ["#hallucinations", "#alignment", "#synthetic", "#inference", "#training", "#low_resource", "#healthcare"], "emoji": "🏥", "ru": {"title": "Эффективная и безопасная медицинская транскрипция с помощью ИИ в браузере", "desc": "В статье описывается разработка и оценка системы медицинской 
[08.07.2025 22:11] Using data from previous issue: {"categories": ["#dataset", "#reasoning", "#benchmark", "#multimodal", "#diffusion"], "emoji": "🎨", "ru": {"title": "X-Planner: умное планирование для точного редактирования изображений", "desc": "X-Planner - это система планирования на основе мультимодальной большой языковой модели для редактирован
[08.07.2025 22:11] Using data from previous issue: {"categories": ["#optimization", "#open_source", "#dataset", "#training", "#data", "#alignment", "#benchmark", "#agents"], "emoji": "🔧", "ru": {"title": "DiaFORGE: точные вызовы API через синтез диалогов и дообучение LLM", "desc": "DiaFORGE - это фреймворк для улучшения способности больших языковых 
[08.07.2025 22:11] Querying the API.
[08.07.2025 22:11] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

R1-RE, a reinforcement learning with verifiable reward framework, enhances out-of-domain robustness in relationship extraction by leveraging small language models' reasoning abilities.  					AI-generated summary 				 Relationship extraction (RE) is a core task in natural language processing. Traditional approaches typically frame RE as a supervised learning problem, directly mapping context to labels-an approach that often suffers from poor out-of-domain (OOD) generalization. Inspired by the workflow of human annotators, we reframe RE as a reasoning task guided by annotation guidelines and introduce R1-RE, the first reinforcement learning with verifiable reward (RLVR) framework for RE tasks. Our method elicits the reasoning abilities of small language models for annotation tasks, resulting in significantly improved OOD robustness. We evaluate our approach on the public Sem-2010 dataset and a private MDKG dataset. The R1-RE-7B model attains an average OOD accuracy of approximately 70%, on par with leading proprietary models such as GPT-4o. Additionally, our comprehensive analysis provides novel insights into the training dynamics and emergent reasoning behaviors of the RLVR paradigm for RE.
[08.07.2025 22:12] Response: {
  "desc": "Статья представляет R1-RE - новый фреймворк обучения с подкреплением для задачи извлечения отношений. R1-RE использует способности малых языковых моделей к рассуждению, что значительно улучшает обобщение вне домена. Метод переосмысливает извлечение отношений как задачу рассуждения, основанную на инструкциях по аннотации. Эксперименты показывают, что R1-RE-7B достигает точности вне домена около 70%, сравнимой с ведущими проприетарными моделями.",
  "emoji": "🧠",
  "title": "Улучшение обобщения в извлечении отношений через обучение с подкреплением"
}
[08.07.2025 22:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"R1-RE, a reinforcement learning with verifiable reward framework, enhances out-of-domain robustness in relationship extraction by leveraging small language models' reasoning abilities.  					AI-generated summary 				 Relationship extraction (RE) is a core task in natural language processing. Traditional approaches typically frame RE as a supervised learning problem, directly mapping context to labels-an approach that often suffers from poor out-of-domain (OOD) generalization. Inspired by the workflow of human annotators, we reframe RE as a reasoning task guided by annotation guidelines and introduce R1-RE, the first reinforcement learning with verifiable reward (RLVR) framework for RE tasks. Our method elicits the reasoning abilities of small language models for annotation tasks, resulting in significantly improved OOD robustness. We evaluate our approach on the public Sem-2010 dataset and a private MDKG dataset. The R1-RE-7B model attains an average OOD accuracy of approximately 70%, on par with leading proprietary models such as GPT-4o. Additionally, our comprehensive analysis provides novel insights into the training dynamics and emergent reasoning behaviors of the RLVR paradigm for RE."

[08.07.2025 22:12] Response: ```python
['RL', 'SMALL_MODELS', 'DATASET', 'TRAINING']
```
[08.07.2025 22:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"R1-RE, a reinforcement learning with verifiable reward framework, enhances out-of-domain robustness in relationship extraction by leveraging small language models' reasoning abilities.  					AI-generated summary 				 Relationship extraction (RE) is a core task in natural language processing. Traditional approaches typically frame RE as a supervised learning problem, directly mapping context to labels-an approach that often suffers from poor out-of-domain (OOD) generalization. Inspired by the workflow of human annotators, we reframe RE as a reasoning task guided by annotation guidelines and introduce R1-RE, the first reinforcement learning with verifiable reward (RLVR) framework for RE tasks. Our method elicits the reasoning abilities of small language models for annotation tasks, resulting in significantly improved OOD robustness. We evaluate our approach on the public Sem-2010 dataset and a private MDKG dataset. The R1-RE-7B model attains an average OOD accuracy of approximately 70%, on par with leading proprietary models such as GPT-4o. Additionally, our comprehensive analysis provides novel insights into the training dynamics and emergent reasoning behaviors of the RLVR paradigm for RE."

[08.07.2025 22:12] Response: ```python
['REASONING', 'OPTIMIZATION']
```
[08.07.2025 22:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The paper introduces R1-RE, a novel framework that applies reinforcement learning with verifiable rewards to improve relationship extraction (RE) in natural language processing. Traditional RE methods struggle with generalizing to new, unseen data, but R1-RE reframes the task as a reasoning challenge, similar to how human annotators work. By utilizing the reasoning capabilities of small language models, this approach enhances robustness against out-of-domain scenarios. The results show that R1-RE achieves competitive accuracy on benchmark datasets, providing valuable insights into the training dynamics of reinforcement learning in RE tasks.","title":"Enhancing Relationship Extraction with Reinforcement Learning and Reasoning"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='The paper introduces R1-RE, a novel framework that applies reinforcement learning with verifiable rewards to improve relationship extraction (RE) in natural language processing. Traditional RE methods struggle with generalizing to new, unseen data, but R1-RE reframes the task as a reasoning challenge, similar to how human annotators work. By utilizing the reasoning capabilities of small language models, this approach enhances robustness against out-of-domain scenarios. The results show that R1-RE achieves competitive accuracy on benchmark datasets, providing valuable insights into the training dynamics of reinforcement learning in RE tasks.', title='Enhancing Relationship Extraction with Reinforcement Learning and Reasoning'))
[08.07.2025 22:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"R1-RE是一种强化学习框架，旨在提高关系提取任务在域外的鲁棒性。该方法通过利用小型语言模型的推理能力，将关系提取重新定义为一个推理任务，而不是传统的监督学习问题。通过引入可验证奖励的强化学习机制，R1-RE显著提升了模型在未见数据上的表现。我们的实验表明，R1-RE-7B模型在Sem-2010和MDKG数据集上达到了约70%的域外准确率，表现与领先的专有模型相当。","title":"提升关系提取的域外鲁棒性"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='R1-RE是一种强化学习框架，旨在提高关系提取任务在域外的鲁棒性。该方法通过利用小型语言模型的推理能力，将关系提取重新定义为一个推理任务，而不是传统的监督学习问题。通过引入可验证奖励的强化学习机制，R1-RE显著提升了模型在未见数据上的表现。我们的实验表明，R1-RE-7B模型在Sem-2010和MDKG数据集上达到了约70%的域外准确率，表现与领先的专有模型相当。', title='提升关系提取的域外鲁棒性'))
[08.07.2025 22:12] Using data from previous issue: {"categories": ["#benchmark", "#reasoning"], "emoji": "🔮", "ru": {"title": "Искусственный интеллект vs суперпрогнозисты: битва предсказателей будущего", "desc": "Современные большие языковые модели (LLM) были протестированы на задачах прогнозирования будущих событий. Результаты показали, что LLM дос
[08.07.2025 22:12] Using data from previous issue: {"categories": ["#optimization", "#interpretability", "#agi", "#agents", "#architecture"], "emoji": "🔀", "ru": {"title": "MOD-X: Новый стандарт для взаимодействия ИИ-агентов", "desc": "Статья представляет MOD-X - новую архитектурную концепцию для обеспечения взаимодействия между ИИ-агентами. MOD-X п
[08.07.2025 22:12] Using data from previous issue: {"categories": ["#architecture", "#3d", "#synthetic", "#optimization"], "emoji": "🎨", "ru": {"title": "SeqTex: прямая генерация UV-текстур с помощью видео-моделей", "desc": "SeqTex - это новый метод генерации текстур для 3D-моделей, использующий предобученные видео-модели. Он генерирует UV-развертки
[08.07.2025 22:12] Renaming data file.
[08.07.2025 22:12] Renaming previous data. hf_papers.json to ./d/2025-07-08.json
[08.07.2025 22:12] Saving new data file.
[08.07.2025 22:12] Generating page.
[08.07.2025 22:12] Renaming previous page.
[08.07.2025 22:12] Renaming previous data. index.html to ./d/2025-07-08.html
[08.07.2025 22:12] Writing result.
[08.07.2025 22:12] Renaming log file.
[08.07.2025 22:12] Renaming previous data. log.txt to ./logs/2025-07-08_last_log.txt
