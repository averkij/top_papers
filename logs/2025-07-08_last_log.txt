[08.07.2025 06:19] Read previous papers.
[08.07.2025 06:19] Generating top page (month).
[08.07.2025 06:19] Writing top page (month).
[08.07.2025 07:13] Read previous papers.
[08.07.2025 07:13] Get feed.
[08.07.2025 07:13] Get page data from previous paper. URL: https://huggingface.co/papers/2507.03724
[08.07.2025 07:13] Get page data from previous paper. URL: https://huggingface.co/papers/2507.05163
[08.07.2025 07:13] Get page data from previous paper. URL: https://huggingface.co/papers/2507.05197
[08.07.2025 07:13] Get page data from previous paper. URL: https://huggingface.co/papers/2507.03483
[08.07.2025 07:13] Get page data from previous paper. URL: https://huggingface.co/papers/2507.04447
[08.07.2025 07:13] Get page data from previous paper. URL: https://huggingface.co/papers/2507.03253
[08.07.2025 07:13] Get page data from previous paper. URL: https://huggingface.co/papers/2507.03745
[08.07.2025 07:13] Get page data from previous paper. URL: https://huggingface.co/papers/2507.04952
[08.07.2025 07:13] Get page data from previous paper. URL: https://huggingface.co/papers/2507.04590
[08.07.2025 07:13] Get page data from previous paper. URL: https://huggingface.co/papers/2507.05259
[08.07.2025 07:13] Get page data from previous paper. URL: https://huggingface.co/papers/2507.04562
[08.07.2025 07:13] Get page data from previous paper. URL: https://huggingface.co/papers/2507.04376
[08.07.2025 07:13] Get page data from previous paper. URL: https://huggingface.co/papers/2507.04036
[08.07.2025 07:13] Get page data from previous paper. URL: https://huggingface.co/papers/2507.03336
[08.07.2025 07:13] Get page data from previous paper. URL: https://huggingface.co/papers/2507.02659
[08.07.2025 07:13] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[08.07.2025 07:13] No deleted papers detected.
[08.07.2025 07:13] Downloading and parsing papers (pdf, html). Total: 15.
[08.07.2025 07:13] Downloading and parsing paper https://huggingface.co/papers/2507.03724.
[08.07.2025 07:13] Extra JSON file exists (./assets/json/2507.03724.json), skip PDF parsing.
[08.07.2025 07:13] Paper image links file exists (./assets/img_data/2507.03724.json), skip HTML parsing.
[08.07.2025 07:13] Success.
[08.07.2025 07:13] Downloading and parsing paper https://huggingface.co/papers/2507.05163.
[08.07.2025 07:13] Extra JSON file exists (./assets/json/2507.05163.json), skip PDF parsing.
[08.07.2025 07:13] Paper image links file exists (./assets/img_data/2507.05163.json), skip HTML parsing.
[08.07.2025 07:13] Success.
[08.07.2025 07:13] Downloading and parsing paper https://huggingface.co/papers/2507.05197.
[08.07.2025 07:13] Extra JSON file exists (./assets/json/2507.05197.json), skip PDF parsing.
[08.07.2025 07:13] Paper image links file exists (./assets/img_data/2507.05197.json), skip HTML parsing.
[08.07.2025 07:13] Success.
[08.07.2025 07:13] Downloading and parsing paper https://huggingface.co/papers/2507.03483.
[08.07.2025 07:13] Extra JSON file exists (./assets/json/2507.03483.json), skip PDF parsing.
[08.07.2025 07:13] Paper image links file exists (./assets/img_data/2507.03483.json), skip HTML parsing.
[08.07.2025 07:13] Success.
[08.07.2025 07:13] Downloading and parsing paper https://huggingface.co/papers/2507.04447.
[08.07.2025 07:13] Extra JSON file exists (./assets/json/2507.04447.json), skip PDF parsing.
[08.07.2025 07:13] Paper image links file exists (./assets/img_data/2507.04447.json), skip HTML parsing.
[08.07.2025 07:13] Success.
[08.07.2025 07:13] Downloading and parsing paper https://huggingface.co/papers/2507.03253.
[08.07.2025 07:13] Extra JSON file exists (./assets/json/2507.03253.json), skip PDF parsing.
[08.07.2025 07:13] Paper image links file exists (./assets/img_data/2507.03253.json), skip HTML parsing.
[08.07.2025 07:13] Success.
[08.07.2025 07:13] Downloading and parsing paper https://huggingface.co/papers/2507.03745.
[08.07.2025 07:13] Extra JSON file exists (./assets/json/2507.03745.json), skip PDF parsing.
[08.07.2025 07:13] Paper image links file exists (./assets/img_data/2507.03745.json), skip HTML parsing.
[08.07.2025 07:13] Success.
[08.07.2025 07:13] Downloading and parsing paper https://huggingface.co/papers/2507.04952.
[08.07.2025 07:13] Extra JSON file exists (./assets/json/2507.04952.json), skip PDF parsing.
[08.07.2025 07:13] Paper image links file exists (./assets/img_data/2507.04952.json), skip HTML parsing.
[08.07.2025 07:13] Success.
[08.07.2025 07:13] Downloading and parsing paper https://huggingface.co/papers/2507.04590.
[08.07.2025 07:13] Extra JSON file exists (./assets/json/2507.04590.json), skip PDF parsing.
[08.07.2025 07:13] Paper image links file exists (./assets/img_data/2507.04590.json), skip HTML parsing.
[08.07.2025 07:13] Success.
[08.07.2025 07:13] Downloading and parsing paper https://huggingface.co/papers/2507.05259.
[08.07.2025 07:13] Extra JSON file exists (./assets/json/2507.05259.json), skip PDF parsing.
[08.07.2025 07:13] Paper image links file exists (./assets/img_data/2507.05259.json), skip HTML parsing.
[08.07.2025 07:13] Success.
[08.07.2025 07:13] Downloading and parsing paper https://huggingface.co/papers/2507.04562.
[08.07.2025 07:13] Extra JSON file exists (./assets/json/2507.04562.json), skip PDF parsing.
[08.07.2025 07:13] Paper image links file exists (./assets/img_data/2507.04562.json), skip HTML parsing.
[08.07.2025 07:13] Success.
[08.07.2025 07:13] Downloading and parsing paper https://huggingface.co/papers/2507.04376.
[08.07.2025 07:13] Extra JSON file exists (./assets/json/2507.04376.json), skip PDF parsing.
[08.07.2025 07:13] Paper image links file exists (./assets/img_data/2507.04376.json), skip HTML parsing.
[08.07.2025 07:13] Success.
[08.07.2025 07:13] Downloading and parsing paper https://huggingface.co/papers/2507.04036.
[08.07.2025 07:13] Extra JSON file exists (./assets/json/2507.04036.json), skip PDF parsing.
[08.07.2025 07:13] Paper image links file exists (./assets/img_data/2507.04036.json), skip HTML parsing.
[08.07.2025 07:13] Success.
[08.07.2025 07:13] Downloading and parsing paper https://huggingface.co/papers/2507.03336.
[08.07.2025 07:13] Extra JSON file exists (./assets/json/2507.03336.json), skip PDF parsing.
[08.07.2025 07:13] Paper image links file exists (./assets/img_data/2507.03336.json), skip HTML parsing.
[08.07.2025 07:13] Success.
[08.07.2025 07:13] Downloading and parsing paper https://huggingface.co/papers/2507.02659.
[08.07.2025 07:13] Extra JSON file exists (./assets/json/2507.02659.json), skip PDF parsing.
[08.07.2025 07:13] Paper image links file exists (./assets/img_data/2507.02659.json), skip HTML parsing.
[08.07.2025 07:13] Success.
[08.07.2025 07:13] Enriching papers with extra data.
[08.07.2025 07:13] ********************************************************************************
[08.07.2025 07:13] Abstract 0. MemOS is proposed as a memory operating system for Large Language Models to enhance memory management, enabling efficient storage and retrieval, and facilitating continual learning and personalized modeling.  					AI-generated summary 				 Large Language Models (LLMs) have become an essential infras...
[08.07.2025 07:13] ********************************************************************************
[08.07.2025 07:13] Abstract 1. A high-speed 4D capturing system using low FPS cameras with asynchronous capture and video-diffusion-based artifact correction enhances reconstruction quality.  					AI-generated summary 				 Reconstructing fast-dynamic scenes from multi-view videos is crucial for high-speed motion analysis and real...
[08.07.2025 07:13] ********************************************************************************
[08.07.2025 07:13] Abstract 2. A scalable reward modeling method, Policy Discriminative Learning (POLAR), enhances reward model performance and generalizes robustly in reinforcement learning through policy comparison.  					AI-generated summary 				 We offer a novel perspective on reward modeling by formulating it as a policy dis...
[08.07.2025 07:13] ********************************************************************************
[08.07.2025 07:13] Abstract 3. A large-scale dataset and verification tool are introduced for assessing and improving cross-disciplinary reasoning capabilities in multimodal models.  					AI-generated summary 				 In this paper, we introduce BMMR, a large-scale bilingual, multimodal, multi-disciplinary reasoning dataset for the c...
[08.07.2025 07:13] ********************************************************************************
[08.07.2025 07:13] Abstract 4. DreamVLA improves robot manipulation through a VLA framework that incorporates world knowledge, dynamic-region guidance, and a diffusion-based transformer to ensure clear, disentangled representations for action planning.  					AI-generated summary 				 Recent advances in vision-language-action (VLA...
[08.07.2025 07:13] ********************************************************************************
[08.07.2025 07:13] Abstract 5. RefineX is a scalable framework for improving the quality of large language model pre-training data through programmatic editing, yielding better performance than alternative methods across various downstream tasks.  					AI-generated summary 				 The foundational capabilities of large language mode...
[08.07.2025 07:13] ********************************************************************************
[08.07.2025 07:13] Abstract 6. A streaming video generation model named StreamDiT, based on transformer-based diffusion models, enables real-time video generation with high content consistency and visual quality.  					AI-generated summary 				 Recently, great progress has been achieved in text-to-video (T2V) generation by scalin...
[08.07.2025 07:13] ********************************************************************************
[08.07.2025 07:13] Abstract 7. ArtifactsBench, a novel benchmark and evaluation framework, automates the assessment of visual code generation quality using temporal screenshots and a multimodal language model judge.  					AI-generated summary 				 The generative capabilities of Large Language Models (LLMs) are rapidly expanding f...
[08.07.2025 07:13] ********************************************************************************
[08.07.2025 07:13] Abstract 8. A unified framework VLM2Vec-V2 is proposed for learning embeddings across diverse visual forms such as videos and documents, demonstrating strong performance on new tasks and improving upon existing benchmarks for images.  					AI-generated summary 				 Multimodal embedding models have been crucial ...
[08.07.2025 07:13] ********************************************************************************
[08.07.2025 07:13] Abstract 9. X-Planner, a planning system utilizing a multimodal large language model, decomposes complex text-guided image editing instructions into precise sub-instructions, ensuring localized, identity-preserving edits and achieving top performance on established benchmarks.  					AI-generated summary 				 Re...
[08.07.2025 07:13] ********************************************************************************
[08.07.2025 07:13] Abstract 10. State-of-the-art large language models are evaluated on forecasting questions and show lower accuracy compared to human superforecasters.  					AI-generated summary 				 Large language models (LLMs) have demonstrated remarkable capabilities across diverse tasks, but their ability to forecast future ...
[08.07.2025 07:13] ********************************************************************************
[08.07.2025 07:13] Abstract 11. As Artificial Intelligence systems evolve from monolithic models to ecosystems of specialized agents, the need for standardized communication protocols becomes increasingly critical. This paper introduces MOD-X (Modular Open Decentralized eXchange), a novel architectural framework proposal for agent...
[08.07.2025 07:13] ********************************************************************************
[08.07.2025 07:13] Abstract 12. A multimodal agent transforms documents into detailed presentation videos with audio, evaluated using a comprehensive framework involving vision-language models.  					AI-generated summary 				 We present PresentAgent, a multimodal agent that transforms long-form documents into narrated presentation...
[08.07.2025 07:13] ********************************************************************************
[08.07.2025 07:13] Abstract 13. DiaFORGE is a disambiguation framework that enhances large language models' ability to invoke enterprise APIs accurately through dialogue synthesis, supervised fine-tuning, and real-world evaluation.  					AI-generated summary 				 Large language models (LLMs) are increasingly tasked with invoking e...
[08.07.2025 07:13] ********************************************************************************
[08.07.2025 07:13] Abstract 14. OmniDraft, a unified framework, addresses cross-vocabulary mismatch and improves decoding speed by allowing a single draft model to interact dynamically with diverse target models in online settings.  					AI-generated summary 				 Speculative decoding generally dictates having a small, efficient dr...
[08.07.2025 07:13] Read previous papers.
[08.07.2025 07:13] Generating reviews via LLM API.
[08.07.2025 07:13] Using data from previous issue: {"categories": ["#training", "#agi", "#rag", "#long_context", "#optimization", "#data"], "emoji": "üß†", "ru": {"title": "MemOS: –æ–ø–µ—Ä–∞—Ü–∏–æ–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –ø–∞–º—è—Ç–∏ –¥–ª—è –±–æ–ª–µ–µ —É–º–Ω—ã—Ö –∏ –∞–¥–∞–ø—Ç–∏–≤–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "MemOS - —ç—Ç–æ –æ–ø–µ—Ä–∞—Ü–∏–æ–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –ø–∞–º—è—Ç–∏ –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM), –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–∞—è
[08.07.2025 07:13] Using data from previous issue: {"categories": ["#optimization", "#diffusion", "#3d", "#video"], "emoji": "üé•", "ru": {"title": "–í—ã—Å–æ–∫–æ—Å–∫–æ—Ä–æ—Å—Ç–Ω–∞—è 4D-—Å—ä–µ–º–∫–∞ –æ–±—ã—á–Ω—ã–º–∏ –∫–∞–º–µ—Ä–∞–º–∏", "desc": "–ü—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è —Å–∏—Å—Ç–µ–º–∞ –≤—ã—Å–æ–∫–æ—Å–∫–æ—Ä–æ—Å—Ç–Ω–æ–π 4D-—Å—ä–µ–º–∫–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∫–∞–º–µ—Ä —Å –Ω–∏–∑–∫–æ–π —á–∞—Å—Ç–æ—Ç–æ–π –∫–∞–¥—Ä–æ–≤ –∏ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–º –∑–∞—Ö–≤–∞—Ç–æ–º. –°–∏—Å—Ç–µ–º–∞ –ø–æ–≤—ã—à–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—É—é
[08.07.2025 07:13] Using data from previous issue: {"categories": ["#optimization", "#rlhf", "#training", "#rl"], "emoji": "üéØ", "ru": {"title": "POLAR: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–π –≤ –æ–±—É—á–µ–Ω–∏–∏ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –Ω–∞–∑–≤–∞–Ω–Ω—ã–π Policy Discriminat
[08.07.2025 07:13] Using data from previous issue: {"categories": ["#multimodal", "#reasoning", "#dataset", "#benchmark", "#open_source", "#data"], "emoji": "üß†", "ru": {"title": "–ù–æ–≤—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º—É–ª—å—Ç–∏–¥–∏—Å—Ü–∏–ø–ª–∏–Ω–∞—Ä–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –ò–ò", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç BMMR - –º–∞—Å—à—Ç–∞–±–Ω—ã–π –¥–≤—É—è–∑—ã—á–Ω—ã–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤
[08.07.2025 07:13] Using data from previous issue: {"categories": ["#robotics", "#reasoning", "#training", "#optimization", "#multimodal", "#diffusion", "#agents", "#games"], "emoji": "ü§ñ", "ru": {"title": "DreamVLA: –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–µ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–µ–π—Å—Ç–≤–∏–π —Ä–æ–±–æ—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –æ–∫—Ä—É–∂–∞—é—â–µ–≥–æ –º–∏—Ä–∞", "desc": "DreamVLA - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ —É
[08.07.2025 07:13] Using data from previous issue: {"categories": ["#training", "#optimization", "#data"], "emoji": "‚úÇÔ∏è", "ru": {"title": "RefineX: —Ö–∏—Ä—É—Ä–≥–∏—á–µ—Å–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –≤ —É–ª—É—á—à–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ò–ò", "desc": "RefineX - —ç—Ç–æ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø—É—Ç–µ–º –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–≥–æ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è. –û
[08.07.2025 07:13] Using data from previous issue: {"categories": ["#diffusion", "#video", "#games"], "emoji": "üé¨", "ru": {"title": "StreamDiT: –†–µ–∞–ª—å–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é –ò–ò", "desc": "StreamDiT - —ç—Ç–æ –º–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ—Ç–æ–∫–æ–≤–æ–≥–æ –≤–∏–¥–µ–æ, –æ—Å–Ω–æ–≤–∞–Ω–Ω–∞—è –Ω–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–Ω—ã—Ö –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö. –û–Ω–∞ –ø–æ–∑–≤–æ–ª—è–µ—Ç –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –≤–∏–¥–µ–æ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º
[08.07.2025 07:13] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#games", "#open_source"], "emoji": "üñ•Ô∏è", "ru": {"title": "–ê–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –∫–æ–¥–∞ —Å –ø–æ–º–æ—â—å—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "ArtifactsBench - —ç—Ç–æ –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –∫
[08.07.2025 07:13] Using data from previous issue: {"categories": ["#games", "#rag", "#survey", "#benchmark", "#transfer_learning", "#multimodal"], "emoji": "üé•", "ru": {"title": "–ï–¥–∏–Ω–∞—è –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è –≤—Å–µ—Ö –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤", "desc": "VLM2Vec-V2 - —ç—Ç–æ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤, –≤–∫–ª—é—á–∞—è –≤
[08.07.2025 07:13] Using data from previous issue: {"categories": ["#dataset", "#reasoning", "#benchmark", "#multimodal", "#diffusion"], "emoji": "üé®", "ru": {"title": "X-Planner: —É–º–Ω–æ–µ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "X-Planner - —ç—Ç–æ —Å–∏—Å—Ç–µ–º–∞ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–π –±–æ–ª—å—à–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω
[08.07.2025 07:13] Using data from previous issue: {"categories": ["#benchmark", "#reasoning"], "emoji": "üîÆ", "ru": {"title": "–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç vs —Å—É–ø–µ—Ä–ø—Ä–æ–≥–Ω–æ–∑–∏—Å—Ç—ã: –±–∏—Ç–≤–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç–µ–ª–µ–π –±—É–¥—É—â–µ–≥–æ", "desc": "–°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (LLM) –±—ã–ª–∏ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω—ã –Ω–∞ –∑–∞–¥–∞—á–∞—Ö –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –±—É–¥—É—â–∏—Ö —Å–æ–±—ã—Ç–∏–π. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ LLM –¥–æ—Å
[08.07.2025 07:13] Using data from previous issue: {"categories": ["#optimization", "#interpretability", "#agi", "#agents", "#architecture"], "emoji": "üîÄ", "ru": {"title": "MOD-X: –ù–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç MOD-X - –Ω–æ–≤—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—É—é –∫–æ–Ω—Ü–µ–ø—Ü–∏—é –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –º–µ–∂–¥—É –ò–ò-–∞–≥–µ–Ω—Ç–∞–º–∏. MOD-X –ø
[08.07.2025 07:13] Using data from previous issue: {"categories": ["#cv", "#multimodal", "#agents", "#optimization", "#dataset", "#benchmark", "#games", "#interpretability"], "emoji": "üé•", "ru": {"title": "–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç —Å–æ–∑–¥–∞–µ—Ç –ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏–∏ –Ω–∞ —É—Ä–æ–≤–Ω–µ —á–µ–ª–æ–≤–µ–∫–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç PresentAgent - –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –∞–≥–µ–Ω—Ç–∞, –ø—Ä–µ–æ–±—Ä–∞–∑—É—é—â
[08.07.2025 07:13] Using data from previous issue: {"categories": ["#optimization", "#open_source", "#dataset", "#training", "#data", "#alignment", "#benchmark", "#agents"], "emoji": "üîß", "ru": {"title": "DiaFORGE: —Ç–æ—á–Ω—ã–µ –≤—ã–∑–æ–≤—ã API —á–µ—Ä–µ–∑ —Å–∏–Ω—Ç–µ–∑ –¥–∏–∞–ª–æ–≥–æ–≤ –∏ –¥–æ–æ–±—É—á–µ–Ω–∏–µ LLM", "desc": "DiaFORGE - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö 
[08.07.2025 07:13] Using data from previous issue: {"categories": ["#optimization", "#games", "#inference", "#training", "#reasoning", "#multimodal"], "emoji": "üöÄ", "ru": {"title": "–û–¥–∏–Ω —á–µ—Ä–Ω–æ–≤–∏–∫ –¥–ª—è –≤—Å–µ—Ö: —É—Å–∫–æ—Ä–µ–Ω–∏–µ –∏ –∞–¥–∞–ø—Ç–∞—Ü–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "OmniDraft - —ç—Ç–æ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è —Ä–∞–±–æ—Ç—ã —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –û–Ω–∞ —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª
[08.07.2025 07:13] Renaming data file.
[08.07.2025 07:13] Renaming previous data. hf_papers.json to ./d/2025-07-08.json
[08.07.2025 07:13] Saving new data file.
[08.07.2025 07:13] Generating page.
[08.07.2025 07:13] Renaming previous page.
[08.07.2025 07:13] Renaming previous data. index.html to ./d/2025-07-08.html
[08.07.2025 07:13] Writing result.
[08.07.2025 07:13] Renaming log file.
[08.07.2025 07:13] Renaming previous data. log.txt to ./logs/2025-07-08_last_log.txt
