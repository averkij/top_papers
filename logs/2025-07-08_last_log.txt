[08.07.2025 18:17] Read previous papers.
[08.07.2025 18:17] Generating top page (month).
[08.07.2025 18:17] Writing top page (month).
[08.07.2025 19:10] Read previous papers.
[08.07.2025 19:10] Get feed.
[08.07.2025 19:10] Get page data from previous paper. URL: https://huggingface.co/papers/2507.03724
[08.07.2025 19:10] Get page data from previous paper. URL: https://huggingface.co/papers/2507.00994
[08.07.2025 19:10] Get page data from previous paper. URL: https://huggingface.co/papers/2507.05163
[08.07.2025 19:10] Get page data from previous paper. URL: https://huggingface.co/papers/2507.04447
[08.07.2025 19:10] Get page data from previous paper. URL: https://huggingface.co/papers/2507.05197
[08.07.2025 19:10] Get page data from previous paper. URL: https://huggingface.co/papers/2507.03483
[08.07.2025 19:10] Get page data from previous paper. URL: https://huggingface.co/papers/2507.02029
[08.07.2025 19:10] Get page data from previous paper. URL: https://huggingface.co/papers/2507.03253
[08.07.2025 19:10] Get page data from previous paper. URL: https://huggingface.co/papers/2507.04009
[08.07.2025 19:10] Get page data from previous paper. URL: https://huggingface.co/papers/2507.03745
[08.07.2025 19:10] Get page data from previous paper. URL: https://huggingface.co/papers/2507.05108
[08.07.2025 19:10] Get page data from previous paper. URL: https://huggingface.co/papers/2507.02659
[08.07.2025 19:10] Get page data from previous paper. URL: https://huggingface.co/papers/2507.03683
[08.07.2025 19:10] Get page data from previous paper. URL: https://huggingface.co/papers/2507.04952
[08.07.2025 19:10] Get page data from previous paper. URL: https://huggingface.co/papers/2506.21884
[08.07.2025 19:10] Get page data from previous paper. URL: https://huggingface.co/papers/2507.03607
[08.07.2025 19:10] Get page data from previous paper. URL: https://huggingface.co/papers/2507.04590
[08.07.2025 19:10] Get page data from previous paper. URL: https://huggingface.co/papers/2507.04036
[08.07.2025 19:10] Get page data from previous paper. URL: https://huggingface.co/papers/2507.03033
[08.07.2025 19:10] Get page data from previous paper. URL: https://huggingface.co/papers/2507.05259
[08.07.2025 19:10] Get page data from previous paper. URL: https://huggingface.co/papers/2507.03336
[08.07.2025 19:10] Get page data from previous paper. URL: https://huggingface.co/papers/2507.04562
[08.07.2025 19:10] Get page data from previous paper. URL: https://huggingface.co/papers/2507.04376
[08.07.2025 19:10] Get page data from previous paper. URL: https://huggingface.co/papers/2507.04285
[08.07.2025 19:10] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[08.07.2025 19:10] No deleted papers detected.
[08.07.2025 19:10] Downloading and parsing papers (pdf, html). Total: 24.
[08.07.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2507.03724.
[08.07.2025 19:10] Extra JSON file exists (./assets/json/2507.03724.json), skip PDF parsing.
[08.07.2025 19:10] Paper image links file exists (./assets/img_data/2507.03724.json), skip HTML parsing.
[08.07.2025 19:10] Success.
[08.07.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2507.00994.
[08.07.2025 19:10] Extra JSON file exists (./assets/json/2507.00994.json), skip PDF parsing.
[08.07.2025 19:10] Paper image links file exists (./assets/img_data/2507.00994.json), skip HTML parsing.
[08.07.2025 19:10] Success.
[08.07.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2507.05163.
[08.07.2025 19:10] Extra JSON file exists (./assets/json/2507.05163.json), skip PDF parsing.
[08.07.2025 19:10] Paper image links file exists (./assets/img_data/2507.05163.json), skip HTML parsing.
[08.07.2025 19:10] Success.
[08.07.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2507.04447.
[08.07.2025 19:10] Extra JSON file exists (./assets/json/2507.04447.json), skip PDF parsing.
[08.07.2025 19:10] Paper image links file exists (./assets/img_data/2507.04447.json), skip HTML parsing.
[08.07.2025 19:10] Success.
[08.07.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2507.05197.
[08.07.2025 19:10] Extra JSON file exists (./assets/json/2507.05197.json), skip PDF parsing.
[08.07.2025 19:10] Paper image links file exists (./assets/img_data/2507.05197.json), skip HTML parsing.
[08.07.2025 19:10] Success.
[08.07.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2507.03483.
[08.07.2025 19:10] Extra JSON file exists (./assets/json/2507.03483.json), skip PDF parsing.
[08.07.2025 19:10] Paper image links file exists (./assets/img_data/2507.03483.json), skip HTML parsing.
[08.07.2025 19:10] Success.
[08.07.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2507.02029.
[08.07.2025 19:10] Extra JSON file exists (./assets/json/2507.02029.json), skip PDF parsing.
[08.07.2025 19:10] Paper image links file exists (./assets/img_data/2507.02029.json), skip HTML parsing.
[08.07.2025 19:10] Success.
[08.07.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2507.03253.
[08.07.2025 19:10] Extra JSON file exists (./assets/json/2507.03253.json), skip PDF parsing.
[08.07.2025 19:10] Paper image links file exists (./assets/img_data/2507.03253.json), skip HTML parsing.
[08.07.2025 19:10] Success.
[08.07.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2507.04009.
[08.07.2025 19:10] Extra JSON file exists (./assets/json/2507.04009.json), skip PDF parsing.
[08.07.2025 19:10] Paper image links file exists (./assets/img_data/2507.04009.json), skip HTML parsing.
[08.07.2025 19:10] Success.
[08.07.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2507.03745.
[08.07.2025 19:10] Extra JSON file exists (./assets/json/2507.03745.json), skip PDF parsing.
[08.07.2025 19:10] Paper image links file exists (./assets/img_data/2507.03745.json), skip HTML parsing.
[08.07.2025 19:10] Success.
[08.07.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2507.05108.
[08.07.2025 19:10] Extra JSON file exists (./assets/json/2507.05108.json), skip PDF parsing.
[08.07.2025 19:10] Paper image links file exists (./assets/img_data/2507.05108.json), skip HTML parsing.
[08.07.2025 19:10] Success.
[08.07.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2507.02659.
[08.07.2025 19:10] Extra JSON file exists (./assets/json/2507.02659.json), skip PDF parsing.
[08.07.2025 19:10] Paper image links file exists (./assets/img_data/2507.02659.json), skip HTML parsing.
[08.07.2025 19:10] Success.
[08.07.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2507.03683.
[08.07.2025 19:10] Extra JSON file exists (./assets/json/2507.03683.json), skip PDF parsing.
[08.07.2025 19:10] Paper image links file exists (./assets/img_data/2507.03683.json), skip HTML parsing.
[08.07.2025 19:10] Success.
[08.07.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2507.04952.
[08.07.2025 19:10] Extra JSON file exists (./assets/json/2507.04952.json), skip PDF parsing.
[08.07.2025 19:10] Paper image links file exists (./assets/img_data/2507.04952.json), skip HTML parsing.
[08.07.2025 19:10] Success.
[08.07.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2506.21884.
[08.07.2025 19:10] Extra JSON file exists (./assets/json/2506.21884.json), skip PDF parsing.
[08.07.2025 19:10] Paper image links file exists (./assets/img_data/2506.21884.json), skip HTML parsing.
[08.07.2025 19:10] Success.
[08.07.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2507.03607.
[08.07.2025 19:10] Extra JSON file exists (./assets/json/2507.03607.json), skip PDF parsing.
[08.07.2025 19:10] Paper image links file exists (./assets/img_data/2507.03607.json), skip HTML parsing.
[08.07.2025 19:10] Success.
[08.07.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2507.04590.
[08.07.2025 19:10] Extra JSON file exists (./assets/json/2507.04590.json), skip PDF parsing.
[08.07.2025 19:10] Paper image links file exists (./assets/img_data/2507.04590.json), skip HTML parsing.
[08.07.2025 19:10] Success.
[08.07.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2507.04036.
[08.07.2025 19:10] Extra JSON file exists (./assets/json/2507.04036.json), skip PDF parsing.
[08.07.2025 19:10] Paper image links file exists (./assets/img_data/2507.04036.json), skip HTML parsing.
[08.07.2025 19:10] Success.
[08.07.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2507.03033.
[08.07.2025 19:10] Downloading paper 2507.03033 from http://arxiv.org/pdf/2507.03033v1...
[08.07.2025 19:10] Extracting affiliations from text.
[08.07.2025 19:10] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Preserving Privacy, Increasing Accessibility, and Reducing Cost: An On-Device Artificial Intelligence Model for Medical Transcription and Note Generation Johnson Thomas1, Ayush Mudgal2, Wendao Liu2, Nisten Tahiraj3, Zeeshaan Mohammed4, Dhruv Diddi "
[08.07.2025 19:10] Response: []
[08.07.2025 19:10] Extracting affiliations from text.
[08.07.2025 19:10] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Preserving Privacy, Increasing Accessibility, and Reducing Cost: An On-Device Artificial Intelligence Model for Medical Transcription and Note Generation Johnson Thomas1, Ayush Mudgal2, Wendao Liu2, Nisten Tahiraj3, Zeeshaan Mohammed4, Dhruv DiddiBackground: Clinical documentation represents significant burden for healthcare providers, with physicians spending up to 2 hours daily on administrative tasks. Recent advances in large language models (LLMs) offer promising solutions, but privacy concerns and computational requirements limit their adoption in healthcare settings. Objective: To develop and evaluate privacy-preserving, on-device medical transcription system using fine-tuned Llama 3.2 1B model capable of generating structured medical notes from medical transcriptions while maintaining complete data sovereignty entirely in the browser. Methods: We fine-tuned Llama 3.2 1B model using Parameter-Efficient Fine-Tuning (PEFT) with LoRA on 1,500 synthetic medical transcription-to-structured note pairs. The model was evaluated against the base Llama 3.2 1B on two datasets: 100 endocrinology transcripts and 140 modified ACI benchmark cases. Evaluation employed both statistical metrics (ROUGE, BERTScore, BLEURT) and LLM-as-judge assessments across multiple clinical quality dimensions. Results: The fine-tuned OnDevice model demonstrated substantial improvements over the base model. On the ACI benchmark, ROUGE-1 scores increased from 0.346 to 0.496, while BERTScore F1 improved from 0.832 to 0.866. Clinical quality assessments showed marked reduction in major hallucinations (from 85 to 35 cases) and enhanced factual correctness (2.81 to 3.54 on 5-point scale). Similar improvements were observed on the internal evaluation dataset, with composite scores increasing from 3.13 to 4.43 (+41.5%). Conclusions: Fine-tuning compact LLMs for medical transcription yields clinically meaningful improvements while enabling complete on-device browser deployment. This approach addresses key barriers to AI adoption in healthcare: privacy preservation, cost reduction, and accessibility for resource-constrained environments. Keywords: Medical transcription, SOAP notes, on-device AI, privacy-preserving healthcare, parameter-efficient fine-tuning, large language models ___________________________________________________________________________ 1Department of Endocrinology, Mercy Hospital, Springfield, Missouri, USA, 2Starfishdata.ai, 3alignmentlab.ai , 4Solo Tech 1. Introduction The administrative burden of clinical documentation has reached crisis proportions in modern healthcare, with physicians dedicating 50% of their time to electronic health record (EHR) tasks[1]. This documentation burden contributes significantly to physician burnout, with studies indicating that for every hour of direct patient care, clinicians spend nearly two hours on EHR-related activities [2,3]. The recent emergence of large language models (LLMs) presents opportunities to automate clinical documentation, particularly through the generation of structured clinical notes from physician-patient conversations [4]. Current AI-powered transcription solutions, while demonstrating impressive capabilities, face significant adoption barriers in healthcare settings. Cloud-based systems raise substantial privacy concerns given the sensitive nature of patient data and strict regulatory requirements under HIPAA and similar frameworks [6]. Cloud-based solutions require transmitting patient data to external servers, raising concerns about data breaches and unauthorized access. Additionally, the computational requirements and associated costs of state-of-the-art models create accessibility barriers for smaller healthcare practices and resource-constrained environments . The advent of smaller, more efficient LLMs has opened new possibilities for on-device deployment. Models like Llama 3.2 1B demonstrate that significant language understanding capabilities can be achieved with substantially reduced computational requirements [5]. When combined with parameter-efficient fine-tuning (PEFT) techniques such as Low-Rank Adaptation (LoRA), these models can be adapted for specialized medical tasks while maintaining feasibility for local deployment [7]. This study presents the development and evaluation of an on-device medical transcription system built upon fine-tuned Llama 3.2 1B model. Our approach addresses the critical need for privacy-preserving, cost-effective clinical documentation tools that can operate entirely on local hardware in browser without compromising patient data security or requiring ongoing subscription costs. 2. Related WorkThe application of LLMs to healthcare has gained significant traction in recent years. Google's Med-PaLM and Med-PaLM 2 demonstrated that LLMs could achieve physician-level performance on medical question-answering tasks [8]. Similarly, Microsoft's BioGPT showed promising results in biomedical text generation and mining [9]. However, these models typically require substantial computational resources, limiting their deployment in resource-constrained clinical environments. Recent work has focused on developing more compact medical LLMs. Chen et al. introduced MedAlpaca, 7B parameter model fine-tuned on medical data [10]. While more efficient than larger models, it still requires dedicated GPU resources for real-time inference. The ChatDoctor model by Li et al. demonstrated that fine-tuning on medical conversations could improve domain-specific performance [11].The Sound of Healthcare study showed that LLMs can achieve state-of-the-art performance in medical transcription tasks, particularly in speaker diarization and Word Error Rate reduction using Chain-of-Thought prompting [12]. Other opensource efforts have explored finetuning compact LLMs (7B mistral model) for specialtyspecific note generation, demonstrating gains compared to similarly sized baselines [13]. These findings establish the technical feasibility of LLM-based medical transcription while highlighting the importance of domain-specific adaptation."
[08.07.2025 19:10] Mistral response. {"object": "error", "message": "Service tier capacity exceeded for this model.", "type": "invalid_request_error", "param": null, "code": null}
[08.07.2025 19:10] Failed to download and parse paper https://huggingface.co/papers/2507.03033: 'choices'
[08.07.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2507.05259.
[08.07.2025 19:10] Extra JSON file exists (./assets/json/2507.05259.json), skip PDF parsing.
[08.07.2025 19:10] Paper image links file exists (./assets/img_data/2507.05259.json), skip HTML parsing.
[08.07.2025 19:10] Success.
[08.07.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2507.03336.
[08.07.2025 19:10] Extra JSON file exists (./assets/json/2507.03336.json), skip PDF parsing.
[08.07.2025 19:10] Paper image links file exists (./assets/img_data/2507.03336.json), skip HTML parsing.
[08.07.2025 19:10] Success.
[08.07.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2507.04562.
[08.07.2025 19:10] Extra JSON file exists (./assets/json/2507.04562.json), skip PDF parsing.
[08.07.2025 19:10] Paper image links file exists (./assets/img_data/2507.04562.json), skip HTML parsing.
[08.07.2025 19:10] Success.
[08.07.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2507.04376.
[08.07.2025 19:10] Extra JSON file exists (./assets/json/2507.04376.json), skip PDF parsing.
[08.07.2025 19:10] Paper image links file exists (./assets/img_data/2507.04376.json), skip HTML parsing.
[08.07.2025 19:10] Success.
[08.07.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2507.04285.
[08.07.2025 19:10] Extra JSON file exists (./assets/json/2507.04285.json), skip PDF parsing.
[08.07.2025 19:10] Paper image links file exists (./assets/img_data/2507.04285.json), skip HTML parsing.
[08.07.2025 19:10] Success.
[08.07.2025 19:10] Enriching papers with extra data.
[08.07.2025 19:10] ********************************************************************************
[08.07.2025 19:10] Abstract 0. MemOS is proposed as a memory operating system for Large Language Models to enhance memory management, enabling efficient storage and retrieval, and facilitating continual learning and personalized modeling.  					AI-generated summary 				 Large Language Models (LLMs) have become an essential infras...
[08.07.2025 19:10] ********************************************************************************
[08.07.2025 19:10] Abstract 1. Learning high-quality text representations is fundamental to a wide range of NLP tasks. While encoder pretraining has traditionally relied on Masked Language Modeling (MLM), recent evidence suggests that decoder models pretrained with Causal Language Modeling (CLM) can be effectively repurposed as e...
[08.07.2025 19:10] ********************************************************************************
[08.07.2025 19:10] Abstract 2. A high-speed 4D capturing system using low FPS cameras with asynchronous capture and video-diffusion-based artifact correction enhances reconstruction quality.  					AI-generated summary 				 Reconstructing fast-dynamic scenes from multi-view videos is crucial for high-speed motion analysis and real...
[08.07.2025 19:10] ********************************************************************************
[08.07.2025 19:10] Abstract 3. DreamVLA improves robot manipulation through a VLA framework that incorporates world knowledge, dynamic-region guidance, and a diffusion-based transformer to ensure clear, disentangled representations for action planning.  					AI-generated summary 				 Recent advances in vision-language-action (VLA...
[08.07.2025 19:10] ********************************************************************************
[08.07.2025 19:10] Abstract 4. A scalable reward modeling method, Policy Discriminative Learning (POLAR), enhances reward model performance and generalizes robustly in reinforcement learning through policy comparison.  					AI-generated summary 				 We offer a novel perspective on reward modeling by formulating it as a policy dis...
[08.07.2025 19:10] ********************************************************************************
[08.07.2025 19:10] Abstract 5. A large-scale dataset and verification tool are introduced for assessing and improving cross-disciplinary reasoning capabilities in multimodal models.  					AI-generated summary 				 In this paper, we introduce BMMR, a large-scale bilingual, multimodal, multi-disciplinary reasoning dataset for the c...
[08.07.2025 19:10] ********************************************************************************
[08.07.2025 19:10] Abstract 6. RoboBrain 2.0, a vision-language foundation model, achieves top performance in embodied tasks through its heterogeneous architecture and multi-stage training strategies.  					AI-generated summary 				 We introduce RoboBrain 2.0, our latest generation of embodied vision-language foundation models, d...
[08.07.2025 19:10] ********************************************************************************
[08.07.2025 19:10] Abstract 7. RefineX is a scalable framework for improving the quality of large language model pre-training data through programmatic editing, yielding better performance than alternative methods across various downstream tasks.  					AI-generated summary 				 The foundational capabilities of large language mode...
[08.07.2025 19:10] ********************************************************************************
[08.07.2025 19:10] Abstract 8. A unified framework called Easy Dataset synthesizes fine-tuning data from unstructured documents using a GUI and LLMs, improving domain-specific performance of LLMs while maintaining general knowledge.  					AI-generated summary 				 Large language models (LLMs) have shown impressive performance on ...
[08.07.2025 19:10] ********************************************************************************
[08.07.2025 19:10] Abstract 9. A streaming video generation model named StreamDiT, based on transformer-based diffusion models, enables real-time video generation with high content consistency and visual quality.  					AI-generated summary 				 Recently, great progress has been achieved in text-to-video (T2V) generation by scalin...
[08.07.2025 19:10] ********************************************************************************
[08.07.2025 19:10] Abstract 10. Historical documents represent an invaluable cultural heritage, yet have undergone significant degradation over time through tears, water erosion, and oxidation. Existing Historical Document Restoration (HDR) methods primarily focus on single modality or limited-size restoration, failing to meet pra...
[08.07.2025 19:10] ********************************************************************************
[08.07.2025 19:10] Abstract 11. OmniDraft, a unified framework, addresses cross-vocabulary mismatch and improves decoding speed by allowing a single draft model to interact dynamically with diverse target models in online settings.  					AI-generated summary 				 Speculative decoding generally dictates having a small, efficient dr...
[08.07.2025 19:10] ********************************************************************************
[08.07.2025 19:10] Abstract 12. Visual embedding models often capture continuous, ordinal attributes along specific axes, enabling effective image ranking with minimal supervision.  					AI-generated summary 				 We study whether visual embedding models capture continuous, ordinal attributes along linear directions, which we term ...
[08.07.2025 19:10] ********************************************************************************
[08.07.2025 19:10] Abstract 13. ArtifactsBench, a novel benchmark and evaluation framework, automates the assessment of visual code generation quality using temporal screenshots and a multimodal language model judge.  					AI-generated summary 				 The generative capabilities of Large Language Models (LLMs) are rapidly expanding f...
[08.07.2025 19:10] ********************************************************************************
[08.07.2025 19:10] Abstract 14. A framework combining NeRF with spectral unmixing yields accurate material segmentation and editing through hyperspectral synthesis.  					AI-generated summary 				 Neural Radiance Field (NeRF)-based segmentation methods focus on object semantics and rely solely on RGB data, lacking intrinsic materi...
[08.07.2025 19:10] ********************************************************************************
[08.07.2025 19:10] Abstract 15. A transformer-based model predicts software vulnerability severity levels directly from text, enhancing triage efficiency and consistency.  					AI-generated summary 				 This paper presents VLAI, a transformer-based model that predicts software vulnerability severity levels directly from text descr...
[08.07.2025 19:10] ********************************************************************************
[08.07.2025 19:10] Abstract 16. A unified framework VLM2Vec-V2 is proposed for learning embeddings across diverse visual forms such as videos and documents, demonstrating strong performance on new tasks and improving upon existing benchmarks for images.  					AI-generated summary 				 Multimodal embedding models have been crucial ...
[08.07.2025 19:10] ********************************************************************************
[08.07.2025 19:10] Abstract 17. A multimodal agent transforms documents into detailed presentation videos with audio, evaluated using a comprehensive framework involving vision-language models.  					AI-generated summary 				 We present PresentAgent, a multimodal agent that transforms long-form documents into narrated presentation...
[08.07.2025 19:10] ********************************************************************************
[08.07.2025 19:10] Abstract 18. A fine-tuned Llama 3.2 1B model using PEFT with LoRA in the browser improves medical transcription accuracy and reduces privacy and computational concerns.  					AI-generated summary 				 Background: Clinical documentation represents a significant burden for healthcare providers, with physicians spe...
[08.07.2025 19:10] ********************************************************************************
[08.07.2025 19:10] Abstract 19. X-Planner, a planning system utilizing a multimodal large language model, decomposes complex text-guided image editing instructions into precise sub-instructions, ensuring localized, identity-preserving edits and achieving top performance on established benchmarks.  					AI-generated summary 				 Re...
[08.07.2025 19:10] ********************************************************************************
[08.07.2025 19:10] Abstract 20. DiaFORGE is a disambiguation framework that enhances large language models' ability to invoke enterprise APIs accurately through dialogue synthesis, supervised fine-tuning, and real-world evaluation.  					AI-generated summary 				 Large language models (LLMs) are increasingly tasked with invoking e...
[08.07.2025 19:10] ********************************************************************************
[08.07.2025 19:10] Abstract 21. State-of-the-art large language models are evaluated on forecasting questions and show lower accuracy compared to human superforecasters.  					AI-generated summary 				 Large language models (LLMs) have demonstrated remarkable capabilities across diverse tasks, but their ability to forecast future ...
[08.07.2025 19:10] ********************************************************************************
[08.07.2025 19:10] Abstract 22. As Artificial Intelligence systems evolve from monolithic models to ecosystems of specialized agents, the need for standardized communication protocols becomes increasingly critical. This paper introduces MOD-X (Modular Open Decentralized eXchange), a novel architectural framework proposal for agent...
[08.07.2025 19:10] ********************************************************************************
[08.07.2025 19:10] Abstract 23. SeqTex leverages pretrained video foundation models to directly generate high-fidelity UV texture maps through a sequence generation approach, enhancing 3D texture generation with superior consistency and alignment.  					AI-generated summary 				 Training native 3D texture generative models remains...
[08.07.2025 19:10] Read previous papers.
[08.07.2025 19:10] Generating reviews via LLM API.
[08.07.2025 19:10] Using data from previous issue: {"categories": ["#training", "#agi", "#rag", "#long_context", "#optimization", "#data"], "emoji": "üß†", "ru": {"title": "MemOS: –æ–ø–µ—Ä–∞—Ü–∏–æ–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –ø–∞–º—è—Ç–∏ –¥–ª—è –±–æ–ª–µ–µ —É–º–Ω—ã—Ö –∏ –∞–¥–∞–ø—Ç–∏–≤–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "MemOS - —ç—Ç–æ –æ–ø–µ—Ä–∞—Ü–∏–æ–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –ø–∞–º—è—Ç–∏ –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM), –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–∞—è
[08.07.2025 19:10] Using data from previous issue: {"categories": ["#survey", "#optimization", "#training", "#dataset"], "emoji": "üß†", "ru": {"title": "–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è —ç–Ω–∫–æ–¥–µ—Ä–æ–≤: —Å–∏–Ω–µ—Ä–≥–∏—è CLM –∏ MLM", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Å—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –º–µ—Ç–æ–¥–æ–≤ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è —ç–Ω–∫–æ–¥–µ—Ä–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —è–∑—ã–∫–æ–≤–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è (MLM) –∏ –∫–∞—É–∑–∞–ª—å
[08.07.2025 19:10] Using data from previous issue: {"categories": ["#optimization", "#diffusion", "#3d", "#video"], "emoji": "üé•", "ru": {"title": "–í—ã—Å–æ–∫–æ—Å–∫–æ—Ä–æ—Å—Ç–Ω–∞—è 4D-—Å—ä–µ–º–∫–∞ –æ–±—ã—á–Ω—ã–º–∏ –∫–∞–º–µ—Ä–∞–º–∏", "desc": "–ü—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è —Å–∏—Å—Ç–µ–º–∞ –≤—ã—Å–æ–∫–æ—Å–∫–æ—Ä–æ—Å—Ç–Ω–æ–π 4D-—Å—ä–µ–º–∫–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∫–∞–º–µ—Ä —Å –Ω–∏–∑–∫–æ–π —á–∞—Å—Ç–æ—Ç–æ–π –∫–∞–¥—Ä–æ–≤ –∏ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–º –∑–∞—Ö–≤–∞—Ç–æ–º. –°–∏—Å—Ç–µ–º–∞ –ø–æ–≤—ã—à–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—É—é
[08.07.2025 19:10] Using data from previous issue: {"categories": ["#robotics", "#reasoning", "#training", "#optimization", "#multimodal", "#diffusion", "#agents", "#games"], "emoji": "ü§ñ", "ru": {"title": "DreamVLA: –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–µ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–µ–π—Å—Ç–≤–∏–π —Ä–æ–±–æ—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –æ–∫—Ä—É–∂–∞—é—â–µ–≥–æ –º–∏—Ä–∞", "desc": "DreamVLA - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ —É
[08.07.2025 19:10] Using data from previous issue: {"categories": ["#optimization", "#rlhf", "#training", "#rl"], "emoji": "üéØ", "ru": {"title": "POLAR: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–π –≤ –æ–±—É—á–µ–Ω–∏–∏ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –Ω–∞–∑–≤–∞–Ω–Ω—ã–π Policy Discriminat
[08.07.2025 19:10] Using data from previous issue: {"categories": ["#multimodal", "#reasoning", "#dataset", "#benchmark", "#open_source", "#data"], "emoji": "üß†", "ru": {"title": "–ù–æ–≤—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º—É–ª—å—Ç–∏–¥–∏—Å—Ü–∏–ø–ª–∏–Ω–∞—Ä–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –ò–ò", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç BMMR - –º–∞—Å—à—Ç–∞–±–Ω—ã–π –¥–≤—É—è–∑—ã—á–Ω—ã–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤
[08.07.2025 19:10] Using data from previous issue: {"categories": ["#agi", "#reasoning", "#architecture", "#benchmark", "#training", "#agents", "#open_source"], "emoji": "ü§ñ", "ru": {"title": "RoboBrain 2.0: –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ò–ò –¥–ª—è –≤–æ–ø–ª–æ—â–µ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á", "desc": "RoboBrain 2.0 - —ç—Ç–æ –Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ –¥–ª—è –≤–æ–ø–ª–æ—â–µ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∞—è
[08.07.2025 19:10] Using data from previous issue: {"categories": ["#training", "#optimization", "#data"], "emoji": "‚úÇÔ∏è", "ru": {"title": "RefineX: —Ö–∏—Ä—É—Ä–≥–∏—á–µ—Å–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –≤ —É–ª—É—á—à–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ò–ò", "desc": "RefineX - —ç—Ç–æ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø—É—Ç–µ–º –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–≥–æ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è. –û
[08.07.2025 19:10] Using data from previous issue: {"categories": ["#synthetic", "#data", "#dataset", "#open_source"], "emoji": "üß†", "ru": {"title": "–°–∏–Ω—Ç–µ–∑ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è LLM —Å –ø–æ–º–æ—â—å—é Easy Dataset", "desc": "Easy Dataset - —ç—Ç–æ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –∏–∑ –Ω–µ—Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥
[08.07.2025 19:10] Using data from previous issue: {"categories": ["#diffusion", "#video", "#games"], "emoji": "üé¨", "ru": {"title": "StreamDiT: –†–µ–∞–ª—å–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é –ò–ò", "desc": "StreamDiT - —ç—Ç–æ –º–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ—Ç–æ–∫–æ–≤–æ–≥–æ –≤–∏–¥–µ–æ, –æ—Å–Ω–æ–≤–∞–Ω–Ω–∞—è –Ω–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–Ω—ã—Ö –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö. –û–Ω–∞ –ø–æ–∑–≤–æ–ª—è–µ—Ç –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –≤–∏–¥–µ–æ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º
[08.07.2025 19:10] Using data from previous issue: {"categories": ["#architecture", "#cv", "#multimodal", "#dataset", "#data"], "emoji": "üìú", "ru": {"title": "AutoHDR: –í–æ–∑—Ä–æ–∂–¥–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ —á–µ—Ä–µ–∑ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Ä–µ—Å—Ç–∞–≤—Ä–∞—Ü–∏–∏ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º AutoHDR. –ê–≤—Ç–æ—Ä
[08.07.2025 19:10] Using data from previous issue: {"categories": ["#optimization", "#games", "#inference", "#training", "#reasoning", "#multimodal"], "emoji": "üöÄ", "ru": {"title": "–û–¥–∏–Ω —á–µ—Ä–Ω–æ–≤–∏–∫ –¥–ª—è –≤—Å–µ—Ö: —É—Å–∫–æ—Ä–µ–Ω–∏–µ –∏ –∞–¥–∞–ø—Ç–∞—Ü–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "OmniDraft - —ç—Ç–æ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è —Ä–∞–±–æ—Ç—ã —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –û–Ω–∞ —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª
[08.07.2025 19:10] Using data from previous issue: {"categories": ["#cv", "#dataset"], "emoji": "üìä", "ru": {"title": "–†–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º –Ω–∞–¥–∑–æ—Ä–æ–º –±–ª–∞–≥–æ–¥–∞—Ä—è —Å–∫—Ä—ã—Ç–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–µ –≤—Å—Ç—Ä–∞–∏–≤–∞–Ω–∏–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≤—Å—Ç—Ä–∞–∏–≤–∞–Ω–∏—è —á–∞—Å—Ç–æ –∑–∞—Ö–≤–∞—Ç—ã–≤–∞—é—Ç –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–µ, –ø–æ—Ä—è–¥–∫–æ–≤—ã–µ –∞—Ç—Ä–∏–±—É—Ç—ã –≤–¥–æ–ª—å –ª–∏–Ω–µ–π–Ω—ã—Ö –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω
[08.07.2025 19:10] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#games", "#open_source"], "emoji": "üñ•Ô∏è", "ru": {"title": "–ê–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –∫–æ–¥–∞ —Å –ø–æ–º–æ—â—å—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "ArtifactsBench - —ç—Ç–æ –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –∫
[08.07.2025 19:10] Using data from previous issue: {"categories": ["#robotics", "#cv", "#3d"], "emoji": "üåà", "ru": {"title": "–ì–∏–ø–µ—Ä—Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã–π NeRF –¥–ª—è —Ç–æ—á–Ω–æ–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –∏ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç UnMix-NeRF - —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∏–π –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Ä–∞–¥–∏–∞–ª—å–Ω—ã–µ –ø–æ–ª—è (NeRF) —Å–æ —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã–º —Ä–∞–∑–ª–æ–∂–µ–Ω–∏–µ–º –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ –≥–∏–ø–µ—Ä—Å–ø–µ–∫—Ç
[08.07.2025 19:10] Using data from previous issue: {"categories": ["#security", "#architecture", "#dataset", "#data", "#training", "#open_source"], "emoji": "üõ°Ô∏è", "ru": {"title": "–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç –Ω–∞ —Å—Ç—Ä–∞–∂–µ –∫–∏–±–µ—Ä–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏: –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ—Ü–µ–Ω–∫–∞ —É—è–∑–≤–∏–º–æ—Å—Ç–µ–π –ü–û", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –º–æ–¥–µ–ª—å VLAI –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç
[08.07.2025 19:10] Using data from previous issue: {"categories": ["#games", "#rag", "#survey", "#benchmark", "#transfer_learning", "#multimodal"], "emoji": "üé•", "ru": {"title": "–ï–¥–∏–Ω–∞—è –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è –≤—Å–µ—Ö –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤", "desc": "VLM2Vec-V2 - —ç—Ç–æ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤, –≤–∫–ª—é—á–∞—è –≤
[08.07.2025 19:10] Using data from previous issue: {"categories": ["#cv", "#multimodal", "#agents", "#optimization", "#dataset", "#benchmark", "#games", "#interpretability"], "emoji": "üé•", "ru": {"title": "–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç —Å–æ–∑–¥–∞–µ—Ç –ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏–∏ –Ω–∞ —É—Ä–æ–≤–Ω–µ —á–µ–ª–æ–≤–µ–∫–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç PresentAgent - –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –∞–≥–µ–Ω—Ç–∞, –ø—Ä–µ–æ–±—Ä–∞–∑—É—é—â
[08.07.2025 19:10] Using data from previous issue: {"categories": ["#hallucinations", "#alignment", "#synthetic", "#inference", "#training", "#low_resource", "#healthcare"], "emoji": "üè•", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –∏ –±–µ–∑–æ–ø–∞—Å–Ω–∞—è –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∞—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è —Å –ø–æ–º–æ—â—å—é –ò–ò –≤ –±—Ä–∞—É–∑–µ—Ä–µ", "desc": "–í —Å—Ç–∞—Ç—å–µ –æ–ø–∏—Å—ã–≤–∞–µ—Ç—Å—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞ –∏ –æ—Ü–µ–Ω–∫–∞ —Å–∏—Å—Ç–µ–º—ã –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–π 
[08.07.2025 19:10] Using data from previous issue: {"categories": ["#dataset", "#reasoning", "#benchmark", "#multimodal", "#diffusion"], "emoji": "üé®", "ru": {"title": "X-Planner: —É–º–Ω–æ–µ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "X-Planner - —ç—Ç–æ —Å–∏—Å—Ç–µ–º–∞ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–π –±–æ–ª—å—à–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω
[08.07.2025 19:10] Using data from previous issue: {"categories": ["#optimization", "#open_source", "#dataset", "#training", "#data", "#alignment", "#benchmark", "#agents"], "emoji": "üîß", "ru": {"title": "DiaFORGE: —Ç–æ—á–Ω—ã–µ –≤—ã–∑–æ–≤—ã API —á–µ—Ä–µ–∑ —Å–∏–Ω—Ç–µ–∑ –¥–∏–∞–ª–æ–≥–æ–≤ –∏ –¥–æ–æ–±—É—á–µ–Ω–∏–µ LLM", "desc": "DiaFORGE - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö 
[08.07.2025 19:10] Using data from previous issue: {"categories": ["#benchmark", "#reasoning"], "emoji": "üîÆ", "ru": {"title": "–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç vs —Å—É–ø–µ—Ä–ø—Ä–æ–≥–Ω–æ–∑–∏—Å—Ç—ã: –±–∏—Ç–≤–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç–µ–ª–µ–π –±—É–¥—É—â–µ–≥–æ", "desc": "–°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (LLM) –±—ã–ª–∏ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω—ã –Ω–∞ –∑–∞–¥–∞—á–∞—Ö –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –±—É–¥—É—â–∏—Ö —Å–æ–±—ã—Ç–∏–π. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ LLM –¥–æ—Å
[08.07.2025 19:10] Using data from previous issue: {"categories": ["#optimization", "#interpretability", "#agi", "#agents", "#architecture"], "emoji": "üîÄ", "ru": {"title": "MOD-X: –ù–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç MOD-X - –Ω–æ–≤—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—É—é –∫–æ–Ω—Ü–µ–ø—Ü–∏—é –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –º–µ–∂–¥—É –ò–ò-–∞–≥–µ–Ω—Ç–∞–º–∏. MOD-X –ø
[08.07.2025 19:10] Using data from previous issue: {"categories": ["#architecture", "#3d", "#synthetic", "#optimization"], "emoji": "üé®", "ru": {"title": "SeqTex: –ø—Ä—è–º–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è UV-—Ç–µ–∫—Å—Ç—É—Ä —Å –ø–æ–º–æ—â—å—é –≤–∏–¥–µ–æ-–º–æ–¥–µ–ª–µ–π", "desc": "SeqTex - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç—É—Ä –¥–ª—è 3D-–º–æ–¥–µ–ª–µ–π, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –≤–∏–¥–µ–æ-–º–æ–¥–µ–ª–∏. –û–Ω –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç UV-—Ä–∞–∑–≤–µ—Ä—Ç–∫–∏
[08.07.2025 19:10] Renaming data file.
[08.07.2025 19:10] Renaming previous data. hf_papers.json to ./d/2025-07-08.json
[08.07.2025 19:10] Saving new data file.
[08.07.2025 19:10] Generating page.
[08.07.2025 19:10] Renaming previous page.
[08.07.2025 19:10] Renaming previous data. index.html to ./d/2025-07-08.html
[08.07.2025 19:10] Writing result.
[08.07.2025 19:10] Renaming log file.
[08.07.2025 19:10] Renaming previous data. log.txt to ./logs/2025-07-08_last_log.txt
