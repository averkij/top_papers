[29.08.2025 00:52] Read previous papers.
[29.08.2025 00:52] Generating top page (month).
[29.08.2025 00:52] Writing top page (month).
[29.08.2025 02:22] Read previous papers.
[29.08.2025 02:22] Get feed.
[29.08.2025 02:22] Extract page data from URL. URL: https://huggingface.co/papers/2508.20722
[29.08.2025 02:22] Extract page data from URL. URL: https://huggingface.co/papers/2508.21058
[29.08.2025 02:22] Extract page data from URL. URL: https://huggingface.co/papers/2508.20453
[29.08.2025 02:22] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[29.08.2025 02:22] Downloading and parsing papers (pdf, html). Total: 3.
[29.08.2025 02:22] Downloading and parsing paper https://huggingface.co/papers/2508.20722.
[29.08.2025 02:22] Downloading paper 2508.20722 from http://arxiv.org/pdf/2508.20722v1...
[29.08.2025 02:22] Extracting affiliations from text.
[29.08.2025 02:22] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"rStar2-Agent: Agentic Reasoning Technical Report Ning Shang Yifei Liu Yi Zhu Li Lyna Zhang Weijiang Xu Xinyu Guan Buze Zhang Bingcheng Dong Xudong Zhou Bowen Zhang Ying Xin Ziming Miao Scarlett Li Fan Yang Mao Yang Abstract We introduce rStar2-Agent, 14B math reasoning model trained with agentic reinforcement learning to achieve frontier-level performance. Beyond current long CoT, the model demonstrates advanced cognitive behaviors, such as thinking carefully before using Python coding tools and reflecting on code execution feedback to autonomously explore, verify, and refine intermediate steps in complex problem-solving. This capability is enabled through three key innovations that makes agentic RL effective at scale: (i) an efficient RL infrastructure with reliable Python code environment that supports high-throughput execution and mitigates the high rollout costs, enabling training on limited GPU resources (64 MI300X GPUs); (ii) GRPO-RoC, an agentic RL algorithm with Resample-on-Correct rollout strategy that addresses the inherent environment noises from coding tools, allowing the model to reason more effectively in code environment; (iii) An efficient agent training recipe that starts with non-reasoning SFT and progresses through multi-RL stages, yielding advanced cognitive abilities with minimal compute cost. To this end, rStar2-Agent boosts pre-trained 14B model to state of the art in only 510 RL steps within one week, achieving average pass@1 scores of 80.6% on AIME24 and 69.8% on AIME25, surpassing DeepSeek-R1 (671B) with significantly shorter responses. Beyond mathematics, rStar2-Agent-14B also demonstrates strong generalization to alignment, scientific reasoning, and agentic tool-use tasks. Code and training recipes are available at https://github.com/microsoft/rStar. Model AIME24 AIME25 HMMT25 OpenAI o3-mini (medium) DeepSeek-R1 (671B) DeepSeek-R1-Zero (671B) Claude-Opus-4.0 (Think) QWQ-32B rStar2-Agent-14B 79.6 79.8 71.0 76.0 79.5 80.6 77.0 70.0 53.3 69."
[29.08.2025 02:22] Response: ```python
[]
```
[29.08.2025 02:22] Extracting affiliations from text.
[29.08.2025 02:22] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"rStar2-Agent: Agentic Reasoning Technical Report Ning Shang Yifei Liu Yi Zhu Li Lyna Zhang Weijiang Xu Xinyu Guan Buze Zhang Bingcheng Dong Xudong Zhou Bowen Zhang Ying Xin Ziming Miao Scarlett Li Fan Yang Mao YangAbstract We introduce rStar2-Agent, 14B math reasoning model trained with agentic reinforcement learning to achieve frontier-level performance. Beyond current long CoT, the model demonstrates advanced cognitive behaviors, such as thinking carefully before using Python coding tools and reflecting on code execution feedback to autonomously explore, verify, and refine intermediate steps in complex problem-solving. This capability is enabled through three key innovations that makes agentic RL effective at scale: (i) an efficient RL infrastructure with reliable Python code environment that supports high-throughput execution and mitigates the high rollout costs, enabling training on limited GPU resources (64 MI300X GPUs); (ii) GRPO-RoC, an agentic RL algorithm with Resample-on-Correct rollout strategy that addresses the inherent environment noises from coding tools, allowing the model to reason more effectively in code environment; (iii) An efficient agent training recipe that starts with non-reasoning SFT and progresses through multi-RL stages, yielding advanced cognitive abilities with minimal compute cost. To this end, rStar2-Agent boosts pre-trained 14B model to state of the art in only 510 RL steps within one week, achieving average pass@1 scores of 80.6% on AIME24 and 69.8% on AIME25, surpassing DeepSeek-R1 (671B) with significantly shorter responses. Beyond mathematics, rStar2-Agent-14B also demonstrates strong generalization to alignment, scientific reasoning, and agentic tool-use tasks. Code and training recipes are available at https://github.com/microsoft/rStar. Model AIME24 AIME25 HMMT25 OpenAI o3-mini (medium) DeepSeek-R1 (671B) DeepSeek-R1-Zero (671B) Claude-Opus-4.0 (Think) QWQ-32B rStar2-Agent-14B 79.6 79.8 71.0 76.0 79.5 80.6 77.0 70.0 53.3 69.2 65.8 69. 53.0 44.4 46.0 - 47.5 52.7 5 2 0 2 8 2 ] . [ 1 2 2 7 0 2 . 8 0 5 2 : r Figure 1: rStar2-Agent-14B reaches frontier-level math reasoning in just 510 RL training steps. The first four authors contributed equally Project leaders; correspondence to lzhani@microsoft.com and maoyang@microsoft.com1 Introduction 2 Agentic Reinforcement Learning Methodology2.2.1 Preliminary: GRPO . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . 3 Large-Scale Agentic RL Infrastructure3 4 4 6 6 7"
[29.08.2025 02:22] Mistral response. {"id": "b7f6bc674772411b90c798374bd78433", "created": 1756434145, "model": "mistral-large-latest", "usage": {"prompt_tokens": 785, "total_tokens": 793, "completion_tokens": 8}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\"Microsoft\"]\n```"}}]}
[29.08.2025 02:22] Response: ```python
["Microsoft"]
```
[29.08.2025 02:22] Deleting PDF ./assets/pdf/2508.20722.pdf.
[29.08.2025 02:22] Success.
[29.08.2025 02:22] Downloading and parsing paper https://huggingface.co/papers/2508.21058.
[29.08.2025 02:22] Downloading paper 2508.21058 from http://arxiv.org/pdf/2508.21058v1...
[29.08.2025 02:22] Extracting affiliations from text.
[29.08.2025 02:22] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 8 2 ] . [ 1 8 5 0 1 2 . 8 0 5 2 : r a Shengqu Cai1,, Ceyuan Yang2,, Lvmin Zhang1, Yuwei Guo4, Junfei Xiao3, Ziyan Yang2, Yinghao Xu1, Zhenheng Yang5, Alan Yuille3, Leonidas Guibas1, Maneesh Agrawala1, Lu Jiang2, Gordon Wetzstein1 1Stanford University, 2ByteDance Seed, 3Johns Hopkins University, 4CUHK, 5ByteDance Work done at ByteDance Seed, Corresponding authors "
[29.08.2025 02:22] Response: ```python
["Stanford University", "ByteDance Seed", "Johns Hopkins University", "CUHK", "ByteDance"]
```
[29.08.2025 02:22] Deleting PDF ./assets/pdf/2508.21058.pdf.
[29.08.2025 02:22] Success.
[29.08.2025 02:22] Downloading and parsing paper https://huggingface.co/papers/2508.20453.
[29.08.2025 02:22] Downloading paper 2508.20453 from http://arxiv.org/pdf/2508.20453v1...
[29.08.2025 02:22] Extracting affiliations from text.
[29.08.2025 02:22] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 8 2 ] . [ 1 3 5 4 0 2 . 8 0 5 2 : r MCP-Bench: Benchmarking Tool-Using LLM Agents with Complex Real-World Tasks via MCP Servers Zhenting Wang1, Qi Chang1, Hemani Patel1,2, Shashank Biju1,2, Cheng-En Wu1, Quan Liu1, Aolin Ding1, Alireza Rezazadeh1, Ankit Shah1, Yujia Bao1, Eugene Siow1 1Center for Advanced AI, Accenture, 2UC Berkeley We introduce MCPBench, benchmark for evaluating large language models (LLMs) on realistic, multi-step tasks that demand tool use, cross-tool coordination, precise parameter control, and planning/reasoning for solving tasks. Built on the Model Context Protocol (MCP), MCP-Bench connects LLMs to 28 representative live MCP servers spanning 250 tools across domains such as finance, traveling, scientific computing, and academic search. Unlike prior API-based benchmarks, each MCP server provides set of complementary tools designed to work together, enabling the construction of authentic, multi-step tasks with rich inputoutput coupling. Also, tasks in MCP-Bench test agents ability to retrieve relevant tools from fuzzy instructions without explicit tool names, plan multi-hop execution trajectories for complex objectives, ground responses in intermediate tool outputs, and orchestrate cross-domain workflowscapabilities not adequately evaluated by existing benchmarks that rely on explicit tool specifications, shallow few-step workflows, and isolated domain operations. We propose multi-faceted evaluation framework covering tool-level schema understanding and usage, trajectorylevel planning and task completion. Experiments on 20 advanced LLMs reveal persistent challenges in MCP-Bench. Code and data: https://github.com/Accenture/mcp-bench. 1. Introduction Recent advances in large language models (LLMs) have enabled new generation of tool-using agents that can interpret natural language instructions, plan multi-step workflows, and interact with external tools to solve complex tasks (OpenAI, 2025c; Comanici et al., 2025; Anthropic, 2025; Yang et "
[29.08.2025 02:22] Response: ```python
["Center for Advanced AI, Accenture", "UC Berkeley"]
```
[29.08.2025 02:22] Deleting PDF ./assets/pdf/2508.20453.pdf.
[29.08.2025 02:22] Success.
[29.08.2025 02:22] Enriching papers with extra data.
[29.08.2025 02:22] ********************************************************************************
[29.08.2025 02:22] Abstract 0. rStar2-Agent, a 14B math reasoning model trained with agentic reinforcement learning, achieves state-of-the-art performance by efficiently handling complex problem-solving with advanced cognitive behaviors and minimal computational resources.  					AI-generated summary 				 We introduce rStar2-Agent...
[29.08.2025 02:22] ********************************************************************************
[29.08.2025 02:22] Abstract 1. Long video generation is addressed by introducing a sparse attention routing module, Mixture of Contexts, to efficiently manage long-term memory and retrieval in diffusion transformers.  					AI-generated summary 				 Long video generation is fundamentally a long context memory problem: models must ...
[29.08.2025 02:22] ********************************************************************************
[29.08.2025 02:22] Abstract 2. MCP-Bench evaluates large language models on complex, multi-step tasks requiring tool use, coordination, and planning across various domains.  					AI-generated summary 				 We introduce MCP-Bench, a benchmark for evaluating large language models (LLMs) on realistic, multi-step tasks that demand too...
[29.08.2025 02:22] Read previous papers.
[29.08.2025 02:22] Generating reviews via LLM API.
[29.08.2025 02:22] Querying the API.
[29.08.2025 02:22] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

rStar2-Agent, a 14B math reasoning model trained with agentic reinforcement learning, achieves state-of-the-art performance by efficiently handling complex problem-solving with advanced cognitive behaviors and minimal computational resources.  					AI-generated summary 				 We introduce rStar2-Agent, a 14B math reasoning model trained with agentic reinforcement learning to achieve frontier-level performance. Beyond current long CoT, the model demonstrates advanced cognitive behaviors, such as thinking carefully before using Python coding tools and reflecting on code execution feedback to autonomously explore, verify, and refine intermediate steps in complex problem-solving. This capability is enabled through three key innovations that makes agentic RL effective at scale: (i) an efficient RL infrastructure with a reliable Python code environment that supports high-throughput execution and mitigates the high rollout costs, enabling training on limited GPU resources (64 MI300X GPUs); (ii) GRPO-RoC, an agentic RL algorithm with a Resample-on-Correct rollout strategy that addresses the inherent environment noises from coding tools, allowing the model to reason more effectively in a code environment; (iii) An efficient agent training recipe that starts with non-reasoning SFT and progresses through multi-RL stages, yielding advanced cognitive abilities with minimal compute cost. To this end, rStar2-Agent boosts a pre-trained 14B model to state of the art in only 510 RL steps within one week, achieving average pass@1 scores of 80.6% on AIME24 and 69.8% on AIME25, surpassing DeepSeek-R1 (671B) with significantly shorter responses. Beyond mathematics, rStar2-Agent-14B also demonstrates strong generalization to alignment, scientific reasoning, and agentic tool-use tasks. Code and training recipes are available at https://github.com/microsoft/rStar.
[29.08.2025 02:23] Response: {
  "desc": "rStar2-Agent - это модель машинного обучения с 14 миллиардами параметров, обученная с помощью агентного обучения с подкреплением для решения математических задач. Модель демонстрирует продвинутое когнитивное поведение, включая использование инструментов Python и анализ обратной связи от выполнения кода. Ключевые инновации включают эффективную инфраструктуру RL, алгоритм GRPO-RoC и эффективный рецепт обучения агента. rStar2-Agent достигает наилучших результатов на ряде математических тестов, превосходя более крупные модели.",

  "emoji": "🧠",

  "title": "Агентное RL создает математического гения с минимальными ресурсами"
}
[29.08.2025 02:23] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"rStar2-Agent, a 14B math reasoning model trained with agentic reinforcement learning, achieves state-of-the-art performance by efficiently handling complex problem-solving with advanced cognitive behaviors and minimal computational resources.  					AI-generated summary 				 We introduce rStar2-Agent, a 14B math reasoning model trained with agentic reinforcement learning to achieve frontier-level performance. Beyond current long CoT, the model demonstrates advanced cognitive behaviors, such as thinking carefully before using Python coding tools and reflecting on code execution feedback to autonomously explore, verify, and refine intermediate steps in complex problem-solving. This capability is enabled through three key innovations that makes agentic RL effective at scale: (i) an efficient RL infrastructure with a reliable Python code environment that supports high-throughput execution and mitigates the high rollout costs, enabling training on limited GPU resources (64 MI300X GPUs); (ii) GRPO-RoC, an agentic RL algorithm with a Resample-on-Correct rollout strategy that addresses the inherent environment noises from coding tools, allowing the model to reason more effectively in a code environment; (iii) An efficient agent training recipe that starts with non-reasoning SFT and progresses through multi-RL stages, yielding advanced cognitive abilities with minimal compute cost. To this end, rStar2-Agent boosts a pre-trained 14B model to state of the art in only 510 RL steps within one week, achieving average pass@1 scores of 80.6% on AIME24 and 69.8% on AIME25, surpassing DeepSeek-R1 (671B) with significantly shorter responses. Beyond mathematics, rStar2-Agent-14B also demonstrates strong generalization to alignment, scientific reasoning, and agentic tool-use tasks. Code and training recipes are available at https://github.com/microsoft/rStar."

[29.08.2025 02:23] Response: ```python
['RL', 'RLHF', 'AGENTS', 'TRAINING']
```
[29.08.2025 02:23] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"rStar2-Agent, a 14B math reasoning model trained with agentic reinforcement learning, achieves state-of-the-art performance by efficiently handling complex problem-solving with advanced cognitive behaviors and minimal computational resources.  					AI-generated summary 				 We introduce rStar2-Agent, a 14B math reasoning model trained with agentic reinforcement learning to achieve frontier-level performance. Beyond current long CoT, the model demonstrates advanced cognitive behaviors, such as thinking carefully before using Python coding tools and reflecting on code execution feedback to autonomously explore, verify, and refine intermediate steps in complex problem-solving. This capability is enabled through three key innovations that makes agentic RL effective at scale: (i) an efficient RL infrastructure with a reliable Python code environment that supports high-throughput execution and mitigates the high rollout costs, enabling training on limited GPU resources (64 MI300X GPUs); (ii) GRPO-RoC, an agentic RL algorithm with a Resample-on-Correct rollout strategy that addresses the inherent environment noises from coding tools, allowing the model to reason more effectively in a code environment; (iii) An efficient agent training recipe that starts with non-reasoning SFT and progresses through multi-RL stages, yielding advanced cognitive abilities with minimal compute cost. To this end, rStar2-Agent boosts a pre-trained 14B model to state of the art in only 510 RL steps within one week, achieving average pass@1 scores of 80.6% on AIME24 and 69.8% on AIME25, surpassing DeepSeek-R1 (671B) with significantly shorter responses. Beyond mathematics, rStar2-Agent-14B also demonstrates strong generalization to alignment, scientific reasoning, and agentic tool-use tasks. Code and training recipes are available at https://github.com/microsoft/rStar."

[29.08.2025 02:23] Response: ```python
['REASONING', 'ALIGNMENT', 'OPTIMIZATION', 'SCIENCE']
```
[29.08.2025 02:23] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The rStar2-Agent is a 14 billion parameter model designed for advanced math reasoning, utilizing agentic reinforcement learning to achieve top performance. It showcases sophisticated cognitive behaviors, such as careful consideration before executing Python code and self-reflection on code outcomes to enhance problem-solving. Key innovations include a robust RL infrastructure that minimizes computational costs, a novel GRPO-RoC algorithm that improves reasoning in coding environments, and a structured training approach that builds cognitive skills efficiently. This model not only excels in mathematical tasks but also generalizes well to other domains like alignment and scientific reasoning.","title":"Revolutionizing Math Reasoning with rStar2-Agent!"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='The rStar2-Agent is a 14 billion parameter model designed for advanced math reasoning, utilizing agentic reinforcement learning to achieve top performance. It showcases sophisticated cognitive behaviors, such as careful consideration before executing Python code and self-reflection on code outcomes to enhance problem-solving. Key innovations include a robust RL infrastructure that minimizes computational costs, a novel GRPO-RoC algorithm that improves reasoning in coding environments, and a structured training approach that builds cognitive skills efficiently. This model not only excels in mathematical tasks but also generalizes well to other domains like alignment and scientific reasoning.', title='Revolutionizing Math Reasoning with rStar2-Agent!'))
[29.08.2025 02:23] Response: ParsedChatCompletionMessage[Article](content='{"desc":"rStar2-Agent是一种14B的数学推理模型，采用代理强化学习进行训练，能够高效处理复杂问题，表现出色。该模型具备先进的认知行为，例如在使用Python编码工具前仔细思考，并根据代码执行反馈进行反思，从而自主探索和验证中间步骤。其成功得益于三项关键创新，包括高效的RL基础设施、GRPO-RoC算法和高效的代理训练方案，使得在有限的计算资源下也能实现卓越性能。rStar2-Agent在仅用510个RL步骤内，便在AIME24和AIME25上取得了80.6%和69.8%的平均通过率，超越了更大模型DeepSeek-R1。","title":"rStar2-Agent：高效的数学推理与认知能力"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='rStar2-Agent是一种14B的数学推理模型，采用代理强化学习进行训练，能够高效处理复杂问题，表现出色。该模型具备先进的认知行为，例如在使用Python编码工具前仔细思考，并根据代码执行反馈进行反思，从而自主探索和验证中间步骤。其成功得益于三项关键创新，包括高效的RL基础设施、GRPO-RoC算法和高效的代理训练方案，使得在有限的计算资源下也能实现卓越性能。rStar2-Agent在仅用510个RL步骤内，便在AIME24和AIME25上取得了80.6%和69.8%的平均通过率，超越了更大模型DeepSeek-R1。', title='rStar2-Agent：高效的数学推理与认知能力'))
[29.08.2025 02:23] Querying the API.
[29.08.2025 02:23] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Long video generation is addressed by introducing a sparse attention routing module, Mixture of Contexts, to efficiently manage long-term memory and retrieval in diffusion transformers.  					AI-generated summary 				 Long video generation is fundamentally a long context memory problem: models must retain and retrieve salient events across a long range without collapsing or drifting. However, scaling diffusion transformers to generate long-context videos is fundamentally limited by the quadratic cost of self-attention, which makes memory and computation intractable and difficult to optimize for long sequences. We recast long-context video generation as an internal information retrieval task and propose a simple, learnable sparse attention routing module, Mixture of Contexts (MoC), as an effective long-term memory retrieval engine. In MoC, each query dynamically selects a few informative chunks plus mandatory anchors (caption, local windows) to attend to, with causal routing that prevents loop closures. As we scale the data and gradually sparsify the routing, the model allocates compute to salient history, preserving identities, actions, and scenes over minutes of content. Efficiency follows as a byproduct of retrieval (near-linear scaling), which enables practical training and synthesis, and the emergence of memory and consistency at the scale of minutes.
[29.08.2025 02:23] Response: {
  "desc": "Эта статья представляет новый метод для генерации длинных видео с использованием разреженного внимания в диффузионных трансформерах. Авторы предлагают модуль Mixture of Contexts (MoC), который эффективно управляет долговременной памятью и извлечением информации. MoC динамически выбирает наиболее информативные фрагменты для обработки, что позволяет сохранять согласованность на протяжении нескольких минут видео. Этот подход решает проблему квадратичной сложности самовнимания и делает возможным практическое обучение и синтез длинных видео.",
  "emoji": "🎬",
  "title": "Эффективная генерация длинных видео с помощью разреженного внимания"
}
[29.08.2025 02:23] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Long video generation is addressed by introducing a sparse attention routing module, Mixture of Contexts, to efficiently manage long-term memory and retrieval in diffusion transformers.  					AI-generated summary 				 Long video generation is fundamentally a long context memory problem: models must retain and retrieve salient events across a long range without collapsing or drifting. However, scaling diffusion transformers to generate long-context videos is fundamentally limited by the quadratic cost of self-attention, which makes memory and computation intractable and difficult to optimize for long sequences. We recast long-context video generation as an internal information retrieval task and propose a simple, learnable sparse attention routing module, Mixture of Contexts (MoC), as an effective long-term memory retrieval engine. In MoC, each query dynamically selects a few informative chunks plus mandatory anchors (caption, local windows) to attend to, with causal routing that prevents loop closures. As we scale the data and gradually sparsify the routing, the model allocates compute to salient history, preserving identities, actions, and scenes over minutes of content. Efficiency follows as a byproduct of retrieval (near-linear scaling), which enables practical training and synthesis, and the emergence of memory and consistency at the scale of minutes."

[29.08.2025 02:23] Response: ```python
['VIDEO', 'MULTIMODAL', 'ARCHITECTURE']
```
[29.08.2025 02:23] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Long video generation is addressed by introducing a sparse attention routing module, Mixture of Contexts, to efficiently manage long-term memory and retrieval in diffusion transformers.  					AI-generated summary 				 Long video generation is fundamentally a long context memory problem: models must retain and retrieve salient events across a long range without collapsing or drifting. However, scaling diffusion transformers to generate long-context videos is fundamentally limited by the quadratic cost of self-attention, which makes memory and computation intractable and difficult to optimize for long sequences. We recast long-context video generation as an internal information retrieval task and propose a simple, learnable sparse attention routing module, Mixture of Contexts (MoC), as an effective long-term memory retrieval engine. In MoC, each query dynamically selects a few informative chunks plus mandatory anchors (caption, local windows) to attend to, with causal routing that prevents loop closures. As we scale the data and gradually sparsify the routing, the model allocates compute to salient history, preserving identities, actions, and scenes over minutes of content. Efficiency follows as a byproduct of retrieval (near-linear scaling), which enables practical training and synthesis, and the emergence of memory and consistency at the scale of minutes."

[29.08.2025 02:23] Response: ```python
["LONG_CONTEXT", "DIFFUSION"]
```
[29.08.2025 02:23] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents a new approach to generating long videos using a method called Mixture of Contexts (MoC), which enhances the efficiency of memory retrieval in diffusion transformers. The challenge of long video generation lies in managing long-term memory to retain important events without losing coherence. MoC addresses this by allowing the model to focus on a few key segments of information while maintaining essential context, thus optimizing the use of computational resources. As a result, this method not only improves the model\'s ability to generate consistent and coherent long videos but also makes the training process more efficient.","title":"Efficient Long Video Generation with Sparse Attention"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="This paper presents a new approach to generating long videos using a method called Mixture of Contexts (MoC), which enhances the efficiency of memory retrieval in diffusion transformers. The challenge of long video generation lies in managing long-term memory to retain important events without losing coherence. MoC addresses this by allowing the model to focus on a few key segments of information while maintaining essential context, thus optimizing the use of computational resources. As a result, this method not only improves the model's ability to generate consistent and coherent long videos but also makes the training process more efficient.", title='Efficient Long Video Generation with Sparse Attention'))
[29.08.2025 02:23] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本文提出了一种稀疏注意力路由模块，称为上下文混合（Mixture of Contexts, MoC），用于高效管理扩散变换器中的长期记忆和检索，以生成长视频。长视频生成本质上是一个长期上下文记忆问题，模型需要在长时间范围内保留和检索重要事件。传统的自注意力机制在处理长序列时计算成本呈二次增长，导致内存和计算变得难以优化。MoC模块通过动态选择信息丰富的片段和必要的锚点，提供了一种有效的长期记忆检索引擎，从而提高了生成长视频的效率和一致性。","title":"高效生成长视频的记忆检索新方法"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本文提出了一种稀疏注意力路由模块，称为上下文混合（Mixture of Contexts, MoC），用于高效管理扩散变换器中的长期记忆和检索，以生成长视频。长视频生成本质上是一个长期上下文记忆问题，模型需要在长时间范围内保留和检索重要事件。传统的自注意力机制在处理长序列时计算成本呈二次增长，导致内存和计算变得难以优化。MoC模块通过动态选择信息丰富的片段和必要的锚点，提供了一种有效的长期记忆检索引擎，从而提高了生成长视频的效率和一致性。', title='高效生成长视频的记忆检索新方法'))
[29.08.2025 02:23] Querying the API.
[29.08.2025 02:23] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

MCP-Bench evaluates large language models on complex, multi-step tasks requiring tool use, coordination, and planning across various domains.  					AI-generated summary 				 We introduce MCP-Bench, a benchmark for evaluating large language models (LLMs) on realistic, multi-step tasks that demand tool use, cross-tool coordination, precise parameter control, and planning/reasoning for solving tasks. Built on the Model Context Protocol (MCP), MCP-Bench connects LLMs to 28 representative live MCP servers spanning 250 tools across domains such as finance, traveling, scientific computing, and academic search. Unlike prior API-based benchmarks, each MCP server provides a set of complementary tools designed to work together, enabling the construction of authentic, multi-step tasks with rich input-output coupling. Tasks in MCP-Bench test agents' ability to retrieve relevant tools from fuzzy instructions without explicit tool names, plan multi-hop execution trajectories for complex objectives, ground responses in intermediate tool outputs, and orchestrate cross-domain workflows - capabilities not adequately evaluated by existing benchmarks that rely on explicit tool specifications, shallow few-step workflows, and isolated domain operations. We propose a multi-faceted evaluation framework covering tool-level schema understanding and usage, trajectory-level planning, and task completion. Experiments on 20 advanced LLMs reveal persistent challenges in MCP-Bench. Code and data: https://github.com/Accenture/mcp-bench.
[29.08.2025 02:23] Response: {
  "desc": "MCP-Bench - это новый бенчмарк для оценки больших языковых моделей (LLM) на сложных многоэтапных задачах, требующих использования инструментов и планирования в различных областях. Он включает 28 серверов MCP с 250 инструментами в таких сферах как финансы, путешествия, научные вычисления и академический поиск. MCP-Bench оценивает способность моделей извлекать релевантные инструменты из нечетких инструкций, планировать многоэтапные траектории выполнения и координировать рабочие процессы между различными доменами. Эксперименты на 20 продвинутых LLM выявили устойчивые проблемы в решении задач MCP-Bench.",
  "emoji": "🧠",
  "title": "MCP-Bench: Новый рубеж в оценке многозадачных языковых моделей"
}
[29.08.2025 02:23] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"MCP-Bench evaluates large language models on complex, multi-step tasks requiring tool use, coordination, and planning across various domains.  					AI-generated summary 				 We introduce MCP-Bench, a benchmark for evaluating large language models (LLMs) on realistic, multi-step tasks that demand tool use, cross-tool coordination, precise parameter control, and planning/reasoning for solving tasks. Built on the Model Context Protocol (MCP), MCP-Bench connects LLMs to 28 representative live MCP servers spanning 250 tools across domains such as finance, traveling, scientific computing, and academic search. Unlike prior API-based benchmarks, each MCP server provides a set of complementary tools designed to work together, enabling the construction of authentic, multi-step tasks with rich input-output coupling. Tasks in MCP-Bench test agents' ability to retrieve relevant tools from fuzzy instructions without explicit tool names, plan multi-hop execution trajectories for complex objectives, ground responses in intermediate tool outputs, and orchestrate cross-domain workflows - capabilities not adequately evaluated by existing benchmarks that rely on explicit tool specifications, shallow few-step workflows, and isolated domain operations. We propose a multi-faceted evaluation framework covering tool-level schema understanding and usage, trajectory-level planning, and task completion. Experiments on 20 advanced LLMs reveal persistent challenges in MCP-Bench. Code and data: https://github.com/Accenture/mcp-bench."

[29.08.2025 02:23] Response: ```python
['BENCHMARK', 'AGENTS']
```
[29.08.2025 02:23] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"MCP-Bench evaluates large language models on complex, multi-step tasks requiring tool use, coordination, and planning across various domains.  					AI-generated summary 				 We introduce MCP-Bench, a benchmark for evaluating large language models (LLMs) on realistic, multi-step tasks that demand tool use, cross-tool coordination, precise parameter control, and planning/reasoning for solving tasks. Built on the Model Context Protocol (MCP), MCP-Bench connects LLMs to 28 representative live MCP servers spanning 250 tools across domains such as finance, traveling, scientific computing, and academic search. Unlike prior API-based benchmarks, each MCP server provides a set of complementary tools designed to work together, enabling the construction of authentic, multi-step tasks with rich input-output coupling. Tasks in MCP-Bench test agents' ability to retrieve relevant tools from fuzzy instructions without explicit tool names, plan multi-hop execution trajectories for complex objectives, ground responses in intermediate tool outputs, and orchestrate cross-domain workflows - capabilities not adequately evaluated by existing benchmarks that rely on explicit tool specifications, shallow few-step workflows, and isolated domain operations. We propose a multi-faceted evaluation framework covering tool-level schema understanding and usage, trajectory-level planning, and task completion. Experiments on 20 advanced LLMs reveal persistent challenges in MCP-Bench. Code and data: https://github.com/Accenture/mcp-bench."

[29.08.2025 02:23] Response: ```python
["REASONING", "OPTIMIZATION"]
```
[29.08.2025 02:23] Response: ParsedChatCompletionMessage[Article](content='{"desc":"MCP-Bench is a new benchmark designed to assess large language models (LLMs) on complex tasks that require the use of multiple tools and careful planning. It connects LLMs to a variety of live servers that provide complementary tools across different domains, allowing for realistic multi-step task execution. The benchmark evaluates the models\' abilities to understand fuzzy instructions, plan execution paths, and coordinate workflows across different domains. Results from testing 20 advanced LLMs show that there are still significant challenges in effectively completing these complex tasks.","title":"MCP-Bench: Evaluating LLMs on Complex Multi-Step Tasks"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="MCP-Bench is a new benchmark designed to assess large language models (LLMs) on complex tasks that require the use of multiple tools and careful planning. It connects LLMs to a variety of live servers that provide complementary tools across different domains, allowing for realistic multi-step task execution. The benchmark evaluates the models' abilities to understand fuzzy instructions, plan execution paths, and coordinate workflows across different domains. Results from testing 20 advanced LLMs show that there are still significant challenges in effectively completing these complex tasks.", title='MCP-Bench: Evaluating LLMs on Complex Multi-Step Tasks'))
[29.08.2025 02:23] Response: ParsedChatCompletionMessage[Article](content='{"desc":"MCP-Bench是一个用于评估大型语言模型（LLMs）的基准，专注于复杂的多步骤任务，这些任务需要工具使用、跨工具协调和规划能力。该基准基于模型上下文协议（MCP），连接到28个代表性的MCP服务器，涵盖250种工具，涉及金融、旅行、科学计算和学术搜索等多个领域。与以往的API基准不同，MCP服务器提供了一组互补工具，能够构建真实的多步骤任务，测试模型在模糊指令下检索相关工具的能力。实验表明，当前的LLMs在MCP-Bench中面临持续的挑战，特别是在工具理解、规划和任务完成方面。","title":"评估大型语言模型的多步骤任务能力"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='MCP-Bench是一个用于评估大型语言模型（LLMs）的基准，专注于复杂的多步骤任务，这些任务需要工具使用、跨工具协调和规划能力。该基准基于模型上下文协议（MCP），连接到28个代表性的MCP服务器，涵盖250种工具，涉及金融、旅行、科学计算和学术搜索等多个领域。与以往的API基准不同，MCP服务器提供了一组互补工具，能够构建真实的多步骤任务，测试模型在模糊指令下检索相关工具的能力。实验表明，当前的LLMs在MCP-Bench中面临持续的挑战，特别是在工具理解、规划和任务完成方面。', title='评估大型语言模型的多步骤任务能力'))
[29.08.2025 02:23] Renaming data file.
[29.08.2025 02:23] Renaming previous data. hf_papers.json to ./d/2025-08-29.json
[29.08.2025 02:23] Saving new data file.
[29.08.2025 02:23] Generating page.
[29.08.2025 02:23] Renaming previous page.
[29.08.2025 02:23] Renaming previous data. index.html to ./d/2025-08-29.html
[29.08.2025 02:23] Writing result.
[29.08.2025 02:23] Renaming log file.
[29.08.2025 02:23] Renaming previous data. log.txt to ./logs/2025-08-29_last_log.txt
