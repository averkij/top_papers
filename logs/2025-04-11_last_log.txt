[11.04.2025 07:11] Read previous papers.
[11.04.2025 07:11] Generating top page (month).
[11.04.2025 07:11] Writing top page (month).
[11.04.2025 08:14] Read previous papers.
[11.04.2025 08:14] Get feed.
[11.04.2025 08:14] Get page data from previous paper. URL: https://huggingface.co/papers/2504.07491
[11.04.2025 08:14] Get page data from previous paper. URL: https://huggingface.co/papers/2504.07956
[11.04.2025 08:14] Get page data from previous paper. URL: https://huggingface.co/papers/2504.07957
[11.04.2025 08:14] Get page data from previous paper. URL: https://huggingface.co/papers/2504.07960
[11.04.2025 08:14] Get page data from previous paper. URL: https://huggingface.co/papers/2504.07943
[11.04.2025 08:14] Get page data from previous paper. URL: https://huggingface.co/papers/2504.07128
[11.04.2025 08:14] Get page data from previous paper. URL: https://huggingface.co/papers/2504.07964
[11.04.2025 08:14] Get page data from previous paper. URL: https://huggingface.co/papers/2504.07830
[11.04.2025 08:14] Get page data from previous paper. URL: https://huggingface.co/papers/2504.07934
[11.04.2025 08:14] Get page data from previous paper. URL: https://huggingface.co/papers/2504.04974
[11.04.2025 08:14] Extract page data from URL. URL: https://huggingface.co/papers/2504.06801
[11.04.2025 08:14] Extract page data from URL. URL: https://huggingface.co/papers/2504.06752
[11.04.2025 08:14] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[11.04.2025 08:14] No deleted papers detected.
[11.04.2025 08:14] Downloading and parsing papers (pdf, html). Total: 12.
[11.04.2025 08:14] Downloading and parsing paper https://huggingface.co/papers/2504.07491.
[11.04.2025 08:14] Extra JSON file exists (./assets/json/2504.07491.json), skip PDF parsing.
[11.04.2025 08:14] Paper image links file exists (./assets/img_data/2504.07491.json), skip HTML parsing.
[11.04.2025 08:14] Success.
[11.04.2025 08:14] Downloading and parsing paper https://huggingface.co/papers/2504.07956.
[11.04.2025 08:14] Extra JSON file exists (./assets/json/2504.07956.json), skip PDF parsing.
[11.04.2025 08:14] Paper image links file exists (./assets/img_data/2504.07956.json), skip HTML parsing.
[11.04.2025 08:14] Success.
[11.04.2025 08:14] Downloading and parsing paper https://huggingface.co/papers/2504.07957.
[11.04.2025 08:14] Extra JSON file exists (./assets/json/2504.07957.json), skip PDF parsing.
[11.04.2025 08:14] Paper image links file exists (./assets/img_data/2504.07957.json), skip HTML parsing.
[11.04.2025 08:14] Success.
[11.04.2025 08:14] Downloading and parsing paper https://huggingface.co/papers/2504.07960.
[11.04.2025 08:14] Extra JSON file exists (./assets/json/2504.07960.json), skip PDF parsing.
[11.04.2025 08:14] Paper image links file exists (./assets/img_data/2504.07960.json), skip HTML parsing.
[11.04.2025 08:14] Success.
[11.04.2025 08:14] Downloading and parsing paper https://huggingface.co/papers/2504.07943.
[11.04.2025 08:14] Extra JSON file exists (./assets/json/2504.07943.json), skip PDF parsing.
[11.04.2025 08:14] Paper image links file exists (./assets/img_data/2504.07943.json), skip HTML parsing.
[11.04.2025 08:14] Success.
[11.04.2025 08:14] Downloading and parsing paper https://huggingface.co/papers/2504.07128.
[11.04.2025 08:14] Extra JSON file exists (./assets/json/2504.07128.json), skip PDF parsing.
[11.04.2025 08:14] Paper image links file exists (./assets/img_data/2504.07128.json), skip HTML parsing.
[11.04.2025 08:14] Success.
[11.04.2025 08:14] Downloading and parsing paper https://huggingface.co/papers/2504.07964.
[11.04.2025 08:14] Extra JSON file exists (./assets/json/2504.07964.json), skip PDF parsing.
[11.04.2025 08:14] Paper image links file exists (./assets/img_data/2504.07964.json), skip HTML parsing.
[11.04.2025 08:14] Success.
[11.04.2025 08:14] Downloading and parsing paper https://huggingface.co/papers/2504.07830.
[11.04.2025 08:14] Extra JSON file exists (./assets/json/2504.07830.json), skip PDF parsing.
[11.04.2025 08:14] Paper image links file exists (./assets/img_data/2504.07830.json), skip HTML parsing.
[11.04.2025 08:14] Success.
[11.04.2025 08:14] Downloading and parsing paper https://huggingface.co/papers/2504.07934.
[11.04.2025 08:14] Extra JSON file exists (./assets/json/2504.07934.json), skip PDF parsing.
[11.04.2025 08:14] Paper image links file exists (./assets/img_data/2504.07934.json), skip HTML parsing.
[11.04.2025 08:14] Success.
[11.04.2025 08:14] Downloading and parsing paper https://huggingface.co/papers/2504.04974.
[11.04.2025 08:14] Extra JSON file exists (./assets/json/2504.04974.json), skip PDF parsing.
[11.04.2025 08:14] Paper image links file exists (./assets/img_data/2504.04974.json), skip HTML parsing.
[11.04.2025 08:14] Success.
[11.04.2025 08:14] Downloading and parsing paper https://huggingface.co/papers/2504.06801.
[11.04.2025 08:14] Downloading paper 2504.06801 from http://arxiv.org/pdf/2504.06801v2...
[11.04.2025 08:14] Extracting affiliations from text.
[11.04.2025 08:14] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"MonoPlace3D: Learning 3D-Aware Object Placement for 3D Monocular Detection Rishubh Parihar* Srinjay Sarkar Sarthak Vora Jogendra Nath Kundu R. Venkatesh Babu 5 2 0 2 0 1 ] . [ 2 1 0 8 6 0 . 4 0 5 2 : r a "
[11.04.2025 08:14] Response: []
[11.04.2025 08:14] Extracting affiliations from text.
[11.04.2025 08:14] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"MonoPlace3D: Learning 3D-Aware Object Placement for 3D Monocular Detection Rishubh Parihar* Srinjay Sarkar Sarthak Vora Jogendra Nath Kundu R. Venkatesh Babu5 2 0 2 0 1 ] . [ 2 1 0 8 6 0 . 4 0 5 2 : r aCurrent monocular 3D detectors are held back by the limited diversity and scale of real-world datasets. While data augmentation certainly helps, its particularly difficult to generate realistic scene-aware augmented data for outdoor settings. Most current approaches to synthetic data generation focus on realistic object appearance through improved rendering techniques. However, we show that where and how objects are positioned is just as crucial for training effective 3D monocular detectors. The key obstacle lies in automatically determining realistic object placement parameters - including position, dimensions, and directional alignment when introducing synthetic objects into actual scenes. To address this, we introduce MonoPlace3D, novel system that considers the 3D scene content to create realistic augmentations. Specifically, given background scene, MonoPlace3D learns distribution over plausible 3D bounding boxes. Subsequently, we render realistic objects and place them according to the locations sampled from the learned distribution. Our comprehensive evaluation on two standard datasets KITTI and NuScenes, demonstrates that MonoPlace3D significantly improves the accuracy of multiple existing monocular 3D detectors while being highly data efficient. project page 1. Introduction Monocular 3D object detection has rapidly progressed recently, enabling its use in autonomous navigation and robotics [18, 32]. However, the performance of 3D detectors relies heavily on the quantity and quality of the training dataset. Given the considerable effort and time required to curate extensive, real-world 3D-annotated datasets, specialized data augmentation for 3D object detection has emerged as promising direction. However designing realistic augmentations for 3D tasks, is non-trivial, as the generated augmentations must adhere to the physical constraints of the real world, such as maintaining 3D geometric consistency and handling collisions. *equal contribution Figure 1. a) We compare augmentations from our learned placement with heuristic-based placements from Lift3D [22]. In our augmentations, vehicles follow the lane orientations and are placed appropriately. b) These realistic augmentations significantly improve the 3D detection performance (KITTI [6] val set, (easy)). Notably, we achieve detection performance comparable to that of the fully labeled dataset using only 50% of the dataset. Existing techniques [14, 25] for 3D augmentation use relatively simple heuristics for placing synthetic objects in an input scene. For instance, in the context of road scenes, recent approach [22] generates realistic cars and places them on the segmented road region. However, such heuristics result in highly unnatural scene augmentations  (Fig. 1)  , resulting in marginal improvement in 3D detection performance. In this work, we ask the following two crucial questions: (1) What key factors are essential for generating realistic augmentations to improve monocular 3D object detection?, and (2) How can these factors be integrated to generate effective scene-aware augmentations? For the first question, we discover two critical factors responsible for generating effective 3D augmentations: 1. Object Placement: Plausible placement of augmented objects, with appropriate object placement (location, scale, and orientation), is essential for rendering realistic scene augmentations. For instance, in road scenes, car should be placed on the road, be of appropriate size based on the distance from the camera, and follow the lane orientation. Augmentations that respect such physical constraints generalize better to real scenes by faithfully modelling the true distribution of the vehicles in the real world. To give an example of how such an augmentation looks, we compare our proposed augmentation approach against heuristicbased placement from Lift3D [22] in Fig. 1. Given the same rendering, our generation looks much more plausible regarding car placement and orientation compared to the baseline approach. Notably, when used for object detection training, our approach leads to significantly greater performance improvement, making the detector not only performant, but also highly data efficient (refer Fig. 1c) 2. Object Appearance: For 3D augmentation, it is desired that the generated objects exhibit realism and seamlessly integrate with the background to preserve visual consistency. This, in turn, minimizes the domain disparity between real and augmented data. Existing augmentation methods for 3D detection [14, 22, 25] primarily focus on the object appearance. This limits their ability to exploit the full potential of the data augmentations for 3D detection. To address both these factors, we propose MonoPlace3D, novel scene-aware augmentation method that generates effective 3D augmentations, as shown in Fig. 1. For plausible object placement, we train 3D Scene-Aware Placement Network (SA-PlaceNet), which maps given scene image to distribution of plausible 3D bounding boxes. It learns realistic object placements that adhere to the physical rules of road scenes, facilitating sampling of diverse and plausible 3D bounding boxes (see Fig. 1a). For training this network, we consider existing 3D detection datasets, which typically contain only limited number of objects per scene, resulting in sparse training signal. Therefore, to enable dense placement prediction, we introduce novel modules based on (1) geometric augmentations of 3D boxes, along with (2) modeling of continuous distribution of 3D boxes. For realistic object appearance, we propose rendering pipeline that leverages synthetic 3D assets and an imageto-image translation model. We translate the synthetic renderings into realistic version using ControlNet [53](see Fig. 1b) and blend them with the background to get final augmentations. This allows us to utilize amateur-quality 3D assets and transform them into diverse, highly realistic car renderings that resemble real-world scenes. Our two-stage augmentation approach is highly effective and modular, allowing seamless integration with advancements in placement and rendering for enhancing 3D object detection datasets. Using our augmentation method on popular 3D detection datasets led to significant improvements over the "
[11.04.2025 08:14] Mistral response. {"id": "616394bd4cdf488f8063555406c3f940", "object": "chat.completion", "created": 1744359291, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "[]"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1475, "total_tokens": 1477, "completion_tokens": 2}}
[11.04.2025 08:14] Response: []
[11.04.2025 08:14] Deleting PDF ./assets/pdf/2504.06801.pdf.
[11.04.2025 08:14] Success.
[11.04.2025 08:14] Downloading and parsing paper https://huggingface.co/papers/2504.06752.
[11.04.2025 08:14] Downloading paper 2504.06752 from http://arxiv.org/pdf/2504.06752v2...
[11.04.2025 08:15] Extracting affiliations from text.
[11.04.2025 08:15] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Compass Control: Multi Object Orientation Control for Text-to-Image Generation Rishubh Parihar1* Vaibhav Agrawal2 Sachidanand VS1 R. Venkatesh Babu1 1IISc Bangalore 2IIIT Hyderabad 5 2 0 2 0 ] . [ 2 2 5 7 6 0 . 4 0 5 2 : r Figure 1. We present Compass Control, method to generate multi-object scenes with orientation control from text-to-image diffusion models. Given text prompt and an orientation of each object (shown as frustum, the colored face is the forward direction), our method generates scenes that align with both the prompt and specified orientations. Additionally, with few ( 10) unposed images of new object, our model is personalized to generate the object in target orientations. "
[11.04.2025 08:15] Response: ```python
["IISc Bangalore", "IIIT Hyderabad"]
```
[11.04.2025 08:15] Deleting PDF ./assets/pdf/2504.06752.pdf.
[11.04.2025 08:15] Success.
[11.04.2025 08:15] Enriching papers with extra data.
[11.04.2025 08:15] ********************************************************************************
[11.04.2025 08:15] Abstract 0. We present Kimi-VL, an efficient open-source Mixture-of-Experts (MoE) vision-language model (VLM) that offers advanced multimodal reasoning, long-context understanding, and strong agent capabilities - all while activating only 2.8B parameters in its language decoder (Kimi-VL-A3B). Kimi-VL demonstrat...
[11.04.2025 08:15] ********************************************************************************
[11.04.2025 08:15] Abstract 1. The advancement of Chain-of-Thought (CoT) reasoning has significantly enhanced the capabilities of large language models (LLMs) and large vision-language models (LVLMs). However, a rigorous evaluation framework for video CoT reasoning remains absent. Current video benchmarks fail to adequately asses...
[11.04.2025 08:15] ********************************************************************************
[11.04.2025 08:15] Abstract 2. The Instruction Following (IF) ability measures how well Multi-modal Large Language Models (MLLMs) understand exactly what users are telling them and whether they are doing it right. Existing multimodal instruction following training data is scarce, the benchmarks are simple with atomic instructions...
[11.04.2025 08:15] ********************************************************************************
[11.04.2025 08:15] Abstract 3. Recent progress in diffusion models significantly advances various image generation tasks. However, the current mainstream approach remains focused on building task-specific models, which have limited efficiency when supporting a wide range of different needs. While universal models attempt to addre...
[11.04.2025 08:15] ********************************************************************************
[11.04.2025 08:15] Abstract 4. 3D part amodal segmentation--decomposing a 3D shape into complete, semantically meaningful parts, even when occluded--is a challenging but crucial task for 3D content creation and understanding. Existing 3D part segmentation methods only identify visible surface patches, limiting their utility. Insp...
[11.04.2025 08:15] ********************************************************************************
[11.04.2025 08:15] Abstract 5. Large Reasoning Models like DeepSeek-R1 mark a fundamental shift in how LLMs approach complex problems. Instead of directly producing an answer for a given input, DeepSeek-R1 creates detailed multi-step reasoning chains, seemingly "thinking" about a problem before providing an answer. This reasoning...
[11.04.2025 08:15] ********************************************************************************
[11.04.2025 08:15] Abstract 6. Mixture-of-Experts (MoE) Large Language Models (LLMs) suffer from severely sub-optimal expert pathways-our study reveals that naive expert selection learned from pretraining leaves a surprising 10-20% accuracy gap for improvement. Motivated by this observation, we develop a novel class of test-time ...
[11.04.2025 08:15] ********************************************************************************
[11.04.2025 08:15] Abstract 7. We present a novel, open-source social network simulation framework, MOSAIC, where generative language agents predict user behaviors such as liking, sharing, and flagging content. This simulation combines LLM agents with a directed social graph to analyze emergent deception behaviors and gain a bett...
[11.04.2025 08:15] ********************************************************************************
[11.04.2025 08:15] Abstract 8. In this paper, we present an effective method to enhance visual reasoning with significantly fewer training samples, relying purely on self-improvement with no knowledge distillation. Our key insight is that the difficulty of training data during reinforcement fine-tuning (RFT) is critical. Appropri...
[11.04.2025 08:15] ********************************************************************************
[11.04.2025 08:15] Abstract 9. Despite the existing evolution of Multimodal Large Language Models (MLLMs), a non-neglectable limitation remains in their struggle with visual text grounding, especially in text-rich images of documents. Document images, such as scanned forms and infographics, highlight critical challenges due to th...
[11.04.2025 08:15] ********************************************************************************
[11.04.2025 08:15] Abstract 10. Current monocular 3D detectors are held back by the limited diversity and scale of real-world datasets. While data augmentation certainly helps, it's particularly difficult to generate realistic scene-aware augmented data for outdoor settings. Most current approaches to synthetic data generation foc...
[11.04.2025 08:15] ********************************************************************************
[11.04.2025 08:15] Abstract 11. Existing approaches for controlling text-to-image diffusion models, while powerful, do not allow for explicit 3D object-centric control, such as precise control of object orientation. In this work, we address the problem of multi-object orientation control in text-to-image diffusion models. This ena...
[11.04.2025 08:15] Read previous papers.
[11.04.2025 08:15] Generating reviews via LLM API.
[11.04.2025 08:15] Using data from previous issue: {"categories": ["#cv", "#agents", "#rl", "#multimodal", "#long_context", "#reasoning", "#small_models", "#training", "#open_source"], "emoji": "ğŸ§ ", "ru": {"title": "Kimi-VL: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹", "desc": "Kimi-VL - ÑÑ‚Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½
[11.04.2025 08:15] Using data from previous issue: {"categories": ["#reasoning", "#benchmark", "#video"], "emoji": "ğŸ¥", "ru": {"title": "VCR-Bench: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ˜Ğ˜ Ğ¿Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾", "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº VCR-Bench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ğ¼Ñ‹ÑĞ»ĞµĞ¹ Ğ¿
[11.04.2025 08:15] Using data from previous issue: {"categories": ["#benchmark", "#training", "#dataset", "#data", "#optimization", "#open_source", "#multimodal", "#alignment"], "emoji": "ğŸ¤–", "ru": {"title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ MM-IFEngine - Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾
[11.04.2025 08:15] Using data from previous issue: {"categories": ["#cv", "#graphs", "#transfer_learning", "#diffusion", "#multimodal", "#dataset"], "emoji": "ğŸ¨", "ru": {"title": "VisualCloze: ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼", "desc": "VisualCloze - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ 
[11.04.2025 08:15] Using data from previous issue: {"categories": ["#3d", "#architecture", "#diffusion", "#benchmark", "#optimization"], "emoji": "ğŸ§©", "ru": {"title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² 3D-ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸: Ğ²Ğ¸Ğ´ĞµÑ‚ÑŒ Ğ½ĞµĞ²Ğ¸Ğ´Ğ¸Ğ¼Ğ¾Ğµ", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ°Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ñ‡Ğ°ÑÑ‚ĞµĞ¹ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´: ÑĞ½Ğ°Ñ‡
[11.04.2025 08:15] Using data from previous issue: {"categories": ["#architecture", "#ethics", "#inference", "#reasoning", "#long_context", "#rl"], "emoji": "ğŸ§ ", "ru": {"title": "DeepSeek-R1: Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ¼Ğ°ÑˆĞ¸Ğ½ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¸Ğ·Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ DeepSeek-R1, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½Ñ‡Ğ°Ñ‚Ñ‹Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°
[11.04.2025 08:15] Using data from previous issue: {"categories": ["#optimization", "#benchmark", "#training", "#architecture"], "emoji": "ğŸ§ ", "ru": {"title": "ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿ÑƒÑ‚ĞµĞ¹ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹", "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ·Ñ‹ĞºĞ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ¼ĞµÑĞ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² (MoE LLM) Ğ¸Ğ¼ĞµÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» 
[11.04.2025 08:15] Using data from previous issue: {"categories": ["#agents", "#graphs", "#reasoning", "#games", "#multimodal", "#open_source"], "emoji": "ğŸ•¸ï¸", "ru": {"title": "Ğ¦Ğ¸Ñ„Ñ€Ğ¾Ğ²Ğ¾Ğµ Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ğ¾Ğ´ Ğ¼Ğ¸ĞºÑ€Ğ¾ÑĞºĞ¾Ğ¿Ğ¾Ğ¼ Ğ˜Ğ˜", "desc": "MOSAIC - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¸Ğ¼Ğ¸Ñ‚Ğ°Ñ†Ğ¸Ğ¸
[11.04.2025 08:15] Using data from previous issue: {"categories": ["#cv", "#data", "#reasoning", "#training", "#open_source", "#benchmark"], "emoji": "ğŸ§ ", "ru": {"title": "ĞœĞµĞ½ÑŒÑˆĞµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¸ Ğ˜Ğ˜", "desc": "Ğ’ ÑÑ‚Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ·Ğ½Ğ°
[11.04.2025 08:15] Using data from previous issue: {"categories": ["#benchmark", "#training", "#dataset", "#synthetic", "#transfer_learning", "#multimodal", "#reasoning"], "emoji": "ğŸ“„", "ru": {"title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ TRIG Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ
[11.04.2025 08:15] Querying the API.
[11.04.2025 08:15] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Current monocular 3D detectors are held back by the limited diversity and scale of real-world datasets. While data augmentation certainly helps, it's particularly difficult to generate realistic scene-aware augmented data for outdoor settings. Most current approaches to synthetic data generation focus on realistic object appearance through improved rendering techniques. However, we show that where and how objects are positioned is just as crucial for training effective 3D monocular detectors. The key obstacle lies in automatically determining realistic object placement parameters - including position, dimensions, and directional alignment when introducing synthetic objects into actual scenes. To address this, we introduce MonoPlace3D, a novel system that considers the 3D scene content to create realistic augmentations. Specifically, given a background scene, MonoPlace3D learns a distribution over plausible 3D bounding boxes. Subsequently, we render realistic objects and place them according to the locations sampled from the learned distribution. Our comprehensive evaluation on two standard datasets KITTI and NuScenes, demonstrates that MonoPlace3D significantly improves the accuracy of multiple existing monocular 3D detectors while being highly data efficient.
[11.04.2025 08:15] Response: {
  "desc": "MonoPlace3D - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ½Ğ¾ĞºÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… 3D Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ². ĞĞ½Ğ° ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ğ¼Ğ¾Ğµ 3D ÑÑ†ĞµĞ½Ñ‹ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ñ‹Ñ… 3D Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ñ€Ğ°Ğ¼Ğ¾Ğº, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ñ‚ Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‰Ğ°ĞµÑ‚ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ½Ğ¾ ÑÑ‚Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… KITTI Ğ¸ NuScenes Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ½Ğ¾ĞºÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… 3D Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….",
  "emoji": "ğŸ”",
  "title": "Ğ£Ğ¼Ğ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‰ĞµĞ½Ğ¸Ğµ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ½Ğ¾ĞºÑƒĞ»ÑÑ€Ğ½Ğ¾Ğ¹ 3D Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ğ¸"
}
[11.04.2025 08:15] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Current monocular 3D detectors are held back by the limited diversity and scale of real-world datasets. While data augmentation certainly helps, it's particularly difficult to generate realistic scene-aware augmented data for outdoor settings. Most current approaches to synthetic data generation focus on realistic object appearance through improved rendering techniques. However, we show that where and how objects are positioned is just as crucial for training effective 3D monocular detectors. The key obstacle lies in automatically determining realistic object placement parameters - including position, dimensions, and directional alignment when introducing synthetic objects into actual scenes. To address this, we introduce MonoPlace3D, a novel system that considers the 3D scene content to create realistic augmentations. Specifically, given a background scene, MonoPlace3D learns a distribution over plausible 3D bounding boxes. Subsequently, we render realistic objects and place them according to the locations sampled from the learned distribution. Our comprehensive evaluation on two standard datasets KITTI and NuScenes, demonstrates that MonoPlace3D significantly improves the accuracy of multiple existing monocular 3D detectors while being highly data efficient."

[11.04.2025 08:15] Response: ```python
['DATASET', '3D']
```
[11.04.2025 08:15] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Current monocular 3D detectors are held back by the limited diversity and scale of real-world datasets. While data augmentation certainly helps, it's particularly difficult to generate realistic scene-aware augmented data for outdoor settings. Most current approaches to synthetic data generation focus on realistic object appearance through improved rendering techniques. However, we show that where and how objects are positioned is just as crucial for training effective 3D monocular detectors. The key obstacle lies in automatically determining realistic object placement parameters - including position, dimensions, and directional alignment when introducing synthetic objects into actual scenes. To address this, we introduce MonoPlace3D, a novel system that considers the 3D scene content to create realistic augmentations. Specifically, given a background scene, MonoPlace3D learns a distribution over plausible 3D bounding boxes. Subsequently, we render realistic objects and place them according to the locations sampled from the learned distribution. Our comprehensive evaluation on two standard datasets KITTI and NuScenes, demonstrates that MonoPlace3D significantly improves the accuracy of multiple existing monocular 3D detectors while being highly data efficient."

[11.04.2025 08:15] Response: ```python
["SYNTHETIC", "OPTIMIZATION"]
```
[11.04.2025 08:15] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper addresses the limitations of current monocular 3D detectors due to the lack of diverse real-world datasets. It highlights the importance of not just realistic object appearance but also the correct placement of objects in 3D scenes for effective training. The authors introduce MonoPlace3D, a system that learns to generate realistic object placements based on the content of the background scene. Their experiments show that MonoPlace3D enhances the performance of existing monocular 3D detectors on standard datasets like KITTI and NuScenes, demonstrating improved accuracy and data efficiency.","title":"Enhancing Monocular 3D Detection with Realistic Object Placement"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper addresses the limitations of current monocular 3D detectors due to the lack of diverse real-world datasets. It highlights the importance of not just realistic object appearance but also the correct placement of objects in 3D scenes for effective training. The authors introduce MonoPlace3D, a system that learns to generate realistic object placements based on the content of the background scene. Their experiments show that MonoPlace3D enhances the performance of existing monocular 3D detectors on standard datasets like KITTI and NuScenes, demonstrating improved accuracy and data efficiency.', title='Enhancing Monocular 3D Detection with Realistic Object Placement'))
[11.04.2025 08:15] Response: ParsedChatCompletionMessage[Article](content='{"desc":"å½“å‰çš„å•ç›®3Dæ£€æµ‹å™¨å—åˆ°çœŸå®ä¸–ç•Œæ•°æ®é›†å¤šæ ·æ€§å’Œè§„æ¨¡çš„é™åˆ¶ã€‚è™½ç„¶æ•°æ®å¢å¼ºæœ‰åŠ©äºæ”¹å–„æ¨¡å‹æ€§èƒ½ï¼Œä½†åœ¨æˆ·å¤–åœºæ™¯ä¸­ç”ŸæˆçœŸå®æ„Ÿçš„å¢å¼ºæ•°æ®å°¤å…¶å›°éš¾ã€‚å¤§å¤šæ•°åˆæˆæ•°æ®ç”Ÿæˆæ–¹æ³•ä¸“æ³¨äºé€šè¿‡æ”¹è¿›æ¸²æŸ“æŠ€æœ¯æ¥æé«˜ç‰©ä½“å¤–è§‚çš„çœŸå®æ„Ÿï¼Œè€Œæˆ‘ä»¬å‘ç°ç‰©ä½“çš„æ”¾ç½®ä½ç½®å’Œæ–¹å¼å¯¹è®­ç»ƒæœ‰æ•ˆçš„3Då•ç›®æ£€æµ‹å™¨åŒæ ·é‡è¦ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†MonoPlace3Dç³»ç»Ÿï¼Œå®ƒè€ƒè™‘3Dåœºæ™¯å†…å®¹æ¥åˆ›å»ºçœŸå®çš„å¢å¼ºæ•°æ®ï¼Œä»è€Œæ˜¾è‘—æé«˜äº†ç°æœ‰å•ç›®3Dæ£€æµ‹å™¨çš„å‡†ç¡®æ€§ã€‚","title":"MonoPlace3Dï¼šæå‡å•ç›®3Dæ£€æµ‹çš„çœŸå®æ„Ÿå¢å¼º"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='å½“å‰çš„å•ç›®3Dæ£€æµ‹å™¨å—åˆ°çœŸå®ä¸–ç•Œæ•°æ®é›†å¤šæ ·æ€§å’Œè§„æ¨¡çš„é™åˆ¶ã€‚è™½ç„¶æ•°æ®å¢å¼ºæœ‰åŠ©äºæ”¹å–„æ¨¡å‹æ€§èƒ½ï¼Œä½†åœ¨æˆ·å¤–åœºæ™¯ä¸­ç”ŸæˆçœŸå®æ„Ÿçš„å¢å¼ºæ•°æ®å°¤å…¶å›°éš¾ã€‚å¤§å¤šæ•°åˆæˆæ•°æ®ç”Ÿæˆæ–¹æ³•ä¸“æ³¨äºé€šè¿‡æ”¹è¿›æ¸²æŸ“æŠ€æœ¯æ¥æé«˜ç‰©ä½“å¤–è§‚çš„çœŸå®æ„Ÿï¼Œè€Œæˆ‘ä»¬å‘ç°ç‰©ä½“çš„æ”¾ç½®ä½ç½®å’Œæ–¹å¼å¯¹è®­ç»ƒæœ‰æ•ˆçš„3Då•ç›®æ£€æµ‹å™¨åŒæ ·é‡è¦ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†MonoPlace3Dç³»ç»Ÿï¼Œå®ƒè€ƒè™‘3Dåœºæ™¯å†…å®¹æ¥åˆ›å»ºçœŸå®çš„å¢å¼ºæ•°æ®ï¼Œä»è€Œæ˜¾è‘—æé«˜äº†ç°æœ‰å•ç›®3Dæ£€æµ‹å™¨çš„å‡†ç¡®æ€§ã€‚', title='MonoPlace3Dï¼šæå‡å•ç›®3Dæ£€æµ‹çš„çœŸå®æ„Ÿå¢å¼º'))
[11.04.2025 08:15] Querying the API.
[11.04.2025 08:15] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Existing approaches for controlling text-to-image diffusion models, while powerful, do not allow for explicit 3D object-centric control, such as precise control of object orientation. In this work, we address the problem of multi-object orientation control in text-to-image diffusion models. This enables the generation of diverse multi-object scenes with precise orientation control for each object. The key idea is to condition the diffusion model with a set of orientation-aware compass tokens, one for each object, along with text tokens. A light-weight encoder network predicts these compass tokens taking object orientation as the input. The model is trained on a synthetic dataset of procedurally generated scenes, each containing one or two 3D assets on a plain background. However, direct training this framework results in poor orientation control as well as leads to entanglement among objects. To mitigate this, we intervene in the generation process and constrain the cross-attention maps of each compass token to its corresponding object regions. The trained model is able to achieve precise orientation control for a) complex objects not seen during training and b) multi-object scenes with more than two objects, indicating strong generalization capabilities. Further, when combined with personalization methods, our method precisely controls the orientation of the new object in diverse contexts. Our method achieves state-of-the-art orientation control and text alignment, quantified with extensive evaluations and a user study.
[11.04.2025 08:15] Response: {
  "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹-ĞºĞ¾Ğ¼Ğ¿Ğ°ÑÑ‹ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒÑÑ‚ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ± Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ğ½Ğ¾ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ÑÑ†ĞµĞ½Ğ°Ğ¼Ğ¸. Ğ”Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ ÑĞ¼ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ€Ñ‚ Ğ¿ĞµÑ€ĞµĞºÑ€ĞµÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ.",
  "emoji": "ğŸ§­",
  "title": "Ğ¢Ğ¾Ñ‡Ğ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹"
}
[11.04.2025 08:15] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Existing approaches for controlling text-to-image diffusion models, while powerful, do not allow for explicit 3D object-centric control, such as precise control of object orientation. In this work, we address the problem of multi-object orientation control in text-to-image diffusion models. This enables the generation of diverse multi-object scenes with precise orientation control for each object. The key idea is to condition the diffusion model with a set of orientation-aware compass tokens, one for each object, along with text tokens. A light-weight encoder network predicts these compass tokens taking object orientation as the input. The model is trained on a synthetic dataset of procedurally generated scenes, each containing one or two 3D assets on a plain background. However, direct training this framework results in poor orientation control as well as leads to entanglement among objects. To mitigate this, we intervene in the generation process and constrain the cross-attention maps of each compass token to its corresponding object regions. The trained model is able to achieve precise orientation control for a) complex objects not seen during training and b) multi-object scenes with more than two objects, indicating strong generalization capabilities. Further, when combined with personalization methods, our method precisely controls the orientation of the new object in diverse contexts. Our method achieves state-of-the-art orientation control and text alignment, quantified with extensive evaluations and a user study."

[11.04.2025 08:15] Response: ```python
["3D", "CV", "TRAINING"]
```
[11.04.2025 08:15] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Existing approaches for controlling text-to-image diffusion models, while powerful, do not allow for explicit 3D object-centric control, such as precise control of object orientation. In this work, we address the problem of multi-object orientation control in text-to-image diffusion models. This enables the generation of diverse multi-object scenes with precise orientation control for each object. The key idea is to condition the diffusion model with a set of orientation-aware compass tokens, one for each object, along with text tokens. A light-weight encoder network predicts these compass tokens taking object orientation as the input. The model is trained on a synthetic dataset of procedurally generated scenes, each containing one or two 3D assets on a plain background. However, direct training this framework results in poor orientation control as well as leads to entanglement among objects. To mitigate this, we intervene in the generation process and constrain the cross-attention maps of each compass token to its corresponding object regions. The trained model is able to achieve precise orientation control for a) complex objects not seen during training and b) multi-object scenes with more than two objects, indicating strong generalization capabilities. Further, when combined with personalization methods, our method precisely controls the orientation of the new object in diverse contexts. Our method achieves state-of-the-art orientation control and text alignment, quantified with extensive evaluations and a user study."

[11.04.2025 08:15] Response: ```python
["DIFFUSION", "SYNTHETIC"]
```
[11.04.2025 08:15] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents a novel approach to enhance text-to-image diffusion models by enabling precise control over the orientation of multiple objects in generated scenes. The authors introduce orientation-aware compass tokens that are conditioned on the object orientation, allowing for better manipulation of object placement and alignment. A lightweight encoder network predicts these tokens based on input orientations, addressing challenges like poor control and object entanglement during training. The proposed method demonstrates strong generalization capabilities, achieving state-of-the-art results in orientation control and text alignment through extensive evaluations and user studies.","title":"Mastering Object Orientation in Text-to-Image Generation"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper presents a novel approach to enhance text-to-image diffusion models by enabling precise control over the orientation of multiple objects in generated scenes. The authors introduce orientation-aware compass tokens that are conditioned on the object orientation, allowing for better manipulation of object placement and alignment. A lightweight encoder network predicts these tokens based on input orientations, addressing challenges like poor control and object entanglement during training. The proposed method demonstrates strong generalization capabilities, achieving state-of-the-art results in orientation control and text alignment through extensive evaluations and user studies.', title='Mastering Object Orientation in Text-to-Image Generation'))
[11.04.2025 08:15] Response: ParsedChatCompletionMessage[Article](content='{"desc":"æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•æ¥æ§åˆ¶æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯å¤šå¯¹è±¡çš„æ–¹å‘æ§åˆ¶ã€‚é€šè¿‡å¼•å…¥æ–¹å‘æ„ŸçŸ¥çš„æŒ‡å—é’ˆæ ‡è®°ï¼Œæ¨¡å‹èƒ½å¤Ÿä¸ºæ¯ä¸ªå¯¹è±¡æä¾›ç²¾ç¡®çš„æ–¹å‘æ§åˆ¶ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨ç”Ÿæˆå¤æ‚å¯¹è±¡å’Œå¤šå¯¹è±¡åœºæ™¯æ—¶è¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚ç»“åˆä¸ªæ€§åŒ–æ–¹æ³•åï¼Œæ¨¡å‹èƒ½å¤Ÿåœ¨å¤šç§ä¸Šä¸‹æ–‡ä¸­ç²¾ç¡®æ§åˆ¶æ–°å¯¹è±¡çš„æ–¹å‘ã€‚","title":"ç²¾ç¡®æ§åˆ¶å¤šå¯¹è±¡æ–¹å‘çš„åˆ›æ–°æ–¹æ³•"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•æ¥æ§åˆ¶æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯å¤šå¯¹è±¡çš„æ–¹å‘æ§åˆ¶ã€‚é€šè¿‡å¼•å…¥æ–¹å‘æ„ŸçŸ¥çš„æŒ‡å—é’ˆæ ‡è®°ï¼Œæ¨¡å‹èƒ½å¤Ÿä¸ºæ¯ä¸ªå¯¹è±¡æä¾›ç²¾ç¡®çš„æ–¹å‘æ§åˆ¶ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨ç”Ÿæˆå¤æ‚å¯¹è±¡å’Œå¤šå¯¹è±¡åœºæ™¯æ—¶è¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚ç»“åˆä¸ªæ€§åŒ–æ–¹æ³•åï¼Œæ¨¡å‹èƒ½å¤Ÿåœ¨å¤šç§ä¸Šä¸‹æ–‡ä¸­ç²¾ç¡®æ§åˆ¶æ–°å¯¹è±¡çš„æ–¹å‘ã€‚', title='ç²¾ç¡®æ§åˆ¶å¤šå¯¹è±¡æ–¹å‘çš„åˆ›æ–°æ–¹æ³•'))
[11.04.2025 08:15] Loading Chinese text from previous data.
[11.04.2025 08:15] Renaming data file.
[11.04.2025 08:15] Renaming previous data. hf_papers.json to ./d/2025-04-11.json
[11.04.2025 08:15] Saving new data file.
[11.04.2025 08:15] Generating page.
[11.04.2025 08:15] Renaming previous page.
[11.04.2025 08:15] Renaming previous data. index.html to ./d/2025-04-11.html
[11.04.2025 08:15] [Experimental] Generating Chinese page for reading.
[11.04.2025 08:15] Chinese vocab [{'word': 'æ‰©æ•£', 'pinyin': 'kuÃ² sÃ n', 'trans': 'diffusion'}, {'word': 'å˜å‹å™¨', 'pinyin': 'biÃ n yÄ qÃ¬', 'trans': 'transformer'}, {'word': 'è´¨é‡', 'pinyin': 'zhÃ¬ liÃ ng', 'trans': 'quality'}, {'word': 'å‡ºè‰²', 'pinyin': 'chÅ« sÃ¨', 'trans': 'outstanding'}, {'word': 'è¿­ä»£', 'pinyin': 'diÃ© dÃ i', 'trans': 'iteration'}, {'word': 'æ¨ç†', 'pinyin': 'tuÄ« lÇ', 'trans': 'inference'}, {'word': 'å»å™ª', 'pinyin': 'qÃ¹ zÃ o', 'trans': 'denoising'}, {'word': 'ç¼–ç ', 'pinyin': 'biÄn mÇ', 'trans': 'encode'}, {'word': 'è¯­ä¹‰', 'pinyin': 'yÇ” yÃ¬', 'trans': 'semantic'}, {'word': 'æˆåˆ†', 'pinyin': 'chÃ©ng fÃ¨n', 'trans': 'component'}, {'word': 'è§£ç ', 'pinyin': 'jiÄ› mÇ', 'trans': 'decode'}, {'word': 'ä¼˜åŒ–', 'pinyin': 'yÅu huÃ ', 'trans': 'optimization'}, {'word': 'å›°å¢ƒ', 'pinyin': 'kÃ¹n jÃ¬ng', 'trans': 'dilemma'}, {'word': 'å‡å°‘', 'pinyin': 'jiÇn shÇo', 'trans': 'reduce'}, {'word': 'çŸ›ç›¾', 'pinyin': 'mÃ¡o dÃ¹n', 'trans': 'contradiction'}, {'word': 'è§£è€¦', 'pinyin': 'jiÄ› Ç’u', 'trans': 'decouple'}, {'word': 'æ¡ä»¶', 'pinyin': 'tiÄo jiÃ n', 'trans': 'condition'}, {'word': 'é€Ÿåº¦', 'pinyin': 'sÃ¹ dÃ¹', 'trans': 'speed'}, {'word': 'è§„æ¨¡', 'pinyin': 'guÄ« mÃ³', 'trans': 'scale'}, {'word': 'æ”¶æ•›', 'pinyin': 'shÅu liÇn', 'trans': 'convergence'}, {'word': 'æ¶æ„', 'pinyin': 'jiÃ  gÃ²u', 'trans': 'architecture'}, {'word': 'å…±äº«', 'pinyin': 'gÃ²ng xiÇng', 'trans': 'share'}, {'word': 'è‡ªæˆ‘', 'pinyin': 'zÃ¬ wÇ’', 'trans': 'self'}, {'word': 'åŠ¨æ€', 'pinyin': 'dÃ²ng tÃ i', 'trans': 'dynamic'}, {'word': 'è§„åˆ’', 'pinyin': 'guÄ« huÃ ', 'trans': 'planning'}, {'word': 'æœ€å°åŒ–', 'pinyin': 'zuÃ¬ xiÇo huÃ ', 'trans': 'minimize'}, {'word': 'ä¸‹é™', 'pinyin': 'xiÃ  jiÃ ng', 'trans': 'decrease'}]
[11.04.2025 08:15] Renaming previous Chinese page.
[11.04.2025 08:15] Renaming previous data. zh.html to ./d/2025-04-10_zh_reading_task.html
[11.04.2025 08:15] Writing Chinese reading task.
[11.04.2025 08:15] Writing result.
[11.04.2025 08:15] Renaming log file.
[11.04.2025 08:15] Renaming previous data. log.txt to ./logs/2025-04-11_last_log.txt
