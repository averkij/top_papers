[11.04.2025 04:14] Read previous papers.
[11.04.2025 04:14] Generating top page (month).
[11.04.2025 04:14] Writing top page (month).
[11.04.2025 05:12] Read previous papers.
[11.04.2025 05:12] Get feed.
[11.04.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.07956
[11.04.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.07957
[11.04.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.07128
[11.04.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.07960
[11.04.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.07943
[11.04.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.07964
[11.04.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.07830
[11.04.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.07934
[11.04.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.04974
[11.04.2025 05:12] Extract page data from URL. URL: https://huggingface.co/papers/2504.07491
[11.04.2025 05:12] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[11.04.2025 05:12] No deleted papers detected.
[11.04.2025 05:12] Downloading and parsing papers (pdf, html). Total: 10.
[11.04.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2504.07956.
[11.04.2025 05:12] Extra JSON file exists (./assets/json/2504.07956.json), skip PDF parsing.
[11.04.2025 05:12] Paper image links file exists (./assets/img_data/2504.07956.json), skip HTML parsing.
[11.04.2025 05:12] Success.
[11.04.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2504.07957.
[11.04.2025 05:12] Extra JSON file exists (./assets/json/2504.07957.json), skip PDF parsing.
[11.04.2025 05:12] Paper image links file exists (./assets/img_data/2504.07957.json), skip HTML parsing.
[11.04.2025 05:12] Success.
[11.04.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2504.07128.
[11.04.2025 05:12] Extra JSON file exists (./assets/json/2504.07128.json), skip PDF parsing.
[11.04.2025 05:12] Paper image links file exists (./assets/img_data/2504.07128.json), skip HTML parsing.
[11.04.2025 05:12] Success.
[11.04.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2504.07960.
[11.04.2025 05:12] Extra JSON file exists (./assets/json/2504.07960.json), skip PDF parsing.
[11.04.2025 05:12] Paper image links file exists (./assets/img_data/2504.07960.json), skip HTML parsing.
[11.04.2025 05:12] Success.
[11.04.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2504.07943.
[11.04.2025 05:12] Extra JSON file exists (./assets/json/2504.07943.json), skip PDF parsing.
[11.04.2025 05:12] Paper image links file exists (./assets/img_data/2504.07943.json), skip HTML parsing.
[11.04.2025 05:12] Success.
[11.04.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2504.07964.
[11.04.2025 05:12] Extra JSON file exists (./assets/json/2504.07964.json), skip PDF parsing.
[11.04.2025 05:12] Paper image links file exists (./assets/img_data/2504.07964.json), skip HTML parsing.
[11.04.2025 05:12] Success.
[11.04.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2504.07830.
[11.04.2025 05:12] Extra JSON file exists (./assets/json/2504.07830.json), skip PDF parsing.
[11.04.2025 05:12] Paper image links file exists (./assets/img_data/2504.07830.json), skip HTML parsing.
[11.04.2025 05:12] Success.
[11.04.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2504.07934.
[11.04.2025 05:12] Extra JSON file exists (./assets/json/2504.07934.json), skip PDF parsing.
[11.04.2025 05:12] Paper image links file exists (./assets/img_data/2504.07934.json), skip HTML parsing.
[11.04.2025 05:12] Success.
[11.04.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2504.04974.
[11.04.2025 05:12] Extra JSON file exists (./assets/json/2504.04974.json), skip PDF parsing.
[11.04.2025 05:12] Paper image links file exists (./assets/img_data/2504.04974.json), skip HTML parsing.
[11.04.2025 05:12] Success.
[11.04.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2504.07491.
[11.04.2025 05:12] Downloading paper 2504.07491 from http://arxiv.org/pdf/2504.07491v1...
[11.04.2025 05:12] Extracting affiliations from text.
[11.04.2025 05:12] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"KIMI-VL TECHNICAL REPORT "
[11.04.2025 05:12] Response: []
[11.04.2025 05:12] Extracting affiliations from text.
[11.04.2025 05:12] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"KIMI-VL TECHNICAL REPORTWe present Kimi-VL, an efficient open-source Mixture-of-Experts (MoE) vision-language model (VLM) that offers advanced multimodal reasoning, long-context understanding, and strong agent capabilitiesall while activating only 2.8B parameters in its language decoder (Kimi-VL-A3B). Kimi-VL demonstrates strong performance across challenging domains: as general-purpose VLM, Kimi-VL excels in multi-turn agent tasks (e.g., OSWorld), matching flagship models. Furthermore, it exhibits remarkable capabilities across diverse challenging vision language tasks, including collegelevel image and video comprehension, OCR, mathematical reasoning, multi-image understanding. In comparative evaluations, it effectively competes with cutting-edge efficient VLMs such as GPT-4omini, Qwen2.5-VL-7B, and Gemma-3-12B-IT, while surpassing GPT-4o in several key domains. Kimi-VL also advances in processing long contexts and perceiving clearly. With 128K extended context window, Kimi-VL can process diverse long inputs, achieving impressive scores of 64.5 on LongVideoBench and 35.1 on MMLongBench-Doc. Its native-resolution vision encoder, MoonViT, further allows it to see and understand ultra-high-resolution visual inputs, achieving 83.2 on InfoVQA and 34.5 on ScreenSpot-Pro, while maintaining lower computational cost for common tasks. Building upon Kimi-VL, we introduce an advanced long-thinking variant: Kimi-VL-Thinking. Developed through long chain-of-thought (CoT) supervised fine-tuning (SFT) and reinforcement learning (RL), this model exhibits strong long-horizon reasoning capabilities. It achieves scores of 61.7 on MMMU, 36.8 on MathVision, and 71.3 on MathVista while maintaining the compact 2.8B activated LLM parameters, setting new standard for efficient yet multimodal thinking models. Code and models are publicly accessible at https://github.com/MoonshotAI/Kimi-VL. 5 2 0 2 0 ] . [ 1 1 9 4 7 0 . 4 0 5 2 : r Figure 1: Comparison between Kimi-VL-Thinking and frontier open-source VLMs, including short-thinking VLMs (e.g. Gemma-3 series, Qwen2.5-VL series) and long-thinking VLMs (QVQ-72B-Preview), on MathVision benchmark. Our model achieves strong multimodal reasoning with just 2.8B LLM activated parameters. KIMI-VL TECHNICAL REPORT Figure 2: Highlights of Kimi-VL performance for wide range of benchmarks like, general benchmarks (MMMU, MMBench), OCR (InfoVQA), multi-image (BLINK), long video (LongVideoBench, Video-MME), long document (MMLongBench-Doc), and agent (ScreenSpot-Pro and OSWorld). Detailed results are presented in Table 3.With the rapid advancement of artificial intelligence, human expectations for AI assistants have transcended traditional language-only interactions, increasingly aligning with the inherently multimodal nature of our world. To better understand and interact with these expectations, new generations of natively multimodal models, such as GPT4o (OpenAI et al. 2024) and Google Gemini (Gemini Team et al. 2024), have emerged with the capability to seamlessly perceive and interpret visual inputs alongside language processing. Most recently, advanced multimodal models, pioneered by OpenAI o1 series (OpenAI 2024) and Kimi k1.5 (K. Team et al. 2025), have further pushed these boundaries by incorporating deeper and longer reasoning on multimodal inputs, thereby tackling more complex problems in the multimodal domain. Nevertheless, development in large VLMs in the open-source community has significantly lagged behind their languageonly counterparts, particularly in aspects of scalability, computational efficiency, and advanced reasoning capabilities. While language-only model DeepSeek R1 (DeepSeek-AI, D. Guo, et al. 2025) has already leveraged the efficient and more scalable mixture-of-experts (MoE) architecture and facilitated sophisticated long chain-of-thought (CoT) reasoning, most recent open-source VLMs, e.g. Qwen2.5-VL (Bai et al. 2025) and Gemma-3 (Gemma Team et al. 2025), continue to rely on dense architectures and do not support long-CoT reasoning. Early explorations into MoE-based vision-language models, such as DeepSeek-VL2 (Zhiyu Wu et al. 2024) and Aria (D. Li et al. 2024), exhibit limitations in other crucial dimensions. Architecturally, both models still adopt relatively traditional fixed-size vision encoders, hindering their adaptability to diverse visual inputs. From capability perspective, DeepSeek-VL2 supports only limited context length (4K), while Aria falls short in fine-grained visual tasks. Additionally, neither of them supports long-thinking abilities. Consequently, there remains pressing need for an open-source VLM that effectively integrates structural innovation, stable capabilities, and enhanced reasoning through long-thinking. In light of this, we present Kimi-VL, vision-language model for the open-source community. Structurally, Kimi-VL consists of our Moonlight (J. Liu et al. 2025a) MoE language model with only 2.8B activated (16B total) parameters, paired with 400M native-resolution MoonViT vision encoder. In terms of capability, as illustrated in Figure 2, Kimi-VL can robustly handle diverse tasks (fine-grained perception, math, college-level problems, OCR, agent, etc.) across broad spectrum of input forms (single-image, multi-image, video, long-document, etc.). Specifically, it features the following exciting abilities: 2 KIMI-VL TECHNICAL REPORT Figure 3: The model architecture of Kimi-VL and Kimi-VL-Thinking, consisting of MoonViT that allows nativeresolution images, an MLP projector, and Mixture-of-Experts (MoE) language decoder. 1) Kimi-VL is smart: it has comparable text ability against efficient pure-text LLMs; without long thinking, Kimi-VL is already competitive in multimodal reasoning and multi-turn agent benchmarks, e.g., MMMU, MathVista, OSWorld. 2) Kimi-VL processes long: it effectively tackles long-context understanding on various multimodal inputs within its 128K context window, far ahead of similar-scale competitors on long video benchmarks and MMLongBench-Doc. 3) Kimi-VL perceives clear: it shows all-round competitive ability over existing efficient dense and MoE VLMs in various vision-language scenarios: visual perception, visual world knowledge, OCR, high-resolution OS screenshot, etc. Furthermore, with long-CoT activation and reinforcement learning (RL), we introduce the long-thinking version of Kimi-VL, Kimi-VL-Thinking, which further substantially improves performance on more complex multimodal reasoning scenario"
[11.04.2025 05:12] Mistral response. {"id": "f5b0a086f526484891f43ba05ee2ae28", "object": "chat.completion", "created": 1744348332, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "[]"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1824, "total_tokens": 1826, "completion_tokens": 2}}
[11.04.2025 05:12] Response: []
[11.04.2025 05:12] Deleting PDF ./assets/pdf/2504.07491.pdf.
[11.04.2025 05:12] Success.
[11.04.2025 05:12] Enriching papers with extra data.
[11.04.2025 05:12] ********************************************************************************
[11.04.2025 05:12] Abstract 0. The advancement of Chain-of-Thought (CoT) reasoning has significantly enhanced the capabilities of large language models (LLMs) and large vision-language models (LVLMs). However, a rigorous evaluation framework for video CoT reasoning remains absent. Current video benchmarks fail to adequately asses...
[11.04.2025 05:12] ********************************************************************************
[11.04.2025 05:12] Abstract 1. The Instruction Following (IF) ability measures how well Multi-modal Large Language Models (MLLMs) understand exactly what users are telling them and whether they are doing it right. Existing multimodal instruction following training data is scarce, the benchmarks are simple with atomic instructions...
[11.04.2025 05:12] ********************************************************************************
[11.04.2025 05:12] Abstract 2. Large Reasoning Models like DeepSeek-R1 mark a fundamental shift in how LLMs approach complex problems. Instead of directly producing an answer for a given input, DeepSeek-R1 creates detailed multi-step reasoning chains, seemingly "thinking" about a problem before providing an answer. This reasoning...
[11.04.2025 05:12] ********************************************************************************
[11.04.2025 05:12] Abstract 3. Recent progress in diffusion models significantly advances various image generation tasks. However, the current mainstream approach remains focused on building task-specific models, which have limited efficiency when supporting a wide range of different needs. While universal models attempt to addre...
[11.04.2025 05:12] ********************************************************************************
[11.04.2025 05:12] Abstract 4. 3D part amodal segmentation--decomposing a 3D shape into complete, semantically meaningful parts, even when occluded--is a challenging but crucial task for 3D content creation and understanding. Existing 3D part segmentation methods only identify visible surface patches, limiting their utility. Insp...
[11.04.2025 05:12] ********************************************************************************
[11.04.2025 05:12] Abstract 5. Mixture-of-Experts (MoE) Large Language Models (LLMs) suffer from severely sub-optimal expert pathways-our study reveals that naive expert selection learned from pretraining leaves a surprising 10-20% accuracy gap for improvement. Motivated by this observation, we develop a novel class of test-time ...
[11.04.2025 05:12] ********************************************************************************
[11.04.2025 05:12] Abstract 6. We present a novel, open-source social network simulation framework, MOSAIC, where generative language agents predict user behaviors such as liking, sharing, and flagging content. This simulation combines LLM agents with a directed social graph to analyze emergent deception behaviors and gain a bett...
[11.04.2025 05:12] ********************************************************************************
[11.04.2025 05:12] Abstract 7. In this paper, we present an effective method to enhance visual reasoning with significantly fewer training samples, relying purely on self-improvement with no knowledge distillation. Our key insight is that the difficulty of training data during reinforcement fine-tuning (RFT) is critical. Appropri...
[11.04.2025 05:12] ********************************************************************************
[11.04.2025 05:12] Abstract 8. Despite the existing evolution of Multimodal Large Language Models (MLLMs), a non-neglectable limitation remains in their struggle with visual text grounding, especially in text-rich images of documents. Document images, such as scanned forms and infographics, highlight critical challenges due to th...
[11.04.2025 05:12] ********************************************************************************
[11.04.2025 05:12] Abstract 9. We present Kimi-VL, an efficient open-source Mixture-of-Experts (MoE) vision-language model (VLM) that offers advanced multimodal reasoning, long-context understanding, and strong agent capabilities - all while activating only 2.8B parameters in its language decoder (Kimi-VL-A3B). Kimi-VL demonstrat...
[11.04.2025 05:12] Read previous papers.
[11.04.2025 05:12] Generating reviews via LLM API.
[11.04.2025 05:12] Using data from previous issue: {"categories": ["#reasoning", "#benchmark", "#video"], "emoji": "🎥", "ru": {"title": "VCR-Bench: новый стандарт оценки рассуждений ИИ по видео", "desc": "В статье представлен новый бенчмарк VCR-Bench для оценки способностей больших визуально-языковых моделей к рассуждениям на основе цепочки мыслей п
[11.04.2025 05:12] Using data from previous issue: {"categories": ["#benchmark", "#training", "#dataset", "#data", "#optimization", "#open_source", "#multimodal", "#alignment"], "emoji": "🤖", "ru": {"title": "Улучшение способности мультимодальных ИИ-моделей следовать инструкциям", "desc": "Статья представляет MM-IFEngine - пайплайн для создания высо
[11.04.2025 05:12] Using data from previous issue: {"categories": ["#architecture", "#ethics", "#inference", "#reasoning", "#long_context", "#rl"], "emoji": "🧠", "ru": {"title": "DeepSeek-R1: мышление машин через призму рассуждений", "desc": "Статья описывает новую модель машинного обучения DeepSeek-R1, которая использует многоступенчатые цепочки ра
[11.04.2025 05:12] Using data from previous issue: {"categories": ["#cv", "#graphs", "#transfer_learning", "#diffusion", "#multimodal", "#dataset"], "emoji": "🎨", "ru": {"title": "VisualCloze: универсальная генерация изображений с визуальным контекстным обучением", "desc": "VisualCloze - это универсальная система генерации изображений, использующая 
[11.04.2025 05:12] Using data from previous issue: {"categories": ["#3d", "#architecture", "#diffusion", "#benchmark", "#optimization"], "emoji": "🧩", "ru": {"title": "Революция в 3D-сегментации: видеть невидимое", "desc": "Статья представляет новый подход к амодальной сегментации трехмерных частей объектов. Авторы предлагают двухэтапный метод: снач
[11.04.2025 05:12] Using data from previous issue: {"categories": ["#optimization", "#benchmark", "#training", "#architecture"], "emoji": "🧠", "ru": {"title": "Оптимизация путей экспертов для повышения эффективности языковых моделей", "desc": "Исследование показывает, что модели языка на основе смеси экспертов (MoE LLM) имеют значительный потенциал 
[11.04.2025 05:12] Using data from previous issue: {"categories": ["#agents", "#graphs", "#reasoning", "#games", "#multimodal", "#open_source"], "emoji": "🕸️", "ru": {"title": "Цифровое общество под микроскопом ИИ", "desc": "MOSAIC - это новая система моделирования социальных сетей с открытым исходным кодом, использующая языковые модели для имитации
[11.04.2025 05:12] Using data from previous issue: {"categories": ["#cv", "#data", "#reasoning", "#training", "#open_source", "#benchmark"], "emoji": "🧠", "ru": {"title": "Меньше данных, больше мышления: революция в визуальном рассуждении ИИ", "desc": "В этой статье представлен эффективный метод улучшения визуального рассуждения с использованием зна
[11.04.2025 05:12] Using data from previous issue: {"categories": ["#benchmark", "#training", "#dataset", "#synthetic", "#transfer_learning", "#multimodal", "#reasoning"], "emoji": "📄", "ru": {"title": "Улучшение понимания текста на изображениях документов для мультимодальных ИИ", "desc": "Статья представляет новую задачу TRIG для оценки и улучшения
[11.04.2025 05:12] Querying the API.
[11.04.2025 05:12] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

We present Kimi-VL, an efficient open-source Mixture-of-Experts (MoE) vision-language model (VLM) that offers advanced multimodal reasoning, long-context understanding, and strong agent capabilities - all while activating only 2.8B parameters in its language decoder (Kimi-VL-A3B). Kimi-VL demonstrates strong performance across challenging domains: as a general-purpose VLM, Kimi-VL excels in multi-turn agent tasks (e.g., OSWorld), matching flagship models. Furthermore, it exhibits remarkable capabilities across diverse challenging vision language tasks, including college-level image and video comprehension, OCR, mathematical reasoning, and multi-image understanding. In comparative evaluations, it effectively competes with cutting-edge efficient VLMs such as GPT-4o-mini, Qwen2.5-VL-7B, and Gemma-3-12B-IT, while surpassing GPT-4o in several key domains. Kimi-VL also advances in processing long contexts and perceiving clearly. With a 128K extended context window, Kimi-VL can process diverse long inputs, achieving impressive scores of 64.5 on LongVideoBench and 35.1 on MMLongBench-Doc. Its native-resolution vision encoder, MoonViT, further allows it to see and understand ultra-high-resolution visual inputs, achieving 83.2 on InfoVQA and 34.5 on ScreenSpot-Pro, while maintaining lower computational cost for common tasks. Building upon Kimi-VL, we introduce an advanced long-thinking variant: Kimi-VL-Thinking. Developed through long chain-of-thought (CoT) supervised fine-tuning (SFT) and reinforcement learning (RL), this model exhibits strong long-horizon reasoning capabilities. It achieves scores of 61.7 on MMMU, 36.8 on MathVision, and 71.3 on MathVista while maintaining the compact 2.8B activated LLM parameters, setting a new standard for efficient multimodal thinking models. Code and models are publicly accessible at https://github.com/MoonshotAI/Kimi-VL.
[11.04.2025 05:12] Response: {
  "desc": "Kimi-VL - это эффективная модель машинного обучения, основанная на архитектуре Mixture-of-Experts (MoE) для обработки визуальной и текстовой информации. Модель демонстрирует высокую производительность в различных сложных задачах, включая многоэтапное взаимодействие с агентами, понимание изображений и видео, оптическое распознавание символов и математические рассуждения. Kimi-VL обладает расширенным контекстным окном в 128K и способна обрабатывать сверхвысокое разрешение визуальных входных данных. Авторы также представляют улучшенную версию модели, Kimi-VL-Thinking, которая демонстрирует сильные способности к долгосрочным рассуждениям при сохранении компактного размера активированных параметров языковой модели.",

  "emoji": "🧠",

  "title": "Kimi-VL: Эффективная мультимодальная модель с расширенными возможностями рассуждений"
}
[11.04.2025 05:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"We present Kimi-VL, an efficient open-source Mixture-of-Experts (MoE) vision-language model (VLM) that offers advanced multimodal reasoning, long-context understanding, and strong agent capabilities - all while activating only 2.8B parameters in its language decoder (Kimi-VL-A3B). Kimi-VL demonstrates strong performance across challenging domains: as a general-purpose VLM, Kimi-VL excels in multi-turn agent tasks (e.g., OSWorld), matching flagship models. Furthermore, it exhibits remarkable capabilities across diverse challenging vision language tasks, including college-level image and video comprehension, OCR, mathematical reasoning, and multi-image understanding. In comparative evaluations, it effectively competes with cutting-edge efficient VLMs such as GPT-4o-mini, Qwen2.5-VL-7B, and Gemma-3-12B-IT, while surpassing GPT-4o in several key domains. Kimi-VL also advances in processing long contexts and perceiving clearly. With a 128K extended context window, Kimi-VL can process diverse long inputs, achieving impressive scores of 64.5 on LongVideoBench and 35.1 on MMLongBench-Doc. Its native-resolution vision encoder, MoonViT, further allows it to see and understand ultra-high-resolution visual inputs, achieving 83.2 on InfoVQA and 34.5 on ScreenSpot-Pro, while maintaining lower computational cost for common tasks. Building upon Kimi-VL, we introduce an advanced long-thinking variant: Kimi-VL-Thinking. Developed through long chain-of-thought (CoT) supervised fine-tuning (SFT) and reinforcement learning (RL), this model exhibits strong long-horizon reasoning capabilities. It achieves scores of 61.7 on MMMU, 36.8 on MathVision, and 71.3 on MathVista while maintaining the compact 2.8B activated LLM parameters, setting a new standard for efficient multimodal thinking models. Code and models are publicly accessible at https://github.com/MoonshotAI/Kimi-VL."

[11.04.2025 05:12] Response: ```python
["MULTIMODAL", "AGENTS", "CV", "RL", "TRAINING", "SMALL_MODELS"]
```
[11.04.2025 05:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"We present Kimi-VL, an efficient open-source Mixture-of-Experts (MoE) vision-language model (VLM) that offers advanced multimodal reasoning, long-context understanding, and strong agent capabilities - all while activating only 2.8B parameters in its language decoder (Kimi-VL-A3B). Kimi-VL demonstrates strong performance across challenging domains: as a general-purpose VLM, Kimi-VL excels in multi-turn agent tasks (e.g., OSWorld), matching flagship models. Furthermore, it exhibits remarkable capabilities across diverse challenging vision language tasks, including college-level image and video comprehension, OCR, mathematical reasoning, and multi-image understanding. In comparative evaluations, it effectively competes with cutting-edge efficient VLMs such as GPT-4o-mini, Qwen2.5-VL-7B, and Gemma-3-12B-IT, while surpassing GPT-4o in several key domains. Kimi-VL also advances in processing long contexts and perceiving clearly. With a 128K extended context window, Kimi-VL can process diverse long inputs, achieving impressive scores of 64.5 on LongVideoBench and 35.1 on MMLongBench-Doc. Its native-resolution vision encoder, MoonViT, further allows it to see and understand ultra-high-resolution visual inputs, achieving 83.2 on InfoVQA and 34.5 on ScreenSpot-Pro, while maintaining lower computational cost for common tasks. Building upon Kimi-VL, we introduce an advanced long-thinking variant: Kimi-VL-Thinking. Developed through long chain-of-thought (CoT) supervised fine-tuning (SFT) and reinforcement learning (RL), this model exhibits strong long-horizon reasoning capabilities. It achieves scores of 61.7 on MMMU, 36.8 on MathVision, and 71.3 on MathVista while maintaining the compact 2.8B activated LLM parameters, setting a new standard for efficient multimodal thinking models. Code and models are publicly accessible at https://github.com/MoonshotAI/Kimi-VL."

[11.04.2025 05:12] Response: ```python
['OPEN_SOURCE', 'LONG_CONTEXT', 'REASONING']
```
[11.04.2025 05:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Kimi-VL is a cutting-edge Mixture-of-Experts vision-language model that efficiently combines multimodal reasoning and long-context understanding while using only 2.8 billion parameters in its language decoder. It excels in various complex tasks, including multi-turn interactions and advanced image and video comprehension, outperforming other leading models in several areas. The model features a 128K extended context window, allowing it to process long inputs effectively, and its native-resolution vision encoder enhances its ability to interpret high-resolution visuals. Additionally, the Kimi-VL-Thinking variant improves long-horizon reasoning through supervised fine-tuning and reinforcement learning, setting a new benchmark for efficient multimodal models.","title":"Kimi-VL: Efficient Multimodal Mastery with Long-Context Reasoning"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Kimi-VL is a cutting-edge Mixture-of-Experts vision-language model that efficiently combines multimodal reasoning and long-context understanding while using only 2.8 billion parameters in its language decoder. It excels in various complex tasks, including multi-turn interactions and advanced image and video comprehension, outperforming other leading models in several areas. The model features a 128K extended context window, allowing it to process long inputs effectively, and its native-resolution vision encoder enhances its ability to interpret high-resolution visuals. Additionally, the Kimi-VL-Thinking variant improves long-horizon reasoning through supervised fine-tuning and reinforcement learning, setting a new benchmark for efficient multimodal models.', title='Kimi-VL: Efficient Multimodal Mastery with Long-Context Reasoning'))
[11.04.2025 05:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Kimi-VL是一种高效的开源混合专家（MoE）视觉语言模型（VLM），具备先进的多模态推理和长文本理解能力。该模型在多轮对话任务和各种视觉语言任务中表现出色，能够与顶尖模型相媲美。Kimi-VL还具备处理长上下文的能力，能够处理多达128K的输入，适用于复杂的视觉理解任务。通过长链思维的监督微调和强化学习，Kimi-VL-Thinking进一步提升了长远推理能力，设定了高效多模态思维模型的新标准。","title":"Kimi-VL：高效的多模态推理新标准"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Kimi-VL是一种高效的开源混合专家（MoE）视觉语言模型（VLM），具备先进的多模态推理和长文本理解能力。该模型在多轮对话任务和各种视觉语言任务中表现出色，能够与顶尖模型相媲美。Kimi-VL还具备处理长上下文的能力，能够处理多达128K的输入，适用于复杂的视觉理解任务。通过长链思维的监督微调和强化学习，Kimi-VL-Thinking进一步提升了长远推理能力，设定了高效多模态思维模型的新标准。', title='Kimi-VL：高效的多模态推理新标准'))
[11.04.2025 05:12] Loading Chinese text from previous data.
[11.04.2025 05:12] Renaming data file.
[11.04.2025 05:12] Renaming previous data. hf_papers.json to ./d/2025-04-11.json
[11.04.2025 05:12] Saving new data file.
[11.04.2025 05:12] Generating page.
[11.04.2025 05:12] Renaming previous page.
[11.04.2025 05:12] Renaming previous data. index.html to ./d/2025-04-11.html
[11.04.2025 05:12] [Experimental] Generating Chinese page for reading.
[11.04.2025 05:12] Chinese vocab [{'word': '扩散', 'pinyin': 'kuò sàn', 'trans': 'diffusion'}, {'word': '变压器', 'pinyin': 'biàn yā qì', 'trans': 'transformer'}, {'word': '质量', 'pinyin': 'zhì liàng', 'trans': 'quality'}, {'word': '出色', 'pinyin': 'chū sè', 'trans': 'outstanding'}, {'word': '迭代', 'pinyin': 'dié dài', 'trans': 'iteration'}, {'word': '推理', 'pinyin': 'tuī lǐ', 'trans': 'inference'}, {'word': '去噪', 'pinyin': 'qù zào', 'trans': 'denoising'}, {'word': '编码', 'pinyin': 'biān mǎ', 'trans': 'encode'}, {'word': '语义', 'pinyin': 'yǔ yì', 'trans': 'semantic'}, {'word': '成分', 'pinyin': 'chéng fèn', 'trans': 'component'}, {'word': '解码', 'pinyin': 'jiě mǎ', 'trans': 'decode'}, {'word': '优化', 'pinyin': 'yōu huà', 'trans': 'optimization'}, {'word': '困境', 'pinyin': 'kùn jìng', 'trans': 'dilemma'}, {'word': '减少', 'pinyin': 'jiǎn shǎo', 'trans': 'reduce'}, {'word': '矛盾', 'pinyin': 'máo dùn', 'trans': 'contradiction'}, {'word': '解耦', 'pinyin': 'jiě ǒu', 'trans': 'decouple'}, {'word': '条件', 'pinyin': 'tiāo jiàn', 'trans': 'condition'}, {'word': '速度', 'pinyin': 'sù dù', 'trans': 'speed'}, {'word': '规模', 'pinyin': 'guī mó', 'trans': 'scale'}, {'word': '收敛', 'pinyin': 'shōu liǎn', 'trans': 'convergence'}, {'word': '架构', 'pinyin': 'jià gòu', 'trans': 'architecture'}, {'word': '共享', 'pinyin': 'gòng xiǎng', 'trans': 'share'}, {'word': '自我', 'pinyin': 'zì wǒ', 'trans': 'self'}, {'word': '动态', 'pinyin': 'dòng tài', 'trans': 'dynamic'}, {'word': '规划', 'pinyin': 'guī huà', 'trans': 'planning'}, {'word': '最小化', 'pinyin': 'zuì xiǎo huà', 'trans': 'minimize'}, {'word': '下降', 'pinyin': 'xià jiàng', 'trans': 'decrease'}]
[11.04.2025 05:12] Renaming previous Chinese page.
[11.04.2025 05:12] Renaming previous data. zh.html to ./d/2025-04-10_zh_reading_task.html
[11.04.2025 05:12] Writing Chinese reading task.
[11.04.2025 05:12] Writing result.
[11.04.2025 05:12] Renaming log file.
[11.04.2025 05:12] Renaming previous data. log.txt to ./logs/2025-04-11_last_log.txt
