[17.05.2025 06:32] Read previous papers.
[17.05.2025 06:32] Generating top page (month).
[17.05.2025 06:32] Writing top page (month).
[17.05.2025 12:43] Read previous papers.
[17.05.2025 12:43] Get feed.
[17.05.2025 12:43] Get page data from previous paper. URL: https://huggingface.co/papers/2505.10554
[17.05.2025 12:43] Get page data from previous paper. URL: https://huggingface.co/papers/2505.09666
[17.05.2025 12:43] Get page data from previous paper. URL: https://huggingface.co/papers/2505.10475
[17.05.2025 12:43] Get page data from previous paper. URL: https://huggingface.co/papers/2505.08617
[17.05.2025 12:43] Get page data from previous paper. URL: https://huggingface.co/papers/2505.09723
[17.05.2025 12:43] Get page data from previous paper. URL: https://huggingface.co/papers/2505.10185
[17.05.2025 12:43] Get page data from previous paper. URL: https://huggingface.co/papers/2505.10562
[17.05.2025 12:43] Get page data from previous paper. URL: https://huggingface.co/papers/2505.09694
[17.05.2025 12:43] Get page data from previous paper. URL: https://huggingface.co/papers/2505.10527
[17.05.2025 12:43] Get page data from previous paper. URL: https://huggingface.co/papers/2505.07782
[17.05.2025 12:43] Get page data from previous paper. URL: https://huggingface.co/papers/2505.06027
[17.05.2025 12:43] Get page data from previous paper. URL: https://huggingface.co/papers/2505.10320
[17.05.2025 12:43] Get page data from previous paper. URL: https://huggingface.co/papers/2505.10558
[17.05.2025 12:43] Get page data from previous paper. URL: https://huggingface.co/papers/2505.09990
[17.05.2025 12:43] Get page data from previous paper. URL: https://huggingface.co/papers/2505.10565
[17.05.2025 12:43] Get page data from previous paper. URL: https://huggingface.co/papers/2505.09738
[17.05.2025 12:43] Get page data from previous paper. URL: https://huggingface.co/papers/2505.10046
[17.05.2025 12:43] Get page data from previous paper. URL: https://huggingface.co/papers/2505.08581
[17.05.2025 12:43] Get page data from previous paper. URL: https://huggingface.co/papers/2505.10566
[17.05.2025 12:43] Get page data from previous paper. URL: https://huggingface.co/papers/2505.10167
[17.05.2025 12:43] Get page data from previous paper. URL: https://huggingface.co/papers/2505.09926
[17.05.2025 12:43] Get page data from previous paper. URL: https://huggingface.co/papers/2505.10468
[17.05.2025 12:43] Get page data from previous paper. URL: https://huggingface.co/papers/2505.09601
[17.05.2025 12:43] Get page data from previous paper. URL: https://huggingface.co/papers/2505.09265
[17.05.2025 12:43] Get page data from previous paper. URL: https://huggingface.co/papers/2505.09264
[17.05.2025 12:43] Get page data from previous paper. URL: https://huggingface.co/papers/2505.09263
[17.05.2025 12:43] Get page data from previous paper. URL: https://huggingface.co/papers/2505.07096
[17.05.2025 12:43] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[17.05.2025 12:43] No deleted papers detected.
[17.05.2025 12:43] Downloading and parsing papers (pdf, html). Total: 27.
[17.05.2025 12:43] Downloading and parsing paper https://huggingface.co/papers/2505.10554.
[17.05.2025 12:43] Extra JSON file exists (./assets/json/2505.10554.json), skip PDF parsing.
[17.05.2025 12:43] Paper image links file exists (./assets/img_data/2505.10554.json), skip HTML parsing.
[17.05.2025 12:43] Success.
[17.05.2025 12:43] Downloading and parsing paper https://huggingface.co/papers/2505.09666.
[17.05.2025 12:43] Extra JSON file exists (./assets/json/2505.09666.json), skip PDF parsing.
[17.05.2025 12:43] Paper image links file exists (./assets/img_data/2505.09666.json), skip HTML parsing.
[17.05.2025 12:43] Success.
[17.05.2025 12:43] Downloading and parsing paper https://huggingface.co/papers/2505.10475.
[17.05.2025 12:43] Extra JSON file exists (./assets/json/2505.10475.json), skip PDF parsing.
[17.05.2025 12:43] Paper image links file exists (./assets/img_data/2505.10475.json), skip HTML parsing.
[17.05.2025 12:43] Success.
[17.05.2025 12:43] Downloading and parsing paper https://huggingface.co/papers/2505.08617.
[17.05.2025 12:43] Extra JSON file exists (./assets/json/2505.08617.json), skip PDF parsing.
[17.05.2025 12:43] Paper image links file exists (./assets/img_data/2505.08617.json), skip HTML parsing.
[17.05.2025 12:43] Success.
[17.05.2025 12:43] Downloading and parsing paper https://huggingface.co/papers/2505.09723.
[17.05.2025 12:43] Extra JSON file exists (./assets/json/2505.09723.json), skip PDF parsing.
[17.05.2025 12:43] Paper image links file exists (./assets/img_data/2505.09723.json), skip HTML parsing.
[17.05.2025 12:43] Success.
[17.05.2025 12:43] Downloading and parsing paper https://huggingface.co/papers/2505.10185.
[17.05.2025 12:43] Extra JSON file exists (./assets/json/2505.10185.json), skip PDF parsing.
[17.05.2025 12:43] Paper image links file exists (./assets/img_data/2505.10185.json), skip HTML parsing.
[17.05.2025 12:43] Success.
[17.05.2025 12:43] Downloading and parsing paper https://huggingface.co/papers/2505.10562.
[17.05.2025 12:43] Extra JSON file exists (./assets/json/2505.10562.json), skip PDF parsing.
[17.05.2025 12:43] Paper image links file exists (./assets/img_data/2505.10562.json), skip HTML parsing.
[17.05.2025 12:43] Success.
[17.05.2025 12:43] Downloading and parsing paper https://huggingface.co/papers/2505.09694.
[17.05.2025 12:43] Extra JSON file exists (./assets/json/2505.09694.json), skip PDF parsing.
[17.05.2025 12:43] Paper image links file exists (./assets/img_data/2505.09694.json), skip HTML parsing.
[17.05.2025 12:43] Success.
[17.05.2025 12:43] Downloading and parsing paper https://huggingface.co/papers/2505.10527.
[17.05.2025 12:43] Extra JSON file exists (./assets/json/2505.10527.json), skip PDF parsing.
[17.05.2025 12:43] Paper image links file exists (./assets/img_data/2505.10527.json), skip HTML parsing.
[17.05.2025 12:43] Success.
[17.05.2025 12:43] Downloading and parsing paper https://huggingface.co/papers/2505.07782.
[17.05.2025 12:43] Extra JSON file exists (./assets/json/2505.07782.json), skip PDF parsing.
[17.05.2025 12:43] Paper image links file exists (./assets/img_data/2505.07782.json), skip HTML parsing.
[17.05.2025 12:43] Success.
[17.05.2025 12:43] Downloading and parsing paper https://huggingface.co/papers/2505.06027.
[17.05.2025 12:43] Extra JSON file exists (./assets/json/2505.06027.json), skip PDF parsing.
[17.05.2025 12:43] Paper image links file exists (./assets/img_data/2505.06027.json), skip HTML parsing.
[17.05.2025 12:43] Success.
[17.05.2025 12:43] Downloading and parsing paper https://huggingface.co/papers/2505.10320.
[17.05.2025 12:43] Extra JSON file exists (./assets/json/2505.10320.json), skip PDF parsing.
[17.05.2025 12:43] Paper image links file exists (./assets/img_data/2505.10320.json), skip HTML parsing.
[17.05.2025 12:43] Success.
[17.05.2025 12:43] Downloading and parsing paper https://huggingface.co/papers/2505.10558.
[17.05.2025 12:43] Extra JSON file exists (./assets/json/2505.10558.json), skip PDF parsing.
[17.05.2025 12:43] Paper image links file exists (./assets/img_data/2505.10558.json), skip HTML parsing.
[17.05.2025 12:43] Success.
[17.05.2025 12:43] Downloading and parsing paper https://huggingface.co/papers/2505.09990.
[17.05.2025 12:43] Extra JSON file exists (./assets/json/2505.09990.json), skip PDF parsing.
[17.05.2025 12:43] Paper image links file exists (./assets/img_data/2505.09990.json), skip HTML parsing.
[17.05.2025 12:43] Success.
[17.05.2025 12:43] Downloading and parsing paper https://huggingface.co/papers/2505.10565.
[17.05.2025 12:43] Extra JSON file exists (./assets/json/2505.10565.json), skip PDF parsing.
[17.05.2025 12:43] Paper image links file exists (./assets/img_data/2505.10565.json), skip HTML parsing.
[17.05.2025 12:43] Success.
[17.05.2025 12:43] Downloading and parsing paper https://huggingface.co/papers/2505.09738.
[17.05.2025 12:43] Extra JSON file exists (./assets/json/2505.09738.json), skip PDF parsing.
[17.05.2025 12:43] Paper image links file exists (./assets/img_data/2505.09738.json), skip HTML parsing.
[17.05.2025 12:43] Success.
[17.05.2025 12:43] Downloading and parsing paper https://huggingface.co/papers/2505.10046.
[17.05.2025 12:43] Extra JSON file exists (./assets/json/2505.10046.json), skip PDF parsing.
[17.05.2025 12:43] Paper image links file exists (./assets/img_data/2505.10046.json), skip HTML parsing.
[17.05.2025 12:43] Success.
[17.05.2025 12:43] Downloading and parsing paper https://huggingface.co/papers/2505.08581.
[17.05.2025 12:43] Extra JSON file exists (./assets/json/2505.08581.json), skip PDF parsing.
[17.05.2025 12:43] Paper image links file exists (./assets/img_data/2505.08581.json), skip HTML parsing.
[17.05.2025 12:43] Success.
[17.05.2025 12:43] Downloading and parsing paper https://huggingface.co/papers/2505.10566.
[17.05.2025 12:43] Extra JSON file exists (./assets/json/2505.10566.json), skip PDF parsing.
[17.05.2025 12:43] Paper image links file exists (./assets/img_data/2505.10566.json), skip HTML parsing.
[17.05.2025 12:43] Success.
[17.05.2025 12:43] Downloading and parsing paper https://huggingface.co/papers/2505.10167.
[17.05.2025 12:43] Extra JSON file exists (./assets/json/2505.10167.json), skip PDF parsing.
[17.05.2025 12:43] Paper image links file exists (./assets/img_data/2505.10167.json), skip HTML parsing.
[17.05.2025 12:43] Success.
[17.05.2025 12:43] Downloading and parsing paper https://huggingface.co/papers/2505.09926.
[17.05.2025 12:43] Extra JSON file exists (./assets/json/2505.09926.json), skip PDF parsing.
[17.05.2025 12:43] Paper image links file exists (./assets/img_data/2505.09926.json), skip HTML parsing.
[17.05.2025 12:43] Success.
[17.05.2025 12:43] Downloading and parsing paper https://huggingface.co/papers/2505.10468.
[17.05.2025 12:43] Extra JSON file exists (./assets/json/2505.10468.json), skip PDF parsing.
[17.05.2025 12:43] Paper image links file exists (./assets/img_data/2505.10468.json), skip HTML parsing.
[17.05.2025 12:43] Success.
[17.05.2025 12:43] Downloading and parsing paper https://huggingface.co/papers/2505.09601.
[17.05.2025 12:43] Extra JSON file exists (./assets/json/2505.09601.json), skip PDF parsing.
[17.05.2025 12:43] Paper image links file exists (./assets/img_data/2505.09601.json), skip HTML parsing.
[17.05.2025 12:43] Success.
[17.05.2025 12:43] Downloading and parsing paper https://huggingface.co/papers/2505.09265.
[17.05.2025 12:43] Extra JSON file exists (./assets/json/2505.09265.json), skip PDF parsing.
[17.05.2025 12:43] Paper image links file exists (./assets/img_data/2505.09265.json), skip HTML parsing.
[17.05.2025 12:43] Success.
[17.05.2025 12:43] Downloading and parsing paper https://huggingface.co/papers/2505.09264.
[17.05.2025 12:43] Extra JSON file exists (./assets/json/2505.09264.json), skip PDF parsing.
[17.05.2025 12:43] Paper image links file exists (./assets/img_data/2505.09264.json), skip HTML parsing.
[17.05.2025 12:43] Success.
[17.05.2025 12:43] Downloading and parsing paper https://huggingface.co/papers/2505.09263.
[17.05.2025 12:43] Extra JSON file exists (./assets/json/2505.09263.json), skip PDF parsing.
[17.05.2025 12:43] Paper image links file exists (./assets/img_data/2505.09263.json), skip HTML parsing.
[17.05.2025 12:43] Success.
[17.05.2025 12:43] Downloading and parsing paper https://huggingface.co/papers/2505.07096.
[17.05.2025 12:43] Extra JSON file exists (./assets/json/2505.07096.json), skip PDF parsing.
[17.05.2025 12:43] Paper image links file exists (./assets/img_data/2505.07096.json), skip HTML parsing.
[17.05.2025 12:43] Success.
[17.05.2025 12:43] Enriching papers with extra data.
[17.05.2025 12:43] ********************************************************************************
[17.05.2025 12:43] Abstract 0. Large reasoning models (LRMs) already possess a latent capacity for long chain-of-thought reasoning. Prior work has shown that outcome-based reinforcement learning (RL) can incidentally elicit advanced reasoning behaviors such as self-correction, backtracking, and verification phenomena often referr...
[17.05.2025 12:43] ********************************************************************************
[17.05.2025 12:43] Abstract 1. Large Language Models (LLMs) have shown remarkable capabilities, with optimizing their input prompts playing a pivotal role in maximizing their performance. However, while LLM prompts consist of both the task-agnostic system prompts and task-specific user prompts, existing work on prompt optimizatio...
[17.05.2025 12:43] ********************************************************************************
[17.05.2025 12:43] Abstract 2. It is commonly believed that scaling language models should commit a significant space or time cost, by increasing the parameters (parameter scaling) or output tokens (inference-time scaling). We introduce the third and more inference-efficient scaling paradigm: increasing the model's parallel compu...
[17.05.2025 12:43] ********************************************************************************
[17.05.2025 12:43] Abstract 3. While humans can flexibly leverage interactive visual cognition for complex problem-solving, enabling Large Vision-Language Models (LVLMs) to learn similarly adaptive behaviors with visual tools remains challenging. A significant hurdle is the current lack of standardized infrastructure, which hinde...
[17.05.2025 12:43] ********************************************************************************
[17.05.2025 12:43] Abstract 4. Robotic imitation learning has advanced from solving static tasks to addressing dynamic interaction scenarios, but testing and evaluation remain costly and challenging due to the need for real-time interaction with dynamic environments. We propose EnerVerse-AC (EVAC), an action-conditional world mod...
[17.05.2025 12:43] ********************************************************************************
[17.05.2025 12:43] Abstract 5. Long chain-of-thought (CoT) is an essential ingredient in effective usage of modern large language models, but our understanding of the reasoning strategies underlying these capabilities remains limited. While some prior works have attempted to categorize CoTs using predefined strategy types, such a...
[17.05.2025 12:43] ********************************************************************************
[17.05.2025 12:43] Abstract 6. Existing vision tokenization isolates the optimization of vision tokenizers from downstream training, implicitly assuming the visual tokens can generalize well across various tasks, e.g., image generation and visual question answering. The vision tokenizer optimized for low-level reconstruction is a...
[17.05.2025 12:43] ********************************************************************************
[17.05.2025 12:43] Abstract 7. Recent advances in creative AI have enabled the synthesis of high-fidelity images and videos conditioned on language instructions. Building on these developments, text-to-video diffusion models have evolved into embodied world models (EWMs) capable of generating physically plausible scenes from lang...
[17.05.2025 12:43] ********************************************************************************
[17.05.2025 12:43] Abstract 8. Motivated by scaling laws in language modeling that demonstrate how test loss scales as a power law with model and dataset sizes, we find that similar laws exist in preference modeling. We propose World Preference Modeling$ (WorldPM) to emphasize this scaling potential, where World Preference embodi...
[17.05.2025 12:43] ********************************************************************************
[17.05.2025 12:43] Abstract 9. We introduce MLE-Dojo, a Gym-style framework for systematically reinforcement learning, evaluating, and improving autonomous large language model (LLM) agents in iterative machine learning engineering (MLE) workflows. Unlike existing benchmarks that primarily rely on static datasets or single-attemp...
[17.05.2025 12:43] ********************************************************************************
[17.05.2025 12:43] Abstract 10. This paper introduces Unilogit, a novel self-distillation method for machine unlearning in Large Language Models. Unilogit addresses the challenge of selectively forgetting specific information while maintaining overall model utility, a critical task in compliance with data privacy regulations like ...
[17.05.2025 12:43] ********************************************************************************
[17.05.2025 12:43] Abstract 11. The progress of AI is bottlenecked by the quality of evaluation, and powerful LLM-as-a-Judge models have proved to be a core solution. Improved judgment ability is enabled by stronger chain-of-thought reasoning, motivating the need to find the best recipes for training such models to think. In this ...
[17.05.2025 12:43] ********************************************************************************
[17.05.2025 12:43] Abstract 12. Scalable Vector Graphics (SVGs) are highly favored by designers due to their resolution independence and well-organized layer structure. Although existing text-to-vector (T2V) generation methods can create SVGs from text prompts, they often overlook an important need in practical applications: style...
[17.05.2025 12:43] ********************************************************************************
[17.05.2025 12:43] Abstract 13. Pointing serves as a fundamental and intuitive mechanism for grounding language within visual contexts, with applications spanning robotics, assistive technologies, and interactive AI systems. While recent multimodal models have started to support pointing capabilities, existing benchmarks typically...
[17.05.2025 12:43] ********************************************************************************
[17.05.2025 12:43] Abstract 14. This work presents Prior Depth Anything, a framework that combines incomplete but precise metric information in depth measurement with relative but complete geometric structures in depth prediction, generating accurate, dense, and detailed metric depth maps for any scene. To this end, we design a co...
[17.05.2025 12:43] ********************************************************************************
[17.05.2025 12:43] Abstract 15. Pretrained language models (LLMs) are often constrained by their fixed tokenization schemes, leading to inefficiencies and performance limitations, particularly for multilingual or specialized applications. This tokenizer lock-in presents significant challenges. standard methods to overcome this oft...
[17.05.2025 12:43] ********************************************************************************
[17.05.2025 12:43] Abstract 16. This paper does not describe a new method; instead, it provides a thorough exploration of an important yet understudied design space related to recent advances in text-to-image synthesis -- specifically, the deep fusion of large language models (LLMs) and diffusion transformers (DiTs) for multi-moda...
[17.05.2025 12:43] ********************************************************************************
[17.05.2025 12:43] Abstract 17. Surgical scene segmentation is critical in computer-assisted surgery and is vital for enhancing surgical quality and patient outcomes. Recently, referring surgical segmentation is emerging, given its advantage of providing surgeons with an interactive experience to segment the target object. However...
[17.05.2025 12:43] ********************************************************************************
[17.05.2025 12:43] Abstract 18. Despite significant advances in modeling image priors via diffusion models, 3D-aware image editing remains challenging, in part because the object is only specified via a single image. To tackle this challenge, we propose 3D-Fixup, a new framework for editing 2D images guided by learned 3D priors. T...
[17.05.2025 12:43] ********************************************************************************
[17.05.2025 12:43] Abstract 19. The emergence of hybrid quantum-classical machine learning (HQML) models opens new horizons of computational intelligence but their fundamental complexity frequently leads to black box behavior that undermines transparency and reliability in their application. Although XAI for quantum systems still ...
[17.05.2025 12:43] ********************************************************************************
[17.05.2025 12:43] Abstract 20. Universal visual anomaly detection aims to identify anomalies from novel or unseen vision domains without additional fine-tuning, which is critical in open scenarios. Recent studies have demonstrated that pre-trained vision-language models like CLIP exhibit strong generalization with just zero or a ...
[17.05.2025 12:43] ********************************************************************************
[17.05.2025 12:43] Abstract 21. This study critically distinguishes between AI Agents and Agentic AI, offering a structured conceptual taxonomy, application mapping, and challenge analysis to clarify their divergent design philosophies and capabilities. We begin by outlining the search strategy and foundational definitions, charac...
[17.05.2025 12:43] ********************************************************************************
[17.05.2025 12:43] Abstract 22. Scaling robot learning requires vast and diverse datasets. Yet the prevailing data collection paradigm-human teleoperation-remains costly and constrained by manual effort and physical robot access. We introduce Real2Render2Real (R2R2R), a novel approach for generating robot training data without rel...
[17.05.2025 12:43] ********************************************************************************
[17.05.2025 12:43] Abstract 23. Zero- and few-shot visual anomaly segmentation relies on powerful vision-language models that detect unseen anomalies using manually designed textual prompts. However, visual representations are inherently independent of language. In this paper, we explore the potential of a pure visual foundation m...
[17.05.2025 12:43] ********************************************************************************
[17.05.2025 12:43] Abstract 24. Unsupervised reconstruction networks using self-attention transformers have achieved state-of-the-art performance for multi-class (unified) anomaly detection with a single model. However, these self-attention reconstruction models primarily operate on target features, which may result in perfect rec...
[17.05.2025 12:43] ********************************************************************************
[17.05.2025 12:43] Abstract 25. Anomaly detection is a practical and challenging task due to the scarcity of anomaly samples in industrial inspection. Some existing anomaly detection methods address this issue by synthesizing anomalies with noise or external data. However, there is always a large semantic gap between synthetic and...
[17.05.2025 12:43] ********************************************************************************
[17.05.2025 12:43] Abstract 26. Human videos offer a scalable way to train robot manipulation policies, but lack the action labels needed by standard imitation learning algorithms. Existing cross-embodiment approaches try to map human motion to robot actions, but often fail when the embodiments differ significantly. We propose X-S...
[17.05.2025 12:43] Read previous papers.
[17.05.2025 12:43] Generating reviews via LLM API.
[17.05.2025 12:43] Using data from previous issue: {"categories": ["#alignment", "#science", "#rl", "#optimization", "#training", "#reasoning", "#benchmark"], "emoji": "ğŸ§ ", "ru": {"title": "Ğ’Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ°-ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ˜Ğ˜", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğµ
[17.05.2025 12:43] Using data from previous issue: {"categories": ["#multimodal", "#training", "#optimization", "#transfer_learning"], "emoji": "ğŸ§ ", "ru": {"title": "Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ LLM", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ 
[17.05.2025 12:43] Using data from previous issue: {"categories": ["#inference", "#low_resource", "#training", "#architecture", "#optimization"], "emoji": "ğŸš€", "ru": {"title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒ
[17.05.2025 12:43] Using data from previous issue: {"categories": ["#cv", "#rl", "#dataset", "#agents", "#training", "#open_source", "#reasoning"], "emoji": "ğŸ”", "ru": {"title": "OpenThinkIMG: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ˜Ğ˜ Ğ¼Ñ‹ÑĞ»Ğ¸Ñ‚ÑŒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ OpenThinkIMG - Ğ¿ĞµÑ€Ğ²ÑƒÑ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚ÑƒÑ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½ÑƒÑ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ñƒ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹
[17.05.2025 12:43] Using data from previous issue: {"categories": ["#agents", "#dataset", "#training", "#optimization", "#open_source", "#robotics", "#video"], "emoji": "ğŸ¤–", "ru": {"title": "EVAC: Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ ÑÑ€ĞµĞ´Ğ° Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ²", "desc": "EVAC - ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ¸Ñ€Ğ° Ñ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ñ‹Ğ¼ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¸Ğ¼Ğ¸Ñ‚Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ². ĞĞ½
[17.05.2025 12:43] Using data from previous issue: {"categories": ["#reasoning", "#data", "#training", "#interpretability", "#multimodal"], "emoji": "ğŸ§ ", "ru": {"title": "Ğ Ğ°ÑÑˆĞ¸Ñ„Ñ€Ğ¾Ğ²ĞºĞ° Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ˜Ğ˜: Ğ¾Ñ‚ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ğº ÑĞ½Ñ†Ğ¸ĞºĞ»Ğ¾Ğ¿ĞµĞ´Ğ¸Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (CoT) Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€
[17.05.2025 12:43] Using data from previous issue: {"categories": ["#alignment", "#cv", "#optimization", "#multimodal", "#training", "#architecture"], "emoji": "ğŸ”", "ru": {"title": "ĞšĞ¾Ğ½ĞµÑ†-Ğ²-ĞºĞ¾Ğ½ĞµÑ† Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ETT Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚
[17.05.2025 12:43] Using data from previous issue: {"categories": ["#diffusion", "#dataset", "#multimodal", "#games", "#video", "#benchmark"], "emoji": "ğŸ¤–", "ru": {"title": "EWMBench: ĞšĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº EWMBench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (Embod
[17.05.2025 12:43] Using data from previous issue: {"categories": ["#data", "#training", "#benchmark", "#optimization", "#dataset", "#rlhf", "#alignment"], "emoji": "ğŸŒ", "ru": {"title": "ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹: Ğ¾Ñ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğº Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ", "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ·Ğ°ĞºĞ¾Ğ½Ñ‹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ‚ĞµĞ¼, Ñ‡Ñ‚Ğ¾ Ğ½Ğ°Ğ±Ğ»ÑĞ´
[17.05.2025 12:43] Using data from previous issue: {"categories": ["#data", "#training", "#architecture", "#benchmark", "#open_source", "#games", "#agents", "#rl"], "emoji": "ğŸ¤–", "ru": {"title": "MLE-Dojo: Ğ˜Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ ÑÑ€ĞµĞ´Ğ° Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²", "desc": "MLE-Dojo - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½
[17.05.2025 12:43] Using data from previous issue: {"categories": ["#training", "#benchmark", "#dataset", "#security", "#ethics"], "emoji": "ğŸ§ ", "ru": {"title": "Unilogit: ÑƒĞ¼Ğ½Ğ¾Ğµ Ğ·Ğ°Ğ±Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Unilogit - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ°Ğ¼Ğ¾Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ±Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. Unilogit Ñ€ĞµÑˆ
[17.05.2025 12:43] Using data from previous issue: {"categories": ["#reasoning", "#training", "#benchmark", "#optimization", "#hallucinations", "#rlhf", "#rl"], "emoji": "ğŸ§ ", "ru": {"title": "J1: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹-ÑÑƒĞ´ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹-ÑÑƒĞ´ĞµĞ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€
[17.05.2025 12:43] Using data from previous issue: {"categories": ["#cv", "#diffusion", "#multimodal", "#optimization"], "emoji": "ğŸ¨", "ru": {"title": "Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ÑÑ‚Ğ¸Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğ¹ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğ¹ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¸ (SVG) Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ ÑÑ‚Ğ¸Ğ»Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ²
[17.05.2025 12:43] Using data from previous issue: {"categories": ["#training", "#multimodal", "#reasoning", "#open_source", "#benchmark", "#robotics", "#dataset"], "emoji": "ğŸ‘†", "ru": {"title": "PointArena: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑƒĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ PointArena - ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½ÑƒÑ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ñƒ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼
[17.05.2025 12:43] Using data from previous issue: {"categories": ["#cv", "#dataset", "#training"], "emoji": "ğŸ”", "ru": {"title": "Ğ¢Ğ¾Ñ‡Ğ½Ñ‹Ğµ ĞºĞ°Ñ€Ñ‚Ñ‹ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ´Ğ»Ñ Ğ»ÑĞ±Ñ‹Ñ… ÑÑ†ĞµĞ½ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¹", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Prior Depth Anything - Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ĞºĞ°Ñ€Ñ‚ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ´Ğ»Ñ Ğ»ÑĞ±Ñ‹Ñ… ÑÑ†ĞµĞ½. ĞĞ½ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ½Ğµ
[17.05.2025 12:43] Using data from previous issue: {"categories": ["#optimization", "#multilingual", "#training", "#transfer_learning", "#data"], "emoji": "ğŸ”„", "ru": {"title": "TokenAdapt: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ·Ğ°Ğ¼ĞµĞ½Ğ° Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ° Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ TokenAdapt Ğ´Ğ»Ñ Ğ·Ğ°Ğ¼ĞµĞ½Ñ‹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ° Ğ² Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… 
[17.05.2025 12:43] Using data from previous issue: {"categories": ["#multimodal", "#training", "#diffusion", "#survey"], "emoji": "ğŸ”¬", "ru": {"title": "Ğ“Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ LLM Ğ¸ DiT Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ ÑĞ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´ÑÑ‚ Ğ´ĞµÑ‚Ğ°
[17.05.2025 12:43] Using data from previous issue: {"categories": ["#cv", "#healthcare", "#multimodal"], "emoji": "ğŸ”ª", "ru": {"title": "ReSurgSAM2: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ…Ğ¸Ñ€ÑƒÑ€Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ†ĞµĞ½", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ReSurgSAM2 - Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ…Ğ¸Ñ€ÑƒÑ€Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ†ĞµĞ½ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². ĞĞ° Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¼ 
[17.05.2025 12:43] Using data from previous issue: {"categories": ["#cv", "#diffusion", "#3d", "#video", "#training"], "emoji": "ğŸ–¼ï¸", "ru": {"title": "3D-Fixup: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ 3D-Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ°Ğ¼Ğ¸", "desc": "3D-Fixup - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ 2D-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… 3D-Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¾Ğ². ĞĞ½ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ»Ğ¾Ğ¶
[17.05.2025 12:43] Using data from previous issue: {"categories": ["#data", "#inference", "#interpretability", "#architecture"], "emoji": "ğŸ”¬", "ru": {"title": "QuXAI: ĞŸÑ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ¾-ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ˜Ğ˜", "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ QuXAI - Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ñ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ñ… ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ¾-ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ (HQML). Ğ’ Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Q
[17.05.2025 12:43] Using data from previous issue: {"categories": ["#cv", "#benchmark", "#open_source", "#transfer_learning", "#healthcare"], "emoji": "ğŸ”", "ru": {"title": "AdaptCLIP: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ CLIP", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Adapt
[17.05.2025 12:43] Using data from previous issue: {"categories": ["#healthcare", "#hallucinations", "#agents", "#architecture", "#multimodal", "#reasoning", "#optimization", "#agi"], "emoji": "ğŸ¤–", "ru": {"title": "ĞÑ‚ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğº Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼Ñƒ Ğ˜Ğ˜: ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼", "desc": "Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ¸ Ğ°
[17.05.2025 12:43] Using data from previous issue: {"categories": ["#optimization", "#dataset", "#synthetic", "#3d", "#robotics"], "emoji": "ğŸ¤–", "ru": {"title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ²: Ğ¾Ñ‚ Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğº Ñ‚Ñ‹ÑÑÑ‡Ğ°Ğ¼ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¹", "desc": "R2R2R - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾
[17.05.2025 12:43] Using data from previous issue: {"categories": ["#cv", "#training", "#optimization", "#synthetic", "#dataset"], "emoji": "ğŸ‘ï¸", "ru": {"title": "Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹ Ğ±ĞµĞ· ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ñ‡Ğ¸ÑÑ‚Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²
[17.05.2025 12:43] Using data from previous issue: {"categories": ["#cv", "#benchmark", "#optimization", "#survey", "#dataset"], "emoji": "ğŸ”", "ru": {"title": "OneNIP: ĞĞ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹ Ğ¿Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ñƒ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ", "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ OneNIP Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹ Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ… Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ²ÑĞµĞ³Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ ÑÑ‚Ğ°Ğ»
[17.05.2025 12:43] Using data from previous issue: {"categories": ["#data", "#cv", "#training", "#benchmark", "#dataset", "#synthetic", "#diffusion"], "emoji": "ğŸ”", "ru": {"title": "Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹ Ğ¿Ğ¾ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ğ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¸Ñ… Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ AnoGen Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹ Ğ² Ğ¿Ñ€Ğ¾Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½ÑĞ¿ĞµĞºÑ†Ğ¸Ğ¸ Ñ Ğ¸Ñ
[17.05.2025 12:43] Using data from previous issue: {"categories": ["#synthetic", "#rl", "#training", "#robotics", "#optimization", "#transfer_learning"], "emoji": "ğŸ¤–", "ru": {"title": "Ğ Ğ¾Ğ±Ğ¾Ñ‚Ñ‹ ÑƒÑ‡Ğ°Ñ‚ÑÑ Ñƒ Ğ»ÑĞ´ĞµĞ¹ Ğ±ĞµĞ· Ğ¿Ñ€ÑĞ¼Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ¿Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹", "desc": "X-Sim - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸ÑĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ»ÑĞ´ÑŒĞ¼Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ 
[17.05.2025 12:43] Trying to get texts in Chinese.
[17.05.2025 12:43] Mistral request. Model: mistral-large-latest. Prompt: Write simple and brief explanation (4-5 sentences) of an article in Chinese. Use short sentences. Text:

Large reasoning models (LRMs) already possess a latent capacity for long chain-of-thought reasoning. Prior work has shown that outcome-based reinforcement learning (RL) can incidentally elicit advanced reasoning behaviors such as self-correction, backtracking, and verification phenomena often referred to as the model's "aha moment". However, the timing and consistency of these emergent behaviors remain unpredictable and uncontrollable, limiting the scalability and reliability of LRMs' reasoning capabilities. To address these limitations, we move beyond reliance on prompts and coincidental "aha moments". Instead, we explicitly align models with three meta-abilities: deduction, induction, and abduction, using automatically generated, self-verifiable tasks. Our three stage-pipeline individual alignment, parameter-space merging, and domain-specific reinforcement learning, boosting performance by over 10\% relative to instruction-tuned baselines. Furthermore, domain-specific RL from the aligned checkpoint yields an additional 2\% average gain in the performance ceiling across math, coding, and science benchmarks, demonstrating that explicit meta-ability alignment offers a scalable and dependable foundation for reasoning. Code is available at: https://github.com/zhiyuanhubj/Meta-Ability-Alignment
[17.05.2025 12:43] Mistral response. {"id": "fca23a35cc5f42ff8e4ad8db9e3d16c0", "object": "chat.completion", "created": 1747485834, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "\u8fd9\u7bc7\u6587\u7ae0\u8ba8\u8bba\u4e86\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u7684\u6f5c\u5728\u957f\u94fe\u63a8\u7406\u80fd\u529b\u3002\u4e4b\u524d\u7684\u7814\u7a76\u8868\u660e\uff0c\u7ed3\u679c\u5bfc\u5411\u7684\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u53ef\u4ee5\u5076\u7136\u5f15\u53d1\u9ad8\u7ea7\u63a8\u7406\u884c\u4e3a\uff0c\u5982\u81ea\u6211\u6821\u6b63\u548c\u9a8c\u8bc1\u73b0\u8c61\uff0c\u88ab\u79f0\u4e3a\u6a21\u578b\u7684\u201c\u987f\u609f\u65f6\u523b\u201d\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u884c\u4e3a\u7684\u65f6\u95f4\u548c\u4e00\u81f4\u6027\u96be\u4ee5\u9884\u6d4b\uff0c\u9650\u5236\u4e86LRMs\u7684\u63a8\u7406\u80fd\u529b\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u4e09\u9636\u6bb5\u7684\u65b9\u6cd5\uff0c\u5c06\u6a21\u578b\u4e0e\u6f14\u7ece\u3001\u5f52\u7eb3\u548c\u6eaf\u56e0\u4e09\u79cd\u5143\u80fd\u529b\u5bf9\u9f50\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u63a8\u7406\u6027\u80fd\u3002\u4ee3\u7801\u53ef\u5728GitHub\u4e0a\u627e\u5230\u3002"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 308, "total_tokens": 506, "completion_tokens": 198}}
[17.05.2025 12:43] Response: è¿™ç¯‡æ–‡ç« è®¨è®ºäº†å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰çš„æ½œåœ¨é•¿é“¾æ¨ç†èƒ½åŠ›ã€‚ä¹‹å‰çš„ç ”ç©¶è¡¨æ˜ï¼Œç»“æœå¯¼å‘çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¯ä»¥å¶ç„¶å¼•å‘é«˜çº§æ¨ç†è¡Œä¸ºï¼Œå¦‚è‡ªæˆ‘æ ¡æ­£å’ŒéªŒè¯ç°è±¡ï¼Œè¢«ç§°ä¸ºæ¨¡å‹çš„â€œé¡¿æ‚Ÿæ—¶åˆ»â€ã€‚ç„¶è€Œï¼Œè¿™äº›è¡Œä¸ºçš„æ—¶é—´å’Œä¸€è‡´æ€§éš¾ä»¥é¢„æµ‹ï¼Œé™åˆ¶äº†LRMsçš„æ¨ç†èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œä½œè€…æå‡ºäº†ä¸€ç§ä¸‰é˜¶æ®µçš„æ–¹æ³•ï¼Œå°†æ¨¡å‹ä¸æ¼”ç»ã€å½’çº³å’Œæº¯å› ä¸‰ç§å…ƒèƒ½åŠ›å¯¹é½ï¼Œä»è€Œæé«˜äº†æ¨ç†æ€§èƒ½ã€‚ä»£ç å¯åœ¨GitHubä¸Šæ‰¾åˆ°ã€‚
[17.05.2025 12:43] Mistral request. Model: mistral-large-latest. Prompt: Write pinyin transcription for text. Text:

è¿™ç¯‡æ–‡ç« è®¨è®ºäº†å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰çš„æ½œåœ¨é•¿é“¾æ¨ç†èƒ½åŠ›ã€‚ä¹‹å‰çš„ç ”ç©¶è¡¨æ˜ï¼Œç»“æœå¯¼å‘çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¯ä»¥å¶ç„¶å¼•å‘é«˜çº§æ¨ç†è¡Œä¸ºï¼Œå¦‚è‡ªæˆ‘æ ¡æ­£å’ŒéªŒè¯ç°è±¡ï¼Œè¢«ç§°ä¸ºæ¨¡å‹çš„â€œé¡¿æ‚Ÿæ—¶åˆ»â€ã€‚ç„¶è€Œï¼Œè¿™äº›è¡Œä¸ºçš„æ—¶é—´å’Œä¸€è‡´æ€§éš¾ä»¥é¢„æµ‹ï¼Œé™åˆ¶äº†LRMsçš„æ¨ç†èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œä½œè€…æå‡ºäº†ä¸€ç§ä¸‰é˜¶æ®µçš„æ–¹æ³•ï¼Œå°†æ¨¡å‹ä¸æ¼”ç»ã€å½’çº³å’Œæº¯å› ä¸‰ç§å…ƒèƒ½åŠ›å¯¹é½ï¼Œä»è€Œæé«˜äº†æ¨ç†æ€§èƒ½ã€‚ä»£ç å¯åœ¨GitHubä¸Šæ‰¾åˆ°ã€‚
[17.05.2025 12:44] Mistral response. {"id": "1850da4980bb4f57b6a6a45ed29868df", "object": "chat.completion", "created": 1747485837, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "\u8fd9\u7bc7\u6587\u7ae0\u8ba8\u8bba\u4e86\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u7684\u6f5c\u5728\u957f\u94fe\u63a8\u7406\u80fd\u529b\u3002\u4e4b\u524d\u7684\u7814\u7a76\u8868\u660e\uff0c\u7ed3\u679c\u5bfc\u5411\u7684\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u53ef\u4ee5\u5076\u7136\u5f15\u53d1\u9ad8\u7ea7\u63a8\u7406\u884c\u4e3a\uff0c\u5982\u81ea\u6211\u6821\u6b63\u548c\u9a8c\u8bc1\u73b0\u8c61\uff0c\u88ab\u79f0\u4e3a\u6a21\u578b\u7684\u201c\u987f\u609f\u65f6\u523b\u201d\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u884c\u4e3a\u7684\u65f6\u95f4\u548c\u4e00\u81f4\u6027\u96be\u4ee5\u9884\u6d4b\uff0c\u9650\u5236\u4e86LRMs\u7684\u63a8\u7406\u80fd\u529b\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u4e09\u9636\u6bb5\u7684\u65b9\u6cd5\uff0c\u5c06\u6a21\u578b\u4e0e\u6f14\u7ece\u3001\u5f52\u7eb3\u548c\u6eaf\u56e0\u4e09\u79cd\u5143\u80fd\u529b\u5bf9\u9f50\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u63a8\u7406\u6027\u80fd\u3002\u4ee3\u7801\u53ef\u5728GitHub\u4e0a\u627e\u5230\u3002\n\nzh\u00e8 pi\u0101n w\u00e9n zh\u0101ng t\u01ceo l\u00f9n le d\u00e0 x\u00edng tu\u012b l\u01d0 m\u00f3 x\u00edng (LRMs) de qi\u00e1n z\u00e0i ch\u00e1ng li\u00e1n tu\u012b l\u01d0 n\u00e9ng l\u00ec. zh\u012b qi\u00e1n de y\u00e1n ji\u016b bi\u01ceo m\u00edng, ji\u00e9 gu\u01d2 d\u01ceo xi\u00e0ng de qi\u00e1ng hu\u00e0 xu\u00e9 x\u00ed (RL) k\u011b y\u01d0 \u01d2u r\u00e1n y\u01d0n f\u0101 g\u0101o j\u00ed tu\u012b l\u01d0 x\u00edng w\u00e9i, r\u00fa z\u00ec w\u01d2 ji\u00e0o zh\u00e8ng h\u00e9 y\u00e0n zh\u00e8ng xi\u00e0n xi\u00e0ng, b\u00e8i ch\u0113ng w\u00e9i m\u00f3 x\u00edng de \u201cd\u00f9n w\u00f9 sh\u00ed k\u00e8\u201d. r\u00e1n \u00e9r, zh\u00e8 xi\u0113 x\u00edng w\u00e9i de sh\u00ed ji\u0101n h\u00e9 y\u012b zh\u00ec x\u00ecng n\u00e1n y\u01d0 y\u00f9 c\u00e8, xi\u00e0n zh\u00ec le LRMs de tu\u012b l\u01d0 n\u00e9ng l\u00ec. w\u00e8i le ji\u011b ju\u00e9 zh\u00e8 xi\u0113 w\u00e8n t\u00ed, zu\u00f2 zh\u011b t\u00ed ch\u016b le y\u012b zh\u01d2ng s\u0101n ji\u0113 du\u00e0n de f\u0101ng f\u01ce, ji\u0101ng m\u00f3 x\u00edng y\u01d4 y\u01cen yu\u00e8, gu\u012b n\u00e0 h\u00e9 s\u00f9 y\u012bn s\u0101n zh\u01d2ng yu\u00e1n n\u00e9ng l\u00ec du\u00ec q\u00ed, c\u00f3ng \u00e9r t\u00ed g\u0101o le tu\u012b l\u01d0 x\u00ecng n\u00e9ng. d\u00e0i m\u01ce k\u011b z\u00e0i GitHub sh\u00e0ng zh\u01ceo d\u00e0o."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 212, "total_tokens": 806, "completion_tokens": 594}}
[17.05.2025 12:44] Response: è¿™ç¯‡æ–‡ç« è®¨è®ºäº†å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰çš„æ½œåœ¨é•¿é“¾æ¨ç†èƒ½åŠ›ã€‚ä¹‹å‰çš„ç ”ç©¶è¡¨æ˜ï¼Œç»“æœå¯¼å‘çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¯ä»¥å¶ç„¶å¼•å‘é«˜çº§æ¨ç†è¡Œä¸ºï¼Œå¦‚è‡ªæˆ‘æ ¡æ­£å’ŒéªŒè¯ç°è±¡ï¼Œè¢«ç§°ä¸ºæ¨¡å‹çš„â€œé¡¿æ‚Ÿæ—¶åˆ»â€ã€‚ç„¶è€Œï¼Œè¿™äº›è¡Œä¸ºçš„æ—¶é—´å’Œä¸€è‡´æ€§éš¾ä»¥é¢„æµ‹ï¼Œé™åˆ¶äº†LRMsçš„æ¨ç†èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œä½œè€…æå‡ºäº†ä¸€ç§ä¸‰é˜¶æ®µçš„æ–¹æ³•ï¼Œå°†æ¨¡å‹ä¸æ¼”ç»ã€å½’çº³å’Œæº¯å› ä¸‰ç§å…ƒèƒ½åŠ›å¯¹é½ï¼Œä»è€Œæé«˜äº†æ¨ç†æ€§èƒ½ã€‚ä»£ç å¯åœ¨GitHubä¸Šæ‰¾åˆ°ã€‚

zhÃ¨ piÄn wÃ©n zhÄng tÇo lÃ¹n le dÃ  xÃ­ng tuÄ« lÇ mÃ³ xÃ­ng (LRMs) de qiÃ¡n zÃ i chÃ¡ng liÃ¡n tuÄ« lÇ nÃ©ng lÃ¬. zhÄ« qiÃ¡n de yÃ¡n jiÅ« biÇo mÃ­ng, jiÃ© guÇ’ dÇo xiÃ ng de qiÃ¡ng huÃ  xuÃ© xÃ­ (RL) kÄ› yÇ Ç’u rÃ¡n yÇn fÄ gÄo jÃ­ tuÄ« lÇ xÃ­ng wÃ©i, rÃº zÃ¬ wÇ’ jiÃ o zhÃ¨ng hÃ© yÃ n zhÃ¨ng xiÃ n xiÃ ng, bÃ¨i chÄ“ng wÃ©i mÃ³ xÃ­ng de â€œdÃ¹n wÃ¹ shÃ­ kÃ¨â€. rÃ¡n Ã©r, zhÃ¨ xiÄ“ xÃ­ng wÃ©i de shÃ­ jiÄn hÃ© yÄ« zhÃ¬ xÃ¬ng nÃ¡n yÇ yÃ¹ cÃ¨, xiÃ n zhÃ¬ le LRMs de tuÄ« lÇ nÃ©ng lÃ¬. wÃ¨i le jiÄ› juÃ© zhÃ¨ xiÄ“ wÃ¨n tÃ­, zuÃ² zhÄ› tÃ­ chÅ« le yÄ« zhÇ’ng sÄn jiÄ“ duÃ n de fÄng fÇ, jiÄng mÃ³ xÃ­ng yÇ” yÇn yuÃ¨, guÄ« nÃ  hÃ© sÃ¹ yÄ«n sÄn zhÇ’ng yuÃ¡n nÃ©ng lÃ¬ duÃ¬ qÃ­, cÃ³ng Ã©r tÃ­ gÄo le tuÄ« lÇ xÃ¬ng nÃ©ng. dÃ i mÇ kÄ› zÃ i GitHub shÃ ng zhÇo dÃ o.
[17.05.2025 12:44] Mistral request. Model: mistral-large-latest. Prompt: Write vocab of difficult words for this text as an array of objects with fields 'word', 'pinyin', 'trans'. Return as python list without formatting. Return list and nothing else. Text:

è¿™ç¯‡æ–‡ç« è®¨è®ºäº†å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰çš„æ½œåœ¨é•¿é“¾æ¨ç†èƒ½åŠ›ã€‚ä¹‹å‰çš„ç ”ç©¶è¡¨æ˜ï¼Œç»“æœå¯¼å‘çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¯ä»¥å¶ç„¶å¼•å‘é«˜çº§æ¨ç†è¡Œä¸ºï¼Œå¦‚è‡ªæˆ‘æ ¡æ­£å’ŒéªŒè¯ç°è±¡ï¼Œè¢«ç§°ä¸ºæ¨¡å‹çš„â€œé¡¿æ‚Ÿæ—¶åˆ»â€ã€‚ç„¶è€Œï¼Œè¿™äº›è¡Œä¸ºçš„æ—¶é—´å’Œä¸€è‡´æ€§éš¾ä»¥é¢„æµ‹ï¼Œé™åˆ¶äº†LRMsçš„æ¨ç†èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œä½œè€…æå‡ºäº†ä¸€ç§ä¸‰é˜¶æ®µçš„æ–¹æ³•ï¼Œå°†æ¨¡å‹ä¸æ¼”ç»ã€å½’çº³å’Œæº¯å› ä¸‰ç§å…ƒèƒ½åŠ›å¯¹é½ï¼Œä»è€Œæé«˜äº†æ¨ç†æ€§èƒ½ã€‚ä»£ç å¯åœ¨GitHubä¸Šæ‰¾åˆ°ã€‚
[17.05.2025 12:44] Mistral response. {"id": "084c716383194d2ca7b1e271df14fd62", "object": "chat.completion", "created": 1747485846, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "[\n    {\"word\": \"\u8ba8\u8bba\", \"pinyin\": \"t\u01ceo l\u00f9n\", \"trans\": \"discuss\"},\n    {\"word\": \"\u6f5c\u5728\", \"pinyin\": \"qi\u00e1n z\u00e0i\", \"trans\": \"potential\"},\n    {\"word\": \"\u957f\u94fe\", \"pinyin\": \"ch\u00e1ng li\u00e0n\", \"trans\": \"long-chain\"},\n    {\"word\": \"\u63a8\u7406\", \"pinyin\": \"tu\u012b l\u01d0\", \"trans\": \"reasoning\"},\n    {\"word\": \"\u80fd\u529b\", \"pinyin\": \"n\u00e9ng l\u00ec\", \"trans\": \"ability\"},\n    {\"word\": \"\u4e4b\u524d\", \"pinyin\": \"zh\u012b qi\u00e1n\", \"trans\": \"before\"},\n    {\"word\": \"\u7814\u7a76\", \"pinyin\": \"y\u00e1n ji\u016b\", \"trans\": \"research\"},\n    {\"word\": \"\u8868\u660e\", \"pinyin\": \"bi\u01ceo m\u00edng\", \"trans\": \"indicate\"},\n    {\"word\": \"\u7ed3\u679c\", \"pinyin\": \"ji\u00e9 gu\u01d2\", \"trans\": \"result\"},\n    {\"word\": \"\u5bfc\u5411\", \"pinyin\": \"d\u01ceo xi\u00e0ng\", \"trans\": \"oriented\"},\n    {\"word\": \"\u5f3a\u5316\", \"pinyin\": \"qi\u00e1ng hu\u00e0\", \"trans\": \"reinforcement\"},\n    {\"word\": \"\u5b66\u4e60\", \"pinyin\": \"xu\u00e9 x\u00ed\", \"trans\": \"learning\"},\n    {\"word\": \"\u5076\u7136\", \"pinyin\": \"\u01d2u r\u00e1n\", \"trans\": \"occasionally\"},\n    {\"word\": \"\u5f15\u53d1\", \"pinyin\": \"y\u01d0n f\u0101\", \"trans\": \"trigger\"},\n    {\"word\": \"\u9ad8\u7ea7\", \"pinyin\": \"g\u0101o j\u00ed\", \"trans\": \"advanced\"},\n    {\"word\": \"\u884c\u4e3a\", \"pinyin\": \"x\u00edng w\u00e9i\", \"trans\": \"behavior\"},\n    {\"word\": \"\u81ea\u6211\", \"pinyin\": \"z\u00ec w\u01d2\", \"trans\": \"self\"},\n    {\"word\": \"\u6821\u6b63\", \"pinyin\": \"ji\u00e0o zh\u00e8ng\", \"trans\": \"correct\"},\n    {\"word\": \"\u9a8c\u8bc1\", \"pinyin\": \"y\u00e0n zh\u00e8ng\", \"trans\": \"verify\"},\n    {\"word\": \"\u73b0\u8c61\", \"pinyin\": \"xi\u00e0n xi\u00e0ng\", \"trans\": \"phenomenon\"},\n    {\"word\": \"\u79f0\u4e3a\", \"pinyin\": \"ch\u0113ng w\u00e9i\", \"trans\": \"called\"},\n    {\"word\": \"\u987f\u609f\", \"pinyin\": \"d\u00f9n w\u00f9\", \"trans\": \"epiphany\"},\n    {\"word\": \"\u65f6\u523b\", \"pinyin\": \"sh\u00ed k\u00e8\", \"trans\": \"moment\"},\n    {\"word\": \"\u4e00\u81f4\u6027\", \"pinyin\": \"y\u012b zh\u00ec x\u00ecng\", \"trans\": \"consistency\"},\n    {\"word\": \"\u9884\u6d4b\", \"pinyin\": \"y\u00f9 c\u00e8\", \"trans\": \"predict\"},\n    {\"word\": \"\u9650\u5236\", \"pinyin\": \"xi\u00e0n zh\u00ec\", \"trans\": \"limit\"},\n    {\"word\": \"\u63d0\u51fa\", \"pinyin\": \"t\u00ed ch\u016b\", \"trans\": \"propose\"},\n    {\"word\": \"\u4e09\u9636\u6bb5\", \"pinyin\": \"s\u0101n ji\u0113 du\u00e0n\", \"trans\": \"three-stage\"},\n    {\"word\": \"\u65b9\u6cd5\", \"pinyin\": \"f\u0101ng f\u01ce\", \"trans\": \"method\"},\n    {\"word\": \"\u5bf9\u9f50\", \"pinyin\": \"du\u00ec q\u00ed\", \"trans\": \"align\"},\n    {\"word\": \"\u6f14\u7ece\", \"pinyin\": \"y\u01cen y\u00ec\", \"trans\": \"deduction\"},\n    {\"word\": \"\u5f52\u7eb3\", \"pinyin\": \"gu\u012b n\u00e0\", \"trans\": \"induction\"},\n    {\"word\": \"\u6eaf\u56e0\", \"pinyin\": \"s\u00f9 y\u012bn\", \"trans\": \"abduction\"},\n    {\"word\": \"\u5143\", \"pinyin\": \"yu\u00e1n\", \"trans\": \"meta\"},\n    {\"word\": \"\u6027\u80fd\", \"pinyin\": \"x\u00ecng n\u00e9ng\", \"trans\": \"performance\"},\n    {\"word\": \"\u4ee3\u7801\", \"pinyin\": \"d\u00e0i m\u01ce\", \"trans\": \"code\"},\n    {\"word\": \"GitHub\", \"pinyin\": \"GitHub\", \"trans\": \"GitHub\"}\n]"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 244, "total_tokens": 1302, "completion_tokens": 1058}}
[17.05.2025 12:44] Response: [
    {"word": "è®¨è®º", "pinyin": "tÇo lÃ¹n", "trans": "discuss"},
    {"word": "æ½œåœ¨", "pinyin": "qiÃ¡n zÃ i", "trans": "potential"},
    {"word": "é•¿é“¾", "pinyin": "chÃ¡ng liÃ n", "trans": "long-chain"},
    {"word": "æ¨ç†", "pinyin": "tuÄ« lÇ", "trans": "reasoning"},
    {"word": "èƒ½åŠ›", "pinyin": "nÃ©ng lÃ¬", "trans": "ability"},
    {"word": "ä¹‹å‰", "pinyin": "zhÄ« qiÃ¡n", "trans": "before"},
    {"word": "ç ”ç©¶", "pinyin": "yÃ¡n jiÅ«", "trans": "research"},
    {"word": "è¡¨æ˜", "pinyin": "biÇo mÃ­ng", "trans": "indicate"},
    {"word": "ç»“æœ", "pinyin": "jiÃ© guÇ’", "trans": "result"},
    {"word": "å¯¼å‘", "pinyin": "dÇo xiÃ ng", "trans": "oriented"},
    {"word": "å¼ºåŒ–", "pinyin": "qiÃ¡ng huÃ ", "trans": "reinforcement"},
    {"word": "å­¦ä¹ ", "pinyin": "xuÃ© xÃ­", "trans": "learning"},
    {"word": "å¶ç„¶", "pinyin": "Ç’u rÃ¡n", "trans": "occasionally"},
    {"word": "å¼•å‘", "pinyin": "yÇn fÄ", "trans": "trigger"},
    {"word": "é«˜çº§", "pinyin": "gÄo jÃ­", "trans": "advanced"},
    {"word": "è¡Œä¸º", "pinyin": "xÃ­ng wÃ©i", "trans": "behavior"},
    {"word": "è‡ªæˆ‘", "pinyin": "zÃ¬ wÇ’", "trans": "self"},
    {"word": "æ ¡æ­£", "pinyin": "jiÃ o zhÃ¨ng", "trans": "correct"},
    {"word": "éªŒè¯", "pinyin": "yÃ n zhÃ¨ng", "trans": "verify"},
    {"word": "ç°è±¡", "pinyin": "xiÃ n xiÃ ng", "trans": "phenomenon"},
    {"word": "ç§°ä¸º", "pinyin": "chÄ“ng wÃ©i", "trans": "called"},
    {"word": "é¡¿æ‚Ÿ", "pinyin": "dÃ¹n wÃ¹", "trans": "epiphany"},
    {"word": "æ—¶åˆ»", "pinyin": "shÃ­ kÃ¨", "trans": "moment"},
    {"word": "ä¸€è‡´æ€§", "pinyin": "yÄ« zhÃ¬ xÃ¬ng", "trans": "consistency"},
    {"word": "é¢„æµ‹", "pinyin": "yÃ¹ cÃ¨", "trans": "predict"},
    {"word": "é™åˆ¶", "pinyin": "xiÃ n zhÃ¬", "trans": "limit"},
    {"word": "æå‡º", "pinyin": "tÃ­ chÅ«", "trans": "propose"},
    {"word": "ä¸‰é˜¶æ®µ", "pinyin": "sÄn jiÄ“ duÃ n", "trans": "three-stage"},
    {"word": "æ–¹æ³•", "pinyin": "fÄng fÇ", "trans": "method"},
    {"word": "å¯¹é½", "pinyin": "duÃ¬ qÃ­", "trans": "align"},
    {"word": "æ¼”ç»", "pinyin": "yÇn yÃ¬", "trans": "deduction"},
    {"word": "å½’çº³", "pinyin": "guÄ« nÃ ", "trans": "induction"},
    {"word": "æº¯å› ", "pinyin": "sÃ¹ yÄ«n", "trans": "abduction"},
    {"word": "å…ƒ", "pinyin": "yuÃ¡n", "trans": "meta"},
    {"word": "æ€§èƒ½", "pinyin": "xÃ¬ng nÃ©ng", "trans": "performance"},
    {"word": "ä»£ç ", "pinyin": "dÃ i mÇ", "trans": "code"},
    {"word": "GitHub", "pinyin": "GitHub", "trans": "GitHub"}
]
[17.05.2025 12:44] Mistral request. Model: mistral-large-latest. Prompt: Translate this text in English. Text:

è¿™ç¯‡æ–‡ç« è®¨è®ºäº†å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰çš„æ½œåœ¨é•¿é“¾æ¨ç†èƒ½åŠ›ã€‚ä¹‹å‰çš„ç ”ç©¶è¡¨æ˜ï¼Œç»“æœå¯¼å‘çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¯ä»¥å¶ç„¶å¼•å‘é«˜çº§æ¨ç†è¡Œä¸ºï¼Œå¦‚è‡ªæˆ‘æ ¡æ­£å’ŒéªŒè¯ç°è±¡ï¼Œè¢«ç§°ä¸ºæ¨¡å‹çš„â€œé¡¿æ‚Ÿæ—¶åˆ»â€ã€‚ç„¶è€Œï¼Œè¿™äº›è¡Œä¸ºçš„æ—¶é—´å’Œä¸€è‡´æ€§éš¾ä»¥é¢„æµ‹ï¼Œé™åˆ¶äº†LRMsçš„æ¨ç†èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œä½œè€…æå‡ºäº†ä¸€ç§ä¸‰é˜¶æ®µçš„æ–¹æ³•ï¼Œå°†æ¨¡å‹ä¸æ¼”ç»ã€å½’çº³å’Œæº¯å› ä¸‰ç§å…ƒèƒ½åŠ›å¯¹é½ï¼Œä»è€Œæé«˜äº†æ¨ç†æ€§èƒ½ã€‚ä»£ç å¯åœ¨GitHubä¸Šæ‰¾åˆ°ã€‚
[17.05.2025 12:44] Mistral response. {"id": "242d4d053eb74ae3b90f2de24a5b4804", "object": "chat.completion", "created": 1747485858, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "This article discusses the potential long-chain reasoning capabilities of Large Reasoning Models (LRMs). Previous research has shown that result-oriented reinforcement learning (RL) can occasionally trigger advanced reasoning behaviors, such as self-correction and verification phenomena, referred to as the model's \"eureka moments.\" However, the timing and consistency of these behaviors are difficult to predict, limiting the reasoning capabilities of LRMs. To address these issues, the authors propose a three-stage method that aligns the model with deductive, inductive, and abductive meta-abilities, thereby enhancing reasoning performance. The code can be found on GitHub."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 210, "total_tokens": 353, "completion_tokens": 143}}
[17.05.2025 12:44] Response: This article discusses the potential long-chain reasoning capabilities of Large Reasoning Models (LRMs). Previous research has shown that result-oriented reinforcement learning (RL) can occasionally trigger advanced reasoning behaviors, such as self-correction and verification phenomena, referred to as the model's "eureka moments." However, the timing and consistency of these behaviors are difficult to predict, limiting the reasoning capabilities of LRMs. To address these issues, the authors propose a three-stage method that aligns the model with deductive, inductive, and abductive meta-abilities, thereby enhancing reasoning performance. The code can be found on GitHub.
[17.05.2025 12:44] Renaming data file.
[17.05.2025 12:44] Renaming previous data. hf_papers.json to ./d/2025-05-16.json
[17.05.2025 12:44] Saving new data file.
[17.05.2025 12:44] Generating page.
[17.05.2025 12:44] Renaming previous page.
[17.05.2025 12:44] Renaming previous data. index.html to ./d/2025-05-16.html
[17.05.2025 12:44] [Experimental] Generating Chinese page for reading.
[17.05.2025 12:44] Chinese vocab [{'word': 'è®¨è®º', 'pinyin': 'tÇo lÃ¹n', 'trans': 'discuss'}, {'word': 'æ½œåœ¨', 'pinyin': 'qiÃ¡n zÃ i', 'trans': 'potential'}, {'word': 'é•¿é“¾', 'pinyin': 'chÃ¡ng liÃ n', 'trans': 'long-chain'}, {'word': 'æ¨ç†', 'pinyin': 'tuÄ« lÇ', 'trans': 'reasoning'}, {'word': 'èƒ½åŠ›', 'pinyin': 'nÃ©ng lÃ¬', 'trans': 'ability'}, {'word': 'ä¹‹å‰', 'pinyin': 'zhÄ« qiÃ¡n', 'trans': 'before'}, {'word': 'ç ”ç©¶', 'pinyin': 'yÃ¡n jiÅ«', 'trans': 'research'}, {'word': 'è¡¨æ˜', 'pinyin': 'biÇo mÃ­ng', 'trans': 'indicate'}, {'word': 'ç»“æœ', 'pinyin': 'jiÃ© guÇ’', 'trans': 'result'}, {'word': 'å¯¼å‘', 'pinyin': 'dÇo xiÃ ng', 'trans': 'oriented'}, {'word': 'å¼ºåŒ–', 'pinyin': 'qiÃ¡ng huÃ ', 'trans': 'reinforcement'}, {'word': 'å­¦ä¹ ', 'pinyin': 'xuÃ© xÃ­', 'trans': 'learning'}, {'word': 'å¶ç„¶', 'pinyin': 'Ç’u rÃ¡n', 'trans': 'occasionally'}, {'word': 'å¼•å‘', 'pinyin': 'yÇn fÄ', 'trans': 'trigger'}, {'word': 'é«˜çº§', 'pinyin': 'gÄo jÃ­', 'trans': 'advanced'}, {'word': 'è¡Œä¸º', 'pinyin': 'xÃ­ng wÃ©i', 'trans': 'behavior'}, {'word': 'è‡ªæˆ‘', 'pinyin': 'zÃ¬ wÇ’', 'trans': 'self'}, {'word': 'æ ¡æ­£', 'pinyin': 'jiÃ o zhÃ¨ng', 'trans': 'correct'}, {'word': 'éªŒè¯', 'pinyin': 'yÃ n zhÃ¨ng', 'trans': 'verify'}, {'word': 'ç°è±¡', 'pinyin': 'xiÃ n xiÃ ng', 'trans': 'phenomenon'}, {'word': 'ç§°ä¸º', 'pinyin': 'chÄ“ng wÃ©i', 'trans': 'called'}, {'word': 'é¡¿æ‚Ÿ', 'pinyin': 'dÃ¹n wÃ¹', 'trans': 'epiphany'}, {'word': 'æ—¶åˆ»', 'pinyin': 'shÃ­ kÃ¨', 'trans': 'moment'}, {'word': 'ä¸€è‡´æ€§', 'pinyin': 'yÄ« zhÃ¬ xÃ¬ng', 'trans': 'consistency'}, {'word': 'é¢„æµ‹', 'pinyin': 'yÃ¹ cÃ¨', 'trans': 'predict'}, {'word': 'é™åˆ¶', 'pinyin': 'xiÃ n zhÃ¬', 'trans': 'limit'}, {'word': 'æå‡º', 'pinyin': 'tÃ­ chÅ«', 'trans': 'propose'}, {'word': 'ä¸‰é˜¶æ®µ', 'pinyin': 'sÄn jiÄ“ duÃ n', 'trans': 'three-stage'}, {'word': 'æ–¹æ³•', 'pinyin': 'fÄng fÇ', 'trans': 'method'}, {'word': 'å¯¹é½', 'pinyin': 'duÃ¬ qÃ­', 'trans': 'align'}, {'word': 'æ¼”ç»', 'pinyin': 'yÇn yÃ¬', 'trans': 'deduction'}, {'word': 'å½’çº³', 'pinyin': 'guÄ« nÃ ', 'trans': 'induction'}, {'word': 'æº¯å› ', 'pinyin': 'sÃ¹ yÄ«n', 'trans': 'abduction'}, {'word': 'å…ƒ', 'pinyin': 'yuÃ¡n', 'trans': 'meta'}, {'word': 'æ€§èƒ½', 'pinyin': 'xÃ¬ng nÃ©ng', 'trans': 'performance'}, {'word': 'ä»£ç ', 'pinyin': 'dÃ i mÇ', 'trans': 'code'}, {'word': 'GitHub', 'pinyin': 'GitHub', 'trans': 'GitHub'}]
[17.05.2025 12:44] Renaming previous Chinese page.
[17.05.2025 12:44] Renaming previous data. zh.html to ./d/2025-05-16_zh_reading_task.html
[17.05.2025 12:44] Writing Chinese reading task.
[17.05.2025 12:44] Writing result.
[17.05.2025 12:44] Renaming log file.
[17.05.2025 12:44] Renaming previous data. log.txt to ./logs/2025-05-17_last_log.txt
