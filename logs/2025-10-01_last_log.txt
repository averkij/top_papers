[30.09.2025 23:11] Read previous papers.
[30.09.2025 23:11] Generating top page (month).
[30.09.2025 23:11] Writing top page (month).
[01.10.2025 00:57] Read previous papers.
[01.10.2025 00:57] Get feed.
[01.10.2025 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2509.24006
[01.10.2025 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2509.22220
[01.10.2025 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2509.23102
[01.10.2025 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2509.24897
[01.10.2025 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2509.24900
[01.10.2025 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2509.23808
[01.10.2025 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2509.24695
[01.10.2025 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2509.25190
[01.10.2025 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2509.23426
[01.10.2025 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2509.22193
[01.10.2025 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2509.25160
[01.10.2025 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2509.23909
[01.10.2025 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2509.24014
[01.10.2025 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2509.25175
[01.10.2025 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2509.24007
[01.10.2025 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2509.25106
[01.10.2025 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2509.24981
[01.10.2025 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2509.22799
[01.10.2025 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2509.22824
[01.10.2025 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2509.25123
[01.10.2025 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2509.24473
[01.10.2025 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2509.22820
[01.10.2025 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2509.25191
[01.10.2025 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2509.25161
[01.10.2025 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2509.25077
[01.10.2025 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2509.24663
[01.10.2025 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2509.25176
[01.10.2025 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2509.25137
[01.10.2025 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2509.25131
[01.10.2025 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2509.25084
[01.10.2025 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2509.23951
[01.10.2025 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2509.23285
[01.10.2025 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2509.22572
[01.10.2025 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2509.23219
[01.10.2025 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2509.23050
[01.10.2025 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2509.22921
[01.10.2025 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2509.23196
[01.10.2025 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2509.23924
[01.10.2025 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2509.23866
[01.10.2025 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2509.23564
[01.10.2025 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2509.21875
[01.10.2025 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2509.25149
[01.10.2025 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2509.24335
[01.10.2025 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2509.24285
[01.10.2025 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2509.24193
[01.10.2025 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2509.24786
[01.10.2025 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2509.23371
[01.10.2025 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2509.21953
[01.10.2025 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2509.25185
[01.10.2025 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2509.25050
[01.10.2025 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2509.24494
[01.10.2025 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2509.23061
[01.10.2025 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2509.24372
[01.10.2025 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2509.24269
[01.10.2025 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2509.24200
[01.10.2025 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2509.23338
[01.10.2025 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2509.23143
[01.10.2025 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2509.22830
[01.10.2025 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2509.22570
[01.10.2025 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2509.25052
[01.10.2025 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2509.24910
[01.10.2025 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2509.24317
[01.10.2025 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2509.24148
[01.10.2025 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2509.23115
[01.10.2025 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2509.22518
[01.10.2025 00:57] Extract page data from URL. URL: https://huggingface.co/papers/2509.24935
[01.10.2025 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2509.24908
[01.10.2025 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2509.24878
[01.10.2025 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2509.24726
[01.10.2025 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2509.24709
[01.10.2025 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2509.24592
[01.10.2025 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2509.23233
[01.10.2025 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2509.21043
[01.10.2025 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2509.18582
[01.10.2025 00:57] Extract page data from URL. URL: https://huggingface.co/papers/2509.25413
[01.10.2025 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2509.25171
[01.10.2025 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2509.24988
[01.10.2025 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2509.22991
[01.10.2025 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2509.19033
[01.10.2025 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2509.16538
[01.10.2025 00:57] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[01.10.2025 00:57] No deleted papers detected.
[01.10.2025 00:57] Downloading and parsing papers (pdf, html). Total: 80.
[01.10.2025 00:57] Downloading and parsing paper https://huggingface.co/papers/2509.24006.
[01.10.2025 00:57] Downloading paper 2509.24006 from http://arxiv.org/pdf/2509.24006v1...
[01.10.2025 00:57] Failed to download and parse paper https://huggingface.co/papers/2509.24006: 'LTChar' object is not iterable
[01.10.2025 00:57] Downloading and parsing paper https://huggingface.co/papers/2509.22220.
[01.10.2025 00:57] Extra JSON file exists (./assets/json/2509.22220.json), skip PDF parsing.
[01.10.2025 00:57] Paper image links file exists (./assets/img_data/2509.22220.json), skip HTML parsing.
[01.10.2025 00:57] Success.
[01.10.2025 00:57] Downloading and parsing paper https://huggingface.co/papers/2509.23102.
[01.10.2025 00:57] Extra JSON file exists (./assets/json/2509.23102.json), skip PDF parsing.
[01.10.2025 00:57] Paper image links file exists (./assets/img_data/2509.23102.json), skip HTML parsing.
[01.10.2025 00:57] Success.
[01.10.2025 00:57] Downloading and parsing paper https://huggingface.co/papers/2509.24897.
[01.10.2025 00:57] Extra JSON file exists (./assets/json/2509.24897.json), skip PDF parsing.
[01.10.2025 00:57] Paper image links file exists (./assets/img_data/2509.24897.json), skip HTML parsing.
[01.10.2025 00:57] Success.
[01.10.2025 00:57] Downloading and parsing paper https://huggingface.co/papers/2509.24900.
[01.10.2025 00:57] Extra JSON file exists (./assets/json/2509.24900.json), skip PDF parsing.
[01.10.2025 00:57] Paper image links file exists (./assets/img_data/2509.24900.json), skip HTML parsing.
[01.10.2025 00:57] Success.
[01.10.2025 00:57] Downloading and parsing paper https://huggingface.co/papers/2509.23808.
[01.10.2025 00:57] Extra JSON file exists (./assets/json/2509.23808.json), skip PDF parsing.
[01.10.2025 00:57] Paper image links file exists (./assets/img_data/2509.23808.json), skip HTML parsing.
[01.10.2025 00:57] Success.
[01.10.2025 00:57] Downloading and parsing paper https://huggingface.co/papers/2509.24695.
[01.10.2025 00:57] Extra JSON file exists (./assets/json/2509.24695.json), skip PDF parsing.
[01.10.2025 00:57] Paper image links file exists (./assets/img_data/2509.24695.json), skip HTML parsing.
[01.10.2025 00:57] Success.
[01.10.2025 00:57] Downloading and parsing paper https://huggingface.co/papers/2509.25190.
[01.10.2025 00:57] Extra JSON file exists (./assets/json/2509.25190.json), skip PDF parsing.
[01.10.2025 00:57] Paper image links file exists (./assets/img_data/2509.25190.json), skip HTML parsing.
[01.10.2025 00:57] Success.
[01.10.2025 00:57] Downloading and parsing paper https://huggingface.co/papers/2509.23426.
[01.10.2025 00:57] Extra JSON file exists (./assets/json/2509.23426.json), skip PDF parsing.
[01.10.2025 00:57] Paper image links file exists (./assets/img_data/2509.23426.json), skip HTML parsing.
[01.10.2025 00:57] Success.
[01.10.2025 00:57] Downloading and parsing paper https://huggingface.co/papers/2509.22193.
[01.10.2025 00:57] Extra JSON file exists (./assets/json/2509.22193.json), skip PDF parsing.
[01.10.2025 00:57] Paper image links file exists (./assets/img_data/2509.22193.json), skip HTML parsing.
[01.10.2025 00:57] Success.
[01.10.2025 00:57] Downloading and parsing paper https://huggingface.co/papers/2509.25160.
[01.10.2025 00:57] Extra JSON file exists (./assets/json/2509.25160.json), skip PDF parsing.
[01.10.2025 00:57] Paper image links file exists (./assets/img_data/2509.25160.json), skip HTML parsing.
[01.10.2025 00:57] Success.
[01.10.2025 00:57] Downloading and parsing paper https://huggingface.co/papers/2509.23909.
[01.10.2025 00:57] Extra JSON file exists (./assets/json/2509.23909.json), skip PDF parsing.
[01.10.2025 00:57] Paper image links file exists (./assets/img_data/2509.23909.json), skip HTML parsing.
[01.10.2025 00:57] Success.
[01.10.2025 00:57] Downloading and parsing paper https://huggingface.co/papers/2509.24014.
[01.10.2025 00:57] Extra JSON file exists (./assets/json/2509.24014.json), skip PDF parsing.
[01.10.2025 00:57] Paper image links file exists (./assets/img_data/2509.24014.json), skip HTML parsing.
[01.10.2025 00:57] Success.
[01.10.2025 00:57] Downloading and parsing paper https://huggingface.co/papers/2509.25175.
[01.10.2025 00:57] Extra JSON file exists (./assets/json/2509.25175.json), skip PDF parsing.
[01.10.2025 00:57] Paper image links file exists (./assets/img_data/2509.25175.json), skip HTML parsing.
[01.10.2025 00:57] Success.
[01.10.2025 00:57] Downloading and parsing paper https://huggingface.co/papers/2509.24007.
[01.10.2025 00:57] Extra JSON file exists (./assets/json/2509.24007.json), skip PDF parsing.
[01.10.2025 00:57] Paper image links file exists (./assets/img_data/2509.24007.json), skip HTML parsing.
[01.10.2025 00:57] Success.
[01.10.2025 00:57] Downloading and parsing paper https://huggingface.co/papers/2509.25106.
[01.10.2025 00:57] Extra JSON file exists (./assets/json/2509.25106.json), skip PDF parsing.
[01.10.2025 00:57] Paper image links file exists (./assets/img_data/2509.25106.json), skip HTML parsing.
[01.10.2025 00:57] Success.
[01.10.2025 00:57] Downloading and parsing paper https://huggingface.co/papers/2509.24981.
[01.10.2025 00:57] Extra JSON file exists (./assets/json/2509.24981.json), skip PDF parsing.
[01.10.2025 00:57] Paper image links file exists (./assets/img_data/2509.24981.json), skip HTML parsing.
[01.10.2025 00:57] Success.
[01.10.2025 00:57] Downloading and parsing paper https://huggingface.co/papers/2509.22799.
[01.10.2025 00:57] Extra JSON file exists (./assets/json/2509.22799.json), skip PDF parsing.
[01.10.2025 00:57] Paper image links file exists (./assets/img_data/2509.22799.json), skip HTML parsing.
[01.10.2025 00:57] Success.
[01.10.2025 00:57] Downloading and parsing paper https://huggingface.co/papers/2509.22824.
[01.10.2025 00:57] Extra JSON file exists (./assets/json/2509.22824.json), skip PDF parsing.
[01.10.2025 00:57] Paper image links file exists (./assets/img_data/2509.22824.json), skip HTML parsing.
[01.10.2025 00:57] Success.
[01.10.2025 00:57] Downloading and parsing paper https://huggingface.co/papers/2509.25123.
[01.10.2025 00:57] Extra JSON file exists (./assets/json/2509.25123.json), skip PDF parsing.
[01.10.2025 00:57] Paper image links file exists (./assets/img_data/2509.25123.json), skip HTML parsing.
[01.10.2025 00:57] Success.
[01.10.2025 00:57] Downloading and parsing paper https://huggingface.co/papers/2509.24473.
[01.10.2025 00:57] Extra JSON file exists (./assets/json/2509.24473.json), skip PDF parsing.
[01.10.2025 00:57] Paper image links file exists (./assets/img_data/2509.24473.json), skip HTML parsing.
[01.10.2025 00:57] Success.
[01.10.2025 00:57] Downloading and parsing paper https://huggingface.co/papers/2509.22820.
[01.10.2025 00:57] Extra JSON file exists (./assets/json/2509.22820.json), skip PDF parsing.
[01.10.2025 00:57] Paper image links file exists (./assets/img_data/2509.22820.json), skip HTML parsing.
[01.10.2025 00:57] Success.
[01.10.2025 00:57] Downloading and parsing paper https://huggingface.co/papers/2509.25191.
[01.10.2025 00:57] Extra JSON file exists (./assets/json/2509.25191.json), skip PDF parsing.
[01.10.2025 00:57] Paper image links file exists (./assets/img_data/2509.25191.json), skip HTML parsing.
[01.10.2025 00:57] Success.
[01.10.2025 00:57] Downloading and parsing paper https://huggingface.co/papers/2509.25161.
[01.10.2025 00:57] Extra JSON file exists (./assets/json/2509.25161.json), skip PDF parsing.
[01.10.2025 00:57] Paper image links file exists (./assets/img_data/2509.25161.json), skip HTML parsing.
[01.10.2025 00:57] Success.
[01.10.2025 00:57] Downloading and parsing paper https://huggingface.co/papers/2509.25077.
[01.10.2025 00:57] Extra JSON file exists (./assets/json/2509.25077.json), skip PDF parsing.
[01.10.2025 00:57] Paper image links file exists (./assets/img_data/2509.25077.json), skip HTML parsing.
[01.10.2025 00:57] Success.
[01.10.2025 00:57] Downloading and parsing paper https://huggingface.co/papers/2509.24663.
[01.10.2025 00:57] Extra JSON file exists (./assets/json/2509.24663.json), skip PDF parsing.
[01.10.2025 00:57] Paper image links file exists (./assets/img_data/2509.24663.json), skip HTML parsing.
[01.10.2025 00:57] Success.
[01.10.2025 00:57] Downloading and parsing paper https://huggingface.co/papers/2509.25176.
[01.10.2025 00:57] Extra JSON file exists (./assets/json/2509.25176.json), skip PDF parsing.
[01.10.2025 00:57] Paper image links file exists (./assets/img_data/2509.25176.json), skip HTML parsing.
[01.10.2025 00:57] Success.
[01.10.2025 00:57] Downloading and parsing paper https://huggingface.co/papers/2509.25137.
[01.10.2025 00:57] Extra JSON file exists (./assets/json/2509.25137.json), skip PDF parsing.
[01.10.2025 00:57] Paper image links file exists (./assets/img_data/2509.25137.json), skip HTML parsing.
[01.10.2025 00:57] Success.
[01.10.2025 00:57] Downloading and parsing paper https://huggingface.co/papers/2509.25131.
[01.10.2025 00:57] Extra JSON file exists (./assets/json/2509.25131.json), skip PDF parsing.
[01.10.2025 00:57] Paper image links file exists (./assets/img_data/2509.25131.json), skip HTML parsing.
[01.10.2025 00:57] Success.
[01.10.2025 00:57] Downloading and parsing paper https://huggingface.co/papers/2509.25084.
[01.10.2025 00:57] Extra JSON file exists (./assets/json/2509.25084.json), skip PDF parsing.
[01.10.2025 00:57] Paper image links file exists (./assets/img_data/2509.25084.json), skip HTML parsing.
[01.10.2025 00:57] Success.
[01.10.2025 00:57] Downloading and parsing paper https://huggingface.co/papers/2509.23951.
[01.10.2025 00:57] Extra JSON file exists (./assets/json/2509.23951.json), skip PDF parsing.
[01.10.2025 00:57] Paper image links file exists (./assets/img_data/2509.23951.json), skip HTML parsing.
[01.10.2025 00:57] Success.
[01.10.2025 00:57] Downloading and parsing paper https://huggingface.co/papers/2509.23285.
[01.10.2025 00:57] Extra JSON file exists (./assets/json/2509.23285.json), skip PDF parsing.
[01.10.2025 00:57] Paper image links file exists (./assets/img_data/2509.23285.json), skip HTML parsing.
[01.10.2025 00:57] Success.
[01.10.2025 00:57] Downloading and parsing paper https://huggingface.co/papers/2509.22572.
[01.10.2025 00:57] Extra JSON file exists (./assets/json/2509.22572.json), skip PDF parsing.
[01.10.2025 00:57] Paper image links file exists (./assets/img_data/2509.22572.json), skip HTML parsing.
[01.10.2025 00:57] Success.
[01.10.2025 00:57] Downloading and parsing paper https://huggingface.co/papers/2509.23219.
[01.10.2025 00:57] Extra JSON file exists (./assets/json/2509.23219.json), skip PDF parsing.
[01.10.2025 00:57] Paper image links file exists (./assets/img_data/2509.23219.json), skip HTML parsing.
[01.10.2025 00:57] Success.
[01.10.2025 00:57] Downloading and parsing paper https://huggingface.co/papers/2509.23050.
[01.10.2025 00:57] Extra JSON file exists (./assets/json/2509.23050.json), skip PDF parsing.
[01.10.2025 00:57] Paper image links file exists (./assets/img_data/2509.23050.json), skip HTML parsing.
[01.10.2025 00:57] Success.
[01.10.2025 00:57] Downloading and parsing paper https://huggingface.co/papers/2509.22921.
[01.10.2025 00:57] Extra JSON file exists (./assets/json/2509.22921.json), skip PDF parsing.
[01.10.2025 00:57] Paper image links file exists (./assets/img_data/2509.22921.json), skip HTML parsing.
[01.10.2025 00:57] Success.
[01.10.2025 00:57] Downloading and parsing paper https://huggingface.co/papers/2509.23196.
[01.10.2025 00:57] Extra JSON file exists (./assets/json/2509.23196.json), skip PDF parsing.
[01.10.2025 00:57] Paper image links file exists (./assets/img_data/2509.23196.json), skip HTML parsing.
[01.10.2025 00:57] Success.
[01.10.2025 00:57] Downloading and parsing paper https://huggingface.co/papers/2509.23924.
[01.10.2025 00:57] Extra JSON file exists (./assets/json/2509.23924.json), skip PDF parsing.
[01.10.2025 00:57] Paper image links file exists (./assets/img_data/2509.23924.json), skip HTML parsing.
[01.10.2025 00:57] Success.
[01.10.2025 00:57] Downloading and parsing paper https://huggingface.co/papers/2509.23866.
[01.10.2025 00:57] Extra JSON file exists (./assets/json/2509.23866.json), skip PDF parsing.
[01.10.2025 00:57] Paper image links file exists (./assets/img_data/2509.23866.json), skip HTML parsing.
[01.10.2025 00:57] Success.
[01.10.2025 00:57] Downloading and parsing paper https://huggingface.co/papers/2509.23564.
[01.10.2025 00:57] Extra JSON file exists (./assets/json/2509.23564.json), skip PDF parsing.
[01.10.2025 00:57] Paper image links file exists (./assets/img_data/2509.23564.json), skip HTML parsing.
[01.10.2025 00:57] Success.
[01.10.2025 00:57] Downloading and parsing paper https://huggingface.co/papers/2509.21875.
[01.10.2025 00:57] Extra JSON file exists (./assets/json/2509.21875.json), skip PDF parsing.
[01.10.2025 00:57] Paper image links file exists (./assets/img_data/2509.21875.json), skip HTML parsing.
[01.10.2025 00:57] Success.
[01.10.2025 00:57] Downloading and parsing paper https://huggingface.co/papers/2509.25149.
[01.10.2025 00:57] Extra JSON file exists (./assets/json/2509.25149.json), skip PDF parsing.
[01.10.2025 00:57] Paper image links file exists (./assets/img_data/2509.25149.json), skip HTML parsing.
[01.10.2025 00:57] Success.
[01.10.2025 00:57] Downloading and parsing paper https://huggingface.co/papers/2509.24335.
[01.10.2025 00:57] Extra JSON file exists (./assets/json/2509.24335.json), skip PDF parsing.
[01.10.2025 00:57] Paper image links file exists (./assets/img_data/2509.24335.json), skip HTML parsing.
[01.10.2025 00:57] Success.
[01.10.2025 00:57] Downloading and parsing paper https://huggingface.co/papers/2509.24285.
[01.10.2025 00:57] Extra JSON file exists (./assets/json/2509.24285.json), skip PDF parsing.
[01.10.2025 00:57] Paper image links file exists (./assets/img_data/2509.24285.json), skip HTML parsing.
[01.10.2025 00:57] Success.
[01.10.2025 00:57] Downloading and parsing paper https://huggingface.co/papers/2509.24193.
[01.10.2025 00:57] Extra JSON file exists (./assets/json/2509.24193.json), skip PDF parsing.
[01.10.2025 00:57] Paper image links file exists (./assets/img_data/2509.24193.json), skip HTML parsing.
[01.10.2025 00:57] Success.
[01.10.2025 00:57] Downloading and parsing paper https://huggingface.co/papers/2509.24786.
[01.10.2025 00:57] Extra JSON file exists (./assets/json/2509.24786.json), skip PDF parsing.
[01.10.2025 00:57] Paper image links file exists (./assets/img_data/2509.24786.json), skip HTML parsing.
[01.10.2025 00:57] Success.
[01.10.2025 00:57] Downloading and parsing paper https://huggingface.co/papers/2509.23371.
[01.10.2025 00:57] Extra JSON file exists (./assets/json/2509.23371.json), skip PDF parsing.
[01.10.2025 00:57] Paper image links file exists (./assets/img_data/2509.23371.json), skip HTML parsing.
[01.10.2025 00:57] Success.
[01.10.2025 00:57] Downloading and parsing paper https://huggingface.co/papers/2509.21953.
[01.10.2025 00:57] Extra JSON file exists (./assets/json/2509.21953.json), skip PDF parsing.
[01.10.2025 00:57] Paper image links file exists (./assets/img_data/2509.21953.json), skip HTML parsing.
[01.10.2025 00:57] Success.
[01.10.2025 00:57] Downloading and parsing paper https://huggingface.co/papers/2509.25185.
[01.10.2025 00:57] Extra JSON file exists (./assets/json/2509.25185.json), skip PDF parsing.
[01.10.2025 00:57] Paper image links file exists (./assets/img_data/2509.25185.json), skip HTML parsing.
[01.10.2025 00:57] Success.
[01.10.2025 00:57] Downloading and parsing paper https://huggingface.co/papers/2509.25050.
[01.10.2025 00:57] Extra JSON file exists (./assets/json/2509.25050.json), skip PDF parsing.
[01.10.2025 00:57] Paper image links file exists (./assets/img_data/2509.25050.json), skip HTML parsing.
[01.10.2025 00:57] Success.
[01.10.2025 00:57] Downloading and parsing paper https://huggingface.co/papers/2509.24494.
[01.10.2025 00:57] Extra JSON file exists (./assets/json/2509.24494.json), skip PDF parsing.
[01.10.2025 00:57] Paper image links file exists (./assets/img_data/2509.24494.json), skip HTML parsing.
[01.10.2025 00:57] Success.
[01.10.2025 00:57] Downloading and parsing paper https://huggingface.co/papers/2509.23061.
[01.10.2025 00:57] Extra JSON file exists (./assets/json/2509.23061.json), skip PDF parsing.
[01.10.2025 00:57] Paper image links file exists (./assets/img_data/2509.23061.json), skip HTML parsing.
[01.10.2025 00:57] Success.
[01.10.2025 00:57] Downloading and parsing paper https://huggingface.co/papers/2509.24372.
[01.10.2025 00:57] Extra JSON file exists (./assets/json/2509.24372.json), skip PDF parsing.
[01.10.2025 00:57] Paper image links file exists (./assets/img_data/2509.24372.json), skip HTML parsing.
[01.10.2025 00:57] Success.
[01.10.2025 00:57] Downloading and parsing paper https://huggingface.co/papers/2509.24269.
[01.10.2025 00:57] Extra JSON file exists (./assets/json/2509.24269.json), skip PDF parsing.
[01.10.2025 00:57] Paper image links file exists (./assets/img_data/2509.24269.json), skip HTML parsing.
[01.10.2025 00:57] Success.
[01.10.2025 00:57] Downloading and parsing paper https://huggingface.co/papers/2509.24200.
[01.10.2025 00:57] Extra JSON file exists (./assets/json/2509.24200.json), skip PDF parsing.
[01.10.2025 00:57] Paper image links file exists (./assets/img_data/2509.24200.json), skip HTML parsing.
[01.10.2025 00:57] Success.
[01.10.2025 00:57] Downloading and parsing paper https://huggingface.co/papers/2509.23338.
[01.10.2025 00:57] Extra JSON file exists (./assets/json/2509.23338.json), skip PDF parsing.
[01.10.2025 00:57] Paper image links file exists (./assets/img_data/2509.23338.json), skip HTML parsing.
[01.10.2025 00:57] Success.
[01.10.2025 00:57] Downloading and parsing paper https://huggingface.co/papers/2509.23143.
[01.10.2025 00:57] Extra JSON file exists (./assets/json/2509.23143.json), skip PDF parsing.
[01.10.2025 00:57] Paper image links file exists (./assets/img_data/2509.23143.json), skip HTML parsing.
[01.10.2025 00:57] Success.
[01.10.2025 00:57] Downloading and parsing paper https://huggingface.co/papers/2509.22830.
[01.10.2025 00:57] Extra JSON file exists (./assets/json/2509.22830.json), skip PDF parsing.
[01.10.2025 00:57] Paper image links file exists (./assets/img_data/2509.22830.json), skip HTML parsing.
[01.10.2025 00:57] Success.
[01.10.2025 00:57] Downloading and parsing paper https://huggingface.co/papers/2509.22570.
[01.10.2025 00:57] Extra JSON file exists (./assets/json/2509.22570.json), skip PDF parsing.
[01.10.2025 00:57] Paper image links file exists (./assets/img_data/2509.22570.json), skip HTML parsing.
[01.10.2025 00:57] Success.
[01.10.2025 00:57] Downloading and parsing paper https://huggingface.co/papers/2509.25052.
[01.10.2025 00:57] Extra JSON file exists (./assets/json/2509.25052.json), skip PDF parsing.
[01.10.2025 00:57] Paper image links file exists (./assets/img_data/2509.25052.json), skip HTML parsing.
[01.10.2025 00:57] Success.
[01.10.2025 00:57] Downloading and parsing paper https://huggingface.co/papers/2509.24910.
[01.10.2025 00:57] Extra JSON file exists (./assets/json/2509.24910.json), skip PDF parsing.
[01.10.2025 00:57] Paper image links file exists (./assets/img_data/2509.24910.json), skip HTML parsing.
[01.10.2025 00:57] Success.
[01.10.2025 00:57] Downloading and parsing paper https://huggingface.co/papers/2509.24317.
[01.10.2025 00:57] Extra JSON file exists (./assets/json/2509.24317.json), skip PDF parsing.
[01.10.2025 00:57] Paper image links file exists (./assets/img_data/2509.24317.json), skip HTML parsing.
[01.10.2025 00:57] Success.
[01.10.2025 00:57] Downloading and parsing paper https://huggingface.co/papers/2509.24148.
[01.10.2025 00:57] Extra JSON file exists (./assets/json/2509.24148.json), skip PDF parsing.
[01.10.2025 00:57] Paper image links file exists (./assets/img_data/2509.24148.json), skip HTML parsing.
[01.10.2025 00:57] Success.
[01.10.2025 00:57] Downloading and parsing paper https://huggingface.co/papers/2509.23115.
[01.10.2025 00:57] Extra JSON file exists (./assets/json/2509.23115.json), skip PDF parsing.
[01.10.2025 00:57] Paper image links file exists (./assets/img_data/2509.23115.json), skip HTML parsing.
[01.10.2025 00:57] Success.
[01.10.2025 00:57] Downloading and parsing paper https://huggingface.co/papers/2509.22518.
[01.10.2025 00:57] Extra JSON file exists (./assets/json/2509.22518.json), skip PDF parsing.
[01.10.2025 00:57] Paper image links file exists (./assets/img_data/2509.22518.json), skip HTML parsing.
[01.10.2025 00:57] Success.
[01.10.2025 00:57] Downloading and parsing paper https://huggingface.co/papers/2509.24935.
[01.10.2025 00:57] Downloading paper 2509.24935 from http://arxiv.org/pdf/2509.24935v1...
[01.10.2025 00:57] Extracting affiliations from text.
[01.10.2025 00:57] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 9 2 ] . [ 1 5 3 9 4 2 . 9 0 5 2 : r a Sangeek Hyun, MinKyu Lee, Jae-Pil Heo Sungkyunkwan University {hse1032, 2minkyulee, jaepilheo}@gmail.com "
[01.10.2025 00:57] Response: ```python
["Sungkyunkwan University"]
```
[01.10.2025 00:57] Deleting PDF ./assets/pdf/2509.24935.pdf.
[01.10.2025 00:57] Success.
[01.10.2025 00:57] Downloading and parsing paper https://huggingface.co/papers/2509.24908.
[01.10.2025 00:57] Extra JSON file exists (./assets/json/2509.24908.json), skip PDF parsing.
[01.10.2025 00:57] Paper image links file exists (./assets/img_data/2509.24908.json), skip HTML parsing.
[01.10.2025 00:57] Success.
[01.10.2025 00:57] Downloading and parsing paper https://huggingface.co/papers/2509.24878.
[01.10.2025 00:57] Extra JSON file exists (./assets/json/2509.24878.json), skip PDF parsing.
[01.10.2025 00:57] Paper image links file exists (./assets/img_data/2509.24878.json), skip HTML parsing.
[01.10.2025 00:57] Success.
[01.10.2025 00:57] Downloading and parsing paper https://huggingface.co/papers/2509.24726.
[01.10.2025 00:57] Extra JSON file exists (./assets/json/2509.24726.json), skip PDF parsing.
[01.10.2025 00:57] Paper image links file exists (./assets/img_data/2509.24726.json), skip HTML parsing.
[01.10.2025 00:57] Success.
[01.10.2025 00:57] Downloading and parsing paper https://huggingface.co/papers/2509.24709.
[01.10.2025 00:57] Extra JSON file exists (./assets/json/2509.24709.json), skip PDF parsing.
[01.10.2025 00:57] Paper image links file exists (./assets/img_data/2509.24709.json), skip HTML parsing.
[01.10.2025 00:57] Success.
[01.10.2025 00:57] Downloading and parsing paper https://huggingface.co/papers/2509.24592.
[01.10.2025 00:57] Extra JSON file exists (./assets/json/2509.24592.json), skip PDF parsing.
[01.10.2025 00:57] Paper image links file exists (./assets/img_data/2509.24592.json), skip HTML parsing.
[01.10.2025 00:57] Success.
[01.10.2025 00:57] Downloading and parsing paper https://huggingface.co/papers/2509.23233.
[01.10.2025 00:57] Extra JSON file exists (./assets/json/2509.23233.json), skip PDF parsing.
[01.10.2025 00:57] Paper image links file exists (./assets/img_data/2509.23233.json), skip HTML parsing.
[01.10.2025 00:57] Success.
[01.10.2025 00:57] Downloading and parsing paper https://huggingface.co/papers/2509.21043.
[01.10.2025 00:57] Extra JSON file exists (./assets/json/2509.21043.json), skip PDF parsing.
[01.10.2025 00:57] Paper image links file exists (./assets/img_data/2509.21043.json), skip HTML parsing.
[01.10.2025 00:57] Success.
[01.10.2025 00:57] Downloading and parsing paper https://huggingface.co/papers/2509.18582.
[01.10.2025 00:57] Extra JSON file exists (./assets/json/2509.18582.json), skip PDF parsing.
[01.10.2025 00:57] Paper image links file exists (./assets/img_data/2509.18582.json), skip HTML parsing.
[01.10.2025 00:57] Success.
[01.10.2025 00:57] Downloading and parsing paper https://huggingface.co/papers/2509.25413.
[01.10.2025 00:57] Downloading paper 2509.25413 from http://arxiv.org/pdf/2509.25413v1...
[01.10.2025 00:58] Extracting affiliations from text.
[01.10.2025 00:58] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 9 2 ] . [ 1 3 1 4 5 2 . 9 0 5 2 : r DepthLM: Metric Depth From Vision Language Models Zhipeng Cai1,, Ching-Feng Yeh1, Hu Xu1, Zhuang Liu2, Gregory P. Meyer1, Xinjie Lei1, Changsheng Zhao1, Shang-Wen Li1, Vikas Chandra1, Yangyang Shi 1Meta, 2Princeton University Project Lead Vision language models (VLMs) can flexibly address various vision tasks through text interactions. Although successful in semantic understanding, state-of-the-art VLMs including GPT-5 still struggle in understanding 3D from 2D inputs. On the other hand, expert pure vision models achieve super-human accuracy in metric depth estimation, key 3D understanding task. However, they require task-specific architectures and losses. Such difference motivates us to ask: Can VLMs reach expert-level accuracy without architecture or loss change? We take per-pixel metric depth estimation as the representative task and show that the answer is yes! Surprisingly, comprehensive analysis shows that text-based supervised-finetuning with sparse labels is sufficient for VLMs to unlock strong 3D understanding, no dense prediction head or complex regression/regularization loss is needed. The bottleneck for VLMs lies actually in pixel reference and cross-dataset camera ambiguity, which we address through visual prompting and intrinsic-conditioned augmentation. With much smaller models, our method DepthLM surpasses the accuracy of most advanced VLMs by over 2x, making VLMs for the first time comparable with pure vision models. Interestingly, without explicit enforcement during training, VLMs trained with DepthLM naturally avoids over-smoothing, having much fewer flying points at boundary regions than pure vision models. The simplicity of DepthLM also enables single VLM to cover various 3D tasks beyond metric depth. Our code and model will be released at the link below. Date: October 1, 2025 Correspondence: Zhipeng Cai: czptc2h@gmail.com Code: https://github.com/facebookresearch/DepthLM Huggingface Model: https://hug"
[01.10.2025 00:58] Response: ```python
["Meta", "Princeton University"]
```
[01.10.2025 00:58] Deleting PDF ./assets/pdf/2509.25413.pdf.
[01.10.2025 00:58] Success.
[01.10.2025 00:58] Downloading and parsing paper https://huggingface.co/papers/2509.25171.
[01.10.2025 00:58] Extra JSON file exists (./assets/json/2509.25171.json), skip PDF parsing.
[01.10.2025 00:58] Paper image links file exists (./assets/img_data/2509.25171.json), skip HTML parsing.
[01.10.2025 00:58] Success.
[01.10.2025 00:58] Downloading and parsing paper https://huggingface.co/papers/2509.24988.
[01.10.2025 00:58] Extra JSON file exists (./assets/json/2509.24988.json), skip PDF parsing.
[01.10.2025 00:58] Paper image links file exists (./assets/img_data/2509.24988.json), skip HTML parsing.
[01.10.2025 00:58] Success.
[01.10.2025 00:58] Downloading and parsing paper https://huggingface.co/papers/2509.22991.
[01.10.2025 00:58] Extra JSON file exists (./assets/json/2509.22991.json), skip PDF parsing.
[01.10.2025 00:58] Paper image links file exists (./assets/img_data/2509.22991.json), skip HTML parsing.
[01.10.2025 00:58] Success.
[01.10.2025 00:58] Downloading and parsing paper https://huggingface.co/papers/2509.19033.
[01.10.2025 00:58] Extra JSON file exists (./assets/json/2509.19033.json), skip PDF parsing.
[01.10.2025 00:58] Paper image links file exists (./assets/img_data/2509.19033.json), skip HTML parsing.
[01.10.2025 00:58] Success.
[01.10.2025 00:58] Downloading and parsing paper https://huggingface.co/papers/2509.16538.
[01.10.2025 00:58] Extra JSON file exists (./assets/json/2509.16538.json), skip PDF parsing.
[01.10.2025 00:58] Paper image links file exists (./assets/img_data/2509.16538.json), skip HTML parsing.
[01.10.2025 00:58] Success.
[01.10.2025 00:58] Enriching papers with extra data.
[01.10.2025 00:58] ********************************************************************************
[01.10.2025 00:58] Abstract 0. SLA, a trainable attention method combining sparse and linear attention, accelerates Diffusion Transformer models for video generation with minimal quality loss.  					AI-generated summary 				 In Diffusion Transformer (DiT) models, particularly for video generation, attention latency is a major bot...
[01.10.2025 00:58] ********************************************************************************
[01.10.2025 00:58] Abstract 1. StableToken, a multi-branch consensus-driven tokenizer, enhances token stability and robustness in speech processing, improving SpeechLLMs' performance under noisy conditions.  					AI-generated summary 				 Prevalent semantic speech tokenizers, designed to capture linguistic content, are surprising...
[01.10.2025 00:58] ********************************************************************************
[01.10.2025 00:58] Abstract 2. Multiplayer Nash Preference Optimization (MNPO) extends Nash learning from human feedback to handle complex, non-transitive human preferences by formulating alignment as an n-player game.  					AI-generated summary 				 Reinforcement learning from human feedback (RLHF) has emerged as the standard pa...
[01.10.2025 00:58] ********************************************************************************
[01.10.2025 00:58] Abstract 3. RealUnify evaluates the bidirectional synergy between understanding and generation in unified multimodal models, revealing that current models lack effective integration despite architectural unification.  					AI-generated summary 				 The integration of visual understanding and generation into uni...
[01.10.2025 00:58] ********************************************************************************
[01.10.2025 00:58] Abstract 4. OpenGPT-4o-Image, a large-scale dataset with hierarchical task taxonomy and automated generation, significantly improves performance in image generation and editing tasks.  					AI-generated summary 				 The performance of unified multimodal models for image generation and editing is fundamentally c...
[01.10.2025 00:58] ********************************************************************************
[01.10.2025 00:58] Abstract 5. Re-examining the exploration-exploitation trade-off in Reinforcement Learning for Verifiable Rewards through hidden-state analysis reveals opportunities for simultaneous enhancement using Effective Rank and its derivatives, leading to improved performance in diverse benchmarks.  					AI-generated su...
[01.10.2025 00:58] ********************************************************************************
[01.10.2025 00:58] Abstract 6. SANA-Video, a small diffusion model, efficiently generates high-resolution, high-quality videos with strong text-video alignment using linear attention and a constant-memory KV cache, achieving competitive performance at a lower cost and faster speed.  					AI-generated summary 				 We introduce SAN...
[01.10.2025 00:58] ********************************************************************************
[01.10.2025 00:58] Abstract 7. Visual Jigsaw, a self-supervised reinforcement learning framework, enhances multimodal large language models' visual understanding through a permutation task without additional annotations or generative components.  					AI-generated summary 				 Reinforcement learning based post-training has recent...
[01.10.2025 00:58] ********************************************************************************
[01.10.2025 00:58] Abstract 8. ToolUniverse is an ecosystem that standardizes and integrates tools, models, and data for AI scientists, enabling automated refinement, creation, and composition of workflows.  					AI-generated summary 				 AI scientists are emerging computational systems that serve as collaborative partners in dis...
[01.10.2025 00:58] ********************************************************************************
[01.10.2025 00:58] Abstract 9. Reasoning models enhance performance across various tasks, surpassing instruction fine-tuned models in reasoning-intensive and open-ended tasks, despite higher computational costs.  					AI-generated summary 				 Large Language Models (LLMs) with reasoning capabilities have achieved state-of-the-art...
[01.10.2025 00:58] ********************************************************************************
[01.10.2025 00:58] Abstract 10. GSM8K-V is a new visual multi-image mathematical reasoning benchmark that highlights the limitations of current vision language models in handling visual mathematical problems.  					AI-generated summary 				 Vision language models (VLMs) achieve unified modeling of images and text, enabling them to...
[01.10.2025 00:58] ********************************************************************************
[01.10.2025 00:58] Abstract 11. A specialized reward model, EditScore, enables effective reinforcement learning for instruction-guided image editing by providing a high-fidelity reward signal.  					AI-generated summary 				 Instruction-guided image editing has achieved remarkable progress, yet current models still face challenges...
[01.10.2025 00:58] ********************************************************************************
[01.10.2025 00:58] Abstract 12. SparseD is a novel sparse attention method for diffusion language models that addresses the high inference latency by pre-computing head-specific sparse patterns and switching to sparse attention in later denoising steps.  					AI-generated summary 				 While diffusion language models (DLMs) offer a...
[01.10.2025 00:58] ********************************************************************************
[01.10.2025 00:58] Abstract 13. EasySteer is a unified framework for efficient and extensible steering of large language models, offering significant speedups and improved functionality over existing methods.  					AI-generated summary 				 Large language model (LLM) steering has emerged as a promising paradigm for controlling mod...
[01.10.2025 00:58] ********************************************************************************
[01.10.2025 00:58] Abstract 14. Sequential Diffusion Language Model (SDLM) enhances pre-trained autoregressive language models by adaptively determining generation length and maintaining KV-cache compatibility, achieving high efficiency and throughput.  					AI-generated summary 				 Diffusion language models (DLMs) have strong th...
[01.10.2025 00:58] ********************************************************************************
[01.10.2025 00:58] Abstract 15. A new benchmark, Personalized Deep Research Bench, evaluates the personalization capabilities of Deep Research Agents across diverse tasks and user profiles using the PQR Evaluation Framework.  					AI-generated summary 				 Deep Research Agents (DRAs) can autonomously conduct complex investigations...
[01.10.2025 00:58] ********************************************************************************
[01.10.2025 00:58] Abstract 16. ROVER, a minimalist RL method, achieves superior performance and diversity in LLM math reasoning by leveraging Q-values from a fixed random policy, bypassing complex policy iteration.  					AI-generated summary 				 RL with Verifiable Rewards (RLVR) has emerged as a promising paradigm for improving ...
[01.10.2025 00:58] ********************************************************************************
[01.10.2025 00:58] Abstract 17. VideoScore2 is a multi-dimensional, interpretable framework for evaluating text-to-video generation, assessing visual quality, alignment, and consistency with detailed rationales.  					AI-generated summary 				 Recent advances in text-to-video generation have produced increasingly realistic and div...
[01.10.2025 00:58] ********************************************************************************
[01.10.2025 00:58] Abstract 18. Critique Reinforcement Learning (CRL) enhances LLMs by teaching them to generate critiques, leading to improved performance on code generation and logic reasoning tasks compared to standard RL.  					AI-generated summary 				 Reinforcement Learning (RL) has emerged as a popular training paradigm, pa...
[01.10.2025 00:58] ********************************************************************************
[01.10.2025 00:58] Abstract 19. Reinforcement learning enables large language models to acquire new compositional skills by combining existing ones, which transfer to different tasks and improve reasoning behaviors.  					AI-generated summary 				 Does RL teach LLMs genuinely new skills, or does it merely activate existing ones? T...
[01.10.2025 00:58] ********************************************************************************
[01.10.2025 00:58] Abstract 20. Geometry-centric fine-tuning using the Euclid30K dataset significantly improves spatial reasoning abilities in multimodal large language models across multiple benchmarks.  					AI-generated summary 				 Spatial intelligence spans a rich suite of abilities, including visualising and transforming sha...
[01.10.2025 00:58] ********************************************************************************
[01.10.2025 00:58] Abstract 21. MMPB is a benchmark for evaluating the personalization capabilities of Vision-Language Models across various tasks and concepts, revealing significant challenges in maintaining consistency and adapting to user preferences.  					AI-generated summary 				 Visual personalization is essential in user-f...
[01.10.2025 00:58] ********************************************************************************
[01.10.2025 00:58] Abstract 22. VGGT-X addresses VRAM and output quality issues in scaling 3D Foundation Models for dense Novel View Synthesis without relying on COLMAP.  					AI-generated summary 				 We study the problem of applying 3D Foundation Models (3DFMs) to dense Novel View Synthesis (NVS). Despite significant progress in...
[01.10.2025 00:58] ********************************************************************************
[01.10.2025 00:58] Abstract 23. Rolling Forcing is a novel video generation technique that reduces error accumulation in long video streams by using joint denoising, attention sink mechanism, and efficient training with non-overlapping windows.  					AI-generated summary 				 Streaming video generation, as one fundamental componen...
[01.10.2025 00:58] ********************************************************************************
[01.10.2025 00:58] Abstract 24. BRIDGE uses RL-optimized depth-to-image generation to create a large, diverse dataset, enhancing monocular depth estimation robustness and performance.  					AI-generated summary 				 Monocular Depth Estimation (MDE) is a foundational task for computer vision. Traditional methods are limited by data...
[01.10.2025 00:58] ********************************************************************************
[01.10.2025 00:58] Abstract 25. A dense-sparse switchable attention framework, InfLLM-V2, enhances long-sequence processing in large language models by efficiently adapting between dense and sparse attention mechanisms.  					AI-generated summary 				 Long-sequence processing is a critical capability for modern large language mode...
[01.10.2025 00:58] ********************************************************************************
[01.10.2025 00:58] Abstract 26. SIRI, a reinforcement learning approach with interleaved compression and expansion, enhances the efficiency and accuracy of large reasoning models by dynamically adjusting the reasoning budget.  					AI-generated summary 				 We introduce SIRI, Scaling Iterative Reinforcement Learning with Interleav...
[01.10.2025 00:58] ********************************************************************************
[01.10.2025 00:58] Abstract 27. Reinforcement Learning from Human Interaction (RLHI) uses in-the-wild user conversations to improve conversational models, enhancing personalization and instruction-following through user-guided rewrites and persona-conditioned rewards.  					AI-generated summary 				 We posit that to achieve contin...
[01.10.2025 00:58] ********************************************************************************
[01.10.2025 00:58] Abstract 28. MGM-Omni is a unified multimodal language model for speech generation and understanding, featuring a dual-track architecture for efficient cross-modal interaction and data-efficient training.  					AI-generated summary 				 We present MGM-Omni, a unified Omni LLM for omni-modal understanding and exp...
[01.10.2025 00:58] ********************************************************************************
[01.10.2025 00:58] Abstract 29. DataMind addresses challenges in building open-source data-analytic agents through task taxonomy, trajectory sampling, dynamic training objectives, and stable multi-turn rollouts, achieving state-of-the-art performance on data analysis benchmarks.  					AI-generated summary 				 Data-analytic agents...
[01.10.2025 00:58] ********************************************************************************
[01.10.2025 00:58] Abstract 30. HunyuanImage 3.0, a multimodal model with an autoregressive framework, achieves state-of-the-art performance in image generation and text-image alignment using a Mixture-of-Experts architecture with over 80 billion parameters.  					AI-generated summary 				 We present HunyuanImage 3.0, a native mul...
[01.10.2025 00:58] ********************************************************************************
[01.10.2025 00:58] Abstract 31. Tool-Light framework improves large language models' tool-integrated reasoning efficiency and accuracy by leveraging information entropy and a two-stage fine-tuning process.  					AI-generated summary 				 Tool-Integrated Reasoning (TIR) enables large language models (LLMs) to improve their internal...
[01.10.2025 00:58] ********************************************************************************
[01.10.2025 00:58] Abstract 32. Dynamic Experts Search (DES) enhances large language models by controlling expert activation during inference, improving accuracy and stability without additional cost.  					AI-generated summary 				 Test-Time Scaling (TTS) enhances the reasoning ability of large language models (LLMs) by allocatin...
[01.10.2025 00:58] ********************************************************************************
[01.10.2025 00:58] Abstract 33. WirelessMathLM, a compact model trained with domain-specific reinforcement learning, achieves high accuracy on wireless mathematics problems and transfers well to general mathematics benchmarks.  					AI-generated summary 				 Large language models (LLMs) excel at general mathematical reasoning but ...
[01.10.2025 00:58] ********************************************************************************
[01.10.2025 00:58] Abstract 34. A systematic analysis of language prior in large vision-language models reveals a Visual Integration Point and introduces a Total Visual Integration estimator to quantify visual influence on response generation.  					AI-generated summary 				 Large vision-language models (LVLMs) achieve strong perf...
[01.10.2025 00:58] ********************************************************************************
[01.10.2025 00:58] Abstract 35. A novel constrained reinforcement learning framework for LLM distillation maximizes task-specific rewards while maintaining constraint satisfaction without state augmentation or dual Lagrangian methods.  					AI-generated summary 				 We introduce a novel approach to large language model (LLM) disti...
[01.10.2025 00:58] ********************************************************************************
[01.10.2025 00:58] Abstract 36. Insight-to-Solve (I2S) and its refined version (I2S+) improve few-shot chain-of-thought performance by converting demonstrations into reusable insights, outperforming direct answering and scaling methods across various models.  					AI-generated summary 				 Recent reasoning LLMs (RLMs), especially ...
[01.10.2025 00:58] ********************************************************************************
[01.10.2025 00:58] Abstract 37. Proposed decoding strategies and reinforcement learning algorithms improve the performance and efficiency of masked diffusion language models during inference.  					AI-generated summary 				 Masked diffusion language models (MDLMs) have recently emerged as a promising alternative to autoregressive ...
[01.10.2025 00:58] ********************************************************************************
[01.10.2025 00:58] Abstract 38. DART, a decoupled reinforcement learning framework for GUI agents, improves efficiency and learning effectiveness through asynchronous modules and adaptive data curation, achieving high task success rates on the OSWorld benchmark.  					AI-generated summary 				 Vision-language model (VLM) based GUI...
[01.10.2025 00:58] ********************************************************************************
[01.10.2025 00:58] Abstract 39. PrefCleanBench evaluates 13 preference data cleaning methods for aligning large language models with human preferences, providing a standardized protocol to assess their effectiveness and generalizability.  					AI-generated summary 				 Human feedback plays a pivotal role in aligning large language...
[01.10.2025 00:58] ********************************************************************************
[01.10.2025 00:58] Abstract 40. LUMINA detects hallucinations in RAG systems by quantifying external context utilization and internal knowledge utilization, outperforming existing methods on benchmarks.  					AI-generated summary 				 Retrieval-Augmented Generation (RAG) aims to mitigate hallucinations in large language models (LL...
[01.10.2025 00:58] ********************************************************************************
[01.10.2025 00:58] Abstract 41. A novel training approach using NVFP4 format with Random Hadamard transforms, two-dimensional quantization, stochastic rounding, and selective high-precision layers enables stable and accurate training of large language models in 4-bit precision.  					AI-generated summary 				 Large Language Models...
[01.10.2025 00:58] ********************************************************************************
[01.10.2025 00:58] Abstract 42. SphereAR, an autoregressive model with hyperspherical constraints, achieves state-of-the-art performance in image generation, surpassing diffusion and masked-generation models at similar parameter scales.  					AI-generated summary 				 Autoregressive (AR) models are promising for image generation, ...
[01.10.2025 00:58] ********************************************************************************
[01.10.2025 00:58] Abstract 43. A framework combining SCI-VerifyBench and SCI-Verifier addresses challenges in verifying LLM-generated scientific answers through cross-disciplinary benchmarks and reasoning-augmented verification.  					AI-generated summary 				 As large language models (LLMs) are increasingly applied to scientific...
[01.10.2025 00:58] ********************************************************************************
[01.10.2025 00:58] Abstract 44. AceSearcher, a cooperative self-play framework, enhances a large language model's reasoning ability by alternating between decomposing queries and solving them, outperforming state-of-the-art models with fewer parameters.  					AI-generated summary 				 Search-augmented LLMs often struggle with comp...
[01.10.2025 00:58] ********************************************************************************
[01.10.2025 00:58] Abstract 45. LOVE-R1, a model with adaptive frame sampling, enhances long video understanding by balancing temporal and spatial details through multi-step reasoning and decoupled reinforcement learning.  					AI-generated summary 				 Long video understanding is still challenging for recent Large Video-Language ...
[01.10.2025 00:58] ********************************************************************************
[01.10.2025 00:58] Abstract 46. Meta-Weighted Adaptive Preference Optimization (MetaAPO) dynamically balances online and offline data to align large language models with human preferences, outperforming existing methods and reducing annotation costs.  					AI-generated summary 				 Preference optimization is crucial for aligning l...
[01.10.2025 00:58] ********************************************************************************
[01.10.2025 00:58] Abstract 47. MultiCrafter framework improves multi-subject image generation by addressing attribute leakage through explicit positional supervision, utilizing a Mixture-of-Experts architecture, and aligning with human preferences via online reinforcement learning.  					AI-generated summary 				 Multi-subject im...
[01.10.2025 00:58] ********************************************************************************
[01.10.2025 00:58] Abstract 48. PixelCraft, a multi-agent system, enhances visual reasoning in multimodal large language models by integrating high-fidelity image processing and flexible reasoning through a dynamic workflow and image memory.  					AI-generated summary 				 Structured images (e.g., charts and geometric diagrams) re...
[01.10.2025 00:58] ********************************************************************************
[01.10.2025 00:58] Abstract 49. Advantage Weighted Matching (AWM) is a policy-gradient method for diffusion models that reduces variance and speeds up convergence compared to Denoising Diffusion Policy Optimization (DDPO).  					AI-generated summary 				 Reinforcement Learning (RL) has emerged as a central paradigm for advancing L...
[01.10.2025 00:58] ********************************************************************************
[01.10.2025 00:58] Abstract 50. GRPO-MA improves the training of Chain-of-Thought reasoning in LLMs and VLMs by addressing gradient coupling, sparse rewards, and unstable advantage estimation through multi-answer generation.  					AI-generated summary 				 Recent progress, such as DeepSeek-R1, has shown that the GRPO algorithm, a ...
[01.10.2025 00:58] ********************************************************************************
[01.10.2025 00:58] Abstract 51. DafnyCOMP evaluates large language models on generating compositional specifications in Dafny, highlighting their weaknesses in cross-functional reasoning.  					AI-generated summary 				 We introduce DafnyCOMP, a benchmark for evaluating large language models (LLMs) on compositional specification g...
[01.10.2025 00:58] ********************************************************************************
[01.10.2025 00:58] Abstract 52. Evolution strategies successfully scale to fine-tune large language models, outperforming reinforcement learning in sample efficiency and robustness.  					AI-generated summary 				 Fine-tuning pre-trained large language models (LLMs) for down-stream tasks is a critical step in the AI deployment pip...
[01.10.2025 00:58] ********************************************************************************
[01.10.2025 00:58] Abstract 53. AdvChain enhances the safety and reliability of large reasoning models by teaching them dynamic self-correction through adversarial chain-of-thought tuning.  					AI-generated summary 				 Large Reasoning Models (LRMs) have demonstrated remarkable capabilities in complex problem-solving through Chai...
[01.10.2025 00:58] ********************************************************************************
[01.10.2025 00:58] Abstract 54. UniVid combines video understanding and generation using an MLLM with a diffusion decoder, achieving state-of-the-art performance through Temperature Modality Alignment and Pyramid Reflection.  					AI-generated summary 				 Unified video modeling that combines generation and understanding capabilit...
[01.10.2025 00:58] ********************************************************************************
[01.10.2025 00:58] Abstract 55. PARROT is a benchmark for evaluating Cross-System SQL Translation across multiple database systems, addressing limitations in existing SQL benchmarks.  					AI-generated summary 				 Large language models (LLMS) have shown increasing effectiveness in Text-to-SQL tasks. However, another closely relat...
[01.10.2025 00:58] ********************************************************************************
[01.10.2025 00:58] Abstract 56. MathBode provides a diagnostic for mathematical reasoning in LLMs by analyzing frequency-resolved metrics of model outputs compared to exact solutions, revealing systematic low-pass behavior and phase lag.  					AI-generated summary 				 This paper presents MathBode, a dynamic diagnostic for mathema...
[01.10.2025 00:58] ********************************************************************************
[01.10.2025 00:58] Abstract 57. ChatInject, a novel attack exploiting structured chat templates and persuasive multi-turn dialogues, significantly enhances attack success rates on large language model-based agents compared to traditional methods.  					AI-generated summary 				 The growing deployment of large language model (LLM) ...
[01.10.2025 00:58] ********************************************************************************
[01.10.2025 00:58] Abstract 58. UniMIC, a unified token-based framework, enhances multimodal communication by using compact tokenized representations and lightweight Transformer-based entropy models, achieving significant bitrate savings without compromising performance.  					AI-generated summary 				 The rapid progress of Large ...
[01.10.2025 00:58] ********************************************************************************
[01.10.2025 00:58] Abstract 59. CEL, a novel agent architecture using a Large Language Model, learns to master complex environments through explicit reasoning and planning, achieving success in diverse grid-world tasks with sparse rewards.  					AI-generated summary 				 The pursuit of artificial agents that can learn to master co...
[01.10.2025 00:58] ********************************************************************************
[01.10.2025 00:58] Abstract 60. SID, a self-improving demonstration approach, enhances exploration and generalization in goal-oriented language-guided navigation tasks, achieving state-of-the-art performance.  					AI-generated summary 				 Goal-oriented language-guided navigation requires robust exploration capabilities for agent...
[01.10.2025 00:58] ********************************************************************************
[01.10.2025 00:58] Abstract 61. SALT, a two-stage training method using a frozen teacher, achieves better video representation learning with higher efficiency and scalability compared to EMA-based approaches.  					AI-generated summary 				 Video Joint Embedding Predictive Architectures (V-JEPA) learn generalizable off-the-shelf v...
[01.10.2025 00:58] ********************************************************************************
[01.10.2025 00:58] Abstract 62. TENET, an LLM agent for TDD in complex repositories, uses a test harness, efficient code retrieval, and iterative refinement to improve code generation accuracy and performance.  					AI-generated summary 				 Test-Driven Development (TDD) is a widely adopted software engineering practice that requi...
[01.10.2025 00:58] ********************************************************************************
[01.10.2025 00:58] Abstract 63. RHYTHM uses hierarchical temporal tokenization and large language models to predict human mobility, capturing long-range dependencies and multi-scale periodic behaviors efficiently.  					AI-generated summary 				 Predicting human mobility is inherently challenging due to complex long-range dependen...
[01.10.2025 00:58] ********************************************************************************
[01.10.2025 00:58] Abstract 64. The Reasoning Manifold framework quantifies and localizes reasoning failures in Large Language Models by analyzing geometric deviations in internal representations.  					AI-generated summary 				 Understanding how Large Language Models (LLMs) perform complex reasoning and their failure mechanisms i...
[01.10.2025 00:58] ********************************************************************************
[01.10.2025 00:58] Abstract 65. Transformers-based GANs trained in latent space achieve state-of-the-art performance with efficient scaling and reliable training across various capacities.  					AI-generated summary 				 Scalability has driven recent advances in generative modeling, yet its principles remain underexplored for adve...
[01.10.2025 00:58] ********************************************************************************
[01.10.2025 00:58] Abstract 66. BOE-XSUM, a dataset of Spanish legal document summaries, demonstrates that fine-tuned medium-sized LLMs outperform general-purpose models in zero-shot summarization tasks.  					AI-generated summary 				 The ability to summarize long documents succinctly is increasingly important in daily life due t...
[01.10.2025 00:58] ********************************************************************************
[01.10.2025 00:58] Abstract 67. ThermalGen, an adaptive flow-based generative model with RGB image conditioning and style-disentangled mechanism, synthesizes thermal images from RGB datasets, achieving superior performance across various benchmarks.  					AI-generated summary 				 Paired RGB-thermal data is crucial for visual-ther...
[01.10.2025 00:58] ********************************************************************************
[01.10.2025 00:58] Abstract 68. A framework called Socratic-Zero autonomously generates high-quality training data through the co-evolution of three agents, improving reasoning tasks in large language models.  					AI-generated summary 				 Recent breakthroughs in large language models (LLMs) on reasoning tasks rely heavily on mas...
[01.10.2025 00:58] ********************************************************************************
[01.10.2025 00:58] Abstract 69. IWR-Bench evaluates Large Vision-Language Models in reconstructing interactive webpages from video, highlighting challenges in multi-modal reasoning and code generation.  					AI-generated summary 				 The webpage-to-code task requires models to understand visual representations of webpages and gene...
[01.10.2025 00:58] ********************************************************************************
[01.10.2025 00:58] Abstract 70. BPMN Assistant uses Large Language Models to create and edit BPMN diagrams, evaluating process generation and editing performance using JSON and XML representations.  					AI-generated summary 				 This paper presents BPMN Assistant, a tool that leverages Large Language Models (LLMs) for natural lan...
[01.10.2025 00:58] ********************************************************************************
[01.10.2025 00:58] Abstract 71. CLAIRE, an agentic system combining LLM reasoning and retrieval, improves Wikipedia accuracy by detecting inconsistencies, with human editors reporting higher confidence and identifying more issues.  					AI-generated summary 				 Wikipedia is the largest open knowledge corpus, widely used worldwide...
[01.10.2025 00:58] ********************************************************************************
[01.10.2025 00:58] Abstract 72. Research explores creativity in LLMs, revealing scaling behaviors, optimal model dimensions, and a persistent novelty-utility tradeoff affecting their creative potential.  					AI-generated summary 				 Artificial intelligence (AI) systems, and Large Language Models (LLMs) in particular, are increas...
[01.10.2025 00:58] ********************************************************************************
[01.10.2025 00:58] Abstract 73. A novel dataset and model enhance the aesthetic understanding of Multimodal Large Language Models by leveraging professional insights and multi-view vision fusion.  					AI-generated summary 				 While editing directly from life, photographers have found it too difficult to see simultaneously both t...
[01.10.2025 00:58] ********************************************************************************
[01.10.2025 00:58] Abstract 74. Text-based supervised fine-tuning with sparse labels enables vision language models to achieve expert-level accuracy in 3D depth estimation without requiring task-specific architectures or losses.  					AI-generated summary 				 Vision language models (VLMs) can flexibly address various vision tasks...
[01.10.2025 00:58] ********************************************************************************
[01.10.2025 00:58] Abstract 75. A novel framework, TR2-D2, uses tree search to optimize reward-guided discrete diffusion trajectories for fine-tuning biological sequence models.  					AI-generated summary 				 Reinforcement learning with stochastic optimal control offers a promising framework for diffusion fine-tuning, where a pre...
[01.10.2025 00:58] ********************************************************************************
[01.10.2025 00:58] Abstract 76. Generalized Correctness Models (GCMs) improve LLM confidence estimation by leveraging historical correctness data, outperforming self-knowledge approaches and demonstrating generalizability across models and datasets.  					AI-generated summary 				 Generating accurate and calibrated confidence esti...
[01.10.2025 00:58] ********************************************************************************
[01.10.2025 00:58] Abstract 77. ADAM provides a framework and benchmark for evaluating and improving multimodal large language models in biographical reasoning, using a diverse dataset and retrieval-augmented generation to enhance accuracy and reduce hallucinations.  					AI-generated summary 				 We introduce ADAM (A Diverse Arch...
[01.10.2025 00:58] ********************************************************************************
[01.10.2025 00:58] Abstract 78. The study analyzes research trends in Italian Computational Linguistics and Natural Language Processing by compiling and examining the proceedings of the CLiC-it conference from 2014 to 2024.  					AI-generated summary 				 Over the past decade, Computational Linguistics (CL) and Natural Language Pr...
[01.10.2025 00:58] ********************************************************************************
[01.10.2025 00:58] Abstract 79. VC-Inspector, a reference-free and factually grounded caption quality evaluator, uses large language models to generate pseudo captions and train a multimodal model, demonstrating superior performance in evaluating video captions across diverse domains.  					AI-generated summary 				 Video captions...
[01.10.2025 00:58] Read previous papers.
[01.10.2025 00:58] Generating reviews via LLM API.
[01.10.2025 00:58] Using data from previous issue: {"categories": ["#video", "#training", "#optimization", "#architecture", "#diffusion"], "emoji": "", "ru": {"title": " -    ", "desc": "    SLA (Sparse-Linear Attention),   Diffusion Transformer   
[01.10.2025 00:58] Using data from previous issue: {"categories": ["#audio", "#multimodal", "#architecture", "#optimization"], "emoji": "", "ru": {"title": "    ", "desc": "                
[01.10.2025 00:58] Using data from previous issue: {"categories": ["#rl", "#training", "#alignment", "#optimization", "#rlhf"], "emoji": "", "ru": {"title": "      ", "desc": "          ,   N
[01.10.2025 00:58] Using data from previous issue: {"categories": ["#training", "#reasoning", "#agi", "#multimodal", "#benchmark", "#survey", "#architecture"], "emoji": "", "ru": {"title": "  :        ", "desc": "  RealUnify -     
[01.10.2025 00:58] Using data from previous issue: {"categories": ["#data", "#optimization", "#benchmark", "#synthetic", "#multimodal", "#dataset"], "emoji": "", "ru": {"title": "           AI", "desc": "  OpenGPT-4o-Image      
[01.10.2025 00:58] Using data from previous issue: {"categories": ["#rl", "#training", "#reasoning", "#benchmark", "#optimization"], "emoji": "", "ru": {"title": "  :       RL", "desc": "          
[01.10.2025 00:58] Using data from previous issue: {"categories": ["#video", "#training", "#inference", "#small_models", "#diffusion"], "emoji": "", "ru": {"title": "       ", "desc": "SANA-Video -           720x1280  
[01.10.2025 00:58] Using data from previous issue: {"categories": ["#rl", "#training", "#reasoning", "#alignment", "#multimodal", "#3d", "#cv"], "emoji": "", "ru": {"title": "       ", "desc": "Visual Jigsaw -    reinforcement learning     
[01.10.2025 00:58] Using data from previous issue: {"categories": ["#data", "#dataset", "#science", "#open_source", "#multimodal", "#agents"], "emoji": "", "ru": {"title": "    AI-", "desc": "ToolUniverse -  ,     ,     AI-, 
[01.10.2025 00:58] Using data from previous issue: {"categories": ["#math", "#reasoning", "#synthetic", "#data", "#training"], "emoji": "", "ru": {"title": "  :  reasoning    IFT ", "desc": "       ,    (Inst
[01.10.2025 00:58] Using data from previous issue: {"categories": ["#survey", "#benchmark", "#reasoning", "#cv", "#dataset"], "emoji": "", "ru": {"title": "   AI  :        ", "desc": "    GSM8K-V     vision-langua
[01.10.2025 00:58] Using data from previous issue: {"categories": ["#rl", "#data", "#training", "#optimization", "#benchmark"], "emoji": "", "ru": {"title": " reward  -   RL   ", "desc": "   reward  EditScore       
[01.10.2025 00:58] Using data from previous issue: {"categories": ["#inference", "#long_context", "#optimization", "#architecture", "#diffusion"], "emoji": "", "ru": {"title": "      ", "desc": "SparseD -         ,   
[01.10.2025 00:58] Using data from previous issue: {"categories": ["#training", "#alignment", "#inference", "#optimization", "#hallucinations", "#architecture"], "emoji": "", "ru": {"title": "  LLM  ", "desc": "EasySteer           
[01.10.2025 00:58] Using data from previous issue: {"categories": ["#architecture", "#diffusion", "#training", "#optimization"], "emoji": "", "ru": {"title": "     ", "desc": "   Sequential Diffusion Language Model (SDLM) -  ,    
[01.10.2025 00:58] Using data from previous issue: {"categories": ["#benchmark", "#agents", "#personalization"], "emoji": "", "ru": {"title": "  AI-   ", "desc": "     Personalized Deep Research Bench       (Deep Re
[01.10.2025 00:58] Using data from previous issue: {"categories": ["#rl", "#training", "#reasoning", "#rlhf", "#optimization"], "emoji": "", "ru": {"title": "       ", "desc": "   ROVER -        
[01.10.2025 00:58] Using data from previous issue: {"categories": ["#benchmark", "#video", "#rlhf", "#interpretability", "#alignment"], "emoji": "", "ru": {"title": "  AI-  ", "desc": "   VideoScore2         ,   .  
[01.10.2025 00:58] Using data from previous issue: {"categories": ["#transfer_learning", "#optimization", "#benchmark", "#rl", "#rlhf", "#reasoning", "#training"], "emoji": "", "ru": {"title": "   AI ", "desc": "    Critique Reinforcement Learning (CRL),   LLM  
[01.10.2025 00:58] Using data from previous issue: {"categories": ["#transfer_learning", "#training", "#reasoning", "#rl", "#synthetic", "#rlhf"], "emoji": "", "ru": {"title": "RL  LLM    ", "desc": " ,  reinforcement learning        
[01.10.2025 00:58] Using data from previous issue: {"categories": ["#training", "#dataset", "#reasoning", "#multimodal", "#benchmark", "#transfer_learning"], "emoji": "", "ru": {"title": "      AI", "desc": "   Euclid30K  30        
[01.10.2025 00:58] Using data from previous issue: {"categories": ["#interpretability", "#alignment", "#cv", "#benchmark", "#multimodal"], "emoji": "", "ru": {"title": " Vision-Language Models:  benchmark   ", "desc": "  MMPB -   benchmark    Vision-L
[01.10.2025 00:58] Using data from previous issue: {"categories": ["#games", "#3d", "#optimization"], "emoji": "", "ru": {"title": "    COLMAP   3D Foundation Models", "desc": "   3D Foundation Models      (Novel View Synthesis).     
[01.10.2025 00:58] Using data from previous issue: {"categories": ["#long_context", "#optimization", "#games", "#video"], "emoji": "", "ru": {"title": "     ", "desc": "Rolling Forcing -     ,         .  
[01.10.2025 00:58] Using data from previous issue: {"categories": ["#training", "#data", "#optimization", "#rl", "#dataset", "#synthetic", "#cv"], "emoji": "", "ru": {"title": "       ", "desc": "    BRIDGE   nocular depth estimation -    
[01.10.2025 00:58] Using data from previous issue: {"categories": ["#architecture", "#open_source", "#reasoning", "#training", "#long_context"], "emoji": "", "ru": {"title": "        ", "desc": "  InfLLM-V2         
[01.10.2025 00:58] Using data from previous issue: {"categories": ["#rl", "#training", "#reasoning", "#open_source", "#optimization"], "emoji": "", "ru": {"title": "      ", "desc": "    SIRI         reinforcement learnin
[01.10.2025 00:58] Using data from previous issue: {"categories": ["#rlhf", "#reasoning", "#multimodal", "#rl", "#alignment"], "emoji": "", "ru": {"title": "     ", "desc": "    RLHI   conversational       ,    
[01.10.2025 00:58] Using data from previous issue: {"categories": ["#training", "#open_source", "#games", "#agi", "#long_context", "#multimodal", "#interpretability", "#audio", "#architecture"], "emoji": "", "ru": {"title": "         -", "desc": "MGM-Omni    
[01.10.2025 00:58] Using data from previous issue: {"categories": ["#benchmark", "#dataset", "#data", "#training", "#science", "#agents", "#open_source"], "emoji": "", "ru": {"title": "Open-source     ", "desc": "DataMind      open-source    ,   
[01.10.2025 00:58] Using data from previous issue: {"categories": ["#architecture", "#data", "#open_source", "#diffusion", "#multimodal", "#training"], "emoji": "", "ru": {"title": "       80  ", "desc": "HunyuanImage 3.0 -      
[01.10.2025 00:58] Using data from previous issue: {"categories": ["#dataset", "#optimization", "#reasoning", "#training", "#rlhf"], "emoji": "", "ru": {"title": "     ", "desc": "   Tool-Light         
[01.10.2025 00:58] Using data from previous issue: {"categories": ["#architecture", "#reasoning", "#training", "#optimization"], "emoji": "", "ru": {"title": "  :      LLM", "desc": "   Dynamic Experts Search (DES),     
[01.10.2025 00:58] Using data from previous issue: {"categories": ["#rl", "#training", "#transfer_learning", "#optimization", "#benchmark", "#small_models"], "emoji": "", "ru": {"title": "      ", "desc": "  WirelessMathLM -   ,   
[01.10.2025 00:58] Using data from previous issue: {"categories": ["#cv", "#benchmark", "#interpretability", "#multimodal", "#games"], "emoji": "", "ru": {"title": "  :    ", "desc": "    ,   vision-language    
[01.10.2025 00:58] Using data from previous issue: {"categories": ["#reasoning", "#optimization", "#rlhf", "#rl", "#training"], "emoji": "", "ru": {"title": " LLM     ", "desc": "        ,       
[01.10.2025 00:58] Using data from previous issue: {"categories": ["#rl", "#training", "#reasoning", "#optimization", "#benchmark"], "emoji": "", "ru": {"title": "   :    few-shot ", "desc": " ,   reasoning LLM       few-sh
[01.10.2025 00:58] Using data from previous issue: {"categories": ["#benchmark", "#rl", "#diffusion", "#reasoning", "#training", "#optimization", "#math", "#inference"], "emoji": "", "ru": {"title": "    RL  masked diffusion  ", "desc": "   masked diffusion language models (MDLM)  
[01.10.2025 00:58] Using data from previous issue: {"categories": ["#data", "#optimization", "#benchmark", "#rl", "#open_source", "#training", "#games", "#agents"], "emoji": "", "ru": {"title": "  GUI    ", "desc": "DART        
[01.10.2025 00:58] Using data from previous issue: {"categories": ["#rlhf", "#benchmark", "#optimization", "#dataset", "#open_source", "#alignment", "#data"], "emoji": "", "ru": {"title": "     LLM", "desc": "  PrefCleanBench       13    
[01.10.2025 00:58] Using data from previous issue: {"categories": ["#hallucinations", "#benchmark", "#interpretability", "#rag"], "emoji": "", "ru": {"title": "LUMINA:         ", "desc": "  LUMINA        RAG-, 
[01.10.2025 00:58] Using data from previous issue: {"categories": ["#inference", "#optimization", "#training"], "emoji": "", "ru": {"title": "   LLM:  4- ", "desc": "          4-  NVFP4   8- 
[01.10.2025 00:58] Using data from previous issue: {"categories": ["#training", "#optimization", "#cv", "#architecture", "#diffusion"], "emoji": "", "ru": {"title": "      ", "desc": "   SphereAR -     , 
[01.10.2025 00:58] Using data from previous issue: {"categories": ["#benchmark", "#dataset", "#reasoning", "#science", "#multimodal"], "emoji": "", "ru": {"title": "    AI   ", "desc": "       ,     (LL
[01.10.2025 00:58] Using data from previous issue: {"categories": ["#rl", "#training", "#reasoning", "#optimization", "#small_models"], "emoji": "", "ru": {"title": "      ", "desc": "   AceSearcher  framework   ,    LLM  
[01.10.2025 00:58] Using data from previous issue: {"categories": ["#video", "#rl", "#training", "#reasoning", "#long_context", "#optimization", "#benchmark"], "emoji": "", "ru": {"title": "       ", "desc": "  LOVE-R1 -     , 
[01.10.2025 00:58] Using data from previous issue: {"categories": ["#optimization", "#training", "#rlhf", "#alignment"], "emoji": "", "ru": {"title": "     LLM   ", "desc": "   MetaAPO -          
[01.10.2025 00:58] Using data from previous issue: {"categories": ["#leakage", "#alignment", "#multimodal", "#training", "#rl", "#architecture", "#synthetic"], "emoji": "", "ru": {"title": "        ", "desc": "  MultiCrafter -    
[01.10.2025 00:58] Using data from previous issue: {"categories": ["#benchmark", "#cv", "#reasoning", "#interpretability", "#multimodal", "#agents"], "emoji": "", "ru": {"title": "    ", "desc": "PixelCraft    ,       LLM 
[01.10.2025 00:58] Using data from previous issue: {"categories": ["#rl", "#diffusion", "#benchmark", "#rlhf", "#optimization", "#training"], "emoji": "", "ru": {"title": "   RL      ", "desc": "  Advantage Weighted Matching (AWM)   policy-gradient  
[01.10.2025 00:58] Using data from previous issue: {"categories": ["#multimodal", "#rl", "#training", "#optimization", "#reasoning"], "emoji": "", "ru": {"title": "     ", "desc": "    GRPO   Chain-of-Thought   LLM  VLM:  
[01.10.2025 00:58] Using data from previous issue: {"categories": ["#interpretability", "#reasoning", "#benchmark"], "emoji": "", "ru": {"title": "  -    LLM", "desc": "  DafnyCOMP -             Dafny.
[01.10.2025 00:58] Using data from previous issue: {"categories": ["#rl", "#training", "#optimization"], "emoji": "", "ru": {"title": " : ES  RL    LLM", "desc": "      (ES)           
[01.10.2025 00:58] Using data from previous issue: {"categories": ["#training", "#dataset", "#reasoning", "#security", "#alignment", "#rlhf"], "emoji": "", "ru": {"title": " AI   adversarial  ", "desc": "          (LRM),  
[01.10.2025 00:58] Using data from previous issue: {"categories": ["#multimodal", "#video", "#diffusion", "#architecture", "#benchmark", "#optimization"], "emoji": "", "ru": {"title": "      ", "desc": "UniVid   ,   MLLM (  
[01.10.2025 00:58] Using data from previous issue: {"categories": ["#benchmark", "#survey", "#open_source"], "emoji": "", "ru": {"title": "PARROT:     SQL    ", "desc": "    PARROT   Cross-System SQL Translation -   SQL-   
[01.10.2025 00:58] Using data from previous issue: {"categories": ["#open_source", "#benchmark", "#reasoning", "#math", "#interpretability", "#dataset"], "emoji": "", "ru": {"title": "     LLM", "desc": "   MathBode -        
[01.10.2025 00:58] Using data from previous issue: {"categories": ["#agents", "#rlhf", "#security"], "emoji": "", "ru": {"title": "  -:    AI-", "desc": "  ChatInject -     LLM-,    chat-    
[01.10.2025 00:58] Using data from previous issue: {"categories": ["#data", "#games", "#optimization", "#multimodal", "#cv"], "emoji": "", "ru": {"title": "     ", "desc": "UniMIC             
[01.10.2025 00:58] Using data from previous issue: {"categories": ["#rl", "#interpretability", "#reasoning", "#games", "#agents"], "emoji": "", "ru": {"title": ",   :     ", "desc": "     CEL (Cogito, ergo ludo),     
[01.10.2025 00:58] Using data from previous issue: {"categories": ["#rl", "#agi", "#optimization", "#transfer_learning", "#agents"], "emoji": "", "ru": {"title": "  ,   ", "desc": "  SID -            
[01.10.2025 00:58] Using data from previous issue: {"categories": ["#video", "#benchmark", "#optimization", "#training"], "emoji": "", "ru": {"title": "  :    ", "desc": "  SALT             
[01.10.2025 00:58] Using data from previous issue: {"categories": ["#optimization", "#agents", "#benchmark", "#training", "#games"], "emoji": "", "ru": {"title": "TENET: LLM-       ", "desc": "TENET   LLM-          
[01.10.2025 00:58] Using data from previous issue: {"categories": ["#architecture", "#data", "#optimization", "#benchmark", "#open_source", "#reasoning", "#training", "#long_context", "#dataset"], "emoji": "", "ru": {"title": " :     ", "desc": "    RHYTHM 
[01.10.2025 00:58] Using data from previous issue: {"categories": ["#architecture", "#data", "#interpretability", "#reasoning", "#multimodal"], "emoji": "", "ru": {"title": " :      ", "desc": "   Reasoning Manifold         
[01.10.2025 00:58] Querying the API.
[01.10.2025 00:58] Claude request. Model: claude-sonnet-4-5-20250929. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Transformers-based GANs trained in latent space achieve state-of-the-art performance with efficient scaling and reliable training across various capacities.  					AI-generated summary 				 Scalability has driven recent advances in generative modeling, yet its principles remain underexplored for adversarial learning. We investigate the scalability of Generative Adversarial Networks (GANs) through two design choices that have proven to be effective in other types of generative models: training in a compact Variational Autoencoder latent space and adopting purely transformer-based generators and discriminators. Training in latent space enables efficient computation while preserving perceptual fidelity, and this efficiency pairs naturally with plain transformers, whose performance scales with computational budget. Building on these choices, we analyze failure modes that emerge when naively scaling GANs. Specifically, we find issues as underutilization of early layers in the generator and optimization instability as the network scales. Accordingly, we provide simple and scale-friendly solutions as lightweight intermediate supervision and width-aware learning-rate adjustment. Our experiments show that GAT, a purely transformer-based and latent-space GANs, can be easily trained reliably across a wide range of capacities (S through XL). Moreover, GAT-XL/2 achieves state-of-the-art single-step, class-conditional generation performance (FID of 2.96) on ImageNet-256 in just 40 epochs, 6x fewer epochs than strong baselines.
[01.10.2025 00:58] Response: ```json
{
  "title": " GAN     ",
  "desc": "   GAN    :      VAE          .    GAN  ,         ,     supervision    learning rate.   GAT         state-of-the-art   ImageNet-256  FID 2.96   40 .           ,     .",
  "emoji": "",
  "desc_length": 4
}
```
[01.10.2025 00:58] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Transformers-based GANs trained in latent space achieve state-of-the-art performance with efficient scaling and reliable training across various capacities.  					AI-generated summary 				 Scalability has driven recent advances in generative modeling, yet its principles remain underexplored for adversarial learning. We investigate the scalability of Generative Adversarial Networks (GANs) through two design choices that have proven to be effective in other types of generative models: training in a compact Variational Autoencoder latent space and adopting purely transformer-based generators and discriminators. Training in latent space enables efficient computation while preserving perceptual fidelity, and this efficiency pairs naturally with plain transformers, whose performance scales with computational budget. Building on these choices, we analyze failure modes that emerge when naively scaling GANs. Specifically, we find issues as underutilization of early layers in the generator and optimization instability as the network scales. Accordingly, we provide simple and scale-friendly solutions as lightweight intermediate supervision and width-aware learning-rate adjustment. Our experiments show that GAT, a purely transformer-based and latent-space GANs, can be easily trained reliably across a wide range of capacities (S through XL). Moreover, GAT-XL/2 achieves state-of-the-art single-step, class-conditional generation performance (FID of 2.96) on ImageNet-256 in just 40 epochs, 6x fewer epochs than strong baselines."

[01.10.2025 00:58] Response: ```python
['GANs', 'ARCHITECTURE', 'TRAINING']
```
[01.10.2025 00:58] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Transformers-based GANs trained in latent space achieve state-of-the-art performance with efficient scaling and reliable training across various capacities.  					AI-generated summary 				 Scalability has driven recent advances in generative modeling, yet its principles remain underexplored for adversarial learning. We investigate the scalability of Generative Adversarial Networks (GANs) through two design choices that have proven to be effective in other types of generative models: training in a compact Variational Autoencoder latent space and adopting purely transformer-based generators and discriminators. Training in latent space enables efficient computation while preserving perceptual fidelity, and this efficiency pairs naturally with plain transformers, whose performance scales with computational budget. Building on these choices, we analyze failure modes that emerge when naively scaling GANs. Specifically, we find issues as underutilization of early layers in the generator and optimization instability as the network scales. Accordingly, we provide simple and scale-friendly solutions as lightweight intermediate supervision and width-aware learning-rate adjustment. Our experiments show that GAT, a purely transformer-based and latent-space GANs, can be easily trained reliably across a wide range of capacities (S through XL). Moreover, GAT-XL/2 achieves state-of-the-art single-step, class-conditional generation performance (FID of 2.96) on ImageNet-256 in just 40 epochs, 6x fewer epochs than strong baselines."

[01.10.2025 00:58] Response: ```python
["OPTIMIZATION", "DIFFUSION"]
```
[01.10.2025 00:58] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper explores the scalability of Generative Adversarial Networks (GANs) by utilizing a compact Variational Autoencoder latent space and transformer architectures for both generators and discriminators. By training in latent space, the authors achieve efficient computation while maintaining high-quality outputs. They identify challenges that arise when scaling GANs, such as underutilization of early layers and optimization instability, and propose solutions like lightweight intermediate supervision and width-aware learning-rate adjustments. Their experiments demonstrate that the proposed GAT model, which is purely transformer-based and operates in latent space, achieves state-of-the-art performance on ImageNet-256 with significantly fewer training epochs than existing methods.","title":"Scaling GANs with Transformers for Efficient Generation"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper explores the scalability of Generative Adversarial Networks (GANs) by utilizing a compact Variational Autoencoder latent space and transformer architectures for both generators and discriminators. By training in latent space, the authors achieve efficient computation while maintaining high-quality outputs. They identify challenges that arise when scaling GANs, such as underutilization of early layers and optimization instability, and propose solutions like lightweight intermediate supervision and width-aware learning-rate adjustments. Their experiments demonstrate that the proposed GAT model, which is purely transformer-based and operates in latent space, achieves state-of-the-art performance on ImageNet-256 with significantly fewer training epochs than existing methods.', title='Scaling GANs with Transformers for Efficient Generation'))
[01.10.2025 00:58] Response: ParsedChatCompletionMessage[Article](content='{"desc":"GANGAN","title":"GAN"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='GANGAN', title='GAN'))
[01.10.2025 00:58] Using data from previous issue: {"categories": ["#benchmark", "#dataset", "#low_resource", "#data", "#machine_translation", "#multilingual"], "emoji": "", "ru": {"title": "      ", "desc": "  BOE-XSUM -   3,648   
[01.10.2025 00:58] Using data from previous issue: {"categories": ["#cv", "#benchmark", "#data", "#multimodal", "#diffusion", "#synthetic", "#dataset"], "emoji": "", "ru": {"title": "     ", "desc": "  ThermalGen         RGB-
[01.10.2025 00:58] Using data from previous issue: {"categories": ["#training", "#data", "#dataset", "#reasoning", "#synthetic", "#math", "#agents"], "emoji": "", "ru": {"title": "    :      reasoning", "desc": "  Socratic-Zero      
[01.10.2025 00:58] Using data from previous issue: {"categories": ["#open_source", "#optimization", "#benchmark", "#reasoning", "#cv", "#multimodal"], "emoji": "", "ru": {"title": "   :    AI    -", "desc": "   IWR-Bench        vision
[01.10.2025 00:58] Using data from previous issue: {"categories": ["#dataset", "#data", "#multimodal", "#graphs", "#optimization", "#benchmark", "#games"], "emoji": "", "ru": {"title": "LLM      -", "desc": "   BPMN Assistant  ,  LLM    
[01.10.2025 00:58] Using data from previous issue: {"categories": ["#interpretability", "#agents", "#rag", "#science", "#reasoning", "#alignment", "#benchmark"], "emoji": "", "ru": {"title": "AI-     ", "desc": "   CLAIRE,    LLM     
[01.10.2025 00:58] Using data from previous issue: {"categories": ["#creativity", "#architecture", "#data", "#training", "#agi"], "emoji": "", "ru": {"title": " LLM:     ", "desc": "           -  
[01.10.2025 00:58] Using data from previous issue: {"categories": ["#architecture", "#dataset", "#optimization", "#interpretability", "#benchmark", "#multimodal", "#cv", "#agi"], "emoji": "", "ru": {"title": " :  AI   ", "desc": "      
[01.10.2025 00:58] Querying the API.
[01.10.2025 00:58] Claude request. Model: claude-sonnet-4-5-20250929. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Text-based supervised fine-tuning with sparse labels enables vision language models to achieve expert-level accuracy in 3D depth estimation without requiring task-specific architectures or losses.  					AI-generated summary 				 Vision language models (VLMs) can flexibly address various vision tasks through text interactions. Although successful in semantic understanding, state-of-the-art VLMs including GPT-5 still struggle in understanding 3D from 2D inputs. On the other hand, expert pure vision models achieve super-human accuracy in metric depth estimation, a key 3D understanding task. However, they require task-specific architectures and losses. Such difference motivates us to ask: Can VLMs reach expert-level accuracy without architecture or loss change? We take per-pixel metric depth estimation as the representative task and show that the answer is yes! Surprisingly, comprehensive analysis shows that text-based supervised-finetuning with sparse labels is sufficient for VLMs to unlock strong 3D understanding, no dense prediction head or complex regression/regularization loss is needed. The bottleneck for VLMs lies actually in pixel reference and cross-dataset camera ambiguity, which we address through visual prompting and intrinsic-conditioned augmentation. With much smaller models, our method DepthLM surpasses the accuracy of most advanced VLMs by over 2x, making VLMs for the first time comparable with pure vision models. Interestingly, without explicit enforcement during training, VLMs trained with DepthLM naturally avoids over-smoothing, having much fewer flying points at boundary regions than pure vision models. The simplicity of DepthLM also enables a single VLM to cover various 3D tasks beyond metric depth. Our code and model will be released at the link below.
[01.10.2025 00:58] Response: ```json
{
  "desc": " ,  vision language models (VLM)        3D        .    ,   supervised fine-tuning        3D   VLM.  DepthLM   pixel reference  -    visual prompting  intrinsic-conditioned augmentation. VLM,   ,       2      over-smoothing   .",
  "emoji": "",
  "title": "VLM      "
}
```
[01.10.2025 00:58] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Text-based supervised fine-tuning with sparse labels enables vision language models to achieve expert-level accuracy in 3D depth estimation without requiring task-specific architectures or losses.  					AI-generated summary 				 Vision language models (VLMs) can flexibly address various vision tasks through text interactions. Although successful in semantic understanding, state-of-the-art VLMs including GPT-5 still struggle in understanding 3D from 2D inputs. On the other hand, expert pure vision models achieve super-human accuracy in metric depth estimation, a key 3D understanding task. However, they require task-specific architectures and losses. Such difference motivates us to ask: Can VLMs reach expert-level accuracy without architecture or loss change? We take per-pixel metric depth estimation as the representative task and show that the answer is yes! Surprisingly, comprehensive analysis shows that text-based supervised-finetuning with sparse labels is sufficient for VLMs to unlock strong 3D understanding, no dense prediction head or complex regression/regularization loss is needed. The bottleneck for VLMs lies actually in pixel reference and cross-dataset camera ambiguity, which we address through visual prompting and intrinsic-conditioned augmentation. With much smaller models, our method DepthLM surpasses the accuracy of most advanced VLMs by over 2x, making VLMs for the first time comparable with pure vision models. Interestingly, without explicit enforcement during training, VLMs trained with DepthLM naturally avoids over-smoothing, having much fewer flying points at boundary regions than pure vision models. The simplicity of DepthLM also enables a single VLM to cover various 3D tasks beyond metric depth. Our code and model will be released at the link below."

[01.10.2025 00:58] Response: ```python
['3D', 'CV', 'TRAINING', 'SMALL_MODELS']
```
[01.10.2025 00:58] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Text-based supervised fine-tuning with sparse labels enables vision language models to achieve expert-level accuracy in 3D depth estimation without requiring task-specific architectures or losses.  					AI-generated summary 				 Vision language models (VLMs) can flexibly address various vision tasks through text interactions. Although successful in semantic understanding, state-of-the-art VLMs including GPT-5 still struggle in understanding 3D from 2D inputs. On the other hand, expert pure vision models achieve super-human accuracy in metric depth estimation, a key 3D understanding task. However, they require task-specific architectures and losses. Such difference motivates us to ask: Can VLMs reach expert-level accuracy without architecture or loss change? We take per-pixel metric depth estimation as the representative task and show that the answer is yes! Surprisingly, comprehensive analysis shows that text-based supervised-finetuning with sparse labels is sufficient for VLMs to unlock strong 3D understanding, no dense prediction head or complex regression/regularization loss is needed. The bottleneck for VLMs lies actually in pixel reference and cross-dataset camera ambiguity, which we address through visual prompting and intrinsic-conditioned augmentation. With much smaller models, our method DepthLM surpasses the accuracy of most advanced VLMs by over 2x, making VLMs for the first time comparable with pure vision models. Interestingly, without explicit enforcement during training, VLMs trained with DepthLM naturally avoids over-smoothing, having much fewer flying points at boundary regions than pure vision models. The simplicity of DepthLM also enables a single VLM to cover various 3D tasks beyond metric depth. Our code and model will be released at the link below."

[01.10.2025 00:58] Response: ```python
["OPTIMIZATION", "GAMES"]
```
[01.10.2025 00:58] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents a method called DepthLM that allows vision language models (VLMs) to achieve expert-level accuracy in 3D depth estimation using text-based supervised fine-tuning with sparse labels. Unlike traditional vision models that require specific architectures and complex loss functions, DepthLM demonstrates that VLMs can excel in 3D understanding without these constraints. The authors identify pixel reference and camera ambiguity as key challenges for VLMs and address them through visual prompting and intrinsic-conditioned augmentation. As a result, DepthLM not only surpasses the accuracy of existing VLMs but also simplifies the approach, enabling a single model to handle multiple 3D tasks effectively.","title":"Unlocking 3D Depth with Simple Text-Based Fine-Tuning"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper presents a method called DepthLM that allows vision language models (VLMs) to achieve expert-level accuracy in 3D depth estimation using text-based supervised fine-tuning with sparse labels. Unlike traditional vision models that require specific architectures and complex loss functions, DepthLM demonstrates that VLMs can excel in 3D understanding without these constraints. The authors identify pixel reference and camera ambiguity as key challenges for VLMs and address them through visual prompting and intrinsic-conditioned augmentation. As a result, DepthLM not only surpasses the accuracy of existing VLMs but also simplifies the approach, enabling a single model to handle multiple 3D tasks effectively.', title='Unlocking 3D Depth with Simple Text-Based Fine-Tuning'))
[01.10.2025 00:58] Response: ParsedChatCompletionMessage[Article](content='{"desc":"VLMs3DVLMs3DDepthLMVLMsVLM3DVLMs","title":"3D"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='VLMs3DVLMs3DDepthLMVLMsVLM3DVLMs', title='3D'))
[01.10.2025 00:58] Using data from previous issue: {"categories": ["#diffusion", "#rl", "#optimization", "#training"], "emoji": "", "ru": {"title": "    -   ", "desc": "  TR2-D2 -    -      
[01.10.2025 00:58] Using data from previous issue: {"categories": ["#interpretability", "#data", "#alignment", "#training", "#dataset"], "emoji": "", "ru": {"title": "        AI", "desc": "  Generalized Correctness Models (GCM)  ,   
[01.10.2025 00:58] Using data from previous issue: {"categories": ["#multimodal", "#reasoning", "#hallucinations", "#dataset", "#rag", "#multilingual", "#open_source", "#benchmark"], "emoji": "", "ru": {"title": "  AI  ", "desc": "ADAM         LLM   
[01.10.2025 00:58] Using data from previous issue: {"categories": ["#science", "#dataset", "#data", "#survey", "#multilingual"], "emoji": "", "ru": {"title": "   :    LLM", "desc": "            
[01.10.2025 00:58] Using data from previous issue: {"categories": ["#interpretability", "#video", "#multimodal", "#benchmark", "#optimization"], "emoji": "", "ru": {"title": "       ", "desc": "  VC-Inspector -      , 
[01.10.2025 00:58] Renaming data file.
[01.10.2025 00:58] Renaming previous data. hf_papers.json to ./d/2025-10-01.json
[01.10.2025 00:58] Saving new data file.
[01.10.2025 00:58] Generating page.
[01.10.2025 00:58] Renaming previous page.
[01.10.2025 00:58] Renaming previous data. index.html to ./d/2025-10-01.html
[01.10.2025 00:58] Writing result.
[01.10.2025 00:58] Renaming log file.
[01.10.2025 00:58] Renaming previous data. log.txt to ./logs/2025-10-01_last_log.txt
