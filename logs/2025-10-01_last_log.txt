[01.10.2025 16:15] Read previous papers.
[01.10.2025 16:15] Generating top page (month).
[01.10.2025 16:15] Writing top page (month).
[01.10.2025 17:10] Read previous papers.
[01.10.2025 17:10] Get feed.
[01.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.24002
[01.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.26507
[01.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.25541
[01.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.23873
[01.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.25760
[01.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.26536
[01.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.26625
[01.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.25848
[01.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.26226
[01.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.25182
[01.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.25154
[01.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.25758
[01.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.26490
[01.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.26488
[01.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.22646
[01.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.26231
[01.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.26391
[01.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.23610
[01.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.26618
[01.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.26603
[01.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.25911
[01.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.26495
[01.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.24207
[01.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.26628
[01.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.25397
[01.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.25339
[01.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.22613
[01.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.26476
[01.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.26030
[01.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.25189
[01.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.26645
[01.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.26542
[01.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.26539
[01.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.23166
[01.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.25716
[01.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.26329
[01.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.26157
[01.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.25085
[01.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.23773
[01.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.23094
[01.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.21361
[01.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.26604
[01.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.26574
[01.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.25134
[01.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.25082
[01.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.24732
[01.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.23695
[01.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.25248
[01.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.23019
[01.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.26555
[01.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.26278
[01.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.25810
[01.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.25666
[01.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.24510
[01.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.22889
[01.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.18538
[01.10.2025 17:10] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[01.10.2025 17:10] No deleted papers detected.
[01.10.2025 17:10] Downloading and parsing papers (pdf, html). Total: 56.
[01.10.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2509.24002.
[01.10.2025 17:10] Extra JSON file exists (./assets/json/2509.24002.json), skip PDF parsing.
[01.10.2025 17:10] Paper image links file exists (./assets/img_data/2509.24002.json), skip HTML parsing.
[01.10.2025 17:10] Success.
[01.10.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2509.26507.
[01.10.2025 17:10] Extra JSON file exists (./assets/json/2509.26507.json), skip PDF parsing.
[01.10.2025 17:10] Paper image links file exists (./assets/img_data/2509.26507.json), skip HTML parsing.
[01.10.2025 17:10] Success.
[01.10.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2509.25541.
[01.10.2025 17:10] Extra JSON file exists (./assets/json/2509.25541.json), skip PDF parsing.
[01.10.2025 17:10] Paper image links file exists (./assets/img_data/2509.25541.json), skip HTML parsing.
[01.10.2025 17:10] Success.
[01.10.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2509.23873.
[01.10.2025 17:10] Extra JSON file exists (./assets/json/2509.23873.json), skip PDF parsing.
[01.10.2025 17:10] Paper image links file exists (./assets/img_data/2509.23873.json), skip HTML parsing.
[01.10.2025 17:10] Success.
[01.10.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2509.25760.
[01.10.2025 17:10] Extra JSON file exists (./assets/json/2509.25760.json), skip PDF parsing.
[01.10.2025 17:10] Paper image links file exists (./assets/img_data/2509.25760.json), skip HTML parsing.
[01.10.2025 17:10] Success.
[01.10.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2509.26536.
[01.10.2025 17:10] Extra JSON file exists (./assets/json/2509.26536.json), skip PDF parsing.
[01.10.2025 17:10] Paper image links file exists (./assets/img_data/2509.26536.json), skip HTML parsing.
[01.10.2025 17:10] Success.
[01.10.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2509.26625.
[01.10.2025 17:10] Extra JSON file exists (./assets/json/2509.26625.json), skip PDF parsing.
[01.10.2025 17:10] Paper image links file exists (./assets/img_data/2509.26625.json), skip HTML parsing.
[01.10.2025 17:10] Success.
[01.10.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2509.25848.
[01.10.2025 17:10] Extra JSON file exists (./assets/json/2509.25848.json), skip PDF parsing.
[01.10.2025 17:10] Paper image links file exists (./assets/img_data/2509.25848.json), skip HTML parsing.
[01.10.2025 17:10] Success.
[01.10.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2509.26226.
[01.10.2025 17:10] Extra JSON file exists (./assets/json/2509.26226.json), skip PDF parsing.
[01.10.2025 17:10] Paper image links file exists (./assets/img_data/2509.26226.json), skip HTML parsing.
[01.10.2025 17:10] Success.
[01.10.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2509.25182.
[01.10.2025 17:10] Extra JSON file exists (./assets/json/2509.25182.json), skip PDF parsing.
[01.10.2025 17:10] Paper image links file exists (./assets/img_data/2509.25182.json), skip HTML parsing.
[01.10.2025 17:10] Success.
[01.10.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2509.25154.
[01.10.2025 17:10] Extra JSON file exists (./assets/json/2509.25154.json), skip PDF parsing.
[01.10.2025 17:10] Paper image links file exists (./assets/img_data/2509.25154.json), skip HTML parsing.
[01.10.2025 17:10] Success.
[01.10.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2509.25758.
[01.10.2025 17:10] Extra JSON file exists (./assets/json/2509.25758.json), skip PDF parsing.
[01.10.2025 17:10] Paper image links file exists (./assets/img_data/2509.25758.json), skip HTML parsing.
[01.10.2025 17:10] Success.
[01.10.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2509.26490.
[01.10.2025 17:10] Extra JSON file exists (./assets/json/2509.26490.json), skip PDF parsing.
[01.10.2025 17:10] Paper image links file exists (./assets/img_data/2509.26490.json), skip HTML parsing.
[01.10.2025 17:10] Success.
[01.10.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2509.26488.
[01.10.2025 17:10] Extra JSON file exists (./assets/json/2509.26488.json), skip PDF parsing.
[01.10.2025 17:10] Paper image links file exists (./assets/img_data/2509.26488.json), skip HTML parsing.
[01.10.2025 17:10] Success.
[01.10.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2509.22646.
[01.10.2025 17:10] Extra JSON file exists (./assets/json/2509.22646.json), skip PDF parsing.
[01.10.2025 17:10] Paper image links file exists (./assets/img_data/2509.22646.json), skip HTML parsing.
[01.10.2025 17:10] Success.
[01.10.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2509.26231.
[01.10.2025 17:10] Extra JSON file exists (./assets/json/2509.26231.json), skip PDF parsing.
[01.10.2025 17:10] Paper image links file exists (./assets/img_data/2509.26231.json), skip HTML parsing.
[01.10.2025 17:10] Success.
[01.10.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2509.26391.
[01.10.2025 17:10] Extra JSON file exists (./assets/json/2509.26391.json), skip PDF parsing.
[01.10.2025 17:10] Paper image links file exists (./assets/img_data/2509.26391.json), skip HTML parsing.
[01.10.2025 17:10] Success.
[01.10.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2509.23610.
[01.10.2025 17:10] Extra JSON file exists (./assets/json/2509.23610.json), skip PDF parsing.
[01.10.2025 17:10] Paper image links file exists (./assets/img_data/2509.23610.json), skip HTML parsing.
[01.10.2025 17:10] Success.
[01.10.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2509.26618.
[01.10.2025 17:10] Extra JSON file exists (./assets/json/2509.26618.json), skip PDF parsing.
[01.10.2025 17:10] Paper image links file exists (./assets/img_data/2509.26618.json), skip HTML parsing.
[01.10.2025 17:10] Success.
[01.10.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2509.26603.
[01.10.2025 17:10] Extra JSON file exists (./assets/json/2509.26603.json), skip PDF parsing.
[01.10.2025 17:10] Paper image links file exists (./assets/img_data/2509.26603.json), skip HTML parsing.
[01.10.2025 17:10] Success.
[01.10.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2509.25911.
[01.10.2025 17:10] Extra JSON file exists (./assets/json/2509.25911.json), skip PDF parsing.
[01.10.2025 17:10] Paper image links file exists (./assets/img_data/2509.25911.json), skip HTML parsing.
[01.10.2025 17:10] Success.
[01.10.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2509.26495.
[01.10.2025 17:10] Extra JSON file exists (./assets/json/2509.26495.json), skip PDF parsing.
[01.10.2025 17:10] Paper image links file exists (./assets/img_data/2509.26495.json), skip HTML parsing.
[01.10.2025 17:10] Success.
[01.10.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2509.24207.
[01.10.2025 17:10] Extra JSON file exists (./assets/json/2509.24207.json), skip PDF parsing.
[01.10.2025 17:10] Paper image links file exists (./assets/img_data/2509.24207.json), skip HTML parsing.
[01.10.2025 17:10] Success.
[01.10.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2509.26628.
[01.10.2025 17:10] Extra JSON file exists (./assets/json/2509.26628.json), skip PDF parsing.
[01.10.2025 17:10] Paper image links file exists (./assets/img_data/2509.26628.json), skip HTML parsing.
[01.10.2025 17:10] Success.
[01.10.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2509.25397.
[01.10.2025 17:10] Extra JSON file exists (./assets/json/2509.25397.json), skip PDF parsing.
[01.10.2025 17:10] Paper image links file exists (./assets/img_data/2509.25397.json), skip HTML parsing.
[01.10.2025 17:10] Success.
[01.10.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2509.25339.
[01.10.2025 17:10] Extra JSON file exists (./assets/json/2509.25339.json), skip PDF parsing.
[01.10.2025 17:10] Paper image links file exists (./assets/img_data/2509.25339.json), skip HTML parsing.
[01.10.2025 17:10] Success.
[01.10.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2509.22613.
[01.10.2025 17:10] Extra JSON file exists (./assets/json/2509.22613.json), skip PDF parsing.
[01.10.2025 17:10] Paper image links file exists (./assets/img_data/2509.22613.json), skip HTML parsing.
[01.10.2025 17:10] Success.
[01.10.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2509.26476.
[01.10.2025 17:10] Extra JSON file exists (./assets/json/2509.26476.json), skip PDF parsing.
[01.10.2025 17:10] Paper image links file exists (./assets/img_data/2509.26476.json), skip HTML parsing.
[01.10.2025 17:10] Success.
[01.10.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2509.26030.
[01.10.2025 17:10] Extra JSON file exists (./assets/json/2509.26030.json), skip PDF parsing.
[01.10.2025 17:10] Paper image links file exists (./assets/img_data/2509.26030.json), skip HTML parsing.
[01.10.2025 17:10] Success.
[01.10.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2509.25189.
[01.10.2025 17:10] Extra JSON file exists (./assets/json/2509.25189.json), skip PDF parsing.
[01.10.2025 17:10] Paper image links file exists (./assets/img_data/2509.25189.json), skip HTML parsing.
[01.10.2025 17:10] Success.
[01.10.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2509.26645.
[01.10.2025 17:10] Extra JSON file exists (./assets/json/2509.26645.json), skip PDF parsing.
[01.10.2025 17:10] Paper image links file exists (./assets/img_data/2509.26645.json), skip HTML parsing.
[01.10.2025 17:10] Success.
[01.10.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2509.26542.
[01.10.2025 17:10] Extra JSON file exists (./assets/json/2509.26542.json), skip PDF parsing.
[01.10.2025 17:10] Paper image links file exists (./assets/img_data/2509.26542.json), skip HTML parsing.
[01.10.2025 17:10] Success.
[01.10.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2509.26539.
[01.10.2025 17:10] Extra JSON file exists (./assets/json/2509.26539.json), skip PDF parsing.
[01.10.2025 17:10] Paper image links file exists (./assets/img_data/2509.26539.json), skip HTML parsing.
[01.10.2025 17:10] Success.
[01.10.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2509.23166.
[01.10.2025 17:10] Extra JSON file exists (./assets/json/2509.23166.json), skip PDF parsing.
[01.10.2025 17:10] Paper image links file exists (./assets/img_data/2509.23166.json), skip HTML parsing.
[01.10.2025 17:10] Success.
[01.10.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2509.25716.
[01.10.2025 17:10] Extra JSON file exists (./assets/json/2509.25716.json), skip PDF parsing.
[01.10.2025 17:10] Paper image links file exists (./assets/img_data/2509.25716.json), skip HTML parsing.
[01.10.2025 17:10] Success.
[01.10.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2509.26329.
[01.10.2025 17:10] Extra JSON file exists (./assets/json/2509.26329.json), skip PDF parsing.
[01.10.2025 17:10] Paper image links file exists (./assets/img_data/2509.26329.json), skip HTML parsing.
[01.10.2025 17:10] Success.
[01.10.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2509.26157.
[01.10.2025 17:10] Extra JSON file exists (./assets/json/2509.26157.json), skip PDF parsing.
[01.10.2025 17:10] Paper image links file exists (./assets/img_data/2509.26157.json), skip HTML parsing.
[01.10.2025 17:10] Success.
[01.10.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2509.25085.
[01.10.2025 17:10] Extra JSON file exists (./assets/json/2509.25085.json), skip PDF parsing.
[01.10.2025 17:10] Paper image links file exists (./assets/img_data/2509.25085.json), skip HTML parsing.
[01.10.2025 17:10] Success.
[01.10.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2509.23773.
[01.10.2025 17:10] Extra JSON file exists (./assets/json/2509.23773.json), skip PDF parsing.
[01.10.2025 17:10] Paper image links file exists (./assets/img_data/2509.23773.json), skip HTML parsing.
[01.10.2025 17:10] Success.
[01.10.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2509.23094.
[01.10.2025 17:10] Extra JSON file exists (./assets/json/2509.23094.json), skip PDF parsing.
[01.10.2025 17:10] Paper image links file exists (./assets/img_data/2509.23094.json), skip HTML parsing.
[01.10.2025 17:10] Success.
[01.10.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2509.21361.
[01.10.2025 17:10] Extra JSON file exists (./assets/json/2509.21361.json), skip PDF parsing.
[01.10.2025 17:10] Paper image links file exists (./assets/img_data/2509.21361.json), skip HTML parsing.
[01.10.2025 17:10] Success.
[01.10.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2509.26604.
[01.10.2025 17:10] Extra JSON file exists (./assets/json/2509.26604.json), skip PDF parsing.
[01.10.2025 17:10] Paper image links file exists (./assets/img_data/2509.26604.json), skip HTML parsing.
[01.10.2025 17:10] Success.
[01.10.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2509.26574.
[01.10.2025 17:10] Extra JSON file exists (./assets/json/2509.26574.json), skip PDF parsing.
[01.10.2025 17:10] Paper image links file exists (./assets/img_data/2509.26574.json), skip HTML parsing.
[01.10.2025 17:10] Success.
[01.10.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2509.25134.
[01.10.2025 17:10] Extra JSON file exists (./assets/json/2509.25134.json), skip PDF parsing.
[01.10.2025 17:10] Paper image links file exists (./assets/img_data/2509.25134.json), skip HTML parsing.
[01.10.2025 17:10] Success.
[01.10.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2509.25082.
[01.10.2025 17:10] Extra JSON file exists (./assets/json/2509.25082.json), skip PDF parsing.
[01.10.2025 17:10] Paper image links file exists (./assets/img_data/2509.25082.json), skip HTML parsing.
[01.10.2025 17:10] Success.
[01.10.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2509.24732.
[01.10.2025 17:10] Extra JSON file exists (./assets/json/2509.24732.json), skip PDF parsing.
[01.10.2025 17:10] Paper image links file exists (./assets/img_data/2509.24732.json), skip HTML parsing.
[01.10.2025 17:10] Success.
[01.10.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2509.23695.
[01.10.2025 17:10] Extra JSON file exists (./assets/json/2509.23695.json), skip PDF parsing.
[01.10.2025 17:10] Paper image links file exists (./assets/img_data/2509.23695.json), skip HTML parsing.
[01.10.2025 17:10] Success.
[01.10.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2509.25248.
[01.10.2025 17:10] Extra JSON file exists (./assets/json/2509.25248.json), skip PDF parsing.
[01.10.2025 17:10] Paper image links file exists (./assets/img_data/2509.25248.json), skip HTML parsing.
[01.10.2025 17:10] Success.
[01.10.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2509.23019.
[01.10.2025 17:10] Extra JSON file exists (./assets/json/2509.23019.json), skip PDF parsing.
[01.10.2025 17:10] Paper image links file exists (./assets/img_data/2509.23019.json), skip HTML parsing.
[01.10.2025 17:10] Success.
[01.10.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2509.26555.
[01.10.2025 17:10] Extra JSON file exists (./assets/json/2509.26555.json), skip PDF parsing.
[01.10.2025 17:10] Paper image links file exists (./assets/img_data/2509.26555.json), skip HTML parsing.
[01.10.2025 17:10] Success.
[01.10.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2509.26278.
[01.10.2025 17:10] Extra JSON file exists (./assets/json/2509.26278.json), skip PDF parsing.
[01.10.2025 17:10] Paper image links file exists (./assets/img_data/2509.26278.json), skip HTML parsing.
[01.10.2025 17:10] Success.
[01.10.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2509.25810.
[01.10.2025 17:10] Extra JSON file exists (./assets/json/2509.25810.json), skip PDF parsing.
[01.10.2025 17:10] Paper image links file exists (./assets/img_data/2509.25810.json), skip HTML parsing.
[01.10.2025 17:10] Success.
[01.10.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2509.25666.
[01.10.2025 17:10] Extra JSON file exists (./assets/json/2509.25666.json), skip PDF parsing.
[01.10.2025 17:10] Paper image links file exists (./assets/img_data/2509.25666.json), skip HTML parsing.
[01.10.2025 17:10] Success.
[01.10.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2509.24510.
[01.10.2025 17:10] Downloading paper 2509.24510 from http://arxiv.org/pdf/2509.24510v1...
[01.10.2025 17:10] Failed to download and parse paper https://huggingface.co/papers/2509.24510: 'LTChar' object is not iterable
[01.10.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2509.22889.
[01.10.2025 17:10] Extra JSON file exists (./assets/json/2509.22889.json), skip PDF parsing.
[01.10.2025 17:10] Paper image links file exists (./assets/img_data/2509.22889.json), skip HTML parsing.
[01.10.2025 17:10] Success.
[01.10.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2509.18538.
[01.10.2025 17:10] Extra JSON file exists (./assets/json/2509.18538.json), skip PDF parsing.
[01.10.2025 17:10] Paper image links file exists (./assets/img_data/2509.18538.json), skip HTML parsing.
[01.10.2025 17:10] Success.
[01.10.2025 17:10] Enriching papers with extra data.
[01.10.2025 17:10] ********************************************************************************
[01.10.2025 17:10] Abstract 0. MCPMark is a comprehensive benchmark for evaluating MCP use in real-world workflows, featuring diverse tasks that require richer interactions with the environment, and reveals that current LLMs perform poorly on these tasks.  					AI-generated summary 				 MCP standardizes how LLMs interact with ext...
[01.10.2025 17:10] ********************************************************************************
[01.10.2025 17:10] Abstract 1. BDH, a biologically inspired Large Language Model, combines scale-free network architecture with Hebbian learning to achieve Transformer-like performance while maintaining interpretability.  					AI-generated summary 				 The relationship between computing systems and the brain has served as motivat...
[01.10.2025 17:10] ********************************************************************************
[01.10.2025 17:10] Abstract 2. Vision-Zero is a domain-agnostic framework that enhances vision-language models through self-improvement in competitive visual games, using Iterative Self-Play Policy Optimization and achieving state-of-the-art performance without human annotation.  					AI-generated summary 				 Although reinforcem...
[01.10.2025 17:10] ********************************************************************************
[01.10.2025 17:10] Abstract 3. Quadrant-based Tuning (Q-Tuning) optimizes both sample and token pruning in supervised fine-tuning of large language models, achieving superior performance with reduced data.  					AI-generated summary 				 As supervised fine-tuning (SFT) evolves from a lightweight post-training step into a compute-...
[01.10.2025 17:10] ********************************************************************************
[01.10.2025 17:10] Abstract 4. TruthRL, a reinforcement learning framework, enhances the truthfulness of large language models by balancing accuracy and abstention, significantly reducing hallucinations and improving performance across benchmarks.  					AI-generated summary 				 While large language models (LLMs) have demonstrate...
[01.10.2025 17:10] ********************************************************************************
[01.10.2025 17:10] Abstract 5. OceanGym is a benchmark for underwater embodied agents using Multi-modal Large Language Models to address challenges in perception, planning, and adaptability in harsh ocean environments.  					AI-generated summary 				 We introduce OceanGym, the first comprehensive benchmark for ocean underwater em...
[01.10.2025 17:10] ********************************************************************************
[01.10.2025 17:10] Abstract 6. LLMs develop visual priors during language pre-training, which can be leveraged for vision tasks with minimal additional data, and these priors are composed of separable perception and reasoning components.  					AI-generated summary 				 Large Language Models (LLMs), despite being trained on text a...
[01.10.2025 17:10] ********************************************************************************
[01.10.2025 17:10] Abstract 7. VAPO-Thinker-7B enhances multimodal reasoning by anchoring the process to visual information, improving performance on visual tasks while maintaining logical inference.  					AI-generated summary 				 Reasoning has emerged as a pivotal capability in Large Language Models (LLMs). Through Reinforcemen...
[01.10.2025 17:10] ********************************************************************************
[01.10.2025 17:10] Abstract 8. TFPI, a simple adaptation to RLVR, improves performance and reduces token usage by discarding thinking content during training, accelerating RL convergence and achieving higher accuracy with less computational cost.  					AI-generated summary 				 Reinforcement Learning with Verifiable Reward (RLVR)...
[01.10.2025 17:10] ********************************************************************************
[01.10.2025 17:10] Abstract 9. DC-VideoGen accelerates video generation by adapting pre-trained diffusion models to a deep compression latent space, reducing inference latency and enabling high-resolution video generation.  					AI-generated summary 				 We introduce DC-VideoGen, a post-training acceleration framework for efficie...
[01.10.2025 17:10] ********************************************************************************
[01.10.2025 17:10] Abstract 10. J-Detector, a neural detector with linguistic and LLM-enhanced features, effectively identifies LLM-generated judgments based on scores and candidate content, addressing biases and vulnerabilities in sensitive scenarios.  					AI-generated summary 				 Large Language Model (LLM)-based judgments leve...
[01.10.2025 17:10] ********************************************************************************
[01.10.2025 17:10] Abstract 11. Post-training techniques like supervised fine-tuning and reinforcement learning lead to the emergence of specialized attention heads that support structured reasoning, with different training regimes affecting their evolution and performance.  					AI-generated summary 				 The remarkable capabiliti...
[01.10.2025 17:10] ********************************************************************************
[01.10.2025 17:10] Abstract 12. VitaBench is a benchmark for evaluating LLM-based agents in complex, real-world interactive tasks using a diverse set of tools and scenarios.  					AI-generated summary 				 As LLM-based agents are increasingly deployed in real-life scenarios, existing benchmarks fail to capture their inherent compl...
[01.10.2025 17:10] ********************************************************************************
[01.10.2025 17:10] Abstract 13. dParallel is a method that enhances the parallel decoding of diffusion large language models, significantly reducing decoding steps without compromising performance.  					AI-generated summary 				 Diffusion large language models (dLLMs) have recently drawn considerable attention within the research...
[01.10.2025 17:10] ********************************************************************************
[01.10.2025 17:10] Abstract 14. DeeptraceReward is a benchmark dataset that annotates human-perceived deepfake traces in videos, used to train multimodal language models for detecting AI-generated videos.  					AI-generated summary 				 Can humans identify AI-generated (fake) videos and provide grounded reasons? While video genera...
[01.10.2025 17:10] ********************************************************************************
[01.10.2025 17:10] Abstract 15. Implicit Multimodal Guidance (IMG) enhances multimodal alignment between diffusion-generated images and prompts without additional data or editing, outperforming existing methods.  					AI-generated summary 				 Ensuring precise multimodal alignment between diffusion-generated images and input promp...
[01.10.2025 17:10] ********************************************************************************
[01.10.2025 17:10] Abstract 16. MotionRAG enhances video generation by integrating motion priors from reference videos using a retrieval-augmented framework, improving motion realism with negligible computational overhead.  					AI-generated summary 				 Image-to-video generation has made remarkable progress with the advancements ...
[01.10.2025 17:10] ********************************************************************************
[01.10.2025 17:10] Abstract 17. Dolphin, an efficient AVSS method, uses a dual-path lightweight video encoder and a lightweight encoder-decoder separator with global-local attention blocks to achieve high separation quality and significant computational efficiency.  					AI-generated summary 				 Audio-visual speech separation (AV...
[01.10.2025 17:10] ********************************************************************************
[01.10.2025 17:10] Abstract 18. DA¬≤, a zero-shot generalizable and fully end-to-end panoramic depth estimator, addresses challenges in panoramic depth estimation by using a data curation engine and SphereViT to handle spherical distortions, achieving state-of-the-art performance.  					AI-generated summary 				 Panorama has a full...
[01.10.2025 17:10] ********************************************************************************
[01.10.2025 17:10] Abstract 19. DeepScientist autonomously conducts scientific discovery through Bayesian Optimization, surpassing human state-of-the-art methods on multiple AI tasks.  					AI-generated summary 				 While previous AI Scientist systems can generate novel findings, they often lack the focus to produce scientifically...
[01.10.2025 17:10] ********************************************************************************
[01.10.2025 17:10] Abstract 20. Mem-alpha, a reinforcement learning framework, enhances memory management in large language models through interaction and feedback, improving performance and generalization in long-term information understanding.  					AI-generated summary 				 Large language model (LLM) agents are constrained by l...
[01.10.2025 17:10] ********************************************************************************
[01.10.2025 17:10] Abstract 21. Operational safety, measured by OffTopicEval, is a critical issue for LLMs, with most models falling short, but prompt-based steering methods show promise in improving out-of-distribution refusal.  					AI-generated summary 				 Large Language Model (LLM) safety is one of the most pressing challenge...
[01.10.2025 17:10] ********************************************************************************
[01.10.2025 17:10] Abstract 22. Online alignment methods like GRPO outperform offline methods like DPO due to better approximation of human-perceived probability distributions, and introducing perceptual biases into offline training can achieve similar performance.  					AI-generated summary 				 Online alignment (e.g., GRPO) is g...
[01.10.2025 17:10] ********************************************************************************
[01.10.2025 17:10] Abstract 23. A novel PSRL framework (AttnRL) enhances exploration efficiency in reasoning models by branching from high attention positions and using an adaptive sampling strategy, outperforming prior methods in mathematical reasoning benchmarks.  					AI-generated summary 				 Reinforcement Learning (RL) has sh...
[01.10.2025 17:10] ********************************************************************************
[01.10.2025 17:10] Abstract 24. Research explores collaboration in open large language models, identifying diverse motivations and organizational models among developers from various sectors.  					AI-generated summary 				 The proliferation of open large language models (LLMs) is fostering a vibrant ecosystem of research and inno...
[01.10.2025 17:10] ********************************************************************************
[01.10.2025 17:10] Abstract 25. VisualOverload is a VQA benchmark that challenges models with simple vision tasks in densely populated scenes, revealing gaps in current VLMs' performance and offering insights into their failure modes.  					AI-generated summary 				 Is basic visual understanding really solved in state-of-the-art V...
[01.10.2025 17:10] ********************************************************************************
[01.10.2025 17:10] Abstract 26. Theoretical analysis of reinforcement learning methods in enhancing LLM planning reveals that while RL improves generalization through exploration, policy gradient suffers from diversity collapse, whereas Q-learning maintains diversity and requires careful reward design.  					AI-generated summary 	...
[01.10.2025 17:10] ********************************************************************************
[01.10.2025 17:10] Abstract 27. A unified Regression Language Model (RLM) predicts numeric outcomes of code executions, including memory footprint, latency, and neural network performance, across multiple languages and hardware platforms.  					AI-generated summary 				 We study code-to-metric regression: predicting numeric outcom...
[01.10.2025 17:10] ********************************************************************************
[01.10.2025 17:10] Abstract 28. Muon optimizer outperforms Adam in training LLMs by effectively optimizing associative memory parameters and balancing learning across classes in heavy-tailed data.  					AI-generated summary 				 The Muon optimizer is consistently faster than Adam in training Large Language Models (LLMs), yet the m...
[01.10.2025 17:10] ********************************************************************************
[01.10.2025 17:10] Abstract 29. InfoAgent, a deep research agent using a custom data synthesis pipeline and search infrastructure, outperforms existing agents by improving tool use and reasoning.  					AI-generated summary 				 Building Large Language Model agents that expand their capabilities by interacting with external tools r...
[01.10.2025 17:10] ********************************************************************************
[01.10.2025 17:10] Abstract 30. TTT3R, a test-time training intervention, enhances length generalization in 3D reconstruction by dynamically adjusting memory updates based on alignment confidence, improving global pose estimation and processing efficiency.  					AI-generated summary 				 Modern Recurrent Neural Networks have becom...
[01.10.2025 17:10] ********************************************************************************
[01.10.2025 17:10] Abstract 31. VERA is a benchmark for evaluating reasoning ability in voice-interactive systems, revealing significant performance gaps compared to text models and highlighting challenges in real-time interaction.  					AI-generated summary 				 We present Voice Evaluation of Reasoning Ability (VERA), a benchmark...
[01.10.2025 17:10] ********************************************************************************
[01.10.2025 17:10] Abstract 32. Ferret-UI Lite, a compact end-to-end GUI agent, achieves competitive performance across diverse platforms using chain-of-thought reasoning, visual tool-use, and reinforcement learning.  					AI-generated summary 				 Developing autonomous agents that effectively interact with Graphic User Interfaces...
[01.10.2025 17:10] ********************************************************************************
[01.10.2025 17:10] Abstract 33. ROSA, a lightweight algorithm, enhances multi-turn interactions in LLMs by adapting to user feedback in real-time, improving both task effectiveness and efficiency.  					AI-generated summary 				 Large Language Models (LLMs) employ multi-turn interaction as a fundamental paradigm for completing com...
[01.10.2025 17:10] ********************************************************************************
[01.10.2025 17:10] Abstract 34. A novel technique for predicting APIs and generating code in real-time using a compact reranker outperforms larger models with reduced latency, addressing API leaks and unclear usage intent in enterprise code.  					AI-generated summary 				 Current search techniques are limited to standard RAG quer...
[01.10.2025 17:10] ********************************************************************************
[01.10.2025 17:10] Abstract 35. TAU, a benchmark of culturally specific Taiwanese soundmarks, reveals that state-of-the-art large audio-language models underperform compared to local humans, highlighting the need for localized evaluations.  					AI-generated summary 				 Large audio-language models are advancing rapidly, yet most ...
[01.10.2025 17:10] ********************************************************************************
[01.10.2025 17:10] Abstract 36. EntroPE, a temporally informed framework using entropy-guided dynamic patching, enhances time series forecasting by preserving temporal coherence and improving accuracy and efficiency.  					AI-generated summary 				 Transformer-based models have significantly advanced time series forecasting, with ...
[01.10.2025 17:10] ********************************************************************************
[01.10.2025 17:10] Abstract 37. A multilingual document reranker using causal self-attention achieves state-of-the-art performance with a compact architecture.  					AI-generated summary 				 jina-reranker-v3 is a 0.6B parameter multilingual document reranker that introduces a novel last but not late interaction. Unlike late inter...
[01.10.2025 17:10] ********************************************************************************
[01.10.2025 17:10] Abstract 38. Graph Neural Network regression models estimate entity-level knowledgeability in Large Language Models to improve active labeling and multi-hop reasoning.  					AI-generated summary 				 Large Language Models (LLMs) have been increasingly studied as neural knowledge bases for supporting knowledge-in...
[01.10.2025 17:10] ********************************************************************************
[01.10.2025 17:10] Abstract 39. Dual aDaptive Cache (d¬≤Cache) accelerates diffusion-based large language model inference by selectively updating key-value states and enabling quasi left-to-right generation, improving both speed and quality.  					AI-generated summary 				 Diffusion-based large language models (dLLMs), despite thei...
[01.10.2025 17:10] ********************************************************************************
[01.10.2025 17:10] Abstract 40. Research reveals significant discrepancies between reported and effective context window sizes in large language models, impacting accuracy and hallucination rates across different problem types.  					AI-generated summary 				 Large language model (LLM) providers boast big numbers for maximum conte...
[01.10.2025 17:10] ********************************************************************************
[01.10.2025 17:10] Abstract 41. SAGANet, a multimodal generative model, enhances audio generation by using object-level segmentation maps, improving control and fidelity in professional Foley workflows.  					AI-generated summary 				 Existing multimodal audio generation models often lack precise user control, which limits their a...
[01.10.2025 17:10] ********************************************************************************
[01.10.2025 17:10] Abstract 42. CritPt, a benchmark for evaluating LLMs on research-level physics tasks, reveals significant gaps between current model capabilities and the demands of physics research.  					AI-generated summary 				 While large language models (LLMs) with reasoning capabilities are progressing rapidly on high-sch...
[01.10.2025 17:10] ********************************************************************************
[01.10.2025 17:10] Abstract 43. LayerD decomposes raster images into editable layers using iterative extraction and refinement, outperforming existing methods and enabling use with advanced image generators.  					AI-generated summary 				 Designers craft and edit graphic designs in a layer representation, but layer-based editing ...
[01.10.2025 17:10] ********************************************************************************
[01.10.2025 17:10] Abstract 44. MANI-Pure, a magnitude-adaptive purification framework using diffusion models, effectively suppresses high-frequency adversarial perturbations while preserving low-frequency content, enhancing robust accuracy.  					AI-generated summary 				 Adversarial purification with diffusion models has emerged...
[01.10.2025 17:10] ********************************************************************************
[01.10.2025 17:10] Abstract 45. A timeline of the evolution of deep residual learning, a key advancement in neural network architecture.  					AI-generated summary 				 Modern AI is based on deep artificial neural networks (NNs). As of 2025, the most cited scientific article of the 21st century is an NN paper on deep residual lear...
[01.10.2025 17:10] ********************************************************************************
[01.10.2025 17:10] Abstract 46. TimeTic is a transferability estimation framework that predicts the performance of time series foundation models after fine-tuning on unseen datasets, using tabular foundation models and entropy evolution for model characterization.  					AI-generated summary 				 Time series foundation models (TSFM...
[01.10.2025 17:10] ********************************************************************************
[01.10.2025 17:10] Abstract 47. A new benchmark, BUILD-BENCH, and an LLM-based agent, OSS-BUILD-AGENT, address the complexities of compiling diverse open-source software projects.  					AI-generated summary 				 Automatically compiling open-source software (OSS) projects is a vital, labor-intensive, and complex task, which makes i...
[01.10.2025 17:10] ********************************************************************************
[01.10.2025 17:10] Abstract 48. The Bias-Inversion Rewriting Attack (BIRA) effectively evades watermarking in large language models by suppressing specific logits, highlighting a significant vulnerability in watermarking techniques.  					AI-generated summary 				 Watermarking for large language models (LLMs) embeds a statistical ...
[01.10.2025 17:10] ********************************************************************************
[01.10.2025 17:10] Abstract 49. Stable Cinemetrics introduces a structured evaluation framework for professional video generation, using taxonomies to assess models across specific filmmaking controls.  					AI-generated summary 				 Recent advances in video generation have enabled high-fidelity video synthesis from user provided ...
[01.10.2025 17:10] ********************************************************************************
[01.10.2025 17:10] Abstract 50. ProfVLM, a compact vision-language model, uses generative reasoning to estimate skill proficiency and generate expert feedback from multi-view videos, outperforming existing methods with fewer parameters and faster training.  					AI-generated summary 				 Existing approaches to skill proficiency es...
[01.10.2025 17:10] ********************************************************************************
[01.10.2025 17:10] Abstract 51. Mid-training with action abstractions enhances reinforcement learning in large language models, improving performance and convergence in code generation tasks.  					AI-generated summary 				 Large language models excel with reinforcement learning (RL), but fully unlocking this potential requires a ...
[01.10.2025 17:10] ********************************************************************************
[01.10.2025 17:10] Abstract 52. NuRL, a nudging method using self-generated hints, enhances the upper limit of LLM reasoning in online reinforcement learning by enabling learning from previously unsolvable problems.  					AI-generated summary 				 Current online reinforcement learning (RL) algorithms like GRPO share a key limitati...
[01.10.2025 17:10] ********************************************************************************
[01.10.2025 17:10] Abstract 53. Test-time training (TTT) improves performance by allowing foundation models to specialize on test tasks, reducing in-distribution test error through a mechanism of focusing on relevant concepts.  					AI-generated summary 				 Recent empirical studies have explored the idea of continuing to train a ...
[01.10.2025 17:10] ********************************************************************************
[01.10.2025 17:10] Abstract 54. The Convolutional Set Transformer (CST) processes image sets directly, combining feature extraction and contextual modeling for improved performance in set classification and anomaly detection, with compatibility for CNN explainability methods.  					AI-generated summary 				 We introduce the Convol...
[01.10.2025 17:10] ********************************************************************************
[01.10.2025 17:10] Abstract 55. A geometry-aware two-stage framework for intelligent image editing effectively removes objects and their causal visual artifacts by decoupling geometry removal and appearance rendering.  					AI-generated summary 				 Towards intelligent image editing, object removal should eliminate both the target...
[01.10.2025 17:10] Read previous papers.
[01.10.2025 17:10] Generating reviews via LLM API.
[01.10.2025 17:10] Using data from previous issue: {"categories": ["#agi", "#agents", "#survey", "#benchmark"], "emoji": "üß™", "ru": {"title": "MCPMark: –±–µ–Ω—á–º–∞—Ä–∫, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∫–∞–∑–∞–ª —Å–ª–∞–±–æ—Å—Ç—å LLM –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö —Ä–∞–±–æ—á–∏—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö", "desc": "MCPMark ‚Äî —ç—Ç–æ –Ω–æ–≤—ã–π –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è MCP (Model Context Protocol) –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö —Ä–∞–±–æ—á–∏—Ö –ø—Ä–æ—Ü–µ—Å
[01.10.2025 17:10] Using data from previous issue: {"categories": ["#graphs", "#multimodal", "#architecture", "#reasoning", "#interpretability"], "emoji": "üß†", "ru": {"title": "BDH: –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–∞—è –º–æ—â—å –±–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–∏ –≤–¥–æ—Ö–Ω–æ–≤–ª—ë–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π", "desc": "BDH ‚Äî —ç—Ç–æ –Ω–æ–≤–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ LLM, –≤–¥–æ—Ö–Ω–æ–≤–ª—ë–Ω–Ω–∞—è –±–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–º–∏ —Å–µ—Ç—è–º–∏, –∫–æ—Ç–æ—Ä–∞—è —Å–æ—á–µ—Ç–∞–µ—Ç –≤ —Å–µ–±–µ –º–∞—Å—à—Ç–∞–±–Ω–æ
[01.10.2025 17:10] Using data from previous issue: {"categories": ["#multimodal", "#reasoning", "#optimization", "#games", "#cv", "#training", "#rl"], "emoji": "üéÆ", "ru": {"title": "–°–∞–º–æ–æ–±—É—á–µ–Ω–∏–µ VLM —á–µ—Ä–µ–∑ –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –∏–≥—Ä—ã –±–µ–∑ —Ä–∞–∑–º–µ—Ç–∫–∏", "desc": "Vision-Zero ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Å–∞–º–æ—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–∏—è vision-language –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ —Å–æ—Ä–µ–≤–Ω–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ –≤–∏–∑—É–∞
[01.10.2025 17:10] Using data from previous issue: {"categories": ["#optimization", "#benchmark", "#data", "#training"], "emoji": "‚úÇÔ∏è", "ru": {"title": "–£–º–Ω–∞—è –æ–±—Ä–µ–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö: –∫–∞–∫ –æ–±—É—á–∞—Ç—å LLM —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Q-Tuning ‚Äî –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ supervised fine-tuning –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ
[01.10.2025 17:10] Using data from previous issue: {"categories": ["#rlhf", "#hallucinations", "#reasoning", "#optimization", "#training", "#rl"], "emoji": "üéØ", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ LLM –≥–æ–≤–æ—Ä–∏—Ç—å –ø—Ä–∞–≤–¥—É —á–µ—Ä–µ–∑ –≤–æ–∑–¥–µ—Ä–∂–∞–Ω–∏–µ –æ—Ç –æ—Ç–≤–µ—Ç–∞", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω TruthRL ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ reinforcement learning –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è –ø—Ä–∞–≤–¥–∏–≤–æ—Å—Ç–∏ –±–æ–ª—å—à
[01.10.2025 17:10] Using data from previous issue: {"categories": ["#games", "#transfer_learning", "#agents", "#benchmark", "#multimodal"], "emoji": "üåä", "ru": {"title": "OceanGym: –¢–µ—Å—Ç–æ–≤–∞—è –ø–ª–æ—â–∞–¥–∫–∞ –¥–ª—è AI-–∞–≥–µ–Ω—Ç–æ–≤ –≤ –Ω–µ–∏–∑–≤–µ–¥–∞–Ω–Ω—ã—Ö –≥–ª—É–±–∏–Ω–∞—Ö –æ–∫–µ–∞–Ω–∞", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω OceanGym ‚Äî –ø–µ—Ä–≤—ã–π –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –ø–æ–¥–≤–æ–¥–Ω—ã—Ö embodied-–∞–≥–µ–Ω—Ç–æ–≤, —Ä–∞–±
[01.10.2025 17:10] Using data from previous issue: {"categories": ["#multimodal", "#reasoning", "#benchmark", "#alignment", "#dataset", "#transfer_learning"], "emoji": "üëÅÔ∏è", "ru": {"title": "–í–∏–∑—É–∞–ª—å–Ω—ã–µ –ø—Ä–∏–æ—Ä—ã –∏–∑ —Ç–µ–∫—Å—Ç–∞: –∫–∞–∫ LLM —É—á–∞—Ç—Å—è –≤–∏–¥–µ—Ç—å –±–µ–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (LLM) –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω–æ —Ä–∞–∑–≤–∏–≤–∞—é—Ç –≤–∏
[01.10.2025 17:10] Using data from previous issue: {"categories": ["#rl", "#benchmark", "#cv", "#multimodal", "#reasoning"], "emoji": "üëÅÔ∏è", "ru": {"title": "–ù–µ –∑–∞–±—ã–≤–∞–π —Å–º–æ—Ç—Ä–µ—Ç—å: –∫–∞–∫ –Ω–∞—É—á–∏—Ç—å AI —Ä–∞—Å—Å—É–∂–¥–∞—Ç—å –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –≤—ã—è–≤–ª—è–µ—Ç –ø—Ä–æ–±–ª–µ–º—É \"–≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –∑–∞–±—ã–≤–∞–Ω–∏—è\" –≤ Vision-Language Models: –ø—Ä–∏ –¥–ª–∏—Ç–µ–ª—å–Ω–æ–º —Ä–∞—Å—Å
[01.10.2025 17:10] Using data from previous issue: {"categories": ["#training", "#long_context", "#rl", "#rlhf", "#optimization"], "emoji": "‚úÇÔ∏è", "ru": {"title": "–û—Ç—Å–µ—á–µ–Ω–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –º–µ—Ç–æ–¥ TFPI –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –∏ –≤–µ—Ä–∏—Ñ–∏—Ü–∏—Ä—É–µ–º—ã–º –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ–º (RLVR).
[01.10.2025 17:10] Using data from previous issue: {"categories": ["#inference", "#video", "#optimization", "#diffusion", "#architecture", "#training"], "emoji": "‚ö°", "ru": {"title": "–£—Å–∫–æ—Ä–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ —á–µ—Ä–µ–∑ –≥–ª—É–±–æ–∫–æ–µ —Å–∂–∞—Ç–∏–µ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞", "desc": "DC-VideoGen ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ, –∫–æ—Ç–æ—Ä—ã–π –∞–¥–∞–ø—Ç–∏—Ä—É–µ—Ç –ø—Ä–µ–¥–æ–±
[01.10.2025 17:10] Using data from previous issue: {"categories": ["#hallucinations", "#data", "#ethics", "#dataset", "#interpretability", "#architecture", "#multimodal"], "emoji": "‚öñÔ∏è", "ru": {"title": "–î–µ—Ç–µ–∫—Ç–æ—Ä –æ—Ü–µ–Ω–æ–∫ –æ—Ç LLM: –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Å—É–∂–¥–µ–Ω–∏–π –ø–æ –±–∞–ª–ª–∞–º", "desc": "–í —Å—Ç–∞—Ç—å–µ —Ñ–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç—Å—è –∑–∞–¥–∞—á–∞ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è —Å—É–∂–¥–µ–Ω–∏–π, —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö
[01.10.2025 17:10] Using data from previous issue: {"categories": ["#interpretability", "#optimization", "#reasoning", "#architecture", "#training"], "emoji": "üß†", "ru": {"title": "–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≥–æ–ª–æ–≤—ã –≤–Ω–∏–º–∞–Ω–∏—è: –∫–ª—é—á –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º LLM", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ post-training —Ç–µ—Ö–Ω–∏–∫–∏, —Ç–∞–∫–∏–µ –∫–∞–∫ supervised fine-tuning –∏ reinforcemen
[01.10.2025 17:10] Using data from previous issue: {"categories": ["#reasoning", "#agents", "#benchmark", "#games", "#survey"], "emoji": "üß≠", "ru": {"title": "VitaBench: –∂–∏–∑–Ω–µ–Ω–Ω—ã–π —ç–∫–∑–∞–º–µ–Ω –¥–ª—è AI-–∞–≥–µ–Ω—Ç–æ–≤ –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö", "desc": "VitaBench ‚Äî —ç—Ç–æ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ LLM-–∞–≥–µ–Ω—Ç–æ–≤ –≤ —Å–ª–æ–∂–Ω—ã—Ö –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö, –ø—Ä–∏–±–ª–∏–∂–µ–Ω–Ω—ã—Ö –∫ —Ä–µ–∞–ª—å–Ω–æ–π –∂–∏–∑–Ω–∏
[01.10.2025 17:10] Using data from previous issue: {"categories": ["#inference", "#optimization", "#benchmark", "#diffusion", "#open_source", "#training"], "emoji": "‚ö°", "ru": {"title": "–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö LLM —Å 10-–∫—Ä–∞—Ç–Ω—ã–º —É—Å–∫–æ—Ä–µ–Ω–∏–µ–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç dParallel ‚Äî –º–µ—Ç–æ–¥ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –≤ –¥–∏—Ñ
[01.10.2025 17:10] Using data from previous issue: {"categories": ["#alignment", "#ethics", "#video", "#multimodal", "#benchmark", "#dataset", "#interpretability"], "emoji": "üîç", "ru": {"title": "–£—á–∏–º –º–æ–¥–µ–ª–∏ –Ω–∞—Ö–æ–¥–∏—Ç—å —Å–ª–µ–¥—ã –¥–∏–ø—Ñ–µ–π–∫–æ–≤ –≥–ª–∞–∑–∞–º–∏ —á–µ–ª–æ–≤–µ–∫–∞", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Å–æ–∑–¥–∞–ª–∏ –¥–∞—Ç–∞—Å–µ—Ç DeeptraceReward —Å 4.3 —Ç—ã—Å—è—á–∞–º–∏ –¥–µ—Ç–∞–ª—å–Ω—ã—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –∏—Å–∫—É—Å—Å—Ç–≤–µ
[01.10.2025 17:10] Using data from previous issue: {"categories": ["#multimodal", "#alignment", "#cv", "#diffusion"], "emoji": "üéØ", "ru": {"title": "–ù–µ—è–≤–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ —Ç–µ–∫—Å—Ç–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ Implicit Multimodal Guidance (IMG) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –º–µ–∂–¥—É —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–º
[01.10.2025 17:10] Using data from previous issue: {"categories": ["#transfer_learning", "#video", "#rag", "#diffusion", "#multimodal"], "emoji": "üé¨", "ru": {"title": "–£–ª—É—á—à–µ–Ω–∏–µ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ—Å—Ç–∏ –¥–≤–∏–∂–µ–Ω–∏—è –≤ –≤–∏–¥–µ–æ —á–µ—Ä–µ–∑ retrieval motion-–ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç MotionRAG ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç retrieval-augme
[01.10.2025 17:10] Using data from previous issue: {"categories": ["#video", "#benchmark", "#audio", "#inference"], "emoji": "üê¨", "ru": {"title": "Dolphin: –ë—ã—Å—Ç—Ä–∞—è –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è —Å–µ–ø–∞—Ä–∞—Ü–∏—è —Ä–µ—á–∏ —Å –ø–æ–º–æ—â—å—é –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –ø–æ–¥—Å–∫–∞–∑–æ–∫", "desc": "Dolphin - —ç—Ç–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥ –∞—É–¥–∏–æ-–≤–∏–∑—É–∞–ª—å–Ω–æ–π —Å–µ–ø–∞—Ä–∞—Ü–∏–∏ —Ä–µ—á–∏ (AVSS), –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –ø–æ–¥—Å–∫–∞–∑–∫–∏ –¥–ª—è 
[01.10.2025 17:10] Using data from previous issue: {"categories": ["#3d", "#optimization", "#data", "#dataset", "#benchmark", "#open_source"], "emoji": "üåê", "ru": {"title": "–û—Ü–µ–Ω–∫–∞ –≥–ª—É–±–∏–Ω—ã –ø–∞–Ω–æ—Ä–∞–º –±–µ–∑ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –∏ —Å –Ω—É–ª–µ–≤—ã–º –æ–±—É—á–µ–Ω–∏–µ–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç DA¬≤ ‚Äî —Å–∏—Å—Ç–µ–º—É –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≥–ª—É–±–∏–Ω—ã –Ω–∞ –ø–∞–Ω–æ—Ä–∞–º–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö —Å –ø–æ–ª–Ω—ã–º –æ–±–∑–æ—Ä–æ–º 360√ó180 
[01.10.2025 17:10] Using data from previous issue: {"categories": ["#science", "#agents", "#open_source", "#rl"], "emoji": "üî¨", "ru": {"title": "AI-—É—á—ë–Ω—ã–π –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–µ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è —á–µ—Ä–µ–∑ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–µ –Ω–∞—É—á–Ω–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ", "desc": "DeepScientist - —ç—Ç–æ —Å–∏—Å—Ç–µ–º–∞, –∫–æ—Ç–æ—Ä–∞—è –∞–≤—Ç–æ–Ω–æ–º–Ω–æ –ø—Ä–æ–≤–æ–¥–∏—Ç –Ω–∞—É—á–Ω—ã–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è, –∏—Å–ø–æ–ª—å–∑—É—è –±–∞–π–µ—Å–æ–≤—Å–∫—É—é –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é
[01.10.2025 17:10] Using data from previous issue: {"categories": ["#rl", "#optimization", "#agents", "#long_context", "#dataset", "#training"], "emoji": "üß†", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ LLM —É–ø—Ä–∞–≤–ª—è—Ç—å –ø–∞–º—è—Ç—å—é —á–µ—Ä–µ–∑ reinforcement learning", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Mem-alpha ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ reinforcement learning –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø–∞–º—è
[01.10.2025 17:10] Using data from previous issue: {"categories": ["#security", "#alignment", "#training", "#ethics", "#agents", "#benchmark"], "emoji": "üö¶", "ru": {"title": "–û–ø–µ—Ä–∞—Ü–∏–æ–Ω–Ω–∞—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å: –Ω–∞—É—á–∏—Ç—å LLM –æ—Ç–∫–∞–∑—ã–≤–∞—Ç—å—Å—è –æ—Ç –Ω–µ–ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ –∫–æ–Ω—Ü–µ–ø—Ü–∏—é –æ–ø–µ—Ä–∞—Ü–∏–æ–Ω–Ω–æ–π –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ ‚Äî —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ LLM –∫–æ—Ä—Ä–µ–∫—Ç–Ω
[01.10.2025 17:10] Using data from previous issue: {"categories": ["#alignment", "#training", "#rl", "#rlhf"], "emoji": "üß†", "ru": {"title": "–ß–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–µ –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –∫–∞–∫ –∫–ª—é—á –∫ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–º—É –æ–±—É—á–µ–Ω–∏—é LLM", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –æ–±—ä—è—Å–Ω—è—é—Ç, –ø–æ—á–µ–º—É –æ–Ω–ª–∞–π–Ω –º–µ—Ç–æ–¥—ã –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è (GRPO) —Ä–∞–±–æ—Ç–∞—é—Ç –ª—É—á—à–µ –æ—Ñ—Ñ–ª–∞–π–Ω –º–µ—Ç–æ–¥–æ–≤ (DPO), –æ–ø–∏—Ä–∞—è—Å—å –Ω–∞ —Ç–µ–æ—Ä–∏—é –ø
[01.10.2025 17:10] Using data from previous issue: {"categories": ["#math", "#optimization", "#reasoning", "#training", "#rl"], "emoji": "üå≥", "ru": {"title": "–£–º–Ω–æ–µ –≤–µ—Ç–≤–ª–µ–Ω–∏–µ —á–µ—Ä–µ–∑ –≤–Ω–∏–º–∞–Ω–∏–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ AttnRL –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è Process-Supervised Reinforcement Learning –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ LLM –º–∞—Ç–µ–º–∞
[01.10.2025 17:10] Using data from previous issue: {"categories": ["#dataset", "#open_source", "#benchmark", "#multilingual"], "emoji": "ü§ù", "ru": {"title": "–≠–∫–æ—Å–∏—Å—Ç–µ–º–∞ –æ—Ç–∫—Ä—ã—Ç—ã—Ö LLM: –º–Ω–æ–≥–æ–æ–±—Ä–∞–∑–∏–µ –º–æ–¥–µ–ª–µ–π —Å–æ—Ç—Ä—É–¥–Ω–∏—á–µ—Å—Ç–≤–∞ –∏ –º–æ—Ç–∏–≤–∞—Ü–∏–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∏–∑—É—á–∞–µ—Ç –º–µ—Ç–æ–¥—ã —Å–æ–≤–º–µ—Å—Ç–Ω–æ–π —Ä–∞–±–æ—Ç—ã –ø—Ä–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ –æ—Ç–∫—Ä—ã—Ç—ã—Ö LLM –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–Ω—Ç–µ—Ä–≤—å—é —Å —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∞–º–∏
[01.10.2025 17:10] Using data from previous issue: {"categories": ["#interpretability", "#reasoning", "#benchmark", "#dataset", "#cv"], "emoji": "üé®", "ru": {"title": "VisualOverload: –∫–æ–≥–¥–∞ —Å–ª–æ–∂–Ω—ã–µ —Å—Ü–µ–Ω—ã —Å—Ç–∞–≤—è—Ç VLM –≤ —Ç—É–ø–∏–∫", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ –±–µ–Ω—á–º–∞—Ä–∫ VisualOverload –¥–ª—è –æ—Ü–µ–Ω–∫–∏ vision-language –º–æ–¥–µ–ª–µ–π –Ω–∞ –∑–∞–¥–∞—á–∞—Ö –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è 
[01.10.2025 17:10] Using data from previous issue: {"categories": ["#optimization", "#reasoning", "#benchmark", "#training", "#rl"], "emoji": "üó∫Ô∏è", "ru": {"title": "–ü–æ—á–µ–º—É Q-learning –ª—É—á—à–µ policy gradient –¥–ª—è –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –≤ LLM", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç, –∫–∞–∫ –º–µ—Ç–æ–¥—ã reinforcement learning —É–ª—É—á—à–∞—é—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ LLM –∫ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—é
[01.10.2025 17:10] Using data from previous issue: {"categories": ["#games", "#data", "#optimization", "#training", "#dataset", "#multilingual", "#small_models"], "emoji": "üìä", "ru": {"title": "–ï–¥–∏–Ω–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∫–æ–¥–∞", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –µ–¥–∏–Ω–∞—è Regression Language Model (RLM), –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ
[01.10.2025 17:10] Using data from previous issue: {"categories": ["#math", "#training", "#optimization"], "emoji": "üöÄ", "ru": {"title": "Muon: –Ω–æ–≤—ã–π –ª–∏–¥–µ—Ä –≤ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ LLM", "desc": "–í —Å—Ç–∞—Ç—å–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä Muon, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç Adam –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ Muon –ª—É—á—à–µ —Å–ø—Ä–∞–≤–ª—è
[01.10.2025 17:10] Using data from previous issue: {"categories": ["#reasoning", "#rl", "#agents", "#training", "#open_source", "#data"], "emoji": "üîç", "ru": {"title": "InfoAgent: –ê–≥–µ–Ω—Ç –≥–ª—É–±–æ–∫–æ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è —Å –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–º –ø–æ–∏—Å–∫–æ–º", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω InfoAgent ‚Äî –∞–≥–µ–Ω—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM –¥–ª—è –≥–ª—É–±–æ–∫–æ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ
[01.10.2025 17:10] Using data from previous issue: {"categories": ["#3d", "#training", "#optimization", "#long_context"], "emoji": "üîÑ", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –ª–µ—Ç—É –¥–ª—è 3D-—Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –±–µ–∑ –≥—Ä–∞–Ω–∏—Ü", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç TTT3R ‚Äî –º–µ—Ç–æ–¥ test-time training –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è 3D-—Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ —Å –ø–æ–º–æ—â—å—é —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω—ã—Ö –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π. –ü—Ä–æ–±–ª–µ–º–∞
[01.10.2025 17:10] Using data from previous issue: {"categories": ["#multimodal", "#reasoning", "#benchmark", "#architecture", "#long_context"], "emoji": "üé§", "ru": {"title": "–ì–æ–ª–æ—Å–æ–≤—ã–µ AI-–ø–æ–º–æ—â–Ω–∏–∫–∏ —Å–∏–ª—å–Ω–æ –æ—Ç—Å—Ç–∞—é—Ç –≤ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —Ä–∞—Å—Å—É–∂–¥–∞—Ç—å", "desc": "VERA ‚Äî —ç—Ç–æ –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é –≤ –≥–æ–ª–æ—Å–æ–≤—ã—Ö –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã—Ö —Å–∏—Å—Ç–µ–º–∞—Ö –≤ —É—Å–ª–æ–≤–∏—è
[01.10.2025 17:10] Using data from previous issue: {"categories": ["#inference", "#agents", "#reasoning", "#small_models", "#synthetic", "#data", "#dataset", "#rl"], "emoji": "üì±", "ru": {"title": "–ö–æ–º–ø–∞–∫—Ç–Ω—ã–π AI-–∞–≥–µ–Ω—Ç –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞–º–∏ –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ", "desc": "Ferret-UI Lite ‚Äî —ç—Ç–æ –∫–æ–º–ø–∞–∫—Ç–Ω–∞—è –º–æ–¥–µ–ª—å —Ä–∞–∑–º–µ—Ä–æ–º 3B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –≤–∑–∞
[01.10.2025 17:10] Using data from previous issue: {"categories": ["#alignment", "#training", "#benchmark", "#rlhf", "#optimization"], "emoji": "üéØ", "ru": {"title": "–ê–¥–∞–ø—Ç–∞—Ü–∏—è AI –∫ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –ø—Ä—è–º–æ –≤–æ –≤—Ä–µ–º—è –¥–∏–∞–ª–æ–≥–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç ROSA ‚Äî –ª—ë–≥–∫–∏–π –∞–ª–≥–æ—Ä–∏—Ç–º –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º–Ω–æ–≥–æ—Ö–æ–¥–æ–≤—ã—Ö –¥–∏–∞–ª–æ–≥–æ–≤ —Å LLM. –ü—Ä–æ–±–ª–µ–º–∞ –≤ —Ç–æ–º, —á—Ç–æ –º–æ–¥
[01.10.2025 17:10] Using data from previous issue: {"categories": ["#data", "#optimization", "#training", "#leakage", "#dataset", "#synthetic", "#rl", "#rag"], "emoji": "üîç", "ru": {"title": "–ö–æ–º–ø–∞–∫—Ç–Ω—ã–π reranker –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è API –∏ –±—ã—Å—Ç—Ä–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è API –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞ –≤ —Ä–µ–∞–ª—å–Ω–æ
[01.10.2025 17:10] Using data from previous issue: {"categories": ["#alignment", "#ethics", "#audio", "#benchmark", "#multimodal"], "emoji": "üîî", "ru": {"title": "–ö—É–ª—å—Ç—É—Ä–Ω—ã–µ –∑–≤—É–∫–∏ –∫–∞–∫ —Ç–µ—Å—Ç –¥–ª—è –∞—É–¥–∏–æ-AI: –º–æ–¥–µ–ª–∏ –Ω–µ —Å–ª—ã—à–∞—Ç –ª–æ–∫–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Å–æ–∑–¥–∞–ª–∏ –±–µ–Ω—á–º–∞—Ä–∫ TAU –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö –∞—É–¥–∏–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞—Ç—å
[01.10.2025 17:10] Using data from previous issue: {"categories": ["#training", "#long_context", "#benchmark", "#optimization", "#data", "#architecture"], "emoji": "üîÄ", "ru": {"title": "–î–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤ –ø–æ —ç–Ω—Ç—Ä–æ–ø–∏–∏ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç EntroPE ‚Äî –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—é –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ
[01.10.2025 17:10] Using data from previous issue: {"categories": ["#machine_translation", "#multilingual", "#architecture"], "emoji": "üîÑ", "ru": {"title": "–ö–æ–º–ø–∞–∫—Ç–Ω—ã–π —Ä–∞–Ω–∂–∏—Ä–æ–≤—â–∏–∫ —Å –ø–µ—Ä–µ–∫—Ä—ë—Å—Ç–Ω—ã–º –≤–Ω–∏–º–∞–Ω–∏–µ–º –ø–æ–±–µ–∂–¥–∞–µ—Ç –≥—Ä–æ–º–æ–∑–¥–∫–∏–µ –º–æ–¥–µ–ª–∏", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–∞—è –º–æ–¥–µ–ª—å jina-reranker-v3 —Å 0.6 –º–∏–ª–ª–∏–∞—Ä–¥–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è –ø–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è –¥–æ–∫—É–º–µ–Ω
[01.10.2025 17:10] Using data from previous issue: {"categories": ["#data", "#reasoning", "#training", "#dataset", "#agents", "#graphs", "#multimodal"], "emoji": "üï∏Ô∏è", "ru": {"title": "–ì—Ä–∞—Ñ –∑–Ω–∞–Ω–∏–π LLM: —Å–æ—Å–µ–¥–∏ –∑–Ω–∞—é—Ç –æ–¥–∏–Ω–∞–∫–æ–≤–æ", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –∏–∑—É—á–∏–ª–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—É—é –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—é –∑–Ω–∞–Ω–∏–π –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö, –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–≤ –∏—Ö –≤ –≤–∏–¥–µ –≥—Ä–∞—Ñ–∞. –û–∫–∞–∑–∞–ª
[01.10.2025 17:10] Using data from previous issue: {"categories": ["#training", "#inference", "#optimization", "#diffusion", "#architecture"], "emoji": "‚ö°", "ru": {"title": "–£—Å–∫–æ—Ä–µ–Ω–∏–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç d¬≤Cache ‚Äî –º–µ—Ç–æ–¥ —É—Å–∫–æ—Ä–µ–Ω–∏—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏
[01.10.2025 17:10] Using data from previous issue: {"categories": ["#long_context", "#training", "#hallucinations", "#benchmark", "#data"], "emoji": "üìâ", "ru": {"title": "–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–∫–Ω–æ LLM: –æ–±–µ—â–∞–Ω–∏—è vs —Ä–µ–∞–ª—å–Ω–æ—Å—Ç—å", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏—è –º–µ–∂–¥—É –∑–∞—è–≤–ª–µ–Ω–Ω—ã–º –∏ —Ä–µ–∞–ª—å–Ω—ã–º —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–º —Ä–∞–∑–º–µ—Ä–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–≥–æ –æ–∫–Ω–∞ –≤ –±–æ–ª—å—à–∏
[01.10.2025 17:10] Using data from previous issue: {"categories": ["#audio", "#synthetic", "#games", "#benchmark", "#dataset", "#multimodal"], "emoji": "üé¨", "ru": {"title": "–¢–æ—á–Ω—ã–π –∫–æ–Ω—Ç—Ä–æ–ª—å –∑–≤—É–∫–∞ —á–µ—Ä–µ–∑ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—é –æ–±—ä–µ–∫—Ç–æ–≤", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ SAGANet ‚Äî –Ω–æ–≤–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∑–≤—É–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–∏–¥–µ–æ —Å –∏—Å–ø–æ–ª—å
[01.10.2025 17:10] Using data from previous issue: {"categories": ["#science", "#reasoning", "#benchmark", "#dataset"], "emoji": "‚öõÔ∏è", "ru": {"title": "–§–∏–∑–∏–∫–∏ –ø—Ä–æ–≤–µ—Ä–∏–ª–∏ LLM –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –∑–∞–¥–∞—á–∞—Ö ‚Äî –∏ –º–æ–¥–µ–ª–∏ –ø—Ä–æ–≤–∞–ª–∏–ª–∏—Å—å", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Å–æ–∑–¥–∞–ª–∏ –±–µ–Ω—á–º–∞—Ä–∫ CritPt –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π LLM —Ä–µ—à–∞—Ç—å –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –∑–∞–¥–∞—á–∏ —É—Ä–æ–≤–Ω—è –Ω
[01.10.2025 17:10] Using data from previous issue: {"categories": ["#cv", "#3d"], "emoji": "üé®", "ru": {"title": "–ü—Ä–µ–≤—Ä–∞—â–∞–µ–º –∫–∞—Ä—Ç–∏–Ω–∫—É –æ–±—Ä–∞—Ç–Ω–æ –≤ —Å–ª–æ–∏ –¥–ª—è —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è", "desc": "LayerD ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥ –¥–ª—è –¥–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏–∏ —Ä–∞—Å—Ç—Ä–æ–≤—ã—Ö –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω—ã–µ —Ä–µ–¥–∞–∫—Ç–∏—Ä—É–µ–º—ã–µ —Å–ª–æ–∏. –°–∏—Å—Ç–µ–º–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ, –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ –∏–∑–≤–ª–µ–∫–∞—è —Å–ª–æ–∏ –ø–µ—Ä–µ–¥–Ω–µ–≥–æ –ø
[01.10.2025 17:10] Using data from previous issue: {"categories": ["#training", "#diffusion", "#cv", "#security"], "emoji": "üîä", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –æ—Ç adversarial –∞—Ç–∞–∫ —á–µ—Ä–µ–∑ —á–∞—Å—Ç–æ—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç MANI-Pure ‚Äî –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∑–∞—â–∏—Ç—ã –æ—Ç adversarial –∞—Ç–∞–∫ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ
[01.10.2025 17:10] Using data from previous issue: {"categories": ["#architecture"], "emoji": "üîó", "ru": {"title": "–ò—Å—Ç–æ—Ä–∏—è –≥–ª—É–±–æ–∫–æ–≥–æ –æ—Å—Ç–∞—Ç–æ—á–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è: –∫—Ç–æ –∏–∑–æ–±—Ä—ë–ª residual connections", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Ö—Ä–æ–Ω–æ–ª–æ–≥–∏—é —Ä–∞–∑–≤–∏—Ç–∏—è –≥–ª—É–±–æ–∫–æ–≥–æ –æ—Å—Ç–∞—Ç–æ—á–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è (deep residual learning) ‚Äî –∫–ª—é—á–µ–≤–æ–≥–æ –ø—Ä–æ—Ä—ã–≤–∞ –≤ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π. –ê–≤—Ç–æ
[01.10.2025 17:10] Using data from previous issue: {"categories": ["#training", "#benchmark", "#transfer_learning", "#dataset"], "emoji": "‚è∞", "ru": {"title": "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ time series –º–æ–¥–µ–ª–µ–π –±–µ–∑ –¥–æ–æ–±—É—á–µ–Ω–∏—è", "desc": "TimeTic ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø–µ—Ä–µ–Ω–æ—Å–∏–º–æ—Å—Ç–∏ foundation –º–æ–¥–µ–ª–µ–π –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –∏—Ö –ø—Ä–æ–∏–∑–≤–æ–¥
[01.10.2025 17:10] Using data from previous issue: {"categories": ["#agents", "#benchmark", "#open_source", "#security"], "emoji": "üî®", "ru": {"title": "LLM-–∞–≥–µ–Ω—Ç —É—á–∏—Ç—Å—è –∫–æ–º–ø–∏–ª–∏—Ä–æ–≤–∞—Ç—å —Å–ª–æ–∂–Ω—ã–µ open-source –ø—Ä–æ–µ–∫—Ç—ã", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ BUILD-BENCH –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ LLM-–∞–≥–µ–Ω—Ç–æ–≤ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∫–æ–º–ø–∏–ª–∏—Ä–æ–≤–∞—Ç—å —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã
[01.10.2025 17:10] Using data from previous issue: {"categories": ["#benchmark", "#inference", "#security"], "emoji": "üíß", "ru": {"title": "–ê—Ç–∞–∫–∞ –ø–µ—Ä–µ–ø–∏—Å—ã–≤–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –æ–±—Ö–æ–¥–∏—Ç –≤–æ–¥—è–Ω—ã–µ –∑–Ω–∞–∫–∏ –≤ LLM", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∞—Ç–∞–∫—É BIRA (Bias-Inversion Rewriting Attack), –∫–æ—Ç–æ—Ä–∞—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –æ–±—Ö–æ–¥–∏—Ç –≤–æ–¥—è–Ω—ã–µ –∑–Ω–∞–∫–∏ –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö. –ú–µ—Ç–æ–¥ —Ä–∞–±–æ
[01.10.2025 17:10] Using data from previous issue: {"categories": ["#benchmark", "#games", "#video", "#optimization"], "emoji": "üé¨", "ru": {"title": "–ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –≤–∏–¥–µ–æ-–≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ –ø—Ä–∏–∑–º—É –∫–∏–Ω–µ–º–∞—Ç–æ–≥—Ä–∞—Ñ–∞", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ Stable Cinemetrics ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é AI
[01.10.2025 17:10] Using data from previous issue: {"categories": ["#multimodal", "#training", "#interpretability", "#optimization", "#reasoning", "#cv"], "emoji": "üéØ", "ru": {"title": "–ö–æ–º–ø–∞–∫—Ç–Ω–∞—è VLM –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –Ω–∞–≤—ã–∫–æ–≤ —Å –æ–±—ä—è—Å–Ω–µ–Ω–∏—è–º–∏", "desc": "ProfVLM ‚Äî —ç—Ç–æ –∫–æ–º–ø–∞–∫—Ç–Ω–∞—è vision-language –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç —É—Ä–æ–≤–µ–Ω—å –≤–ª–∞–¥–µ–Ω–∏—è –Ω–∞–≤—ã–∫–∞–º–∏ –∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç
[01.10.2025 17:10] Using data from previous issue: {"categories": ["#optimization", "#reasoning", "#training", "#rlhf", "#rl", "#games"], "emoji": "üéØ", "ru": {"title": "–ê–±—Å—Ç—Ä–∞–∫—Ü–∏–∏ –¥–µ–π—Å—Ç–≤–∏–π —É—Å–∫–æ—Ä—è—é—Ç –æ–±—É—á–µ–Ω–∏–µ LLM –ø–∏—Å–∞—Ç—å –∫–æ–¥", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–∞—è —Å—Ç–∞–¥–∏—è –æ–±—É—á–µ–Ω–∏—è (mid-training) –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–∞ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è 
[01.10.2025 17:10] Using data from previous issue: {"categories": ["#training", "#rlhf", "#reasoning", "#optimization", "#benchmark", "#rl"], "emoji": "üí°", "ru": {"title": "–ü–æ–¥—Å–∫–∞–∑–∫–∏ —Å–µ–±–µ: –∫–∞–∫ –Ω–∞—É—á–∏—Ç—å LLM —Ä–µ—à–∞—Ç—å –Ω–µ–≤–æ–∑–º–æ–∂–Ω—ã–µ –∑–∞–¥–∞—á–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ NuRL, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–º–æ–≥–∞–µ—Ç —è–∑—ã–∫–æ–≤—ã–º –º–æ–¥–µ–ª—è–º —É—á–∏—Ç—å—Å—è –Ω–∞ –∑–∞–¥–∞—á–∞—Ö, –∫–æ—Ç–æ—Ä—ã–µ –æ–Ω–∏ —Ä–∞–Ω—å—à–µ –Ω–µ –º
[01.10.2025 17:10] Using data from previous issue: {"categories": ["#optimization", "#training"], "emoji": "üéØ", "ru": {"title": "–°–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ—Å–ª–µ –æ–±–æ–±—â–µ–Ω–∏—è: –∫–∞–∫ –¥–æ–æ–±—É—á–µ–Ω–∏–µ –Ω–∞ —Ç–µ—Å—Ç–µ —É–ª—É—á—à–∞–µ—Ç –º–æ–¥–µ–ª–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –æ–±—ä—è—Å–Ω—è–µ—Ç, –ø–æ—á–µ–º—É test-time training (TTT) ‚Äî –¥–æ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–µ–ø–æ—Å—Ä–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –∑–∞–¥–∞—á–µ ‚Äî –ø–æ–≤—ã—à–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ —Ä–∞–±–æ—Ç—ã fo
[01.10.2025 17:10] Using data from previous issue: {"categories": ["#training", "#transfer_learning", "#interpretability", "#games", "#open_source", "#dataset", "#cv", "#architecture"], "emoji": "üé¥", "ru": {"title": "–û–±—Ä–∞–±–æ—Ç–∫–∞ –º–Ω–æ–∂–µ—Å—Ç–≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –±–µ–∑ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω Convolutional Set Transformer (CST) ‚Äî –Ω–æ–≤
[01.10.2025 17:10] Using data from previous issue: {"categories": ["#cv", "#3d", "#benchmark", "#optimization"], "emoji": "üé≠", "ru": {"title": "–ì–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏-–æ—Å–æ–∑–Ω–∞–Ω–Ω–æ–µ —É–¥–∞–ª–µ–Ω–∏–µ –æ–±—ä–µ–∫—Ç–æ–≤ –∏ –∏—Ö —Ç–µ–Ω–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–¥–∞–ª–µ–Ω–∏—é –æ–±—ä–µ–∫—Ç–æ–≤ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä—ã–π —É—á–∏—Ç—ã–≤–∞–µ—Ç –Ω–µ —Ç–æ–ª—å–∫–æ —Å–∞–º –æ–±—ä–µ–∫—Ç, –Ω–æ –∏ –µ–≥–æ –≤–∏–∑—É–∞–ª—å–Ω—ã–µ —ç—Ñ—Ñ–µ–∫—Ç—ã, —Ç–∞–∫–∏–µ
[01.10.2025 17:10] Renaming data file.
[01.10.2025 17:10] Renaming previous data. hf_papers.json to ./d/2025-10-01.json
[01.10.2025 17:10] Saving new data file.
[01.10.2025 17:10] Generating page.
[01.10.2025 17:10] Renaming previous page.
[01.10.2025 17:10] Renaming previous data. index.html to ./d/2025-10-01.html
[01.10.2025 17:10] Writing result.
[01.10.2025 17:10] Renaming log file.
[01.10.2025 17:10] Renaming previous data. log.txt to ./logs/2025-10-01_last_log.txt
