[22.08.2025 05:14] Read previous papers.
[22.08.2025 05:14] Generating top page (month).
[22.08.2025 05:14] Writing top page (month).
[22.08.2025 06:17] Read previous papers.
[22.08.2025 06:17] Get feed.
[22.08.2025 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2508.15763
[22.08.2025 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2508.15144
[22.08.2025 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2508.15260
[22.08.2025 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2508.15769
[22.08.2025 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2508.15767
[22.08.2025 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2508.15761
[22.08.2025 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2508.15752
[22.08.2025 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2508.15361
[22.08.2025 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2508.15202
[22.08.2025 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2508.15641
[22.08.2025 06:17] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[22.08.2025 06:17] No deleted papers detected.
[22.08.2025 06:17] Downloading and parsing papers (pdf, html). Total: 10.
[22.08.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2508.15763.
[22.08.2025 06:17] Extra JSON file exists (./assets/json/2508.15763.json), skip PDF parsing.
[22.08.2025 06:17] Paper image links file exists (./assets/img_data/2508.15763.json), skip HTML parsing.
[22.08.2025 06:17] Success.
[22.08.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2508.15144.
[22.08.2025 06:17] Extra JSON file exists (./assets/json/2508.15144.json), skip PDF parsing.
[22.08.2025 06:17] Paper image links file exists (./assets/img_data/2508.15144.json), skip HTML parsing.
[22.08.2025 06:17] Success.
[22.08.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2508.15260.
[22.08.2025 06:17] Extra JSON file exists (./assets/json/2508.15260.json), skip PDF parsing.
[22.08.2025 06:17] Paper image links file exists (./assets/img_data/2508.15260.json), skip HTML parsing.
[22.08.2025 06:17] Success.
[22.08.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2508.15769.
[22.08.2025 06:17] Extra JSON file exists (./assets/json/2508.15769.json), skip PDF parsing.
[22.08.2025 06:17] Paper image links file exists (./assets/img_data/2508.15769.json), skip HTML parsing.
[22.08.2025 06:17] Success.
[22.08.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2508.15767.
[22.08.2025 06:17] Extra JSON file exists (./assets/json/2508.15767.json), skip PDF parsing.
[22.08.2025 06:17] Paper image links file exists (./assets/img_data/2508.15767.json), skip HTML parsing.
[22.08.2025 06:17] Success.
[22.08.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2508.15761.
[22.08.2025 06:17] Downloading paper 2508.15761 from http://arxiv.org/pdf/2508.15761v1...
[22.08.2025 06:17] Extracting affiliations from text.
[22.08.2025 06:17] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 1 2 ] . [ 1 1 6 7 5 1 . 8 0 5 2 : r WAVER: WAVE YOUR WAY TO LIFELIKE VIDEO GENERATION "
[22.08.2025 06:17] Response: []
[22.08.2025 06:17] Extracting affiliations from text.
[22.08.2025 06:17] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 1 2 ] . [ 1 1 6 7 5 1 . 8 0 5 2 : r WAVER: WAVE YOUR WAY TO LIFELIKE VIDEO GENERATIONWe present Waver, high-performance foundation model for unified image and video generation. Waver can directly generate videos with durations ranging from 5 to 10 seconds at native resolution of 720p, which are subsequently upscaled to 1080p. The model simultaneously supports text-to-video (T2V), imageto-video (I2V), and text-to-image (T2I) generation within single, integrated framework. We introduce Hybrid Stream DiT architecture to enhance modality alignment and accelerate training convergence. To ensure training data quality, we establish comprehensive data curation pipeline and manually annotate and train an MLLM-based video quality model to filter for the highest-quality samples. Furthermore, we provide detailed training and inference recipes to facilitate the generation of high-quality videos. Building on these contributions, Waver excels at capturing complex motion, achieving superior motion amplitude and temporal consistency in video synthesis. Notably, it ranks among the Top 3 on both the T2V and I2V leaderboards at Artificial Analysis (data as of 2025-07-30 10:00 GMT+8), consistently outperforming existing open-source models and matching or surpassing state-of-the-art commercial solutions. We hope this technical report will help the community more efficiently train high-quality video generation models and accelerate progress in video generation technologies. Official page: https: //github.com/FoundationVision/Waver. Figure 1: Left: Human evaluation win rates (GSB) of Waver compared to Veo3, Kling2.0, and Wan2.1 on the Waver-bench 1.0 Text-to-Video (T2V) dataset across three dimensions: motion quality, visual quality, and prompt following. Waver-bench 1.0 covers wide range of scenarios, including sports, daily activities, landscapes, animals, animations, and more. Right: Human evaluation win rates (GSB) on the Hermes Motion Testset across three dimensions: motion quality, visual quality, and prompt following. The Hermes Motion Testset encompasses complex motion scenarios such as tennis, basketball, gymnastics, and others. We can observe that, compared to general scenarios, Waver demonstrates more pronounced advantage in complex motion scenarios.2.1 Task-Unified DiT . 2.2 Cascade Refiner . . . . .3.1 Data Pre-Processing . 3.2 Quality Model 3.3 Caption Model . . . . . . . . 3. Semantic Balancing . . . . . . . . . . . . . . . . . . . 3.5 Hierarchical Data Filtering .4.1 Multi-stage Training . . . 4.2 Representation Alignment"
[22.08.2025 06:17] Mistral response. {"id": "f8ff5f88416043e7ae4a2c2387f5b8b5", "created": 1755843471, "model": "mistral-large-latest", "usage": {"prompt_tokens": 681, "total_tokens": 683, "completion_tokens": 2}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "[]"}}]}
[22.08.2025 06:17] Response: []
[22.08.2025 06:17] Deleting PDF ./assets/pdf/2508.15761.pdf.
[22.08.2025 06:17] Success.
[22.08.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2508.15752.
[22.08.2025 06:17] Extra JSON file exists (./assets/json/2508.15752.json), skip PDF parsing.
[22.08.2025 06:17] Paper image links file exists (./assets/img_data/2508.15752.json), skip HTML parsing.
[22.08.2025 06:17] Success.
[22.08.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2508.15361.
[22.08.2025 06:17] Extra JSON file exists (./assets/json/2508.15361.json), skip PDF parsing.
[22.08.2025 06:17] Paper image links file exists (./assets/img_data/2508.15361.json), skip HTML parsing.
[22.08.2025 06:17] Success.
[22.08.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2508.15202.
[22.08.2025 06:17] Extra JSON file exists (./assets/json/2508.15202.json), skip PDF parsing.
[22.08.2025 06:17] Paper image links file exists (./assets/img_data/2508.15202.json), skip HTML parsing.
[22.08.2025 06:17] Success.
[22.08.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2508.15641.
[22.08.2025 06:17] Downloading paper 2508.15641 from http://arxiv.org/pdf/2508.15641v1...
[22.08.2025 06:17] Extracting affiliations from text.
[22.08.2025 06:17] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 1 When and What: Diffusion-Grounded VideoLLM with Entity-Aware Segmentation for Long Video Understanding Pengcheng Fang, Yuxia Chen, Rui Guo 5 2 0 2 1 2 ] . [ 1 1 4 6 5 1 . 8 0 5 2 : r Fig. 1: Teaser of our videolanguage model. Given video and natural-language instruction, we extract key nouns (e.g., dog, frisbee) and introduce special timestamp tokens (e.g., <24>, <96>--<120>) to enable precise temporal grounding. The nouns are linked to segmentation-guided object embeddings (Obj-Seg row), while video frames provide visual evidence along the timeline. The model handles three query types: (1) event localization (When does the dog first touch the frisbee? returns time), (2) interval description (Describe the scene between <96> and <120>), and (3) object-centric tracking (Track the red frisbee; does the dog pick it up?). Blue bubbles show concise, time-aligned answers produced by the LLM conditioned on video frames, object masks, and the mixed token sequence. AbstractUnderstanding videos requires more than answering open-ended questionsit demands the ability to pinpoint when events occur and how entities interact across time. While recent Video-LLMs have achieved remarkable progress in holistic reasoning, they remain coarse in temporal perception: timestamps are encoded only implicitly, frame-level features are weak in capturing continuity, and languagevision alignment often drifts from the entities of interest. In this paper, we present GroundedVideoDiT, Video-LLM designed to overcome these limitations by introducing three key innovations. First, Diffusion Temporal Latent (DTL) encoder enhances boundary sensitivity and maintains temporal consistency. Second, object-grounded representations explicitly bind query entities to localized visual evidence, strengthening alignment. Third, mixed token scheme with discrete temporal tokens provides explicit timestamp modeling, enabling fine-grained temporal reasoning. To"
[22.08.2025 06:17] Response: ```python
[]
```
[22.08.2025 06:17] Extracting affiliations from text.
[22.08.2025 06:17] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 1 When and What: Diffusion-Grounded VideoLLM with Entity-Aware Segmentation for Long Video Understanding Pengcheng Fang, Yuxia Chen, Rui Guo 5 2 0 2 1 2 ] . [ 1 1 4 6 5 1 . 8 0 5 2 : r Fig. 1: Teaser of our videolanguage model. Given video and natural-language instruction, we extract key nouns (e.g., dog, frisbee) and introduce special timestamp tokens (e.g., <24>, <96>--<120>) to enable precise temporal grounding. The nouns are linked to segmentation-guided object embeddings (Obj-Seg row), while video frames provide visual evidence along the timeline. The model handles three query types: (1) event localization (When does the dog first touch the frisbee? returns time), (2) interval description (Describe the scene between <96> and <120>), and (3) object-centric tracking (Track the red frisbee; does the dog pick it up?). Blue bubbles show concise, time-aligned answers produced by the LLM conditioned on video frames, object masks, and the mixed token sequence. AbstractUnderstanding videos requires more than answering open-ended questionsit demands the ability to pinpoint when events occur and how entities interact across time. While recent Video-LLMs have achieved remarkable progress in holistic reasoning, they remain coarse in temporal perception: timestamps are encoded only implicitly, frame-level features are weak in capturing continuity, and languagevision alignment often drifts from the entities of interest. In this paper, we present GroundedVideoDiT, Video-LLM designed to overcome these limitations by introducing three key innovations. First, Diffusion Temporal Latent (DTL) encoder enhances boundary sensitivity and maintains temporal consistency. Second, object-grounded representations explicitly bind query entities to localized visual evidence, strengthening alignment. Third, mixed token scheme with discrete temporal tokens provides explicit timestamp modeling, enabling fine-grained temporal reasoning. Together, these designs equip Grounded-VideoDiT with robust grounding capabilities, as validated by state-of-the-art results on Charades-STA, NExTGQA, and multiple VideoQA benchmarks. I. INTRODUCTION With the exponential growth of video data, video understanding has become central focus in multimodal research. In tasks such as question answering and caption generation, there is growing demand for fine-grained modeling of when, where, and who is doing what within video. Unlike static long videos span extended durations and contain images, JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 2 dynamic, time-evolving content. Effective understanding requires not only recognizing spatial semantics but also capturing temporal dependencies, tracking object trajectories, and reasoning about the timing and location of specific events. This involves addressing challenges such as redundant frames, frequent scene changes, multi-entity interactions, and longrange temporal dependencies. Designing unified videolanguage framework with strong temporal modeling, precise object perception, and robust tracking remains key research problem. Recent Video-LLMs have attempted to bridge the gap between visual perception and temporal reasoning by adapting static vision-language frameworks with video-specific enhancements. These methods typically rely on frame sampling and image encoders to extract appearance features, followed by token compression or projection modules that interface with frozen LLMs. In addition, some models enhance temporal awareness through positional encodings, discrete timestamp tokens, or external prompts, while others attach segmentation masks or object anchors post-generation for entity visualization. Although these strategies improve basic temporal sensitivity and visual grounding, they often fail to explicitly model temporal evolution. First, temporal modeling is usually delegated to positional cues, lacking learnable latent variables that evolve over time. Second, segmentation and tracking are typically applied after language modeling, preventing early attention alignment and weakening reasoning consistency in multi-object scenarios. Third, token-level reasoning over time remains limited, as temporal structures are neither disentangled nor explicitly incorporated into the language modeling pipeline. To address the aforementioned limitations in long-video understanding, we propose Diffusion-Grounded VideoLLM, unified and lightweight framework that integrates structured temporal and entity-aware information prior to language modeling. This framework redefines the role of diffusion models in video understandingnot as generative models, but as efficient temporal feature extractors that capture dynamic changes across frames via conditional denoising. In parallel, we incorporate semantic segmentationbased object representations and apply cross-frame tracking to explicitly model the evolution of entities over time. To support joint reasoning, we design mixed-token input layout that encodes temporal, spatial, and linguistic information into unified sequence, providing the language model with structured and time-sensitive context. The overall architecture maintains high scalability and inference efficiency, significantly enhancing the models ability to localize key events, track entities, and reason about temporal relationships in long videos. Our contributions are summarized as follows: (1) We introduce diffusion-based video encoder to capture inter-frame dynamics and temporal structures, generating differentiable temporal latent tokens that serve as informative inputs to the LLM. (2) Semantic segmentation is incorporated before language modeling, enabling object-level representation and crossframe consistency to support robust multi-entity tracking and grounding. (3) We design unified mixed-token input structure that encodes visual, textual, temporal, and object-level information into single sequence, enabling joint spatiotemporal reasoning in an end-to-end manner. (4) Our framework achieves state-of-the-art performance on long-video understanding benchmarks, including NExTQA and ActivityNet-Captions. II. RELATED WORK A. Video Large Language Models With the success of large language models (LLMs) on visual tasks, researchers have begun exploring their extension to video understanding [1][3]. Flamingo [4] first demonstrated that simply inserting visual tokens into the text stream, while keeping the LLM frozen, enables "
[22.08.2025 06:18] Mistral response. {"id": "780f4115b32a4faca9eee5d5fc752c50", "created": 1755843479, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1307, "total_tokens": 1356, "completion_tokens": 49}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\n    \"Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China\",\n    \"University of Chinese Academy of Sciences, Beijing, China\",\n    \"Tencent AI Lab, Shenzhen, China\"\n]\n```"}}]}
[22.08.2025 06:18] Response: ```python
[
    "Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China",
    "University of Chinese Academy of Sciences, Beijing, China",
    "Tencent AI Lab, Shenzhen, China"
]
```
[22.08.2025 06:18] Deleting PDF ./assets/pdf/2508.15641.pdf.
[22.08.2025 06:18] Success.
[22.08.2025 06:18] Enriching papers with extra data.
[22.08.2025 06:18] ********************************************************************************
[22.08.2025 06:18] Abstract 0. Intern-S1, a multimodal Mixture-of-Experts model with extensive pre-training and reinforcement learning, achieves top-tier performance in general reasoning and outperforms closed-source models in scientific tasks.  					AI-generated summary 				 In recent years, a plethora of open-source foundation ...
[22.08.2025 06:18] ********************************************************************************
[22.08.2025 06:18] Abstract 1. GUI-Owl and Mobile-Agent-v3 are open-source GUI agent models and frameworks that achieve state-of-the-art performance across various benchmarks using innovations in environment infrastructure, agent capabilities, and scalable reinforcement learning.  					AI-generated summary 				 This paper introdu...
[22.08.2025 06:18] ********************************************************************************
[22.08.2025 06:18] Abstract 2. DeepConf enhances reasoning efficiency and performance by filtering low-quality reasoning traces using model-internal confidence signals, achieving high accuracy and reducing token generation.  					AI-generated summary 				 Large Language Models (LLMs) have shown great potential in reasoning tasks ...
[22.08.2025 06:18] ********************************************************************************
[22.08.2025 06:18] Abstract 3. SceneGen generates multiple 3D assets from a single scene image using a novel framework that integrates local and global scene information, enabling efficient and robust 3D content creation.  					AI-generated summary 				 3D content generation has recently attracted significant research interest du...
[22.08.2025 06:18] ********************************************************************************
[22.08.2025 06:18] Abstract 4. ATLAS is a high-fidelity body model that decouples shape and skeleton bases, improving pose accuracy and shape expressivity using 600k high-resolution scans.  					AI-generated summary 				 Parametric body models offer expressive 3D representation of humans across a wide range of poses, shapes, and ...
[22.08.2025 06:18] ********************************************************************************
[22.08.2025 06:18] Abstract 5. Waver, a high-performance foundation model, generates high-quality videos and images using a Hybrid Stream DiT architecture, excelling in text-to-video, image-to-video, and text-to-image tasks.  					AI-generated summary 				 We present Waver, a high-performance foundation model for unified image an...
[22.08.2025 06:18] ********************************************************************************
[22.08.2025 06:18] Abstract 6. Geo-Visual Agents integrate geospatial images and GIS data to answer nuanced visual-spatial inquiries about the world.  					AI-generated summary 				 Interactive digital maps have revolutionized how people travel and learn about the world; however, they rely on pre-existing structured data in GIS d...
[22.08.2025 06:18] ********************************************************************************
[22.08.2025 06:18] Abstract 7. A systematic review of large language model benchmarks identifies issues such as data contamination, cultural biases, and lack of process credibility, and proposes a design paradigm for future improvements.  					AI-generated summary 				 In recent years, with the rapid development of the depth and ...
[22.08.2025 06:18] ********************************************************************************
[22.08.2025 06:18] Abstract 8. Fin-PRM, a domain-specialized reward model for finance, enhances intermediate reasoning in LLMs through step-level and trajectory-level supervision, improving performance in supervised learning, reinforcement learning, and test-time inference.  					AI-generated summary 				 Process Reward Models (P...
[22.08.2025 06:18] ********************************************************************************
[22.08.2025 06:18] Abstract 9. Grounded VideoDiT addresses temporal perception and entity interaction in videos through a Diffusion Temporal Latent encoder, object grounded representations, and mixed token scheme, achieving state-of-the-art results on VideoQA benchmarks.  					AI-generated summary 				 Understanding videos requir...
[22.08.2025 06:18] Read previous papers.
[22.08.2025 06:18] Generating reviews via LLM API.
[22.08.2025 06:18] Using data from previous issue: {"categories": ["#agi", "#science", "#reasoning", "#dataset", "#open_source", "#rl", "#training", "#multimodal", "#benchmark"], "emoji": "üß†", "ru": {"title": "Intern-S1: –ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å-—ç–∫—Å–ø–µ—Ä—Ç –¥–ª—è –Ω–∞—É—á–Ω—ã—Ö –∑–∞–¥–∞—á", "desc": "Intern-S1 - —ç—Ç–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å —Ç–∏–ø–∞ Mixture-of-Experts —Å 28 –º–∏
[22.08.2025 06:18] Using data from previous issue: {"categories": ["#games", "#open_source", "#agents", "#rl", "#training"], "emoji": "ü§ñ", "ru": {"title": "–û—Ç–∫—Ä—ã—Ç—ã–µ –º–æ–¥–µ–ª–∏ GUI-–∞–≥–µ–Ω—Ç–æ–≤ –¥–æ—Å—Ç–∏–≥–∞—é—Ç –ø—Ä–æ—Ä—ã–≤–∞ –≤ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç GUI-Owl –∏ Mobile-Agent-v3 - –æ—Ç–∫—Ä—ã—Ç—ã–µ –º–æ–¥–µ–ª–∏ –∏ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∏ –¥–ª—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–º 
[22.08.2025 06:18] Using data from previous issue: {"categories": ["#reasoning", "#training", "#inference", "#optimization", "#benchmark"], "emoji": "üß†", "ru": {"title": "–£–º–Ω–µ–µ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ: DeepConf –ø–æ–≤—ã—à–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –ò–ò", "desc": "DeepConf - —ç—Ç–æ –º–µ—Ç–æ–¥, —É–ª—É—á—à–∞—é—â–∏–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö
[22.08.2025 06:18] Using data from previous issue: {"categories": ["#3d"], "emoji": "üñºÔ∏è", "ru": {"title": "–û—Ç 2D –∫ 3D: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç—Ä–µ—Ö–º–µ—Ä–Ω—ã—Ö —Å—Ü–µ–Ω", "desc": "SceneGen - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö 3D-–æ–±—ä–µ–∫—Ç–æ–≤ –∏–∑ –æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å—Ü–µ–Ω—ã. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∏–π –ª–æ–∫–∞–ª—å–Ω—É—é –∏ –≥–ª–æ–±–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ
[22.08.2025 06:18] Using data from previous issue: {"categories": ["#3d"], "emoji": "ü¶æ", "ru": {"title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ 3D-–º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–∏ —Ç–µ–ª–∞: —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Ñ–æ—Ä–º—ã –∏ —Å–∫–µ–ª–µ—Ç–∞ –¥–ª—è —É–ª—É—á—à–µ–Ω–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏", "desc": "ATLAS - —ç—Ç–æ –≤—ã—Å–æ–∫–æ—Ç–æ—á–Ω–∞—è –º–æ–¥–µ–ª—å —Ç–µ–ª–∞ —á–µ–ª–æ–≤–µ–∫–∞, –∫–æ—Ç–æ—Ä–∞—è —Ä–∞–∑–¥–µ–ª—è–µ—Ç –æ—Å–Ω–æ–≤—ã —Ñ–æ—Ä–º—ã –∏ —Å–∫–µ–ª–µ—Ç–∞, —É–ª—É—á—à–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –ø–æ–∑—ã –∏ –≤—ã—Ä–∞–∑–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ñ–æ—Ä–º—ã —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω
[22.08.2025 06:18] Using data from previous issue: {"categories": ["#games", "#training", "#multimodal", "#data", "#open_source", "#video"], "emoji": "üé¨", "ru": {"title": "Waver: –ï–¥–∏–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–æ–≤–æ–≥–æ –ø–æ–∫–æ–ª–µ–Ω–∏—è", "desc": "Waver - —ç—Ç–æ –≤—ã—Å–æ–∫–æ–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–∞—è —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤—ã—Å–æ
[22.08.2025 06:18] Using data from previous issue: {"categories": ["#multimodal", "#agents", "#cv"], "emoji": "üåç", "ru": {"title": "–ì–µ–æ-–í–∏–∑—É–∞–ª—å–Ω—ã–µ –ê–≥–µ–Ω—Ç—ã: –ù–æ–≤—ã–π –≤–∑–≥–ª—è–¥ –Ω–∞ –∞–Ω–∞–ª–∏–∑ –≥–µ–æ–ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∫–æ–Ω—Ü–µ–ø—Ü–∏—é –ì–µ–æ-–í–∏–∑—É–∞–ª—å–Ω—ã—Ö –ê–≥–µ–Ω—Ç–æ–≤ - –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤, —Å–ø–æ—Å–æ–±–Ω—ã—Ö –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∏ –æ—Ç–≤–µ—á–∞—Ç—å –Ω–∞ —Å–ª–æ–∂–Ω—ã–µ –≤–∏–∑—É–∞–ª
[22.08.2025 06:18] Using data from previous issue: {"categories": ["#ethics", "#benchmark", "#survey"], "emoji": "üéØ", "ru": {"title": "–ù–æ–≤—ã–π –≤–∑–≥–ª—è–¥ –Ω–∞ –æ—Ü–µ–Ω–∫—É —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π: –æ—Ç –ø—Ä–æ–±–ª–µ–º –∫ –∏–Ω–Ω–æ–≤–∞—Ü–∏—è–º", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–æ–≤–æ–¥–∏—Ç—Å—è —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –æ–±–∑–æ—Ä –±–µ–Ω—á–º–∞—Ä–∫–æ–≤ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –ê–≤—Ç–æ—Ä—ã –≤—ã—è–≤–ª—è—é—Ç –ø—Ä–æ–±–ª–µ–º—ã —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –±–µ–Ω—á–º–∞
[22.08.2025 06:18] Using data from previous issue: {"categories": ["#science", "#reasoning", "#rlhf", "#healthcare", "#training", "#optimization"], "emoji": "üíπ", "ru": {"title": "Fin-PRM: –£–ª—É—á—à–µ–Ω–∏–µ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ LLM —Å –ø–æ–º–æ—â—å—é —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è", "desc": "Fin-PRM - —ç—Ç–æ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –¥–ª—è —Ñ–∏–Ω
[22.08.2025 06:18] Using data from previous issue: {"categories": ["#multimodal", "#reasoning", "#diffusion", "#benchmark", "#video"], "emoji": "üé¨", "ru": {"title": "–¢–æ—á–Ω–æ–µ –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ –≤—Ä–µ–º–µ–Ω–∏ –∏ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é –ò–ò", "desc": "Grounded VideoDiT - —ç—Ç–æ –Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –≤–∏–¥–µ–æ. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —ç–Ω–∫–æ–¥–µ—Ä Dif
[22.08.2025 06:18] Renaming data file.
[22.08.2025 06:18] Renaming previous data. hf_papers.json to ./d/2025-08-22.json
[22.08.2025 06:18] Saving new data file.
[22.08.2025 06:18] Generating page.
[22.08.2025 06:18] Renaming previous page.
[22.08.2025 06:18] Renaming previous data. index.html to ./d/2025-08-22.html
[22.08.2025 06:18] Writing result.
[22.08.2025 06:18] Renaming log file.
[22.08.2025 06:18] Renaming previous data. log.txt to ./logs/2025-08-22_last_log.txt
