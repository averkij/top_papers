[11.11.2024 04:15] Read previous papers.
[11.11.2024 04:15] Generating top page (month).
[11.11.2024 04:15] Writing top page (month).
[11.11.2024 06:17] Read previous papers.
[11.11.2024 06:17] Get feed.
[11.11.2024 06:17] Extract page data from URL. URL: https://huggingface.co/papers/2411.05288
[11.11.2024 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2411.02462
[11.11.2024 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2411.05738
[11.11.2024 06:17] Extract page data from URL. URL: https://huggingface.co/papers/2411.04425
[11.11.2024 06:17] ********************************************************************************
[11.11.2024 06:17] Abstract 0. Pipeline parallelism is widely used to scale the training of transformer-based large language models, various works have been done to improve its throughput and memory footprint. In this paper, we address a frequently overlooked issue: the vocabulary layers can cause imbalanced computation and memor...
[11.11.2024 06:17] ********************************************************************************
[11.11.2024 06:17] Abstract 1. The advent of large language models (LLMs) like GitHub Copilot has significantly enhanced programmers' productivity, particularly in code generation. However, these models often struggle with real-world tasks without fine-tuning. As LLMs grow larger and more performant, fine-tuning for specialized t...
[11.11.2024 06:17] ********************************************************************************
[11.11.2024 06:17] Abstract 2. We present StdGEN, an innovative pipeline for generating semantically decomposed high-quality 3D characters from single images, enabling broad applications in virtual reality, gaming, and filmmaking, etc. Unlike previous methods which struggle with limited decomposability, unsatisfactory quality, an...
[11.11.2024 06:17] ********************************************************************************
[11.11.2024 06:17] Abstract 3. Fine-tuning large language models (LLMs) is essential for enhancing their performance on specific tasks but is often resource-intensive due to redundant or uninformative data. To address this inefficiency, we introduce DELIFT (Data Efficient Language model Instruction Fine-Tuning), a novel algorithm...
[11.11.2024 06:17] Read previous papers.
[11.11.2024 06:17] Generating reviews via LLM API.
[11.11.2024 06:17] Querying the API.
[11.11.2024 06:17] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Pipeline parallelism is widely used to scale the training of transformer-based large language models, various works have been done to improve its throughput and memory footprint. In this paper, we address a frequently overlooked issue: the vocabulary layers can cause imbalanced computation and memory usage across pipeline stages, worsening pipeline bubbles and the memory bottleneck. To tackle this, we partition the vocabulary layers evenly across pipeline devices and group the computation into pipeline passes. To reduce the activation memory overhead, we propose several algorithms to reduce communication barriers within vocabulary layers. Additionally, we utilize a generalizable method to integrate Vocabulary Parallelism with existing pipeline schedules. By combining these techniques, our methods effectively balance the computation and parameter memory, with only a small constant activation memory overhead. Notably, when combined with activation memory-balanced schedules like V-Half, our approach achieves perfect balance in both memory and computation. Extensive evaluations demonstrate that our method achieves computation and memory balance regardless of the vocabulary size, resulting in a 5% to 51% improvement in throughput compared to naive approaches, meanwhile significantly reducing peak memory usage especially for large vocabulary scenarios. Our implementation is open-sourced at https://github.com/sail-sg/VocabularyParallelism .
[11.11.2024 06:17] Response: {
  "desc": "Статья предлагает новый метод распараллеливания обучения больших языковых моделей, фокусируясь на эффективном распределении слоев словаря. Авторы разработали алгоритмы для равномерного распределения вычислений и памяти между устройствами в конвейере, что позволяет уменьшить простои и оптимизировать использование памяти. Метод интегрируется с существующими схемами конвейерного параллелизма и особенно эффективен в сочетании с методами балансировки активационной памяти. Эксперименты показывают значительное улучшение пропускной способности и снижение пикового использования памяти, особенно для моделей с большими словарями.",
  "emoji": "⚡",
  "title": "Эффективное распараллеливание словарных слоев для ускорения обучения больших языковых моделей"
}
[11.11.2024 06:17] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Pipeline parallelism is widely used to scale the training of transformer-based large language models, various works have been done to improve its throughput and memory footprint. In this paper, we address a frequently overlooked issue: the vocabulary layers can cause imbalanced computation and memory usage across pipeline stages, worsening pipeline bubbles and the memory bottleneck. To tackle this, we partition the vocabulary layers evenly across pipeline devices and group the computation into pipeline passes. To reduce the activation memory overhead, we propose several algorithms to reduce communication barriers within vocabulary layers. Additionally, we utilize a generalizable method to integrate Vocabulary Parallelism with existing pipeline schedules. By combining these techniques, our methods effectively balance the computation and parameter memory, with only a small constant activation memory overhead. Notably, when combined with activation memory-balanced schedules like V-Half, our approach achieves perfect balance in both memory and computation. Extensive evaluations demonstrate that our method achieves computation and memory balance regardless of the vocabulary size, resulting in a 5% to 51% improvement in throughput compared to naive approaches, meanwhile significantly reducing peak memory usage especially for large vocabulary scenarios. Our implementation is open-sourced at https://github.com/sail-sg/VocabularyParallelism ."

[11.11.2024 06:17] Response: ```python
["TRAINING", "INFERENCE", "ARCHITECTURE"]
```
[11.11.2024 06:17] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Pipeline parallelism is widely used to scale the training of transformer-based large language models, various works have been done to improve its throughput and memory footprint. In this paper, we address a frequently overlooked issue: the vocabulary layers can cause imbalanced computation and memory usage across pipeline stages, worsening pipeline bubbles and the memory bottleneck. To tackle this, we partition the vocabulary layers evenly across pipeline devices and group the computation into pipeline passes. To reduce the activation memory overhead, we propose several algorithms to reduce communication barriers within vocabulary layers. Additionally, we utilize a generalizable method to integrate Vocabulary Parallelism with existing pipeline schedules. By combining these techniques, our methods effectively balance the computation and parameter memory, with only a small constant activation memory overhead. Notably, when combined with activation memory-balanced schedules like V-Half, our approach achieves perfect balance in both memory and computation. Extensive evaluations demonstrate that our method achieves computation and memory balance regardless of the vocabulary size, resulting in a 5% to 51% improvement in throughput compared to naive approaches, meanwhile significantly reducing peak memory usage especially for large vocabulary scenarios. Our implementation is open-sourced at https://github.com/sail-sg/VocabularyParallelism ."

[11.11.2024 06:17] Response: ```python
['OPTIMIZATION', 'OPEN_SOURCE']
```
[11.11.2024 06:17] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper focuses on improving the efficiency of training large language models by addressing the imbalance caused by vocabulary layers in pipeline parallelism. The authors propose a method to evenly distribute vocabulary layers across different pipeline devices, which helps to balance computation and memory usage. They introduce algorithms to minimize communication delays within these layers and integrate their approach with existing pipeline schedules. The results show significant improvements in throughput and reduced memory usage, making the training process more efficient, especially for models with large vocabularies.","title":"Balancing Memory and Computation in Pipeline Parallelism for Language Models"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[], parsed=Article(desc='This paper focuses on improving the efficiency of training large language models by addressing the imbalance caused by vocabulary layers in pipeline parallelism. The authors propose a method to evenly distribute vocabulary layers across different pipeline devices, which helps to balance computation and memory usage. They introduce algorithms to minimize communication delays within these layers and integrate their approach with existing pipeline schedules. The results show significant improvements in throughput and reduced memory usage, making the training process more efficient, especially for models with large vocabularies.', title='Balancing Memory and Computation in Pipeline Parallelism for Language Models'))
[11.11.2024 06:17] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本文探讨了在训练大型语言模型时，词汇层导致的计算和内存不平衡问题。我们提出了一种方法，将词汇层均匀分配到管道设备上，并将计算分组到管道传递中。为了减少激活内存开销，我们设计了几种算法来降低词汇层内的通信障碍。通过结合这些技术，我们的方法在计算和参数内存之间实现了有效平衡，并在大词汇场景下显著降低了峰值内存使用。","title":"优化词汇层以平衡计算与内存"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[], parsed=Article(desc='本文探讨了在训练大型语言模型时，词汇层导致的计算和内存不平衡问题。我们提出了一种方法，将词汇层均匀分配到管道设备上，并将计算分组到管道传递中。为了减少激活内存开销，我们设计了几种算法来降低词汇层内的通信障碍。通过结合这些技术，我们的方法在计算和参数内存之间实现了有效平衡，并在大词汇场景下显著降低了峰值内存使用。', title='优化词汇层以平衡计算与内存'))
[11.11.2024 06:17] Using data from previous issue: {"categories": ["#optimization", "#training", "#benchmark", "#plp"], "emoji": "🧪", "ru": {"title": "Эффективная настройка языковых моделей для генерации модульных тестов", "desc": "Эта статья исследует применение методов эффективной настройки параметров (PEFT) для больших языковых моделей (LLM) в за
[11.11.2024 06:17] Using data from previous issue: {"categories": ["#3d", "#diffusion", "#games"], "emoji": "🎭", "ru": {"title": "StdGEN: Революция в создании 3D-персонажей с семантическим разделением", "desc": "StdGEN - это инновационный конвейер для генерации семантически декомпозированных трехмерных персонажей высокого качества из одиночных изобр
[11.11.2024 06:17] Querying the API.
[11.11.2024 06:17] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Fine-tuning large language models (LLMs) is essential for enhancing their performance on specific tasks but is often resource-intensive due to redundant or uninformative data. To address this inefficiency, we introduce DELIFT (Data Efficient Language model Instruction Fine-Tuning), a novel algorithm that systematically optimizes data selection across the three key stages of fine-tuning: (1) instruction tuning, (2) task-specific fine-tuning (e.g., reasoning, question-answering), and (3) continual fine-tuning (e.g., incorporating new data versions). Unlike existing methods that focus on single-stage optimization or rely on computationally intensive gradient calculations, DELIFT operates efficiently across all stages. Central to our approach is a pairwise utility metric that quantifies how beneficial a data sample is for improving the model's responses to other samples, effectively measuring the informational value relative to the model's current capabilities. By leveraging different submodular functions applied to this metric, DELIFT selects diverse and optimal subsets that are useful across all stages of fine-tuning. Experiments across various tasks and model scales demonstrate that DELIFT can reduce the fine-tuning data size by up to 70% without compromising performance, offering significant computational savings and outperforming existing methods in both efficiency and efficacy.
[11.11.2024 06:17] Response: {
  "desc": "Статья представляет новый алгоритм DELIFT для оптимизации выбора данных при файн-тюнинге больших языковых моделей (LLM). DELIFT использует попарную метрику полезности для оценки информативности образцов данных относительно текущих возможностей модели. Алгоритм эффективно работает на всех этапах файн-тюнинга: инструктирование, обучение специфичным задачам и непрерывное обучение. Эксперименты показали, что DELIFT может сократить объем данных для файн-тюнинга на 70% без ущерба для производительности.",
  "emoji": "🔍",
  "title": "Эффективный файн-тюнинг языковых моделей с меньшими данными"
}
[11.11.2024 06:17] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Fine-tuning large language models (LLMs) is essential for enhancing their performance on specific tasks but is often resource-intensive due to redundant or uninformative data. To address this inefficiency, we introduce DELIFT (Data Efficient Language model Instruction Fine-Tuning), a novel algorithm that systematically optimizes data selection across the three key stages of fine-tuning: (1) instruction tuning, (2) task-specific fine-tuning (e.g., reasoning, question-answering), and (3) continual fine-tuning (e.g., incorporating new data versions). Unlike existing methods that focus on single-stage optimization or rely on computationally intensive gradient calculations, DELIFT operates efficiently across all stages. Central to our approach is a pairwise utility metric that quantifies how beneficial a data sample is for improving the model's responses to other samples, effectively measuring the informational value relative to the model's current capabilities. By leveraging different submodular functions applied to this metric, DELIFT selects diverse and optimal subsets that are useful across all stages of fine-tuning. Experiments across various tasks and model scales demonstrate that DELIFT can reduce the fine-tuning data size by up to 70% without compromising performance, offering significant computational savings and outperforming existing methods in both efficiency and efficacy."

[11.11.2024 06:17] Response: ```python
['DATA', 'TRAINING']
```
[11.11.2024 06:17] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Fine-tuning large language models (LLMs) is essential for enhancing their performance on specific tasks but is often resource-intensive due to redundant or uninformative data. To address this inefficiency, we introduce DELIFT (Data Efficient Language model Instruction Fine-Tuning), a novel algorithm that systematically optimizes data selection across the three key stages of fine-tuning: (1) instruction tuning, (2) task-specific fine-tuning (e.g., reasoning, question-answering), and (3) continual fine-tuning (e.g., incorporating new data versions). Unlike existing methods that focus on single-stage optimization or rely on computationally intensive gradient calculations, DELIFT operates efficiently across all stages. Central to our approach is a pairwise utility metric that quantifies how beneficial a data sample is for improving the model's responses to other samples, effectively measuring the informational value relative to the model's current capabilities. By leveraging different submodular functions applied to this metric, DELIFT selects diverse and optimal subsets that are useful across all stages of fine-tuning. Experiments across various tasks and model scales demonstrate that DELIFT can reduce the fine-tuning data size by up to 70% without compromising performance, offering significant computational savings and outperforming existing methods in both efficiency and efficacy."

[11.11.2024 06:17] Response: ```python
["OPTIMIZATION"]
```
[11.11.2024 06:18] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents DELIFT, a new algorithm designed to improve the fine-tuning process of large language models (LLMs) by optimizing data selection. DELIFT focuses on three stages of fine-tuning: instruction tuning, task-specific fine-tuning, and continual fine-tuning, making it more efficient than traditional methods. It uses a pairwise utility metric to evaluate the informational value of data samples, ensuring that only the most beneficial data is selected. The results show that DELIFT can significantly reduce the amount of data needed for fine-tuning by up to 70%, while maintaining or even enhancing model performance.","title":"Optimize Data, Maximize Performance with DELIFT!"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[], parsed=Article(desc='This paper presents DELIFT, a new algorithm designed to improve the fine-tuning process of large language models (LLMs) by optimizing data selection. DELIFT focuses on three stages of fine-tuning: instruction tuning, task-specific fine-tuning, and continual fine-tuning, making it more efficient than traditional methods. It uses a pairwise utility metric to evaluate the informational value of data samples, ensuring that only the most beneficial data is selected. The results show that DELIFT can significantly reduce the amount of data needed for fine-tuning by up to 70%, while maintaining or even enhancing model performance.', title='Optimize Data, Maximize Performance with DELIFT!'))
[11.11.2024 06:18] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本论文提出了一种名为DELIFT的新算法，用于提高大型语言模型（LLMs）在特定任务上的性能，同时减少冗余和无效数据的使用。DELIFT通过优化数据选择，系统性地改进了三个关键的微调阶段：指令微调、任务特定微调和持续微调。该方法使用了一种成对效用度量，量化数据样本对模型响应其他样本的改善程度，从而有效评估信息价值。实验结果表明，DELIFT能够在不降低性能的情况下，将微调数据量减少多达70%，显著提高了计算效率。","title":"高效微调：DELIFT算法的创新之路"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[], parsed=Article(desc='本论文提出了一种名为DELIFT的新算法，用于提高大型语言模型（LLMs）在特定任务上的性能，同时减少冗余和无效数据的使用。DELIFT通过优化数据选择，系统性地改进了三个关键的微调阶段：指令微调、任务特定微调和持续微调。该方法使用了一种成对效用度量，量化数据样本对模型响应其他样本的改善程度，从而有效评估信息价值。实验结果表明，DELIFT能够在不降低性能的情况下，将微调数据量减少多达70%，显著提高了计算效率。', title='高效微调：DELIFT算法的创新之路'))
[11.11.2024 06:18] Loading Chinese text from previous data.
[11.11.2024 06:18] Renaming data file.
[11.11.2024 06:18] Renaming previous data. hf_papers.json to ./d/2024-11-11.json
[11.11.2024 06:18] Saving new data file.
[11.11.2024 06:18] Generating page.
[11.11.2024 06:18] Renaming previous page.
[11.11.2024 06:18] Renaming previous data. index.html to ./d/2024-11-11.html
[11.11.2024 06:18] [Experimental] Generating Chinese page for reading.
[11.11.2024 06:18] Chinese vocab [{'word': '讨论', 'pinyin': 'tǎo lùn', 'trans': 'discuss'}, {'word': '大型', 'pinyin': 'dà xíng', 'trans': 'large-scale'}, {'word': '语言模型', 'pinyin': 'yǔ yán mó xíng', 'trans': 'language model'}, {'word': '代码生成', 'pinyin': 'dài mǎ shēng chéng', 'trans': 'code generation'}, {'word': '推理任务', 'pinyin': 'tuī lǐ rèn wù', 'trans': 'reasoning tasks'}, {'word': '代理系统', 'pinyin': 'dài lǐ xì tǒng', 'trans': 'proxy system'}, {'word': '重要性', 'pinyin': 'zhòng yào xìng', 'trans': 'importance'}, {'word': '开放访问', 'pinyin': 'kāi fàng fǎng wèn', 'trans': 'open access'}, {'word': '性能', 'pinyin': 'xìng néng', 'trans': 'performance'}, {'word': '接近', 'pinyin': 'jiē jìn', 'trans': 'close to'}, {'word': '专有模型', 'pinyin': 'zhuān yǒu mó xíng', 'trans': 'proprietary model'}, {'word': '适合', 'pinyin': 'shì hé', 'trans': 'suitable'}, {'word': '严格', 'pinyin': 'yán gé', 'trans': 'strict'}, {'word': '科学研究', 'pinyin': 'kē xué yán jiū', 'trans': 'scientific research'}, {'word': '高质量', 'pinyin': 'gāo zhì liàng', 'trans': 'high quality'}, {'word': '有限', 'pinyin': 'yǒu xiàn', 'trans': 'limited'}, {'word': '填补', 'pinyin': 'tián bǔ', 'trans': 'fill'}, {'word': '空白', 'pinyin': 'kòng bái', 'trans': 'gap'}, {'word': '作者', 'pinyin': 'zuò zhě', 'trans': 'author'}, {'word': '介绍', 'pinyin': 'jiè shào', 'trans': 'introduce'}, {'word': 'OpenCoder', 'pinyin': 'OpenCoder', 'trans': 'OpenCoder'}, {'word': '顶尖', 'pinyin': 'dǐng jiān', 'trans': 'top-notch'}, {'word': '媲美', 'pinyin': 'pì měi', 'trans': 'rival'}, {'word': '领先模型', 'pinyin': 'lǐng xiān mó xíng', 'trans': 'leading model'}, {'word': '详细', 'pinyin': 'xiáng xì', 'trans': 'detailed'}, {'word': '训练数据', 'pinyin': 'xùn liàn shù jù', 'trans': 'training data'}, {'word': '协议', 'pinyin': 'xié yì', 'trans': 'protocol'}, {'word': '开放性', 'pinyin': 'kāi fàng xìng', 'trans': 'openness'}, {'word': '加速', 'pinyin': 'jiā sù', 'trans': 'accelerate'}, {'word': '可重复', 'pinyin': 'kě chóng fù', 'trans': 'reproducible'}, {'word': '进展', 'pinyin': 'jìn zhǎn', 'trans': 'progress'}]
[11.11.2024 06:18] Renaming previous Chinese page.
[11.11.2024 06:18] Renaming previous data. zh.html to ./d/2024-11-10_zh_reading_task.html
[11.11.2024 06:18] Writing Chinese reading task.
[11.11.2024 06:18] Writing result.
[11.11.2024 06:18] Renaming log file.
[11.11.2024 06:18] Renaming previous data. log.txt to ./logs/2024-11-11_last_log.txt
