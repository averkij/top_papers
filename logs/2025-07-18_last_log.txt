[18.07.2025 18:17] Read previous papers.
[18.07.2025 18:17] Generating top page (month).
[18.07.2025 18:17] Writing top page (month).
[18.07.2025 19:11] Read previous papers.
[18.07.2025 19:11] Get feed.
[18.07.2025 19:11] Get page data from previous paper. URL: https://huggingface.co/papers/2507.13334
[18.07.2025 19:11] Get page data from previous paper. URL: https://huggingface.co/papers/2507.13348
[18.07.2025 19:11] Get page data from previous paper. URL: https://huggingface.co/papers/2507.13347
[18.07.2025 19:11] Get page data from previous paper. URL: https://huggingface.co/papers/2507.13332
[18.07.2025 19:11] Get page data from previous paper. URL: https://huggingface.co/papers/2507.12841
[18.07.2025 19:11] Get page data from previous paper. URL: https://huggingface.co/papers/2507.13344
[18.07.2025 19:11] Get page data from previous paper. URL: https://huggingface.co/papers/2507.12142
[18.07.2025 19:11] Get page data from previous paper. URL: https://huggingface.co/papers/2507.12508
[18.07.2025 19:11] Get page data from previous paper. URL: https://huggingface.co/papers/2507.12956
[18.07.2025 19:11] Get page data from previous paper. URL: https://huggingface.co/papers/2507.13264
[18.07.2025 19:11] Get page data from previous paper. URL: https://huggingface.co/papers/2507.13300
[18.07.2025 19:11] Get page data from previous paper. URL: https://huggingface.co/papers/2507.12990
[18.07.2025 19:11] Get page data from previous paper. URL: https://huggingface.co/papers/2507.12720
[18.07.2025 19:11] Get page data from previous paper. URL: https://huggingface.co/papers/2507.04984
[18.07.2025 19:11] Get page data from previous paper. URL: https://huggingface.co/papers/2507.13255
[18.07.2025 19:11] Get page data from previous paper. URL: https://huggingface.co/papers/2507.11589
[18.07.2025 19:11] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[18.07.2025 19:11] No deleted papers detected.
[18.07.2025 19:11] Downloading and parsing papers (pdf, html). Total: 16.
[18.07.2025 19:11] Downloading and parsing paper https://huggingface.co/papers/2507.13334.
[18.07.2025 19:11] Extra JSON file exists (./assets/json/2507.13334.json), skip PDF parsing.
[18.07.2025 19:11] Paper image links file exists (./assets/img_data/2507.13334.json), skip HTML parsing.
[18.07.2025 19:11] Success.
[18.07.2025 19:11] Downloading and parsing paper https://huggingface.co/papers/2507.13348.
[18.07.2025 19:11] Extra JSON file exists (./assets/json/2507.13348.json), skip PDF parsing.
[18.07.2025 19:11] Paper image links file exists (./assets/img_data/2507.13348.json), skip HTML parsing.
[18.07.2025 19:11] Success.
[18.07.2025 19:11] Downloading and parsing paper https://huggingface.co/papers/2507.13347.
[18.07.2025 19:11] Extra JSON file exists (./assets/json/2507.13347.json), skip PDF parsing.
[18.07.2025 19:11] Paper image links file exists (./assets/img_data/2507.13347.json), skip HTML parsing.
[18.07.2025 19:11] Success.
[18.07.2025 19:11] Downloading and parsing paper https://huggingface.co/papers/2507.13332.
[18.07.2025 19:11] Extra JSON file exists (./assets/json/2507.13332.json), skip PDF parsing.
[18.07.2025 19:11] Paper image links file exists (./assets/img_data/2507.13332.json), skip HTML parsing.
[18.07.2025 19:11] Success.
[18.07.2025 19:11] Downloading and parsing paper https://huggingface.co/papers/2507.12841.
[18.07.2025 19:11] Extra JSON file exists (./assets/json/2507.12841.json), skip PDF parsing.
[18.07.2025 19:11] Paper image links file exists (./assets/img_data/2507.12841.json), skip HTML parsing.
[18.07.2025 19:11] Success.
[18.07.2025 19:11] Downloading and parsing paper https://huggingface.co/papers/2507.13344.
[18.07.2025 19:11] Extra JSON file exists (./assets/json/2507.13344.json), skip PDF parsing.
[18.07.2025 19:11] Paper image links file exists (./assets/img_data/2507.13344.json), skip HTML parsing.
[18.07.2025 19:11] Success.
[18.07.2025 19:11] Downloading and parsing paper https://huggingface.co/papers/2507.12142.
[18.07.2025 19:11] Extra JSON file exists (./assets/json/2507.12142.json), skip PDF parsing.
[18.07.2025 19:11] Paper image links file exists (./assets/img_data/2507.12142.json), skip HTML parsing.
[18.07.2025 19:11] Success.
[18.07.2025 19:11] Downloading and parsing paper https://huggingface.co/papers/2507.12508.
[18.07.2025 19:11] Extra JSON file exists (./assets/json/2507.12508.json), skip PDF parsing.
[18.07.2025 19:11] Paper image links file exists (./assets/img_data/2507.12508.json), skip HTML parsing.
[18.07.2025 19:11] Success.
[18.07.2025 19:11] Downloading and parsing paper https://huggingface.co/papers/2507.12956.
[18.07.2025 19:11] Extra JSON file exists (./assets/json/2507.12956.json), skip PDF parsing.
[18.07.2025 19:11] Paper image links file exists (./assets/img_data/2507.12956.json), skip HTML parsing.
[18.07.2025 19:11] Success.
[18.07.2025 19:11] Downloading and parsing paper https://huggingface.co/papers/2507.13264.
[18.07.2025 19:11] Extra JSON file exists (./assets/json/2507.13264.json), skip PDF parsing.
[18.07.2025 19:11] Paper image links file exists (./assets/img_data/2507.13264.json), skip HTML parsing.
[18.07.2025 19:11] Success.
[18.07.2025 19:11] Downloading and parsing paper https://huggingface.co/papers/2507.13300.
[18.07.2025 19:11] Extra JSON file exists (./assets/json/2507.13300.json), skip PDF parsing.
[18.07.2025 19:11] Paper image links file exists (./assets/img_data/2507.13300.json), skip HTML parsing.
[18.07.2025 19:11] Success.
[18.07.2025 19:11] Downloading and parsing paper https://huggingface.co/papers/2507.12990.
[18.07.2025 19:11] Extra JSON file exists (./assets/json/2507.12990.json), skip PDF parsing.
[18.07.2025 19:11] Paper image links file exists (./assets/img_data/2507.12990.json), skip HTML parsing.
[18.07.2025 19:11] Success.
[18.07.2025 19:11] Downloading and parsing paper https://huggingface.co/papers/2507.12720.
[18.07.2025 19:11] Extra JSON file exists (./assets/json/2507.12720.json), skip PDF parsing.
[18.07.2025 19:11] Paper image links file exists (./assets/img_data/2507.12720.json), skip HTML parsing.
[18.07.2025 19:11] Success.
[18.07.2025 19:11] Downloading and parsing paper https://huggingface.co/papers/2507.04984.
[18.07.2025 19:11] Extra JSON file exists (./assets/json/2507.04984.json), skip PDF parsing.
[18.07.2025 19:11] Paper image links file exists (./assets/img_data/2507.04984.json), skip HTML parsing.
[18.07.2025 19:11] Success.
[18.07.2025 19:11] Downloading and parsing paper https://huggingface.co/papers/2507.13255.
[18.07.2025 19:11] Extra JSON file exists (./assets/json/2507.13255.json), skip PDF parsing.
[18.07.2025 19:11] Paper image links file exists (./assets/img_data/2507.13255.json), skip HTML parsing.
[18.07.2025 19:11] Success.
[18.07.2025 19:11] Downloading and parsing paper https://huggingface.co/papers/2507.11589.
[18.07.2025 19:11] Extra JSON file exists (./assets/json/2507.11589.json), skip PDF parsing.
[18.07.2025 19:11] Paper image links file exists (./assets/img_data/2507.11589.json), skip HTML parsing.
[18.07.2025 19:11] Success.
[18.07.2025 19:11] Enriching papers with extra data.
[18.07.2025 19:11] ********************************************************************************
[18.07.2025 19:11] Abstract 0. Context Engineering systematically optimizes information payloads for Large Language Models, addressing gaps in generating sophisticated, long-form outputs.  					AI-generated summary 				 The performance of Large Language Models (LLMs) is fundamentally determined by the contextual information provi...
[18.07.2025 19:11] ********************************************************************************
[18.07.2025 19:11] Abstract 1. VisionThink dynamically adjusts image resolution and visual token processing for efficient and effective vision-language tasks, improving performance on OCR tasks while reducing token usage in simpler tasks.  					AI-generated summary 				 Recent advancements in vision-language models (VLMs) have im...
[18.07.2025 19:11] ********************************************************************************
[18.07.2025 19:11] Abstract 2. A permutation-equivariant neural network, $\pi^3$, reconstructs visual geometry without a fixed reference view, achieving state-of-the-art performance in camera pose estimation, depth estimation, and point map reconstruction.  					AI-generated summary 				 We introduce pi^3, a feed-forward neural n...
[18.07.2025 19:11] ********************************************************************************
[18.07.2025 19:11] Abstract 3. TAIL, a method that imitates Turing Machine execution processes, enhances the length generalization and performance of LLMs by synthesizing chain-of-thought data and reducing shortcut learning.  					AI-generated summary 				 Length generalization, the ability to solve problems of longer sequences t...
[18.07.2025 19:11] ********************************************************************************
[18.07.2025 19:11] Abstract 4. The AnyCap Project introduces a framework, dataset, and evaluation protocol to enhance controllability and reliability in multimodal captioning.  					AI-generated summary 				 Controllable captioning is essential for precise multimodal alignment and instruction following, yet existing models often ...
[18.07.2025 19:11] ********************************************************************************
[18.07.2025 19:11] Abstract 5. A sliding iterative denoising process is proposed to enhance spatio-temporal consistency in 4D diffusion models for high-fidelity view synthesis from sparse-view videos.  					AI-generated summary 				 This paper addresses the challenge of high-fidelity view synthesis of humans with sparse-view vide...
[18.07.2025 19:11] ********************************************************************************
[18.07.2025 19:11] Abstract 6. RiemannLoRA addresses initialization and overparametrization in LoRA by treating LoRA matrices as a smooth manifold, improving convergence speed and performance in LLMs and diffusion models.  					AI-generated summary 				 Low-Rank Adaptation (LoRA) has become a widely adopted standard for parameter...
[18.07.2025 19:11] ********************************************************************************
[18.07.2025 19:11] Abstract 7. MindJourney enhances vision-language models with 3D reasoning by coupling them with a video diffusion-based world model, achieving improved performance on spatial reasoning tasks without fine-tuning.  					AI-generated summary 				 Spatial reasoning in 3D space is central to human cognition and indi...
[18.07.2025 19:11] ********************************************************************************
[18.07.2025 19:11] Abstract 8. FantasyPortrait, a diffusion transformer framework, generates high-fidelity and emotion-rich facial animations for single and multi-character scenarios using implicit representations and a masked cross-attention mechanism.  					AI-generated summary 				 Producing expressive facial animations from s...
[18.07.2025 19:11] ********************************************************************************
[18.07.2025 19:11] Abstract 9. Voxtral Mini and Voxtral Small are multimodal audio chat models that excel in understanding spoken audio and text, with a 32K context window for handling long audio files and conversations.  					AI-generated summary 				 We present Voxtral Mini and Voxtral Small, two multimodal audio chat models. V...
[18.07.2025 19:11] ********************************************************************************
[18.07.2025 19:11] Abstract 10. AbGen evaluates LLMs in designing ablation studies for scientific research, revealing performance gaps compared to human experts and highlighting the unreliability of current automated evaluation methods.  					AI-generated summary 				 We introduce AbGen, the first benchmark designed to evaluate th...
[18.07.2025 19:11] ********************************************************************************
[18.07.2025 19:11] Abstract 11. A residual learning approach enhances Sparse Autoencoders to capture domain-specific features without retraining, improving interpretability and performance on specialized domains.  					AI-generated summary 				 Sparse Autoencoders have emerged as powerful tools for interpreting the internal repres...
[18.07.2025 19:11] ********************************************************************************
[18.07.2025 19:11] Abstract 12. FLEXITOKENS, a byte-level language model with a learnable tokenizer, reduces token over-fragmentation and improves performance across multilingual and morphologically diverse tasks.  					AI-generated summary 				 Language models (LMs) are challenging to adapt to new data distributions by simple fin...
[18.07.2025 19:11] ********************************************************************************
[18.07.2025 19:11] Abstract 13. Temporal-Aware Latent Brownian Bridge Diffusion for Video Frame Interpolation (TLB-VFI) improves video frame interpolation by efficiently extracting temporal information, reducing parameters, and requiring less training data compared to existing methods.  					AI-generated summary 				 Video Frame I...
[18.07.2025 19:11] ********************************************************************************
[18.07.2025 19:11] Abstract 14. AutoSteer, a modular inference-time intervention technology, enhances the safety of Multimodal Large Language Models by reducing attack success rates across various threats without fine-tuning.  					AI-generated summary 				 Recent progress in Multimodal Large Language Models (MLLMs) has unlocked p...
[18.07.2025 19:11] ********************************************************************************
[18.07.2025 19:11] Abstract 15. Einstein Fields, a neural tensor field representation, compresses four-dimensional numerical relativity simulations into neural network weights, enabling automatic differentiation and natural emergence of dynamics.  					AI-generated summary 				 We introduce Einstein Fields, a neural representation...
[18.07.2025 19:11] Read previous papers.
[18.07.2025 19:11] Generating reviews via LLM API.
[18.07.2025 19:11] Using data from previous issue: {"categories": ["#data", "#survey", "#multimodal", "#long_context", "#architecture", "#rag"], "emoji": "üß†", "ru": {"title": "–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–∞—è –∏–Ω–∂–µ–Ω–µ—Ä–∏—è: –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ä–∞—Å–∫—Ä—ã—Ç–∏—è –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª–∞ LLM", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –æ–±–∑–æ—Ä –æ–±–ª–∞—Å—Ç–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–π –∏–Ω–∂–µ–Ω–µ—Ä–∏–∏ –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã
[18.07.2025 19:11] Using data from previous issue: {"categories": ["#cv", "#optimization", "#training", "#rl", "#games"], "emoji": "üîç", "ru": {"title": "–£–º–Ω–æ–µ –∑—Ä–µ–Ω–∏–µ: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å —á–µ—Ä–µ–∑ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–µ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ", "desc": "VisionThink - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –≤ –∑–∞–¥–∞—á–∞—Ö –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞. –û–Ω –¥–∏
[18.07.2025 19:11] Using data from previous issue: {"categories": ["#open_source", "#cv", "#optimization", "#architecture"], "emoji": "üî¨", "ru": {"title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ 3D —Å—Ü–µ–Ω –±–µ–∑ –æ–ø–æ—Ä–Ω—ã—Ö –∫–∞–¥—Ä–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –Ω–µ–π—Ä–æ–Ω–Ω—É—é —Å–µ—Ç—å œÄ^3 –¥–ª—è —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –≤–∏–∑—É–∞–ª—å–Ω–æ–π –≥–µ–æ–º–µ—Ç—Ä–∏–∏ –±–µ–∑ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –æ–ø–æ—Ä–Ω–æ–≥–æ –≤–∏–¥–∞. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç 
[18.07.2025 19:11] Using data from previous issue: {"categories": ["#data", "#synthetic", "#training", "#architecture", "#dataset", "#reasoning"], "emoji": "üß†", "ru": {"title": "–ò–º–∏—Ç–∞—Ü–∏—è –º–∞—à–∏–Ω—ã –¢—å—é—Ä–∏–Ω–≥–∞ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±–æ–±—â–∞—é—â–µ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–ú–µ—Ç–æ–¥ TAIL –∏–º–∏—Ç–∏—Ä—É–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å—ã –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –º–∞—à–∏–Ω—ã –¢—å—é—Ä–∏–Ω–≥–∞ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ 
[18.07.2025 19:11] Using data from previous issue: {"categories": ["#multimodal", "#optimization", "#benchmark", "#games", "#dataset"], "emoji": "üéõÔ∏è", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç—Ä–æ–ª—å –Ω–∞–¥ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–º–∏ –ø–æ–¥–ø–∏—Å—è–º–∏", "desc": "–ü—Ä–æ–µ–∫—Ç AnyCap –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ—Å—Ç–∏ –∏ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ —Å–æ–∑–¥–∞–Ω–∏—è –ø
[18.07.2025 19:11] Using data from previous issue: {"categories": ["#video", "#cv", "#diffusion", "#dataset"], "emoji": "üé•", "ru": {"title": "–£–ª—É—á—à–µ–Ω–∏–µ 4D-—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –≤ —Å–∏–Ω—Ç–µ–∑–µ –≤–∏–¥–æ–≤ —Å –ø–æ–º–æ—â—å—é —Å–∫–æ–ª—å–∑—è—â–µ–≥–æ —à—É–º–æ–ø–æ–¥–∞–≤–ª–µ–Ω–∏—è", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ-–≤—Ä–µ–º–µ–Ω–Ω–æ–π —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –≤ 4D-–¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö –¥–ª—è 
[18.07.2025 19:11] Using data from previous issue: {"categories": ["#training", "#optimization", "#architecture"], "emoji": "üß†", "ru": {"title": "–ì–µ–æ–º–µ—Ç—Ä–∏—è –Ω–∞ —Å–ª—É–∂–±–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π", "desc": "RiemannLoRA - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –∏ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—é –≤ –º–µ—Ç–æ–¥–µ LoRA –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –û–Ω —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç –º–∞—Ç—Ä–∏—Ü—ã LoRA –∫–∞–∫ –≥
[18.07.2025 19:11] Using data from previous issue: {"categories": ["#cv", "#3d", "#benchmark", "#diffusion", "#rl", "#reasoning", "#video"], "emoji": "üß†", "ru": {"title": "MindJourney: 3D-—Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –¥–ª—è –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è –±–µ–∑ –¥–æ–æ–±—É—á–µ–Ω–∏—è", "desc": "MindJourney - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ
[18.07.2025 19:11] Using data from previous issue: {"categories": ["#dataset", "#games", "#diffusion", "#benchmark", "#cv"], "emoji": "üé≠", "ru": {"title": "–û–∂–∏–≤–ª–µ–Ω–∏–µ –ø–æ—Ä—Ç—Ä–µ—Ç–æ–≤: –æ—Ç —Å—Ç–∞—Ç–∏–∫–∏ –∫ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–π –¥–∏–Ω–∞–º–∏–∫–µ", "desc": "FantasyPortrait - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–æ–≥–æ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∏ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ –±–æ–≥–∞—Ç—ã—Ö –∞–Ω
[18.07.2025 19:11] Using data from previous issue: {"categories": ["#benchmark", "#audio", "#small_models", "#multimodal", "#long_context", "#open_source"], "emoji": "üéôÔ∏è", "ru": {"title": "–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ Voxtral: –ø–æ–Ω–∏–º–∞–Ω–∏–µ —Ä–µ—á–∏ –∏ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –Ω–æ–≤–æ–º —É—Ä–æ–≤–Ω–µ", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã Voxtral Mini –∏ Voxtral Small - –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –∞—É–¥–∏–æ-—á–∞
[18.07.2025 19:11] Using data from previous issue: {"categories": ["#benchmark", "#dataset", "#science", "#interpretability"], "emoji": "üß™", "ru": {"title": "AbGen: –í—ã—è–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–±–µ–ª–æ–≤ –≤ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—è—Ö –ò–ò –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞—Ç—å –Ω–∞—É—á–Ω—ã–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã", "desc": "AbGen - —ç—Ç–æ –ø–µ—Ä–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ –∞–±–ª–∞—Ü–∏–æ–Ω–Ω—ã—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞
[18.07.2025 19:11] Using data from previous issue: {"categories": ["#training", "#architecture", "#data", "#interpretability", "#optimization"], "emoji": "üß†", "ru": {"title": "–ü–æ–≤—ã—à–µ–Ω–∏–µ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –ø–æ–º–æ—â—å—é –æ—Å—Ç–∞—Ç–æ—á–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã—Ö –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–æ–≤ (S
[18.07.2025 19:11] Using data from previous issue: {"categories": ["#low_resource", "#training", "#architecture", "#dataset", "#optimization", "#multilingual"], "emoji": "üß©", "ru": {"title": "–ì–∏–±–∫–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "FLEXITOKENS - —ç—Ç–æ –±–∞–π—Ç–æ–≤–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å —Å –æ–±—É—á–∞–µ–º—ã–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–º, –∫–æ—Ç–æ—Ä–∞—è —Å–Ω–∏–∂–∞–µ—Ç —á—Ä–µ–∑–º–µ—Ä–Ω—É—é —Ñ
[18.07.2025 19:11] Using data from previous issue: {"categories": ["#video", "#training", "#data", "#diffusion"], "emoji": "üéûÔ∏è", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—è –≤–∏–¥–µ–æ–∫–∞–¥—Ä–æ–≤ —Å –ø–æ–º–æ—â—å—é –≤—Ä–µ–º–µ–Ω–Ω–æ-–æ—Å–≤–µ–¥–æ–º–ª–µ–Ω–Ω–æ–π –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏–∏ –≤–∏–¥–µ–æ–∫–∞–¥—Ä–æ–≤ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º TLB-VFI, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω
[18.07.2025 19:11] Using data from previous issue: {"categories": ["#benchmark", "#inference", "#multimodal", "#security", "#interpretability"], "emoji": "üõ°Ô∏è", "ru": {"title": "AutoSteer: –∑–∞—â–∏—Ç–∞ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –ò–ò-—Å–∏—Å—Ç–µ–º –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è", "desc": "AutoSteer - —ç—Ç–æ –º–æ–¥—É–ª—å–Ω–∞—è —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è –≤–º–µ—à–∞—Ç–µ–ª—å—Å—Ç–≤–∞ –≤–æ –≤—Ä–µ–º—è –≤—ã–≤–æ–¥–∞, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–≤—ã—à–∞–µ—Ç –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –º—É–ª—å
[18.07.2025 19:11] Using data from previous issue: {"categories": ["#dataset", "#science", "#open_source", "#architecture"], "emoji": "üåå", "ru": {"title": "Einstein Fields: –ù–µ–π—Ä–æ–Ω–Ω–æ–µ —Å–∂–∞—Ç–∏–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞-–≤—Ä–µ–º–µ–Ω–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Einstein Fields - –Ω–µ–π—Ä–æ–Ω–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –¥–ª—è —Å–∂–∞—Ç–∏—è —á–µ—Ç—ã—Ä–µ—Ö–º–µ—Ä–Ω—ã—Ö —Å–∏–º—É–ª—è—Ü–∏–π —á–∏—Å–ª–µ–Ω–Ω–æ–π —Ç–µ–æ—Ä–∏–∏ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
[18.07.2025 19:11] Renaming data file.
[18.07.2025 19:11] Renaming previous data. hf_papers.json to ./d/2025-07-18.json
[18.07.2025 19:11] Saving new data file.
[18.07.2025 19:11] Generating page.
[18.07.2025 19:11] Renaming previous page.
[18.07.2025 19:11] Renaming previous data. index.html to ./d/2025-07-18.html
[18.07.2025 19:11] Writing result.
[18.07.2025 19:11] Renaming log file.
[18.07.2025 19:11] Renaming previous data. log.txt to ./logs/2025-07-18_last_log.txt
