[09.10.2025 06:19] Read previous papers.
[09.10.2025 06:19] Generating top page (month).
[09.10.2025 06:19] Writing top page (month).
[09.10.2025 07:12] Read previous papers.
[09.10.2025 07:12] Get feed.
[09.10.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.06917
[09.10.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.06308
[09.10.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.07315
[09.10.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.07310
[09.10.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.03215
[09.10.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.04678
[09.10.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.06751
[09.10.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.05862
[09.10.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.07318
[09.10.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.07238
[09.10.2025 07:12] Extract page data from URL. URL: https://huggingface.co/papers/2510.05057
[09.10.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.04212
[09.10.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.07143
[09.10.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.07313
[09.10.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.07307
[09.10.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.06783
[09.10.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.04999
[09.10.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.06261
[09.10.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.01982
[09.10.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.06855
[09.10.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.07041
[09.10.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.07037
[09.10.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.05891
[09.10.2025 07:12] Extract page data from URL. URL: https://huggingface.co/papers/2510.06673
[09.10.2025 07:12] Extract page data from URL. URL: https://huggingface.co/papers/2510.05491
[09.10.2025 07:12] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[09.10.2025 07:12] No deleted papers detected.
[09.10.2025 07:12] Downloading and parsing papers (pdf, html). Total: 25.
[09.10.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2510.06917.
[09.10.2025 07:12] Extra JSON file exists (./assets/json/2510.06917.json), skip PDF parsing.
[09.10.2025 07:12] Paper image links file exists (./assets/img_data/2510.06917.json), skip HTML parsing.
[09.10.2025 07:12] Success.
[09.10.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2510.06308.
[09.10.2025 07:12] Extra JSON file exists (./assets/json/2510.06308.json), skip PDF parsing.
[09.10.2025 07:12] Paper image links file exists (./assets/img_data/2510.06308.json), skip HTML parsing.
[09.10.2025 07:12] Success.
[09.10.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2510.07315.
[09.10.2025 07:12] Extra JSON file exists (./assets/json/2510.07315.json), skip PDF parsing.
[09.10.2025 07:12] Paper image links file exists (./assets/img_data/2510.07315.json), skip HTML parsing.
[09.10.2025 07:12] Success.
[09.10.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2510.07310.
[09.10.2025 07:12] Extra JSON file exists (./assets/json/2510.07310.json), skip PDF parsing.
[09.10.2025 07:12] Paper image links file exists (./assets/img_data/2510.07310.json), skip HTML parsing.
[09.10.2025 07:12] Success.
[09.10.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2510.03215.
[09.10.2025 07:12] Extra JSON file exists (./assets/json/2510.03215.json), skip PDF parsing.
[09.10.2025 07:12] Paper image links file exists (./assets/img_data/2510.03215.json), skip HTML parsing.
[09.10.2025 07:12] Success.
[09.10.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2510.04678.
[09.10.2025 07:12] Extra JSON file exists (./assets/json/2510.04678.json), skip PDF parsing.
[09.10.2025 07:12] Paper image links file exists (./assets/img_data/2510.04678.json), skip HTML parsing.
[09.10.2025 07:12] Success.
[09.10.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2510.06751.
[09.10.2025 07:12] Extra JSON file exists (./assets/json/2510.06751.json), skip PDF parsing.
[09.10.2025 07:12] Paper image links file exists (./assets/img_data/2510.06751.json), skip HTML parsing.
[09.10.2025 07:12] Success.
[09.10.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2510.05862.
[09.10.2025 07:12] Extra JSON file exists (./assets/json/2510.05862.json), skip PDF parsing.
[09.10.2025 07:12] Paper image links file exists (./assets/img_data/2510.05862.json), skip HTML parsing.
[09.10.2025 07:12] Success.
[09.10.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2510.07318.
[09.10.2025 07:12] Extra JSON file exists (./assets/json/2510.07318.json), skip PDF parsing.
[09.10.2025 07:12] Paper image links file exists (./assets/img_data/2510.07318.json), skip HTML parsing.
[09.10.2025 07:12] Success.
[09.10.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2510.07238.
[09.10.2025 07:12] Extra JSON file exists (./assets/json/2510.07238.json), skip PDF parsing.
[09.10.2025 07:12] Paper image links file exists (./assets/img_data/2510.07238.json), skip HTML parsing.
[09.10.2025 07:12] Success.
[09.10.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2510.05057.
[09.10.2025 07:12] Downloading paper 2510.05057 from http://arxiv.org/pdf/2510.05057v1...
[09.10.2025 07:12] Extracting affiliations from text.
[09.10.2025 07:12] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 6 ] . [ 1 7 5 0 5 0 . 0 1 5 2 : r a STAMO: UNSUPERVISED LEARNING OF GENERALIZABLE ROBOT MOTION FROM COMPACT STATE REPRESENTATION Mingyu Liu1 Jiuhe Shu1 Hui Chen1 Zeju Li1 Canyu Zhao1 Jiange Yang2 Shenyuan Gao3 Hao Chen1 Chunhua Shen1 1 Zhejiang University 3 Hong Kong University of Science and Technology 2 Nanjing University "
[09.10.2025 07:12] Response: ```python
["Zhejiang University", "Hong Kong University of Science and Technology", "Nanjing University"]
```
[09.10.2025 07:12] Deleting PDF ./assets/pdf/2510.05057.pdf.
[09.10.2025 07:12] Success.
[09.10.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2510.04212.
[09.10.2025 07:12] Extra JSON file exists (./assets/json/2510.04212.json), skip PDF parsing.
[09.10.2025 07:12] Paper image links file exists (./assets/img_data/2510.04212.json), skip HTML parsing.
[09.10.2025 07:12] Success.
[09.10.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2510.07143.
[09.10.2025 07:12] Extra JSON file exists (./assets/json/2510.07143.json), skip PDF parsing.
[09.10.2025 07:12] Paper image links file exists (./assets/img_data/2510.07143.json), skip HTML parsing.
[09.10.2025 07:12] Success.
[09.10.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2510.07313.
[09.10.2025 07:12] Extra JSON file exists (./assets/json/2510.07313.json), skip PDF parsing.
[09.10.2025 07:12] Paper image links file exists (./assets/img_data/2510.07313.json), skip HTML parsing.
[09.10.2025 07:12] Success.
[09.10.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2510.07307.
[09.10.2025 07:12] Extra JSON file exists (./assets/json/2510.07307.json), skip PDF parsing.
[09.10.2025 07:12] Paper image links file exists (./assets/img_data/2510.07307.json), skip HTML parsing.
[09.10.2025 07:12] Success.
[09.10.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2510.06783.
[09.10.2025 07:12] Extra JSON file exists (./assets/json/2510.06783.json), skip PDF parsing.
[09.10.2025 07:12] Paper image links file exists (./assets/img_data/2510.06783.json), skip HTML parsing.
[09.10.2025 07:12] Success.
[09.10.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2510.04999.
[09.10.2025 07:12] Extra JSON file exists (./assets/json/2510.04999.json), skip PDF parsing.
[09.10.2025 07:12] Paper image links file exists (./assets/img_data/2510.04999.json), skip HTML parsing.
[09.10.2025 07:12] Success.
[09.10.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2510.06261.
[09.10.2025 07:12] Extra JSON file exists (./assets/json/2510.06261.json), skip PDF parsing.
[09.10.2025 07:12] Paper image links file exists (./assets/img_data/2510.06261.json), skip HTML parsing.
[09.10.2025 07:12] Success.
[09.10.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2510.01982.
[09.10.2025 07:12] Extra JSON file exists (./assets/json/2510.01982.json), skip PDF parsing.
[09.10.2025 07:12] Paper image links file exists (./assets/img_data/2510.01982.json), skip HTML parsing.
[09.10.2025 07:12] Success.
[09.10.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2510.06855.
[09.10.2025 07:12] Extra JSON file exists (./assets/json/2510.06855.json), skip PDF parsing.
[09.10.2025 07:12] Paper image links file exists (./assets/img_data/2510.06855.json), skip HTML parsing.
[09.10.2025 07:12] Success.
[09.10.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2510.07041.
[09.10.2025 07:12] Extra JSON file exists (./assets/json/2510.07041.json), skip PDF parsing.
[09.10.2025 07:12] Paper image links file exists (./assets/img_data/2510.07041.json), skip HTML parsing.
[09.10.2025 07:12] Success.
[09.10.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2510.07037.
[09.10.2025 07:12] Extra JSON file exists (./assets/json/2510.07037.json), skip PDF parsing.
[09.10.2025 07:12] Paper image links file exists (./assets/img_data/2510.07037.json), skip HTML parsing.
[09.10.2025 07:12] Success.
[09.10.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2510.05891.
[09.10.2025 07:12] Extra JSON file exists (./assets/json/2510.05891.json), skip PDF parsing.
[09.10.2025 07:12] Paper image links file exists (./assets/img_data/2510.05891.json), skip HTML parsing.
[09.10.2025 07:12] Success.
[09.10.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2510.06673.
[09.10.2025 07:12] Downloading paper 2510.06673 from http://arxiv.org/pdf/2510.06673v1...
[09.10.2025 07:12] Extracting affiliations from text.
[09.10.2025 07:12] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 8 ] . [ 1 3 7 6 6 0 . 0 1 5 2 : r Heptapod: Language Modeling on Visual Signals Yongxin Zhu1,, Jiawei Chen1, Yuanzhe Chen1, Zhuo Chen1, Dongya Jia1, Jian Cong1, Xiaobin Zhuang1, Yuping Wang1, Yuxuan Wang1 1ByteDance Seed Work done during an internship at ByteDance Seed "
[09.10.2025 07:12] Response: ```python
["ByteDance Seed"]
```
[09.10.2025 07:12] Deleting PDF ./assets/pdf/2510.06673.pdf.
[09.10.2025 07:12] Success.
[09.10.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2510.05491.
[09.10.2025 07:12] Downloading paper 2510.05491 from http://arxiv.org/pdf/2510.05491v1...
[09.10.2025 07:12] Extracting affiliations from text.
[09.10.2025 07:12] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"NorMuon: Making Muon more efficient and scalable Zichong Li 1, Liming Liu1, Chen Liang2, Weizhu Chen2, Tuo Zhao1 1Georgia Tech 2Microsoft Abstract The choice of optimizer significantly impacts the training efficiency and computational costs of large language models (LLMs). Recently, the Muon optimizer has demonstrated promising results by orthogonalizing parameter updates, improving optimization geometry through better conditioning. Despite Muons emergence as candidate successor to Adam, the potential for jointly leveraging their strengthshas not been systematically explored. In this work, we bridge this gap by proposing NorMuon (Neuron-wise Normalized Muon), an optimizer that synergistically combines orthogonalization with neuron-level adaptive learning rates. Our analysis reveals that while Muon effectively reduces condition numbers, the resulting updates exhibit highly non-uniform neuron norms, causing certain neurons to dominate the optimization process. NorMuon addresses this imbalance by maintaining second-order momentum statistics for each neuron and applying row-wise normalization after orthogonalization, ensuring balanced parameter utilization while preserving Muons conditioning benefits. To enable practical deployment at scale, we develop an efficient distributed implementation under the FSDP2 framework that strategically distributes orthogonalization computations across devices. Experiments across multiple model scales demonstrate that NorMuon consistently outperforms both Adam and Muon, achieving 21.74% better training efficiency than Adam and 11.31% improvement over Muon on 1.1B pretraining setting, while maintaining comparable memory footprint to Muon. Our findings suggest that orthogonalization and adaptive learning rates are complementary rather than competing approaches, opening new avenues for optimizer design in large-scale deep learning. We open-source our implementation at https://github.com/zichongli5/NorMuon.git. 5 2 0 2 7 ] . [ 1 1 9 4 5 0 . "
[09.10.2025 07:12] Response: ```python
["Georgia Tech", "Microsoft"]
```
[09.10.2025 07:12] Deleting PDF ./assets/pdf/2510.05491.pdf.
[09.10.2025 07:12] Success.
[09.10.2025 07:12] Enriching papers with extra data.
[09.10.2025 07:12] ********************************************************************************
[09.10.2025 07:12] Abstract 0. SHANKS, a general inference framework, enables spoken language models to generate unspoken reasoning while listening to user input, enhancing real-time interaction and task completion.  					AI-generated summary 				 Current large language models (LLMs) and spoken language models (SLMs) begin thinki...
[09.10.2025 07:12] ********************************************************************************
[09.10.2025 07:12] Abstract 1. Lumina-DiMOO, an open-source foundational model, uses fully discrete diffusion modeling for efficient multi-modal generation and understanding, outperforming existing models.  					AI-generated summary 				 We introduce Lumina-DiMOO, an open-source foundational model for seamless multi-modal generat...
[09.10.2025 07:12] ********************************************************************************
[09.10.2025 07:12] Abstract 2. Vibe Checker evaluates LLMs by combining functional correctness and instruction following to better align with human coding preferences.  					AI-generated summary 				 Large Language Models (LLMs) have catalyzed vibe coding, where users leverage LLMs to generate and iteratively refine code through ...
[09.10.2025 07:12] ********************************************************************************
[09.10.2025 07:12] Abstract 3. MATRIX-11K dataset and MATRIX regularization enhance interaction fidelity and semantic alignment in video DiTs by aligning attention with multi-instance mask tracks.  					AI-generated summary 				 Video DiTs have advanced video generation, yet they still struggle to model multi-instance or subject-...
[09.10.2025 07:12] ********************************************************************************
[09.10.2025 07:12] Abstract 4. Cache-to-Cache (C2C) enables direct semantic communication between LLMs using neural network projections, improving accuracy and reducing latency compared to text-based communication.  					AI-generated summary 				 Multi-LLM systems harness the complementary strengths of diverse Large Language Mode...
[09.10.2025 07:12] ********************************************************************************
[09.10.2025 07:12] Abstract 5. MATPO, a reinforcement learning method, optimizes tool-integrated multi-agent roles within a single LLM, improving performance and robustness over single-agent systems.  					AI-generated summary 				 Large language models (LLMs) increasingly rely on multi-turn tool-integrated planning for knowledge...
[09.10.2025 07:12] ********************************************************************************
[09.10.2025 07:12] Abstract 6. OBS-Diff is a novel one-shot pruning framework that compresses large-scale text-to-image diffusion models with minimal quality loss and significant inference acceleration.  					AI-generated summary 				 Large-scale text-to-image diffusion models, while powerful, suffer from prohibitive computationa...
[09.10.2025 07:12] ********************************************************************************
[09.10.2025 07:12] Abstract 7. Context Denoising Training (CDT) improves long-context models' performance by mitigating contextual noise and enhancing attention on critical tokens.  					AI-generated summary 				 Long-context models (LCMs) have demonstrated great potential in processing long sequences, facilitating many real-worl...
[09.10.2025 07:12] ********************************************************************************
[09.10.2025 07:12] Abstract 8. A memory framework combining short-term and long-term memory in neural networks improves long-sequence modeling efficiency and performance.  					AI-generated summary 				 Long-sequence modeling faces a fundamental trade-off between the efficiency of compressive fixed-size memory in RNN-like models ...
[09.10.2025 07:12] ********************************************************************************
[09.10.2025 07:12] Abstract 9. Research investigates the aging of factuality benchmarks and its impact on evaluating the factuality of large language models, revealing significant unreliability due to outdated samples.  					AI-generated summary 				 The rapid evolution of large language models (LLMs) and the real world has outpa...
[09.10.2025 07:12] ********************************************************************************
[09.10.2025 07:12] Abstract 10. An unsupervised method learns a compact state representation using a lightweight encoder and Diffusion Transformer decoder, improving robotic performance and enabling latent action decoding from static images.  					AI-generated summary 				 A fundamental challenge in embodied intelligence is develo...
[09.10.2025 07:12] ********************************************************************************
[09.10.2025 07:12] Abstract 11. Low-precision training of transformer models with flash attention suffers from catastrophic loss explosions due to low-rank representations and biased rounding errors, which are addressed by a minimal modification to the flash attention mechanism.  					AI-generated summary 				 The pursuit of compu...
[09.10.2025 07:12] ********************************************************************************
[09.10.2025 07:12] Abstract 12. VTC-Bench is introduced to provide a fair evaluation framework for visual token compression by incorporating a data filtering mechanism to denoise existing benchmarks.  					AI-generated summary 				 Recent endeavors to accelerate inference in Multimodal Large Language Models (MLLMs) have primarily ...
[09.10.2025 07:12] ********************************************************************************
[09.10.2025 07:12] Abstract 13. WristWorld is a 4D world model that generates wrist-view videos from anchor views, improving video generation consistency and VLA performance.  					AI-generated summary 				 Wrist-view observations are crucial for VLA models as they capture fine-grained hand-object interactions that directly enhanc...
[09.10.2025 07:12] ********************************************************************************
[09.10.2025 07:12] Abstract 14. MLE-Smith automates the creation of high-quality, diverse MLE tasks from raw datasets using a multi-agent pipeline, improving scalability and maintaining task quality.  					AI-generated summary 				 While Language Models (LMs) have made significant progress in automating machine learning engineerin...
[09.10.2025 07:12] ********************************************************************************
[09.10.2025 07:12] Abstract 15. TTRV enhances vision language understanding through test-time reinforcement learning, improving performance on object recognition and VQA without labeled data.  					AI-generated summary 				 Existing methods for extracting reward signals in Reinforcement Learning typically rely on labeled data and ...
[09.10.2025 07:12] ********************************************************************************
[09.10.2025 07:12] Abstract 16. A survey of text-to-video generative models from GANs and VAEs to hybrid Diffusion-Transformer architectures, detailing their development, limitations, and future directions.  					AI-generated summary 				 Text-to-video (T2V) generation technology holds potential to transform multiple domains such ...
[09.10.2025 07:12] ********************************************************************************
[09.10.2025 07:12] Abstract 17. AlphaApollo, a self-evolving reasoning system, enhances foundation model performance through tool integration and iterative refinement, achieving significant improvements in accuracy and pass rates.  					AI-generated summary 				 We present AlphaApollo, a self-evolving agentic reasoning system that...
[09.10.2025 07:12] ********************************************************************************
[09.10.2025 07:12] Abstract 18. A novel Granular-GRPO framework enhances reinforcement learning in diffusion and flow models by improving reward assessment and reducing bias in denoising.  					AI-generated summary 				 The integration of online reinforcement learning (RL) into diffusion and flow models has recently emerged as a p...
[09.10.2025 07:12] ********************************************************************************
[09.10.2025 07:12] Abstract 19. A novel framework for real-time event boundary detection in streaming videos uses prediction and error measurement to identify subtle event changes.  					AI-generated summary 				 Generic Event Boundary Detection (GEBD) aims to interpret long-form videos through the lens of human perception. Howeve...
[09.10.2025 07:12] ********************************************************************************
[09.10.2025 07:12] Abstract 20. U-Bench is a comprehensive benchmark evaluating 100 U-Net variants across 28 datasets and 10 imaging modalities, focusing on statistical robustness, zero-shot generalization, and computational efficiency.  					AI-generated summary 				 Over the past decade, U-Net has been the dominant architecture ...
[09.10.2025 07:12] ********************************************************************************
[09.10.2025 07:12] Abstract 21. This survey analyzes the current state of code-switching aware large language models, highlighting advancements and challenges in multilingual NLP.  					AI-generated summary 				 Code-switching (CSW), the alternation of languages and scripts within a single utterance, remains a fundamental challeng...
[09.10.2025 07:12] ********************************************************************************
[09.10.2025 07:12] Abstract 22. A novel method using Discrete Distribution Discrepancy-aware Quantization Error (D$^3$QE) detects images generated by visual autoregressive models by analyzing codebook frequency statistics and quantization errors.  					AI-generated summary 				 The emergence of visual autoregressive (AR) models ha...
[09.10.2025 07:12] ********************************************************************************
[09.10.2025 07:12] Abstract 23. Heptapod, an image autoregressive model using causal attention and next 2D distribution prediction, achieves superior performance on ImageNet generation by combining sequential modeling with holistic self-supervised learning.  					AI-generated summary 				 We introduce Heptapod, an image autoregres...
[09.10.2025 07:12] ********************************************************************************
[09.10.2025 07:12] Abstract 24. NorMuon, a novel optimizer combining orthogonalization with neuron-level adaptive learning rates, enhances training efficiency and balances parameter utilization in large language models.  					AI-generated summary 				 The choice of optimizer significantly impacts the training efficiency and comput...
[09.10.2025 07:12] Read previous papers.
[09.10.2025 07:12] Generating reviews via LLM API.
[09.10.2025 07:12] Using data from previous issue: {"categories": ["#multimodal", "#reasoning", "#inference", "#long_context"], "emoji": "üéß", "ru": {"title": "–î—É–º–∞–π –ø–æ–∫–∞ —Å–ª—É—à–∞–µ—à—å: —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏ –¥–ª—è —Ä–∞–∑–≥–æ–≤–æ—Ä–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω SHANKS ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è spoken language models (SLM), –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª—è–º –≥–µ–Ω–µ—Ä
[09.10.2025 07:12] Using data from previous issue: {"categories": ["#open_source", "#multimodal", "#benchmark", "#diffusion", "#architecture"], "emoji": "üé≠", "ru": {"title": "–î–∏—Å–∫—Ä–µ—Ç–Ω–∞—è –¥–∏—Ñ—Ñ—É–∑–∏—è –¥–ª—è —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–∏", "desc": "Lumina-DiMOO - —ç—Ç–æ –æ—Ç–∫—Ä—ã—Ç–∞—è foundational –º–æ–¥–µ–ª—å –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞. –í –æ—Ç–ª–∏—á–∏–µ
[09.10.2025 07:12] Using data from previous issue: {"categories": ["#interpretability", "#alignment", "#training", "#benchmark", "#plp"], "emoji": "‚ú®", "ru": {"title": "Vibe Check: –∫–æ–≥–¥–∞ –∫–æ–¥ –¥–æ–ª–∂–µ–Ω –Ω–µ —Ç–æ–ª—å–∫–æ —Ä–∞–±–æ—Ç–∞—Ç—å, –Ω–æ –∏ –Ω—Ä–∞–≤–∏—Ç—å—Å—è", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ Vibe Checker ‚Äî –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ—Ü–µ–Ω–∫–µ LLM –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞, –∫–æ—Ç–æ—Ä—ã–π —É—á–∏—Ç—ã–≤–∞–µ—Ç –Ω–µ
[09.10.2025 07:12] Using data from previous issue: {"categories": ["#dataset", "#hallucinations", "#benchmark", "#video", "#interpretability"], "emoji": "üé≠", "ru": {"title": "–£–ª—É—á—à–µ–Ω–∏–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π –≤ –≤–∏–¥–µ–æ —á–µ—Ä–µ–∑ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –≤–Ω–∏–º–∞–Ω–∏—è –ø–æ –º–∞—Å–∫–∞–º –æ–±—ä–µ–∫—Ç–æ–≤", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Å–æ–∑–¥–∞–ª–∏ –¥–∞—Ç–∞—Å–µ—Ç MATRIX-11K —Å –≤–∏–¥–µ–æ, —Å–æ–¥–µ—Ä–∂–∞—â–∏–º–∏ –æ–ø–∏—Å–∞–Ω–∏—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π
[09.10.2025 07:12] Using data from previous issue: {"categories": ["#architecture", "#training", "#multimodal", "#optimization", "#agi"], "emoji": "üîÑ", "ru": {"title": "–û–±—â–µ–Ω–∏–µ LLM –±–µ–∑ —Å–ª–æ–≤: –ø—Ä—è–º–∞—è –ø–µ—Ä–µ–¥–∞—á–∞ —Å–µ–º–∞–Ω—Ç–∏–∫–∏ —á–µ—Ä–µ–∑ –∫—ç—à", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ Cache-to-Cache (C2C) –¥–ª—è –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏ –º–µ–∂–¥—É –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ LLM, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥
[09.10.2025 07:12] Using data from previous issue: {"categories": ["#training", "#reasoning", "#optimization", "#agents", "#rl"], "emoji": "üé≠", "ru": {"title": "–û–¥–∏–Ω LLM –≤ —Ä–æ–ª–∏ —Ü–µ–ª–æ–π –∫–æ–º–∞–Ω–¥—ã –∞–≥–µ–Ω—Ç–æ–≤", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ MATPO ‚Äî –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º –≤–Ω—É—Ç—Ä–∏ –æ–¥–Ω–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏. –¢—Ä–∞–¥–∏—Ü–∏–æ
[09.10.2025 07:12] Using data from previous issue: {"categories": ["#optimization", "#training", "#inference", "#diffusion", "#architecture"], "emoji": "‚úÇÔ∏è", "ru": {"title": "–£–º–Ω–æ–µ —Å–∂–∞—Ç–∏–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∑–∞ –æ–¥–∏–Ω –ø—Ä–æ—Ö–æ–¥", "desc": "OBS-Diff ‚Äî —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–¥–Ω–æ—à–∞–≥–æ–≤–æ–π –æ–±—Ä–µ–∑–∫–∏ (pruning) –¥–ª—è —Å–∂–∞—Ç–∏—è –±–æ–ª—å—à–∏—Ö —Ç–µ–∫—Å—Ç-–≤-–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
[09.10.2025 07:12] Using data from previous issue: {"categories": ["#training", "#long_context", "#optimization"], "emoji": "üéØ", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ —Å –æ—á–∏—Å—Ç–∫–æ–π –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: —Ñ–æ–∫—É—Å –Ω–∞ –≤–∞–∂–Ω–æ–º", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥ Context Denoising Training (CDT) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞–±–æ—Ç—ã –º–æ–¥–µ–ª–µ–π —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º. –ü—Ä–æ–±–ª–µ–º–∞ –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —Ç–æ–º, —á—Ç–æ 
[09.10.2025 07:12] Using data from previous issue: {"categories": ["#architecture", "#training", "#benchmark", "#optimization", "#long_context"], "emoji": "üß†", "ru": {"title": "–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –≥–∏–ø–ø–æ–∫–∞–º–ø –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –ø–∞–º—è—Ç–∏ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –ø–∞–º—è—Ç–∏ –¥–ª—è –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π, –≤–¥–æ—Ö–Ω–æ–≤–ª—ë–Ω–Ω—É—é –º–æ–¥–µ–ª—å—é –º–Ω–æ–≥–æ–∫–æ–º–ø–æ–Ω–µ–Ω—Ç–Ω–æ–π –ø–∞–º—è—Ç
[09.10.2025 07:12] Using data from previous issue: {"categories": ["#interpretability", "#hallucinations", "#benchmark"], "emoji": "‚è≥", "ru": {"title": "–ö–æ–≥–¥–∞ –±–µ–Ω—á–º–∞—Ä–∫–∏ —Å—Ç–∞—Ä–µ—é—Ç: –ø—Ä–æ–±–ª–µ–º–∞ —É—Å—Ç–∞—Ä–µ–≤—à–∏—Ö —Ç–µ—Å—Ç–æ–≤ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ñ–∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç–∏ LLM", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ –±–µ–Ω—á–º–∞—Ä–∫–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ñ–∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç–∏ LLM —É—Å—Ç–∞—Ä–µ–≤–∞—é—Ç —Å–æ –≤—Ä–µ–º–µ–Ω–µ–º, —á—Ç–æ
[09.10.2025 07:12] Querying the API.
[09.10.2025 07:12] Claude request. Model: claude-sonnet-4-5-20250929. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

An unsupervised method learns a compact state representation using a lightweight encoder and Diffusion Transformer decoder, improving robotic performance and enabling latent action decoding from static images.  					AI-generated summary 				 A fundamental challenge in embodied intelligence is developing expressive and compact state representations for efficient world modeling and decision making. However, existing methods often fail to achieve this balance, yielding representations that are either overly redundant or lacking in task-critical information. We propose an unsupervised approach that learns a highly compressed two-token state representation using a lightweight encoder and a pre-trained Diffusion Transformer (DiT) decoder, capitalizing on its strong generative prior. Our representation is efficient, interpretable, and integrates seamlessly into existing VLA-based models, improving performance by 14.3% on LIBERO and 30% in real-world task success with minimal inference overhead. More importantly, we find that the difference between these tokens, obtained via latent interpolation, naturally serves as a highly effective latent action, which can be further decoded into executable robot actions. This emergent capability reveals that our representation captures structured dynamics without explicit supervision. We name our method StaMo for its ability to learn generalizable robotic Motion from compact State representation, which is encoded from static images, challenging the prevalent dependence to learning latent action on complex architectures and video data. The resulting latent actions also enhance policy co-training, outperforming prior methods by 10.4% with improved interpretability. Moreover, our approach scales effectively across diverse data sources, including real-world robot data, simulation, and human egocentric video.
[09.10.2025 07:12] Response: ```json
{
  "title": "–î–≤–∞ —Ç–æ–∫–µ–Ω–∞ –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ä–æ–±–æ—Ç–æ–º: –∫–æ–º–ø–∞–∫—Ç–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –∏ –¥–µ–π—Å—Ç–≤–∏—è",
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –º–µ—Ç–æ–¥ StaMo –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∫–æ–º–ø–∞–∫—Ç–Ω–æ–≥–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏—è —Ä–æ–±–æ—Ç–∞ –≤—Å–µ–≥–æ –≤ –¥–≤–∞ —Ç–æ–∫–µ–Ω–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ª–µ–≥–∫–æ–≤–µ—Å–Ω–æ–≥–æ —ç–Ω–∫–æ–¥–µ—Ä–∞ –∏ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–≥–æ Diffusion Transformer –¥–µ–∫–æ–¥–µ—Ä–∞. –ö–ª—é—á–µ–≤–∞—è –Ω–∞—Ö–æ–¥–∫–∞: —Ä–∞–∑–Ω–æ—Å—Ç—å –º–µ–∂–¥—É —ç—Ç–∏–º–∏ —Ç–æ–∫–µ–Ω–∞–º–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–º –æ–±—Ä–∞–∑–æ–º —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç –ª–∞—Ç–µ–Ω—Ç–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ, –∫–æ—Ç–æ—Ä–æ–µ –º–æ–∂–Ω–æ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å –≤ –∫–æ–º–∞–Ω–¥—ã –¥–ª—è —Ä–æ–±–æ—Ç–∞, –ø—Ä–∏—á—ë–º —ç—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –±–µ–∑ —è–≤–Ω–æ–≥–æ supervised –æ–±—É—á–µ–Ω–∏—è. –ú–µ—Ç–æ–¥ —É–ª—É—á—à–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å VLA-–º–æ–¥–µ–ª–µ–π –Ω–∞ 14.3% –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–µ LIBERO –∏ –Ω–∞ 30% –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö, –ø—Ä–∏ —ç—Ç–æ–º —Ä–∞–±–æ—Ç–∞—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–º–∏ –∑–∞—Ç—Ä–∞—Ç–∞–º–∏. –ü–æ–¥—Ö–æ–¥ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ—Ç—Å—è –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–∞–Ω–Ω—ã—Ö –≤–∫–ª—é—á–∞—è —Ä–µ–∞–ª—å–Ω—ã–µ —Ä–æ–±–æ—Ç—ã, —Å–∏–º—É–ª—è—Ü–∏–∏ –∏ egocentric –≤–∏–¥–µ–æ –ª—é–¥–µ–π, –æ–±—É—á–∞—è –ª–∞—Ç–µ–Ω—Ç–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—è –∏–∑ —Å—Ç–∞—Ç–∏—á–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –±–µ–∑ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –≤–∏–¥–µ–æ–¥–∞–Ω–Ω—ã—Ö.",
  "emoji": "ü§ñ",
  "desc_en": ""
}
```
[09.10.2025 07:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"An unsupervised method learns a compact state representation using a lightweight encoder and Diffusion Transformer decoder, improving robotic performance and enabling latent action decoding from static images.  					AI-generated summary 				 A fundamental challenge in embodied intelligence is developing expressive and compact state representations for efficient world modeling and decision making. However, existing methods often fail to achieve this balance, yielding representations that are either overly redundant or lacking in task-critical information. We propose an unsupervised approach that learns a highly compressed two-token state representation using a lightweight encoder and a pre-trained Diffusion Transformer (DiT) decoder, capitalizing on its strong generative prior. Our representation is efficient, interpretable, and integrates seamlessly into existing VLA-based models, improving performance by 14.3% on LIBERO and 30% in real-world task success with minimal inference overhead. More importantly, we find that the difference between these tokens, obtained via latent interpolation, naturally serves as a highly effective latent action, which can be further decoded into executable robot actions. This emergent capability reveals that our representation captures structured dynamics without explicit supervision. We name our method StaMo for its ability to learn generalizable robotic Motion from compact State representation, which is encoded from static images, challenging the prevalent dependence to learning latent action on complex architectures and video data. The resulting latent actions also enhance policy co-training, outperforming prior methods by 10.4% with improved interpretability. Moreover, our approach scales effectively across diverse data sources, including real-world robot data, simulation, and human egocentric video."

[09.10.2025 07:12] Response: ```python
['AGENTS', 'ROBOTICS', 'TRAINING']
```
[09.10.2025 07:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"An unsupervised method learns a compact state representation using a lightweight encoder and Diffusion Transformer decoder, improving robotic performance and enabling latent action decoding from static images.  					AI-generated summary 				 A fundamental challenge in embodied intelligence is developing expressive and compact state representations for efficient world modeling and decision making. However, existing methods often fail to achieve this balance, yielding representations that are either overly redundant or lacking in task-critical information. We propose an unsupervised approach that learns a highly compressed two-token state representation using a lightweight encoder and a pre-trained Diffusion Transformer (DiT) decoder, capitalizing on its strong generative prior. Our representation is efficient, interpretable, and integrates seamlessly into existing VLA-based models, improving performance by 14.3% on LIBERO and 30% in real-world task success with minimal inference overhead. More importantly, we find that the difference between these tokens, obtained via latent interpolation, naturally serves as a highly effective latent action, which can be further decoded into executable robot actions. This emergent capability reveals that our representation captures structured dynamics without explicit supervision. We name our method StaMo for its ability to learn generalizable robotic Motion from compact State representation, which is encoded from static images, challenging the prevalent dependence to learning latent action on complex architectures and video data. The resulting latent actions also enhance policy co-training, outperforming prior methods by 10.4% with improved interpretability. Moreover, our approach scales effectively across diverse data sources, including real-world robot data, simulation, and human egocentric video."

[09.10.2025 07:12] Response: ```python
["DIFFUSION", "INTERPRETABILITY", "OPTIMIZATION"]
```
[09.10.2025 07:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents an unsupervised method called StaMo that learns a compact state representation for robotic applications using a lightweight encoder and a Diffusion Transformer decoder. The method addresses the challenge of creating efficient and expressive state representations, which are crucial for decision-making in robotics. By utilizing a two-token representation, StaMo improves performance significantly on various benchmarks while enabling the decoding of latent actions from static images. This approach not only enhances interpretability and efficiency but also demonstrates the ability to capture structured dynamics without requiring explicit supervision.","title":"Efficient State Representation for Enhanced Robotic Motion"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper presents an unsupervised method called StaMo that learns a compact state representation for robotic applications using a lightweight encoder and a Diffusion Transformer decoder. The method addresses the challenge of creating efficient and expressive state representations, which are crucial for decision-making in robotics. By utilizing a two-token representation, StaMo improves performance significantly on various benchmarks while enabling the decoding of latent actions from static images. This approach not only enhances interpretability and efficiency but also demonstrates the ability to capture structured dynamics without requiring explicit supervision.', title='Efficient State Representation for Enhanced Robotic Motion'))
[09.10.2025 07:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊó†ÁõëÁù£ÁöÑÊñπÊ≥ïÔºåÈÄöËøáËΩªÈáèÁ∫ßÁºñÁ†ÅÂô®ÂíåÊâ©Êï£ÂèòÊç¢Âô®Ëß£Á†ÅÂô®Â≠¶‰π†Á¥ßÂáëÁöÑÁä∂ÊÄÅË°®Á§∫„ÄÇËøôÁßçÊñπÊ≥ïËÉΩÂ§ü‰ªéÈùôÊÄÅÂõæÂÉè‰∏≠ÊèêÂèñÂá∫È´òÊïà‰∏îÂèØËß£ÈáäÁöÑÁä∂ÊÄÅË°®Á§∫ÔºåÊòæËëóÊèêÈ´ò‰∫ÜÊú∫Âô®‰∫∫Âú®ÁúüÂÆû‰ªªÂä°‰∏≠ÁöÑÊàêÂäüÁéá„ÄÇÊàë‰ª¨ÁöÑÊñπÊ≥ïÂú®LIBEROÊï∞ÊçÆÈõÜ‰∏äÊèêÈ´ò‰∫Ü14.3%ÁöÑÊÄßËÉΩÔºåÂπ∂Âú®ÂÆûÈôÖ‰ªªÂä°‰∏≠ÊèêÂçá‰∫Ü30%„ÄÇÊ≠§Â§ñÔºåÈÄöËøáÊΩúÂú®ÊèíÂÄºËé∑ÂæóÁöÑÁä∂ÊÄÅÂ∑ÆÂºÇÂèØ‰ª•Ëá™ÁÑ∂Âú∞‰Ωú‰∏∫ÊúâÊïàÁöÑÊΩúÂú®Âä®‰ΩúÔºåËøõ‰∏ÄÊ≠•Ëß£Á†Å‰∏∫ÂèØÊâßË°åÁöÑÊú∫Âô®‰∫∫Âä®‰Ωú„ÄÇ","title":"Á¥ßÂáëÁä∂ÊÄÅË°®Á§∫ÔºåÊèêÂçáÊú∫Âô®‰∫∫Êô∫ËÉΩ"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊó†ÁõëÁù£ÁöÑÊñπÊ≥ïÔºåÈÄöËøáËΩªÈáèÁ∫ßÁºñÁ†ÅÂô®ÂíåÊâ©Êï£ÂèòÊç¢Âô®Ëß£Á†ÅÂô®Â≠¶‰π†Á¥ßÂáëÁöÑÁä∂ÊÄÅË°®Á§∫„ÄÇËøôÁßçÊñπÊ≥ïËÉΩÂ§ü‰ªéÈùôÊÄÅÂõæÂÉè‰∏≠ÊèêÂèñÂá∫È´òÊïà‰∏îÂèØËß£ÈáäÁöÑÁä∂ÊÄÅË°®Á§∫ÔºåÊòæËëóÊèêÈ´ò‰∫ÜÊú∫Âô®‰∫∫Âú®ÁúüÂÆû‰ªªÂä°‰∏≠ÁöÑÊàêÂäüÁéá„ÄÇÊàë‰ª¨ÁöÑÊñπÊ≥ïÂú®LIBEROÊï∞ÊçÆÈõÜ‰∏äÊèêÈ´ò‰∫Ü14.3%ÁöÑÊÄßËÉΩÔºåÂπ∂Âú®ÂÆûÈôÖ‰ªªÂä°‰∏≠ÊèêÂçá‰∫Ü30%„ÄÇÊ≠§Â§ñÔºåÈÄöËøáÊΩúÂú®ÊèíÂÄºËé∑ÂæóÁöÑÁä∂ÊÄÅÂ∑ÆÂºÇÂèØ‰ª•Ëá™ÁÑ∂Âú∞‰Ωú‰∏∫ÊúâÊïàÁöÑÊΩúÂú®Âä®‰ΩúÔºåËøõ‰∏ÄÊ≠•Ëß£Á†Å‰∏∫ÂèØÊâßË°åÁöÑÊú∫Âô®‰∫∫Âä®‰Ωú„ÄÇ', title='Á¥ßÂáëÁä∂ÊÄÅË°®Á§∫ÔºåÊèêÂçáÊú∫Âô®‰∫∫Êô∫ËÉΩ'))
[09.10.2025 07:12] Using data from previous issue: {"categories": ["#architecture", "#training", "#optimization"], "emoji": "üí•", "ru": {"title": "–£–∫—Ä–æ—â–µ–Ω–∏–µ –≤–∑—Ä—ã–≤–æ–≤: —Å—Ç–∞–±–∏–ª—å–Ω–∞—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ —Å –Ω–∏–∑–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –≤—ã—è—Å–Ω–∏–ª–∏, –ø–æ—á–µ–º—É –æ–±—É—á–µ–Ω–∏–µ transformer-–º–æ–¥–µ–ª–µ–π —Å flash attention –≤ –Ω–∏–∑–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –∫–∞—Ç–∞—Å—Ç—Ä–æ—Ñ–∏—á–µ—Å
[09.10.2025 07:12] Using data from previous issue: {"categories": ["#optimization", "#benchmark", "#dataset"], "emoji": "üî¨", "ru": {"title": "VTC-Bench: —á–µ—Å—Ç–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —Å–∂–∞—Ç–∏—è –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö LLM", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç VTC-Bench ‚Äî –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —á–µ—Å—Ç–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –º–µ—Ç–æ–¥–æ–≤ —Å–∂–∞—Ç–∏—è –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à
[09.10.2025 07:12] Using data from previous issue: {"categories": ["#optimization", "#synthetic", "#cv", "#video"], "emoji": "ü§ñ", "ru": {"title": "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∏–¥–µ–æ —Å –∑–∞–ø—è—Å—Ç—å—è –∏–∑ –æ–±—ã—á–Ω—ã—Ö –∫–∞–º–µ—Ä –¥–ª—è —Ä–æ–±–æ—Ç–æ–≤", "desc": "WristWorld ‚Äî —ç—Ç–æ 4D –º–æ–¥–µ–ª—å –º–∏—Ä–∞, –∫–æ—Ç–æ—Ä–∞—è –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –≤–∏–¥–µ–æ —Å —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è –∑–∞–ø—è—Å—Ç—å—è —Ä–æ–±–æ—Ç–∞, –∏—Å–ø–æ–ª—å–∑—É—è —Ç–æ–ª—å–∫–æ –∑–∞–ø–∏—Å–∏ —Å –æ–±—ã—á–Ω—ã—Ö —Å—Ç–∞—Ü–∏–æ–Ω–∞—Ä–Ω—ã—Ö 
[09.10.2025 07:12] Using data from previous issue: {"categories": ["#survey", "#optimization", "#dataset", "#benchmark", "#data", "#agents"], "emoji": "üè≠", "ru": {"title": "–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Ñ–∞–±—Ä–∏–∫–∞ ML-–∑–∞–¥–∞—á –∏–∑ —Å—ã—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç MLE-Smith ‚Äî –ø–æ–ª–Ω–æ—Å—Ç—å—é –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º—É–ª—å—Ç–∏-–∞–≥–µ–Ω—Ç–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á –ø
[09.10.2025 07:12] Using data from previous issue: {"categories": ["#optimization", "#rl", "#multimodal", "#rlhf", "#cv", "#transfer_learning"], "emoji": "üéØ", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –Ω–∞ –ª–µ—Ç—É: –º–æ–¥–µ–ª–∏ —É—á–∞—Ç—Å—è –ø—Ä—è–º–æ –≤–æ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –º–µ—Ç–æ–¥ TTRV, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç –ø–æ–Ω–∏–º–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä
[09.10.2025 07:12] Using data from previous issue: {"categories": ["#architecture", "#training", "#dataset", "#benchmark", "#video", "#diffusion", "#survey"], "emoji": "üé¨", "ru": {"title": "–û—Ç GAN –∫ Diffusion: —ç–≤–æ–ª—é—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –∏–∑ —Ç–µ–∫—Å—Ç–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –æ–±–∑–æ—Ä –º–æ–¥–µ–ª–µ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –∏–∑ —Ç–µ–∫—Å—Ç–∞ (text-to-video), –ø—Ä–æ—Å–ª–µ–∂–∏–≤–∞—è –∏—Ö —Ä–∞–∑
[09.10.2025 07:12] Using data from previous issue: {"categories": ["#optimization", "#reasoning", "#training", "#agents"], "emoji": "üîÑ", "ru": {"title": "–°–∞–º–æ—ç–≤–æ–ª—é—Ü–∏—è —á–µ—Ä–µ–∑ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã: AlphaApollo –ø–æ–¥–Ω–∏–º–∞–µ—Ç –ø–æ—Ç–æ–ª–æ–∫ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π LLM", "desc": "AlphaApollo ‚Äî —ç—Ç–æ —Å–∞–º–æ–æ–±—É—á–∞—é—â–∞—è—Å—è —Å–∏—Å—Ç–µ–º–∞ –∞–≥–µ–Ω—Ç–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä–∞—è —Ä–µ—à–∞–µ—Ç –¥–≤–µ –∫–ª—é—á–µ–≤—ã–µ –ø—Ä–æ–±–ª–µ–º—ã found
[09.10.2025 07:12] Using data from previous issue: {"categories": ["#games", "#alignment", "#rlhf", "#diffusion", "#optimization", "#training", "#rl"], "emoji": "üéØ", "ru": {"title": "–¢–æ—á–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –º—É–ª—å—Ç–∏–º–∞—Å—à—Ç–∞–±–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ Granular-GRPO –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏
[09.10.2025 07:12] Using data from previous issue: {"categories": ["#interpretability", "#video", "#benchmark", "#long_context"], "emoji": "üé¨", "ru": {"title": "–†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –≥—Ä–∞–Ω–∏—Ü —Å–æ–±—ã—Ç–∏–π –≤ –≤–∏–¥–µ–æ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏ —á–µ—Ä–µ–∑ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∏ –æ—à–∏–±–∫—É", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –∑–∞–¥–∞—á—É Online Generic Event Boundary Detection (On-GEBD) ‚Äî –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏
[09.10.2025 07:12] Using data from previous issue: {"categories": ["#cv", "#open_source", "#dataset", "#benchmark", "#optimization", "#survey"], "emoji": "üè•", "ru": {"title": "U-Bench: –≤—Å–µ—Å—Ç–æ—Ä–æ–Ω–Ω–∏–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è U-Net –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä –≤ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏", "desc": "U-Bench –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –ø–µ—Ä–≤—ã–π –º–∞—Å—à—Ç–∞–±–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ—Ü–µ–Ω–∫–∏ U-Net
[09.10.2025 07:12] Using data from previous issue: {"categories": ["#training", "#multilingual", "#architecture", "#benchmark", "#survey", "#dataset", "#low_resource"], "emoji": "üîÄ", "ru": {"title": "–ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤: –≤—ã–∑–æ–≤ –¥–ª—è —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö LLM", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –æ–±–∑–æ—Ä —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM), —Å–ø–æ—Å–æ–±
[09.10.2025 07:12] Using data from previous issue: {"categories": ["#security", "#synthetic", "#cv", "#dataset", "#inference"], "emoji": "üîç", "ru": {"title": "–ü–æ–∏—Å–∫ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —á–µ—Ä–µ–∑ –∞–Ω–∞–ª–∏–∑ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ D¬≥QE –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤–∏–∑—É–∞–ª—å–Ω—ã–º–∏ autoregressive –º–æ–¥–µ–ª—è–º–∏. –ú–µ—Ç–æ–¥ –∞–Ω–∞–ª–∏
[09.10.2025 07:12] Querying the API.
[09.10.2025 07:12] Claude request. Model: claude-sonnet-4-5-20250929. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Heptapod, an image autoregressive model using causal attention and next 2D distribution prediction, achieves superior performance on ImageNet generation by combining sequential modeling with holistic self-supervised learning.  					AI-generated summary 				 We introduce Heptapod, an image autoregressive model that adheres to the foundational principles of language modeling. Heptapod employs causal attention, eliminates reliance on CFG, and eschews the trend of semantic tokenizers. Our key innovation is next 2D distribution prediction: a causal Transformer with reconstruction-focused visual tokenizer, learns to predict the distribution over the entire 2D spatial grid of images at each timestep. This learning objective unifies the sequential modeling of autoregressive framework with the holistic self-supervised learning of masked autoencoding, enabling the model to capture comprehensive image semantics via generative training. On the ImageNet generation benchmark, Heptapod achieves an FID of 2.70, significantly outperforming previous causal autoregressive approaches. We hope our work inspires a principled rethinking of language modeling on visual signals and beyond.
[09.10.2025 07:12] Response: ```json
{
  "title": "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ 2D —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π –¥–ª—è –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π",
  "emoji": "üêô",
  "desc": "Heptapod ‚Äî —ç—Ç–æ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä–∞—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç causal attention –∏ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –≤—Å–µ–π 2D –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–π —Å–µ—Ç–∫–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ. –ú–æ–¥–µ–ª—å –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –∞–≤—Ç–æreg—Ä–µ—Å—Å–∏–≤–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞ —Å —Ü–µ–ª–æ—Å—Ç–Ω—ã–º self-supervised –æ–±—É—á–µ–Ω–∏–µ–º masked autoencoding, –∏—Å–ø–æ–ª—å–∑—É—è –≤–∏–∑—É–∞–ª—å–Ω—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä, –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—é. –ù–∞ –±–µ–Ω—á–º–∞—Ä–∫–µ ImageNet –º–æ–¥–µ–ª—å –¥–æ—Å—Ç–∏–≥–∞–µ—Ç FID 2.70, –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è –ø—Ä–µ–¥—ã–¥—É—â–∏–µ causal –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã. –†–∞–±–æ—Ç–∞ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –ø—Ä–∏–Ω—Ü–∏–ø–∏–∞–ª—å–Ω–æ –Ω–æ–≤—ã–π –≤–∑–≥–ª—è–¥ –Ω–∞ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –∫ –≤–∏–∑—É–∞–ª—å–Ω—ã–º –¥–∞–Ω–Ω—ã–º."
}
```
[09.10.2025 07:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Heptapod, an image autoregressive model using causal attention and next 2D distribution prediction, achieves superior performance on ImageNet generation by combining sequential modeling with holistic self-supervised learning.  					AI-generated summary 				 We introduce Heptapod, an image autoregressive model that adheres to the foundational principles of language modeling. Heptapod employs causal attention, eliminates reliance on CFG, and eschews the trend of semantic tokenizers. Our key innovation is next 2D distribution prediction: a causal Transformer with reconstruction-focused visual tokenizer, learns to predict the distribution over the entire 2D spatial grid of images at each timestep. This learning objective unifies the sequential modeling of autoregressive framework with the holistic self-supervised learning of masked autoencoding, enabling the model to capture comprehensive image semantics via generative training. On the ImageNet generation benchmark, Heptapod achieves an FID of 2.70, significantly outperforming previous causal autoregressive approaches. We hope our work inspires a principled rethinking of language modeling on visual signals and beyond."

[09.10.2025 07:12] Response: ```python
['CV', 'ARCHITECTURE', 'BENCHMARK']
```
[09.10.2025 07:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Heptapod, an image autoregressive model using causal attention and next 2D distribution prediction, achieves superior performance on ImageNet generation by combining sequential modeling with holistic self-supervised learning.  					AI-generated summary 				 We introduce Heptapod, an image autoregressive model that adheres to the foundational principles of language modeling. Heptapod employs causal attention, eliminates reliance on CFG, and eschews the trend of semantic tokenizers. Our key innovation is next 2D distribution prediction: a causal Transformer with reconstruction-focused visual tokenizer, learns to predict the distribution over the entire 2D spatial grid of images at each timestep. This learning objective unifies the sequential modeling of autoregressive framework with the holistic self-supervised learning of masked autoencoding, enabling the model to capture comprehensive image semantics via generative training. On the ImageNet generation benchmark, Heptapod achieves an FID of 2.70, significantly outperforming previous causal autoregressive approaches. We hope our work inspires a principled rethinking of language modeling on visual signals and beyond."

[09.10.2025 07:12] Response: ```python
["GAMES", "OPTIMIZATION"]
```
[09.10.2025 07:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Heptapod is an advanced image autoregressive model that utilizes causal attention to enhance image generation. It innovatively predicts the next 2D distribution of images, allowing it to learn from the entire spatial grid at each step. By integrating sequential modeling with holistic self-supervised learning, Heptapod effectively captures complex image semantics. Its performance on the ImageNet benchmark, achieving an FID of 2.70, demonstrates its superiority over earlier causal autoregressive models.","title":"Heptapod: Redefining Image Generation with Causal Attention"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Heptapod is an advanced image autoregressive model that utilizes causal attention to enhance image generation. It innovatively predicts the next 2D distribution of images, allowing it to learn from the entire spatial grid at each step. By integrating sequential modeling with holistic self-supervised learning, Heptapod effectively captures complex image semantics. Its performance on the ImageNet benchmark, achieving an FID of 2.70, demonstrates its superiority over earlier causal autoregressive models.', title='Heptapod: Redefining Image Generation with Causal Attention'))
[09.10.2025 07:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"HeptapodÊòØ‰∏ÄÁßçÂõæÂÉèËá™ÂõûÂΩíÊ®°ÂûãÔºåÈááÁî®Âõ†ÊûúÊ≥®ÊÑèÂäõÂíå‰∏ã‰∏ÄÊ≠•‰∫åÁª¥ÂàÜÂ∏ÉÈ¢ÑÊµãÁöÑÊñπÊ≥ï„ÄÇÂÆÉÁªìÂêà‰∫ÜÈ°∫Â∫èÂª∫Ê®°ÂíåÊï¥‰ΩìËá™ÁõëÁù£Â≠¶‰π†ÔºåÊòæËëóÊèêÈ´ò‰∫ÜÂú®ImageNetÁîüÊàê‰ªªÂä°‰∏äÁöÑË°®Áé∞„ÄÇËØ•Ê®°ÂûãÈÄöËøáÈáçÂª∫‰∏∫‰∏≠ÂøÉÁöÑËßÜËßâÊ†áËÆ∞Âô®ÔºåÂ≠¶‰π†Âú®ÊØè‰∏™Êó∂Èó¥Ê≠•È¢ÑÊµãÊï¥‰∏™‰∫åÁª¥Á©∫Èó¥ÁΩëÊ†ºÁöÑÂàÜÂ∏É„ÄÇHeptapodÁöÑÂàõÊñ∞‰πãÂ§ÑÂú®‰∫éÂÖ∂Â≠¶‰π†ÁõÆÊ†áÔºåÁªü‰∏Ä‰∫ÜËá™ÂõûÂΩíÊ°ÜÊû∂ÁöÑÈ°∫Â∫èÂª∫Ê®°‰∏éÊé©Á†ÅËá™ÁºñÁ†ÅÁöÑËá™ÁõëÁù£Â≠¶‰π†„ÄÇ","title":"HeptapodÔºöÂõæÂÉèÁîüÊàêÁöÑÊñ∞ÊÄùË∑Ø"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='HeptapodÊòØ‰∏ÄÁßçÂõæÂÉèËá™ÂõûÂΩíÊ®°ÂûãÔºåÈááÁî®Âõ†ÊûúÊ≥®ÊÑèÂäõÂíå‰∏ã‰∏ÄÊ≠•‰∫åÁª¥ÂàÜÂ∏ÉÈ¢ÑÊµãÁöÑÊñπÊ≥ï„ÄÇÂÆÉÁªìÂêà‰∫ÜÈ°∫Â∫èÂª∫Ê®°ÂíåÊï¥‰ΩìËá™ÁõëÁù£Â≠¶‰π†ÔºåÊòæËëóÊèêÈ´ò‰∫ÜÂú®ImageNetÁîüÊàê‰ªªÂä°‰∏äÁöÑË°®Áé∞„ÄÇËØ•Ê®°ÂûãÈÄöËøáÈáçÂª∫‰∏∫‰∏≠ÂøÉÁöÑËßÜËßâÊ†áËÆ∞Âô®ÔºåÂ≠¶‰π†Âú®ÊØè‰∏™Êó∂Èó¥Ê≠•È¢ÑÊµãÊï¥‰∏™‰∫åÁª¥Á©∫Èó¥ÁΩëÊ†ºÁöÑÂàÜÂ∏É„ÄÇHeptapodÁöÑÂàõÊñ∞‰πãÂ§ÑÂú®‰∫éÂÖ∂Â≠¶‰π†ÁõÆÊ†áÔºåÁªü‰∏Ä‰∫ÜËá™ÂõûÂΩíÊ°ÜÊû∂ÁöÑÈ°∫Â∫èÂª∫Ê®°‰∏éÊé©Á†ÅËá™ÁºñÁ†ÅÁöÑËá™ÁõëÁù£Â≠¶‰π†„ÄÇ', title='HeptapodÔºöÂõæÂÉèÁîüÊàêÁöÑÊñ∞ÊÄùË∑Ø'))
[09.10.2025 07:13] Querying the API.
[09.10.2025 07:13] Claude request. Model: claude-sonnet-4-5-20250929. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

NorMuon, a novel optimizer combining orthogonalization with neuron-level adaptive learning rates, enhances training efficiency and balances parameter utilization in large language models.  					AI-generated summary 				 The choice of optimizer significantly impacts the training efficiency and computational costs of large language models (LLMs). Recently, the Muon optimizer has demonstrated promising results by orthogonalizing parameter updates, improving optimization geometry through better conditioning. Despite Muon's emergence as a candidate successor to Adam, the potential for jointly leveraging their strengths has not been systematically explored. In this work, we bridge this gap by proposing NorMuon (Neuron-wise Normalized Muon), an optimizer that synergistically combines orthogonalization with neuron-level adaptive learning rates. Our analysis reveals that while Muon effectively reduces condition numbers, the resulting updates exhibit highly non-uniform neuron norms, causing certain neurons to dominate the optimization process. NorMuon addresses this imbalance by maintaining second-order momentum statistics for each neuron and applying row-wise normalization after orthogonalization, ensuring balanced parameter utilization while preserving Muon's conditioning benefits. To enable practical deployment at scale, we develop an efficient distributed implementation under the FSDP2 framework that strategically distributes orthogonalization computations across devices. Experiments across multiple model scales demonstrate that NorMuon consistently outperforms both Adam and Muon, achieving 21.74% better training efficiency than Adam and 11.31% improvement over Muon on 1.1 B pretraining setting, while maintaining a comparable memory footprint to Muon. Our findings suggest that orthogonalization and adaptive learning rates are complementary rather than competing approaches, opening new avenues for optimizer design in large-scale deep learning.
[09.10.2025 07:13] Response: ```json
{
  "title": "–ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –Ω–µ–π—Ä–æ–Ω–æ–≤ —á–µ—Ä–µ–∑ –æ—Ä—Ç–æ–≥–æ–Ω–∞–ª–∏–∑–∞—Ü–∏—é –∏ –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–µ —à–∞–≥–∏ –æ–±—É—á–µ–Ω–∏—è",
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç NorMuon ‚Äî –Ω–æ–≤—ã–π –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–π –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –æ—Ä—Ç–æ–≥–æ–Ω–∞–ª–∏–∑–∞—Ü–∏—é –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–º–∏ learning rates –Ω–∞ —É—Ä–æ–≤–Ω–µ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –Ω–µ–π—Ä–æ–Ω–æ–≤. –ê–≤—Ç–æ—Ä—ã –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ —Ö–æ—Ç—è –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä Muon —É–ª—É—á—à–∞–µ—Ç –≥–µ–æ–º–µ—Ç—Ä–∏—é –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ –æ—Ä—Ç–æ–≥–æ–Ω–∞–ª–∏–∑–∞—Ü–∏—é, –æ–Ω —Å–æ–∑–¥–∞—ë—Ç –¥–∏—Å–±–∞–ª–∞–Ω—Å –≤ –Ω–æ—Ä–º–∞—Ö –Ω–µ–π—Ä–æ–Ω–æ–≤, –∏–∑-–∑–∞ —á–µ–≥–æ –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –Ω–µ–π—Ä–æ–Ω—ã –¥–æ–º–∏–Ω–∏—Ä—É—é—Ç –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –æ–±—É—á–µ–Ω–∏—è. NorMuon —Ä–µ—à–∞–µ—Ç —ç—Ç—É –ø—Ä–æ–±–ª–µ–º—É, –ø—Ä–∏–º–µ–Ω—è—è –ø–æ—Å—Ç—Ä–æ—á–Ω—É—é –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é –ø–æ—Å–ª–µ –æ—Ä—Ç–æ–≥–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏ –∏ —Å–æ—Ö—Ä–∞–Ω—è—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≤—Ç–æ—Ä–æ–≥–æ –ø–æ—Ä—è–¥–∫–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –Ω–µ–π—Ä–æ–Ω–∞. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏ —É–ª—É—á—à–µ–Ω–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ 21.74% –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å Adam –∏ –Ω–∞ 11.31% –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å Muon –Ω–∞ –º–æ–¥–µ–ª–∏ —Ä–∞–∑–º–µ—Ä–æ–º 1.1B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.",
  "emoji": "‚öñÔ∏è"
}
```
[09.10.2025 07:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"NorMuon, a novel optimizer combining orthogonalization with neuron-level adaptive learning rates, enhances training efficiency and balances parameter utilization in large language models.  					AI-generated summary 				 The choice of optimizer significantly impacts the training efficiency and computational costs of large language models (LLMs). Recently, the Muon optimizer has demonstrated promising results by orthogonalizing parameter updates, improving optimization geometry through better conditioning. Despite Muon's emergence as a candidate successor to Adam, the potential for jointly leveraging their strengths has not been systematically explored. In this work, we bridge this gap by proposing NorMuon (Neuron-wise Normalized Muon), an optimizer that synergistically combines orthogonalization with neuron-level adaptive learning rates. Our analysis reveals that while Muon effectively reduces condition numbers, the resulting updates exhibit highly non-uniform neuron norms, causing certain neurons to dominate the optimization process. NorMuon addresses this imbalance by maintaining second-order momentum statistics for each neuron and applying row-wise normalization after orthogonalization, ensuring balanced parameter utilization while preserving Muon's conditioning benefits. To enable practical deployment at scale, we develop an efficient distributed implementation under the FSDP2 framework that strategically distributes orthogonalization computations across devices. Experiments across multiple model scales demonstrate that NorMuon consistently outperforms both Adam and Muon, achieving 21.74% better training efficiency than Adam and 11.31% improvement over Muon on 1.1 B pretraining setting, while maintaining a comparable memory footprint to Muon. Our findings suggest that orthogonalization and adaptive learning rates are complementary rather than competing approaches, opening new avenues for optimizer design in large-scale deep learning."

[09.10.2025 07:13] Response: ```python
["TRAINING", "ARCHITECTURE"]
```
[09.10.2025 07:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"NorMuon, a novel optimizer combining orthogonalization with neuron-level adaptive learning rates, enhances training efficiency and balances parameter utilization in large language models.  					AI-generated summary 				 The choice of optimizer significantly impacts the training efficiency and computational costs of large language models (LLMs). Recently, the Muon optimizer has demonstrated promising results by orthogonalizing parameter updates, improving optimization geometry through better conditioning. Despite Muon's emergence as a candidate successor to Adam, the potential for jointly leveraging their strengths has not been systematically explored. In this work, we bridge this gap by proposing NorMuon (Neuron-wise Normalized Muon), an optimizer that synergistically combines orthogonalization with neuron-level adaptive learning rates. Our analysis reveals that while Muon effectively reduces condition numbers, the resulting updates exhibit highly non-uniform neuron norms, causing certain neurons to dominate the optimization process. NorMuon addresses this imbalance by maintaining second-order momentum statistics for each neuron and applying row-wise normalization after orthogonalization, ensuring balanced parameter utilization while preserving Muon's conditioning benefits. To enable practical deployment at scale, we develop an efficient distributed implementation under the FSDP2 framework that strategically distributes orthogonalization computations across devices. Experiments across multiple model scales demonstrate that NorMuon consistently outperforms both Adam and Muon, achieving 21.74% better training efficiency than Adam and 11.31% improvement over Muon on 1.1 B pretraining setting, while maintaining a comparable memory footprint to Muon. Our findings suggest that orthogonalization and adaptive learning rates are complementary rather than competing approaches, opening new avenues for optimizer design in large-scale deep learning."

[09.10.2025 07:13] Response: ```python
["OPTIMIZATION"]
```
[09.10.2025 07:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"NorMuon is a new optimizer designed to improve the training of large language models by combining orthogonalization with neuron-level adaptive learning rates. This approach enhances training efficiency and ensures that all parameters are utilized effectively, preventing any single neuron from dominating the optimization process. By maintaining second-order momentum statistics and applying normalization, NorMuon balances the updates across neurons while benefiting from improved optimization geometry. Experiments show that NorMuon outperforms traditional optimizers like Adam and Muon, achieving significant gains in training efficiency without increasing memory usage.","title":"NorMuon: Balancing Efficiency and Utilization in Large Language Model Training"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='NorMuon is a new optimizer designed to improve the training of large language models by combining orthogonalization with neuron-level adaptive learning rates. This approach enhances training efficiency and ensures that all parameters are utilized effectively, preventing any single neuron from dominating the optimization process. By maintaining second-order momentum statistics and applying normalization, NorMuon balances the updates across neurons while benefiting from improved optimization geometry. Experiments show that NorMuon outperforms traditional optimizers like Adam and Muon, achieving significant gains in training efficiency without increasing memory usage.', title='NorMuon: Balancing Efficiency and Utilization in Large Language Model Training'))
[09.10.2025 07:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"NorMuonÊòØ‰∏ÄÁßçÊñ∞Âûã‰ºòÂåñÂô®ÔºåÂÆÉÁªìÂêà‰∫ÜÊ≠£‰∫§ÂåñÂíåÁ•ûÁªèÂÖÉÁ∫ßËá™ÈÄÇÂ∫îÂ≠¶‰π†ÁéáÔºåÊèêÂçá‰∫ÜÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑËÆ≠ÁªÉÊïàÁéá„ÄÇËØ•ÊñπÊ≥ïÈÄöËøá‰øùÊåÅÊØè‰∏™Á•ûÁªèÂÖÉÁöÑ‰∫åÈò∂Âä®ÈáèÁªüËÆ°ÔºåÂπ∂Âú®Ê≠£‰∫§ÂåñÂêéËøõË°åË°åÂΩí‰∏ÄÂåñÔºåËß£ÂÜ≥‰∫ÜÂèÇÊï∞Âà©Áî®‰∏çÂùáË°°ÁöÑÈóÆÈ¢ò„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåNorMuonÂú®Â§ö‰∏™Ê®°ÂûãËßÑÊ®°‰∏äÂùá‰ºò‰∫éAdamÂíåMuonÔºåËÆ≠ÁªÉÊïàÁéáÊèêÈ´ò‰∫Ü21.74%„ÄÇËøôÈ°πÁ†îÁ©∂Ë°®ÊòéÔºåÊ≠£‰∫§ÂåñÂíåËá™ÈÄÇÂ∫îÂ≠¶‰π†ÁéáÊòØ‰∫íË°•ÁöÑÔºåÂèØ‰ª•‰∏∫Â§ßËßÑÊ®°Ê∑±Â∫¶Â≠¶‰π†ÁöÑ‰ºòÂåñÂô®ËÆæËÆ°ÂºÄËæüÊñ∞ÁöÑÊñπÂêë„ÄÇ","title":"NorMuonÔºöÊèêÂçáÂ§ßËØ≠Ë®ÄÊ®°ÂûãËÆ≠ÁªÉÊïàÁéáÁöÑÊñ∞‰ºòÂåñÂô®"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='NorMuonÊòØ‰∏ÄÁßçÊñ∞Âûã‰ºòÂåñÂô®ÔºåÂÆÉÁªìÂêà‰∫ÜÊ≠£‰∫§ÂåñÂíåÁ•ûÁªèÂÖÉÁ∫ßËá™ÈÄÇÂ∫îÂ≠¶‰π†ÁéáÔºåÊèêÂçá‰∫ÜÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑËÆ≠ÁªÉÊïàÁéá„ÄÇËØ•ÊñπÊ≥ïÈÄöËøá‰øùÊåÅÊØè‰∏™Á•ûÁªèÂÖÉÁöÑ‰∫åÈò∂Âä®ÈáèÁªüËÆ°ÔºåÂπ∂Âú®Ê≠£‰∫§ÂåñÂêéËøõË°åË°åÂΩí‰∏ÄÂåñÔºåËß£ÂÜ≥‰∫ÜÂèÇÊï∞Âà©Áî®‰∏çÂùáË°°ÁöÑÈóÆÈ¢ò„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåNorMuonÂú®Â§ö‰∏™Ê®°ÂûãËßÑÊ®°‰∏äÂùá‰ºò‰∫éAdamÂíåMuonÔºåËÆ≠ÁªÉÊïàÁéáÊèêÈ´ò‰∫Ü21.74%„ÄÇËøôÈ°πÁ†îÁ©∂Ë°®ÊòéÔºåÊ≠£‰∫§ÂåñÂíåËá™ÈÄÇÂ∫îÂ≠¶‰π†ÁéáÊòØ‰∫íË°•ÁöÑÔºåÂèØ‰ª•‰∏∫Â§ßËßÑÊ®°Ê∑±Â∫¶Â≠¶‰π†ÁöÑ‰ºòÂåñÂô®ËÆæËÆ°ÂºÄËæüÊñ∞ÁöÑÊñπÂêë„ÄÇ', title='NorMuonÔºöÊèêÂçáÂ§ßËØ≠Ë®ÄÊ®°ÂûãËÆ≠ÁªÉÊïàÁéáÁöÑÊñ∞‰ºòÂåñÂô®'))
[09.10.2025 07:13] Renaming data file.
[09.10.2025 07:13] Renaming previous data. hf_papers.json to ./d/2025-10-09.json
[09.10.2025 07:13] Saving new data file.
[09.10.2025 07:13] Generating page.
[09.10.2025 07:13] Renaming previous page.
[09.10.2025 07:13] Renaming previous data. index.html to ./d/2025-10-09.html
[09.10.2025 07:13] Writing result.
[09.10.2025 07:13] Renaming log file.
[09.10.2025 07:13] Renaming previous data. log.txt to ./logs/2025-10-09_last_log.txt
