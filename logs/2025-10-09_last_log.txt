[09.10.2025 03:31] Read previous papers.
[09.10.2025 03:31] Generating top page (month).
[09.10.2025 03:31] Writing top page (month).
[09.10.2025 04:14] Read previous papers.
[09.10.2025 04:14] Get feed.
[09.10.2025 04:14] Get page data from previous paper. URL: https://huggingface.co/papers/2510.06917
[09.10.2025 04:14] Get page data from previous paper. URL: https://huggingface.co/papers/2510.06308
[09.10.2025 04:14] Get page data from previous paper. URL: https://huggingface.co/papers/2510.07315
[09.10.2025 04:14] Get page data from previous paper. URL: https://huggingface.co/papers/2510.03215
[09.10.2025 04:14] Get page data from previous paper. URL: https://huggingface.co/papers/2510.04678
[09.10.2025 04:14] Get page data from previous paper. URL: https://huggingface.co/papers/2510.06751
[09.10.2025 04:14] Get page data from previous paper. URL: https://huggingface.co/papers/2510.07318
[09.10.2025 04:14] Get page data from previous paper. URL: https://huggingface.co/papers/2510.07310
[09.10.2025 04:14] Get page data from previous paper. URL: https://huggingface.co/papers/2510.04212
[09.10.2025 04:14] Get page data from previous paper. URL: https://huggingface.co/papers/2510.05862
[09.10.2025 04:14] Get page data from previous paper. URL: https://huggingface.co/papers/2510.07313
[09.10.2025 04:14] Get page data from previous paper. URL: https://huggingface.co/papers/2510.07238
[09.10.2025 04:14] Get page data from previous paper. URL: https://huggingface.co/papers/2510.07307
[09.10.2025 04:14] Get page data from previous paper. URL: https://huggingface.co/papers/2510.06783
[09.10.2025 04:14] Get page data from previous paper. URL: https://huggingface.co/papers/2510.04999
[09.10.2025 04:14] Get page data from previous paper. URL: https://huggingface.co/papers/2510.06261
[09.10.2025 04:14] Get page data from previous paper. URL: https://huggingface.co/papers/2510.07041
[09.10.2025 04:14] Get page data from previous paper. URL: https://huggingface.co/papers/2510.05891
[09.10.2025 04:14] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[09.10.2025 04:14] No deleted papers detected.
[09.10.2025 04:14] Downloading and parsing papers (pdf, html). Total: 18.
[09.10.2025 04:14] Downloading and parsing paper https://huggingface.co/papers/2510.06917.
[09.10.2025 04:14] Extra JSON file exists (./assets/json/2510.06917.json), skip PDF parsing.
[09.10.2025 04:14] Paper image links file exists (./assets/img_data/2510.06917.json), skip HTML parsing.
[09.10.2025 04:14] Success.
[09.10.2025 04:14] Downloading and parsing paper https://huggingface.co/papers/2510.06308.
[09.10.2025 04:14] Extra JSON file exists (./assets/json/2510.06308.json), skip PDF parsing.
[09.10.2025 04:14] Paper image links file exists (./assets/img_data/2510.06308.json), skip HTML parsing.
[09.10.2025 04:14] Success.
[09.10.2025 04:14] Downloading and parsing paper https://huggingface.co/papers/2510.07315.
[09.10.2025 04:14] Extra JSON file exists (./assets/json/2510.07315.json), skip PDF parsing.
[09.10.2025 04:14] Paper image links file exists (./assets/img_data/2510.07315.json), skip HTML parsing.
[09.10.2025 04:14] Success.
[09.10.2025 04:14] Downloading and parsing paper https://huggingface.co/papers/2510.03215.
[09.10.2025 04:14] Extra JSON file exists (./assets/json/2510.03215.json), skip PDF parsing.
[09.10.2025 04:14] Paper image links file exists (./assets/img_data/2510.03215.json), skip HTML parsing.
[09.10.2025 04:14] Success.
[09.10.2025 04:14] Downloading and parsing paper https://huggingface.co/papers/2510.04678.
[09.10.2025 04:14] Extra JSON file exists (./assets/json/2510.04678.json), skip PDF parsing.
[09.10.2025 04:14] Paper image links file exists (./assets/img_data/2510.04678.json), skip HTML parsing.
[09.10.2025 04:14] Success.
[09.10.2025 04:14] Downloading and parsing paper https://huggingface.co/papers/2510.06751.
[09.10.2025 04:14] Extra JSON file exists (./assets/json/2510.06751.json), skip PDF parsing.
[09.10.2025 04:14] Paper image links file exists (./assets/img_data/2510.06751.json), skip HTML parsing.
[09.10.2025 04:14] Success.
[09.10.2025 04:14] Downloading and parsing paper https://huggingface.co/papers/2510.07318.
[09.10.2025 04:14] Extra JSON file exists (./assets/json/2510.07318.json), skip PDF parsing.
[09.10.2025 04:14] Paper image links file exists (./assets/img_data/2510.07318.json), skip HTML parsing.
[09.10.2025 04:14] Success.
[09.10.2025 04:14] Downloading and parsing paper https://huggingface.co/papers/2510.07310.
[09.10.2025 04:14] Extra JSON file exists (./assets/json/2510.07310.json), skip PDF parsing.
[09.10.2025 04:14] Paper image links file exists (./assets/img_data/2510.07310.json), skip HTML parsing.
[09.10.2025 04:14] Success.
[09.10.2025 04:14] Downloading and parsing paper https://huggingface.co/papers/2510.04212.
[09.10.2025 04:14] Extra JSON file exists (./assets/json/2510.04212.json), skip PDF parsing.
[09.10.2025 04:14] Paper image links file exists (./assets/img_data/2510.04212.json), skip HTML parsing.
[09.10.2025 04:14] Success.
[09.10.2025 04:14] Downloading and parsing paper https://huggingface.co/papers/2510.05862.
[09.10.2025 04:14] Extra JSON file exists (./assets/json/2510.05862.json), skip PDF parsing.
[09.10.2025 04:14] Paper image links file exists (./assets/img_data/2510.05862.json), skip HTML parsing.
[09.10.2025 04:14] Success.
[09.10.2025 04:14] Downloading and parsing paper https://huggingface.co/papers/2510.07313.
[09.10.2025 04:14] Extra JSON file exists (./assets/json/2510.07313.json), skip PDF parsing.
[09.10.2025 04:14] Paper image links file exists (./assets/img_data/2510.07313.json), skip HTML parsing.
[09.10.2025 04:14] Success.
[09.10.2025 04:14] Downloading and parsing paper https://huggingface.co/papers/2510.07238.
[09.10.2025 04:14] Extra JSON file exists (./assets/json/2510.07238.json), skip PDF parsing.
[09.10.2025 04:14] Paper image links file exists (./assets/img_data/2510.07238.json), skip HTML parsing.
[09.10.2025 04:14] Success.
[09.10.2025 04:14] Downloading and parsing paper https://huggingface.co/papers/2510.07307.
[09.10.2025 04:14] Extra JSON file exists (./assets/json/2510.07307.json), skip PDF parsing.
[09.10.2025 04:14] Paper image links file exists (./assets/img_data/2510.07307.json), skip HTML parsing.
[09.10.2025 04:14] Success.
[09.10.2025 04:14] Downloading and parsing paper https://huggingface.co/papers/2510.06783.
[09.10.2025 04:14] Extra JSON file exists (./assets/json/2510.06783.json), skip PDF parsing.
[09.10.2025 04:14] Paper image links file exists (./assets/img_data/2510.06783.json), skip HTML parsing.
[09.10.2025 04:14] Success.
[09.10.2025 04:14] Downloading and parsing paper https://huggingface.co/papers/2510.04999.
[09.10.2025 04:14] Extra JSON file exists (./assets/json/2510.04999.json), skip PDF parsing.
[09.10.2025 04:14] Paper image links file exists (./assets/img_data/2510.04999.json), skip HTML parsing.
[09.10.2025 04:14] Success.
[09.10.2025 04:14] Downloading and parsing paper https://huggingface.co/papers/2510.06261.
[09.10.2025 04:14] Extra JSON file exists (./assets/json/2510.06261.json), skip PDF parsing.
[09.10.2025 04:14] Paper image links file exists (./assets/img_data/2510.06261.json), skip HTML parsing.
[09.10.2025 04:14] Success.
[09.10.2025 04:14] Downloading and parsing paper https://huggingface.co/papers/2510.07041.
[09.10.2025 04:14] Extra JSON file exists (./assets/json/2510.07041.json), skip PDF parsing.
[09.10.2025 04:14] Paper image links file exists (./assets/img_data/2510.07041.json), skip HTML parsing.
[09.10.2025 04:14] Success.
[09.10.2025 04:14] Downloading and parsing paper https://huggingface.co/papers/2510.05891.
[09.10.2025 04:14] Extra JSON file exists (./assets/json/2510.05891.json), skip PDF parsing.
[09.10.2025 04:14] Paper image links file exists (./assets/img_data/2510.05891.json), skip HTML parsing.
[09.10.2025 04:14] Success.
[09.10.2025 04:14] Enriching papers with extra data.
[09.10.2025 04:14] ********************************************************************************
[09.10.2025 04:14] Abstract 0. SHANKS, a general inference framework, enables spoken language models to generate unspoken reasoning while listening to user input, enhancing real-time interaction and task completion.  					AI-generated summary 				 Current large language models (LLMs) and spoken language models (SLMs) begin thinki...
[09.10.2025 04:14] ********************************************************************************
[09.10.2025 04:14] Abstract 1. Lumina-DiMOO, an open-source foundational model, uses fully discrete diffusion modeling for efficient multi-modal generation and understanding, outperforming existing models.  					AI-generated summary 				 We introduce Lumina-DiMOO, an open-source foundational model for seamless multi-modal generat...
[09.10.2025 04:14] ********************************************************************************
[09.10.2025 04:14] Abstract 2. Vibe Checker evaluates LLMs by combining functional correctness and instruction following to better align with human coding preferences.  					AI-generated summary 				 Large Language Models (LLMs) have catalyzed vibe coding, where users leverage LLMs to generate and iteratively refine code through ...
[09.10.2025 04:14] ********************************************************************************
[09.10.2025 04:14] Abstract 3. Cache-to-Cache (C2C) enables direct semantic communication between LLMs using neural network projections, improving accuracy and reducing latency compared to text-based communication.  					AI-generated summary 				 Multi-LLM systems harness the complementary strengths of diverse Large Language Mode...
[09.10.2025 04:14] ********************************************************************************
[09.10.2025 04:14] Abstract 4. MATPO, a reinforcement learning method, optimizes tool-integrated multi-agent roles within a single LLM, improving performance and robustness over single-agent systems.  					AI-generated summary 				 Large language models (LLMs) increasingly rely on multi-turn tool-integrated planning for knowledge...
[09.10.2025 04:14] ********************************************************************************
[09.10.2025 04:14] Abstract 5. OBS-Diff is a novel one-shot pruning framework that compresses large-scale text-to-image diffusion models with minimal quality loss and significant inference acceleration.  					AI-generated summary 				 Large-scale text-to-image diffusion models, while powerful, suffer from prohibitive computationa...
[09.10.2025 04:14] ********************************************************************************
[09.10.2025 04:14] Abstract 6. A memory framework combining short-term and long-term memory in neural networks improves long-sequence modeling efficiency and performance.  					AI-generated summary 				 Long-sequence modeling faces a fundamental trade-off between the efficiency of compressive fixed-size memory in RNN-like models ...
[09.10.2025 04:14] ********************************************************************************
[09.10.2025 04:14] Abstract 7. MATRIX-11K dataset and MATRIX regularization enhance interaction fidelity and semantic alignment in video DiTs by aligning attention with multi-instance mask tracks.  					AI-generated summary 				 Video DiTs have advanced video generation, yet they still struggle to model multi-instance or subject-...
[09.10.2025 04:14] ********************************************************************************
[09.10.2025 04:14] Abstract 8. Low-precision training of transformer models with flash attention suffers from catastrophic loss explosions due to low-rank representations and biased rounding errors, which are addressed by a minimal modification to the flash attention mechanism.  					AI-generated summary 				 The pursuit of compu...
[09.10.2025 04:14] ********************************************************************************
[09.10.2025 04:14] Abstract 9. Context Denoising Training (CDT) improves long-context models' performance by mitigating contextual noise and enhancing attention on critical tokens.  					AI-generated summary 				 Long-context models (LCMs) have demonstrated great potential in processing long sequences, facilitating many real-worl...
[09.10.2025 04:14] ********************************************************************************
[09.10.2025 04:14] Abstract 10. WristWorld is a 4D world model that generates wrist-view videos from anchor views, improving video generation consistency and VLA performance.  					AI-generated summary 				 Wrist-view observations are crucial for VLA models as they capture fine-grained hand-object interactions that directly enhanc...
[09.10.2025 04:14] ********************************************************************************
[09.10.2025 04:14] Abstract 11. Research investigates the aging of factuality benchmarks and its impact on evaluating the factuality of large language models, revealing significant unreliability due to outdated samples.  					AI-generated summary 				 The rapid evolution of large language models (LLMs) and the real world has outpa...
[09.10.2025 04:14] ********************************************************************************
[09.10.2025 04:14] Abstract 12. MLE-Smith automates the creation of high-quality, diverse MLE tasks from raw datasets using a multi-agent pipeline, improving scalability and maintaining task quality.  					AI-generated summary 				 While Language Models (LMs) have made significant progress in automating machine learning engineerin...
[09.10.2025 04:14] ********************************************************************************
[09.10.2025 04:14] Abstract 13. TTRV enhances vision language understanding through test-time reinforcement learning, improving performance on object recognition and VQA without labeled data.  					AI-generated summary 				 Existing methods for extracting reward signals in Reinforcement Learning typically rely on labeled data and ...
[09.10.2025 04:14] ********************************************************************************
[09.10.2025 04:14] Abstract 14. A survey of text-to-video generative models from GANs and VAEs to hybrid Diffusion-Transformer architectures, detailing their development, limitations, and future directions.  					AI-generated summary 				 Text-to-video (T2V) generation technology holds potential to transform multiple domains such ...
[09.10.2025 04:14] ********************************************************************************
[09.10.2025 04:14] Abstract 15. AlphaApollo, a self-evolving reasoning system, enhances foundation model performance through tool integration and iterative refinement, achieving significant improvements in accuracy and pass rates.  					AI-generated summary 				 We present AlphaApollo, a self-evolving agentic reasoning system that...
[09.10.2025 04:14] ********************************************************************************
[09.10.2025 04:14] Abstract 16. U-Bench is a comprehensive benchmark evaluating 100 U-Net variants across 28 datasets and 10 imaging modalities, focusing on statistical robustness, zero-shot generalization, and computational efficiency.  					AI-generated summary 				 Over the past decade, U-Net has been the dominant architecture ...
[09.10.2025 04:14] ********************************************************************************
[09.10.2025 04:14] Abstract 17. A novel method using Discrete Distribution Discrepancy-aware Quantization Error (D$^3$QE) detects images generated by visual autoregressive models by analyzing codebook frequency statistics and quantization errors.  					AI-generated summary 				 The emergence of visual autoregressive (AR) models ha...
[09.10.2025 04:14] Read previous papers.
[09.10.2025 04:14] Generating reviews via LLM API.
[09.10.2025 04:14] Using data from previous issue: {"categories": ["#multimodal", "#reasoning", "#inference", "#long_context"], "emoji": "üéß", "ru": {"title": "–î—É–º–∞–π –ø–æ–∫–∞ —Å–ª—É—à–∞–µ—à—å: —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏ –¥–ª—è —Ä–∞–∑–≥–æ–≤–æ—Ä–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω SHANKS ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è spoken language models (SLM), –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª—è–º –≥–µ–Ω–µ—Ä
[09.10.2025 04:14] Using data from previous issue: {"categories": ["#open_source", "#multimodal", "#benchmark", "#diffusion", "#architecture"], "emoji": "üé≠", "ru": {"title": "–î–∏—Å–∫—Ä–µ—Ç–Ω–∞—è –¥–∏—Ñ—Ñ—É–∑–∏—è –¥–ª—è —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–∏", "desc": "Lumina-DiMOO - —ç—Ç–æ –æ—Ç–∫—Ä—ã—Ç–∞—è foundational –º–æ–¥–µ–ª—å –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞. –í –æ—Ç–ª–∏—á–∏–µ
[09.10.2025 04:14] Using data from previous issue: {"categories": ["#interpretability", "#alignment", "#training", "#benchmark", "#plp"], "emoji": "‚ú®", "ru": {"title": "Vibe Check: –∫–æ–≥–¥–∞ –∫–æ–¥ –¥–æ–ª–∂–µ–Ω –Ω–µ —Ç–æ–ª—å–∫–æ —Ä–∞–±–æ—Ç–∞—Ç—å, –Ω–æ –∏ –Ω—Ä–∞–≤–∏—Ç—å—Å—è", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ Vibe Checker ‚Äî –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ—Ü–µ–Ω–∫–µ LLM –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞, –∫–æ—Ç–æ—Ä—ã–π —É—á–∏—Ç—ã–≤–∞–µ—Ç –Ω–µ
[09.10.2025 04:14] Using data from previous issue: {"categories": ["#architecture", "#training", "#multimodal", "#optimization", "#agi"], "emoji": "üîÑ", "ru": {"title": "–û–±—â–µ–Ω–∏–µ LLM –±–µ–∑ —Å–ª–æ–≤: –ø—Ä—è–º–∞—è –ø–µ—Ä–µ–¥–∞—á–∞ —Å–µ–º–∞–Ω—Ç–∏–∫–∏ —á–µ—Ä–µ–∑ –∫—ç—à", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ Cache-to-Cache (C2C) –¥–ª—è –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏ –º–µ–∂–¥—É –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ LLM, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥
[09.10.2025 04:14] Using data from previous issue: {"categories": ["#training", "#reasoning", "#optimization", "#agents", "#rl"], "emoji": "üé≠", "ru": {"title": "–û–¥–∏–Ω LLM –≤ —Ä–æ–ª–∏ —Ü–µ–ª–æ–π –∫–æ–º–∞–Ω–¥—ã –∞–≥–µ–Ω—Ç–æ–≤", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ MATPO ‚Äî –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º –≤–Ω—É—Ç—Ä–∏ –æ–¥–Ω–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏. –¢—Ä–∞–¥–∏—Ü–∏–æ
[09.10.2025 04:14] Using data from previous issue: {"categories": ["#optimization", "#training", "#inference", "#diffusion", "#architecture"], "emoji": "‚úÇÔ∏è", "ru": {"title": "–£–º–Ω–æ–µ —Å–∂–∞—Ç–∏–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∑–∞ –æ–¥–∏–Ω –ø—Ä–æ—Ö–æ–¥", "desc": "OBS-Diff ‚Äî —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–¥–Ω–æ—à–∞–≥–æ–≤–æ–π –æ–±—Ä–µ–∑–∫–∏ (pruning) –¥–ª—è —Å–∂–∞—Ç–∏—è –±–æ–ª—å—à–∏—Ö —Ç–µ–∫—Å—Ç-–≤-–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
[09.10.2025 04:14] Using data from previous issue: {"categories": ["#architecture", "#training", "#benchmark", "#optimization", "#long_context"], "emoji": "üß†", "ru": {"title": "–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –≥–∏–ø–ø–æ–∫–∞–º–ø –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –ø–∞–º—è—Ç–∏ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –ø–∞–º—è—Ç–∏ –¥–ª—è –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π, –≤–¥–æ—Ö–Ω–æ–≤–ª—ë–Ω–Ω—É—é –º–æ–¥–µ–ª—å—é –º–Ω–æ–≥–æ–∫–æ–º–ø–æ–Ω–µ–Ω—Ç–Ω–æ–π –ø–∞–º—è—Ç
[09.10.2025 04:14] Using data from previous issue: {"categories": ["#dataset", "#hallucinations", "#benchmark", "#video", "#interpretability"], "emoji": "üé≠", "ru": {"title": "–£–ª—É—á—à–µ–Ω–∏–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π –≤ –≤–∏–¥–µ–æ —á–µ—Ä–µ–∑ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –≤–Ω–∏–º–∞–Ω–∏—è –ø–æ –º–∞—Å–∫–∞–º –æ–±—ä–µ–∫—Ç–æ–≤", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Å–æ–∑–¥–∞–ª–∏ –¥–∞—Ç–∞—Å–µ—Ç MATRIX-11K —Å –≤–∏–¥–µ–æ, —Å–æ–¥–µ—Ä–∂–∞—â–∏–º–∏ –æ–ø–∏—Å–∞–Ω–∏—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π
[09.10.2025 04:14] Using data from previous issue: {"categories": ["#architecture", "#training", "#optimization"], "emoji": "üí•", "ru": {"title": "–£–∫—Ä–æ—â–µ–Ω–∏–µ –≤–∑—Ä—ã–≤–æ–≤: —Å—Ç–∞–±–∏–ª—å–Ω–∞—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ —Å –Ω–∏–∑–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –≤—ã—è—Å–Ω–∏–ª–∏, –ø–æ—á–µ–º—É –æ–±—É—á–µ–Ω–∏–µ transformer-–º–æ–¥–µ–ª–µ–π —Å flash attention –≤ –Ω–∏–∑–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –∫–∞—Ç–∞—Å—Ç—Ä–æ—Ñ–∏—á–µ—Å
[09.10.2025 04:14] Using data from previous issue: {"categories": ["#training", "#long_context", "#optimization"], "emoji": "üéØ", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ —Å –æ—á–∏—Å—Ç–∫–æ–π –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: —Ñ–æ–∫—É—Å –Ω–∞ –≤–∞–∂–Ω–æ–º", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥ Context Denoising Training (CDT) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞–±–æ—Ç—ã –º–æ–¥–µ–ª–µ–π —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º. –ü—Ä–æ–±–ª–µ–º–∞ –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —Ç–æ–º, —á—Ç–æ 
[09.10.2025 04:14] Using data from previous issue: {"categories": ["#optimization", "#synthetic", "#cv", "#video"], "emoji": "ü§ñ", "ru": {"title": "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∏–¥–µ–æ —Å –∑–∞–ø—è—Å—Ç—å—è –∏–∑ –æ–±—ã—á–Ω—ã—Ö –∫–∞–º–µ—Ä –¥–ª—è —Ä–æ–±–æ—Ç–æ–≤", "desc": "WristWorld ‚Äî —ç—Ç–æ 4D –º–æ–¥–µ–ª—å –º–∏—Ä–∞, –∫–æ—Ç–æ—Ä–∞—è –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –≤–∏–¥–µ–æ —Å —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è –∑–∞–ø—è—Å—Ç—å—è —Ä–æ–±–æ—Ç–∞, –∏—Å–ø–æ–ª—å–∑—É—è —Ç–æ–ª—å–∫–æ –∑–∞–ø–∏—Å–∏ —Å –æ–±—ã—á–Ω—ã—Ö —Å—Ç–∞—Ü–∏–æ–Ω–∞—Ä–Ω—ã—Ö 
[09.10.2025 04:14] Using data from previous issue: {"categories": ["#interpretability", "#hallucinations", "#benchmark"], "emoji": "‚è≥", "ru": {"title": "–ö–æ–≥–¥–∞ –±–µ–Ω—á–º–∞—Ä–∫–∏ —Å—Ç–∞—Ä–µ—é—Ç: –ø—Ä–æ–±–ª–µ–º–∞ —É—Å—Ç–∞—Ä–µ–≤—à–∏—Ö —Ç–µ—Å—Ç–æ–≤ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ñ–∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç–∏ LLM", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ –±–µ–Ω—á–º–∞—Ä–∫–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ñ–∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç–∏ LLM —É—Å—Ç–∞—Ä–µ–≤–∞—é—Ç —Å–æ –≤—Ä–µ–º–µ–Ω–µ–º, —á—Ç–æ
[09.10.2025 04:14] Using data from previous issue: {"categories": ["#survey", "#optimization", "#dataset", "#benchmark", "#data", "#agents"], "emoji": "üè≠", "ru": {"title": "–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Ñ–∞–±—Ä–∏–∫–∞ ML-–∑–∞–¥–∞—á –∏–∑ —Å—ã—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç MLE-Smith ‚Äî –ø–æ–ª–Ω–æ—Å—Ç—å—é –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º—É–ª—å—Ç–∏-–∞–≥–µ–Ω—Ç–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á –ø
[09.10.2025 04:14] Using data from previous issue: {"categories": ["#optimization", "#rl", "#multimodal", "#rlhf", "#cv", "#transfer_learning"], "emoji": "üéØ", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –Ω–∞ –ª–µ—Ç—É: –º–æ–¥–µ–ª–∏ —É—á–∞—Ç—Å—è –ø—Ä—è–º–æ –≤–æ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –º–µ—Ç–æ–¥ TTRV, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç –ø–æ–Ω–∏–º–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä
[09.10.2025 04:14] Using data from previous issue: {"categories": ["#architecture", "#training", "#dataset", "#benchmark", "#video", "#diffusion", "#survey"], "emoji": "üé¨", "ru": {"title": "–û—Ç GAN –∫ Diffusion: —ç–≤–æ–ª—é—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –∏–∑ —Ç–µ–∫—Å—Ç–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –æ–±–∑–æ—Ä –º–æ–¥–µ–ª–µ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –∏–∑ —Ç–µ–∫—Å—Ç–∞ (text-to-video), –ø—Ä–æ—Å–ª–µ–∂–∏–≤–∞—è –∏—Ö —Ä–∞–∑
[09.10.2025 04:14] Using data from previous issue: {"categories": ["#optimization", "#reasoning", "#training", "#agents"], "emoji": "üîÑ", "ru": {"title": "–°–∞–º–æ—ç–≤–æ–ª—é—Ü–∏—è —á–µ—Ä–µ–∑ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã: AlphaApollo –ø–æ–¥–Ω–∏–º–∞–µ—Ç –ø–æ—Ç–æ–ª–æ–∫ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π LLM", "desc": "AlphaApollo ‚Äî —ç—Ç–æ —Å–∞–º–æ–æ–±—É—á–∞—é—â–∞—è—Å—è —Å–∏—Å—Ç–µ–º–∞ –∞–≥–µ–Ω—Ç–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä–∞—è —Ä–µ—à–∞–µ—Ç –¥–≤–µ –∫–ª—é—á–µ–≤—ã–µ –ø—Ä–æ–±–ª–µ–º—ã found
[09.10.2025 04:14] Using data from previous issue: {"categories": ["#cv", "#open_source", "#dataset", "#benchmark", "#optimization", "#survey"], "emoji": "üè•", "ru": {"title": "U-Bench: –≤—Å–µ—Å—Ç–æ—Ä–æ–Ω–Ω–∏–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è U-Net –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä –≤ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏", "desc": "U-Bench –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –ø–µ—Ä–≤—ã–π –º–∞—Å—à—Ç–∞–±–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ—Ü–µ–Ω–∫–∏ U-Net
[09.10.2025 04:14] Using data from previous issue: {"categories": ["#security", "#synthetic", "#cv", "#dataset", "#inference"], "emoji": "üîç", "ru": {"title": "–ü–æ–∏—Å–∫ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —á–µ—Ä–µ–∑ –∞–Ω–∞–ª–∏–∑ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ D¬≥QE –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤–∏–∑—É–∞–ª—å–Ω—ã–º–∏ autoregressive –º–æ–¥–µ–ª—è–º–∏. –ú–µ—Ç–æ–¥ –∞–Ω–∞–ª–∏
[09.10.2025 04:14] Renaming data file.
[09.10.2025 04:14] Renaming previous data. hf_papers.json to ./d/2025-10-09.json
[09.10.2025 04:14] Saving new data file.
[09.10.2025 04:14] Generating page.
[09.10.2025 04:14] Renaming previous page.
[09.10.2025 04:14] Renaming previous data. index.html to ./d/2025-10-09.html
[09.10.2025 04:14] Writing result.
[09.10.2025 04:14] Renaming log file.
[09.10.2025 04:14] Renaming previous data. log.txt to ./logs/2025-10-09_last_log.txt
