[09.10.2025 18:16] Read previous papers.
[09.10.2025 18:16] Generating top page (month).
[09.10.2025 18:16] Writing top page (month).
[09.10.2025 19:10] Read previous papers.
[09.10.2025 19:10] Get feed.
[09.10.2025 19:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.03215
[09.10.2025 19:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.06590
[09.10.2025 19:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.06308
[09.10.2025 19:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.06917
[09.10.2025 19:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.07310
[09.10.2025 19:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.06710
[09.10.2025 19:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.07315
[09.10.2025 19:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.04678
[09.10.2025 19:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.04204
[09.10.2025 19:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.07318
[09.10.2025 19:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.04212
[09.10.2025 19:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.04230
[09.10.2025 19:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.07019
[09.10.2025 19:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.06751
[09.10.2025 19:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.05862
[09.10.2025 19:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.06557
[09.10.2025 19:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.05644
[09.10.2025 19:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.07238
[09.10.2025 19:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.07143
[09.10.2025 19:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.05057
[09.10.2025 19:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.01954
[09.10.2025 19:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.06783
[09.10.2025 19:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.07313
[09.10.2025 19:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.07307
[09.10.2025 19:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.01982
[09.10.2025 19:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.06953
[09.10.2025 19:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.06855
[09.10.2025 19:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.06673
[09.10.2025 19:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.05491
[09.10.2025 19:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.04999
[09.10.2025 19:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.06261
[09.10.2025 19:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.07041
[09.10.2025 19:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.07037
[09.10.2025 19:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.21842
[09.10.2025 19:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.05891
[09.10.2025 19:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.06475
[09.10.2025 19:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.06426
[09.10.2025 19:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.04910
[09.10.2025 19:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.05152
[09.10.2025 19:10] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[09.10.2025 19:10] No deleted papers detected.
[09.10.2025 19:10] Downloading and parsing papers (pdf, html). Total: 39.
[09.10.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2510.03215.
[09.10.2025 19:10] Extra JSON file exists (./assets/json/2510.03215.json), skip PDF parsing.
[09.10.2025 19:10] Paper image links file exists (./assets/img_data/2510.03215.json), skip HTML parsing.
[09.10.2025 19:10] Success.
[09.10.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2510.06590.
[09.10.2025 19:10] Extra JSON file exists (./assets/json/2510.06590.json), skip PDF parsing.
[09.10.2025 19:10] Paper image links file exists (./assets/img_data/2510.06590.json), skip HTML parsing.
[09.10.2025 19:10] Success.
[09.10.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2510.06308.
[09.10.2025 19:10] Extra JSON file exists (./assets/json/2510.06308.json), skip PDF parsing.
[09.10.2025 19:10] Paper image links file exists (./assets/img_data/2510.06308.json), skip HTML parsing.
[09.10.2025 19:10] Success.
[09.10.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2510.06917.
[09.10.2025 19:10] Extra JSON file exists (./assets/json/2510.06917.json), skip PDF parsing.
[09.10.2025 19:10] Paper image links file exists (./assets/img_data/2510.06917.json), skip HTML parsing.
[09.10.2025 19:10] Success.
[09.10.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2510.07310.
[09.10.2025 19:10] Extra JSON file exists (./assets/json/2510.07310.json), skip PDF parsing.
[09.10.2025 19:10] Paper image links file exists (./assets/img_data/2510.07310.json), skip HTML parsing.
[09.10.2025 19:10] Success.
[09.10.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2510.06710.
[09.10.2025 19:10] Extra JSON file exists (./assets/json/2510.06710.json), skip PDF parsing.
[09.10.2025 19:10] Paper image links file exists (./assets/img_data/2510.06710.json), skip HTML parsing.
[09.10.2025 19:10] Success.
[09.10.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2510.07315.
[09.10.2025 19:10] Extra JSON file exists (./assets/json/2510.07315.json), skip PDF parsing.
[09.10.2025 19:10] Paper image links file exists (./assets/img_data/2510.07315.json), skip HTML parsing.
[09.10.2025 19:10] Success.
[09.10.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2510.04678.
[09.10.2025 19:10] Extra JSON file exists (./assets/json/2510.04678.json), skip PDF parsing.
[09.10.2025 19:10] Paper image links file exists (./assets/img_data/2510.04678.json), skip HTML parsing.
[09.10.2025 19:10] Success.
[09.10.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2510.04204.
[09.10.2025 19:10] Extra JSON file exists (./assets/json/2510.04204.json), skip PDF parsing.
[09.10.2025 19:10] Paper image links file exists (./assets/img_data/2510.04204.json), skip HTML parsing.
[09.10.2025 19:10] Success.
[09.10.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2510.07318.
[09.10.2025 19:10] Extra JSON file exists (./assets/json/2510.07318.json), skip PDF parsing.
[09.10.2025 19:10] Paper image links file exists (./assets/img_data/2510.07318.json), skip HTML parsing.
[09.10.2025 19:10] Success.
[09.10.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2510.04212.
[09.10.2025 19:10] Extra JSON file exists (./assets/json/2510.04212.json), skip PDF parsing.
[09.10.2025 19:10] Paper image links file exists (./assets/img_data/2510.04212.json), skip HTML parsing.
[09.10.2025 19:10] Success.
[09.10.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2510.04230.
[09.10.2025 19:10] Extra JSON file exists (./assets/json/2510.04230.json), skip PDF parsing.
[09.10.2025 19:10] Paper image links file exists (./assets/img_data/2510.04230.json), skip HTML parsing.
[09.10.2025 19:10] Success.
[09.10.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2510.07019.
[09.10.2025 19:10] Extra JSON file exists (./assets/json/2510.07019.json), skip PDF parsing.
[09.10.2025 19:10] Paper image links file exists (./assets/img_data/2510.07019.json), skip HTML parsing.
[09.10.2025 19:10] Success.
[09.10.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2510.06751.
[09.10.2025 19:10] Extra JSON file exists (./assets/json/2510.06751.json), skip PDF parsing.
[09.10.2025 19:10] Paper image links file exists (./assets/img_data/2510.06751.json), skip HTML parsing.
[09.10.2025 19:10] Success.
[09.10.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2510.05862.
[09.10.2025 19:10] Extra JSON file exists (./assets/json/2510.05862.json), skip PDF parsing.
[09.10.2025 19:10] Paper image links file exists (./assets/img_data/2510.05862.json), skip HTML parsing.
[09.10.2025 19:10] Success.
[09.10.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2510.06557.
[09.10.2025 19:10] Extra JSON file exists (./assets/json/2510.06557.json), skip PDF parsing.
[09.10.2025 19:10] Paper image links file exists (./assets/img_data/2510.06557.json), skip HTML parsing.
[09.10.2025 19:10] Success.
[09.10.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2510.05644.
[09.10.2025 19:10] Extra JSON file exists (./assets/json/2510.05644.json), skip PDF parsing.
[09.10.2025 19:10] Paper image links file exists (./assets/img_data/2510.05644.json), skip HTML parsing.
[09.10.2025 19:10] Success.
[09.10.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2510.07238.
[09.10.2025 19:10] Extra JSON file exists (./assets/json/2510.07238.json), skip PDF parsing.
[09.10.2025 19:10] Paper image links file exists (./assets/img_data/2510.07238.json), skip HTML parsing.
[09.10.2025 19:10] Success.
[09.10.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2510.07143.
[09.10.2025 19:10] Extra JSON file exists (./assets/json/2510.07143.json), skip PDF parsing.
[09.10.2025 19:10] Paper image links file exists (./assets/img_data/2510.07143.json), skip HTML parsing.
[09.10.2025 19:10] Success.
[09.10.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2510.05057.
[09.10.2025 19:10] Extra JSON file exists (./assets/json/2510.05057.json), skip PDF parsing.
[09.10.2025 19:10] Paper image links file exists (./assets/img_data/2510.05057.json), skip HTML parsing.
[09.10.2025 19:10] Success.
[09.10.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2510.01954.
[09.10.2025 19:10] Extra JSON file exists (./assets/json/2510.01954.json), skip PDF parsing.
[09.10.2025 19:10] Paper image links file exists (./assets/img_data/2510.01954.json), skip HTML parsing.
[09.10.2025 19:10] Success.
[09.10.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2510.06783.
[09.10.2025 19:10] Extra JSON file exists (./assets/json/2510.06783.json), skip PDF parsing.
[09.10.2025 19:10] Paper image links file exists (./assets/img_data/2510.06783.json), skip HTML parsing.
[09.10.2025 19:10] Success.
[09.10.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2510.07313.
[09.10.2025 19:10] Extra JSON file exists (./assets/json/2510.07313.json), skip PDF parsing.
[09.10.2025 19:10] Paper image links file exists (./assets/img_data/2510.07313.json), skip HTML parsing.
[09.10.2025 19:10] Success.
[09.10.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2510.07307.
[09.10.2025 19:10] Extra JSON file exists (./assets/json/2510.07307.json), skip PDF parsing.
[09.10.2025 19:10] Paper image links file exists (./assets/img_data/2510.07307.json), skip HTML parsing.
[09.10.2025 19:10] Success.
[09.10.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2510.01982.
[09.10.2025 19:10] Extra JSON file exists (./assets/json/2510.01982.json), skip PDF parsing.
[09.10.2025 19:10] Paper image links file exists (./assets/img_data/2510.01982.json), skip HTML parsing.
[09.10.2025 19:10] Success.
[09.10.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2510.06953.
[09.10.2025 19:10] Extra JSON file exists (./assets/json/2510.06953.json), skip PDF parsing.
[09.10.2025 19:10] Paper image links file exists (./assets/img_data/2510.06953.json), skip HTML parsing.
[09.10.2025 19:10] Success.
[09.10.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2510.06855.
[09.10.2025 19:10] Extra JSON file exists (./assets/json/2510.06855.json), skip PDF parsing.
[09.10.2025 19:10] Paper image links file exists (./assets/img_data/2510.06855.json), skip HTML parsing.
[09.10.2025 19:10] Success.
[09.10.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2510.06673.
[09.10.2025 19:10] Extra JSON file exists (./assets/json/2510.06673.json), skip PDF parsing.
[09.10.2025 19:10] Paper image links file exists (./assets/img_data/2510.06673.json), skip HTML parsing.
[09.10.2025 19:10] Success.
[09.10.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2510.05491.
[09.10.2025 19:10] Extra JSON file exists (./assets/json/2510.05491.json), skip PDF parsing.
[09.10.2025 19:10] Paper image links file exists (./assets/img_data/2510.05491.json), skip HTML parsing.
[09.10.2025 19:10] Success.
[09.10.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2510.04999.
[09.10.2025 19:10] Extra JSON file exists (./assets/json/2510.04999.json), skip PDF parsing.
[09.10.2025 19:10] Paper image links file exists (./assets/img_data/2510.04999.json), skip HTML parsing.
[09.10.2025 19:10] Success.
[09.10.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2510.06261.
[09.10.2025 19:10] Extra JSON file exists (./assets/json/2510.06261.json), skip PDF parsing.
[09.10.2025 19:10] Paper image links file exists (./assets/img_data/2510.06261.json), skip HTML parsing.
[09.10.2025 19:10] Success.
[09.10.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2510.07041.
[09.10.2025 19:10] Extra JSON file exists (./assets/json/2510.07041.json), skip PDF parsing.
[09.10.2025 19:10] Paper image links file exists (./assets/img_data/2510.07041.json), skip HTML parsing.
[09.10.2025 19:10] Success.
[09.10.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2510.07037.
[09.10.2025 19:10] Extra JSON file exists (./assets/json/2510.07037.json), skip PDF parsing.
[09.10.2025 19:10] Paper image links file exists (./assets/img_data/2510.07037.json), skip HTML parsing.
[09.10.2025 19:10] Success.
[09.10.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2509.21842.
[09.10.2025 19:10] Extra JSON file exists (./assets/json/2509.21842.json), skip PDF parsing.
[09.10.2025 19:10] Paper image links file exists (./assets/img_data/2509.21842.json), skip HTML parsing.
[09.10.2025 19:10] Success.
[09.10.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2510.05891.
[09.10.2025 19:10] Extra JSON file exists (./assets/json/2510.05891.json), skip PDF parsing.
[09.10.2025 19:10] Paper image links file exists (./assets/img_data/2510.05891.json), skip HTML parsing.
[09.10.2025 19:10] Success.
[09.10.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2510.06475.
[09.10.2025 19:10] Extra JSON file exists (./assets/json/2510.06475.json), skip PDF parsing.
[09.10.2025 19:10] Paper image links file exists (./assets/img_data/2510.06475.json), skip HTML parsing.
[09.10.2025 19:10] Success.
[09.10.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2510.06426.
[09.10.2025 19:10] Extra JSON file exists (./assets/json/2510.06426.json), skip PDF parsing.
[09.10.2025 19:10] Paper image links file exists (./assets/img_data/2510.06426.json), skip HTML parsing.
[09.10.2025 19:10] Success.
[09.10.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2510.04910.
[09.10.2025 19:10] Extra JSON file exists (./assets/json/2510.04910.json), skip PDF parsing.
[09.10.2025 19:10] Paper image links file exists (./assets/img_data/2510.04910.json), skip HTML parsing.
[09.10.2025 19:10] Success.
[09.10.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2510.05152.
[09.10.2025 19:10] Extra JSON file exists (./assets/json/2510.05152.json), skip PDF parsing.
[09.10.2025 19:10] Paper image links file exists (./assets/img_data/2510.05152.json), skip HTML parsing.
[09.10.2025 19:10] Success.
[09.10.2025 19:10] Enriching papers with extra data.
[09.10.2025 19:10] ********************************************************************************
[09.10.2025 19:10] Abstract 0. Cache-to-Cache (C2C) enables direct semantic communication between LLMs using neural network projections, improving accuracy and reducing latency compared to text-based communication.  					AI-generated summary 				 Multi-LLM systems harness the complementary strengths of diverse Large Language Mode...
[09.10.2025 19:10] ********************************************************************************
[09.10.2025 19:10] Abstract 1. MingTok, a continuous latent space visual tokenizer, unifies vision-language understanding and generation within an autoregressive framework, achieving state-of-the-art performance across both domains.  					AI-generated summary 				 Visual tokenization remains a core challenge in unifying visual un...
[09.10.2025 19:10] ********************************************************************************
[09.10.2025 19:10] Abstract 2. Lumina-DiMOO, an open-source foundational model, uses fully discrete diffusion modeling for efficient multi-modal generation and understanding, outperforming existing models.  					AI-generated summary 				 We introduce Lumina-DiMOO, an open-source foundational model for seamless multi-modal generat...
[09.10.2025 19:10] ********************************************************************************
[09.10.2025 19:10] Abstract 3. SHANKS, a general inference framework, enables spoken language models to generate unspoken reasoning while listening to user input, enhancing real-time interaction and task completion.  					AI-generated summary 				 Current large language models (LLMs) and spoken language models (SLMs) begin thinki...
[09.10.2025 19:10] ********************************************************************************
[09.10.2025 19:10] Abstract 4. MATRIX-11K dataset and MATRIX regularization enhance interaction fidelity and semantic alignment in video DiTs by aligning attention with multi-instance mask tracks.  					AI-generated summary 				 Video DiTs have advanced video generation, yet they still struggle to model multi-instance or subject-...
[09.10.2025 19:10] ********************************************************************************
[09.10.2025 19:10] Abstract 5. RLinf-VLA is a unified framework for scalable reinforcement learning training of vision-language-action models, offering improved performance and generalization compared to supervised fine-tuning.  					AI-generated summary 				 Recent progress in vision and language foundation models has significan...
[09.10.2025 19:10] ********************************************************************************
[09.10.2025 19:10] Abstract 6. Vibe Checker evaluates LLMs by combining functional correctness and instruction following to better align with human coding preferences.  					AI-generated summary 				 Large Language Models (LLMs) have catalyzed vibe coding, where users leverage LLMs to generate and iteratively refine code through ...
[09.10.2025 19:10] ********************************************************************************
[09.10.2025 19:10] Abstract 7. MATPO, a reinforcement learning method, optimizes tool-integrated multi-agent roles within a single LLM, improving performance and robustness over single-agent systems.  					AI-generated summary 				 Large language models (LLMs) increasingly rely on multi-turn tool-integrated planning for knowledge...
[09.10.2025 19:10] ********************************************************************************
[09.10.2025 19:10] Abstract 8. CALM framework uses expert interventions to refine LRM reasoning for optimization tasks, achieving high accuracy with fewer modifications compared to traditional methods.  					AI-generated summary 				 Large Reasoning Models (LRMs) have demonstrated strong capabilities in complex multi-step reasoni...
[09.10.2025 19:10] ********************************************************************************
[09.10.2025 19:10] Abstract 9. A memory framework combining short-term and long-term memory in neural networks improves long-sequence modeling efficiency and performance.  					AI-generated summary 				 Long-sequence modeling faces a fundamental trade-off between the efficiency of compressive fixed-size memory in RNN-like models ...
[09.10.2025 19:10] ********************************************************************************
[09.10.2025 19:10] Abstract 10. Low-precision training of transformer models with flash attention suffers from catastrophic loss explosions due to low-rank representations and biased rounding errors, which are addressed by a minimal modification to the flash attention mechanism.  					AI-generated summary 				 The pursuit of compu...
[09.10.2025 19:10] ********************************************************************************
[09.10.2025 19:10] Abstract 11. A language-mixed chain-of-thought reasoning approach improves performance in Korean-specific tasks by switching between English and Korean, achieving state-of-the-art results across various benchmarks.  					AI-generated summary 				 Recent frontier models employ long chain-of-thought reasoning to e...
[09.10.2025 19:10] ********************************************************************************
[09.10.2025 19:10] Abstract 12. Native Hybrid Attention (NHA) combines linear and full attention mechanisms to maintain long-term context while improving efficiency, outperforming Transformers in recall-intensive tasks and offering efficiency gains in pretrained LLMs.  					AI-generated summary 				 Transformers excel at sequence ...
[09.10.2025 19:10] ********************************************************************************
[09.10.2025 19:10] Abstract 13. OBS-Diff is a novel one-shot pruning framework that compresses large-scale text-to-image diffusion models with minimal quality loss and significant inference acceleration.  					AI-generated summary 				 Large-scale text-to-image diffusion models, while powerful, suffer from prohibitive computationa...
[09.10.2025 19:10] ********************************************************************************
[09.10.2025 19:10] Abstract 14. Context Denoising Training (CDT) improves long-context models' performance by mitigating contextual noise and enhancing attention on critical tokens.  					AI-generated summary 				 Long-context models (LCMs) have demonstrated great potential in processing long sequences, facilitating many real-worl...
[09.10.2025 19:10] ********************************************************************************
[09.10.2025 19:10] Abstract 15. Markovian Thinking, implemented in Delethink, enables efficient and scalable reinforcement learning for long-chain-of-thought reasoning in LLMs by decoupling thinking length from context size, resulting in linear compute and constant memory usage.  					AI-generated summary 				 Reinforcement learni...
[09.10.2025 19:10] ********************************************************************************
[09.10.2025 19:10] Abstract 16. The African Languages Lab addresses the underserved status of African languages in NLP by creating a large dataset and demonstrating improved model performance through fine-tuning.  					AI-generated summary 				 Despite representing nearly one-third of the world's languages, African languages remai...
[09.10.2025 19:10] ********************************************************************************
[09.10.2025 19:10] Abstract 17. Research investigates the aging of factuality benchmarks and its impact on evaluating the factuality of large language models, revealing significant unreliability due to outdated samples.  					AI-generated summary 				 The rapid evolution of large language models (LLMs) and the real world has outpa...
[09.10.2025 19:10] ********************************************************************************
[09.10.2025 19:10] Abstract 18. VTC-Bench is introduced to provide a fair evaluation framework for visual token compression by incorporating a data filtering mechanism to denoise existing benchmarks.  					AI-generated summary 				 Recent endeavors to accelerate inference in Multimodal Large Language Models (MLLMs) have primarily ...
[09.10.2025 19:10] ********************************************************************************
[09.10.2025 19:10] Abstract 19. An unsupervised method learns a compact state representation using a lightweight encoder and Diffusion Transformer decoder, improving robotic performance and enabling latent action decoding from static images.  					AI-generated summary 				 A fundamental challenge in embodied intelligence is develo...
[09.10.2025 19:10] ********************************************************************************
[09.10.2025 19:10] Abstract 20. PaDT, a unified paradigm for multimodal large language models, directly generates both textual and visual outputs, achieving state-of-the-art performance in visual perception tasks.  					AI-generated summary 				 Multimodal large language models (MLLMs) have advanced rapidly in recent years. Howeve...
[09.10.2025 19:10] ********************************************************************************
[09.10.2025 19:10] Abstract 21. TTRV enhances vision language understanding through test-time reinforcement learning, improving performance on object recognition and VQA without labeled data.  					AI-generated summary 				 Existing methods for extracting reward signals in Reinforcement Learning typically rely on labeled data and ...
[09.10.2025 19:10] ********************************************************************************
[09.10.2025 19:10] Abstract 22. WristWorld is a 4D world model that generates wrist-view videos from anchor views, improving video generation consistency and VLA performance.  					AI-generated summary 				 Wrist-view observations are crucial for VLA models as they capture fine-grained hand-object interactions that directly enhanc...
[09.10.2025 19:10] ********************************************************************************
[09.10.2025 19:10] Abstract 23. MLE-Smith automates the creation of high-quality, diverse MLE tasks from raw datasets using a multi-agent pipeline, improving scalability and maintaining task quality.  					AI-generated summary 				 While Language Models (LMs) have made significant progress in automating machine learning engineerin...
[09.10.2025 19:10] ********************************************************************************
[09.10.2025 19:10] Abstract 24. A novel Granular-GRPO framework enhances reinforcement learning in diffusion and flow models by improving reward assessment and reducing bias in denoising.  					AI-generated summary 				 The integration of online reinforcement learning (RL) into diffusion and flow models has recently emerged as a p...
[09.10.2025 19:10] ********************************************************************************
[09.10.2025 19:10] Abstract 25. Step-level uniformity in information density, measured using entropy-based metrics, improves reasoning accuracy in large language models across various benchmarks.  					AI-generated summary 				 The Uniform Information Density (UID) hypothesis suggests that effective communication maintains a stabl...
[09.10.2025 19:10] ********************************************************************************
[09.10.2025 19:10] Abstract 26. A novel framework for real-time event boundary detection in streaming videos uses prediction and error measurement to identify subtle event changes.  					AI-generated summary 				 Generic Event Boundary Detection (GEBD) aims to interpret long-form videos through the lens of human perception. Howeve...
[09.10.2025 19:10] ********************************************************************************
[09.10.2025 19:10] Abstract 27. Heptapod, an image autoregressive model using causal attention and next 2D distribution prediction, achieves superior performance on ImageNet generation by combining sequential modeling with holistic self-supervised learning.  					AI-generated summary 				 We introduce Heptapod, an image autoregres...
[09.10.2025 19:10] ********************************************************************************
[09.10.2025 19:10] Abstract 28. NorMuon, a novel optimizer combining orthogonalization with neuron-level adaptive learning rates, enhances training efficiency and balances parameter utilization in large language models.  					AI-generated summary 				 The choice of optimizer significantly impacts the training efficiency and comput...
[09.10.2025 19:10] ********************************************************************************
[09.10.2025 19:10] Abstract 29. A survey of text-to-video generative models from GANs and VAEs to hybrid Diffusion-Transformer architectures, detailing their development, limitations, and future directions.  					AI-generated summary 				 Text-to-video (T2V) generation technology holds potential to transform multiple domains such ...
[09.10.2025 19:10] ********************************************************************************
[09.10.2025 19:10] Abstract 30. AlphaApollo, a self-evolving reasoning system, enhances foundation model performance through tool integration and iterative refinement, achieving significant improvements in accuracy and pass rates.  					AI-generated summary 				 We present AlphaApollo, a self-evolving agentic reasoning system that...
[09.10.2025 19:10] ********************************************************************************
[09.10.2025 19:10] Abstract 31. U-Bench is a comprehensive benchmark evaluating 100 U-Net variants across 28 datasets and 10 imaging modalities, focusing on statistical robustness, zero-shot generalization, and computational efficiency.  					AI-generated summary 				 Over the past decade, U-Net has been the dominant architecture ...
[09.10.2025 19:10] ********************************************************************************
[09.10.2025 19:10] Abstract 32. This survey analyzes the current state of code-switching aware large language models, highlighting advancements and challenges in multilingual NLP.  					AI-generated summary 				 Code-switching (CSW), the alternation of languages and scripts within a single utterance, remains a fundamental challeng...
[09.10.2025 19:10] ********************************************************************************
[09.10.2025 19:10] Abstract 33. DeepTravel is an end-to-end reinforcement learning framework for autonomous travel planning that uses a hierarchical reward system and reply-augmented learning to improve performance over existing models.  					AI-generated summary 				 Travel planning (TP) agent has recently worked as an emerging b...
[09.10.2025 19:10] ********************************************************************************
[09.10.2025 19:10] Abstract 34. A novel method using Discrete Distribution Discrepancy-aware Quantization Error (D$^3$QE) detects images generated by visual autoregressive models by analyzing codebook frequency statistics and quantization errors.  					AI-generated summary 				 The emergence of visual autoregressive (AR) models ha...
[09.10.2025 19:10] ********************************************************************************
[09.10.2025 19:10] Abstract 35. PuzzlePlex benchmark assesses reasoning and planning capabilities of foundation models through diverse puzzles, providing metrics and insights into their performance and scalability.  					AI-generated summary 				 This work investigates the reasoning and planning capabilities of foundation models a...
[09.10.2025 19:10] ********************************************************************************
[09.10.2025 19:10] Abstract 36. FinLFQA evaluates LLMs' ability to provide reliable and nuanced attributions in long-form financial question answering through human and automatic assessments.  					AI-generated summary 				 Large Language Models (LLMs) frequently hallucinate to long-form questions, producing plausible yet factuall...
[09.10.2025 19:10] ********************************************************************************
[09.10.2025 19:10] Abstract 37. A new training paradigm, Glocal Information Bottleneck, improves time series imputation by aligning latent representations to retain global structure and local details under high missingness.  					AI-generated summary 				 Time Series Imputation (TSI), which aims to recover missing values in tempor...
[09.10.2025 19:10] ********************************************************************************
[09.10.2025 19:10] Abstract 38. The choice of delimiter in formatting in-context examples significantly impacts the performance of large language models across different families and tasks.  					AI-generated summary 				 Common Large Language model (LLM) evaluations rely on demonstration examples to steer models' responses to the...
[09.10.2025 19:10] Read previous papers.
[09.10.2025 19:10] Generating reviews via LLM API.
[09.10.2025 19:10] Using data from previous issue: {"categories": ["#architecture", "#training", "#multimodal", "#optimization", "#agi"], "emoji": "üîÑ", "ru": {"title": "–û–±—â–µ–Ω–∏–µ LLM –±–µ–∑ —Å–ª–æ–≤: –ø—Ä—è–º–∞—è –ø–µ—Ä–µ–¥–∞—á–∞ —Å–µ–º–∞–Ω—Ç–∏–∫–∏ —á–µ—Ä–µ–∑ –∫—ç—à", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ Cache-to-Cache (C2C) –¥–ª—è –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏ –º–µ–∂–¥—É –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ LLM, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥
[09.10.2025 19:10] Using data from previous issue: {"categories": ["#games", "#multimodal", "#optimization", "#cv", "#architecture", "#open_source"], "emoji": "üñºÔ∏è", "ru": {"title": "MingTok: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ –≤–∏–∑—É–∞–ª—å–Ω–æ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏", "desc": "MingTok ‚Äî —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≤–∏–∑—É–∞–ª—å–Ω–æ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–µ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –¥–ª—è –æ–±—ä
[09.10.2025 19:10] Using data from previous issue: {"categories": ["#open_source", "#multimodal", "#benchmark", "#diffusion", "#architecture"], "emoji": "üé≠", "ru": {"title": "–î–∏—Å–∫—Ä–µ—Ç–Ω–∞—è –¥–∏—Ñ—Ñ—É–∑–∏—è –¥–ª—è —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–∏", "desc": "Lumina-DiMOO - —ç—Ç–æ –æ—Ç–∫—Ä—ã—Ç–∞—è foundational –º–æ–¥–µ–ª—å –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞. –í –æ—Ç–ª–∏—á–∏–µ
[09.10.2025 19:10] Using data from previous issue: {"categories": ["#multimodal", "#reasoning", "#inference", "#long_context"], "emoji": "üéß", "ru": {"title": "–î—É–º–∞–π –ø–æ–∫–∞ —Å–ª—É—à–∞–µ—à—å: —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏ –¥–ª—è —Ä–∞–∑–≥–æ–≤–æ—Ä–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω SHANKS ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è spoken language models (SLM), –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª—è–º –≥–µ–Ω–µ—Ä
[09.10.2025 19:10] Using data from previous issue: {"categories": ["#dataset", "#hallucinations", "#benchmark", "#video", "#interpretability"], "emoji": "üé≠", "ru": {"title": "–£–ª—É—á—à–µ–Ω–∏–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π –≤ –≤–∏–¥–µ–æ —á–µ—Ä–µ–∑ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –≤–Ω–∏–º–∞–Ω–∏—è –ø–æ –º–∞—Å–∫–∞–º –æ–±—ä–µ–∫—Ç–æ–≤", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Å–æ–∑–¥–∞–ª–∏ –¥–∞—Ç–∞—Å–µ—Ç MATRIX-11K —Å –≤–∏–¥–µ–æ, —Å–æ–¥–µ—Ä–∂–∞—â–∏–º–∏ –æ–ø–∏—Å–∞–Ω–∏—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π
[09.10.2025 19:10] Using data from previous issue: {"categories": ["#agents", "#multimodal", "#training", "#reasoning", "#optimization", "#rl", "#robotics"], "emoji": "ü§ñ", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ —Ä–æ–±–æ—Ç–æ–≤ —á–µ—Ä–µ–∑ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ: RL –ø–æ–±–µ–∂–¥–∞–µ—Ç –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π supervised learning", "desc": "RLinf-VLA ‚Äî —ç—Ç–æ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –º–∞—Å—à—Ç–∞–±–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
[09.10.2025 19:10] Using data from previous issue: {"categories": ["#interpretability", "#alignment", "#training", "#benchmark", "#plp"], "emoji": "‚ú®", "ru": {"title": "Vibe Check: –∫–æ–≥–¥–∞ –∫–æ–¥ –¥–æ–ª–∂–µ–Ω –Ω–µ —Ç–æ–ª—å–∫–æ —Ä–∞–±–æ—Ç–∞—Ç—å, –Ω–æ –∏ –Ω—Ä–∞–≤–∏—Ç—å—Å—è", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ Vibe Checker ‚Äî –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ—Ü–µ–Ω–∫–µ LLM –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞, –∫–æ—Ç–æ—Ä—ã–π —É—á–∏—Ç—ã–≤–∞–µ—Ç –Ω–µ
[09.10.2025 19:10] Using data from previous issue: {"categories": ["#training", "#reasoning", "#optimization", "#agents", "#rl"], "emoji": "üé≠", "ru": {"title": "–û–¥–∏–Ω LLM –≤ —Ä–æ–ª–∏ —Ü–µ–ª–æ–π –∫–æ–º–∞–Ω–¥—ã –∞–≥–µ–Ω—Ç–æ–≤", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ MATPO ‚Äî –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º –≤–Ω—É—Ç—Ä–∏ –æ–¥–Ω–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏. –¢—Ä–∞–¥–∏—Ü–∏–æ
[09.10.2025 19:10] Using data from previous issue: {"categories": ["#benchmark", "#dataset", "#optimization", "#training", "#reasoning", "#rl"], "emoji": "üéØ", "ru": {"title": "–¢–æ—á–µ—á–Ω—ã–µ –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏ –≤–º–µ—Å—Ç–æ —Ç–æ—Ç–∞–ª—å–Ω–æ–≥–æ —Ñ–∞–π–Ω-—Ç—é–Ω–∏–Ω–≥–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç CALM ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –±–æ–ª—å—à–∏—Ö reasoning-–º–æ–¥–µ–ª–µ–π (LRM) –∫ –∑–∞–¥–∞—á–∞–º –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –º–æ–¥–µ
[09.10.2025 19:10] Using data from previous issue: {"categories": ["#architecture", "#training", "#benchmark", "#optimization", "#long_context"], "emoji": "üß†", "ru": {"title": "–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –≥–∏–ø–ø–æ–∫–∞–º–ø –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –ø–∞–º—è—Ç–∏ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –ø–∞–º—è—Ç–∏ –¥–ª—è –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π, –≤–¥–æ—Ö–Ω–æ–≤–ª—ë–Ω–Ω—É—é –º–æ–¥–µ–ª—å—é –º–Ω–æ–≥–æ–∫–æ–º–ø–æ–Ω–µ–Ω—Ç–Ω–æ–π –ø–∞–º—è—Ç
[09.10.2025 19:10] Using data from previous issue: {"categories": ["#architecture", "#training", "#optimization"], "emoji": "üí•", "ru": {"title": "–£–∫—Ä–æ—â–µ–Ω–∏–µ –≤–∑—Ä—ã–≤–æ–≤: —Å—Ç–∞–±–∏–ª—å–Ω–∞—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ —Å –Ω–∏–∑–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –≤—ã—è—Å–Ω–∏–ª–∏, –ø–æ—á–µ–º—É –æ–±—É—á–µ–Ω–∏–µ transformer-–º–æ–¥–µ–ª–µ–π —Å flash attention –≤ –Ω–∏–∑–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –∫–∞—Ç–∞—Å—Ç—Ä–æ—Ñ–∏—á–µ—Å
[09.10.2025 19:10] Using data from previous issue: {"categories": ["#open_source", "#reasoning", "#low_resource", "#benchmark", "#multilingual", "#dataset", "#long_context", "#training", "#data"], "emoji": "üá∞üá∑", "ru": {"title": "–°–º–µ—à–∞–Ω–Ω—ã–π —è–∑—ã–∫–æ–≤–æ–π reasoning: –∞–Ω–≥–ª–∏–π—Å–∫–∏–π –∫–∞–∫ —è–∫–æ—Ä—å –¥–ª—è —É—Å–∏–ª–µ–Ω–∏—è –∫–æ—Ä–µ–π—Å–∫–∏—Ö LLM", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –º–µ—Ç–æ–¥ La
[09.10.2025 19:10] Using data from previous issue: {"categories": ["#reasoning", "#training", "#optimization", "#architecture", "#long_context"], "emoji": "üîÄ", "ru": {"title": "–ì–∏–±—Ä–∏–¥–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ª–∏–Ω–µ–π–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Å —Ç–æ—á–Ω–æ—Å—Ç—å—é Transformer", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Native Hybrid Attention (NHA) ‚Äî –Ω–æ–≤—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É, –∫–æ—Ç–æ—Ä–∞—è –æ–±—ä–µ–¥–∏–Ω—è–µ
[09.10.2025 19:10] Using data from previous issue: {"categories": ["#optimization", "#training", "#inference", "#diffusion", "#architecture"], "emoji": "‚úÇÔ∏è", "ru": {"title": "–£–º–Ω–æ–µ —Å–∂–∞—Ç–∏–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∑–∞ –æ–¥–∏–Ω –ø—Ä–æ—Ö–æ–¥", "desc": "OBS-Diff ‚Äî —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–¥–Ω–æ—à–∞–≥–æ–≤–æ–π –æ–±—Ä–µ–∑–∫–∏ (pruning) –¥–ª—è —Å–∂–∞—Ç–∏—è –±–æ–ª—å—à–∏—Ö —Ç–µ–∫—Å—Ç-–≤-–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
[09.10.2025 19:10] Using data from previous issue: {"categories": ["#training", "#long_context", "#optimization"], "emoji": "üéØ", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ —Å –æ—á–∏—Å—Ç–∫–æ–π –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: —Ñ–æ–∫—É—Å –Ω–∞ –≤–∞–∂–Ω–æ–º", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥ Context Denoising Training (CDT) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞–±–æ—Ç—ã –º–æ–¥–µ–ª–µ–π —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º. –ü—Ä–æ–±–ª–µ–º–∞ –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —Ç–æ–º, —á—Ç–æ 
[09.10.2025 19:10] Using data from previous issue: {"categories": ["#training", "#reasoning", "#rlhf", "#long_context", "#rl", "#optimization"], "emoji": "üß©", "ru": {"title": "–ú–∞—Ä–∫–æ–≤—Å–∫–æ–µ –º—ã—à–ª–µ–Ω–∏–µ: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ –¥–ª–∏–Ω–Ω—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –±–µ–∑ –∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω—ã—Ö –∑–∞—Ç—Ä–∞—Ç", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Markovian Thinking –∏ —Å–∏—Å—Ç–µ–º—É Delethink ‚Äî –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é —è–∑
[09.10.2025 19:10] Using data from previous issue: {"categories": ["#dataset", "#training", "#data", "#multilingual", "#low_resource"], "emoji": "üåç", "ru": {"title": "–ê—Ñ—Ä–∏–∫–∞–Ω—Å–∫–∏–µ —è–∑—ã–∫–∏ –≤—ã—Ö–æ–¥—è—Ç –∏–∑ —Ç–µ–Ω–∏: –º–∞—Å—à—Ç–∞–±–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è NLP", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Å–æ–∑–¥–∞–ª–∏ African Languages Lab –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º—ã –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–π –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ—Å—Ç–∏ –∞—Ñ—Ä–∏–∫–∞–Ω—Å–∫–∏—Ö —è–∑—ã
[09.10.2025 19:10] Using data from previous issue: {"categories": ["#interpretability", "#hallucinations", "#benchmark"], "emoji": "‚è≥", "ru": {"title": "–ö–æ–≥–¥–∞ –±–µ–Ω—á–º–∞—Ä–∫–∏ —Å—Ç–∞—Ä–µ—é—Ç: –ø—Ä–æ–±–ª–µ–º–∞ —É—Å—Ç–∞—Ä–µ–≤—à–∏—Ö —Ç–µ—Å—Ç–æ–≤ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ñ–∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç–∏ LLM", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ –±–µ–Ω—á–º–∞—Ä–∫–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ñ–∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç–∏ LLM —É—Å—Ç–∞—Ä–µ–≤–∞—é—Ç —Å–æ –≤—Ä–µ–º–µ–Ω–µ–º, —á—Ç–æ
[09.10.2025 19:10] Using data from previous issue: {"categories": ["#optimization", "#benchmark", "#dataset"], "emoji": "üî¨", "ru": {"title": "VTC-Bench: —á–µ—Å—Ç–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —Å–∂–∞—Ç–∏—è –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö LLM", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç VTC-Bench ‚Äî –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —á–µ—Å—Ç–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –º–µ—Ç–æ–¥–æ–≤ —Å–∂–∞—Ç–∏—è –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à
[09.10.2025 19:10] Using data from previous issue: {"categories": ["#interpretability", "#training", "#agents", "#optimization", "#robotics", "#diffusion"], "emoji": "ü§ñ", "ru": {"title": "–î–≤–∞ —Ç–æ–∫–µ–Ω–∞ –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ä–æ–±–æ—Ç–æ–º: –∫–æ–º–ø–∞–∫—Ç–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –∏ –¥–µ–π—Å—Ç–≤–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –º–µ—Ç–æ–¥ StaMo –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∫–æ–º–ø–∞–∫—Ç–Ω–æ–≥–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Å–æ
[09.10.2025 19:10] Using data from previous issue: {"categories": ["#games", "#multimodal", "#training", "#cv", "#optimization", "#open_source"], "emoji": "üéØ", "ru": {"title": "–ü—Ä—è–º–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —á–µ—Ä–µ–∑ —Ç–æ–∫–µ–Ω—ã-–ø–∞—Ç—á–∏", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω PaDT ‚Äî –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö LLM, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç –Ω–∞–ø—Ä—è–º—É—é –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç
[09.10.2025 19:10] Using data from previous issue: {"categories": ["#optimization", "#rl", "#multimodal", "#rlhf", "#cv", "#transfer_learning"], "emoji": "üéØ", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –Ω–∞ –ª–µ—Ç—É: –º–æ–¥–µ–ª–∏ —É—á–∞—Ç—Å—è –ø—Ä—è–º–æ –≤–æ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –º–µ—Ç–æ–¥ TTRV, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç –ø–æ–Ω–∏–º–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä
[09.10.2025 19:10] Using data from previous issue: {"categories": ["#optimization", "#synthetic", "#cv", "#video"], "emoji": "ü§ñ", "ru": {"title": "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∏–¥–µ–æ —Å –∑–∞–ø—è—Å—Ç—å—è –∏–∑ –æ–±—ã—á–Ω—ã—Ö –∫–∞–º–µ—Ä –¥–ª—è —Ä–æ–±–æ—Ç–æ–≤", "desc": "WristWorld ‚Äî —ç—Ç–æ 4D –º–æ–¥–µ–ª—å –º–∏—Ä–∞, –∫–æ—Ç–æ—Ä–∞—è –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –≤–∏–¥–µ–æ —Å —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è –∑–∞–ø—è—Å—Ç—å—è —Ä–æ–±–æ—Ç–∞, –∏—Å–ø–æ–ª—å–∑—É—è —Ç–æ–ª—å–∫–æ –∑–∞–ø–∏—Å–∏ —Å –æ–±—ã—á–Ω—ã—Ö —Å—Ç–∞—Ü–∏–æ–Ω–∞—Ä–Ω—ã—Ö 
[09.10.2025 19:10] Using data from previous issue: {"categories": ["#survey", "#optimization", "#dataset", "#benchmark", "#data", "#agents"], "emoji": "üè≠", "ru": {"title": "–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Ñ–∞–±—Ä–∏–∫–∞ ML-–∑–∞–¥–∞—á –∏–∑ —Å—ã—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç MLE-Smith ‚Äî –ø–æ–ª–Ω–æ—Å—Ç—å—é –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º—É–ª—å—Ç–∏-–∞–≥–µ–Ω—Ç–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á –ø
[09.10.2025 19:10] Using data from previous issue: {"categories": ["#games", "#alignment", "#rlhf", "#diffusion", "#optimization", "#training", "#rl"], "emoji": "üéØ", "ru": {"title": "–¢–æ—á–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –º—É–ª—å—Ç–∏–º–∞—Å—à—Ç–∞–±–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ Granular-GRPO –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏
[09.10.2025 19:10] Using data from previous issue: {"categories": ["#reasoning", "#benchmark", "#training"], "emoji": "üìä", "ru": {"title": "–†–∞–≤–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ ‚Äî –∫–ª—é—á –∫ —Ç–æ—á–Ω–æ—Å—Ç–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π LLM", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–∏–º–µ–Ω–∏–ª–∏ –≥–∏–ø–æ—Ç–µ–∑—É —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–π –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ (UID) –∫ –∞–Ω–∞–ª–∏–∑—É —Ü–µ–ø–æ—á–µ–∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –û–Ω–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª
[09.10.2025 19:10] Using data from previous issue: {"categories": ["#interpretability", "#video", "#benchmark", "#long_context"], "emoji": "üé¨", "ru": {"title": "–†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –≥—Ä–∞–Ω–∏—Ü —Å–æ–±—ã—Ç–∏–π –≤ –≤–∏–¥–µ–æ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏ —á–µ—Ä–µ–∑ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∏ –æ—à–∏–±–∫—É", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –∑–∞–¥–∞—á—É Online Generic Event Boundary Detection (On-GEBD) ‚Äî –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏
[09.10.2025 19:10] Using data from previous issue: {"categories": ["#architecture", "#optimization", "#games", "#benchmark", "#cv"], "emoji": "üêô", "ru": {"title": "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ 2D —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π –¥–ª—è –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "Heptapod ‚Äî —ç—Ç–æ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä–∞—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç causal attention –∏
[09.10.2025 19:10] Using data from previous issue: {"categories": ["#architecture", "#training", "#optimization"], "emoji": "‚öñÔ∏è", "ru": {"title": "–ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –Ω–µ–π—Ä–æ–Ω–æ–≤ —á–µ—Ä–µ–∑ –æ—Ä—Ç–æ–≥–æ–Ω–∞–ª–∏–∑–∞—Ü–∏—é –∏ –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–µ —à–∞–≥–∏ –æ–±—É—á–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç NorMuon ‚Äî –Ω–æ–≤—ã–π –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–π –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –æ—Ä—Ç–æ–≥–æ–Ω–∞–ª–∏–∑–∞—Ü–∏—é 
[09.10.2025 19:10] Using data from previous issue: {"categories": ["#architecture", "#training", "#dataset", "#benchmark", "#video", "#diffusion", "#survey"], "emoji": "üé¨", "ru": {"title": "–û—Ç GAN –∫ Diffusion: —ç–≤–æ–ª—é—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –∏–∑ —Ç–µ–∫—Å—Ç–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –æ–±–∑–æ—Ä –º–æ–¥–µ–ª–µ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –∏–∑ —Ç–µ–∫—Å—Ç–∞ (text-to-video), –ø—Ä–æ—Å–ª–µ–∂–∏–≤–∞—è –∏—Ö —Ä–∞–∑
[09.10.2025 19:10] Using data from previous issue: {"categories": ["#optimization", "#reasoning", "#training", "#agents"], "emoji": "üîÑ", "ru": {"title": "–°–∞–º–æ—ç–≤–æ–ª—é—Ü–∏—è —á–µ—Ä–µ–∑ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã: AlphaApollo –ø–æ–¥–Ω–∏–º–∞–µ—Ç –ø–æ—Ç–æ–ª–æ–∫ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π LLM", "desc": "AlphaApollo ‚Äî —ç—Ç–æ —Å–∞–º–æ–æ–±—É—á–∞—é—â–∞—è—Å—è —Å–∏—Å—Ç–µ–º–∞ –∞–≥–µ–Ω—Ç–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä–∞—è —Ä–µ—à–∞–µ—Ç –¥–≤–µ –∫–ª—é—á–µ–≤—ã–µ –ø—Ä–æ–±–ª–µ–º—ã found
[09.10.2025 19:10] Using data from previous issue: {"categories": ["#cv", "#open_source", "#dataset", "#benchmark", "#optimization", "#survey"], "emoji": "üè•", "ru": {"title": "U-Bench: –≤—Å–µ—Å—Ç–æ—Ä–æ–Ω–Ω–∏–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è U-Net –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä –≤ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏", "desc": "U-Bench –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –ø–µ—Ä–≤—ã–π –º–∞—Å—à—Ç–∞–±–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ—Ü–µ–Ω–∫–∏ U-Net
[09.10.2025 19:10] Using data from previous issue: {"categories": ["#training", "#multilingual", "#architecture", "#benchmark", "#survey", "#dataset", "#low_resource"], "emoji": "üîÄ", "ru": {"title": "–ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤: –≤—ã–∑–æ–≤ –¥–ª—è —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö LLM", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –æ–±–∑–æ—Ä —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM), —Å–ø–æ—Å–æ–±
[09.10.2025 19:10] Using data from previous issue: {"categories": ["#agents", "#games", "#reasoning", "#training", "#optimization", "#rl"], "emoji": "üó∫Ô∏è", "ru": {"title": "–ê–≤—Ç–æ–Ω–æ–º–Ω—ã–π AI-–∞–≥–µ–Ω—Ç –¥–ª—è –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –ø—É—Ç–µ—à–µ—Å—Ç–≤–∏–π –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç frontier –º–æ–¥–µ–ª–∏", "desc": "DeepTravel ‚Äî —ç—Ç–æ end-to-end —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ reinforcement learning –¥–ª—è –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –ø–ª–∞–Ω
[09.10.2025 19:10] Using data from previous issue: {"categories": ["#security", "#synthetic", "#cv", "#dataset", "#inference"], "emoji": "üîç", "ru": {"title": "–ü–æ–∏—Å–∫ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —á–µ—Ä–µ–∑ –∞–Ω–∞–ª–∏–∑ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ D¬≥QE –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤–∏–∑—É–∞–ª—å–Ω—ã–º–∏ autoregressive –º–æ–¥–µ–ª—è–º–∏. –ú–µ—Ç–æ–¥ –∞–Ω–∞–ª–∏
[09.10.2025 19:10] Using data from previous issue: {"categories": ["#training", "#reasoning", "#games", "#benchmark"], "emoji": "üß©", "ru": {"title": "–ì–æ–ª–æ–≤–æ–ª–æ–º–∫–∏ –∫–∞–∫ –∏—Å–ø—ã—Ç–∞–Ω–∏–µ –¥–ª—è AI: –ø—Ä–æ–≤–µ—Ä–∫–∞ –ª–æ–≥–∏–∫–∏ –∏ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ PuzzlePlex ‚Äî –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π foundation models –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é –∏ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—é —á–µ—Ä–µ–∑ 
[09.10.2025 19:10] Using data from previous issue: {"categories": ["#long_context", "#hallucinations", "#benchmark", "#reasoning", "#multimodal"], "emoji": "üí∞", "ru": {"title": "–ü—Ä–æ–≤–µ—Ä–∫–∞ LLM –Ω–∞ —á–µ—Å—Ç–Ω–æ—Å—Ç—å –≤ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –æ—Ç–≤–µ—Ç–∞—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç FinLFQA ‚Äî –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ LLM –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ä–∞–∑–≤—ë—Ä–Ω—É—Ç—ã–µ –æ—Ç–≤–µ—Ç—ã –Ω–∞ —Å–ª–æ–∂–Ω—ã–µ —Ñ–∏–Ω–∞–Ω—Å
[09.10.2025 19:10] Using data from previous issue: {"categories": ["#dataset", "#optimization", "#training"], "emoji": "üåç", "ru": {"title": "–ë–∞–ª–∞–Ω—Å –º–µ–∂–¥—É –≥–ª–æ–±–∞–ª—å–Ω—ã–º –∏ –ª–æ–∫–∞–ª—å–Ω—ã–º: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—é –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –ø–∞—Ä–∞–¥–∏–≥–º—É –æ–±—É—á–µ–Ω–∏—è Glocal Information Bottleneck –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –≤
[09.10.2025 19:10] Using data from previous issue: {"categories": ["#interpretability", "#training", "#data", "#benchmark", "#optimization"], "emoji": "üîó", "ru": {"title": "–û–¥–∏–Ω —Å–∏–º–≤–æ–ª –º–æ–∂–µ—Ç –∏–∑–º–µ–Ω–∏—Ç—å –≤—Å—ë: –∫–∞–∫ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏ –≤–ª–∏—è—é—Ç –Ω–∞ —Ä–∞–±–æ—Ç—É LLM", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –≤—ã–±–æ—Ä —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è –º–µ–∂–¥—É –ø—Ä–∏–º–µ—Ä–∞–º–∏ –≤ –ø—Ä–æ–º–ø—Ç–µ (–∑–∞–ø—è—Ç–∞—è, —Ç–æ—á–∫–∞ —Å –∑–∞–ø—è—Ç–æ–π
[09.10.2025 19:10] Renaming data file.
[09.10.2025 19:10] Renaming previous data. hf_papers.json to ./d/2025-10-09.json
[09.10.2025 19:10] Saving new data file.
[09.10.2025 19:10] Generating page.
[09.10.2025 19:10] Renaming previous page.
[09.10.2025 19:10] Renaming previous data. index.html to ./d/2025-10-09.html
[09.10.2025 19:10] Writing result.
[09.10.2025 19:10] Renaming log file.
[09.10.2025 19:10] Renaming previous data. log.txt to ./logs/2025-10-09_last_log.txt
