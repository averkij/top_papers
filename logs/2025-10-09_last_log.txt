[09.10.2025 15:28] Read previous papers.
[09.10.2025 15:28] Generating top page (month).
[09.10.2025 15:28] Writing top page (month).
[09.10.2025 16:15] Read previous papers.
[09.10.2025 16:15] Get feed.
[09.10.2025 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.06590
[09.10.2025 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.03215
[09.10.2025 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.06308
[09.10.2025 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.06917
[09.10.2025 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.06710
[09.10.2025 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.07310
[09.10.2025 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.07315
[09.10.2025 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.04678
[09.10.2025 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.04204
[09.10.2025 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.04212
[09.10.2025 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.06751
[09.10.2025 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.04230
[09.10.2025 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.07318
[09.10.2025 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.07019
[09.10.2025 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.05862
[09.10.2025 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.07143
[09.10.2025 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.06557
[09.10.2025 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.05057
[09.10.2025 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.07238
[09.10.2025 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.01954
[09.10.2025 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.06783
[09.10.2025 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.05644
[09.10.2025 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.07313
[09.10.2025 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.01982
[09.10.2025 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.07307
[09.10.2025 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.06953
[09.10.2025 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.06855
[09.10.2025 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.04999
[09.10.2025 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.06261
[09.10.2025 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.07041
[09.10.2025 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.07037
[09.10.2025 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.06673
[09.10.2025 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.05491
[09.10.2025 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2509.21842
[09.10.2025 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.05891
[09.10.2025 16:15] Extract page data from URL. URL: https://huggingface.co/papers/2510.06475
[09.10.2025 16:15] Extract page data from URL. URL: https://huggingface.co/papers/2510.06426
[09.10.2025 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.04910
[09.10.2025 16:15] Extract page data from URL. URL: https://huggingface.co/papers/2510.05152
[09.10.2025 16:15] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[09.10.2025 16:15] No deleted papers detected.
[09.10.2025 16:15] Downloading and parsing papers (pdf, html). Total: 39.
[09.10.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2510.06590.
[09.10.2025 16:15] Extra JSON file exists (./assets/json/2510.06590.json), skip PDF parsing.
[09.10.2025 16:15] Paper image links file exists (./assets/img_data/2510.06590.json), skip HTML parsing.
[09.10.2025 16:15] Success.
[09.10.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2510.03215.
[09.10.2025 16:15] Extra JSON file exists (./assets/json/2510.03215.json), skip PDF parsing.
[09.10.2025 16:15] Paper image links file exists (./assets/img_data/2510.03215.json), skip HTML parsing.
[09.10.2025 16:15] Success.
[09.10.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2510.06308.
[09.10.2025 16:15] Extra JSON file exists (./assets/json/2510.06308.json), skip PDF parsing.
[09.10.2025 16:15] Paper image links file exists (./assets/img_data/2510.06308.json), skip HTML parsing.
[09.10.2025 16:15] Success.
[09.10.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2510.06917.
[09.10.2025 16:15] Extra JSON file exists (./assets/json/2510.06917.json), skip PDF parsing.
[09.10.2025 16:15] Paper image links file exists (./assets/img_data/2510.06917.json), skip HTML parsing.
[09.10.2025 16:15] Success.
[09.10.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2510.06710.
[09.10.2025 16:15] Extra JSON file exists (./assets/json/2510.06710.json), skip PDF parsing.
[09.10.2025 16:15] Paper image links file exists (./assets/img_data/2510.06710.json), skip HTML parsing.
[09.10.2025 16:15] Success.
[09.10.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2510.07310.
[09.10.2025 16:15] Extra JSON file exists (./assets/json/2510.07310.json), skip PDF parsing.
[09.10.2025 16:15] Paper image links file exists (./assets/img_data/2510.07310.json), skip HTML parsing.
[09.10.2025 16:15] Success.
[09.10.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2510.07315.
[09.10.2025 16:15] Extra JSON file exists (./assets/json/2510.07315.json), skip PDF parsing.
[09.10.2025 16:15] Paper image links file exists (./assets/img_data/2510.07315.json), skip HTML parsing.
[09.10.2025 16:15] Success.
[09.10.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2510.04678.
[09.10.2025 16:15] Extra JSON file exists (./assets/json/2510.04678.json), skip PDF parsing.
[09.10.2025 16:15] Paper image links file exists (./assets/img_data/2510.04678.json), skip HTML parsing.
[09.10.2025 16:15] Success.
[09.10.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2510.04204.
[09.10.2025 16:15] Extra JSON file exists (./assets/json/2510.04204.json), skip PDF parsing.
[09.10.2025 16:15] Paper image links file exists (./assets/img_data/2510.04204.json), skip HTML parsing.
[09.10.2025 16:15] Success.
[09.10.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2510.04212.
[09.10.2025 16:15] Extra JSON file exists (./assets/json/2510.04212.json), skip PDF parsing.
[09.10.2025 16:15] Paper image links file exists (./assets/img_data/2510.04212.json), skip HTML parsing.
[09.10.2025 16:15] Success.
[09.10.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2510.06751.
[09.10.2025 16:15] Extra JSON file exists (./assets/json/2510.06751.json), skip PDF parsing.
[09.10.2025 16:15] Paper image links file exists (./assets/img_data/2510.06751.json), skip HTML parsing.
[09.10.2025 16:15] Success.
[09.10.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2510.04230.
[09.10.2025 16:15] Extra JSON file exists (./assets/json/2510.04230.json), skip PDF parsing.
[09.10.2025 16:15] Paper image links file exists (./assets/img_data/2510.04230.json), skip HTML parsing.
[09.10.2025 16:15] Success.
[09.10.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2510.07318.
[09.10.2025 16:15] Extra JSON file exists (./assets/json/2510.07318.json), skip PDF parsing.
[09.10.2025 16:15] Paper image links file exists (./assets/img_data/2510.07318.json), skip HTML parsing.
[09.10.2025 16:15] Success.
[09.10.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2510.07019.
[09.10.2025 16:15] Extra JSON file exists (./assets/json/2510.07019.json), skip PDF parsing.
[09.10.2025 16:15] Paper image links file exists (./assets/img_data/2510.07019.json), skip HTML parsing.
[09.10.2025 16:15] Success.
[09.10.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2510.05862.
[09.10.2025 16:15] Extra JSON file exists (./assets/json/2510.05862.json), skip PDF parsing.
[09.10.2025 16:15] Paper image links file exists (./assets/img_data/2510.05862.json), skip HTML parsing.
[09.10.2025 16:15] Success.
[09.10.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2510.07143.
[09.10.2025 16:15] Extra JSON file exists (./assets/json/2510.07143.json), skip PDF parsing.
[09.10.2025 16:15] Paper image links file exists (./assets/img_data/2510.07143.json), skip HTML parsing.
[09.10.2025 16:15] Success.
[09.10.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2510.06557.
[09.10.2025 16:15] Extra JSON file exists (./assets/json/2510.06557.json), skip PDF parsing.
[09.10.2025 16:15] Paper image links file exists (./assets/img_data/2510.06557.json), skip HTML parsing.
[09.10.2025 16:15] Success.
[09.10.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2510.05057.
[09.10.2025 16:15] Extra JSON file exists (./assets/json/2510.05057.json), skip PDF parsing.
[09.10.2025 16:15] Paper image links file exists (./assets/img_data/2510.05057.json), skip HTML parsing.
[09.10.2025 16:15] Success.
[09.10.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2510.07238.
[09.10.2025 16:15] Extra JSON file exists (./assets/json/2510.07238.json), skip PDF parsing.
[09.10.2025 16:15] Paper image links file exists (./assets/img_data/2510.07238.json), skip HTML parsing.
[09.10.2025 16:15] Success.
[09.10.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2510.01954.
[09.10.2025 16:15] Extra JSON file exists (./assets/json/2510.01954.json), skip PDF parsing.
[09.10.2025 16:15] Paper image links file exists (./assets/img_data/2510.01954.json), skip HTML parsing.
[09.10.2025 16:15] Success.
[09.10.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2510.06783.
[09.10.2025 16:15] Extra JSON file exists (./assets/json/2510.06783.json), skip PDF parsing.
[09.10.2025 16:15] Paper image links file exists (./assets/img_data/2510.06783.json), skip HTML parsing.
[09.10.2025 16:15] Success.
[09.10.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2510.05644.
[09.10.2025 16:15] Extra JSON file exists (./assets/json/2510.05644.json), skip PDF parsing.
[09.10.2025 16:15] Paper image links file exists (./assets/img_data/2510.05644.json), skip HTML parsing.
[09.10.2025 16:15] Success.
[09.10.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2510.07313.
[09.10.2025 16:15] Extra JSON file exists (./assets/json/2510.07313.json), skip PDF parsing.
[09.10.2025 16:15] Paper image links file exists (./assets/img_data/2510.07313.json), skip HTML parsing.
[09.10.2025 16:15] Success.
[09.10.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2510.01982.
[09.10.2025 16:15] Extra JSON file exists (./assets/json/2510.01982.json), skip PDF parsing.
[09.10.2025 16:15] Paper image links file exists (./assets/img_data/2510.01982.json), skip HTML parsing.
[09.10.2025 16:15] Success.
[09.10.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2510.07307.
[09.10.2025 16:15] Extra JSON file exists (./assets/json/2510.07307.json), skip PDF parsing.
[09.10.2025 16:15] Paper image links file exists (./assets/img_data/2510.07307.json), skip HTML parsing.
[09.10.2025 16:15] Success.
[09.10.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2510.06953.
[09.10.2025 16:15] Extra JSON file exists (./assets/json/2510.06953.json), skip PDF parsing.
[09.10.2025 16:15] Paper image links file exists (./assets/img_data/2510.06953.json), skip HTML parsing.
[09.10.2025 16:15] Success.
[09.10.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2510.06855.
[09.10.2025 16:15] Extra JSON file exists (./assets/json/2510.06855.json), skip PDF parsing.
[09.10.2025 16:15] Paper image links file exists (./assets/img_data/2510.06855.json), skip HTML parsing.
[09.10.2025 16:15] Success.
[09.10.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2510.04999.
[09.10.2025 16:15] Extra JSON file exists (./assets/json/2510.04999.json), skip PDF parsing.
[09.10.2025 16:15] Paper image links file exists (./assets/img_data/2510.04999.json), skip HTML parsing.
[09.10.2025 16:15] Success.
[09.10.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2510.06261.
[09.10.2025 16:15] Extra JSON file exists (./assets/json/2510.06261.json), skip PDF parsing.
[09.10.2025 16:15] Paper image links file exists (./assets/img_data/2510.06261.json), skip HTML parsing.
[09.10.2025 16:15] Success.
[09.10.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2510.07041.
[09.10.2025 16:15] Extra JSON file exists (./assets/json/2510.07041.json), skip PDF parsing.
[09.10.2025 16:15] Paper image links file exists (./assets/img_data/2510.07041.json), skip HTML parsing.
[09.10.2025 16:15] Success.
[09.10.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2510.07037.
[09.10.2025 16:15] Extra JSON file exists (./assets/json/2510.07037.json), skip PDF parsing.
[09.10.2025 16:15] Paper image links file exists (./assets/img_data/2510.07037.json), skip HTML parsing.
[09.10.2025 16:15] Success.
[09.10.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2510.06673.
[09.10.2025 16:15] Extra JSON file exists (./assets/json/2510.06673.json), skip PDF parsing.
[09.10.2025 16:15] Paper image links file exists (./assets/img_data/2510.06673.json), skip HTML parsing.
[09.10.2025 16:15] Success.
[09.10.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2510.05491.
[09.10.2025 16:15] Extra JSON file exists (./assets/json/2510.05491.json), skip PDF parsing.
[09.10.2025 16:15] Paper image links file exists (./assets/img_data/2510.05491.json), skip HTML parsing.
[09.10.2025 16:15] Success.
[09.10.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2509.21842.
[09.10.2025 16:15] Extra JSON file exists (./assets/json/2509.21842.json), skip PDF parsing.
[09.10.2025 16:15] Paper image links file exists (./assets/img_data/2509.21842.json), skip HTML parsing.
[09.10.2025 16:15] Success.
[09.10.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2510.05891.
[09.10.2025 16:15] Extra JSON file exists (./assets/json/2510.05891.json), skip PDF parsing.
[09.10.2025 16:15] Paper image links file exists (./assets/img_data/2510.05891.json), skip HTML parsing.
[09.10.2025 16:15] Success.
[09.10.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2510.06475.
[09.10.2025 16:15] Downloading paper 2510.06475 from http://arxiv.org/pdf/2510.06475v1...
[09.10.2025 16:15] Extracting affiliations from text.
[09.10.2025 16:15] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"PUZZLEPLEX: Benchmarking Foundation Models on Reasoning and Planning with Puzzles Yitao Long1 Yuru Jiang2 Hongjun Liu1 Yilun Zhao3 Jingchen Sun4 Yiqiu Shen1,5 Chen Zhao1 Arman Cohan 3 Dennis Shasha1 1New York University 2Zhejiang University 3Yale University 4University at Buffalo, SUNY 5NYU Grossman School of Medicine https://github.com/yitaoLong/PuzzlePlex 5 2 0 2 7 ] A . [ 1 5 7 4 6 0 . 0 1 5 2 : r a "
[09.10.2025 16:15] Response: ```python
[
    "New York University",
    "Zhejiang University",
    "Yale University",
    "University at Buffalo, SUNY",
    "NYU Grossman School of Medicine"
]
```
[09.10.2025 16:15] Deleting PDF ./assets/pdf/2510.06475.pdf.
[09.10.2025 16:15] Success.
[09.10.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2510.06426.
[09.10.2025 16:15] Downloading paper 2510.06426 from http://arxiv.org/pdf/2510.06426v1...
[09.10.2025 16:15] Extracting affiliations from text.
[09.10.2025 16:15] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 7 ] . [ 1 6 2 4 6 0 . 0 1 5 2 : r FINLFQA: Evaluating Attributed Text Generation of LLMs in Financial Long-Form Question Answering Yitao Long3* Tiansheng Hu1 Yilun Zhao2 Arman Cohan2 Chen Zhao1,3 1NYU Shanghai 2Yale University 3 New York University https://github.com/yitaoLong/FinLFQA "
[09.10.2025 16:15] Response: ```python
["NYU Shanghai", "Yale University", "New York University"]
```
[09.10.2025 16:15] Deleting PDF ./assets/pdf/2510.06426.pdf.
[09.10.2025 16:15] Success.
[09.10.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2510.04910.
[09.10.2025 16:15] Extra JSON file exists (./assets/json/2510.04910.json), skip PDF parsing.
[09.10.2025 16:15] Paper image links file exists (./assets/img_data/2510.04910.json), skip HTML parsing.
[09.10.2025 16:15] Success.
[09.10.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2510.05152.
[09.10.2025 16:15] Downloading paper 2510.05152 from http://arxiv.org/pdf/2510.05152v1...
[09.10.2025 16:15] Extracting affiliations from text.
[09.10.2025 16:15] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 2 ] . [ 1 2 5 1 5 0 . 0 1 5 2 : r single character can make or break your LLM evals Jingtong Su1,2, Jianyu Zhang1,2, Karen Ullrich1, L√©on Bottou1,2, Mark Ibrahim1 1FAIR at Meta, 2New York University Common Large Language model (LLM) evaluations rely on demonstration examples to steer models responses to the desired style. While the number of examples used has been studied and standardized, the choice of how to format examples is less investigated. In evaluation protocols and real world usage, users face the choice how to separate in-context examples: use comma? new line? semi-colon? #, etc? Surprisingly, we find this seemingly minor choice can dramatically alter model response quality. Across leading model families (Llama, Qwen, Gemma), performance on mmlu for example can vary by 23% depending on the choice of delimiter. In fact, one can manipulate model rankings to put any model in the lead by only modifying the single character separating examples. We find LLMs brittleness pervades topics, model families, and doesnt improve with scale. By probing attention head scores, we find that good-performing delimiters steer attention towards key tokens in the input. Finally, we explore methods to improve LLMs robustness to the choice of delimiter. We find specifying the selected delimiter in the prompt boosts robustness and offer practical recommendations for the best-performing delimiters to select. Date: September 30, Prompting language model is fickle craft. The quality of language models response is sensitive to how user crafts their prompt (Liu et al., 2023). Commonly reported large language model (LLM) evaluations, however, use fixed prompt templates that do not capture models sensitivity to prompts in real world usage. This blind spot in LLM evaluations affects how we measure progress, compare models, and research new ways of training models. To muddy matters further, LLM evaluation protocols for many benchmarks rely on demonstration examples to steer model o"
[09.10.2025 16:15] Response: ```python
["FAIR at Meta", "New York University"]
```
[09.10.2025 16:15] Deleting PDF ./assets/pdf/2510.05152.pdf.
[09.10.2025 16:15] Success.
[09.10.2025 16:15] Enriching papers with extra data.
[09.10.2025 16:15] ********************************************************************************
[09.10.2025 16:15] Abstract 0. MingTok, a continuous latent space visual tokenizer, unifies vision-language understanding and generation within an autoregressive framework, achieving state-of-the-art performance across both domains.  					AI-generated summary 				 Visual tokenization remains a core challenge in unifying visual un...
[09.10.2025 16:15] ********************************************************************************
[09.10.2025 16:15] Abstract 1. Cache-to-Cache (C2C) enables direct semantic communication between LLMs using neural network projections, improving accuracy and reducing latency compared to text-based communication.  					AI-generated summary 				 Multi-LLM systems harness the complementary strengths of diverse Large Language Mode...
[09.10.2025 16:15] ********************************************************************************
[09.10.2025 16:15] Abstract 2. Lumina-DiMOO, an open-source foundational model, uses fully discrete diffusion modeling for efficient multi-modal generation and understanding, outperforming existing models.  					AI-generated summary 				 We introduce Lumina-DiMOO, an open-source foundational model for seamless multi-modal generat...
[09.10.2025 16:15] ********************************************************************************
[09.10.2025 16:15] Abstract 3. SHANKS, a general inference framework, enables spoken language models to generate unspoken reasoning while listening to user input, enhancing real-time interaction and task completion.  					AI-generated summary 				 Current large language models (LLMs) and spoken language models (SLMs) begin thinki...
[09.10.2025 16:15] ********************************************************************************
[09.10.2025 16:15] Abstract 4. RLinf-VLA is a unified framework for scalable reinforcement learning training of vision-language-action models, offering improved performance and generalization compared to supervised fine-tuning.  					AI-generated summary 				 Recent progress in vision and language foundation models has significan...
[09.10.2025 16:15] ********************************************************************************
[09.10.2025 16:15] Abstract 5. MATRIX-11K dataset and MATRIX regularization enhance interaction fidelity and semantic alignment in video DiTs by aligning attention with multi-instance mask tracks.  					AI-generated summary 				 Video DiTs have advanced video generation, yet they still struggle to model multi-instance or subject-...
[09.10.2025 16:15] ********************************************************************************
[09.10.2025 16:15] Abstract 6. Vibe Checker evaluates LLMs by combining functional correctness and instruction following to better align with human coding preferences.  					AI-generated summary 				 Large Language Models (LLMs) have catalyzed vibe coding, where users leverage LLMs to generate and iteratively refine code through ...
[09.10.2025 16:15] ********************************************************************************
[09.10.2025 16:15] Abstract 7. MATPO, a reinforcement learning method, optimizes tool-integrated multi-agent roles within a single LLM, improving performance and robustness over single-agent systems.  					AI-generated summary 				 Large language models (LLMs) increasingly rely on multi-turn tool-integrated planning for knowledge...
[09.10.2025 16:15] ********************************************************************************
[09.10.2025 16:15] Abstract 8. CALM framework uses expert interventions to refine LRM reasoning for optimization tasks, achieving high accuracy with fewer modifications compared to traditional methods.  					AI-generated summary 				 Large Reasoning Models (LRMs) have demonstrated strong capabilities in complex multi-step reasoni...
[09.10.2025 16:15] ********************************************************************************
[09.10.2025 16:15] Abstract 9. Low-precision training of transformer models with flash attention suffers from catastrophic loss explosions due to low-rank representations and biased rounding errors, which are addressed by a minimal modification to the flash attention mechanism.  					AI-generated summary 				 The pursuit of compu...
[09.10.2025 16:15] ********************************************************************************
[09.10.2025 16:15] Abstract 10. OBS-Diff is a novel one-shot pruning framework that compresses large-scale text-to-image diffusion models with minimal quality loss and significant inference acceleration.  					AI-generated summary 				 Large-scale text-to-image diffusion models, while powerful, suffer from prohibitive computationa...
[09.10.2025 16:15] ********************************************************************************
[09.10.2025 16:15] Abstract 11. A language-mixed chain-of-thought reasoning approach improves performance in Korean-specific tasks by switching between English and Korean, achieving state-of-the-art results across various benchmarks.  					AI-generated summary 				 Recent frontier models employ long chain-of-thought reasoning to e...
[09.10.2025 16:15] ********************************************************************************
[09.10.2025 16:15] Abstract 12. A memory framework combining short-term and long-term memory in neural networks improves long-sequence modeling efficiency and performance.  					AI-generated summary 				 Long-sequence modeling faces a fundamental trade-off between the efficiency of compressive fixed-size memory in RNN-like models ...
[09.10.2025 16:15] ********************************************************************************
[09.10.2025 16:15] Abstract 13. Native Hybrid Attention (NHA) combines linear and full attention mechanisms to maintain long-term context while improving efficiency, outperforming Transformers in recall-intensive tasks and offering efficiency gains in pretrained LLMs.  					AI-generated summary 				 Transformers excel at sequence ...
[09.10.2025 16:15] ********************************************************************************
[09.10.2025 16:15] Abstract 14. Context Denoising Training (CDT) improves long-context models' performance by mitigating contextual noise and enhancing attention on critical tokens.  					AI-generated summary 				 Long-context models (LCMs) have demonstrated great potential in processing long sequences, facilitating many real-worl...
[09.10.2025 16:15] ********************************************************************************
[09.10.2025 16:15] Abstract 15. VTC-Bench is introduced to provide a fair evaluation framework for visual token compression by incorporating a data filtering mechanism to denoise existing benchmarks.  					AI-generated summary 				 Recent endeavors to accelerate inference in Multimodal Large Language Models (MLLMs) have primarily ...
[09.10.2025 16:15] ********************************************************************************
[09.10.2025 16:15] Abstract 16. Markovian Thinking, implemented in Delethink, enables efficient and scalable reinforcement learning for long-chain-of-thought reasoning in LLMs by decoupling thinking length from context size, resulting in linear compute and constant memory usage.  					AI-generated summary 				 Reinforcement learni...
[09.10.2025 16:15] ********************************************************************************
[09.10.2025 16:15] Abstract 17. An unsupervised method learns a compact state representation using a lightweight encoder and Diffusion Transformer decoder, improving robotic performance and enabling latent action decoding from static images.  					AI-generated summary 				 A fundamental challenge in embodied intelligence is develo...
[09.10.2025 16:15] ********************************************************************************
[09.10.2025 16:15] Abstract 18. Research investigates the aging of factuality benchmarks and its impact on evaluating the factuality of large language models, revealing significant unreliability due to outdated samples.  					AI-generated summary 				 The rapid evolution of large language models (LLMs) and the real world has outpa...
[09.10.2025 16:15] ********************************************************************************
[09.10.2025 16:15] Abstract 19. PaDT, a unified paradigm for multimodal large language models, directly generates both textual and visual outputs, achieving state-of-the-art performance in visual perception tasks.  					AI-generated summary 				 Multimodal large language models (MLLMs) have advanced rapidly in recent years. Howeve...
[09.10.2025 16:15] ********************************************************************************
[09.10.2025 16:15] Abstract 20. TTRV enhances vision language understanding through test-time reinforcement learning, improving performance on object recognition and VQA without labeled data.  					AI-generated summary 				 Existing methods for extracting reward signals in Reinforcement Learning typically rely on labeled data and ...
[09.10.2025 16:15] ********************************************************************************
[09.10.2025 16:15] Abstract 21. The African Languages Lab addresses the underserved status of African languages in NLP by creating a large dataset and demonstrating improved model performance through fine-tuning.  					AI-generated summary 				 Despite representing nearly one-third of the world's languages, African languages remai...
[09.10.2025 16:15] ********************************************************************************
[09.10.2025 16:15] Abstract 22. WristWorld is a 4D world model that generates wrist-view videos from anchor views, improving video generation consistency and VLA performance.  					AI-generated summary 				 Wrist-view observations are crucial for VLA models as they capture fine-grained hand-object interactions that directly enhanc...
[09.10.2025 16:15] ********************************************************************************
[09.10.2025 16:15] Abstract 23. A novel Granular-GRPO framework enhances reinforcement learning in diffusion and flow models by improving reward assessment and reducing bias in denoising.  					AI-generated summary 				 The integration of online reinforcement learning (RL) into diffusion and flow models has recently emerged as a p...
[09.10.2025 16:15] ********************************************************************************
[09.10.2025 16:15] Abstract 24. MLE-Smith automates the creation of high-quality, diverse MLE tasks from raw datasets using a multi-agent pipeline, improving scalability and maintaining task quality.  					AI-generated summary 				 While Language Models (LMs) have made significant progress in automating machine learning engineerin...
[09.10.2025 16:15] ********************************************************************************
[09.10.2025 16:15] Abstract 25. Step-level uniformity in information density, measured using entropy-based metrics, improves reasoning accuracy in large language models across various benchmarks.  					AI-generated summary 				 The Uniform Information Density (UID) hypothesis suggests that effective communication maintains a stabl...
[09.10.2025 16:15] ********************************************************************************
[09.10.2025 16:15] Abstract 26. A novel framework for real-time event boundary detection in streaming videos uses prediction and error measurement to identify subtle event changes.  					AI-generated summary 				 Generic Event Boundary Detection (GEBD) aims to interpret long-form videos through the lens of human perception. Howeve...
[09.10.2025 16:15] ********************************************************************************
[09.10.2025 16:15] Abstract 27. A survey of text-to-video generative models from GANs and VAEs to hybrid Diffusion-Transformer architectures, detailing their development, limitations, and future directions.  					AI-generated summary 				 Text-to-video (T2V) generation technology holds potential to transform multiple domains such ...
[09.10.2025 16:15] ********************************************************************************
[09.10.2025 16:15] Abstract 28. AlphaApollo, a self-evolving reasoning system, enhances foundation model performance through tool integration and iterative refinement, achieving significant improvements in accuracy and pass rates.  					AI-generated summary 				 We present AlphaApollo, a self-evolving agentic reasoning system that...
[09.10.2025 16:15] ********************************************************************************
[09.10.2025 16:15] Abstract 29. U-Bench is a comprehensive benchmark evaluating 100 U-Net variants across 28 datasets and 10 imaging modalities, focusing on statistical robustness, zero-shot generalization, and computational efficiency.  					AI-generated summary 				 Over the past decade, U-Net has been the dominant architecture ...
[09.10.2025 16:15] ********************************************************************************
[09.10.2025 16:15] Abstract 30. This survey analyzes the current state of code-switching aware large language models, highlighting advancements and challenges in multilingual NLP.  					AI-generated summary 				 Code-switching (CSW), the alternation of languages and scripts within a single utterance, remains a fundamental challeng...
[09.10.2025 16:15] ********************************************************************************
[09.10.2025 16:15] Abstract 31. Heptapod, an image autoregressive model using causal attention and next 2D distribution prediction, achieves superior performance on ImageNet generation by combining sequential modeling with holistic self-supervised learning.  					AI-generated summary 				 We introduce Heptapod, an image autoregres...
[09.10.2025 16:15] ********************************************************************************
[09.10.2025 16:15] Abstract 32. NorMuon, a novel optimizer combining orthogonalization with neuron-level adaptive learning rates, enhances training efficiency and balances parameter utilization in large language models.  					AI-generated summary 				 The choice of optimizer significantly impacts the training efficiency and comput...
[09.10.2025 16:15] ********************************************************************************
[09.10.2025 16:15] Abstract 33. DeepTravel is an end-to-end reinforcement learning framework for autonomous travel planning that uses a hierarchical reward system and reply-augmented learning to improve performance over existing models.  					AI-generated summary 				 Travel planning (TP) agent has recently worked as an emerging b...
[09.10.2025 16:15] ********************************************************************************
[09.10.2025 16:15] Abstract 34. A novel method using Discrete Distribution Discrepancy-aware Quantization Error (D$^3$QE) detects images generated by visual autoregressive models by analyzing codebook frequency statistics and quantization errors.  					AI-generated summary 				 The emergence of visual autoregressive (AR) models ha...
[09.10.2025 16:15] ********************************************************************************
[09.10.2025 16:15] Abstract 35. PuzzlePlex benchmark assesses reasoning and planning capabilities of foundation models through diverse puzzles, providing metrics and insights into their performance and scalability.  					AI-generated summary 				 This work investigates the reasoning and planning capabilities of foundation models a...
[09.10.2025 16:15] ********************************************************************************
[09.10.2025 16:15] Abstract 36. FinLFQA evaluates LLMs' ability to provide reliable and nuanced attributions in long-form financial question answering through human and automatic assessments.  					AI-generated summary 				 Large Language Models (LLMs) frequently hallucinate to long-form questions, producing plausible yet factuall...
[09.10.2025 16:15] ********************************************************************************
[09.10.2025 16:15] Abstract 37. A new training paradigm, Glocal Information Bottleneck, improves time series imputation by aligning latent representations to retain global structure and local details under high missingness.  					AI-generated summary 				 Time Series Imputation (TSI), which aims to recover missing values in tempor...
[09.10.2025 16:15] ********************************************************************************
[09.10.2025 16:15] Abstract 38. The choice of delimiter in formatting in-context examples significantly impacts the performance of large language models across different families and tasks.  					AI-generated summary 				 Common Large Language model (LLM) evaluations rely on demonstration examples to steer models' responses to the...
[09.10.2025 16:15] Read previous papers.
[09.10.2025 16:15] Generating reviews via LLM API.
[09.10.2025 16:15] Using data from previous issue: {"categories": ["#games", "#multimodal", "#optimization", "#cv", "#architecture", "#open_source"], "emoji": "üñºÔ∏è", "ru": {"title": "MingTok: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ –≤–∏–∑—É–∞–ª—å–Ω–æ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏", "desc": "MingTok ‚Äî —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≤–∏–∑—É–∞–ª—å–Ω–æ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–µ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –¥–ª—è –æ–±—ä
[09.10.2025 16:15] Using data from previous issue: {"categories": ["#architecture", "#training", "#multimodal", "#optimization", "#agi"], "emoji": "üîÑ", "ru": {"title": "–û–±—â–µ–Ω–∏–µ LLM –±–µ–∑ —Å–ª–æ–≤: –ø—Ä—è–º–∞—è –ø–µ—Ä–µ–¥–∞—á–∞ —Å–µ–º–∞–Ω—Ç–∏–∫–∏ —á–µ—Ä–µ–∑ –∫—ç—à", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ Cache-to-Cache (C2C) –¥–ª—è –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏ –º–µ–∂–¥—É –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ LLM, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥
[09.10.2025 16:15] Using data from previous issue: {"categories": ["#open_source", "#multimodal", "#benchmark", "#diffusion", "#architecture"], "emoji": "üé≠", "ru": {"title": "–î–∏—Å–∫—Ä–µ—Ç–Ω–∞—è –¥–∏—Ñ—Ñ—É–∑–∏—è –¥–ª—è —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–∏", "desc": "Lumina-DiMOO - —ç—Ç–æ –æ—Ç–∫—Ä—ã—Ç–∞—è foundational –º–æ–¥–µ–ª—å –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞. –í –æ—Ç–ª–∏—á–∏–µ
[09.10.2025 16:15] Using data from previous issue: {"categories": ["#multimodal", "#reasoning", "#inference", "#long_context"], "emoji": "üéß", "ru": {"title": "–î—É–º–∞–π –ø–æ–∫–∞ —Å–ª—É—à–∞–µ—à—å: —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏ –¥–ª—è —Ä–∞–∑–≥–æ–≤–æ—Ä–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω SHANKS ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è spoken language models (SLM), –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª—è–º –≥–µ–Ω–µ—Ä
[09.10.2025 16:15] Using data from previous issue: {"categories": ["#agents", "#multimodal", "#training", "#reasoning", "#optimization", "#rl", "#robotics"], "emoji": "ü§ñ", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ —Ä–æ–±–æ—Ç–æ–≤ —á–µ—Ä–µ–∑ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ: RL –ø–æ–±–µ–∂–¥–∞–µ—Ç –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π supervised learning", "desc": "RLinf-VLA ‚Äî —ç—Ç–æ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –º–∞—Å—à—Ç–∞–±–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
[09.10.2025 16:15] Using data from previous issue: {"categories": ["#dataset", "#hallucinations", "#benchmark", "#video", "#interpretability"], "emoji": "üé≠", "ru": {"title": "–£–ª—É—á—à–µ–Ω–∏–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π –≤ –≤–∏–¥–µ–æ —á–µ—Ä–µ–∑ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –≤–Ω–∏–º–∞–Ω–∏—è –ø–æ –º–∞—Å–∫–∞–º –æ–±—ä–µ–∫—Ç–æ–≤", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Å–æ–∑–¥–∞–ª–∏ –¥–∞—Ç–∞—Å–µ—Ç MATRIX-11K —Å –≤–∏–¥–µ–æ, —Å–æ–¥–µ—Ä–∂–∞—â–∏–º–∏ –æ–ø–∏—Å–∞–Ω–∏—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π
[09.10.2025 16:15] Using data from previous issue: {"categories": ["#interpretability", "#alignment", "#training", "#benchmark", "#plp"], "emoji": "‚ú®", "ru": {"title": "Vibe Check: –∫–æ–≥–¥–∞ –∫–æ–¥ –¥–æ–ª–∂–µ–Ω –Ω–µ —Ç–æ–ª—å–∫–æ —Ä–∞–±–æ—Ç–∞—Ç—å, –Ω–æ –∏ –Ω—Ä–∞–≤–∏—Ç—å—Å—è", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ Vibe Checker ‚Äî –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ—Ü–µ–Ω–∫–µ LLM –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞, –∫–æ—Ç–æ—Ä—ã–π —É—á–∏—Ç—ã–≤–∞–µ—Ç –Ω–µ
[09.10.2025 16:15] Using data from previous issue: {"categories": ["#training", "#reasoning", "#optimization", "#agents", "#rl"], "emoji": "üé≠", "ru": {"title": "–û–¥–∏–Ω LLM –≤ —Ä–æ–ª–∏ —Ü–µ–ª–æ–π –∫–æ–º–∞–Ω–¥—ã –∞–≥–µ–Ω—Ç–æ–≤", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ MATPO ‚Äî –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º –≤–Ω—É—Ç—Ä–∏ –æ–¥–Ω–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏. –¢—Ä–∞–¥–∏—Ü–∏–æ
[09.10.2025 16:15] Using data from previous issue: {"categories": ["#benchmark", "#dataset", "#optimization", "#training", "#reasoning", "#rl"], "emoji": "üéØ", "ru": {"title": "–¢–æ—á–µ—á–Ω—ã–µ –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏ –≤–º–µ—Å—Ç–æ —Ç–æ—Ç–∞–ª—å–Ω–æ–≥–æ —Ñ–∞–π–Ω-—Ç—é–Ω–∏–Ω–≥–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç CALM ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –±–æ–ª—å—à–∏—Ö reasoning-–º–æ–¥–µ–ª–µ–π (LRM) –∫ –∑–∞–¥–∞—á–∞–º –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –º–æ–¥–µ
[09.10.2025 16:15] Using data from previous issue: {"categories": ["#architecture", "#training", "#optimization"], "emoji": "üí•", "ru": {"title": "–£–∫—Ä–æ—â–µ–Ω–∏–µ –≤–∑—Ä—ã–≤–æ–≤: —Å—Ç–∞–±–∏–ª—å–Ω–∞—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ —Å –Ω–∏–∑–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –≤—ã—è—Å–Ω–∏–ª–∏, –ø–æ—á–µ–º—É –æ–±—É—á–µ–Ω–∏–µ transformer-–º–æ–¥–µ–ª–µ–π —Å flash attention –≤ –Ω–∏–∑–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –∫–∞—Ç–∞—Å—Ç—Ä–æ—Ñ–∏—á–µ—Å
[09.10.2025 16:15] Using data from previous issue: {"categories": ["#optimization", "#training", "#inference", "#diffusion", "#architecture"], "emoji": "‚úÇÔ∏è", "ru": {"title": "–£–º–Ω–æ–µ —Å–∂–∞—Ç–∏–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∑–∞ –æ–¥–∏–Ω –ø—Ä–æ—Ö–æ–¥", "desc": "OBS-Diff ‚Äî —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–¥–Ω–æ—à–∞–≥–æ–≤–æ–π –æ–±—Ä–µ–∑–∫–∏ (pruning) –¥–ª—è —Å–∂–∞—Ç–∏—è –±–æ–ª—å—à–∏—Ö —Ç–µ–∫—Å—Ç-–≤-–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
[09.10.2025 16:15] Using data from previous issue: {"categories": ["#open_source", "#reasoning", "#low_resource", "#benchmark", "#multilingual", "#dataset", "#long_context", "#training", "#data"], "emoji": "üá∞üá∑", "ru": {"title": "–°–º–µ—à–∞–Ω–Ω—ã–π —è–∑—ã–∫–æ–≤–æ–π reasoning: –∞–Ω–≥–ª–∏–π—Å–∫–∏–π –∫–∞–∫ —è–∫–æ—Ä—å –¥–ª—è —É—Å–∏–ª–µ–Ω–∏—è –∫–æ—Ä–µ–π—Å–∫–∏—Ö LLM", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –º–µ—Ç–æ–¥ La
[09.10.2025 16:15] Using data from previous issue: {"categories": ["#architecture", "#training", "#benchmark", "#optimization", "#long_context"], "emoji": "üß†", "ru": {"title": "–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –≥–∏–ø–ø–æ–∫–∞–º–ø –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –ø–∞–º—è—Ç–∏ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –ø–∞–º—è—Ç–∏ –¥–ª—è –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π, –≤–¥–æ—Ö–Ω–æ–≤–ª—ë–Ω–Ω—É—é –º–æ–¥–µ–ª—å—é –º–Ω–æ–≥–æ–∫–æ–º–ø–æ–Ω–µ–Ω—Ç–Ω–æ–π –ø–∞–º—è—Ç
[09.10.2025 16:15] Using data from previous issue: {"categories": ["#reasoning", "#training", "#optimization", "#architecture", "#long_context"], "emoji": "üîÄ", "ru": {"title": "–ì–∏–±—Ä–∏–¥–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ª–∏–Ω–µ–π–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Å —Ç–æ—á–Ω–æ—Å—Ç—å—é Transformer", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Native Hybrid Attention (NHA) ‚Äî –Ω–æ–≤—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É, –∫–æ—Ç–æ—Ä–∞—è –æ–±—ä–µ–¥–∏–Ω—è–µ
[09.10.2025 16:15] Using data from previous issue: {"categories": ["#training", "#long_context", "#optimization"], "emoji": "üéØ", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ —Å –æ—á–∏—Å—Ç–∫–æ–π –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: —Ñ–æ–∫—É—Å –Ω–∞ –≤–∞–∂–Ω–æ–º", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥ Context Denoising Training (CDT) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞–±–æ—Ç—ã –º–æ–¥–µ–ª–µ–π —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º. –ü—Ä–æ–±–ª–µ–º–∞ –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —Ç–æ–º, —á—Ç–æ 
[09.10.2025 16:15] Using data from previous issue: {"categories": ["#optimization", "#benchmark", "#dataset"], "emoji": "üî¨", "ru": {"title": "VTC-Bench: —á–µ—Å—Ç–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —Å–∂–∞—Ç–∏—è –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö LLM", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç VTC-Bench ‚Äî –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —á–µ—Å—Ç–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –º–µ—Ç–æ–¥–æ–≤ —Å–∂–∞—Ç–∏—è –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à
[09.10.2025 16:15] Using data from previous issue: {"categories": ["#training", "#reasoning", "#rlhf", "#long_context", "#rl", "#optimization"], "emoji": "üß©", "ru": {"title": "–ú–∞—Ä–∫–æ–≤—Å–∫–æ–µ –º—ã—à–ª–µ–Ω–∏–µ: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ –¥–ª–∏–Ω–Ω—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –±–µ–∑ –∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω—ã—Ö –∑–∞—Ç—Ä–∞—Ç", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Markovian Thinking –∏ —Å–∏—Å—Ç–µ–º—É Delethink ‚Äî –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é —è–∑
[09.10.2025 16:15] Using data from previous issue: {"categories": ["#interpretability", "#training", "#agents", "#optimization", "#robotics", "#diffusion"], "emoji": "ü§ñ", "ru": {"title": "–î–≤–∞ —Ç–æ–∫–µ–Ω–∞ –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ä–æ–±–æ—Ç–æ–º: –∫–æ–º–ø–∞–∫—Ç–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –∏ –¥–µ–π—Å—Ç–≤–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –º–µ—Ç–æ–¥ StaMo –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∫–æ–º–ø–∞–∫—Ç–Ω–æ–≥–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Å–æ
[09.10.2025 16:15] Using data from previous issue: {"categories": ["#interpretability", "#hallucinations", "#benchmark"], "emoji": "‚è≥", "ru": {"title": "–ö–æ–≥–¥–∞ –±–µ–Ω—á–º–∞—Ä–∫–∏ —Å—Ç–∞—Ä–µ—é—Ç: –ø—Ä–æ–±–ª–µ–º–∞ —É—Å—Ç–∞—Ä–µ–≤—à–∏—Ö —Ç–µ—Å—Ç–æ–≤ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ñ–∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç–∏ LLM", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ –±–µ–Ω—á–º–∞—Ä–∫–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ñ–∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç–∏ LLM —É—Å—Ç–∞—Ä–µ–≤–∞—é—Ç —Å–æ –≤—Ä–µ–º–µ–Ω–µ–º, —á—Ç–æ
[09.10.2025 16:15] Using data from previous issue: {"categories": ["#games", "#multimodal", "#training", "#cv", "#optimization", "#open_source"], "emoji": "üéØ", "ru": {"title": "–ü—Ä—è–º–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —á–µ—Ä–µ–∑ —Ç–æ–∫–µ–Ω—ã-–ø–∞—Ç—á–∏", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω PaDT ‚Äî –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö LLM, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç –Ω–∞–ø—Ä—è–º—É—é –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç
[09.10.2025 16:15] Using data from previous issue: {"categories": ["#optimization", "#rl", "#multimodal", "#rlhf", "#cv", "#transfer_learning"], "emoji": "üéØ", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –Ω–∞ –ª–µ—Ç—É: –º–æ–¥–µ–ª–∏ —É—á–∞—Ç—Å—è –ø—Ä—è–º–æ –≤–æ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –º–µ—Ç–æ–¥ TTRV, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç –ø–æ–Ω–∏–º–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä
[09.10.2025 16:15] Using data from previous issue: {"categories": ["#dataset", "#training", "#data", "#multilingual", "#low_resource"], "emoji": "üåç", "ru": {"title": "–ê—Ñ—Ä–∏–∫–∞–Ω—Å–∫–∏–µ —è–∑—ã–∫–∏ –≤—ã—Ö–æ–¥—è—Ç –∏–∑ —Ç–µ–Ω–∏: –º–∞—Å—à—Ç–∞–±–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è NLP", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Å–æ–∑–¥–∞–ª–∏ African Languages Lab –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º—ã –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–π –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ—Å—Ç–∏ –∞—Ñ—Ä–∏–∫–∞–Ω—Å–∫–∏—Ö —è–∑—ã
[09.10.2025 16:15] Using data from previous issue: {"categories": ["#optimization", "#synthetic", "#cv", "#video"], "emoji": "ü§ñ", "ru": {"title": "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∏–¥–µ–æ —Å –∑–∞–ø—è—Å—Ç—å—è –∏–∑ –æ–±—ã—á–Ω—ã—Ö –∫–∞–º–µ—Ä –¥–ª—è —Ä–æ–±–æ—Ç–æ–≤", "desc": "WristWorld ‚Äî —ç—Ç–æ 4D –º–æ–¥–µ–ª—å –º–∏—Ä–∞, –∫–æ—Ç–æ—Ä–∞—è –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –≤–∏–¥–µ–æ —Å —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è –∑–∞–ø—è—Å—Ç—å—è —Ä–æ–±–æ—Ç–∞, –∏—Å–ø–æ–ª—å–∑—É—è —Ç–æ–ª—å–∫–æ –∑–∞–ø–∏—Å–∏ —Å –æ–±—ã—á–Ω—ã—Ö —Å—Ç–∞—Ü–∏–æ–Ω–∞—Ä–Ω—ã—Ö 
[09.10.2025 16:15] Using data from previous issue: {"categories": ["#games", "#alignment", "#rlhf", "#diffusion", "#optimization", "#training", "#rl"], "emoji": "üéØ", "ru": {"title": "–¢–æ—á–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –º—É–ª—å—Ç–∏–º–∞—Å—à—Ç–∞–±–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ Granular-GRPO –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏
[09.10.2025 16:15] Using data from previous issue: {"categories": ["#survey", "#optimization", "#dataset", "#benchmark", "#data", "#agents"], "emoji": "üè≠", "ru": {"title": "–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Ñ–∞–±—Ä–∏–∫–∞ ML-–∑–∞–¥–∞—á –∏–∑ —Å—ã—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç MLE-Smith ‚Äî –ø–æ–ª–Ω–æ—Å—Ç—å—é –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º—É–ª—å—Ç–∏-–∞–≥–µ–Ω—Ç–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á –ø
[09.10.2025 16:15] Using data from previous issue: {"categories": ["#reasoning", "#benchmark", "#training"], "emoji": "üìä", "ru": {"title": "–†–∞–≤–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ ‚Äî –∫–ª—é—á –∫ —Ç–æ—á–Ω–æ—Å—Ç–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π LLM", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–∏–º–µ–Ω–∏–ª–∏ –≥–∏–ø–æ—Ç–µ–∑—É —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–π –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ (UID) –∫ –∞–Ω–∞–ª–∏–∑—É —Ü–µ–ø–æ—á–µ–∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –û–Ω–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª
[09.10.2025 16:15] Using data from previous issue: {"categories": ["#interpretability", "#video", "#benchmark", "#long_context"], "emoji": "üé¨", "ru": {"title": "–†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –≥—Ä–∞–Ω–∏—Ü —Å–æ–±—ã—Ç–∏–π –≤ –≤–∏–¥–µ–æ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏ —á–µ—Ä–µ–∑ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∏ –æ—à–∏–±–∫—É", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –∑–∞–¥–∞—á—É Online Generic Event Boundary Detection (On-GEBD) ‚Äî –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏
[09.10.2025 16:15] Using data from previous issue: {"categories": ["#architecture", "#training", "#dataset", "#benchmark", "#video", "#diffusion", "#survey"], "emoji": "üé¨", "ru": {"title": "–û—Ç GAN –∫ Diffusion: —ç–≤–æ–ª—é—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –∏–∑ —Ç–µ–∫—Å—Ç–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –æ–±–∑–æ—Ä –º–æ–¥–µ–ª–µ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –∏–∑ —Ç–µ–∫—Å—Ç–∞ (text-to-video), –ø—Ä–æ—Å–ª–µ–∂–∏–≤–∞—è –∏—Ö —Ä–∞–∑
[09.10.2025 16:15] Using data from previous issue: {"categories": ["#optimization", "#reasoning", "#training", "#agents"], "emoji": "üîÑ", "ru": {"title": "–°–∞–º–æ—ç–≤–æ–ª—é—Ü–∏—è —á–µ—Ä–µ–∑ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã: AlphaApollo –ø–æ–¥–Ω–∏–º–∞–µ—Ç –ø–æ—Ç–æ–ª–æ–∫ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π LLM", "desc": "AlphaApollo ‚Äî —ç—Ç–æ —Å–∞–º–æ–æ–±—É—á–∞—é—â–∞—è—Å—è —Å–∏—Å—Ç–µ–º–∞ –∞–≥–µ–Ω—Ç–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä–∞—è —Ä–µ—à–∞–µ—Ç –¥–≤–µ –∫–ª—é—á–µ–≤—ã–µ –ø—Ä–æ–±–ª–µ–º—ã found
[09.10.2025 16:15] Using data from previous issue: {"categories": ["#cv", "#open_source", "#dataset", "#benchmark", "#optimization", "#survey"], "emoji": "üè•", "ru": {"title": "U-Bench: –≤—Å–µ—Å—Ç–æ—Ä–æ–Ω–Ω–∏–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è U-Net –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä –≤ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏", "desc": "U-Bench –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –ø–µ—Ä–≤—ã–π –º–∞—Å—à—Ç–∞–±–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ—Ü–µ–Ω–∫–∏ U-Net
[09.10.2025 16:15] Using data from previous issue: {"categories": ["#training", "#multilingual", "#architecture", "#benchmark", "#survey", "#dataset", "#low_resource"], "emoji": "üîÄ", "ru": {"title": "–ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤: –≤—ã–∑–æ–≤ –¥–ª—è —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö LLM", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –æ–±–∑–æ—Ä —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM), —Å–ø–æ—Å–æ–±
[09.10.2025 16:15] Using data from previous issue: {"categories": ["#architecture", "#optimization", "#games", "#benchmark", "#cv"], "emoji": "üêô", "ru": {"title": "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ 2D —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π –¥–ª—è –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "Heptapod ‚Äî —ç—Ç–æ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä–∞—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç causal attention –∏
[09.10.2025 16:15] Using data from previous issue: {"categories": ["#architecture", "#training", "#optimization"], "emoji": "‚öñÔ∏è", "ru": {"title": "–ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –Ω–µ–π—Ä–æ–Ω–æ–≤ —á–µ—Ä–µ–∑ –æ—Ä—Ç–æ–≥–æ–Ω–∞–ª–∏–∑–∞—Ü–∏—é –∏ –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–µ —à–∞–≥–∏ –æ–±—É—á–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç NorMuon ‚Äî –Ω–æ–≤—ã–π –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–π –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –æ—Ä—Ç–æ–≥–æ–Ω–∞–ª–∏–∑–∞—Ü–∏—é 
[09.10.2025 16:15] Using data from previous issue: {"categories": ["#agents", "#games", "#reasoning", "#training", "#optimization", "#rl"], "emoji": "üó∫Ô∏è", "ru": {"title": "–ê–≤—Ç–æ–Ω–æ–º–Ω—ã–π AI-–∞–≥–µ–Ω—Ç –¥–ª—è –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –ø—É—Ç–µ—à–µ—Å—Ç–≤–∏–π –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç frontier –º–æ–¥–µ–ª–∏", "desc": "DeepTravel ‚Äî —ç—Ç–æ end-to-end —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ reinforcement learning –¥–ª—è –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –ø–ª–∞–Ω
[09.10.2025 16:15] Using data from previous issue: {"categories": ["#security", "#synthetic", "#cv", "#dataset", "#inference"], "emoji": "üîç", "ru": {"title": "–ü–æ–∏—Å–∫ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —á–µ—Ä–µ–∑ –∞–Ω–∞–ª–∏–∑ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ D¬≥QE –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤–∏–∑—É–∞–ª—å–Ω—ã–º–∏ autoregressive –º–æ–¥–µ–ª—è–º–∏. –ú–µ—Ç–æ–¥ –∞–Ω–∞–ª–∏
[09.10.2025 16:15] Querying the API.
[09.10.2025 16:15] Claude request. Model: claude-sonnet-4-5-20250929. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

PuzzlePlex benchmark assesses reasoning and planning capabilities of foundation models through diverse puzzles, providing metrics and insights into their performance and scalability.  					AI-generated summary 				 This work investigates the reasoning and planning capabilities of foundation models and their scalability in complex, dynamic environments. We introduce PuzzlePlex, a benchmark designed to assess these capabilities through a diverse set of puzzles. PuzzlePlex consists of 15 types of puzzles, including deterministic and stochastic games of varying difficulty, as well as single-player and two-player scenarios. The PuzzlePlex framework provides a comprehensive environment for each game, and supports extensibility to generate more challenging instances as foundation models evolve. Additionally, we implement customized game-playing strategies for comparison. Building on this benchmark, we develop fine-grained metrics to measure performance and conduct an in-depth analysis of frontier foundation models across two settings: instruction-based and code-based. Furthermore, we systematically investigate their scaling limits. Our findings show that reasoning models outperform others in instruction-based settings, while code-based execution presents greater challenges but offers a scalable and efficient alternative. PuzzlePlex enables targeted evaluation and guides future improvements in reasoning, planning, and generalization for foundation models.
[09.10.2025 16:15] Response: ```json
{
  "title": "–ì–æ–ª–æ–≤–æ–ª–æ–º–∫–∏ –∫–∞–∫ –∏—Å–ø—ã—Ç–∞–Ω–∏–µ –¥–ª—è AI: –ø—Ä–æ–≤–µ—Ä–∫–∞ –ª–æ–≥–∏–∫–∏ –∏ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è",
  "emoji": "üß©",
  "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ PuzzlePlex ‚Äî –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π foundation models –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é –∏ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—é —á–µ—Ä–µ–∑ —Ä–µ—à–µ–Ω–∏–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –≥–æ–ª–æ–≤–æ–ª–æ–º–æ–∫. –ë–µ–Ω—á–º–∞—Ä–∫ –≤–∫–ª—é—á–∞–µ—Ç 15 —Ç–∏–ø–æ–≤ –∑–∞–¥–∞—á —Ä–∞–∑–ª–∏—á–Ω–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç–∏: –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∏ —Å—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–∏–µ –∏–≥—Ä—ã, –æ–¥–∏–Ω–æ—á–Ω—ã–µ –∏ –¥–≤—É—Ö–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ reasoning models –ª—É—á—à–µ —Å–ø—Ä–∞–≤–ª—è—é—Ç—Å—è —Å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏ –Ω–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–º —è–∑—ã–∫–µ, –≤ —Ç–æ –≤—Ä–µ–º—è –∫–∞–∫ –ø–æ–¥—Ö–æ–¥ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–¥–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –±–æ–ª—å—à—É—é —Å–ª–æ–∂–Ω–æ—Å—Ç—å, –Ω–æ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å. PuzzlePlex –ø–æ–∑–≤–æ–ª—è–µ—Ç –¥–µ—Ç–∞–ª—å–Ω–æ –∏–∑–º–µ—Ä–∏—Ç—å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π –∏ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –∫ –æ–±–æ–±—â–µ–Ω–∏—é –∏ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—é.",
  "emoji": "üß©"
}
```
[09.10.2025 16:15] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"PuzzlePlex benchmark assesses reasoning and planning capabilities of foundation models through diverse puzzles, providing metrics and insights into their performance and scalability.  					AI-generated summary 				 This work investigates the reasoning and planning capabilities of foundation models and their scalability in complex, dynamic environments. We introduce PuzzlePlex, a benchmark designed to assess these capabilities through a diverse set of puzzles. PuzzlePlex consists of 15 types of puzzles, including deterministic and stochastic games of varying difficulty, as well as single-player and two-player scenarios. The PuzzlePlex framework provides a comprehensive environment for each game, and supports extensibility to generate more challenging instances as foundation models evolve. Additionally, we implement customized game-playing strategies for comparison. Building on this benchmark, we develop fine-grained metrics to measure performance and conduct an in-depth analysis of frontier foundation models across two settings: instruction-based and code-based. Furthermore, we systematically investigate their scaling limits. Our findings show that reasoning models outperform others in instruction-based settings, while code-based execution presents greater challenges but offers a scalable and efficient alternative. PuzzlePlex enables targeted evaluation and guides future improvements in reasoning, planning, and generalization for foundation models."

[09.10.2025 16:15] Response: ```python
['BENCHMARK', 'TRAINING']
```
[09.10.2025 16:15] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"PuzzlePlex benchmark assesses reasoning and planning capabilities of foundation models through diverse puzzles, providing metrics and insights into their performance and scalability.  					AI-generated summary 				 This work investigates the reasoning and planning capabilities of foundation models and their scalability in complex, dynamic environments. We introduce PuzzlePlex, a benchmark designed to assess these capabilities through a diverse set of puzzles. PuzzlePlex consists of 15 types of puzzles, including deterministic and stochastic games of varying difficulty, as well as single-player and two-player scenarios. The PuzzlePlex framework provides a comprehensive environment for each game, and supports extensibility to generate more challenging instances as foundation models evolve. Additionally, we implement customized game-playing strategies for comparison. Building on this benchmark, we develop fine-grained metrics to measure performance and conduct an in-depth analysis of frontier foundation models across two settings: instruction-based and code-based. Furthermore, we systematically investigate their scaling limits. Our findings show that reasoning models outperform others in instruction-based settings, while code-based execution presents greater challenges but offers a scalable and efficient alternative. PuzzlePlex enables targeted evaluation and guides future improvements in reasoning, planning, and generalization for foundation models."

[09.10.2025 16:16] Response: ```python
["REASONING", "GAMES"]
```
[09.10.2025 16:16] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The paper introduces PuzzlePlex, a benchmark designed to evaluate the reasoning and planning abilities of foundation models through a variety of puzzles. It includes 15 different types of puzzles, ranging from deterministic to stochastic games, and accommodates both single-player and two-player scenarios. The framework allows for the generation of increasingly complex puzzles as models improve, and it provides metrics for assessing model performance in instruction-based and code-based settings. The results indicate that while reasoning models excel in instruction-based tasks, code-based tasks present more challenges but are scalable and efficient alternatives.","title":"PuzzlePlex: Benchmarking Reasoning and Planning in Foundation Models"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='The paper introduces PuzzlePlex, a benchmark designed to evaluate the reasoning and planning abilities of foundation models through a variety of puzzles. It includes 15 different types of puzzles, ranging from deterministic to stochastic games, and accommodates both single-player and two-player scenarios. The framework allows for the generation of increasingly complex puzzles as models improve, and it provides metrics for assessing model performance in instruction-based and code-based settings. The results indicate that while reasoning models excel in instruction-based tasks, code-based tasks present more challenges but are scalable and efficient alternatives.', title='PuzzlePlex: Benchmarking Reasoning and Planning in Foundation Models'))
[09.10.2025 16:16] Response: ParsedChatCompletionMessage[Article](content='{"desc":"PuzzlePlexÂü∫ÂáÜÊµãËØïËØÑ‰º∞Âü∫Á°ÄÊ®°ÂûãÁöÑÊé®ÁêÜÂíåËßÑÂàíËÉΩÂäõÔºå‰ΩøÁî®Â§öÊ†∑ÂåñÁöÑÈöæÈ¢òÊèê‰æõÊÄßËÉΩÂíåÂèØÊâ©Â±ïÊÄßÁöÑÊåáÊ†áÂíåËßÅËß£„ÄÇËØ•Âü∫ÂáÜÂåÖÂê´15ÁßçÁ±ªÂûãÁöÑÈöæÈ¢òÔºåÂåÖÊã¨Á°ÆÂÆöÊÄßÂíåÈöèÊú∫ÊÄßÊ∏∏ÊàèÔºåÈöæÂ∫¶ÂêÑÂºÇÔºåÈÄÇÁî®‰∫éÂçï‰∫∫ÂíåÂèå‰∫∫Âú∫ÊôØ„ÄÇPuzzlePlexÊ°ÜÊû∂‰∏∫ÊØè‰∏™Ê∏∏ÊàèÊèê‰æõÂÖ®Èù¢ÁöÑÁéØÂ¢ÉÔºåÂπ∂ÊîØÊåÅÁîüÊàêÊõ¥ÂÖ∑ÊåëÊàòÊÄßÁöÑÂÆû‰æãÔºå‰ª•ÈÄÇÂ∫îÂü∫Á°ÄÊ®°ÂûãÁöÑÂèëÂ±ï„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ÂºÄÂèë‰∫ÜÁªÜËá¥ÁöÑÊåáÊ†áÊù•ÊµãÈáèÊÄßËÉΩÔºåÂπ∂ÂØπÂâçÊ≤øÂü∫Á°ÄÊ®°ÂûãÂú®Êåá‰ª§Âü∫Á°ÄÂíå‰ª£Á†ÅÂü∫Á°ÄÁöÑ‰∏§ÁßçËÆæÁΩÆ‰∏ãËøõË°å‰∫ÜÊ∑±ÂÖ•ÂàÜÊûê„ÄÇ","title":"PuzzlePlexÔºöËØÑ‰º∞Êé®ÁêÜ‰∏éËßÑÂàíËÉΩÂäõÁöÑÂü∫ÂáÜ"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='PuzzlePlexÂü∫ÂáÜÊµãËØïËØÑ‰º∞Âü∫Á°ÄÊ®°ÂûãÁöÑÊé®ÁêÜÂíåËßÑÂàíËÉΩÂäõÔºå‰ΩøÁî®Â§öÊ†∑ÂåñÁöÑÈöæÈ¢òÊèê‰æõÊÄßËÉΩÂíåÂèØÊâ©Â±ïÊÄßÁöÑÊåáÊ†áÂíåËßÅËß£„ÄÇËØ•Âü∫ÂáÜÂåÖÂê´15ÁßçÁ±ªÂûãÁöÑÈöæÈ¢òÔºåÂåÖÊã¨Á°ÆÂÆöÊÄßÂíåÈöèÊú∫ÊÄßÊ∏∏ÊàèÔºåÈöæÂ∫¶ÂêÑÂºÇÔºåÈÄÇÁî®‰∫éÂçï‰∫∫ÂíåÂèå‰∫∫Âú∫ÊôØ„ÄÇPuzzlePlexÊ°ÜÊû∂‰∏∫ÊØè‰∏™Ê∏∏ÊàèÊèê‰æõÂÖ®Èù¢ÁöÑÁéØÂ¢ÉÔºåÂπ∂ÊîØÊåÅÁîüÊàêÊõ¥ÂÖ∑ÊåëÊàòÊÄßÁöÑÂÆû‰æãÔºå‰ª•ÈÄÇÂ∫îÂü∫Á°ÄÊ®°ÂûãÁöÑÂèëÂ±ï„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ÂºÄÂèë‰∫ÜÁªÜËá¥ÁöÑÊåáÊ†áÊù•ÊµãÈáèÊÄßËÉΩÔºåÂπ∂ÂØπÂâçÊ≤øÂü∫Á°ÄÊ®°ÂûãÂú®Êåá‰ª§Âü∫Á°ÄÂíå‰ª£Á†ÅÂü∫Á°ÄÁöÑ‰∏§ÁßçËÆæÁΩÆ‰∏ãËøõË°å‰∫ÜÊ∑±ÂÖ•ÂàÜÊûê„ÄÇ', title='PuzzlePlexÔºöËØÑ‰º∞Êé®ÁêÜ‰∏éËßÑÂàíËÉΩÂäõÁöÑÂü∫ÂáÜ'))
[09.10.2025 16:16] Querying the API.
[09.10.2025 16:16] Claude request. Model: claude-sonnet-4-5-20250929. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

FinLFQA evaluates LLMs' ability to provide reliable and nuanced attributions in long-form financial question answering through human and automatic assessments.  					AI-generated summary 				 Large Language Models (LLMs) frequently hallucinate to long-form questions, producing plausible yet factually incorrect answers. A common mitigation strategy is to provide attribution to LLM outputs. However, existing benchmarks primarily focus on simple attribution that retrieves supporting textual evidence as references. We argue that in real-world scenarios such as financial applications, attribution goes beyond reference retrieval. We introduce FinLFQA, a benchmark designed to evaluate the ability of LLMs to generate long-form answers to complex financial questions with reliable and nuanced attributions. FinLFQA evaluates three critical aspects of attribution through human annotations: (1) supporting evidence extracted from financial reports, (2) intermediate numerical reasoning steps, and (3) domain-specific financial knowledge that informs the reasoning process. We further provide an automatic evaluation framework covering both answer quality and attribution quality. Through extensive experiments on eight LLMs across multiple attribution-generation paradigms, we find that fine-grained metrics are important to distinguish model capabilities, that end-to-end generation achieves comparable performance to post-hoc approaches, and that iterative refinement only helps when guided by external feedback.
[09.10.2025 16:16] Response: ```json
{
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç FinLFQA ‚Äî –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ LLM –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ä–∞–∑–≤—ë—Ä–Ω—É—Ç—ã–µ –æ—Ç–≤–µ—Ç—ã –Ω–∞ —Å–ª–æ–∂–Ω—ã–µ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –≤–æ–ø—Ä–æ—Å—ã —Å –Ω–∞–¥—ë–∂–Ω–æ–π –∞—Ç—Ä–∏–±—É—Ü–∏–µ–π –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤. –ü—Ä–æ–±–ª–µ–º–∞ –≤ —Ç–æ–º, —á—Ç–æ LLM —á–∞—Å—Ç–æ –≥–∞–ª–ª—é—Ü–∏–Ω–∏—Ä—É—é—Ç, –≤—ã–¥–∞–≤–∞—è –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–Ω—ã–µ, –Ω–æ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏ –Ω–µ–≤–µ—Ä–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã, –æ—Å–æ–±–µ–Ω–Ω–æ –≤ —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–π —Å—Ñ–µ—Ä–µ. –ë–µ–Ω—á–º–∞—Ä–∫ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç —Ç—Ä–∏ –∞—Å–ø–µ–∫—Ç–∞ –∞—Ç—Ä–∏–±—É—Ü–∏–∏: –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –æ—Ç—á—ë—Ç–æ–≤, –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —à–∞–≥–∏ —á–∏—Å–ª–µ–Ω–Ω—ã—Ö —Ä–∞—Å—á—ë—Ç–æ–≤ –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏—Ö –∑–Ω–∞–Ω–∏–π –∏–∑ —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–π –æ–±–ª–∞—Å—Ç–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ –≤–æ—Å—å–º–∏ LLM –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –¥–µ—Ç–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –≤–∞–∂–Ω—ã –¥–ª—è —Ä–∞–∑–ª–∏—á–µ–Ω–∏—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –º–æ–¥–µ–ª–µ–π, –∞ end-to-end –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç–∞–µ—Ç —Å—Ä–∞–≤–Ω–∏–º–æ —Å post-hoc –ø–æ–¥—Ö–æ–¥–∞–º–∏.",
  "emoji": "üí∞",
  "title": "–ü—Ä–æ–≤–µ—Ä–∫–∞ LLM –Ω–∞ —á–µ—Å—Ç–Ω–æ—Å—Ç—å –≤ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –æ—Ç–≤–µ—Ç–∞—Ö"
}
```
[09.10.2025 16:16] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"FinLFQA evaluates LLMs' ability to provide reliable and nuanced attributions in long-form financial question answering through human and automatic assessments.  					AI-generated summary 				 Large Language Models (LLMs) frequently hallucinate to long-form questions, producing plausible yet factually incorrect answers. A common mitigation strategy is to provide attribution to LLM outputs. However, existing benchmarks primarily focus on simple attribution that retrieves supporting textual evidence as references. We argue that in real-world scenarios such as financial applications, attribution goes beyond reference retrieval. We introduce FinLFQA, a benchmark designed to evaluate the ability of LLMs to generate long-form answers to complex financial questions with reliable and nuanced attributions. FinLFQA evaluates three critical aspects of attribution through human annotations: (1) supporting evidence extracted from financial reports, (2) intermediate numerical reasoning steps, and (3) domain-specific financial knowledge that informs the reasoning process. We further provide an automatic evaluation framework covering both answer quality and attribution quality. Through extensive experiments on eight LLMs across multiple attribution-generation paradigms, we find that fine-grained metrics are important to distinguish model capabilities, that end-to-end generation achieves comparable performance to post-hoc approaches, and that iterative refinement only helps when guided by external feedback."

[09.10.2025 16:16] Response: ```python
['BENCHMARK', 'MULTIMODAL']
```
[09.10.2025 16:16] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"FinLFQA evaluates LLMs' ability to provide reliable and nuanced attributions in long-form financial question answering through human and automatic assessments.  					AI-generated summary 				 Large Language Models (LLMs) frequently hallucinate to long-form questions, producing plausible yet factually incorrect answers. A common mitigation strategy is to provide attribution to LLM outputs. However, existing benchmarks primarily focus on simple attribution that retrieves supporting textual evidence as references. We argue that in real-world scenarios such as financial applications, attribution goes beyond reference retrieval. We introduce FinLFQA, a benchmark designed to evaluate the ability of LLMs to generate long-form answers to complex financial questions with reliable and nuanced attributions. FinLFQA evaluates three critical aspects of attribution through human annotations: (1) supporting evidence extracted from financial reports, (2) intermediate numerical reasoning steps, and (3) domain-specific financial knowledge that informs the reasoning process. We further provide an automatic evaluation framework covering both answer quality and attribution quality. Through extensive experiments on eight LLMs across multiple attribution-generation paradigms, we find that fine-grained metrics are important to distinguish model capabilities, that end-to-end generation achieves comparable performance to post-hoc approaches, and that iterative refinement only helps when guided by external feedback."

[09.10.2025 16:16] Response: ```python
["HALLUCINATIONS", "REASONING", "LONG_CONTEXT"]
```
[09.10.2025 16:16] Response: ParsedChatCompletionMessage[Article](content='{"desc":"FinLFQA is a benchmark that assesses how well Large Language Models (LLMs) can answer complex financial questions while providing reliable attributions. It highlights the issue of LLMs generating plausible but incorrect answers, known as hallucinations, and emphasizes the need for nuanced attribution beyond simple reference retrieval. The evaluation focuses on three key aspects: extracting supporting evidence from financial reports, demonstrating intermediate numerical reasoning, and applying domain-specific financial knowledge. The study shows that fine-grained metrics are crucial for evaluating model performance and that iterative refinement is beneficial when guided by external feedback.","title":"Enhancing Financial Question Answering with Reliable Attributions"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='FinLFQA is a benchmark that assesses how well Large Language Models (LLMs) can answer complex financial questions while providing reliable attributions. It highlights the issue of LLMs generating plausible but incorrect answers, known as hallucinations, and emphasizes the need for nuanced attribution beyond simple reference retrieval. The evaluation focuses on three key aspects: extracting supporting evidence from financial reports, demonstrating intermediate numerical reasoning, and applying domain-specific financial knowledge. The study shows that fine-grained metrics are crucial for evaluating model performance and that iterative refinement is beneficial when guided by external feedback.', title='Enhancing Financial Question Answering with Reliable Attributions'))
[09.10.2025 16:16] Response: ParsedChatCompletionMessage[Article](content='{"desc":"FinLFQAÊòØ‰∏Ä‰∏™ËØÑ‰º∞Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®ÈïøÁØáÈáëËûçÈóÆÁ≠î‰∏≠Êèê‰æõÂèØÈù†ÂíåÁªÜËá¥ÂΩíÂõ†ËÉΩÂäõÁöÑÂü∫ÂáÜ„ÄÇÁé∞ÊúâÁöÑËØÑ‰º∞‰∏ªË¶ÅÂÖ≥Ê≥®ÁÆÄÂçïÁöÑÂºïÁî®Ê£ÄÁ¥¢ÔºåËÄåÊàë‰ª¨ËÆ§‰∏∫Âú®ÈáëËûçÂ∫îÁî®‰∏≠ÔºåÂΩíÂõ†Â∫îË∂ÖË∂äËøô‰∏ÄÁÇπ„ÄÇFinLFQAÈÄöËøá‰∫∫Á±ªÊ≥®ÈáäËØÑ‰º∞‰∏â‰∏™ÂÖ≥ÈîÆÊñπÈù¢Ôºö‰ªéË¥¢Âä°Êä•Âëä‰∏≠ÊèêÂèñÁöÑÊîØÊåÅËØÅÊçÆ„ÄÅ‰∏≠Èó¥Êï∞ÂÄºÊé®ÁêÜÊ≠•È™§Ôºå‰ª•ÂèäÂΩ±ÂìçÊé®ÁêÜËøáÁ®ãÁöÑÈ¢ÜÂüüÁâπÂÆöÈáëËûçÁü•ËØÜ„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ËøòÊèê‰æõ‰∫Ü‰∏Ä‰∏™Ëá™Âä®ËØÑ‰º∞Ê°ÜÊû∂ÔºåÊ∂µÁõñÁ≠îÊ°àË¥®ÈáèÂíåÂΩíÂõ†Ë¥®Èáè„ÄÇ","title":"ËØÑ‰º∞ÈáëËûçÈóÆÁ≠î‰∏≠ÁöÑÂΩíÂõ†ËÉΩÂäõ"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='FinLFQAÊòØ‰∏Ä‰∏™ËØÑ‰º∞Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®ÈïøÁØáÈáëËûçÈóÆÁ≠î‰∏≠Êèê‰æõÂèØÈù†ÂíåÁªÜËá¥ÂΩíÂõ†ËÉΩÂäõÁöÑÂü∫ÂáÜ„ÄÇÁé∞ÊúâÁöÑËØÑ‰º∞‰∏ªË¶ÅÂÖ≥Ê≥®ÁÆÄÂçïÁöÑÂºïÁî®Ê£ÄÁ¥¢ÔºåËÄåÊàë‰ª¨ËÆ§‰∏∫Âú®ÈáëËûçÂ∫îÁî®‰∏≠ÔºåÂΩíÂõ†Â∫îË∂ÖË∂äËøô‰∏ÄÁÇπ„ÄÇFinLFQAÈÄöËøá‰∫∫Á±ªÊ≥®ÈáäËØÑ‰º∞‰∏â‰∏™ÂÖ≥ÈîÆÊñπÈù¢Ôºö‰ªéË¥¢Âä°Êä•Âëä‰∏≠ÊèêÂèñÁöÑÊîØÊåÅËØÅÊçÆ„ÄÅ‰∏≠Èó¥Êï∞ÂÄºÊé®ÁêÜÊ≠•È™§Ôºå‰ª•ÂèäÂΩ±ÂìçÊé®ÁêÜËøáÁ®ãÁöÑÈ¢ÜÂüüÁâπÂÆöÈáëËûçÁü•ËØÜ„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ËøòÊèê‰æõ‰∫Ü‰∏Ä‰∏™Ëá™Âä®ËØÑ‰º∞Ê°ÜÊû∂ÔºåÊ∂µÁõñÁ≠îÊ°àË¥®ÈáèÂíåÂΩíÂõ†Ë¥®Èáè„ÄÇ', title='ËØÑ‰º∞ÈáëËûçÈóÆÁ≠î‰∏≠ÁöÑÂΩíÂõ†ËÉΩÂäõ'))
[09.10.2025 16:16] Using data from previous issue: {"categories": ["#dataset", "#optimization", "#training"], "emoji": "üåç", "ru": {"title": "–ë–∞–ª–∞–Ω—Å –º–µ–∂–¥—É –≥–ª–æ–±–∞–ª—å–Ω—ã–º –∏ –ª–æ–∫–∞–ª—å–Ω—ã–º: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—é –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –ø–∞—Ä–∞–¥–∏–≥–º—É –æ–±—É—á–µ–Ω–∏—è Glocal Information Bottleneck –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –≤
[09.10.2025 16:16] Querying the API.
[09.10.2025 16:16] Claude request. Model: claude-sonnet-4-5-20250929. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

The choice of delimiter in formatting in-context examples significantly impacts the performance of large language models across different families and tasks.  					AI-generated summary 				 Common Large Language model (LLM) evaluations rely on demonstration examples to steer models' responses to the desired style. While the number of examples used has been studied and standardized, the choice of how to format examples is less investigated. In evaluation protocols and real world usage, users face the choice how to separate in-context examples: use a comma? new line? semi-colon? hashtag? etc.? Surprisingly, we find this seemingly minor choice can dramatically alter model response quality. Across leading model families (Llama, Qwen, Gemma), performance on MMLU for example can vary by pm 23% depending on the choice of delimiter. In fact, one can manipulate model rankings to put any model in the lead by only modifying the single character separating examples. We find LLMs' brittleness pervades topics, model families, and doesn't improve with scale. By probing attention head scores, we find that good-performing delimiters steer attention towards key tokens in the input. Finally, we explore methods to improve LLMs' robustness to the choice of delimiter. We find specifying the selected delimiter in the prompt boosts robustness and offer practical recommendations for the best-performing delimiters to select.
[09.10.2025 16:16] Response: ```json
{
  "title": "–û–¥–∏–Ω —Å–∏–º–≤–æ–ª –º–æ–∂–µ—Ç –∏–∑–º–µ–Ω–∏—Ç—å –≤—Å—ë: –∫–∞–∫ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏ –≤–ª–∏—è—é—Ç –Ω–∞ —Ä–∞–±–æ—Ç—É LLM",
  "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –≤—ã–±–æ—Ä —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è –º–µ–∂–¥—É –ø—Ä–∏–º–µ—Ä–∞–º–∏ –≤ –ø—Ä–æ–º–ø—Ç–µ (–∑–∞–ø—è—Ç–∞—è, —Ç–æ—á–∫–∞ —Å –∑–∞–ø—è—Ç–æ–π, –Ω–æ–≤–∞—è —Å—Ç—Ä–æ–∫–∞) –¥—Ä–∞–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤–ª–∏—è–µ—Ç –Ω–∞ –∫–∞—á–µ—Å—Ç–≤–æ –æ—Ç–≤–µ—Ç–æ–≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ù–∞ —Ç–µ—Å—Ç–µ MMLU –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–∂–µ—Ç –º–µ–Ω—è—Ç—å—Å—è –Ω–∞ ¬±23% –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –æ–¥–Ω–æ–≥–æ —Å–∏–º–≤–æ–ª–∞-—Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è, –ø—Ä–∏—á—ë–º –º–æ–∂–Ω–æ –º–∞–Ω–∏–ø—É–ª–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–π—Ç–∏–Ω–≥–æ–º –º–æ–¥–µ–ª–µ–π, –º–µ–Ω—è—è —Ç–æ–ª—å–∫–æ —ç—Ç–æ—Ç —Å–∏–º–≤–æ–ª. –ê–Ω–∞–ª–∏–∑ attention heads –ø–æ–∫–∞–∑–∞–ª, —á—Ç–æ —É—Å–ø–µ—à–Ω—ã–µ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏ –Ω–∞–ø—Ä–∞–≤–ª—è—é—Ç –≤–Ω–∏–º–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –∫–ª—é—á–µ–≤—ã–µ —Ç–æ–∫–µ–Ω—ã –≤–æ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –≤—ã–±–æ—Ä—É —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–π –∏ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —è–≤–Ω–æ–µ —É–∫–∞–∑–∞–Ω–∏–µ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è –≤ –ø—Ä–æ–º–ø—Ç–µ –ø–æ–≤—ã—à–∞–µ—Ç —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –º–æ–¥–µ–ª–∏.",
  "emoji": "üîó",
  "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –≤—ã–±–æ—Ä —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è –º–µ–∂–¥—É –ø—Ä–∏–º–µ—Ä–∞–º–∏ –≤ –ø—Ä–æ–º–ø—Ç–µ (–∑–∞–ø—è—Ç–∞—è, —Ç–æ—á–∫–∞ —Å –∑–∞–ø—è—Ç–æ–π, –Ω–æ–≤–∞—è —Å—Ç—Ä–æ–∫–∞) –¥—Ä–∞–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤–ª–∏—è–µ—Ç –Ω–∞ –∫–∞—á–µ—Å—Ç–≤–æ –æ—Ç–≤–µ—Ç–æ–≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ù–∞ —Ç–µ—Å—Ç–µ MMLU –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–∂–µ—Ç –º–µ–Ω—è—Ç—å—Å—è –Ω–∞ ¬±23% –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –æ–¥–Ω–æ–≥–æ —Å–∏–º–≤–æ–ª–∞-—Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è, –ø—Ä–∏—á—ë–º –º–æ–∂–Ω–æ –º–∞–Ω–∏–ø—É–ª–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–π—Ç–∏–Ω–≥–æ–º –º–æ–¥–µ–ª–µ–π, –º–µ–Ω—è—è —Ç–æ–ª—å–∫–æ —ç—Ç–æ—Ç —Å–∏–º–≤–æ–ª. –ê–Ω–∞–ª–∏–∑ attention heads –ø–æ–∫–∞–∑–∞–ª, —á—Ç–æ —É—Å–ø–µ—à–Ω—ã–µ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏ –Ω–∞–ø—Ä–∞–≤–ª—è—é—Ç –≤–Ω–∏–º–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –∫–ª—é—á–µ–≤—ã–µ —Ç–æ–∫–µ–Ω—ã –≤–æ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –≤—ã–±–æ—Ä—É —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–π –∏ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —è–≤–Ω–æ–µ —É–∫–∞–∑–∞–Ω–∏–µ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è –≤ –ø—Ä–æ–º–ø—Ç–µ –ø–æ–≤—ã—à–∞–µ—Ç —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –º–æ–¥–µ–ª–∏."
}
```
[09.10.2025 16:16] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"The choice of delimiter in formatting in-context examples significantly impacts the performance of large language models across different families and tasks.  					AI-generated summary 				 Common Large Language model (LLM) evaluations rely on demonstration examples to steer models' responses to the desired style. While the number of examples used has been studied and standardized, the choice of how to format examples is less investigated. In evaluation protocols and real world usage, users face the choice how to separate in-context examples: use a comma? new line? semi-colon? hashtag? etc.? Surprisingly, we find this seemingly minor choice can dramatically alter model response quality. Across leading model families (Llama, Qwen, Gemma), performance on MMLU for example can vary by pm 23% depending on the choice of delimiter. In fact, one can manipulate model rankings to put any model in the lead by only modifying the single character separating examples. We find LLMs' brittleness pervades topics, model families, and doesn't improve with scale. By probing attention head scores, we find that good-performing delimiters steer attention towards key tokens in the input. Finally, we explore methods to improve LLMs' robustness to the choice of delimiter. We find specifying the selected delimiter in the prompt boosts robustness and offer practical recommendations for the best-performing delimiters to select."

[09.10.2025 16:16] Response: ```python
['DATA', 'BENCHMARK', 'TRAINING']
```
[09.10.2025 16:16] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"The choice of delimiter in formatting in-context examples significantly impacts the performance of large language models across different families and tasks.  					AI-generated summary 				 Common Large Language model (LLM) evaluations rely on demonstration examples to steer models' responses to the desired style. While the number of examples used has been studied and standardized, the choice of how to format examples is less investigated. In evaluation protocols and real world usage, users face the choice how to separate in-context examples: use a comma? new line? semi-colon? hashtag? etc.? Surprisingly, we find this seemingly minor choice can dramatically alter model response quality. Across leading model families (Llama, Qwen, Gemma), performance on MMLU for example can vary by pm 23% depending on the choice of delimiter. In fact, one can manipulate model rankings to put any model in the lead by only modifying the single character separating examples. We find LLMs' brittleness pervades topics, model families, and doesn't improve with scale. By probing attention head scores, we find that good-performing delimiters steer attention towards key tokens in the input. Finally, we explore methods to improve LLMs' robustness to the choice of delimiter. We find specifying the selected delimiter in the prompt boosts robustness and offer practical recommendations for the best-performing delimiters to select."

[09.10.2025 16:16] Response: ```python
["INTERPRETABILITY", "OPTIMIZATION"]
```
[09.10.2025 16:16] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper investigates how the choice of delimiter in formatting in-context examples affects the performance of large language models (LLMs). It reveals that even minor formatting decisions, such as using a comma or a new line, can lead to significant variations in model performance, with differences up to 23% on tasks like MMLU. The study shows that this sensitivity is consistent across various model families and does not improve with larger models. Additionally, the authors propose methods to enhance LLM robustness by specifying delimiters in prompts and provide recommendations for optimal delimiter choices.","title":"Delimiter Decisions Matter: Boosting LLM Performance!"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper investigates how the choice of delimiter in formatting in-context examples affects the performance of large language models (LLMs). It reveals that even minor formatting decisions, such as using a comma or a new line, can lead to significant variations in model performance, with differences up to 23% on tasks like MMLU. The study shows that this sensitivity is consistent across various model families and does not improve with larger models. Additionally, the authors propose methods to enhance LLM robustness by specifying delimiters in prompts and provide recommendations for optimal delimiter choices.', title='Delimiter Decisions Matter: Boosting LLM Performance!'))
[09.10.2025 16:16] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Âú®ËøôÁØáËÆ∫Êñá‰∏≠ÔºåÊàë‰ª¨Á†îÁ©∂‰∫ÜÂú®‰∏ä‰∏ãÊñáÁ§∫‰æã‰∏≠ÈÄâÊã©ÂàÜÈöîÁ¨¶ÂØπÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÊÄßËÉΩÁöÑÂΩ±Âìç„ÄÇÂ∞ΩÁÆ°Á§∫‰æãÁöÑÊï∞ÈáèÂ∑≤ÁªèË¢´Á†îÁ©∂ÂíåÊ†áÂáÜÂåñÔºå‰ΩÜÂàÜÈöîÁ¨¶ÁöÑÈÄâÊã©Âç¥È≤úÊúâÊé¢ËÆ®„ÄÇÊàë‰ª¨ÁöÑÂÆûÈ™åË°®ÊòéÔºåÂàÜÈöîÁ¨¶ÁöÑ‰∏çÂêåÈÄâÊã©ÂèØ‰ª•ÂØºËá¥Ê®°ÂûãÂìçÂ∫îË¥®ÈáèÁöÑÊòæËëóÂèòÂåñÔºåÁîöËá≥ÂΩ±ÂìçÊ®°ÂûãÁöÑÊéíÂêç„ÄÇÊàë‰ª¨ËøòÊèêÂá∫‰∫Ü‰∏Ä‰∫õÊñπÊ≥ïÊù•ÊèêÈ´òLLMÂØπÂàÜÈöîÁ¨¶ÈÄâÊã©ÁöÑÈ≤ÅÊ£íÊÄßÔºåÂª∫ËÆÆÂú®ÊèêÁ§∫‰∏≠ÊòéÁ°ÆÊåáÂÆöÂàÜÈöîÁ¨¶‰ª•ÊèêÂçáÊÄßËÉΩ„ÄÇ","title":"ÂàÜÈöîÁ¨¶ÈÄâÊã©ÂΩ±ÂìçÊ®°ÂûãË°®Áé∞ÁöÑÂÖ≥ÈîÆ"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Âú®ËøôÁØáËÆ∫Êñá‰∏≠ÔºåÊàë‰ª¨Á†îÁ©∂‰∫ÜÂú®‰∏ä‰∏ãÊñáÁ§∫‰æã‰∏≠ÈÄâÊã©ÂàÜÈöîÁ¨¶ÂØπÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÊÄßËÉΩÁöÑÂΩ±Âìç„ÄÇÂ∞ΩÁÆ°Á§∫‰æãÁöÑÊï∞ÈáèÂ∑≤ÁªèË¢´Á†îÁ©∂ÂíåÊ†áÂáÜÂåñÔºå‰ΩÜÂàÜÈöîÁ¨¶ÁöÑÈÄâÊã©Âç¥È≤úÊúâÊé¢ËÆ®„ÄÇÊàë‰ª¨ÁöÑÂÆûÈ™åË°®ÊòéÔºåÂàÜÈöîÁ¨¶ÁöÑ‰∏çÂêåÈÄâÊã©ÂèØ‰ª•ÂØºËá¥Ê®°ÂûãÂìçÂ∫îË¥®ÈáèÁöÑÊòæËëóÂèòÂåñÔºåÁîöËá≥ÂΩ±ÂìçÊ®°ÂûãÁöÑÊéíÂêç„ÄÇÊàë‰ª¨ËøòÊèêÂá∫‰∫Ü‰∏Ä‰∫õÊñπÊ≥ïÊù•ÊèêÈ´òLLMÂØπÂàÜÈöîÁ¨¶ÈÄâÊã©ÁöÑÈ≤ÅÊ£íÊÄßÔºåÂª∫ËÆÆÂú®ÊèêÁ§∫‰∏≠ÊòéÁ°ÆÊåáÂÆöÂàÜÈöîÁ¨¶‰ª•ÊèêÂçáÊÄßËÉΩ„ÄÇ', title='ÂàÜÈöîÁ¨¶ÈÄâÊã©ÂΩ±ÂìçÊ®°ÂûãË°®Áé∞ÁöÑÂÖ≥ÈîÆ'))
[09.10.2025 16:16] Renaming data file.
[09.10.2025 16:16] Renaming previous data. hf_papers.json to ./d/2025-10-09.json
[09.10.2025 16:16] Saving new data file.
[09.10.2025 16:16] Generating page.
[09.10.2025 16:16] Renaming previous page.
[09.10.2025 16:16] Renaming previous data. index.html to ./d/2025-10-09.html
[09.10.2025 16:16] Writing result.
[09.10.2025 16:16] Renaming log file.
[09.10.2025 16:16] Renaming previous data. log.txt to ./logs/2025-10-09_last_log.txt
