[29.12.2025 21:20] Read previous papers.
[29.12.2025 21:20] Generating top page (month).
[29.12.2025 21:20] Writing top page (month).
[29.12.2025 22:21] Read previous papers.
[29.12.2025 22:21] Get feed.
[29.12.2025 22:21] Get page data from previous paper. URL: https://huggingface.co/papers/2512.17504
[29.12.2025 22:21] Get page data from previous paper. URL: https://huggingface.co/papers/2512.17220
[29.12.2025 22:21] Get page data from previous paper. URL: https://huggingface.co/papers/2512.22047
[29.12.2025 22:21] Get page data from previous paper. URL: https://huggingface.co/papers/2512.21675
[29.12.2025 22:21] Get page data from previous paper. URL: https://huggingface.co/papers/2512.22118
[29.12.2025 22:21] Get page data from previous paper. URL: https://huggingface.co/papers/2512.21859
[29.12.2025 22:21] Get page data from previous paper. URL: https://huggingface.co/papers/2512.22120
[29.12.2025 22:21] Get page data from previous paper. URL: https://huggingface.co/papers/2512.21643
[29.12.2025 22:21] Get page data from previous paper. URL: https://huggingface.co/papers/2512.18745
[29.12.2025 22:21] Get page data from previous paper. URL: https://huggingface.co/papers/2512.21919
[29.12.2025 22:21] Get page data from previous paper. URL: https://huggingface.co/papers/2512.21507
[29.12.2025 22:21] Get page data from previous paper. URL: https://huggingface.co/papers/2512.20292
[29.12.2025 22:21] Get page data from previous paper. URL: https://huggingface.co/papers/2512.21980
[29.12.2025 22:21] Get page data from previous paper. URL: https://huggingface.co/papers/2512.21625
[29.12.2025 22:21] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[29.12.2025 22:21] No deleted papers detected.
[29.12.2025 22:21] Downloading and parsing papers (pdf, html). Total: 14.
[29.12.2025 22:21] Downloading and parsing paper https://huggingface.co/papers/2512.17504.
[29.12.2025 22:21] Extra JSON file exists (./assets/json/2512.17504.json), skip PDF parsing.
[29.12.2025 22:21] Paper image links file exists (./assets/img_data/2512.17504.json), skip HTML parsing.
[29.12.2025 22:21] Success.
[29.12.2025 22:21] Downloading and parsing paper https://huggingface.co/papers/2512.17220.
[29.12.2025 22:21] Extra JSON file exists (./assets/json/2512.17220.json), skip PDF parsing.
[29.12.2025 22:21] Paper image links file exists (./assets/img_data/2512.17220.json), skip HTML parsing.
[29.12.2025 22:21] Success.
[29.12.2025 22:21] Downloading and parsing paper https://huggingface.co/papers/2512.22047.
[29.12.2025 22:21] Extra JSON file exists (./assets/json/2512.22047.json), skip PDF parsing.
[29.12.2025 22:21] Paper image links file exists (./assets/img_data/2512.22047.json), skip HTML parsing.
[29.12.2025 22:21] Success.
[29.12.2025 22:21] Downloading and parsing paper https://huggingface.co/papers/2512.21675.
[29.12.2025 22:21] Extra JSON file exists (./assets/json/2512.21675.json), skip PDF parsing.
[29.12.2025 22:21] Paper image links file exists (./assets/img_data/2512.21675.json), skip HTML parsing.
[29.12.2025 22:21] Success.
[29.12.2025 22:21] Downloading and parsing paper https://huggingface.co/papers/2512.22118.
[29.12.2025 22:21] Extra JSON file exists (./assets/json/2512.22118.json), skip PDF parsing.
[29.12.2025 22:21] Paper image links file exists (./assets/img_data/2512.22118.json), skip HTML parsing.
[29.12.2025 22:21] Success.
[29.12.2025 22:21] Downloading and parsing paper https://huggingface.co/papers/2512.21859.
[29.12.2025 22:21] Extra JSON file exists (./assets/json/2512.21859.json), skip PDF parsing.
[29.12.2025 22:21] Paper image links file exists (./assets/img_data/2512.21859.json), skip HTML parsing.
[29.12.2025 22:21] Success.
[29.12.2025 22:21] Downloading and parsing paper https://huggingface.co/papers/2512.22120.
[29.12.2025 22:21] Extra JSON file exists (./assets/json/2512.22120.json), skip PDF parsing.
[29.12.2025 22:21] Paper image links file exists (./assets/img_data/2512.22120.json), skip HTML parsing.
[29.12.2025 22:21] Success.
[29.12.2025 22:21] Downloading and parsing paper https://huggingface.co/papers/2512.21643.
[29.12.2025 22:21] Extra JSON file exists (./assets/json/2512.21643.json), skip PDF parsing.
[29.12.2025 22:21] Paper image links file exists (./assets/img_data/2512.21643.json), skip HTML parsing.
[29.12.2025 22:21] Success.
[29.12.2025 22:21] Downloading and parsing paper https://huggingface.co/papers/2512.18745.
[29.12.2025 22:21] Extra JSON file exists (./assets/json/2512.18745.json), skip PDF parsing.
[29.12.2025 22:21] Paper image links file exists (./assets/img_data/2512.18745.json), skip HTML parsing.
[29.12.2025 22:21] Success.
[29.12.2025 22:21] Downloading and parsing paper https://huggingface.co/papers/2512.21919.
[29.12.2025 22:21] Extra JSON file exists (./assets/json/2512.21919.json), skip PDF parsing.
[29.12.2025 22:21] Paper image links file exists (./assets/img_data/2512.21919.json), skip HTML parsing.
[29.12.2025 22:21] Success.
[29.12.2025 22:21] Downloading and parsing paper https://huggingface.co/papers/2512.21507.
[29.12.2025 22:21] Extra JSON file exists (./assets/json/2512.21507.json), skip PDF parsing.
[29.12.2025 22:21] Paper image links file exists (./assets/img_data/2512.21507.json), skip HTML parsing.
[29.12.2025 22:21] Success.
[29.12.2025 22:21] Downloading and parsing paper https://huggingface.co/papers/2512.20292.
[29.12.2025 22:21] Extra JSON file exists (./assets/json/2512.20292.json), skip PDF parsing.
[29.12.2025 22:21] Paper image links file exists (./assets/img_data/2512.20292.json), skip HTML parsing.
[29.12.2025 22:21] Success.
[29.12.2025 22:21] Downloading and parsing paper https://huggingface.co/papers/2512.21980.
[29.12.2025 22:21] Extra JSON file exists (./assets/json/2512.21980.json), skip PDF parsing.
[29.12.2025 22:21] Paper image links file exists (./assets/img_data/2512.21980.json), skip HTML parsing.
[29.12.2025 22:21] Success.
[29.12.2025 22:21] Downloading and parsing paper https://huggingface.co/papers/2512.21625.
[29.12.2025 22:21] Extra JSON file exists (./assets/json/2512.21625.json), skip PDF parsing.
[29.12.2025 22:21] Paper image links file exists (./assets/img_data/2512.21625.json), skip HTML parsing.
[29.12.2025 22:21] Success.
[29.12.2025 22:21] Enriching papers with extra data.
[29.12.2025 22:21] ********************************************************************************
[29.12.2025 22:21] Abstract 0. InsertAnywhere framework enhances video object insertion by generating geometrically consistent and visually coherent scenarios through 4D aware mask generation and diffusion-based synthesis.  					AI-generated summary 				 Recent advances in diffusion-based video generation have opened new possibil...
[29.12.2025 22:21] ********************************************************************************
[29.12.2025 22:21] Abstract 1. MiA-RAG, a Mindscape-Aware Retrieval-Augmented Generation system, enhances LLM-based RAG with global context awareness through hierarchical summarization, improving long-context tasks and evidence-based understanding.  					AI-generated summary 				 Humans understand long and complex texts by relyin...
[29.12.2025 22:21] ********************************************************************************
[29.12.2025 22:21] Abstract 2. The development of GUI agents could revolutionize the next generation of human-computer interaction. Motivated by this vision, we present MAI-UI, a family of foundation GUI agents spanning the full spectrum of sizes, including 2B, 8B, 32B, and 235B-A22B variants. We identify four key challenges to r...
[29.12.2025 22:21] ********************************************************************************
[29.12.2025 22:21] Abstract 3. Multimodal large language models (MLLMs) have achieved remarkable progress in visual understanding tasks such as visual grounding, segmentation, and captioning. However, their ability to perceive perceptual-level image features remains limited. In this work, we present UniPercept-Bench, a unified fr...
[29.12.2025 22:21] ********************************************************************************
[29.12.2025 22:21] Abstract 4. Inversion-based visual editing provides an effective and training-free way to edit an image or a video based on user instructions. Existing methods typically inject source image information during the sampling process to maintain editing consistency. However, this sampling strategy overly relies on ...
[29.12.2025 22:21] ********************************************************************************
[29.12.2025 22:21] Abstract 5. Large Language Models (LLMs) are increasingly deployed in time-critical systems, such as robotics, autonomous driving, embodied intelligence, and industrial automation, where generating accurate responses within a given time budget is crucial for decision-making, control, or safety-critical tasks. H...
[29.12.2025 22:21] ********************************************************************************
[29.12.2025 22:21] Abstract 6. Large vision-language models (VLMs) often benefit from intermediate visual cues, either injected via external tools or generated as latent visual tokens during reasoning, but these mechanisms still overlook fine-grained visual evidence (e.g., polylines in charts), generalize poorly across domains, a...
[29.12.2025 22:21] ********************************************************************************
[29.12.2025 22:21] Abstract 7. Omni-Weather is a multimodal foundation model that integrates weather generation and understanding using a shared self-attention mechanism and a Chain-of-Thought dataset to enable interpretable, high-quality outputs.  					AI-generated summary 				 Weather modeling requires both accurate prediction ...
[29.12.2025 22:21] ********************************************************************************
[29.12.2025 22:21] Abstract 8. O3-Bench evaluates multimodal reasoning with interleaved attention to visual details, while InSight-o3 uses a multi-agent framework to improve performance through specialized visual search and reasoning tasks.  					AI-generated summary 				 The ability for AI agents to "think with images" requires ...
[29.12.2025 22:21] ********************************************************************************
[29.12.2025 22:21] Abstract 9. Execution-based feedback like unit testing is widely used in the development of coding agents through test-time scaling (TTS) and reinforcement learning (RL). This paradigm requires scalable and reliable collection of unit test cases to provide accurate feedback, and the resulting feedback is often ...
[29.12.2025 22:21] ********************************************************************************
[29.12.2025 22:21] Abstract 10. Recent text-to-video generation models exhibit remarkable progress in visual realism, motion fidelity, and text-video alignment, yet they remain fundamentally limited in their ability to generate socially coherent behavior. Unlike humans, who effortlessly infer intentions, beliefs, emotions, and soc...
[29.12.2025 22:21] ********************************************************************************
[29.12.2025 22:21] Abstract 11. SlideTailor, an agentic framework, generates user-aligned slides using implicit preferences from example pairs and visual templates, incorporating a chain-of-speech mechanism for oral narration alignment and a benchmark dataset for evaluation.  					AI-generated summary 				 Automatic presentation s...
[29.12.2025 22:21] ********************************************************************************
[29.12.2025 22:21] Abstract 12. This paper presents a new state-of-the-art algorithm for exact 3times3 matrix multiplication over general non-commutative rings, achieving a rank-23 scheme with only 58 scalar additions. This improves the previous best additive complexity of 60 additions without a change of basis. The result was dis...
[29.12.2025 22:21] ********************************************************************************
[29.12.2025 22:21] Abstract 13. Large reasoning models (LRMs) are typically trained using reinforcement learning with verifiable reward (RLVR) to enhance their reasoning abilities. In this paradigm, policies are updated using both positive and negative self-generated rollouts, which correspond to distinct sample polarities. In thi...
[29.12.2025 22:21] Read previous papers.
[29.12.2025 22:21] Generating reviews via LLM API.
[29.12.2025 22:21] Using data from previous issue: {"categories": ["#dataset", "#multimodal", "#video", "#diffusion", "#synthetic"], "emoji": "üé¨", "ru": {"title": "–†–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ–µ –≤—Å—Ç—Ä–∞–∏–≤–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –≤–∏–¥–µ–æ —á–µ—Ä–µ–∑ 4D-–æ—Å–æ–∑–Ω–∞–Ω–Ω–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ —Å—Ü–µ–Ω—ã", "desc": "InsertAnywhere ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –≤—Å—Ç—Ä–∞–∏–≤–∞–Ω–∏—è –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –≤–∏–¥–µ–æ, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç 4D-–æ—Å–æ–∑–Ω–∞–Ω–Ω–æ–µ 
[29.12.2025 22:21] Using data from previous issue: {"categories": ["#long_context", "#benchmark", "#reasoning", "#multilingual", "#rag"], "emoji": "üß†", "ru": {"title": "–ì–ª–æ–±–∞–ª—å–Ω–æ–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ —Å–æ–∑–Ω–∞–Ω–∏–µ –¥–ª—è RAG —Å–∏—Å—Ç–µ–º", "desc": "MiA-RAG ‚Äî —ç—Ç–æ —Å–∏—Å—Ç–µ–º–∞ retrieval-augmented generation, –∫–æ—Ç–æ—Ä–∞—è —Ä–∞—Å—à–∏—Ä—è–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ LLM, –¥–æ–±–∞–≤–ª—è—è –≥–ª–æ–±–∞–ª—å–Ω–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ
[29.12.2025 22:21] Using data from previous issue: {"categories": ["#dataset", "#multimodal", "#long_context", "#optimization", "#open_source", "#benchmark", "#training", "#agents", "#rl"], "emoji": "ü§ñ", "ru": {"title": "–ê–≥–µ–Ω—Ç—ã –¥–ª—è –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞–º–∏ –≤ –ª—é–±–æ–º –º–∞—Å—à—Ç–∞–±–µ", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ MAI-UI ‚Äî —Å–µ–º–µ–π—Å—Ç–≤–æ foundation –º–æ–¥–µ–ª–µ
[29.12.2025 22:21] Using data from previous issue: {"categories": ["#dataset", "#multimodal", "#cv", "#benchmark", "#rl"], "emoji": "üé®", "ru": {"title": "–í–æ—Å–ø—Ä–∏—è—Ç–∏–µ –∫—Ä–∞—Å–æ—Ç—ã –∏ –∫–∞—á–µ—Å—Ç–≤–∞: –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ –ø–µ—Ä—Ü–µ–ø—Ç–∏–≤–Ω–æ–º —É—Ä–æ–≤–Ω–µ", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω UniPercept-Bench ‚Äî –µ–¥–∏–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ —É—Ä–æ–≤–Ω–µ —ç—Å—Ç–µ—Ç–∏–∫
[29.12.2025 22:21] Using data from previous issue: {"categories": ["#video", "#cv", "#multimodal"], "emoji": "‚úèÔ∏è", "ru": {"title": "–û—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏–µ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –æ—Ç –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç ProEdit ‚Äî –º–µ—Ç–æ–¥ –¥–ª—è —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –≤–∏–¥–µ–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è, –∏—Å–ø–æ–ª—å–∑—É—è –∏–Ω–≤–µ—Ä—Å–∏—é –¥–∏—Ñ—Ñ—É–∑–∏
[29.12.2025 22:21] Using data from previous issue: {"categories": ["#optimization", "#robotics", "#inference"], "emoji": "‚è±Ô∏è", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏ —Å –∫–æ–Ω—Ç—Ä–æ–ª–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–Ω–æ–º—É –±—é–¥–∂–µ—Ç—É", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ TimeBill –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è—Ö, —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª
[29.12.2025 22:21] Using data from previous issue: {"categories": ["#benchmark", "#cv", "#multimodal", "#training"], "emoji": "üëÅÔ∏è", "ru": {"title": "–î–≤—É–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–µ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –∑—Ä–∏—Ç–µ–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –º–µ—Ç–æ–¥ Bi-directional Perceptual Shaping (BiPS) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö vision-language –º–æ–¥–µ–ª–µ
[29.12.2025 22:21] Using data from previous issue: {"categories": ["#architecture", "#science", "#dataset", "#multimodal", "#interpretability", "#training", "#reasoning"], "emoji": "üå¶Ô∏è", "ru": {"title": "–ï–¥–∏–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è –ø–æ–≥–æ–¥—ã", "desc": "Omni-Weather ‚Äî —ç—Ç–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è foundation model, –∫–æ—Ç–æ—Ä–∞—è –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –∑–∞–¥–∞—á–∏ –≥–µ–Ω–µ—Ä–∞—Ü
[29.12.2025 22:21] Using data from previous issue: {"categories": ["#open_source", "#benchmark", "#dataset", "#reasoning", "#multimodal", "#agents", "#rl"], "emoji": "üîç", "ru": {"title": "–í–∏–¥–µ—Ç—å –∏ —Ä–∞–∑–º—ã—à–ª—è—Ç—å: –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –≥–ª—É–±–æ–∫–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω O3-Bench ‚Äî –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ
[29.12.2025 22:21] Using data from previous issue: {"categories": ["#optimization", "#open_source", "#benchmark", "#training", "#plp", "#agents", "#rl"], "emoji": "üèÜ", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤ –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–π –∏–Ω–∂–µ–Ω–µ—Ä–∏–∏", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –º–µ—Ç–æ–¥–∏–∫–∞ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –º–æ–¥–µ–ª–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è (reward mod
[29.12.2025 22:21] Using data from previous issue: {"categories": ["#video", "#benchmark", "#multimodal", "#agents"], "emoji": "üß†", "ru": {"title": "–û—Ü–µ–Ω–∫–∞ —Å–æ—Ü–∏–∞–ª—å–Ω–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è –≤ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –≤–∏–¥–µ–æ–º–æ–¥–µ–ª—è—Ö", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –ø–µ—Ä–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–æ—Ü–∏–∞–ª—å–Ω–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è –≤ –º–æ–¥–µ–ª—è—Ö –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –∏–∑ —Ç–µ–∫—Å—Ç–∞. –ê–≤—Ç–æ—Ä—ã —Å–æ–∑–¥–∞–ª–∏ –Ω–∞–±–æ—Ä —Ç–µ—Å—Ç–æ–≤ –Ω
[29.12.2025 22:21] Using data from previous issue: {"categories": ["#open_source", "#alignment", "#agents", "#benchmark", "#dataset", "#multimodal"], "emoji": "üéØ", "ru": {"title": "–ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–ª–∞–π–¥–æ–≤ —á–µ—Ä–µ–∑ –Ω–µ—è–≤–Ω—ã–µ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è", "desc": "SlideTailor ‚Äî —ç—Ç–æ –∞–≥–µ–Ω—Ç–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞, –∫–æ—Ç–æ—Ä–∞—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å–ª–∞–π–¥—ã –ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏–π, –∞–¥–∞–ø—Ç
[29.12.2025 22:21] Using data from previous issue: {"categories": [], "emoji": "‚ö°", "ru": {"title": "–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –º–∞—Ç—Ä–∏—á–Ω–æ–≥–æ —É–º–Ω–æ–∂–µ–Ω–∏—è —á–µ—Ä–µ–∑ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏ –∞—Ä–∏—Ñ–º–µ—Ç–∏—á–µ—Å–∫–∏–º–∏ –æ–ø–µ—Ä–∞—Ü–∏—è–º–∏", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –¥–ª—è —É–º–Ω–æ–∂–µ–Ω–∏—è –º–∞—Ç—Ä–∏—Ü —Ä–∞–∑–º–µ—Ä–æ–º 3√ó3 –Ω–∞–¥ –Ω–µ–∫–æ–º–º—É—Ç–∞—Ç–∏–≤–Ω—ã–º–∏ –∫–æ–ª—å—Ü–∞–º–∏, –¥–æ—Å—Ç–∏–≥–∞—é—â–∏–π —Ä–∞–Ω–≥–∞ 23 —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
[29.12.2025 22:21] Using data from previous issue: {"categories": ["#benchmark", "#optimization", "#training", "#reasoning", "#rl"], "emoji": "üß†", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–π –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è", "desc": "–í —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è, –∫–∞–∫ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –∏ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã –≤–ª–∏—è—é—Ç –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ –±–æ–ª—å—à–∏—Ö 
[29.12.2025 22:21] Renaming data file.
[29.12.2025 22:21] Renaming previous data. hf_papers.json to ./d/2025-12-29.json
[29.12.2025 22:21] Saving new data file.
[29.12.2025 22:21] Generating page.
[29.12.2025 22:21] Renaming previous page.
[29.12.2025 22:21] Renaming previous data. index.html to ./d/2025-12-29.html
[29.12.2025 22:21] Writing result.
[29.12.2025 22:21] Renaming log file.
[29.12.2025 22:21] Renaming previous data. log.txt to ./logs/2025-12-29_last_log.txt
