[29.12.2025 04:51] Read previous papers.
[29.12.2025 04:51] Generating top page (month).
[29.12.2025 04:51] Writing top page (month).
[29.12.2025 05:30] Read previous papers.
[29.12.2025 05:30] Get feed.
[29.12.2025 05:30] Get page data from previous paper. URL: https://huggingface.co/papers/2512.17220
[29.12.2025 05:30] Get page data from previous paper. URL: https://huggingface.co/papers/2512.17504
[29.12.2025 05:30] Get page data from previous paper. URL: https://huggingface.co/papers/2512.22047
[29.12.2025 05:30] Get page data from previous paper. URL: https://huggingface.co/papers/2512.22120
[29.12.2025 05:30] Get page data from previous paper. URL: https://huggingface.co/papers/2512.21859
[29.12.2025 05:30] Get page data from previous paper. URL: https://huggingface.co/papers/2512.21507
[29.12.2025 05:30] Get page data from previous paper. URL: https://huggingface.co/papers/2512.21919
[29.12.2025 05:30] Get page data from previous paper. URL: https://huggingface.co/papers/2512.21675
[29.12.2025 05:30] Get page data from previous paper. URL: https://huggingface.co/papers/2512.22118
[29.12.2025 05:30] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[29.12.2025 05:30] No deleted papers detected.
[29.12.2025 05:30] Downloading and parsing papers (pdf, html). Total: 9.
[29.12.2025 05:30] Downloading and parsing paper https://huggingface.co/papers/2512.17220.
[29.12.2025 05:30] Extra JSON file exists (./assets/json/2512.17220.json), skip PDF parsing.
[29.12.2025 05:30] Paper image links file exists (./assets/img_data/2512.17220.json), skip HTML parsing.
[29.12.2025 05:30] Success.
[29.12.2025 05:30] Downloading and parsing paper https://huggingface.co/papers/2512.17504.
[29.12.2025 05:30] Extra JSON file exists (./assets/json/2512.17504.json), skip PDF parsing.
[29.12.2025 05:30] Paper image links file exists (./assets/img_data/2512.17504.json), skip HTML parsing.
[29.12.2025 05:30] Success.
[29.12.2025 05:30] Downloading and parsing paper https://huggingface.co/papers/2512.22047.
[29.12.2025 05:30] Extra JSON file exists (./assets/json/2512.22047.json), skip PDF parsing.
[29.12.2025 05:30] Paper image links file exists (./assets/img_data/2512.22047.json), skip HTML parsing.
[29.12.2025 05:30] Success.
[29.12.2025 05:30] Downloading and parsing paper https://huggingface.co/papers/2512.22120.
[29.12.2025 05:30] Extra JSON file exists (./assets/json/2512.22120.json), skip PDF parsing.
[29.12.2025 05:30] Paper image links file exists (./assets/img_data/2512.22120.json), skip HTML parsing.
[29.12.2025 05:30] Success.
[29.12.2025 05:30] Downloading and parsing paper https://huggingface.co/papers/2512.21859.
[29.12.2025 05:30] Extra JSON file exists (./assets/json/2512.21859.json), skip PDF parsing.
[29.12.2025 05:30] Paper image links file exists (./assets/img_data/2512.21859.json), skip HTML parsing.
[29.12.2025 05:30] Success.
[29.12.2025 05:30] Downloading and parsing paper https://huggingface.co/papers/2512.21507.
[29.12.2025 05:30] Extra JSON file exists (./assets/json/2512.21507.json), skip PDF parsing.
[29.12.2025 05:30] Paper image links file exists (./assets/img_data/2512.21507.json), skip HTML parsing.
[29.12.2025 05:30] Success.
[29.12.2025 05:30] Downloading and parsing paper https://huggingface.co/papers/2512.21919.
[29.12.2025 05:30] Extra JSON file exists (./assets/json/2512.21919.json), skip PDF parsing.
[29.12.2025 05:30] Paper image links file exists (./assets/img_data/2512.21919.json), skip HTML parsing.
[29.12.2025 05:30] Success.
[29.12.2025 05:30] Downloading and parsing paper https://huggingface.co/papers/2512.21675.
[29.12.2025 05:30] Extra JSON file exists (./assets/json/2512.21675.json), skip PDF parsing.
[29.12.2025 05:30] Paper image links file exists (./assets/img_data/2512.21675.json), skip HTML parsing.
[29.12.2025 05:30] Success.
[29.12.2025 05:30] Downloading and parsing paper https://huggingface.co/papers/2512.22118.
[29.12.2025 05:30] Extra JSON file exists (./assets/json/2512.22118.json), skip PDF parsing.
[29.12.2025 05:30] Paper image links file exists (./assets/img_data/2512.22118.json), skip HTML parsing.
[29.12.2025 05:30] Success.
[29.12.2025 05:30] Enriching papers with extra data.
[29.12.2025 05:30] ********************************************************************************
[29.12.2025 05:30] Abstract 0. MiA-RAG, a Mindscape-Aware Retrieval-Augmented Generation system, enhances LLM-based RAG with global context awareness through hierarchical summarization, improving long-context tasks and evidence-based understanding.  					AI-generated summary 				 Humans understand long and complex texts by relyin...
[29.12.2025 05:30] ********************************************************************************
[29.12.2025 05:30] Abstract 1. InsertAnywhere framework enhances video object insertion by generating geometrically consistent and visually coherent scenarios through 4D aware mask generation and diffusion-based synthesis.  					AI-generated summary 				 Recent advances in diffusion-based video generation have opened new possibil...
[29.12.2025 05:30] ********************************************************************************
[29.12.2025 05:30] Abstract 2. The development of GUI agents could revolutionize the next generation of human-computer interaction. Motivated by this vision, we present MAI-UI, a family of foundation GUI agents spanning the full spectrum of sizes, including 2B, 8B, 32B, and 235B-A22B variants. We identify four key challenges to r...
[29.12.2025 05:30] ********************************************************************************
[29.12.2025 05:30] Abstract 3. Large vision-language models (VLMs) often benefit from intermediate visual cues, either injected via external tools or generated as latent visual tokens during reasoning, but these mechanisms still overlook fine-grained visual evidence (e.g., polylines in charts), generalize poorly across domains, a...
[29.12.2025 05:30] ********************************************************************************
[29.12.2025 05:30] Abstract 4. Large Language Models (LLMs) are increasingly deployed in time-critical systems, such as robotics, autonomous driving, embodied intelligence, and industrial automation, where generating accurate responses within a given time budget is crucial for decision-making, control, or safety-critical tasks. H...
[29.12.2025 05:30] ********************************************************************************
[29.12.2025 05:30] Abstract 5. Recent text-to-video generation models exhibit remarkable progress in visual realism, motion fidelity, and text-video alignment, yet they remain fundamentally limited in their ability to generate socially coherent behavior. Unlike humans, who effortlessly infer intentions, beliefs, emotions, and soc...
[29.12.2025 05:30] ********************************************************************************
[29.12.2025 05:30] Abstract 6. Execution-based feedback like unit testing is widely used in the development of coding agents through test-time scaling (TTS) and reinforcement learning (RL). This paradigm requires scalable and reliable collection of unit test cases to provide accurate feedback, and the resulting feedback is often ...
[29.12.2025 05:30] ********************************************************************************
[29.12.2025 05:30] Abstract 7. Multimodal large language models (MLLMs) have achieved remarkable progress in visual understanding tasks such as visual grounding, segmentation, and captioning. However, their ability to perceive perceptual-level image features remains limited. In this work, we present UniPercept-Bench, a unified fr...
[29.12.2025 05:30] ********************************************************************************
[29.12.2025 05:30] Abstract 8. Inversion-based visual editing provides an effective and training-free way to edit an image or a video based on user instructions. Existing methods typically inject source image information during the sampling process to maintain editing consistency. However, this sampling strategy overly relies on ...
[29.12.2025 05:30] Read previous papers.
[29.12.2025 05:30] Generating reviews via LLM API.
[29.12.2025 05:30] Using data from previous issue: {"categories": ["#long_context", "#benchmark", "#reasoning", "#multilingual", "#rag"], "emoji": "üß†", "ru": {"title": "–ì–ª–æ–±–∞–ª—å–Ω–æ–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ —Å–æ–∑–Ω–∞–Ω–∏–µ –¥–ª—è RAG —Å–∏—Å—Ç–µ–º", "desc": "MiA-RAG ‚Äî —ç—Ç–æ —Å–∏—Å—Ç–µ–º–∞ retrieval-augmented generation, –∫–æ—Ç–æ—Ä–∞—è —Ä–∞—Å—à–∏—Ä—è–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ LLM, –¥–æ–±–∞–≤–ª—è—è –≥–ª–æ–±–∞–ª—å–Ω–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ
[29.12.2025 05:30] Using data from previous issue: {"categories": ["#dataset", "#multimodal", "#video", "#diffusion", "#synthetic"], "emoji": "üé¨", "ru": {"title": "–†–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ–µ –≤—Å—Ç—Ä–∞–∏–≤–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –≤–∏–¥–µ–æ —á–µ—Ä–µ–∑ 4D-–æ—Å–æ–∑–Ω–∞–Ω–Ω–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ —Å—Ü–µ–Ω—ã", "desc": "InsertAnywhere ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –≤—Å—Ç—Ä–∞–∏–≤–∞–Ω–∏—è –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –≤–∏–¥–µ–æ, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç 4D-–æ—Å–æ–∑–Ω–∞–Ω–Ω–æ–µ 
[29.12.2025 05:30] Using data from previous issue: {"categories": ["#dataset", "#multimodal", "#long_context", "#optimization", "#open_source", "#benchmark", "#training", "#agents", "#rl"], "emoji": "ü§ñ", "ru": {"title": "–ê–≥–µ–Ω—Ç—ã –¥–ª—è –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞–º–∏ –≤ –ª—é–±–æ–º –º–∞—Å—à—Ç–∞–±–µ", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ MAI-UI ‚Äî —Å–µ–º–µ–π—Å—Ç–≤–æ foundation –º–æ–¥–µ–ª–µ
[29.12.2025 05:30] Using data from previous issue: {"categories": ["#benchmark", "#cv", "#multimodal", "#training"], "emoji": "üëÅÔ∏è", "ru": {"title": "–î–≤—É–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–µ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –∑—Ä–∏—Ç–µ–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –º–µ—Ç–æ–¥ Bi-directional Perceptual Shaping (BiPS) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö vision-language –º–æ–¥–µ–ª–µ
[29.12.2025 05:30] Using data from previous issue: {"categories": ["#optimization", "#robotics", "#inference"], "emoji": "‚è±Ô∏è", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏ —Å –∫–æ–Ω—Ç—Ä–æ–ª–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–Ω–æ–º—É –±—é–¥–∂–µ—Ç—É", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ TimeBill –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è—Ö, —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª
[29.12.2025 05:30] Using data from previous issue: {"categories": ["#video", "#benchmark", "#multimodal", "#agents"], "emoji": "üß†", "ru": {"title": "–û—Ü–µ–Ω–∫–∞ —Å–æ—Ü–∏–∞–ª—å–Ω–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è –≤ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –≤–∏–¥–µ–æ–º–æ–¥–µ–ª—è—Ö", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –ø–µ—Ä–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–æ—Ü–∏–∞–ª—å–Ω–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è –≤ –º–æ–¥–µ–ª—è—Ö –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –∏–∑ —Ç–µ–∫—Å—Ç–∞. –ê–≤—Ç–æ—Ä—ã —Å–æ–∑–¥–∞–ª–∏ –Ω–∞–±–æ—Ä —Ç–µ—Å—Ç–æ–≤ –Ω
[29.12.2025 05:30] Using data from previous issue: {"categories": ["#optimization", "#open_source", "#benchmark", "#training", "#plp", "#agents", "#rl"], "emoji": "üèÜ", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤ –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–π –∏–Ω–∂–µ–Ω–µ—Ä–∏–∏", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –º–µ—Ç–æ–¥–∏–∫–∞ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –º–æ–¥–µ–ª–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è (reward mod
[29.12.2025 05:30] Using data from previous issue: {"categories": ["#dataset", "#multimodal", "#cv", "#benchmark", "#rl"], "emoji": "üé®", "ru": {"title": "–í–æ—Å–ø—Ä–∏—è—Ç–∏–µ –∫—Ä–∞—Å–æ—Ç—ã –∏ –∫–∞—á–µ—Å—Ç–≤–∞: –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ –ø–µ—Ä—Ü–µ–ø—Ç–∏–≤–Ω–æ–º —É—Ä–æ–≤–Ω–µ", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω UniPercept-Bench ‚Äî –µ–¥–∏–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ —É—Ä–æ–≤–Ω–µ —ç—Å—Ç–µ—Ç–∏–∫
[29.12.2025 05:30] Using data from previous issue: {"categories": ["#video", "#cv", "#multimodal"], "emoji": "‚úèÔ∏è", "ru": {"title": "–û—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏–µ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –æ—Ç –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç ProEdit ‚Äî –º–µ—Ç–æ–¥ –¥–ª—è —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –≤–∏–¥–µ–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è, –∏—Å–ø–æ–ª—å–∑—É—è –∏–Ω–≤–µ—Ä—Å–∏—é –¥–∏—Ñ—Ñ—É–∑–∏
[29.12.2025 05:30] Renaming data file.
[29.12.2025 05:30] Renaming previous data. hf_papers.json to ./d/2025-12-29.json
[29.12.2025 05:30] Saving new data file.
[29.12.2025 05:30] Generating page.
[29.12.2025 05:30] Renaming previous page.
[29.12.2025 05:30] Renaming previous data. index.html to ./d/2025-12-29.html
[29.12.2025 05:30] Writing result.
[29.12.2025 05:30] Renaming log file.
[29.12.2025 05:30] Renaming previous data. log.txt to ./logs/2025-12-29_last_log.txt
