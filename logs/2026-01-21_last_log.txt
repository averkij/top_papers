[21.01.2026 12:54] Read previous papers.
[21.01.2026 12:54] Generating top page (month).
[21.01.2026 12:54] Writing top page (month).
[21.01.2026 13:48] Read previous papers.
[21.01.2026 13:48] Get feed.
[21.01.2026 13:48] Get page data from previous paper. URL: https://huggingface.co/papers/2601.11655
[21.01.2026 13:48] Get page data from previous paper. URL: https://huggingface.co/papers/2601.12993
[21.01.2026 13:48] Get page data from previous paper. URL: https://huggingface.co/papers/2601.14192
[21.01.2026 13:48] Get page data from previous paper. URL: https://huggingface.co/papers/2601.13836
[21.01.2026 13:48] Get page data from previous paper. URL: https://huggingface.co/papers/2601.14250
[21.01.2026 13:48] Get page data from previous paper. URL: https://huggingface.co/papers/2601.11969
[21.01.2026 13:48] Get page data from previous paper. URL: https://huggingface.co/papers/2601.13029
[21.01.2026 13:48] Get page data from previous paper. URL: https://huggingface.co/papers/2601.11522
[21.01.2026 13:48] Get page data from previous paper. URL: https://huggingface.co/papers/2601.12294
[21.01.2026 13:48] Get page data from previous paper. URL: https://huggingface.co/papers/2601.13247
[21.01.2026 13:48] Get page data from previous paper. URL: https://huggingface.co/papers/2601.14232
[21.01.2026 13:48] Get page data from previous paper. URL: https://huggingface.co/papers/2601.11888
[21.01.2026 13:48] Get page data from previous paper. URL: https://huggingface.co/papers/2601.14251
[21.01.2026 13:48] Get page data from previous paper. URL: https://huggingface.co/papers/2601.14046
[21.01.2026 13:48] Get page data from previous paper. URL: https://huggingface.co/papers/2601.13976
[21.01.2026 13:48] Get page data from previous paper. URL: https://huggingface.co/papers/2601.13288
[21.01.2026 13:48] Get page data from previous paper. URL: https://huggingface.co/papers/2601.12937
[21.01.2026 13:48] Get page data from previous paper. URL: https://huggingface.co/papers/2601.10237
[21.01.2026 13:48] Get page data from previous paper. URL: https://huggingface.co/papers/2601.13075
[21.01.2026 13:48] Get page data from previous paper. URL: https://huggingface.co/papers/2601.12910
[21.01.2026 13:48] Get page data from previous paper. URL: https://huggingface.co/papers/2601.10700
[21.01.2026 13:48] Get page data from previous paper. URL: https://huggingface.co/papers/2601.13253
[21.01.2026 13:48] Get page data from previous paper. URL: https://huggingface.co/papers/2601.13251
[21.01.2026 13:48] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[21.01.2026 13:48] No deleted papers detected.
[21.01.2026 13:48] Downloading and parsing papers (pdf, html). Total: 23.
[21.01.2026 13:48] Downloading and parsing paper https://huggingface.co/papers/2601.11655.
[21.01.2026 13:48] Extra JSON file exists (./assets/json/2601.11655.json), skip PDF parsing.
[21.01.2026 13:48] Paper image links file exists (./assets/img_data/2601.11655.json), skip HTML parsing.
[21.01.2026 13:48] Success.
[21.01.2026 13:48] Downloading and parsing paper https://huggingface.co/papers/2601.12993.
[21.01.2026 13:48] Extra JSON file exists (./assets/json/2601.12993.json), skip PDF parsing.
[21.01.2026 13:48] Paper image links file exists (./assets/img_data/2601.12993.json), skip HTML parsing.
[21.01.2026 13:48] Success.
[21.01.2026 13:48] Downloading and parsing paper https://huggingface.co/papers/2601.14192.
[21.01.2026 13:48] Extra JSON file exists (./assets/json/2601.14192.json), skip PDF parsing.
[21.01.2026 13:48] Paper image links file exists (./assets/img_data/2601.14192.json), skip HTML parsing.
[21.01.2026 13:48] Success.
[21.01.2026 13:48] Downloading and parsing paper https://huggingface.co/papers/2601.13836.
[21.01.2026 13:48] Extra JSON file exists (./assets/json/2601.13836.json), skip PDF parsing.
[21.01.2026 13:48] Paper image links file exists (./assets/img_data/2601.13836.json), skip HTML parsing.
[21.01.2026 13:48] Success.
[21.01.2026 13:48] Downloading and parsing paper https://huggingface.co/papers/2601.14250.
[21.01.2026 13:48] Extra JSON file exists (./assets/json/2601.14250.json), skip PDF parsing.
[21.01.2026 13:48] Paper image links file exists (./assets/img_data/2601.14250.json), skip HTML parsing.
[21.01.2026 13:48] Success.
[21.01.2026 13:48] Downloading and parsing paper https://huggingface.co/papers/2601.11969.
[21.01.2026 13:48] Extra JSON file exists (./assets/json/2601.11969.json), skip PDF parsing.
[21.01.2026 13:48] Paper image links file exists (./assets/img_data/2601.11969.json), skip HTML parsing.
[21.01.2026 13:48] Success.
[21.01.2026 13:48] Downloading and parsing paper https://huggingface.co/papers/2601.13029.
[21.01.2026 13:48] Extra JSON file exists (./assets/json/2601.13029.json), skip PDF parsing.
[21.01.2026 13:48] Paper image links file exists (./assets/img_data/2601.13029.json), skip HTML parsing.
[21.01.2026 13:48] Success.
[21.01.2026 13:48] Downloading and parsing paper https://huggingface.co/papers/2601.11522.
[21.01.2026 13:48] Extra JSON file exists (./assets/json/2601.11522.json), skip PDF parsing.
[21.01.2026 13:48] Paper image links file exists (./assets/img_data/2601.11522.json), skip HTML parsing.
[21.01.2026 13:48] Success.
[21.01.2026 13:48] Downloading and parsing paper https://huggingface.co/papers/2601.12294.
[21.01.2026 13:48] Extra JSON file exists (./assets/json/2601.12294.json), skip PDF parsing.
[21.01.2026 13:48] Paper image links file exists (./assets/img_data/2601.12294.json), skip HTML parsing.
[21.01.2026 13:48] Success.
[21.01.2026 13:48] Downloading and parsing paper https://huggingface.co/papers/2601.13247.
[21.01.2026 13:48] Extra JSON file exists (./assets/json/2601.13247.json), skip PDF parsing.
[21.01.2026 13:48] Paper image links file exists (./assets/img_data/2601.13247.json), skip HTML parsing.
[21.01.2026 13:48] Success.
[21.01.2026 13:48] Downloading and parsing paper https://huggingface.co/papers/2601.14232.
[21.01.2026 13:48] Extra JSON file exists (./assets/json/2601.14232.json), skip PDF parsing.
[21.01.2026 13:48] Paper image links file exists (./assets/img_data/2601.14232.json), skip HTML parsing.
[21.01.2026 13:48] Success.
[21.01.2026 13:48] Downloading and parsing paper https://huggingface.co/papers/2601.11888.
[21.01.2026 13:48] Extra JSON file exists (./assets/json/2601.11888.json), skip PDF parsing.
[21.01.2026 13:48] Paper image links file exists (./assets/img_data/2601.11888.json), skip HTML parsing.
[21.01.2026 13:48] Success.
[21.01.2026 13:48] Downloading and parsing paper https://huggingface.co/papers/2601.14251.
[21.01.2026 13:48] Extra JSON file exists (./assets/json/2601.14251.json), skip PDF parsing.
[21.01.2026 13:48] Paper image links file exists (./assets/img_data/2601.14251.json), skip HTML parsing.
[21.01.2026 13:48] Success.
[21.01.2026 13:48] Downloading and parsing paper https://huggingface.co/papers/2601.14046.
[21.01.2026 13:48] Extra JSON file exists (./assets/json/2601.14046.json), skip PDF parsing.
[21.01.2026 13:48] Paper image links file exists (./assets/img_data/2601.14046.json), skip HTML parsing.
[21.01.2026 13:48] Success.
[21.01.2026 13:48] Downloading and parsing paper https://huggingface.co/papers/2601.13976.
[21.01.2026 13:48] Extra JSON file exists (./assets/json/2601.13976.json), skip PDF parsing.
[21.01.2026 13:48] Paper image links file exists (./assets/img_data/2601.13976.json), skip HTML parsing.
[21.01.2026 13:48] Success.
[21.01.2026 13:48] Downloading and parsing paper https://huggingface.co/papers/2601.13288.
[21.01.2026 13:48] Extra JSON file exists (./assets/json/2601.13288.json), skip PDF parsing.
[21.01.2026 13:48] Paper image links file exists (./assets/img_data/2601.13288.json), skip HTML parsing.
[21.01.2026 13:48] Success.
[21.01.2026 13:48] Downloading and parsing paper https://huggingface.co/papers/2601.12937.
[21.01.2026 13:48] Extra JSON file exists (./assets/json/2601.12937.json), skip PDF parsing.
[21.01.2026 13:48] Paper image links file exists (./assets/img_data/2601.12937.json), skip HTML parsing.
[21.01.2026 13:48] Success.
[21.01.2026 13:48] Downloading and parsing paper https://huggingface.co/papers/2601.10237.
[21.01.2026 13:48] Extra JSON file exists (./assets/json/2601.10237.json), skip PDF parsing.
[21.01.2026 13:48] Paper image links file exists (./assets/img_data/2601.10237.json), skip HTML parsing.
[21.01.2026 13:48] Success.
[21.01.2026 13:48] Downloading and parsing paper https://huggingface.co/papers/2601.13075.
[21.01.2026 13:48] Extra JSON file exists (./assets/json/2601.13075.json), skip PDF parsing.
[21.01.2026 13:48] Paper image links file exists (./assets/img_data/2601.13075.json), skip HTML parsing.
[21.01.2026 13:48] Success.
[21.01.2026 13:48] Downloading and parsing paper https://huggingface.co/papers/2601.12910.
[21.01.2026 13:48] Extra JSON file exists (./assets/json/2601.12910.json), skip PDF parsing.
[21.01.2026 13:48] Paper image links file exists (./assets/img_data/2601.12910.json), skip HTML parsing.
[21.01.2026 13:48] Success.
[21.01.2026 13:48] Downloading and parsing paper https://huggingface.co/papers/2601.10700.
[21.01.2026 13:48] Extra JSON file exists (./assets/json/2601.10700.json), skip PDF parsing.
[21.01.2026 13:48] Paper image links file exists (./assets/img_data/2601.10700.json), skip HTML parsing.
[21.01.2026 13:48] Success.
[21.01.2026 13:48] Downloading and parsing paper https://huggingface.co/papers/2601.13253.
[21.01.2026 13:48] Extra JSON file exists (./assets/json/2601.13253.json), skip PDF parsing.
[21.01.2026 13:48] Paper image links file exists (./assets/img_data/2601.13253.json), skip HTML parsing.
[21.01.2026 13:48] Success.
[21.01.2026 13:48] Downloading and parsing paper https://huggingface.co/papers/2601.13251.
[21.01.2026 13:48] Extra JSON file exists (./assets/json/2601.13251.json), skip PDF parsing.
[21.01.2026 13:48] Paper image links file exists (./assets/img_data/2601.13251.json), skip HTML parsing.
[21.01.2026 13:48] Success.
[21.01.2026 13:48] Enriching papers with extra data.
[21.01.2026 13:48] ********************************************************************************
[21.01.2026 13:48] Abstract 0. Large language models face significant challenges in software issue resolution, prompting the development of autonomous coding agents through various training-free and training-based methodologies.  					AI-generated summary 				 Issue resolution, a complex Software Engineering (SWE) task integral t...
[21.01.2026 13:48] ********************************************************************************
[21.01.2026 13:48] Abstract 1. Being-H0.5 is a Vision-Language-Action model that enables robust cross-embodiment generalization through human-centric learning and a Mixture-of-Transformers architecture with specialized embodiment handling.  					AI-generated summary 				 We introduce Being-H0.5, a foundational Vision-Language-Act...
[21.01.2026 13:48] ********************************************************************************
[21.01.2026 13:48] Abstract 2. Efficiency in agentic systems is examined across memory, tool learning, and planning components, analyzing trade-offs between effectiveness and computational costs through various optimization strategies and benchmarks.  					AI-generated summary 				 Recent years have witnessed increasing interest ...
[21.01.2026 13:48] ********************************************************************************
[21.01.2026 13:48] Abstract 3. FutureOmni presents the first benchmark for evaluating multimodal models' ability to forecast future events from audio-visual data, revealing current limitations and proposing an improved training strategy for better performance.  					AI-generated summary 				 Although Multimodal Large Language Mod...
[21.01.2026 13:48] ********************************************************************************
[21.01.2026 13:48] Abstract 4. OmniTransfer presents a unified framework for spatio-temporal video transfer that enhances appearance consistency and temporal control through multi-view information and multimodal semantic guidance.  					AI-generated summary 				 Videos convey richer information than images or text, capturing both...
[21.01.2026 13:48] ********************************************************************************
[21.01.2026 13:48] Abstract 5. A benchmark called MemoryRewardBench is introduced to systematically evaluate reward models' ability to assess long-term memory management in large language models across various context lengths and memory patterns.  					AI-generated summary 				 Existing works increasingly adopt memory-centric mec...
[21.01.2026 13:48] ********************************************************************************
[21.01.2026 13:48] Abstract 6. Think3D enhances vision-language models' 3D reasoning capabilities by enabling interactive spatial exploration through 3D reconstruction and camera-based operations, improving performance without additional training.  					AI-generated summary 				 Understanding and reasoning about the physical worl...
[21.01.2026 13:48] ********************************************************************************
[21.01.2026 13:48] Abstract 7. UniX presents a unified medical foundation model that decouples visual understanding and generation tasks using distinct autoregressive and diffusion branches with cross-modal attention for enhanced performance.  					AI-generated summary 				 Despite recent progress, medical foundation models still...
[21.01.2026 13:48] ********************************************************************************
[21.01.2026 13:48] Abstract 8. ToolPRMBench is introduced as a large-scale benchmark for evaluating process reward models in tool-using agents, featuring step-level test cases and multi-LLM verification to ensure data quality.  					AI-generated summary 				 Reward-guided search methods have demonstrated strong potential in enhan...
[21.01.2026 13:48] ********************************************************************************
[21.01.2026 13:48] Abstract 9. WorldMind addresses the modal disconnect in LLMs by autonomously building a symbolic world knowledge repository that enhances physical feasibility and task optimality through experience-based learning.  					AI-generated summary 				 Current Large Language Models (LLMs) exhibit a critical modal disc...
[21.01.2026 13:48] ********************************************************************************
[21.01.2026 13:48] Abstract 10. KAGE-Env is a JAX-native 2D platformer environment that isolates visual shifts from underlying control problems, enabling systematic analysis of visual generalization in reinforcement learning.  					AI-generated summary 				 Pixel-based reinforcement learning agents often fail under purely visual d...
[21.01.2026 13:48] ********************************************************************************
[21.01.2026 13:48] Abstract 11. A novel retriever training framework for agentic search that uses both local relevance and global answer correctness metrics with iterative optimization between the search agent and retriever.  					AI-generated summary 				 Agentic search has recently emerged as a powerful paradigm, where an agent ...
[21.01.2026 13:48] ********************************************************************************
[21.01.2026 13:48] Abstract 12. LightOnOCR-2-1B is a compact 1B-parameter vision-language model that performs end-to-end document image-to-text conversion with improved localization and robustness through specialized training techniques.  					AI-generated summary 				 We present LightOnOCR-2-1B, a 1B-parameter end-to-end multilin...
[21.01.2026 13:48] ********************************************************************************
[21.01.2026 13:48] Abstract 13. PRiSM benchmark evaluates phonetic perception in speech models through standardized transcription-based metrics and downstream applications across clinical, educational, and multilingual domains.  					AI-generated summary 				 Phone recognition (PR) serves as the atomic interface for language-agnos...
[21.01.2026 13:48] ********************************************************************************
[21.01.2026 13:48] Abstract 14. FantasyVLN presents a unified implicit reasoning framework for vision-and-language navigation that enhances reasoning capabilities without explicit token overhead, achieving real-time performance with improved accuracy.  					AI-generated summary 				 Achieving human-level performance in Vision-and-...
[21.01.2026 13:48] ********************************************************************************
[21.01.2026 13:48] Abstract 15. Lightweight probes trained on hidden states of LLMs enable efficient classification tasks without additional computational overhead, improving safety and sentiment analysis performance.  					AI-generated summary 				 Production LLM systems often rely on separate models for safety and other classifi...
[21.01.2026 13:48] ********************************************************************************
[21.01.2026 13:48] Abstract 16. Membership inference attacks fail to reliably detect copyrighted text usage in large language models when training data is paraphrased using structure-aware methods that preserve semantic content.  					AI-generated summary 				 As large language models (LLMs) are trained on increasingly opaque corp...
[21.01.2026 13:48] ********************************************************************************
[21.01.2026 13:48] Abstract 17. Differentially private stochastic gradient descent with shuffled sampling faces fundamental privacy-utility trade-offs that require substantial noise for meaningful privacy protection, limiting practical performance.  					AI-generated summary 				 Differentially Private Stochastic Gradient Descent ...
[21.01.2026 13:48] ********************************************************************************
[21.01.2026 13:48] Abstract 18. AI mentor METIS outperforms GPT-5 and Claude Sonnet 4.5 in supporting undergraduate research writing across multiple stages, with higher student scores and improved document-grounded outputs, though challenges remain in tool routing and stage classification.  					AI-generated summary 				 Many stud...
[21.01.2026 13:48] ********************************************************************************
[21.01.2026 13:48] Abstract 19. SciCoQA is a dataset for identifying mismatches between scientific publications and code implementations, containing 611 discrepancies across multiple disciplines and demonstrating the challenge of detecting such issues even for advanced language models.  					AI-generated summary 				 We present Sc...
[21.01.2026 13:48] ********************************************************************************
[21.01.2026 13:48] Abstract 20. A framework for generating structured counterfactual pairs using LLMs and SCMs enables improved evaluation and analysis of concept-based explanations in high-stakes domains.  					AI-generated summary 				 Concept-based explanations quantify how high-level concepts (e.g., gender or experience) influ...
[21.01.2026 13:48] ********************************************************************************
[21.01.2026 13:48] Abstract 21. A hybrid methodology combining FastText embeddings, clustering, and AI classification generates a large-scale Turkish semantic relations dataset with high accuracy validation.  					AI-generated summary 				 We present a hybrid methodology for generating large-scale semantic relationship datasets in...
[21.01.2026 13:48] ********************************************************************************
[21.01.2026 13:48] Abstract 22. A large-scale semantic clustering system addresses the limitation of neural embeddings in distinguishing synonyms from antonyms through a specialized three-way discriminator and novel clustering algorithm.  					AI-generated summary 				 Neural embeddings have a notorious blind spot: they can't reli...
[21.01.2026 13:48] Read previous papers.
[21.01.2026 13:48] Generating reviews via LLM API.
[21.01.2026 13:48] Using data from previous issue: {"categories": ["#rl", "#benchmark", "#dataset", "#agents", "#data", "#training", "#open_source", "#plp", "#survey"], "emoji": "ü§ñ", "ru": {"title": "–ê–≤—Ç–æ–Ω–æ–º–Ω—ã–µ –∞–≥–µ–Ω—Ç—ã –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–≥—Ä–∞–º–º–Ω—ã—Ö –æ—à–∏–±–æ–∫", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç—Å—è —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –æ–±–∑–æ—Ä –∞–≤—Ç–æ–Ω–æ–º–Ω—ã—Ö –∫–æ–¥–∏—Ä—É—é—â–∏—Ö –∞–≥–µ–Ω
[21.01.2026 13:48] Using data from previous issue: {"categories": ["#architecture", "#dataset", "#training", "#multimodal", "#robotics"], "emoji": "ü§ñ", "ru": {"title": "–ß–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–π —è–∑—ã–∫ –¥–ª—è –≤—Å–µ—Ö —Ä–æ–±–æ—Ç–æ–≤: universal VLA —Å –∫—Ä–æ—Å—Å–ø–ª–∞—Ç—Ñ–æ—Ä–º–µ–Ω–Ω—ã–º –æ–±–æ–±—â–µ–Ω–∏–µ–º", "desc": "Being-H0.5 ‚Äî —ç—Ç–æ –º–æ–¥–µ–ª—å Vision-Language-Action (VLA), —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –¥–ª—è –æ–±–æ–±—â–µ–Ω–∏—è —Ä–æ–±–æ—Ç–æ—Ç
[21.01.2026 13:48] Using data from previous issue: {"categories": ["#rl", "#optimization", "#long_context", "#agents", "#benchmark"], "emoji": "‚ö°", "ru": {"title": "–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º: –±–∞–ª–∞–Ω—Å –º–µ–∂–¥—É –∫–∞—á–µ—Å—Ç–≤–æ–º –∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–π —Å—Ç–æ–∏–º–æ—Å—Ç—å—é", "desc": "–í —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∞–Ω–∞–ª–∏–∑–∏—Ä
[21.01.2026 13:48] Using data from previous issue: {"categories": ["#audio", "#video", "#benchmark", "#dataset", "#training", "#multimodal"], "emoji": "üîÆ", "ru": {"title": "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –±—É–¥—É—â–µ–≥–æ: —É—á–∏–º –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≤–∏–¥–µ—Ç—å –∑–∞–≤—Ç—Ä–∞", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ FutureOmni ‚Äî –ø–µ—Ä–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –ø—Ä–µ–¥—Å–∫–∞
[21.01.2026 13:48] Using data from previous issue: {"categories": ["#video", "#transfer_learning", "#architecture", "#multimodal"], "emoji": "üé¨", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π —Ç—Ä–∞–Ω—Å—Ñ–µ—Ä –≤–∏–¥–µ–æ —á–µ—Ä–µ–∑ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ-–≤—Ä–µ–º–µ–Ω–Ω–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ", "desc": "OmniTransfer –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –µ–¥–∏–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Ç—Ä–∞–Ω—Å—Ñ–µ—Ä–∞ –≤–∏–¥–µ–æ, –∫–æ—Ç–æ—Ä—ã–π —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –ø—Ä–æ—Å
[21.01.2026 13:48] Using data from previous issue: {"categories": ["#long_context", "#survey", "#benchmark", "#dataset"], "emoji": "üß†", "ru": {"title": "–û—Ü–µ–Ω–∫–∞ –ø–∞–º—è—Ç–∏: –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω MemoryRewardBench ‚Äî –ø–µ—Ä–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ
[21.01.2026 13:48] Using data from previous issue: {"categories": ["#open_source", "#reasoning"], "emoji": "üßä", "ru": {"title": "–¢—Ä—ë—Ö–º–µ—Ä–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ –¥–ª—è –≤–∏–¥–µ–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "Think3D ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –∫–æ—Ç–æ—Ä—ã–π —Ä–∞—Å—à–∏—Ä—è–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –≤–∏–¥–µ–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ç—Ä—ë—Ö–º–µ—Ä–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, –ø–æ–∑–≤–æ–ª—è—è –∏–º –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç—å –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ —á–µ—Ä–µ–∑ 3
[21.01.2026 13:48] Using data from previous issue: {"categories": ["#diffusion", "#training", "#data", "#architecture", "#open_source", "#multimodal", "#healthcare"], "emoji": "ü´Ä", "ru": {"title": "–°–∏–Ω–µ—Ä–≥–∏—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ —Å–æ–∑–¥–∞–Ω–∏—è: —Ä–∞–∑–¥–µ–ª—ë–Ω–Ω—ã–µ –≤–µ—Ç–≤–∏ –¥–ª—è —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞", "desc": "UniX ‚Äî —ç—Ç–æ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∞—è –º–æ–¥–µ–ª—å-—Ñ—É–Ω–¥–∞
[21.01.2026 13:48] Using data from previous issue: {"categories": ["#agents", "#benchmark", "#dataset", "#rl"], "emoji": "üîß", "ru": {"title": "–°–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ—Ü–µ–Ω–∏–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –¥–ª—è –∞–≥–µ–Ω—Ç–æ–≤ —Å –¥–æ—Å—Ç—É–ø–æ–º –∫ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ ToolPRMBench ‚Äî –º–∞—Å—à—Ç–∞–±–Ω–∞—è –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –Ω–∞ —É—Ä–æ–≤–Ω–µ —à–∞–≥–æ–≤
[21.01.2026 13:48] Using data from previous issue: {"categories": ["#transfer_learning", "#alignment", "#hallucinations", "#reasoning"], "emoji": "üåç", "ru": {"title": "–ì—Äounded –º–∏—Ä –¥–ª—è LLM: –æ—Ç –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π –∫ —Ñ–∏–∑–∏—á–µ—Å–∫–∏ –æ—Å—É—â–µ—Å—Ç–≤–∏–º—ã–º –ø–ª–∞–Ω–∞–º", "desc": "WorldMind —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –º–æ–¥–∞–ª—å–Ω–æ–π —Ä–∞—Å—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö –ø—É—Ç—ë–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥
[21.01.2026 13:48] Using data from previous issue: {"categories": ["#optimization", "#cv", "#games", "#benchmark", "#rl"], "emoji": "üéÆ", "ru": {"title": "–ß–∏—Å—Ç—ã–π –∞–Ω–∞–ª–∏–∑ –≤–∏–∑—É–∞–ª—å–Ω–æ–π –æ–±–æ–±—â–∞–µ–º–æ—Å—Ç–∏ –≤ –æ–±—É—á–µ–Ω–∏–∏ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º —á–µ—Ä–µ–∑ —Ñ–∞–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—é –Ω–∞–±–ª—é–¥–µ–Ω–∏–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ KAGE-Env ‚Äî —Å—Ä–µ–¥–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –Ω–∞ JAX, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–∑–≤–æ–ª—è
[21.01.2026 13:48] Using data from previous issue: {"categories": ["#agents", "#benchmark", "#training", "#rag"], "emoji": "üîÑ", "ru": {"title": "–î–≤—É—Å—Ç–æ—Ä–æ–Ω–Ω—è—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∞–≥–µ–Ω—Ç–∞ –ø–æ–∏—Å–∫–∞ –∏ –ø–æ–∏—Å–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è —Ç–æ—á–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –Ω–æ–≤–∞—èÊ°ÜÊû∂ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è retriever'–∞, –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–Ω–∞—è —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∞–≥–µ–Ω—Ç–æ–≤. 
[21.01.2026 13:48] Using data from previous issue: {"categories": ["#training", "#multimodal", "#small_models", "#dataset", "#cv", "#multilingual", "#benchmark"], "emoji": "üìÑ", "ru": {"title": "–õ—ë–≥–∫–∞—è –∏ –±—ã—Å—Ç—Ä–∞—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö —Å –µ–¥–∏–Ω–æ–π –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç—å—é", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω LightOnOCR-2-1B ‚Äî –∫–æ–º–ø–∞–∫—Ç–Ω–∞—è vision-language –º–æ–¥–µ–ª—å —Å 1 –º
[21.01.2026 13:48] Using data from previous issue: {"categories": ["#open_source", "#audio", "#dataset", "#low_resource", "#multilingual", "#benchmark"], "emoji": "üî§", "ru": {"title": "–ì–ª—É–±–æ–∫–∞—è –æ—Ü–µ–Ω–∫–∞ —Ñ–æ–Ω–µ—Ç–∏—á–µ—Å–∫–æ–≥–æ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è –≤ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö —Ä–µ—á–µ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –±–µ–Ω—á–º–∞—Ä–∫ PRiSM –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è —Ñ–æ–Ω–µ—Ç–∏–∫–∏ –≤ —Å–∏—Å—Ç–µ–º–∞—Ö —Ä–∞—Å–ø–æ
[21.01.2026 13:48] Using data from previous issue: {"categories": ["#training", "#agents", "#interpretability", "#multimodal", "#reasoning", "#architecture"], "emoji": "üß≠", "ru": {"title": "–°–∫—Ä—ã—Ç–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –Ω–∞–≤–∏–≥–∞—Ü–∏–∏ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏", "desc": "FantasyVLN –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–ª—è –Ω–∞–≤–∏–≥–∞—Ü–∏–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∑—Ä–µ–Ω–∏—è
[21.01.2026 13:48] Using data from previous issue: {"categories": ["#training", "#inference", "#small_models"], "emoji": "üîç", "ru": {"title": "–õ—ë–≥–∫–∏–µ –∑–æ–Ω–¥—ã –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏: –ø–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π LLM –±–µ–∑ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è –ª—ë–≥–∫–∏—Ö –∑–æ–Ω–¥–æ–≤ (probes) –Ω–∞ —Å–∫—Ä—ã—Ç—ã—Ö —Å–æ—Å—Ç–æ—è–Ω–∏—è—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –≤—ã
[21.01.2026 13:48] Using data from previous issue: {"categories": ["#training", "#security", "#leakage"], "emoji": "üîì", "ru": {"title": "–•—Ä—É–ø–∫–æ—Å—Ç—å –∞—Ç–∞–∫ –≤—ã–≤–æ–¥–∞ —á–ª–µ–Ω—Å—Ç–≤–∞ –ø–µ—Ä–µ–¥ —Å–µ–º–∞–Ω—Ç–∏–∫–æ-—Å–æ—Ö—Ä–∞–Ω—è—é—â–∏–º –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∞–Ω–∏–µ–º", "desc": "–í —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –Ω–∞–¥—ë–∂–Ω–æ—Å—Ç—å –∞—Ç–∞–∫ –≤—ã–≤–æ–¥–∞ —á–ª–µ–Ω—Å—Ç–≤–∞ (MIA) –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∑–∞—â–∏—â—ë–Ω–Ω–æ–≥–æ –∞–≤—Ç–æ—Ä—Å–∫–∏–º –ø—Ä–∞–≤–æ–º —Ç–µ–∫—Å—Ç–∞ –ø
[21.01.2026 13:48] Using data from previous issue: {"categories": ["#security", "#optimization", "#training", "#math"], "emoji": "üîê", "ru": {"title": "–§—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π –∫–æ–º–ø—Ä–æ–º–∏—Å—Å –º–µ–∂–¥—É –ø—Ä–∏–≤–∞—Ç–Ω–æ—Å—Ç—å—é –∏ —Ç–æ—á–Ω–æ—Å—Ç—å—é –≤ DP-SGD", "desc": "–í —Å—Ç–∞—Ç—å–µ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç—Å—è –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞–ª—å–Ω–æ –ø—Ä–∏–≤–∞—Ç–Ω—ã–π —Å—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–∏–π –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫ (DP-SGD) –≤ —Ä–∞–º–∫–∞—Ö f-–¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–π –ø—Ä–∏
[21.01.2026 13:48] Using data from previous issue: {"categories": ["#benchmark", "#training", "#agents", "#science"], "emoji": "üìö", "ru": {"title": "–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ò–ò-–Ω–∞—Å—Ç–∞–≤–Ω–∏–∫ –¥–ª—è –Ω–∞—É—á–Ω–æ–≥–æ –ø–∏—Å—å–º–∞ —Å—Ç—É–¥–µ–Ω—Ç–æ–≤", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ METIS ‚Äî –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –Ω–∞—Å—Ç–∞–≤–Ω–∏–∫, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–º–æ–≥–∞–µ—Ç —Å—Ç—É–¥–µ–Ω—Ç–∞–º –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —ç—Ç–∞–ø–∞—Ö –Ω–∞–ø–∏—Å–∞–Ω–∏—è –∏—Å—Å–ª–µ–¥–æ–≤–∞
[21.01.2026 13:48] Using data from previous issue: {"categories": ["#long_context", "#hallucinations", "#synthetic", "#science"], "emoji": "üîç", "ru": {"title": "–í—ã—è–≤–ª–µ–Ω–∏–µ —Å–∫—Ä—ã—Ç—ã—Ö –æ—à–∏–±–æ–∫: –∫–æ–≥–¥–∞ –∫–æ–¥ –Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –Ω–∞—É—á–Ω–æ–π —Å—Ç–∞—Ç—å–µ", "desc": "SciCoQA ‚Äî —ç—Ç–æ –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–π –º–µ–∂–¥—É –æ–ø–∏—Å–∞–Ω–∏–µ–º –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –≤ –Ω–∞—É—á–Ω—ã—Ö –ø—É–±–ª–∏–∫–∞—Ü–∏—è—Ö –∏ –∏—Ö —Ä–µ–∞–ª–∏–∑–∞—Ü–∏
[21.01.2026 13:48] Using data from previous issue: {"categories": ["#healthcare", "#interpretability", "#ethics", "#benchmark", "#dataset", "#synthetic"], "emoji": "üîç", "ru": {"title": "–û–±—ä–µ–∫—Ç–∏–≤–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±—ä—è—Å–Ω–∏–º–æ—Å—Ç–∏ —á–µ—Ä–µ–∑ –ø—Ä–∏—á–∏–Ω–Ω—ã–µ –∫–æ–Ω—Ç—Ä—Ñ–∞–∫—Ç—ã", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç LIBERTy ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ —Å –∫–æ–Ω—Ç—Ä—Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–º–∏ –ø–∞—Ä–∞–º–∏
[21.01.2026 13:48] Using data from previous issue: {"categories": ["#multilingual", "#data", "#low_resource", "#dataset", "#open_source", "#synthetic"], "emoji": "üáπüá∑", "ru": {"title": "–ú–∞—Å—à—Ç–∞–±–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –æ—Ç–Ω–æ—à–µ–Ω–∏–π —Ç—É—Ä–µ—Ü–∫–æ–≥–æ —è–∑—ã–∫–∞ —á–µ—Ä–µ–∑ –≥–∏–±—Ä–∏–¥–Ω—É—é –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—é", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –≥–∏–±—Ä–∏–¥–Ω—ã–π –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—é –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –±–æ–ª
[21.01.2026 13:48] Using data from previous issue: {"categories": [], "emoji": "üéØ", "ru": {"title": "–†–∞–∑–ª–∏—á–∏–µ —Å–∏–Ω–æ–Ω–∏–º–æ–≤ –∏ –∞–Ω—Ç–æ–Ω–∏–º–æ–≤: —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –±–µ–∑ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –¥—Ä–µ–π—Ñ–∞", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –Ω–µ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Ä–∞–∑–ª–∏—á–∞—Ç—å —Å–∏–Ω–æ–Ω–∏–º—ã –∏ –∞–Ω—Ç–æ–Ω–∏–º—ã. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—É—é —Å–∏—Å—Ç–µ–º—É —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π
[21.01.2026 13:48] Renaming data file.
[21.01.2026 13:48] Renaming previous data. hf_papers.json to ./d/2026-01-21.json
[21.01.2026 13:48] Saving new data file.
[21.01.2026 13:48] Generating page.
[21.01.2026 13:48] Renaming previous page.
[21.01.2026 13:48] Renaming previous data. index.html to ./d/2026-01-21.html
[21.01.2026 13:48] Writing result.
[21.01.2026 13:48] Renaming log file.
[21.01.2026 13:48] Renaming previous data. log.txt to ./logs/2026-01-21_last_log.txt
