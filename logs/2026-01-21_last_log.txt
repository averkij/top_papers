[21.01.2026 06:40] Read previous papers.
[21.01.2026 06:40] Generating top page (month).
[21.01.2026 06:40] Writing top page (month).
[21.01.2026 07:30] Read previous papers.
[21.01.2026 07:30] Get feed.
[21.01.2026 07:30] Get page data from previous paper. URL: https://huggingface.co/papers/2601.11655
[21.01.2026 07:30] Get page data from previous paper. URL: https://huggingface.co/papers/2601.11969
[21.01.2026 07:30] Get page data from previous paper. URL: https://huggingface.co/papers/2601.11522
[21.01.2026 07:30] Get page data from previous paper. URL: https://huggingface.co/papers/2601.12993
[21.01.2026 07:30] Get page data from previous paper. URL: https://huggingface.co/papers/2601.14250
[21.01.2026 07:30] Get page data from previous paper. URL: https://huggingface.co/papers/2601.12294
[21.01.2026 07:30] Get page data from previous paper. URL: https://huggingface.co/papers/2601.13247
[21.01.2026 07:30] Get page data from previous paper. URL: https://huggingface.co/papers/2601.14192
[21.01.2026 07:30] Get page data from previous paper. URL: https://huggingface.co/papers/2601.14046
[21.01.2026 07:30] Extract page data from URL. URL: https://huggingface.co/papers/2601.13836
[21.01.2026 07:30] Get page data from previous paper. URL: https://huggingface.co/papers/2601.13288
[21.01.2026 07:30] Extract page data from URL. URL: https://huggingface.co/papers/2601.12910
[21.01.2026 07:30] Extract page data from URL. URL: https://huggingface.co/papers/2601.11888
[21.01.2026 07:30] Get page data from previous paper. URL: https://huggingface.co/papers/2601.10700
[21.01.2026 07:30] Get page data from previous paper. URL: https://huggingface.co/papers/2601.13253
[21.01.2026 07:30] Get page data from previous paper. URL: https://huggingface.co/papers/2601.13251
[21.01.2026 07:30] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[21.01.2026 07:30] No deleted papers detected.
[21.01.2026 07:30] Downloading and parsing papers (pdf, html). Total: 16.
[21.01.2026 07:30] Downloading and parsing paper https://huggingface.co/papers/2601.11655.
[21.01.2026 07:30] Extra JSON file exists (./assets/json/2601.11655.json), skip PDF parsing.
[21.01.2026 07:30] Paper image links file exists (./assets/img_data/2601.11655.json), skip HTML parsing.
[21.01.2026 07:30] Success.
[21.01.2026 07:30] Downloading and parsing paper https://huggingface.co/papers/2601.11969.
[21.01.2026 07:30] Extra JSON file exists (./assets/json/2601.11969.json), skip PDF parsing.
[21.01.2026 07:30] Paper image links file exists (./assets/img_data/2601.11969.json), skip HTML parsing.
[21.01.2026 07:30] Success.
[21.01.2026 07:30] Downloading and parsing paper https://huggingface.co/papers/2601.11522.
[21.01.2026 07:30] Extra JSON file exists (./assets/json/2601.11522.json), skip PDF parsing.
[21.01.2026 07:30] Paper image links file exists (./assets/img_data/2601.11522.json), skip HTML parsing.
[21.01.2026 07:30] Success.
[21.01.2026 07:30] Downloading and parsing paper https://huggingface.co/papers/2601.12993.
[21.01.2026 07:30] Extra JSON file exists (./assets/json/2601.12993.json), skip PDF parsing.
[21.01.2026 07:30] Paper image links file exists (./assets/img_data/2601.12993.json), skip HTML parsing.
[21.01.2026 07:30] Success.
[21.01.2026 07:30] Downloading and parsing paper https://huggingface.co/papers/2601.14250.
[21.01.2026 07:30] Extra JSON file exists (./assets/json/2601.14250.json), skip PDF parsing.
[21.01.2026 07:30] Paper image links file exists (./assets/img_data/2601.14250.json), skip HTML parsing.
[21.01.2026 07:30] Success.
[21.01.2026 07:30] Downloading and parsing paper https://huggingface.co/papers/2601.12294.
[21.01.2026 07:30] Extra JSON file exists (./assets/json/2601.12294.json), skip PDF parsing.
[21.01.2026 07:30] Paper image links file exists (./assets/img_data/2601.12294.json), skip HTML parsing.
[21.01.2026 07:30] Success.
[21.01.2026 07:30] Downloading and parsing paper https://huggingface.co/papers/2601.13247.
[21.01.2026 07:30] Extra JSON file exists (./assets/json/2601.13247.json), skip PDF parsing.
[21.01.2026 07:30] Paper image links file exists (./assets/img_data/2601.13247.json), skip HTML parsing.
[21.01.2026 07:30] Success.
[21.01.2026 07:30] Downloading and parsing paper https://huggingface.co/papers/2601.14192.
[21.01.2026 07:30] Extra JSON file exists (./assets/json/2601.14192.json), skip PDF parsing.
[21.01.2026 07:30] Paper image links file exists (./assets/img_data/2601.14192.json), skip HTML parsing.
[21.01.2026 07:30] Success.
[21.01.2026 07:30] Downloading and parsing paper https://huggingface.co/papers/2601.14046.
[21.01.2026 07:30] Extra JSON file exists (./assets/json/2601.14046.json), skip PDF parsing.
[21.01.2026 07:30] Paper image links file exists (./assets/img_data/2601.14046.json), skip HTML parsing.
[21.01.2026 07:30] Success.
[21.01.2026 07:30] Downloading and parsing paper https://huggingface.co/papers/2601.13836.
[21.01.2026 07:30] Downloading paper 2601.13836 from https://arxiv.org/pdf/2601.13836v1...
[21.01.2026 07:30] Extracting affiliations from text.
[21.01.2026 07:30] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 0 2 ] . [ 1 6 3 8 3 1 . 1 0 6 2 : r FutureOmni: Evaluating Future Forecasting from Omni-Modal Context for Multimodal LLMs Qian Chen1 Jinlan Fu1,3, Changsong Li1,2 See-Kiong Ng3 Xipeng Qiu1,2 1Fudan University, 2Shanghai Innovation Institute, 3National University of Singapore "
[21.01.2026 07:30] Response: ```python
['Fudan University', 'Shanghai Innovation Institute', 'National University of Singapore']
```
[21.01.2026 07:30] Deleting PDF ./assets/pdf/2601.13836.pdf.
[21.01.2026 07:30] Success.
[21.01.2026 07:30] Downloading and parsing paper https://huggingface.co/papers/2601.13288.
[21.01.2026 07:30] Extra JSON file exists (./assets/json/2601.13288.json), skip PDF parsing.
[21.01.2026 07:30] Paper image links file exists (./assets/img_data/2601.13288.json), skip HTML parsing.
[21.01.2026 07:30] Success.
[21.01.2026 07:30] Downloading and parsing paper https://huggingface.co/papers/2601.12910.
[21.01.2026 07:30] Downloading paper 2601.12910 from https://arxiv.org/pdf/2601.12910v1...
[21.01.2026 07:30] Extracting affiliations from text.
[21.01.2026 07:30] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"SCICOQA: Quality Assurance for Scientific PaperCode Alignment Tim Baumg√§rtner and Iryna Gurevych Ubiquitous Knowledge Processing Lab (UKP Lab), Department of Computer Science, TU Darmstadt and National Research Center for Applied Cybersecurity ATHENE Code: https://github.com/ukplab/scicoqa Data: https://hf.co/datasets/ukplab/scicoqa 6 2 0 2 9 ] . [ 1 0 1 9 2 1 . 1 0 6 2 : r a "
[21.01.2026 07:30] Response: ```python
[
    "Ubiquitous Knowledge Processing Lab (UKP Lab), Department of Computer Science, TU Darmstadt",
    "National Research Center for Applied Cybersecurity ATHENE"
]
```
[21.01.2026 07:30] Deleting PDF ./assets/pdf/2601.12910.pdf.
[21.01.2026 07:30] Success.
[21.01.2026 07:30] Downloading and parsing paper https://huggingface.co/papers/2601.11888.
[21.01.2026 07:30] Downloading paper 2601.11888 from https://arxiv.org/pdf/2601.11888v1...
[21.01.2026 07:31] Extracting affiliations from text.
[21.01.2026 07:31] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Agentic-R: Learning to Retrieve for Agentic Search Wenhan Liu1, Xinyu Ma2, Yutao Zhu1, Yuchen Li2, Daiting Shi2 Dawei Yin2 and Zhicheng Dou1* 1Gaoling School of Artificial Intelligence, Renmin University of China 2Baidu Inc., Beijing, China lwh@ruc.edu.cn, xinyuma2016@gmail.com, dou@ruc.edu.cn 6 2 0 2 7 1 ] . [ 1 8 8 8 1 1 . 1 0 6 2 : r a "
[21.01.2026 07:31] Response: ```python
[
    "Gaoling School of Artificial Intelligence, Renmin University of China",
    "Baidu Inc., Beijing, China"
]
```
[21.01.2026 07:31] Deleting PDF ./assets/pdf/2601.11888.pdf.
[21.01.2026 07:31] Success.
[21.01.2026 07:31] Downloading and parsing paper https://huggingface.co/papers/2601.10700.
[21.01.2026 07:31] Extra JSON file exists (./assets/json/2601.10700.json), skip PDF parsing.
[21.01.2026 07:31] Paper image links file exists (./assets/img_data/2601.10700.json), skip HTML parsing.
[21.01.2026 07:31] Success.
[21.01.2026 07:31] Downloading and parsing paper https://huggingface.co/papers/2601.13253.
[21.01.2026 07:31] Extra JSON file exists (./assets/json/2601.13253.json), skip PDF parsing.
[21.01.2026 07:31] Paper image links file exists (./assets/img_data/2601.13253.json), skip HTML parsing.
[21.01.2026 07:31] Success.
[21.01.2026 07:31] Downloading and parsing paper https://huggingface.co/papers/2601.13251.
[21.01.2026 07:31] Extra JSON file exists (./assets/json/2601.13251.json), skip PDF parsing.
[21.01.2026 07:31] Paper image links file exists (./assets/img_data/2601.13251.json), skip HTML parsing.
[21.01.2026 07:31] Success.
[21.01.2026 07:31] Enriching papers with extra data.
[21.01.2026 07:31] ********************************************************************************
[21.01.2026 07:31] Abstract 0. Large language models face significant challenges in software issue resolution, prompting the development of autonomous coding agents through various training-free and training-based methodologies.  					AI-generated summary 				 Issue resolution, a complex Software Engineering (SWE) task integral t...
[21.01.2026 07:31] ********************************************************************************
[21.01.2026 07:31] Abstract 1. A benchmark called MemoryRewardBench is introduced to systematically evaluate reward models' ability to assess long-term memory management in large language models across various context lengths and memory patterns.  					AI-generated summary 				 Existing works increasingly adopt memory-centric mec...
[21.01.2026 07:31] ********************************************************************************
[21.01.2026 07:31] Abstract 2. UniX presents a unified medical foundation model that decouples visual understanding and generation tasks using distinct autoregressive and diffusion branches with cross-modal attention for enhanced performance.  					AI-generated summary 				 Despite recent progress, medical foundation models still...
[21.01.2026 07:31] ********************************************************************************
[21.01.2026 07:31] Abstract 3. Being-H0.5 is a Vision-Language-Action model that enables robust cross-embodiment generalization through human-centric learning and a Mixture-of-Transformers architecture with specialized embodiment handling.  					AI-generated summary 				 We introduce Being-H0.5, a foundational Vision-Language-Act...
[21.01.2026 07:31] ********************************************************************************
[21.01.2026 07:31] Abstract 4. OmniTransfer presents a unified framework for spatio-temporal video transfer that enhances appearance consistency and temporal control through multi-view information and multimodal semantic guidance.  					AI-generated summary 				 Videos convey richer information than images or text, capturing both...
[21.01.2026 07:31] ********************************************************************************
[21.01.2026 07:31] Abstract 5. ToolPRMBench is introduced as a large-scale benchmark for evaluating process reward models in tool-using agents, featuring step-level test cases and multi-LLM verification to ensure data quality.  					AI-generated summary 				 Reward-guided search methods have demonstrated strong potential in enhan...
[21.01.2026 07:31] ********************************************************************************
[21.01.2026 07:31] Abstract 6. WorldMind addresses the modal disconnect in LLMs by autonomously building a symbolic world knowledge repository that enhances physical feasibility and task optimality through experience-based learning.  					AI-generated summary 				 Current Large Language Models (LLMs) exhibit a critical modal disc...
[21.01.2026 07:31] ********************************************************************************
[21.01.2026 07:31] Abstract 7. Efficiency in agentic systems is examined across memory, tool learning, and planning components, analyzing trade-offs between effectiveness and computational costs through various optimization strategies and benchmarks.  					AI-generated summary 				 Recent years have witnessed increasing interest ...
[21.01.2026 07:31] ********************************************************************************
[21.01.2026 07:31] Abstract 8. PRiSM benchmark evaluates phonetic perception in speech models through standardized transcription-based metrics and downstream applications across clinical, educational, and multilingual domains.  					AI-generated summary 				 Phone recognition (PR) serves as the atomic interface for language-agnos...
[21.01.2026 07:31] ********************************************************************************
[21.01.2026 07:31] Abstract 9. FutureOmni presents the first benchmark for evaluating multimodal models' ability to forecast future events from audio-visual data, revealing current limitations and proposing an improved training strategy for better performance.  					AI-generated summary 				 Although Multimodal Large Language Mod...
[21.01.2026 07:31] ********************************************************************************
[21.01.2026 07:31] Abstract 10. Lightweight probes trained on hidden states of LLMs enable efficient classification tasks without additional computational overhead, improving safety and sentiment analysis performance.  					AI-generated summary 				 Production LLM systems often rely on separate models for safety and other classifi...
[21.01.2026 07:31] ********************************************************************************
[21.01.2026 07:31] Abstract 11. SciCoQA is a dataset for identifying mismatches between scientific publications and code implementations, containing 611 discrepancies across multiple disciplines and demonstrating the challenge of detecting such issues even for advanced language models.  					AI-generated summary 				 We present Sc...
[21.01.2026 07:31] ********************************************************************************
[21.01.2026 07:31] Abstract 12. A novel retriever training framework for agentic search that uses both local relevance and global answer correctness metrics with iterative optimization between the search agent and retriever.  					AI-generated summary 				 Agentic search has recently emerged as a powerful paradigm, where an agent ...
[21.01.2026 07:31] ********************************************************************************
[21.01.2026 07:31] Abstract 13. A framework for generating structured counterfactual pairs using LLMs and SCMs enables improved evaluation and analysis of concept-based explanations in high-stakes domains.  					AI-generated summary 				 Concept-based explanations quantify how high-level concepts (e.g., gender or experience) influ...
[21.01.2026 07:31] ********************************************************************************
[21.01.2026 07:31] Abstract 14. A hybrid methodology combining FastText embeddings, clustering, and AI classification generates a large-scale Turkish semantic relations dataset with high accuracy validation.  					AI-generated summary 				 We present a hybrid methodology for generating large-scale semantic relationship datasets in...
[21.01.2026 07:31] ********************************************************************************
[21.01.2026 07:31] Abstract 15. A large-scale semantic clustering system addresses the limitation of neural embeddings in distinguishing synonyms from antonyms through a specialized three-way discriminator and novel clustering algorithm.  					AI-generated summary 				 Neural embeddings have a notorious blind spot: they can't reli...
[21.01.2026 07:31] Read previous papers.
[21.01.2026 07:31] Generating reviews via LLM API.
[21.01.2026 07:31] Using data from previous issue: {"categories": ["#rl", "#benchmark", "#dataset", "#agents", "#data", "#training", "#open_source", "#plp", "#survey"], "emoji": "ü§ñ", "ru": {"title": "–ê–≤—Ç–æ–Ω–æ–º–Ω—ã–µ –∞–≥–µ–Ω—Ç—ã –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–≥—Ä–∞–º–º–Ω—ã—Ö –æ—à–∏–±–æ–∫", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç—Å—è —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –æ–±–∑–æ—Ä –∞–≤—Ç–æ–Ω–æ–º–Ω—ã—Ö –∫–æ–¥–∏—Ä—É—é—â–∏—Ö –∞–≥–µ–Ω
[21.01.2026 07:31] Using data from previous issue: {"categories": ["#long_context", "#survey", "#benchmark", "#dataset"], "emoji": "üß†", "ru": {"title": "–û—Ü–µ–Ω–∫–∞ –ø–∞–º—è—Ç–∏: –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω MemoryRewardBench ‚Äî –ø–µ—Ä–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ
[21.01.2026 07:31] Using data from previous issue: {"categories": ["#diffusion", "#training", "#data", "#architecture", "#open_source", "#multimodal", "#healthcare"], "emoji": "ü´Ä", "ru": {"title": "–°–∏–Ω–µ—Ä–≥–∏—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ —Å–æ–∑–¥–∞–Ω–∏—è: —Ä–∞–∑–¥–µ–ª—ë–Ω–Ω—ã–µ –≤–µ—Ç–≤–∏ –¥–ª—è —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞", "desc": "UniX ‚Äî —ç—Ç–æ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∞—è –º–æ–¥–µ–ª—å-—Ñ—É–Ω–¥–∞
[21.01.2026 07:31] Using data from previous issue: {"categories": ["#architecture", "#dataset", "#training", "#multimodal", "#robotics"], "emoji": "ü§ñ", "ru": {"title": "–ß–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–π —è–∑—ã–∫ –¥–ª—è –≤—Å–µ—Ö —Ä–æ–±–æ—Ç–æ–≤: universal VLA —Å –∫—Ä–æ—Å—Å–ø–ª–∞—Ç—Ñ–æ—Ä–º–µ–Ω–Ω—ã–º –æ–±–æ–±—â–µ–Ω–∏–µ–º", "desc": "Being-H0.5 ‚Äî —ç—Ç–æ –º–æ–¥–µ–ª—å Vision-Language-Action (VLA), —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –¥–ª—è –æ–±–æ–±—â–µ–Ω–∏—è —Ä–æ–±–æ—Ç–æ—Ç
[21.01.2026 07:31] Using data from previous issue: {"categories": ["#video", "#transfer_learning", "#architecture", "#multimodal"], "emoji": "üé¨", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π —Ç—Ä–∞–Ω—Å—Ñ–µ—Ä –≤–∏–¥–µ–æ —á–µ—Ä–µ–∑ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ-–≤—Ä–µ–º–µ–Ω–Ω–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ", "desc": "OmniTransfer –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –µ–¥–∏–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Ç—Ä–∞–Ω—Å—Ñ–µ—Ä–∞ –≤–∏–¥–µ–æ, –∫–æ—Ç–æ—Ä—ã–π —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –ø—Ä–æ—Å
[21.01.2026 07:31] Using data from previous issue: {"categories": ["#agents", "#benchmark", "#dataset", "#rl"], "emoji": "üîß", "ru": {"title": "–°–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ—Ü–µ–Ω–∏–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –¥–ª—è –∞–≥–µ–Ω—Ç–æ–≤ —Å –¥–æ—Å—Ç—É–ø–æ–º –∫ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ ToolPRMBench ‚Äî –º–∞—Å—à—Ç–∞–±–Ω–∞—è –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –Ω–∞ —É—Ä–æ–≤–Ω–µ —à–∞–≥–æ–≤
[21.01.2026 07:31] Using data from previous issue: {"categories": ["#transfer_learning", "#alignment", "#hallucinations", "#reasoning"], "emoji": "üåç", "ru": {"title": "–ì—Äounded –º–∏—Ä –¥–ª—è LLM: –æ—Ç –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π –∫ —Ñ–∏–∑–∏—á–µ—Å–∫–∏ –æ—Å—É—â–µ—Å—Ç–≤–∏–º—ã–º –ø–ª–∞–Ω–∞–º", "desc": "WorldMind —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –º–æ–¥–∞–ª—å–Ω–æ–π —Ä–∞—Å—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö –ø—É—Ç—ë–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥
[21.01.2026 07:31] Using data from previous issue: {"categories": ["#rl", "#optimization", "#long_context", "#agents", "#benchmark"], "emoji": "‚ö°", "ru": {"title": "–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º: –±–∞–ª–∞–Ω—Å –º–µ–∂–¥—É –∫–∞—á–µ—Å—Ç–≤–æ–º –∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–π —Å—Ç–æ–∏–º–æ—Å—Ç—å—é", "desc": "–í —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∞–Ω–∞–ª–∏–∑–∏—Ä
[21.01.2026 07:31] Using data from previous issue: {"categories": ["#open_source", "#audio", "#dataset", "#low_resource", "#multilingual", "#benchmark"], "emoji": "üî§", "ru": {"title": "–ì–ª—É–±–æ–∫–∞—è –æ—Ü–µ–Ω–∫–∞ —Ñ–æ–Ω–µ—Ç–∏—á–µ—Å–∫–æ–≥–æ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è –≤ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö —Ä–µ—á–µ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –±–µ–Ω—á–º–∞—Ä–∫ PRiSM –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è —Ñ–æ–Ω–µ—Ç–∏–∫–∏ –≤ —Å–∏—Å—Ç–µ–º–∞—Ö —Ä–∞—Å–ø–æ
[21.01.2026 07:31] Querying the API.
[21.01.2026 07:31] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

FutureOmni presents the first benchmark for evaluating multimodal models' ability to forecast future events from audio-visual data, revealing current limitations and proposing an improved training strategy for better performance.  					AI-generated summary 				 Although Multimodal Large Language Models (MLLMs) demonstrate strong omni-modal perception, their ability to forecast future events from audio-visual cues remains largely unexplored, as existing benchmarks focus mainly on retrospective understanding. To bridge this gap, we introduce FutureOmni, the first benchmark designed to evaluate omni-modal future forecasting from audio-visual environments. The evaluated models are required to perform cross-modal causal and temporal reasoning, as well as effectively leverage internal knowledge to predict future events. FutureOmni is constructed via a scalable LLM-assisted, human-in-the-loop pipeline and contains 919 videos and 1,034 multiple-choice QA pairs across 8 primary domains. Evaluations on 13 omni-modal and 7 video-only models show that current systems struggle with audio-visual future prediction, particularly in speech-heavy scenarios, with the best accuracy of 64.8% achieved by Gemini 3 Flash. To mitigate this limitation, we curate a 7K-sample instruction-tuning dataset and propose an Omni-Modal Future Forecasting (OFF) training strategy. Evaluations on FutureOmni and popular audio-visual and video-only benchmarks demonstrate that OFF enhances future forecasting and generalization. We publicly release all code (https://github.com/OpenMOSS/FutureOmni) and datasets (https://huggingface.co/datasets/OpenMOSS-Team/FutureOmni).
[21.01.2026 07:31] Response: ```json
{
  "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ FutureOmni ‚Äî –ø–µ—Ä–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å –±—É–¥—É—â–∏–µ —Å–æ–±—ã—Ç–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞—É–¥–∏–æ–≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑–∞–ª–æ, —á—Ç–æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã —Å–ª–∞–±–æ —Å–ø—Ä–∞–≤–ª—è—é—Ç—Å—è —Å —ç—Ç–æ–π –∑–∞–¥–∞—á–µ–π, –æ—Å–æ–±–µ–Ω–Ω–æ –∫–æ–≥–¥–∞ –≤ –≤–∏–¥–µ–æ –º–Ω–æ–≥–æ —Ä–µ—á–∏, –¥–æ—Å—Ç–∏–≥–∞—è –ª—É—á—à–µ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ —Ç–æ–ª—å–∫–æ 64.8%. –î–ª—è —Ä–µ—à–µ–Ω–∏—è —ç—Ç–æ–π –ø—Ä–æ–±–ª–µ–º—ã –∞–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –æ–±—É—á–µ–Ω–∏—è Omni-Modal Future Forecasting (OFF) –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –æ—Ç–æ–±—Ä–∞–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏–∑ 7000 –ø—Ä–∏–º–µ—Ä–æ–≤. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π –∫ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—é –±—É–¥—É—â–∏—Ö —Å–æ–±—ã—Ç–∏–π –∏ –∏—Ö –æ–±–æ–±—â–µ–Ω–∏—é –Ω–∞ –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ.",
  "emoji": "üîÆ",
  "title": "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –±—É–¥—É—â–µ–≥–æ: —É—á–∏–º –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≤–∏–¥–µ—Ç—å –∑–∞–≤—Ç—Ä–∞"
}
```
[21.01.2026 07:31] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"FutureOmni presents the first benchmark for evaluating multimodal models' ability to forecast future events from audio-visual data, revealing current limitations and proposing an improved training strategy for better performance.  					AI-generated summary 				 Although Multimodal Large Language Models (MLLMs) demonstrate strong omni-modal perception, their ability to forecast future events from audio-visual cues remains largely unexplored, as existing benchmarks focus mainly on retrospective understanding. To bridge this gap, we introduce FutureOmni, the first benchmark designed to evaluate omni-modal future forecasting from audio-visual environments. The evaluated models are required to perform cross-modal causal and temporal reasoning, as well as effectively leverage internal knowledge to predict future events. FutureOmni is constructed via a scalable LLM-assisted, human-in-the-loop pipeline and contains 919 videos and 1,034 multiple-choice QA pairs across 8 primary domains. Evaluations on 13 omni-modal and 7 video-only models show that current systems struggle with audio-visual future prediction, particularly in speech-heavy scenarios, with the best accuracy of 64.8% achieved by Gemini 3 Flash. To mitigate this limitation, we curate a 7K-sample instruction-tuning dataset and propose an Omni-Modal Future Forecasting (OFF) training strategy. Evaluations on FutureOmni and popular audio-visual and video-only benchmarks demonstrate that OFF enhances future forecasting and generalization. We publicly release all code (https://github.com/OpenMOSS/FutureOmni) and datasets (https://huggingface.co/datasets/OpenMOSS-Team/FutureOmni)."

[21.01.2026 07:31] Response: ```python
["BENCHMARK", "DATASET", "MULTIMODAL", "VIDEO", "AUDIO", "TRAINING"]
```
[21.01.2026 07:31] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"FutureOmni presents the first benchmark for evaluating multimodal models' ability to forecast future events from audio-visual data, revealing current limitations and proposing an improved training strategy for better performance.  					AI-generated summary 				 Although Multimodal Large Language Models (MLLMs) demonstrate strong omni-modal perception, their ability to forecast future events from audio-visual cues remains largely unexplored, as existing benchmarks focus mainly on retrospective understanding. To bridge this gap, we introduce FutureOmni, the first benchmark designed to evaluate omni-modal future forecasting from audio-visual environments. The evaluated models are required to perform cross-modal causal and temporal reasoning, as well as effectively leverage internal knowledge to predict future events. FutureOmni is constructed via a scalable LLM-assisted, human-in-the-loop pipeline and contains 919 videos and 1,034 multiple-choice QA pairs across 8 primary domains. Evaluations on 13 omni-modal and 7 video-only models show that current systems struggle with audio-visual future prediction, particularly in speech-heavy scenarios, with the best accuracy of 64.8% achieved by Gemini 3 Flash. To mitigate this limitation, we curate a 7K-sample instruction-tuning dataset and propose an Omni-Modal Future Forecasting (OFF) training strategy. Evaluations on FutureOmni and popular audio-visual and video-only benchmarks demonstrate that OFF enhances future forecasting and generalization. We publicly release all code (https://github.com/OpenMOSS/FutureOmni) and datasets (https://huggingface.co/datasets/OpenMOSS-Team/FutureOmni)."

[21.01.2026 07:31] Response: ```python
['REASONING', 'OPEN_SOURCE']
```

**Justification:**

- **REASONING**: The paper explicitly mentions that evaluated models are "required to perform cross-modal causal and temporal reasoning" to predict future events, which directly relates to enhancing logical reasoning capabilities.

- **OPEN_SOURCE**: The paper states "We publicly release all code (https://github.com/OpenMOSS/FutureOmni) and datasets (https://huggingface.co/datasets/OpenMOSS-Team/FutureOmni)" to the public, contributing to open-source projects by releasing models, datasets, and frameworks.
[21.01.2026 07:31] Error. Failed to parse JSON from LLM. ["REASONING", "OPEN_SOURCE"]


**Justification:**

- **REASONING**: The paper explicitly mentions that evaluated models are "required to perform cross-modal causal and temporal reasoning" to predict future events, which directly relates to enhancing logical reasoning capabilities.

- **OPEN_SOURCE**: The paper states "We publicly release all code (https://github.com/OpenMOSS/FutureOmni) and datasets (https://huggingface.co/datasets/OpenMOSS-Team/FutureOmni)" to the public, contributing to open-source projects by releasing models, datasets, and frameworks.
[21.01.2026 07:31] Response: ParsedChatCompletionMessage[Article](content='{"desc":"FutureOmni introduces a new benchmark to assess how well multimodal models can predict future events using audio and visual data. It highlights the shortcomings of current models, especially in scenarios with a lot of speech, where they struggle to make accurate predictions. The paper proposes a new training strategy called Omni-Modal Future Forecasting (OFF) to improve these models\' performance. By providing a large dataset and evaluation framework, FutureOmni aims to enhance the ability of models to reason across different modalities and improve their forecasting capabilities.","title":"FutureOmni: Advancing Multimodal Event Forecasting"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="FutureOmni introduces a new benchmark to assess how well multimodal models can predict future events using audio and visual data. It highlights the shortcomings of current models, especially in scenarios with a lot of speech, where they struggle to make accurate predictions. The paper proposes a new training strategy called Omni-Modal Future Forecasting (OFF) to improve these models' performance. By providing a large dataset and evaluation framework, FutureOmni aims to enhance the ability of models to reason across different modalities and improve their forecasting capabilities.", title='FutureOmni: Advancing Multimodal Event Forecasting'))
[21.01.2026 07:31] Response: ParsedChatCompletionMessage[Article](content='{"desc":"FutureOmniÊòØÁ¨¨‰∏Ä‰∏™Áî®‰∫éËØÑ‰º∞Â§öÊ®°ÊÄÅÊ®°Âûã‰ªéÈü≥ËßÜÈ¢ëÊï∞ÊçÆÈ¢ÑÊµãÊú™Êù•‰∫ã‰ª∂ËÉΩÂäõÁöÑÂü∫ÂáÜÔºåÊè≠Á§∫‰∫ÜÂΩìÂâçÁöÑÂ±ÄÈôêÊÄßÔºåÂπ∂ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊîπËøõÁöÑËÆ≠ÁªÉÁ≠ñÁï•‰ª•ÊèêÈ´òÊÄßËÉΩ„ÄÇÂ∞ΩÁÆ°Â§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâÂú®Â§öÁßçÊÑüÁü•‰∏äË°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÂÆÉ‰ª¨Âú®Èü≥ËßÜÈ¢ëÁ∫øÁ¥¢‰∏≠È¢ÑÊµãÊú™Êù•‰∫ã‰ª∂ÁöÑËÉΩÂäõ‰ªçÁÑ∂Êú™Ë¢´ÂÖÖÂàÜÊé¢Á¥¢„ÄÇFutureOmniË¶ÅÊ±ÇËØÑ‰º∞ÁöÑÊ®°ÂûãËøõË°åË∑®Ê®°ÊÄÅÂõ†ÊûúÂíåÊó∂Èó¥Êé®ÁêÜÔºåÂπ∂ÊúâÊïàÂà©Áî®ÂÜÖÈÉ®Áü•ËØÜÊù•È¢ÑÊµãÊú™Êù•‰∫ã‰ª∂„ÄÇÈÄöËøáÊûÑÂª∫‰∏Ä‰∏™ÂåÖÂê´919‰∏™ËßÜÈ¢ëÂíå1034‰∏™Â§öÈ°πÈÄâÊã©ÈóÆÁ≠îÂØπÁöÑÂü∫ÂáÜÔºåFutureOmni‰∏∫Êú™Êù•È¢ÑÊµãÊèê‰æõ‰∫ÜÊñ∞ÁöÑËØÑ‰º∞Ê†áÂáÜ„ÄÇ","title":"Êú™Êù•È¢ÑÊµãÁöÑÊñ∞Âü∫ÂáÜÔºöFutureOmni"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='FutureOmniÊòØÁ¨¨‰∏Ä‰∏™Áî®‰∫éËØÑ‰º∞Â§öÊ®°ÊÄÅÊ®°Âûã‰ªéÈü≥ËßÜÈ¢ëÊï∞ÊçÆÈ¢ÑÊµãÊú™Êù•‰∫ã‰ª∂ËÉΩÂäõÁöÑÂü∫ÂáÜÔºåÊè≠Á§∫‰∫ÜÂΩìÂâçÁöÑÂ±ÄÈôêÊÄßÔºåÂπ∂ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊîπËøõÁöÑËÆ≠ÁªÉÁ≠ñÁï•‰ª•ÊèêÈ´òÊÄßËÉΩ„ÄÇÂ∞ΩÁÆ°Â§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâÂú®Â§öÁßçÊÑüÁü•‰∏äË°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÂÆÉ‰ª¨Âú®Èü≥ËßÜÈ¢ëÁ∫øÁ¥¢‰∏≠È¢ÑÊµãÊú™Êù•‰∫ã‰ª∂ÁöÑËÉΩÂäõ‰ªçÁÑ∂Êú™Ë¢´ÂÖÖÂàÜÊé¢Á¥¢„ÄÇFutureOmniË¶ÅÊ±ÇËØÑ‰º∞ÁöÑÊ®°ÂûãËøõË°åË∑®Ê®°ÊÄÅÂõ†ÊûúÂíåÊó∂Èó¥Êé®ÁêÜÔºåÂπ∂ÊúâÊïàÂà©Áî®ÂÜÖÈÉ®Áü•ËØÜÊù•È¢ÑÊµãÊú™Êù•‰∫ã‰ª∂„ÄÇÈÄöËøáÊûÑÂª∫‰∏Ä‰∏™ÂåÖÂê´919‰∏™ËßÜÈ¢ëÂíå1034‰∏™Â§öÈ°πÈÄâÊã©ÈóÆÁ≠îÂØπÁöÑÂü∫ÂáÜÔºåFutureOmni‰∏∫Êú™Êù•È¢ÑÊµãÊèê‰æõ‰∫ÜÊñ∞ÁöÑËØÑ‰º∞Ê†áÂáÜ„ÄÇ', title='Êú™Êù•È¢ÑÊµãÁöÑÊñ∞Âü∫ÂáÜÔºöFutureOmni'))
[21.01.2026 07:31] Using data from previous issue: {"categories": ["#training", "#inference", "#small_models"], "emoji": "üîç", "ru": {"title": "–õ—ë–≥–∫–∏–µ –∑–æ–Ω–¥—ã –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏: –ø–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π LLM –±–µ–∑ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è –ª—ë–≥–∫–∏—Ö –∑–æ–Ω–¥–æ–≤ (probes) –Ω–∞ —Å–∫—Ä—ã—Ç—ã—Ö —Å–æ—Å—Ç–æ—è–Ω–∏—è—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –≤—ã
[21.01.2026 07:31] Querying the API.
[21.01.2026 07:31] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

SciCoQA is a dataset for identifying mismatches between scientific publications and code implementations, containing 611 discrepancies across multiple disciplines and demonstrating the challenge of detecting such issues even for advanced language models.  					AI-generated summary 				 We present SciCoQA, a dataset for detecting discrepancies between scientific publications and their codebases to ensure faithful implementations. We construct SciCoQA from GitHub issues and reproducibility papers, and to scale our dataset, we propose a synthetic data generation method for constructing paper-code discrepancies. We analyze the paper-code discrepancies in detail and propose discrepancy types and categories to better understand the occurring mismatches. In total, our dataset consists of 611 paper-code discrepancies (81 real, 530 synthetic), spanning diverse computational science disciplines, including AI, Physics, Quantitative Biology, and others. Our evaluation of 21 LLMs highlights the difficulty of SciCoQA, particularly for instances involving omitted paper details, long-context inputs, and data outside the models' pre-training corpus. The best performing model in our evaluation, GPT-5, can only detect 45.7\% of real-world paper-code discrepancies.
[21.01.2026 07:31] Response: ```json
{
  "desc": "SciCoQA ‚Äî —ç—Ç–æ –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–π –º–µ–∂–¥—É –æ–ø–∏—Å–∞–Ω–∏–µ–º –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –≤ –Ω–∞—É—á–Ω—ã—Ö –ø—É–±–ª–∏–∫–∞—Ü–∏—è—Ö –∏ –∏—Ö —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–µ–π –≤ –∫–æ–¥–µ, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π 611 –ø—Ä–∏–º–µ—Ä–æ–≤ –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–π –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –æ–±–ª–∞—Å—Ç–µ–π –Ω–∞—É–∫–∏. –ê–≤—Ç–æ—Ä—ã —Å–æ–±—Ä–∞–ª–∏ –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ GitHub-issues –∏ —Å—Ç–∞—Ç–µ–π –æ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏, –∞ —Ç–∞–∫–∂–µ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –º–µ—Ç–æ–¥ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è. –û–Ω–∏ –ø—Ä–æ–≤–µ–ª–∏ –∞–Ω–∞–ª–∏–∑ —Ç–∏–ø–æ–≤ –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–π –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã—Ö –æ—à–∏–±–æ–∫, –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–≤ –∏—Ö –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è. –û—Ü–µ–Ω–∫–∞ 21 LLM –ø–æ–∫–∞–∑–∞–ª–∞, —á—Ç–æ –¥–∞–∂–µ –ª—É—á—à–∏–µ –º–æ–¥–µ–ª–∏ —Å–ø—Ä–∞–≤–ª—è—é—Ç—Å—è —Ç–æ–ª—å–∫–æ –Ω–∞ 45.7%, —á—Ç–æ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—É—é —Å–ª–æ–∂–Ω–æ—Å—Ç—å –∑–∞–¥–∞—á–∏, –æ—Å–æ–±–µ–Ω–Ω–æ –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –¥–µ—Ç–∞–ª–µ–π –∏ –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.",
  "emoji": "üîç",
  "title": "–í—ã—è–≤–ª–µ–Ω–∏–µ —Å–∫—Ä—ã—Ç—ã—Ö –æ—à–∏–±–æ–∫: –∫–æ–≥–¥–∞ –∫–æ–¥ –Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –Ω–∞—É—á–Ω–æ–π —Å—Ç–∞—Ç—å–µ"
}
```
[21.01.2026 07:31] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"SciCoQA is a dataset for identifying mismatches between scientific publications and code implementations, containing 611 discrepancies across multiple disciplines and demonstrating the challenge of detecting such issues even for advanced language models.  					AI-generated summary 				 We present SciCoQA, a dataset for detecting discrepancies between scientific publications and their codebases to ensure faithful implementations. We construct SciCoQA from GitHub issues and reproducibility papers, and to scale our dataset, we propose a synthetic data generation method for constructing paper-code discrepancies. We analyze the paper-code discrepancies in detail and propose discrepancy types and categories to better understand the occurring mismatches. In total, our dataset consists of 611 paper-code discrepancies (81 real, 530 synthetic), spanning diverse computational science disciplines, including AI, Physics, Quantitative Biology, and others. Our evaluation of 21 LLMs highlights the difficulty of SciCoQA, particularly for instances involving omitted paper details, long-context inputs, and data outside the models' pre-training corpus. The best performing model in our evaluation, GPT-5, can only detect 45.7\% of real-world paper-code discrepancies."

[21.01.2026 07:31] Response: ```python
["DATASET", "PLP", "BENCHMARK"]
```

**Justification:**
- **DATASET**: The paper explicitly introduces SciCoQA, a new dataset containing 611 paper-code discrepancies for detecting mismatches between scientific publications and code implementations.
- **PLP**: The paper focuses on analyzing code implementations and discrepancies, which relates to programming language processing.
- **BENCHMARK**: The paper evaluates 21 LLMs on the proposed dataset, establishing a benchmark for detecting paper-code discrepancies.
[21.01.2026 07:31] Error. Failed to parse JSON from LLM. ["DATASET", "PLP", "BENCHMARK"]


**Justification:**
- **DATASET**: The paper explicitly introduces SciCoQA, a new dataset containing 611 paper-code discrepancies for detecting mismatches between scientific publications and code implementations.
- **PLP**: The paper focuses on analyzing code implementations and discrepancies, which relates to programming language processing.
- **BENCHMARK**: The paper evaluates 21 LLMs on the proposed dataset, establishing a benchmark for detecting paper-code discrepancies.
[21.01.2026 07:31] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"SciCoQA is a dataset for identifying mismatches between scientific publications and code implementations, containing 611 discrepancies across multiple disciplines and demonstrating the challenge of detecting such issues even for advanced language models.  					AI-generated summary 				 We present SciCoQA, a dataset for detecting discrepancies between scientific publications and their codebases to ensure faithful implementations. We construct SciCoQA from GitHub issues and reproducibility papers, and to scale our dataset, we propose a synthetic data generation method for constructing paper-code discrepancies. We analyze the paper-code discrepancies in detail and propose discrepancy types and categories to better understand the occurring mismatches. In total, our dataset consists of 611 paper-code discrepancies (81 real, 530 synthetic), spanning diverse computational science disciplines, including AI, Physics, Quantitative Biology, and others. Our evaluation of 21 LLMs highlights the difficulty of SciCoQA, particularly for instances involving omitted paper details, long-context inputs, and data outside the models' pre-training corpus. The best performing model in our evaluation, GPT-5, can only detect 45.7\% of real-world paper-code discrepancies."

[21.01.2026 07:31] Response: ```python
['SCIENCE', 'SYNTHETIC', 'LONG_CONTEXT', 'HALLUCINATIONS']
```
[21.01.2026 07:31] Response: ParsedChatCompletionMessage[Article](content='{"desc":"SciCoQA is a newly created dataset designed to identify mismatches between scientific papers and their corresponding code implementations. It includes 611 discrepancies, with a mix of real and synthetic examples, to highlight the challenges faced by language models in detecting these issues. The dataset spans various fields such as AI, Physics, and Quantitative Biology, and categorizes the types of discrepancies for better analysis. Despite testing 21 large language models, the best performer, GPT-5, only managed to detect 45.7% of the real discrepancies, showcasing the complexity of the task.","title":"Bridging the Gap: Detecting Mismatches in Science and Code"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='SciCoQA is a newly created dataset designed to identify mismatches between scientific papers and their corresponding code implementations. It includes 611 discrepancies, with a mix of real and synthetic examples, to highlight the challenges faced by language models in detecting these issues. The dataset spans various fields such as AI, Physics, and Quantitative Biology, and categorizes the types of discrepancies for better analysis. Despite testing 21 large language models, the best performer, GPT-5, only managed to detect 45.7% of the real discrepancies, showcasing the complexity of the task.', title='Bridging the Gap: Detecting Mismatches in Science and Code'))
[21.01.2026 07:31] Response: ParsedChatCompletionMessage[Article](content='{"desc":"SciCoQAÊòØ‰∏Ä‰∏™Áî®‰∫éËØÜÂà´ÁßëÂ≠¶Âá∫ÁâàÁâ©‰∏é‰ª£Á†ÅÂÆûÁé∞‰πãÈó¥‰∏çÂåπÈÖçÁöÑÊï∞ÊçÆÈõÜÔºåÂåÖÂê´611‰∏™Ë∑®Â≠¶ÁßëÁöÑÂ∑ÆÂºÇ„ÄÇËøô‰∫õÂ∑ÆÂºÇÂ±ïÁ§∫‰∫ÜÂç≥‰ΩøÊòØÂÖàËøõÁöÑËØ≠Ë®ÄÊ®°Âûã‰πüÈöæ‰ª•Ê£ÄÊµãÊ≠§Á±ªÈóÆÈ¢ò„ÄÇÊàë‰ª¨‰ªéGitHubÈóÆÈ¢òÂíåÂèØÈáçÂ§çÊÄßËÆ∫Êñá‰∏≠ÊûÑÂª∫SciCoQAÔºåÂπ∂ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêàÊàêÊï∞ÊçÆÁîüÊàêÊñπÊ≥ïÊù•Êâ©Â±ïÊï∞ÊçÆÈõÜ„ÄÇÈÄöËøáËØ¶ÁªÜÂàÜÊûêËÆ∫Êñá‰∏é‰ª£Á†Å‰πãÈó¥ÁöÑÂ∑ÆÂºÇÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜÂ∑ÆÂºÇÁ±ªÂûãÂíåÁ±ªÂà´Ôºå‰ª•Êõ¥Â•ΩÂú∞ÁêÜËß£Ëøô‰∫õ‰∏çÂåπÈÖçÁöÑÊÉÖÂÜµ„ÄÇ","title":"ËØÜÂà´ÁßëÂ≠¶Âá∫ÁâàÁâ©‰∏é‰ª£Á†ÅÂÆûÁé∞ÁöÑ‰∏çÂåπÈÖç"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='SciCoQAÊòØ‰∏Ä‰∏™Áî®‰∫éËØÜÂà´ÁßëÂ≠¶Âá∫ÁâàÁâ©‰∏é‰ª£Á†ÅÂÆûÁé∞‰πãÈó¥‰∏çÂåπÈÖçÁöÑÊï∞ÊçÆÈõÜÔºåÂåÖÂê´611‰∏™Ë∑®Â≠¶ÁßëÁöÑÂ∑ÆÂºÇ„ÄÇËøô‰∫õÂ∑ÆÂºÇÂ±ïÁ§∫‰∫ÜÂç≥‰ΩøÊòØÂÖàËøõÁöÑËØ≠Ë®ÄÊ®°Âûã‰πüÈöæ‰ª•Ê£ÄÊµãÊ≠§Á±ªÈóÆÈ¢ò„ÄÇÊàë‰ª¨‰ªéGitHubÈóÆÈ¢òÂíåÂèØÈáçÂ§çÊÄßËÆ∫Êñá‰∏≠ÊûÑÂª∫SciCoQAÔºåÂπ∂ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêàÊàêÊï∞ÊçÆÁîüÊàêÊñπÊ≥ïÊù•Êâ©Â±ïÊï∞ÊçÆÈõÜ„ÄÇÈÄöËøáËØ¶ÁªÜÂàÜÊûêËÆ∫Êñá‰∏é‰ª£Á†Å‰πãÈó¥ÁöÑÂ∑ÆÂºÇÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜÂ∑ÆÂºÇÁ±ªÂûãÂíåÁ±ªÂà´Ôºå‰ª•Êõ¥Â•ΩÂú∞ÁêÜËß£Ëøô‰∫õ‰∏çÂåπÈÖçÁöÑÊÉÖÂÜµ„ÄÇ', title='ËØÜÂà´ÁßëÂ≠¶Âá∫ÁâàÁâ©‰∏é‰ª£Á†ÅÂÆûÁé∞ÁöÑ‰∏çÂåπÈÖç'))
[21.01.2026 07:31] Querying the API.
[21.01.2026 07:31] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A novel retriever training framework for agentic search that uses both local relevance and global answer correctness metrics with iterative optimization between the search agent and retriever.  					AI-generated summary 				 Agentic search has recently emerged as a powerful paradigm, where an agent interleaves multi-step reasoning with on-demand retrieval to solve complex questions. Despite its success, how to design a retriever for agentic search remains largely underexplored. Existing search agents typically rely on similarity-based retrievers, while similar passages are not always useful for final answer generation. In this paper, we propose a novel retriever training framework tailored for agentic search. Unlike retrievers designed for single-turn retrieval-augmented generation (RAG) that only rely on local passage utility, we propose to use both local query-passage relevance and global answer correctness to measure passage utility in a multi-turn agentic search. We further introduce an iterative training strategy, where the search agent and the retriever are optimized bidirectionally and iteratively. Different from RAG retrievers that are only trained once with fixed questions, our retriever is continuously improved using evolving and higher-quality queries from the agent. Extensive experiments on seven single-hop and multi-hop QA benchmarks demonstrate that our retriever, termed , consistently outperforms strong baselines across different search agents. Our codes are available at: https://github.com/8421BCD/Agentic-R.
[21.01.2026 07:31] Response: ```json
{
  "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –Ω–æ–≤–∞—èÊ°ÜÊû∂ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è retriever'–∞, –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–Ω–∞—è —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∞–≥–µ–Ω—Ç–æ–≤. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã—Ö –ø–æ–¥—Ö–æ–¥–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –æ–ø–∏—Ä–∞—é—Ç—Å—è —Ç–æ–ª—å–∫–æ –Ω–∞ –ª–æ–∫–∞–ª—å–Ω—É—é —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–æ–º –∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–º, –∞–≤—Ç–æ—Ä—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç –∫–∞–∫ –ª–æ–∫–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ (—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å), —Ç–∞–∫ –∏ –≥–ª–æ–±–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ (–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞) –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø–æ–ª–µ–∑–Ω–æ—Å—Ç–∏ –Ω–∞–π–¥–µ–Ω–Ω–æ–≥–æ –ø–∞—Å—Å–∞–∂–∞. –§—Ä–µ–π–º–≤–æ—Ä–∫ –≤–∫–ª—é—á–∞–µ—Ç –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –æ–±—É—á–µ–Ω–∏—è, –≥–¥–µ search agent –∏ retriever –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É—é—Ç—Å—è –≤–∑–∞–∏–º–Ω–æ –∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç retriever'—É –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ —É–ª—É—á—à–∞—Ç—å—Å—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞–∑–≤–∏–≤–∞—é—â–∏—Ö—Å—è –∑–∞–ø—Ä–æ—Å–æ–≤ –æ—Ç –∞–≥–µ–Ω—Ç–∞. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ —Å–µ–º–∏ –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –±–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏.",
  "emoji": "üîÑ",
  "title": "–î–≤—É—Å—Ç–æ—Ä–æ–Ω–Ω—è—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∞–≥–µ–Ω—Ç–∞ –ø–æ–∏—Å–∫–∞ –∏ –ø–æ–∏—Å–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è —Ç–æ—á–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤"
}
```
[21.01.2026 07:31] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A novel retriever training framework for agentic search that uses both local relevance and global answer correctness metrics with iterative optimization between the search agent and retriever.  					AI-generated summary 				 Agentic search has recently emerged as a powerful paradigm, where an agent interleaves multi-step reasoning with on-demand retrieval to solve complex questions. Despite its success, how to design a retriever for agentic search remains largely underexplored. Existing search agents typically rely on similarity-based retrievers, while similar passages are not always useful for final answer generation. In this paper, we propose a novel retriever training framework tailored for agentic search. Unlike retrievers designed for single-turn retrieval-augmented generation (RAG) that only rely on local passage utility, we propose to use both local query-passage relevance and global answer correctness to measure passage utility in a multi-turn agentic search. We further introduce an iterative training strategy, where the search agent and the retriever are optimized bidirectionally and iteratively. Different from RAG retrievers that are only trained once with fixed questions, our retriever is continuously improved using evolving and higher-quality queries from the agent. Extensive experiments on seven single-hop and multi-hop QA benchmarks demonstrate that our retriever, termed , consistently outperforms strong baselines across different search agents. Our codes are available at: https://github.com/8421BCD/Agentic-R."

[21.01.2026 07:31] Response: ```python
["AGENTS", "RAG", "BENCHMARK", "TRAINING"]
```
[21.01.2026 07:31] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A novel retriever training framework for agentic search that uses both local relevance and global answer correctness metrics with iterative optimization between the search agent and retriever.  					AI-generated summary 				 Agentic search has recently emerged as a powerful paradigm, where an agent interleaves multi-step reasoning with on-demand retrieval to solve complex questions. Despite its success, how to design a retriever for agentic search remains largely underexplored. Existing search agents typically rely on similarity-based retrievers, while similar passages are not always useful for final answer generation. In this paper, we propose a novel retriever training framework tailored for agentic search. Unlike retrievers designed for single-turn retrieval-augmented generation (RAG) that only rely on local passage utility, we propose to use both local query-passage relevance and global answer correctness to measure passage utility in a multi-turn agentic search. We further introduce an iterative training strategy, where the search agent and the retriever are optimized bidirectionally and iteratively. Different from RAG retrievers that are only trained once with fixed questions, our retriever is continuously improved using evolving and higher-quality queries from the agent. Extensive experiments on seven single-hop and multi-hop QA benchmarks demonstrate that our retriever, termed , consistently outperforms strong baselines across different search agents. Our codes are available at: https://github.com/8421BCD/Agentic-R."

[21.01.2026 07:31] Response: ```python
["REASONING", "OPTIMIZATION", "OPEN_SOURCE"]
```

**Justification:**

1. **REASONING**: The paper explicitly discusses "multi-step reasoning" and addresses "complex questions" through an agentic search paradigm that interleaves reasoning with retrieval, which is directly related to enhancing logical reasoning capabilities.

2. **OPTIMIZATION**: The paper proposes an "iterative training strategy" and "iterative optimization between the search agent and retriever," focusing on training optimization methods to improve model performance.

3. **OPEN_SOURCE**: The paper states "Our codes are available at: https://github.com/8421BCD/Agentic-R," indicating the authors are releasing code/framework to the public.
[21.01.2026 07:31] Error. Failed to parse JSON from LLM. ["REASONING", "OPTIMIZATION", "OPEN_SOURCE"]


**Justification:**

1. **REASONING**: The paper explicitly discusses "multi-step reasoning" and addresses "complex questions" through an agentic search paradigm that interleaves reasoning with retrieval, which is directly related to enhancing logical reasoning capabilities.

2. **OPTIMIZATION**: The paper proposes an "iterative training strategy" and "iterative optimization between the search agent and retriever," focusing on training optimization methods to improve model performance.

3. **OPEN_SOURCE**: The paper states "Our codes are available at: https://github.com/8421BCD/Agentic-R," indicating the authors are releasing code/framework to the public.
[21.01.2026 07:31] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces a new training framework for retrievers used in agentic search, which combines local relevance and global answer correctness metrics. The framework allows for iterative optimization between the search agent and the retriever, enhancing their performance in multi-step reasoning tasks. Unlike traditional retrievers that focus solely on similarity, this approach continuously improves the retriever using evolving queries from the agent. The results show that this novel retriever outperforms existing methods across various question-answering benchmarks.","title":"Enhancing Agentic Search with Iterative Retriever Training"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper introduces a new training framework for retrievers used in agentic search, which combines local relevance and global answer correctness metrics. The framework allows for iterative optimization between the search agent and the retriever, enhancing their performance in multi-step reasoning tasks. Unlike traditional retrievers that focus solely on similarity, this approach continuously improves the retriever using evolving queries from the agent. The results show that this novel retriever outperforms existing methods across various question-answering benchmarks.', title='Enhancing Agentic Search with Iterative Retriever Training'))
[21.01.2026 07:31] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÊ£ÄÁ¥¢Âô®ËÆ≠ÁªÉÊ°ÜÊû∂Ôºå‰∏ì‰∏∫‰ª£ÁêÜÊêúÁ¥¢ËÆæËÆ°„ÄÇËØ•Ê°ÜÊû∂ÁªìÂêà‰∫ÜÂ±ÄÈÉ®Áõ∏ÂÖ≥ÊÄßÂíåÂÖ®Â±ÄÁ≠îÊ°àÊ≠£Á°ÆÊÄßÊåáÊ†áÔºåÈÄöËøáÊêúÁ¥¢‰ª£ÁêÜÂíåÊ£ÄÁ¥¢Âô®‰πãÈó¥ÁöÑËø≠‰ª£‰ºòÂåñÊù•ÊèêÂçáÊÄßËÉΩ„ÄÇ‰∏é‰º†ÁªüÁöÑÂü∫‰∫éÁõ∏‰ººÊÄßÁöÑÊ£ÄÁ¥¢Âô®‰∏çÂêåÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÂú®Â§öËΩÆ‰ª£ÁêÜÊêúÁ¥¢‰∏≠ÂêåÊó∂ËÄÉËôëÊü•ËØ¢-ÊÆµËêΩÁöÑÁõ∏ÂÖ≥ÊÄßÂíåÁ≠îÊ°àÁöÑÂÖ®Â±ÄÊ≠£Á°ÆÊÄß„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊàë‰ª¨ÁöÑÊ£ÄÁ¥¢Âô®Âú®Â§ö‰∏™ÈóÆÁ≠îÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞‰ºò‰∫éÁé∞ÊúâÁöÑÂº∫Âü∫Á∫ø„ÄÇ","title":"ÂàõÊñ∞ÁöÑ‰ª£ÁêÜÊêúÁ¥¢Ê£ÄÁ¥¢Âô®ËÆ≠ÁªÉÊ°ÜÊû∂"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÊ£ÄÁ¥¢Âô®ËÆ≠ÁªÉÊ°ÜÊû∂Ôºå‰∏ì‰∏∫‰ª£ÁêÜÊêúÁ¥¢ËÆæËÆ°„ÄÇËØ•Ê°ÜÊû∂ÁªìÂêà‰∫ÜÂ±ÄÈÉ®Áõ∏ÂÖ≥ÊÄßÂíåÂÖ®Â±ÄÁ≠îÊ°àÊ≠£Á°ÆÊÄßÊåáÊ†áÔºåÈÄöËøáÊêúÁ¥¢‰ª£ÁêÜÂíåÊ£ÄÁ¥¢Âô®‰πãÈó¥ÁöÑËø≠‰ª£‰ºòÂåñÊù•ÊèêÂçáÊÄßËÉΩ„ÄÇ‰∏é‰º†ÁªüÁöÑÂü∫‰∫éÁõ∏‰ººÊÄßÁöÑÊ£ÄÁ¥¢Âô®‰∏çÂêåÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÂú®Â§öËΩÆ‰ª£ÁêÜÊêúÁ¥¢‰∏≠ÂêåÊó∂ËÄÉËôëÊü•ËØ¢-ÊÆµËêΩÁöÑÁõ∏ÂÖ≥ÊÄßÂíåÁ≠îÊ°àÁöÑÂÖ®Â±ÄÊ≠£Á°ÆÊÄß„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊàë‰ª¨ÁöÑÊ£ÄÁ¥¢Âô®Âú®Â§ö‰∏™ÈóÆÁ≠îÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞‰ºò‰∫éÁé∞ÊúâÁöÑÂº∫Âü∫Á∫ø„ÄÇ', title='ÂàõÊñ∞ÁöÑ‰ª£ÁêÜÊêúÁ¥¢Ê£ÄÁ¥¢Âô®ËÆ≠ÁªÉÊ°ÜÊû∂'))
[21.01.2026 07:31] Using data from previous issue: {"categories": ["#healthcare", "#interpretability", "#ethics", "#benchmark", "#dataset", "#synthetic"], "emoji": "üîç", "ru": {"title": "–û–±—ä–µ–∫—Ç–∏–≤–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±—ä—è—Å–Ω–∏–º–æ—Å—Ç–∏ —á–µ—Ä–µ–∑ –ø—Ä–∏—á–∏–Ω–Ω—ã–µ –∫–æ–Ω—Ç—Ä—Ñ–∞–∫—Ç—ã", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç LIBERTy ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ —Å –∫–æ–Ω—Ç—Ä—Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–º–∏ –ø–∞—Ä–∞–º–∏
[21.01.2026 07:31] Using data from previous issue: {"categories": ["#multilingual", "#data", "#low_resource", "#dataset", "#open_source", "#synthetic"], "emoji": "üáπüá∑", "ru": {"title": "–ú–∞—Å—à—Ç–∞–±–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –æ—Ç–Ω–æ—à–µ–Ω–∏–π —Ç—É—Ä–µ—Ü–∫–æ–≥–æ —è–∑—ã–∫–∞ —á–µ—Ä–µ–∑ –≥–∏–±—Ä–∏–¥–Ω—É—é –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—é", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –≥–∏–±—Ä–∏–¥–Ω—ã–π –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—é –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –±–æ–ª
[21.01.2026 07:31] Using data from previous issue: {"categories": [], "emoji": "üéØ", "ru": {"title": "–†–∞–∑–ª–∏—á–∏–µ —Å–∏–Ω–æ–Ω–∏–º–æ–≤ –∏ –∞–Ω—Ç–æ–Ω–∏–º–æ–≤: —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –±–µ–∑ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –¥—Ä–µ–π—Ñ–∞", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –Ω–µ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Ä–∞–∑–ª–∏—á–∞—Ç—å —Å–∏–Ω–æ–Ω–∏–º—ã –∏ –∞–Ω—Ç–æ–Ω–∏–º—ã. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—É—é —Å–∏—Å—Ç–µ–º—É —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π
[21.01.2026 07:31] Renaming data file.
[21.01.2026 07:31] Renaming previous data. hf_papers.json to ./d/2026-01-21.json
[21.01.2026 07:31] Saving new data file.
[21.01.2026 07:31] Generating page.
[21.01.2026 07:31] Renaming previous page.
[21.01.2026 07:31] Renaming previous data. index.html to ./d/2026-01-21.html
[21.01.2026 07:31] Writing result.
[21.01.2026 07:31] Renaming log file.
[21.01.2026 07:31] Renaming previous data. log.txt to ./logs/2026-01-21_last_log.txt
