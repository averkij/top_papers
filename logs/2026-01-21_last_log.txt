[21.01.2026 03:47] Read previous papers.
[21.01.2026 03:47] Generating top page (month).
[21.01.2026 03:47] Writing top page (month).
[21.01.2026 04:46] Read previous papers.
[21.01.2026 04:46] Get feed.
[21.01.2026 04:46] Get page data from previous paper. URL: https://huggingface.co/papers/2601.11522
[21.01.2026 04:46] Get page data from previous paper. URL: https://huggingface.co/papers/2601.13247
[21.01.2026 04:46] Get page data from previous paper. URL: https://huggingface.co/papers/2601.11969
[21.01.2026 04:46] Extract page data from URL. URL: https://huggingface.co/papers/2601.12294
[21.01.2026 04:46] Get page data from previous paper. URL: https://huggingface.co/papers/2601.11655
[21.01.2026 04:46] Extract page data from URL. URL: https://huggingface.co/papers/2601.12993
[21.01.2026 04:46] Get page data from previous paper. URL: https://huggingface.co/papers/2601.13288
[21.01.2026 04:46] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[21.01.2026 04:46] No deleted papers detected.
[21.01.2026 04:46] Downloading and parsing papers (pdf, html). Total: 7.
[21.01.2026 04:46] Downloading and parsing paper https://huggingface.co/papers/2601.11522.
[21.01.2026 04:46] Extra JSON file exists (./assets/json/2601.11522.json), skip PDF parsing.
[21.01.2026 04:46] Paper image links file exists (./assets/img_data/2601.11522.json), skip HTML parsing.
[21.01.2026 04:46] Success.
[21.01.2026 04:46] Downloading and parsing paper https://huggingface.co/papers/2601.13247.
[21.01.2026 04:46] Downloading paper 2601.13247 from https://arxiv.org/pdf/2601.13247v1...
[21.01.2026 04:46] Extracting affiliations from text.
[21.01.2026 04:46] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Baochang Ren, Yunzhi Yao, Rui Sun, Shuofei Qiao, Ningyu Zhang*, Huajun Chen* Zhejiang University University of California, Los Angeles {baochang.ren, zhangningyu}@zju.edu.cn Code "
[21.01.2026 04:46] Response: ```python
[
    "Zhejiang University",
    "University of California, Los Angeles"
]
```
[21.01.2026 04:46] Deleting PDF ./assets/pdf/2601.13247.pdf.
[21.01.2026 04:46] Success.
[21.01.2026 04:46] Downloading and parsing paper https://huggingface.co/papers/2601.11969.
[21.01.2026 04:46] Extra JSON file exists (./assets/json/2601.11969.json), skip PDF parsing.
[21.01.2026 04:46] Paper image links file exists (./assets/img_data/2601.11969.json), skip HTML parsing.
[21.01.2026 04:46] Success.
[21.01.2026 04:46] Downloading and parsing paper https://huggingface.co/papers/2601.12294.
[21.01.2026 04:46] Downloading paper 2601.12294 from https://arxiv.org/pdf/2601.12294v1...
[21.01.2026 04:46] Extracting affiliations from text.
[21.01.2026 04:46] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"ToolPRMBench: Evaluating and Advancing Process Reward Models for Tool-using Agents Dawei Li, Yuguang Yao, Zhen Tan, Huan Liu, Ruocheng Guo Arizona State University, Intuit AI Research {daweili5, ztan36, huanliu}@asu.edu {yuguang_yao,ruocheng_guo}@intuit.com "
[21.01.2026 04:46] Response: ```python
["Arizona State University", "Intuit AI Research"]
```
[21.01.2026 04:46] Deleting PDF ./assets/pdf/2601.12294.pdf.
[21.01.2026 04:46] Success.
[21.01.2026 04:46] Downloading and parsing paper https://huggingface.co/papers/2601.11655.
[21.01.2026 04:46] Extra JSON file exists (./assets/json/2601.11655.json), skip PDF parsing.
[21.01.2026 04:46] Paper image links file exists (./assets/img_data/2601.11655.json), skip HTML parsing.
[21.01.2026 04:46] Success.
[21.01.2026 04:46] Downloading and parsing paper https://huggingface.co/papers/2601.12993.
[21.01.2026 04:46] Downloading paper 2601.12993 from https://arxiv.org/pdf/2601.12993v1...
[21.01.2026 04:47] Extracting affiliations from text.
[21.01.2026 04:47] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Being-H0.5: Scaling Human-Centric Robot Learning for Cross-Embodiment Generalization https://research.beingbeyond.com/being-h05 6 2 0 2 9 1 ] . [ 1 3 9 9 2 1 . 1 0 6 2 : r Figure 1: Being-H0.5 at Glance. We scale human-centric robot learning with Being-H0.5 toward cross-embodiment generalization. We introduce UniHand-2.0, large-scale corpus exceeding 35,000 hours that spans both cross-Embodiment physical control and general visual-text understanding. Building on this data, we unify human hand motion and diverse robot embodiments with Unified Action Space, and train all heterogeneous supervision through unified sequence modeling under single framework. This yields single foundation model that can perceive, describe, and act within one framework, enabling robust cross-embodiment generalization and real-world deployment across diverse robots and tasks. We empirically deploy single checkpoint of Being-H0.5 to control PND Adam-U, Franka+Inspire, Unitree G1, BeingBeyond D1, and LeRobot SO-101 to accomplish diverse tasks. "
[21.01.2026 04:47] Response: ```python
[]
```
[21.01.2026 04:47] Extracting affiliations from text.
[21.01.2026 04:47] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Being-H0.5: Scaling Human-Centric Robot Learning for Cross-Embodiment Generalizationhttps://research.beingbeyond.com/being-h05 6 2 0 2 9 1 ] . [ 1 3 9 9 2 1 . 1 0 6 2 : r Figure 1: Being-H0.5 at Glance. We scale human-centric robot learning with Being-H0.5 toward cross-embodiment generalization. We introduce UniHand-2.0, large-scale corpus exceeding 35,000 hours that spans both cross-Embodiment physical control and general visual-text understanding. Building on this data, we unify human hand motion and diverse robot embodiments with Unified Action Space, and train all heterogeneous supervision through unified sequence modeling under single framework. This yields single foundation model that can perceive, describe, and act within one framework, enabling robust cross-embodiment generalization and real-world deployment across diverse robots and tasks. We empirically deploy single checkpoint of Being-H0.5 to control PND Adam-U, Franka+Inspire, Unitree G1, BeingBeyond D1, and LeRobot SO-101 to accomplish diverse tasks.We introduce Being-H0.5, foundational Vision-Language-Action (VLA) model designed for robust cross-embodiment generalization across diverse robotic platforms. While existing VLAs often struggle with morphological heterogeneity and data scarcity, we propose human-centric learning paradigm that treats human interaction traces as universal mother tongue for physical interaction. To support this, we present UniHand-2.0, the largest embodied pre-training recipe to date, comprising over 35,000 hours of multimodal data across 30 distinct robotic embodiments. Our approach introduces Unified Action Space that maps heterogeneous robot controls into semantically aligned slots, enabling low-resource robots to bootstrap skills from human data and high-resource platforms. Built upon this human-centric foundation, we design unified sequential modeling and multi-task pre-training paradigm to bridge human demonstrations and robotic execution. Architecturally, Being-H0.5 utilizes Mixture-of-Transformers design featuring novel Mixture-of-Flow (MoF) framework to decouple shared motor primitives from specialized embodiment-specific experts. Finally, to make cross-embodiment policies stable in the real world, we introduce Manifold-Preserving Gating for robustness under sensory shift and Universal Async Chunking to universalize chunked control across embodiments with different latency and control profiles. We empirically demonstrate that Being-H0.5 achieves state-of-the-art results on simulated benchmarks such as LIBERO (98.9%) and RoboCasa (53.9%), meanwhile performing strong cross-embodiment capabilities on five robotic platforms. Date: Jan 20,1 Introduction 2 Related Work 3 UniHand-2.0: Recipe for Large-Scale Human-Centric Learning 3.1 Human Demonstration Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2 Robot Manipulation Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.3 Visual-Text Understanding Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 UniCraftor: System for Portable, Extensible, and Affordable Data Collection 5 Being-H0.5: Foundational VLA Unifying Cross-Embodiment Control 5.1 Model Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.1.1 Unified State-Action Space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.1.2 Mixture of Flow . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.2 Pre-Training: Human-Centric Robot Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.2.1 Unified Sequence Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.2.2 Human-Centric Multi-Task Objective . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.2.3 Hybrid Human Motion Representation . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.3 Post-Training: Towards Cross-Embodiment Adaptation . . . . . . . . . . . . . . . . . . . . . 5.3.1 Embodiment-Specific Adaptation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.3.2 Manifold-Preserving Gating . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 4 6 7 10 11 11 14 14 15 16 17 17 17 18 19"
[21.01.2026 04:47] Mistral response. {"id": "3d9b843ce9914a9ba4080f85514c964f", "created": 1768970832, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1319, "total_tokens": 1325, "completion_tokens": 6}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[]\n```"}}]}
[21.01.2026 04:47] Response: ```python
[]
```
[21.01.2026 04:47] Deleting PDF ./assets/pdf/2601.12993.pdf.
[21.01.2026 04:47] Success.
[21.01.2026 04:47] Downloading and parsing paper https://huggingface.co/papers/2601.13288.
[21.01.2026 04:47] Extra JSON file exists (./assets/json/2601.13288.json), skip PDF parsing.
[21.01.2026 04:47] Paper image links file exists (./assets/img_data/2601.13288.json), skip HTML parsing.
[21.01.2026 04:47] Success.
[21.01.2026 04:47] Enriching papers with extra data.
[21.01.2026 04:47] ********************************************************************************
[21.01.2026 04:47] Abstract 0. UniX presents a unified medical foundation model that decouples visual understanding and generation tasks using distinct autoregressive and diffusion branches with cross-modal attention for enhanced performance.  					AI-generated summary 				 Despite recent progress, medical foundation models still...
[21.01.2026 04:47] ********************************************************************************
[21.01.2026 04:47] Abstract 1. WorldMind addresses the modal disconnect in LLMs by autonomously building a symbolic world knowledge repository that enhances physical feasibility and task optimality through experience-based learning.  					AI-generated summary 				 Current Large Language Models (LLMs) exhibit a critical modal disc...
[21.01.2026 04:47] ********************************************************************************
[21.01.2026 04:47] Abstract 2. A benchmark called MemoryRewardBench is introduced to systematically evaluate reward models' ability to assess long-term memory management in large language models across various context lengths and memory patterns.  					AI-generated summary 				 Existing works increasingly adopt memory-centric mec...
[21.01.2026 04:47] ********************************************************************************
[21.01.2026 04:47] Abstract 3. ToolPRMBench is introduced as a large-scale benchmark for evaluating process reward models in tool-using agents, featuring step-level test cases and multi-LLM verification to ensure data quality.  					AI-generated summary 				 Reward-guided search methods have demonstrated strong potential in enhan...
[21.01.2026 04:47] ********************************************************************************
[21.01.2026 04:47] Abstract 4. Large language models face significant challenges in software issue resolution, prompting the development of autonomous coding agents through various training-free and training-based methodologies.  					AI-generated summary 				 Issue resolution, a complex Software Engineering (SWE) task integral t...
[21.01.2026 04:47] ********************************************************************************
[21.01.2026 04:47] Abstract 5. Being-H0.5 is a Vision-Language-Action model that enables robust cross-embodiment generalization through human-centric learning and a Mixture-of-Transformers architecture with specialized embodiment handling.  					AI-generated summary 				 We introduce Being-H0.5, a foundational Vision-Language-Act...
[21.01.2026 04:47] ********************************************************************************
[21.01.2026 04:47] Abstract 6. Lightweight probes trained on hidden states of LLMs enable efficient classification tasks without additional computational overhead, improving safety and sentiment analysis performance.  					AI-generated summary 				 Production LLM systems often rely on separate models for safety and other classifi...
[21.01.2026 04:47] Read previous papers.
[21.01.2026 04:47] Generating reviews via LLM API.
[21.01.2026 04:47] Using data from previous issue: {"categories": ["#diffusion", "#training", "#data", "#architecture", "#open_source", "#multimodal", "#healthcare"], "emoji": "ğŸ«€", "ru": {"title": "Ğ¡Ğ¸Ğ½ĞµÑ€Ğ³Ğ¸Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ: Ñ€Ğ°Ğ·Ğ´ĞµĞ»Ñ‘Ğ½Ğ½Ñ‹Ğµ Ğ²ĞµÑ‚Ğ²Ğ¸ Ğ´Ğ»Ñ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°", "desc": "UniX â€” ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ-Ñ„ÑƒĞ½Ğ´Ğ°
[21.01.2026 04:47] Using data from previous issue: {"categories": ["#transfer_learning", "#alignment", "#hallucinations", "#reasoning"], "emoji": "ğŸŒ", "ru": {"title": "Ğ“Ñ€ounded Ğ¼Ğ¸Ñ€ Ğ´Ğ»Ñ LLM: Ğ¾Ñ‚ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğº Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¸Ğ¼Ñ‹Ğ¼ Ğ¿Ğ»Ğ°Ğ½Ğ°Ğ¼", "desc": "WorldMind Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ€Ğ°ÑÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³
[21.01.2026 04:47] Using data from previous issue: {"categories": ["#long_context", "#survey", "#benchmark", "#dataset"], "emoji": "ğŸ§ ", "ru": {"title": "ĞÑ†ĞµĞ½ĞºĞ° Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸: Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…", "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ MemoryRewardBench â€” Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´Ğµ
[21.01.2026 04:47] Querying the API.
[21.01.2026 04:47] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

ToolPRMBench is introduced as a large-scale benchmark for evaluating process reward models in tool-using agents, featuring step-level test cases and multi-LLM verification to ensure data quality.  					AI-generated summary 				 Reward-guided search methods have demonstrated strong potential in enhancing tool-using agents by effectively guiding sampling and exploration over complex action spaces. As a core design, those search methods utilize process reward models (PRMs) to provide step-level rewards, enabling more fine-grained monitoring. However, there is a lack of systematic and reliable evaluation benchmarks for PRMs in tool-using settings. In this paper, we introduce ToolPRMBench, a large-scale benchmark specifically designed to evaluate PRMs for tool-using agents. ToolPRMBench is built on top of several representative tool-using benchmarks and converts agent trajectories into step-level test cases. Each case contains the interaction history, a correct action, a plausible but incorrect alternative, and relevant tool metadata. We respectively utilize offline sampling to isolate local single-step errors and online sampling to capture realistic multi-step failures from full agent rollouts. A multi-LLM verification pipeline is proposed to reduce label noise and ensure data quality. We conduct extensive experiments across large language models, general PRMs, and tool-specialized PRMs on ToolPRMBench. The results reveal clear differences in PRM effectiveness and highlight the potential of specialized PRMs for tool-using. Code and data will be released at https://github.com/David-Li0406/ToolPRMBench.
[21.01.2026 04:47] Response: ```json
{
  "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ToolPRMBench â€” Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ°Ñ Ğ±Ğ°Ğ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑˆĞ°Ğ³Ğ¾Ğ² (process reward models) Ğ² Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ñ…, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹. Ğ”Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğµ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ğµ ÑĞ»ÑƒÑ‡Ğ°Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ² Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ĞºĞ°Ğº Ğ¾Ñ„Ğ»Ğ°Ğ¹Ğ½-ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº, Ñ‚Ğ°Ğº Ğ¸ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ¸Ğ· Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ² ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸.",
  "emoji": "ğŸ”§",
  "title": "Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ¾Ğ¼ Ğº Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼"
}
```
[21.01.2026 04:47] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"ToolPRMBench is introduced as a large-scale benchmark for evaluating process reward models in tool-using agents, featuring step-level test cases and multi-LLM verification to ensure data quality.  					AI-generated summary 				 Reward-guided search methods have demonstrated strong potential in enhancing tool-using agents by effectively guiding sampling and exploration over complex action spaces. As a core design, those search methods utilize process reward models (PRMs) to provide step-level rewards, enabling more fine-grained monitoring. However, there is a lack of systematic and reliable evaluation benchmarks for PRMs in tool-using settings. In this paper, we introduce ToolPRMBench, a large-scale benchmark specifically designed to evaluate PRMs for tool-using agents. ToolPRMBench is built on top of several representative tool-using benchmarks and converts agent trajectories into step-level test cases. Each case contains the interaction history, a correct action, a plausible but incorrect alternative, and relevant tool metadata. We respectively utilize offline sampling to isolate local single-step errors and online sampling to capture realistic multi-step failures from full agent rollouts. A multi-LLM verification pipeline is proposed to reduce label noise and ensure data quality. We conduct extensive experiments across large language models, general PRMs, and tool-specialized PRMs on ToolPRMBench. The results reveal clear differences in PRM effectiveness and highlight the potential of specialized PRMs for tool-using. Code and data will be released at https://github.com/David-Li0406/ToolPRMBench."

[21.01.2026 04:47] Response: ```python
["BENCHMARK", "AGENTS", "DATASET", "RL"]
```
[21.01.2026 04:47] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"ToolPRMBench is introduced as a large-scale benchmark for evaluating process reward models in tool-using agents, featuring step-level test cases and multi-LLM verification to ensure data quality.  					AI-generated summary 				 Reward-guided search methods have demonstrated strong potential in enhancing tool-using agents by effectively guiding sampling and exploration over complex action spaces. As a core design, those search methods utilize process reward models (PRMs) to provide step-level rewards, enabling more fine-grained monitoring. However, there is a lack of systematic and reliable evaluation benchmarks for PRMs in tool-using settings. In this paper, we introduce ToolPRMBench, a large-scale benchmark specifically designed to evaluate PRMs for tool-using agents. ToolPRMBench is built on top of several representative tool-using benchmarks and converts agent trajectories into step-level test cases. Each case contains the interaction history, a correct action, a plausible but incorrect alternative, and relevant tool metadata. We respectively utilize offline sampling to isolate local single-step errors and online sampling to capture realistic multi-step failures from full agent rollouts. A multi-LLM verification pipeline is proposed to reduce label noise and ensure data quality. We conduct extensive experiments across large language models, general PRMs, and tool-specialized PRMs on ToolPRMBench. The results reveal clear differences in PRM effectiveness and highlight the potential of specialized PRMs for tool-using. Code and data will be released at https://github.com/David-Li0406/ToolPRMBench."

[21.01.2026 04:47] Response: ```python
["REASONING", "OPEN_SOURCE"]
```

**Justification:**

1. **REASONING**: The paper focuses on process reward models (PRMs) that provide step-level rewards to guide sampling and exploration in tool-using agents. This directly relates to enhancing logical reasoning capabilities through reward-guided search methods that enable fine-grained monitoring of agent actions.

2. **OPEN_SOURCE**: The paper explicitly states "Code and data will be released at https://github.com/David-Li0406/ToolPRMBench," indicating the authors are contributing to open-source by releasing models, datasets, and frameworks to the public.
[21.01.2026 04:47] Error. Failed to parse JSON from LLM. ["REASONING", "OPEN_SOURCE"]


**Justification:**

1. **REASONING**: The paper focuses on process reward models (PRMs) that provide step-level rewards to guide sampling and exploration in tool-using agents. This directly relates to enhancing logical reasoning capabilities through reward-guided search methods that enable fine-grained monitoring of agent actions.

2. **OPEN_SOURCE**: The paper explicitly states "Code and data will be released at https://github.com/David-Li0406/ToolPRMBench," indicating the authors are contributing to open-source by releasing models, datasets, and frameworks to the public.
[21.01.2026 04:47] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents ToolPRMBench, a comprehensive benchmark designed to evaluate process reward models (PRMs) specifically for tool-using agents. It addresses the need for systematic evaluation by converting agent trajectories into detailed step-level test cases, which include correct and plausible incorrect actions. The benchmark employs both offline and online sampling methods to identify single-step and multi-step errors, ensuring a thorough assessment of PRM performance. The findings demonstrate significant variations in the effectiveness of different PRMs, emphasizing the advantages of specialized models for enhancing tool-using capabilities.","title":"ToolPRMBench: Elevating Evaluation of Process Reward Models for Tool-Using Agents"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper presents ToolPRMBench, a comprehensive benchmark designed to evaluate process reward models (PRMs) specifically for tool-using agents. It addresses the need for systematic evaluation by converting agent trajectories into detailed step-level test cases, which include correct and plausible incorrect actions. The benchmark employs both offline and online sampling methods to identify single-step and multi-step errors, ensuring a thorough assessment of PRM performance. The findings demonstrate significant variations in the effectiveness of different PRMs, emphasizing the advantages of specialized models for enhancing tool-using capabilities.', title='ToolPRMBench: Elevating Evaluation of Process Reward Models for Tool-Using Agents'))
[21.01.2026 04:47] Response: ParsedChatCompletionMessage[Article](content='{"desc":"æœ¬æ–‡ä»‹ç»äº†ToolPRMBenchï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°å·¥å…·ä½¿ç”¨ä»£ç†çš„è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰çš„è§„æ¨¡åŒ–åŸºå‡†ã€‚è¯¥åŸºå‡†é€šè¿‡å°†ä»£ç†è½¨è¿¹è½¬æ¢ä¸ºé€æ­¥æµ‹è¯•æ¡ˆä¾‹ï¼Œæä¾›äº†æ›´ç»†è‡´çš„ç›‘æ§å’Œè¯„ä¼°ã€‚æˆ‘ä»¬é‡‡ç”¨ç¦»çº¿é‡‡æ ·å’Œåœ¨çº¿é‡‡æ ·çš„æ–¹æ³•ï¼Œåˆ†åˆ«æ•æ‰å±€éƒ¨å•æ­¥é”™è¯¯å’ŒçœŸå®çš„å¤šæ­¥å¤±è´¥ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œä¸“é—¨åŒ–çš„PRMåœ¨å·¥å…·ä½¿ç”¨ä¸­çš„æœ‰æ•ˆæ€§æ˜æ˜¾ä¼˜äºé€šç”¨PRMï¼Œå±•ç¤ºäº†å…¶æ½œåŠ›ã€‚","title":"ToolPRMBenchï¼šè¯„ä¼°å·¥å…·ä½¿ç”¨ä»£ç†çš„è¿‡ç¨‹å¥–åŠ±æ¨¡å‹æ–°åŸºå‡†"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='æœ¬æ–‡ä»‹ç»äº†ToolPRMBenchï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°å·¥å…·ä½¿ç”¨ä»£ç†çš„è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰çš„è§„æ¨¡åŒ–åŸºå‡†ã€‚è¯¥åŸºå‡†é€šè¿‡å°†ä»£ç†è½¨è¿¹è½¬æ¢ä¸ºé€æ­¥æµ‹è¯•æ¡ˆä¾‹ï¼Œæä¾›äº†æ›´ç»†è‡´çš„ç›‘æ§å’Œè¯„ä¼°ã€‚æˆ‘ä»¬é‡‡ç”¨ç¦»çº¿é‡‡æ ·å’Œåœ¨çº¿é‡‡æ ·çš„æ–¹æ³•ï¼Œåˆ†åˆ«æ•æ‰å±€éƒ¨å•æ­¥é”™è¯¯å’ŒçœŸå®çš„å¤šæ­¥å¤±è´¥ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œä¸“é—¨åŒ–çš„PRMåœ¨å·¥å…·ä½¿ç”¨ä¸­çš„æœ‰æ•ˆæ€§æ˜æ˜¾ä¼˜äºé€šç”¨PRMï¼Œå±•ç¤ºäº†å…¶æ½œåŠ›ã€‚', title='ToolPRMBenchï¼šè¯„ä¼°å·¥å…·ä½¿ç”¨ä»£ç†çš„è¿‡ç¨‹å¥–åŠ±æ¨¡å‹æ–°åŸºå‡†'))
[21.01.2026 04:47] Using data from previous issue: {"categories": ["#rl", "#benchmark", "#dataset", "#agents", "#data", "#training", "#open_source", "#plp", "#survey"], "emoji": "ğŸ¤–", "ru": {"title": "ĞĞ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ñ‹Ñ… Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº", "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ÑÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ñ… Ğ°Ğ³ĞµĞ½
[21.01.2026 04:47] Querying the API.
[21.01.2026 04:47] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Being-H0.5 is a Vision-Language-Action model that enables robust cross-embodiment generalization through human-centric learning and a Mixture-of-Transformers architecture with specialized embodiment handling.  					AI-generated summary 				 We introduce Being-H0.5, a foundational Vision-Language-Action (VLA) model designed for robust cross-embodiment generalization across diverse robotic platforms. While existing VLAs often struggle with morphological heterogeneity and data scarcity, we propose a human-centric learning paradigm that treats human interaction traces as a universal "mother tongue" for physical interaction. To support this, we present UniHand-2.0, the largest embodied pre-training recipe to date, comprising over 35,000 hours of multimodal data across 30 distinct robotic embodiments. Our approach introduces a Unified Action Space that maps heterogeneous robot controls into semantically aligned slots, enabling low-resource robots to bootstrap skills from human data and high-resource platforms. Built upon this human-centric foundation, we design a unified sequential modeling and multi-task pre-training paradigm to bridge human demonstrations and robotic execution. Architecturally, Being-H0.5 utilizes a Mixture-of-Transformers design featuring a novel Mixture-of-Flow (MoF) framework to decouple shared motor primitives from specialized embodiment-specific experts. Finally, to make cross-embodiment policies stable in the real world, we introduce Manifold-Preserving Gating for robustness under sensory shift and Universal Async Chunking to universalize chunked control across embodiments with different latency and control profiles. We empirically demonstrate that Being-H0.5 achieves state-of-the-art results on simulated benchmarks, such as LIBERO (98.9%) and RoboCasa (53.9%), while also exhibiting strong cross-embodiment capabilities on five robotic platforms.
[21.01.2026 04:47] Response: ```json
{
  "desc": "Being-H0.5 â€” ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Vision-Language-Action (VLA), Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ Ğ´Ğ»Ñ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ¼Ğ¸ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾-Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡Ğ½Ğ¾Ğ¼Ñƒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñƒ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ°Ğº ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞ·Ñ‹Ğº Ğ´Ğ»Ñ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ UniHand-2.0 â€” ĞºÑ€ÑƒĞ¿Ğ½ĞµĞ¹ÑˆĞ¸Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¸Ğ· 35 Ñ‚Ñ‹ÑÑÑ‡ Ñ‡Ğ°ÑĞ¾Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ 30 Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°. ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Mixture-of-Transformers Ñ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Mixture-of-Flow (MoF) Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ¾Ğ±Ñ‰Ğ¸Ğµ Ğ¼Ğ¾Ñ‚Ğ¾Ñ€Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ğ²Ñ‹ Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ñ‚Ğ¸Ğ¿Ğ° Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… LIBERO (98.9%) Ğ¸ RoboCasa (53.9%), Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¿ÑÑ‚Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ….",
  "emoji": "ğŸ¤–",
  "title": "Ğ§ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¹ ÑĞ·Ñ‹Ğº Ğ´Ğ»Ñ Ğ²ÑĞµÑ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ²: universal VLA Ñ ĞºÑ€Ğ¾ÑÑĞ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸ĞµĞ¼"
}
```
[21.01.2026 04:47] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Being-H0.5 is a Vision-Language-Action model that enables robust cross-embodiment generalization through human-centric learning and a Mixture-of-Transformers architecture with specialized embodiment handling.  					AI-generated summary 				 We introduce Being-H0.5, a foundational Vision-Language-Action (VLA) model designed for robust cross-embodiment generalization across diverse robotic platforms. While existing VLAs often struggle with morphological heterogeneity and data scarcity, we propose a human-centric learning paradigm that treats human interaction traces as a universal "mother tongue" for physical interaction. To support this, we present UniHand-2.0, the largest embodied pre-training recipe to date, comprising over 35,000 hours of multimodal data across 30 distinct robotic embodiments. Our approach introduces a Unified Action Space that maps heterogeneous robot controls into semantically aligned slots, enabling low-resource robots to bootstrap skills from human data and high-resource platforms. Built upon this human-centric foundation, we design a unified sequential modeling and multi-task pre-training paradigm to bridge human demonstrations and robotic execution. Architecturally, Being-H0.5 utilizes a Mixture-of-Transformers design featuring a novel Mixture-of-Flow (MoF) framework to decouple shared motor primitives from specialized embodiment-specific experts. Finally, to make cross-embodiment policies stable in the real world, we introduce Manifold-Preserving Gating for robustness under sensory shift and Universal Async Chunking to universalize chunked control across embodiments with different latency and control profiles. We empirically demonstrate that Being-H0.5 achieves state-of-the-art results on simulated benchmarks, such as LIBERO (98.9%) and RoboCasa (53.9%), while also exhibiting strong cross-embodiment capabilities on five robotic platforms."

[21.01.2026 04:47] Response: ```python
['ROBOTICS', 'MULTIMODAL', 'ARCHITECTURE', 'DATASET', 'TRAINING']
```
[21.01.2026 04:47] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Being-H0.5 is a Vision-Language-Action model that enables robust cross-embodiment generalization through human-centric learning and a Mixture-of-Transformers architecture with specialized embodiment handling.  					AI-generated summary 				 We introduce Being-H0.5, a foundational Vision-Language-Action (VLA) model designed for robust cross-embodiment generalization across diverse robotic platforms. While existing VLAs often struggle with morphological heterogeneity and data scarcity, we propose a human-centric learning paradigm that treats human interaction traces as a universal "mother tongue" for physical interaction. To support this, we present UniHand-2.0, the largest embodied pre-training recipe to date, comprising over 35,000 hours of multimodal data across 30 distinct robotic embodiments. Our approach introduces a Unified Action Space that maps heterogeneous robot controls into semantically aligned slots, enabling low-resource robots to bootstrap skills from human data and high-resource platforms. Built upon this human-centric foundation, we design a unified sequential modeling and multi-task pre-training paradigm to bridge human demonstrations and robotic execution. Architecturally, Being-H0.5 utilizes a Mixture-of-Transformers design featuring a novel Mixture-of-Flow (MoF) framework to decouple shared motor primitives from specialized embodiment-specific experts. Finally, to make cross-embodiment policies stable in the real world, we introduce Manifold-Preserving Gating for robustness under sensory shift and Universal Async Chunking to universalize chunked control across embodiments with different latency and control profiles. We empirically demonstrate that Being-H0.5 achieves state-of-the-art results on simulated benchmarks, such as LIBERO (98.9%) and RoboCasa (53.9%), while also exhibiting strong cross-embodiment capabilities on five robotic platforms."

[21.01.2026 04:47] Response: ```python
['TRANSFER_LEARNING', 'OPTIMIZATION', 'OPEN_SOURCE']
```

**Justification:**

- **TRANSFER_LEARNING**: The paper explicitly discusses cross-embodiment generalization and knowledge transfer across diverse robotic platforms. The human-centric learning paradigm treats human interaction traces as a universal foundation for physical interaction, enabling low-resource robots to bootstrap skills from human data and high-resource platforms.

- **OPTIMIZATION**: The paper presents architectural innovations (Mixture-of-Transformers, Mixture-of-Flow framework, Manifold-Preserving Gating, Universal Async Chunking) and multi-task pre-training paradigms designed to optimize model performance and robustness.

- **OPEN_SOURCE**: The paper introduces UniHand-2.0, described as "the largest embodied pre-training recipe to date, comprising over 35,000 hours of multimodal data across 30 distinct robotic embodiments," which represents a significant contribution of a dataset/resource to the community.
[21.01.2026 04:47] Error. Failed to parse JSON from LLM. ["TRANSFER_LEARNING", "OPTIMIZATION", "OPEN_SOURCE"]


**Justification:**

- **TRANSFER_LEARNING**: The paper explicitly discusses cross-embodiment generalization and knowledge transfer across diverse robotic platforms. The human-centric learning paradigm treats human interaction traces as a universal foundation for physical interaction, enabling low-resource robots to bootstrap skills from human data and high-resource platforms.

- **OPTIMIZATION**: The paper presents architectural innovations (Mixture-of-Transformers, Mixture-of-Flow framework, Manifold-Preserving Gating, Universal Async Chunking) and multi-task pre-training paradigms designed to optimize model performance and robustness.

- **OPEN_SOURCE**: The paper introduces UniHand-2.0, described as "the largest embodied pre-training recipe to date, comprising over 35,000 hours of multimodal data across 30 distinct robotic embodiments," which represents a significant contribution of a dataset/resource to the community.
[21.01.2026 04:47] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Being-H0.5 is a Vision-Language-Action model that enhances the ability of robots to generalize across different physical forms. It employs a human-centric learning approach, using human interaction data as a foundational guide for robotic actions. The model is built on a Mixture-of-Transformers architecture, which allows it to effectively manage various robotic embodiments and their unique control requirements. With its innovative techniques like Manifold-Preserving Gating and Universal Async Chunking, Being-H0.5 demonstrates impressive performance on multiple robotic platforms, achieving state-of-the-art results in simulated environments.","title":"Empowering Robots with Human-Centric Learning for Cross-Embodiment Mastery"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Being-H0.5 is a Vision-Language-Action model that enhances the ability of robots to generalize across different physical forms. It employs a human-centric learning approach, using human interaction data as a foundational guide for robotic actions. The model is built on a Mixture-of-Transformers architecture, which allows it to effectively manage various robotic embodiments and their unique control requirements. With its innovative techniques like Manifold-Preserving Gating and Universal Async Chunking, Being-H0.5 demonstrates impressive performance on multiple robotic platforms, achieving state-of-the-art results in simulated environments.', title='Empowering Robots with Human-Centric Learning for Cross-Embodiment Mastery'))
[21.01.2026 04:47] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Being-H0.5æ˜¯ä¸€ç§è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼Œæ—¨åœ¨é€šè¿‡ä»¥äººä¸ºä¸­å¿ƒçš„å­¦ä¹ å’Œæ··åˆå˜æ¢å™¨æ¶æ„å®ç°å¼ºå¤§çš„è·¨ä½“ç°æ³›åŒ–èƒ½åŠ›ã€‚è¯¥æ¨¡å‹åˆ©ç”¨äººç±»äº¤äº’è½¨è¿¹ä½œä¸ºç‰©ç†äº¤äº’çš„é€šç”¨â€œæ¯è¯­â€ï¼Œè§£å†³äº†ç°æœ‰æ¨¡å‹åœ¨å½¢æ€å¼‚è´¨æ€§å’Œæ•°æ®ç¨€ç¼ºæ–¹é¢çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºçš„UniHand-2.0æ˜¯è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„å…·èº«é¢„è®­ç»ƒæ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡35,000å°æ—¶çš„å¤šæ¨¡æ€æ•°æ®ï¼Œæ¶µç›–30ç§ä¸åŒçš„æœºå™¨äººä½“ç°ã€‚é€šè¿‡ç»Ÿä¸€çš„åŠ¨ä½œç©ºé—´å’Œå¤šä»»åŠ¡é¢„è®­ç»ƒèŒƒå¼ï¼ŒBeing-H0.5èƒ½å¤Ÿåœ¨ä¸åŒçš„æœºå™¨äººå¹³å°ä¸Šå®ç°ç¨³å®šçš„è·¨ä½“ç°ç­–ç•¥ã€‚","title":"è·¨ä½“ç°æ³›åŒ–çš„å¼ºå¤§æ¨¡å‹"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Being-H0.5æ˜¯ä¸€ç§è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼Œæ—¨åœ¨é€šè¿‡ä»¥äººä¸ºä¸­å¿ƒçš„å­¦ä¹ å’Œæ··åˆå˜æ¢å™¨æ¶æ„å®ç°å¼ºå¤§çš„è·¨ä½“ç°æ³›åŒ–èƒ½åŠ›ã€‚è¯¥æ¨¡å‹åˆ©ç”¨äººç±»äº¤äº’è½¨è¿¹ä½œä¸ºç‰©ç†äº¤äº’çš„é€šç”¨â€œæ¯è¯­â€ï¼Œè§£å†³äº†ç°æœ‰æ¨¡å‹åœ¨å½¢æ€å¼‚è´¨æ€§å’Œæ•°æ®ç¨€ç¼ºæ–¹é¢çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºçš„UniHand-2.0æ˜¯è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„å…·èº«é¢„è®­ç»ƒæ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡35,000å°æ—¶çš„å¤šæ¨¡æ€æ•°æ®ï¼Œæ¶µç›–30ç§ä¸åŒçš„æœºå™¨äººä½“ç°ã€‚é€šè¿‡ç»Ÿä¸€çš„åŠ¨ä½œç©ºé—´å’Œå¤šä»»åŠ¡é¢„è®­ç»ƒèŒƒå¼ï¼ŒBeing-H0.5èƒ½å¤Ÿåœ¨ä¸åŒçš„æœºå™¨äººå¹³å°ä¸Šå®ç°ç¨³å®šçš„è·¨ä½“ç°ç­–ç•¥ã€‚', title='è·¨ä½“ç°æ³›åŒ–çš„å¼ºå¤§æ¨¡å‹'))
[21.01.2026 04:47] Using data from previous issue: {"categories": ["#training", "#inference", "#small_models"], "emoji": "ğŸ”", "ru": {"title": "Ğ›Ñ‘Ğ³ĞºĞ¸Ğµ Ğ·Ğ¾Ğ½Ğ´Ñ‹ Ğ´Ğ»Ñ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸: Ğ¿ĞµÑ€ĞµĞ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ LLM Ğ±ĞµĞ· Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹", "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ»Ñ‘Ğ³ĞºĞ¸Ñ… Ğ·Ğ¾Ğ½Ğ´Ğ¾Ğ² (probes) Ğ½Ğ° ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸ÑÑ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ²Ñ‹
[21.01.2026 04:47] Renaming data file.
[21.01.2026 04:47] Renaming previous data. hf_papers.json to ./d/2026-01-21.json
[21.01.2026 04:47] Saving new data file.
[21.01.2026 04:47] Generating page.
[21.01.2026 04:47] Renaming previous page.
[21.01.2026 04:47] Renaming previous data. index.html to ./d/2026-01-21.html
[21.01.2026 04:47] Writing result.
[21.01.2026 04:47] Renaming log file.
[21.01.2026 04:47] Renaming previous data. log.txt to ./logs/2026-01-21_last_log.txt
