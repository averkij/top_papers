[21.01.2026 05:31] Read previous papers.
[21.01.2026 05:31] Generating top page (month).
[21.01.2026 05:31] Writing top page (month).
[21.01.2026 06:39] Read previous papers.
[21.01.2026 06:39] Get feed.
[21.01.2026 06:39] Get page data from previous paper. URL: https://huggingface.co/papers/2601.11522
[21.01.2026 06:39] Get page data from previous paper. URL: https://huggingface.co/papers/2601.11655
[21.01.2026 06:39] Get page data from previous paper. URL: https://huggingface.co/papers/2601.12993
[21.01.2026 06:39] Get page data from previous paper. URL: https://huggingface.co/papers/2601.12294
[21.01.2026 06:39] Get page data from previous paper. URL: https://huggingface.co/papers/2601.11969
[21.01.2026 06:39] Get page data from previous paper. URL: https://huggingface.co/papers/2601.13247
[21.01.2026 06:39] Get page data from previous paper. URL: https://huggingface.co/papers/2601.14250
[21.01.2026 06:39] Get page data from previous paper. URL: https://huggingface.co/papers/2601.14192
[21.01.2026 06:39] Get page data from previous paper. URL: https://huggingface.co/papers/2601.14046
[21.01.2026 06:39] Get page data from previous paper. URL: https://huggingface.co/papers/2601.13288
[21.01.2026 06:39] Extract page data from URL. URL: https://huggingface.co/papers/2601.10700
[21.01.2026 06:39] Extract page data from URL. URL: https://huggingface.co/papers/2601.13253
[21.01.2026 06:39] Extract page data from URL. URL: https://huggingface.co/papers/2601.13251
[21.01.2026 06:39] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[21.01.2026 06:39] No deleted papers detected.
[21.01.2026 06:39] Downloading and parsing papers (pdf, html). Total: 13.
[21.01.2026 06:39] Downloading and parsing paper https://huggingface.co/papers/2601.11522.
[21.01.2026 06:39] Extra JSON file exists (./assets/json/2601.11522.json), skip PDF parsing.
[21.01.2026 06:39] Paper image links file exists (./assets/img_data/2601.11522.json), skip HTML parsing.
[21.01.2026 06:39] Success.
[21.01.2026 06:39] Downloading and parsing paper https://huggingface.co/papers/2601.11655.
[21.01.2026 06:39] Extra JSON file exists (./assets/json/2601.11655.json), skip PDF parsing.
[21.01.2026 06:39] Paper image links file exists (./assets/img_data/2601.11655.json), skip HTML parsing.
[21.01.2026 06:39] Success.
[21.01.2026 06:39] Downloading and parsing paper https://huggingface.co/papers/2601.12993.
[21.01.2026 06:39] Extra JSON file exists (./assets/json/2601.12993.json), skip PDF parsing.
[21.01.2026 06:39] Paper image links file exists (./assets/img_data/2601.12993.json), skip HTML parsing.
[21.01.2026 06:39] Success.
[21.01.2026 06:39] Downloading and parsing paper https://huggingface.co/papers/2601.12294.
[21.01.2026 06:39] Extra JSON file exists (./assets/json/2601.12294.json), skip PDF parsing.
[21.01.2026 06:39] Paper image links file exists (./assets/img_data/2601.12294.json), skip HTML parsing.
[21.01.2026 06:39] Success.
[21.01.2026 06:39] Downloading and parsing paper https://huggingface.co/papers/2601.11969.
[21.01.2026 06:39] Extra JSON file exists (./assets/json/2601.11969.json), skip PDF parsing.
[21.01.2026 06:39] Paper image links file exists (./assets/img_data/2601.11969.json), skip HTML parsing.
[21.01.2026 06:39] Success.
[21.01.2026 06:39] Downloading and parsing paper https://huggingface.co/papers/2601.13247.
[21.01.2026 06:39] Extra JSON file exists (./assets/json/2601.13247.json), skip PDF parsing.
[21.01.2026 06:39] Paper image links file exists (./assets/img_data/2601.13247.json), skip HTML parsing.
[21.01.2026 06:39] Success.
[21.01.2026 06:39] Downloading and parsing paper https://huggingface.co/papers/2601.14250.
[21.01.2026 06:39] Extra JSON file exists (./assets/json/2601.14250.json), skip PDF parsing.
[21.01.2026 06:39] Paper image links file exists (./assets/img_data/2601.14250.json), skip HTML parsing.
[21.01.2026 06:39] Success.
[21.01.2026 06:39] Downloading and parsing paper https://huggingface.co/papers/2601.14192.
[21.01.2026 06:39] Extra JSON file exists (./assets/json/2601.14192.json), skip PDF parsing.
[21.01.2026 06:39] Paper image links file exists (./assets/img_data/2601.14192.json), skip HTML parsing.
[21.01.2026 06:39] Success.
[21.01.2026 06:39] Downloading and parsing paper https://huggingface.co/papers/2601.14046.
[21.01.2026 06:39] Extra JSON file exists (./assets/json/2601.14046.json), skip PDF parsing.
[21.01.2026 06:39] Paper image links file exists (./assets/img_data/2601.14046.json), skip HTML parsing.
[21.01.2026 06:39] Success.
[21.01.2026 06:39] Downloading and parsing paper https://huggingface.co/papers/2601.13288.
[21.01.2026 06:39] Extra JSON file exists (./assets/json/2601.13288.json), skip PDF parsing.
[21.01.2026 06:39] Paper image links file exists (./assets/img_data/2601.13288.json), skip HTML parsing.
[21.01.2026 06:39] Success.
[21.01.2026 06:39] Downloading and parsing paper https://huggingface.co/papers/2601.10700.
[21.01.2026 06:39] Downloading paper 2601.10700 from https://arxiv.org/pdf/2601.10700v2...
[21.01.2026 06:39] Extracting affiliations from text.
[21.01.2026 06:39] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 8 1 ] . [ 2 0 0 7 0 1 . 1 0 6 2 : r LIBERTy: Causal Framework for Benchmarking Concept-Based Explanations of LLMs with Structural Counterfactuals Gilat Toker*, Nitay Calderon*, Ohad Amosy, Roi Reichart Faculty of Data and Decision Sciences, Technion Israel Institute of Technology {gilatt, nitay}@campus.technion.ac.il, roiri@technion.ac.il *Second author supervised the project and led the writing. "
[21.01.2026 06:39] Response: ```python
["Technion Israel Institute of Technology"]
```
[21.01.2026 06:39] Deleting PDF ./assets/pdf/2601.10700.pdf.
[21.01.2026 06:39] Success.
[21.01.2026 06:39] Downloading and parsing paper https://huggingface.co/papers/2601.13253.
[21.01.2026 06:39] Downloading paper 2601.13253 from https://arxiv.org/pdf/2601.13253v1...
[21.01.2026 06:39] Extracting affiliations from text.
[21.01.2026 06:39] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 9 1 ] . [ 1 3 5 2 3 1 . 1 0 6 2 : r Hybrid Protocol for Large-Scale Semantic Dataset Generation in Low-Resource Languages: The Turkish Semantic Relations Corpus Ebubekir Tosun ebubekirsoftware@gmail.com Mehmet Emin Buldur mehmeteminbuldur@outlook.com √ñzay Ezerceli ezerceli804@gmail.com Mahmoud ElHussieni mahmoud.elhussieni@outlook.com "
[21.01.2026 06:39] Response: ```python
[]
```
[21.01.2026 06:39] Extracting affiliations from text.
[21.01.2026 06:39] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 9 1 ] . [ 1 3 5 2 3 1 . 1 0 6 2 : r Hybrid Protocol for Large-Scale Semantic Dataset Generation in Low-Resource Languages: The Turkish Semantic Relations Corpus Ebubekir Tosun ebubekirsoftware@gmail.com Mehmet Emin Buldur mehmeteminbuldur@outlook.com √ñzay Ezerceli ezerceli804@gmail.com Mahmoud ElHussieni mahmoud.elhussieni@outlook.comWe present hybrid methodology for generating large-scale semantic relationship datasets in low-resource languages, demonstrated through comprehensive Turkish semantic relations corpus. Our approach integrates three phases: (1) FastText embeddings with Agglomerative Clustering to identify semantic clusters, (2) Gemini 2.5-Flash for automated semantic relationship classification, and (3) integration with curated dictionary sources. The resulting dataset comprises 843,000 unique Turkish semantic pairs across three relationship types (synonyms, antonyms, co-hyponyms) representing 10 scale increase over existing resources at minimal cost ($65). We validate the dataset through two downstream tasks: an embedding model achieving 90% top-1 retrieval accuracy and classification model attaining 90% F1-macro. Our scalable protocol addresses critical data scarcity in Turkish NLP and demonstrates applicability to other lowresource languages. We publicly release the dataset and models.Turkish, despite having population of over 88 million speakers, lacks comprehensive semantic relationship datasets comparable to those available for English and other resource-rich languages (Miller, 1995; Navigli and Ponzetto, 2012). This scarcity impedes the development of semantic parsing systems, word sense disambiguation tools, and semantic similarity models for Turkish NLP applications. While significant efforts have been made to construct Turkish resources through manual annotation and dictionary mining (Bakay et al., 2021; Ehsani et al., 2018), the limited scale and domain coverage of existing resources present bottleneck for modern neural NLP systems that require millions of training examples (Devlin et al., 2019). The challenge of building semantic resources for morphologically rich languages like Turkish is 1 compounded by several factors. First, the agglutinative nature of Turkish means that single root can generate hundreds of valid word forms through productive suffixation, requiring substantially larger vocabularies to achieve equivalent coverage compared to analytic languages. Second, existing resources such as Turkish Tree Bank (Ehsani et al., 2018) and KeNet (Bakay et al., 2021) rely primarily on translation-based projections from English WordNet or limited manual curation, inheriting biases and leaving domain-specific terminology (particularly in legal, medical, and technical domains) largely uncovered. Third, the cost of manual annotation at scale is prohibitive for academic research budgets. We propose hybrid protocol that targets both computational efficiency and linguistic quality. The method proceeds in three phases. Context Preparation We start from an expertcurated lexicon of 77,000 terms and expand it to 110,000 entries using Named Entity Recognitionbased augmentation. We then compute FastText embeddings and apply Agglomerative Clustering, yielding 13,000 semantic clusters that act as contextual structure for downstream relationship classification. LLM-Based Semantic Enrichment In the second phase, we employ Gemini 2.5-Flash to automatically induce and label diverse semantic relationships within clusters. The model exploits both cluster-level context and its multilingual knowledge to generate high-quality relationship labels at an overall cost of about $65. Dictionary Integration Finally, we incorporate an external Turkish Synonym Dictionary comprising roughly 20,000 entries. After applying strict filters that retain only high-precision cases with at most two synonym candidates, we obtain 16,000 validated synonym pairs. This hybrid design couples large-scale automation with explicit quality control and yields 843,000 unique semantic pairs, while dictionary-based validation helps preserve semantic coherence across the induced relations. We make three primary contributions: 1. scalable hybrid methodology combining embedding-based clustering with LLM enrichment that is generalizable to other lowresource languages with minimal languagespecific adaptation. 2. The Turkish Semantic Relations Corpus, comprising 843,000 annotated semantic pairs across three relationship types (synonym, antonym, co-hyponym) in JSONL format, representing the largest native Turkish semantic resource to date. 3. Validation through downstream models: an embedding model achieving 90% top-1 retrieval accuracy for synonym pairs and classification model achieving 90% F1-macro, demonstrating the datasets utility for training production-quality semantic systems.2.2 LLM-Augmented Dataset Generation The emergence of large language models has opened new possibilities for dataset generation at scale. Recent work has demonstrated that LLMs can effectively generate training data for various NLP tasks, including semantic similarity (Schick and Sch√ºtze, 2021), natural language inference (Wang et al., 2021), and question answering (Alberti et al., 2019). The key insight is that LLMs, having been trained on massive multilingual corpora, encode substantial world knowledge that can be extracted through careful prompting. Several studies have specifically examined LLMgenerated semantic relationships. Chen et al. (2023) showed that GPT-4 can generate highquality synonym and antonym pairs with accuracy comparable to crowdsourced annotations when provided with appropriate context. Kumar et al. (2023) demonstrated that data augmentation through LLM paraphrasing significantly improves downstream task performance for low-resource settings. Our work extends this line of research by combining LLM generation with clustering-based context provision and dictionary validation. 2.1 Semantic Resources for Low-Resource 2.3 Embedding-Based Clustering for Languages Semantic Organization The construction of large-scale semantic resources has historically been dominated by manual curation efforts. Princeton WordNet (Miller, 1995) established the paradigm of organizing lexical knowledge through synsets connected by semantic relations, inspiring similar projects across dozens of languages through the Global WordNet Association (Bond and Paik, 2012). However, the laborintensive nature of WordNet construction has limite"
[21.01.2026 06:39] Mistral response. {"id": "0e831a0e89154a0182d2d5a231cbd7e6", "created": 1768977570, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1402, "total_tokens": 1408, "completion_tokens": 6}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[]\n```"}}]}
[21.01.2026 06:39] Response: ```python
[]
```
[21.01.2026 06:39] Deleting PDF ./assets/pdf/2601.13253.pdf.
[21.01.2026 06:39] Success.
[21.01.2026 06:39] Downloading and parsing paper https://huggingface.co/papers/2601.13251.
[21.01.2026 06:39] Downloading paper 2601.13251 from https://arxiv.org/pdf/2601.13251v1...
[21.01.2026 06:39] Extracting affiliations from text.
[21.01.2026 06:39] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 9 1 ] . [ 1 1 5 2 3 1 . 1 0 6 2 : r Beyond Cosine Similarity: Taming Semantic Drift and Antonym Intrusion in 15-Million Node Turkish Synonym Graph Ebubekir Tosun ebubekirsoftware@gmail.com Mehmet Emin Buldur mehmeteminbuldur@outlook.com √ñzay Ezerceli ezerceli804@gmail.com Mahmoud ElHussieni mahmoud.elhussieni@outlook.com "
[21.01.2026 06:39] Response: ```python
[]
```
[21.01.2026 06:39] Extracting affiliations from text.
[21.01.2026 06:39] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 9 1 ] . [ 1 1 5 2 3 1 . 1 0 6 2 : r Beyond Cosine Similarity: Taming Semantic Drift and Antonym Intrusion in 15-Million Node Turkish Synonym Graph Ebubekir Tosun ebubekirsoftware@gmail.com Mehmet Emin Buldur mehmeteminbuldur@outlook.com √ñzay Ezerceli ezerceli804@gmail.com Mahmoud ElHussieni mahmoud.elhussieni@outlook.comNeural embeddings have notorious blind spot: they cant reliably tell synonyms apart from antonyms. Consequently, increasing similarity thresholds often fails to prevent opposites from being grouped together. Weve built large-scale semantic clustering system specifically designed to tackle this problem headon. Our pipeline chews through 15 million lexical items, evaluates massive 520 million potential relationships, and ultimately generates 2.9 million high-precision semantic clusters. The system makes three primary contributions. First, we introduce labeled dataset of 843,000 concept pairs spanning synonymy, antonymy, and co-hyponymy, constructed via Gemini 2.5Flash LLM augmentation and verified using human-curated dictionary resources. Second, we propose specialized three-way semantic relation discriminator that achieves 90% macro-F1, enabling robust disambiguation beyond raw embedding similarity. Third, we introduce novel soft-to-hard clustering algorithm that mitigates semantic drift preventing erroneous transitive chains (e.g., hot spicy pain depression) while simultaneously resolving polysemy. Our approach employs topology-aware two-stage expansionpruning procedure with topological voting, ensuring that each term is assigned to exactly one semantically coherent cluster. The resulting resource enables high-precision semantic search and retrieval-augmented generation, particularly for morphologically rich and low-resource languages where existing synonym databases remain sparse.The foundational assumption underlying most neural semantic search systems that high cosine similarity between embedding vectors correlates with synonymy is demonstrably false. Embedding models trained on contrastive objectives (Wang and Isola, 2020) or word2vec-style objectives (Levy 1 and Goldberg, 2014) optimize for capturing distributional similarity, which conflates multiple semantic relationships into single continuous measure (Elekes et al., 2017; Schneidermann et al., 2020). Antonyms and synonyms alike often occupy proximate regions in embedding space (Scheible et al., 2013; Rizkallah et al., 2020; Samenko et al., 2020) because they appear in nearly identical linguistic contexts (Liu et al., 2017; Chaurasia and McDonnell, 2016), merely differing in implicit negation or opposition. This issue is especially pronounced when synonym databases are built by applying high similarity cutoff (e.g., cosine similarity 0.85), which often admits false positives such as antonyms or broadly related terms. At scale, two factors further aggravate semantic ambiguity in graph clustering. First, modularitybased methods such as Louvain (Blondel et al., 2008) and Leiden (Traag et al., 2019) impose hard partitions, forcing polysemous words into single cluster. For example, the Turkish word y√ºz (face / 100) cannot be represented in multiple senses simultaneously, which either removes valid synonym links or collapses unrelated regions into noisy clusters. Second, semantic drift arises from transitivity: chains of locally plausible links can connect semantically distant concepts (e.g., Sƒ±cak hot Acƒ± spicy √úz√ºnt√º pain Depresyon depression), and standard clustering methods (e.g., connected components or hierarchical agglomeration) typically cannot distinguish such spurious paths from genuine synonym structure. These difficulties are amplified in morphologically rich, non-English settings. Turkish, in particular, lacks large machine-readable synonym resources comparable to WordNet (Miller, 1995) or BabelNet (Navigli and Ponzetto, 2012). Existing resources suffer from two fundamental limitations: (1) they are sparse, containing insufficient coverage of domain-specific terminology (particularly in legal, medical, and technical domains); (2) they are brittle, often created through heuristic rule-based systems or simple distributional similarity thresholds without explicit disambiguation of semantic relationships. The construction of Turkish synonym resource at scale requires overcoming both antonym intrusion (false positives) and semantic drift (transitivity artifacts). This paper contributes three substantial advances addressing these challenges: Large-Scale Disambiguated Training Dataset We present rigorous methodology for generating 843,000 labeled semantic relationship pairs (Synonym, Antonym, Co-hyponym classes) through LLM-augmented synthesis paired with high-confidence dictionary extraction. We generated this dataset using Gemini 2.5-Flash (Comanici et al., 2025) for about $65 which is pretty costeffective, all things considered. It serves dual purpose: its novel research contribution in its own right, and it provides the training foundation for our discriminator model. The dataset strikes balance between synthetic LLM-generated pairs (827,000 instances) and carefully curated dictionary entries (16,000 instances). Whats especially useful here is that weve established reproducible pipeline that anyone can use to generate supervised semantic relationship data for any language as long as youve got sufficient terminological resources to work with. Specialized Multi-Class Discriminator Instead of just tweaking unsupervised similarity thresholds and hoping for the best, we train explicit three-way classifiers that can confidently tell synonyms apart from antonyms and co-hyponyms. Our best model hits 90% F1-macro with confidence scores above 0.98 on clear-cut cases, and weve validated that it shows solid cross-model consensus in practice. Think of this discriminator as strict semantic gatekeeper which it only lets through high-confidence synonym predictions (confidence 0.70) while actively filtering out antonyms and those murky low-confidence co-hyponyms. Novel Disambiguation-Aware Clustering Algorithm We propose two-stage graph clustering procedure (Expansion followed by Reduction/Pruning) that departs from standard community detection. In the Expansion stage, we apply soft clustering via overlap-based neighbor inclusion, allowing terms to participate in multiple candidate clusters to reflect polysemy. In the Reduction stage, we resolve ambiguity through topological voting, threelevel hierarchical sc"
[21.01.2026 06:39] Mistral response. {"id": "24535e9701af49bf814f5e61ed8ca5dc", "created": 1768977575, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1448, "total_tokens": 1454, "completion_tokens": 6}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[]\n```"}}]}
[21.01.2026 06:39] Response: ```python
[]
```
[21.01.2026 06:39] Deleting PDF ./assets/pdf/2601.13251.pdf.
[21.01.2026 06:39] Success.
[21.01.2026 06:39] Enriching papers with extra data.
[21.01.2026 06:39] ********************************************************************************
[21.01.2026 06:39] Abstract 0. UniX presents a unified medical foundation model that decouples visual understanding and generation tasks using distinct autoregressive and diffusion branches with cross-modal attention for enhanced performance.  					AI-generated summary 				 Despite recent progress, medical foundation models still...
[21.01.2026 06:39] ********************************************************************************
[21.01.2026 06:39] Abstract 1. Large language models face significant challenges in software issue resolution, prompting the development of autonomous coding agents through various training-free and training-based methodologies.  					AI-generated summary 				 Issue resolution, a complex Software Engineering (SWE) task integral t...
[21.01.2026 06:39] ********************************************************************************
[21.01.2026 06:39] Abstract 2. Being-H0.5 is a Vision-Language-Action model that enables robust cross-embodiment generalization through human-centric learning and a Mixture-of-Transformers architecture with specialized embodiment handling.  					AI-generated summary 				 We introduce Being-H0.5, a foundational Vision-Language-Act...
[21.01.2026 06:39] ********************************************************************************
[21.01.2026 06:39] Abstract 3. ToolPRMBench is introduced as a large-scale benchmark for evaluating process reward models in tool-using agents, featuring step-level test cases and multi-LLM verification to ensure data quality.  					AI-generated summary 				 Reward-guided search methods have demonstrated strong potential in enhan...
[21.01.2026 06:39] ********************************************************************************
[21.01.2026 06:39] Abstract 4. A benchmark called MemoryRewardBench is introduced to systematically evaluate reward models' ability to assess long-term memory management in large language models across various context lengths and memory patterns.  					AI-generated summary 				 Existing works increasingly adopt memory-centric mec...
[21.01.2026 06:39] ********************************************************************************
[21.01.2026 06:39] Abstract 5. WorldMind addresses the modal disconnect in LLMs by autonomously building a symbolic world knowledge repository that enhances physical feasibility and task optimality through experience-based learning.  					AI-generated summary 				 Current Large Language Models (LLMs) exhibit a critical modal disc...
[21.01.2026 06:39] ********************************************************************************
[21.01.2026 06:39] Abstract 6. OmniTransfer presents a unified framework for spatio-temporal video transfer that enhances appearance consistency and temporal control through multi-view information and multimodal semantic guidance.  					AI-generated summary 				 Videos convey richer information than images or text, capturing both...
[21.01.2026 06:39] ********************************************************************************
[21.01.2026 06:39] Abstract 7. Efficiency in agentic systems is examined across memory, tool learning, and planning components, analyzing trade-offs between effectiveness and computational costs through various optimization strategies and benchmarks.  					AI-generated summary 				 Recent years have witnessed increasing interest ...
[21.01.2026 06:39] ********************************************************************************
[21.01.2026 06:39] Abstract 8. PRiSM benchmark evaluates phonetic perception in speech models through standardized transcription-based metrics and downstream applications across clinical, educational, and multilingual domains.  					AI-generated summary 				 Phone recognition (PR) serves as the atomic interface for language-agnos...
[21.01.2026 06:39] ********************************************************************************
[21.01.2026 06:39] Abstract 9. Lightweight probes trained on hidden states of LLMs enable efficient classification tasks without additional computational overhead, improving safety and sentiment analysis performance.  					AI-generated summary 				 Production LLM systems often rely on separate models for safety and other classifi...
[21.01.2026 06:39] ********************************************************************************
[21.01.2026 06:39] Abstract 10. A framework for generating structured counterfactual pairs using LLMs and SCMs enables improved evaluation and analysis of concept-based explanations in high-stakes domains.  					AI-generated summary 				 Concept-based explanations quantify how high-level concepts (e.g., gender or experience) influ...
[21.01.2026 06:39] ********************************************************************************
[21.01.2026 06:39] Abstract 11. A hybrid methodology combining FastText embeddings, clustering, and AI classification generates a large-scale Turkish semantic relations dataset with high accuracy validation.  					AI-generated summary 				 We present a hybrid methodology for generating large-scale semantic relationship datasets in...
[21.01.2026 06:39] ********************************************************************************
[21.01.2026 06:39] Abstract 12. A large-scale semantic clustering system addresses the limitation of neural embeddings in distinguishing synonyms from antonyms through a specialized three-way discriminator and novel clustering algorithm.  					AI-generated summary 				 Neural embeddings have a notorious blind spot: they can't reli...
[21.01.2026 06:39] Read previous papers.
[21.01.2026 06:39] Generating reviews via LLM API.
[21.01.2026 06:39] Using data from previous issue: {"categories": ["#diffusion", "#training", "#data", "#architecture", "#open_source", "#multimodal", "#healthcare"], "emoji": "ü´Ä", "ru": {"title": "–°–∏–Ω–µ—Ä–≥–∏—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ —Å–æ–∑–¥–∞–Ω–∏—è: —Ä–∞–∑–¥–µ–ª—ë–Ω–Ω—ã–µ –≤–µ—Ç–≤–∏ –¥–ª—è —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞", "desc": "UniX ‚Äî —ç—Ç–æ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∞—è –º–æ–¥–µ–ª—å-—Ñ—É–Ω–¥–∞
[21.01.2026 06:39] Using data from previous issue: {"categories": ["#rl", "#benchmark", "#dataset", "#agents", "#data", "#training", "#open_source", "#plp", "#survey"], "emoji": "ü§ñ", "ru": {"title": "–ê–≤—Ç–æ–Ω–æ–º–Ω—ã–µ –∞–≥–µ–Ω—Ç—ã –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–≥—Ä–∞–º–º–Ω—ã—Ö –æ—à–∏–±–æ–∫", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç—Å—è —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –æ–±–∑–æ—Ä –∞–≤—Ç–æ–Ω–æ–º–Ω—ã—Ö –∫–æ–¥–∏—Ä—É—é—â–∏—Ö –∞–≥–µ–Ω
[21.01.2026 06:39] Using data from previous issue: {"categories": ["#architecture", "#dataset", "#training", "#multimodal", "#robotics"], "emoji": "ü§ñ", "ru": {"title": "–ß–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–π —è–∑—ã–∫ –¥–ª—è –≤—Å–µ—Ö —Ä–æ–±–æ—Ç–æ–≤: universal VLA —Å –∫—Ä–æ—Å—Å–ø–ª–∞—Ç—Ñ–æ—Ä–º–µ–Ω–Ω—ã–º –æ–±–æ–±—â–µ–Ω–∏–µ–º", "desc": "Being-H0.5 ‚Äî —ç—Ç–æ –º–æ–¥–µ–ª—å Vision-Language-Action (VLA), —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –¥–ª—è –æ–±–æ–±—â–µ–Ω–∏—è —Ä–æ–±–æ—Ç–æ—Ç
[21.01.2026 06:39] Using data from previous issue: {"categories": ["#agents", "#benchmark", "#dataset", "#rl"], "emoji": "üîß", "ru": {"title": "–°–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ—Ü–µ–Ω–∏–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –¥–ª—è –∞–≥–µ–Ω—Ç–æ–≤ —Å –¥–æ—Å—Ç—É–ø–æ–º –∫ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ ToolPRMBench ‚Äî –º–∞—Å—à—Ç–∞–±–Ω–∞—è –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –Ω–∞ —É—Ä–æ–≤–Ω–µ —à–∞–≥–æ–≤
[21.01.2026 06:39] Using data from previous issue: {"categories": ["#long_context", "#survey", "#benchmark", "#dataset"], "emoji": "üß†", "ru": {"title": "–û—Ü–µ–Ω–∫–∞ –ø–∞–º—è—Ç–∏: –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω MemoryRewardBench ‚Äî –ø–µ—Ä–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ
[21.01.2026 06:39] Using data from previous issue: {"categories": ["#transfer_learning", "#alignment", "#hallucinations", "#reasoning"], "emoji": "üåç", "ru": {"title": "–ì—Äounded –º–∏—Ä –¥–ª—è LLM: –æ—Ç –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π –∫ —Ñ–∏–∑–∏—á–µ—Å–∫–∏ –æ—Å—É—â–µ—Å—Ç–≤–∏–º—ã–º –ø–ª–∞–Ω–∞–º", "desc": "WorldMind —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –º–æ–¥–∞–ª—å–Ω–æ–π —Ä–∞—Å—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö –ø—É—Ç—ë–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥
[21.01.2026 06:39] Using data from previous issue: {"categories": ["#video", "#transfer_learning", "#architecture", "#multimodal"], "emoji": "üé¨", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π —Ç—Ä–∞–Ω—Å—Ñ–µ—Ä –≤–∏–¥–µ–æ —á–µ—Ä–µ–∑ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ-–≤—Ä–µ–º–µ–Ω–Ω–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ", "desc": "OmniTransfer –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –µ–¥–∏–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Ç—Ä–∞–Ω—Å—Ñ–µ—Ä–∞ –≤–∏–¥–µ–æ, –∫–æ—Ç–æ—Ä—ã–π —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –ø—Ä–æ—Å
[21.01.2026 06:39] Using data from previous issue: {"categories": ["#rl", "#optimization", "#long_context", "#agents", "#benchmark"], "emoji": "‚ö°", "ru": {"title": "–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º: –±–∞–ª–∞–Ω—Å –º–µ–∂–¥—É –∫–∞—á–µ—Å—Ç–≤–æ–º –∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–π —Å—Ç–æ–∏–º–æ—Å—Ç—å—é", "desc": "–í —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∞–Ω–∞–ª–∏–∑–∏—Ä
[21.01.2026 06:39] Using data from previous issue: {"categories": ["#open_source", "#audio", "#dataset", "#low_resource", "#multilingual", "#benchmark"], "emoji": "üî§", "ru": {"title": "–ì–ª—É–±–æ–∫–∞—è –æ—Ü–µ–Ω–∫–∞ —Ñ–æ–Ω–µ—Ç–∏—á–µ—Å–∫–æ–≥–æ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è –≤ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö —Ä–µ—á–µ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –±–µ–Ω—á–º–∞—Ä–∫ PRiSM –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è —Ñ–æ–Ω–µ—Ç–∏–∫–∏ –≤ —Å–∏—Å—Ç–µ–º–∞—Ö —Ä–∞—Å–ø–æ
[21.01.2026 06:39] Using data from previous issue: {"categories": ["#training", "#inference", "#small_models"], "emoji": "üîç", "ru": {"title": "–õ—ë–≥–∫–∏–µ –∑–æ–Ω–¥—ã –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏: –ø–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π LLM –±–µ–∑ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è –ª—ë–≥–∫–∏—Ö –∑–æ–Ω–¥–æ–≤ (probes) –Ω–∞ —Å–∫—Ä—ã—Ç—ã—Ö —Å–æ—Å—Ç–æ—è–Ω–∏—è—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –≤—ã
[21.01.2026 06:39] Querying the API.
[21.01.2026 06:39] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A framework for generating structured counterfactual pairs using LLMs and SCMs enables improved evaluation and analysis of concept-based explanations in high-stakes domains.  					AI-generated summary 				 Concept-based explanations quantify how high-level concepts (e.g., gender or experience) influence model behavior, which is crucial for decision-makers in high-stakes domains. Recent work evaluates the faithfulness of such explanations by comparing them to reference causal effects estimated from counterfactuals. In practice, existing benchmarks rely on costly human-written counterfactuals that serve as an imperfect proxy. To address this, we introduce a framework for constructing datasets containing structural counterfactual pairs: LIBERTy (LLM-based Interventional Benchmark for Explainability with Reference Targets). LIBERTy is grounded in explicitly defined Structured Causal Models (SCMs) of the text generation, interventions on a concept propagate through the SCM until an LLM generates the counterfactual. We introduce three datasets (disease detection, CV screening, and workplace violence prediction) together with a new evaluation metric, order-faithfulness. Using them, we evaluate a wide range of methods across five models and identify substantial headroom for improving concept-based explanations. LIBERTy also enables systematic analysis of model sensitivity to interventions: we find that proprietary LLMs show markedly reduced sensitivity to demographic concepts, likely due to post-training mitigation. Overall, LIBERTy provides a much-needed benchmark for developing faithful explainability methods.
[21.01.2026 06:39] Response: ```json
{
  "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç LIBERTy ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ —Å –∫–æ–Ω—Ç—Ä—Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–º–∏ –ø–∞—Ä–∞–º–∏, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã—Ö –ø—Ä–∏—á–∏–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö (SCM) –∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –±–æ–ª–µ–µ –æ–±—ä–µ–∫—Ç–∏–≤–Ω–æ –æ—Ü–µ–Ω–∏–≤–∞—Ç—å –≤–µ—Ä–Ω–æ—Å—Ç—å –∫–æ–Ω—Ü–µ–ø—Ç—É–∞–ª—å–Ω—ã—Ö –æ–±—ä—è—Å–Ω–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –≤–ª–∏—è–Ω–∏–µ –≤—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã—Ö –∫–æ–Ω—Ü–µ–ø—Ü–∏–π –Ω–∞ –ø–æ–≤–µ–¥–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –≤ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω—ã—Ö –æ–±–ª–∞—Å—Ç—è—Ö. –ê–≤—Ç–æ—Ä—ã —Å–æ–∑–¥–∞–ª–∏ —Ç—Ä–∏ —ç—Ç–∞–ª–æ–Ω–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –Ω–æ–≤—É—é –º–µ—Ç—Ä–∏–∫—É order-faithfulness –¥–ª—è —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ—Ü–µ–Ω–∫–∏ –º–µ—Ç–æ–¥–æ–≤ –æ–±—ä—è—Å–Ω–∏–º–æ—Å—Ç–∏. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –≤—ã—è–≤–∏–ª–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º–µ—Ç–æ–¥–æ–≤ –∫–æ–Ω—Ü–µ–ø—Ç—É–∞–ª—å–Ω—ã—Ö –æ–±—ä—è—Å–Ω–µ–Ω–∏–π –∏ –æ–±–Ω–∞—Ä—É–∂–∏–ª–æ, —á—Ç–æ –ø—Ä–æ–ø—Ä–∏–µ—Ç–∞—Ä–Ω—ã–µ LLM –º–µ–Ω–µ–µ —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã –∫ –¥–µ–º–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–º –∫–æ–Ω—Ü–µ–ø—Ü–∏—è–º –∏–∑-–∑–∞ –ø–æ—Å—Ç–æ–±—É—á–µ–Ω–∏—è.",
  "emoji": "üîç",
  "title": "–û–±—ä–µ–∫—Ç–∏–≤–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±—ä—è—Å–Ω–∏–º–æ—Å—Ç–∏ —á–µ—Ä–µ–∑ –ø—Ä–∏—á–∏–Ω–Ω—ã–µ –∫–æ–Ω—Ç—Ä—Ñ–∞–∫—Ç—ã"
}
```
[21.01.2026 06:39] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A framework for generating structured counterfactual pairs using LLMs and SCMs enables improved evaluation and analysis of concept-based explanations in high-stakes domains.  					AI-generated summary 				 Concept-based explanations quantify how high-level concepts (e.g., gender or experience) influence model behavior, which is crucial for decision-makers in high-stakes domains. Recent work evaluates the faithfulness of such explanations by comparing them to reference causal effects estimated from counterfactuals. In practice, existing benchmarks rely on costly human-written counterfactuals that serve as an imperfect proxy. To address this, we introduce a framework for constructing datasets containing structural counterfactual pairs: LIBERTy (LLM-based Interventional Benchmark for Explainability with Reference Targets). LIBERTy is grounded in explicitly defined Structured Causal Models (SCMs) of the text generation, interventions on a concept propagate through the SCM until an LLM generates the counterfactual. We introduce three datasets (disease detection, CV screening, and workplace violence prediction) together with a new evaluation metric, order-faithfulness. Using them, we evaluate a wide range of methods across five models and identify substantial headroom for improving concept-based explanations. LIBERTy also enables systematic analysis of model sensitivity to interventions: we find that proprietary LLMs show markedly reduced sensitivity to demographic concepts, likely due to post-training mitigation. Overall, LIBERTy provides a much-needed benchmark for developing faithful explainability methods."

[21.01.2026 06:39] Response: ```python
["DATASET", "BENCHMARK", "HEALTHCARE"]
```
[21.01.2026 06:39] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A framework for generating structured counterfactual pairs using LLMs and SCMs enables improved evaluation and analysis of concept-based explanations in high-stakes domains.  					AI-generated summary 				 Concept-based explanations quantify how high-level concepts (e.g., gender or experience) influence model behavior, which is crucial for decision-makers in high-stakes domains. Recent work evaluates the faithfulness of such explanations by comparing them to reference causal effects estimated from counterfactuals. In practice, existing benchmarks rely on costly human-written counterfactuals that serve as an imperfect proxy. To address this, we introduce a framework for constructing datasets containing structural counterfactual pairs: LIBERTy (LLM-based Interventional Benchmark for Explainability with Reference Targets). LIBERTy is grounded in explicitly defined Structured Causal Models (SCMs) of the text generation, interventions on a concept propagate through the SCM until an LLM generates the counterfactual. We introduce three datasets (disease detection, CV screening, and workplace violence prediction) together with a new evaluation metric, order-faithfulness. Using them, we evaluate a wide range of methods across five models and identify substantial headroom for improving concept-based explanations. LIBERTy also enables systematic analysis of model sensitivity to interventions: we find that proprietary LLMs show markedly reduced sensitivity to demographic concepts, likely due to post-training mitigation. Overall, LIBERTy provides a much-needed benchmark for developing faithful explainability methods."

[21.01.2026 06:39] Response: ```python
['INTERPRETABILITY', 'ETHICS', 'SYNTHETIC']
```
[21.01.2026 06:39] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents LIBERTy, a framework that generates structured counterfactual pairs using Large Language Models (LLMs) and Structured Causal Models (SCMs) to enhance the evaluation of concept-based explanations in critical areas. Concept-based explanations help understand how high-level factors, like gender or experience, affect model decisions, which is vital for stakeholders in high-stakes situations. The framework addresses the limitations of existing benchmarks that depend on expensive human-generated counterfactuals by creating datasets that allow for systematic analysis of model behavior. By introducing new datasets and an evaluation metric called order-faithfulness, the study reveals significant opportunities for improving the reliability of concept-based explanations and highlights the sensitivity of models to demographic interventions.","title":"Enhancing Explainability with LIBERTy: A New Benchmark for Counterfactual Analysis"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper presents LIBERTy, a framework that generates structured counterfactual pairs using Large Language Models (LLMs) and Structured Causal Models (SCMs) to enhance the evaluation of concept-based explanations in critical areas. Concept-based explanations help understand how high-level factors, like gender or experience, affect model decisions, which is vital for stakeholders in high-stakes situations. The framework addresses the limitations of existing benchmarks that depend on expensive human-generated counterfactuals by creating datasets that allow for systematic analysis of model behavior. By introducing new datasets and an evaluation metric called order-faithfulness, the study reveals significant opportunities for improving the reliability of concept-based explanations and highlights the sensitivity of models to demographic interventions.', title='Enhancing Explainability with LIBERTy: A New Benchmark for Counterfactual Analysis'))
[21.01.2026 06:39] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂà©Áî®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂíåÁªìÊûÑÂõ†ÊûúÊ®°ÂûãÔºàSCMsÔºâÁîüÊàêÁªìÊûÑÂåñÂèç‰∫ãÂÆûÂØπÁöÑÊ°ÜÊû∂ÔºåÊó®Âú®ÊîπÂñÑÈ´òÈ£éÈô©È¢ÜÂüü‰∏≠Âü∫‰∫éÊ¶ÇÂøµÁöÑËß£ÈáäÁöÑËØÑ‰º∞ÂíåÂàÜÊûê„ÄÇÂü∫‰∫éÊ¶ÇÂøµÁöÑËß£ÈáäÈáèÂåñ‰∫ÜÈ´òÂ±ÇÊ¨°Ê¶ÇÂøµÔºàÂ¶ÇÊÄßÂà´ÊàñÁªèÈ™åÔºâÂØπÊ®°ÂûãË°å‰∏∫ÁöÑÂΩ±ÂìçÔºåËøôÂØπÂÜ≥Á≠ñËÄÖËá≥ÂÖ≥ÈáçË¶Å„ÄÇÁé∞ÊúâÁöÑËØÑ‰º∞ÊñπÊ≥ï‰æùËµñ‰∫é‰∫∫Â∑•ÁºñÂÜôÁöÑÂèç‰∫ãÂÆûÔºåÊàêÊú¨È´ò‰∏î‰∏çÂ§üÂáÜÁ°Æ„ÄÇLIBERTyÊ°ÜÊû∂ÈÄöËøáÊûÑÂª∫ÁªìÊûÑÂèç‰∫ãÂÆûÂØπÁöÑÊï∞ÊçÆÈõÜÔºåÊèê‰æõ‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑËØÑ‰º∞Âü∫ÂáÜÔºåÂ∏ÆÂä©Á≥ªÁªüÂàÜÊûêÊ®°ÂûãÂØπÂπ≤È¢ÑÁöÑÊïèÊÑüÊÄß„ÄÇ","title":"LIBERTyÔºöÊèêÂçáÈ´òÈ£éÈô©È¢ÜÂüüËß£ÈáäÊÄßÁöÑÊñπÊ≥ï"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂà©Áî®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂíåÁªìÊûÑÂõ†ÊûúÊ®°ÂûãÔºàSCMsÔºâÁîüÊàêÁªìÊûÑÂåñÂèç‰∫ãÂÆûÂØπÁöÑÊ°ÜÊû∂ÔºåÊó®Âú®ÊîπÂñÑÈ´òÈ£éÈô©È¢ÜÂüü‰∏≠Âü∫‰∫éÊ¶ÇÂøµÁöÑËß£ÈáäÁöÑËØÑ‰º∞ÂíåÂàÜÊûê„ÄÇÂü∫‰∫éÊ¶ÇÂøµÁöÑËß£ÈáäÈáèÂåñ‰∫ÜÈ´òÂ±ÇÊ¨°Ê¶ÇÂøµÔºàÂ¶ÇÊÄßÂà´ÊàñÁªèÈ™åÔºâÂØπÊ®°ÂûãË°å‰∏∫ÁöÑÂΩ±ÂìçÔºåËøôÂØπÂÜ≥Á≠ñËÄÖËá≥ÂÖ≥ÈáçË¶Å„ÄÇÁé∞ÊúâÁöÑËØÑ‰º∞ÊñπÊ≥ï‰æùËµñ‰∫é‰∫∫Â∑•ÁºñÂÜôÁöÑÂèç‰∫ãÂÆûÔºåÊàêÊú¨È´ò‰∏î‰∏çÂ§üÂáÜÁ°Æ„ÄÇLIBERTyÊ°ÜÊû∂ÈÄöËøáÊûÑÂª∫ÁªìÊûÑÂèç‰∫ãÂÆûÂØπÁöÑÊï∞ÊçÆÈõÜÔºåÊèê‰æõ‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑËØÑ‰º∞Âü∫ÂáÜÔºåÂ∏ÆÂä©Á≥ªÁªüÂàÜÊûêÊ®°ÂûãÂØπÂπ≤È¢ÑÁöÑÊïèÊÑüÊÄß„ÄÇ', title='LIBERTyÔºöÊèêÂçáÈ´òÈ£éÈô©È¢ÜÂüüËß£ÈáäÊÄßÁöÑÊñπÊ≥ï'))
[21.01.2026 06:39] Querying the API.
[21.01.2026 06:39] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A hybrid methodology combining FastText embeddings, clustering, and AI classification generates a large-scale Turkish semantic relations dataset with high accuracy validation.  					AI-generated summary 				 We present a hybrid methodology for generating large-scale semantic relationship datasets in low-resource languages, demonstrated through a comprehensive Turkish semantic relations corpus. Our approach integrates three phases: (1) FastText embeddings with Agglomerative Clustering to identify semantic clusters, (2) Gemini 2.5-Flash for automated semantic relationship classification, and (3) integration with curated dictionary sources. The resulting dataset comprises 843,000 unique Turkish semantic pairs across three relationship types (synonyms, antonyms, co-hyponyms) representing a 10x scale increase over existing resources at minimal cost ($65). We validate the dataset through two downstream tasks: an embedding model achieving 90% top-1 retrieval accuracy and a classification model attaining 90% F1-macro. Our scalable protocol addresses critical data scarcity in Turkish NLP and demonstrates applicability to other low-resource languages. We publicly release the dataset and models.
[21.01.2026 06:39] Response: ```json
{
  "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –≥–∏–±—Ä–∏–¥–Ω—ã–π –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—é –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –±–æ–ª—å—à–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –æ—Ç–Ω–æ—à–µ–Ω–∏–π –≤ —Ç—É—Ä–µ—Ü–∫–æ–º —è–∑—ã–∫–µ, –∏—Å–ø–æ–ª—å–∑—É—è embeddings FastText –∏ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é. –°–∏—Å—Ç–µ–º–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —Å–≤—è–∑–∏ (—Å–∏–Ω–æ–Ω–∏–º—ã, –∞–Ω—Ç–æ–Ω–∏–º—ã, –∫–æ-–≥–∏–ø–æ–Ω–∏–º—ã) —Å –ø–æ–º–æ—â—å—é –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤–æ–π –º–æ–¥–µ–ª–∏ Gemini 2.5-Flash. –ü–æ–ª—É—á–µ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç 843 —Ç—ã—Å—è—á–∏ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø–∞—Ä —Ç—É—Ä–µ—Ü–∫–∏—Ö —Å–ª–æ–≤ –∏ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç 90% —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –∑–∞–¥–∞—á–∞—Ö –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏. –ú–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –¥–µ—Ñ–∏—Ü–∏—Ç–∞ —è–∑—ã–∫–æ–≤—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤ –¥–ª—è –Ω–∏–∑–∫–æ—Ä–µ—Å—É—Ä—Å–Ω—ã—Ö —è–∑—ã–∫–æ–≤ –∏ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—Ä–∏–º–µ–Ω–µ–Ω–∞ –∫ –¥—Ä—É–≥–∏–º —è–∑—ã–∫–∞–º.",
  "emoji": "üáπüá∑",
  "title": "–ú–∞—Å—à—Ç–∞–±–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –æ—Ç–Ω–æ—à–µ–Ω–∏–π —Ç—É—Ä–µ—Ü–∫–æ–≥–æ —è–∑—ã–∫–∞ —á–µ—Ä–µ–∑ –≥–∏–±—Ä–∏–¥–Ω—É—é –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—é"
}
```
[21.01.2026 06:39] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A hybrid methodology combining FastText embeddings, clustering, and AI classification generates a large-scale Turkish semantic relations dataset with high accuracy validation.  					AI-generated summary 				 We present a hybrid methodology for generating large-scale semantic relationship datasets in low-resource languages, demonstrated through a comprehensive Turkish semantic relations corpus. Our approach integrates three phases: (1) FastText embeddings with Agglomerative Clustering to identify semantic clusters, (2) Gemini 2.5-Flash for automated semantic relationship classification, and (3) integration with curated dictionary sources. The resulting dataset comprises 843,000 unique Turkish semantic pairs across three relationship types (synonyms, antonyms, co-hyponyms) representing a 10x scale increase over existing resources at minimal cost ($65). We validate the dataset through two downstream tasks: an embedding model achieving 90% top-1 retrieval accuracy and a classification model attaining 90% F1-macro. Our scalable protocol addresses critical data scarcity in Turkish NLP and demonstrates applicability to other low-resource languages. We publicly release the dataset and models."

[21.01.2026 06:39] Response: ```python
["DATASET", "DATA", "MULTILINGUAL"]
```
[21.01.2026 06:39] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A hybrid methodology combining FastText embeddings, clustering, and AI classification generates a large-scale Turkish semantic relations dataset with high accuracy validation.  					AI-generated summary 				 We present a hybrid methodology for generating large-scale semantic relationship datasets in low-resource languages, demonstrated through a comprehensive Turkish semantic relations corpus. Our approach integrates three phases: (1) FastText embeddings with Agglomerative Clustering to identify semantic clusters, (2) Gemini 2.5-Flash for automated semantic relationship classification, and (3) integration with curated dictionary sources. The resulting dataset comprises 843,000 unique Turkish semantic pairs across three relationship types (synonyms, antonyms, co-hyponyms) representing a 10x scale increase over existing resources at minimal cost ($65). We validate the dataset through two downstream tasks: an embedding model achieving 90% top-1 retrieval accuracy and a classification model attaining 90% F1-macro. Our scalable protocol addresses critical data scarcity in Turkish NLP and demonstrates applicability to other low-resource languages. We publicly release the dataset and models."

[21.01.2026 06:39] Response: ```python
['LOW_RESOURCE', 'SYNTHETIC', 'OPEN_SOURCE']
```
[21.01.2026 06:39] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces a hybrid methodology that combines FastText embeddings, clustering, and AI classification to create a large-scale dataset of semantic relations in Turkish. The process involves using Agglomerative Clustering to group similar semantic concepts, followed by automated classification with Gemini 2.5-Flash. The resulting dataset includes 843,000 unique pairs of Turkish words categorized into synonyms, antonyms, and co-hyponyms, significantly expanding existing resources. Validation of the dataset shows high accuracy in both retrieval and classification tasks, addressing the challenge of data scarcity in Turkish natural language processing.","title":"Scaling Turkish NLP with Hybrid Semantic Relations Dataset"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper introduces a hybrid methodology that combines FastText embeddings, clustering, and AI classification to create a large-scale dataset of semantic relations in Turkish. The process involves using Agglomerative Clustering to group similar semantic concepts, followed by automated classification with Gemini 2.5-Flash. The resulting dataset includes 843,000 unique pairs of Turkish words categorized into synonyms, antonyms, and co-hyponyms, significantly expanding existing resources. Validation of the dataset shows high accuracy in both retrieval and classification tasks, addressing the challenge of data scarcity in Turkish natural language processing.', title='Scaling Turkish NLP with Hybrid Semantic Relations Dataset'))
[21.01.2026 06:40] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨Á†îÁ©∂ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊ∑∑ÂêàÊñπÊ≥ïÔºåÁªìÂêà‰∫ÜFastTextÂµåÂÖ•„ÄÅËÅöÁ±ªÂíå‰∫∫Â∑•Êô∫ËÉΩÂàÜÁ±ªÔºåÁîüÊàê‰∫Ü‰∏Ä‰∏™Â§ßËßÑÊ®°ÁöÑÂúüËÄ≥ÂÖ∂ËØ≠ËØ≠‰πâÂÖ≥Á≥ªÊï∞ÊçÆÈõÜ„ÄÇËØ•ÊñπÊ≥ïÂàÜ‰∏∫‰∏â‰∏™Èò∂ÊÆµÔºöÈ¶ñÂÖà‰ΩøÁî®FastTextÂµåÂÖ•ÂíåËÅöÁ±ªÁÆóÊ≥ïËØÜÂà´ËØ≠‰πâÁ∞áÔºõÂÖ∂Ê¨°Âà©Áî®Gemini 2.5-FlashËøõË°åËá™Âä®ËØ≠‰πâÂÖ≥Á≥ªÂàÜÁ±ªÔºõÊúÄÂêé‰∏éÁªèËøáÊï¥ÁêÜÁöÑËØçÂÖ∏ËµÑÊ∫êÊï¥Âêà„ÄÇÊúÄÁªàÁîüÊàêÁöÑÊï∞ÊçÆÈõÜÂåÖÂê´843,000ÂØπÁã¨ÁâπÁöÑÂúüËÄ≥ÂÖ∂ËØ≠ËØ≠‰πâÂØπÔºåÊ∂µÁõñÂêå‰πâËØç„ÄÅÂèç‰πâËØçÂíåÂêå‰πâËØçÁæ§Á≠â‰∏âÁßçÂÖ≥Á≥ªÁ±ªÂûãÔºåËßÑÊ®°ÊØîÁé∞ÊúâËµÑÊ∫êÂ¢ûÂä†‰∫Ü10ÂÄçÔºå‰∏îÊàêÊú¨‰ªÖ‰∏∫65ÁæéÂÖÉ„ÄÇÊàë‰ª¨ÈÄöËøá‰∏§‰∏™‰∏ãÊ∏∏‰ªªÂä°È™åËØÅ‰∫ÜÊï∞ÊçÆÈõÜÁöÑÂáÜÁ°ÆÊÄßÔºåÂµåÂÖ•Ê®°ÂûãËææÂà∞‰∫Ü90%ÁöÑÊ£ÄÁ¥¢ÂáÜÁ°ÆÁéáÔºåÂàÜÁ±ªÊ®°ÂûãÁöÑF1-macroÂÄº‰πüËææÂà∞‰∫Ü90%„ÄÇ","title":"ÂàõÊñ∞Ê∑∑ÂêàÊñπÊ≥ïÔºåÊèêÂçáÂúüËÄ≥ÂÖ∂ËØ≠ËØ≠‰πâÊï∞ÊçÆÈõÜÁöÑËßÑÊ®°‰∏éÂáÜÁ°ÆÊÄß"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨Á†îÁ©∂ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊ∑∑ÂêàÊñπÊ≥ïÔºåÁªìÂêà‰∫ÜFastTextÂµåÂÖ•„ÄÅËÅöÁ±ªÂíå‰∫∫Â∑•Êô∫ËÉΩÂàÜÁ±ªÔºåÁîüÊàê‰∫Ü‰∏Ä‰∏™Â§ßËßÑÊ®°ÁöÑÂúüËÄ≥ÂÖ∂ËØ≠ËØ≠‰πâÂÖ≥Á≥ªÊï∞ÊçÆÈõÜ„ÄÇËØ•ÊñπÊ≥ïÂàÜ‰∏∫‰∏â‰∏™Èò∂ÊÆµÔºöÈ¶ñÂÖà‰ΩøÁî®FastTextÂµåÂÖ•ÂíåËÅöÁ±ªÁÆóÊ≥ïËØÜÂà´ËØ≠‰πâÁ∞áÔºõÂÖ∂Ê¨°Âà©Áî®Gemini 2.5-FlashËøõË°åËá™Âä®ËØ≠‰πâÂÖ≥Á≥ªÂàÜÁ±ªÔºõÊúÄÂêé‰∏éÁªèËøáÊï¥ÁêÜÁöÑËØçÂÖ∏ËµÑÊ∫êÊï¥Âêà„ÄÇÊúÄÁªàÁîüÊàêÁöÑÊï∞ÊçÆÈõÜÂåÖÂê´843,000ÂØπÁã¨ÁâπÁöÑÂúüËÄ≥ÂÖ∂ËØ≠ËØ≠‰πâÂØπÔºåÊ∂µÁõñÂêå‰πâËØç„ÄÅÂèç‰πâËØçÂíåÂêå‰πâËØçÁæ§Á≠â‰∏âÁßçÂÖ≥Á≥ªÁ±ªÂûãÔºåËßÑÊ®°ÊØîÁé∞ÊúâËµÑÊ∫êÂ¢ûÂä†‰∫Ü10ÂÄçÔºå‰∏îÊàêÊú¨‰ªÖ‰∏∫65ÁæéÂÖÉ„ÄÇÊàë‰ª¨ÈÄöËøá‰∏§‰∏™‰∏ãÊ∏∏‰ªªÂä°È™åËØÅ‰∫ÜÊï∞ÊçÆÈõÜÁöÑÂáÜÁ°ÆÊÄßÔºåÂµåÂÖ•Ê®°ÂûãËææÂà∞‰∫Ü90%ÁöÑÊ£ÄÁ¥¢ÂáÜÁ°ÆÁéáÔºåÂàÜÁ±ªÊ®°ÂûãÁöÑF1-macroÂÄº‰πüËææÂà∞‰∫Ü90%„ÄÇ', title='ÂàõÊñ∞Ê∑∑ÂêàÊñπÊ≥ïÔºåÊèêÂçáÂúüËÄ≥ÂÖ∂ËØ≠ËØ≠‰πâÊï∞ÊçÆÈõÜÁöÑËßÑÊ®°‰∏éÂáÜÁ°ÆÊÄß'))
[21.01.2026 06:40] Querying the API.
[21.01.2026 06:40] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A large-scale semantic clustering system addresses the limitation of neural embeddings in distinguishing synonyms from antonyms through a specialized three-way discriminator and novel clustering algorithm.  					AI-generated summary 				 Neural embeddings have a notorious blind spot: they can't reliably tell synonyms apart from antonyms. Consequently, increasing similarity thresholds often fails to prevent opposites from being grouped together. We've built a large-scale semantic clustering system specifically designed to tackle this problem head on. Our pipeline chews through 15 million lexical items, evaluates a massive 520 million potential relationships, and ultimately generates 2.9 million high-precision semantic clusters. The system makes three primary contributions. First, we introduce a labeled dataset of 843,000 concept pairs spanning synonymy, antonymy, and co-hyponymy, constructed via Gemini 2.5-Flash LLM augmentation and verified using human-curated dictionary resources. Second, we propose a specialized three-way semantic relation discriminator that achieves 90% macro-F1, enabling robust disambiguation beyond raw embedding similarity. Third, we introduce a novel soft-to-hard clustering algorithm that mitigates semantic drift preventing erroneous transitive chains (e.g., hot -> spicy -> pain -> depression) while simultaneously resolving polysemy. Our approach employs a topology-aware two-stage expansion-pruning procedure with topological voting, ensuring that each term is assigned to exactly one semantically coherent cluster. The resulting resource enables high-precision semantic search and retrieval-augmented generation, particularly for morphologically rich and low-resource languages where existing synonym databases remain sparse.
[21.01.2026 06:40] Response: ```json
{
  "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –Ω–µ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Ä–∞–∑–ª–∏—á–∞—Ç—å —Å–∏–Ω–æ–Ω–∏–º—ã –∏ –∞–Ω—Ç–æ–Ω–∏–º—ã. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—É—é —Å–∏—Å—Ç–µ–º—É —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏, –æ–±—Ä–∞–±–æ—Ç–∞–≤—à—É—é 15 –º–∏–ª–ª–∏–æ–Ω–æ–≤ –ª–µ–∫—Å–∏—á–µ—Å–∫–∏—Ö –µ–¥–∏–Ω–∏—Ü –∏ 520 –º–∏–ª–ª–∏–æ–Ω–æ–≤ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–≤—è–∑–µ–π. –ö–ª—é—á–µ–≤–æ–π –≤–∫–ª–∞–¥ –≤–∫–ª—é—á–∞–µ—Ç —Ç—Ä—ë—Ö—Å—Ç–æ—Ä–æ–Ω–Ω—é—é –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∏—Ä—É—é—â—É—é –º–æ–¥–µ–ª—å (90% macro-F1) –¥–ª—è —Ä–∞–∑–ª–∏—á–µ–Ω–∏—è —Å–∏–Ω–æ–Ω–∏–º–∏–∏, –∞–Ω—Ç–æ–Ω–∏–º–∏–∏ –∏ –∫–æ-–≥–∏–ø–æ–Ω–∏–º–∏–∏, –∞ —Ç–∞–∫–∂–µ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –º—è–≥–∫–æ-–∂—ë—Å—Ç–∫–æ–π –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ —Å —Ç–æ–ø–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–º –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ–º. –°–∏—Å—Ç–µ–º–∞ —Å–æ–∑–¥–∞–ª–∞ 2,9 –º–∏–ª–ª–∏–æ–Ω–∞ –≤—ã—Å–æ–∫–æ—Ç–æ—á–Ω—ã—Ö —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –∏ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –Ω–∞–¥—ë–∂–Ω—ã–π —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫, –æ—Å–æ–±–µ–Ω–Ω–æ –¥–ª—è –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏ –±–æ–≥–∞—Ç—ã—Ö –∏ –Ω–∏–∑–∫–æ—Ä–µ—Å—É—Ä—Å–Ω—ã—Ö —è–∑—ã–∫–æ–≤.",
  "emoji": "üéØ",
  "title": "–†–∞–∑–ª–∏—á–∏–µ —Å–∏–Ω–æ–Ω–∏–º–æ–≤ –∏ –∞–Ω—Ç–æ–Ω–∏–º–æ–≤: —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –±–µ–∑ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –¥—Ä–µ–π—Ñ–∞"
}
```
[21.01.2026 06:40] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A large-scale semantic clustering system addresses the limitation of neural embeddings in distinguishing synonyms from antonyms through a specialized three-way discriminator and novel clustering algorithm.  					AI-generated summary 				 Neural embeddings have a notorious blind spot: they can't reliably tell synonyms apart from antonyms. Consequently, increasing similarity thresholds often fails to prevent opposites from being grouped together. We've built a large-scale semantic clustering system specifically designed to tackle this problem head on. Our pipeline chews through 15 million lexical items, evaluates a massive 520 million potential relationships, and ultimately generates 2.9 million high-precision semantic clusters. The system makes three primary contributions. First, we introduce a labeled dataset of 843,000 concept pairs spanning synonymy, antonymy, and co-hyponymy, constructed via Gemini 2.5-Flash LLM augmentation and verified using human-curated dictionary resources. Second, we propose a specialized three-way semantic relation discriminator that achieves 90% macro-F1, enabling robust disambiguation beyond raw embedding similarity. Third, we introduce a novel soft-to-hard clustering algorithm that mitigates semantic drift preventing erroneous transitive chains (e.g., hot -> spicy -> pain -> depression) while simultaneously resolving polysemy. Our approach employs a topology-aware two-stage expansion-pruning procedure with topological voting, ensuring that each term is assigned to exactly one semantically coherent cluster. The resulting resource enables high-precision semantic search and retrieval-augmented generation, particularly for morphologically rich and low-resource languages where existing synonym databases remain sparse."

[21.01.2026 06:40] Response: ```python
["DATASET", "DATA", "MULTIMODAL", "RAG"]
```

**Justification:**

- **DATASET**: The paper introduces a labeled dataset of 843,000 concept pairs for semantic relations (synonymy, antonymy, co-hyponymy).

- **DATA**: The paper focuses on data processing and curation methodologies, including dataset construction via LLM augmentation and human verification, and processing of 15 million lexical items and 520 million relationships.

- **MULTIMODAL**: The paper mentions use of Gemini 2.5-Flash LLM for data augmentation, indicating combination of language model capabilities with structured data processing.

- **RAG**: The paper explicitly states that "The resulting resource enables high-precision semantic search and retrieval-augmented generation," directly mentioning RAG as an application.
[21.01.2026 06:40] Error. Failed to parse JSON from LLM. ["DATASET", "DATA", "MULTIMODAL", "RAG"]


**Justification:**

- **DATASET**: The paper introduces a labeled dataset of 843,000 concept pairs for semantic relations (synonymy, antonymy, co-hyponymy).

- **DATA**: The paper focuses on data processing and curation methodologies, including dataset construction via LLM augmentation and human verification, and processing of 15 million lexical items and 520 million relationships.

- **MULTIMODAL**: The paper mentions use of Gemini 2.5-Flash LLM for data augmentation, indicating combination of language model capabilities with structured data processing.

- **RAG**: The paper explicitly states that "The resulting resource enables high-precision semantic search and retrieval-augmented generation," directly mentioning RAG as an application.
[21.01.2026 06:40] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A large-scale semantic clustering system addresses the limitation of neural embeddings in distinguishing synonyms from antonyms through a specialized three-way discriminator and novel clustering algorithm.  					AI-generated summary 				 Neural embeddings have a notorious blind spot: they can't reliably tell synonyms apart from antonyms. Consequently, increasing similarity thresholds often fails to prevent opposites from being grouped together. We've built a large-scale semantic clustering system specifically designed to tackle this problem head on. Our pipeline chews through 15 million lexical items, evaluates a massive 520 million potential relationships, and ultimately generates 2.9 million high-precision semantic clusters. The system makes three primary contributions. First, we introduce a labeled dataset of 843,000 concept pairs spanning synonymy, antonymy, and co-hyponymy, constructed via Gemini 2.5-Flash LLM augmentation and verified using human-curated dictionary resources. Second, we propose a specialized three-way semantic relation discriminator that achieves 90% macro-F1, enabling robust disambiguation beyond raw embedding similarity. Third, we introduce a novel soft-to-hard clustering algorithm that mitigates semantic drift preventing erroneous transitive chains (e.g., hot -> spicy -> pain -> depression) while simultaneously resolving polysemy. Our approach employs a topology-aware two-stage expansion-pruning procedure with topological voting, ensuring that each term is assigned to exactly one semantically coherent cluster. The resulting resource enables high-precision semantic search and retrieval-augmented generation, particularly for morphologically rich and low-resource languages where existing synonym databases remain sparse."

[21.01.2026 06:40] Response: ```python
["OPTIMIZATION", "LOW_RESOURCE", "SYNTHETIC"]
```

**Justification:**

1. **OPTIMIZATION**: The paper presents novel optimization methods including a "specialized three-way semantic relation discriminator" and a "novel soft-to-hard clustering algorithm" designed to improve the efficiency and accuracy of semantic clustering at large scale.

2. **LOW_RESOURCE**: The paper explicitly mentions that "the resulting resource enables high-precision semantic search and retrieval-augmented generation, particularly for morphologically rich and low-resource languages where existing synonym databases remain sparse."

3. **SYNTHETIC**: The paper describes creating a labeled dataset of 843,000 concept pairs "constructed via Gemini 2.5-Flash LLM augmentation," which represents synthetic data generation for training purposes.
[21.01.2026 06:40] Error. Failed to parse JSON from LLM. ["OPTIMIZATION", "LOW_RESOURCE", "SYNTHETIC"]


**Justification:**

1. **OPTIMIZATION**: The paper presents novel optimization methods including a "specialized three-way semantic relation discriminator" and a "novel soft-to-hard clustering algorithm" designed to improve the efficiency and accuracy of semantic clustering at large scale.

2. **LOW_RESOURCE**: The paper explicitly mentions that "the resulting resource enables high-precision semantic search and retrieval-augmented generation, particularly for morphologically rich and low-resource languages where existing synonym databases remain sparse."

3. **SYNTHETIC**: The paper describes creating a labeled dataset of 843,000 concept pairs "constructed via Gemini 2.5-Flash LLM augmentation," which represents synthetic data generation for training purposes.
[21.01.2026 06:40] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents a large-scale semantic clustering system that improves the differentiation between synonyms and antonyms, which is a common limitation of neural embeddings. The system utilizes a three-way discriminator and a novel clustering algorithm to accurately group lexical items. It processes a vast dataset of 15 million items and identifies 2.9 million high-precision semantic clusters. Key innovations include a labeled dataset for semantic relationships, a robust discriminator achieving 90% macro-F1, and a clustering method that prevents semantic drift and resolves polysemy.","title":"Revolutionizing Semantic Clustering: Distinguishing Synonyms from Antonyms"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper presents a large-scale semantic clustering system that improves the differentiation between synonyms and antonyms, which is a common limitation of neural embeddings. The system utilizes a three-way discriminator and a novel clustering algorithm to accurately group lexical items. It processes a vast dataset of 15 million items and identifies 2.9 million high-precision semantic clusters. Key innovations include a labeled dataset for semantic relationships, a robust discriminator achieving 90% macro-F1, and a clustering method that prevents semantic drift and resolves polysemy.', title='Revolutionizing Semantic Clustering: Distinguishing Synonyms from Antonyms'))
[21.01.2026 06:40] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂ§ßËßÑÊ®°ËØ≠‰πâËÅöÁ±ªÁ≥ªÁªüÔºåÊó®Âú®Ëß£ÂÜ≥Á•ûÁªèÂµåÂÖ•Âú®Âå∫ÂàÜÂêå‰πâËØçÂíåÂèç‰πâËØçÊñπÈù¢ÁöÑÂ±ÄÈôêÊÄß„ÄÇËØ•Á≥ªÁªü‰ΩøÁî®‰∫Ü‰∏Ä‰∏™‰∏ìÈó®ÁöÑ‰∏âÂÖÉÂà§Âà´Âô®ÂíåÊñ∞È¢ñÁöÑËÅöÁ±ªÁÆóÊ≥ïÔºåËÉΩÂ§üÂ§ÑÁêÜ1500‰∏á‰∏™ËØçÊ±áÈ°πÔºåÂπ∂ËØÑ‰º∞5.2‰∫ø‰∏™ÊΩúÂú®ÂÖ≥Á≥ªÔºåÊúÄÁªàÁîüÊàê290‰∏á‰∏™È´òÁ≤æÂ∫¶ÁöÑËØ≠‰πâËÅöÁ±ª„ÄÇÊàë‰ª¨ÊûÑÂª∫‰∫Ü‰∏Ä‰∏™ÂåÖÂê´843,000ÂØπÊ¶ÇÂøµÁöÑÊ†áÊ≥®Êï∞ÊçÆÈõÜÔºåÊ∂µÁõñÂêå‰πâËØç„ÄÅÂèç‰πâËØçÂíåÂÖ±Âêå‰∏ã‰ΩçËØçÔºåÂπ∂ÈÄöËøá‰∫∫Á±ªÊ†°ÂØπÁöÑÂ≠óÂÖ∏ËµÑÊ∫êËøõË°åÈ™åËØÅ„ÄÇÊ≠§Â§ñÔºåÁ≥ªÁªüÈááÁî®‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑËΩØÂà∞Á°¨ËÅöÁ±ªÁÆóÊ≥ïÔºåÊúâÊïàÈò≤Ê≠¢ËØ≠‰πâÊºÇÁßªÂíåÂ§ö‰πâËØçÈóÆÈ¢òÔºåÁ°Æ‰øùÊØè‰∏™ÊúØËØ≠Ë¢´ÂàÜÈÖçÂà∞‰∏Ä‰∏™ËØ≠‰πâ‰∏ÄËá¥ÁöÑËÅöÁ±ª‰∏≠„ÄÇ","title":"Á™ÅÁ†¥Á•ûÁªèÂµåÂÖ•ÁöÑËØ≠‰πâËÅöÁ±ªÊñ∞ÊñπÊ≥ï"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂ§ßËßÑÊ®°ËØ≠‰πâËÅöÁ±ªÁ≥ªÁªüÔºåÊó®Âú®Ëß£ÂÜ≥Á•ûÁªèÂµåÂÖ•Âú®Âå∫ÂàÜÂêå‰πâËØçÂíåÂèç‰πâËØçÊñπÈù¢ÁöÑÂ±ÄÈôêÊÄß„ÄÇËØ•Á≥ªÁªü‰ΩøÁî®‰∫Ü‰∏Ä‰∏™‰∏ìÈó®ÁöÑ‰∏âÂÖÉÂà§Âà´Âô®ÂíåÊñ∞È¢ñÁöÑËÅöÁ±ªÁÆóÊ≥ïÔºåËÉΩÂ§üÂ§ÑÁêÜ1500‰∏á‰∏™ËØçÊ±áÈ°πÔºåÂπ∂ËØÑ‰º∞5.2‰∫ø‰∏™ÊΩúÂú®ÂÖ≥Á≥ªÔºåÊúÄÁªàÁîüÊàê290‰∏á‰∏™È´òÁ≤æÂ∫¶ÁöÑËØ≠‰πâËÅöÁ±ª„ÄÇÊàë‰ª¨ÊûÑÂª∫‰∫Ü‰∏Ä‰∏™ÂåÖÂê´843,000ÂØπÊ¶ÇÂøµÁöÑÊ†áÊ≥®Êï∞ÊçÆÈõÜÔºåÊ∂µÁõñÂêå‰πâËØç„ÄÅÂèç‰πâËØçÂíåÂÖ±Âêå‰∏ã‰ΩçËØçÔºåÂπ∂ÈÄöËøá‰∫∫Á±ªÊ†°ÂØπÁöÑÂ≠óÂÖ∏ËµÑÊ∫êËøõË°åÈ™åËØÅ„ÄÇÊ≠§Â§ñÔºåÁ≥ªÁªüÈááÁî®‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑËΩØÂà∞Á°¨ËÅöÁ±ªÁÆóÊ≥ïÔºåÊúâÊïàÈò≤Ê≠¢ËØ≠‰πâÊºÇÁßªÂíåÂ§ö‰πâËØçÈóÆÈ¢òÔºåÁ°Æ‰øùÊØè‰∏™ÊúØËØ≠Ë¢´ÂàÜÈÖçÂà∞‰∏Ä‰∏™ËØ≠‰πâ‰∏ÄËá¥ÁöÑËÅöÁ±ª‰∏≠„ÄÇ', title='Á™ÅÁ†¥Á•ûÁªèÂµåÂÖ•ÁöÑËØ≠‰πâËÅöÁ±ªÊñ∞ÊñπÊ≥ï'))
[21.01.2026 06:40] Renaming data file.
[21.01.2026 06:40] Renaming previous data. hf_papers.json to ./d/2026-01-21.json
[21.01.2026 06:40] Saving new data file.
[21.01.2026 06:40] Generating page.
[21.01.2026 06:40] Renaming previous page.
[21.01.2026 06:40] Renaming previous data. index.html to ./d/2026-01-21.html
[21.01.2026 06:40] Writing result.
[21.01.2026 06:40] Renaming log file.
[21.01.2026 06:40] Renaming previous data. log.txt to ./logs/2026-01-21_last_log.txt
