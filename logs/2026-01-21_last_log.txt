[21.01.2026 10:31] Read previous papers.
[21.01.2026 10:31] Generating top page (month).
[21.01.2026 10:31] Writing top page (month).
[21.01.2026 11:26] Read previous papers.
[21.01.2026 11:26] Get feed.
[21.01.2026 11:26] Get page data from previous paper. URL: https://huggingface.co/papers/2601.11655
[21.01.2026 11:26] Get page data from previous paper. URL: https://huggingface.co/papers/2601.13836
[21.01.2026 11:26] Get page data from previous paper. URL: https://huggingface.co/papers/2601.14192
[21.01.2026 11:26] Get page data from previous paper. URL: https://huggingface.co/papers/2601.12993
[21.01.2026 11:26] Get page data from previous paper. URL: https://huggingface.co/papers/2601.14250
[21.01.2026 11:26] Get page data from previous paper. URL: https://huggingface.co/papers/2601.11969
[21.01.2026 11:26] Get page data from previous paper. URL: https://huggingface.co/papers/2601.13029
[21.01.2026 11:26] Get page data from previous paper. URL: https://huggingface.co/papers/2601.11522
[21.01.2026 11:26] Get page data from previous paper. URL: https://huggingface.co/papers/2601.12294
[21.01.2026 11:26] Get page data from previous paper. URL: https://huggingface.co/papers/2601.13247
[21.01.2026 11:26] Get page data from previous paper. URL: https://huggingface.co/papers/2601.11888
[21.01.2026 11:26] Extract page data from URL. URL: https://huggingface.co/papers/2601.14251
[21.01.2026 11:26] Get page data from previous paper. URL: https://huggingface.co/papers/2601.14046
[21.01.2026 11:26] Get page data from previous paper. URL: https://huggingface.co/papers/2601.12937
[21.01.2026 11:26] Get page data from previous paper. URL: https://huggingface.co/papers/2601.10237
[21.01.2026 11:26] Get page data from previous paper. URL: https://huggingface.co/papers/2601.13288
[21.01.2026 11:26] Get page data from previous paper. URL: https://huggingface.co/papers/2601.12910
[21.01.2026 11:26] Get page data from previous paper. URL: https://huggingface.co/papers/2601.10700
[21.01.2026 11:26] Extract page data from URL. URL: https://huggingface.co/papers/2601.14232
[21.01.2026 11:26] Extract page data from URL. URL: https://huggingface.co/papers/2601.13976
[21.01.2026 11:26] Get page data from previous paper. URL: https://huggingface.co/papers/2601.13253
[21.01.2026 11:26] Get page data from previous paper. URL: https://huggingface.co/papers/2601.13251
[21.01.2026 11:26] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[21.01.2026 11:26] No deleted papers detected.
[21.01.2026 11:26] Downloading and parsing papers (pdf, html). Total: 22.
[21.01.2026 11:26] Downloading and parsing paper https://huggingface.co/papers/2601.11655.
[21.01.2026 11:26] Extra JSON file exists (./assets/json/2601.11655.json), skip PDF parsing.
[21.01.2026 11:26] Paper image links file exists (./assets/img_data/2601.11655.json), skip HTML parsing.
[21.01.2026 11:26] Success.
[21.01.2026 11:26] Downloading and parsing paper https://huggingface.co/papers/2601.13836.
[21.01.2026 11:26] Extra JSON file exists (./assets/json/2601.13836.json), skip PDF parsing.
[21.01.2026 11:26] Paper image links file exists (./assets/img_data/2601.13836.json), skip HTML parsing.
[21.01.2026 11:26] Success.
[21.01.2026 11:26] Downloading and parsing paper https://huggingface.co/papers/2601.14192.
[21.01.2026 11:26] Extra JSON file exists (./assets/json/2601.14192.json), skip PDF parsing.
[21.01.2026 11:26] Paper image links file exists (./assets/img_data/2601.14192.json), skip HTML parsing.
[21.01.2026 11:26] Success.
[21.01.2026 11:26] Downloading and parsing paper https://huggingface.co/papers/2601.12993.
[21.01.2026 11:26] Extra JSON file exists (./assets/json/2601.12993.json), skip PDF parsing.
[21.01.2026 11:26] Paper image links file exists (./assets/img_data/2601.12993.json), skip HTML parsing.
[21.01.2026 11:26] Success.
[21.01.2026 11:26] Downloading and parsing paper https://huggingface.co/papers/2601.14250.
[21.01.2026 11:26] Extra JSON file exists (./assets/json/2601.14250.json), skip PDF parsing.
[21.01.2026 11:26] Paper image links file exists (./assets/img_data/2601.14250.json), skip HTML parsing.
[21.01.2026 11:26] Success.
[21.01.2026 11:26] Downloading and parsing paper https://huggingface.co/papers/2601.11969.
[21.01.2026 11:26] Extra JSON file exists (./assets/json/2601.11969.json), skip PDF parsing.
[21.01.2026 11:26] Paper image links file exists (./assets/img_data/2601.11969.json), skip HTML parsing.
[21.01.2026 11:26] Success.
[21.01.2026 11:26] Downloading and parsing paper https://huggingface.co/papers/2601.13029.
[21.01.2026 11:26] Extra JSON file exists (./assets/json/2601.13029.json), skip PDF parsing.
[21.01.2026 11:26] Paper image links file exists (./assets/img_data/2601.13029.json), skip HTML parsing.
[21.01.2026 11:26] Success.
[21.01.2026 11:26] Downloading and parsing paper https://huggingface.co/papers/2601.11522.
[21.01.2026 11:26] Extra JSON file exists (./assets/json/2601.11522.json), skip PDF parsing.
[21.01.2026 11:26] Paper image links file exists (./assets/img_data/2601.11522.json), skip HTML parsing.
[21.01.2026 11:26] Success.
[21.01.2026 11:26] Downloading and parsing paper https://huggingface.co/papers/2601.12294.
[21.01.2026 11:26] Extra JSON file exists (./assets/json/2601.12294.json), skip PDF parsing.
[21.01.2026 11:26] Paper image links file exists (./assets/img_data/2601.12294.json), skip HTML parsing.
[21.01.2026 11:26] Success.
[21.01.2026 11:26] Downloading and parsing paper https://huggingface.co/papers/2601.13247.
[21.01.2026 11:26] Extra JSON file exists (./assets/json/2601.13247.json), skip PDF parsing.
[21.01.2026 11:26] Paper image links file exists (./assets/img_data/2601.13247.json), skip HTML parsing.
[21.01.2026 11:26] Success.
[21.01.2026 11:26] Downloading and parsing paper https://huggingface.co/papers/2601.11888.
[21.01.2026 11:26] Extra JSON file exists (./assets/json/2601.11888.json), skip PDF parsing.
[21.01.2026 11:26] Paper image links file exists (./assets/img_data/2601.11888.json), skip HTML parsing.
[21.01.2026 11:26] Success.
[21.01.2026 11:26] Downloading and parsing paper https://huggingface.co/papers/2601.14251.
[21.01.2026 11:26] Downloading paper 2601.14251 from https://arxiv.org/pdf/2601.14251v1...
[21.01.2026 11:26] Extracting affiliations from text.
[21.01.2026 11:26] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"LIGHTONOCR: 1B END-TO-END MULTILINGUAL VISION-LANGUAGE MODEL FOR STATE-OF-THE-ART OCR 6 2 0 2 0 2 ] . [ 1 1 5 2 4 1 . 1 0 6 2 : r Said Taghadouini Adrien Cavaill√®s Baptiste Aubertin LightOn "
[21.01.2026 11:26] Response: ```python
["LightOn"]
```
[21.01.2026 11:26] Deleting PDF ./assets/pdf/2601.14251.pdf.
[21.01.2026 11:26] Success.
[21.01.2026 11:26] Downloading and parsing paper https://huggingface.co/papers/2601.14046.
[21.01.2026 11:26] Extra JSON file exists (./assets/json/2601.14046.json), skip PDF parsing.
[21.01.2026 11:26] Paper image links file exists (./assets/img_data/2601.14046.json), skip HTML parsing.
[21.01.2026 11:26] Success.
[21.01.2026 11:26] Downloading and parsing paper https://huggingface.co/papers/2601.12937.
[21.01.2026 11:26] Extra JSON file exists (./assets/json/2601.12937.json), skip PDF parsing.
[21.01.2026 11:26] Paper image links file exists (./assets/img_data/2601.12937.json), skip HTML parsing.
[21.01.2026 11:26] Success.
[21.01.2026 11:26] Downloading and parsing paper https://huggingface.co/papers/2601.10237.
[21.01.2026 11:26] Extra JSON file exists (./assets/json/2601.10237.json), skip PDF parsing.
[21.01.2026 11:26] Paper image links file exists (./assets/img_data/2601.10237.json), skip HTML parsing.
[21.01.2026 11:26] Success.
[21.01.2026 11:26] Downloading and parsing paper https://huggingface.co/papers/2601.13288.
[21.01.2026 11:26] Extra JSON file exists (./assets/json/2601.13288.json), skip PDF parsing.
[21.01.2026 11:26] Paper image links file exists (./assets/img_data/2601.13288.json), skip HTML parsing.
[21.01.2026 11:26] Success.
[21.01.2026 11:26] Downloading and parsing paper https://huggingface.co/papers/2601.12910.
[21.01.2026 11:26] Extra JSON file exists (./assets/json/2601.12910.json), skip PDF parsing.
[21.01.2026 11:26] Paper image links file exists (./assets/img_data/2601.12910.json), skip HTML parsing.
[21.01.2026 11:26] Success.
[21.01.2026 11:26] Downloading and parsing paper https://huggingface.co/papers/2601.10700.
[21.01.2026 11:26] Extra JSON file exists (./assets/json/2601.10700.json), skip PDF parsing.
[21.01.2026 11:26] Paper image links file exists (./assets/img_data/2601.10700.json), skip HTML parsing.
[21.01.2026 11:26] Success.
[21.01.2026 11:26] Downloading and parsing paper https://huggingface.co/papers/2601.14232.
[21.01.2026 11:26] Downloading paper 2601.14232 from https://arxiv.org/pdf/2601.14232v1...
[21.01.2026 11:26] Extracting affiliations from text.
[21.01.2026 11:26] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"KAGE-Bench: Fast Known-Axis Visual Generalization Evaluation for Reinforcement Learning Egor Cherepannov 1 2 Daniil Zelezetsky 1 Alexey K. Kovalev 1 2 Aleksandr I. Panov 1 2 Project Page: avanturist322.github.io/KAGEBench 6 2 0 2 0 2 ] . [ 1 2 3 2 4 1 . 1 0 6 2 : r a "
[21.01.2026 11:26] Response: ```python
[]
```
[21.01.2026 11:26] Extracting affiliations from text.
[21.01.2026 11:26] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"KAGE-Bench: Fast Known-Axis Visual Generalization Evaluation for Reinforcement Learning Egor Cherepannov 1 2 Daniil Zelezetsky 1 Alexey K. Kovalev 1 2 Aleksandr I. Panov 1 2 Project Page: avanturist322.github.io/KAGEBench 6 2 0 2 0 2 ] . [ 1 2 3 2 4 1 . 1 0 6 2 : r aPixel-based reinforcement learning agents often fail under purely visual distribution shift even when latent dynamics and rewards are unchanged, but existing benchmarks entangle multiple sources of shift and hinder systematic analysis. We introduce KAGE-Env, JAX-native 2D platformer that factorizes the observation process into independently controllable visual axes while keeping the underlying control problem fixed. By construction, varying visual axis affects performance only through the induced state-conditional action distribution of pixel policy, providing clean abstraction for visual generalization. Building on this environment, we define KAGE-Bench, benchmark of six known-axis suites comprising 34 trainevaluation configuration pairs that isolate individual visual shifts. Using standard PPOCNN baseline, we observe strong axis-dependent failures, with background and photometric shifts often collapsing success, while agent-appearance shifts are comparatively benign. Several shifts preserve forward motion while breaking task completion, showing that return alone can obscure generalization failures. Finally, the fully vectorized JAX implementation enables up to 33M environment steps per second on single GPU, enabling fast and reproducible sweeps over visual factors. Code: https://avanturist322. github.io/KAGEBench/. 1. Introduction Reinforcement learning (RL) agents trained from highdimensional pixel observations are brittle to changes in appearance, lighting, and other visual nuisance factors (Cetin et al., 2022; Yuan et al., 2023; Klepach et al., 2025). Policies that perform well in-distribution can degrade sharply under 1MIRIAI, Moscow, Russia 2Cognitive AI Systems Lab, Correspondence to: Egor Cherepanov Moscow, Russia. <cherepanov.e@miriai.org>. Preprint. January 21, 2026. 1 Figure 1. Representative observations from KAGE-Env illustrating controlled, known-axis visual variation. Each panel differs along one or more explicitly configurable axes, including background imagery and color, agent appearance and animation, moving distractors, photometric filters, and dynamic lighting effects, while task semantics and underlying dynamics are held fixed. purely visual distribution shifts, even when task semantics, transition dynamics, and rewards are unchanged (Staroverov et al., 2023; Kachaev et al., 2025; Mirjalili et al., 2025). This brittleness poses fundamental obstacle to real-world deployment, where observations inevitably vary due to viewpoint changes, illumination, surface appearance, and sensor noise while the control-relevant latent state remains fixed (Raileanu et al., 2020; Kostrikov et al., 2020; Kirilenko et al., 2023; Korchemnyi et al., 2024; Yang et al., 2024; Ugadiarov et al., 2026). As result, pixel-based RL policies that rely on incidental visual correlations can fail abruptly despite convergence, undermining reliability in robotics, autonomous navigation, and interactive environments (Stone et al., 2021; Yuan et al., 2023). More broadly, visual generalization is needed wherever models must robustly extract information from visual structure, even in scientific texts and figures (Sherki et al., 2025). Despite substantial progress in representation learning (Mazoure et al., 2021; Rahman & Xue, 2022; Ortiz et al., 2024) and data augmentation (Laskin et al., 2020; Raileanu et al., 2020; Hansen & Wang, 2021), understanding visual generalization failures remains challenging. central obstacle lies in evaluation benchmarks, which often entangle multiple KAGE-Bench: Known-Axis Visual Generalization Figure 2. KAGE-Bench: Motivation. Existing generalization benchmarks entangle multiple sources of visual shift between training and evaluation, making failures difficult to attribute. KAGEBench factorizes observations into independently controllable axes and constructs trainevaluation splits that vary one (or selected set) of axes at time, enabling precise diagnosis of which visual factors drive generalization gaps. The observation vector notation œà is used for intuition only. visual and structural changes such as background appearance, geometry, dynamics, and distractors (Cobbe et al., 2020; Stone et al., 2021; Yuan et al., 2023). In these settings, trainevaluation performance gaps cannot be cleanly attributed to specific sources of shift, and failures may reflect visual sensitivity, altered task structure, or interactions between confounded factors. Compounding this issue, many pixel-based RL environments are computationally expensive to simulate, limiting large-scale ablations and slowing hypothesis testing. We address these limitations with KAGE-Bench (KnownAxis Generalization Evaluation Benchmark), visual generalization benchmark in which sources of distribution shift are isolated by construction. KAGE-Bench is built on KAGE-Env (Figure 1), JAX-native (Bradbury et al., 2018) 2D platformer whose observation process is factorized into independently controllable visual axes while latent dynamics and rewards are held fixed (see Figure 2). Under this known-axis design, each axis corresponds to well-defined component of the observation kernel, and any trainevaluation performance difference arises solely from how fixed observation-based policy responds to different renderings of the same latent states, enabling unambiguous attribution of visual generalization failures. Systematic analysis of visual generalization requires evaluating many controlled shifts at scale. KAGE-Env is implemented entirely in JAX with end-to-end jit compilation and vectorized execution via vmap and lax.scan, enabling efficient large-batch simulation on single accelIn practice, this design scales up to 216 parallel erator. environments on one GPU and achieves up to 33M environment steps per second (see Figure 3), making exhaustive sweeps over visual parameters and fine-grained diagnosis of generalization behavior feasible. 1https://colab.research.google.com/ (a) Easy configuration. (b) Hard configuration. Figure 3. Environment stepping throughput vs. parallelism. Environment stepping throughput (steps per second, higher is better) as function of the number of parallel environments nenvs for KAGE-Env across heterogeneous hardware backends. GPU r"
[21.01.2026 11:26] Mistral response. {"id": "0454f8d122bb479e8f0fd1e683347341", "created": 1768994799, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1485, "total_tokens": 1506, "completion_tokens": 21}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\"MIRIAI, Moscow, Russia\", \"Cognitive AI Systems Lab\"]\n```"}}]}
[21.01.2026 11:26] Response: ```python
["MIRIAI, Moscow, Russia", "Cognitive AI Systems Lab"]
```
[21.01.2026 11:26] Deleting PDF ./assets/pdf/2601.14232.pdf.
[21.01.2026 11:26] Success.
[21.01.2026 11:26] Downloading and parsing paper https://huggingface.co/papers/2601.13976.
[21.01.2026 11:26] Downloading paper 2601.13976 from https://arxiv.org/pdf/2601.13976v1...
[21.01.2026 11:26] Extracting affiliations from text.
[21.01.2026 11:26] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 0 2 ] . [ 1 6 7 9 3 1 . 1 0 6 2 : r a FantasyVLN: Unified Multimodal Chain-ofThought Reasoning for Vision-Language Navigation Jing Zuo1,2*, Lingzhou Mu1,3*, Fan Jiang1*, Chengcheng Ma1, Mu Xu1, Yonggang Qi2 1Fantasy AIGC Team, 2Beijing University of Posts and Telecommunications, 3Tsinghua University jiangfan0576@gmail.com qiyg@bupt.edu.cn "
[21.01.2026 11:26] Response: ```python
[
    "Fantasy AIGC Team",
    "Beijing University of Posts and Telecommunications",
    "Tsinghua University"
]
```
[21.01.2026 11:26] Deleting PDF ./assets/pdf/2601.13976.pdf.
[21.01.2026 11:26] Success.
[21.01.2026 11:26] Downloading and parsing paper https://huggingface.co/papers/2601.13253.
[21.01.2026 11:26] Extra JSON file exists (./assets/json/2601.13253.json), skip PDF parsing.
[21.01.2026 11:26] Paper image links file exists (./assets/img_data/2601.13253.json), skip HTML parsing.
[21.01.2026 11:26] Success.
[21.01.2026 11:26] Downloading and parsing paper https://huggingface.co/papers/2601.13251.
[21.01.2026 11:26] Extra JSON file exists (./assets/json/2601.13251.json), skip PDF parsing.
[21.01.2026 11:26] Paper image links file exists (./assets/img_data/2601.13251.json), skip HTML parsing.
[21.01.2026 11:26] Success.
[21.01.2026 11:26] Enriching papers with extra data.
[21.01.2026 11:26] ********************************************************************************
[21.01.2026 11:26] Abstract 0. Large language models face significant challenges in software issue resolution, prompting the development of autonomous coding agents through various training-free and training-based methodologies.  					AI-generated summary 				 Issue resolution, a complex Software Engineering (SWE) task integral t...
[21.01.2026 11:26] ********************************************************************************
[21.01.2026 11:26] Abstract 1. FutureOmni presents the first benchmark for evaluating multimodal models' ability to forecast future events from audio-visual data, revealing current limitations and proposing an improved training strategy for better performance.  					AI-generated summary 				 Although Multimodal Large Language Mod...
[21.01.2026 11:26] ********************************************************************************
[21.01.2026 11:26] Abstract 2. Efficiency in agentic systems is examined across memory, tool learning, and planning components, analyzing trade-offs between effectiveness and computational costs through various optimization strategies and benchmarks.  					AI-generated summary 				 Recent years have witnessed increasing interest ...
[21.01.2026 11:26] ********************************************************************************
[21.01.2026 11:26] Abstract 3. Being-H0.5 is a Vision-Language-Action model that enables robust cross-embodiment generalization through human-centric learning and a Mixture-of-Transformers architecture with specialized embodiment handling.  					AI-generated summary 				 We introduce Being-H0.5, a foundational Vision-Language-Act...
[21.01.2026 11:26] ********************************************************************************
[21.01.2026 11:26] Abstract 4. OmniTransfer presents a unified framework for spatio-temporal video transfer that enhances appearance consistency and temporal control through multi-view information and multimodal semantic guidance.  					AI-generated summary 				 Videos convey richer information than images or text, capturing both...
[21.01.2026 11:26] ********************************************************************************
[21.01.2026 11:26] Abstract 5. A benchmark called MemoryRewardBench is introduced to systematically evaluate reward models' ability to assess long-term memory management in large language models across various context lengths and memory patterns.  					AI-generated summary 				 Existing works increasingly adopt memory-centric mec...
[21.01.2026 11:26] ********************************************************************************
[21.01.2026 11:26] Abstract 6. Think3D enhances vision-language models' 3D reasoning capabilities by enabling interactive spatial exploration through 3D reconstruction and camera-based operations, improving performance without additional training.  					AI-generated summary 				 Understanding and reasoning about the physical worl...
[21.01.2026 11:26] ********************************************************************************
[21.01.2026 11:26] Abstract 7. UniX presents a unified medical foundation model that decouples visual understanding and generation tasks using distinct autoregressive and diffusion branches with cross-modal attention for enhanced performance.  					AI-generated summary 				 Despite recent progress, medical foundation models still...
[21.01.2026 11:26] ********************************************************************************
[21.01.2026 11:26] Abstract 8. ToolPRMBench is introduced as a large-scale benchmark for evaluating process reward models in tool-using agents, featuring step-level test cases and multi-LLM verification to ensure data quality.  					AI-generated summary 				 Reward-guided search methods have demonstrated strong potential in enhan...
[21.01.2026 11:26] ********************************************************************************
[21.01.2026 11:26] Abstract 9. WorldMind addresses the modal disconnect in LLMs by autonomously building a symbolic world knowledge repository that enhances physical feasibility and task optimality through experience-based learning.  					AI-generated summary 				 Current Large Language Models (LLMs) exhibit a critical modal disc...
[21.01.2026 11:26] ********************************************************************************
[21.01.2026 11:26] Abstract 10. A novel retriever training framework for agentic search that uses both local relevance and global answer correctness metrics with iterative optimization between the search agent and retriever.  					AI-generated summary 				 Agentic search has recently emerged as a powerful paradigm, where an agent ...
[21.01.2026 11:26] ********************************************************************************
[21.01.2026 11:26] Abstract 11. LightOnOCR-2-1B is a compact 1B-parameter vision-language model that performs end-to-end document image-to-text conversion with improved localization and robustness through specialized training techniques.  					AI-generated summary 				 We present LightOnOCR-2-1B, a 1B-parameter end-to-end multilin...
[21.01.2026 11:26] ********************************************************************************
[21.01.2026 11:26] Abstract 12. PRiSM benchmark evaluates phonetic perception in speech models through standardized transcription-based metrics and downstream applications across clinical, educational, and multilingual domains.  					AI-generated summary 				 Phone recognition (PR) serves as the atomic interface for language-agnos...
[21.01.2026 11:26] ********************************************************************************
[21.01.2026 11:26] Abstract 13. Membership inference attacks fail to reliably detect copyrighted text usage in large language models when training data is paraphrased using structure-aware methods that preserve semantic content.  					AI-generated summary 				 As large language models (LLMs) are trained on increasingly opaque corp...
[21.01.2026 11:26] ********************************************************************************
[21.01.2026 11:26] Abstract 14. Differentially private stochastic gradient descent with shuffled sampling faces fundamental privacy-utility trade-offs that require substantial noise for meaningful privacy protection, limiting practical performance.  					AI-generated summary 				 Differentially Private Stochastic Gradient Descent ...
[21.01.2026 11:26] ********************************************************************************
[21.01.2026 11:26] Abstract 15. Lightweight probes trained on hidden states of LLMs enable efficient classification tasks without additional computational overhead, improving safety and sentiment analysis performance.  					AI-generated summary 				 Production LLM systems often rely on separate models for safety and other classifi...
[21.01.2026 11:26] ********************************************************************************
[21.01.2026 11:26] Abstract 16. SciCoQA is a dataset for identifying mismatches between scientific publications and code implementations, containing 611 discrepancies across multiple disciplines and demonstrating the challenge of detecting such issues even for advanced language models.  					AI-generated summary 				 We present Sc...
[21.01.2026 11:26] ********************************************************************************
[21.01.2026 11:26] Abstract 17. A framework for generating structured counterfactual pairs using LLMs and SCMs enables improved evaluation and analysis of concept-based explanations in high-stakes domains.  					AI-generated summary 				 Concept-based explanations quantify how high-level concepts (e.g., gender or experience) influ...
[21.01.2026 11:26] ********************************************************************************
[21.01.2026 11:26] Abstract 18. KAGE-Env is a JAX-native 2D platformer environment that isolates visual shifts from underlying control problems, enabling systematic analysis of visual generalization in reinforcement learning.  					AI-generated summary 				 Pixel-based reinforcement learning agents often fail under purely visual d...
[21.01.2026 11:26] ********************************************************************************
[21.01.2026 11:26] Abstract 19. FantasyVLN presents a unified implicit reasoning framework for vision-and-language navigation that enhances reasoning capabilities without explicit token overhead, achieving real-time performance with improved accuracy.  					AI-generated summary 				 Achieving human-level performance in Vision-and-...
[21.01.2026 11:26] ********************************************************************************
[21.01.2026 11:26] Abstract 20. A hybrid methodology combining FastText embeddings, clustering, and AI classification generates a large-scale Turkish semantic relations dataset with high accuracy validation.  					AI-generated summary 				 We present a hybrid methodology for generating large-scale semantic relationship datasets in...
[21.01.2026 11:26] ********************************************************************************
[21.01.2026 11:26] Abstract 21. A large-scale semantic clustering system addresses the limitation of neural embeddings in distinguishing synonyms from antonyms through a specialized three-way discriminator and novel clustering algorithm.  					AI-generated summary 				 Neural embeddings have a notorious blind spot: they can't reli...
[21.01.2026 11:26] Read previous papers.
[21.01.2026 11:26] Generating reviews via LLM API.
[21.01.2026 11:26] Using data from previous issue: {"categories": ["#rl", "#benchmark", "#dataset", "#agents", "#data", "#training", "#open_source", "#plp", "#survey"], "emoji": "ü§ñ", "ru": {"title": "–ê–≤—Ç–æ–Ω–æ–º–Ω—ã–µ –∞–≥–µ–Ω—Ç—ã –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–≥—Ä–∞–º–º–Ω—ã—Ö –æ—à–∏–±–æ–∫", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç—Å—è —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –æ–±–∑–æ—Ä –∞–≤—Ç–æ–Ω–æ–º–Ω—ã—Ö –∫–æ–¥–∏—Ä—É—é—â–∏—Ö –∞–≥–µ–Ω
[21.01.2026 11:26] Using data from previous issue: {"categories": ["#audio", "#video", "#benchmark", "#dataset", "#training", "#multimodal"], "emoji": "üîÆ", "ru": {"title": "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –±—É–¥—É—â–µ–≥–æ: —É—á–∏–º –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≤–∏–¥–µ—Ç—å –∑–∞–≤—Ç—Ä–∞", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ FutureOmni ‚Äî –ø–µ—Ä–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –ø—Ä–µ–¥—Å–∫–∞
[21.01.2026 11:26] Using data from previous issue: {"categories": ["#rl", "#optimization", "#long_context", "#agents", "#benchmark"], "emoji": "‚ö°", "ru": {"title": "–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º: –±–∞–ª–∞–Ω—Å –º–µ–∂–¥—É –∫–∞—á–µ—Å—Ç–≤–æ–º –∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–π —Å—Ç–æ–∏–º–æ—Å—Ç—å—é", "desc": "–í —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∞–Ω–∞–ª–∏–∑–∏—Ä
[21.01.2026 11:26] Using data from previous issue: {"categories": ["#architecture", "#dataset", "#training", "#multimodal", "#robotics"], "emoji": "ü§ñ", "ru": {"title": "–ß–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–π —è–∑—ã–∫ –¥–ª—è –≤—Å–µ—Ö —Ä–æ–±–æ—Ç–æ–≤: universal VLA —Å –∫—Ä–æ—Å—Å–ø–ª–∞—Ç—Ñ–æ—Ä–º–µ–Ω–Ω—ã–º –æ–±–æ–±—â–µ–Ω–∏–µ–º", "desc": "Being-H0.5 ‚Äî —ç—Ç–æ –º–æ–¥–µ–ª—å Vision-Language-Action (VLA), —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –¥–ª—è –æ–±–æ–±—â–µ–Ω–∏—è —Ä–æ–±–æ—Ç–æ—Ç
[21.01.2026 11:26] Using data from previous issue: {"categories": ["#video", "#transfer_learning", "#architecture", "#multimodal"], "emoji": "üé¨", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π —Ç—Ä–∞–Ω—Å—Ñ–µ—Ä –≤–∏–¥–µ–æ —á–µ—Ä–µ–∑ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ-–≤—Ä–µ–º–µ–Ω–Ω–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ", "desc": "OmniTransfer –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –µ–¥–∏–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Ç—Ä–∞–Ω—Å—Ñ–µ—Ä–∞ –≤–∏–¥–µ–æ, –∫–æ—Ç–æ—Ä—ã–π —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –ø—Ä–æ—Å
[21.01.2026 11:26] Using data from previous issue: {"categories": ["#long_context", "#survey", "#benchmark", "#dataset"], "emoji": "üß†", "ru": {"title": "–û—Ü–µ–Ω–∫–∞ –ø–∞–º—è—Ç–∏: –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω MemoryRewardBench ‚Äî –ø–µ—Ä–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ
[21.01.2026 11:26] Using data from previous issue: {"categories": ["#open_source", "#reasoning"], "emoji": "üßä", "ru": {"title": "–¢—Ä—ë—Ö–º–µ—Ä–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ –¥–ª—è –≤–∏–¥–µ–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "Think3D ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –∫–æ—Ç–æ—Ä—ã–π —Ä–∞—Å—à–∏—Ä—è–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –≤–∏–¥–µ–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ç—Ä—ë—Ö–º–µ—Ä–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, –ø–æ–∑–≤–æ–ª—è—è –∏–º –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç—å –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ —á–µ—Ä–µ–∑ 3
[21.01.2026 11:26] Using data from previous issue: {"categories": ["#diffusion", "#training", "#data", "#architecture", "#open_source", "#multimodal", "#healthcare"], "emoji": "ü´Ä", "ru": {"title": "–°–∏–Ω–µ—Ä–≥–∏—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ —Å–æ–∑–¥–∞–Ω–∏—è: —Ä–∞–∑–¥–µ–ª—ë–Ω–Ω—ã–µ –≤–µ—Ç–≤–∏ –¥–ª—è —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞", "desc": "UniX ‚Äî —ç—Ç–æ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∞—è –º–æ–¥–µ–ª—å-—Ñ—É–Ω–¥–∞
[21.01.2026 11:26] Using data from previous issue: {"categories": ["#agents", "#benchmark", "#dataset", "#rl"], "emoji": "üîß", "ru": {"title": "–°–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ—Ü–µ–Ω–∏–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –¥–ª—è –∞–≥–µ–Ω—Ç–æ–≤ —Å –¥–æ—Å—Ç—É–ø–æ–º –∫ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ ToolPRMBench ‚Äî –º–∞—Å—à—Ç–∞–±–Ω–∞—è –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –Ω–∞ —É—Ä–æ–≤–Ω–µ —à–∞–≥–æ–≤
[21.01.2026 11:26] Using data from previous issue: {"categories": ["#transfer_learning", "#alignment", "#hallucinations", "#reasoning"], "emoji": "üåç", "ru": {"title": "–ì—Äounded –º–∏—Ä –¥–ª—è LLM: –æ—Ç –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π –∫ —Ñ–∏–∑–∏—á–µ—Å–∫–∏ –æ—Å—É—â–µ—Å—Ç–≤–∏–º—ã–º –ø–ª–∞–Ω–∞–º", "desc": "WorldMind —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –º–æ–¥–∞–ª—å–Ω–æ–π —Ä–∞—Å—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö –ø—É—Ç—ë–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥
[21.01.2026 11:26] Using data from previous issue: {"categories": ["#agents", "#benchmark", "#training", "#rag"], "emoji": "üîÑ", "ru": {"title": "–î–≤—É—Å—Ç–æ—Ä–æ–Ω–Ω—è—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∞–≥–µ–Ω—Ç–∞ –ø–æ–∏—Å–∫–∞ –∏ –ø–æ–∏—Å–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è —Ç–æ—á–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –Ω–æ–≤–∞—èÊ°ÜÊû∂ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è retriever'–∞, –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–Ω–∞—è —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∞–≥–µ–Ω—Ç–æ–≤. 
[21.01.2026 11:26] Querying the API.
[21.01.2026 11:26] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

LightOnOCR-2-1B is a compact 1B-parameter vision-language model that performs end-to-end document image-to-text conversion with improved localization and robustness through specialized training techniques.  					AI-generated summary 				 We present LightOnOCR-2-1B, a 1B-parameter end-to-end multilingual vision--language model that converts document images (e.g., PDFs) into clean, naturally ordered text without brittle OCR pipelines. Trained on a large-scale, high-quality distillation mix with strong coverage of scans, French documents, and scientific PDFs, LightOnOCR-2 achieves state-of-the-art results on OlmOCR-Bench while being 9times smaller and substantially faster than prior best-performing models. We further extend the output format to predict normalized bounding boxes for embedded images, introducing localization during pretraining via a resume strategy and refining it with RLVR using IoU-based rewards. Finally, we improve robustness with checkpoint averaging and task-arithmetic merging. We release model checkpoints under Apache 2.0, and publicly release the dataset and LightOnOCR-bbox-bench evaluation under their respective licenses.
[21.01.2026 11:26] Response: ```json
{
  "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω LightOnOCR-2-1B ‚Äî –∫–æ–º–ø–∞–∫—Ç–Ω–∞—è vision-language –º–æ–¥–µ–ª—å —Å 1 –º–∏–ª–ª–∏–∞—Ä–¥–æ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è end-to-end —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö. –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –Ω–∞ –±–æ–ª—å—à–æ–º –Ω–∞–±–æ—Ä–µ –≤—ã—Å–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –∑–Ω–∞–Ω–∏–π, —á—Ç–æ –ø–æ–∑–≤–æ–ª–∏–ª–æ –µ–π –¥–æ—Å—Ç–∏—á—å –ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–µ OlmOCR-Bench –ø—Ä–∏ —Ä–∞–∑–º–µ—Ä–µ –≤ 9 —Ä–∞–∑ –º–µ–Ω—å—à–µ —á–µ–º –∫–æ–Ω–∫—É—Ä–∏—Ä—É—é—â–∏–µ —Ä–µ—à–µ–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã –¥–æ–±–∞–≤–∏–ª–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö bounding box –¥–ª—è –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º RLVR –∏ IoU-based –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–π –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏. –ù–∞–¥–µ–∂–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –ø–æ–≤—ã—à–µ–Ω–∞ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ–º —É—Å—Ä–µ–¥–Ω–µ–Ω–∏—è –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã—Ö —Ç–æ—á–µ–∫ –∏ —Ç–µ—Ö–Ω–∏–∫–∏ task-arithmetic merging.",
  "emoji": "üìÑ",
  "title": "–õ—ë–≥–∫–∞—è –∏ –±—ã—Å—Ç—Ä–∞—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö —Å –µ–¥–∏–Ω–æ–π –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç—å—é"
}
```
[21.01.2026 11:26] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"LightOnOCR-2-1B is a compact 1B-parameter vision-language model that performs end-to-end document image-to-text conversion with improved localization and robustness through specialized training techniques.  					AI-generated summary 				 We present LightOnOCR-2-1B, a 1B-parameter end-to-end multilingual vision--language model that converts document images (e.g., PDFs) into clean, naturally ordered text without brittle OCR pipelines. Trained on a large-scale, high-quality distillation mix with strong coverage of scans, French documents, and scientific PDFs, LightOnOCR-2 achieves state-of-the-art results on OlmOCR-Bench while being 9times smaller and substantially faster than prior best-performing models. We further extend the output format to predict normalized bounding boxes for embedded images, introducing localization during pretraining via a resume strategy and refining it with RLVR using IoU-based rewards. Finally, we improve robustness with checkpoint averaging and task-arithmetic merging. We release model checkpoints under Apache 2.0, and publicly release the dataset and LightOnOCR-bbox-bench evaluation under their respective licenses."

[21.01.2026 11:26] Response: ```python
["CV", "MULTIMODAL", "SMALL_MODELS", "DATASET", "BENCHMARK", "TRAINING", "MULTILINGUAL"]
```
[21.01.2026 11:26] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"LightOnOCR-2-1B is a compact 1B-parameter vision-language model that performs end-to-end document image-to-text conversion with improved localization and robustness through specialized training techniques.  					AI-generated summary 				 We present LightOnOCR-2-1B, a 1B-parameter end-to-end multilingual vision--language model that converts document images (e.g., PDFs) into clean, naturally ordered text without brittle OCR pipelines. Trained on a large-scale, high-quality distillation mix with strong coverage of scans, French documents, and scientific PDFs, LightOnOCR-2 achieves state-of-the-art results on OlmOCR-Bench while being 9times smaller and substantially faster than prior best-performing models. We further extend the output format to predict normalized bounding boxes for embedded images, introducing localization during pretraining via a resume strategy and refining it with RLVR using IoU-based rewards. Finally, we improve robustness with checkpoint averaging and task-arithmetic merging. We release model checkpoints under Apache 2.0, and publicly release the dataset and LightOnOCR-bbox-bench evaluation under their respective licenses."

[21.01.2026 11:26] Response: ```python
["OPEN_SOURCE", "OPTIMIZATION"]
```

**Justification:**

- **OPEN_SOURCE**: The paper explicitly states "We release model checkpoints under Apache 2.0, and publicly release the dataset and LightOnOCR-bbox-bench evaluation under their respective licenses." This indicates the authors are contributing open-source models, datasets, and evaluation benchmarks to the public.

- **OPTIMIZATION**: The paper discusses several optimization techniques including "checkpoint averaging and task-arithmetic merging" to improve model performance and robustness, as well as training optimization through "RLVR using IoU-based rewards.
[21.01.2026 11:26] Error. Failed to parse JSON from LLM. ["OPEN_SOURCE", "OPTIMIZATION"]


**Justification:**

- **OPEN_SOURCE**: The paper explicitly states "We release model checkpoints under Apache 2.0, and publicly release the dataset and LightOnOCR-bbox-bench evaluation under their respective licenses." This indicates the authors are contributing open-source models, datasets, and evaluation benchmarks to the public.

- **OPTIMIZATION**: The paper discusses several optimization techniques including "checkpoint averaging and task-arithmetic merging" to improve model performance and robustness, as well as training optimization through "RLVR using IoU-based rewards.
[21.01.2026 11:26] Response: ParsedChatCompletionMessage[Article](content='{"desc":"LightOnOCR-2-1B is a vision-language model designed for converting document images into text efficiently. It utilizes 1 billion parameters and employs advanced training techniques to enhance localization and robustness, making it more reliable than traditional OCR methods. The model is trained on a diverse dataset, achieving state-of-the-art performance while being significantly smaller and faster than previous models. Additionally, it introduces features like predicting bounding boxes for images and employs reinforcement learning for improved accuracy.","title":"Efficient Document Image-to-Text Conversion with LightOnOCR-2-1B"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='LightOnOCR-2-1B is a vision-language model designed for converting document images into text efficiently. It utilizes 1 billion parameters and employs advanced training techniques to enhance localization and robustness, making it more reliable than traditional OCR methods. The model is trained on a diverse dataset, achieving state-of-the-art performance while being significantly smaller and faster than previous models. Additionally, it introduces features like predicting bounding boxes for images and employs reinforcement learning for improved accuracy.', title='Efficient Document Image-to-Text Conversion with LightOnOCR-2-1B'))
[21.01.2026 11:27] Response: ParsedChatCompletionMessage[Article](content='{"desc":"LightOnOCR-2-1B ÊòØ‰∏Ä‰∏™Á¥ßÂáëÁöÑ1BÂèÇÊï∞ËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÔºåËÉΩÂ§üÂÆûÁé∞ÊñáÊ°£ÂõæÂÉèÂà∞ÊñáÊú¨ÁöÑÁ´ØÂà∞Á´ØËΩ¨Êç¢„ÄÇÈÄöËøá‰∏ìÈó®ÁöÑËÆ≠ÁªÉÊäÄÊúØÔºåÂÆÉÂú®ÂÆö‰ΩçÂíåÈ≤ÅÊ£íÊÄßÊñπÈù¢ÂæóÂà∞‰∫ÜÊîπÂñÑ„ÄÇËØ•Ê®°ÂûãÂú®Â§ßËßÑÊ®°È´òË¥®ÈáèÁöÑÊï∞ÊçÆÈõÜ‰∏äËøõË°åËÆ≠ÁªÉÔºåËÉΩÂ§üÂ§ÑÁêÜÂ§öÁßçËØ≠Ë®ÄÁöÑÊñáÊ°£ÔºåÂπ∂Âú®OlmOCR-Bench‰∏äÂèñÂæó‰∫ÜÊúÄÂÖàËøõÁöÑÁªìÊûú„ÄÇÊàë‰ª¨ËøòÂºïÂÖ•‰∫ÜÂΩí‰∏ÄÂåñËæπÁïåÊ°ÜÁöÑÈ¢ÑÊµãÔºåÂ¢ûÂº∫‰∫ÜÊ®°ÂûãÁöÑËæìÂá∫Ê†ºÂºèÂíåÈ≤ÅÊ£íÊÄß„ÄÇ","title":"ËΩªÈáèÁ∫ßÊñáÊ°£ÂõæÂÉèËΩ¨ÊñáÊú¨ÁöÑÈù©ÂëΩÊÄßÊ®°Âûã"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='LightOnOCR-2-1B ÊòØ‰∏Ä‰∏™Á¥ßÂáëÁöÑ1BÂèÇÊï∞ËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÔºåËÉΩÂ§üÂÆûÁé∞ÊñáÊ°£ÂõæÂÉèÂà∞ÊñáÊú¨ÁöÑÁ´ØÂà∞Á´ØËΩ¨Êç¢„ÄÇÈÄöËøá‰∏ìÈó®ÁöÑËÆ≠ÁªÉÊäÄÊúØÔºåÂÆÉÂú®ÂÆö‰ΩçÂíåÈ≤ÅÊ£íÊÄßÊñπÈù¢ÂæóÂà∞‰∫ÜÊîπÂñÑ„ÄÇËØ•Ê®°ÂûãÂú®Â§ßËßÑÊ®°È´òË¥®ÈáèÁöÑÊï∞ÊçÆÈõÜ‰∏äËøõË°åËÆ≠ÁªÉÔºåËÉΩÂ§üÂ§ÑÁêÜÂ§öÁßçËØ≠Ë®ÄÁöÑÊñáÊ°£ÔºåÂπ∂Âú®OlmOCR-Bench‰∏äÂèñÂæó‰∫ÜÊúÄÂÖàËøõÁöÑÁªìÊûú„ÄÇÊàë‰ª¨ËøòÂºïÂÖ•‰∫ÜÂΩí‰∏ÄÂåñËæπÁïåÊ°ÜÁöÑÈ¢ÑÊµãÔºåÂ¢ûÂº∫‰∫ÜÊ®°ÂûãÁöÑËæìÂá∫Ê†ºÂºèÂíåÈ≤ÅÊ£íÊÄß„ÄÇ', title='ËΩªÈáèÁ∫ßÊñáÊ°£ÂõæÂÉèËΩ¨ÊñáÊú¨ÁöÑÈù©ÂëΩÊÄßÊ®°Âûã'))
[21.01.2026 11:27] Using data from previous issue: {"categories": ["#open_source", "#audio", "#dataset", "#low_resource", "#multilingual", "#benchmark"], "emoji": "üî§", "ru": {"title": "–ì–ª—É–±–æ–∫–∞—è –æ—Ü–µ–Ω–∫–∞ —Ñ–æ–Ω–µ—Ç–∏—á–µ—Å–∫–æ–≥–æ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è –≤ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö —Ä–µ—á–µ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –±–µ–Ω—á–º–∞—Ä–∫ PRiSM –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è —Ñ–æ–Ω–µ—Ç–∏–∫–∏ –≤ —Å–∏—Å—Ç–µ–º–∞—Ö —Ä–∞—Å–ø–æ
[21.01.2026 11:27] Using data from previous issue: {"categories": ["#training", "#security", "#leakage"], "emoji": "üîì", "ru": {"title": "–•—Ä—É–ø–∫–æ—Å—Ç—å –∞—Ç–∞–∫ –≤—ã–≤–æ–¥–∞ —á–ª–µ–Ω—Å—Ç–≤–∞ –ø–µ—Ä–µ–¥ —Å–µ–º–∞–Ω—Ç–∏–∫–æ-—Å–æ—Ö—Ä–∞–Ω—è—é—â–∏–º –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∞–Ω–∏–µ–º", "desc": "–í —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –Ω–∞–¥—ë–∂–Ω–æ—Å—Ç—å –∞—Ç–∞–∫ –≤—ã–≤–æ–¥–∞ —á–ª–µ–Ω—Å—Ç–≤–∞ (MIA) –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∑–∞—â–∏—â—ë–Ω–Ω–æ–≥–æ –∞–≤—Ç–æ—Ä—Å–∫–∏–º –ø—Ä–∞–≤–æ–º —Ç–µ–∫—Å—Ç–∞ –ø
[21.01.2026 11:27] Using data from previous issue: {"categories": ["#security", "#optimization", "#training", "#math"], "emoji": "üîê", "ru": {"title": "–§—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π –∫–æ–º–ø—Ä–æ–º–∏—Å—Å –º–µ–∂–¥—É –ø—Ä–∏–≤–∞—Ç–Ω–æ—Å—Ç—å—é –∏ —Ç–æ—á–Ω–æ—Å—Ç—å—é –≤ DP-SGD", "desc": "–í —Å—Ç–∞—Ç—å–µ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç—Å—è –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞–ª—å–Ω–æ –ø—Ä–∏–≤–∞—Ç–Ω—ã–π —Å—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–∏–π –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫ (DP-SGD) –≤ —Ä–∞–º–∫–∞—Ö f-–¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–π –ø—Ä–∏
[21.01.2026 11:27] Using data from previous issue: {"categories": ["#training", "#inference", "#small_models"], "emoji": "üîç", "ru": {"title": "–õ—ë–≥–∫–∏–µ –∑–æ–Ω–¥—ã –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏: –ø–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π LLM –±–µ–∑ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è –ª—ë–≥–∫–∏—Ö –∑–æ–Ω–¥–æ–≤ (probes) –Ω–∞ —Å–∫—Ä—ã—Ç—ã—Ö —Å–æ—Å—Ç–æ—è–Ω–∏—è—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –≤—ã
[21.01.2026 11:27] Using data from previous issue: {"categories": ["#long_context", "#hallucinations", "#synthetic", "#science"], "emoji": "üîç", "ru": {"title": "–í—ã—è–≤–ª–µ–Ω–∏–µ —Å–∫—Ä—ã—Ç—ã—Ö –æ—à–∏–±–æ–∫: –∫–æ–≥–¥–∞ –∫–æ–¥ –Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –Ω–∞—É—á–Ω–æ–π —Å—Ç–∞—Ç—å–µ", "desc": "SciCoQA ‚Äî —ç—Ç–æ –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–π –º–µ–∂–¥—É –æ–ø–∏—Å–∞–Ω–∏–µ–º –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –≤ –Ω–∞—É—á–Ω—ã—Ö –ø—É–±–ª–∏–∫–∞—Ü–∏—è—Ö –∏ –∏—Ö —Ä–µ–∞–ª–∏–∑–∞—Ü–∏
[21.01.2026 11:27] Using data from previous issue: {"categories": ["#healthcare", "#interpretability", "#ethics", "#benchmark", "#dataset", "#synthetic"], "emoji": "üîç", "ru": {"title": "–û–±—ä–µ–∫—Ç–∏–≤–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±—ä—è—Å–Ω–∏–º–æ—Å—Ç–∏ —á–µ—Ä–µ–∑ –ø—Ä–∏—á–∏–Ω–Ω—ã–µ –∫–æ–Ω—Ç—Ä—Ñ–∞–∫—Ç—ã", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç LIBERTy ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ —Å –∫–æ–Ω—Ç—Ä—Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–º–∏ –ø–∞—Ä–∞–º–∏
[21.01.2026 11:27] Querying the API.
[21.01.2026 11:27] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

KAGE-Env is a JAX-native 2D platformer environment that isolates visual shifts from underlying control problems, enabling systematic analysis of visual generalization in reinforcement learning.  					AI-generated summary 				 Pixel-based reinforcement learning agents often fail under purely visual distribution shift even when latent dynamics and rewards are unchanged, but existing benchmarks entangle multiple sources of shift and hinder systematic analysis. We introduce KAGE-Env, a JAX-native 2D platformer that factorizes the observation process into independently controllable visual axes while keeping the underlying control problem fixed. By construction, varying a visual axis affects performance only through the induced state-conditional action distribution of a pixel policy, providing a clean abstraction for visual generalization. Building on this environment, we define KAGE-Bench, a benchmark of six known-axis suites comprising 34 train-evaluation configuration pairs that isolate individual visual shifts. Using a standard PPO-CNN baseline, we observe strong axis-dependent failures, with background and photometric shifts often collapsing success, while agent-appearance shifts are comparatively benign. Several shifts preserve forward motion while breaking task completion, showing that return alone can obscure generalization failures. Finally, the fully vectorized JAX implementation enables up to 33M environment steps per second on a single GPU, enabling fast and reproducible sweeps over visual factors. Code: https://avanturist322.github.io/KAGEBench/.
[21.01.2026 11:27] Response: ```json
{
  "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ KAGE-Env ‚Äî —Å—Ä–µ–¥–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –Ω–∞ JAX, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏–∑–æ–ª–∏—Ä–æ–≤–∞—Ç—å –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è –æ—Ç –±–∞–∑–æ–≤–æ–π –∑–∞–¥–∞—á–∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –±–µ–Ω—á–º–∞—Ä–∫ KAGE-Bench —Å 34 –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è–º–∏, —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤–∞—Ä—å–∏—Ä—É—é—â–∏–º–∏ —Ä–∞–∑–ª–∏—á–Ω—ã–µ –æ—Å–∏ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–π (—Ñ–æ–Ω, —Ñ–æ—Ç–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–µ —Å–¥–≤–∏–≥–∏, –≤–Ω–µ—à–Ω–∏–π –≤–∏–¥ –∞–≥–µ–Ω—Ç–∞). –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –ø–∏–∫—Å–µ–ª—å–Ω—ã–µ –ø–æ–ª–∏—Ç–∏–∫–∏ PPO-CNN —Å–∏–ª—å–Ω–æ —Å—Ç—Ä–∞–¥–∞—é—Ç –æ—Ç –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Å–¥–≤–∏–≥–æ–≤ (–æ—Å–æ–±–µ–Ω–Ω–æ —Ñ–æ–Ω–∞ –∏ —Ñ–æ—Ç–æ–º–µ—Ç—Ä–∏–∏), –ø—Ä–∏ —ç—Ç–æ–º –Ω–∞–≥—Ä–∞–¥–∞ –º–æ–∂–µ—Ç —Å–∫—Ä—ã–≤–∞—Ç—å —Ä–µ–∞–ª—å–Ω—ã–µ –æ—à–∏–±–∫–∏ –æ–±–æ–±—â–µ–Ω–∏—è. –ë–ª–∞–≥–æ–¥–∞—Ä—è –ø–æ–ª–Ω–æ–π –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ –Ω–∞ JAX —Å—Ä–µ–¥–∞ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç 33 –º–ª–Ω —à–∞–≥–æ–≤ –≤ —Å–µ–∫—É–Ω–¥—É –Ω–∞ –æ–¥–Ω–æ–º GPU, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è –±—ã—Å—Ç—Ä—ã–µ –∏ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º—ã–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã.",
  "emoji": "üéÆ",
  "title": "–ß–∏—Å—Ç—ã–π –∞–Ω–∞–ª–∏–∑ –≤–∏–∑—É–∞–ª—å–Ω–æ–π –æ–±–æ–±—â–∞–µ–º–æ—Å—Ç–∏ –≤ –æ–±—É—á–µ–Ω–∏–∏ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º —á–µ—Ä–µ–∑ —Ñ–∞–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—é –Ω–∞–±–ª—é–¥–µ–Ω–∏–π"
}
```
[21.01.2026 11:27] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"KAGE-Env is a JAX-native 2D platformer environment that isolates visual shifts from underlying control problems, enabling systematic analysis of visual generalization in reinforcement learning.  					AI-generated summary 				 Pixel-based reinforcement learning agents often fail under purely visual distribution shift even when latent dynamics and rewards are unchanged, but existing benchmarks entangle multiple sources of shift and hinder systematic analysis. We introduce KAGE-Env, a JAX-native 2D platformer that factorizes the observation process into independently controllable visual axes while keeping the underlying control problem fixed. By construction, varying a visual axis affects performance only through the induced state-conditional action distribution of a pixel policy, providing a clean abstraction for visual generalization. Building on this environment, we define KAGE-Bench, a benchmark of six known-axis suites comprising 34 train-evaluation configuration pairs that isolate individual visual shifts. Using a standard PPO-CNN baseline, we observe strong axis-dependent failures, with background and photometric shifts often collapsing success, while agent-appearance shifts are comparatively benign. Several shifts preserve forward motion while breaking task completion, showing that return alone can obscure generalization failures. Finally, the fully vectorized JAX implementation enables up to 33M environment steps per second on a single GPU, enabling fast and reproducible sweeps over visual factors. Code: https://avanturist322.github.io/KAGEBench/."

[21.01.2026 11:27] Response: ```python
["RL", "BENCHMARK", "CV"]
```
[21.01.2026 11:27] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"KAGE-Env is a JAX-native 2D platformer environment that isolates visual shifts from underlying control problems, enabling systematic analysis of visual generalization in reinforcement learning.  					AI-generated summary 				 Pixel-based reinforcement learning agents often fail under purely visual distribution shift even when latent dynamics and rewards are unchanged, but existing benchmarks entangle multiple sources of shift and hinder systematic analysis. We introduce KAGE-Env, a JAX-native 2D platformer that factorizes the observation process into independently controllable visual axes while keeping the underlying control problem fixed. By construction, varying a visual axis affects performance only through the induced state-conditional action distribution of a pixel policy, providing a clean abstraction for visual generalization. Building on this environment, we define KAGE-Bench, a benchmark of six known-axis suites comprising 34 train-evaluation configuration pairs that isolate individual visual shifts. Using a standard PPO-CNN baseline, we observe strong axis-dependent failures, with background and photometric shifts often collapsing success, while agent-appearance shifts are comparatively benign. Several shifts preserve forward motion while breaking task completion, showing that return alone can obscure generalization failures. Finally, the fully vectorized JAX implementation enables up to 33M environment steps per second on a single GPU, enabling fast and reproducible sweeps over visual factors. Code: https://avanturist322.github.io/KAGEBench/."

[21.01.2026 11:27] Response: ```python
["GAMES", "OPTIMIZATION"]
```
[21.01.2026 11:27] Response: ParsedChatCompletionMessage[Article](content='{"desc":"KAGE-Env is a new 2D platformer environment designed for reinforcement learning that separates visual changes from control challenges. This allows researchers to study how visual shifts affect the performance of AI agents without interference from other factors. The environment includes a benchmark called KAGE-Bench, which tests agents on different visual conditions while keeping the control tasks constant. Results show that certain visual changes can significantly impact agent performance, highlighting the importance of understanding visual generalization in machine learning.","title":"Isolating Visual Shifts for Better AI Learning"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='KAGE-Env is a new 2D platformer environment designed for reinforcement learning that separates visual changes from control challenges. This allows researchers to study how visual shifts affect the performance of AI agents without interference from other factors. The environment includes a benchmark called KAGE-Bench, which tests agents on different visual conditions while keeping the control tasks constant. Results show that certain visual changes can significantly impact agent performance, highlighting the importance of understanding visual generalization in machine learning.', title='Isolating Visual Shifts for Better AI Learning'))
[21.01.2026 11:27] Response: ParsedChatCompletionMessage[Article](content='{"desc":"KAGE-EnvÊòØ‰∏Ä‰∏™Âü∫‰∫éJAXÁöÑ‰∫åÁª¥Âπ≥Âè∞ÁéØÂ¢ÉÔºåÊó®Âú®Â∞ÜËßÜËßâÂèòÂåñ‰∏éÊéßÂà∂ÈóÆÈ¢òÂàÜÁ¶ªÔºå‰ªéËÄåÁ≥ªÁªüÂú∞ÂàÜÊûêÂº∫ÂåñÂ≠¶‰π†‰∏≠ÁöÑËßÜËßâÊ≥õÂåñ„ÄÇ‰º†ÁªüÁöÑÂÉèÁ¥†Á∫ßÂº∫ÂåñÂ≠¶‰π†‰ª£ÁêÜÂú®ËßÜËßâÂàÜÂ∏ÉÂèòÂåñÊó∂Â∏∏Â∏∏Â§±Ë¥•ÔºåÂç≥‰ΩøÊΩúÂú®Âä®ÊÄÅÂíåÂ•ñÂä±‰øùÊåÅ‰∏çÂèò„ÄÇKAGE-EnvÈÄöËøáÂ∞ÜËßÇÂØüËøáÁ®ãÂàÜËß£‰∏∫Áã¨Á´ãÂèØÊéßÁöÑËßÜËßâËΩ¥ÔºåÊèê‰æõ‰∫Ü‰∏Ä‰∏™Ê∏ÖÊô∞ÁöÑËßÜËßâÊ≥õÂåñÊäΩË±°„ÄÇÂü∫‰∫éÊ≠§ÁéØÂ¢ÉÔºåÊàë‰ª¨ÂÆö‰πâ‰∫ÜKAGE-BenchÔºå‰∏Ä‰∏™ÂåÖÂê´34‰∏™ËÆ≠ÁªÉ-ËØÑ‰º∞ÈÖçÁΩÆÂØπÁöÑÂü∫ÂáÜÔºå‰∏ìÊ≥®‰∫éÈöîÁ¶ªÂçï‰∏ÄÁöÑËßÜËßâÂèòÂåñ„ÄÇ","title":"KAGE-EnvÔºöËßÜËßâÊ≥õÂåñÁöÑÁ≥ªÁªüÂàÜÊûêÂπ≥Âè∞"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='KAGE-EnvÊòØ‰∏Ä‰∏™Âü∫‰∫éJAXÁöÑ‰∫åÁª¥Âπ≥Âè∞ÁéØÂ¢ÉÔºåÊó®Âú®Â∞ÜËßÜËßâÂèòÂåñ‰∏éÊéßÂà∂ÈóÆÈ¢òÂàÜÁ¶ªÔºå‰ªéËÄåÁ≥ªÁªüÂú∞ÂàÜÊûêÂº∫ÂåñÂ≠¶‰π†‰∏≠ÁöÑËßÜËßâÊ≥õÂåñ„ÄÇ‰º†ÁªüÁöÑÂÉèÁ¥†Á∫ßÂº∫ÂåñÂ≠¶‰π†‰ª£ÁêÜÂú®ËßÜËßâÂàÜÂ∏ÉÂèòÂåñÊó∂Â∏∏Â∏∏Â§±Ë¥•ÔºåÂç≥‰ΩøÊΩúÂú®Âä®ÊÄÅÂíåÂ•ñÂä±‰øùÊåÅ‰∏çÂèò„ÄÇKAGE-EnvÈÄöËøáÂ∞ÜËßÇÂØüËøáÁ®ãÂàÜËß£‰∏∫Áã¨Á´ãÂèØÊéßÁöÑËßÜËßâËΩ¥ÔºåÊèê‰æõ‰∫Ü‰∏Ä‰∏™Ê∏ÖÊô∞ÁöÑËßÜËßâÊ≥õÂåñÊäΩË±°„ÄÇÂü∫‰∫éÊ≠§ÁéØÂ¢ÉÔºåÊàë‰ª¨ÂÆö‰πâ‰∫ÜKAGE-BenchÔºå‰∏Ä‰∏™ÂåÖÂê´34‰∏™ËÆ≠ÁªÉ-ËØÑ‰º∞ÈÖçÁΩÆÂØπÁöÑÂü∫ÂáÜÔºå‰∏ìÊ≥®‰∫éÈöîÁ¶ªÂçï‰∏ÄÁöÑËßÜËßâÂèòÂåñ„ÄÇ', title='KAGE-EnvÔºöËßÜËßâÊ≥õÂåñÁöÑÁ≥ªÁªüÂàÜÊûêÂπ≥Âè∞'))
[21.01.2026 11:27] Querying the API.
[21.01.2026 11:27] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

FantasyVLN presents a unified implicit reasoning framework for vision-and-language navigation that enhances reasoning capabilities without explicit token overhead, achieving real-time performance with improved accuracy.  					AI-generated summary 				 Achieving human-level performance in Vision-and-Language Navigation (VLN) requires an embodied agent to jointly understand multimodal instructions and visual-spatial context while reasoning over long action sequences. Recent works, such as NavCoT and NavGPT-2, demonstrate the potential of Chain-of-Thought (CoT) reasoning for improving interpretability and long-horizon planning. Moreover, multimodal extensions like OctoNav-R1 and CoT-VLA further validate CoT as a promising pathway toward human-like navigation reasoning. However, existing approaches face critical drawbacks: purely textual CoTs lack spatial grounding and easily overfit to sparse annotated reasoning steps, while multimodal CoTs incur severe token inflation by generating imagined visual observations, making real-time navigation impractical. In this work, we propose FantasyVLN, a unified implicit reasoning framework that preserves the benefits of CoT reasoning without explicit token overhead. Specifically, imagined visual tokens are encoded into a compact latent space using a pretrained Visual AutoRegressor (VAR) during CoT reasoning training, and the model jointly learns from textual, visual, and multimodal CoT modes under a unified multi-CoT strategy. At inference, our model performs direct instruction-to-action mapping while still enjoying reasoning-aware representations. Extensive experiments on LH-VLN show that our approach achieves reasoning-aware yet real-time navigation, improving success rates and efficiency while reducing inference latency by an order of magnitude compared to explicit CoT methods.
[21.01.2026 11:27] Response: ```json
{
  "desc": "FantasyVLN –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–ª—è –Ω–∞–≤–∏–≥–∞—Ü–∏–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∑—Ä–µ–Ω–∏—è –∏ —è–∑—ã–∫–∞, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–∏–º–µ–Ω—è–µ—Ç –Ω–µ—è–≤–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ —Ç–∏–ø–∞ Chain-of-Thought –±–µ–∑ —è–≤–Ω–æ–≥–æ —É–≤–µ–ª–∏—á–µ–Ω–∏—è –¥–ª–∏–Ω—ã —Ç–æ–∫–µ–Ω–æ–≤. –ú–µ—Ç–æ–¥ –∫–æ–¥–∏—Ä—É–µ—Ç –≤–æ–æ–±—Ä–∞–∂–∞–µ–º—ã–µ –≤–∏–∑—É–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –≤ –∫–æ–º–ø–∞–∫—Ç–Ω–æ–µ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ —Å –ø–æ–º–æ—â—å—é –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–≥–æ Visual AutoRegressor, –ø–æ–∑–≤–æ–ª—è—è –º–æ–¥–µ–ª–∏ –æ–±—É—á–∞—Ç—å—Å—è –Ω–∞ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö, –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —Ü–µ–ø–æ—á–∫–∞—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –ù–∞ —ç—Ç–∞–ø–µ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ —Å–∏—Å—Ç–µ–º–∞ –≤—ã–ø–æ–ª–Ω—è–µ—Ç –ø—Ä—è–º–æ–µ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –≤ –¥–µ–π—Å—Ç–≤–∏—è, —Å–æ—Ö—Ä–∞–Ω—è—è –ø—Ä–∏ —ç—Ç–æ–º –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è, –æ—Å–æ–∑–Ω–∞—é—â–∏–µ –ø—Ä–æ—Ü–µ—Å—Å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. –ü–æ–¥—Ö–æ–¥ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–≥–æ —É–ª—É—á—à–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏ —Å–∫–æ—Ä–æ—Å—Ç–∏, —Å–æ–∫—Ä–∞—â–∞—è –ª–∞—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –≤ –¥–µ—Å—è—Ç—å —Ä–∞–∑ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –º–µ—Ç–æ–¥–∞–º–∏ —è–≤–Ω–æ–≥–æ CoT.",
  "emoji": "üß≠",
  "title": "–°–∫—Ä—ã—Ç–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –Ω–∞–≤–∏–≥–∞—Ü–∏–∏ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏"
}
```
[21.01.2026 11:27] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"FantasyVLN presents a unified implicit reasoning framework for vision-and-language navigation that enhances reasoning capabilities without explicit token overhead, achieving real-time performance with improved accuracy.  					AI-generated summary 				 Achieving human-level performance in Vision-and-Language Navigation (VLN) requires an embodied agent to jointly understand multimodal instructions and visual-spatial context while reasoning over long action sequences. Recent works, such as NavCoT and NavGPT-2, demonstrate the potential of Chain-of-Thought (CoT) reasoning for improving interpretability and long-horizon planning. Moreover, multimodal extensions like OctoNav-R1 and CoT-VLA further validate CoT as a promising pathway toward human-like navigation reasoning. However, existing approaches face critical drawbacks: purely textual CoTs lack spatial grounding and easily overfit to sparse annotated reasoning steps, while multimodal CoTs incur severe token inflation by generating imagined visual observations, making real-time navigation impractical. In this work, we propose FantasyVLN, a unified implicit reasoning framework that preserves the benefits of CoT reasoning without explicit token overhead. Specifically, imagined visual tokens are encoded into a compact latent space using a pretrained Visual AutoRegressor (VAR) during CoT reasoning training, and the model jointly learns from textual, visual, and multimodal CoT modes under a unified multi-CoT strategy. At inference, our model performs direct instruction-to-action mapping while still enjoying reasoning-aware representations. Extensive experiments on LH-VLN show that our approach achieves reasoning-aware yet real-time navigation, improving success rates and efficiency while reducing inference latency by an order of magnitude compared to explicit CoT methods."

[21.01.2026 11:27] Response: ```python
['AGENTS', 'MULTIMODAL', 'TRAINING', 'ARCHITECTURE']
```
[21.01.2026 11:27] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"FantasyVLN presents a unified implicit reasoning framework for vision-and-language navigation that enhances reasoning capabilities without explicit token overhead, achieving real-time performance with improved accuracy.  					AI-generated summary 				 Achieving human-level performance in Vision-and-Language Navigation (VLN) requires an embodied agent to jointly understand multimodal instructions and visual-spatial context while reasoning over long action sequences. Recent works, such as NavCoT and NavGPT-2, demonstrate the potential of Chain-of-Thought (CoT) reasoning for improving interpretability and long-horizon planning. Moreover, multimodal extensions like OctoNav-R1 and CoT-VLA further validate CoT as a promising pathway toward human-like navigation reasoning. However, existing approaches face critical drawbacks: purely textual CoTs lack spatial grounding and easily overfit to sparse annotated reasoning steps, while multimodal CoTs incur severe token inflation by generating imagined visual observations, making real-time navigation impractical. In this work, we propose FantasyVLN, a unified implicit reasoning framework that preserves the benefits of CoT reasoning without explicit token overhead. Specifically, imagined visual tokens are encoded into a compact latent space using a pretrained Visual AutoRegressor (VAR) during CoT reasoning training, and the model jointly learns from textual, visual, and multimodal CoT modes under a unified multi-CoT strategy. At inference, our model performs direct instruction-to-action mapping while still enjoying reasoning-aware representations. Extensive experiments on LH-VLN show that our approach achieves reasoning-aware yet real-time navigation, improving success rates and efficiency while reducing inference latency by an order of magnitude compared to explicit CoT methods."

[21.01.2026 11:27] Response: ```python
['REASONING', 'INTERPRETABILITY']
```
[21.01.2026 11:27] Response: ParsedChatCompletionMessage[Article](content='{"desc":"FantasyVLN introduces a new framework for vision-and-language navigation that enhances reasoning abilities without adding extra computational load. It leverages a compact latent space to encode visual information, allowing for efficient processing during reasoning tasks. By integrating multimodal Chain-of-Thought (CoT) strategies, it improves the model\'s ability to understand and act on complex instructions in real-time. The results show significant improvements in navigation success rates and reduced latency compared to traditional methods.","title":"Real-Time Navigation with Implicit Reasoning"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="FantasyVLN introduces a new framework for vision-and-language navigation that enhances reasoning abilities without adding extra computational load. It leverages a compact latent space to encode visual information, allowing for efficient processing during reasoning tasks. By integrating multimodal Chain-of-Thought (CoT) strategies, it improves the model's ability to understand and act on complex instructions in real-time. The results show significant improvements in navigation success rates and reduced latency compared to traditional methods.", title='Real-Time Navigation with Implicit Reasoning'))
[21.01.2026 11:27] Response: ParsedChatCompletionMessage[Article](content='{"desc":"FantasyVLNÊèêÂá∫‰∫Ü‰∏ÄÁßçÁªü‰∏ÄÁöÑÈöêÂºèÊé®ÁêÜÊ°ÜÊû∂ÔºåÁî®‰∫éËßÜËßâ‰∏éËØ≠Ë®ÄÂØºËà™ÔºåÂ¢ûÂº∫‰∫ÜÊé®ÁêÜËÉΩÂäõËÄåÊó†ÈúÄÊòæÂºèÁöÑÊ†áËÆ∞ÂºÄÈîÄÔºå‰ªéËÄåÂÆûÁé∞‰∫ÜÂÆûÊó∂ÊÄßËÉΩÂíåÊõ¥È´òÁöÑÂáÜÁ°ÆÊÄß„ÄÇËØ•ÊñπÊ≥ïÈÄöËøá‰ΩøÁî®È¢ÑËÆ≠ÁªÉÁöÑËßÜËßâËá™ÂõûÂΩíÊ®°ÂûãÔºàVARÔºâÂ∞ÜÊÉ≥Ë±°ÁöÑËßÜËßâÊ†áËÆ∞ÁºñÁ†ÅÂà∞Á¥ßÂáëÁöÑÊΩúÂú®Á©∫Èó¥‰∏≠ÔºåÈÅøÂÖç‰∫Ü‰º†ÁªüÊñπÊ≥ï‰∏≠Áî±‰∫éÁîüÊàêËôöÊûÑËßÜËßâËßÇÂØüËÄåÂØºËá¥ÁöÑÊ†áËÆ∞ËÜ®ËÉÄÈóÆÈ¢ò„ÄÇÊ®°ÂûãÂú®Êé®ÁêÜÊó∂ËÉΩÂ§üÁõ¥Êé•Â∞ÜÊåá‰ª§Êò†Â∞ÑÂà∞Âä®‰ΩúÔºåÂêåÊó∂‰øùÊåÅÊé®ÁêÜÊÑüÁü•ÁöÑË°®Á§∫„ÄÇÂ§ßÈáèÂÆûÈ™åË°®ÊòéÔºåFantasyVLNÂú®ÊàêÂäüÁéáÂíåÊïàÁéá‰∏äÈÉΩÊúâÊòæËëóÊèêÂçáÔºåÊé®ÁêÜÂª∂ËøüÊØîÊòæÂºèÈìæÂºèÊé®ÁêÜÊñπÊ≥ïÂáèÂ∞ë‰∫Ü‰∏Ä‰∏™Êï∞ÈáèÁ∫ß„ÄÇ","title":"Êó†Ê†áËÆ∞ÂºÄÈîÄÁöÑÂÆûÊó∂Êé®ÁêÜÂØºËà™"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='FantasyVLNÊèêÂá∫‰∫Ü‰∏ÄÁßçÁªü‰∏ÄÁöÑÈöêÂºèÊé®ÁêÜÊ°ÜÊû∂ÔºåÁî®‰∫éËßÜËßâ‰∏éËØ≠Ë®ÄÂØºËà™ÔºåÂ¢ûÂº∫‰∫ÜÊé®ÁêÜËÉΩÂäõËÄåÊó†ÈúÄÊòæÂºèÁöÑÊ†áËÆ∞ÂºÄÈîÄÔºå‰ªéËÄåÂÆûÁé∞‰∫ÜÂÆûÊó∂ÊÄßËÉΩÂíåÊõ¥È´òÁöÑÂáÜÁ°ÆÊÄß„ÄÇËØ•ÊñπÊ≥ïÈÄöËøá‰ΩøÁî®È¢ÑËÆ≠ÁªÉÁöÑËßÜËßâËá™ÂõûÂΩíÊ®°ÂûãÔºàVARÔºâÂ∞ÜÊÉ≥Ë±°ÁöÑËßÜËßâÊ†áËÆ∞ÁºñÁ†ÅÂà∞Á¥ßÂáëÁöÑÊΩúÂú®Á©∫Èó¥‰∏≠ÔºåÈÅøÂÖç‰∫Ü‰º†ÁªüÊñπÊ≥ï‰∏≠Áî±‰∫éÁîüÊàêËôöÊûÑËßÜËßâËßÇÂØüËÄåÂØºËá¥ÁöÑÊ†áËÆ∞ËÜ®ËÉÄÈóÆÈ¢ò„ÄÇÊ®°ÂûãÂú®Êé®ÁêÜÊó∂ËÉΩÂ§üÁõ¥Êé•Â∞ÜÊåá‰ª§Êò†Â∞ÑÂà∞Âä®‰ΩúÔºåÂêåÊó∂‰øùÊåÅÊé®ÁêÜÊÑüÁü•ÁöÑË°®Á§∫„ÄÇÂ§ßÈáèÂÆûÈ™åË°®ÊòéÔºåFantasyVLNÂú®ÊàêÂäüÁéáÂíåÊïàÁéá‰∏äÈÉΩÊúâÊòæËëóÊèêÂçáÔºåÊé®ÁêÜÂª∂ËøüÊØîÊòæÂºèÈìæÂºèÊé®ÁêÜÊñπÊ≥ïÂáèÂ∞ë‰∫Ü‰∏Ä‰∏™Êï∞ÈáèÁ∫ß„ÄÇ', title='Êó†Ê†áËÆ∞ÂºÄÈîÄÁöÑÂÆûÊó∂Êé®ÁêÜÂØºËà™'))
[21.01.2026 11:27] Using data from previous issue: {"categories": ["#multilingual", "#data", "#low_resource", "#dataset", "#open_source", "#synthetic"], "emoji": "üáπüá∑", "ru": {"title": "–ú–∞—Å—à—Ç–∞–±–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –æ—Ç–Ω–æ—à–µ–Ω–∏–π —Ç—É—Ä–µ—Ü–∫–æ–≥–æ —è–∑—ã–∫–∞ —á–µ—Ä–µ–∑ –≥–∏–±—Ä–∏–¥–Ω—É—é –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—é", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –≥–∏–±—Ä–∏–¥–Ω—ã–π –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—é –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –±–æ–ª
[21.01.2026 11:27] Using data from previous issue: {"categories": [], "emoji": "üéØ", "ru": {"title": "–†–∞–∑–ª–∏—á–∏–µ —Å–∏–Ω–æ–Ω–∏–º–æ–≤ –∏ –∞–Ω—Ç–æ–Ω–∏–º–æ–≤: —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –±–µ–∑ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –¥—Ä–µ–π—Ñ–∞", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –Ω–µ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Ä–∞–∑–ª–∏—á–∞—Ç—å —Å–∏–Ω–æ–Ω–∏–º—ã –∏ –∞–Ω—Ç–æ–Ω–∏–º—ã. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—É—é —Å–∏—Å—Ç–µ–º—É —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π
[21.01.2026 11:27] Renaming data file.
[21.01.2026 11:27] Renaming previous data. hf_papers.json to ./d/2026-01-21.json
[21.01.2026 11:27] Saving new data file.
[21.01.2026 11:27] Generating page.
[21.01.2026 11:27] Renaming previous page.
[21.01.2026 11:27] Renaming previous data. index.html to ./d/2026-01-21.html
[21.01.2026 11:27] Writing result.
[21.01.2026 11:27] Renaming log file.
[21.01.2026 11:27] Renaming previous data. log.txt to ./logs/2026-01-21_last_log.txt
