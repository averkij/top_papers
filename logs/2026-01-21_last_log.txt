[21.01.2026 07:31] Read previous papers.
[21.01.2026 07:31] Generating top page (month).
[21.01.2026 07:31] Writing top page (month).
[21.01.2026 08:34] Read previous papers.
[21.01.2026 08:34] Get feed.
[21.01.2026 08:34] Get page data from previous paper. URL: https://huggingface.co/papers/2601.11655
[21.01.2026 08:34] Get page data from previous paper. URL: https://huggingface.co/papers/2601.12993
[21.01.2026 08:34] Get page data from previous paper. URL: https://huggingface.co/papers/2601.13836
[21.01.2026 08:34] Get page data from previous paper. URL: https://huggingface.co/papers/2601.14250
[21.01.2026 08:34] Get page data from previous paper. URL: https://huggingface.co/papers/2601.14192
[21.01.2026 08:34] Get page data from previous paper. URL: https://huggingface.co/papers/2601.11969
[21.01.2026 08:34] Get page data from previous paper. URL: https://huggingface.co/papers/2601.11522
[21.01.2026 08:34] Get page data from previous paper. URL: https://huggingface.co/papers/2601.12294
[21.01.2026 08:34] Get page data from previous paper. URL: https://huggingface.co/papers/2601.13247
[21.01.2026 08:34] Get page data from previous paper. URL: https://huggingface.co/papers/2601.14046
[21.01.2026 08:34] Get page data from previous paper. URL: https://huggingface.co/papers/2601.11888
[21.01.2026 08:34] Extract page data from URL. URL: https://huggingface.co/papers/2601.12937
[21.01.2026 08:34] Get page data from previous paper. URL: https://huggingface.co/papers/2601.13288
[21.01.2026 08:34] Get page data from previous paper. URL: https://huggingface.co/papers/2601.12910
[21.01.2026 08:34] Get page data from previous paper. URL: https://huggingface.co/papers/2601.10700
[21.01.2026 08:34] Get page data from previous paper. URL: https://huggingface.co/papers/2601.13253
[21.01.2026 08:34] Get page data from previous paper. URL: https://huggingface.co/papers/2601.13251
[21.01.2026 08:34] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[21.01.2026 08:34] No deleted papers detected.
[21.01.2026 08:34] Downloading and parsing papers (pdf, html). Total: 17.
[21.01.2026 08:34] Downloading and parsing paper https://huggingface.co/papers/2601.11655.
[21.01.2026 08:34] Extra JSON file exists (./assets/json/2601.11655.json), skip PDF parsing.
[21.01.2026 08:34] Paper image links file exists (./assets/img_data/2601.11655.json), skip HTML parsing.
[21.01.2026 08:34] Success.
[21.01.2026 08:34] Downloading and parsing paper https://huggingface.co/papers/2601.12993.
[21.01.2026 08:34] Extra JSON file exists (./assets/json/2601.12993.json), skip PDF parsing.
[21.01.2026 08:34] Paper image links file exists (./assets/img_data/2601.12993.json), skip HTML parsing.
[21.01.2026 08:34] Success.
[21.01.2026 08:34] Downloading and parsing paper https://huggingface.co/papers/2601.13836.
[21.01.2026 08:34] Extra JSON file exists (./assets/json/2601.13836.json), skip PDF parsing.
[21.01.2026 08:34] Paper image links file exists (./assets/img_data/2601.13836.json), skip HTML parsing.
[21.01.2026 08:34] Success.
[21.01.2026 08:34] Downloading and parsing paper https://huggingface.co/papers/2601.14250.
[21.01.2026 08:34] Extra JSON file exists (./assets/json/2601.14250.json), skip PDF parsing.
[21.01.2026 08:34] Paper image links file exists (./assets/img_data/2601.14250.json), skip HTML parsing.
[21.01.2026 08:34] Success.
[21.01.2026 08:34] Downloading and parsing paper https://huggingface.co/papers/2601.14192.
[21.01.2026 08:34] Extra JSON file exists (./assets/json/2601.14192.json), skip PDF parsing.
[21.01.2026 08:34] Paper image links file exists (./assets/img_data/2601.14192.json), skip HTML parsing.
[21.01.2026 08:34] Success.
[21.01.2026 08:34] Downloading and parsing paper https://huggingface.co/papers/2601.11969.
[21.01.2026 08:34] Extra JSON file exists (./assets/json/2601.11969.json), skip PDF parsing.
[21.01.2026 08:34] Paper image links file exists (./assets/img_data/2601.11969.json), skip HTML parsing.
[21.01.2026 08:34] Success.
[21.01.2026 08:34] Downloading and parsing paper https://huggingface.co/papers/2601.11522.
[21.01.2026 08:34] Extra JSON file exists (./assets/json/2601.11522.json), skip PDF parsing.
[21.01.2026 08:34] Paper image links file exists (./assets/img_data/2601.11522.json), skip HTML parsing.
[21.01.2026 08:34] Success.
[21.01.2026 08:34] Downloading and parsing paper https://huggingface.co/papers/2601.12294.
[21.01.2026 08:34] Extra JSON file exists (./assets/json/2601.12294.json), skip PDF parsing.
[21.01.2026 08:34] Paper image links file exists (./assets/img_data/2601.12294.json), skip HTML parsing.
[21.01.2026 08:34] Success.
[21.01.2026 08:34] Downloading and parsing paper https://huggingface.co/papers/2601.13247.
[21.01.2026 08:34] Extra JSON file exists (./assets/json/2601.13247.json), skip PDF parsing.
[21.01.2026 08:34] Paper image links file exists (./assets/img_data/2601.13247.json), skip HTML parsing.
[21.01.2026 08:34] Success.
[21.01.2026 08:34] Downloading and parsing paper https://huggingface.co/papers/2601.14046.
[21.01.2026 08:34] Extra JSON file exists (./assets/json/2601.14046.json), skip PDF parsing.
[21.01.2026 08:34] Paper image links file exists (./assets/img_data/2601.14046.json), skip HTML parsing.
[21.01.2026 08:34] Success.
[21.01.2026 08:34] Downloading and parsing paper https://huggingface.co/papers/2601.11888.
[21.01.2026 08:34] Extra JSON file exists (./assets/json/2601.11888.json), skip PDF parsing.
[21.01.2026 08:34] Paper image links file exists (./assets/img_data/2601.11888.json), skip HTML parsing.
[21.01.2026 08:34] Success.
[21.01.2026 08:34] Downloading and parsing paper https://huggingface.co/papers/2601.12937.
[21.01.2026 08:34] Downloading paper 2601.12937 from https://arxiv.org/pdf/2601.12937v1...
[21.01.2026 08:35] Extracting affiliations from text.
[21.01.2026 08:35] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 9 1 ] . [ 1 7 3 9 2 1 . 1 0 6 2 : r a Murat Bilgehan Ertan 1 2 Emirhan Boge 3 Min Chen 2 Kaleel Mahmood 4 * Marten van Dijk 1 2 * "
[21.01.2026 08:35] Response: ```python
[]
```
[21.01.2026 08:35] Extracting affiliations from text.
[21.01.2026 08:35] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 9 1 ] . [ 1 7 3 9 2 1 . 1 0 6 2 : r aMurat Bilgehan Ertan 1 2 Emirhan Boge 3 Min Chen 2 Kaleel Mahmood 4 * Marten van Dijk 1 2 *As large language models (LLMs) are trained on increasingly opaque corpora, membership inference attacks (MIAs) have been proposed to audit whether copyrighted texts were used during training, despite growing concerns about their reliability under realistic conditions. We ask whether MIAs can serve as admissible evidence in adversarial copyright disputes where an accused model developer may obfuscate training data while preserving semantic content, and formalize this setting through judge-prosecutor-accused communication protocol. To test robustness under this protocol, we introduce SAGE (Structure-Aware SAE-Guided Extraction), paraphrasing framework guided by Sparse Autoencoders (SAEs) that rewrites training data to alter lexical structure while preserving semantic content and downstream utility. Our experiments show that stateof-the-art MIAs degrade when models are finetuned on SAGE-generated paraphrases, indicating that their signals are not robust to semanticspreserving transformations. While some leakage remains in certain fine-tuning regimes, these results suggest that MIAs are brittle in adversarial settings and insufficient, on their own, as standalone mechanism for copyright auditing of LLMs. 1. Introduction Large language models (LLMs) are increasingly trained and fine-tuned on massive text corpora whose precise composition is rarely disclosed (Longpre et al., 2023; Touvron et al., 2023), raising concern that training datasets may contain copyrighted material used without permission. These conThese authors contributed equally to this work.*Equally contributed insights, supervision, and responsibility for the successful completion of this work. 1Centrum Wiskunde & Informatica (CWI), Amsterdam, The Netherlands 2Vrije Universiteit Amsterdam, Amsterdam, The Netherlands 3Independent Researcher 4University of Rhode Island, Kingston, RI, United States. Correspondence to: Murat Bilgehan Ertan <bilgehan.ertan@cwi.nl>. Preprint. January 21, 2026. cerns are amplified by evidence that LLMs can memorize and reproduce training data (Carlini et al., 2021; Mireshghallah et al., 2022; Nasr et al., 2025; Ahmed et al., 2026), and by demonstrated vulnerabilities to extraction and inference attacks (Carlini et al., 2021; Chu et al., 2024; Guo et al., 2024). In practice, fine-tuning is widely used in sensitive domains such as medical (Singhal et al., 2023), legal (Guha et al., 2023), financial (Wu et al., 2023), and code-generation (Li et al., 2023) applications, where it often relies on private, regulated, or copyrighted data. As courts begin to confront these issues, central question emerges: what technical evidence, if any, can reliably demonstrate that particular copyrighted work was used to train model? recent U.S. federal ruling highlights the stakes of this question. In Bartz v. Anthropic PBC (2025), the court held that Anthropics use of books for model training did not violate copyright law2. Notably, the decision relied entirely on documentary evidence and company admissions, without considering any technical analysis of model behavior. While such evidence may suffice in cases involving voluntary disclosures, future disputes are likely to arise in settings where factual admissions are absent, adversarial incentives are strong (Henderson et al., 2023), and credible technical evidence would be required to substantiate claims of unauthorized data use. Membership inference attacks (MIAs) (Shokri et al., 2017; Carlini et al., 2021) are natural candidate for providing such technical evidence. Originally developed to detect whether record belonged to the training set of classifier, MIAs for LLMs typically rely on loss reduction, likelihood ratios, or calibration signals (Meeus et al., 2024a; Maini et al., 2024). Prior work has shown that LLMs can be vulnerable to MIAs under certain fine-tuning regimes (Chen et al., 2024; Zhang et al., 2025). Recent studies reveal substantial limitations: high reported AUCs often arise from temporal or distributional artifacts rather than genuine memorization (Duan et al., 2024; Meeus et al., 2024b; 2025; Maini et al., 2024), and MIAs against pre-training data are largely ineffective because each pre-training sample is typically seen only once (Komatsuzaki, 2019; Kandpal et al., 2022; Muennighoff et al., 2023; Das et al., 2025). Any method introduced in court must remain robust to ma2See Bartz v. Anthropic PBC, No. 24-05417 (N.D. Cal. 2025).Prosecutor Judge Accused / Defendant Prosecutor claim & evidence Judge sets threshold œÑ & decidessubmits claim & evidence provides model access sets œÑ / decides Suspect text allegedly from inputs Membership Inference compute score s(x); compare to œÑ audit Trained LLM released / audited Threshold selection: choose œÑ train / fine-tune Paraphrased dataset semantic-preserving variants (SAGE) Figure 1. JudgeProsecutorAccused protocol and artifact flow. Party-owned artifacts (prosecutor evidence; accused model/data) are color-coded by role, while auditing and obfuscation techniques (membership inference; paraphrasing/SAGE) are shown in neutral gray. nip"
[21.01.2026 08:35] Mistral response. {"id": "6bb60faaa58b4bc8a4e96529dd4e1cdf", "created": 1768984518, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1352, "total_tokens": 1411, "completion_tokens": 59}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\n    \"Centrum Wiskunde & Informatica (CWI), Amsterdam, The Netherlands\",\n    \"Vrije Universiteit Amsterdam, Amsterdam, The Netherlands\",\n    \"Independent Researcher\",\n    \"University of Rhode Island, Kingston, RI, United States\"\n]\n```"}}]}
[21.01.2026 08:35] Response: ```python
[
    "Centrum Wiskunde & Informatica (CWI), Amsterdam, The Netherlands",
    "Vrije Universiteit Amsterdam, Amsterdam, The Netherlands",
    "Independent Researcher",
    "University of Rhode Island, Kingston, RI, United States"
]
```
[21.01.2026 08:35] Deleting PDF ./assets/pdf/2601.12937.pdf.
[21.01.2026 08:35] Success.
[21.01.2026 08:35] Downloading and parsing paper https://huggingface.co/papers/2601.13288.
[21.01.2026 08:35] Extra JSON file exists (./assets/json/2601.13288.json), skip PDF parsing.
[21.01.2026 08:35] Paper image links file exists (./assets/img_data/2601.13288.json), skip HTML parsing.
[21.01.2026 08:35] Success.
[21.01.2026 08:35] Downloading and parsing paper https://huggingface.co/papers/2601.12910.
[21.01.2026 08:35] Extra JSON file exists (./assets/json/2601.12910.json), skip PDF parsing.
[21.01.2026 08:35] Paper image links file exists (./assets/img_data/2601.12910.json), skip HTML parsing.
[21.01.2026 08:35] Success.
[21.01.2026 08:35] Downloading and parsing paper https://huggingface.co/papers/2601.10700.
[21.01.2026 08:35] Extra JSON file exists (./assets/json/2601.10700.json), skip PDF parsing.
[21.01.2026 08:35] Paper image links file exists (./assets/img_data/2601.10700.json), skip HTML parsing.
[21.01.2026 08:35] Success.
[21.01.2026 08:35] Downloading and parsing paper https://huggingface.co/papers/2601.13253.
[21.01.2026 08:35] Extra JSON file exists (./assets/json/2601.13253.json), skip PDF parsing.
[21.01.2026 08:35] Paper image links file exists (./assets/img_data/2601.13253.json), skip HTML parsing.
[21.01.2026 08:35] Success.
[21.01.2026 08:35] Downloading and parsing paper https://huggingface.co/papers/2601.13251.
[21.01.2026 08:35] Extra JSON file exists (./assets/json/2601.13251.json), skip PDF parsing.
[21.01.2026 08:35] Paper image links file exists (./assets/img_data/2601.13251.json), skip HTML parsing.
[21.01.2026 08:35] Success.
[21.01.2026 08:35] Enriching papers with extra data.
[21.01.2026 08:35] ********************************************************************************
[21.01.2026 08:35] Abstract 0. Large language models face significant challenges in software issue resolution, prompting the development of autonomous coding agents through various training-free and training-based methodologies.  					AI-generated summary 				 Issue resolution, a complex Software Engineering (SWE) task integral t...
[21.01.2026 08:35] ********************************************************************************
[21.01.2026 08:35] Abstract 1. Being-H0.5 is a Vision-Language-Action model that enables robust cross-embodiment generalization through human-centric learning and a Mixture-of-Transformers architecture with specialized embodiment handling.  					AI-generated summary 				 We introduce Being-H0.5, a foundational Vision-Language-Act...
[21.01.2026 08:35] ********************************************************************************
[21.01.2026 08:35] Abstract 2. FutureOmni presents the first benchmark for evaluating multimodal models' ability to forecast future events from audio-visual data, revealing current limitations and proposing an improved training strategy for better performance.  					AI-generated summary 				 Although Multimodal Large Language Mod...
[21.01.2026 08:35] ********************************************************************************
[21.01.2026 08:35] Abstract 3. OmniTransfer presents a unified framework for spatio-temporal video transfer that enhances appearance consistency and temporal control through multi-view information and multimodal semantic guidance.  					AI-generated summary 				 Videos convey richer information than images or text, capturing both...
[21.01.2026 08:35] ********************************************************************************
[21.01.2026 08:35] Abstract 4. Efficiency in agentic systems is examined across memory, tool learning, and planning components, analyzing trade-offs between effectiveness and computational costs through various optimization strategies and benchmarks.  					AI-generated summary 				 Recent years have witnessed increasing interest ...
[21.01.2026 08:35] ********************************************************************************
[21.01.2026 08:35] Abstract 5. A benchmark called MemoryRewardBench is introduced to systematically evaluate reward models' ability to assess long-term memory management in large language models across various context lengths and memory patterns.  					AI-generated summary 				 Existing works increasingly adopt memory-centric mec...
[21.01.2026 08:35] ********************************************************************************
[21.01.2026 08:35] Abstract 6. UniX presents a unified medical foundation model that decouples visual understanding and generation tasks using distinct autoregressive and diffusion branches with cross-modal attention for enhanced performance.  					AI-generated summary 				 Despite recent progress, medical foundation models still...
[21.01.2026 08:35] ********************************************************************************
[21.01.2026 08:35] Abstract 7. ToolPRMBench is introduced as a large-scale benchmark for evaluating process reward models in tool-using agents, featuring step-level test cases and multi-LLM verification to ensure data quality.  					AI-generated summary 				 Reward-guided search methods have demonstrated strong potential in enhan...
[21.01.2026 08:35] ********************************************************************************
[21.01.2026 08:35] Abstract 8. WorldMind addresses the modal disconnect in LLMs by autonomously building a symbolic world knowledge repository that enhances physical feasibility and task optimality through experience-based learning.  					AI-generated summary 				 Current Large Language Models (LLMs) exhibit a critical modal disc...
[21.01.2026 08:35] ********************************************************************************
[21.01.2026 08:35] Abstract 9. PRiSM benchmark evaluates phonetic perception in speech models through standardized transcription-based metrics and downstream applications across clinical, educational, and multilingual domains.  					AI-generated summary 				 Phone recognition (PR) serves as the atomic interface for language-agnos...
[21.01.2026 08:35] ********************************************************************************
[21.01.2026 08:35] Abstract 10. A novel retriever training framework for agentic search that uses both local relevance and global answer correctness metrics with iterative optimization between the search agent and retriever.  					AI-generated summary 				 Agentic search has recently emerged as a powerful paradigm, where an agent ...
[21.01.2026 08:35] ********************************************************************************
[21.01.2026 08:35] Abstract 11. Membership inference attacks fail to reliably detect copyrighted text usage in large language models when training data is paraphrased using structure-aware methods that preserve semantic content.  					AI-generated summary 				 As large language models (LLMs) are trained on increasingly opaque corp...
[21.01.2026 08:35] ********************************************************************************
[21.01.2026 08:35] Abstract 12. Lightweight probes trained on hidden states of LLMs enable efficient classification tasks without additional computational overhead, improving safety and sentiment analysis performance.  					AI-generated summary 				 Production LLM systems often rely on separate models for safety and other classifi...
[21.01.2026 08:35] ********************************************************************************
[21.01.2026 08:35] Abstract 13. SciCoQA is a dataset for identifying mismatches between scientific publications and code implementations, containing 611 discrepancies across multiple disciplines and demonstrating the challenge of detecting such issues even for advanced language models.  					AI-generated summary 				 We present Sc...
[21.01.2026 08:35] ********************************************************************************
[21.01.2026 08:35] Abstract 14. A framework for generating structured counterfactual pairs using LLMs and SCMs enables improved evaluation and analysis of concept-based explanations in high-stakes domains.  					AI-generated summary 				 Concept-based explanations quantify how high-level concepts (e.g., gender or experience) influ...
[21.01.2026 08:35] ********************************************************************************
[21.01.2026 08:35] Abstract 15. A hybrid methodology combining FastText embeddings, clustering, and AI classification generates a large-scale Turkish semantic relations dataset with high accuracy validation.  					AI-generated summary 				 We present a hybrid methodology for generating large-scale semantic relationship datasets in...
[21.01.2026 08:35] ********************************************************************************
[21.01.2026 08:35] Abstract 16. A large-scale semantic clustering system addresses the limitation of neural embeddings in distinguishing synonyms from antonyms through a specialized three-way discriminator and novel clustering algorithm.  					AI-generated summary 				 Neural embeddings have a notorious blind spot: they can't reli...
[21.01.2026 08:35] Read previous papers.
[21.01.2026 08:35] Generating reviews via LLM API.
[21.01.2026 08:35] Using data from previous issue: {"categories": ["#rl", "#benchmark", "#dataset", "#agents", "#data", "#training", "#open_source", "#plp", "#survey"], "emoji": "ü§ñ", "ru": {"title": "–ê–≤—Ç–æ–Ω–æ–º–Ω—ã–µ –∞–≥–µ–Ω—Ç—ã –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–≥—Ä–∞–º–º–Ω—ã—Ö –æ—à–∏–±–æ–∫", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç—Å—è —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –æ–±–∑–æ—Ä –∞–≤—Ç–æ–Ω–æ–º–Ω—ã—Ö –∫–æ–¥–∏—Ä—É—é—â–∏—Ö –∞–≥–µ–Ω
[21.01.2026 08:35] Using data from previous issue: {"categories": ["#architecture", "#dataset", "#training", "#multimodal", "#robotics"], "emoji": "ü§ñ", "ru": {"title": "–ß–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–π —è–∑—ã–∫ –¥–ª—è –≤—Å–µ—Ö —Ä–æ–±–æ—Ç–æ–≤: universal VLA —Å –∫—Ä–æ—Å—Å–ø–ª–∞—Ç—Ñ–æ—Ä–º–µ–Ω–Ω—ã–º –æ–±–æ–±—â–µ–Ω–∏–µ–º", "desc": "Being-H0.5 ‚Äî —ç—Ç–æ –º–æ–¥–µ–ª—å Vision-Language-Action (VLA), —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –¥–ª—è –æ–±–æ–±—â–µ–Ω–∏—è —Ä–æ–±–æ—Ç–æ—Ç
[21.01.2026 08:35] Using data from previous issue: {"categories": ["#audio", "#video", "#benchmark", "#dataset", "#training", "#multimodal"], "emoji": "üîÆ", "ru": {"title": "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –±—É–¥—É—â–µ–≥–æ: —É—á–∏–º –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≤–∏–¥–µ—Ç—å –∑–∞–≤—Ç—Ä–∞", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ FutureOmni ‚Äî –ø–µ—Ä–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –ø—Ä–µ–¥—Å–∫–∞
[21.01.2026 08:35] Using data from previous issue: {"categories": ["#video", "#transfer_learning", "#architecture", "#multimodal"], "emoji": "üé¨", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π —Ç—Ä–∞–Ω—Å—Ñ–µ—Ä –≤–∏–¥–µ–æ —á–µ—Ä–µ–∑ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ-–≤—Ä–µ–º–µ–Ω–Ω–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ", "desc": "OmniTransfer –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –µ–¥–∏–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Ç—Ä–∞–Ω—Å—Ñ–µ—Ä–∞ –≤–∏–¥–µ–æ, –∫–æ—Ç–æ—Ä—ã–π —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –ø—Ä–æ—Å
[21.01.2026 08:35] Using data from previous issue: {"categories": ["#rl", "#optimization", "#long_context", "#agents", "#benchmark"], "emoji": "‚ö°", "ru": {"title": "–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º: –±–∞–ª–∞–Ω—Å –º–µ–∂–¥—É –∫–∞—á–µ—Å—Ç–≤–æ–º –∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–π —Å—Ç–æ–∏–º–æ—Å—Ç—å—é", "desc": "–í —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∞–Ω–∞–ª–∏–∑–∏—Ä
[21.01.2026 08:35] Using data from previous issue: {"categories": ["#long_context", "#survey", "#benchmark", "#dataset"], "emoji": "üß†", "ru": {"title": "–û—Ü–µ–Ω–∫–∞ –ø–∞–º—è—Ç–∏: –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω MemoryRewardBench ‚Äî –ø–µ—Ä–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ
[21.01.2026 08:35] Using data from previous issue: {"categories": ["#diffusion", "#training", "#data", "#architecture", "#open_source", "#multimodal", "#healthcare"], "emoji": "ü´Ä", "ru": {"title": "–°–∏–Ω–µ—Ä–≥–∏—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ —Å–æ–∑–¥–∞–Ω–∏—è: —Ä–∞–∑–¥–µ–ª—ë–Ω–Ω—ã–µ –≤–µ—Ç–≤–∏ –¥–ª—è —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞", "desc": "UniX ‚Äî —ç—Ç–æ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∞—è –º–æ–¥–µ–ª—å-—Ñ—É–Ω–¥–∞
[21.01.2026 08:35] Using data from previous issue: {"categories": ["#agents", "#benchmark", "#dataset", "#rl"], "emoji": "üîß", "ru": {"title": "–°–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ—Ü–µ–Ω–∏–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –¥–ª—è –∞–≥–µ–Ω—Ç–æ–≤ —Å –¥–æ—Å—Ç—É–ø–æ–º –∫ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ ToolPRMBench ‚Äî –º–∞—Å—à—Ç–∞–±–Ω–∞—è –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –Ω–∞ —É—Ä–æ–≤–Ω–µ —à–∞–≥–æ–≤
[21.01.2026 08:35] Using data from previous issue: {"categories": ["#transfer_learning", "#alignment", "#hallucinations", "#reasoning"], "emoji": "üåç", "ru": {"title": "–ì—Äounded –º–∏—Ä –¥–ª—è LLM: –æ—Ç –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π –∫ —Ñ–∏–∑–∏—á–µ—Å–∫–∏ –æ—Å—É—â–µ—Å—Ç–≤–∏–º—ã–º –ø–ª–∞–Ω–∞–º", "desc": "WorldMind —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –º–æ–¥–∞–ª—å–Ω–æ–π —Ä–∞—Å—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö –ø—É—Ç—ë–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥
[21.01.2026 08:35] Using data from previous issue: {"categories": ["#open_source", "#audio", "#dataset", "#low_resource", "#multilingual", "#benchmark"], "emoji": "üî§", "ru": {"title": "–ì–ª—É–±–æ–∫–∞—è –æ—Ü–µ–Ω–∫–∞ —Ñ–æ–Ω–µ—Ç–∏—á–µ—Å–∫–æ–≥–æ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è –≤ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö —Ä–µ—á–µ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –±–µ–Ω—á–º–∞—Ä–∫ PRiSM –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è —Ñ–æ–Ω–µ—Ç–∏–∫–∏ –≤ —Å–∏—Å—Ç–µ–º–∞—Ö —Ä–∞—Å–ø–æ
[21.01.2026 08:35] Using data from previous issue: {"categories": ["#agents", "#benchmark", "#training", "#rag"], "emoji": "üîÑ", "ru": {"title": "–î–≤—É—Å—Ç–æ—Ä–æ–Ω–Ω—è—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∞–≥–µ–Ω—Ç–∞ –ø–æ–∏—Å–∫–∞ –∏ –ø–æ–∏—Å–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è —Ç–æ—á–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –Ω–æ–≤–∞—èÊ°ÜÊû∂ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è retriever'–∞, –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–Ω–∞—è —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∞–≥–µ–Ω—Ç–æ–≤. 
[21.01.2026 08:35] Querying the API.
[21.01.2026 08:35] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Membership inference attacks fail to reliably detect copyrighted text usage in large language models when training data is paraphrased using structure-aware methods that preserve semantic content.  					AI-generated summary 				 As large language models (LLMs) are trained on increasingly opaque corpora, membership inference attacks (MIAs) have been proposed to audit whether copyrighted texts were used during training, despite growing concerns about their reliability under realistic conditions. We ask whether MIAs can serve as admissible evidence in adversarial copyright disputes where an accused model developer may obfuscate training data while preserving semantic content, and formalize this setting through a judge-prosecutor-accused communication protocol. To test robustness under this protocol, we introduce SAGE (Structure-Aware SAE-Guided Extraction), a paraphrasing framework guided by Sparse Autoencoders (SAEs) that rewrites training data to alter lexical structure while preserving semantic content and downstream utility. Our experiments show that state-of-the-art MIAs degrade when models are fine-tuned on SAGE-generated paraphrases, indicating that their signals are not robust to semantics-preserving transformations. While some leakage remains in certain fine-tuning regimes, these results suggest that MIAs are brittle in adversarial settings and insufficient, on their own, as a standalone mechanism for copyright auditing of LLMs.
[21.01.2026 08:35] Response: ```json
{
  "desc": "–í —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –Ω–∞–¥—ë–∂–Ω–æ—Å—Ç—å –∞—Ç–∞–∫ –≤—ã–≤–æ–¥–∞ —á–ª–µ–Ω—Å—Ç–≤–∞ (MIA) –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∑–∞—â–∏—â—ë–Ω–Ω–æ–≥–æ –∞–≤—Ç–æ—Ä—Å–∫–∏–º –ø—Ä–∞–≤–æ–º —Ç–µ–∫—Å—Ç–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ SAGE, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–µ –∞–≤—Ç–æ–∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–∏ (SAE) –¥–ª—è –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –æ–±—É—á–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ, –Ω–æ –∏–∑–º–µ–Ω—è–µ—Ç –ª–µ–∫—Å–∏—á–µ—Å–∫—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã MIA –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –¥–µ–≥—Ä–∞–¥–∏—Ä—É—é—Ç –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å –¥–∞–Ω–Ω—ã–º–∏, –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–º–∏ SAGE, —á—Ç–æ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —Ö—Ä—É–ø–∫–æ—Å—Ç—å —ç—Ç–∏—Ö –∞—Ç–∞–∫. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–¥—á—ë—Ä–∫–∏–≤–∞–µ—Ç, —á—Ç–æ MIA –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω—ã –∫–∞–∫ —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω—ã–π –º–µ—Ö–∞–Ω–∏–∑–º –¥–ª—è –∞—É–¥–∏—Ç–∞ –∞–≤—Ç–æ—Ä—Å–∫–∏—Ö –ø—Ä–∞–≤ LLM –∏ –º–æ–≥—É—Ç –±—ã—Ç—å –æ–±–æ–π–¥–µ–Ω—ã –ø—Ä–∏ —Å–µ–º–∞–Ω—Ç–∏–∫–æ-—Å–æ—Ö—Ä–∞–Ω—è—é—â–∏—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è—Ö —Ç–µ–∫—Å—Ç–∞.",
  "emoji": "üîì",
  "title": "–•—Ä—É–ø–∫–æ—Å—Ç—å –∞—Ç–∞–∫ –≤—ã–≤–æ–¥–∞ —á–ª–µ–Ω—Å—Ç–≤–∞ –ø–µ—Ä–µ–¥ —Å–µ–º–∞–Ω—Ç–∏–∫–æ-—Å–æ—Ö—Ä–∞–Ω—è—é—â–∏–º –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∞–Ω–∏–µ–º"
}
```
[21.01.2026 08:35] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Membership inference attacks fail to reliably detect copyrighted text usage in large language models when training data is paraphrased using structure-aware methods that preserve semantic content.  					AI-generated summary 				 As large language models (LLMs) are trained on increasingly opaque corpora, membership inference attacks (MIAs) have been proposed to audit whether copyrighted texts were used during training, despite growing concerns about their reliability under realistic conditions. We ask whether MIAs can serve as admissible evidence in adversarial copyright disputes where an accused model developer may obfuscate training data while preserving semantic content, and formalize this setting through a judge-prosecutor-accused communication protocol. To test robustness under this protocol, we introduce SAGE (Structure-Aware SAE-Guided Extraction), a paraphrasing framework guided by Sparse Autoencoders (SAEs) that rewrites training data to alter lexical structure while preserving semantic content and downstream utility. Our experiments show that state-of-the-art MIAs degrade when models are fine-tuned on SAGE-generated paraphrases, indicating that their signals are not robust to semantics-preserving transformations. While some leakage remains in certain fine-tuning regimes, these results suggest that MIAs are brittle in adversarial settings and insufficient, on their own, as a standalone mechanism for copyright auditing of LLMs."

[21.01.2026 08:35] Response: ```python
["TRAINING"]
```
[21.01.2026 08:35] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Membership inference attacks fail to reliably detect copyrighted text usage in large language models when training data is paraphrased using structure-aware methods that preserve semantic content.  					AI-generated summary 				 As large language models (LLMs) are trained on increasingly opaque corpora, membership inference attacks (MIAs) have been proposed to audit whether copyrighted texts were used during training, despite growing concerns about their reliability under realistic conditions. We ask whether MIAs can serve as admissible evidence in adversarial copyright disputes where an accused model developer may obfuscate training data while preserving semantic content, and formalize this setting through a judge-prosecutor-accused communication protocol. To test robustness under this protocol, we introduce SAGE (Structure-Aware SAE-Guided Extraction), a paraphrasing framework guided by Sparse Autoencoders (SAEs) that rewrites training data to alter lexical structure while preserving semantic content and downstream utility. Our experiments show that state-of-the-art MIAs degrade when models are fine-tuned on SAGE-generated paraphrases, indicating that their signals are not robust to semantics-preserving transformations. While some leakage remains in certain fine-tuning regimes, these results suggest that MIAs are brittle in adversarial settings and insufficient, on their own, as a standalone mechanism for copyright auditing of LLMs."

[21.01.2026 08:35] Response: ```python
['SECURITY', 'LEAKAGE']
```
[21.01.2026 08:35] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper investigates the effectiveness of membership inference attacks (MIAs) in detecting the use of copyrighted text in large language models (LLMs) when the training data is paraphrased using a method that maintains its meaning. The authors introduce a new framework called SAGE, which uses Sparse Autoencoders to create paraphrases that change the wording but keep the original content intact. Their experiments reveal that MIAs struggle to identify copyrighted material when models are trained on these paraphrased texts, indicating that MIAs are not reliable in adversarial situations. Overall, the findings suggest that MIAs alone cannot be trusted for copyright auditing of LLMs, especially when the training data is obfuscated.","title":"MIAs Struggle Against Paraphrased Training Data"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper investigates the effectiveness of membership inference attacks (MIAs) in detecting the use of copyrighted text in large language models (LLMs) when the training data is paraphrased using a method that maintains its meaning. The authors introduce a new framework called SAGE, which uses Sparse Autoencoders to create paraphrases that change the wording but keep the original content intact. Their experiments reveal that MIAs struggle to identify copyrighted material when models are trained on these paraphrased texts, indicating that MIAs are not reliable in adversarial situations. Overall, the findings suggest that MIAs alone cannot be trusted for copyright auditing of LLMs, especially when the training data is obfuscated.', title='MIAs Struggle Against Paraphrased Training Data'))
[21.01.2026 08:35] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨ÊñáÊé¢ËÆ®‰∫Ü‰ºöÂëòÊé®Êñ≠ÊîªÂáªÔºàMIAÔºâÂú®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâ‰∏≠Ê£ÄÊµãÁâàÊùÉÊñáÊú¨‰ΩøÁî®ÁöÑÊúâÊïàÊÄß„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÂΩìËÆ≠ÁªÉÊï∞ÊçÆÈÄöËøáÁªìÊûÑÊÑüÁü•ÁöÑÊñπÊ≥ïËøõË°åÊîπÂÜôÊó∂ÔºåMIAÁöÑÂèØÈù†ÊÄßÊòæËëó‰∏ãÈôç„ÄÇËøôÁßçÊñπÊ≥ï‰ΩøÁî®Á®ÄÁñèËá™ÁºñÁ†ÅÂô®ÔºàSAEÔºâÊù•‰øùÊåÅËØ≠‰πâÂÜÖÂÆπÔºåÂêåÊó∂ÊîπÂèòËØçÊ±áÁªìÊûÑ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåMIAÂú®Èù¢ÂØπËøôÁßçËØ≠‰πâ‰øùÊåÅÁöÑÊîπÂÜôÊó∂Ë°®Áé∞Âá∫ËÑÜÂº±ÊÄßÔºåÊó†Ê≥ïÂçïÁã¨‰Ωú‰∏∫ÁâàÊùÉÂÆ°ËÆ°ÁöÑÊúâÊïàÊú∫Âà∂„ÄÇ","title":"‰ºöÂëòÊé®Êñ≠ÊîªÂáªÂú®ËØ≠‰πâ‰øùÊåÅ‰∏ãÁöÑËÑÜÂº±ÊÄß"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨ÊñáÊé¢ËÆ®‰∫Ü‰ºöÂëòÊé®Êñ≠ÊîªÂáªÔºàMIAÔºâÂú®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâ‰∏≠Ê£ÄÊµãÁâàÊùÉÊñáÊú¨‰ΩøÁî®ÁöÑÊúâÊïàÊÄß„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÂΩìËÆ≠ÁªÉÊï∞ÊçÆÈÄöËøáÁªìÊûÑÊÑüÁü•ÁöÑÊñπÊ≥ïËøõË°åÊîπÂÜôÊó∂ÔºåMIAÁöÑÂèØÈù†ÊÄßÊòæËëó‰∏ãÈôç„ÄÇËøôÁßçÊñπÊ≥ï‰ΩøÁî®Á®ÄÁñèËá™ÁºñÁ†ÅÂô®ÔºàSAEÔºâÊù•‰øùÊåÅËØ≠‰πâÂÜÖÂÆπÔºåÂêåÊó∂ÊîπÂèòËØçÊ±áÁªìÊûÑ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåMIAÂú®Èù¢ÂØπËøôÁßçËØ≠‰πâ‰øùÊåÅÁöÑÊîπÂÜôÊó∂Ë°®Áé∞Âá∫ËÑÜÂº±ÊÄßÔºåÊó†Ê≥ïÂçïÁã¨‰Ωú‰∏∫ÁâàÊùÉÂÆ°ËÆ°ÁöÑÊúâÊïàÊú∫Âà∂„ÄÇ', title='‰ºöÂëòÊé®Êñ≠ÊîªÂáªÂú®ËØ≠‰πâ‰øùÊåÅ‰∏ãÁöÑËÑÜÂº±ÊÄß'))
[21.01.2026 08:35] Using data from previous issue: {"categories": ["#training", "#inference", "#small_models"], "emoji": "üîç", "ru": {"title": "–õ—ë–≥–∫–∏–µ –∑–æ–Ω–¥—ã –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏: –ø–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π LLM –±–µ–∑ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è –ª—ë–≥–∫–∏—Ö –∑–æ–Ω–¥–æ–≤ (probes) –Ω–∞ —Å–∫—Ä—ã—Ç—ã—Ö —Å–æ—Å—Ç–æ—è–Ω–∏—è—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –≤—ã
[21.01.2026 08:35] Using data from previous issue: {"categories": ["#long_context", "#hallucinations", "#synthetic", "#science"], "emoji": "üîç", "ru": {"title": "–í—ã—è–≤–ª–µ–Ω–∏–µ —Å–∫—Ä—ã—Ç—ã—Ö –æ—à–∏–±–æ–∫: –∫–æ–≥–¥–∞ –∫–æ–¥ –Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –Ω–∞—É—á–Ω–æ–π —Å—Ç–∞—Ç—å–µ", "desc": "SciCoQA ‚Äî —ç—Ç–æ –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–π –º–µ–∂–¥—É –æ–ø–∏—Å–∞–Ω–∏–µ–º –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –≤ –Ω–∞—É—á–Ω—ã—Ö –ø—É–±–ª–∏–∫–∞—Ü–∏—è—Ö –∏ –∏—Ö —Ä–µ–∞–ª–∏–∑–∞—Ü–∏
[21.01.2026 08:35] Using data from previous issue: {"categories": ["#healthcare", "#interpretability", "#ethics", "#benchmark", "#dataset", "#synthetic"], "emoji": "üîç", "ru": {"title": "–û–±—ä–µ–∫—Ç–∏–≤–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±—ä—è—Å–Ω–∏–º–æ—Å—Ç–∏ —á–µ—Ä–µ–∑ –ø—Ä–∏—á–∏–Ω–Ω—ã–µ –∫–æ–Ω—Ç—Ä—Ñ–∞–∫—Ç—ã", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç LIBERTy ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ —Å –∫–æ–Ω—Ç—Ä—Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–º–∏ –ø–∞—Ä–∞–º–∏
[21.01.2026 08:35] Using data from previous issue: {"categories": ["#multilingual", "#data", "#low_resource", "#dataset", "#open_source", "#synthetic"], "emoji": "üáπüá∑", "ru": {"title": "–ú–∞—Å—à—Ç–∞–±–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –æ—Ç–Ω–æ—à–µ–Ω–∏–π —Ç—É—Ä–µ—Ü–∫–æ–≥–æ —è–∑—ã–∫–∞ —á–µ—Ä–µ–∑ –≥–∏–±—Ä–∏–¥–Ω—É—é –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—é", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –≥–∏–±—Ä–∏–¥–Ω—ã–π –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—é –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –±–æ–ª
[21.01.2026 08:35] Using data from previous issue: {"categories": [], "emoji": "üéØ", "ru": {"title": "–†–∞–∑–ª–∏—á–∏–µ —Å–∏–Ω–æ–Ω–∏–º–æ–≤ –∏ –∞–Ω—Ç–æ–Ω–∏–º–æ–≤: —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –±–µ–∑ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –¥—Ä–µ–π—Ñ–∞", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –Ω–µ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Ä–∞–∑–ª–∏—á–∞—Ç—å —Å–∏–Ω–æ–Ω–∏–º—ã –∏ –∞–Ω—Ç–æ–Ω–∏–º—ã. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—É—é —Å–∏—Å—Ç–µ–º—É —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π
[21.01.2026 08:35] Renaming data file.
[21.01.2026 08:35] Renaming previous data. hf_papers.json to ./d/2026-01-21.json
[21.01.2026 08:35] Saving new data file.
[21.01.2026 08:35] Generating page.
[21.01.2026 08:35] Renaming previous page.
[21.01.2026 08:35] Renaming previous data. index.html to ./d/2026-01-21.html
[21.01.2026 08:35] Writing result.
[21.01.2026 08:35] Renaming log file.
[21.01.2026 08:35] Renaming previous data. log.txt to ./logs/2026-01-21_last_log.txt
