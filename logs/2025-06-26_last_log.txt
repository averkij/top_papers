[26.06.2025 08:17] Read previous papers.
[26.06.2025 08:17] Generating top page (month).
[26.06.2025 08:17] Writing top page (month).
[26.06.2025 09:13] Read previous papers.
[26.06.2025 09:13] Get feed.
[26.06.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2506.18095
[26.06.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2506.19697
[26.06.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2506.16012
[26.06.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2506.18315
[26.06.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2506.18088
[26.06.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2506.18674
[26.06.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2506.20544
[26.06.2025 09:13] Extract page data from URL. URL: https://huggingface.co/papers/2506.20495
[26.06.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2506.20452
[26.06.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2506.18403
[26.06.2025 09:13] Extract page data from URL. URL: https://huggingface.co/papers/2506.20512
[26.06.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2506.19502
[26.06.2025 09:13] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[26.06.2025 09:13] No deleted papers detected.
[26.06.2025 09:13] Downloading and parsing papers (pdf, html). Total: 12.
[26.06.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2506.18095.
[26.06.2025 09:13] Extra JSON file exists (./assets/json/2506.18095.json), skip PDF parsing.
[26.06.2025 09:13] Paper image links file exists (./assets/img_data/2506.18095.json), skip HTML parsing.
[26.06.2025 09:13] Success.
[26.06.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2506.19697.
[26.06.2025 09:13] Extra JSON file exists (./assets/json/2506.19697.json), skip PDF parsing.
[26.06.2025 09:13] Paper image links file exists (./assets/img_data/2506.19697.json), skip HTML parsing.
[26.06.2025 09:13] Success.
[26.06.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2506.16012.
[26.06.2025 09:13] Extra JSON file exists (./assets/json/2506.16012.json), skip PDF parsing.
[26.06.2025 09:13] Paper image links file exists (./assets/img_data/2506.16012.json), skip HTML parsing.
[26.06.2025 09:13] Success.
[26.06.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2506.18315.
[26.06.2025 09:13] Extra JSON file exists (./assets/json/2506.18315.json), skip PDF parsing.
[26.06.2025 09:13] Paper image links file exists (./assets/img_data/2506.18315.json), skip HTML parsing.
[26.06.2025 09:13] Success.
[26.06.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2506.18088.
[26.06.2025 09:13] Extra JSON file exists (./assets/json/2506.18088.json), skip PDF parsing.
[26.06.2025 09:13] Paper image links file exists (./assets/img_data/2506.18088.json), skip HTML parsing.
[26.06.2025 09:13] Success.
[26.06.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2506.18674.
[26.06.2025 09:13] Extra JSON file exists (./assets/json/2506.18674.json), skip PDF parsing.
[26.06.2025 09:13] Paper image links file exists (./assets/img_data/2506.18674.json), skip HTML parsing.
[26.06.2025 09:13] Success.
[26.06.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2506.20544.
[26.06.2025 09:13] Extra JSON file exists (./assets/json/2506.20544.json), skip PDF parsing.
[26.06.2025 09:13] Paper image links file exists (./assets/img_data/2506.20544.json), skip HTML parsing.
[26.06.2025 09:13] Success.
[26.06.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2506.20495.
[26.06.2025 09:13] Downloading paper 2506.20495 from http://arxiv.org/pdf/2506.20495v1...
[26.06.2025 09:13] Extracting affiliations from text.
[26.06.2025 09:13] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"ReCode: Updating Code API Knowledge with Reinforcement Learning Haoze Wu1, Yunzhi Yao1, Wenhao Yu2*, Huajun Chen1, Ningyu Zhang1 1Zhejiang University 2Tencent AI, Seattle Lab wuhz1020@gmail.com, yyztodd@zju.edu.cn, wenhaoyu97@gmail.com, huajunsir@zju.edu.cn, zhangningyu@zju.edu.cn 5 2 0 2 5 2 ] . [ 1 5 9 4 0 2 . 6 0 5 2 : r a "
[26.06.2025 09:13] Response: ```python
["Zhejiang University", "Tencent AI, Seattle Lab"]
```
[26.06.2025 09:13] Deleting PDF ./assets/pdf/2506.20495.pdf.
[26.06.2025 09:13] Success.
[26.06.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2506.20452.
[26.06.2025 09:13] Extra JSON file exists (./assets/json/2506.20452.json), skip PDF parsing.
[26.06.2025 09:13] Paper image links file exists (./assets/img_data/2506.20452.json), skip HTML parsing.
[26.06.2025 09:13] Success.
[26.06.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2506.18403.
[26.06.2025 09:13] Extra JSON file exists (./assets/json/2506.18403.json), skip PDF parsing.
[26.06.2025 09:13] Paper image links file exists (./assets/img_data/2506.18403.json), skip HTML parsing.
[26.06.2025 09:13] Success.
[26.06.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2506.20512.
[26.06.2025 09:13] Downloading paper 2506.20512 from http://arxiv.org/pdf/2506.20512v1...
[26.06.2025 09:13] Extracting affiliations from text.
[26.06.2025 09:13] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"2025-6-26 OctoThinker: Mid-training Incentivizes Reinforcement Learning Scaling Zengzhi Wang*, Fan Zhou*, Xuefeng Li*, Pengfei Liu Shanghai Jiao Tong University, SII, GAIR Lab {zengzhi.wang, zhoufan98, xuefengli, pengfei}@sjtu.edu.cn GAIR-NLP/OctoThinker OctoThinker Different base language model familiessuch as Llama and Qwenexhibit divergent behaviors during post-training with reinforcement learning (RL), especially on reasoning-intensive tasks. What makes base language model suitable for reinforcement learning? Gaining deeper insight into this question is essential for developing RL-scalable foundation models of the next generation. In this work, we investigate how mid-training strategies shape RL dynamics, focusing on two representative model families: Qwen and Llama. Our study reveals that (1) high-quality mathematical corpora, such as MegaMath-Web-Pro, significantly improve both base model and RL performance, while existing alternatives (e.g., FineMath-4plus) fail to do so; (2) further adding QA-style data, particularly long chain-of-thought (CoT) reasoning examples, enhances RL outcomes, and instruction data further unlocks this effect; (3) while long-CoT improves reasoning depth, it can also induce verbosity of model responses and unstability of RL training, underscoring the importance of data formatting; (4) scaling mid-training consistently leads to stronger downstream RL performance. Building on these insights, we introduce two-stage mid-training strategyStable-then-Decayin which base models are first trained on 200B tokens with constant learning rate, followed by 20B tokens across three CoT-focused branches with learning rate decay. This yields OctoThinker, family of models demonstrating strong RL compatibility and closing the performance gap with more RL-friendly model families, i.e., Qwen. We hope our work will help shape pre-training strategies for foundation models in the RL era. To support further research, we release our open-source models along wit"
[26.06.2025 09:13] Response: ```python
["Shanghai Jiao Tong University, SII, GAIR Lab"]
```
[26.06.2025 09:13] Deleting PDF ./assets/pdf/2506.20512.pdf.
[26.06.2025 09:13] Success.
[26.06.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2506.19502.
[26.06.2025 09:13] Extra JSON file exists (./assets/json/2506.19502.json), skip PDF parsing.
[26.06.2025 09:13] Paper image links file exists (./assets/img_data/2506.19502.json), skip HTML parsing.
[26.06.2025 09:13] Success.
[26.06.2025 09:13] Enriching papers with extra data.
[26.06.2025 09:13] ********************************************************************************
[26.06.2025 09:13] Abstract 0. ShareGPT-4o-Image and Janus-4o enable open research in photorealistic, instruction-aligned image generation through a large dataset and multimodal model.  					AI-generated summary 				 Recent advances in multimodal generative models have unlocked photorealistic, instruction-aligned image generation...
[26.06.2025 09:13] ********************************************************************************
[26.06.2025 09:13] Abstract 1. Outlier-Safe Pre-Training improves large language model quantization performance by preventing extreme activation outliers through innovative training techniques.  					AI-generated summary 				 Extreme activation outliers in Large Language Models (LLMs) critically degrade quantization performance, ...
[26.06.2025 09:13] ********************************************************************************
[26.06.2025 09:13] Abstract 2. A simulator named DualTHOR for training dual-arm humanoid robots integrates real-world assets and physics to enhance the robustness and generalization of Vision Language Models.  					AI-generated summary 				 Developing embodied agents capable of performing complex interactive tasks in real-world s...
[26.06.2025 09:13] ********************************************************************************
[26.06.2025 09:13] Abstract 3. A novel framework using Property-Based Testing and collaborative LLM-based agents improves code generation correctness and generalization.  					AI-generated summary 				 Large Language Models (LLMs) excel at code generation, but ensuring their outputs to be functionally correct, especially in compl...
[26.06.2025 09:13] ********************************************************************************
[26.06.2025 09:13] Abstract 4. RoboTwin 2.0 is a scalable simulation framework for bimanual robotic manipulation that uses expert data synthesis and structured domain randomization to generate diverse and realistic synthetic data, improving sim-to-real transfer and generalization.  					AI-generated summary 				 Simulation-based ...
[26.06.2025 09:13] ********************************************************************************
[26.06.2025 09:13] Abstract 5. Optimizing tokenizers for chatbot conversations reduces computational costs and energy usage with minimal impact on training corpus performance.  					AI-generated summary 				 The computational and energy costs of Large Language Models (LLMs) have increased exponentially driven by the growing model...
[26.06.2025 09:13] ********************************************************************************
[26.06.2025 09:13] Abstract 6. The study examines and proposes new sampling and selection strategies to enhance inference-time compute for multilingual and multi-task large language models, demonstrating significant improvements in win-rates across various languages and tasks.  					AI-generated summary 				 Recent advancements i...
[26.06.2025 09:13] ********************************************************************************
[26.06.2025 09:13] Abstract 7. ReCode, a rule-based reinforcement learning framework, enhances large language models' adaptation to API updates without compromising their general code generation capabilities.  					AI-generated summary 				 Large Language Models (LLMs) exhibit remarkable code generation capabilities but falter wh...
[26.06.2025 09:13] ********************************************************************************
[26.06.2025 09:13] Abstract 8. HiWave enhances ultra-high-resolution image synthesis using pretrained diffusion models through a two-stage pipeline involving DDIM inversion and wavelet-based detail enhancement, improving visual fidelity and reducing artifacts.  					AI-generated summary 				 Diffusion models have emerged as the l...
[26.06.2025 09:13] ********************************************************************************
[26.06.2025 09:13] Abstract 9. The Debugging Decay Index (DDI) quantifies and optimizes the effectiveness of iterative AI debugging by predicting intervention points to revive and enhance debugging capability.  					AI-generated summary 				 The effectiveness of AI debugging follows a predictable exponential decay pattern; most m...
[26.06.2025 09:13] ********************************************************************************
[26.06.2025 09:13] Abstract 10. Investigating mid-training strategies reveals that high-quality mathematical corpora and well-formatted chain-of-thought reasoning examples enhance reinforcement learning performance in language models, leading to the development of OctoThinker.  					AI-generated summary 				 Different base languag...
[26.06.2025 09:13] ********************************************************************************
[26.06.2025 09:13] Abstract 11. MATE, a multimodal accessibility multi-agent system, converts data into understandable formats based on user needs, supporting various disabilities and integrating with institutional technologies.  					AI-generated summary 				 Accessibility remains a critical concern in today's society, as many te...
[26.06.2025 09:13] Read previous papers.
[26.06.2025 09:13] Generating reviews via LLM API.
[26.06.2025 09:13] Using data from previous issue: {"categories": ["#cv", "#dataset", "#open_source", "#multimodal", "#synthetic"], "emoji": "🖼️", "ru": {"title": "Демократизация фотореалистичной генерации изображений с помощью открытых данных и моделей", "desc": "Статья представляет ShareGPT-4o-Image - первый набор данных, содержащий 45 тысяч приме
[26.06.2025 09:13] Using data from previous issue: {"categories": ["#inference", "#training", "#optimization", "#open_source"], "emoji": "🚀", "ru": {"title": "Безопасное предобучение LLM для эффективного квантования", "desc": "Статья представляет новый метод предварительного обучения больших языковых моделей (LLM), называемый Outlier-Safe Pre-Traini
[26.06.2025 09:13] Using data from previous issue: {"categories": ["#transfer_learning", "#cv", "#games", "#robotics", "#agents"], "emoji": "🤖", "ru": {"title": "DualTHOR: Реалистичное обучение роботов в виртуальной среде", "desc": "DualTHOR - это симулятор для обучения двуруких гуманоидных роботов, интегрирующий реальные активы и физику для улучшен
[26.06.2025 09:13] Using data from previous issue: {"categories": ["#plp", "#benchmark", "#optimization", "#training", "#agents"], "emoji": "🧪", "ru": {"title": "Улучшение генерации кода с помощью Property-Based Testing и LLM-агентов", "desc": "Эта статья представляет новый фреймворк под названием Property-Generated Solver для улучшения генерации ко
[26.06.2025 09:13] Using data from previous issue: {"categories": ["#transfer_learning", "#synthetic", "#dataset", "#benchmark", "#optimization", "#data", "#robotics"], "emoji": "🤖", "ru": {"title": "Виртуальный близнец для обучения роботов-манипуляторов", "desc": "RoboTwin 2.0 - это масштабируемая система симуляции для бимануальной роботизированной
[26.06.2025 09:13] Using data from previous issue: {"categories": ["#training", "#optimization", "#data"], "emoji": "🤖", "ru": {"title": "Экономия энергии через оптимизацию токенизации для чат-ботов", "desc": "Исследование показывает, что оптимизация токенизаторов для чат-ботов может снизить вычислительные затраты и энергопотребление на 5-10%. Автор
[26.06.2025 09:13] Using data from previous issue: {"categories": ["#multilingual", "#inference", "#low_resource"], "emoji": "🌐", "ru": {"title": "Повышение эффективности многоязычных LLM через оптимизацию вывода", "desc": "Исследование посвящено новым стратегиям выборки и отбора для улучшения вычислений во время вывода многоязычных и многозадачных 
[26.06.2025 09:13] Querying the API.
[26.06.2025 09:13] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

ReCode, a rule-based reinforcement learning framework, enhances large language models' adaptation to API updates without compromising their general code generation capabilities.  					AI-generated summary 				 Large Language Models (LLMs) exhibit remarkable code generation capabilities but falter when adapting to frequent updates in external library APIs. This critical limitation, stemming from reliance on outdated API knowledge from their training data, even with access to current documentation, impedes reliable code generation in dynamic environments. To tackle this issue, we propose ReCode (rule-based Reinforcement learning for Code Update), a novel framework that mimics human programmer adaptation to API changes. Specifically, we construct a dataset of approximately 2,000 data entries to train the LLMs to perform version migration based on updated information. Then, we introduce a modified string similarity metric for code evaluation as the reward for reinforcement learning. Our experiments demonstrate that ReCode substantially boosts LLMs' code generation performance in dynamic API scenarios, especially on the unseen CodeUpdateArena task. Crucially, compared to supervised fine-tuning, ReCode has less impact on LLMs' general code generation abilities. We apply ReCode on various LLMs and reinforcement learning algorithms (GRPO and DAPO), all achieving consistent improvements. Notably, after training, Qwen2.5-Coder-7B outperforms that of the 32B parameter code instruction-tuned model and the reasoning model with the same architecture. Code is available at https://github.com/zjunlp/ReCode.
[26.06.2025 09:13] Response: {
  "desc": "Предложена новая система ReCode для улучшения адаптации больших языковых моделей (LLM) к обновлениям API без ущерба для общих способностей генерации кода. ReCode использует обучение с подкреплением на основе правил, имитируя адаптацию программистов к изменениям API. Система обучается на наборе данных из 2000 примеров миграции версий и использует модифицированную метрику сходства строк для оценки кода в качестве награды. Эксперименты показывают значительное улучшение производительности LLM при работе с динамическими API, особенно на новых задачах CodeUpdateArena.",
  "emoji": "🔄",
  "title": "ReCode: Адаптация языковых моделей к изменениям API без потери универсальности"
}
[26.06.2025 09:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"ReCode, a rule-based reinforcement learning framework, enhances large language models' adaptation to API updates without compromising their general code generation capabilities.  					AI-generated summary 				 Large Language Models (LLMs) exhibit remarkable code generation capabilities but falter when adapting to frequent updates in external library APIs. This critical limitation, stemming from reliance on outdated API knowledge from their training data, even with access to current documentation, impedes reliable code generation in dynamic environments. To tackle this issue, we propose ReCode (rule-based Reinforcement learning for Code Update), a novel framework that mimics human programmer adaptation to API changes. Specifically, we construct a dataset of approximately 2,000 data entries to train the LLMs to perform version migration based on updated information. Then, we introduce a modified string similarity metric for code evaluation as the reward for reinforcement learning. Our experiments demonstrate that ReCode substantially boosts LLMs' code generation performance in dynamic API scenarios, especially on the unseen CodeUpdateArena task. Crucially, compared to supervised fine-tuning, ReCode has less impact on LLMs' general code generation abilities. We apply ReCode on various LLMs and reinforcement learning algorithms (GRPO and DAPO), all achieving consistent improvements. Notably, after training, Qwen2.5-Coder-7B outperforms that of the 32B parameter code instruction-tuned model and the reasoning model with the same architecture. Code is available at https://github.com/zjunlp/ReCode."

[26.06.2025 09:13] Response: ```python
['RL', 'RLHF', 'DATASET', 'TRAINING']
```
[26.06.2025 09:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"ReCode, a rule-based reinforcement learning framework, enhances large language models' adaptation to API updates without compromising their general code generation capabilities.  					AI-generated summary 				 Large Language Models (LLMs) exhibit remarkable code generation capabilities but falter when adapting to frequent updates in external library APIs. This critical limitation, stemming from reliance on outdated API knowledge from their training data, even with access to current documentation, impedes reliable code generation in dynamic environments. To tackle this issue, we propose ReCode (rule-based Reinforcement learning for Code Update), a novel framework that mimics human programmer adaptation to API changes. Specifically, we construct a dataset of approximately 2,000 data entries to train the LLMs to perform version migration based on updated information. Then, we introduce a modified string similarity metric for code evaluation as the reward for reinforcement learning. Our experiments demonstrate that ReCode substantially boosts LLMs' code generation performance in dynamic API scenarios, especially on the unseen CodeUpdateArena task. Crucially, compared to supervised fine-tuning, ReCode has less impact on LLMs' general code generation abilities. We apply ReCode on various LLMs and reinforcement learning algorithms (GRPO and DAPO), all achieving consistent improvements. Notably, after training, Qwen2.5-Coder-7B outperforms that of the 32B parameter code instruction-tuned model and the reasoning model with the same architecture. Code is available at https://github.com/zjunlp/ReCode."

[26.06.2025 09:13] Response: ```python
["OPTIMIZATION", "REASONING"]
```
[26.06.2025 09:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"ReCode is a rule-based reinforcement learning framework designed to improve large language models\' (LLMs) ability to adapt to changes in external library APIs while maintaining their general code generation skills. The framework addresses the challenge that LLMs face due to outdated API knowledge, which hinders their performance in dynamic coding environments. By creating a dataset of around 2,000 entries for training and implementing a modified string similarity metric for evaluating code, ReCode effectively enhances the LLMs\' adaptation capabilities. Experimental results show that ReCode significantly improves code generation performance in scenarios with frequent API updates, outperforming traditional supervised fine-tuning methods.","title":"Adapting Code Generation with ReCode: Reinforcement Learning for API Updates"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="ReCode is a rule-based reinforcement learning framework designed to improve large language models' (LLMs) ability to adapt to changes in external library APIs while maintaining their general code generation skills. The framework addresses the challenge that LLMs face due to outdated API knowledge, which hinders their performance in dynamic coding environments. By creating a dataset of around 2,000 entries for training and implementing a modified string similarity metric for evaluating code, ReCode effectively enhances the LLMs' adaptation capabilities. Experimental results show that ReCode significantly improves code generation performance in scenarios with frequent API updates, outperforming traditional supervised fine-tuning methods.", title='Adapting Code Generation with ReCode: Reinforcement Learning for API Updates'))
[26.06.2025 09:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"ReCode是一个基于规则的强化学习框架，旨在提高大型语言模型（LLMs）对API更新的适应能力，同时保持其代码生成的通用性。该框架通过构建一个包含约2000个数据条目的数据集，训练LLMs进行版本迁移，以应对动态环境中的API变化。我们还引入了一种修改过的字符串相似度度量作为强化学习的奖励，以评估代码的质量。实验结果表明，ReCode显著提升了LLMs在动态API场景下的代码生成性能，尤其是在未见过的CodeUpdateArena任务上。","title":"ReCode：提升代码生成与API适应能力的创新框架"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='ReCode是一个基于规则的强化学习框架，旨在提高大型语言模型（LLMs）对API更新的适应能力，同时保持其代码生成的通用性。该框架通过构建一个包含约2000个数据条目的数据集，训练LLMs进行版本迁移，以应对动态环境中的API变化。我们还引入了一种修改过的字符串相似度度量作为强化学习的奖励，以评估代码的质量。实验结果表明，ReCode显著提升了LLMs在动态API场景下的代码生成性能，尤其是在未见过的CodeUpdateArena任务上。', title='ReCode：提升代码生成与API适应能力的创新框架'))
[26.06.2025 09:14] Using data from previous issue: {"categories": ["#dataset", "#diffusion", "#cv"], "emoji": "🖼️", "ru": {"title": "HiWave: Революция в синтезе изображений сверхвысокого разрешения", "desc": "HiWave - это метод улучшения синтеза изображений сверхвысокого разрешения с использованием предобученных диффузионных моделей. Он включает дву
[26.06.2025 09:14] Using data from previous issue: {"categories": ["#optimization", "#training", "#math"], "emoji": "🔍", "ru": {"title": "Оптимизация отладки ИИ: измерение и преодоление затухания эффективности", "desc": "Статья представляет Индекс Затухания Отладки (DDI), который количественно оценивает эффективность итеративной отладки ИИ. Авторы о
[26.06.2025 09:14] Querying the API.
[26.06.2025 09:14] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Investigating mid-training strategies reveals that high-quality mathematical corpora and well-formatted chain-of-thought reasoning examples enhance reinforcement learning performance in language models, leading to the development of OctoThinker.  					AI-generated summary 				 Different base language model families, such as Llama and Qwen, exhibit divergent behaviors during post-training with reinforcement learning (RL), especially on reasoning-intensive tasks. What makes a base language model suitable for reinforcement learning? Gaining deeper insight into this question is essential for developing RL-scalable foundation models of the next generation. In this work, we investigate how mid-training strategies shape RL dynamics, focusing on two representative model families: Qwen and Llama. Our study reveals that (1) high-quality mathematical corpora, such as MegaMath-Web-Pro, significantly improve both base model and RL performance, while existing alternatives (e.g., FineMath-4plus) fail to do so; (2) further adding QA-style data, particularly long chain-of-thought (CoT) reasoning examples, enhances RL outcomes, and instruction data further unlocks this effect; (3) while long-CoT improves reasoning depth, it can also induce verbosity of model responses and unstability of RL training, underscoring the importance of data formatting; (4) scaling mid-training consistently leads to stronger downstream RL performance. Building on these insights, we introduce a two-stage mid-training strategy, Stable-then-Decay, in which base models are first trained on 200B tokens with a constant learning rate, followed by 20B tokens across three CoT-focused branches with learning rate decay. This yields OctoThinker, a family of models demonstrating strong RL compatibility and closing the performance gap with more RL-friendly model families, i.e., Qwen. We hope our work will help shape pre-training strategies for foundation models in the RL era. To support further research, we release our open-source models along with a curated math reasoning-intensive corpus of over 70 billion tokens (i.e., MegaMath-Web-Pro-Max).
[26.06.2025 09:14] Response: {
  "desc": "Исследование стратегий промежуточного обучения показывает, что высококачественные математические корпусы и хорошо отформатированные примеры рассуждений по цепочке улучшают производительность обучения с подкреплением в языковых моделях. Различные семейства базовых языковых моделей, такие как Llama и Qwen, демонстрируют разное поведение при пост-обучении с помощью обучения с подкреплением, особенно на задачах, требующих интенсивных рассуждений. На основе этих выводов авторы представляют двухэтапную стратегию промежуточного обучения, названную Stable-then-Decay, которая привела к созданию семейства моделей OctoThinker. Эти модели демонстрируют сильную совместимость с обучением с подкреплением и сокращают разрыв в производительности с более дружественными к RL семействами моделей.",

  "emoji": "🧠",

  "title": "Улучшение языковых моделей через оптимизацию промежуточного обучения"
}
[26.06.2025 09:14] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Investigating mid-training strategies reveals that high-quality mathematical corpora and well-formatted chain-of-thought reasoning examples enhance reinforcement learning performance in language models, leading to the development of OctoThinker.  					AI-generated summary 				 Different base language model families, such as Llama and Qwen, exhibit divergent behaviors during post-training with reinforcement learning (RL), especially on reasoning-intensive tasks. What makes a base language model suitable for reinforcement learning? Gaining deeper insight into this question is essential for developing RL-scalable foundation models of the next generation. In this work, we investigate how mid-training strategies shape RL dynamics, focusing on two representative model families: Qwen and Llama. Our study reveals that (1) high-quality mathematical corpora, such as MegaMath-Web-Pro, significantly improve both base model and RL performance, while existing alternatives (e.g., FineMath-4plus) fail to do so; (2) further adding QA-style data, particularly long chain-of-thought (CoT) reasoning examples, enhances RL outcomes, and instruction data further unlocks this effect; (3) while long-CoT improves reasoning depth, it can also induce verbosity of model responses and unstability of RL training, underscoring the importance of data formatting; (4) scaling mid-training consistently leads to stronger downstream RL performance. Building on these insights, we introduce a two-stage mid-training strategy, Stable-then-Decay, in which base models are first trained on 200B tokens with a constant learning rate, followed by 20B tokens across three CoT-focused branches with learning rate decay. This yields OctoThinker, a family of models demonstrating strong RL compatibility and closing the performance gap with more RL-friendly model families, i.e., Qwen. We hope our work will help shape pre-training strategies for foundation models in the RL era. To support further research, we release our open-source models along with a curated math reasoning-intensive corpus of over 70 billion tokens (i.e., MegaMath-Web-Pro-Max)."

[26.06.2025 09:14] Response: ```python
["RL", "DATASET", "TRAINING"]
```
[26.06.2025 09:14] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Investigating mid-training strategies reveals that high-quality mathematical corpora and well-formatted chain-of-thought reasoning examples enhance reinforcement learning performance in language models, leading to the development of OctoThinker.  					AI-generated summary 				 Different base language model families, such as Llama and Qwen, exhibit divergent behaviors during post-training with reinforcement learning (RL), especially on reasoning-intensive tasks. What makes a base language model suitable for reinforcement learning? Gaining deeper insight into this question is essential for developing RL-scalable foundation models of the next generation. In this work, we investigate how mid-training strategies shape RL dynamics, focusing on two representative model families: Qwen and Llama. Our study reveals that (1) high-quality mathematical corpora, such as MegaMath-Web-Pro, significantly improve both base model and RL performance, while existing alternatives (e.g., FineMath-4plus) fail to do so; (2) further adding QA-style data, particularly long chain-of-thought (CoT) reasoning examples, enhances RL outcomes, and instruction data further unlocks this effect; (3) while long-CoT improves reasoning depth, it can also induce verbosity of model responses and unstability of RL training, underscoring the importance of data formatting; (4) scaling mid-training consistently leads to stronger downstream RL performance. Building on these insights, we introduce a two-stage mid-training strategy, Stable-then-Decay, in which base models are first trained on 200B tokens with a constant learning rate, followed by 20B tokens across three CoT-focused branches with learning rate decay. This yields OctoThinker, a family of models demonstrating strong RL compatibility and closing the performance gap with more RL-friendly model families, i.e., Qwen. We hope our work will help shape pre-training strategies for foundation models in the RL era. To support further research, we release our open-source models along with a curated math reasoning-intensive corpus of over 70 billion tokens (i.e., MegaMath-Web-Pro-Max)."

[26.06.2025 09:14] Response: ```python
['REASONING', 'OPTIMIZATION', 'OPEN_SOURCE']
```
[26.06.2025 09:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper explores how mid-training strategies can improve reinforcement learning (RL) performance in language models, specifically focusing on the Qwen and Llama families. It finds that using high-quality mathematical datasets and well-structured chain-of-thought reasoning examples significantly enhances the models\' reasoning capabilities. The authors introduce a two-stage mid-training approach called Stable-then-Decay, which optimizes learning rates to improve RL outcomes. The resulting model, OctoThinker, shows improved compatibility with RL tasks, bridging the performance gap with other models designed for RL.","title":"Enhancing RL Performance with Strategic Mid-Training"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="This paper explores how mid-training strategies can improve reinforcement learning (RL) performance in language models, specifically focusing on the Qwen and Llama families. It finds that using high-quality mathematical datasets and well-structured chain-of-thought reasoning examples significantly enhances the models' reasoning capabilities. The authors introduce a two-stage mid-training approach called Stable-then-Decay, which optimizes learning rates to improve RL outcomes. The resulting model, OctoThinker, shows improved compatibility with RL tasks, bridging the performance gap with other models designed for RL.", title='Enhancing RL Performance with Strategic Mid-Training'))
[26.06.2025 09:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本研究探讨了中期训练策略如何影响强化学习（RL）在语言模型中的表现，特别是针对推理密集型任务。我们发现高质量的数学语料库和格式良好的链式推理示例显著提升了模型的RL性能。通过引入稳定-衰减的两阶段中期训练策略，我们开发了OctoThinker模型系列，展示了与其他RL友好模型的竞争力。我们的工作旨在为基础模型的预训练策略提供指导，特别是在强化学习时代。","title":"提升语言模型的强化学习性能"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本研究探讨了中期训练策略如何影响强化学习（RL）在语言模型中的表现，特别是针对推理密集型任务。我们发现高质量的数学语料库和格式良好的链式推理示例显著提升了模型的RL性能。通过引入稳定-衰减的两阶段中期训练策略，我们开发了OctoThinker模型系列，展示了与其他RL友好模型的竞争力。我们的工作旨在为基础模型的预训练策略提供指导，特别是在强化学习时代。', title='提升语言模型的强化学习性能'))
[26.06.2025 09:14] Using data from previous issue: {"categories": ["#healthcare", "#agents", "#multimodal", "#ethics", "#open_source"], "emoji": "♿", "ru": {"title": "MATE: Интеллектуальная система для преодоления барьеров доступности", "desc": "MATE - это мультимодальная система мультиагентов для обеспечения доступности, которая преобразует данные 
[26.06.2025 09:14] Renaming data file.
[26.06.2025 09:14] Renaming previous data. hf_papers.json to ./d/2025-06-26.json
[26.06.2025 09:14] Saving new data file.
[26.06.2025 09:14] Generating page.
[26.06.2025 09:14] Renaming previous page.
[26.06.2025 09:14] Renaming previous data. index.html to ./d/2025-06-26.html
[26.06.2025 09:14] Writing result.
[26.06.2025 09:14] Renaming log file.
[26.06.2025 09:14] Renaming previous data. log.txt to ./logs/2025-06-26_last_log.txt
