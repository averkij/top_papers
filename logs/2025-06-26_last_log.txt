[26.06.2025 09:14] Read previous papers.
[26.06.2025 09:14] Generating top page (month).
[26.06.2025 09:14] Writing top page (month).
[26.06.2025 10:12] Read previous papers.
[26.06.2025 10:12] Get feed.
[26.06.2025 10:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.18095
[26.06.2025 10:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.19697
[26.06.2025 10:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.16012
[26.06.2025 10:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.18315
[26.06.2025 10:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.18088
[26.06.2025 10:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.18674
[26.06.2025 10:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.20544
[26.06.2025 10:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.20495
[26.06.2025 10:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.20452
[26.06.2025 10:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.18403
[26.06.2025 10:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.20512
[26.06.2025 10:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.19502
[26.06.2025 10:12] Extract page data from URL. URL: https://huggingface.co/papers/2506.20331
[26.06.2025 10:12] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[26.06.2025 10:12] No deleted papers detected.
[26.06.2025 10:12] Downloading and parsing papers (pdf, html). Total: 13.
[26.06.2025 10:12] Downloading and parsing paper https://huggingface.co/papers/2506.18095.
[26.06.2025 10:12] Extra JSON file exists (./assets/json/2506.18095.json), skip PDF parsing.
[26.06.2025 10:12] Paper image links file exists (./assets/img_data/2506.18095.json), skip HTML parsing.
[26.06.2025 10:12] Success.
[26.06.2025 10:12] Downloading and parsing paper https://huggingface.co/papers/2506.19697.
[26.06.2025 10:12] Extra JSON file exists (./assets/json/2506.19697.json), skip PDF parsing.
[26.06.2025 10:12] Paper image links file exists (./assets/img_data/2506.19697.json), skip HTML parsing.
[26.06.2025 10:12] Success.
[26.06.2025 10:12] Downloading and parsing paper https://huggingface.co/papers/2506.16012.
[26.06.2025 10:12] Extra JSON file exists (./assets/json/2506.16012.json), skip PDF parsing.
[26.06.2025 10:12] Paper image links file exists (./assets/img_data/2506.16012.json), skip HTML parsing.
[26.06.2025 10:12] Success.
[26.06.2025 10:12] Downloading and parsing paper https://huggingface.co/papers/2506.18315.
[26.06.2025 10:12] Extra JSON file exists (./assets/json/2506.18315.json), skip PDF parsing.
[26.06.2025 10:12] Paper image links file exists (./assets/img_data/2506.18315.json), skip HTML parsing.
[26.06.2025 10:12] Success.
[26.06.2025 10:12] Downloading and parsing paper https://huggingface.co/papers/2506.18088.
[26.06.2025 10:12] Extra JSON file exists (./assets/json/2506.18088.json), skip PDF parsing.
[26.06.2025 10:12] Paper image links file exists (./assets/img_data/2506.18088.json), skip HTML parsing.
[26.06.2025 10:12] Success.
[26.06.2025 10:12] Downloading and parsing paper https://huggingface.co/papers/2506.18674.
[26.06.2025 10:12] Extra JSON file exists (./assets/json/2506.18674.json), skip PDF parsing.
[26.06.2025 10:12] Paper image links file exists (./assets/img_data/2506.18674.json), skip HTML parsing.
[26.06.2025 10:12] Success.
[26.06.2025 10:12] Downloading and parsing paper https://huggingface.co/papers/2506.20544.
[26.06.2025 10:12] Extra JSON file exists (./assets/json/2506.20544.json), skip PDF parsing.
[26.06.2025 10:12] Paper image links file exists (./assets/img_data/2506.20544.json), skip HTML parsing.
[26.06.2025 10:12] Success.
[26.06.2025 10:12] Downloading and parsing paper https://huggingface.co/papers/2506.20495.
[26.06.2025 10:12] Extra JSON file exists (./assets/json/2506.20495.json), skip PDF parsing.
[26.06.2025 10:12] Paper image links file exists (./assets/img_data/2506.20495.json), skip HTML parsing.
[26.06.2025 10:12] Success.
[26.06.2025 10:12] Downloading and parsing paper https://huggingface.co/papers/2506.20452.
[26.06.2025 10:12] Extra JSON file exists (./assets/json/2506.20452.json), skip PDF parsing.
[26.06.2025 10:12] Paper image links file exists (./assets/img_data/2506.20452.json), skip HTML parsing.
[26.06.2025 10:12] Success.
[26.06.2025 10:12] Downloading and parsing paper https://huggingface.co/papers/2506.18403.
[26.06.2025 10:12] Extra JSON file exists (./assets/json/2506.18403.json), skip PDF parsing.
[26.06.2025 10:12] Paper image links file exists (./assets/img_data/2506.18403.json), skip HTML parsing.
[26.06.2025 10:12] Success.
[26.06.2025 10:12] Downloading and parsing paper https://huggingface.co/papers/2506.20512.
[26.06.2025 10:12] Extra JSON file exists (./assets/json/2506.20512.json), skip PDF parsing.
[26.06.2025 10:12] Paper image links file exists (./assets/img_data/2506.20512.json), skip HTML parsing.
[26.06.2025 10:12] Success.
[26.06.2025 10:12] Downloading and parsing paper https://huggingface.co/papers/2506.19502.
[26.06.2025 10:12] Extra JSON file exists (./assets/json/2506.19502.json), skip PDF parsing.
[26.06.2025 10:12] Paper image links file exists (./assets/img_data/2506.19502.json), skip HTML parsing.
[26.06.2025 10:12] Success.
[26.06.2025 10:12] Downloading and parsing paper https://huggingface.co/papers/2506.20331.
[26.06.2025 10:12] Downloading paper 2506.20331 from http://arxiv.org/pdf/2506.20331v1...
[26.06.2025 10:12] Extracting affiliations from text.
[26.06.2025 10:12] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Preprint. Under review. Biomed-Enriched: Biomedical Dataset Enriched with LLMs for Pretraining and Extracting Rare and Hidden Content 5 2 0 2 5 2 ] . [ 1 1 3 3 0 2 . 6 0 5 2 : r Rian Touchent, Nathan Godey & Eric de la Clergerie Sorbonne Université INRIA Paris 48 rue Barrault, Paris 75013, France {firstname.lastname}@inria.fr almanach/Biomed-Enriched "
[26.06.2025 10:13] Response: ```python
["Sorbonne Université INRIA Paris"]
```
[26.06.2025 10:13] Deleting PDF ./assets/pdf/2506.20331.pdf.
[26.06.2025 10:13] Success.
[26.06.2025 10:13] Enriching papers with extra data.
[26.06.2025 10:13] ********************************************************************************
[26.06.2025 10:13] Abstract 0. ShareGPT-4o-Image and Janus-4o enable open research in photorealistic, instruction-aligned image generation through a large dataset and multimodal model.  					AI-generated summary 				 Recent advances in multimodal generative models have unlocked photorealistic, instruction-aligned image generation...
[26.06.2025 10:13] ********************************************************************************
[26.06.2025 10:13] Abstract 1. Outlier-Safe Pre-Training improves large language model quantization performance by preventing extreme activation outliers through innovative training techniques.  					AI-generated summary 				 Extreme activation outliers in Large Language Models (LLMs) critically degrade quantization performance, ...
[26.06.2025 10:13] ********************************************************************************
[26.06.2025 10:13] Abstract 2. A simulator named DualTHOR for training dual-arm humanoid robots integrates real-world assets and physics to enhance the robustness and generalization of Vision Language Models.  					AI-generated summary 				 Developing embodied agents capable of performing complex interactive tasks in real-world s...
[26.06.2025 10:13] ********************************************************************************
[26.06.2025 10:13] Abstract 3. A novel framework using Property-Based Testing and collaborative LLM-based agents improves code generation correctness and generalization.  					AI-generated summary 				 Large Language Models (LLMs) excel at code generation, but ensuring their outputs to be functionally correct, especially in compl...
[26.06.2025 10:13] ********************************************************************************
[26.06.2025 10:13] Abstract 4. RoboTwin 2.0 is a scalable simulation framework for bimanual robotic manipulation that uses expert data synthesis and structured domain randomization to generate diverse and realistic synthetic data, improving sim-to-real transfer and generalization.  					AI-generated summary 				 Simulation-based ...
[26.06.2025 10:13] ********************************************************************************
[26.06.2025 10:13] Abstract 5. Optimizing tokenizers for chatbot conversations reduces computational costs and energy usage with minimal impact on training corpus performance.  					AI-generated summary 				 The computational and energy costs of Large Language Models (LLMs) have increased exponentially driven by the growing model...
[26.06.2025 10:13] ********************************************************************************
[26.06.2025 10:13] Abstract 6. The study examines and proposes new sampling and selection strategies to enhance inference-time compute for multilingual and multi-task large language models, demonstrating significant improvements in win-rates across various languages and tasks.  					AI-generated summary 				 Recent advancements i...
[26.06.2025 10:13] ********************************************************************************
[26.06.2025 10:13] Abstract 7. ReCode, a rule-based reinforcement learning framework, enhances large language models' adaptation to API updates without compromising their general code generation capabilities.  					AI-generated summary 				 Large Language Models (LLMs) exhibit remarkable code generation capabilities but falter wh...
[26.06.2025 10:13] ********************************************************************************
[26.06.2025 10:13] Abstract 8. HiWave enhances ultra-high-resolution image synthesis using pretrained diffusion models through a two-stage pipeline involving DDIM inversion and wavelet-based detail enhancement, improving visual fidelity and reducing artifacts.  					AI-generated summary 				 Diffusion models have emerged as the l...
[26.06.2025 10:13] ********************************************************************************
[26.06.2025 10:13] Abstract 9. The Debugging Decay Index (DDI) quantifies and optimizes the effectiveness of iterative AI debugging by predicting intervention points to revive and enhance debugging capability.  					AI-generated summary 				 The effectiveness of AI debugging follows a predictable exponential decay pattern; most m...
[26.06.2025 10:13] ********************************************************************************
[26.06.2025 10:13] Abstract 10. Investigating mid-training strategies reveals that high-quality mathematical corpora and well-formatted chain-of-thought reasoning examples enhance reinforcement learning performance in language models, leading to the development of OctoThinker.  					AI-generated summary 				 Different base languag...
[26.06.2025 10:13] ********************************************************************************
[26.06.2025 10:13] Abstract 11. MATE, a multimodal accessibility multi-agent system, converts data into understandable formats based on user needs, supporting various disabilities and integrating with institutional technologies.  					AI-generated summary 				 Accessibility remains a critical concern in today's society, as many te...
[26.06.2025 10:13] ********************************************************************************
[26.06.2025 10:13] Abstract 12. A biomedical text dataset, constructed from PubMed, uses a two-stage annotation process involving large and small language models to fine-tune and extract subsets for clinical NLP, improving pretraining efficiency and performance.  					AI-generated summary 				 We introduce Biomed-Enriched, a biome...
[26.06.2025 10:13] Read previous papers.
[26.06.2025 10:13] Generating reviews via LLM API.
[26.06.2025 10:13] Using data from previous issue: {"categories": ["#cv", "#dataset", "#open_source", "#multimodal", "#synthetic"], "emoji": "🖼️", "ru": {"title": "Демократизация фотореалистичной генерации изображений с помощью открытых данных и моделей", "desc": "Статья представляет ShareGPT-4o-Image - первый набор данных, содержащий 45 тысяч приме
[26.06.2025 10:13] Using data from previous issue: {"categories": ["#inference", "#training", "#optimization", "#open_source"], "emoji": "🚀", "ru": {"title": "Безопасное предобучение LLM для эффективного квантования", "desc": "Статья представляет новый метод предварительного обучения больших языковых моделей (LLM), называемый Outlier-Safe Pre-Traini
[26.06.2025 10:13] Using data from previous issue: {"categories": ["#transfer_learning", "#cv", "#games", "#robotics", "#agents"], "emoji": "🤖", "ru": {"title": "DualTHOR: Реалистичное обучение роботов в виртуальной среде", "desc": "DualTHOR - это симулятор для обучения двуруких гуманоидных роботов, интегрирующий реальные активы и физику для улучшен
[26.06.2025 10:13] Using data from previous issue: {"categories": ["#plp", "#benchmark", "#optimization", "#training", "#agents"], "emoji": "🧪", "ru": {"title": "Улучшение генерации кода с помощью Property-Based Testing и LLM-агентов", "desc": "Эта статья представляет новый фреймворк под названием Property-Generated Solver для улучшения генерации ко
[26.06.2025 10:13] Using data from previous issue: {"categories": ["#transfer_learning", "#synthetic", "#dataset", "#benchmark", "#optimization", "#data", "#robotics"], "emoji": "🤖", "ru": {"title": "Виртуальный близнец для обучения роботов-манипуляторов", "desc": "RoboTwin 2.0 - это масштабируемая система симуляции для бимануальной роботизированной
[26.06.2025 10:13] Using data from previous issue: {"categories": ["#training", "#optimization", "#data"], "emoji": "🤖", "ru": {"title": "Экономия энергии через оптимизацию токенизации для чат-ботов", "desc": "Исследование показывает, что оптимизация токенизаторов для чат-ботов может снизить вычислительные затраты и энергопотребление на 5-10%. Автор
[26.06.2025 10:13] Using data from previous issue: {"categories": ["#multilingual", "#inference", "#low_resource"], "emoji": "🌐", "ru": {"title": "Повышение эффективности многоязычных LLM через оптимизацию вывода", "desc": "Исследование посвящено новым стратегиям выборки и отбора для улучшения вычислений во время вывода многоязычных и многозадачных 
[26.06.2025 10:13] Using data from previous issue: {"categories": ["#rl", "#training", "#rlhf", "#optimization", "#dataset", "#reasoning"], "emoji": "🔄", "ru": {"title": "ReCode: Адаптация языковых моделей к изменениям API без потери универсальности", "desc": "Предложена новая система ReCode для улучшения адаптации больших языковых моделей (LLM) к о
[26.06.2025 10:13] Using data from previous issue: {"categories": ["#dataset", "#diffusion", "#cv"], "emoji": "🖼️", "ru": {"title": "HiWave: Революция в синтезе изображений сверхвысокого разрешения", "desc": "HiWave - это метод улучшения синтеза изображений сверхвысокого разрешения с использованием предобученных диффузионных моделей. Он включает дву
[26.06.2025 10:13] Using data from previous issue: {"categories": ["#optimization", "#training", "#math"], "emoji": "🔍", "ru": {"title": "Оптимизация отладки ИИ: измерение и преодоление затухания эффективности", "desc": "Статья представляет Индекс Затухания Отладки (DDI), который количественно оценивает эффективность итеративной отладки ИИ. Авторы о
[26.06.2025 10:13] Using data from previous issue: {"categories": ["#rl", "#training", "#open_source", "#optimization", "#dataset", "#reasoning"], "emoji": "🧠", "ru": {"title": "Улучшение языковых моделей через оптимизацию промежуточного обучения", "desc": "Исследование стратегий промежуточного обучения показывает, что высококачественные математичес
[26.06.2025 10:13] Using data from previous issue: {"categories": ["#healthcare", "#agents", "#multimodal", "#ethics", "#open_source"], "emoji": "♿", "ru": {"title": "MATE: Интеллектуальная система для преодоления барьеров доступности", "desc": "MATE - это мультимодальная система мультиагентов для обеспечения доступности, которая преобразует данные 
[26.06.2025 10:13] Querying the API.
[26.06.2025 10:13] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A biomedical text dataset, constructed from PubMed, uses a two-stage annotation process involving large and small language models to fine-tune and extract subsets for clinical NLP, improving pretraining efficiency and performance.  					AI-generated summary 				 We introduce Biomed-Enriched, a biomedical text dataset constructed from PubMed via a two-stage annotation process. In the first stage, a large language model annotates 400K paragraphs from PubMed scientific articles, assigning scores for their type (review, study, clinical case, other), domain (clinical, biomedical, other), and educational quality. The educational quality score (rated 1 to 5) estimates how useful a paragraph is for college-level learning. These annotations are then used to fine-tune a small language model, which propagates the labels across the full PMC-OA corpus. The resulting metadata allows us to extract refined subsets, including 2M clinical case paragraphs with over 450K high-quality ones from articles with commercial-use licenses, and to construct several variants via quality filtering and domain upsampling. Clinical text is typically difficult to access due to privacy constraints, as hospital records cannot be publicly shared. Hence, our dataset provides an alternative large-scale, openly available collection of clinical cases from PubMed, making it a valuable resource for biomedical and clinical NLP. Preliminary continual-pretraining experiments with OLMo2 suggest these curated subsets enable targeted improvements, with clinical upsampling boosting performance by ~5% on MMLU ProfMed and educational quality filtering improving MedQA and MedMCQA by ~1%. Combinations of these techniques led to faster convergence, reaching same performance with a third of training tokens, indicating potential for more efficient and effective biomedical pretraining strategies.
[26.06.2025 10:13] Response: {
  "desc": "Представлен новый биомедицинский текстовый датасет Biomed-Enriched, созданный на основе PubMed с использованием двухэтапного процесса аннотации. На первом этапе большая языковая модель (БЯМ) аннотирует 400 тысяч абзацев, присваивая им оценки по типу, домену и образовательному качеству. Затем эти аннотации используются для дообучения малой языковой модели (МЯМ), которая распространяет метки на весь корпус PMC-OA. Полученные метаданные позволяют извлекать уточненные подмножества, включая 2 миллиона абзацев с клиническими случаями, что делает датасет ценным ресурсом для биомедицинской и клинической обработки естественного языка.",
  "emoji": "🧬",
  "title": "Biomed-Enriched: Улучшение предобучения языковых моделей для биомедицины"
}
[26.06.2025 10:13] Renaming some terms.
[26.06.2025 10:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A biomedical text dataset, constructed from PubMed, uses a two-stage annotation process involving large and small language models to fine-tune and extract subsets for clinical NLP, improving pretraining efficiency and performance.  					AI-generated summary 				 We introduce Biomed-Enriched, a biomedical text dataset constructed from PubMed via a two-stage annotation process. In the first stage, a large language model annotates 400K paragraphs from PubMed scientific articles, assigning scores for their type (review, study, clinical case, other), domain (clinical, biomedical, other), and educational quality. The educational quality score (rated 1 to 5) estimates how useful a paragraph is for college-level learning. These annotations are then used to fine-tune a small language model, which propagates the labels across the full PMC-OA corpus. The resulting metadata allows us to extract refined subsets, including 2M clinical case paragraphs with over 450K high-quality ones from articles with commercial-use licenses, and to construct several variants via quality filtering and domain upsampling. Clinical text is typically difficult to access due to privacy constraints, as hospital records cannot be publicly shared. Hence, our dataset provides an alternative large-scale, openly available collection of clinical cases from PubMed, making it a valuable resource for biomedical and clinical NLP. Preliminary continual-pretraining experiments with OLMo2 suggest these curated subsets enable targeted improvements, with clinical upsampling boosting performance by ~5% on MMLU ProfMed and educational quality filtering improving MedQA and MedMCQA by ~1%. Combinations of these techniques led to faster convergence, reaching same performance with a third of training tokens, indicating potential for more efficient and effective biomedical pretraining strategies."

[26.06.2025 10:13] Response: ```python
["DATASET", "DATA", "TRAINING", "HEALTHCARE"]
```
[26.06.2025 10:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A biomedical text dataset, constructed from PubMed, uses a two-stage annotation process involving large and small language models to fine-tune and extract subsets for clinical NLP, improving pretraining efficiency and performance.  					AI-generated summary 				 We introduce Biomed-Enriched, a biomedical text dataset constructed from PubMed via a two-stage annotation process. In the first stage, a large language model annotates 400K paragraphs from PubMed scientific articles, assigning scores for their type (review, study, clinical case, other), domain (clinical, biomedical, other), and educational quality. The educational quality score (rated 1 to 5) estimates how useful a paragraph is for college-level learning. These annotations are then used to fine-tune a small language model, which propagates the labels across the full PMC-OA corpus. The resulting metadata allows us to extract refined subsets, including 2M clinical case paragraphs with over 450K high-quality ones from articles with commercial-use licenses, and to construct several variants via quality filtering and domain upsampling. Clinical text is typically difficult to access due to privacy constraints, as hospital records cannot be publicly shared. Hence, our dataset provides an alternative large-scale, openly available collection of clinical cases from PubMed, making it a valuable resource for biomedical and clinical NLP. Preliminary continual-pretraining experiments with OLMo2 suggest these curated subsets enable targeted improvements, with clinical upsampling boosting performance by ~5% on MMLU ProfMed and educational quality filtering improving MedQA and MedMCQA by ~1%. Combinations of these techniques led to faster convergence, reaching same performance with a third of training tokens, indicating potential for more efficient and effective biomedical pretraining strategies."

[26.06.2025 10:13] Response: ```python
['OPEN_SOURCE', 'SCIENCE']
```
[26.06.2025 10:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The paper presents Biomed-Enriched, a biomedical text dataset derived from PubMed using a two-stage annotation process. Initially, a large language model annotates 400,000 paragraphs, categorizing them by type, domain, and educational quality. This annotated data is then used to fine-tune a smaller language model, which helps in labeling the entire PMC-OA corpus. The resulting dataset, which includes high-quality clinical case paragraphs, enhances the efficiency of pretraining and improves performance in clinical NLP tasks.","title":"Enhancing Clinical NLP with Biomed-Enriched Dataset"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='The paper presents Biomed-Enriched, a biomedical text dataset derived from PubMed using a two-stage annotation process. Initially, a large language model annotates 400,000 paragraphs, categorizing them by type, domain, and educational quality. This annotated data is then used to fine-tune a smaller language model, which helps in labeling the entire PMC-OA corpus. The resulting dataset, which includes high-quality clinical case paragraphs, enhances the efficiency of pretraining and improves performance in clinical NLP tasks.', title='Enhancing Clinical NLP with Biomed-Enriched Dataset'))
[26.06.2025 10:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本文介绍了一个名为Biomed-Enriched的生物医学文本数据集，该数据集来源于PubMed，并采用了两阶段的注释过程。第一阶段使用大型语言模型对40万段PubMed科学文章进行注释，评估其类型、领域和教育质量。通过对小型语言模型的微调，进一步传播这些标签，从而提取出高质量的临床案例段落。该数据集为生物医学和临床自然语言处理提供了一个开放的资源，能够提高预训练的效率和性能。","title":"生物医学文本数据集的高效构建与应用"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本文介绍了一个名为Biomed-Enriched的生物医学文本数据集，该数据集来源于PubMed，并采用了两阶段的注释过程。第一阶段使用大型语言模型对40万段PubMed科学文章进行注释，评估其类型、领域和教育质量。通过对小型语言模型的微调，进一步传播这些标签，从而提取出高质量的临床案例段落。该数据集为生物医学和临床自然语言处理提供了一个开放的资源，能够提高预训练的效率和性能。', title='生物医学文本数据集的高效构建与应用'))
[26.06.2025 10:13] Renaming data file.
[26.06.2025 10:13] Renaming previous data. hf_papers.json to ./d/2025-06-26.json
[26.06.2025 10:13] Saving new data file.
[26.06.2025 10:13] Generating page.
[26.06.2025 10:13] Renaming previous page.
[26.06.2025 10:13] Renaming previous data. index.html to ./d/2025-06-26.html
[26.06.2025 10:13] Writing result.
[26.06.2025 10:13] Renaming log file.
[26.06.2025 10:13] Renaming previous data. log.txt to ./logs/2025-06-26_last_log.txt
