[26.06.2025 05:13] Read previous papers.
[26.06.2025 05:13] Generating top page (month).
[26.06.2025 05:13] Writing top page (month).
[26.06.2025 06:17] Read previous papers.
[26.06.2025 06:17] Get feed.
[26.06.2025 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2506.19697
[26.06.2025 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2506.18095
[26.06.2025 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2506.18403
[26.06.2025 06:17] Extract page data from URL. URL: https://huggingface.co/papers/2506.20544
[26.06.2025 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2506.19502
[26.06.2025 06:17] Extract page data from URL. URL: https://huggingface.co/papers/2506.18674
[26.06.2025 06:17] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[26.06.2025 06:17] No deleted papers detected.
[26.06.2025 06:17] Downloading and parsing papers (pdf, html). Total: 6.
[26.06.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2506.19697.
[26.06.2025 06:17] Extra JSON file exists (./assets/json/2506.19697.json), skip PDF parsing.
[26.06.2025 06:17] Paper image links file exists (./assets/img_data/2506.19697.json), skip HTML parsing.
[26.06.2025 06:17] Success.
[26.06.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2506.18095.
[26.06.2025 06:17] Extra JSON file exists (./assets/json/2506.18095.json), skip PDF parsing.
[26.06.2025 06:17] Paper image links file exists (./assets/img_data/2506.18095.json), skip HTML parsing.
[26.06.2025 06:17] Success.
[26.06.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2506.18403.
[26.06.2025 06:17] Extra JSON file exists (./assets/json/2506.18403.json), skip PDF parsing.
[26.06.2025 06:17] Paper image links file exists (./assets/img_data/2506.18403.json), skip HTML parsing.
[26.06.2025 06:17] Success.
[26.06.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2506.20544.
[26.06.2025 06:17] Downloading paper 2506.20544 from http://arxiv.org/pdf/2506.20544v1...
[26.06.2025 06:17] Extracting affiliations from text.
[26.06.2025 06:17] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Ammar Khairi1, Daniel Dsouza1, Ye Shen2, Julia Kreutzer1, and Sara Hooker1 1Cohere Labs, 2Cohere Corresponding authors: {ammar, juliakreutzer, sarahooker}@cohere.com Abstract Recent advancements in large language models (LLMs) have shifted focus toward scaling inferencetime computeimproving performance without retraining the model. common approach is to sample multiple outputs in parallel, and select one of these as the final output. However, work to date has focused on English and handful of domains such as math and code. In contrast, we are most interested in techniques that generalize across open-ended tasks, formally verifiable tasks, and across languages. In this work, we study how to robustly scale inference-time compute for open-ended generative tasks in multilingual, multi-task setting. Our findings show that both sampling strategybased on temperature variationand selection strategy must be adapted to account for diverse domains and varied language settings. We evaluate existing selection methods, revealing that strategies effective in English often fail to generalize across languages. We propose novel sampling and selection strategies specifically adapted for multilingual and multi-task inference scenarios, and show they yield notable gains across languages and tasks. In particular, our combined sampling and selection methods lead to an average +6.8 jump in win-rates for our 8B models on m-ArenaHard-v2.0 prompts, against proprietary models such as Gemini. At larger scale, Command-A (111B model) equipped with our methods, shows +9.0 improvement in win-rates on the same benchmark with just five samples against single-sample decoding, substantial increase at minimal cost. Our results underscore the need for languageand task-aware approaches to inference-time compute, aiming to democratize performance improvements in underrepresented languages. 5 2 0 2 J 5 2 ] . [ 1 4 4 5 0 2 . 6 0 5 2 : r a Traditionally, if you wanted higher performance from machine learning "
[26.06.2025 06:17] Response: ```python
["Cohere Labs", "Cohere"]
```
[26.06.2025 06:17] Deleting PDF ./assets/pdf/2506.20544.pdf.
[26.06.2025 06:17] Success.
[26.06.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2506.19502.
[26.06.2025 06:17] Extra JSON file exists (./assets/json/2506.19502.json), skip PDF parsing.
[26.06.2025 06:17] Paper image links file exists (./assets/img_data/2506.19502.json), skip HTML parsing.
[26.06.2025 06:17] Success.
[26.06.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2506.18674.
[26.06.2025 06:17] Downloading paper 2506.18674 from http://arxiv.org/pdf/2506.18674v1...
[26.06.2025 06:17] Extracting affiliations from text.
[26.06.2025 06:17] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 3 2 ] . [ 1 4 7 6 8 1 . 6 0 5 2 : r Is There Case for Conversation Optimized Tokenizers in Large Language Models? R. Ferrando, J. Conde. G. MartÃ­nez and P. Reviriego, ETSI de TelecomunicaciÃ³n Universidad PolitÃ©cnica de Madrid 28040 Madrid, Spain "
[26.06.2025 06:17] Response: ```python
["ETSI de TelecomunicaciÃ³n Universidad PolitÃ©cnica de Madrid 28040 Madrid, Spain"]
```
[26.06.2025 06:17] Deleting PDF ./assets/pdf/2506.18674.pdf.
[26.06.2025 06:17] Success.
[26.06.2025 06:17] Enriching papers with extra data.
[26.06.2025 06:17] ********************************************************************************
[26.06.2025 06:17] Abstract 0. Outlier-Safe Pre-Training improves large language model quantization performance by preventing extreme activation outliers through innovative training techniques.  					AI-generated summary 				 Extreme activation outliers in Large Language Models (LLMs) critically degrade quantization performance, ...
[26.06.2025 06:17] ********************************************************************************
[26.06.2025 06:17] Abstract 1. ShareGPT-4o-Image and Janus-4o enable open research in photorealistic, instruction-aligned image generation through a large dataset and multimodal model.  					AI-generated summary 				 Recent advances in multimodal generative models have unlocked photorealistic, instruction-aligned image generation...
[26.06.2025 06:17] ********************************************************************************
[26.06.2025 06:17] Abstract 2. The Debugging Decay Index (DDI) quantifies and optimizes the effectiveness of iterative AI debugging by predicting intervention points to revive and enhance debugging capability.  					AI-generated summary 				 The effectiveness of AI debugging follows a predictable exponential decay pattern; most m...
[26.06.2025 06:17] ********************************************************************************
[26.06.2025 06:17] Abstract 3. The study examines and proposes new sampling and selection strategies to enhance inference-time compute for multilingual and multi-task large language models, demonstrating significant improvements in win-rates across various languages and tasks.  					AI-generated summary 				 Recent advancements i...
[26.06.2025 06:17] ********************************************************************************
[26.06.2025 06:17] Abstract 4. MATE, a multimodal accessibility multi-agent system, converts data into understandable formats based on user needs, supporting various disabilities and integrating with institutional technologies.  					AI-generated summary 				 Accessibility remains a critical concern in today's society, as many te...
[26.06.2025 06:17] ********************************************************************************
[26.06.2025 06:17] Abstract 5. Optimizing tokenizers for chatbot conversations reduces computational costs and energy usage with minimal impact on training corpus performance.  					AI-generated summary 				 The computational and energy costs of Large Language Models (LLMs) have increased exponentially driven by the growing model...
[26.06.2025 06:17] Read previous papers.
[26.06.2025 06:17] Generating reviews via LLM API.
[26.06.2025 06:17] Using data from previous issue: {"categories": ["#inference", "#training", "#optimization", "#open_source"], "emoji": "ğŸš€", "ru": {"title": "Ğ‘ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ LLM Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM), Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ Outlier-Safe Pre-Traini
[26.06.2025 06:17] Using data from previous issue: {"categories": ["#cv", "#dataset", "#open_source", "#multimodal", "#synthetic"], "emoji": "ğŸ–¼ï¸", "ru": {"title": "Ğ”ĞµĞ¼Ğ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ„Ğ¾Ñ‚Ğ¾Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ShareGPT-4o-Image - Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ 45 Ñ‚Ñ‹ÑÑÑ‡ Ğ¿Ñ€Ğ¸Ğ¼Ğµ
[26.06.2025 06:17] Using data from previous issue: {"categories": ["#optimization", "#training", "#math"], "emoji": "ğŸ”", "ru": {"title": "ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¾Ñ‚Ğ»Ğ°Ğ´ĞºĞ¸ Ğ˜Ğ˜: Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ñ‚ÑƒÑ…Ğ°Ğ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ˜Ğ½Ğ´ĞµĞºÑ Ğ—Ğ°Ñ‚ÑƒÑ…Ğ°Ğ½Ğ¸Ñ ĞÑ‚Ğ»Ğ°Ğ´ĞºĞ¸ (DDI), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¾Ñ‚Ğ»Ğ°Ğ´ĞºĞ¸ Ğ˜Ğ˜. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾
[26.06.2025 06:17] Querying the API.
[26.06.2025 06:17] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

The study examines and proposes new sampling and selection strategies to enhance inference-time compute for multilingual and multi-task large language models, demonstrating significant improvements in win-rates across various languages and tasks.  					AI-generated summary 				 Recent advancements in large language models (LLMs) have shifted focus toward scaling inference-time compute, improving performance without retraining the model. A common approach is to sample multiple outputs in parallel, and select one of these as the final output. However, work to date has focused on English and a handful of domains such as math and code. In contrast, we are most interested in techniques that generalize across open-ended tasks, formally verifiable tasks, and across languages. In this work, we study how to robustly scale inference-time compute for open-ended generative tasks in a multilingual, multi-task setting.   Our findings show that both sampling strategy based on temperature variation and selection strategy must be adapted to account for diverse domains and varied language settings. We evaluate existing selection methods, revealing that strategies effective in English often fail to generalize across languages. We propose novel sampling and selection strategies specifically adapted for multilingual and multi-task inference scenarios, and show they yield notable gains across languages and tasks. In particular, our combined sampling and selection methods lead to an average +6.8 jump in win-rates for our 8B models on m-ArenaHard-v2.0 prompts, against proprietary models such as Gemini. At larger scale, Command-A (111B model) equipped with our methods, shows +9.0 improvement in win-rates on the same benchmark with just five samples against single-sample decoding, a substantial increase at minimal cost. Our results underscore the need for language- and task-aware approaches to inference-time compute, aiming to democratize performance improvements in underrepresented languages.
[26.06.2025 06:17] Response: {
  "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¼ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸ÑĞ¼ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ Ğ¸ Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (Ğ‘Ğ¯Ğœ). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ² Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ². ĞÑĞ¾Ğ±Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ÑƒĞ´ĞµĞ»ÑĞµÑ‚ÑÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ ÑƒÑ‡ĞµÑ‚Ğ° ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸ĞºĞ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ² Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ñ€Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ‘Ğ¯Ğœ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°.",
  "emoji": "ğŸŒ",
  "title": "ĞŸĞ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ‘Ğ¯Ğœ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°"
}
[26.06.2025 06:17] Renaming some terms.
[26.06.2025 06:17] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"The study examines and proposes new sampling and selection strategies to enhance inference-time compute for multilingual and multi-task large language models, demonstrating significant improvements in win-rates across various languages and tasks.  					AI-generated summary 				 Recent advancements in large language models (LLMs) have shifted focus toward scaling inference-time compute, improving performance without retraining the model. A common approach is to sample multiple outputs in parallel, and select one of these as the final output. However, work to date has focused on English and a handful of domains such as math and code. In contrast, we are most interested in techniques that generalize across open-ended tasks, formally verifiable tasks, and across languages. In this work, we study how to robustly scale inference-time compute for open-ended generative tasks in a multilingual, multi-task setting.   Our findings show that both sampling strategy based on temperature variation and selection strategy must be adapted to account for diverse domains and varied language settings. We evaluate existing selection methods, revealing that strategies effective in English often fail to generalize across languages. We propose novel sampling and selection strategies specifically adapted for multilingual and multi-task inference scenarios, and show they yield notable gains across languages and tasks. In particular, our combined sampling and selection methods lead to an average +6.8 jump in win-rates for our 8B models on m-ArenaHard-v2.0 prompts, against proprietary models such as Gemini. At larger scale, Command-A (111B model) equipped with our methods, shows +9.0 improvement in win-rates on the same benchmark with just five samples against single-sample decoding, a substantial increase at minimal cost. Our results underscore the need for language- and task-aware approaches to inference-time compute, aiming to democratize performance improvements in underrepresented languages."

[26.06.2025 06:17] Response: ```python
['INFERENCE', 'MULTILINGUAL']
```
[26.06.2025 06:17] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"The study examines and proposes new sampling and selection strategies to enhance inference-time compute for multilingual and multi-task large language models, demonstrating significant improvements in win-rates across various languages and tasks.  					AI-generated summary 				 Recent advancements in large language models (LLMs) have shifted focus toward scaling inference-time compute, improving performance without retraining the model. A common approach is to sample multiple outputs in parallel, and select one of these as the final output. However, work to date has focused on English and a handful of domains such as math and code. In contrast, we are most interested in techniques that generalize across open-ended tasks, formally verifiable tasks, and across languages. In this work, we study how to robustly scale inference-time compute for open-ended generative tasks in a multilingual, multi-task setting.   Our findings show that both sampling strategy based on temperature variation and selection strategy must be adapted to account for diverse domains and varied language settings. We evaluate existing selection methods, revealing that strategies effective in English often fail to generalize across languages. We propose novel sampling and selection strategies specifically adapted for multilingual and multi-task inference scenarios, and show they yield notable gains across languages and tasks. In particular, our combined sampling and selection methods lead to an average +6.8 jump in win-rates for our 8B models on m-ArenaHard-v2.0 prompts, against proprietary models such as Gemini. At larger scale, Command-A (111B model) equipped with our methods, shows +9.0 improvement in win-rates on the same benchmark with just five samples against single-sample decoding, a substantial increase at minimal cost. Our results underscore the need for language- and task-aware approaches to inference-time compute, aiming to democratize performance improvements in underrepresented languages."

[26.06.2025 06:18] Response: ```python
["LOW_RESOURCE"]
```
[26.06.2025 06:18] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper explores new methods for sampling and selecting outputs from large language models (LLMs) during inference to improve their performance across multiple languages and tasks. The authors highlight that traditional strategies often focus on English and specific domains, which limits their effectiveness in diverse settings. They propose innovative sampling techniques that adjust for temperature variations and selection methods tailored to different languages and tasks. The results demonstrate significant improvements in win-rates, showcasing the importance of adapting inference strategies to enhance multilingual and multi-task capabilities of LLMs.","title":"Enhancing Multilingual Performance in Large Language Models"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper explores new methods for sampling and selecting outputs from large language models (LLMs) during inference to improve their performance across multiple languages and tasks. The authors highlight that traditional strategies often focus on English and specific domains, which limits their effectiveness in diverse settings. They propose innovative sampling techniques that adjust for temperature variations and selection methods tailored to different languages and tasks. The results demonstrate significant improvements in win-rates, showcasing the importance of adapting inference strategies to enhance multilingual and multi-task capabilities of LLMs.', title='Enhancing Multilingual Performance in Large Language Models'))
[26.06.2025 06:18] Response: ParsedChatCompletionMessage[Article](content='{"desc":"æœ¬ç ”ç©¶æ¢è®¨å¹¶æå‡ºäº†æ–°çš„é‡‡æ ·å’Œé€‰æ‹©ç­–ç•¥ï¼Œä»¥å¢å¼ºå¤šè¯­è¨€å’Œå¤šä»»åŠ¡å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†æ—¶é—´è®¡ç®—æ•ˆç‡ã€‚æˆ‘ä»¬å‘ç°ï¼ŒåŸºäºæ¸©åº¦å˜åŒ–çš„é‡‡æ ·ç­–ç•¥å’Œé€‰æ‹©ç­–ç•¥å¿…é¡»é€‚åº”ä¸åŒé¢†åŸŸå’Œè¯­è¨€ç¯å¢ƒã€‚é€šè¿‡è¯„ä¼°ç°æœ‰çš„é€‰æ‹©æ–¹æ³•ï¼Œæˆ‘ä»¬æ­ç¤ºäº†åœ¨è‹±è¯­ä¸­æœ‰æ•ˆçš„ç­–ç•¥å¾€å¾€æ— æ³•è·¨è¯­è¨€æ¨å¹¿ã€‚æˆ‘ä»¬æå‡ºçš„åˆ›æ–°æ–¹æ³•åœ¨å¤šè¯­è¨€å’Œå¤šä»»åŠ¡æ¨ç†åœºæ™¯ä¸­è¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œç‰¹åˆ«æ˜¯åœ¨m-ArenaHard-v2.0åŸºå‡†æµ‹è¯•ä¸­ï¼Œ8Bæ¨¡å‹çš„èƒœç‡å¹³å‡æé«˜äº†6.8ä¸ªç™¾åˆ†ç‚¹ã€‚","title":"æå‡å¤šè¯­è¨€æ¨¡å‹æ¨ç†æ•ˆç‡çš„æ–°ç­–ç•¥"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='æœ¬ç ”ç©¶æ¢è®¨å¹¶æå‡ºäº†æ–°çš„é‡‡æ ·å’Œé€‰æ‹©ç­–ç•¥ï¼Œä»¥å¢å¼ºå¤šè¯­è¨€å’Œå¤šä»»åŠ¡å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†æ—¶é—´è®¡ç®—æ•ˆç‡ã€‚æˆ‘ä»¬å‘ç°ï¼ŒåŸºäºæ¸©åº¦å˜åŒ–çš„é‡‡æ ·ç­–ç•¥å’Œé€‰æ‹©ç­–ç•¥å¿…é¡»é€‚åº”ä¸åŒé¢†åŸŸå’Œè¯­è¨€ç¯å¢ƒã€‚é€šè¿‡è¯„ä¼°ç°æœ‰çš„é€‰æ‹©æ–¹æ³•ï¼Œæˆ‘ä»¬æ­ç¤ºäº†åœ¨è‹±è¯­ä¸­æœ‰æ•ˆçš„ç­–ç•¥å¾€å¾€æ— æ³•è·¨è¯­è¨€æ¨å¹¿ã€‚æˆ‘ä»¬æå‡ºçš„åˆ›æ–°æ–¹æ³•åœ¨å¤šè¯­è¨€å’Œå¤šä»»åŠ¡æ¨ç†åœºæ™¯ä¸­è¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œç‰¹åˆ«æ˜¯åœ¨m-ArenaHard-v2.0åŸºå‡†æµ‹è¯•ä¸­ï¼Œ8Bæ¨¡å‹çš„èƒœç‡å¹³å‡æé«˜äº†6.8ä¸ªç™¾åˆ†ç‚¹ã€‚', title='æå‡å¤šè¯­è¨€æ¨¡å‹æ¨ç†æ•ˆç‡çš„æ–°ç­–ç•¥'))
[26.06.2025 06:18] Using data from previous issue: {"categories": ["#healthcare", "#agents", "#multimodal", "#ethics", "#open_source"], "emoji": "â™¿", "ru": {"title": "MATE: Ğ˜Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ñ Ğ±Ğ°Ñ€ÑŒĞµÑ€Ğ¾Ğ² Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾ÑÑ‚Ğ¸", "desc": "MATE - ÑÑ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾ÑÑ‚Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ 
[26.06.2025 06:18] Querying the API.
[26.06.2025 06:18] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Optimizing tokenizers for chatbot conversations reduces computational costs and energy usage with minimal impact on training corpus performance.  					AI-generated summary 				 The computational and energy costs of Large Language Models (LLMs) have increased exponentially driven by the growing model sizes and the massive adoption of LLMs by hundreds of millions of users. The unit cost of an LLM is the computation of a token. Therefore, the tokenizer plays an important role in the efficiency of a model, and they are carefully optimized to minimize the number of tokens for the text in their training corpus. One of the most popular applications of LLMs are chatbots that interact with users. A key observation is that, for those chatbots, what is important is the performance of the tokenizer in the user text input and the chatbot responses. Those are most likely different from the text in the training corpus. So, a question that immediately arises is whether there is a potential benefit in optimizing tokenizers for chatbot conversations. In this paper, this idea is explored for different tokenizers by using a publicly available corpus of chatbot conversations to redesign their vocabularies and evaluate their performance in this domain. The results show that conversation-optimized tokenizers consistently reduce the number of tokens in chatbot dialogues, which can lead to meaningful energy savings, in the range of 5% to 10% while having minimal or even slightly positive impact on tokenization efficiency for the original training corpus.
[26.06.2025 06:18] Response: {
  "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ»Ñ Ñ‡Ğ°Ñ‚-Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑĞ½Ğ¸Ğ·Ğ¸Ñ‚ÑŒ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ¸ ÑĞ½ĞµÑ€Ğ³Ğ¾Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ğµ Ğ½Ğ° 5-10%. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿ĞµÑ€ĞµÑÑ‚Ñ€Ğ¾Ğ¸Ğ»Ğ¸ ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ĞºĞ¾Ñ€Ğ¿ÑƒÑ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ² Ñ Ñ‡Ğ°Ñ‚-Ğ±Ğ¾Ñ‚Ğ°Ğ¼Ğ¸. ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ğ»Ğ¸ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°Ñ…, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ²Ğ»Ğ¸ÑÑ Ğ½Ğ° ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰ĞµĞ³Ğ¾ ĞºĞ¾Ñ€Ğ¿ÑƒÑĞ°. Ğ­Ñ‚Ğ¾ Ğ²Ğ°Ğ¶Ğ½Ğ¾, Ñ‚Ğ°Ğº ĞºĞ°Ğº Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ - ĞºĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM).",
  "emoji": "ğŸ¤–",
  "title": "Ğ­ĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ ÑĞ½ĞµÑ€Ğ³Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ñ‡Ğ°Ñ‚-Ğ±Ğ¾Ñ‚Ğ¾Ğ²"
}
[26.06.2025 06:18] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Optimizing tokenizers for chatbot conversations reduces computational costs and energy usage with minimal impact on training corpus performance.  					AI-generated summary 				 The computational and energy costs of Large Language Models (LLMs) have increased exponentially driven by the growing model sizes and the massive adoption of LLMs by hundreds of millions of users. The unit cost of an LLM is the computation of a token. Therefore, the tokenizer plays an important role in the efficiency of a model, and they are carefully optimized to minimize the number of tokens for the text in their training corpus. One of the most popular applications of LLMs are chatbots that interact with users. A key observation is that, for those chatbots, what is important is the performance of the tokenizer in the user text input and the chatbot responses. Those are most likely different from the text in the training corpus. So, a question that immediately arises is whether there is a potential benefit in optimizing tokenizers for chatbot conversations. In this paper, this idea is explored for different tokenizers by using a publicly available corpus of chatbot conversations to redesign their vocabularies and evaluate their performance in this domain. The results show that conversation-optimized tokenizers consistently reduce the number of tokens in chatbot dialogues, which can lead to meaningful energy savings, in the range of 5% to 10% while having minimal or even slightly positive impact on tokenization efficiency for the original training corpus."

[26.06.2025 06:18] Response: ```python
['DATA', 'TRAINING']
```
[26.06.2025 06:18] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Optimizing tokenizers for chatbot conversations reduces computational costs and energy usage with minimal impact on training corpus performance.  					AI-generated summary 				 The computational and energy costs of Large Language Models (LLMs) have increased exponentially driven by the growing model sizes and the massive adoption of LLMs by hundreds of millions of users. The unit cost of an LLM is the computation of a token. Therefore, the tokenizer plays an important role in the efficiency of a model, and they are carefully optimized to minimize the number of tokens for the text in their training corpus. One of the most popular applications of LLMs are chatbots that interact with users. A key observation is that, for those chatbots, what is important is the performance of the tokenizer in the user text input and the chatbot responses. Those are most likely different from the text in the training corpus. So, a question that immediately arises is whether there is a potential benefit in optimizing tokenizers for chatbot conversations. In this paper, this idea is explored for different tokenizers by using a publicly available corpus of chatbot conversations to redesign their vocabularies and evaluate their performance in this domain. The results show that conversation-optimized tokenizers consistently reduce the number of tokens in chatbot dialogues, which can lead to meaningful energy savings, in the range of 5% to 10% while having minimal or even slightly positive impact on tokenization efficiency for the original training corpus."

[26.06.2025 06:18] Response: ```python
["OPTIMIZATION"]
```
[26.06.2025 06:18] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper explores the optimization of tokenizers specifically for chatbot conversations to enhance efficiency and reduce costs. It highlights that the computational expense of Large Language Models (LLMs) is closely tied to the number of tokens processed, making tokenizer performance crucial. By redesigning vocabularies based on a corpus of chatbot dialogues, the study demonstrates that conversation-optimized tokenizers can decrease token counts by 5% to 10%. Importantly, this optimization has little to no negative effect on the performance of the original training corpus, suggesting a dual benefit of energy savings and maintained efficiency.","title":"Optimize Tokenizers, Save Energy!"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper explores the optimization of tokenizers specifically for chatbot conversations to enhance efficiency and reduce costs. It highlights that the computational expense of Large Language Models (LLMs) is closely tied to the number of tokens processed, making tokenizer performance crucial. By redesigning vocabularies based on a corpus of chatbot dialogues, the study demonstrates that conversation-optimized tokenizers can decrease token counts by 5% to 10%. Importantly, this optimization has little to no negative effect on the performance of the original training corpus, suggesting a dual benefit of energy savings and maintained efficiency.', title='Optimize Tokenizers, Save Energy!'))
[26.06.2025 06:18] Response: ParsedChatCompletionMessage[Article](content='{"desc":"æœ¬è®ºæ–‡æ¢è®¨äº†ä¸ºèŠå¤©æœºå™¨äººå¯¹è¯ä¼˜åŒ–åˆ†è¯å™¨çš„æ½œåœ¨å¥½å¤„ã€‚éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¹¿æ³›åº”ç”¨ï¼Œè®¡ç®—å’Œèƒ½æºæˆæœ¬æ€¥å‰§ä¸Šå‡ï¼Œè€Œåˆ†è¯å™¨åœ¨æ¨¡å‹æ•ˆç‡ä¸­æ‰®æ¼”ç€é‡è¦è§’è‰²ã€‚ç ”ç©¶è¡¨æ˜ï¼Œé’ˆå¯¹èŠå¤©å¯¹è¯ä¼˜åŒ–çš„åˆ†è¯å™¨èƒ½å¤Ÿæ˜¾è‘—å‡å°‘å¯¹è¯ä¸­çš„æ ‡è®°æ•°é‡ï¼Œä»è€ŒèŠ‚çœ5%åˆ°10%çš„èƒ½æºæ¶ˆè€—ï¼ŒåŒæ—¶å¯¹åŸå§‹è®­ç»ƒè¯­æ–™çš„æ ‡è®°åŒ–æ•ˆç‡å½±å“è¾ƒå°æˆ–ç•¥æœ‰æ­£é¢æ•ˆæœã€‚é€šè¿‡é‡æ–°è®¾è®¡åˆ†è¯å™¨çš„è¯æ±‡ï¼Œæœ¬æ–‡å±•ç¤ºäº†åœ¨èŠå¤©æœºå™¨äººé¢†åŸŸä¼˜åŒ–åˆ†è¯å™¨çš„æœ‰æ•ˆæ€§ã€‚","title":"ä¼˜åŒ–åˆ†è¯å™¨ï¼ŒèŠ‚çœèƒ½æºä¸æˆæœ¬"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='æœ¬è®ºæ–‡æ¢è®¨äº†ä¸ºèŠå¤©æœºå™¨äººå¯¹è¯ä¼˜åŒ–åˆ†è¯å™¨çš„æ½œåœ¨å¥½å¤„ã€‚éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¹¿æ³›åº”ç”¨ï¼Œè®¡ç®—å’Œèƒ½æºæˆæœ¬æ€¥å‰§ä¸Šå‡ï¼Œè€Œåˆ†è¯å™¨åœ¨æ¨¡å‹æ•ˆç‡ä¸­æ‰®æ¼”ç€é‡è¦è§’è‰²ã€‚ç ”ç©¶è¡¨æ˜ï¼Œé’ˆå¯¹èŠå¤©å¯¹è¯ä¼˜åŒ–çš„åˆ†è¯å™¨èƒ½å¤Ÿæ˜¾è‘—å‡å°‘å¯¹è¯ä¸­çš„æ ‡è®°æ•°é‡ï¼Œä»è€ŒèŠ‚çœ5%åˆ°10%çš„èƒ½æºæ¶ˆè€—ï¼ŒåŒæ—¶å¯¹åŸå§‹è®­ç»ƒè¯­æ–™çš„æ ‡è®°åŒ–æ•ˆç‡å½±å“è¾ƒå°æˆ–ç•¥æœ‰æ­£é¢æ•ˆæœã€‚é€šè¿‡é‡æ–°è®¾è®¡åˆ†è¯å™¨çš„è¯æ±‡ï¼Œæœ¬æ–‡å±•ç¤ºäº†åœ¨èŠå¤©æœºå™¨äººé¢†åŸŸä¼˜åŒ–åˆ†è¯å™¨çš„æœ‰æ•ˆæ€§ã€‚', title='ä¼˜åŒ–åˆ†è¯å™¨ï¼ŒèŠ‚çœèƒ½æºä¸æˆæœ¬'))
[26.06.2025 06:18] Renaming data file.
[26.06.2025 06:18] Renaming previous data. hf_papers.json to ./d/2025-06-26.json
[26.06.2025 06:18] Saving new data file.
[26.06.2025 06:18] Generating page.
[26.06.2025 06:18] Renaming previous page.
[26.06.2025 06:18] Renaming previous data. index.html to ./d/2025-06-26.html
[26.06.2025 06:18] Writing result.
[26.06.2025 06:18] Renaming log file.
[26.06.2025 06:18] Renaming previous data. log.txt to ./logs/2025-06-26_last_log.txt
