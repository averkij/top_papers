[04.06.2025 04:20] Read previous papers.
[04.06.2025 04:20] Generating top page (month).
[04.06.2025 04:20] Writing top page (month).
[04.06.2025 05:12] Read previous papers.
[04.06.2025 05:12] Get feed.
[04.06.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.03147
[04.06.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.02387
[04.06.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.01674
[04.06.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.02096
[04.06.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.03136
[04.06.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.03131
[04.06.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.24714
[04.06.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.23061
[04.06.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.03143
[04.06.2025 05:12] Extract page data from URL. URL: https://huggingface.co/papers/2506.00910
[04.06.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.02497
[04.06.2025 05:12] Extract page data from URL. URL: https://huggingface.co/papers/2506.01144
[04.06.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.02528
[04.06.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.01789
[04.06.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.02338
[04.06.2025 05:12] Extract page data from URL. URL: https://huggingface.co/papers/2506.00413
[04.06.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.02510
[04.06.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.02454
[04.06.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.16994
[04.06.2025 05:12] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[04.06.2025 05:12] No deleted papers detected.
[04.06.2025 05:12] Downloading and parsing papers (pdf, html). Total: 19.
[04.06.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2506.03147.
[04.06.2025 05:12] Extra JSON file exists (./assets/json/2506.03147.json), skip PDF parsing.
[04.06.2025 05:12] Paper image links file exists (./assets/img_data/2506.03147.json), skip HTML parsing.
[04.06.2025 05:12] Success.
[04.06.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2506.02387.
[04.06.2025 05:12] Extra JSON file exists (./assets/json/2506.02387.json), skip PDF parsing.
[04.06.2025 05:12] Paper image links file exists (./assets/img_data/2506.02387.json), skip HTML parsing.
[04.06.2025 05:12] Success.
[04.06.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2506.01674.
[04.06.2025 05:12] Extra JSON file exists (./assets/json/2506.01674.json), skip PDF parsing.
[04.06.2025 05:12] Paper image links file exists (./assets/img_data/2506.01674.json), skip HTML parsing.
[04.06.2025 05:12] Success.
[04.06.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2506.02096.
[04.06.2025 05:12] Extra JSON file exists (./assets/json/2506.02096.json), skip PDF parsing.
[04.06.2025 05:12] Paper image links file exists (./assets/img_data/2506.02096.json), skip HTML parsing.
[04.06.2025 05:12] Success.
[04.06.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2506.03136.
[04.06.2025 05:12] Extra JSON file exists (./assets/json/2506.03136.json), skip PDF parsing.
[04.06.2025 05:12] Paper image links file exists (./assets/img_data/2506.03136.json), skip HTML parsing.
[04.06.2025 05:12] Success.
[04.06.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2506.03131.
[04.06.2025 05:12] Extra JSON file exists (./assets/json/2506.03131.json), skip PDF parsing.
[04.06.2025 05:12] Paper image links file exists (./assets/img_data/2506.03131.json), skip HTML parsing.
[04.06.2025 05:12] Success.
[04.06.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2505.24714.
[04.06.2025 05:12] Extra JSON file exists (./assets/json/2505.24714.json), skip PDF parsing.
[04.06.2025 05:12] Paper image links file exists (./assets/img_data/2505.24714.json), skip HTML parsing.
[04.06.2025 05:12] Success.
[04.06.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2505.23061.
[04.06.2025 05:12] Extra JSON file exists (./assets/json/2505.23061.json), skip PDF parsing.
[04.06.2025 05:12] Paper image links file exists (./assets/img_data/2505.23061.json), skip HTML parsing.
[04.06.2025 05:12] Success.
[04.06.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2506.03143.
[04.06.2025 05:12] Extra JSON file exists (./assets/json/2506.03143.json), skip PDF parsing.
[04.06.2025 05:12] Paper image links file exists (./assets/img_data/2506.03143.json), skip HTML parsing.
[04.06.2025 05:12] Success.
[04.06.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2506.00910.
[04.06.2025 05:12] Downloading paper 2506.00910 from http://arxiv.org/pdf/2506.00910v1...
[04.06.2025 05:16] Extracting affiliations from text.
[04.06.2025 05:16] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 1 ] . [ 1 0 1 9 0 0 . 6 0 5 2 : r PCoreSet: Effective Active Learning through Knowledge Distillation from Vision-Language Models Seongjae Kang, Dong Bok Lee, Hyungjoon Jang Dongseop Kim VUNO Inc. Sung Ju Hwang, KAIST DeepAuto.ai {seongjae.kang,hyungjoon.jang,dongseop.kim}@vuno.co {markhi, sjhwang}@kaist.ac.kr Equal contribution "
[04.06.2025 05:16] Response: ```python
["VUNO Inc.", "KAIST", "DeepAuto.ai"]
```
[04.06.2025 05:16] Deleting PDF ./assets/pdf/2506.00910.pdf.
[04.06.2025 05:16] Success.
[04.06.2025 05:16] Downloading and parsing paper https://huggingface.co/papers/2506.02497.
[04.06.2025 05:16] Extra JSON file exists (./assets/json/2506.02497.json), skip PDF parsing.
[04.06.2025 05:16] Paper image links file exists (./assets/img_data/2506.02497.json), skip HTML parsing.
[04.06.2025 05:16] Success.
[04.06.2025 05:16] Downloading and parsing paper https://huggingface.co/papers/2506.01144.
[04.06.2025 05:16] Downloading paper 2506.01144 from http://arxiv.org/pdf/2506.01144v1...
[04.06.2025 05:16] Extracting affiliations from text.
[04.06.2025 05:16] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 1 ] . [ 1 4 4 1 1 0 . 6 0 5 2 : r FlowMo: Variance-Based Flow Guidance for Coherent Motion in Video Generation Ariel Shaulov Itay Hazan Lior Wolf Hila Chefer School of Computer Science Tel Aviv University, Israel Figure 1: Text-to-video results before and after applying FlowMo on (a) Wan2.1 [1] and CogVideoX-5B [2]. We present FlowMo, an inference-time guidance method to enhance temporal coherence in text-to-video models. Our method mitigates severe temporal artifacts, such as additional limbs (woman, 1st row, 2nd row), objects that appear or disappear (flamingo, 2nd row), and object distortions (woman, dolphin, 1st row), without requiring additional training or conditioning signals. "
[04.06.2025 05:16] Response: ```python
["School of Computer Science Tel Aviv University, Israel"]
```
[04.06.2025 05:16] Deleting PDF ./assets/pdf/2506.01144.pdf.
[04.06.2025 05:16] Success.
[04.06.2025 05:16] Downloading and parsing paper https://huggingface.co/papers/2506.02528.
[04.06.2025 05:16] Extra JSON file exists (./assets/json/2506.02528.json), skip PDF parsing.
[04.06.2025 05:16] Paper image links file exists (./assets/img_data/2506.02528.json), skip HTML parsing.
[04.06.2025 05:16] Success.
[04.06.2025 05:16] Downloading and parsing paper https://huggingface.co/papers/2506.01789.
[04.06.2025 05:16] Extra JSON file exists (./assets/json/2506.01789.json), skip PDF parsing.
[04.06.2025 05:16] Paper image links file exists (./assets/img_data/2506.01789.json), skip HTML parsing.
[04.06.2025 05:16] Success.
[04.06.2025 05:16] Downloading and parsing paper https://huggingface.co/papers/2506.02338.
[04.06.2025 05:16] Extra JSON file exists (./assets/json/2506.02338.json), skip PDF parsing.
[04.06.2025 05:16] Paper image links file exists (./assets/img_data/2506.02338.json), skip HTML parsing.
[04.06.2025 05:16] Success.
[04.06.2025 05:16] Downloading and parsing paper https://huggingface.co/papers/2506.00413.
[04.06.2025 05:16] Downloading paper 2506.00413 from http://arxiv.org/pdf/2506.00413v1...
[04.06.2025 05:16] Extracting affiliations from text.
[04.06.2025 05:16] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 1 3 ] . [ 1 3 1 4 0 0 . 6 0 5 2 : r a Daniel Israel Department of Computer Science University of California, Los Angeles disrael@cs.ucla.edu Guy Van den Broeck Department of Computer Science University of California, Los Angeles guyvdb@cs.ucla.edu Aditya Grover Department of Computer Science University of California, Los Angeles adityag@cs.ucla.edu "
[04.06.2025 05:16] Response: ```python
["Department of Computer Science University of California, Los Angeles"]
```
[04.06.2025 05:16] Deleting PDF ./assets/pdf/2506.00413.pdf.
[04.06.2025 05:16] Success.
[04.06.2025 05:16] Downloading and parsing paper https://huggingface.co/papers/2506.02510.
[04.06.2025 05:16] Extra JSON file exists (./assets/json/2506.02510.json), skip PDF parsing.
[04.06.2025 05:16] Paper image links file exists (./assets/img_data/2506.02510.json), skip HTML parsing.
[04.06.2025 05:16] Success.
[04.06.2025 05:16] Downloading and parsing paper https://huggingface.co/papers/2506.02454.
[04.06.2025 05:16] Extra JSON file exists (./assets/json/2506.02454.json), skip PDF parsing.
[04.06.2025 05:16] Paper image links file exists (./assets/img_data/2506.02454.json), skip HTML parsing.
[04.06.2025 05:16] Success.
[04.06.2025 05:16] Downloading and parsing paper https://huggingface.co/papers/2505.16994.
[04.06.2025 05:16] Extra JSON file exists (./assets/json/2505.16994.json), skip PDF parsing.
[04.06.2025 05:16] Paper image links file exists (./assets/img_data/2505.16994.json), skip HTML parsing.
[04.06.2025 05:16] Success.
[04.06.2025 05:16] Enriching papers with extra data.
[04.06.2025 05:16] ********************************************************************************
[04.06.2025 05:16] Abstract 0. A unified generative framework called UniWorld uses semantic features from visual-language models for image perception and manipulation, outperforming BAGEL with reduced data.  					AI-generated summary 				 Although existing unified models deliver strong performance on vision-language understanding...
[04.06.2025 05:16] ********************************************************************************
[04.06.2025 05:16] Abstract 1. VS-Bench is a multimodal benchmark designed to evaluate Vision Language Models' strategic reasoning and decision-making in complex multi-agent environments.  					AI-generated summary 				 Recent advancements in Vision Language Models (VLMs) have expanded their capabilities to interactive agent task...
[04.06.2025 05:16] ********************************************************************************
[04.06.2025 05:16] Abstract 2. MotionSight, a zero-shot method using object-centric visual spotlight and motion blur as prompts, enhances fine-grained video motion understanding and achieves state-of-the-art performance on MotionVid-QA, a large-scale dataset with hierarchical annotations.  					AI-generated summary 				 Despite a...
[04.06.2025 05:16] ********************************************************************************
[04.06.2025 05:16] Abstract 3. SynthRL, a scalable pipeline for data synthesis in reinforcement learning with verifiable rewards, enhances visual math reasoning VLMs by generating challenging, verifiable questions.  					AI-generated summary 				 Vision-language models (VLMs) trained via reinforcement learning with verifiable rew...
[04.06.2025 05:16] ********************************************************************************
[04.06.2025 05:16] Abstract 4. CURE, a reinforcement learning framework, improves code and unit test generation accuracy without ground-truth supervision, enhancing performance in various coding and testing tasks.  					AI-generated summary 				 We propose CURE, a novel reinforcement learning framework with a dedicated reward des...
[04.06.2025 05:16] ********************************************************************************
[04.06.2025 05:16] Abstract 5. A novel generative model, Native-resolution diffusion Transformer (NiT), synthesizes high-resolution and varied aspect ratio images with state-of-the-art performance and zero-shot generalization capabilities.  					AI-generated summary 				 We introduce native-resolution image synthesis, a novel gen...
[04.06.2025 05:16] ********************************************************************************
[04.06.2025 05:16] Abstract 6. FinMME is a comprehensive multimodal dataset for financial research and FinScore is an evaluation system that highlights the challenges faced by even advanced models like GPT-4o in the finance domain.  					AI-generated summary 				 Multimodal Large Language Models (MLLMs) have experienced rapid dev...
[04.06.2025 05:16] ********************************************************************************
[04.06.2025 05:16] Abstract 7. DINGO, a dynamic programming-based decoding strategy, enhances diffusion language models by enforcing structured output constraints, significantly improving performance on symbolic math and JSON generation tasks.  					AI-generated summary 				 Diffusion LLMs have emerged as a promising alternative ...
[04.06.2025 05:16] ********************************************************************************
[04.06.2025 05:16] Abstract 8. GUI-Actor, a VLM-based method using attention for coordinate-free GUI grounding, outperforms existing methods with better generalization and efficient fine-tuning.  					AI-generated summary 				 One of the principal challenges in building VLM-powered GUI agents is visual grounding, i.e., localizing...
[04.06.2025 05:16] ********************************************************************************
[04.06.2025 05:16] Abstract 9. ActiveKD integrates active learning with knowledge distillation using large vision-language models to efficiently select diverse, unlabeled samples for annotation.  					AI-generated summary 				 Knowledge distillation (KD) is a widely used framework for training compact, task-specific models by lev...
[04.06.2025 05:16] ********************************************************************************
[04.06.2025 05:16] Abstract 10. LumosFlow uses LMTV-DM for key frame generation and LOF-DM followed by MotionControlNet for smooth intermediate frame interpolation, ensuring temporally coherent long video generation.  					AI-generated summary 				 Long video generation has gained increasing attention due to its widespread applica...
[04.06.2025 05:16] ********************************************************************************
[04.06.2025 05:16] Abstract 11. FlowMo, a training-free method, enhances motion coherence in pre-trained text-to-video diffusion models by leveraging their own predictions to reduce patch-wise temporal variance.  					AI-generated summary 				 Text-to-video diffusion models are notoriously limited in their ability to model tempora...
[04.06.2025 05:16] ********************************************************************************
[04.06.2025 05:16] Abstract 12. RelationAdapter, a lightweight module, enhances Diffusion Transformer models to capture and apply visual transformations effectively using source-target image pairs, improving editing performance and generalization on diverse tasks.  					AI-generated summary 				 Inspired by the in-context learning...
[04.06.2025 05:16] ********************************************************************************
[04.06.2025 05:16] Abstract 13. High-quality datasets are fundamental to training and evaluating machine learning models, yet their creation-especially with accurate human annotations-remains a significant challenge. Many dataset paper submissions lack originality, diversity, or rigorous quality control, and these shortcomings are...
[04.06.2025 05:16] ********************************************************************************
[04.06.2025 05:16] Abstract 14. The Long CoT Collection dataset, generated by short CoT LLMs, enhances general reasoning skills and provides a strong foundation for reinforcement learning, achieving quality comparable to R1.  					AI-generated summary 				 With the release of R1, a publicly available large reasoning model (LRM), r...
[04.06.2025 05:16] ********************************************************************************
[04.06.2025 05:16] Abstract 15. Adaptive parallel decoding (APD) enhances the throughput of diffusion large language models (dLLMs) by dynamically adjusting parallel token generation without significantly diminishing quality.  					AI-generated summary 				 The generation speed of LLMs are bottlenecked by autoregressive decoding, ...
[04.06.2025 05:16] ********************************************************************************
[04.06.2025 05:16] Abstract 16. A new multilingual, multi-sector, and multi-task benchmark, M³FinMeeting, evaluates large language models' performance in understanding financial meetings across different languages and industries.  					AI-generated summary 				 Recent breakthroughs in large language models (LLMs) have led to the d...
[04.06.2025 05:16] ********************************************************************************
[04.06.2025 05:16] Abstract 17. A new framework, Multimodal DeepResearcher, enables Large Language Models to generate high-quality multimodal reports combining text and diverse visualizations through structured textual representations.  					AI-generated summary 				 Visualizations play a crucial part in effective communication of...
[04.06.2025 05:16] ********************************************************************************
[04.06.2025 05:16] Abstract 18. A unified large recommender model with intrinsic reasoning capabilities is proposed, facilitating interleaved reasoning and recommendation using a reinforcement learning framework called RecPO.  					AI-generated summary 				 Large recommender models have extended LLMs as powerful recommenders via e...
[04.06.2025 05:16] Read previous papers.
[04.06.2025 05:16] Generating reviews via LLM API.
[04.06.2025 05:16] Using data from previous issue: {"categories": ["#dataset", "#cv", "#multimodal", "#open_source", "#benchmark"], "emoji": "🎨", "ru": {"title": "UniWorld: Мощная модель для работы с изображениями на основе семантических признаков", "desc": "UniWorld - это унифицированная генеративная модель для восприятия и редактирования изображен
[04.06.2025 05:16] Using data from previous issue: {"categories": ["#reasoning", "#multimodal", "#agents", "#benchmark", "#games"], "emoji": "🤖", "ru": {"title": "VS-Bench: Новый рубеж в оценке стратегического мышления мультимодальных ИИ-агентов", "desc": "VS-Bench - это мультимодальный бенчмарк для оценки стратегического мышления и принятия решений
[04.06.2025 05:16] Using data from previous issue: {"categories": ["#dataset", "#cv", "#multimodal", "#open_source", "#games"], "emoji": "🎥", "ru": {"title": "Новый взгляд на движение: MotionSight улучшает понимание видео без обучения", "desc": "MotionSight - это новый метод без предварительного обучения для улучшения понимания движения в видео муль
[04.06.2025 05:16] Using data from previous issue: {"categories": ["#reasoning", "#dataset", "#training", "#synthetic", "#rl"], "emoji": "🧠", "ru": {"title": "SynthRL: Синтез данных для улучшения математических рассуждений ИИ", "desc": "SynthRL - это масштабируемый конвейер для синтеза данных в обучении с подкреплением с проверяемыми наградами. Он у
[04.06.2025 05:16] Using data from previous issue: {"categories": ["#training", "#optimization", "#agents", "#rl", "#games"], "emoji": "🧠", "ru": {"title": "CURE: эволюция кодирования и тестирования без эталонов", "desc": "CURE - это новая система обучения с подкреплением для улучшения генерации кода и модульных тестов без использования эталонного к
[04.06.2025 05:16] Using data from previous issue: {"categories": ["#architecture", "#diffusion", "#cv", "#synthetic", "#benchmark"], "emoji": "🖼️", "ru": {"title": "NiT: Революция в генерации изображений без ограничений разрешения", "desc": "Представлена новая генеративная модель NiT (Native-resolution diffusion Transformer), способная синтезироват
[04.06.2025 05:16] Using data from previous issue: {"categories": ["#dataset", "#hallucinations", "#science", "#multimodal", "#benchmark"], "emoji": "📊", "ru": {"title": "FinMME и FinScore: Новый стандарт для оценки мультимодальных языковых моделей в финансовой сфере", "desc": "FinMME - это обширный мультимодальный датасет для финансовых исследовани
[04.06.2025 05:16] Using data from previous issue: {"categories": ["#training", "#architecture", "#benchmark", "#optimization", "#diffusion"], "emoji": "🧮", "ru": {"title": "DINGO: Структурированное декодирование для диффузионных языковых моделей", "desc": "Статья представляет DINGO - новую стратегию декодирования для диффузионных языковых моделей. 
[04.06.2025 05:16] Using data from previous issue: {"categories": ["#multimodal", "#training", "#cv", "#benchmark", "#optimization", "#games"], "emoji": "🖱️", "ru": {"title": "GUI-Actor: Революция в локализации элементов интерфейса с помощью VLM", "desc": "GUI-Actor - это метод на основе VLM, использующий механизм внимания для безкоординатной локали
[04.06.2025 05:16] Querying the API.
[04.06.2025 05:16] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

ActiveKD integrates active learning with knowledge distillation using large vision-language models to efficiently select diverse, unlabeled samples for annotation.  					AI-generated summary 				 Knowledge distillation (KD) is a widely used framework for training compact, task-specific models by leveraging the knowledge of teacher models. However, its application to active learning (AL), which aims to minimize annotation costs through iterative sample selection, remains underexplored. This gap stems from the fact that KD typically assumes access to sufficient labeled data, whereas AL operates in data-scarce scenarios where task-specific teacher models are often unavailable. In this paper, we introduce ActiveKD, a framework that integrates AL with KD by leveraging the zero- and few-shot capabilities of large vision-language models (VLMs). A key aspect of ActiveKD is the structured prediction bias of VLMs -- i.e., their predictions form clusters in the probability space. We regard this structure as an inductive bias of the teacher model, capturing generalizable output patterns beneficial to student learning. To exploit this bias, we propose Probabilistic CoreSet (PCoreSet), a selection strategy that maximizes coverage in the probability space rather than the feature space. PCoreSet strategically selects categorically diverse unlabeled samples, facilitating more efficient transfer of teacher knowledge under limited annotation budgets. Evaluations on 11 datasets show that PCoreSet consistently outperforms existing selection methods within the ActiveKD framework, advancing research at the intersection of AL and KD.
[04.06.2025 05:16] Response: {
  "desc": "ActiveKD - это новый подход, объединяющий активное обучение и дистилляцию знаний с использованием крупных визуально-языковых моделей для эффективного выбора разнообразных неразмеченных образцов для аннотации. Ключевым аспектом ActiveKD является структурированное смещение предсказаний визуально-языковых моделей, которое рассматривается как индуктивное смещение учительской модели. Для использования этого смещения предлагается стратегия Probabilistic CoreSet (PCoreSet), максимизирующая покрытие в пространстве вероятностей. Оценки на 11 наборах данных показывают, что PCoreSet стабильно превосходит существующие методы выбора в рамках ActiveKD.",
  "emoji": "🧠",
  "title": "ActiveKD: Эффективное обучение с ограниченными данными через синергию активного обучения и дистилляции знаний"
}
[04.06.2025 05:16] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"ActiveKD integrates active learning with knowledge distillation using large vision-language models to efficiently select diverse, unlabeled samples for annotation.  					AI-generated summary 				 Knowledge distillation (KD) is a widely used framework for training compact, task-specific models by leveraging the knowledge of teacher models. However, its application to active learning (AL), which aims to minimize annotation costs through iterative sample selection, remains underexplored. This gap stems from the fact that KD typically assumes access to sufficient labeled data, whereas AL operates in data-scarce scenarios where task-specific teacher models are often unavailable. In this paper, we introduce ActiveKD, a framework that integrates AL with KD by leveraging the zero- and few-shot capabilities of large vision-language models (VLMs). A key aspect of ActiveKD is the structured prediction bias of VLMs -- i.e., their predictions form clusters in the probability space. We regard this structure as an inductive bias of the teacher model, capturing generalizable output patterns beneficial to student learning. To exploit this bias, we propose Probabilistic CoreSet (PCoreSet), a selection strategy that maximizes coverage in the probability space rather than the feature space. PCoreSet strategically selects categorically diverse unlabeled samples, facilitating more efficient transfer of teacher knowledge under limited annotation budgets. Evaluations on 11 datasets show that PCoreSet consistently outperforms existing selection methods within the ActiveKD framework, advancing research at the intersection of AL and KD."

[04.06.2025 05:16] Response: ```python
["DATASET", "DATA", "TRAINING"]
```
[04.06.2025 05:16] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"ActiveKD integrates active learning with knowledge distillation using large vision-language models to efficiently select diverse, unlabeled samples for annotation.  					AI-generated summary 				 Knowledge distillation (KD) is a widely used framework for training compact, task-specific models by leveraging the knowledge of teacher models. However, its application to active learning (AL), which aims to minimize annotation costs through iterative sample selection, remains underexplored. This gap stems from the fact that KD typically assumes access to sufficient labeled data, whereas AL operates in data-scarce scenarios where task-specific teacher models are often unavailable. In this paper, we introduce ActiveKD, a framework that integrates AL with KD by leveraging the zero- and few-shot capabilities of large vision-language models (VLMs). A key aspect of ActiveKD is the structured prediction bias of VLMs -- i.e., their predictions form clusters in the probability space. We regard this structure as an inductive bias of the teacher model, capturing generalizable output patterns beneficial to student learning. To exploit this bias, we propose Probabilistic CoreSet (PCoreSet), a selection strategy that maximizes coverage in the probability space rather than the feature space. PCoreSet strategically selects categorically diverse unlabeled samples, facilitating more efficient transfer of teacher knowledge under limited annotation budgets. Evaluations on 11 datasets show that PCoreSet consistently outperforms existing selection methods within the ActiveKD framework, advancing research at the intersection of AL and KD."

[04.06.2025 05:16] Response: ```python
['TRANSFER_LEARNING', 'OPTIMIZATION']
```
[04.06.2025 05:16] Response: ParsedChatCompletionMessage[Article](content='{"desc":"ActiveKD is a novel framework that combines active learning (AL) with knowledge distillation (KD) to enhance the selection of diverse, unlabeled samples for annotation. It addresses the challenge of limited labeled data by utilizing large vision-language models (VLMs) that can perform well even with few examples. The framework introduces a selection strategy called Probabilistic CoreSet (PCoreSet), which focuses on maximizing coverage in the probability space, allowing for more effective knowledge transfer from teacher models to student models. Evaluations demonstrate that ActiveKD, through PCoreSet, significantly improves sample selection efficiency compared to traditional methods.","title":"Efficient Sample Selection through ActiveKD: Merging Active Learning and Knowledge Distillation"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='ActiveKD is a novel framework that combines active learning (AL) with knowledge distillation (KD) to enhance the selection of diverse, unlabeled samples for annotation. It addresses the challenge of limited labeled data by utilizing large vision-language models (VLMs) that can perform well even with few examples. The framework introduces a selection strategy called Probabilistic CoreSet (PCoreSet), which focuses on maximizing coverage in the probability space, allowing for more effective knowledge transfer from teacher models to student models. Evaluations demonstrate that ActiveKD, through PCoreSet, significantly improves sample selection efficiency compared to traditional methods.', title='Efficient Sample Selection through ActiveKD: Merging Active Learning and Knowledge Distillation'))
[04.06.2025 05:16] Response: ParsedChatCompletionMessage[Article](content='{"desc":"ActiveKD是一个将主动学习与知识蒸馏相结合的框架，旨在高效选择多样化的未标注样本进行标注。知识蒸馏通常需要足够的标注数据，而主动学习则在数据稀缺的情况下工作，因此两者的结合尚未得到充分探索。ActiveKD利用大型视觉-语言模型的零样本和少样本能力，通过概率空间中的结构化预测偏差来优化样本选择。我们提出的概率核心集（PCoreSet）策略能够在有限的标注预算下，选择具有类别多样性的未标注样本，从而更有效地传递教师模型的知识。","title":"主动学习与知识蒸馏的高效结合"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='ActiveKD是一个将主动学习与知识蒸馏相结合的框架，旨在高效选择多样化的未标注样本进行标注。知识蒸馏通常需要足够的标注数据，而主动学习则在数据稀缺的情况下工作，因此两者的结合尚未得到充分探索。ActiveKD利用大型视觉-语言模型的零样本和少样本能力，通过概率空间中的结构化预测偏差来优化样本选择。我们提出的概率核心集（PCoreSet）策略能够在有限的标注预算下，选择具有类别多样性的未标注样本，从而更有效地传递教师模型的知识。', title='主动学习与知识蒸馏的高效结合'))
[04.06.2025 05:16] Using data from previous issue: {"categories": ["#open_source", "#video", "#diffusion"], "emoji": "🎬", "ru": {"title": "LumosFlow: плавная генерация длинных видео с помощью иерархической интерполяции кадров", "desc": "LumosFlow - это новый подход к генерации длинных видео с использованием иерархического пайплайна. Система применяе
[04.06.2025 05:16] Querying the API.
[04.06.2025 05:16] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

FlowMo, a training-free method, enhances motion coherence in pre-trained text-to-video diffusion models by leveraging their own predictions to reduce patch-wise temporal variance.  					AI-generated summary 				 Text-to-video diffusion models are notoriously limited in their ability to model temporal aspects such as motion, physics, and dynamic interactions. Existing approaches address this limitation by retraining the model or introducing external conditioning signals to enforce temporal consistency. In this work, we explore whether a meaningful temporal representation can be extracted directly from the predictions of a pre-trained model without any additional training or auxiliary inputs. We introduce FlowMo, a novel training-free guidance method that enhances motion coherence using only the model's own predictions in each diffusion step. FlowMo first derives an appearance-debiased temporal representation by measuring the distance between latents corresponding to consecutive frames. This highlights the implicit temporal structure predicted by the model. It then estimates motion coherence by measuring the patch-wise variance across the temporal dimension and guides the model to reduce this variance dynamically during sampling. Extensive experiments across multiple text-to-video models demonstrate that FlowMo significantly improves motion coherence without sacrificing visual quality or prompt alignment, offering an effective plug-and-play solution for enhancing the temporal fidelity of pre-trained video diffusion models.
[04.06.2025 05:16] Response: {
  "desc": "FlowMo - это метод без дополнительного обучения, который улучшает согласованность движения в предобученных диффузионных моделях для генерации видео по тексту. Он использует собственные предсказания модели для уменьшения покадровой временной вариативности. FlowMo извлекает временное представление, свободное от влияния внешнего вида, измеряя расстояние между латентными представлениями последовательных кадров. Затем метод оценивает согласованность движения, измеряя вариативность по временной оси, и направляет модель на уменьшение этой вариативности во время семплирования.",
  "emoji": "🎬",
  "title": "Улучшение движения в видео без переобучения модели"
}
[04.06.2025 05:16] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"FlowMo, a training-free method, enhances motion coherence in pre-trained text-to-video diffusion models by leveraging their own predictions to reduce patch-wise temporal variance.  					AI-generated summary 				 Text-to-video diffusion models are notoriously limited in their ability to model temporal aspects such as motion, physics, and dynamic interactions. Existing approaches address this limitation by retraining the model or introducing external conditioning signals to enforce temporal consistency. In this work, we explore whether a meaningful temporal representation can be extracted directly from the predictions of a pre-trained model without any additional training or auxiliary inputs. We introduce FlowMo, a novel training-free guidance method that enhances motion coherence using only the model's own predictions in each diffusion step. FlowMo first derives an appearance-debiased temporal representation by measuring the distance between latents corresponding to consecutive frames. This highlights the implicit temporal structure predicted by the model. It then estimates motion coherence by measuring the patch-wise variance across the temporal dimension and guides the model to reduce this variance dynamically during sampling. Extensive experiments across multiple text-to-video models demonstrate that FlowMo significantly improves motion coherence without sacrificing visual quality or prompt alignment, offering an effective plug-and-play solution for enhancing the temporal fidelity of pre-trained video diffusion models."

[04.06.2025 05:16] Response: ```python
['VIDEO', 'TRAINING']
```
[04.06.2025 05:16] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"FlowMo, a training-free method, enhances motion coherence in pre-trained text-to-video diffusion models by leveraging their own predictions to reduce patch-wise temporal variance.  					AI-generated summary 				 Text-to-video diffusion models are notoriously limited in their ability to model temporal aspects such as motion, physics, and dynamic interactions. Existing approaches address this limitation by retraining the model or introducing external conditioning signals to enforce temporal consistency. In this work, we explore whether a meaningful temporal representation can be extracted directly from the predictions of a pre-trained model without any additional training or auxiliary inputs. We introduce FlowMo, a novel training-free guidance method that enhances motion coherence using only the model's own predictions in each diffusion step. FlowMo first derives an appearance-debiased temporal representation by measuring the distance between latents corresponding to consecutive frames. This highlights the implicit temporal structure predicted by the model. It then estimates motion coherence by measuring the patch-wise variance across the temporal dimension and guides the model to reduce this variance dynamically during sampling. Extensive experiments across multiple text-to-video models demonstrate that FlowMo significantly improves motion coherence without sacrificing visual quality or prompt alignment, offering an effective plug-and-play solution for enhancing the temporal fidelity of pre-trained video diffusion models."

[04.06.2025 05:16] Response: ```python
["DIFFUSION"]
```
[04.06.2025 05:16] Response: ParsedChatCompletionMessage[Article](content='{"desc":"FlowMo is a novel method that improves the motion coherence of pre-trained text-to-video diffusion models without requiring any additional training. It works by using the model\'s own predictions to create a temporal representation that captures the dynamics of motion across frames. By measuring the variance in motion across patches, FlowMo guides the model to reduce inconsistencies during the video generation process. This approach enhances the temporal fidelity of the generated videos while maintaining high visual quality and alignment with input prompts.","title":"Enhancing Motion Coherence in Video Generation Without Training"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="FlowMo is a novel method that improves the motion coherence of pre-trained text-to-video diffusion models without requiring any additional training. It works by using the model's own predictions to create a temporal representation that captures the dynamics of motion across frames. By measuring the variance in motion across patches, FlowMo guides the model to reduce inconsistencies during the video generation process. This approach enhances the temporal fidelity of the generated videos while maintaining high visual quality and alignment with input prompts.", title='Enhancing Motion Coherence in Video Generation Without Training'))
[04.06.2025 05:16] Response: ParsedChatCompletionMessage[Article](content='{"desc":"FlowMo是一种无训练的方法，旨在提高预训练文本到视频扩散模型中的运动一致性。它通过利用模型自身的预测，减少了时间维度上的补丁方差，从而增强了运动的连贯性。与传统方法不同，FlowMo不需要重新训练模型或引入外部条件信号，而是直接从模型的预测中提取有意义的时间表示。实验结果表明，FlowMo在多个文本到视频模型中显著提高了运动一致性，同时保持了视觉质量和提示对齐，提供了一种有效的即插即用解决方案。","title":"FlowMo：提升视频生成运动一致性的新方法"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='FlowMo是一种无训练的方法，旨在提高预训练文本到视频扩散模型中的运动一致性。它通过利用模型自身的预测，减少了时间维度上的补丁方差，从而增强了运动的连贯性。与传统方法不同，FlowMo不需要重新训练模型或引入外部条件信号，而是直接从模型的预测中提取有意义的时间表示。实验结果表明，FlowMo在多个文本到视频模型中显著提高了运动一致性，同时保持了视觉质量和提示对齐，提供了一种有效的即插即用解决方案。', title='FlowMo：提升视频生成运动一致性的新方法'))
[04.06.2025 05:16] Using data from previous issue: {"categories": ["#cv", "#dataset", "#transfer_learning", "#diffusion"], "emoji": "🖼️", "ru": {"title": "RelationAdapter: Умное редактирование изображений по паре примеров", "desc": "RelationAdapter - это легковесный модуль, улучшающий работу моделей Diffusion Transformer для захвата и применения виз
[04.06.2025 05:16] Using data from previous issue: {"categories": ["#open_source", "#data", "#synthetic", "#dataset"], "emoji": "📊", "ru": {"title": "DataRubrics: Новый стандарт качества датасетов в эпоху ИИ", "desc": "Эта статья предлагает новый подход к оценке качества наборов данных для машинного обучения. Авторы вводят концепцию DataRubrics - ст
[04.06.2025 05:16] Using data from previous issue: {"categories": ["#rl", "#transfer_learning", "#dataset", "#reasoning"], "emoji": "🧠", "ru": {"title": "Длинные цепочки мышления для ИИ: новый подход к обучению моделей рассуждениям", "desc": "Статья представляет набор данных Long CoT Collection, созданный с помощью коротких моделей цепочек рассужден
[04.06.2025 05:16] Querying the API.
[04.06.2025 05:16] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Adaptive parallel decoding (APD) enhances the throughput of diffusion large language models (dLLMs) by dynamically adjusting parallel token generation without significantly diminishing quality.  					AI-generated summary 				 The generation speed of LLMs are bottlenecked by autoregressive decoding, where tokens are predicted sequentially one by one. Alternatively, diffusion large language models (dLLMs) theoretically allow for parallel token generation, but in practice struggle to achieve the speed of autoregressive models without significantly sacrificing quality. We therefore introduce adaptive parallel decoding (APD), a novel method that dynamically adjusts the number of tokens sampled in parallel. We achieve this by defining a multiplicative mixture between the dLLM marginal probabilities and the joint probability of sequences under a small auxiliary autoregressive model. This inverts the standard setup of speculative decoding, where the goal is to sample from a large autoregressive verifier by drafting from a smaller model. We further optimize APD by enabling KV caching and limiting the size of the masked input. Altogether, our method puts forward three tunable parameters to flexibly tradeoff throughput and quality. We show that APD provides markedly higher throughput with minimal quality degradations on downstream benchmarks.
[04.06.2025 05:16] Response: {
  "desc": "Статья представляет новый метод адаптивного параллельного декодирования (APD) для диффузионных больших языковых моделей (dLLM). APD динамически регулирует количество токенов, генерируемых параллельно, что позволяет значительно увеличить пропускную способность без существенного ухудшения качества. Метод использует мультипликативную смесь маргинальных вероятностей dLLM и совместной вероятности последовательностей под небольшой вспомогательной авторегрессионной моделью. APD оптимизирован с помощью KV-кэширования и ограничения размера маскированного входа, что обеспечивает гибкий компромисс между пропускной способностью и качеством.",
  "emoji": "🚀",
  "title": "Ускорение языковых моделей без потери качества"
}
[04.06.2025 05:16] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Adaptive parallel decoding (APD) enhances the throughput of diffusion large language models (dLLMs) by dynamically adjusting parallel token generation without significantly diminishing quality.  					AI-generated summary 				 The generation speed of LLMs are bottlenecked by autoregressive decoding, where tokens are predicted sequentially one by one. Alternatively, diffusion large language models (dLLMs) theoretically allow for parallel token generation, but in practice struggle to achieve the speed of autoregressive models without significantly sacrificing quality. We therefore introduce adaptive parallel decoding (APD), a novel method that dynamically adjusts the number of tokens sampled in parallel. We achieve this by defining a multiplicative mixture between the dLLM marginal probabilities and the joint probability of sequences under a small auxiliary autoregressive model. This inverts the standard setup of speculative decoding, where the goal is to sample from a large autoregressive verifier by drafting from a smaller model. We further optimize APD by enabling KV caching and limiting the size of the masked input. Altogether, our method puts forward three tunable parameters to flexibly tradeoff throughput and quality. We show that APD provides markedly higher throughput with minimal quality degradations on downstream benchmarks."

[04.06.2025 05:16] Response: ```python
['TRAINING', 'ARCHITECTURE', 'BENCHMARK']
```
[04.06.2025 05:16] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Adaptive parallel decoding (APD) enhances the throughput of diffusion large language models (dLLMs) by dynamically adjusting parallel token generation without significantly diminishing quality.  					AI-generated summary 				 The generation speed of LLMs are bottlenecked by autoregressive decoding, where tokens are predicted sequentially one by one. Alternatively, diffusion large language models (dLLMs) theoretically allow for parallel token generation, but in practice struggle to achieve the speed of autoregressive models without significantly sacrificing quality. We therefore introduce adaptive parallel decoding (APD), a novel method that dynamically adjusts the number of tokens sampled in parallel. We achieve this by defining a multiplicative mixture between the dLLM marginal probabilities and the joint probability of sequences under a small auxiliary autoregressive model. This inverts the standard setup of speculative decoding, where the goal is to sample from a large autoregressive verifier by drafting from a smaller model. We further optimize APD by enabling KV caching and limiting the size of the masked input. Altogether, our method puts forward three tunable parameters to flexibly tradeoff throughput and quality. We show that APD provides markedly higher throughput with minimal quality degradations on downstream benchmarks."

[04.06.2025 05:16] Response: ```python
["DIFFUSION", "OPTIMIZATION"]
```
[04.06.2025 05:16] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Adaptive parallel decoding (APD) is a new technique that improves the speed of diffusion large language models (dLLMs) by allowing multiple tokens to be generated at the same time. Traditional methods use autoregressive decoding, which predicts tokens one after another, slowing down the process. APD changes this by dynamically adjusting how many tokens are generated in parallel, balancing speed and quality. By combining probabilities from both dLLMs and a smaller autoregressive model, APD achieves higher throughput with only slight reductions in output quality.","title":"Boosting Speed in Language Models with Adaptive Parallel Decoding"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Adaptive parallel decoding (APD) is a new technique that improves the speed of diffusion large language models (dLLMs) by allowing multiple tokens to be generated at the same time. Traditional methods use autoregressive decoding, which predicts tokens one after another, slowing down the process. APD changes this by dynamically adjusting how many tokens are generated in parallel, balancing speed and quality. By combining probabilities from both dLLMs and a smaller autoregressive model, APD achieves higher throughput with only slight reductions in output quality.', title='Boosting Speed in Language Models with Adaptive Parallel Decoding'))
[04.06.2025 05:17] Response: ParsedChatCompletionMessage[Article](content='{"desc":"自适应并行解码（APD）通过动态调整并行生成的标记数量，提升了扩散大型语言模型（dLLMs）的吞吐量，而不会显著降低质量。传统的自回归解码方法使得生成速度受到限制，因为标记是一个接一个地预测的。尽管理论上扩散大型语言模型允许并行生成标记，但在实际应用中，往往难以在不牺牲质量的情况下达到自回归模型的速度。我们的方法通过定义dLLM边际概率与小型自回归模型下序列的联合概率之间的乘法混合，来实现这一目标，并通过启用KV缓存和限制掩码输入的大小进一步优化APD。","title":"自适应并行解码：提升生成速度与质量的平衡"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='自适应并行解码（APD）通过动态调整并行生成的标记数量，提升了扩散大型语言模型（dLLMs）的吞吐量，而不会显著降低质量。传统的自回归解码方法使得生成速度受到限制，因为标记是一个接一个地预测的。尽管理论上扩散大型语言模型允许并行生成标记，但在实际应用中，往往难以在不牺牲质量的情况下达到自回归模型的速度。我们的方法通过定义dLLM边际概率与小型自回归模型下序列的联合概率之间的乘法混合，来实现这一目标，并通过启用KV缓存和限制掩码输入的大小进一步优化APD。', title='自适应并行解码：提升生成速度与质量的平衡'))
[04.06.2025 05:17] Using data from previous issue: {"categories": ["#dataset", "#machine_translation", "#science", "#multilingual", "#benchmark"], "emoji": "📊", "ru": {"title": "M³FinMeeting: многоязычный бенчмарк для оценки понимания финансовых встреч языковыми моделями", "desc": "Статья представляет новый бенчмарк M³FinMeeting для оценки больших я
[04.06.2025 05:17] Using data from previous issue: {"categories": ["#agents", "#multimodal", "#reasoning", "#optimization", "#games", "#benchmark"], "emoji": "📊", "ru": {"title": "Мультимодальные отчеты: новый уровень генерации контента с помощью ИИ", "desc": "Новая система Multimodal DeepResearcher позволяет большим языковым моделям (LLM) создавать
[04.06.2025 05:17] Using data from previous issue: {"categories": ["#rl", "#training", "#reasoning", "#architecture", "#optimization"], "emoji": "🧠", "ru": {"title": "Единая модель для рассуждений и рекомендаций", "desc": "Предложена унифицированная крупная рекомендательная модель с внутренними возможностями рассуждений. Модель использует обучение с
[04.06.2025 05:17] Loading Chinese text from previous data.
[04.06.2025 05:17] Renaming data file.
[04.06.2025 05:17] Renaming previous data. hf_papers.json to ./d/2025-06-04.json
[04.06.2025 05:17] Saving new data file.
[04.06.2025 05:17] Generating page.
[04.06.2025 05:17] Renaming previous page.
[04.06.2025 05:17] Renaming previous data. index.html to ./d/2025-06-04.html
[04.06.2025 05:17] [Experimental] Generating Chinese page for reading.
[04.06.2025 05:17] Chinese vocab [{'word': '探讨', 'pinyin': 'tàn tǎo', 'trans': 'discuss'}, {'word': '增强', 'pinyin': 'zēng qiáng', 'trans': 'enhance'}, {'word': '推理', 'pinyin': 'tuī lǐ', 'trans': 'reasoning'}, {'word': '能力', 'pinyin': 'néng lì', 'trans': 'ability'}, {'word': '方法', 'pinyin': 'fāng fǎ', 'trans': 'method'}, {'word': '可验证', 'pinyin': 'kě yàn zhèng', 'trans': 'verifiable'}, {'word': '奖励', 'pinyin': 'jiǎng lì', 'trans': 'reward'}, {'word': '强化', 'pinyin': 'qiáng huà', 'trans': 'reinforce'}, {'word': '学习', 'pinyin': 'xué xí', 'trans': 'learning'}, {'word': '研究', 'pinyin': 'yán jiū', 'trans': 'research'}, {'word': '发现', 'pinyin': 'fā xiàn', 'trans': 'discover'}, {'word': '熵值', 'pinyin': 'shāng zhí', 'trans': 'entropy value'}, {'word': '显著', 'pinyin': 'xiǎn zhù', 'trans': 'significant'}, {'word': '影响', 'pinyin': 'yǐng xiǎng', 'trans': 'impact'}, {'word': '优化', 'pinyin': 'yōu huà', 'trans': 'optimization'}, {'word': '模式', 'pinyin': 'mó shì', 'trans': 'pattern'}, {'word': '观察', 'pinyin': 'guān chá', 'trans': 'observe'}, {'word': '少量', 'pinyin': 'shǎo liàng', 'trans': 'small amount'}, {'word': '决定', 'pinyin': 'jué dìng', 'trans': 'determine'}, {'word': '路径', 'pinyin': 'lù jìng', 'trans': 'path'}, {'word': '调整', 'pinyin': 'tiáo zhěng', 'trans': 'adjust'}, {'word': '策略', 'pinyin': 'cè lüè', 'trans': 'strategy'}, {'word': '梯度', 'pinyin': 'tī dù', 'trans': 'gradient'}, {'word': '更新', 'pinyin': 'gèng xīn', 'trans': 'update'}, {'word': '取得', 'pinyin': 'qǔ dé', 'trans': 'achieve'}, {'word': '性能', 'pinyin': 'xìng néng', 'trans': 'performance'}]
[04.06.2025 05:17] Renaming previous Chinese page.
[04.06.2025 05:17] Renaming previous data. zh.html to ./d/2025-06-03_zh_reading_task.html
[04.06.2025 05:17] Writing Chinese reading task.
[04.06.2025 05:17] Writing result.
[04.06.2025 05:17] Renaming log file.
[04.06.2025 05:17] Renaming previous data. log.txt to ./logs/2025-06-04_last_log.txt
