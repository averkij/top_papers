[04.06.2025 15:37] Read previous papers.
[04.06.2025 15:37] Generating top page (month).
[04.06.2025 15:37] Writing top page (month).
[04.06.2025 16:14] Read previous papers.
[04.06.2025 16:14] Get feed.
[04.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2505.24726
[04.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.02387
[04.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.03147
[04.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2505.24120
[04.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.02096
[04.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.00123
[04.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.03135
[04.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.02397
[04.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2505.24714
[04.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.01674
[04.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.03143
[04.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.03065
[04.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2505.23061
[04.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.03136
[04.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.03126
[04.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.00070
[04.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.03131
[04.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.02497
[04.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.02528
[04.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.00910
[04.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.01144
[04.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.03123
[04.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.01789
[04.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.01274
[04.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.03096
[04.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.03079
[04.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.02454
[04.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.02338
[04.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.00413
[04.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.00391
[04.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2505.24273
[04.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2505.16994
[04.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.03144
[04.06.2025 16:14] Extract page data from URL. URL: https://huggingface.co/papers/2506.02678
[04.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.02510
[04.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.02295
[04.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.01716
[04.06.2025 16:14] Extract page data from URL. URL: https://huggingface.co/papers/2506.01265
[04.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2505.24362
[04.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2505.18079
[04.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.02138
[04.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.01565
[04.06.2025 16:14] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[04.06.2025 16:14] No deleted papers detected.
[04.06.2025 16:14] Downloading and parsing papers (pdf, html). Total: 42.
[04.06.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2505.24726.
[04.06.2025 16:14] Extra JSON file exists (./assets/json/2505.24726.json), skip PDF parsing.
[04.06.2025 16:14] Paper image links file exists (./assets/img_data/2505.24726.json), skip HTML parsing.
[04.06.2025 16:14] Success.
[04.06.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2506.02387.
[04.06.2025 16:14] Extra JSON file exists (./assets/json/2506.02387.json), skip PDF parsing.
[04.06.2025 16:14] Paper image links file exists (./assets/img_data/2506.02387.json), skip HTML parsing.
[04.06.2025 16:14] Success.
[04.06.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2506.03147.
[04.06.2025 16:14] Extra JSON file exists (./assets/json/2506.03147.json), skip PDF parsing.
[04.06.2025 16:14] Paper image links file exists (./assets/img_data/2506.03147.json), skip HTML parsing.
[04.06.2025 16:14] Success.
[04.06.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2505.24120.
[04.06.2025 16:14] Extra JSON file exists (./assets/json/2505.24120.json), skip PDF parsing.
[04.06.2025 16:14] Paper image links file exists (./assets/img_data/2505.24120.json), skip HTML parsing.
[04.06.2025 16:14] Success.
[04.06.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2506.02096.
[04.06.2025 16:14] Extra JSON file exists (./assets/json/2506.02096.json), skip PDF parsing.
[04.06.2025 16:14] Paper image links file exists (./assets/img_data/2506.02096.json), skip HTML parsing.
[04.06.2025 16:14] Success.
[04.06.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2506.00123.
[04.06.2025 16:14] Extra JSON file exists (./assets/json/2506.00123.json), skip PDF parsing.
[04.06.2025 16:14] Paper image links file exists (./assets/img_data/2506.00123.json), skip HTML parsing.
[04.06.2025 16:14] Success.
[04.06.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2506.03135.
[04.06.2025 16:14] Extra JSON file exists (./assets/json/2506.03135.json), skip PDF parsing.
[04.06.2025 16:14] Paper image links file exists (./assets/img_data/2506.03135.json), skip HTML parsing.
[04.06.2025 16:14] Success.
[04.06.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2506.02397.
[04.06.2025 16:14] Extra JSON file exists (./assets/json/2506.02397.json), skip PDF parsing.
[04.06.2025 16:14] Paper image links file exists (./assets/img_data/2506.02397.json), skip HTML parsing.
[04.06.2025 16:14] Success.
[04.06.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2505.24714.
[04.06.2025 16:14] Extra JSON file exists (./assets/json/2505.24714.json), skip PDF parsing.
[04.06.2025 16:14] Paper image links file exists (./assets/img_data/2505.24714.json), skip HTML parsing.
[04.06.2025 16:14] Success.
[04.06.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2506.01674.
[04.06.2025 16:14] Extra JSON file exists (./assets/json/2506.01674.json), skip PDF parsing.
[04.06.2025 16:14] Paper image links file exists (./assets/img_data/2506.01674.json), skip HTML parsing.
[04.06.2025 16:14] Success.
[04.06.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2506.03143.
[04.06.2025 16:14] Extra JSON file exists (./assets/json/2506.03143.json), skip PDF parsing.
[04.06.2025 16:14] Paper image links file exists (./assets/img_data/2506.03143.json), skip HTML parsing.
[04.06.2025 16:14] Success.
[04.06.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2506.03065.
[04.06.2025 16:14] Extra JSON file exists (./assets/json/2506.03065.json), skip PDF parsing.
[04.06.2025 16:14] Paper image links file exists (./assets/img_data/2506.03065.json), skip HTML parsing.
[04.06.2025 16:14] Success.
[04.06.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2505.23061.
[04.06.2025 16:14] Extra JSON file exists (./assets/json/2505.23061.json), skip PDF parsing.
[04.06.2025 16:14] Paper image links file exists (./assets/img_data/2505.23061.json), skip HTML parsing.
[04.06.2025 16:14] Success.
[04.06.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2506.03136.
[04.06.2025 16:14] Extra JSON file exists (./assets/json/2506.03136.json), skip PDF parsing.
[04.06.2025 16:14] Paper image links file exists (./assets/img_data/2506.03136.json), skip HTML parsing.
[04.06.2025 16:14] Success.
[04.06.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2506.03126.
[04.06.2025 16:14] Extra JSON file exists (./assets/json/2506.03126.json), skip PDF parsing.
[04.06.2025 16:14] Paper image links file exists (./assets/img_data/2506.03126.json), skip HTML parsing.
[04.06.2025 16:14] Success.
[04.06.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2506.00070.
[04.06.2025 16:14] Extra JSON file exists (./assets/json/2506.00070.json), skip PDF parsing.
[04.06.2025 16:14] Paper image links file exists (./assets/img_data/2506.00070.json), skip HTML parsing.
[04.06.2025 16:14] Success.
[04.06.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2506.03131.
[04.06.2025 16:14] Extra JSON file exists (./assets/json/2506.03131.json), skip PDF parsing.
[04.06.2025 16:14] Paper image links file exists (./assets/img_data/2506.03131.json), skip HTML parsing.
[04.06.2025 16:14] Success.
[04.06.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2506.02497.
[04.06.2025 16:14] Extra JSON file exists (./assets/json/2506.02497.json), skip PDF parsing.
[04.06.2025 16:14] Paper image links file exists (./assets/img_data/2506.02497.json), skip HTML parsing.
[04.06.2025 16:14] Success.
[04.06.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2506.02528.
[04.06.2025 16:14] Extra JSON file exists (./assets/json/2506.02528.json), skip PDF parsing.
[04.06.2025 16:14] Paper image links file exists (./assets/img_data/2506.02528.json), skip HTML parsing.
[04.06.2025 16:14] Success.
[04.06.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2506.00910.
[04.06.2025 16:14] Extra JSON file exists (./assets/json/2506.00910.json), skip PDF parsing.
[04.06.2025 16:14] Paper image links file exists (./assets/img_data/2506.00910.json), skip HTML parsing.
[04.06.2025 16:14] Success.
[04.06.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2506.01144.
[04.06.2025 16:14] Extra JSON file exists (./assets/json/2506.01144.json), skip PDF parsing.
[04.06.2025 16:14] Paper image links file exists (./assets/img_data/2506.01144.json), skip HTML parsing.
[04.06.2025 16:14] Success.
[04.06.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2506.03123.
[04.06.2025 16:14] Extra JSON file exists (./assets/json/2506.03123.json), skip PDF parsing.
[04.06.2025 16:14] Paper image links file exists (./assets/img_data/2506.03123.json), skip HTML parsing.
[04.06.2025 16:14] Success.
[04.06.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2506.01789.
[04.06.2025 16:14] Extra JSON file exists (./assets/json/2506.01789.json), skip PDF parsing.
[04.06.2025 16:14] Paper image links file exists (./assets/img_data/2506.01789.json), skip HTML parsing.
[04.06.2025 16:14] Success.
[04.06.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2506.01274.
[04.06.2025 16:14] Extra JSON file exists (./assets/json/2506.01274.json), skip PDF parsing.
[04.06.2025 16:14] Paper image links file exists (./assets/img_data/2506.01274.json), skip HTML parsing.
[04.06.2025 16:14] Success.
[04.06.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2506.03096.
[04.06.2025 16:14] Extra JSON file exists (./assets/json/2506.03096.json), skip PDF parsing.
[04.06.2025 16:14] Paper image links file exists (./assets/img_data/2506.03096.json), skip HTML parsing.
[04.06.2025 16:14] Success.
[04.06.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2506.03079.
[04.06.2025 16:14] Extra JSON file exists (./assets/json/2506.03079.json), skip PDF parsing.
[04.06.2025 16:14] Paper image links file exists (./assets/img_data/2506.03079.json), skip HTML parsing.
[04.06.2025 16:14] Success.
[04.06.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2506.02454.
[04.06.2025 16:14] Extra JSON file exists (./assets/json/2506.02454.json), skip PDF parsing.
[04.06.2025 16:14] Paper image links file exists (./assets/img_data/2506.02454.json), skip HTML parsing.
[04.06.2025 16:14] Success.
[04.06.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2506.02338.
[04.06.2025 16:14] Extra JSON file exists (./assets/json/2506.02338.json), skip PDF parsing.
[04.06.2025 16:14] Paper image links file exists (./assets/img_data/2506.02338.json), skip HTML parsing.
[04.06.2025 16:14] Success.
[04.06.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2506.00413.
[04.06.2025 16:14] Extra JSON file exists (./assets/json/2506.00413.json), skip PDF parsing.
[04.06.2025 16:14] Paper image links file exists (./assets/img_data/2506.00413.json), skip HTML parsing.
[04.06.2025 16:14] Success.
[04.06.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2506.00391.
[04.06.2025 16:14] Extra JSON file exists (./assets/json/2506.00391.json), skip PDF parsing.
[04.06.2025 16:14] Paper image links file exists (./assets/img_data/2506.00391.json), skip HTML parsing.
[04.06.2025 16:14] Success.
[04.06.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2505.24273.
[04.06.2025 16:14] Extra JSON file exists (./assets/json/2505.24273.json), skip PDF parsing.
[04.06.2025 16:14] Paper image links file exists (./assets/img_data/2505.24273.json), skip HTML parsing.
[04.06.2025 16:14] Success.
[04.06.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2505.16994.
[04.06.2025 16:14] Extra JSON file exists (./assets/json/2505.16994.json), skip PDF parsing.
[04.06.2025 16:14] Paper image links file exists (./assets/img_data/2505.16994.json), skip HTML parsing.
[04.06.2025 16:14] Success.
[04.06.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2506.03144.
[04.06.2025 16:14] Extra JSON file exists (./assets/json/2506.03144.json), skip PDF parsing.
[04.06.2025 16:14] Paper image links file exists (./assets/img_data/2506.03144.json), skip HTML parsing.
[04.06.2025 16:14] Success.
[04.06.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2506.02678.
[04.06.2025 16:14] Downloading paper 2506.02678 from http://arxiv.org/pdf/2506.02678v1...
[04.06.2025 16:14] Extracting affiliations from text.
[04.06.2025 16:14] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"TL;DR: Too Long, Do Re-weighting for Effcient LLM Reasoning Compression Zhong-Zhi Liχπ , Xiao Liangργ , Zihao Tangφ , Lei Jiφ , Peijie Wangχπ , Haotian Xuγ , Xing Wπ , Haizhen Huangφ , Weiwei Dengφ , Ying Nian Wuρ , Yeyun Gongφ , Zhijiang Guoθ β , Xiao Liuφ , Fei Yinχπ , Cheng-Lin Liuχπ χ School of Artificial Intelligence, Chinese Academy of Sciences π Institute of Automation, Chinese Academy of Sciences γ Tsinghua University ρ University of California, Los Angeles φ Microsoft β Hong Kong University of Science and Technology θ Hong Kong University of Science and Technology (Guangzhou) https://github.com/zzli2022/TLDR "
[04.06.2025 16:14] Response: ```python
[
    "School of Artificial Intelligence, Chinese Academy of Sciences",
    "Institute of Automation, Chinese Academy of Sciences",
    "Tsinghua University",
    "University of California, Los Angeles",
    "Microsoft",
    "Hong Kong University of Science and Technology",
    "Hong Kong University of Science and Technology (Guangzhou)"
]
```
[04.06.2025 16:14] Deleting PDF ./assets/pdf/2506.02678.pdf.
[04.06.2025 16:14] Success.
[04.06.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2506.02510.
[04.06.2025 16:14] Extra JSON file exists (./assets/json/2506.02510.json), skip PDF parsing.
[04.06.2025 16:14] Paper image links file exists (./assets/img_data/2506.02510.json), skip HTML parsing.
[04.06.2025 16:14] Success.
[04.06.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2506.02295.
[04.06.2025 16:14] Extra JSON file exists (./assets/json/2506.02295.json), skip PDF parsing.
[04.06.2025 16:14] Paper image links file exists (./assets/img_data/2506.02295.json), skip HTML parsing.
[04.06.2025 16:14] Success.
[04.06.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2506.01716.
[04.06.2025 16:14] Extra JSON file exists (./assets/json/2506.01716.json), skip PDF parsing.
[04.06.2025 16:14] Paper image links file exists (./assets/img_data/2506.01716.json), skip HTML parsing.
[04.06.2025 16:14] Success.
[04.06.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2506.01265.
[04.06.2025 16:14] Downloading paper 2506.01265 from http://arxiv.org/pdf/2506.01265v1...
[04.06.2025 16:15] Extracting affiliations from text.
[04.06.2025 16:15] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Beyond In-Context Learning: Aligning Long-form Generation of Large Language Models via Task-Inherent Attribute Guidelines Do Xuan Long1,3, Duong Ngoc Yen2, Do Xuan Trong1*, Luu Anh Tuan2, Kenji Kawaguchi1, Shafiq Joty4, Min-Yen Kan1, Nancy F. Chen3 1National University of Singapore, 2Nanyang Technological University, Singapore, 3Institute for Infocomm Research (I2R), A*STAR, 4Salesforce AI Research xuanlong.do@u.nus.edu 5 2 0 2 2 ] . [ 1 5 6 2 1 0 . 6 0 5 2 : r a "
[04.06.2025 16:15] Response: ```python
[
    "National University of Singapore",
    "Nanyang Technological University, Singapore",
    "Institute for Infocomm Research (I2R), A*STAR",
    "Salesforce AI Research"
]
```
[04.06.2025 16:15] Deleting PDF ./assets/pdf/2506.01265.pdf.
[04.06.2025 16:15] Success.
[04.06.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2505.24362.
[04.06.2025 16:15] Extra JSON file exists (./assets/json/2505.24362.json), skip PDF parsing.
[04.06.2025 16:15] Paper image links file exists (./assets/img_data/2505.24362.json), skip HTML parsing.
[04.06.2025 16:15] Success.
[04.06.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2505.18079.
[04.06.2025 16:15] Extra JSON file exists (./assets/json/2505.18079.json), skip PDF parsing.
[04.06.2025 16:15] Paper image links file exists (./assets/img_data/2505.18079.json), skip HTML parsing.
[04.06.2025 16:15] Success.
[04.06.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2506.02138.
[04.06.2025 16:15] Extra JSON file exists (./assets/json/2506.02138.json), skip PDF parsing.
[04.06.2025 16:15] Paper image links file exists (./assets/img_data/2506.02138.json), skip HTML parsing.
[04.06.2025 16:15] Success.
[04.06.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2506.01565.
[04.06.2025 16:15] Extra JSON file exists (./assets/json/2506.01565.json), skip PDF parsing.
[04.06.2025 16:15] Paper image links file exists (./assets/img_data/2506.01565.json), skip HTML parsing.
[04.06.2025 16:15] Success.
[04.06.2025 16:15] Enriching papers with extra data.
[04.06.2025 16:15] ********************************************************************************
[04.06.2025 16:15] Abstract 0. A method using self-reflection and reinforcement learning improves the performance of large language models, especially with limited feedback, by rewarding self-reflections that lead to better task performance.  					AI-generated summary 				 We explore a method for improving the performance of larg...
[04.06.2025 16:15] ********************************************************************************
[04.06.2025 16:15] Abstract 1. VS-Bench is a multimodal benchmark designed to evaluate Vision Language Models' strategic reasoning and decision-making in complex multi-agent environments.  					AI-generated summary 				 Recent advancements in Vision Language Models (VLMs) have expanded their capabilities to interactive agent task...
[04.06.2025 16:15] ********************************************************************************
[04.06.2025 16:15] Abstract 2. A unified generative framework called UniWorld uses semantic features from visual-language models for image perception and manipulation, outperforming BAGEL with reduced data.  					AI-generated summary 				 Although existing unified models deliver strong performance on vision-language understanding...
[04.06.2025 16:15] ********************************************************************************
[04.06.2025 16:15] Abstract 3. A new benchmark, CSVQA, evaluates scientific reasoning in vision-language models through domain-specific visual question answering, highlighting the need for improvement in these models.  					AI-generated summary 				 Vision-Language Models (VLMs) have demonstrated remarkable progress in multimodal...
[04.06.2025 16:15] ********************************************************************************
[04.06.2025 16:15] Abstract 4. SynthRL, a scalable pipeline for data synthesis in reinforcement learning with verifiable rewards, enhances visual math reasoning VLMs by generating challenging, verifiable questions.  					AI-generated summary 				 Vision-language models (VLMs) trained via reinforcement learning with verifiable rew...
[04.06.2025 16:15] ********************************************************************************
[04.06.2025 16:15] Abstract 5. VeBrain is a unified framework that integrates multimodal understanding, visual-spatial reasoning, and physical interaction for legged robots, demonstrating superior performance compared to existing methods across various benchmarks.  					AI-generated summary 				 The remarkable progress of Multimo...
[04.06.2025 16:15] ********************************************************************************
[04.06.2025 16:15] Abstract 6. A comprehensive benchmark called OmniSpatial evaluates vision-language models' understanding of advanced spatial reasoning tasks, revealing significant limitations across various models.  					AI-generated summary 				 Spatial reasoning is a key aspect of cognitive psychology and remains a major bot...
[04.06.2025 16:15] ********************************************************************************
[04.06.2025 16:15] Abstract 7. OThink-R1 is introduced to reduce reasoning redundancy in complex problem-solving by classifying reasoning steps as essential or redundant and dynamically switching thinking modes based on task complexity.  					AI-generated summary 				 Recent advanced large reasoning models (LRMs) leverage extende...
[04.06.2025 16:15] ********************************************************************************
[04.06.2025 16:15] Abstract 8. FinMME is a comprehensive multimodal dataset for financial research and FinScore is an evaluation system that highlights the challenges faced by even advanced models like GPT-4o in the finance domain.  					AI-generated summary 				 Multimodal Large Language Models (MLLMs) have experienced rapid dev...
[04.06.2025 16:15] ********************************************************************************
[04.06.2025 16:15] Abstract 9. MotionSight, a zero-shot method using object-centric visual spotlight and motion blur as prompts, enhances fine-grained video motion understanding and achieves state-of-the-art performance on MotionVid-QA, a large-scale dataset with hierarchical annotations.  					AI-generated summary 				 Despite a...
[04.06.2025 16:15] ********************************************************************************
[04.06.2025 16:15] Abstract 10. GUI-Actor, a VLM-based method using attention for coordinate-free GUI grounding, outperforms existing methods with better generalization and efficient fine-tuning.  					AI-generated summary 				 One of the principal challenges in building VLM-powered GUI agents is visual grounding, i.e., localizing...
[04.06.2025 16:15] ********************************************************************************
[04.06.2025 16:15] Abstract 11. Sparse-vDiT accelerates Video Diffusion Transformer (vDiT) by leveraging sparsity patterns in attention maps, reducing theoretical FLOPs and improving inference speed without significant loss in visual quality.  					AI-generated summary 				 While Diffusion Transformers (DiTs) have achieved breakth...
[04.06.2025 16:15] ********************************************************************************
[04.06.2025 16:15] Abstract 12. DINGO, a dynamic programming-based decoding strategy, enhances diffusion language models by enforcing structured output constraints, significantly improving performance on symbolic math and JSON generation tasks.  					AI-generated summary 				 Diffusion LLMs have emerged as a promising alternative ...
[04.06.2025 16:15] ********************************************************************************
[04.06.2025 16:15] Abstract 13. CURE, a reinforcement learning framework, improves code and unit test generation accuracy without ground-truth supervision, enhancing performance in various coding and testing tasks.  					AI-generated summary 				 We propose CURE, a novel reinforcement learning framework with a dedicated reward des...
[04.06.2025 16:15] ********************************************************************************
[04.06.2025 16:15] Abstract 14. AnimeShooter, a reference-guided multi-shot animation dataset, enhances coherent animated video generation by incorporating comprehensive hierarchical annotations and visual consistency, and AnimeShooterGen leverages MLLMs and video diffusion models to achieve superior results.  					AI-generated su...
[04.06.2025 16:15] ********************************************************************************
[04.06.2025 16:15] Abstract 15. Large Vision-Language Models (LVLMs) have recently shown great promise in advancing robotics by combining embodied reasoning with robot control. A common approach involves training on embodied reasoning tasks related to robot control using Supervised Fine-Tuning (SFT). However, SFT datasets are ofte...
[04.06.2025 16:15] ********************************************************************************
[04.06.2025 16:15] Abstract 16. A novel generative model, Native-resolution diffusion Transformer (NiT), synthesizes high-resolution and varied aspect ratio images with state-of-the-art performance and zero-shot generalization capabilities.  					AI-generated summary 				 We introduce native-resolution image synthesis, a novel gen...
[04.06.2025 16:15] ********************************************************************************
[04.06.2025 16:15] Abstract 17. LumosFlow uses LMTV-DM for key frame generation and LOF-DM followed by MotionControlNet for smooth intermediate frame interpolation, ensuring temporally coherent long video generation.  					AI-generated summary 				 Long video generation has gained increasing attention due to its widespread applica...
[04.06.2025 16:15] ********************************************************************************
[04.06.2025 16:15] Abstract 18. RelationAdapter, a lightweight module, enhances Diffusion Transformer models to capture and apply visual transformations effectively using source-target image pairs, improving editing performance and generalization on diverse tasks.  					AI-generated summary 				 Inspired by the in-context learning...
[04.06.2025 16:15] ********************************************************************************
[04.06.2025 16:15] Abstract 19. ActiveKD integrates active learning with knowledge distillation using large vision-language models to efficiently select diverse, unlabeled samples for annotation.  					AI-generated summary 				 Knowledge distillation (KD) is a widely used framework for training compact, task-specific models by lev...
[04.06.2025 16:15] ********************************************************************************
[04.06.2025 16:15] Abstract 20. FlowMo, a training-free method, enhances motion coherence in pre-trained text-to-video diffusion models by leveraging their own predictions to reduce patch-wise temporal variance.  					AI-generated summary 				 Text-to-video diffusion models are notoriously limited in their ability to model tempora...
[04.06.2025 16:15] ********************************************************************************
[04.06.2025 16:15] Abstract 21. Diffusion Models have achieved remarkable results in video synthesis but require iterative denoising steps, leading to substantial computational overhead. Consistency Models have made significant progress in accelerating diffusion models. However, directly applying them to video diffusion models oft...
[04.06.2025 16:15] ********************************************************************************
[04.06.2025 16:15] Abstract 22. High-quality datasets are fundamental to training and evaluating machine learning models, yet their creation-especially with accurate human annotations-remains a significant challenge. Many dataset paper submissions lack originality, diversity, or rigorous quality control, and these shortcomings are...
[04.06.2025 16:15] ********************************************************************************
[04.06.2025 16:15] Abstract 23. ReFoCUS uses reinforcement learning to optimize frame selection for video-LLM, enhancing reasoning performance in video QA by aligning with model preferences.  					AI-generated summary 				 Recent progress in Large Multi-modal Models (LMMs) has enabled effective vision-language reasoning, yet the a...
[04.06.2025 16:15] ********************************************************************************
[04.06.2025 16:15] Abstract 24. Contrastive language-image pre-training aligns the features of text-image pairs in a common latent space via distinct encoders for each modality. While this approach achieves impressive performance in several zero-shot tasks, it cannot natively handle multimodal inputs, i.e., encoding image and text...
[04.06.2025 16:15] ********************************************************************************
[04.06.2025 16:15] Abstract 25. ORV, an Occupancy-centric Robot Video generation framework, uses 4D semantic occupancy sequences to produce photorealistic, temporally consistent, and precisely controllable robot videos, enhancing existing methods.  					AI-generated summary 				 Acquiring real-world robotic simulation data through...
[04.06.2025 16:15] ********************************************************************************
[04.06.2025 16:15] Abstract 26. A new framework, Multimodal DeepResearcher, enables Large Language Models to generate high-quality multimodal reports combining text and diverse visualizations through structured textual representations.  					AI-generated summary 				 Visualizations play a crucial part in effective communication of...
[04.06.2025 16:15] ********************************************************************************
[04.06.2025 16:15] Abstract 27. The Long CoT Collection dataset, generated by short CoT LLMs, enhances general reasoning skills and provides a strong foundation for reinforcement learning, achieving quality comparable to R1.  					AI-generated summary 				 With the release of R1, a publicly available large reasoning model (LRM), r...
[04.06.2025 16:15] ********************************************************************************
[04.06.2025 16:15] Abstract 28. Adaptive parallel decoding (APD) enhances the throughput of diffusion large language models (dLLMs) by dynamically adjusting parallel token generation without significantly diminishing quality.  					AI-generated summary 				 The generation speed of LLMs are bottlenecked by autoregressive decoding, ...
[04.06.2025 16:15] ********************************************************************************
[04.06.2025 16:15] Abstract 29. SHARE, an SLM-based Hierarchical Action RECorrection system, enhances LLMs in text-to-SQL by transforming SQL queries into action trajectories and employing a granular refinement process, improving error detection and correction efficiency.  					AI-generated summary 				 Current self-correction app...
[04.06.2025 16:15] ********************************************************************************
[04.06.2025 16:15] Abstract 30. This study investigates the interplay between supervised fine-tuning and reinforcement learning in large language models, focusing on the role of backtracking in enhancing reasoning capabilities across various tasks.  					AI-generated summary 				 Recent breakthroughs in large language models (LLMs...
[04.06.2025 16:15] ********************************************************************************
[04.06.2025 16:15] Abstract 31. A unified large recommender model with intrinsic reasoning capabilities is proposed, facilitating interleaved reasoning and recommendation using a reinforcement learning framework called RecPO.  					AI-generated summary 				 Large recommender models have extended LLMs as powerful recommenders via e...
[04.06.2025 16:15] ********************************************************************************
[04.06.2025 16:15] Abstract 32. Semantic retrieval is crucial for modern applications yet remains underexplored in current research. Existing datasets are limited to single languages, single images, or singular retrieval conditions, often failing to fully exploit the expressive capacity of visual information as evidenced by mainta...
[04.06.2025 16:15] ********************************************************************************
[04.06.2025 16:15] Abstract 33. Large Language Models (LLMs) have recently achieved remarkable progress by leveraging Reinforcement Learning and extended Chain-of-Thought (CoT) techniques. However, the challenge of performing efficient language reasoning--especially during inference with extremely long outputs--has drawn increasin...
[04.06.2025 16:15] ********************************************************************************
[04.06.2025 16:15] Abstract 34. A new multilingual, multi-sector, and multi-task benchmark, M³FinMeeting, evaluates large language models' performance in understanding financial meetings across different languages and industries.  					AI-generated summary 				 Recent breakthroughs in large language models (LLMs) have led to the d...
[04.06.2025 16:15] ********************************************************************************
[04.06.2025 16:15] Abstract 35. Qari-OCR, a series of fine-tuned vision-language models, achieves state-of-the-art performance in Arabic OCR through iterative optimization on specialized datasets, handling diacritics, fonts, layouts, and low-resolution images.  					AI-generated summary 				 The inherent complexities of Arabic scr...
[04.06.2025 16:15] ********************************************************************************
[04.06.2025 16:15] Abstract 36. The Self-Challenging framework trains intelligent agents using self-generated tasks defined as Code-as-Task, improving performance in multi-turn tool-use benchmarks.  					AI-generated summary 				 Large language models are quickly becoming the foundation for intelligent agents that are capable of u...
[04.06.2025 16:15] ********************************************************************************
[04.06.2025 16:15] Abstract 37. In-context learning (ICL) is an important yet not fully understood ability of pre-trained large language models (LLMs). It can greatly enhance task performance using a few examples, termed demonstrations, without fine-tuning. Although effective in question answering, ICL often underperforms in long-...
[04.06.2025 16:15] ********************************************************************************
[04.06.2025 16:15] Abstract 38. We investigate whether the success of a zero-shot Chain-of-Thought (CoT) process can be predicted before completion. We discover that a probing classifier, based on LLM representations, performs well even before a single token is generated, suggesting that crucial information about the reasoning pro...
[04.06.2025 16:15] ********************************************************************************
[04.06.2025 16:15] Abstract 39. The Deep Video Discovery agent uses an autonomous agentic search strategy with large language models to overcome limitations in long-form video understanding, achieving state-of-the-art results on benchmarks like LVBench.  					AI-generated summary 				 Long-form video understanding presents signifi...
[04.06.2025 16:15] ********************************************************************************
[04.06.2025 16:15] Abstract 40. A specialized LRP method for Transformer explainability considers positional encoding, improving relevance propagation and outperforming existing methods.  					AI-generated summary 				 The development of effective explainability tools for Transformers is a crucial pursuit in deep learning research...
[04.06.2025 16:15] ********************************************************************************
[04.06.2025 16:15] Abstract 41. Culture is a rich and dynamic domain that evolves across both geography and time. However, existing studies on cultural understanding with vision-language models (VLMs) primarily emphasize geographic diversity, often overlooking the critical temporal dimensions. To bridge this gap, we introduce Hanf...
[04.06.2025 16:15] Read previous papers.
[04.06.2025 16:15] Generating reviews via LLM API.
[04.06.2025 16:15] Using data from previous issue: {"categories": ["#optimization", "#training", "#rlhf", "#small_models", "#reasoning", "#rl"], "emoji": "🧠", "ru": {"title": "Самоанализ и обучение с подкреплением повышают эффективность языковых моделей", "desc": "Исследователи предложили метод улучшения работы больших языковых моделей с помощью сам
[04.06.2025 16:15] Using data from previous issue: {"categories": ["#reasoning", "#multimodal", "#agents", "#benchmark", "#games"], "emoji": "🤖", "ru": {"title": "VS-Bench: Новый рубеж в оценке стратегического мышления мультимодальных ИИ-агентов", "desc": "VS-Bench - это мультимодальный бенчмарк для оценки стратегического мышления и принятия решений
[04.06.2025 16:15] Using data from previous issue: {"categories": ["#dataset", "#cv", "#multimodal", "#open_source", "#benchmark"], "emoji": "🎨", "ru": {"title": "UniWorld: Мощная модель для работы с изображениями на основе семантических признаков", "desc": "UniWorld - это унифицированная генеративная модель для восприятия и редактирования изображен
[04.06.2025 16:15] Using data from previous issue: {"categories": ["#science", "#multimodal", "#benchmark", "#reasoning"], "emoji": "🔬", "ru": {"title": "CSVQA: новый рубеж в оценке научного мышления ИИ", "desc": "Представлен новый бенчмарк CSVQA для оценки научного мышления в мультимодальных моделях через задачи ответов на вопросы по изображениям в
[04.06.2025 16:15] Using data from previous issue: {"categories": ["#reasoning", "#dataset", "#training", "#synthetic", "#rl"], "emoji": "🧠", "ru": {"title": "SynthRL: Синтез данных для улучшения математических рассуждений ИИ", "desc": "SynthRL - это масштабируемый конвейер для синтеза данных в обучении с подкреплением с проверяемыми наградами. Он у
[04.06.2025 16:15] Using data from previous issue: {"categories": ["#games", "#robotics", "#multimodal", "#reasoning", "#benchmark", "#dataset"], "emoji": "🤖", "ru": {"title": "VeBrain: Единый мозг для восприятия, мышления и управления роботами", "desc": "VeBrain - это унифицированная система для роботов с ногами, объединяющая мультимодальное понима
[04.06.2025 16:15] Using data from previous issue: {"categories": ["#cv", "#benchmark", "#reasoning"], "emoji": "🧠", "ru": {"title": "OmniSpatial: новый рубеж в оценке пространственного мышления ИИ", "desc": "OmniSpatial - это комплексный тест для оценки понимания пространственных задач моделями компьютерного зрения и обработки естественного языка. 
[04.06.2025 16:15] Using data from previous issue: {"categories": ["#training", "#math", "#reasoning"], "emoji": "🧠", "ru": {"title": "Оптимизация рассуждений ИИ: эффективность без потери точности", "desc": "OThink-R1 - это новый метод машинного обучения, направленный на уменьшение избыточности рассуждений при решении сложных задач. Он классифицируе
[04.06.2025 16:15] Using data from previous issue: {"categories": ["#dataset", "#hallucinations", "#science", "#multimodal", "#benchmark"], "emoji": "📊", "ru": {"title": "FinMME и FinScore: Новый стандарт для оценки мультимодальных языковых моделей в финансовой сфере", "desc": "FinMME - это обширный мультимодальный датасет для финансовых исследовани
[04.06.2025 16:15] Using data from previous issue: {"categories": ["#dataset", "#cv", "#multimodal", "#open_source", "#games"], "emoji": "🎥", "ru": {"title": "Новый взгляд на движение: MotionSight улучшает понимание видео без обучения", "desc": "MotionSight - это новый метод без предварительного обучения для улучшения понимания движения в видео муль
[04.06.2025 16:15] Using data from previous issue: {"categories": ["#multimodal", "#training", "#cv", "#benchmark", "#optimization", "#games"], "emoji": "🖱️", "ru": {"title": "GUI-Actor: Революция в локализации элементов интерфейса с помощью VLM", "desc": "GUI-Actor - это метод на основе VLM, использующий механизм внимания для безкоординатной локали
[04.06.2025 16:15] Using data from previous issue: {"categories": ["#optimization", "#architecture", "#video", "#inference", "#diffusion"], "emoji": "🎞️", "ru": {"title": "Ускорение генерации видео с помощью разреженных трансформеров", "desc": "Исследователи предложили метод Sparse-vDiT для ускорения Video Diffusion Transformer (vDiT), используя раз
[04.06.2025 16:15] Using data from previous issue: {"categories": ["#training", "#architecture", "#benchmark", "#optimization", "#diffusion"], "emoji": "🧮", "ru": {"title": "DINGO: Структурированное декодирование для диффузионных языковых моделей", "desc": "Статья представляет DINGO - новую стратегию декодирования для диффузионных языковых моделей. 
[04.06.2025 16:15] Using data from previous issue: {"categories": ["#training", "#optimization", "#agents", "#rl", "#games"], "emoji": "🧠", "ru": {"title": "CURE: эволюция кодирования и тестирования без эталонов", "desc": "CURE - это новая система обучения с подкреплением для улучшения генерации кода и модульных тестов без использования эталонного к
[04.06.2025 16:15] Using data from previous issue: {"categories": ["#video", "#multimodal", "#story_generation", "#diffusion", "#dataset"], "emoji": "🎬", "ru": {"title": "AnimeShooter: новый уровень генерации связной анимации с опорными изображениями", "desc": "AnimeShooter - это набор данных для создания многокадровой анимации с использованием опор
[04.06.2025 16:15] Using data from previous issue: {"categories": ["#optimization", "#robotics", "#training", "#rl", "#reasoning"], "emoji": "🤖", "ru": {"title": "Robot-R1: Революция в обучении роботов через воплощенные рассуждения", "desc": "Статья представляет новый подход Robot-R1 для улучшения воплощенных рассуждений в робототехнике с использова
[04.06.2025 16:15] Using data from previous issue: {"categories": ["#architecture", "#diffusion", "#cv", "#synthetic", "#benchmark"], "emoji": "🖼️", "ru": {"title": "NiT: Революция в генерации изображений без ограничений разрешения", "desc": "Представлена новая генеративная модель NiT (Native-resolution diffusion Transformer), способная синтезироват
[04.06.2025 16:15] Using data from previous issue: {"categories": ["#open_source", "#video", "#diffusion"], "emoji": "🎬", "ru": {"title": "LumosFlow: плавная генерация длинных видео с помощью иерархической интерполяции кадров", "desc": "LumosFlow - это новый подход к генерации длинных видео с использованием иерархического пайплайна. Система применяе
[04.06.2025 16:15] Using data from previous issue: {"categories": ["#cv", "#dataset", "#transfer_learning", "#diffusion"], "emoji": "🖼️", "ru": {"title": "RelationAdapter: Умное редактирование изображений по паре примеров", "desc": "RelationAdapter - это легковесный модуль, улучшающий работу моделей Diffusion Transformer для захвата и применения виз
[04.06.2025 16:15] Using data from previous issue: {"categories": ["#data", "#optimization", "#training", "#dataset", "#transfer_learning"], "emoji": "🧠", "ru": {"title": "ActiveKD: Эффективное обучение с ограниченными данными через синергию активного обучения и дистилляции знаний", "desc": "ActiveKD - это новый подход, объединяющий активное обучени
[04.06.2025 16:15] Using data from previous issue: {"categories": ["#video", "#training", "#diffusion"], "emoji": "🎬", "ru": {"title": "Улучшение движения в видео без переобучения модели", "desc": "FlowMo - это метод без дополнительного обучения, который улучшает согласованность движения в предобученных диффузионных моделях для генерации видео по те
[04.06.2025 16:15] Using data from previous issue: {"categories": ["#training", "#video", "#multimodal", "#diffusion", "#optimization", "#architecture"], "emoji": "🎬", "ru": {"title": "Эффективный синтез видео с помощью двух экспертов", "desc": "Авторы статьи предлагают новый подход к ускорению диффузионных моделей для синтеза видео. Они выявили про
[04.06.2025 16:15] Using data from previous issue: {"categories": ["#open_source", "#data", "#synthetic", "#dataset"], "emoji": "📊", "ru": {"title": "DataRubrics: Новый стандарт качества датасетов в эпоху ИИ", "desc": "Эта статья предлагает новый подход к оценке качества наборов данных для машинного обучения. Авторы вводят концепцию DataRubrics - ст
[04.06.2025 16:15] Using data from previous issue: {"categories": ["#reasoning", "#multimodal", "#video", "#optimization", "#rl", "#benchmark"], "emoji": "🎞️", "ru": {"title": "Умный выбор кадров для лучшего понимания видео ИИ", "desc": "ReFoCUS - это новый подход к оптимизации выбора кадров для видео-LLM с использованием обучения с подкреплением. О
[04.06.2025 16:15] Using data from previous issue: {"categories": ["#architecture", "#dataset", "#games", "#alignment", "#multimodal"], "emoji": "🔀", "ru": {"title": "Ранняя фьюжн для мультимодального ИИ", "desc": "FuseLIP - это новая архитектура для мультимодального встраивания, использующая единую трансформерную модель для обработки текстовых и из
[04.06.2025 16:15] Using data from previous issue: {"categories": ["#games", "#robotics", "#optimization", "#video"], "emoji": "🤖", "ru": {"title": "ORV: Точный контроль роботов через 4D семантическую занятость", "desc": "ORV - это фреймворк для генерации видео с роботами, использующий последовательности 4D семантической занятости. Он позволяет созд
[04.06.2025 16:15] Using data from previous issue: {"categories": ["#agents", "#multimodal", "#reasoning", "#optimization", "#games", "#benchmark"], "emoji": "📊", "ru": {"title": "Мультимодальные отчеты: новый уровень генерации контента с помощью ИИ", "desc": "Новая система Multimodal DeepResearcher позволяет большим языковым моделям (LLM) создавать
[04.06.2025 16:15] Using data from previous issue: {"categories": ["#rl", "#transfer_learning", "#dataset", "#reasoning"], "emoji": "🧠", "ru": {"title": "Длинные цепочки мышления для ИИ: новый подход к обучению моделей рассуждениям", "desc": "Статья представляет набор данных Long CoT Collection, созданный с помощью коротких моделей цепочек рассужден
[04.06.2025 16:15] Using data from previous issue: {"categories": ["#optimization", "#training", "#benchmark", "#diffusion", "#architecture"], "emoji": "🚀", "ru": {"title": "Ускорение языковых моделей без потери качества", "desc": "Статья представляет новый метод адаптивного параллельного декодирования (APD) для диффузионных больших языковых моделей
[04.06.2025 16:15] Using data from previous issue: {"categories": ["#low_resource", "#training", "#optimization", "#dataset", "#reasoning", "#small_models", "#data"], "emoji": "🔍", "ru": {"title": "Точная коррекция SQL с помощью иерархического анализа действий", "desc": "SHARE - это система коррекции действий на основе SLM, которая улучшает работу б
[04.06.2025 16:15] Using data from previous issue: {"categories": ["#training", "#rl", "#synthetic", "#reasoning", "#transfer_learning"], "emoji": "🧠", "ru": {"title": "Возврат в рассуждениях: ключ к улучшению LLM", "desc": "Исследование изучает взаимодействие между обучением с учителем и обучением с подкреплением в больших языковых моделях (LLM), ф
[04.06.2025 16:15] Using data from previous issue: {"categories": ["#rl", "#training", "#reasoning", "#architecture", "#optimization"], "emoji": "🧠", "ru": {"title": "Единая модель для рассуждений и рекомендаций", "desc": "Предложена унифицированная крупная рекомендательная модель с внутренними возможностями рассуждений. Модель использует обучение с
[04.06.2025 16:15] Using data from previous issue: {"categories": ["#training", "#dataset", "#benchmark", "#multilingual", "#transfer_learning", "#open_source"], "emoji": "🔍", "ru": {"title": "Революция в многоязычном семантическом поиске: MERIT и Coral", "desc": "Статья представляет MERIT - первый многоязычный набор данных для семантического поиска
[04.06.2025 16:15] Querying the API.
[04.06.2025 16:15] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Large Language Models (LLMs) have recently achieved remarkable progress by leveraging Reinforcement Learning and extended Chain-of-Thought (CoT) techniques. However, the challenge of performing efficient language reasoning--especially during inference with extremely long outputs--has drawn increasing attention from the research community. In this work, we propose a dynamic ratio-based training pipeline that does not rely on sophisticated data annotations or interpolation between multiple models. We continuously balance the weights between the model's System-1 and System-2 data to eliminate redundant reasoning processes while preserving the model's reasoning capability. We validate our approach across models on DeepSeek-R1-Distill-7B and DeepSeek-R1-Distill-14B and on a diverse set of benchmarks with varying difficulty levels. Our method significantly reduces the number of output tokens by nearly 40% while maintaining the accuracy of the reasoning. Our code and data will be available soon.
[04.06.2025 16:15] Response: {
  "desc": "Эта статья представляет новый метод обучения больших языковых моделей (LLM), который улучшает эффективность рассуждений без потери точности. Авторы предлагают динамическую систему обучения, основанную на балансировке весов между данными System-1 и System-2. Этот подход позволяет сократить количество выходных токенов почти на 40%, сохраняя при этом способность модели к рассуждениям. Метод был успешно протестирован на различных моделях и наборах данных разной сложности.",
  "emoji": "⚖️",
  "title": "Эффективное рассуждение в LLM: баланс между краткостью и точностью"
}
[04.06.2025 16:15] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Large Language Models (LLMs) have recently achieved remarkable progress by leveraging Reinforcement Learning and extended Chain-of-Thought (CoT) techniques. However, the challenge of performing efficient language reasoning--especially during inference with extremely long outputs--has drawn increasing attention from the research community. In this work, we propose a dynamic ratio-based training pipeline that does not rely on sophisticated data annotations or interpolation between multiple models. We continuously balance the weights between the model's System-1 and System-2 data to eliminate redundant reasoning processes while preserving the model's reasoning capability. We validate our approach across models on DeepSeek-R1-Distill-7B and DeepSeek-R1-Distill-14B and on a diverse set of benchmarks with varying difficulty levels. Our method significantly reduces the number of output tokens by nearly 40% while maintaining the accuracy of the reasoning. Our code and data will be available soon."

[04.06.2025 16:15] Response: ```python
["RL", "TRAINING", "BENCHMARK"]
```
[04.06.2025 16:15] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Large Language Models (LLMs) have recently achieved remarkable progress by leveraging Reinforcement Learning and extended Chain-of-Thought (CoT) techniques. However, the challenge of performing efficient language reasoning--especially during inference with extremely long outputs--has drawn increasing attention from the research community. In this work, we propose a dynamic ratio-based training pipeline that does not rely on sophisticated data annotations or interpolation between multiple models. We continuously balance the weights between the model's System-1 and System-2 data to eliminate redundant reasoning processes while preserving the model's reasoning capability. We validate our approach across models on DeepSeek-R1-Distill-7B and DeepSeek-R1-Distill-14B and on a diverse set of benchmarks with varying difficulty levels. Our method significantly reduces the number of output tokens by nearly 40% while maintaining the accuracy of the reasoning. Our code and data will be available soon."

[04.06.2025 16:15] Response: ```python
["REASONING", "LONG_CONTEXT", "OPTIMIZATION"]
```
[04.06.2025 16:15] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents a new training method for Large Language Models (LLMs) that improves their reasoning efficiency during inference, especially for long outputs. The proposed dynamic ratio-based training pipeline balances the model\'s System-1 and System-2 data without needing complex data annotations. By doing so, it reduces unnecessary reasoning steps while keeping the model\'s reasoning abilities intact. The results show a significant reduction in output tokens by about 40%, while still achieving high accuracy across various benchmarks.","title":"Efficient Reasoning in LLMs with Dynamic Training"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="This paper presents a new training method for Large Language Models (LLMs) that improves their reasoning efficiency during inference, especially for long outputs. The proposed dynamic ratio-based training pipeline balances the model's System-1 and System-2 data without needing complex data annotations. By doing so, it reduces unnecessary reasoning steps while keeping the model's reasoning abilities intact. The results show a significant reduction in output tokens by about 40%, while still achieving high accuracy across various benchmarks.", title='Efficient Reasoning in LLMs with Dynamic Training'))
[04.06.2025 16:15] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本文提出了一种基于动态比例的训练流程，旨在提高大型语言模型（LLMs）在推理时的效率，特别是在处理极长输出时。我们的方法通过平衡模型的System-1和System-2数据的权重，消除冗余推理过程，同时保持模型的推理能力。实验结果表明，该方法在DeepSeek-R1-Distill-7B和DeepSeek-R1-Distill-14B模型上显著减少了近40%的输出标记，同时保持了推理的准确性。我们的代码和数据将很快公开。","title":"动态比例训练，提升推理效率！"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本文提出了一种基于动态比例的训练流程，旨在提高大型语言模型（LLMs）在推理时的效率，特别是在处理极长输出时。我们的方法通过平衡模型的System-1和System-2数据的权重，消除冗余推理过程，同时保持模型的推理能力。实验结果表明，该方法在DeepSeek-R1-Distill-7B和DeepSeek-R1-Distill-14B模型上显著减少了近40%的输出标记，同时保持了推理的准确性。我们的代码和数据将很快公开。', title='动态比例训练，提升推理效率！'))
[04.06.2025 16:15] Using data from previous issue: {"categories": ["#dataset", "#machine_translation", "#science", "#multilingual", "#benchmark"], "emoji": "📊", "ru": {"title": "M³FinMeeting: многоязычный бенчмарк для оценки понимания финансовых встреч языковыми моделями", "desc": "Статья представляет новый бенчмарк M³FinMeeting для оценки больших я
[04.06.2025 16:15] Using data from previous issue: {"categories": ["#multilingual", "#low_resource", "#dataset", "#synthetic", "#cv", "#open_source", "#optimization"], "emoji": "📚", "ru": {"title": "Прорыв в распознавании арабского текста с помощью глубокого обучения", "desc": "Qari-OCR представляет собой серию моделей компьютерного зрения и обработ
[04.06.2025 16:15] Using data from previous issue: {"categories": ["#optimization", "#agi", "#training", "#agents", "#rl"], "emoji": "🤖", "ru": {"title": "Самообучение ИИ: агент бросает вызов самому себе", "desc": "Статья представляет новый подход к обучению интеллектуальных агентов, называемый Self-Challenging. В этой модели агент сам генерирует за
[04.06.2025 16:15] Querying the API.
[04.06.2025 16:15] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

In-context learning (ICL) is an important yet not fully understood ability of pre-trained large language models (LLMs). It can greatly enhance task performance using a few examples, termed demonstrations, without fine-tuning. Although effective in question answering, ICL often underperforms in long-form generation tasks such as summarization. Under appropriately realistic assumptions, we empirically and theoretically show that ICL demonstrations alone are insufficient to teach LLMs the task language and format distributions for generation. We argue for explicit exposure to the task distributions and hypothesize that defining them by prompting enhances model performance. To this end, we present LongGuide, which efficiently generates two parallel streams of guidelines capturing task language and format properties: (i) Metric Guidelines (MGs) that instruct models to optimize self-evaluated metrics; and (ii) Output Constraint Guidelines (OCGs) that constrain generation at both token and sentence levels. LongGuide automatically selects the best combination of guidelines, improving both strong open- and closed-source LLMs by over 5% in both zero- and few-shot settings. We show that LongGuide is generalizable, learnable by weak models to enhance strong ones, and integrates synergistically with automatic prompt optimizers.
[04.06.2025 16:15] Response: {
  "desc": "Статья исследует возможности обучения по контексту (ICL) для крупных языковых моделей (LLM) в задачах генерации длинных текстов. Авторы показывают, что одних только демонстраций ICL недостаточно для обучения LLM распределениям языка и формата задачи. Они предлагают метод LongGuide, который генерирует два потока рекомендаций для улучшения производительности модели. LongGuide автоматически выбирает лучшую комбинацию рекомендаций, повышая эффективность как открытых, так и закрытых LLM.",
  "emoji": "📝",
  "title": "LongGuide: улучшение генерации длинных текстов с помощью автоматических рекомендаций"
}
[04.06.2025 16:15] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"In-context learning (ICL) is an important yet not fully understood ability of pre-trained large language models (LLMs). It can greatly enhance task performance using a few examples, termed demonstrations, without fine-tuning. Although effective in question answering, ICL often underperforms in long-form generation tasks such as summarization. Under appropriately realistic assumptions, we empirically and theoretically show that ICL demonstrations alone are insufficient to teach LLMs the task language and format distributions for generation. We argue for explicit exposure to the task distributions and hypothesize that defining them by prompting enhances model performance. To this end, we present LongGuide, which efficiently generates two parallel streams of guidelines capturing task language and format properties: (i) Metric Guidelines (MGs) that instruct models to optimize self-evaluated metrics; and (ii) Output Constraint Guidelines (OCGs) that constrain generation at both token and sentence levels. LongGuide automatically selects the best combination of guidelines, improving both strong open- and closed-source LLMs by over 5% in both zero- and few-shot settings. We show that LongGuide is generalizable, learnable by weak models to enhance strong ones, and integrates synergistically with automatic prompt optimizers."

[04.06.2025 16:15] Response: ```python
['DATA', 'TRAINING', 'MULTIMODAL']
```
[04.06.2025 16:15] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"In-context learning (ICL) is an important yet not fully understood ability of pre-trained large language models (LLMs). It can greatly enhance task performance using a few examples, termed demonstrations, without fine-tuning. Although effective in question answering, ICL often underperforms in long-form generation tasks such as summarization. Under appropriately realistic assumptions, we empirically and theoretically show that ICL demonstrations alone are insufficient to teach LLMs the task language and format distributions for generation. We argue for explicit exposure to the task distributions and hypothesize that defining them by prompting enhances model performance. To this end, we present LongGuide, which efficiently generates two parallel streams of guidelines capturing task language and format properties: (i) Metric Guidelines (MGs) that instruct models to optimize self-evaluated metrics; and (ii) Output Constraint Guidelines (OCGs) that constrain generation at both token and sentence levels. LongGuide automatically selects the best combination of guidelines, improving both strong open- and closed-source LLMs by over 5% in both zero- and few-shot settings. We show that LongGuide is generalizable, learnable by weak models to enhance strong ones, and integrates synergistically with automatic prompt optimizers."

[04.06.2025 16:15] Response: ```python
["LONG_CONTEXT", "OPTIMIZATION"]
```
[04.06.2025 16:15] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper explores the concept of in-context learning (ICL) in large language models (LLMs), highlighting its effectiveness in improving task performance with minimal examples. However, it identifies limitations in long-form generation tasks, such as summarization, where ICL alone does not adequately teach the necessary language and format distributions. The authors propose a solution called LongGuide, which generates guidelines to enhance model performance by providing explicit exposure to task distributions. LongGuide combines Metric Guidelines and Output Constraint Guidelines to optimize generation, resulting in significant performance improvements across various LLMs in both zero- and few-shot scenarios.","title":"Enhancing Long-Form Generation with LongGuide"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper explores the concept of in-context learning (ICL) in large language models (LLMs), highlighting its effectiveness in improving task performance with minimal examples. However, it identifies limitations in long-form generation tasks, such as summarization, where ICL alone does not adequately teach the necessary language and format distributions. The authors propose a solution called LongGuide, which generates guidelines to enhance model performance by providing explicit exposure to task distributions. LongGuide combines Metric Guidelines and Output Constraint Guidelines to optimize generation, resulting in significant performance improvements across various LLMs in both zero- and few-shot scenarios.', title='Enhancing Long-Form Generation with LongGuide'))
[04.06.2025 16:15] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本文探讨了预训练大型语言模型（LLMs）在上下文学习（ICL）中的能力，尤其是在长文本生成任务中的表现。研究表明，仅依靠示例（demonstrations）不足以让模型掌握生成任务的语言和格式分布。为此，提出了LongGuide，通过生成两条并行的指导流，帮助模型更好地理解任务要求。实验结果显示，LongGuide能显著提升模型在零样本和少样本设置下的表现，且具有良好的通用性和学习能力。","title":"提升生成任务性能的LongGuide方法"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本文探讨了预训练大型语言模型（LLMs）在上下文学习（ICL）中的能力，尤其是在长文本生成任务中的表现。研究表明，仅依靠示例（demonstrations）不足以让模型掌握生成任务的语言和格式分布。为此，提出了LongGuide，通过生成两条并行的指导流，帮助模型更好地理解任务要求。实验结果显示，LongGuide能显著提升模型在零样本和少样本设置下的表现，且具有良好的通用性和学习能力。', title='提升生成任务性能的LongGuide方法'))
[04.06.2025 16:15] Using data from previous issue: {"categories": ["#reasoning", "#optimization", "#training", "#rl"], "emoji": "🧠", "ru": {"title": "Раннее предсказание успеха рассуждений в языковых моделях", "desc": "Исследователи изучают возможность предсказания успеха процесса рассуждений с нулевым выстрелом (zero-shot Chain-of-Thought) до его з
[04.06.2025 16:15] Using data from previous issue: {"categories": ["#benchmark", "#video", "#long_context", "#reasoning", "#agents"], "emoji": "🎥", "ru": {"title": "Интеллектуальный агент для глубокого анализа длинных видео", "desc": "Статья представляет агента Deep Video Discovery для анализа длинных видео с использованием автономной стратегии поис
[04.06.2025 16:15] Using data from previous issue: {"categories": ["#training", "#interpretability", "#open_source", "#architecture"], "emoji": "🔍", "ru": {"title": "Улучшение объяснимости трансформеров с учетом позиционного кодирования", "desc": "Статья представляет специализированный метод Layer-wise Relevance Propagation (LRP) для объяснимости мо
[04.06.2025 16:15] Using data from previous issue: {"categories": ["#dataset", "#multimodal", "#benchmark"], "emoji": "👘", "ru": {"title": "Оценка культурного понимания и креативности ИИ через призму традиционной китайской одежды", "desc": "Статья представляет Hanfu-Bench - новый мультимодальный датасет для оценки понимания культурных аспектов модел
[04.06.2025 16:15] Loading Chinese text from previous data.
[04.06.2025 16:15] Renaming data file.
[04.06.2025 16:15] Renaming previous data. hf_papers.json to ./d/2025-06-04.json
[04.06.2025 16:15] Saving new data file.
[04.06.2025 16:15] Generating page.
[04.06.2025 16:15] Renaming previous page.
[04.06.2025 16:15] Renaming previous data. index.html to ./d/2025-06-04.html
[04.06.2025 16:15] [Experimental] Generating Chinese page for reading.
[04.06.2025 16:15] Chinese vocab [{'word': '多模态', 'pinyin': 'duō mó tài', 'trans': 'multimodal'}, {'word': '基准', 'pinyin': 'jī zhǔn', 'trans': 'benchmark'}, {'word': '视觉语言模型', 'pinyin': 'shì jué yǔ yán mó xíng', 'trans': 'vision-language model'}, {'word': '复杂', 'pinyin': 'fù zá', 'trans': 'complex'}, {'word': '多智能体', 'pinyin': 'duō zhì néng tǐ', 'trans': 'multi-agent'}, {'word': '环境', 'pinyin': 'huán jìng', 'trans': 'environment'}, {'word': '策略推理', 'pinyin': 'cè lüè tuī lǐ', 'trans': 'strategic reasoning'}, {'word': '决策', 'pinyin': 'jué cè', 'trans': 'decision-making'}, {'word': '能力', 'pinyin': 'néng lì', 'trans': 'ability'}, {'word': '局限于', 'pinyin': 'jú xiàn yú', 'trans': 'limited to'}, {'word': '单智能体', 'pinyin': 'dān zhì néng tǐ', 'trans': 'single-agent'}, {'word': '仅', 'pinyin': 'jǐn', 'trans': 'only'}, {'word': '文本', 'pinyin': 'wén běn', 'trans': 'text'}, {'word': '涵盖', 'pinyin': 'hán gài', 'trans': 'cover'}, {'word': '合作', 'pinyin': 'hé zuò', 'trans': 'cooperation'}, {'word': '竞争', 'pinyin': 'jìng zhēng', 'trans': 'competition'}, {'word': '混合', 'pinyin': 'hùn hé', 'trans': 'hybrid'}, {'word': '动机', 'pinyin': 'dòng jī', 'trans': 'motivation'}, {'word': '互动', 'pinyin': 'hù dòng', 'trans': 'interaction'}, {'word': '研究', 'pinyin': 'yán jiū', 'trans': 'research'}, {'word': '发现', 'pinyin': 'fā xiàn', 'trans': 'find'}, {'word': '预测', 'pinyin': 'yù cè', 'trans': 'prediction'}, {'word': '准确性', 'pinyin': 'zhǔn què xìng', 'trans': 'accuracy'}, {'word': '归一化', 'pinyin': 'guī yī huà', 'trans': 'normalization'}, {'word': '回报', 'pinyin': 'huí bào', 'trans': 'reward'}, {'word': '方面', 'pinyin': 'fāng miàn', 'trans': 'aspect'}, {'word': '存在', 'pinyin': 'cún zài', 'trans': 'exist'}, {'word': '显著', 'pinyin': 'xiǎn zhù', 'trans': 'significant'}, {'word': '差距', 'pinyin': 'chā jù', 'trans': 'gap'}, {'word': '旨在', 'pinyin': 'zhǐ zài', 'trans': 'aim to'}, {'word': '标准化', 'pinyin': 'biāo zhǔn huà', 'trans': 'standardize'}, {'word': '评估', 'pinyin': 'píng gū', 'trans': 'evaluate'}, {'word': '指出', 'pinyin': 'zhǐ chū', 'trans': 'point out'}, {'word': '局限性', 'pinyin': 'jú xiàn xìng', 'trans': 'limitation'}, {'word': '推动', 'pinyin': 'tuī dòng', 'trans': 'promote'}, {'word': '未来', 'pinyin': 'wèi lái', 'trans': 'future'}, {'word': '代码', 'pinyin': 'dài mǎ', 'trans': 'code'}, {'word': '数据', 'pinyin': 'shù jù', 'trans': 'data'}, {'word': '获取', 'pinyin': 'huò qǔ', 'trans': 'obtain'}]
[04.06.2025 16:15] Renaming previous Chinese page.
[04.06.2025 16:15] Renaming previous data. zh.html to ./d/2025-06-03_zh_reading_task.html
[04.06.2025 16:15] Writing Chinese reading task.
[04.06.2025 16:15] Writing result.
[04.06.2025 16:15] Renaming log file.
[04.06.2025 16:15] Renaming previous data. log.txt to ./logs/2025-06-04_last_log.txt
