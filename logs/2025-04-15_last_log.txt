[15.04.2025 08:18] Read previous papers.
[15.04.2025 08:18] Generating top page (month).
[15.04.2025 08:18] Writing top page (month).
[15.04.2025 09:12] Read previous papers.
[15.04.2025 09:12] Get feed.
[15.04.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.10479
[15.04.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.08791
[15.04.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.09925
[15.04.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.08837
[15.04.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.10068
[15.04.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.08942
[15.04.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.10368
[15.04.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.09710
[15.04.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.08003
[15.04.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.10157
[15.04.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.10415
[15.04.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.09641
[15.04.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.10127
[15.04.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.09689
[15.04.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.10430
[15.04.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.09763
[15.04.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.09130
[15.04.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.10449
[15.04.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.09522
[15.04.2025 09:12] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[15.04.2025 09:12] No deleted papers detected.
[15.04.2025 09:12] Downloading and parsing papers (pdf, html). Total: 19.
[15.04.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2504.10479.
[15.04.2025 09:12] Extra JSON file exists (./assets/json/2504.10479.json), skip PDF parsing.
[15.04.2025 09:12] Paper image links file exists (./assets/img_data/2504.10479.json), skip HTML parsing.
[15.04.2025 09:12] Success.
[15.04.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2504.08791.
[15.04.2025 09:12] Extra JSON file exists (./assets/json/2504.08791.json), skip PDF parsing.
[15.04.2025 09:12] Paper image links file exists (./assets/img_data/2504.08791.json), skip HTML parsing.
[15.04.2025 09:12] Success.
[15.04.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2504.09925.
[15.04.2025 09:12] Extra JSON file exists (./assets/json/2504.09925.json), skip PDF parsing.
[15.04.2025 09:12] Paper image links file exists (./assets/img_data/2504.09925.json), skip HTML parsing.
[15.04.2025 09:12] Success.
[15.04.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2504.08837.
[15.04.2025 09:12] Extra JSON file exists (./assets/json/2504.08837.json), skip PDF parsing.
[15.04.2025 09:12] Paper image links file exists (./assets/img_data/2504.08837.json), skip HTML parsing.
[15.04.2025 09:12] Success.
[15.04.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2504.10068.
[15.04.2025 09:12] Extra JSON file exists (./assets/json/2504.10068.json), skip PDF parsing.
[15.04.2025 09:12] Paper image links file exists (./assets/img_data/2504.10068.json), skip HTML parsing.
[15.04.2025 09:12] Success.
[15.04.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2504.08942.
[15.04.2025 09:12] Extra JSON file exists (./assets/json/2504.08942.json), skip PDF parsing.
[15.04.2025 09:12] Paper image links file exists (./assets/img_data/2504.08942.json), skip HTML parsing.
[15.04.2025 09:12] Success.
[15.04.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2504.10368.
[15.04.2025 09:12] Extra JSON file exists (./assets/json/2504.10368.json), skip PDF parsing.
[15.04.2025 09:12] Paper image links file exists (./assets/img_data/2504.10368.json), skip HTML parsing.
[15.04.2025 09:12] Success.
[15.04.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2504.09710.
[15.04.2025 09:12] Extra JSON file exists (./assets/json/2504.09710.json), skip PDF parsing.
[15.04.2025 09:12] Paper image links file exists (./assets/img_data/2504.09710.json), skip HTML parsing.
[15.04.2025 09:12] Success.
[15.04.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2504.08003.
[15.04.2025 09:12] Extra JSON file exists (./assets/json/2504.08003.json), skip PDF parsing.
[15.04.2025 09:12] Paper image links file exists (./assets/img_data/2504.08003.json), skip HTML parsing.
[15.04.2025 09:12] Success.
[15.04.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2504.10157.
[15.04.2025 09:12] Extra JSON file exists (./assets/json/2504.10157.json), skip PDF parsing.
[15.04.2025 09:12] Paper image links file exists (./assets/img_data/2504.10157.json), skip HTML parsing.
[15.04.2025 09:12] Success.
[15.04.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2504.10415.
[15.04.2025 09:12] Extra JSON file exists (./assets/json/2504.10415.json), skip PDF parsing.
[15.04.2025 09:12] Paper image links file exists (./assets/img_data/2504.10415.json), skip HTML parsing.
[15.04.2025 09:12] Success.
[15.04.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2504.09641.
[15.04.2025 09:12] Extra JSON file exists (./assets/json/2504.09641.json), skip PDF parsing.
[15.04.2025 09:12] Paper image links file exists (./assets/img_data/2504.09641.json), skip HTML parsing.
[15.04.2025 09:12] Success.
[15.04.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2504.10127.
[15.04.2025 09:12] Extra JSON file exists (./assets/json/2504.10127.json), skip PDF parsing.
[15.04.2025 09:12] Paper image links file exists (./assets/img_data/2504.10127.json), skip HTML parsing.
[15.04.2025 09:12] Success.
[15.04.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2504.09689.
[15.04.2025 09:12] Extra JSON file exists (./assets/json/2504.09689.json), skip PDF parsing.
[15.04.2025 09:12] Paper image links file exists (./assets/img_data/2504.09689.json), skip HTML parsing.
[15.04.2025 09:12] Success.
[15.04.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2504.10430.
[15.04.2025 09:12] Extra JSON file exists (./assets/json/2504.10430.json), skip PDF parsing.
[15.04.2025 09:12] Paper image links file exists (./assets/img_data/2504.10430.json), skip HTML parsing.
[15.04.2025 09:12] Success.
[15.04.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2504.09763.
[15.04.2025 09:12] Extra JSON file exists (./assets/json/2504.09763.json), skip PDF parsing.
[15.04.2025 09:12] Paper image links file exists (./assets/img_data/2504.09763.json), skip HTML parsing.
[15.04.2025 09:12] Success.
[15.04.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2504.09130.
[15.04.2025 09:12] Extra JSON file exists (./assets/json/2504.09130.json), skip PDF parsing.
[15.04.2025 09:12] Paper image links file exists (./assets/img_data/2504.09130.json), skip HTML parsing.
[15.04.2025 09:12] Success.
[15.04.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2504.10449.
[15.04.2025 09:12] Extra JSON file exists (./assets/json/2504.10449.json), skip PDF parsing.
[15.04.2025 09:12] Paper image links file exists (./assets/img_data/2504.10449.json), skip HTML parsing.
[15.04.2025 09:12] Success.
[15.04.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2504.09522.
[15.04.2025 09:12] Extra JSON file exists (./assets/json/2504.09522.json), skip PDF parsing.
[15.04.2025 09:12] Paper image links file exists (./assets/img_data/2504.09522.json), skip HTML parsing.
[15.04.2025 09:12] Success.
[15.04.2025 09:12] Enriching papers with extra data.
[15.04.2025 09:12] ********************************************************************************
[15.04.2025 09:12] Abstract 0. We introduce InternVL3, a significant advancement in the InternVL series featuring a native multimodal pre-training paradigm. Rather than adapting a text-only large language model (LLM) into a multimodal large language model (MLLM) that supports visual inputs, InternVL3 jointly acquires multimodal a...
[15.04.2025 09:12] ********************************************************************************
[15.04.2025 09:12] Abstract 1. Emergency of DeepSeek R1 and QwQ 32B have broken through performance barriers for running frontier large language models (LLMs) on home devices. While consumer hardware is getting stronger and model quantization is improving, existing end-side solutions still demand GPU clusters, large RAM/VRAM, and...
[15.04.2025 09:12] ********************************************************************************
[15.04.2025 09:12] Abstract 2. We introduce FUSION, a family of multimodal large language models (MLLMs) with a fully vision-language alignment and integration paradigm. Unlike existing methods that primarily rely on late-stage modality interaction during LLM decoding, our approach achieves deep, dynamic integration throughout th...
[15.04.2025 09:12] ********************************************************************************
[15.04.2025 09:12] Abstract 3. Recently, slow-thinking systems like GPT-o1 and DeepSeek-R1 have demonstrated great potential in solving challenging problems through explicit reflection. They significantly outperform the best fast-thinking models, such as GPT-4o, on various math and science benchmarks. However, their multimodal re...
[15.04.2025 09:12] ********************************************************************************
[15.04.2025 09:12] Abstract 4. Long-context video understanding in multimodal large language models (MLLMs) faces a critical challenge: balancing computational efficiency with the retention of fine-grained spatio-temporal patterns. Existing approaches (e.g., sparse sampling, dense sampling with low resolution, and token compressi...
[15.04.2025 09:12] ********************************************************************************
[15.04.2025 09:12] Abstract 5. Web agents enable users to perform tasks on web browsers through natural language interaction. Evaluating web agents trajectories is an important problem, since it helps us determine whether the agent successfully completed the tasks. Rule-based methods are widely used for this purpose, but they are...
[15.04.2025 09:12] ********************************************************************************
[15.04.2025 09:12] Abstract 6. We introduce S1-Bench, a novel benchmark designed to evaluate Large Reasoning Models' (LRMs) performance on simple tasks that favor intuitive system 1 thinking rather than deliberative system 2 reasoning. While LRMs have achieved significant breakthroughs in complex reasoning tasks through explicit ...
[15.04.2025 09:12] ********************************************************************************
[15.04.2025 09:12] Abstract 7. Recent advances in reinforcement learning (RL)-based post-training have led to notable improvements in large language models (LLMs), particularly in enhancing their reasoning capabilities to handle complex tasks. However, most existing methods treat the training data as a unified whole, overlooking ...
[15.04.2025 09:12] ********************************************************************************
[15.04.2025 09:12] Abstract 8. OpenAI's multimodal GPT-4o has demonstrated remarkable capabilities in image generation and editing, yet its ability to achieve world knowledge-informed semantic synthesis--seamlessly integrating domain knowledge, contextual reasoning, and instruction adherence--remains unproven. In this study, we s...
[15.04.2025 09:12] ********************************************************************************
[15.04.2025 09:12] Abstract 9. Social simulation is transforming traditional social science research by modeling human behavior through interactions between virtual individuals and their environments. With recent advances in large language models (LLMs), this approach has shown growing potential in capturing individual difference...
[15.04.2025 09:12] ********************************************************************************
[15.04.2025 09:12] Abstract 10. Scientific equation discovery is a fundamental task in the history of scientific progress, enabling the derivation of laws governing natural phenomena. Recently, Large Language Models (LLMs) have gained interest for this task due to their potential to leverage embedded scientific knowledge for hypot...
[15.04.2025 09:12] ********************************************************************************
[15.04.2025 09:12] Abstract 11. Recently, improving the reasoning ability of large multimodal models (LMMs) through reinforcement learning has made great progress. However, most existing works are based on highly reasoning-intensive datasets such as mathematics and code, and researchers generally choose large-scale models as the f...
[15.04.2025 09:12] ********************************************************************************
[15.04.2025 09:12] Abstract 12. Graphical User Interface (GUI) agents offer cross-platform solutions for automating complex digital tasks, with significant potential to transform productivity workflows. However, their performance is often constrained by the scarcity of high-quality trajectory data. To address this limitation, we p...
[15.04.2025 09:12] ********************************************************************************
[15.04.2025 09:12] Abstract 13. The rise of LLM-driven AI characters raises safety concerns, particularly for vulnerable human users with psychological disorders. To address these risks, we propose EmoAgent, a multi-agent AI framework designed to evaluate and mitigate mental health hazards in human-AI interactions. EmoAgent compri...
[15.04.2025 09:12] ********************************************************************************
[15.04.2025 09:12] Abstract 14. Recent advancements in Large Language Models (LLMs) have enabled them to approach human-level persuasion capabilities. However, such potential also raises concerns about the safety risks of LLM-driven persuasion, particularly their potential for unethical influence through manipulation, deception, e...
[15.04.2025 09:12] ********************************************************************************
[15.04.2025 09:12] Abstract 15. Scientists often infer abstract procedures from specific instances of problems and use the abstractions to generate new, related instances. For example, programs encoding the formal rules and properties of a system have been useful in fields ranging from RL (procedural environments) to physics (simu...
[15.04.2025 09:12] ********************************************************************************
[15.04.2025 09:12] Abstract 16. Recent advancements in Large Vision-Language Models have showcased remarkable capabilities. However, they often falter when confronted with complex reasoning tasks that humans typically address through visual aids and deliberate, step-by-step thinking. While existing methods have explored text-based...
[15.04.2025 09:12] ********************************************************************************
[15.04.2025 09:12] Abstract 17. Effective reasoning is crucial to solving complex mathematical problems. Recent large language models (LLMs) have boosted performance by scaling test-time computation through long chain-of-thought reasoning. However, transformer-based models are inherently limited in extending context length due to ...
[15.04.2025 09:12] ********************************************************************************
[15.04.2025 09:12] Abstract 18. Large language models learn and continually learn through the accumulation of gradient-based updates, but how individual pieces of new information affect existing knowledge, leading to both beneficial generalization and problematic hallucination, remains poorly understood. We demonstrate that when l...
[15.04.2025 09:12] Read previous papers.
[15.04.2025 09:12] Generating reviews via LLM API.
[15.04.2025 09:12] Using data from previous issue: {"categories": ["#open_source", "#benchmark", "#dataset", "#agi", "#training", "#multimodal"], "emoji": "ğŸ§ ", "ru": {"title": "InternVL3: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹", "desc": "InternVL3 Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLL
[15.04.2025 09:12] Using data from previous issue: {"categories": ["#inference", "#optimization", "#open_source", "#architecture"], "emoji": "ğŸ ", "ru": {"title": "ĞŸÑ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‚ĞµĞ¿ĞµÑ€ÑŒ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹ Ğ½Ğ° Ğ´Ğ¾Ğ¼Ğ°ÑˆĞ½Ğ¸Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ…", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ prima.cpp - Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰ÑƒÑ Ğ·Ğ°Ğ¿ÑƒÑĞºĞ°Ñ‚ÑŒ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹
[15.04.2025 09:12] Using data from previous issue: {"categories": ["#benchmark", "#multimodal", "#open_source", "#dataset"], "emoji": "ğŸ§ ", "ru": {"title": "FUSION: Ğ“Ğ»ÑƒĞ±Ğ¾ĞºĞ°Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ° Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…", "desc": "FUSION - ÑÑ‚Ğ¾ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾
[15.04.2025 09:12] Using data from previous issue: {"categories": ["#reasoning", "#open_source", "#rl", "#math", "#training", "#rlhf", "#optimization", "#multimodal"], "emoji": "ğŸ§ ", "ru": {"title": "ĞŸĞµÑ€ĞµĞ¾ÑĞ¼Ñ‹ÑĞ»ĞµĞ½Ğ¸Ğµ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ°: Ğ¿Ñ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ¼ĞµĞ´Ğ»ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğ¸ Ğ˜Ğ˜", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…
[15.04.2025 09:12] Using data from previous issue: {"categories": ["#reasoning", "#video", "#benchmark", "#architecture", "#long_context", "#multimodal"], "emoji": "ğŸ¥", "ru": {"title": "Mavors: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¼ĞµĞ»ĞºĞ¸Ñ… Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Mavors Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»
[15.04.2025 09:12] Using data from previous issue: {"categories": ["#interpretability", "#agents", "#benchmark", "#optimization"], "emoji": "ğŸ•¸ï¸", "ru": {"title": "AgentRewardBench: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ²ĞµĞ±-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ LLM", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ AgentRewardBench - Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ
[15.04.2025 09:12] Using data from previous issue: {"categories": ["#benchmark", "#reasoning", "#multilingual"], "emoji": "ğŸ§ ", "ru": {"title": "Ğ‘Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½ÑƒĞ¶Ğ´Ğ°ÑÑ‚ÑÑ Ğ² Ğ¸Ğ½Ñ‚ÑƒĞ¸Ñ†Ğ¸Ğ¸", "desc": "S1-Bench - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ‘Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ĞœĞ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (LRM) Ğ½Ğ° Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ¸Ğ½Ñ‚ÑƒĞ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ ÑĞ¸
[15.04.2025 09:12] Using data from previous issue: {"categories": ["#reasoning", "#rl", "#optimization", "#training"], "emoji": "ğŸ§ ", "ru": {"title": "ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ LLM Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ñ€Ğ°Ğ·Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL). ĞĞ²Ñ‚Ğ¾Ñ€
[15.04.2025 09:12] Using data from previous issue: {"categories": ["#benchmark", "#optimization", "#alignment", "#multimodal", "#training", "#reasoning"], "emoji": "ğŸ§ ", "ru": {"title": "GPT-4o: ĞĞ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ² ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğµ Ğ¸ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸", "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ 
[15.04.2025 09:12] Using data from previous issue: {"categories": ["#rl", "#agents", "#social_simulation", "#alignment", "#multimodal"], "emoji": "ğŸŒ", "ru": {"title": "SocioVerse: Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¼Ğ¸Ñ€ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²Ğ°", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ SocioVerse - Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². Ğ¤Ñ€ĞµĞ¹
[15.04.2025 09:12] Using data from previous issue: {"categories": ["#benchmark", "#math", "#reasoning", "#synthetic", "#science"], "emoji": "ğŸ§ª", "ru": {"title": "LLM-SRBench: Ğ’Ñ‹Ğ·Ğ¾Ğ² Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ñƒ Ğ² Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ğ¸ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… ÑƒÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¹", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ LLM-SRBench - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (L
[15.04.2025 09:12] Using data from previous issue: {"categories": ["#reasoning", "#video", "#rl", "#small_models", "#multimodal"], "emoji": "ğŸ¥", "ru": {"title": "ĞœĞ°Ğ»ĞµĞ½ÑŒĞºĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ - Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ: TinyLLaVA-Video-R1 Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ TinyLLaVA-Video-R1 - Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğœ
[15.04.2025 09:12] Using data from previous issue: {"categories": ["#agents", "#dataset", "#training", "#optimization", "#multimodal", "#transfer_learning"], "emoji": "ğŸ¤–", "ru": {"title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ GUI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞº
[15.04.2025 09:12] Using data from previous issue: {"categories": ["#agents", "#security", "#ethics", "#healthcare"], "emoji": "ğŸ§ ", "ru": {"title": "Ğ—Ğ°Ñ‰Ğ¸Ñ‚Ğ° Ğ¿ÑĞ¸Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ·Ğ´Ğ¾Ñ€Ğ¾Ğ²ÑŒÑ Ğ² ÑĞ¿Ğ¾Ñ…Ñƒ Ğ˜Ğ˜-Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ EmoAgent - Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ€Ğ¸ÑĞºĞ¾Ğ² Ğ´Ğ»Ñ Ğ¿ÑĞ¸Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ·Ğ´Ğ¾Ñ€Ğ¾Ğ²ÑŒÑ Ğ¿Ñ€Ğ¸ Ğ²
[15.04.2025 09:12] Using data from previous issue: {"categories": ["#alignment", "#ethics", "#rlhf", "#healthcare", "#multimodal"], "emoji": "ğŸ”", "ru": {"title": "Ğ‘ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² ÑĞ¿Ğ¾Ñ…Ñƒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ¸ÑĞºĞ¾Ğ²", "desc": "Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½
[15.04.2025 09:12] Using data from previous issue: {"categories": ["#reasoning", "#optimization", "#dataset", "#math", "#data", "#training"], "emoji": "ğŸ§®", "ru": {"title": "ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ†Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡", "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ EFA (Executable Functional Abstraction) - Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼Ñ‹Ñ… Ñ„ÑƒĞ½Ğº
[15.04.2025 09:12] Using data from previous issue: {"categories": ["#inference", "#reasoning", "#multimodal", "#cv"], "emoji": "ğŸ§ ", "ru": {"title": "VisuoThink: Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-Ğ²ĞµÑ€Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ˜Ğ˜", "desc": "VisuoThink - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°. ĞĞ½Ğ° 
[15.04.2025 09:12] Using data from previous issue: {"categories": ["#inference", "#architecture", "#reasoning", "#training", "#math", "#long_context"], "emoji": "ğŸ§ ", "ru": {"title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ¾Ğ¹ RNN Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğ¹ RNN, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½
[15.04.2025 09:12] Using data from previous issue: {"categories": ["#architecture", "#optimization", "#dataset", "#training", "#hallucinations", "#long_context", "#data"], "emoji": "ğŸ§ ", "ru": {"title": "ĞšĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ° Ğ¿Ñ€Ğ°Ğ¹Ğ¼Ğ¸Ğ½Ğ³Ğ° Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚ Ğ¿Ñ€Ğ°Ğ¹Ğ¼Ğ¸Ğ½Ğ³Ğ° Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM) Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ 
[15.04.2025 09:12] Trying to get texts in Chinese.
[15.04.2025 09:12] Mistral request. Model: mistral-large-latest. Prompt: Write simple and brief explanation (4-5 sentences) of an article in Chinese. Use short sentences. Text:

We introduce InternVL3, a significant advancement in the InternVL series featuring a native multimodal pre-training paradigm. Rather than adapting a text-only large language model (LLM) into a multimodal large language model (MLLM) that supports visual inputs, InternVL3 jointly acquires multimodal and linguistic capabilities from both diverse multimodal data and pure-text corpora during a single pre-training stage. This unified training paradigm effectively addresses the complexities and alignment challenges commonly encountered in conventional post-hoc training pipelines for MLLMs. To further improve performance and scalability, InternVL3 incorporates variable visual position encoding (V2PE) to support extended multimodal contexts, employs advanced post-training techniques such as supervised fine-tuning (SFT) and mixed preference optimization (MPO), and adopts test-time scaling strategies alongside an optimized training infrastructure. Extensive empirical evaluations demonstrate that InternVL3 delivers superior performance across a wide range of multi-modal tasks. In particular, InternVL3-78B achieves a score of 72.2 on the MMMU benchmark, setting a new state-of-the-art among open-source MLLMs. Its capabilities remain highly competitive with leading proprietary models, including ChatGPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro, while also maintaining strong pure-language proficiency. In pursuit of open-science principles, we will publicly release both the training data and model weights to foster further research and development in next-generation MLLMs.
[15.04.2025 09:12] Mistral response. {"id": "0816274ee0ae469bbd2b749a5df9940c", "object": "chat.completion", "created": 1744708339, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "\u6211\u4eec\u4ecb\u7ecd\u4e86 InternVL3\uff0c\u8fd9\u662f InternVL \u7cfb\u5217\u7684\u91cd\u5927\u8fdb\u6b65\uff0c\u91c7\u7528\u4e86\u672c\u5730\u591a\u6a21\u6001\u9884\u8bad\u7ec3\u8303\u5f0f\u3002\u4e0e\u5176\u4ed6\u65b9\u6cd5\u4e0d\u540c\uff0cInternVL3 \u5728\u5355\u4e2a\u9884\u8bad\u7ec3\u9636\u6bb5\u5185\uff0c\u4ece\u591a\u6a21\u6001\u6570\u636e\u548c\u7eaf\u6587\u672c\u6570\u636e\u4e2d\u540c\u65f6\u83b7\u53d6\u591a\u6a21\u6001\u548c\u8bed\u8a00\u80fd\u529b\u3002\u8fd9\u79cd\u7edf\u4e00\u7684\u8bad\u7ec3\u8303\u5f0f\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u8bad\u7ec3\u6d41\u7a0b\u4e2d\u7684\u590d\u6742\u6027\u548c\u5bf9\u9f50\u6311\u6218\u3002\u4e3a\u4e86\u8fdb\u4e00\u6b65\u63d0\u9ad8\u6027\u80fd\u548c\u53ef\u6269\u5c55\u6027\uff0cInternVL3 \u4f7f\u7528\u4e86\u53ef\u53d8\u89c6\u89c9\u4f4d\u7f6e\u7f16\u7801\u548c\u5148\u8fdb\u7684\u540e\u8bad\u7ec3\u6280\u672f\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cInternVL3 \u5728\u591a\u79cd\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u7279\u522b\u662f InternVL3-78B \u5728 MMMU \u57fa\u51c6\u6d4b\u8bd5\u4e2d\u83b7\u5f97\u4e86 72.2 \u7684\u5206\u6570\uff0c\u521b\u4e0b\u65b0\u7eaa\u5f55\u3002\u6211\u4eec\u5c06\u516c\u5f00\u53d1\u5e03\u8bad\u7ec3\u6570\u636e\u548c\u6a21\u578b\u6743\u91cd\uff0c\u4ee5\u4fc3\u8fdb\u4e0b\u4e00\u4ee3 MLLMs \u7684\u7814\u7a76\u548c\u5f00\u53d1\u3002"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 385, "total_tokens": 643, "completion_tokens": 258}}
[15.04.2025 09:12] Response: æˆ‘ä»¬ä»‹ç»äº† InternVL3ï¼Œè¿™æ˜¯ InternVL ç³»åˆ—çš„é‡å¤§è¿›æ­¥ï¼Œé‡‡ç”¨äº†æœ¬åœ°å¤šæ¨¡æ€é¢„è®­ç»ƒèŒƒå¼ã€‚ä¸å…¶ä»–æ–¹æ³•ä¸åŒï¼ŒInternVL3 åœ¨å•ä¸ªé¢„è®­ç»ƒé˜¶æ®µå†…ï¼Œä»å¤šæ¨¡æ€æ•°æ®å’Œçº¯æ–‡æœ¬æ•°æ®ä¸­åŒæ—¶è·å–å¤šæ¨¡æ€å’Œè¯­è¨€èƒ½åŠ›ã€‚è¿™ç§ç»Ÿä¸€çš„è®­ç»ƒèŒƒå¼æœ‰æ•ˆè§£å†³äº†ä¼ ç»Ÿè®­ç»ƒæµç¨‹ä¸­çš„å¤æ‚æ€§å’Œå¯¹é½æŒ‘æˆ˜ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜æ€§èƒ½å’Œå¯æ‰©å±•æ€§ï¼ŒInternVL3 ä½¿ç”¨äº†å¯å˜è§†è§‰ä½ç½®ç¼–ç å’Œå…ˆè¿›çš„åè®­ç»ƒæŠ€æœ¯ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒInternVL3 åœ¨å¤šç§å¤šæ¨¡æ€ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç‰¹åˆ«æ˜¯ InternVL3-78B åœ¨ MMMU åŸºå‡†æµ‹è¯•ä¸­è·å¾—äº† 72.2 çš„åˆ†æ•°ï¼Œåˆ›ä¸‹æ–°çºªå½•ã€‚æˆ‘ä»¬å°†å…¬å¼€å‘å¸ƒè®­ç»ƒæ•°æ®å’Œæ¨¡å‹æƒé‡ï¼Œä»¥ä¿ƒè¿›ä¸‹ä¸€ä»£ MLLMs çš„ç ”ç©¶å’Œå¼€å‘ã€‚
[15.04.2025 09:12] Mistral request. Model: mistral-large-latest. Prompt: Write pinyin transcription for text. Text:

æˆ‘ä»¬ä»‹ç»äº† InternVL3ï¼Œè¿™æ˜¯ InternVL ç³»åˆ—çš„é‡å¤§è¿›æ­¥ï¼Œé‡‡ç”¨äº†æœ¬åœ°å¤šæ¨¡æ€é¢„è®­ç»ƒèŒƒå¼ã€‚ä¸å…¶ä»–æ–¹æ³•ä¸åŒï¼ŒInternVL3 åœ¨å•ä¸ªé¢„è®­ç»ƒé˜¶æ®µå†…ï¼Œä»å¤šæ¨¡æ€æ•°æ®å’Œçº¯æ–‡æœ¬æ•°æ®ä¸­åŒæ—¶è·å–å¤šæ¨¡æ€å’Œè¯­è¨€èƒ½åŠ›ã€‚è¿™ç§ç»Ÿä¸€çš„è®­ç»ƒèŒƒå¼æœ‰æ•ˆè§£å†³äº†ä¼ ç»Ÿè®­ç»ƒæµç¨‹ä¸­çš„å¤æ‚æ€§å’Œå¯¹é½æŒ‘æˆ˜ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜æ€§èƒ½å’Œå¯æ‰©å±•æ€§ï¼ŒInternVL3 ä½¿ç”¨äº†å¯å˜è§†è§‰ä½ç½®ç¼–ç å’Œå…ˆè¿›çš„åè®­ç»ƒæŠ€æœ¯ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒInternVL3 åœ¨å¤šç§å¤šæ¨¡æ€ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç‰¹åˆ«æ˜¯ InternVL3-78B åœ¨ MMMU åŸºå‡†æµ‹è¯•ä¸­è·å¾—äº† 72.2 çš„åˆ†æ•°ï¼Œåˆ›ä¸‹æ–°çºªå½•ã€‚æˆ‘ä»¬å°†å…¬å¼€å‘å¸ƒè®­ç»ƒæ•°æ®å’Œæ¨¡å‹æƒé‡ï¼Œä»¥ä¿ƒè¿›ä¸‹ä¸€ä»£ MLLMs çš„ç ”ç©¶å’Œå¼€å‘ã€‚
[15.04.2025 09:12] Mistral response. {"id": "d339526fca5e474da0a8126e9fc75083", "object": "chat.completion", "created": 1744708345, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "W\u01d2men ji\u00e8sh\u00e0o le InternVL3, zh\u00e8 sh\u00ec InternVL x\u00ecli\u00e8 de zh\u00f2ngd\u00e0 j\u00ecnb\u00f9, c\u01ceiy\u00f2ng le b\u011bnd\u00ec du\u014dm\u00f3sh\u012b y\u00f9x\u00f9nli\u00e0n f\u00e0nsh\u00ec. Y\u01d4 q\u00edt\u0101 f\u0101ngf\u01ce b\u00f9t\u00f3ng, InternVL3 z\u00e0i d\u0101n g\u00e8 y\u00f9x\u00f9nli\u00e0n ji\u0113du\u00e0n n\u00e8i, c\u00f3ng du\u014dm\u00f3sh\u012b sh\u00f9j\u00f9 h\u00e9 ch\u00fan w\u00e9nb\u011bn sh\u00f9j\u00f9 zh\u014dng t\u00f3ngsh\u00ed hu\u00f2d\u00e9 du\u014dm\u00f3sh\u012b h\u00e9 y\u01d4y\u00e1n n\u00e9ngl\u00ec. Zh\u00e8 zh\u01d2ng t\u01d2ngy\u012b de x\u00f9nli\u00e0n f\u00e0nsh\u00ec y\u01d2uxi\u00e0o ji\u011bju\u00e9 le chu\u00e1nt\u01d2ng x\u00f9nli\u00e0n li\u00fach\u00e9ng zh\u014dng de f\u00f9z\u00e1x\u00ecng h\u00e9 du\u00ecq\u01d0 ti\u01ceozh\u00e0n. W\u00e8ile j\u00ecnf\u016b t\u012bg\u0101o x\u00edngn\u00e9ng h\u00e9 k\u011bku\u00f2zh\u01cen x\u00ecng, InternVL3 sh\u01d0y\u00f2ng le k\u011bbi\u00e0n sh\u00ecju\u00e9 w\u00e8izh\u00ec bi\u0101nm\u01ce h\u00e9 xi\u0101nj\u00ecn de h\u00f2ux\u00f9nli\u00e0n j\u00ecsh\u00f9. Sh\u00edy\u00e0n ji\u00e9gu\u01d2 xi\u01censh\u00ec, InternVL3 z\u00e0i du\u014dzh\u01d2ng du\u014dm\u00f3sh\u012b r\u00e8nw\u00f9 zh\u014dng bi\u01ceoxi\u00e0n y\u014duy\u00f9, t\u00e8bi\u00e9 sh\u00ec InternVL3-78B z\u00e0i MMMU j\u012bzh\u01d4n c\u00e8sh\u00ec zh\u014dng hu\u00f2d\u00e9 le 72.2 de f\u0113nsh\u00f9, chu\u00e0ng xi\u00e0 x\u012bn j\u00ecl\u00f9. W\u01d2men ji\u0101ng g\u014dngk\u0101i f\u0101b\u00f9 x\u00f9nli\u00e0n sh\u00f9j\u00f9 h\u00e9 m\u00f3x\u00edng qu\u00e1nzh\u00f2ng, y\u01d0 c\u00f9j\u00ecn xi\u00e0 y\u012bd\u00e0i MLLMs de y\u00e1nji\u016b h\u00e9 k\u0101if\u0101."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 272, "total_tokens": 780, "completion_tokens": 508}}
[15.04.2025 09:12] Response: WÇ’men jiÃ¨shÃ o le InternVL3, zhÃ¨ shÃ¬ InternVL xÃ¬liÃ¨ de zhÃ²ngdÃ  jÃ¬nbÃ¹, cÇiyÃ²ng le bÄ›ndÃ¬ duÅmÃ³shÄ« yÃ¹xÃ¹nliÃ n fÃ nshÃ¬. YÇ” qÃ­tÄ fÄngfÇ bÃ¹tÃ³ng, InternVL3 zÃ i dÄn gÃ¨ yÃ¹xÃ¹nliÃ n jiÄ“duÃ n nÃ¨i, cÃ³ng duÅmÃ³shÄ« shÃ¹jÃ¹ hÃ© chÃºn wÃ©nbÄ›n shÃ¹jÃ¹ zhÅng tÃ³ngshÃ­ huÃ²dÃ© duÅmÃ³shÄ« hÃ© yÇ”yÃ¡n nÃ©nglÃ¬. ZhÃ¨ zhÇ’ng tÇ’ngyÄ« de xÃ¹nliÃ n fÃ nshÃ¬ yÇ’uxiÃ o jiÄ›juÃ© le chuÃ¡ntÇ’ng xÃ¹nliÃ n liÃºchÃ©ng zhÅng de fÃ¹zÃ¡xÃ¬ng hÃ© duÃ¬qÇ tiÇozhÃ n. WÃ¨ile jÃ¬nfÅ« tÄ«gÄo xÃ­ngnÃ©ng hÃ© kÄ›kuÃ²zhÇn xÃ¬ng, InternVL3 shÇyÃ²ng le kÄ›biÃ n shÃ¬juÃ© wÃ¨izhÃ¬ biÄnmÇ hÃ© xiÄnjÃ¬n de hÃ²uxÃ¹nliÃ n jÃ¬shÃ¹. ShÃ­yÃ n jiÃ©guÇ’ xiÇnshÃ¬, InternVL3 zÃ i duÅzhÇ’ng duÅmÃ³shÄ« rÃ¨nwÃ¹ zhÅng biÇoxiÃ n yÅuyÃ¹, tÃ¨biÃ© shÃ¬ InternVL3-78B zÃ i MMMU jÄ«zhÇ”n cÃ¨shÃ¬ zhÅng huÃ²dÃ© le 72.2 de fÄ“nshÃ¹, chuÃ ng xiÃ  xÄ«n jÃ¬lÃ¹. WÇ’men jiÄng gÅngkÄi fÄbÃ¹ xÃ¹nliÃ n shÃ¹jÃ¹ hÃ© mÃ³xÃ­ng quÃ¡nzhÃ²ng, yÇ cÃ¹jÃ¬n xiÃ  yÄ«dÃ i MLLMs de yÃ¡njiÅ« hÃ© kÄifÄ.
[15.04.2025 09:12] Mistral request. Model: mistral-large-latest. Prompt: Write vocab of difficult words for this text as an array of objects with fields 'word', 'pinyin', 'trans'. Return as python list without formatting. Return list and nothing else. Text:

æˆ‘ä»¬ä»‹ç»äº† InternVL3ï¼Œè¿™æ˜¯ InternVL ç³»åˆ—çš„é‡å¤§è¿›æ­¥ï¼Œé‡‡ç”¨äº†æœ¬åœ°å¤šæ¨¡æ€é¢„è®­ç»ƒèŒƒå¼ã€‚ä¸å…¶ä»–æ–¹æ³•ä¸åŒï¼ŒInternVL3 åœ¨å•ä¸ªé¢„è®­ç»ƒé˜¶æ®µå†…ï¼Œä»å¤šæ¨¡æ€æ•°æ®å’Œçº¯æ–‡æœ¬æ•°æ®ä¸­åŒæ—¶è·å–å¤šæ¨¡æ€å’Œè¯­è¨€èƒ½åŠ›ã€‚è¿™ç§ç»Ÿä¸€çš„è®­ç»ƒèŒƒå¼æœ‰æ•ˆè§£å†³äº†ä¼ ç»Ÿè®­ç»ƒæµç¨‹ä¸­çš„å¤æ‚æ€§å’Œå¯¹é½æŒ‘æˆ˜ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜æ€§èƒ½å’Œå¯æ‰©å±•æ€§ï¼ŒInternVL3 ä½¿ç”¨äº†å¯å˜è§†è§‰ä½ç½®ç¼–ç å’Œå…ˆè¿›çš„åè®­ç»ƒæŠ€æœ¯ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒInternVL3 åœ¨å¤šç§å¤šæ¨¡æ€ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç‰¹åˆ«æ˜¯ InternVL3-78B åœ¨ MMMU åŸºå‡†æµ‹è¯•ä¸­è·å¾—äº† 72.2 çš„åˆ†æ•°ï¼Œåˆ›ä¸‹æ–°çºªå½•ã€‚æˆ‘ä»¬å°†å…¬å¼€å‘å¸ƒè®­ç»ƒæ•°æ®å’Œæ¨¡å‹æƒé‡ï¼Œä»¥ä¿ƒè¿›ä¸‹ä¸€ä»£ MLLMs çš„ç ”ç©¶å’Œå¼€å‘ã€‚
[15.04.2025 09:12] Mistral response. {"id": "f965f34d0c7f4391b04ef8acb1550b00", "object": "chat.completion", "created": 1744708356, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "[{'word': '\u4ecb\u7ecd', 'pinyin': 'ji\u00e8 sh\u00e0o', 'trans': 'introduce'},\n{'word': '\u91cd\u5927', 'pinyin': 'zh\u00f2ng d\u00e0', 'trans': 'major'},\n{'word': '\u8fdb\u6b65', 'pinyin': 'j\u00ecn b\u00f9', 'trans': 'progress'},\n{'word': '\u91c7\u7528', 'pinyin': 'c\u01cei y\u00f2ng', 'trans': 'adopt'},\n{'word': '\u8303\u5f0f', 'pinyin': 'f\u00e0n sh\u00ec', 'trans': 'paradigm'},\n{'word': '\u591a\u6a21\u6001', 'pinyin': 'du\u014d m\u00f3 t\u00e0i', 'trans': 'multimodal'},\n{'word': '\u9884\u8bad\u7ec3', 'pinyin': 'y\u00f9 x\u00f9n li\u00e0n', 'trans': 'pre-training'},\n{'word': '\u9636\u6bb5', 'pinyin': 'ji\u0113 du\u00e0n', 'trans': 'stage'},\n{'word': '\u83b7\u53d6', 'pinyin': 'hu\u00f2 q\u01d4', 'trans': 'obtain'},\n{'word': '\u80fd\u529b', 'pinyin': 'n\u00e9ng l\u00ec', 'trans': 'ability'},\n{'word': '\u7edf\u4e00', 'pinyin': 't\u01d2ng y\u012b', 'trans': 'unified'},\n{'word': '\u89e3\u51b3', 'pinyin': 'ji\u011b ju\u00e9', 'trans': 'solve'},\n{'word': '\u590d\u6742\u6027', 'pinyin': 'f\u00f9 z\u00e1 x\u00ecng', 'trans': 'complexity'},\n{'word': '\u5bf9\u9f50', 'pinyin': 'du\u00ec q\u00ed', 'trans': 'alignment'},\n{'word': '\u6311\u6218', 'pinyin': 'ti\u01ceo zh\u00e0n', 'trans': 'challenge'},\n{'word': '\u53ef\u53d8', 'pinyin': 'k\u011b bi\u00e0n', 'trans': 'variable'},\n{'word': '\u89c6\u89c9', 'pinyin': 'sh\u00ec ju\u00e9', 'trans': 'visual'},\n{'word': '\u4f4d\u7f6e', 'pinyin': 'w\u00e8i zh\u00ec', 'trans': 'position'},\n{'word': '\u7f16\u7801', 'pinyin': 'bi\u0101n m\u01ce', 'trans': 'encoding'},\n{'word': '\u5148\u8fdb', 'pinyin': 'xi\u0101n j\u00ecn', 'trans': 'advanced'},\n{'word': '\u540e\u8bad\u7ec3', 'pinyin': 'h\u00f2u x\u00f9n li\u00e0n', 'trans': 'post-training'},\n{'word': '\u6280\u672f', 'pinyin': 'j\u00ec sh\u00f9', 'trans': 'technology'},\n{'word': '\u8868\u73b0', 'pinyin': 'bi\u01ceo xi\u00e0n', 'trans': 'performance'},\n{'word': '\u4f18\u5f02', 'pinyin': 'y\u014du y\u00ec', 'trans': 'excellent'},\n{'word': '\u7279\u522b', 'pinyin': 't\u00e8 bi\u00e9', 'trans': 'especially'},\n{'word': '\u57fa\u51c6', 'pinyin': 'j\u012b zh\u01d4n', 'trans': 'benchmark'},\n{'word': '\u6d4b\u8bd5', 'pinyin': 'c\u00e8 sh\u00ec', 'trans': 'test'},\n{'word': '\u5206\u6570', 'pinyin': 'f\u0113n sh\u00f9', 'trans': 'score'},\n{'word': '\u7eaa\u5f55', 'pinyin': 'j\u00ec l\u00f9', 'trans': 'record'},\n{'word': '\u516c\u5f00', 'pinyin': 'g\u014dng k\u0101i', 'trans': 'public'},\n{'word': '\u53d1\u5e03', 'pinyin': 'f\u0101 b\u00f9', 'trans': 'release'},\n{'word': '\u6743\u91cd', 'pinyin': 'qu\u00e1n zh\u00f2ng', 'trans': 'weights'},\n{'word': '\u4fc3\u8fdb', 'pinyin': 'c\u00f9 j\u00ecn', 'trans': 'promote'},\n{'word': '\u4e0b\u4e00\u4ee3', 'pinyin': 'xi\u00e0 y\u012b d\u00e0i', 'trans': 'next generation'},\n{'word': '\u7814\u7a76', 'pinyin': 'y\u00e1n ji\u016b', 'trans': 'research'},\n{'word': '\u5f00\u53d1', 'pinyin': 'k\u0101i f\u0101', 'trans': 'development'}]"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 304, "total_tokens": 1292, "completion_tokens": 988}}
[15.04.2025 09:12] Response: [{'word': 'ä»‹ç»', 'pinyin': 'jiÃ¨ shÃ o', 'trans': 'introduce'},
{'word': 'é‡å¤§', 'pinyin': 'zhÃ²ng dÃ ', 'trans': 'major'},
{'word': 'è¿›æ­¥', 'pinyin': 'jÃ¬n bÃ¹', 'trans': 'progress'},
{'word': 'é‡‡ç”¨', 'pinyin': 'cÇi yÃ²ng', 'trans': 'adopt'},
{'word': 'èŒƒå¼', 'pinyin': 'fÃ n shÃ¬', 'trans': 'paradigm'},
{'word': 'å¤šæ¨¡æ€', 'pinyin': 'duÅ mÃ³ tÃ i', 'trans': 'multimodal'},
{'word': 'é¢„è®­ç»ƒ', 'pinyin': 'yÃ¹ xÃ¹n liÃ n', 'trans': 'pre-training'},
{'word': 'é˜¶æ®µ', 'pinyin': 'jiÄ“ duÃ n', 'trans': 'stage'},
{'word': 'è·å–', 'pinyin': 'huÃ² qÇ”', 'trans': 'obtain'},
{'word': 'èƒ½åŠ›', 'pinyin': 'nÃ©ng lÃ¬', 'trans': 'ability'},
{'word': 'ç»Ÿä¸€', 'pinyin': 'tÇ’ng yÄ«', 'trans': 'unified'},
{'word': 'è§£å†³', 'pinyin': 'jiÄ› juÃ©', 'trans': 'solve'},
{'word': 'å¤æ‚æ€§', 'pinyin': 'fÃ¹ zÃ¡ xÃ¬ng', 'trans': 'complexity'},
{'word': 'å¯¹é½', 'pinyin': 'duÃ¬ qÃ­', 'trans': 'alignment'},
{'word': 'æŒ‘æˆ˜', 'pinyin': 'tiÇo zhÃ n', 'trans': 'challenge'},
{'word': 'å¯å˜', 'pinyin': 'kÄ› biÃ n', 'trans': 'variable'},
{'word': 'è§†è§‰', 'pinyin': 'shÃ¬ juÃ©', 'trans': 'visual'},
{'word': 'ä½ç½®', 'pinyin': 'wÃ¨i zhÃ¬', 'trans': 'position'},
{'word': 'ç¼–ç ', 'pinyin': 'biÄn mÇ', 'trans': 'encoding'},
{'word': 'å…ˆè¿›', 'pinyin': 'xiÄn jÃ¬n', 'trans': 'advanced'},
{'word': 'åè®­ç»ƒ', 'pinyin': 'hÃ²u xÃ¹n liÃ n', 'trans': 'post-training'},
{'word': 'æŠ€æœ¯', 'pinyin': 'jÃ¬ shÃ¹', 'trans': 'technology'},
{'word': 'è¡¨ç°', 'pinyin': 'biÇo xiÃ n', 'trans': 'performance'},
{'word': 'ä¼˜å¼‚', 'pinyin': 'yÅu yÃ¬', 'trans': 'excellent'},
{'word': 'ç‰¹åˆ«', 'pinyin': 'tÃ¨ biÃ©', 'trans': 'especially'},
{'word': 'åŸºå‡†', 'pinyin': 'jÄ« zhÇ”n', 'trans': 'benchmark'},
{'word': 'æµ‹è¯•', 'pinyin': 'cÃ¨ shÃ¬', 'trans': 'test'},
{'word': 'åˆ†æ•°', 'pinyin': 'fÄ“n shÃ¹', 'trans': 'score'},
{'word': 'çºªå½•', 'pinyin': 'jÃ¬ lÃ¹', 'trans': 'record'},
{'word': 'å…¬å¼€', 'pinyin': 'gÅng kÄi', 'trans': 'public'},
{'word': 'å‘å¸ƒ', 'pinyin': 'fÄ bÃ¹', 'trans': 'release'},
{'word': 'æƒé‡', 'pinyin': 'quÃ¡n zhÃ²ng', 'trans': 'weights'},
{'word': 'ä¿ƒè¿›', 'pinyin': 'cÃ¹ jÃ¬n', 'trans': 'promote'},
{'word': 'ä¸‹ä¸€ä»£', 'pinyin': 'xiÃ  yÄ« dÃ i', 'trans': 'next generation'},
{'word': 'ç ”ç©¶', 'pinyin': 'yÃ¡n jiÅ«', 'trans': 'research'},
{'word': 'å¼€å‘', 'pinyin': 'kÄi fÄ', 'trans': 'development'}]
[15.04.2025 09:12] Mistral request. Model: mistral-large-latest. Prompt: Translate this text in English. Text:

æˆ‘ä»¬ä»‹ç»äº† InternVL3ï¼Œè¿™æ˜¯ InternVL ç³»åˆ—çš„é‡å¤§è¿›æ­¥ï¼Œé‡‡ç”¨äº†æœ¬åœ°å¤šæ¨¡æ€é¢„è®­ç»ƒèŒƒå¼ã€‚ä¸å…¶ä»–æ–¹æ³•ä¸åŒï¼ŒInternVL3 åœ¨å•ä¸ªé¢„è®­ç»ƒé˜¶æ®µå†…ï¼Œä»å¤šæ¨¡æ€æ•°æ®å’Œçº¯æ–‡æœ¬æ•°æ®ä¸­åŒæ—¶è·å–å¤šæ¨¡æ€å’Œè¯­è¨€èƒ½åŠ›ã€‚è¿™ç§ç»Ÿä¸€çš„è®­ç»ƒèŒƒå¼æœ‰æ•ˆè§£å†³äº†ä¼ ç»Ÿè®­ç»ƒæµç¨‹ä¸­çš„å¤æ‚æ€§å’Œå¯¹é½æŒ‘æˆ˜ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜æ€§èƒ½å’Œå¯æ‰©å±•æ€§ï¼ŒInternVL3 ä½¿ç”¨äº†å¯å˜è§†è§‰ä½ç½®ç¼–ç å’Œå…ˆè¿›çš„åè®­ç»ƒæŠ€æœ¯ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒInternVL3 åœ¨å¤šç§å¤šæ¨¡æ€ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç‰¹åˆ«æ˜¯ InternVL3-78B åœ¨ MMMU åŸºå‡†æµ‹è¯•ä¸­è·å¾—äº† 72.2 çš„åˆ†æ•°ï¼Œåˆ›ä¸‹æ–°çºªå½•ã€‚æˆ‘ä»¬å°†å…¬å¼€å‘å¸ƒè®­ç»ƒæ•°æ®å’Œæ¨¡å‹æƒé‡ï¼Œä»¥ä¿ƒè¿›ä¸‹ä¸€ä»£ MLLMs çš„ç ”ç©¶å’Œå¼€å‘ã€‚
[15.04.2025 09:13] Mistral response. {"id": "689a350fc3a246139d9e01abd829da76", "object": "chat.completion", "created": 1744708376, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "We introduced InternVL3, a significant advancement in the InternVL series, which adopts a local multimodal pretraining paradigm. Unlike other methods, InternVL3 simultaneously acquires multimodal and language capabilities from both multimodal data and pure text data within a single pretraining stage. This unified training paradigm effectively addresses the complexity and alignment challenges of traditional training processes. To further enhance performance and scalability, InternVL3 employs variable visual positional encoding and advanced post-training techniques. Experimental results demonstrate that InternVL3 performs exceptionally well across various multimodal tasks, particularly with InternVL3-78B achieving a score of 72.2 on the MMMU benchmark, setting a new record. We will publicly release the training data and model weights to promote research and development of the next generation of MLLMs."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 270, "total_tokens": 457, "completion_tokens": 187}}
[15.04.2025 09:13] Response: We introduced InternVL3, a significant advancement in the InternVL series, which adopts a local multimodal pretraining paradigm. Unlike other methods, InternVL3 simultaneously acquires multimodal and language capabilities from both multimodal data and pure text data within a single pretraining stage. This unified training paradigm effectively addresses the complexity and alignment challenges of traditional training processes. To further enhance performance and scalability, InternVL3 employs variable visual positional encoding and advanced post-training techniques. Experimental results demonstrate that InternVL3 performs exceptionally well across various multimodal tasks, particularly with InternVL3-78B achieving a score of 72.2 on the MMMU benchmark, setting a new record. We will publicly release the training data and model weights to promote research and development of the next generation of MLLMs.
[15.04.2025 09:13] Renaming data file.
[15.04.2025 09:13] Renaming previous data. hf_papers.json to ./d/2025-04-15.json
[15.04.2025 09:13] Saving new data file.
[15.04.2025 09:13] Generating page.
[15.04.2025 09:13] Renaming previous page.
[15.04.2025 09:13] Renaming previous data. index.html to ./d/2025-04-15.html
[15.04.2025 09:13] [Experimental] Generating Chinese page for reading.
[15.04.2025 09:13] Chinese vocab [{'word': 'ä»‹ç»', 'pinyin': 'jiÃ¨ shÃ o', 'trans': 'introduce'}, {'word': 'é‡å¤§', 'pinyin': 'zhÃ²ng dÃ ', 'trans': 'major'}, {'word': 'è¿›æ­¥', 'pinyin': 'jÃ¬n bÃ¹', 'trans': 'progress'}, {'word': 'é‡‡ç”¨', 'pinyin': 'cÇi yÃ²ng', 'trans': 'adopt'}, {'word': 'èŒƒå¼', 'pinyin': 'fÃ n shÃ¬', 'trans': 'paradigm'}, {'word': 'å¤šæ¨¡æ€', 'pinyin': 'duÅ mÃ³ tÃ i', 'trans': 'multimodal'}, {'word': 'é¢„è®­ç»ƒ', 'pinyin': 'yÃ¹ xÃ¹n liÃ n', 'trans': 'pre-training'}, {'word': 'é˜¶æ®µ', 'pinyin': 'jiÄ“ duÃ n', 'trans': 'stage'}, {'word': 'è·å–', 'pinyin': 'huÃ² qÇ”', 'trans': 'obtain'}, {'word': 'èƒ½åŠ›', 'pinyin': 'nÃ©ng lÃ¬', 'trans': 'ability'}, {'word': 'ç»Ÿä¸€', 'pinyin': 'tÇ’ng yÄ«', 'trans': 'unified'}, {'word': 'è§£å†³', 'pinyin': 'jiÄ› juÃ©', 'trans': 'solve'}, {'word': 'å¤æ‚æ€§', 'pinyin': 'fÃ¹ zÃ¡ xÃ¬ng', 'trans': 'complexity'}, {'word': 'å¯¹é½', 'pinyin': 'duÃ¬ qÃ­', 'trans': 'alignment'}, {'word': 'æŒ‘æˆ˜', 'pinyin': 'tiÇo zhÃ n', 'trans': 'challenge'}, {'word': 'å¯å˜', 'pinyin': 'kÄ› biÃ n', 'trans': 'variable'}, {'word': 'è§†è§‰', 'pinyin': 'shÃ¬ juÃ©', 'trans': 'visual'}, {'word': 'ä½ç½®', 'pinyin': 'wÃ¨i zhÃ¬', 'trans': 'position'}, {'word': 'ç¼–ç ', 'pinyin': 'biÄn mÇ', 'trans': 'encoding'}, {'word': 'å…ˆè¿›', 'pinyin': 'xiÄn jÃ¬n', 'trans': 'advanced'}, {'word': 'åè®­ç»ƒ', 'pinyin': 'hÃ²u xÃ¹n liÃ n', 'trans': 'post-training'}, {'word': 'æŠ€æœ¯', 'pinyin': 'jÃ¬ shÃ¹', 'trans': 'technology'}, {'word': 'è¡¨ç°', 'pinyin': 'biÇo xiÃ n', 'trans': 'performance'}, {'word': 'ä¼˜å¼‚', 'pinyin': 'yÅu yÃ¬', 'trans': 'excellent'}, {'word': 'ç‰¹åˆ«', 'pinyin': 'tÃ¨ biÃ©', 'trans': 'especially'}, {'word': 'åŸºå‡†', 'pinyin': 'jÄ« zhÇ”n', 'trans': 'benchmark'}, {'word': 'æµ‹è¯•', 'pinyin': 'cÃ¨ shÃ¬', 'trans': 'test'}, {'word': 'åˆ†æ•°', 'pinyin': 'fÄ“n shÃ¹', 'trans': 'score'}, {'word': 'çºªå½•', 'pinyin': 'jÃ¬ lÃ¹', 'trans': 'record'}, {'word': 'å…¬å¼€', 'pinyin': 'gÅng kÄi', 'trans': 'public'}, {'word': 'å‘å¸ƒ', 'pinyin': 'fÄ bÃ¹', 'trans': 'release'}, {'word': 'æƒé‡', 'pinyin': 'quÃ¡n zhÃ²ng', 'trans': 'weights'}, {'word': 'ä¿ƒè¿›', 'pinyin': 'cÃ¹ jÃ¬n', 'trans': 'promote'}, {'word': 'ä¸‹ä¸€ä»£', 'pinyin': 'xiÃ  yÄ« dÃ i', 'trans': 'next generation'}, {'word': 'ç ”ç©¶', 'pinyin': 'yÃ¡n jiÅ«', 'trans': 'research'}, {'word': 'å¼€å‘', 'pinyin': 'kÄi fÄ', 'trans': 'development'}]
[15.04.2025 09:13] Renaming previous Chinese page.
[15.04.2025 09:13] Renaming previous data. zh.html to ./d/2025-04-14_zh_reading_task.html
[15.04.2025 09:13] Writing Chinese reading task.
[15.04.2025 09:13] Writing result.
[15.04.2025 09:13] Renaming log file.
[15.04.2025 09:13] Renaming previous data. log.txt to ./logs/2025-04-15_last_log.txt
