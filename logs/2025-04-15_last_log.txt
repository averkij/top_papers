[15.04.2025 04:14] Read previous papers.
[15.04.2025 04:14] Generating top page (month).
[15.04.2025 04:14] Writing top page (month).
[15.04.2025 05:11] Read previous papers.
[15.04.2025 05:11] Get feed.
[15.04.2025 05:11] Get page data from previous paper. URL: https://huggingface.co/papers/2504.08791
[15.04.2025 05:11] Get page data from previous paper. URL: https://huggingface.co/papers/2504.09925
[15.04.2025 05:11] Get page data from previous paper. URL: https://huggingface.co/papers/2504.10068
[15.04.2025 05:11] Get page data from previous paper. URL: https://huggingface.co/papers/2504.08837
[15.04.2025 05:11] Get page data from previous paper. URL: https://huggingface.co/papers/2504.10479
[15.04.2025 05:11] Get page data from previous paper. URL: https://huggingface.co/papers/2504.08942
[15.04.2025 05:11] Get page data from previous paper. URL: https://huggingface.co/papers/2504.08003
[15.04.2025 05:11] Get page data from previous paper. URL: https://huggingface.co/papers/2504.09710
[15.04.2025 05:11] Extract page data from URL. URL: https://huggingface.co/papers/2504.10415
[15.04.2025 05:11] Get page data from previous paper. URL: https://huggingface.co/papers/2504.09641
[15.04.2025 05:11] Get page data from previous paper. URL: https://huggingface.co/papers/2504.10157
[15.04.2025 05:11] Extract page data from URL. URL: https://huggingface.co/papers/2504.10368
[15.04.2025 05:11] Get page data from previous paper. URL: https://huggingface.co/papers/2504.09689
[15.04.2025 05:11] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[15.04.2025 05:11] No deleted papers detected.
[15.04.2025 05:11] Downloading and parsing papers (pdf, html). Total: 13.
[15.04.2025 05:11] Downloading and parsing paper https://huggingface.co/papers/2504.08791.
[15.04.2025 05:11] Extra JSON file exists (./assets/json/2504.08791.json), skip PDF parsing.
[15.04.2025 05:11] Paper image links file exists (./assets/img_data/2504.08791.json), skip HTML parsing.
[15.04.2025 05:11] Success.
[15.04.2025 05:11] Downloading and parsing paper https://huggingface.co/papers/2504.09925.
[15.04.2025 05:11] Extra JSON file exists (./assets/json/2504.09925.json), skip PDF parsing.
[15.04.2025 05:11] Paper image links file exists (./assets/img_data/2504.09925.json), skip HTML parsing.
[15.04.2025 05:11] Success.
[15.04.2025 05:11] Downloading and parsing paper https://huggingface.co/papers/2504.10068.
[15.04.2025 05:11] Extra JSON file exists (./assets/json/2504.10068.json), skip PDF parsing.
[15.04.2025 05:11] Paper image links file exists (./assets/img_data/2504.10068.json), skip HTML parsing.
[15.04.2025 05:11] Success.
[15.04.2025 05:11] Downloading and parsing paper https://huggingface.co/papers/2504.08837.
[15.04.2025 05:11] Extra JSON file exists (./assets/json/2504.08837.json), skip PDF parsing.
[15.04.2025 05:11] Paper image links file exists (./assets/img_data/2504.08837.json), skip HTML parsing.
[15.04.2025 05:11] Success.
[15.04.2025 05:11] Downloading and parsing paper https://huggingface.co/papers/2504.10479.
[15.04.2025 05:11] Extra JSON file exists (./assets/json/2504.10479.json), skip PDF parsing.
[15.04.2025 05:11] Paper image links file exists (./assets/img_data/2504.10479.json), skip HTML parsing.
[15.04.2025 05:11] Success.
[15.04.2025 05:11] Downloading and parsing paper https://huggingface.co/papers/2504.08942.
[15.04.2025 05:11] Extra JSON file exists (./assets/json/2504.08942.json), skip PDF parsing.
[15.04.2025 05:11] Paper image links file exists (./assets/img_data/2504.08942.json), skip HTML parsing.
[15.04.2025 05:11] Success.
[15.04.2025 05:11] Downloading and parsing paper https://huggingface.co/papers/2504.08003.
[15.04.2025 05:11] Extra JSON file exists (./assets/json/2504.08003.json), skip PDF parsing.
[15.04.2025 05:11] Paper image links file exists (./assets/img_data/2504.08003.json), skip HTML parsing.
[15.04.2025 05:11] Success.
[15.04.2025 05:11] Downloading and parsing paper https://huggingface.co/papers/2504.09710.
[15.04.2025 05:11] Extra JSON file exists (./assets/json/2504.09710.json), skip PDF parsing.
[15.04.2025 05:11] Paper image links file exists (./assets/img_data/2504.09710.json), skip HTML parsing.
[15.04.2025 05:11] Success.
[15.04.2025 05:11] Downloading and parsing paper https://huggingface.co/papers/2504.10415.
[15.04.2025 05:11] Downloading paper 2504.10415 from http://arxiv.org/pdf/2504.10415v1...
[15.04.2025 05:12] Extracting affiliations from text.
[15.04.2025 05:12] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"LLM-SRBench: New Benchmark for Scientific Equation Discovery with Large Language Models Parshin Shojaee 1 Ngoc-Hieu Nguyen 2 Kazem Meidani 3 Amir Barati Farimani 3 Khoa Doan 2 Chandan Reddy 1 Website: https://github.com/deep-symbolic-mathematics/llm-srbench 5 2 0 2 4 1 ] . [ 1 5 1 4 0 1 . 4 0 5 2 : r a "
[15.04.2025 05:12] Response: ```python
[]
```
[15.04.2025 05:12] Extracting affiliations from text.
[15.04.2025 05:12] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"LLM-SRBench: New Benchmark for Scientific Equation Discovery with Large Language Models Parshin Shojaee 1 Ngoc-Hieu Nguyen 2 Kazem Meidani 3 Amir Barati Farimani 3 Khoa Doan 2 Chandan Reddy 1 Website: https://github.com/deep-symbolic-mathematics/llm-srbench 5 2 0 2 4 1 ] . [ 1 5 1 4 0 1 . 4 0 5 2 : r aScientific equation discovery is fundamental task in the history of scientific progress, enabling the derivation of laws governing natural phenomena. Recently, Large Language Models (LLMs) have gained interest for this task due to their potential to leverage embedded scientific knowledge for hypothesis generation. However, evaluating the true discovery capabilities of these methods remains challenging, as existing benchmarks often rely on common equations that are susceptible to memorization by LLMs, leading to inflated performance metrics that do not reflect discovery. In this paper, we introduce LLM-SRBench, comprehensive benchmark with 239 challenging problems across four scientific domains specifically designed to evaluate LLM-based scientific equation discovery methods while preventing trivial memorization. Our benchmark comprises two main categories: LSR-Transform, which transforms common physical models into less common mathematical representations to test reasoning beyond memorized forms, and LSR-Synth, which introduces synthetic, discovery-driven problems requiring data-driven reasoning. Through extensive evaluation of several state-of-the-art methods, using both open and closed LLMs, we find that the best-performing system so far achieves only 31.5% symbolic accuracy. These findings highlight the challenges of scientific equation discovery, positioning LLM-SRBench as valuable resource for future research. *Equal contribution 1Virginia Tech 2VinUniversity 3Carnegie Mellon University. Correspondence to: Parshin Shojaee <parshinshojaee@vt.edu>. Proceedings of the 42 nd International Conference on Machine Learning, Vancouver, Canada. PMLR 267, 2025. Copyright 2025 by the author(s). 1 Figure 1. Error analysis comparing simple LLM sampling (Llama3.1-8B) on 100 Feynman problems versus LLM-SRBench datasets (LSR-Transform and LSR-Synth). The sharp drops in numeric error curves and considerably lower symbolic error for Feynman problems suggest memorization rather than gradual discovery. 1. Introduction Equation discovery, the process of uncovering symbolic mathematical expressions from observational data, has been cornerstone of scientific advancement. This task, also known as symbolic regression (SR), goes beyond mere datadriven predictive modeling by seeking interpretable mathematical relations that reveal the underlying mechanisms of natural phenomena. When scientists derive mathematical equations from empirical data, they gain more than just predictive power they obtain insights into fundamental physical principles, enable extrapolation beyond observed data, and facilitate knowledge transfer across scientific domains (Langley, 1981; Schmidt & Lipson, 2009). Standard approaches to equation discovery have primarily relied on genetic programming (GP) and evolutionary algorithms (Cranmer, 2023; La Cava et al., 2021), which represent mathematical expressions as trees and navigate the vast space of possible equations through evolutionary search techniques. However, these methods face two fundamental challenges. First, the NP-hard nature of equation discovery (Virgolin & Pissis, 2022) makes their random mutation and crossover operations computationally prohibitive across vast search spaces. Second, unlike human scientists who LLM-SRBench: New Benchmark for Scientific Equation Discovery with Large Language Models Figure 2. Overview of the LLM-based Scientific Equation Discovery. The benchmark tasks (left) combine scientific context with numerical data. The discovery process (middle) iteratively leverages LLMs scientific knowledge and data-driven reasoning to generate hypotheses for underlying equations. Discovered hypotheses, represented as equation strings, trees, or programs, are then evaluated (right) using multiple metrics including data fidelity, symbolic accuracy, and computational efficiency. leverage their domain knowledge and expertise to guide hypothesis formation, these approaches are mostly purely data-driven, and isolated from existing scientific knowledge. These limitations have motivated researchers to develop methods that incorporate scientific domain knowledge into the equation discovery process. Large Language Models (LLMs) have recently emerged as promising solution to these challenges, offering new paradigm for scientific equation discovery. LLMs, trained on vast corpora of scientific literature, possess extensive embedded scientific knowledge. This has sparked significant interest in leveraging LLMs for scientific equation discovery, with several recent works demonstrating their potential (Shojaee et al., 2024b; Ma et al., 2024; Grayeli et al., 2024; Merler et al., 2024; Du et al., 2024; Reddy & Shojaee, 2024; Zhang et al., 2024). These LLM-based approaches have shown to enhance the equation hypothesis generation process by incorporating scientific priors, guiding the exploration of equation search spaces more efficiently, and providing interpretable reasoning for the search process. Despite the promising potential of LLM-based equation discovery methods, their rigorous and robust evaluation still remains an open challenge. The current scientific equation discovery benchmarks are primarily represented by SRBench (La Cava et al., 2021) and SRSD (Matsubara et al., 2022). SRBench incorporates two key data groups for this purpose: the Feynman physics equations (Udrescu & Tegmark, 2020), and Strogatz dynamical systems (La Cava et al., 2016; Strogatz, 2018). notable extension to this framework is SRSD (Matsubara et al., 2022), which enhances the Feynman benchmark by incorporating physically meaningful sampling ranges for data points. However, these benchmarks exhibit significant limitations for the evaluation of LLM-based methods. Their problems are mostly based on known physics equations from textbooks, which makes them often subject to memorization by LLMs. As noted by (Shojaee et al., 2024b), LLMs frequently succeed on these common equation discovery benchmarks through simple recitation based on variable names and problem descriptions, rather than the actual process of datadriven discovery and reasoning. Our analysis (shown in Fig. 1) also confirms this finding - the"
[15.04.2025 05:12] Mistral response. {"id": "1efed6d553574ef1a0f0e0201eff0823", "object": "chat.completion", "created": 1744693923, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "```python\n['Virginia Tech', 'VinUniversity', 'Carnegie Mellon University']\n```"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1564, "total_tokens": 1593, "completion_tokens": 29}}
[15.04.2025 05:12] Response: ```python
['Virginia Tech', 'VinUniversity', 'Carnegie Mellon University']
```
[15.04.2025 05:12] Deleting PDF ./assets/pdf/2504.10415.pdf.
[15.04.2025 05:12] Success.
[15.04.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2504.09641.
[15.04.2025 05:12] Extra JSON file exists (./assets/json/2504.09641.json), skip PDF parsing.
[15.04.2025 05:12] Paper image links file exists (./assets/img_data/2504.09641.json), skip HTML parsing.
[15.04.2025 05:12] Success.
[15.04.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2504.10157.
[15.04.2025 05:12] Extra JSON file exists (./assets/json/2504.10157.json), skip PDF parsing.
[15.04.2025 05:12] Paper image links file exists (./assets/img_data/2504.10157.json), skip HTML parsing.
[15.04.2025 05:12] Success.
[15.04.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2504.10368.
[15.04.2025 05:12] Extra JSON file exists (./assets/json/2504.10368.json), skip PDF parsing.
[15.04.2025 05:12] Paper image links file exists (./assets/img_data/2504.10368.json), skip HTML parsing.
[15.04.2025 05:12] Success.
[15.04.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2504.09689.
[15.04.2025 05:12] Extra JSON file exists (./assets/json/2504.09689.json), skip PDF parsing.
[15.04.2025 05:12] Paper image links file exists (./assets/img_data/2504.09689.json), skip HTML parsing.
[15.04.2025 05:12] Success.
[15.04.2025 05:12] Enriching papers with extra data.
[15.04.2025 05:12] ********************************************************************************
[15.04.2025 05:12] Abstract 0. Emergency of DeepSeek R1 and QwQ 32B have broken through performance barriers for running frontier large language models (LLMs) on home devices. While consumer hardware is getting stronger and model quantization is improving, existing end-side solutions still demand GPU clusters, large RAM/VRAM, and...
[15.04.2025 05:12] ********************************************************************************
[15.04.2025 05:12] Abstract 1. We introduce FUSION, a family of multimodal large language models (MLLMs) with a fully vision-language alignment and integration paradigm. Unlike existing methods that primarily rely on late-stage modality interaction during LLM decoding, our approach achieves deep, dynamic integration throughout th...
[15.04.2025 05:12] ********************************************************************************
[15.04.2025 05:12] Abstract 2. Long-context video understanding in multimodal large language models (MLLMs) faces a critical challenge: balancing computational efficiency with the retention of fine-grained spatio-temporal patterns. Existing approaches (e.g., sparse sampling, dense sampling with low resolution, and token compressi...
[15.04.2025 05:12] ********************************************************************************
[15.04.2025 05:12] Abstract 3. Recently, slow-thinking systems like GPT-o1 and DeepSeek-R1 have demonstrated great potential in solving challenging problems through explicit reflection. They significantly outperform the best fast-thinking models, such as GPT-4o, on various math and science benchmarks. However, their multimodal re...
[15.04.2025 05:12] ********************************************************************************
[15.04.2025 05:12] Abstract 4. We introduce InternVL3, a significant advancement in the InternVL series featuring a native multimodal pre-training paradigm. Rather than adapting a text-only large language model (LLM) into a multimodal large language model (MLLM) that supports visual inputs, InternVL3 jointly acquires multimodal a...
[15.04.2025 05:12] ********************************************************************************
[15.04.2025 05:12] Abstract 5. Web agents enable users to perform tasks on web browsers through natural language interaction. Evaluating web agents trajectories is an important problem, since it helps us determine whether the agent successfully completed the tasks. Rule-based methods are widely used for this purpose, but they are...
[15.04.2025 05:12] ********************************************************************************
[15.04.2025 05:12] Abstract 6. OpenAI's multimodal GPT-4o has demonstrated remarkable capabilities in image generation and editing, yet its ability to achieve world knowledge-informed semantic synthesis--seamlessly integrating domain knowledge, contextual reasoning, and instruction adherence--remains unproven. In this study, we s...
[15.04.2025 05:12] ********************************************************************************
[15.04.2025 05:12] Abstract 7. Recent advances in reinforcement learning (RL)-based post-training have led to notable improvements in large language models (LLMs), particularly in enhancing their reasoning capabilities to handle complex tasks. However, most existing methods treat the training data as a unified whole, overlooking ...
[15.04.2025 05:12] ********************************************************************************
[15.04.2025 05:12] Abstract 8. Scientific equation discovery is a fundamental task in the history of scientific progress, enabling the derivation of laws governing natural phenomena. Recently, Large Language Models (LLMs) have gained interest for this task due to their potential to leverage embedded scientific knowledge for hypot...
[15.04.2025 05:12] ********************************************************************************
[15.04.2025 05:12] Abstract 9. Recently, improving the reasoning ability of large multimodal models (LMMs) through reinforcement learning has made great progress. However, most existing works are based on highly reasoning-intensive datasets such as mathematics and code, and researchers generally choose large-scale models as the f...
[15.04.2025 05:12] ********************************************************************************
[15.04.2025 05:12] Abstract 10. Social simulation is transforming traditional social science research by modeling human behavior through interactions between virtual individuals and their environments. With recent advances in large language models (LLMs), this approach has shown growing potential in capturing individual difference...
[15.04.2025 05:12] ********************************************************************************
[15.04.2025 05:12] Abstract 11. We introduce S1-Bench, a novel benchmark designed to evaluate Large Reasoning Models' (LRMs) performance on simple tasks that favor intuitive system 1 thinking rather than deliberative system 2 reasoning. While LRMs have achieved significant breakthroughs in complex reasoning tasks through explicit ...
[15.04.2025 05:12] ********************************************************************************
[15.04.2025 05:12] Abstract 12. The rise of LLM-driven AI characters raises safety concerns, particularly for vulnerable human users with psychological disorders. To address these risks, we propose EmoAgent, a multi-agent AI framework designed to evaluate and mitigate mental health hazards in human-AI interactions. EmoAgent compri...
[15.04.2025 05:12] Read previous papers.
[15.04.2025 05:12] Generating reviews via LLM API.
[15.04.2025 05:12] Using data from previous issue: {"categories": ["#inference", "#optimization", "#open_source", "#architecture"], "emoji": "🏠", "ru": {"title": "Продвинутые языковые модели теперь доступны на домашних устройствах", "desc": "Статья представляет prima.cpp - распределенную систему вывода, позволяющую запускать крупномасштабные языковы
[15.04.2025 05:12] Using data from previous issue: {"categories": ["#benchmark", "#multimodal", "#open_source", "#dataset"], "emoji": "🧠", "ru": {"title": "FUSION: Глубокая интеграция зрения и языка в мультимодальных ИИ-моделях", "desc": "FUSION - это семейство мультимодальных больших языковых моделей с полной интеграцией зрения и языка. Модель испо
[15.04.2025 05:12] Using data from previous issue: {"categories": ["#reasoning", "#video", "#benchmark", "#architecture", "#long_context", "#multimodal"], "emoji": "🎥", "ru": {"title": "Mavors: Эффективное понимание длинных видео с сохранением мелких деталей", "desc": "Статья представляет новый подход Mavors для понимания длинных видео в мультимодал
[15.04.2025 05:12] Using data from previous issue: {"categories": ["#reasoning", "#open_source", "#rl", "#math", "#training", "#rlhf", "#optimization", "#multimodal"], "emoji": "🧠", "ru": {"title": "Переосмысление зрения и языка: прорыв в медленном мышлении ИИ", "desc": "Статья описывает новый подход к улучшению мультимодальных рассуждений в моделях
[15.04.2025 05:12] Using data from previous issue: {"categories": ["#open_source", "#benchmark", "#dataset", "#agi", "#training", "#multimodal"], "emoji": "🧠", "ru": {"title": "InternVL3: Революция в мультимодальном обучении языковых моделей", "desc": "InternVL3 представляет собой значительный прогресс в области мультимодальных языковых моделей (MLL
[15.04.2025 05:12] Using data from previous issue: {"categories": ["#interpretability", "#agents", "#benchmark", "#optimization"], "emoji": "🕸️", "ru": {"title": "AgentRewardBench: новый подход к оценке веб-агентов с помощью LLM", "desc": "Статья представляет AgentRewardBench - первый бенчмарк для оценки эффективности языковых моделей (LLM) в оценке
[15.04.2025 05:12] Using data from previous issue: {"categories": ["#benchmark", "#optimization", "#alignment", "#multimodal", "#training", "#reasoning"], "emoji": "🧠", "ru": {"title": "GPT-4o: Ограничения в семантическом синтезе и необходимость улучшения мультимодальной генерации", "desc": "Исследование оценивает способности мультимодальной модели 
[15.04.2025 05:12] Using data from previous issue: {"categories": ["#reasoning", "#rl", "#optimization", "#training"], "emoji": "🧠", "ru": {"title": "Адаптивное постобучение LLM с учетом разнородности данных", "desc": "Статья представляет новый подход к постобучению больших языковых моделей (LLM) с использованием обучения с подкреплением (RL). Автор
[15.04.2025 05:12] Querying the API.
[15.04.2025 05:12] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Scientific equation discovery is a fundamental task in the history of scientific progress, enabling the derivation of laws governing natural phenomena. Recently, Large Language Models (LLMs) have gained interest for this task due to their potential to leverage embedded scientific knowledge for hypothesis generation. However, evaluating the true discovery capabilities of these methods remains challenging, as existing benchmarks often rely on common equations that are susceptible to memorization by LLMs, leading to inflated performance metrics that do not reflect discovery. In this paper, we introduce LLM-SRBench, a comprehensive benchmark with 239 challenging problems across four scientific domains specifically designed to evaluate LLM-based scientific equation discovery methods while preventing trivial memorization. Our benchmark comprises two main categories: LSR-Transform, which transforms common physical models into less common mathematical representations to test reasoning beyond memorized forms, and LSR-Synth, which introduces synthetic, discovery-driven problems requiring data-driven reasoning. Through extensive evaluation of several state-of-the-art methods, using both open and closed LLMs, we find that the best-performing system so far achieves only 31.5% symbolic accuracy. These findings highlight the challenges of scientific equation discovery, positioning LLM-SRBench as a valuable resource for future research.
[15.04.2025 05:12] Response: {
  "desc": "Статья представляет LLM-SRBench - новый набор данных для оценки способностей больших языковых моделей (LLM) к открытию научных уравнений. Бенчмарк содержит 239 сложных задач из четырех научных областей, специально разработанных для предотвращения простого запоминания уравнений моделями. LLM-SRBench включает две категории: LSR-Transform с трансформированными физическими моделями и LSR-Synth с синтетическими задачами, требующими рассуждений на основе данных. Тестирование современных методов показало, что лучшая система достигает лишь 31.5% символьной точности, подчеркивая сложность задачи открытия научных уравнений.",
  "emoji": "🧪",
  "title": "LLM-SRBench: Вызов искусственному интеллекту в открытии научных уравнений"
}
[15.04.2025 05:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Scientific equation discovery is a fundamental task in the history of scientific progress, enabling the derivation of laws governing natural phenomena. Recently, Large Language Models (LLMs) have gained interest for this task due to their potential to leverage embedded scientific knowledge for hypothesis generation. However, evaluating the true discovery capabilities of these methods remains challenging, as existing benchmarks often rely on common equations that are susceptible to memorization by LLMs, leading to inflated performance metrics that do not reflect discovery. In this paper, we introduce LLM-SRBench, a comprehensive benchmark with 239 challenging problems across four scientific domains specifically designed to evaluate LLM-based scientific equation discovery methods while preventing trivial memorization. Our benchmark comprises two main categories: LSR-Transform, which transforms common physical models into less common mathematical representations to test reasoning beyond memorized forms, and LSR-Synth, which introduces synthetic, discovery-driven problems requiring data-driven reasoning. Through extensive evaluation of several state-of-the-art methods, using both open and closed LLMs, we find that the best-performing system so far achieves only 31.5% symbolic accuracy. These findings highlight the challenges of scientific equation discovery, positioning LLM-SRBench as a valuable resource for future research."

[15.04.2025 05:12] Response: ```python
['BENCHMARK', 'MATH']
```
[15.04.2025 05:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Scientific equation discovery is a fundamental task in the history of scientific progress, enabling the derivation of laws governing natural phenomena. Recently, Large Language Models (LLMs) have gained interest for this task due to their potential to leverage embedded scientific knowledge for hypothesis generation. However, evaluating the true discovery capabilities of these methods remains challenging, as existing benchmarks often rely on common equations that are susceptible to memorization by LLMs, leading to inflated performance metrics that do not reflect discovery. In this paper, we introduce LLM-SRBench, a comprehensive benchmark with 239 challenging problems across four scientific domains specifically designed to evaluate LLM-based scientific equation discovery methods while preventing trivial memorization. Our benchmark comprises two main categories: LSR-Transform, which transforms common physical models into less common mathematical representations to test reasoning beyond memorized forms, and LSR-Synth, which introduces synthetic, discovery-driven problems requiring data-driven reasoning. Through extensive evaluation of several state-of-the-art methods, using both open and closed LLMs, we find that the best-performing system so far achieves only 31.5% symbolic accuracy. These findings highlight the challenges of scientific equation discovery, positioning LLM-SRBench as a valuable resource for future research."

[15.04.2025 05:12] Response: ```python
['SCIENCE', 'SYNTHETIC', 'REASONING']
```
[15.04.2025 05:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper discusses the challenges of using Large Language Models (LLMs) for discovering scientific equations, which are essential for understanding natural laws. The authors introduce LLM-SRBench, a new benchmark with 239 difficult problems across four scientific fields, designed to assess the true discovery capabilities of LLMs without the influence of memorization. The benchmark includes two categories: LSR-Transform, which tests reasoning by altering common models, and LSR-Synth, which presents synthetic problems that require data-driven reasoning. The evaluation shows that even the best LLMs achieve only 31.5% accuracy, underscoring the difficulties in scientific equation discovery and the importance of LLM-SRBench for future studies.","title":"LLM-SRBench: A New Frontier in Scientific Equation Discovery"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper discusses the challenges of using Large Language Models (LLMs) for discovering scientific equations, which are essential for understanding natural laws. The authors introduce LLM-SRBench, a new benchmark with 239 difficult problems across four scientific fields, designed to assess the true discovery capabilities of LLMs without the influence of memorization. The benchmark includes two categories: LSR-Transform, which tests reasoning by altering common models, and LSR-Synth, which presents synthetic problems that require data-driven reasoning. The evaluation shows that even the best LLMs achieve only 31.5% accuracy, underscoring the difficulties in scientific equation discovery and the importance of LLM-SRBench for future studies.', title='LLM-SRBench: A New Frontier in Scientific Equation Discovery'))
[15.04.2025 05:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"科学方程发现是科学进步中的一项基本任务，能够推导出自然现象的规律。最近，大型语言模型（LLMs）因其利用嵌入科学知识进行假设生成的潜力而受到关注。然而，评估这些方法的真实发现能力仍然具有挑战性，因为现有基准往往依赖于容易被LLMs记忆的常见方程，导致性能指标被夸大。本文介绍了LLM-SRBench，这是一个包含239个具有挑战性问题的综合基准，旨在评估基于LLM的科学方程发现方法，同时防止简单的记忆化。","title":"科学方程发现的新基准：LLM-SRBench"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='科学方程发现是科学进步中的一项基本任务，能够推导出自然现象的规律。最近，大型语言模型（LLMs）因其利用嵌入科学知识进行假设生成的潜力而受到关注。然而，评估这些方法的真实发现能力仍然具有挑战性，因为现有基准往往依赖于容易被LLMs记忆的常见方程，导致性能指标被夸大。本文介绍了LLM-SRBench，这是一个包含239个具有挑战性问题的综合基准，旨在评估基于LLM的科学方程发现方法，同时防止简单的记忆化。', title='科学方程发现的新基准：LLM-SRBench'))
[15.04.2025 05:12] Using data from previous issue: {"categories": ["#reasoning", "#video", "#rl", "#small_models", "#multimodal"], "emoji": "🎥", "ru": {"title": "Маленькая модель - большие рассуждения: TinyLLaVA-Video-R1 для анализа видео", "desc": "Статья представляет TinyLLaVA-Video-R1 - небольшую мультимодальную модель для рассуждений по видео. М
[15.04.2025 05:12] Using data from previous issue: {"categories": ["#rl", "#agents", "#social_simulation", "#alignment", "#multimodal"], "emoji": "🌐", "ru": {"title": "SocioVerse: виртуальный мир для моделирования общества", "desc": "Статья представляет SocioVerse - модель социальной симуляции на основе больших языковых моделей (LLM) и агентов. Фрей
[15.04.2025 05:12] Querying the API.
[15.04.2025 05:12] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

We introduce S1-Bench, a novel benchmark designed to evaluate Large Reasoning Models' (LRMs) performance on simple tasks that favor intuitive system 1 thinking rather than deliberative system 2 reasoning. While LRMs have achieved significant breakthroughs in complex reasoning tasks through explicit chains of thought, their reliance on deep analytical thinking may limit their system 1 thinking capabilities. Moreover, a lack of benchmark currently exists to evaluate LRMs' performance in tasks that require such capabilities. To fill this gap, S1-Bench presents a set of simple, diverse, and naturally clear questions across multiple domains and languages, specifically designed to assess LRMs' performance in such tasks. Our comprehensive evaluation of 22 LRMs reveals significant lower efficiency tendencies, with outputs averaging 15.5 times longer than those of traditional small LLMs. Additionally, LRMs often identify correct answers early but continue unnecessary deliberation, with some models even producing numerous errors. These findings highlight the rigid reasoning patterns of current LRMs and underscore the substantial development needed to achieve balanced dual-system thinking capabilities that can adapt appropriately to task complexity.
[15.04.2025 05:12] Response: {
  "desc": "S1-Bench - это новый бенчмарк для оценки производительности Больших Моделей Рассуждений (LRM) на простых задачах, требующих интуитивного мышления системы 1. Исследование показало, что LRM демонстрируют низкую эффективность на таких задачах, выдавая ответы в среднем в 15,5 раз длиннее, чем традиционные малые языковые модели. Модели часто находят правильные ответы рано, но продолжают ненужные рассуждения, иногда допуская множество ошибок. Результаты указывают на необходимость развития сбалансированных возможностей двойственного мышления в LRM.",
  "emoji": "🧠",
  "title": "Большие модели рассуждений нуждаются в интуиции"
}
[15.04.2025 05:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"We introduce S1-Bench, a novel benchmark designed to evaluate Large Reasoning Models' (LRMs) performance on simple tasks that favor intuitive system 1 thinking rather than deliberative system 2 reasoning. While LRMs have achieved significant breakthroughs in complex reasoning tasks through explicit chains of thought, their reliance on deep analytical thinking may limit their system 1 thinking capabilities. Moreover, a lack of benchmark currently exists to evaluate LRMs' performance in tasks that require such capabilities. To fill this gap, S1-Bench presents a set of simple, diverse, and naturally clear questions across multiple domains and languages, specifically designed to assess LRMs' performance in such tasks. Our comprehensive evaluation of 22 LRMs reveals significant lower efficiency tendencies, with outputs averaging 15.5 times longer than those of traditional small LLMs. Additionally, LRMs often identify correct answers early but continue unnecessary deliberation, with some models even producing numerous errors. These findings highlight the rigid reasoning patterns of current LRMs and underscore the substantial development needed to achieve balanced dual-system thinking capabilities that can adapt appropriately to task complexity."

[15.04.2025 05:12] Response: ```python
["BENCHMARK", "MULTILINGUAL"]
```
[15.04.2025 05:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"We introduce S1-Bench, a novel benchmark designed to evaluate Large Reasoning Models' (LRMs) performance on simple tasks that favor intuitive system 1 thinking rather than deliberative system 2 reasoning. While LRMs have achieved significant breakthroughs in complex reasoning tasks through explicit chains of thought, their reliance on deep analytical thinking may limit their system 1 thinking capabilities. Moreover, a lack of benchmark currently exists to evaluate LRMs' performance in tasks that require such capabilities. To fill this gap, S1-Bench presents a set of simple, diverse, and naturally clear questions across multiple domains and languages, specifically designed to assess LRMs' performance in such tasks. Our comprehensive evaluation of 22 LRMs reveals significant lower efficiency tendencies, with outputs averaging 15.5 times longer than those of traditional small LLMs. Additionally, LRMs often identify correct answers early but continue unnecessary deliberation, with some models even producing numerous errors. These findings highlight the rigid reasoning patterns of current LRMs and underscore the substantial development needed to achieve balanced dual-system thinking capabilities that can adapt appropriately to task complexity."

[15.04.2025 05:12] Response: ```python
["REASONING"]
```
[15.04.2025 05:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The paper introduces S1-Bench, a benchmark aimed at assessing Large Reasoning Models (LRMs) on tasks that require quick, intuitive responses, akin to system 1 thinking. It highlights that while LRMs excel in complex reasoning, they struggle with simpler tasks that demand rapid decision-making. The study evaluates 22 LRMs, revealing that they often produce longer outputs and unnecessary deliberation, even when they identify correct answers early. These results indicate that current LRMs exhibit rigid reasoning patterns, suggesting a need for further development to enhance their ability to balance intuitive and analytical thinking.","title":"Evaluating Intuition: S1-Bench for Large Reasoning Models"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='The paper introduces S1-Bench, a benchmark aimed at assessing Large Reasoning Models (LRMs) on tasks that require quick, intuitive responses, akin to system 1 thinking. It highlights that while LRMs excel in complex reasoning, they struggle with simpler tasks that demand rapid decision-making. The study evaluates 22 LRMs, revealing that they often produce longer outputs and unnecessary deliberation, even when they identify correct answers early. These results indicate that current LRMs exhibit rigid reasoning patterns, suggesting a need for further development to enhance their ability to balance intuitive and analytical thinking.', title='Evaluating Intuition: S1-Bench for Large Reasoning Models'))
[15.04.2025 05:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"我们介绍了S1-Bench，这是一个新颖的基准测试，旨在评估大型推理模型（LRMs）在简单任务上的表现，这些任务更倾向于直观的系统1思维，而非深思熟虑的系统2推理。尽管LRMs在复杂推理任务中取得了显著突破，但它们对深度分析思维的依赖可能限制了其系统1思维能力。目前缺乏评估LRMs在需要这种能力的任务表现的基准。S1-Bench提供了一组简单、多样且自然清晰的问题，专门设计用于评估LRMs在这些任务中的表现。","title":"评估大型推理模型的直观思维能力"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='我们介绍了S1-Bench，这是一个新颖的基准测试，旨在评估大型推理模型（LRMs）在简单任务上的表现，这些任务更倾向于直观的系统1思维，而非深思熟虑的系统2推理。尽管LRMs在复杂推理任务中取得了显著突破，但它们对深度分析思维的依赖可能限制了其系统1思维能力。目前缺乏评估LRMs在需要这种能力的任务表现的基准。S1-Bench提供了一组简单、多样且自然清晰的问题，专门设计用于评估LRMs在这些任务中的表现。', title='评估大型推理模型的直观思维能力'))
[15.04.2025 05:12] Using data from previous issue: {"categories": ["#agents", "#security", "#ethics", "#healthcare"], "emoji": "🧠", "ru": {"title": "Защита психического здоровья в эпоху ИИ-персонажей", "desc": "Статья представляет EmoAgent - мультиагентную систему искусственного интеллекта для оценки и снижения рисков для психического здоровья при в
[15.04.2025 05:12] Loading Chinese text from previous data.
[15.04.2025 05:12] Renaming data file.
[15.04.2025 05:12] Renaming previous data. hf_papers.json to ./d/2025-04-15.json
[15.04.2025 05:12] Saving new data file.
[15.04.2025 05:12] Generating page.
[15.04.2025 05:12] Renaming previous page.
[15.04.2025 05:12] Renaming previous data. index.html to ./d/2025-04-15.html
[15.04.2025 05:12] [Experimental] Generating Chinese page for reading.
[15.04.2025 05:12] Chinese vocab [{'word': '技术报告', 'pinyin': 'jìshù bàogào', 'trans': 'technical report'}, {'word': '视频生成', 'pinyin': 'shìpín shēngchéng', 'trans': 'video generation'}, {'word': '基础模型', 'pinyin': 'jīchǔ móxíng', 'trans': 'foundational model'}, {'word': '成本效益', 'pinyin': 'chéngběn xiàoyì', 'trans': 'cost-effectiveness'}, {'word': '策略', 'pinyin': 'cèlüè', 'trans': 'strategy'}, {'word': '中型', 'pinyin': 'zhōngxíng', 'trans': 'medium-sized'}, {'word': '研究模型', 'pinyin': 'yánjiū móxíng', 'trans': 'research model'}, {'word': '参数', 'pinyin': 'cānshǔ', 'trans': 'parameters'}, {'word': '从头训练', 'pinyin': 'cóngtóu xùnliàn', 'trans': 'train from scratch'}, {'word': '计算资源', 'pinyin': 'jìsuàn zīyuán', 'trans': 'computational resources'}, {'word': '适中', 'pinyin': 'shìzhōng', 'trans': 'moderate'}, {'word': '媲美', 'pinyin': 'pìměi', 'trans': 'rival'}, {'word': '设计选择', 'pinyin': 'shèjì xuǎnzé', 'trans': 'design choices'}, {'word': '受限', 'pinyin': 'shòuxiàn', 'trans': 'constrained'}, {'word': '环境', 'pinyin': 'huánjìng', 'trans': 'environment'}, {'word': '尤为', 'pinyin': 'yóuwéi', 'trans': 'especially'}, {'word': '重要', 'pinyin': 'zhòngyào', 'trans': 'important'}, {'word': '强调', 'pinyin': 'qiángdiào', 'trans': 'emphasize'}, {'word': '提升', 'pinyin': 'tíshēng', 'trans': 'enhance'}, {'word': '扩散模型', 'pinyin': 'kuòsàn móxíng', 'trans': 'diffusion model'}, {'word': '性能', 'pinyin': 'xíngnéng', 'trans': 'performance'}, {'word': '关键', 'pinyin': 'guǎnjiàn', 'trans': 'key'}, {'word': '设计决策', 'pinyin': 'shèjì juécè', 'trans': 'design decisions'}, {'word': '经验', 'pinyin': 'jīngyàn', 'trans': 'experience'}, {'word': '观察结果', 'pinyin': 'guānchá jiéguǒ', 'trans': 'observation results'}, {'word': '泛化能力', 'pinyin': 'fànhuà nénglì', 'trans': 'generalization capability'}, {'word': '轻量微调', 'pinyin': 'qīngliàng wēitiáo', 'trans': 'lightweight fine-tuning'}, {'word': '继续训练', 'pinyin': 'jìxù xùnliàn', 'trans': 'continued training'}, {'word': '下游应用', 'pinyin': 'xiàyóu yìngyòng', 'trans': 'downstream applications'}, {'word': '项目页面', 'pinyin': 'xiàngmù yèmiàn', 'trans': 'project page'}]
[15.04.2025 05:12] Renaming previous Chinese page.
[15.04.2025 05:12] Renaming previous data. zh.html to ./d/2025-04-14_zh_reading_task.html
[15.04.2025 05:12] Writing Chinese reading task.
[15.04.2025 05:12] Writing result.
[15.04.2025 05:12] Renaming log file.
[15.04.2025 05:12] Renaming previous data. log.txt to ./logs/2025-04-15_last_log.txt
