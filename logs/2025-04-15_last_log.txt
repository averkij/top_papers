[15.04.2025 08:18] Read previous papers.
[15.04.2025 08:18] Generating top page (month).
[15.04.2025 08:18] Writing top page (month).
[15.04.2025 09:12] Read previous papers.
[15.04.2025 09:12] Get feed.
[15.04.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.10479
[15.04.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.08791
[15.04.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.09925
[15.04.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.08837
[15.04.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.10068
[15.04.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.08942
[15.04.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.10368
[15.04.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.09710
[15.04.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.08003
[15.04.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.10157
[15.04.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.10415
[15.04.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.09641
[15.04.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.10127
[15.04.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.09689
[15.04.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.10430
[15.04.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.09763
[15.04.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.09130
[15.04.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.10449
[15.04.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.09522
[15.04.2025 09:12] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[15.04.2025 09:12] No deleted papers detected.
[15.04.2025 09:12] Downloading and parsing papers (pdf, html). Total: 19.
[15.04.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2504.10479.
[15.04.2025 09:12] Extra JSON file exists (./assets/json/2504.10479.json), skip PDF parsing.
[15.04.2025 09:12] Paper image links file exists (./assets/img_data/2504.10479.json), skip HTML parsing.
[15.04.2025 09:12] Success.
[15.04.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2504.08791.
[15.04.2025 09:12] Extra JSON file exists (./assets/json/2504.08791.json), skip PDF parsing.
[15.04.2025 09:12] Paper image links file exists (./assets/img_data/2504.08791.json), skip HTML parsing.
[15.04.2025 09:12] Success.
[15.04.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2504.09925.
[15.04.2025 09:12] Extra JSON file exists (./assets/json/2504.09925.json), skip PDF parsing.
[15.04.2025 09:12] Paper image links file exists (./assets/img_data/2504.09925.json), skip HTML parsing.
[15.04.2025 09:12] Success.
[15.04.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2504.08837.
[15.04.2025 09:12] Extra JSON file exists (./assets/json/2504.08837.json), skip PDF parsing.
[15.04.2025 09:12] Paper image links file exists (./assets/img_data/2504.08837.json), skip HTML parsing.
[15.04.2025 09:12] Success.
[15.04.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2504.10068.
[15.04.2025 09:12] Extra JSON file exists (./assets/json/2504.10068.json), skip PDF parsing.
[15.04.2025 09:12] Paper image links file exists (./assets/img_data/2504.10068.json), skip HTML parsing.
[15.04.2025 09:12] Success.
[15.04.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2504.08942.
[15.04.2025 09:12] Extra JSON file exists (./assets/json/2504.08942.json), skip PDF parsing.
[15.04.2025 09:12] Paper image links file exists (./assets/img_data/2504.08942.json), skip HTML parsing.
[15.04.2025 09:12] Success.
[15.04.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2504.10368.
[15.04.2025 09:12] Extra JSON file exists (./assets/json/2504.10368.json), skip PDF parsing.
[15.04.2025 09:12] Paper image links file exists (./assets/img_data/2504.10368.json), skip HTML parsing.
[15.04.2025 09:12] Success.
[15.04.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2504.09710.
[15.04.2025 09:12] Extra JSON file exists (./assets/json/2504.09710.json), skip PDF parsing.
[15.04.2025 09:12] Paper image links file exists (./assets/img_data/2504.09710.json), skip HTML parsing.
[15.04.2025 09:12] Success.
[15.04.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2504.08003.
[15.04.2025 09:12] Extra JSON file exists (./assets/json/2504.08003.json), skip PDF parsing.
[15.04.2025 09:12] Paper image links file exists (./assets/img_data/2504.08003.json), skip HTML parsing.
[15.04.2025 09:12] Success.
[15.04.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2504.10157.
[15.04.2025 09:12] Extra JSON file exists (./assets/json/2504.10157.json), skip PDF parsing.
[15.04.2025 09:12] Paper image links file exists (./assets/img_data/2504.10157.json), skip HTML parsing.
[15.04.2025 09:12] Success.
[15.04.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2504.10415.
[15.04.2025 09:12] Extra JSON file exists (./assets/json/2504.10415.json), skip PDF parsing.
[15.04.2025 09:12] Paper image links file exists (./assets/img_data/2504.10415.json), skip HTML parsing.
[15.04.2025 09:12] Success.
[15.04.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2504.09641.
[15.04.2025 09:12] Extra JSON file exists (./assets/json/2504.09641.json), skip PDF parsing.
[15.04.2025 09:12] Paper image links file exists (./assets/img_data/2504.09641.json), skip HTML parsing.
[15.04.2025 09:12] Success.
[15.04.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2504.10127.
[15.04.2025 09:12] Extra JSON file exists (./assets/json/2504.10127.json), skip PDF parsing.
[15.04.2025 09:12] Paper image links file exists (./assets/img_data/2504.10127.json), skip HTML parsing.
[15.04.2025 09:12] Success.
[15.04.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2504.09689.
[15.04.2025 09:12] Extra JSON file exists (./assets/json/2504.09689.json), skip PDF parsing.
[15.04.2025 09:12] Paper image links file exists (./assets/img_data/2504.09689.json), skip HTML parsing.
[15.04.2025 09:12] Success.
[15.04.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2504.10430.
[15.04.2025 09:12] Extra JSON file exists (./assets/json/2504.10430.json), skip PDF parsing.
[15.04.2025 09:12] Paper image links file exists (./assets/img_data/2504.10430.json), skip HTML parsing.
[15.04.2025 09:12] Success.
[15.04.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2504.09763.
[15.04.2025 09:12] Extra JSON file exists (./assets/json/2504.09763.json), skip PDF parsing.
[15.04.2025 09:12] Paper image links file exists (./assets/img_data/2504.09763.json), skip HTML parsing.
[15.04.2025 09:12] Success.
[15.04.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2504.09130.
[15.04.2025 09:12] Extra JSON file exists (./assets/json/2504.09130.json), skip PDF parsing.
[15.04.2025 09:12] Paper image links file exists (./assets/img_data/2504.09130.json), skip HTML parsing.
[15.04.2025 09:12] Success.
[15.04.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2504.10449.
[15.04.2025 09:12] Extra JSON file exists (./assets/json/2504.10449.json), skip PDF parsing.
[15.04.2025 09:12] Paper image links file exists (./assets/img_data/2504.10449.json), skip HTML parsing.
[15.04.2025 09:12] Success.
[15.04.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2504.09522.
[15.04.2025 09:12] Extra JSON file exists (./assets/json/2504.09522.json), skip PDF parsing.
[15.04.2025 09:12] Paper image links file exists (./assets/img_data/2504.09522.json), skip HTML parsing.
[15.04.2025 09:12] Success.
[15.04.2025 09:12] Enriching papers with extra data.
[15.04.2025 09:12] ********************************************************************************
[15.04.2025 09:12] Abstract 0. We introduce InternVL3, a significant advancement in the InternVL series featuring a native multimodal pre-training paradigm. Rather than adapting a text-only large language model (LLM) into a multimodal large language model (MLLM) that supports visual inputs, InternVL3 jointly acquires multimodal a...
[15.04.2025 09:12] ********************************************************************************
[15.04.2025 09:12] Abstract 1. Emergency of DeepSeek R1 and QwQ 32B have broken through performance barriers for running frontier large language models (LLMs) on home devices. While consumer hardware is getting stronger and model quantization is improving, existing end-side solutions still demand GPU clusters, large RAM/VRAM, and...
[15.04.2025 09:12] ********************************************************************************
[15.04.2025 09:12] Abstract 2. We introduce FUSION, a family of multimodal large language models (MLLMs) with a fully vision-language alignment and integration paradigm. Unlike existing methods that primarily rely on late-stage modality interaction during LLM decoding, our approach achieves deep, dynamic integration throughout th...
[15.04.2025 09:12] ********************************************************************************
[15.04.2025 09:12] Abstract 3. Recently, slow-thinking systems like GPT-o1 and DeepSeek-R1 have demonstrated great potential in solving challenging problems through explicit reflection. They significantly outperform the best fast-thinking models, such as GPT-4o, on various math and science benchmarks. However, their multimodal re...
[15.04.2025 09:12] ********************************************************************************
[15.04.2025 09:12] Abstract 4. Long-context video understanding in multimodal large language models (MLLMs) faces a critical challenge: balancing computational efficiency with the retention of fine-grained spatio-temporal patterns. Existing approaches (e.g., sparse sampling, dense sampling with low resolution, and token compressi...
[15.04.2025 09:12] ********************************************************************************
[15.04.2025 09:12] Abstract 5. Web agents enable users to perform tasks on web browsers through natural language interaction. Evaluating web agents trajectories is an important problem, since it helps us determine whether the agent successfully completed the tasks. Rule-based methods are widely used for this purpose, but they are...
[15.04.2025 09:12] ********************************************************************************
[15.04.2025 09:12] Abstract 6. We introduce S1-Bench, a novel benchmark designed to evaluate Large Reasoning Models' (LRMs) performance on simple tasks that favor intuitive system 1 thinking rather than deliberative system 2 reasoning. While LRMs have achieved significant breakthroughs in complex reasoning tasks through explicit ...
[15.04.2025 09:12] ********************************************************************************
[15.04.2025 09:12] Abstract 7. Recent advances in reinforcement learning (RL)-based post-training have led to notable improvements in large language models (LLMs), particularly in enhancing their reasoning capabilities to handle complex tasks. However, most existing methods treat the training data as a unified whole, overlooking ...
[15.04.2025 09:12] ********************************************************************************
[15.04.2025 09:12] Abstract 8. OpenAI's multimodal GPT-4o has demonstrated remarkable capabilities in image generation and editing, yet its ability to achieve world knowledge-informed semantic synthesis--seamlessly integrating domain knowledge, contextual reasoning, and instruction adherence--remains unproven. In this study, we s...
[15.04.2025 09:12] ********************************************************************************
[15.04.2025 09:12] Abstract 9. Social simulation is transforming traditional social science research by modeling human behavior through interactions between virtual individuals and their environments. With recent advances in large language models (LLMs), this approach has shown growing potential in capturing individual difference...
[15.04.2025 09:12] ********************************************************************************
[15.04.2025 09:12] Abstract 10. Scientific equation discovery is a fundamental task in the history of scientific progress, enabling the derivation of laws governing natural phenomena. Recently, Large Language Models (LLMs) have gained interest for this task due to their potential to leverage embedded scientific knowledge for hypot...
[15.04.2025 09:12] ********************************************************************************
[15.04.2025 09:12] Abstract 11. Recently, improving the reasoning ability of large multimodal models (LMMs) through reinforcement learning has made great progress. However, most existing works are based on highly reasoning-intensive datasets such as mathematics and code, and researchers generally choose large-scale models as the f...
[15.04.2025 09:12] ********************************************************************************
[15.04.2025 09:12] Abstract 12. Graphical User Interface (GUI) agents offer cross-platform solutions for automating complex digital tasks, with significant potential to transform productivity workflows. However, their performance is often constrained by the scarcity of high-quality trajectory data. To address this limitation, we p...
[15.04.2025 09:12] ********************************************************************************
[15.04.2025 09:12] Abstract 13. The rise of LLM-driven AI characters raises safety concerns, particularly for vulnerable human users with psychological disorders. To address these risks, we propose EmoAgent, a multi-agent AI framework designed to evaluate and mitigate mental health hazards in human-AI interactions. EmoAgent compri...
[15.04.2025 09:12] ********************************************************************************
[15.04.2025 09:12] Abstract 14. Recent advancements in Large Language Models (LLMs) have enabled them to approach human-level persuasion capabilities. However, such potential also raises concerns about the safety risks of LLM-driven persuasion, particularly their potential for unethical influence through manipulation, deception, e...
[15.04.2025 09:12] ********************************************************************************
[15.04.2025 09:12] Abstract 15. Scientists often infer abstract procedures from specific instances of problems and use the abstractions to generate new, related instances. For example, programs encoding the formal rules and properties of a system have been useful in fields ranging from RL (procedural environments) to physics (simu...
[15.04.2025 09:12] ********************************************************************************
[15.04.2025 09:12] Abstract 16. Recent advancements in Large Vision-Language Models have showcased remarkable capabilities. However, they often falter when confronted with complex reasoning tasks that humans typically address through visual aids and deliberate, step-by-step thinking. While existing methods have explored text-based...
[15.04.2025 09:12] ********************************************************************************
[15.04.2025 09:12] Abstract 17. Effective reasoning is crucial to solving complex mathematical problems. Recent large language models (LLMs) have boosted performance by scaling test-time computation through long chain-of-thought reasoning. However, transformer-based models are inherently limited in extending context length due to ...
[15.04.2025 09:12] ********************************************************************************
[15.04.2025 09:12] Abstract 18. Large language models learn and continually learn through the accumulation of gradient-based updates, but how individual pieces of new information affect existing knowledge, leading to both beneficial generalization and problematic hallucination, remains poorly understood. We demonstrate that when l...
[15.04.2025 09:12] Read previous papers.
[15.04.2025 09:12] Generating reviews via LLM API.
[15.04.2025 09:12] Using data from previous issue: {"categories": ["#open_source", "#benchmark", "#dataset", "#agi", "#training", "#multimodal"], "emoji": "🧠", "ru": {"title": "InternVL3: Революция в мультимодальном обучении языковых моделей", "desc": "InternVL3 представляет собой значительный прогресс в области мультимодальных языковых моделей (MLL
[15.04.2025 09:12] Using data from previous issue: {"categories": ["#inference", "#optimization", "#open_source", "#architecture"], "emoji": "🏠", "ru": {"title": "Продвинутые языковые модели теперь доступны на домашних устройствах", "desc": "Статья представляет prima.cpp - распределенную систему вывода, позволяющую запускать крупномасштабные языковы
[15.04.2025 09:12] Using data from previous issue: {"categories": ["#benchmark", "#multimodal", "#open_source", "#dataset"], "emoji": "🧠", "ru": {"title": "FUSION: Глубокая интеграция зрения и языка в мультимодальных ИИ-моделях", "desc": "FUSION - это семейство мультимодальных больших языковых моделей с полной интеграцией зрения и языка. Модель испо
[15.04.2025 09:12] Using data from previous issue: {"categories": ["#reasoning", "#open_source", "#rl", "#math", "#training", "#rlhf", "#optimization", "#multimodal"], "emoji": "🧠", "ru": {"title": "Переосмысление зрения и языка: прорыв в медленном мышлении ИИ", "desc": "Статья описывает новый подход к улучшению мультимодальных рассуждений в моделях
[15.04.2025 09:12] Using data from previous issue: {"categories": ["#reasoning", "#video", "#benchmark", "#architecture", "#long_context", "#multimodal"], "emoji": "🎥", "ru": {"title": "Mavors: Эффективное понимание длинных видео с сохранением мелких деталей", "desc": "Статья представляет новый подход Mavors для понимания длинных видео в мультимодал
[15.04.2025 09:12] Using data from previous issue: {"categories": ["#interpretability", "#agents", "#benchmark", "#optimization"], "emoji": "🕸️", "ru": {"title": "AgentRewardBench: новый подход к оценке веб-агентов с помощью LLM", "desc": "Статья представляет AgentRewardBench - первый бенчмарк для оценки эффективности языковых моделей (LLM) в оценке
[15.04.2025 09:12] Using data from previous issue: {"categories": ["#benchmark", "#reasoning", "#multilingual"], "emoji": "🧠", "ru": {"title": "Большие модели рассуждений нуждаются в интуиции", "desc": "S1-Bench - это новый бенчмарк для оценки производительности Больших Моделей Рассуждений (LRM) на простых задачах, требующих интуитивного мышления си
[15.04.2025 09:12] Using data from previous issue: {"categories": ["#reasoning", "#rl", "#optimization", "#training"], "emoji": "🧠", "ru": {"title": "Адаптивное постобучение LLM с учетом разнородности данных", "desc": "Статья представляет новый подход к постобучению больших языковых моделей (LLM) с использованием обучения с подкреплением (RL). Автор
[15.04.2025 09:12] Using data from previous issue: {"categories": ["#benchmark", "#optimization", "#alignment", "#multimodal", "#training", "#reasoning"], "emoji": "🧠", "ru": {"title": "GPT-4o: Ограничения в семантическом синтезе и необходимость улучшения мультимодальной генерации", "desc": "Исследование оценивает способности мультимодальной модели 
[15.04.2025 09:12] Using data from previous issue: {"categories": ["#rl", "#agents", "#social_simulation", "#alignment", "#multimodal"], "emoji": "🌐", "ru": {"title": "SocioVerse: виртуальный мир для моделирования общества", "desc": "Статья представляет SocioVerse - модель социальной симуляции на основе больших языковых моделей (LLM) и агентов. Фрей
[15.04.2025 09:12] Using data from previous issue: {"categories": ["#benchmark", "#math", "#reasoning", "#synthetic", "#science"], "emoji": "🧪", "ru": {"title": "LLM-SRBench: Вызов искусственному интеллекту в открытии научных уравнений", "desc": "Статья представляет LLM-SRBench - новый набор данных для оценки способностей больших языковых моделей (L
[15.04.2025 09:12] Using data from previous issue: {"categories": ["#reasoning", "#video", "#rl", "#small_models", "#multimodal"], "emoji": "🎥", "ru": {"title": "Маленькая модель - большие рассуждения: TinyLLaVA-Video-R1 для анализа видео", "desc": "Статья представляет TinyLLaVA-Video-R1 - небольшую мультимодальную модель для рассуждений по видео. М
[15.04.2025 09:12] Using data from previous issue: {"categories": ["#agents", "#dataset", "#training", "#optimization", "#multimodal", "#transfer_learning"], "emoji": "🤖", "ru": {"title": "Улучшение GUI-агентов через обучение на разнообразных задачах рассуждения", "desc": "Статья предлагает новый подход к обучению агентов графического пользовательск
[15.04.2025 09:12] Using data from previous issue: {"categories": ["#agents", "#security", "#ethics", "#healthcare"], "emoji": "🧠", "ru": {"title": "Защита психического здоровья в эпоху ИИ-персонажей", "desc": "Статья представляет EmoAgent - мультиагентную систему искусственного интеллекта для оценки и снижения рисков для психического здоровья при в
[15.04.2025 09:12] Using data from previous issue: {"categories": ["#alignment", "#ethics", "#rlhf", "#healthcare", "#multimodal"], "emoji": "🔍", "ru": {"title": "Безопасность убеждения в эпоху языковых моделей: выявление этических рисков", "desc": "Это исследование посвящено анализу безопасности крупных языковых моделей (LLM) в контексте их способн
[15.04.2025 09:12] Using data from previous issue: {"categories": ["#reasoning", "#optimization", "#dataset", "#math", "#data", "#training"], "emoji": "🧮", "ru": {"title": "Автоматическая генерация абстракций для продвинутых математических задач", "desc": "Исследователи представили концепцию EFA (Executable Functional Abstraction) - исполняемых функ
[15.04.2025 09:12] Using data from previous issue: {"categories": ["#inference", "#reasoning", "#multimodal", "#cv"], "emoji": "🧠", "ru": {"title": "VisuoThink: Визуально-вербальное рассуждение для ИИ", "desc": "VisuoThink - это новая система, которая объединяет визуальное и языковое мышление для улучшения рассуждений искусственного интеллекта. Она 
[15.04.2025 09:12] Using data from previous issue: {"categories": ["#inference", "#architecture", "#reasoning", "#training", "#math", "#long_context"], "emoji": "🧠", "ru": {"title": "Эффективное масштабирование рассуждений с помощью гибридной RNN модели", "desc": "Статья представляет новую гибридную модель рассуждений на основе линейной RNN, названн
[15.04.2025 09:12] Using data from previous issue: {"categories": ["#architecture", "#optimization", "#dataset", "#training", "#hallucinations", "#long_context", "#data"], "emoji": "🧠", "ru": {"title": "Контроль эффекта прайминга при обучении языковых моделей", "desc": "Статья исследует эффект прайминга в больших языковых моделях (LLM) при обучении 
[15.04.2025 09:12] Trying to get texts in Chinese.
[15.04.2025 09:12] Mistral request. Model: mistral-large-latest. Prompt: Write simple and brief explanation (4-5 sentences) of an article in Chinese. Use short sentences. Text:

We introduce InternVL3, a significant advancement in the InternVL series featuring a native multimodal pre-training paradigm. Rather than adapting a text-only large language model (LLM) into a multimodal large language model (MLLM) that supports visual inputs, InternVL3 jointly acquires multimodal and linguistic capabilities from both diverse multimodal data and pure-text corpora during a single pre-training stage. This unified training paradigm effectively addresses the complexities and alignment challenges commonly encountered in conventional post-hoc training pipelines for MLLMs. To further improve performance and scalability, InternVL3 incorporates variable visual position encoding (V2PE) to support extended multimodal contexts, employs advanced post-training techniques such as supervised fine-tuning (SFT) and mixed preference optimization (MPO), and adopts test-time scaling strategies alongside an optimized training infrastructure. Extensive empirical evaluations demonstrate that InternVL3 delivers superior performance across a wide range of multi-modal tasks. In particular, InternVL3-78B achieves a score of 72.2 on the MMMU benchmark, setting a new state-of-the-art among open-source MLLMs. Its capabilities remain highly competitive with leading proprietary models, including ChatGPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro, while also maintaining strong pure-language proficiency. In pursuit of open-science principles, we will publicly release both the training data and model weights to foster further research and development in next-generation MLLMs.
[15.04.2025 09:12] Mistral response. {"id": "0816274ee0ae469bbd2b749a5df9940c", "object": "chat.completion", "created": 1744708339, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "\u6211\u4eec\u4ecb\u7ecd\u4e86 InternVL3\uff0c\u8fd9\u662f InternVL \u7cfb\u5217\u7684\u91cd\u5927\u8fdb\u6b65\uff0c\u91c7\u7528\u4e86\u672c\u5730\u591a\u6a21\u6001\u9884\u8bad\u7ec3\u8303\u5f0f\u3002\u4e0e\u5176\u4ed6\u65b9\u6cd5\u4e0d\u540c\uff0cInternVL3 \u5728\u5355\u4e2a\u9884\u8bad\u7ec3\u9636\u6bb5\u5185\uff0c\u4ece\u591a\u6a21\u6001\u6570\u636e\u548c\u7eaf\u6587\u672c\u6570\u636e\u4e2d\u540c\u65f6\u83b7\u53d6\u591a\u6a21\u6001\u548c\u8bed\u8a00\u80fd\u529b\u3002\u8fd9\u79cd\u7edf\u4e00\u7684\u8bad\u7ec3\u8303\u5f0f\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u8bad\u7ec3\u6d41\u7a0b\u4e2d\u7684\u590d\u6742\u6027\u548c\u5bf9\u9f50\u6311\u6218\u3002\u4e3a\u4e86\u8fdb\u4e00\u6b65\u63d0\u9ad8\u6027\u80fd\u548c\u53ef\u6269\u5c55\u6027\uff0cInternVL3 \u4f7f\u7528\u4e86\u53ef\u53d8\u89c6\u89c9\u4f4d\u7f6e\u7f16\u7801\u548c\u5148\u8fdb\u7684\u540e\u8bad\u7ec3\u6280\u672f\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cInternVL3 \u5728\u591a\u79cd\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u7279\u522b\u662f InternVL3-78B \u5728 MMMU \u57fa\u51c6\u6d4b\u8bd5\u4e2d\u83b7\u5f97\u4e86 72.2 \u7684\u5206\u6570\uff0c\u521b\u4e0b\u65b0\u7eaa\u5f55\u3002\u6211\u4eec\u5c06\u516c\u5f00\u53d1\u5e03\u8bad\u7ec3\u6570\u636e\u548c\u6a21\u578b\u6743\u91cd\uff0c\u4ee5\u4fc3\u8fdb\u4e0b\u4e00\u4ee3 MLLMs \u7684\u7814\u7a76\u548c\u5f00\u53d1\u3002"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 385, "total_tokens": 643, "completion_tokens": 258}}
[15.04.2025 09:12] Response: 我们介绍了 InternVL3，这是 InternVL 系列的重大进步，采用了本地多模态预训练范式。与其他方法不同，InternVL3 在单个预训练阶段内，从多模态数据和纯文本数据中同时获取多模态和语言能力。这种统一的训练范式有效解决了传统训练流程中的复杂性和对齐挑战。为了进一步提高性能和可扩展性，InternVL3 使用了可变视觉位置编码和先进的后训练技术。实验结果显示，InternVL3 在多种多模态任务中表现优异，特别是 InternVL3-78B 在 MMMU 基准测试中获得了 72.2 的分数，创下新纪录。我们将公开发布训练数据和模型权重，以促进下一代 MLLMs 的研究和开发。
[15.04.2025 09:12] Mistral request. Model: mistral-large-latest. Prompt: Write pinyin transcription for text. Text:

我们介绍了 InternVL3，这是 InternVL 系列的重大进步，采用了本地多模态预训练范式。与其他方法不同，InternVL3 在单个预训练阶段内，从多模态数据和纯文本数据中同时获取多模态和语言能力。这种统一的训练范式有效解决了传统训练流程中的复杂性和对齐挑战。为了进一步提高性能和可扩展性，InternVL3 使用了可变视觉位置编码和先进的后训练技术。实验结果显示，InternVL3 在多种多模态任务中表现优异，特别是 InternVL3-78B 在 MMMU 基准测试中获得了 72.2 的分数，创下新纪录。我们将公开发布训练数据和模型权重，以促进下一代 MLLMs 的研究和开发。
[15.04.2025 09:12] Mistral response. {"id": "d339526fca5e474da0a8126e9fc75083", "object": "chat.completion", "created": 1744708345, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "W\u01d2men ji\u00e8sh\u00e0o le InternVL3, zh\u00e8 sh\u00ec InternVL x\u00ecli\u00e8 de zh\u00f2ngd\u00e0 j\u00ecnb\u00f9, c\u01ceiy\u00f2ng le b\u011bnd\u00ec du\u014dm\u00f3sh\u012b y\u00f9x\u00f9nli\u00e0n f\u00e0nsh\u00ec. Y\u01d4 q\u00edt\u0101 f\u0101ngf\u01ce b\u00f9t\u00f3ng, InternVL3 z\u00e0i d\u0101n g\u00e8 y\u00f9x\u00f9nli\u00e0n ji\u0113du\u00e0n n\u00e8i, c\u00f3ng du\u014dm\u00f3sh\u012b sh\u00f9j\u00f9 h\u00e9 ch\u00fan w\u00e9nb\u011bn sh\u00f9j\u00f9 zh\u014dng t\u00f3ngsh\u00ed hu\u00f2d\u00e9 du\u014dm\u00f3sh\u012b h\u00e9 y\u01d4y\u00e1n n\u00e9ngl\u00ec. Zh\u00e8 zh\u01d2ng t\u01d2ngy\u012b de x\u00f9nli\u00e0n f\u00e0nsh\u00ec y\u01d2uxi\u00e0o ji\u011bju\u00e9 le chu\u00e1nt\u01d2ng x\u00f9nli\u00e0n li\u00fach\u00e9ng zh\u014dng de f\u00f9z\u00e1x\u00ecng h\u00e9 du\u00ecq\u01d0 ti\u01ceozh\u00e0n. W\u00e8ile j\u00ecnf\u016b t\u012bg\u0101o x\u00edngn\u00e9ng h\u00e9 k\u011bku\u00f2zh\u01cen x\u00ecng, InternVL3 sh\u01d0y\u00f2ng le k\u011bbi\u00e0n sh\u00ecju\u00e9 w\u00e8izh\u00ec bi\u0101nm\u01ce h\u00e9 xi\u0101nj\u00ecn de h\u00f2ux\u00f9nli\u00e0n j\u00ecsh\u00f9. Sh\u00edy\u00e0n ji\u00e9gu\u01d2 xi\u01censh\u00ec, InternVL3 z\u00e0i du\u014dzh\u01d2ng du\u014dm\u00f3sh\u012b r\u00e8nw\u00f9 zh\u014dng bi\u01ceoxi\u00e0n y\u014duy\u00f9, t\u00e8bi\u00e9 sh\u00ec InternVL3-78B z\u00e0i MMMU j\u012bzh\u01d4n c\u00e8sh\u00ec zh\u014dng hu\u00f2d\u00e9 le 72.2 de f\u0113nsh\u00f9, chu\u00e0ng xi\u00e0 x\u012bn j\u00ecl\u00f9. W\u01d2men ji\u0101ng g\u014dngk\u0101i f\u0101b\u00f9 x\u00f9nli\u00e0n sh\u00f9j\u00f9 h\u00e9 m\u00f3x\u00edng qu\u00e1nzh\u00f2ng, y\u01d0 c\u00f9j\u00ecn xi\u00e0 y\u012bd\u00e0i MLLMs de y\u00e1nji\u016b h\u00e9 k\u0101if\u0101."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 272, "total_tokens": 780, "completion_tokens": 508}}
[15.04.2025 09:12] Response: Wǒmen jièshào le InternVL3, zhè shì InternVL xìliè de zhòngdà jìnbù, cǎiyòng le běndì duōmóshī yùxùnliàn fànshì. Yǔ qítā fāngfǎ bùtóng, InternVL3 zài dān gè yùxùnliàn jiēduàn nèi, cóng duōmóshī shùjù hé chún wénběn shùjù zhōng tóngshí huòdé duōmóshī hé yǔyán nénglì. Zhè zhǒng tǒngyī de xùnliàn fànshì yǒuxiào jiějué le chuántǒng xùnliàn liúchéng zhōng de fùzáxìng hé duìqǐ tiǎozhàn. Wèile jìnfū tīgāo xíngnéng hé kěkuòzhǎn xìng, InternVL3 shǐyòng le kěbiàn shìjué wèizhì biānmǎ hé xiānjìn de hòuxùnliàn jìshù. Shíyàn jiéguǒ xiǎnshì, InternVL3 zài duōzhǒng duōmóshī rènwù zhōng biǎoxiàn yōuyù, tèbié shì InternVL3-78B zài MMMU jīzhǔn cèshì zhōng huòdé le 72.2 de fēnshù, chuàng xià xīn jìlù. Wǒmen jiāng gōngkāi fābù xùnliàn shùjù hé móxíng quánzhòng, yǐ cùjìn xià yīdài MLLMs de yánjiū hé kāifā.
[15.04.2025 09:12] Mistral request. Model: mistral-large-latest. Prompt: Write vocab of difficult words for this text as an array of objects with fields 'word', 'pinyin', 'trans'. Return as python list without formatting. Return list and nothing else. Text:

我们介绍了 InternVL3，这是 InternVL 系列的重大进步，采用了本地多模态预训练范式。与其他方法不同，InternVL3 在单个预训练阶段内，从多模态数据和纯文本数据中同时获取多模态和语言能力。这种统一的训练范式有效解决了传统训练流程中的复杂性和对齐挑战。为了进一步提高性能和可扩展性，InternVL3 使用了可变视觉位置编码和先进的后训练技术。实验结果显示，InternVL3 在多种多模态任务中表现优异，特别是 InternVL3-78B 在 MMMU 基准测试中获得了 72.2 的分数，创下新纪录。我们将公开发布训练数据和模型权重，以促进下一代 MLLMs 的研究和开发。
[15.04.2025 09:12] Mistral response. {"id": "f965f34d0c7f4391b04ef8acb1550b00", "object": "chat.completion", "created": 1744708356, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "[{'word': '\u4ecb\u7ecd', 'pinyin': 'ji\u00e8 sh\u00e0o', 'trans': 'introduce'},\n{'word': '\u91cd\u5927', 'pinyin': 'zh\u00f2ng d\u00e0', 'trans': 'major'},\n{'word': '\u8fdb\u6b65', 'pinyin': 'j\u00ecn b\u00f9', 'trans': 'progress'},\n{'word': '\u91c7\u7528', 'pinyin': 'c\u01cei y\u00f2ng', 'trans': 'adopt'},\n{'word': '\u8303\u5f0f', 'pinyin': 'f\u00e0n sh\u00ec', 'trans': 'paradigm'},\n{'word': '\u591a\u6a21\u6001', 'pinyin': 'du\u014d m\u00f3 t\u00e0i', 'trans': 'multimodal'},\n{'word': '\u9884\u8bad\u7ec3', 'pinyin': 'y\u00f9 x\u00f9n li\u00e0n', 'trans': 'pre-training'},\n{'word': '\u9636\u6bb5', 'pinyin': 'ji\u0113 du\u00e0n', 'trans': 'stage'},\n{'word': '\u83b7\u53d6', 'pinyin': 'hu\u00f2 q\u01d4', 'trans': 'obtain'},\n{'word': '\u80fd\u529b', 'pinyin': 'n\u00e9ng l\u00ec', 'trans': 'ability'},\n{'word': '\u7edf\u4e00', 'pinyin': 't\u01d2ng y\u012b', 'trans': 'unified'},\n{'word': '\u89e3\u51b3', 'pinyin': 'ji\u011b ju\u00e9', 'trans': 'solve'},\n{'word': '\u590d\u6742\u6027', 'pinyin': 'f\u00f9 z\u00e1 x\u00ecng', 'trans': 'complexity'},\n{'word': '\u5bf9\u9f50', 'pinyin': 'du\u00ec q\u00ed', 'trans': 'alignment'},\n{'word': '\u6311\u6218', 'pinyin': 'ti\u01ceo zh\u00e0n', 'trans': 'challenge'},\n{'word': '\u53ef\u53d8', 'pinyin': 'k\u011b bi\u00e0n', 'trans': 'variable'},\n{'word': '\u89c6\u89c9', 'pinyin': 'sh\u00ec ju\u00e9', 'trans': 'visual'},\n{'word': '\u4f4d\u7f6e', 'pinyin': 'w\u00e8i zh\u00ec', 'trans': 'position'},\n{'word': '\u7f16\u7801', 'pinyin': 'bi\u0101n m\u01ce', 'trans': 'encoding'},\n{'word': '\u5148\u8fdb', 'pinyin': 'xi\u0101n j\u00ecn', 'trans': 'advanced'},\n{'word': '\u540e\u8bad\u7ec3', 'pinyin': 'h\u00f2u x\u00f9n li\u00e0n', 'trans': 'post-training'},\n{'word': '\u6280\u672f', 'pinyin': 'j\u00ec sh\u00f9', 'trans': 'technology'},\n{'word': '\u8868\u73b0', 'pinyin': 'bi\u01ceo xi\u00e0n', 'trans': 'performance'},\n{'word': '\u4f18\u5f02', 'pinyin': 'y\u014du y\u00ec', 'trans': 'excellent'},\n{'word': '\u7279\u522b', 'pinyin': 't\u00e8 bi\u00e9', 'trans': 'especially'},\n{'word': '\u57fa\u51c6', 'pinyin': 'j\u012b zh\u01d4n', 'trans': 'benchmark'},\n{'word': '\u6d4b\u8bd5', 'pinyin': 'c\u00e8 sh\u00ec', 'trans': 'test'},\n{'word': '\u5206\u6570', 'pinyin': 'f\u0113n sh\u00f9', 'trans': 'score'},\n{'word': '\u7eaa\u5f55', 'pinyin': 'j\u00ec l\u00f9', 'trans': 'record'},\n{'word': '\u516c\u5f00', 'pinyin': 'g\u014dng k\u0101i', 'trans': 'public'},\n{'word': '\u53d1\u5e03', 'pinyin': 'f\u0101 b\u00f9', 'trans': 'release'},\n{'word': '\u6743\u91cd', 'pinyin': 'qu\u00e1n zh\u00f2ng', 'trans': 'weights'},\n{'word': '\u4fc3\u8fdb', 'pinyin': 'c\u00f9 j\u00ecn', 'trans': 'promote'},\n{'word': '\u4e0b\u4e00\u4ee3', 'pinyin': 'xi\u00e0 y\u012b d\u00e0i', 'trans': 'next generation'},\n{'word': '\u7814\u7a76', 'pinyin': 'y\u00e1n ji\u016b', 'trans': 'research'},\n{'word': '\u5f00\u53d1', 'pinyin': 'k\u0101i f\u0101', 'trans': 'development'}]"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 304, "total_tokens": 1292, "completion_tokens": 988}}
[15.04.2025 09:12] Response: [{'word': '介绍', 'pinyin': 'jiè shào', 'trans': 'introduce'},
{'word': '重大', 'pinyin': 'zhòng dà', 'trans': 'major'},
{'word': '进步', 'pinyin': 'jìn bù', 'trans': 'progress'},
{'word': '采用', 'pinyin': 'cǎi yòng', 'trans': 'adopt'},
{'word': '范式', 'pinyin': 'fàn shì', 'trans': 'paradigm'},
{'word': '多模态', 'pinyin': 'duō mó tài', 'trans': 'multimodal'},
{'word': '预训练', 'pinyin': 'yù xùn liàn', 'trans': 'pre-training'},
{'word': '阶段', 'pinyin': 'jiē duàn', 'trans': 'stage'},
{'word': '获取', 'pinyin': 'huò qǔ', 'trans': 'obtain'},
{'word': '能力', 'pinyin': 'néng lì', 'trans': 'ability'},
{'word': '统一', 'pinyin': 'tǒng yī', 'trans': 'unified'},
{'word': '解决', 'pinyin': 'jiě jué', 'trans': 'solve'},
{'word': '复杂性', 'pinyin': 'fù zá xìng', 'trans': 'complexity'},
{'word': '对齐', 'pinyin': 'duì qí', 'trans': 'alignment'},
{'word': '挑战', 'pinyin': 'tiǎo zhàn', 'trans': 'challenge'},
{'word': '可变', 'pinyin': 'kě biàn', 'trans': 'variable'},
{'word': '视觉', 'pinyin': 'shì jué', 'trans': 'visual'},
{'word': '位置', 'pinyin': 'wèi zhì', 'trans': 'position'},
{'word': '编码', 'pinyin': 'biān mǎ', 'trans': 'encoding'},
{'word': '先进', 'pinyin': 'xiān jìn', 'trans': 'advanced'},
{'word': '后训练', 'pinyin': 'hòu xùn liàn', 'trans': 'post-training'},
{'word': '技术', 'pinyin': 'jì shù', 'trans': 'technology'},
{'word': '表现', 'pinyin': 'biǎo xiàn', 'trans': 'performance'},
{'word': '优异', 'pinyin': 'yōu yì', 'trans': 'excellent'},
{'word': '特别', 'pinyin': 'tè bié', 'trans': 'especially'},
{'word': '基准', 'pinyin': 'jī zhǔn', 'trans': 'benchmark'},
{'word': '测试', 'pinyin': 'cè shì', 'trans': 'test'},
{'word': '分数', 'pinyin': 'fēn shù', 'trans': 'score'},
{'word': '纪录', 'pinyin': 'jì lù', 'trans': 'record'},
{'word': '公开', 'pinyin': 'gōng kāi', 'trans': 'public'},
{'word': '发布', 'pinyin': 'fā bù', 'trans': 'release'},
{'word': '权重', 'pinyin': 'quán zhòng', 'trans': 'weights'},
{'word': '促进', 'pinyin': 'cù jìn', 'trans': 'promote'},
{'word': '下一代', 'pinyin': 'xià yī dài', 'trans': 'next generation'},
{'word': '研究', 'pinyin': 'yán jiū', 'trans': 'research'},
{'word': '开发', 'pinyin': 'kāi fā', 'trans': 'development'}]
[15.04.2025 09:12] Mistral request. Model: mistral-large-latest. Prompt: Translate this text in English. Text:

我们介绍了 InternVL3，这是 InternVL 系列的重大进步，采用了本地多模态预训练范式。与其他方法不同，InternVL3 在单个预训练阶段内，从多模态数据和纯文本数据中同时获取多模态和语言能力。这种统一的训练范式有效解决了传统训练流程中的复杂性和对齐挑战。为了进一步提高性能和可扩展性，InternVL3 使用了可变视觉位置编码和先进的后训练技术。实验结果显示，InternVL3 在多种多模态任务中表现优异，特别是 InternVL3-78B 在 MMMU 基准测试中获得了 72.2 的分数，创下新纪录。我们将公开发布训练数据和模型权重，以促进下一代 MLLMs 的研究和开发。
[15.04.2025 09:13] Mistral response. {"id": "689a350fc3a246139d9e01abd829da76", "object": "chat.completion", "created": 1744708376, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "We introduced InternVL3, a significant advancement in the InternVL series, which adopts a local multimodal pretraining paradigm. Unlike other methods, InternVL3 simultaneously acquires multimodal and language capabilities from both multimodal data and pure text data within a single pretraining stage. This unified training paradigm effectively addresses the complexity and alignment challenges of traditional training processes. To further enhance performance and scalability, InternVL3 employs variable visual positional encoding and advanced post-training techniques. Experimental results demonstrate that InternVL3 performs exceptionally well across various multimodal tasks, particularly with InternVL3-78B achieving a score of 72.2 on the MMMU benchmark, setting a new record. We will publicly release the training data and model weights to promote research and development of the next generation of MLLMs."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 270, "total_tokens": 457, "completion_tokens": 187}}
[15.04.2025 09:13] Response: We introduced InternVL3, a significant advancement in the InternVL series, which adopts a local multimodal pretraining paradigm. Unlike other methods, InternVL3 simultaneously acquires multimodal and language capabilities from both multimodal data and pure text data within a single pretraining stage. This unified training paradigm effectively addresses the complexity and alignment challenges of traditional training processes. To further enhance performance and scalability, InternVL3 employs variable visual positional encoding and advanced post-training techniques. Experimental results demonstrate that InternVL3 performs exceptionally well across various multimodal tasks, particularly with InternVL3-78B achieving a score of 72.2 on the MMMU benchmark, setting a new record. We will publicly release the training data and model weights to promote research and development of the next generation of MLLMs.
[15.04.2025 09:13] Renaming data file.
[15.04.2025 09:13] Renaming previous data. hf_papers.json to ./d/2025-04-15.json
[15.04.2025 09:13] Saving new data file.
[15.04.2025 09:13] Generating page.
[15.04.2025 09:13] Renaming previous page.
[15.04.2025 09:13] Renaming previous data. index.html to ./d/2025-04-15.html
[15.04.2025 09:13] [Experimental] Generating Chinese page for reading.
[15.04.2025 09:13] Chinese vocab [{'word': '介绍', 'pinyin': 'jiè shào', 'trans': 'introduce'}, {'word': '重大', 'pinyin': 'zhòng dà', 'trans': 'major'}, {'word': '进步', 'pinyin': 'jìn bù', 'trans': 'progress'}, {'word': '采用', 'pinyin': 'cǎi yòng', 'trans': 'adopt'}, {'word': '范式', 'pinyin': 'fàn shì', 'trans': 'paradigm'}, {'word': '多模态', 'pinyin': 'duō mó tài', 'trans': 'multimodal'}, {'word': '预训练', 'pinyin': 'yù xùn liàn', 'trans': 'pre-training'}, {'word': '阶段', 'pinyin': 'jiē duàn', 'trans': 'stage'}, {'word': '获取', 'pinyin': 'huò qǔ', 'trans': 'obtain'}, {'word': '能力', 'pinyin': 'néng lì', 'trans': 'ability'}, {'word': '统一', 'pinyin': 'tǒng yī', 'trans': 'unified'}, {'word': '解决', 'pinyin': 'jiě jué', 'trans': 'solve'}, {'word': '复杂性', 'pinyin': 'fù zá xìng', 'trans': 'complexity'}, {'word': '对齐', 'pinyin': 'duì qí', 'trans': 'alignment'}, {'word': '挑战', 'pinyin': 'tiǎo zhàn', 'trans': 'challenge'}, {'word': '可变', 'pinyin': 'kě biàn', 'trans': 'variable'}, {'word': '视觉', 'pinyin': 'shì jué', 'trans': 'visual'}, {'word': '位置', 'pinyin': 'wèi zhì', 'trans': 'position'}, {'word': '编码', 'pinyin': 'biān mǎ', 'trans': 'encoding'}, {'word': '先进', 'pinyin': 'xiān jìn', 'trans': 'advanced'}, {'word': '后训练', 'pinyin': 'hòu xùn liàn', 'trans': 'post-training'}, {'word': '技术', 'pinyin': 'jì shù', 'trans': 'technology'}, {'word': '表现', 'pinyin': 'biǎo xiàn', 'trans': 'performance'}, {'word': '优异', 'pinyin': 'yōu yì', 'trans': 'excellent'}, {'word': '特别', 'pinyin': 'tè bié', 'trans': 'especially'}, {'word': '基准', 'pinyin': 'jī zhǔn', 'trans': 'benchmark'}, {'word': '测试', 'pinyin': 'cè shì', 'trans': 'test'}, {'word': '分数', 'pinyin': 'fēn shù', 'trans': 'score'}, {'word': '纪录', 'pinyin': 'jì lù', 'trans': 'record'}, {'word': '公开', 'pinyin': 'gōng kāi', 'trans': 'public'}, {'word': '发布', 'pinyin': 'fā bù', 'trans': 'release'}, {'word': '权重', 'pinyin': 'quán zhòng', 'trans': 'weights'}, {'word': '促进', 'pinyin': 'cù jìn', 'trans': 'promote'}, {'word': '下一代', 'pinyin': 'xià yī dài', 'trans': 'next generation'}, {'word': '研究', 'pinyin': 'yán jiū', 'trans': 'research'}, {'word': '开发', 'pinyin': 'kāi fā', 'trans': 'development'}]
[15.04.2025 09:13] Renaming previous Chinese page.
[15.04.2025 09:13] Renaming previous data. zh.html to ./d/2025-04-14_zh_reading_task.html
[15.04.2025 09:13] Writing Chinese reading task.
[15.04.2025 09:13] Writing result.
[15.04.2025 09:13] Renaming log file.
[15.04.2025 09:13] Renaming previous data. log.txt to ./logs/2025-04-15_last_log.txt
