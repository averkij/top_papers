[18.05.2025 06:33] Read previous papers.
[18.05.2025 06:33] Generating top page (month).
[18.05.2025 06:33] Writing top page (month).
[18.05.2025 12:44] Read previous papers.
[18.05.2025 12:44] Get feed.
[18.05.2025 12:44] Get page data from previous paper. URL: https://huggingface.co/papers/2505.10554
[18.05.2025 12:44] Get page data from previous paper. URL: https://huggingface.co/papers/2505.10475
[18.05.2025 12:44] Get page data from previous paper. URL: https://huggingface.co/papers/2505.09666
[18.05.2025 12:44] Get page data from previous paper. URL: https://huggingface.co/papers/2505.08617
[18.05.2025 12:44] Get page data from previous paper. URL: https://huggingface.co/papers/2505.09723
[18.05.2025 12:44] Get page data from previous paper. URL: https://huggingface.co/papers/2505.10527
[18.05.2025 12:44] Get page data from previous paper. URL: https://huggingface.co/papers/2505.10185
[18.05.2025 12:44] Get page data from previous paper. URL: https://huggingface.co/papers/2505.10562
[18.05.2025 12:44] Get page data from previous paper. URL: https://huggingface.co/papers/2505.10320
[18.05.2025 12:44] Get page data from previous paper. URL: https://huggingface.co/papers/2505.09694
[18.05.2025 12:44] Get page data from previous paper. URL: https://huggingface.co/papers/2505.06027
[18.05.2025 12:44] Get page data from previous paper. URL: https://huggingface.co/papers/2505.07782
[18.05.2025 12:44] Get page data from previous paper. URL: https://huggingface.co/papers/2505.10558
[18.05.2025 12:44] Get page data from previous paper. URL: https://huggingface.co/papers/2505.10565
[18.05.2025 12:44] Get page data from previous paper. URL: https://huggingface.co/papers/2505.09990
[18.05.2025 12:44] Get page data from previous paper. URL: https://huggingface.co/papers/2505.10046
[18.05.2025 12:44] Get page data from previous paper. URL: https://huggingface.co/papers/2505.09738
[18.05.2025 12:44] Get page data from previous paper. URL: https://huggingface.co/papers/2505.10566
[18.05.2025 12:44] Get page data from previous paper. URL: https://huggingface.co/papers/2505.10468
[18.05.2025 12:44] Get page data from previous paper. URL: https://huggingface.co/papers/2505.08581
[18.05.2025 12:44] Get page data from previous paper. URL: https://huggingface.co/papers/2505.10167
[18.05.2025 12:44] Get page data from previous paper. URL: https://huggingface.co/papers/2505.09926
[18.05.2025 12:44] Get page data from previous paper. URL: https://huggingface.co/papers/2505.09601
[18.05.2025 12:44] Get page data from previous paper. URL: https://huggingface.co/papers/2505.09265
[18.05.2025 12:44] Get page data from previous paper. URL: https://huggingface.co/papers/2505.09264
[18.05.2025 12:44] Get page data from previous paper. URL: https://huggingface.co/papers/2505.09263
[18.05.2025 12:44] Get page data from previous paper. URL: https://huggingface.co/papers/2505.07096
[18.05.2025 12:44] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[18.05.2025 12:44] No deleted papers detected.
[18.05.2025 12:44] Downloading and parsing papers (pdf, html). Total: 27.
[18.05.2025 12:44] Downloading and parsing paper https://huggingface.co/papers/2505.10554.
[18.05.2025 12:44] Extra JSON file exists (./assets/json/2505.10554.json), skip PDF parsing.
[18.05.2025 12:44] Paper image links file exists (./assets/img_data/2505.10554.json), skip HTML parsing.
[18.05.2025 12:44] Success.
[18.05.2025 12:44] Downloading and parsing paper https://huggingface.co/papers/2505.10475.
[18.05.2025 12:44] Extra JSON file exists (./assets/json/2505.10475.json), skip PDF parsing.
[18.05.2025 12:44] Paper image links file exists (./assets/img_data/2505.10475.json), skip HTML parsing.
[18.05.2025 12:44] Success.
[18.05.2025 12:44] Downloading and parsing paper https://huggingface.co/papers/2505.09666.
[18.05.2025 12:44] Extra JSON file exists (./assets/json/2505.09666.json), skip PDF parsing.
[18.05.2025 12:44] Paper image links file exists (./assets/img_data/2505.09666.json), skip HTML parsing.
[18.05.2025 12:44] Success.
[18.05.2025 12:44] Downloading and parsing paper https://huggingface.co/papers/2505.08617.
[18.05.2025 12:44] Extra JSON file exists (./assets/json/2505.08617.json), skip PDF parsing.
[18.05.2025 12:44] Paper image links file exists (./assets/img_data/2505.08617.json), skip HTML parsing.
[18.05.2025 12:44] Success.
[18.05.2025 12:44] Downloading and parsing paper https://huggingface.co/papers/2505.09723.
[18.05.2025 12:44] Extra JSON file exists (./assets/json/2505.09723.json), skip PDF parsing.
[18.05.2025 12:44] Paper image links file exists (./assets/img_data/2505.09723.json), skip HTML parsing.
[18.05.2025 12:44] Success.
[18.05.2025 12:44] Downloading and parsing paper https://huggingface.co/papers/2505.10527.
[18.05.2025 12:44] Extra JSON file exists (./assets/json/2505.10527.json), skip PDF parsing.
[18.05.2025 12:44] Paper image links file exists (./assets/img_data/2505.10527.json), skip HTML parsing.
[18.05.2025 12:44] Success.
[18.05.2025 12:44] Downloading and parsing paper https://huggingface.co/papers/2505.10185.
[18.05.2025 12:44] Extra JSON file exists (./assets/json/2505.10185.json), skip PDF parsing.
[18.05.2025 12:44] Paper image links file exists (./assets/img_data/2505.10185.json), skip HTML parsing.
[18.05.2025 12:44] Success.
[18.05.2025 12:44] Downloading and parsing paper https://huggingface.co/papers/2505.10562.
[18.05.2025 12:44] Extra JSON file exists (./assets/json/2505.10562.json), skip PDF parsing.
[18.05.2025 12:44] Paper image links file exists (./assets/img_data/2505.10562.json), skip HTML parsing.
[18.05.2025 12:44] Success.
[18.05.2025 12:44] Downloading and parsing paper https://huggingface.co/papers/2505.10320.
[18.05.2025 12:44] Extra JSON file exists (./assets/json/2505.10320.json), skip PDF parsing.
[18.05.2025 12:44] Paper image links file exists (./assets/img_data/2505.10320.json), skip HTML parsing.
[18.05.2025 12:44] Success.
[18.05.2025 12:44] Downloading and parsing paper https://huggingface.co/papers/2505.09694.
[18.05.2025 12:44] Extra JSON file exists (./assets/json/2505.09694.json), skip PDF parsing.
[18.05.2025 12:44] Paper image links file exists (./assets/img_data/2505.09694.json), skip HTML parsing.
[18.05.2025 12:44] Success.
[18.05.2025 12:44] Downloading and parsing paper https://huggingface.co/papers/2505.06027.
[18.05.2025 12:44] Extra JSON file exists (./assets/json/2505.06027.json), skip PDF parsing.
[18.05.2025 12:44] Paper image links file exists (./assets/img_data/2505.06027.json), skip HTML parsing.
[18.05.2025 12:44] Success.
[18.05.2025 12:44] Downloading and parsing paper https://huggingface.co/papers/2505.07782.
[18.05.2025 12:44] Extra JSON file exists (./assets/json/2505.07782.json), skip PDF parsing.
[18.05.2025 12:44] Paper image links file exists (./assets/img_data/2505.07782.json), skip HTML parsing.
[18.05.2025 12:44] Success.
[18.05.2025 12:44] Downloading and parsing paper https://huggingface.co/papers/2505.10558.
[18.05.2025 12:44] Extra JSON file exists (./assets/json/2505.10558.json), skip PDF parsing.
[18.05.2025 12:44] Paper image links file exists (./assets/img_data/2505.10558.json), skip HTML parsing.
[18.05.2025 12:44] Success.
[18.05.2025 12:44] Downloading and parsing paper https://huggingface.co/papers/2505.10565.
[18.05.2025 12:44] Extra JSON file exists (./assets/json/2505.10565.json), skip PDF parsing.
[18.05.2025 12:44] Paper image links file exists (./assets/img_data/2505.10565.json), skip HTML parsing.
[18.05.2025 12:44] Success.
[18.05.2025 12:44] Downloading and parsing paper https://huggingface.co/papers/2505.09990.
[18.05.2025 12:44] Extra JSON file exists (./assets/json/2505.09990.json), skip PDF parsing.
[18.05.2025 12:44] Paper image links file exists (./assets/img_data/2505.09990.json), skip HTML parsing.
[18.05.2025 12:44] Success.
[18.05.2025 12:44] Downloading and parsing paper https://huggingface.co/papers/2505.10046.
[18.05.2025 12:44] Extra JSON file exists (./assets/json/2505.10046.json), skip PDF parsing.
[18.05.2025 12:44] Paper image links file exists (./assets/img_data/2505.10046.json), skip HTML parsing.
[18.05.2025 12:44] Success.
[18.05.2025 12:44] Downloading and parsing paper https://huggingface.co/papers/2505.09738.
[18.05.2025 12:44] Extra JSON file exists (./assets/json/2505.09738.json), skip PDF parsing.
[18.05.2025 12:44] Paper image links file exists (./assets/img_data/2505.09738.json), skip HTML parsing.
[18.05.2025 12:44] Success.
[18.05.2025 12:44] Downloading and parsing paper https://huggingface.co/papers/2505.10566.
[18.05.2025 12:44] Extra JSON file exists (./assets/json/2505.10566.json), skip PDF parsing.
[18.05.2025 12:44] Paper image links file exists (./assets/img_data/2505.10566.json), skip HTML parsing.
[18.05.2025 12:44] Success.
[18.05.2025 12:44] Downloading and parsing paper https://huggingface.co/papers/2505.10468.
[18.05.2025 12:44] Extra JSON file exists (./assets/json/2505.10468.json), skip PDF parsing.
[18.05.2025 12:44] Paper image links file exists (./assets/img_data/2505.10468.json), skip HTML parsing.
[18.05.2025 12:44] Success.
[18.05.2025 12:44] Downloading and parsing paper https://huggingface.co/papers/2505.08581.
[18.05.2025 12:44] Extra JSON file exists (./assets/json/2505.08581.json), skip PDF parsing.
[18.05.2025 12:44] Paper image links file exists (./assets/img_data/2505.08581.json), skip HTML parsing.
[18.05.2025 12:44] Success.
[18.05.2025 12:44] Downloading and parsing paper https://huggingface.co/papers/2505.10167.
[18.05.2025 12:44] Extra JSON file exists (./assets/json/2505.10167.json), skip PDF parsing.
[18.05.2025 12:44] Paper image links file exists (./assets/img_data/2505.10167.json), skip HTML parsing.
[18.05.2025 12:44] Success.
[18.05.2025 12:44] Downloading and parsing paper https://huggingface.co/papers/2505.09926.
[18.05.2025 12:44] Extra JSON file exists (./assets/json/2505.09926.json), skip PDF parsing.
[18.05.2025 12:44] Paper image links file exists (./assets/img_data/2505.09926.json), skip HTML parsing.
[18.05.2025 12:44] Success.
[18.05.2025 12:44] Downloading and parsing paper https://huggingface.co/papers/2505.09601.
[18.05.2025 12:44] Extra JSON file exists (./assets/json/2505.09601.json), skip PDF parsing.
[18.05.2025 12:44] Paper image links file exists (./assets/img_data/2505.09601.json), skip HTML parsing.
[18.05.2025 12:44] Success.
[18.05.2025 12:44] Downloading and parsing paper https://huggingface.co/papers/2505.09265.
[18.05.2025 12:44] Extra JSON file exists (./assets/json/2505.09265.json), skip PDF parsing.
[18.05.2025 12:44] Paper image links file exists (./assets/img_data/2505.09265.json), skip HTML parsing.
[18.05.2025 12:44] Success.
[18.05.2025 12:44] Downloading and parsing paper https://huggingface.co/papers/2505.09264.
[18.05.2025 12:44] Extra JSON file exists (./assets/json/2505.09264.json), skip PDF parsing.
[18.05.2025 12:44] Paper image links file exists (./assets/img_data/2505.09264.json), skip HTML parsing.
[18.05.2025 12:44] Success.
[18.05.2025 12:44] Downloading and parsing paper https://huggingface.co/papers/2505.09263.
[18.05.2025 12:44] Extra JSON file exists (./assets/json/2505.09263.json), skip PDF parsing.
[18.05.2025 12:44] Paper image links file exists (./assets/img_data/2505.09263.json), skip HTML parsing.
[18.05.2025 12:44] Success.
[18.05.2025 12:44] Downloading and parsing paper https://huggingface.co/papers/2505.07096.
[18.05.2025 12:44] Extra JSON file exists (./assets/json/2505.07096.json), skip PDF parsing.
[18.05.2025 12:44] Paper image links file exists (./assets/img_data/2505.07096.json), skip HTML parsing.
[18.05.2025 12:44] Success.
[18.05.2025 12:44] Enriching papers with extra data.
[18.05.2025 12:44] ********************************************************************************
[18.05.2025 12:44] Abstract 0. Large reasoning models (LRMs) already possess a latent capacity for long chain-of-thought reasoning. Prior work has shown that outcome-based reinforcement learning (RL) can incidentally elicit advanced reasoning behaviors such as self-correction, backtracking, and verification phenomena often referr...
[18.05.2025 12:44] ********************************************************************************
[18.05.2025 12:44] Abstract 1. It is commonly believed that scaling language models should commit a significant space or time cost, by increasing the parameters (parameter scaling) or output tokens (inference-time scaling). We introduce the third and more inference-efficient scaling paradigm: increasing the model's parallel compu...
[18.05.2025 12:44] ********************************************************************************
[18.05.2025 12:44] Abstract 2. Large Language Models (LLMs) have shown remarkable capabilities, with optimizing their input prompts playing a pivotal role in maximizing their performance. However, while LLM prompts consist of both the task-agnostic system prompts and task-specific user prompts, existing work on prompt optimizatio...
[18.05.2025 12:44] ********************************************************************************
[18.05.2025 12:44] Abstract 3. While humans can flexibly leverage interactive visual cognition for complex problem-solving, enabling Large Vision-Language Models (LVLMs) to learn similarly adaptive behaviors with visual tools remains challenging. A significant hurdle is the current lack of standardized infrastructure, which hinde...
[18.05.2025 12:44] ********************************************************************************
[18.05.2025 12:44] Abstract 4. Robotic imitation learning has advanced from solving static tasks to addressing dynamic interaction scenarios, but testing and evaluation remain costly and challenging due to the need for real-time interaction with dynamic environments. We propose EnerVerse-AC (EVAC), an action-conditional world mod...
[18.05.2025 12:44] ********************************************************************************
[18.05.2025 12:44] Abstract 5. Motivated by scaling laws in language modeling that demonstrate how test loss scales as a power law with model and dataset sizes, we find that similar laws exist in preference modeling. We propose World Preference Modeling$ (WorldPM) to emphasize this scaling potential, where World Preference embodi...
[18.05.2025 12:44] ********************************************************************************
[18.05.2025 12:44] Abstract 6. Long chain-of-thought (CoT) is an essential ingredient in effective usage of modern large language models, but our understanding of the reasoning strategies underlying these capabilities remains limited. While some prior works have attempted to categorize CoTs using predefined strategy types, such a...
[18.05.2025 12:44] ********************************************************************************
[18.05.2025 12:44] Abstract 7. Existing vision tokenization isolates the optimization of vision tokenizers from downstream training, implicitly assuming the visual tokens can generalize well across various tasks, e.g., image generation and visual question answering. The vision tokenizer optimized for low-level reconstruction is a...
[18.05.2025 12:44] ********************************************************************************
[18.05.2025 12:44] Abstract 8. The progress of AI is bottlenecked by the quality of evaluation, and powerful LLM-as-a-Judge models have proved to be a core solution. Improved judgment ability is enabled by stronger chain-of-thought reasoning, motivating the need to find the best recipes for training such models to think. In this ...
[18.05.2025 12:44] ********************************************************************************
[18.05.2025 12:44] Abstract 9. Recent advances in creative AI have enabled the synthesis of high-fidelity images and videos conditioned on language instructions. Building on these developments, text-to-video diffusion models have evolved into embodied world models (EWMs) capable of generating physically plausible scenes from lang...
[18.05.2025 12:44] ********************************************************************************
[18.05.2025 12:44] Abstract 10. This paper introduces Unilogit, a novel self-distillation method for machine unlearning in Large Language Models. Unilogit addresses the challenge of selectively forgetting specific information while maintaining overall model utility, a critical task in compliance with data privacy regulations like ...
[18.05.2025 12:44] ********************************************************************************
[18.05.2025 12:44] Abstract 11. We introduce MLE-Dojo, a Gym-style framework for systematically reinforcement learning, evaluating, and improving autonomous large language model (LLM) agents in iterative machine learning engineering (MLE) workflows. Unlike existing benchmarks that primarily rely on static datasets or single-attemp...
[18.05.2025 12:44] ********************************************************************************
[18.05.2025 12:44] Abstract 12. Scalable Vector Graphics (SVGs) are highly favored by designers due to their resolution independence and well-organized layer structure. Although existing text-to-vector (T2V) generation methods can create SVGs from text prompts, they often overlook an important need in practical applications: style...
[18.05.2025 12:44] ********************************************************************************
[18.05.2025 12:44] Abstract 13. This work presents Prior Depth Anything, a framework that combines incomplete but precise metric information in depth measurement with relative but complete geometric structures in depth prediction, generating accurate, dense, and detailed metric depth maps for any scene. To this end, we design a co...
[18.05.2025 12:44] ********************************************************************************
[18.05.2025 12:44] Abstract 14. Pointing serves as a fundamental and intuitive mechanism for grounding language within visual contexts, with applications spanning robotics, assistive technologies, and interactive AI systems. While recent multimodal models have started to support pointing capabilities, existing benchmarks typically...
[18.05.2025 12:44] ********************************************************************************
[18.05.2025 12:44] Abstract 15. This paper does not describe a new method; instead, it provides a thorough exploration of an important yet understudied design space related to recent advances in text-to-image synthesis -- specifically, the deep fusion of large language models (LLMs) and diffusion transformers (DiTs) for multi-moda...
[18.05.2025 12:44] ********************************************************************************
[18.05.2025 12:44] Abstract 16. Pretrained language models (LLMs) are often constrained by their fixed tokenization schemes, leading to inefficiencies and performance limitations, particularly for multilingual or specialized applications. This tokenizer lock-in presents significant challenges. standard methods to overcome this oft...
[18.05.2025 12:44] ********************************************************************************
[18.05.2025 12:44] Abstract 17. Despite significant advances in modeling image priors via diffusion models, 3D-aware image editing remains challenging, in part because the object is only specified via a single image. To tackle this challenge, we propose 3D-Fixup, a new framework for editing 2D images guided by learned 3D priors. T...
[18.05.2025 12:44] ********************************************************************************
[18.05.2025 12:44] Abstract 18. This study critically distinguishes between AI Agents and Agentic AI, offering a structured conceptual taxonomy, application mapping, and challenge analysis to clarify their divergent design philosophies and capabilities. We begin by outlining the search strategy and foundational definitions, charac...
[18.05.2025 12:44] ********************************************************************************
[18.05.2025 12:44] Abstract 19. Surgical scene segmentation is critical in computer-assisted surgery and is vital for enhancing surgical quality and patient outcomes. Recently, referring surgical segmentation is emerging, given its advantage of providing surgeons with an interactive experience to segment the target object. However...
[18.05.2025 12:44] ********************************************************************************
[18.05.2025 12:44] Abstract 20. The emergence of hybrid quantum-classical machine learning (HQML) models opens new horizons of computational intelligence but their fundamental complexity frequently leads to black box behavior that undermines transparency and reliability in their application. Although XAI for quantum systems still ...
[18.05.2025 12:44] ********************************************************************************
[18.05.2025 12:44] Abstract 21. Universal visual anomaly detection aims to identify anomalies from novel or unseen vision domains without additional fine-tuning, which is critical in open scenarios. Recent studies have demonstrated that pre-trained vision-language models like CLIP exhibit strong generalization with just zero or a ...
[18.05.2025 12:44] ********************************************************************************
[18.05.2025 12:44] Abstract 22. Scaling robot learning requires vast and diverse datasets. Yet the prevailing data collection paradigm-human teleoperation-remains costly and constrained by manual effort and physical robot access. We introduce Real2Render2Real (R2R2R), a novel approach for generating robot training data without rel...
[18.05.2025 12:44] ********************************************************************************
[18.05.2025 12:44] Abstract 23. Zero- and few-shot visual anomaly segmentation relies on powerful vision-language models that detect unseen anomalies using manually designed textual prompts. However, visual representations are inherently independent of language. In this paper, we explore the potential of a pure visual foundation m...
[18.05.2025 12:44] ********************************************************************************
[18.05.2025 12:44] Abstract 24. Unsupervised reconstruction networks using self-attention transformers have achieved state-of-the-art performance for multi-class (unified) anomaly detection with a single model. However, these self-attention reconstruction models primarily operate on target features, which may result in perfect rec...
[18.05.2025 12:44] ********************************************************************************
[18.05.2025 12:44] Abstract 25. Anomaly detection is a practical and challenging task due to the scarcity of anomaly samples in industrial inspection. Some existing anomaly detection methods address this issue by synthesizing anomalies with noise or external data. However, there is always a large semantic gap between synthetic and...
[18.05.2025 12:44] ********************************************************************************
[18.05.2025 12:44] Abstract 26. Human videos offer a scalable way to train robot manipulation policies, but lack the action labels needed by standard imitation learning algorithms. Existing cross-embodiment approaches try to map human motion to robot actions, but often fail when the embodiments differ significantly. We propose X-S...
[18.05.2025 12:44] Read previous papers.
[18.05.2025 12:44] Generating reviews via LLM API.
[18.05.2025 12:44] Using data from previous issue: {"categories": ["#alignment", "#science", "#rl", "#optimization", "#training", "#reasoning", "#benchmark"], "emoji": "🧠", "ru": {"title": "Выравнивание мета-способностей для улучшения рассуждений ИИ", "desc": "Статья описывает новый подход к улучшению способностей больших языковых моделей к рассужде
[18.05.2025 12:44] Using data from previous issue: {"categories": ["#inference", "#low_resource", "#training", "#architecture", "#optimization"], "emoji": "🚀", "ru": {"title": "Эффективное масштабирование языковых моделей через параллельные вычисления", "desc": "Статья представляет новый подход к масштабированию языковых моделей, названный параллель
[18.05.2025 12:44] Using data from previous issue: {"categories": ["#multimodal", "#training", "#optimization", "#transfer_learning"], "emoji": "🧠", "ru": {"title": "Универсальные системные промпты: новый уровень эффективности LLM", "desc": "Статья представляет новый подход к оптимизации системных промптов для больших языковых моделей (LLM). Авторы 
[18.05.2025 12:44] Using data from previous issue: {"categories": ["#cv", "#rl", "#dataset", "#agents", "#training", "#open_source", "#reasoning"], "emoji": "🔍", "ru": {"title": "OpenThinkIMG: Революция в обучении ИИ мыслить визуально", "desc": "Статья представляет OpenThinkIMG - первую открытую комплексную платформу для обучения мультимодальных язы
[18.05.2025 12:44] Using data from previous issue: {"categories": ["#agents", "#dataset", "#training", "#optimization", "#open_source", "#robotics", "#video"], "emoji": "🤖", "ru": {"title": "EVAC: виртуальная среда для эффективного обучения и оценки роботов", "desc": "EVAC - это модель мира с условным действием для имитационного обучения роботов. Он
[18.05.2025 12:44] Using data from previous issue: {"categories": ["#data", "#training", "#benchmark", "#optimization", "#dataset", "#rlhf", "#alignment"], "emoji": "🌍", "ru": {"title": "Масштабирование моделей предпочтений: от данных к глобальному пониманию", "desc": "Исследователи обнаружили, что законы масштабирования, аналогичные тем, что наблюд
[18.05.2025 12:44] Using data from previous issue: {"categories": ["#reasoning", "#data", "#training", "#interpretability", "#multimodal"], "emoji": "🧠", "ru": {"title": "Расшифровка мышления ИИ: от цепочек к энциклопедии рассуждений", "desc": "Статья представляет новый подход к анализу цепочек рассуждений (CoT) в больших языковых моделях. Авторы пр
[18.05.2025 12:44] Using data from previous issue: {"categories": ["#alignment", "#cv", "#optimization", "#multimodal", "#training", "#architecture"], "emoji": "🔍", "ru": {"title": "Конец-в-конец настройка визуальных токенизаторов для улучшения мультимодальных моделей", "desc": "Статья предлагает новый подход ETT для оптимизации визуальных токенизат
[18.05.2025 12:44] Using data from previous issue: {"categories": ["#reasoning", "#training", "#benchmark", "#optimization", "#hallucinations", "#rlhf", "#rl"], "emoji": "🧠", "ru": {"title": "J1: Революция в обучении моделей-судей через рассуждения", "desc": "Статья представляет новый подход к обучению моделей-судей с использованием обучения с подкр
[18.05.2025 12:44] Using data from previous issue: {"categories": ["#diffusion", "#dataset", "#multimodal", "#games", "#video", "#benchmark"], "emoji": "🤖", "ru": {"title": "EWMBench: Комплексная оценка воплощенных мировых моделей для генерации видео", "desc": "Статья представляет новый бенчмарк EWMBench для оценки воплощенных мировых моделей (Embod
[18.05.2025 12:44] Using data from previous issue: {"categories": ["#training", "#benchmark", "#dataset", "#security", "#ethics"], "emoji": "🧠", "ru": {"title": "Unilogit: умное забывание для больших языковых моделей", "desc": "Статья представляет Unilogit - новый метод самодистилляции для машинного забывания в больших языковых моделях. Unilogit реш
[18.05.2025 12:44] Using data from previous issue: {"categories": ["#data", "#training", "#architecture", "#benchmark", "#open_source", "#games", "#agents", "#rl"], "emoji": "🤖", "ru": {"title": "MLE-Dojo: Интерактивная среда для обучения автономных LLM-агентов", "desc": "MLE-Dojo - это фреймворк для обучения, оценки и улучшения автономных агентов н
[18.05.2025 12:44] Using data from previous issue: {"categories": ["#cv", "#diffusion", "#multimodal", "#optimization"], "emoji": "🎨", "ru": {"title": "Генерация стилизованной векторной графики с помощью ИИ", "desc": "Статья представляет новый метод генерации масштабируемой векторной графики (SVG) с возможностью настройки стиля. Авторы предлагают дв
[18.05.2025 12:44] Using data from previous issue: {"categories": ["#cv", "#dataset", "#training"], "emoji": "🔍", "ru": {"title": "Точные карты глубины для любых сцен через объединение измерений и предсказаний", "desc": "Статья представляет Prior Depth Anything - фреймворк для создания точных и детальных карт глубины для любых сцен. Он объединяет не
[18.05.2025 12:44] Using data from previous issue: {"categories": ["#training", "#multimodal", "#reasoning", "#open_source", "#benchmark", "#robotics", "#dataset"], "emoji": "👆", "ru": {"title": "PointArena: новый стандарт оценки мультимодальных моделей в задачах указания", "desc": "Статья представляет PointArena - комплексную платформу для оценки м
[18.05.2025 12:44] Using data from previous issue: {"categories": ["#multimodal", "#training", "#diffusion", "#survey"], "emoji": "🔬", "ru": {"title": "Глубокий анализ слияния LLM и DiT для генерации изображений по тексту", "desc": "Статья представляет собой эмпирическое исследование генерации изображений по текстовому описанию. Авторы проводят дета
[18.05.2025 12:44] Using data from previous issue: {"categories": ["#optimization", "#multilingual", "#training", "#transfer_learning", "#data"], "emoji": "🔄", "ru": {"title": "TokenAdapt: Эффективная замена токенизатора в языковых моделях", "desc": "Статья представляет новый метод TokenAdapt для замены токенизатора в предобученных языковых моделях 
[18.05.2025 12:44] Using data from previous issue: {"categories": ["#cv", "#diffusion", "#3d", "#video", "#training"], "emoji": "🖼️", "ru": {"title": "3D-Fixup: Новый уровень редактирования изображений с 3D-приорами", "desc": "3D-Fixup - это новый фреймворк для редактирования 2D-изображений с использованием изученных 3D-приоров. Он поддерживает слож
[18.05.2025 12:44] Using data from previous issue: {"categories": ["#healthcare", "#hallucinations", "#agents", "#architecture", "#multimodal", "#reasoning", "#optimization", "#agi"], "emoji": "🤖", "ru": {"title": "От ИИ-агентов к агентному ИИ: эволюция автономных систем", "desc": "Это исследование проводит критическое различие между ИИ-агентами и а
[18.05.2025 12:44] Using data from previous issue: {"categories": ["#cv", "#healthcare", "#multimodal"], "emoji": "🔪", "ru": {"title": "ReSurgSAM2: Революция в интерактивной сегментации хирургических сцен", "desc": "Статья представляет ReSurgSAM2 - двухэтапную систему для сегментации хирургических сцен с использованием текстовых запросов. На первом 
[18.05.2025 12:44] Using data from previous issue: {"categories": ["#data", "#inference", "#interpretability", "#architecture"], "emoji": "🔬", "ru": {"title": "QuXAI: Прозрачность квантово-классического ИИ", "desc": "Эта статья представляет QuXAI - фреймворк для объяснения гибридных квантово-классических моделей машинного обучения (HQML). В основе Q
[18.05.2025 12:44] Using data from previous issue: {"categories": ["#cv", "#benchmark", "#open_source", "#transfer_learning", "#healthcare"], "emoji": "🔍", "ru": {"title": "AdaptCLIP: Эффективное обнаружение аномалий с помощью адаптации CLIP", "desc": "Статья представляет новый метод универсального обнаружения визуальных аномалий под названием Adapt
[18.05.2025 12:44] Using data from previous issue: {"categories": ["#optimization", "#dataset", "#synthetic", "#3d", "#robotics"], "emoji": "🤖", "ru": {"title": "Революция в обучении роботов: от одной демонстрации к тысячам симуляций", "desc": "R2R2R - новый подход к генерации обучающих данных для роботов без использования симуляции динамики объекто
[18.05.2025 12:44] Using data from previous issue: {"categories": ["#cv", "#training", "#optimization", "#synthetic", "#dataset"], "emoji": "👁️", "ru": {"title": "Универсальная сегментация аномалий без языковых подсказок", "desc": "Статья представляет новый подход к сегментации визуальных аномалий, основанный на чисто визуальной модели без использов
[18.05.2025 12:44] Using data from previous issue: {"categories": ["#cv", "#benchmark", "#optimization", "#survey", "#dataset"], "emoji": "🔍", "ru": {"title": "OneNIP: Обнаружение аномалий по одному нормальному изображению", "desc": "Эта статья представляет новый метод OneNIP для обнаружения аномалий в изображениях с использованием всего одного этал
[18.05.2025 12:44] Using data from previous issue: {"categories": ["#data", "#cv", "#training", "#benchmark", "#dataset", "#synthetic", "#diffusion"], "emoji": "🔍", "ru": {"title": "Генерация аномалий по нескольким примерам для улучшения их обнаружения", "desc": "Статья представляет метод AnoGen для обнаружения аномалий в промышленной инспекции с ис
[18.05.2025 12:44] Using data from previous issue: {"categories": ["#synthetic", "#rl", "#training", "#robotics", "#optimization", "#transfer_learning"], "emoji": "🤖", "ru": {"title": "Роботы учатся у людей без прямого копирования движений", "desc": "X-Sim - это новый подход к обучению роботов манипуляциям на основе видео с людьми. Метод использует 
[18.05.2025 12:44] Trying to get texts in Chinese.
[18.05.2025 12:44] Mistral request. Model: mistral-large-latest. Prompt: Write simple and brief explanation (4-5 sentences) of an article in Chinese. Use short sentences. Text:

Large reasoning models (LRMs) already possess a latent capacity for long chain-of-thought reasoning. Prior work has shown that outcome-based reinforcement learning (RL) can incidentally elicit advanced reasoning behaviors such as self-correction, backtracking, and verification phenomena often referred to as the model's "aha moment". However, the timing and consistency of these emergent behaviors remain unpredictable and uncontrollable, limiting the scalability and reliability of LRMs' reasoning capabilities. To address these limitations, we move beyond reliance on prompts and coincidental "aha moments". Instead, we explicitly align models with three meta-abilities: deduction, induction, and abduction, using automatically generated, self-verifiable tasks. Our three stage-pipeline individual alignment, parameter-space merging, and domain-specific reinforcement learning, boosting performance by over 10\% relative to instruction-tuned baselines. Furthermore, domain-specific RL from the aligned checkpoint yields an additional 2\% average gain in the performance ceiling across math, coding, and science benchmarks, demonstrating that explicit meta-ability alignment offers a scalable and dependable foundation for reasoning. Code is available at: https://github.com/zhiyuanhubj/Meta-Ability-Alignment
[18.05.2025 12:44] Mistral response. {"id": "6a56e5237cef4c5fbfb282299feb638a", "object": "chat.completion", "created": 1747572289, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "\u8fd9\u7bc7\u6587\u7ae0\u8ba8\u8bba\u4e86\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u7684\u6f5c\u5728\u957f\u94fe\u63a8\u7406\u80fd\u529b\u3002\u4e4b\u524d\u7684\u7814\u7a76\u8868\u660e\uff0c\u7ed3\u679c\u5bfc\u5411\u7684\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u53ef\u4ee5\u5076\u7136\u5f15\u53d1\u81ea\u6211\u6821\u6b63\u3001\u56de\u6eaf\u548c\u9a8c\u8bc1\u7b49\u9ad8\u7ea7\u63a8\u7406\u884c\u4e3a\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u884c\u4e3a\u7684\u65f6\u95f4\u548c\u4e00\u81f4\u6027\u96be\u4ee5\u9884\u6d4b\u548c\u63a7\u5236\uff0c\u9650\u5236\u4e86LRMs\u7684\u63a8\u7406\u80fd\u529b\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u663e\u5f0f\u5730\u5c06\u6a21\u578b\u4e0e\u4e09\u79cd\u5143\u80fd\u529b\uff08\u6f14\u7ece\u3001\u5f52\u7eb3\u548c\u6eaf\u56e0\uff09\u5bf9\u9f50\u7684\u65b9\u6cd5\u3002\u901a\u8fc7\u4e09\u9636\u6bb5\u6d41\u6c34\u7ebf\uff0c\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u5e76\u5728\u6570\u5b66\u3001\u7f16\u7a0b\u548c\u79d1\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u8868\u73b0\u3002\u4ee3\u7801\u53ef\u5728GitHub\u4e0a\u627e\u5230\u3002"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 308, "total_tokens": 521, "completion_tokens": 213}}
[18.05.2025 12:44] Response: 这篇文章讨论了大型推理模型（LRMs）的潜在长链推理能力。之前的研究表明，结果导向的强化学习（RL）可以偶然引发自我校正、回溯和验证等高级推理行为。然而，这些行为的时间和一致性难以预测和控制，限制了LRMs的推理能力。为了解决这些问题，作者提出了显式地将模型与三种元能力（演绎、归纳和溯因）对齐的方法。通过三阶段流水线，提升了性能，并在数学、编程和科学基准测试中显著提高了表现。代码可在GitHub上找到。
[18.05.2025 12:44] Mistral request. Model: mistral-large-latest. Prompt: Write pinyin transcription for text. Text:

这篇文章讨论了大型推理模型（LRMs）的潜在长链推理能力。之前的研究表明，结果导向的强化学习（RL）可以偶然引发自我校正、回溯和验证等高级推理行为。然而，这些行为的时间和一致性难以预测和控制，限制了LRMs的推理能力。为了解决这些问题，作者提出了显式地将模型与三种元能力（演绎、归纳和溯因）对齐的方法。通过三阶段流水线，提升了性能，并在数学、编程和科学基准测试中显著提高了表现。代码可在GitHub上找到。
[18.05.2025 12:45] Mistral response. {"id": "2520e4c79d2d407cab6a640e0f43f95c", "object": "chat.completion", "created": 1747572294, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "\u8fd9\u7bc7\u6587\u7ae0\u8ba8\u8bba\u4e86\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u7684\u6f5c\u5728\u957f\u94fe\u63a8\u7406\u80fd\u529b\u3002\u4e4b\u524d\u7684\u7814\u7a76\u8868\u660e\uff0c\u7ed3\u679c\u5bfc\u5411\u7684\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u53ef\u4ee5\u5076\u7136\u5f15\u53d1\u81ea\u6211\u6821\u6b63\u3001\u56de\u6eaf\u548c\u9a8c\u8bc1\u7b49\u9ad8\u7ea7\u63a8\u7406\u884c\u4e3a\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u884c\u4e3a\u7684\u65f6\u95f4\u548c\u4e00\u81f4\u6027\u96be\u4ee5\u9884\u6d4b\u548c\u63a7\u5236\uff0c\u9650\u5236\u4e86LRMs\u7684\u63a8\u7406\u80fd\u529b\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u663e\u5f0f\u5730\u5c06\u6a21\u578b\u4e0e\u4e09\u79cd\u5143\u80fd\u529b\uff08\u6f14\u7ece\u3001\u5f52\u7eb3\u548c\u6eaf\u56e0\uff09\u5bf9\u9f50\u7684\u65b9\u6cd5\u3002\u901a\u8fc7\u4e09\u9636\u6bb5\u6d41\u6c34\u7ebf\uff0c\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u5e76\u5728\u6570\u5b66\u3001\u7f16\u7a0b\u548c\u79d1\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u8868\u73b0\u3002\u4ee3\u7801\u53ef\u5728GitHub\u4e0a\u627e\u5230\u3002\n\nZh\u00e8 pi\u0101n w\u00e9nzh\u0101ng t\u01ceol\u00f9n le d\u00e0x\u00edng tu\u012bl\u01d0 m\u00f3x\u00edng (LRMs) de qi\u00e1nz\u00e0i ch\u00e1ngli\u00e1n tu\u012bl\u01d0 n\u00e9ngl\u00ec. Zh\u012bqi\u00e1n de y\u00e1nji\u016b bi\u01ceom\u00edng, ji\u00e9gu\u01d2 d\u01ceoxi\u00e0ng de qi\u00e1ngzh\u00ec xu\u00e9x\u00ed (RL) k\u011by\u01d0 \u01d2ur\u00e1n y\u01d0nf\u0101 z\u00ecw\u01d2 ji\u00e0ozh\u00e8ng, hu\u00edtu\u00ec h\u00e9 y\u00e0nzh\u00e8ng d\u011bng g\u0101oj\u00ed tu\u012bl\u01d0 x\u00edngw\u00e9i. R\u00e1n'\u00e9r, zh\u00e8xi\u0113 x\u00edngw\u00e9i de sh\u00edji\u0101n h\u00e9 y\u012bzh\u00ecx\u00ecng n\u00e1n y\u01d0 y\u00f9c\u00e8 h\u00e9 k\u00f2ngzh\u00ec, xi\u00e0nzh\u00ec le LRMs de tu\u012bl\u01d0 n\u00e9ngl\u00ec. W\u00e8ile ji\u011bju\u00e9 zh\u00e8xi\u0113 w\u00e8nt\u00ed, zu\u00f2zh\u011b t\u00edch\u016b le xi\u01censh\u00ec de ji\u0101ng m\u00f3x\u00edng y\u01d4 s\u0101n zh\u01d2ng yu\u00e1n n\u00e9ngl\u00ec (y\u01ceny\u00ec, gu\u012bn\u00e0 h\u00e9 s\u00f9y\u012bn) du\u00ecq\u00ed de f\u0101ngf\u01ce. T\u014dnggu\u00f2 s\u0101n ji\u0113du\u00e0n li\u00fashu\u01d0xi\u00e0n, t\u00edsh\u0113ng le x\u00ecngn\u00e9ng, b\u00ecng z\u00e0i sh\u00f9xu\u00e9, bi\u0101nch\u00e9ng h\u00e9 k\u0113xu\u00e9 j\u012bzh\u01d4n c\u00e8sh\u00ec zh\u014dng xi\u01cenzh\u00f9 t\u012bg\u0101o le bi\u01ceoxi\u00e0n. D\u00e0im\u01ce k\u011b z\u00e0i GitHub sh\u00e0ng zh\u01ceo d\u00e0o."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 227, "total_tokens": 860, "completion_tokens": 633}}
[18.05.2025 12:45] Response: 这篇文章讨论了大型推理模型（LRMs）的潜在长链推理能力。之前的研究表明，结果导向的强化学习（RL）可以偶然引发自我校正、回溯和验证等高级推理行为。然而，这些行为的时间和一致性难以预测和控制，限制了LRMs的推理能力。为了解决这些问题，作者提出了显式地将模型与三种元能力（演绎、归纳和溯因）对齐的方法。通过三阶段流水线，提升了性能，并在数学、编程和科学基准测试中显著提高了表现。代码可在GitHub上找到。

Zhè piān wénzhāng tǎolùn le dàxíng tuīlǐ móxíng (LRMs) de qiánzài chánglián tuīlǐ nénglì. Zhīqián de yánjiū biǎomíng, jiéguǒ dǎoxiàng de qiángzhì xuéxí (RL) kěyǐ ǒurán yǐnfā zìwǒ jiàozhèng, huítuì hé yànzhèng děng gāojí tuīlǐ xíngwéi. Rán'ér, zhèxiē xíngwéi de shíjiān hé yīzhìxìng nán yǐ yùcè hé kòngzhì, xiànzhì le LRMs de tuīlǐ nénglì. Wèile jiějué zhèxiē wèntí, zuòzhě tíchū le xiǎnshì de jiāng móxíng yǔ sān zhǒng yuán nénglì (yǎnyì, guīnà hé sùyīn) duìqí de fāngfǎ. Tōngguò sān jiēduàn liúshuǐxiàn, tíshēng le xìngnéng, bìng zài shùxué, biānchéng hé kēxué jīzhǔn cèshì zhōng xiǎnzhù tīgāo le biǎoxiàn. Dàimǎ kě zài GitHub shàng zhǎo dào.
[18.05.2025 12:45] Mistral request. Model: mistral-large-latest. Prompt: Write vocab of difficult words for this text as an array of objects with fields 'word', 'pinyin', 'trans'. Return as python list without formatting. Return list and nothing else. Text:

这篇文章讨论了大型推理模型（LRMs）的潜在长链推理能力。之前的研究表明，结果导向的强化学习（RL）可以偶然引发自我校正、回溯和验证等高级推理行为。然而，这些行为的时间和一致性难以预测和控制，限制了LRMs的推理能力。为了解决这些问题，作者提出了显式地将模型与三种元能力（演绎、归纳和溯因）对齐的方法。通过三阶段流水线，提升了性能，并在数学、编程和科学基准测试中显著提高了表现。代码可在GitHub上找到。
[18.05.2025 12:45] Mistral response. {"id": "5fc68396d3ac436ba8eb265da6f85a24", "object": "chat.completion", "created": 1747572305, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "[\n    {\"word\": \"\u8ba8\u8bba\", \"pinyin\": \"t\u01ceo l\u00f9n\", \"trans\": \"discuss\"},\n    {\"word\": \"\u6f5c\u5728\", \"pinyin\": \"qi\u00e1n z\u00e0i\", \"trans\": \"potential\"},\n    {\"word\": \"\u957f\u94fe\", \"pinyin\": \"ch\u00e1ng li\u00e0n\", \"trans\": \"long-chain\"},\n    {\"word\": \"\u63a8\u7406\", \"pinyin\": \"tu\u012b l\u01d0\", \"trans\": \"reasoning\"},\n    {\"word\": \"\u80fd\u529b\", \"pinyin\": \"n\u00e9ng l\u00ec\", \"trans\": \"ability\"},\n    {\"word\": \"\u4e4b\u524d\", \"pinyin\": \"zh\u012b qi\u00e1n\", \"trans\": \"before\"},\n    {\"word\": \"\u7814\u7a76\", \"pinyin\": \"y\u00e1n ji\u016b\", \"trans\": \"research\"},\n    {\"word\": \"\u8868\u660e\", \"pinyin\": \"bi\u01ceo m\u00edng\", \"trans\": \"indicate\"},\n    {\"word\": \"\u7ed3\u679c\", \"pinyin\": \"ji\u00e9 gu\u01d2\", \"trans\": \"result\"},\n    {\"word\": \"\u5bfc\u5411\", \"pinyin\": \"d\u01ceo xi\u00e0ng\", \"trans\": \"oriented\"},\n    {\"word\": \"\u5f3a\u5316\", \"pinyin\": \"qi\u00e1ng hu\u00e0\", \"trans\": \"reinforcement\"},\n    {\"word\": \"\u5b66\u4e60\", \"pinyin\": \"xu\u00e9 x\u00ed\", \"trans\": \"learning\"},\n    {\"word\": \"\u5076\u7136\", \"pinyin\": \"\u01d2u r\u00e1n\", \"trans\": \"occasionally\"},\n    {\"word\": \"\u5f15\u53d1\", \"pinyin\": \"y\u01d0n f\u0101\", \"trans\": \"trigger\"},\n    {\"word\": \"\u81ea\u6211\", \"pinyin\": \"z\u00ec w\u01d2\", \"trans\": \"self\"},\n    {\"word\": \"\u6821\u6b63\", \"pinyin\": \"ji\u00e0o zh\u00e8ng\", \"trans\": \"correct\"},\n    {\"word\": \"\u56de\u6eaf\", \"pinyin\": \"hu\u00ed s\u00f9\", \"trans\": \"backtrack\"},\n    {\"word\": \"\u9a8c\u8bc1\", \"pinyin\": \"y\u00e0n zh\u00e8ng\", \"trans\": \"verify\"},\n    {\"word\": \"\u884c\u4e3a\", \"pinyin\": \"x\u00edng w\u00e9i\", \"trans\": \"behavior\"},\n    {\"word\": \"\u4e00\u81f4\u6027\", \"pinyin\": \"y\u012b zh\u00ec x\u00ecng\", \"trans\": \"consistency\"},\n    {\"word\": \"\u9884\u6d4b\", \"pinyin\": \"y\u00f9 c\u00e8\", \"trans\": \"predict\"},\n    {\"word\": \"\u63a7\u5236\", \"pinyin\": \"k\u00f2ng zh\u00ec\", \"trans\": \"control\"},\n    {\"word\": \"\u9650\u5236\", \"pinyin\": \"xi\u00e0n zh\u00ec\", \"trans\": \"limit\"},\n    {\"word\": \"\u63d0\u51fa\", \"pinyin\": \"t\u00ed ch\u016b\", \"trans\": \"propose\"},\n    {\"word\": \"\u663e\u5f0f\", \"pinyin\": \"xi\u01cen sh\u00ec\", \"trans\": \"explicit\"},\n    {\"word\": \"\u5bf9\u9f50\", \"pinyin\": \"du\u00ec q\u00ed\", \"trans\": \"align\"},\n    {\"word\": \"\u65b9\u6cd5\", \"pinyin\": \"f\u0101ng f\u01ce\", \"trans\": \"method\"},\n    {\"word\": \"\u5143\", \"pinyin\": \"yu\u00e1n\", \"trans\": \"meta\"},\n    {\"word\": \"\u80fd\u529b\", \"pinyin\": \"n\u00e9ng l\u00ec\", \"trans\": \"ability\"},\n    {\"word\": \"\u6f14\u7ece\", \"pinyin\": \"y\u01cen y\u00ec\", \"trans\": \"deduction\"},\n    {\"word\": \"\u5f52\u7eb3\", \"pinyin\": \"gu\u012b n\u00e0\", \"trans\": \"induction\"},\n    {\"word\": \"\u6eaf\u56e0\", \"pinyin\": \"s\u00f9 y\u012bn\", \"trans\": \"abduction\"},\n    {\"word\": \"\u4e09\u9636\u6bb5\", \"pinyin\": \"s\u0101n ji\u0113 du\u00e0n\", \"trans\": \"three-stage\"},\n    {\"word\": \"\u6d41\u6c34\u7ebf\", \"pinyin\": \"li\u00fa shu\u01d0 xi\u00e0n\", \"trans\": \"pipeline\"},\n    {\"word\": \"\u63d0\u5347\", \"pinyin\": \"t\u00ed sh\u0113ng\", \"trans\": \"enhance\"},\n    {\"word\": \"\u6027\u80fd\", \"pinyin\": \"x\u00ecng n\u00e9ng\", \"trans\": \"performance\"},\n    {\"word\": \"\u663e\u8457\", \"pinyin\": \"xi\u01cen zh\u00f9\", \"trans\": \"significant\"},\n    {\"word\": \"\u63d0\u9ad8\", \"pinyin\": \"t\u00ed g\u0101o\", \"trans\": \"improve\"},\n    {\"word\": \"\u8868\u73b0\", \"pinyin\": \"bi\u01ceo xi\u00e0n\", \"trans\": \"performance\"},\n    {\"word\": \"\u4ee3\u7801\", \"pinyin\": \"d\u00e0i m\u01ce\", \"trans\": \"code\"},\n    {\"word\": \"GitHub\", \"pinyin\": \"GitHub\", \"trans\": \"GitHub\"}\n]"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 259, "total_tokens": 1430, "completion_tokens": 1171}}
[18.05.2025 12:45] Response: [
    {"word": "讨论", "pinyin": "tǎo lùn", "trans": "discuss"},
    {"word": "潜在", "pinyin": "qián zài", "trans": "potential"},
    {"word": "长链", "pinyin": "cháng liàn", "trans": "long-chain"},
    {"word": "推理", "pinyin": "tuī lǐ", "trans": "reasoning"},
    {"word": "能力", "pinyin": "néng lì", "trans": "ability"},
    {"word": "之前", "pinyin": "zhī qián", "trans": "before"},
    {"word": "研究", "pinyin": "yán jiū", "trans": "research"},
    {"word": "表明", "pinyin": "biǎo míng", "trans": "indicate"},
    {"word": "结果", "pinyin": "jié guǒ", "trans": "result"},
    {"word": "导向", "pinyin": "dǎo xiàng", "trans": "oriented"},
    {"word": "强化", "pinyin": "qiáng huà", "trans": "reinforcement"},
    {"word": "学习", "pinyin": "xué xí", "trans": "learning"},
    {"word": "偶然", "pinyin": "ǒu rán", "trans": "occasionally"},
    {"word": "引发", "pinyin": "yǐn fā", "trans": "trigger"},
    {"word": "自我", "pinyin": "zì wǒ", "trans": "self"},
    {"word": "校正", "pinyin": "jiào zhèng", "trans": "correct"},
    {"word": "回溯", "pinyin": "huí sù", "trans": "backtrack"},
    {"word": "验证", "pinyin": "yàn zhèng", "trans": "verify"},
    {"word": "行为", "pinyin": "xíng wéi", "trans": "behavior"},
    {"word": "一致性", "pinyin": "yī zhì xìng", "trans": "consistency"},
    {"word": "预测", "pinyin": "yù cè", "trans": "predict"},
    {"word": "控制", "pinyin": "kòng zhì", "trans": "control"},
    {"word": "限制", "pinyin": "xiàn zhì", "trans": "limit"},
    {"word": "提出", "pinyin": "tí chū", "trans": "propose"},
    {"word": "显式", "pinyin": "xiǎn shì", "trans": "explicit"},
    {"word": "对齐", "pinyin": "duì qí", "trans": "align"},
    {"word": "方法", "pinyin": "fāng fǎ", "trans": "method"},
    {"word": "元", "pinyin": "yuán", "trans": "meta"},
    {"word": "能力", "pinyin": "néng lì", "trans": "ability"},
    {"word": "演绎", "pinyin": "yǎn yì", "trans": "deduction"},
    {"word": "归纳", "pinyin": "guī nà", "trans": "induction"},
    {"word": "溯因", "pinyin": "sù yīn", "trans": "abduction"},
    {"word": "三阶段", "pinyin": "sān jiē duàn", "trans": "three-stage"},
    {"word": "流水线", "pinyin": "liú shuǐ xiàn", "trans": "pipeline"},
    {"word": "提升", "pinyin": "tí shēng", "trans": "enhance"},
    {"word": "性能", "pinyin": "xìng néng", "trans": "performance"},
    {"word": "显著", "pinyin": "xiǎn zhù", "trans": "significant"},
    {"word": "提高", "pinyin": "tí gāo", "trans": "improve"},
    {"word": "表现", "pinyin": "biǎo xiàn", "trans": "performance"},
    {"word": "代码", "pinyin": "dài mǎ", "trans": "code"},
    {"word": "GitHub", "pinyin": "GitHub", "trans": "GitHub"}
]
[18.05.2025 12:45] Mistral request. Model: mistral-large-latest. Prompt: Translate this text in English. Text:

这篇文章讨论了大型推理模型（LRMs）的潜在长链推理能力。之前的研究表明，结果导向的强化学习（RL）可以偶然引发自我校正、回溯和验证等高级推理行为。然而，这些行为的时间和一致性难以预测和控制，限制了LRMs的推理能力。为了解决这些问题，作者提出了显式地将模型与三种元能力（演绎、归纳和溯因）对齐的方法。通过三阶段流水线，提升了性能，并在数学、编程和科学基准测试中显著提高了表现。代码可在GitHub上找到。
[18.05.2025 12:45] Mistral response. {"id": "7b646ea97f594518878649634ff293d4", "object": "chat.completion", "created": 1747572315, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "This article discusses the potential long-chain reasoning capabilities of Large Reasoning Models (LRMs). Previous research has shown that result-oriented reinforcement learning (RL) can occasionally trigger advanced reasoning behaviors such as self-correction, backtracking, and verification. However, the timing and consistency of these behaviors are difficult to predict and control, limiting the reasoning capabilities of LRMs. To address these issues, the authors propose a method that explicitly aligns the model with three meta-abilities: deduction, induction, and abduction. Through a three-stage pipeline, performance is enhanced, and significant improvements are achieved in mathematical, programming, and scientific benchmark tests. The code can be found on GitHub."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 225, "total_tokens": 378, "completion_tokens": 153}}
[18.05.2025 12:45] Response: This article discusses the potential long-chain reasoning capabilities of Large Reasoning Models (LRMs). Previous research has shown that result-oriented reinforcement learning (RL) can occasionally trigger advanced reasoning behaviors such as self-correction, backtracking, and verification. However, the timing and consistency of these behaviors are difficult to predict and control, limiting the reasoning capabilities of LRMs. To address these issues, the authors propose a method that explicitly aligns the model with three meta-abilities: deduction, induction, and abduction. Through a three-stage pipeline, performance is enhanced, and significant improvements are achieved in mathematical, programming, and scientific benchmark tests. The code can be found on GitHub.
[18.05.2025 12:45] Renaming data file.
[18.05.2025 12:45] Renaming previous data. hf_papers.json to ./d/2025-05-16.json
[18.05.2025 12:45] Saving new data file.
[18.05.2025 12:45] Generating page.
[18.05.2025 12:45] Renaming previous page.
[18.05.2025 12:45] Renaming previous data. index.html to ./d/2025-05-16.html
[18.05.2025 12:45] [Experimental] Generating Chinese page for reading.
[18.05.2025 12:45] Chinese vocab [{'word': '讨论', 'pinyin': 'tǎo lùn', 'trans': 'discuss'}, {'word': '潜在', 'pinyin': 'qián zài', 'trans': 'potential'}, {'word': '长链', 'pinyin': 'cháng liàn', 'trans': 'long-chain'}, {'word': '推理', 'pinyin': 'tuī lǐ', 'trans': 'reasoning'}, {'word': '能力', 'pinyin': 'néng lì', 'trans': 'ability'}, {'word': '之前', 'pinyin': 'zhī qián', 'trans': 'before'}, {'word': '研究', 'pinyin': 'yán jiū', 'trans': 'research'}, {'word': '表明', 'pinyin': 'biǎo míng', 'trans': 'indicate'}, {'word': '结果', 'pinyin': 'jié guǒ', 'trans': 'result'}, {'word': '导向', 'pinyin': 'dǎo xiàng', 'trans': 'oriented'}, {'word': '强化', 'pinyin': 'qiáng huà', 'trans': 'reinforcement'}, {'word': '学习', 'pinyin': 'xué xí', 'trans': 'learning'}, {'word': '偶然', 'pinyin': 'ǒu rán', 'trans': 'occasionally'}, {'word': '引发', 'pinyin': 'yǐn fā', 'trans': 'trigger'}, {'word': '自我', 'pinyin': 'zì wǒ', 'trans': 'self'}, {'word': '校正', 'pinyin': 'jiào zhèng', 'trans': 'correct'}, {'word': '回溯', 'pinyin': 'huí sù', 'trans': 'backtrack'}, {'word': '验证', 'pinyin': 'yàn zhèng', 'trans': 'verify'}, {'word': '行为', 'pinyin': 'xíng wéi', 'trans': 'behavior'}, {'word': '一致性', 'pinyin': 'yī zhì xìng', 'trans': 'consistency'}, {'word': '预测', 'pinyin': 'yù cè', 'trans': 'predict'}, {'word': '控制', 'pinyin': 'kòng zhì', 'trans': 'control'}, {'word': '限制', 'pinyin': 'xiàn zhì', 'trans': 'limit'}, {'word': '提出', 'pinyin': 'tí chū', 'trans': 'propose'}, {'word': '显式', 'pinyin': 'xiǎn shì', 'trans': 'explicit'}, {'word': '对齐', 'pinyin': 'duì qí', 'trans': 'align'}, {'word': '方法', 'pinyin': 'fāng fǎ', 'trans': 'method'}, {'word': '元', 'pinyin': 'yuán', 'trans': 'meta'}, {'word': '能力', 'pinyin': 'néng lì', 'trans': 'ability'}, {'word': '演绎', 'pinyin': 'yǎn yì', 'trans': 'deduction'}, {'word': '归纳', 'pinyin': 'guī nà', 'trans': 'induction'}, {'word': '溯因', 'pinyin': 'sù yīn', 'trans': 'abduction'}, {'word': '三阶段', 'pinyin': 'sān jiē duàn', 'trans': 'three-stage'}, {'word': '流水线', 'pinyin': 'liú shuǐ xiàn', 'trans': 'pipeline'}, {'word': '提升', 'pinyin': 'tí shēng', 'trans': 'enhance'}, {'word': '性能', 'pinyin': 'xìng néng', 'trans': 'performance'}, {'word': '显著', 'pinyin': 'xiǎn zhù', 'trans': 'significant'}, {'word': '提高', 'pinyin': 'tí gāo', 'trans': 'improve'}, {'word': '表现', 'pinyin': 'biǎo xiàn', 'trans': 'performance'}, {'word': '代码', 'pinyin': 'dài mǎ', 'trans': 'code'}, {'word': 'GitHub', 'pinyin': 'GitHub', 'trans': 'GitHub'}]
[18.05.2025 12:45] Renaming previous Chinese page.
[18.05.2025 12:45] Renaming previous data. zh.html to ./d/2025-05-17_zh_reading_task.html
[18.05.2025 12:45] Writing Chinese reading task.
[18.05.2025 12:45] Writing result.
[18.05.2025 12:45] Renaming log file.
[18.05.2025 12:45] Renaming previous data. log.txt to ./logs/2025-05-18_last_log.txt
