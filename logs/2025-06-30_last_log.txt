[30.06.2025 00:59] Read previous papers.
[30.06.2025 00:59] Generating top page (month).
[30.06.2025 00:59] Writing top page (month).
[30.06.2025 02:54] Read previous papers.
[30.06.2025 02:54] Get feed.
[30.06.2025 02:54] Extract page data from URL. URL: https://huggingface.co/papers/2506.21862
[30.06.2025 02:54] Extract page data from URL. URL: https://huggingface.co/papers/2506.21656
[30.06.2025 02:54] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[30.06.2025 02:54] Downloading and parsing papers (pdf, html). Total: 2.
[30.06.2025 02:54] Downloading and parsing paper https://huggingface.co/papers/2506.21862.
[30.06.2025 02:54] Downloading paper 2506.21862 from http://arxiv.org/pdf/2506.21862v1...
[30.06.2025 02:54] Extracting affiliations from text.
[30.06.2025 02:54] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 7 2 ] . [ 1 2 6 8 1 2 . 6 0 5 2 : r LLaVA-Scissor: Token Compression with Semantic Connected Components for Video LLMs Boyuan Sun1,2 Jiaxing Zhao2 Xihan Wei2 Qibin Hou1 1VCIP, School of Computer Science, Nankai University 2Tongyi Lab, Alibaba Group boyuansun@mail.nankai.edu.cn, houqb@nankai.edu.cn {zjx244036, xihan.wxh}@alibaba-inc.com "
[30.06.2025 02:54] Response: ```python
["VCIP, School of Computer Science, Nankai University", "Tongyi Lab, Alibaba Group"]
```
[30.06.2025 02:54] Deleting PDF ./assets/pdf/2506.21862.pdf.
[30.06.2025 02:54] Success.
[30.06.2025 02:54] Downloading and parsing paper https://huggingface.co/papers/2506.21656.
[30.06.2025 02:54] Downloading paper 2506.21656 from http://arxiv.org/pdf/2506.21656v1...
[30.06.2025 02:54] Extracting affiliations from text.
[30.06.2025 02:54] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Fine-Grained Preference Optimization Improves Spatial Reasoning in VLMs Yifan Shen1, Yuanzhe Liu2, Jingyuan Zhu2, Xu Cao1, Xiaofeng Zhang3, Yixiao He1, Wenming Ye4, James Matthew Rehg1, Ismini Lourentzou1 {yifan26,lourent2}@illinois.edu 1University of Illinois Urbana-Champaign 2University of Pennsylvania 3Shanghai Jiao Tong University 4Google 5 2 0 2 6 2 ] . [ 1 6 5 6 1 2 . 6 0 5 2 : r Abstract. Current Vision-Language Models (VLMs) struggle with fine-grained spatial reasoning, particularly when multi-step logic and precise spatial alignment are required. In this work, we introduce SpatialReasoner-R1, vision-language reasoning model designed to address these limitations. To construct high-quality supervision for spatial reasoning, we design Multi-Model Monte Carlo Tree Search (M3CTS) method that generates diverse, logically consistent Long Chain-of-Thought (LongCoT) reasoning trajectories. In addition, we propose fine-grained Direct Preference Optimization (f DPO), which introduces segment-specific preference granularity for descriptive grounding and logical reasoning, guided by spatial reward mechanism that evaluates candidate responses based on visual consistency, spatial grounding, and logical coherence. Experimental results demonstrate that DPO achieves an average improvement of 4.1% over standard DPO across spatial quality tasks, and 9.0% gain in spatial quantity tasks. SpatialReasoner-R1, trained with DPO, sets new SoTA on SpatialRGPT-Bench, outperforming the strongest baseline by 9.8% in average accuracy, while maintaining competitive performance on general vision-language tasks. (cid:128) https://plan-lab.github.io/spatialreasoner/ 1. Introduction Vision-Language Models (VLMs) have demonstrated significant advancements in multimodal understanding tasks, such as image captioning, visual question answering, object detection, and video interpretation [2, 31, 53, 61, 93]. However, their ability to perform spatial reasoning remains limited, especially in scenario"
[30.06.2025 02:54] Response: ```python
[
    "University of Illinois Urbana-Champaign",
    "University of Pennsylvania",
    "Shanghai Jiao Tong University",
    "Google"
]
```
[30.06.2025 02:54] Deleting PDF ./assets/pdf/2506.21656.pdf.
[30.06.2025 02:54] Success.
[30.06.2025 02:54] Enriching papers with extra data.
[30.06.2025 02:54] ********************************************************************************
[30.06.2025 02:54] Abstract 0. LLaVA-Scissor, a token compression strategy for video multimodal large language models, uses Semantic Connected Components to compress tokens effectively while maintaining semantic coverage and outperforming other methods.  					AI-generated summary 				 In this paper, we present LLaVA-Scissor, a tr...
[30.06.2025 02:54] ********************************************************************************
[30.06.2025 02:54] Abstract 1. SpatialReasoner-R1, a vision-language reasoning model, uses Multi-Model Monte Carlo Tree Search and fine-grained Direct Preference Optimization to improve spatial reasoning, setting a new state-of-the-art on SPATIALRGPT-Bench.  					AI-generated summary 				 Current Vision-Language Models (VLMs) str...
[30.06.2025 02:54] Read previous papers.
[30.06.2025 02:54] Generating reviews via LLM API.
[30.06.2025 02:54] Querying the API.
[30.06.2025 02:54] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

LLaVA-Scissor, a token compression strategy for video multimodal large language models, uses Semantic Connected Components to compress tokens effectively while maintaining semantic coverage and outperforming other methods.  					AI-generated summary 				 In this paper, we present LLaVA-Scissor, a training-free token compression strategy designed for video multimodal large language models. Previous methods mostly attempt to compress tokens based on attention scores, but fail to effectively capture all semantic regions and often lead to token redundancy. Differently, we propose to leverage the Semantic Connected Components (SCC) approach that assigns tokens to distinct semantic regions within the token set, ensuring comprehensive semantic coverage. The outcome is a two-step spatio-temporal token compression strategy that utilizes SCC in both spatial and temporal domains. This strategy can effectively compress tokens by representing the entire video with a set of non-overlapping semantic tokens. We conduct extensive evaluations of the token compression capabilities of LLaVA-Scissor across diverse video understanding benchmarks, including video question answering, long video understanding, and comprehensive multi-choices benchmarks. Experimental results show that the proposed LLaVA-Scissor outperforms other token compression methods, achieving superior performance in various video understanding benchmarks, particularly at low token retention ratios. Project page: https://github.com/HumanMLLM/LLaVA-Scissor.
[30.06.2025 02:55] Error getting data: Error code: 529 - {'type': 'error', 'error': {'type': 'overloaded_error', 'message': 'Overloaded'}}
[30.06.2025 02:55] Querying the API.
[30.06.2025 02:55] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

SpatialReasoner-R1, a vision-language reasoning model, uses Multi-Model Monte Carlo Tree Search and fine-grained Direct Preference Optimization to improve spatial reasoning, setting a new state-of-the-art on SPATIALRGPT-Bench.  					AI-generated summary 				 Current Vision-Language Models (VLMs) struggle with fine-grained spatial reasoning, particularly when multi-step logic and precise spatial alignment are required. In this work, we introduce SpatialReasoner-R1, a vision-language reasoning model designed to address these limitations. To construct high-quality supervision for spatial reasoning, we design a Multi-Model Monte Carlo Tree Search (M3CTS) method that generates diverse, logically consistent Long Chain-of-Thought (LongCoT) reasoning trajectories. In addition, we propose fine-grained Direct Preference Optimization (fDPO), which introduces segment-specific preference granularity for descriptive grounding and logical reasoning, guided by a spatial reward mechanism that evaluates candidate responses based on visual consistency, spatial grounding, and logical coherence. Experimental results demonstrate that fDPO achieves an average improvement of 4.1% over standard DPO across spatial quality tasks, and a 9.0% gain in spatial quantity tasks. SpatialReasoner-R1, trained with fDPO, sets a new SoTA on SPATIALRGPT-Bench, outperforming the strongest baseline by 9.8% in average accuracy, while maintaining competitive performance on general vision-language tasks.
[30.06.2025 02:55] Error getting data: Error code: 529 - {'type': 'error', 'error': {'type': 'overloaded_error', 'message': 'Overloaded'}}
[30.06.2025 02:55] Renaming data file.
[30.06.2025 02:55] Renaming previous data. hf_papers.json to ./d/2025-06-30.json
[30.06.2025 02:55] Saving new data file.
[30.06.2025 02:55] Generating page.
[30.06.2025 02:55] Renaming previous page.
[30.06.2025 02:55] Renaming previous data. index.html to ./d/2025-06-30.html
[30.06.2025 02:55] Writing result.
[30.06.2025 02:55] Renaming log file.
[30.06.2025 02:55] Renaming previous data. log.txt to ./logs/2025-06-30_last_log.txt
