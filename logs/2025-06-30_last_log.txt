[30.06.2025 16:15] Read previous papers.
[30.06.2025 16:15] Generating top page (month).
[30.06.2025 16:15] Writing top page (month).
[30.06.2025 17:10] Read previous papers.
[30.06.2025 17:10] Get feed.
[30.06.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2506.17450
[30.06.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2506.21862
[30.06.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2506.21416
[30.06.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2506.21356
[30.06.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2506.20279
[30.06.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2506.21628
[30.06.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21411
[30.06.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2506.22432
[30.06.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2506.22434
[30.06.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2506.21656
[30.06.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2506.22419
[30.06.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2506.19741
[30.06.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2506.21594
[30.06.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2506.18330
[30.06.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2506.21718
[30.06.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2506.21355
[30.06.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2506.17859
[30.06.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2506.22149
[30.06.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2506.22049
[30.06.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2506.21476
[30.06.2025 17:10] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[30.06.2025 17:10] No deleted papers detected.
[30.06.2025 17:10] Downloading and parsing papers (pdf, html). Total: 20.
[30.06.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2506.17450.
[30.06.2025 17:10] Extra JSON file exists (./assets/json/2506.17450.json), skip PDF parsing.
[30.06.2025 17:10] Paper image links file exists (./assets/img_data/2506.17450.json), skip HTML parsing.
[30.06.2025 17:10] Success.
[30.06.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2506.21862.
[30.06.2025 17:10] Extra JSON file exists (./assets/json/2506.21862.json), skip PDF parsing.
[30.06.2025 17:10] Paper image links file exists (./assets/img_data/2506.21862.json), skip HTML parsing.
[30.06.2025 17:10] Success.
[30.06.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2506.21416.
[30.06.2025 17:10] Extra JSON file exists (./assets/json/2506.21416.json), skip PDF parsing.
[30.06.2025 17:10] Paper image links file exists (./assets/img_data/2506.21416.json), skip HTML parsing.
[30.06.2025 17:10] Success.
[30.06.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2506.21356.
[30.06.2025 17:10] Extra JSON file exists (./assets/json/2506.21356.json), skip PDF parsing.
[30.06.2025 17:10] Paper image links file exists (./assets/img_data/2506.21356.json), skip HTML parsing.
[30.06.2025 17:10] Success.
[30.06.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2506.20279.
[30.06.2025 17:10] Extra JSON file exists (./assets/json/2506.20279.json), skip PDF parsing.
[30.06.2025 17:10] Paper image links file exists (./assets/img_data/2506.20279.json), skip HTML parsing.
[30.06.2025 17:10] Success.
[30.06.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2506.21628.
[30.06.2025 17:10] Extra JSON file exists (./assets/json/2506.21628.json), skip PDF parsing.
[30.06.2025 17:10] Paper image links file exists (./assets/img_data/2506.21628.json), skip HTML parsing.
[30.06.2025 17:10] Success.
[30.06.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2505.21411.
[30.06.2025 17:10] Extra JSON file exists (./assets/json/2505.21411.json), skip PDF parsing.
[30.06.2025 17:10] Paper image links file exists (./assets/img_data/2505.21411.json), skip HTML parsing.
[30.06.2025 17:10] Success.
[30.06.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2506.22432.
[30.06.2025 17:10] Extra JSON file exists (./assets/json/2506.22432.json), skip PDF parsing.
[30.06.2025 17:10] Paper image links file exists (./assets/img_data/2506.22432.json), skip HTML parsing.
[30.06.2025 17:10] Success.
[30.06.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2506.22434.
[30.06.2025 17:10] Extra JSON file exists (./assets/json/2506.22434.json), skip PDF parsing.
[30.06.2025 17:10] Paper image links file exists (./assets/img_data/2506.22434.json), skip HTML parsing.
[30.06.2025 17:10] Success.
[30.06.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2506.21656.
[30.06.2025 17:10] Extra JSON file exists (./assets/json/2506.21656.json), skip PDF parsing.
[30.06.2025 17:10] Paper image links file exists (./assets/img_data/2506.21656.json), skip HTML parsing.
[30.06.2025 17:10] Success.
[30.06.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2506.22419.
[30.06.2025 17:10] Extra JSON file exists (./assets/json/2506.22419.json), skip PDF parsing.
[30.06.2025 17:10] Paper image links file exists (./assets/img_data/2506.22419.json), skip HTML parsing.
[30.06.2025 17:10] Success.
[30.06.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2506.19741.
[30.06.2025 17:10] Extra JSON file exists (./assets/json/2506.19741.json), skip PDF parsing.
[30.06.2025 17:10] Paper image links file exists (./assets/img_data/2506.19741.json), skip HTML parsing.
[30.06.2025 17:10] Success.
[30.06.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2506.21594.
[30.06.2025 17:10] Extra JSON file exists (./assets/json/2506.21594.json), skip PDF parsing.
[30.06.2025 17:10] Paper image links file exists (./assets/img_data/2506.21594.json), skip HTML parsing.
[30.06.2025 17:10] Success.
[30.06.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2506.18330.
[30.06.2025 17:10] Extra JSON file exists (./assets/json/2506.18330.json), skip PDF parsing.
[30.06.2025 17:10] Paper image links file exists (./assets/img_data/2506.18330.json), skip HTML parsing.
[30.06.2025 17:10] Success.
[30.06.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2506.21718.
[30.06.2025 17:10] Extra JSON file exists (./assets/json/2506.21718.json), skip PDF parsing.
[30.06.2025 17:10] Paper image links file exists (./assets/img_data/2506.21718.json), skip HTML parsing.
[30.06.2025 17:10] Success.
[30.06.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2506.21355.
[30.06.2025 17:10] Extra JSON file exists (./assets/json/2506.21355.json), skip PDF parsing.
[30.06.2025 17:10] Paper image links file exists (./assets/img_data/2506.21355.json), skip HTML parsing.
[30.06.2025 17:10] Success.
[30.06.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2506.17859.
[30.06.2025 17:10] Extra JSON file exists (./assets/json/2506.17859.json), skip PDF parsing.
[30.06.2025 17:10] Paper image links file exists (./assets/img_data/2506.17859.json), skip HTML parsing.
[30.06.2025 17:10] Success.
[30.06.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2506.22149.
[30.06.2025 17:10] Extra JSON file exists (./assets/json/2506.22149.json), skip PDF parsing.
[30.06.2025 17:10] Paper image links file exists (./assets/img_data/2506.22149.json), skip HTML parsing.
[30.06.2025 17:10] Success.
[30.06.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2506.22049.
[30.06.2025 17:10] Extra JSON file exists (./assets/json/2506.22049.json), skip PDF parsing.
[30.06.2025 17:10] Paper image links file exists (./assets/img_data/2506.22049.json), skip HTML parsing.
[30.06.2025 17:10] Success.
[30.06.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2506.21476.
[30.06.2025 17:10] Extra JSON file exists (./assets/json/2506.21476.json), skip PDF parsing.
[30.06.2025 17:10] Paper image links file exists (./assets/img_data/2506.21476.json), skip HTML parsing.
[30.06.2025 17:10] Success.
[30.06.2025 17:10] Enriching papers with extra data.
[30.06.2025 17:10] ********************************************************************************
[30.06.2025 17:10] Abstract 0. A generative visual compositing framework using a diffusion model for scene editing and composition with source masking and simulated object jittering.  					AI-generated summary 				 We present BlenderFusion, a generative visual compositing framework that synthesizes new scenes by recomposing objec...
[30.06.2025 17:10] ********************************************************************************
[30.06.2025 17:10] Abstract 1. LLaVA-Scissor, a token compression strategy for video multimodal large language models, uses Semantic Connected Components to compress tokens effectively while maintaining semantic coverage and outperforming other methods.  					AI-generated summary 				 In this paper, we present LLaVA-Scissor, a tr...
[30.06.2025 17:10] ********************************************************************************
[30.06.2025 17:10] Abstract 2. XVerse enhances text-to-image generation by enabling precise and independent control over multiple subjects using token-specific text-stream modulation, improving image coherence and fidelity.  					AI-generated summary 				 Achieving fine-grained control over subject identity and semantic attribute...
[30.06.2025 17:10] ********************************************************************************
[30.06.2025 17:10] Abstract 3. ShotBench and ShotQA datasets, along with ShotVL model, enhance AI's understanding and generation capabilities by specifically targeting nuanced cinematic language comprehension.  					AI-generated summary 				 Cinematography, the fundamental visual language of film, is essential for conveying narra...
[30.06.2025 17:10] ********************************************************************************
[30.06.2025 17:10] Abstract 4. DenseDiT, a generative model-based approach, achieves superior performance in real-world dense prediction tasks using minimal training data compared to existing methods.  					AI-generated summary 				 Dense prediction tasks hold significant importance of computer vision, aiming to learn pixel-wise ...
[30.06.2025 17:10] ********************************************************************************
[30.06.2025 17:10] Abstract 5. ARK is an open-source Python-first framework that integrates modern imitation-learning algorithms and seamless simulation-physical robot interactions to simplify robotics development and deployment.  					AI-generated summary 				 Robotics has made remarkable hardware strides-from DARPA's Urban and ...
[30.06.2025 17:10] ********************************************************************************
[30.06.2025 17:10] Abstract 6. Mixture of Grouped Experts (MoGE) improves expert load balancing and execution efficiency for large language models, enhancing throughput and cost-to-performance on Ascend NPUs.  					AI-generated summary 				 The surgence of Mixture of Experts (MoE) in Large Language Models promises a small price o...
[30.06.2025 17:10] ********************************************************************************
[30.06.2025 17:10] Abstract 7. A novel framework integrates 3D proxy meshes and a decoupled video diffusion model to achieve precise and consistent video editing.  					AI-generated summary 				 Recent advances in deep generative modeling have unlocked unprecedented opportunities for video synthesis. In real-world applications, h...
[30.06.2025 17:10] ********************************************************************************
[30.06.2025 17:10] Abstract 8. Self-supervised learning using image triplets enhances the reasoning ability of Vision-Language Models (VLMs) on multi-image tasks without the need for human-annotated question-answer pairs.  					AI-generated summary 				 This work explores enabling Chain-of-Thought (CoT) reasoning to link visual c...
[30.06.2025 17:10] ********************************************************************************
[30.06.2025 17:10] Abstract 9. SpatialReasoner-R1, a vision-language reasoning model, uses Multi-Model Monte Carlo Tree Search and fine-grained Direct Preference Optimization to improve spatial reasoning, setting a new state-of-the-art on SPATIALRGPT-Bench.  					AI-generated summary 				 Current Vision-Language Models (VLMs) str...
[30.06.2025 17:10] ********************************************************************************
[30.06.2025 17:10] Abstract 10. An Automated LLM Speedrunning Benchmark evaluates AI agents' ability to reproduce scientific results by leveraging NanoGPT speedrun tasks, indicating that even recent reasoning LLMs struggle with re-implementing known improvements.  					AI-generated summary 				 Rapid advancements in large language...
[30.06.2025 17:10] ********************************************************************************
[30.06.2025 17:10] Abstract 11. A novel Noise Consistency Training approach integrates new control signals into pre-trained one-step generators efficiently without retraining, outperforming existing methods in quality and computational efficiency.  					AI-generated summary 				 The pursuit of efficient and controllable high-quali...
[30.06.2025 17:10] ********************************************************************************
[30.06.2025 17:10] Abstract 12. Gazal-R1, a 32-billion-parameter language model, achieves top performance in medical reasoning through strategic training, including advanced parameter-efficient techniques and reinforcement learning, providing detailed explanations for clinical decisions.  					AI-generated summary 				 We present ...
[30.06.2025 17:10] ********************************************************************************
[30.06.2025 17:10] Abstract 13. Confucius3-Math, a 14B parameter large language model, achieves state-of-the-art performance on mathematical reasoning tasks using reinforcement learning techniques and is optimized for education in China.  					AI-generated summary 				 We introduce Confucius3-Math, an open-source large language mo...
[30.06.2025 17:10] ********************************************************************************
[30.06.2025 17:10] Abstract 14. A text-to-text regression model achieves high accuracy in predicting resource efficiency for Google's Borg system, surpassing tabular methods, and demonstrates adaptability and uncertainty quantification.  					AI-generated summary 				 In many industries, predicting metric outcomes of large systems...
[30.06.2025 17:10] ********************************************************************************
[30.06.2025 17:10] Abstract 15. Current multimodal large language models show moderate to poor performance in multimodal in-context learning for medical tasks, with sensitivity to example relevance and ordering.  					AI-generated summary 				 Multimodal in-context learning (ICL) remains underexplored despite significant potential...
[30.06.2025 17:10] ********************************************************************************
[30.06.2025 17:10] Abstract 16. A hierarchical Bayesian framework explains in-context learning behavior by modeling it as a tradeoff between strategy loss and complexity, offering both explanatory power and predictive insights.  					AI-generated summary 				 Recent work analyzing in-context learning (ICL) has identified a broad s...
[30.06.2025 17:10] ********************************************************************************
[30.06.2025 17:10] Abstract 17. RetFiner, a vision-language refinement scheme, enhances self-supervised foundation models for OCT by leveraging textual data, improving their downstream performance in retinal disease classification tasks.  					AI-generated summary 				 The rise of imaging techniques such as optical coherence tomog...
[30.06.2025 17:10] ********************************************************************************
[30.06.2025 17:10] Abstract 18. Gradient-Preserving Activation Scaling (GPAS) mitigates activation variance issues in Pre-LayerNorm Transformers and enhances training dynamics across different architectures.  					AI-generated summary 				 Modern Large Language Models, such as the LLaMA, Qwen and DeepSeek series, predominantly ado...
[30.06.2025 17:10] ********************************************************************************
[30.06.2025 17:10] Abstract 19. Radial Cross-Modal Embeddings enable explicit modeling of transitive entailment in vision-language models, leading to improved performance in hierarchical species classification and retrieval tasks.  					AI-generated summary 				 Learning the hierarchical structure of data in vision-language models...
[30.06.2025 17:10] Read previous papers.
[30.06.2025 17:10] Generating reviews via LLM API.
[30.06.2025 17:10] Using data from previous issue: {"categories": ["#cv", "#diffusion", "#3d", "#training"], "emoji": "üé≠", "ru": {"title": "–ì–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –∫–æ–º–ø–æ–∑–∏—Ü–∏—è —Å—Ü–µ–Ω —Å 3D-–∫–æ–Ω—Ç—Ä–æ–ª–µ–º", "desc": "BlenderFusion - —ç—Ç–æ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –≤–∏–∑—É–∞–ª—å–Ω–æ–π –∫–æ–º–ø–æ–∑–∏—Ü–∏–∏, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏ —Å–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Å—Ü–µ–Ω. –û–Ω–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç –ø–æ –ø—Ä–∏–Ω
[30.06.2025 17:10] Using data from previous issue: {"categories": ["#training", "#benchmark", "#multimodal", "#long_context", "#dataset", "#video"], "emoji": "‚úÇÔ∏è", "ru": {"title": "–£–º–Ω–æ–µ —Å–∂–∞—Ç–∏–µ –¥–ª—è —É–º–Ω—ã—Ö –≤–∏–¥–µ–æ–º–æ–¥–µ–ª–µ–π", "desc": "LLaVA-Scissor - —ç—Ç–æ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è —Å–∂–∞—Ç–∏—è —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –≤–∏–¥–µ–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–µ—Ç–æ–¥ –°–µ–º–∞–Ω
[30.06.2025 17:10] Using data from previous issue: {"categories": ["#diffusion", "#cv"], "emoji": "üé®", "ru": {"title": "–¢–æ—á–Ω—ã–π –∫–æ–Ω—Ç—Ä–æ–ª—å –Ω–∞–¥ –º–Ω–æ–∂–µ—Å—Ç–≤–æ–º –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "XVerse - —ç—Ç–æ –Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –æ–±—ä–µ–∫—Ç–∞–º–∏. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–æ–¥—É–ª—è—Ü–∏—é —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –ø–æ—Ç–æ–∫–∞ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –∏ –Ω–µ–∑–∞–≤–∏—Å
[30.06.2025 17:10] Using data from previous issue: {"categories": ["#dataset", "#games", "#training", "#benchmark", "#open_source", "#multimodal", "#video"], "emoji": "üé¨", "ru": {"title": "–ù–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è —è–∑—ã–∫–∞ –∫–∏–Ω–æ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–æ–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö ShotBench –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∫–∏–Ω–µ–º–∞—Ç–æ
[30.06.2025 17:10] Using data from previous issue: {"categories": ["#training", "#dataset", "#optimization", "#cv", "#synthetic", "#benchmark"], "emoji": "üîç", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –ø–ª–æ—Ç–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å –º–∏–Ω–∏–º—É–º–æ–º –¥–∞–Ω–Ω—ã—Ö", "desc": "DenseDiT - —ç—Ç–æ –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–º –º–æ–¥–µ–ª—è–º –¥–ª—è –∑–∞–¥–∞—á –ø–ª–æ—Ç–Ω–æ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–º –∑—Ä–µ–Ω–∏–∏. –û–Ω –¥–æ—Å—Ç–∏–≥–∞–µ—Ç
[30.06.2025 17:10] Using data from previous issue: {"categories": ["#open_source", "#games", "#agents", "#robotics"], "emoji": "ü§ñ", "ru": {"title": "ARK: –û–±—ä–µ–¥–∏–Ω—è—è —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫—É –∏ –ò–ò –ø–æ–¥ –∫—Ä—ã–ª–æ–º Python", "desc": "ARK - —ç—Ç–æ –æ—Ç–∫—Ä—ã—Ç–∞—è Python-—Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∏, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∞—è —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã –∏–º–∏—Ç–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ —Å–∏–º—É–ª—è—Ü–∏–∏
[30.06.2025 17:10] Using data from previous issue: {"categories": ["#training", "#optimization", "#architecture", "#inference"], "emoji": "üß†", "ru": {"title": "MoGE: –ü–æ–≤—ã—à–µ–Ω–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –≥—Ä—É–ø–ø–æ–≤—É—é —ç–∫—Å–ø–µ—Ä—Ç–∏–∑—É", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ Mixture of Grouped Experts (MoGE) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –Ω–∞–≥—Ä—É
[30.06.2025 17:10] Using data from previous issue: {"categories": ["#3d", "#diffusion", "#video"], "emoji": "üé¨", "ru": {"title": "3D-–ø—Ä–æ–∫—Å–∏ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ–≥–æ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –≤–∏–¥–µ–æ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Shape-for-Motion –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ–≥–æ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –≤–∏–¥–µ–æ. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç 3D-–ø—Ä–æ–∫—Å–∏-—Å–µ—Ç–∫–∏ –∏ —Ä
[30.06.2025 17:10] Using data from previous issue: {"categories": ["#optimization", "#rl", "#benchmark", "#cv", "#dataset", "#reasoning"], "emoji": "üîç", "ru": {"title": "–°–∞–º–æ–æ–±—É—á–µ–Ω–∏–µ –ò–ò –≤–∏–∑—É–∞–ª—å–Ω—ã–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º –±–µ–∑ —É—á–∞—Å—Ç–∏—è —á–µ–ª–æ–≤–µ–∫–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ (VLM) –¥–ª—è
[30.06.2025 17:10] Using data from previous issue: {"categories": ["#training", "#architecture", "#optimization", "#multimodal", "#rlhf", "#cv", "#reasoning"], "emoji": "üß†", "ru": {"title": "–ü—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ –ò–ò –≤—ã—Ö–æ–¥–∏—Ç –Ω–∞ –Ω–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å", "desc": "SpatialReasoner-R1 - —ç—Ç–æ –Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å –∑—Ä–∏—Ç–µ–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä–∞—è —É–ª—É—á—à–∞–µ—Ç –ø—Ä–æ—Å—Ç—Ä–∞
[30.06.2025 17:10] Using data from previous issue: {"categories": ["#optimization", "#reasoning", "#science", "#agents", "#training", "#benchmark"], "emoji": "üèÉ‚Äç‚ôÇÔ∏è", "ru": {"title": "–ò–ò –ø–æ–∫–∞ –Ω–µ –≥–æ—Ç–æ–≤ –∫ –Ω–∞—É—á–Ω–æ–º—É —Å–ø–∏–¥—Ä–∞–Ω–Ω–∏–Ω–≥—É", "desc": "–≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –ê–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ —Å–ø–∏–¥—Ä–∞–Ω–Ω–∏–Ω–≥–∞ –Ø–ë–ú –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤ –≤–æ—Å–ø—Ä–æ–∏
[30.06.2025 17:10] Using data from previous issue: {"categories": ["#cv", "#training", "#diffusion", "#optimization"], "emoji": "üéõÔ∏è", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ Noise Consistency Training (NCT) –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –Ω–æ–≤—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –≤ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –æ–¥–Ω–æ—à–∞–≥
[30.06.2025 17:10] Using data from previous issue: {"categories": ["#healthcare", "#rl", "#optimization", "#survey", "#training", "#interpretability", "#reasoning"], "emoji": "ü©∫", "ru": {"title": "–ü—Ä–æ—Ä—ã–≤ –≤ –ò–ò –¥–ª—è –º–µ–¥–∏—Ü–∏–Ω—ã: Gazal-R1 - —Ç–æ—á–Ω–æ—Å—Ç—å, —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å, –æ–±—ä—è—Å–Ω–∏–º–æ—Å—Ç—å", "desc": "Gazal-R1 - —ç—Ç–æ —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å —Å 32 –º–∏–ª–ª–∏–∞—Ä–¥–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –¥–æ—Å—Ç–∏–≥–∞—é
[30.06.2025 17:10] Using data from previous issue: {"categories": ["#agi", "#open_source", "#reasoning", "#training", "#rl", "#math"], "emoji": "üßÆ", "ru": {"title": "Confucius3-Math: –ú–æ—â–Ω—ã–π –ò–ò –¥–ª—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –≤ –ö–∏—Ç–∞–µ", "desc": "Confucius3-Math - —ç—Ç–æ –±–æ–ª—å—à–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å —Å 14 –º–∏–ª–ª–∏–∞—Ä–¥–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –¥–ª—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å
[30.06.2025 17:10] Using data from previous issue: {"categories": ["#optimization", "#transfer_learning", "#training", "#architecture", "#data"], "emoji": "ü§ñ", "ru": {"title": "–¢–µ–∫—Å—Ç–æ–≤–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Ç–∞–±–ª–∏—á–Ω—ã–µ –º–µ—Ç–æ–¥—ã –≤ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ —Å–ª–æ–∂–Ω—ã—Ö —Å–∏—Å—Ç–µ–º", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç–æ–≤—É—é —Ä–µ–≥—Ä–µ—Å—Å–∏—é –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑
[30.06.2025 17:10] Using data from previous issue: {"categories": ["#games", "#healthcare", "#science", "#multimodal", "#interpretability", "#reasoning", "#benchmark"], "emoji": "ü©∫", "ru": {"title": "–û–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–∞—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–º—É –æ–±—É—á–µ–Ω–∏—é –≤ –º–µ–¥–∏—Ü–∏–Ω–µ", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –±–æ–ª—å—à–∏
[30.06.2025 17:10] Using data from previous issue: {"categories": ["#training", "#interpretability", "#reasoning", "#math"], "emoji": "üß†", "ru": {"title": "–ë–∞–π–µ—Å–æ–≤—Å–∫–∏–π –≤–∑–≥–ª—è–¥ –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ: –±–∞–ª–∞–Ω—Å –º–µ–∂–¥—É —Å—Ç—Ä–∞—Ç–µ–≥–∏–µ–π –∏ —Å–ª–æ–∂–Ω–æ—Å—Ç—å—é", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫—É—é –±–∞–π–µ—Å–æ–≤—Å–∫—É—é –º–æ–¥–µ–ª—å –¥–ª—è –æ–±—ä—è—Å–Ω–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ (ICL) –≤ –Ω–µ–π—Ä–æ–Ω
[30.06.2025 17:10] Using data from previous issue: {"categories": ["#cv", "#open_source", "#dataset", "#optimization", "#training", "#healthcare", "#data"], "emoji": "üëÅÔ∏è", "ru": {"title": "RetFiner: —É–ª—É—á—à–µ–Ω–∏–µ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –û–ö–¢ —Å –ø–æ–º–æ—â—å—é —è–∑—ã–∫–æ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏", "desc": "RetFiner - —ç—Ç–æ —Å—Ö–µ–º–∞ —É—Ç–æ—á–Ω–µ–Ω–∏—è –∑—Ä–µ–Ω–∏—è –∏ —è–∑—ã–∫–∞, –∫–æ—Ç–æ—Ä–∞—è —É–ª—É—á—à–∞–µ—Ç —Å–∞–º–æ–æ–±—É—á–∞
[30.06.2025 17:10] Using data from previous issue: {"categories": ["#training", "#architecture", "#optimization"], "emoji": "üìä", "ru": {"title": "GPAS: –£–ª—É—á—à–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ Gradient-Preserving Activation Scaling (GPAS) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ —Å –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π –Ω–æ—Ä–º–∞–ª
[30.06.2025 17:10] Using data from previous issue: {"categories": ["#multimodal", "#alignment", "#cv", "#open_source"], "emoji": "üå≥", "ru": {"title": "–†–∞–¥–∏–∞–ª—å–Ω—ã–µ –∫—Ä–æ—Å—Å-–º–æ–¥–∞–ª—å–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –∏–µ—Ä–∞—Ä—Ö–∏–π –≤ AI", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—é –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ
[30.06.2025 17:10] Renaming data file.
[30.06.2025 17:10] Renaming previous data. hf_papers.json to ./d/2025-06-30.json
[30.06.2025 17:10] Saving new data file.
[30.06.2025 17:10] Generating page.
[30.06.2025 17:10] Renaming previous page.
[30.06.2025 17:10] Renaming previous data. index.html to ./d/2025-06-30.html
[30.06.2025 17:10] Writing result.
[30.06.2025 17:10] Renaming log file.
[30.06.2025 17:10] Renaming previous data. log.txt to ./logs/2025-06-30_last_log.txt
