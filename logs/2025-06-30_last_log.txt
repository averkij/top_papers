[30.06.2025 21:10] Read previous papers.
[30.06.2025 21:10] Generating top page (month).
[30.06.2025 21:10] Writing top page (month).
[30.06.2025 22:11] Read previous papers.
[30.06.2025 22:11] Get feed.
[30.06.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.17450
[30.06.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.21862
[30.06.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.21416
[30.06.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.21356
[30.06.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.20279
[30.06.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.21628
[30.06.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21411
[30.06.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.22434
[30.06.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.22432
[30.06.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.21656
[30.06.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.22419
[30.06.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.21876
[30.06.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.21355
[30.06.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.17859
[30.06.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.21594
[30.06.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.19741
[30.06.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.18330
[30.06.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.21458
[30.06.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.21718
[30.06.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.22149
[30.06.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.21476
[30.06.2025 22:11] Extract page data from URL. URL: https://huggingface.co/papers/2506.15882
[30.06.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.22049
[30.06.2025 22:11] Extract page data from URL. URL: https://huggingface.co/papers/2506.19592
[30.06.2025 22:11] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[30.06.2025 22:11] No deleted papers detected.
[30.06.2025 22:11] Downloading and parsing papers (pdf, html). Total: 24.
[30.06.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2506.17450.
[30.06.2025 22:11] Extra JSON file exists (./assets/json/2506.17450.json), skip PDF parsing.
[30.06.2025 22:11] Paper image links file exists (./assets/img_data/2506.17450.json), skip HTML parsing.
[30.06.2025 22:11] Success.
[30.06.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2506.21862.
[30.06.2025 22:11] Extra JSON file exists (./assets/json/2506.21862.json), skip PDF parsing.
[30.06.2025 22:11] Paper image links file exists (./assets/img_data/2506.21862.json), skip HTML parsing.
[30.06.2025 22:11] Success.
[30.06.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2506.21416.
[30.06.2025 22:11] Extra JSON file exists (./assets/json/2506.21416.json), skip PDF parsing.
[30.06.2025 22:11] Paper image links file exists (./assets/img_data/2506.21416.json), skip HTML parsing.
[30.06.2025 22:11] Success.
[30.06.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2506.21356.
[30.06.2025 22:11] Extra JSON file exists (./assets/json/2506.21356.json), skip PDF parsing.
[30.06.2025 22:11] Paper image links file exists (./assets/img_data/2506.21356.json), skip HTML parsing.
[30.06.2025 22:11] Success.
[30.06.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2506.20279.
[30.06.2025 22:11] Extra JSON file exists (./assets/json/2506.20279.json), skip PDF parsing.
[30.06.2025 22:11] Paper image links file exists (./assets/img_data/2506.20279.json), skip HTML parsing.
[30.06.2025 22:11] Success.
[30.06.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2506.21628.
[30.06.2025 22:11] Extra JSON file exists (./assets/json/2506.21628.json), skip PDF parsing.
[30.06.2025 22:11] Paper image links file exists (./assets/img_data/2506.21628.json), skip HTML parsing.
[30.06.2025 22:11] Success.
[30.06.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2505.21411.
[30.06.2025 22:11] Extra JSON file exists (./assets/json/2505.21411.json), skip PDF parsing.
[30.06.2025 22:11] Paper image links file exists (./assets/img_data/2505.21411.json), skip HTML parsing.
[30.06.2025 22:11] Success.
[30.06.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2506.22434.
[30.06.2025 22:11] Extra JSON file exists (./assets/json/2506.22434.json), skip PDF parsing.
[30.06.2025 22:11] Paper image links file exists (./assets/img_data/2506.22434.json), skip HTML parsing.
[30.06.2025 22:11] Success.
[30.06.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2506.22432.
[30.06.2025 22:11] Extra JSON file exists (./assets/json/2506.22432.json), skip PDF parsing.
[30.06.2025 22:11] Paper image links file exists (./assets/img_data/2506.22432.json), skip HTML parsing.
[30.06.2025 22:11] Success.
[30.06.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2506.21656.
[30.06.2025 22:11] Extra JSON file exists (./assets/json/2506.21656.json), skip PDF parsing.
[30.06.2025 22:11] Paper image links file exists (./assets/img_data/2506.21656.json), skip HTML parsing.
[30.06.2025 22:11] Success.
[30.06.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2506.22419.
[30.06.2025 22:11] Extra JSON file exists (./assets/json/2506.22419.json), skip PDF parsing.
[30.06.2025 22:11] Paper image links file exists (./assets/img_data/2506.22419.json), skip HTML parsing.
[30.06.2025 22:11] Success.
[30.06.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2506.21876.
[30.06.2025 22:11] Extra JSON file exists (./assets/json/2506.21876.json), skip PDF parsing.
[30.06.2025 22:11] Paper image links file exists (./assets/img_data/2506.21876.json), skip HTML parsing.
[30.06.2025 22:11] Success.
[30.06.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2506.21355.
[30.06.2025 22:11] Extra JSON file exists (./assets/json/2506.21355.json), skip PDF parsing.
[30.06.2025 22:11] Paper image links file exists (./assets/img_data/2506.21355.json), skip HTML parsing.
[30.06.2025 22:11] Success.
[30.06.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2506.17859.
[30.06.2025 22:11] Extra JSON file exists (./assets/json/2506.17859.json), skip PDF parsing.
[30.06.2025 22:11] Paper image links file exists (./assets/img_data/2506.17859.json), skip HTML parsing.
[30.06.2025 22:11] Success.
[30.06.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2506.21594.
[30.06.2025 22:11] Extra JSON file exists (./assets/json/2506.21594.json), skip PDF parsing.
[30.06.2025 22:11] Paper image links file exists (./assets/img_data/2506.21594.json), skip HTML parsing.
[30.06.2025 22:11] Success.
[30.06.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2506.19741.
[30.06.2025 22:11] Extra JSON file exists (./assets/json/2506.19741.json), skip PDF parsing.
[30.06.2025 22:11] Paper image links file exists (./assets/img_data/2506.19741.json), skip HTML parsing.
[30.06.2025 22:11] Success.
[30.06.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2506.18330.
[30.06.2025 22:11] Extra JSON file exists (./assets/json/2506.18330.json), skip PDF parsing.
[30.06.2025 22:11] Paper image links file exists (./assets/img_data/2506.18330.json), skip HTML parsing.
[30.06.2025 22:11] Success.
[30.06.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2506.21458.
[30.06.2025 22:11] Extra JSON file exists (./assets/json/2506.21458.json), skip PDF parsing.
[30.06.2025 22:11] Paper image links file exists (./assets/img_data/2506.21458.json), skip HTML parsing.
[30.06.2025 22:11] Success.
[30.06.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2506.21718.
[30.06.2025 22:11] Extra JSON file exists (./assets/json/2506.21718.json), skip PDF parsing.
[30.06.2025 22:11] Paper image links file exists (./assets/img_data/2506.21718.json), skip HTML parsing.
[30.06.2025 22:11] Success.
[30.06.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2506.22149.
[30.06.2025 22:11] Extra JSON file exists (./assets/json/2506.22149.json), skip PDF parsing.
[30.06.2025 22:11] Paper image links file exists (./assets/img_data/2506.22149.json), skip HTML parsing.
[30.06.2025 22:11] Success.
[30.06.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2506.21476.
[30.06.2025 22:11] Extra JSON file exists (./assets/json/2506.21476.json), skip PDF parsing.
[30.06.2025 22:11] Paper image links file exists (./assets/img_data/2506.21476.json), skip HTML parsing.
[30.06.2025 22:11] Success.
[30.06.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2506.15882.
[30.06.2025 22:11] Downloading paper 2506.15882 from http://arxiv.org/pdf/2506.15882v1...
[30.06.2025 22:11] Extracting affiliations from text.
[30.06.2025 22:11] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 8 1 ] . [ 1 2 8 8 5 1 . 6 0 5 2 : r a Sheng Liu , Tianlang Chen , Pan Lu , Haotian Ye , Yizheng Chen , Lei Xing , James Zou * Equal technical contribution {shengl,jamesz}@stanford.edu "
[30.06.2025 22:11] Response: ```python
["Stanford University"]
```
[30.06.2025 22:11] Deleting PDF ./assets/pdf/2506.15882.pdf.
[30.06.2025 22:11] Success.
[30.06.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2506.22049.
[30.06.2025 22:11] Extra JSON file exists (./assets/json/2506.22049.json), skip PDF parsing.
[30.06.2025 22:11] Paper image links file exists (./assets/img_data/2506.22049.json), skip HTML parsing.
[30.06.2025 22:11] Success.
[30.06.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2506.19592.
[30.06.2025 22:11] Downloading paper 2506.19592 from http://arxiv.org/pdf/2506.19592v1...
[30.06.2025 22:11] Extracting affiliations from text.
[30.06.2025 22:11] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Adaptive Domain Modeling with Language Models: Multi-Agent Approach to Task Planning Harisankar Babu1,2, Philipp Schillinger1, and Tamim Asfour2 5 2 0 2 4 2 ] . [ 1 2 9 5 9 1 . 6 0 5 2 : r AbstractWe introduce TAPAS (Task-based Adaptation and Planning using AgentS), multi-agent framework that integrates Large Language Models (LLMs) with symbolic planning to solve complex tasks without the need for manually defined environment models. TAPAS employs specialized LLM-based agents that collaboratively generate and adapt domain models, initial states, and goal specifications as needed using structured tool-calling mechanisms. Through this tool-based interaction, downstream agents can request modifications from upstream agents, enabling adaptation to novel attributes and constraints without manual domain redefinition. ReAct (Reason+Act)- style execution agent, coupled with natural language plan translation, bridges the gap between dynamically generated plans and real-world robot capabilities. TAPAS demonstrates strong performance in benchmark planning domains and in the VirtualHome simulated real-world environment. I. Introduction Adaptability to unforeseen situations and evolving requirements is crucial for effective task planning in unstructured, open-world environments. For instance, dynamic environments require robots to handle incomplete information and changing task specifications. Large Language Models (LLMs) demonstrate strong generative capabilities, enabling the parsing of natural language into structured problem representations. However, leveraging these capabilities for automated planning remains an open challenge, especially in dynamic environments requiring contextual grounding, adaptability, and reasoning under uncertainty. Classical symbolic planning frameworks, while powerful, face notable limitations. They rely on meticulously defined domain models that require significant human expertise. This makes them rigid and difficult to adapt to novel scenarios, t"
[30.06.2025 22:11] Response: ```python
[]
```
[30.06.2025 22:11] Extracting affiliations from text.
[30.06.2025 22:11] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Adaptive Domain Modeling with Language Models: Multi-Agent Approach to Task Planning Harisankar Babu1,2, Philipp Schillinger1, and Tamim Asfour2 5 2 0 2 4 2 ] . [ 1 2 9 5 9 1 . 6 0 5 2 : r AbstractWe introduce TAPAS (Task-based Adaptation and Planning using AgentS), multi-agent framework that integrates Large Language Models (LLMs) with symbolic planning to solve complex tasks without the need for manually defined environment models. TAPAS employs specialized LLM-based agents that collaboratively generate and adapt domain models, initial states, and goal specifications as needed using structured tool-calling mechanisms. Through this tool-based interaction, downstream agents can request modifications from upstream agents, enabling adaptation to novel attributes and constraints without manual domain redefinition. ReAct (Reason+Act)- style execution agent, coupled with natural language plan translation, bridges the gap between dynamically generated plans and real-world robot capabilities. TAPAS demonstrates strong performance in benchmark planning domains and in the VirtualHome simulated real-world environment. I. Introduction Adaptability to unforeseen situations and evolving requirements is crucial for effective task planning in unstructured, open-world environments. For instance, dynamic environments require robots to handle incomplete information and changing task specifications. Large Language Models (LLMs) demonstrate strong generative capabilities, enabling the parsing of natural language into structured problem representations. However, leveraging these capabilities for automated planning remains an open challenge, especially in dynamic environments requiring contextual grounding, adaptability, and reasoning under uncertainty. Classical symbolic planning frameworks, while powerful, face notable limitations. They rely on meticulously defined domain models that require significant human expertise. This makes them rigid and difficult to adapt to novel scenarios, thus limiting their applicability in dynamic, realworld settings. Current LLM-based approaches, conversely, lack the structured reasoning capabilities of symbolic planners. This work addresses the critical need for planning system that combines the strengths of both: the adaptability of LLMs and the rigor of symbolic planning, thereby enhancing trustworthiness. We introduce TAPAS (Task-based Adaptation and Planning using AgentS), multi-agent framework that bridges natural language understanding and symbolic planning for adaptive task planning. Unlike traditional 1Bosch Center for Artificial Intelligence, Renningen, Germany. {harisankar.babu, philipp.schillinger}@bosch.com Institute of Technology, Karlsruhe, Germany. 2Karlsruhe asfour@kit.edu Stack b2 on b3 (cid:160) Make tower with the largest block on the bottom, the red block in the middle, and the green block on top. (cid:160) b2 b1 b3 Adding goal: (on b2 b3) Solving... Plan found! Executing... b3 b1 b2 b3 Missing predicates: size, color. Updating domain: Adding fluents: size, color. Updating stack action precondition. Adding goals... b3 b2 b2 Initial State Goal State Figure 1. TAPAS dynamically adapts domain models to accommodate new goal constraints. If goal can be represented with existing predicates, it directly generates and executes plan. Otherwise, TAPAS detects missing predicates, updates the domain model by modifying action constraints, and integrates the new goal before solving. This allows symbolic planners to generalize beyond predefined representations while adapting to evolving task requirements. approaches, TAPAS employs set of specialized LLMbased agents, each responsible for distinct phase of problem formulation: domain modeling, initial state generation, and goal specification. This modular design ensures structured problem representation while allowing flexible adaptation to new task constraints. Using the Unified Planning (UP) framework, TAPAS adaptively updates domain representations through iterative refinement, automatically incorporating novel attributes such as object properties or action constraints. Figure 1 illustrates how TAPAS dynamically adapts domain models to new goal constraints. Key contributions of TAPAS include: 1) An approach to reason over and dynamically adapt to unexpected goal specifications, including those that introduce novel attributes or constraints, without requiring manual domain redefinition. This adaptation is achieved through modular, collaborative framework where LLM agents autonomously generate and update structured domain models, initial states, and goals. 2) robust planning and execution pipeline that bridges the gap between dynamically generated symbolic plans and real-world robot capabilities. This is achieved through natural language translation of plans, ReAct (Reason+Act)-based execution to handle domain-skill discrepancies, and iterative feedbackdriven validation. By leveraging these features, TAPAS provides scalable and adaptive approach to automated planning, addressing traditional barriers in symbolic planning. Its modular The AIPlan4EU Unified Planning Library: https://github.com/ aiplan4eu/unified-planning agent-based design and feedback-driven execution make it well suited for open-world environments where contextual reasoning and adaptability are essential. II. Related Work Effective task planning in dynamic, open-world environments requires reasoning and adaptability, which traditional methods often lack. Recently, LLMs have shown remarkable progress in reasoning, offering new avenues for these challenges. Foundational prompting techniques, such as Chain of Thought (CoT) [1, 2], established that LLMs can perform in-context reasoning. Subsequent methods like Tree of Thoughts (ToT) [3] and Graph of Thoughts (GoT) [4] further enhanced problemsolving by introducing more structured exploration of the solution space. Building on these advancements, LLMs have been directly applied to task planning as planners. Early approaches explored translating high-level language into actionable steps [5] and grounding plans in robotic affordances [6]. To improve robustness, subsequent methods introduced iterative self-refinement through feedback loops [7, 8] and tool-interactive correction [9]. ReAct [10] enables interleaved reasoning and action execution, improving adaptability in dynamic tasks. Other works have focused on grounding plans in 3D scene representations [11], integrating strategic look-ahead with Mont"
[30.06.2025 22:11] Mistral response. {"id": "824a5c2450704b6a899b45b5f52dc7b1", "object": "chat.completion", "created": 1751321516, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\"Bosch Center for Artificial Intelligence, Renningen, Germany\", \"Institute of Technology, Karlsruhe, Germany\"]\n```"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1446, "total_tokens": 1481, "completion_tokens": 35}}
[30.06.2025 22:11] Response: ```python
["Bosch Center for Artificial Intelligence, Renningen, Germany", "Institute of Technology, Karlsruhe, Germany"]
```
[30.06.2025 22:11] Deleting PDF ./assets/pdf/2506.19592.pdf.
[30.06.2025 22:11] Success.
[30.06.2025 22:11] Enriching papers with extra data.
[30.06.2025 22:11] ********************************************************************************
[30.06.2025 22:11] Abstract 0. A generative visual compositing framework using a diffusion model for scene editing and composition with source masking and simulated object jittering.  					AI-generated summary 				 We present BlenderFusion, a generative visual compositing framework that synthesizes new scenes by recomposing objec...
[30.06.2025 22:11] ********************************************************************************
[30.06.2025 22:11] Abstract 1. LLaVA-Scissor, a token compression strategy for video multimodal large language models, uses Semantic Connected Components to compress tokens effectively while maintaining semantic coverage and outperforming other methods.  					AI-generated summary 				 In this paper, we present LLaVA-Scissor, a tr...
[30.06.2025 22:11] ********************************************************************************
[30.06.2025 22:11] Abstract 2. XVerse enhances text-to-image generation by enabling precise and independent control over multiple subjects using token-specific text-stream modulation, improving image coherence and fidelity.  					AI-generated summary 				 Achieving fine-grained control over subject identity and semantic attribute...
[30.06.2025 22:11] ********************************************************************************
[30.06.2025 22:11] Abstract 3. ShotBench and ShotQA datasets, along with ShotVL model, enhance AI's understanding and generation capabilities by specifically targeting nuanced cinematic language comprehension.  					AI-generated summary 				 Cinematography, the fundamental visual language of film, is essential for conveying narra...
[30.06.2025 22:11] ********************************************************************************
[30.06.2025 22:11] Abstract 4. DenseDiT, a generative model-based approach, achieves superior performance in real-world dense prediction tasks using minimal training data compared to existing methods.  					AI-generated summary 				 Dense prediction tasks hold significant importance of computer vision, aiming to learn pixel-wise ...
[30.06.2025 22:11] ********************************************************************************
[30.06.2025 22:11] Abstract 5. ARK is an open-source Python-first framework that integrates modern imitation-learning algorithms and seamless simulation-physical robot interactions to simplify robotics development and deployment.  					AI-generated summary 				 Robotics has made remarkable hardware strides-from DARPA's Urban and ...
[30.06.2025 22:11] ********************************************************************************
[30.06.2025 22:11] Abstract 6. Mixture of Grouped Experts (MoGE) improves expert load balancing and execution efficiency for large language models, enhancing throughput and cost-to-performance on Ascend NPUs.  					AI-generated summary 				 The surgence of Mixture of Experts (MoE) in Large Language Models promises a small price o...
[30.06.2025 22:11] ********************************************************************************
[30.06.2025 22:11] Abstract 7. Self-supervised learning using image triplets enhances the reasoning ability of Vision-Language Models (VLMs) on multi-image tasks without the need for human-annotated question-answer pairs.  					AI-generated summary 				 This work explores enabling Chain-of-Thought (CoT) reasoning to link visual c...
[30.06.2025 22:11] ********************************************************************************
[30.06.2025 22:11] Abstract 8. A novel framework integrates 3D proxy meshes and a decoupled video diffusion model to achieve precise and consistent video editing.  					AI-generated summary 				 Recent advances in deep generative modeling have unlocked unprecedented opportunities for video synthesis. In real-world applications, h...
[30.06.2025 22:11] ********************************************************************************
[30.06.2025 22:11] Abstract 9. SpatialReasoner-R1, a vision-language reasoning model, uses Multi-Model Monte Carlo Tree Search and fine-grained Direct Preference Optimization to improve spatial reasoning, setting a new state-of-the-art on SPATIALRGPT-Bench.  					AI-generated summary 				 Current Vision-Language Models (VLMs) str...
[30.06.2025 22:11] ********************************************************************************
[30.06.2025 22:11] Abstract 10. An Automated LLM Speedrunning Benchmark evaluates AI agents' ability to reproduce scientific results by leveraging NanoGPT speedrun tasks, indicating that even recent reasoning LLMs struggle with re-implementing known improvements.  					AI-generated summary 				 Rapid advancements in large language...
[30.06.2025 22:11] ********************************************************************************
[30.06.2025 22:11] Abstract 11. A benchmark framework evaluates the world modeling capabilities of Vision-Language Models, highlighting their limitations in perception and prediction.  					AI-generated summary 				 Internal world models (WMs) enable agents to understand the world's state and predict transitions, serving as the ba...
[30.06.2025 22:11] ********************************************************************************
[30.06.2025 22:11] Abstract 12. Current multimodal large language models show moderate to poor performance in multimodal in-context learning for medical tasks, with sensitivity to example relevance and ordering.  					AI-generated summary 				 Multimodal in-context learning (ICL) remains underexplored despite significant potential...
[30.06.2025 22:11] ********************************************************************************
[30.06.2025 22:11] Abstract 13. A hierarchical Bayesian framework explains in-context learning behavior by modeling it as a tradeoff between strategy loss and complexity, offering both explanatory power and predictive insights.  					AI-generated summary 				 Recent work analyzing in-context learning (ICL) has identified a broad s...
[30.06.2025 22:11] ********************************************************************************
[30.06.2025 22:11] Abstract 14. Gazal-R1, a 32-billion-parameter language model, achieves top performance in medical reasoning through strategic training, including advanced parameter-efficient techniques and reinforcement learning, providing detailed explanations for clinical decisions.  					AI-generated summary 				 We present ...
[30.06.2025 22:11] ********************************************************************************
[30.06.2025 22:11] Abstract 15. A novel Noise Consistency Training approach integrates new control signals into pre-trained one-step generators efficiently without retraining, outperforming existing methods in quality and computational efficiency.  					AI-generated summary 				 The pursuit of efficient and controllable high-quali...
[30.06.2025 22:11] ********************************************************************************
[30.06.2025 22:11] Abstract 16. Confucius3-Math, a 14B parameter large language model, achieves state-of-the-art performance on mathematical reasoning tasks using reinforcement learning techniques and is optimized for education in China.  					AI-generated summary 				 We introduce Confucius3-Math, an open-source large language mo...
[30.06.2025 22:11] ********************************************************************************
[30.06.2025 22:11] Abstract 17. A new benchmark, MindCube, shows that VLMs can improve their understanding of unseen spaces by forming internal spatial representations and reasoning over them.  					AI-generated summary 				 Can Vision Language Models (VLMs) imagine the full scene from just a few views, like humans do? Humans form...
[30.06.2025 22:11] ********************************************************************************
[30.06.2025 22:11] Abstract 18. A text-to-text regression model achieves high accuracy in predicting resource efficiency for Google's Borg system, surpassing tabular methods, and demonstrates adaptability and uncertainty quantification.  					AI-generated summary 				 In many industries, predicting metric outcomes of large systems...
[30.06.2025 22:11] ********************************************************************************
[30.06.2025 22:11] Abstract 19. RetFiner, a vision-language refinement scheme, enhances self-supervised foundation models for OCT by leveraging textual data, improving their downstream performance in retinal disease classification tasks.  					AI-generated summary 				 The rise of imaging techniques such as optical coherence tomog...
[30.06.2025 22:11] ********************************************************************************
[30.06.2025 22:11] Abstract 20. Radial Cross-Modal Embeddings enable explicit modeling of transitive entailment in vision-language models, leading to improved performance in hierarchical species classification and retrieval tasks.  					AI-generated summary 				 Learning the hierarchical structure of data in vision-language models...
[30.06.2025 22:11] ********************************************************************************
[30.06.2025 22:11] Abstract 21. Fractional Reasoning dynamically adjusts reasoning depth during inference to enhance the performance of large language models across various tasks.  					AI-generated summary 				 Test-time compute has emerged as a powerful paradigm for improving the performance of large language models (LLMs), wher...
[30.06.2025 22:11] ********************************************************************************
[30.06.2025 22:11] Abstract 22. Gradient-Preserving Activation Scaling (GPAS) mitigates activation variance issues in Pre-LayerNorm Transformers and enhances training dynamics across different architectures.  					AI-generated summary 				 Modern Large Language Models, such as the LLaMA, Qwen and DeepSeek series, predominantly ado...
[30.06.2025 22:11] ********************************************************************************
[30.06.2025 22:11] Abstract 23. TAPAS integrates LLMs with symbolic planning to dynamically adapt and generate domain models, initial states, and goals for complex tasks, achieving strong performance in various environments and with real-world robots.  					AI-generated summary 				 We introduce TAPAS (Task-based Adaptation and Pl...
[30.06.2025 22:11] Read previous papers.
[30.06.2025 22:11] Generating reviews via LLM API.
[30.06.2025 22:11] Using data from previous issue: {"categories": ["#cv", "#diffusion", "#3d", "#training"], "emoji": "üé≠", "ru": {"title": "–ì–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –∫–æ–º–ø–æ–∑–∏—Ü–∏—è —Å—Ü–µ–Ω —Å 3D-–∫–æ–Ω—Ç—Ä–æ–ª–µ–º", "desc": "BlenderFusion - —ç—Ç–æ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –≤–∏–∑—É–∞–ª—å–Ω–æ–π –∫–æ–º–ø–æ–∑–∏—Ü–∏–∏, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏ —Å–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Å—Ü–µ–Ω. –û–Ω–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç –ø–æ –ø—Ä–∏–Ω
[30.06.2025 22:11] Using data from previous issue: {"categories": ["#training", "#benchmark", "#multimodal", "#long_context", "#dataset", "#video"], "emoji": "‚úÇÔ∏è", "ru": {"title": "–£–º–Ω–æ–µ —Å–∂–∞—Ç–∏–µ –¥–ª—è —É–º–Ω—ã—Ö –≤–∏–¥–µ–æ–º–æ–¥–µ–ª–µ–π", "desc": "LLaVA-Scissor - —ç—Ç–æ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è —Å–∂–∞—Ç–∏—è —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –≤–∏–¥–µ–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–µ—Ç–æ–¥ –°–µ–º–∞–Ω
[30.06.2025 22:11] Using data from previous issue: {"categories": ["#diffusion", "#cv"], "emoji": "üé®", "ru": {"title": "–¢–æ—á–Ω—ã–π –∫–æ–Ω—Ç—Ä–æ–ª—å –Ω–∞–¥ –º–Ω–æ–∂–µ—Å—Ç–≤–æ–º –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "XVerse - —ç—Ç–æ –Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –æ–±—ä–µ–∫—Ç–∞–º–∏. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–æ–¥—É–ª—è—Ü–∏—é —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –ø–æ—Ç–æ–∫–∞ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –∏ –Ω–µ–∑–∞–≤–∏—Å
[30.06.2025 22:11] Using data from previous issue: {"categories": ["#dataset", "#games", "#training", "#benchmark", "#open_source", "#multimodal", "#video"], "emoji": "üé¨", "ru": {"title": "–ù–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è —è–∑—ã–∫–∞ –∫–∏–Ω–æ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–æ–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö ShotBench –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∫–∏–Ω–µ–º–∞—Ç–æ
[30.06.2025 22:11] Using data from previous issue: {"categories": ["#training", "#dataset", "#optimization", "#cv", "#synthetic", "#benchmark"], "emoji": "üîç", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –ø–ª–æ—Ç–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å –º–∏–Ω–∏–º—É–º–æ–º –¥–∞–Ω–Ω—ã—Ö", "desc": "DenseDiT - —ç—Ç–æ –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–º –º–æ–¥–µ–ª—è–º –¥–ª—è –∑–∞–¥–∞—á –ø–ª–æ—Ç–Ω–æ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–º –∑—Ä–µ–Ω–∏–∏. –û–Ω –¥–æ—Å—Ç–∏–≥–∞–µ—Ç
[30.06.2025 22:11] Using data from previous issue: {"categories": ["#open_source", "#games", "#agents", "#robotics"], "emoji": "ü§ñ", "ru": {"title": "ARK: –û–±—ä–µ–¥–∏–Ω—è—è —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫—É –∏ –ò–ò –ø–æ–¥ –∫—Ä—ã–ª–æ–º Python", "desc": "ARK - —ç—Ç–æ –æ—Ç–∫—Ä—ã—Ç–∞—è Python-—Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∏, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∞—è —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã –∏–º–∏—Ç–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ —Å–∏–º—É–ª—è—Ü–∏–∏
[30.06.2025 22:11] Using data from previous issue: {"categories": ["#training", "#optimization", "#architecture", "#inference"], "emoji": "üß†", "ru": {"title": "MoGE: –ü–æ–≤—ã—à–µ–Ω–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –≥—Ä—É–ø–ø–æ–≤—É—é —ç–∫—Å–ø–µ—Ä—Ç–∏–∑—É", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ Mixture of Grouped Experts (MoGE) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –Ω–∞–≥—Ä—É
[30.06.2025 22:11] Using data from previous issue: {"categories": ["#optimization", "#rl", "#benchmark", "#cv", "#dataset", "#reasoning"], "emoji": "üîç", "ru": {"title": "–°–∞–º–æ–æ–±—É—á–µ–Ω–∏–µ –ò–ò –≤–∏–∑—É–∞–ª—å–Ω—ã–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º –±–µ–∑ —É—á–∞—Å—Ç–∏—è —á–µ–ª–æ–≤–µ–∫–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ (VLM) –¥–ª—è
[30.06.2025 22:11] Using data from previous issue: {"categories": ["#3d", "#diffusion", "#video"], "emoji": "üé¨", "ru": {"title": "3D-–ø—Ä–æ–∫—Å–∏ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ–≥–æ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –≤–∏–¥–µ–æ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Shape-for-Motion –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ–≥–æ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –≤–∏–¥–µ–æ. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç 3D-–ø—Ä–æ–∫—Å–∏-—Å–µ—Ç–∫–∏ –∏ —Ä
[30.06.2025 22:11] Using data from previous issue: {"categories": ["#training", "#architecture", "#optimization", "#multimodal", "#rlhf", "#cv", "#reasoning"], "emoji": "üß†", "ru": {"title": "–ü—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ –ò–ò –≤—ã—Ö–æ–¥–∏—Ç –Ω–∞ –Ω–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å", "desc": "SpatialReasoner-R1 - —ç—Ç–æ –Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å –∑—Ä–∏—Ç–µ–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä–∞—è —É–ª—É—á—à–∞–µ—Ç –ø—Ä–æ—Å—Ç—Ä–∞
[30.06.2025 22:11] Using data from previous issue: {"categories": ["#optimization", "#reasoning", "#science", "#agents", "#training", "#benchmark"], "emoji": "üèÉ‚Äç‚ôÇÔ∏è", "ru": {"title": "–ò–ò –ø–æ–∫–∞ –Ω–µ –≥–æ—Ç–æ–≤ –∫ –Ω–∞—É—á–Ω–æ–º—É —Å–ø–∏–¥—Ä–∞–Ω–Ω–∏–Ω–≥—É", "desc": "–≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –ê–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ —Å–ø–∏–¥—Ä–∞–Ω–Ω–∏–Ω–≥–∞ –Ø–ë–ú –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤ –≤–æ—Å–ø—Ä–æ–∏
[30.06.2025 22:11] Using data from previous issue: {"categories": ["#multimodal", "#interpretability", "#cv", "#reasoning", "#benchmark"], "emoji": "üß†", "ru": {"title": "–û–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ—Å—Ç—å VLM –≤ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–∏ –º–∏—Ä–∞", "desc": "–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –º–æ–¥–µ–ª–µ–π –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ (VLM) –∫ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—é –º–∏—Ä–∞. –§—Ä
[30.06.2025 22:11] Using data from previous issue: {"categories": ["#games", "#healthcare", "#science", "#multimodal", "#interpretability", "#reasoning", "#benchmark"], "emoji": "ü©∫", "ru": {"title": "–û–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–∞—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–º—É –æ–±—É—á–µ–Ω–∏—é –≤ –º–µ–¥–∏—Ü–∏–Ω–µ", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –±–æ–ª—å—à–∏
[30.06.2025 22:11] Using data from previous issue: {"categories": ["#training", "#interpretability", "#reasoning", "#math"], "emoji": "üß†", "ru": {"title": "–ë–∞–π–µ—Å–æ–≤—Å–∫–∏–π –≤–∑–≥–ª—è–¥ –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ: –±–∞–ª–∞–Ω—Å –º–µ–∂–¥—É —Å—Ç—Ä–∞—Ç–µ–≥–∏–µ–π –∏ —Å–ª–æ–∂–Ω–æ—Å—Ç—å—é", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫—É—é –±–∞–π–µ—Å–æ–≤—Å–∫—É—é –º–æ–¥–µ–ª—å –¥–ª—è –æ–±—ä—è—Å–Ω–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ (ICL) –≤ –Ω–µ–π—Ä–æ–Ω
[30.06.2025 22:11] Using data from previous issue: {"categories": ["#healthcare", "#rl", "#optimization", "#survey", "#training", "#interpretability", "#reasoning"], "emoji": "ü©∫", "ru": {"title": "–ü—Ä–æ—Ä—ã–≤ –≤ –ò–ò –¥–ª—è –º–µ–¥–∏—Ü–∏–Ω—ã: Gazal-R1 - —Ç–æ—á–Ω–æ—Å—Ç—å, —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å, –æ–±—ä—è—Å–Ω–∏–º–æ—Å—Ç—å", "desc": "Gazal-R1 - —ç—Ç–æ —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å —Å 32 –º–∏–ª–ª–∏–∞—Ä–¥–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –¥–æ—Å—Ç–∏–≥–∞—é
[30.06.2025 22:11] Using data from previous issue: {"categories": ["#cv", "#training", "#diffusion", "#optimization"], "emoji": "üéõÔ∏è", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ Noise Consistency Training (NCT) –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –Ω–æ–≤—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –≤ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –æ–¥–Ω–æ—à–∞–≥
[30.06.2025 22:11] Using data from previous issue: {"categories": ["#agi", "#open_source", "#reasoning", "#training", "#rl", "#math"], "emoji": "üßÆ", "ru": {"title": "Confucius3-Math: –ú–æ—â–Ω—ã–π –ò–ò –¥–ª—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –≤ –ö–∏—Ç–∞–µ", "desc": "Confucius3-Math - —ç—Ç–æ –±–æ–ª—å—à–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å —Å 14 –º–∏–ª–ª–∏–∞—Ä–¥–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –¥–ª—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å
[30.06.2025 22:11] Using data from previous issue: {"categories": ["#games", "#rl", "#cv", "#reasoning", "#benchmark"], "emoji": "üß†", "ru": {"title": "–ü—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ –¥–ª—è –ò–ò: –æ—Ç –∫–∞—Ä—Ç—ã –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ MindCube, –∫–æ—Ç–æ—Ä—ã–π –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑
[30.06.2025 22:11] Using data from previous issue: {"categories": ["#optimization", "#transfer_learning", "#training", "#architecture", "#data"], "emoji": "ü§ñ", "ru": {"title": "–¢–µ–∫—Å—Ç–æ–≤–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Ç–∞–±–ª–∏—á–Ω—ã–µ –º–µ—Ç–æ–¥—ã –≤ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ —Å–ª–æ–∂–Ω—ã—Ö —Å–∏—Å—Ç–µ–º", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç–æ–≤—É—é —Ä–µ–≥—Ä–µ—Å—Å–∏—é –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑
[30.06.2025 22:11] Using data from previous issue: {"categories": ["#cv", "#open_source", "#dataset", "#optimization", "#training", "#healthcare", "#data"], "emoji": "üëÅÔ∏è", "ru": {"title": "RetFiner: —É–ª—É—á—à–µ–Ω–∏–µ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –û–ö–¢ —Å –ø–æ–º–æ—â—å—é —è–∑—ã–∫–æ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏", "desc": "RetFiner - —ç—Ç–æ —Å—Ö–µ–º–∞ —É—Ç–æ—á–Ω–µ–Ω–∏—è –∑—Ä–µ–Ω–∏—è –∏ —è–∑—ã–∫–∞, –∫–æ—Ç–æ—Ä–∞—è —É–ª—É—á—à–∞–µ—Ç —Å–∞–º–æ–æ–±—É—á–∞
[30.06.2025 22:11] Using data from previous issue: {"categories": ["#multimodal", "#alignment", "#cv", "#open_source"], "emoji": "üå≥", "ru": {"title": "–†–∞–¥–∏–∞–ª—å–Ω—ã–µ –∫—Ä–æ—Å—Å-–º–æ–¥–∞–ª—å–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –∏–µ—Ä–∞—Ä—Ö–∏–π –≤ AI", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—é –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ
[30.06.2025 22:11] Querying the API.
[30.06.2025 22:11] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Fractional Reasoning dynamically adjusts reasoning depth during inference to enhance the performance of large language models across various tasks.  					AI-generated summary 				 Test-time compute has emerged as a powerful paradigm for improving the performance of large language models (LLMs), where generating multiple outputs or refining individual chains can significantly boost answer accuracy. However, existing methods like Best-of-N, majority voting, and self-reflection typically apply reasoning in a uniform way across inputs, overlooking the fact that different problems may require different levels of reasoning depth. In this work, we propose Fractional Reasoning, a training-free and model-agnostic framework that enables continuous control over reasoning intensity at inference time, going beyond the limitations of fixed instructional prompts. Our method operates by extracting the latent steering vector associated with deeper reasoning and reapplying it with a tunable scaling factor, allowing the model to tailor its reasoning process to the complexity of each input. This supports two key modes of test-time scaling: (1) improving output quality in breadth-based strategies (e.g., Best-of-N, majority voting), and (2) enhancing the correctness of individual reasoning chains in depth-based strategies (e.g., self-reflection). Experiments on GSM8K, MATH500, and GPQA demonstrate that Fractional Reasoning consistently improves performance across diverse reasoning tasks and models.
[30.06.2025 22:12] Response: {
  "desc": "Fractional Reasoning - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é —Ä–∞–±–æ—Ç—ã –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≤–æ –≤—Ä–µ–º—è –≤—ã–≤–æ–¥–∞. –û–Ω –ø–æ–∑–≤–æ–ª—è–µ—Ç –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ —Ä–µ–≥—É–ª–∏—Ä–æ–≤–∞—Ç—å –≥–ª—É–±–∏–Ω—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –º–æ–¥–µ–ª–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –∑–∞–¥–∞—á–∏. –ú–µ—Ç–æ–¥ –∏–∑–≤–ª–µ–∫–∞–µ—Ç –ª–∞—Ç–µ–Ω—Ç–Ω—ã–π –≤–µ–∫—Ç–æ—Ä, —Å–≤—è–∑–∞–Ω–Ω—ã–π —Å –±–æ–ª–µ–µ –≥–ª—É–±–æ–∫–∏–º–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º–∏, –∏ –ø—Ä–∏–º–µ–Ω—è–µ—Ç –µ–≥–æ —Å –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º—ã–º –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–º –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ Fractional Reasoning —É–ª—É—á—à–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π.",
  "emoji": "üß†",
  "title": "–ì–∏–±–∫–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –¥–ª—è —É–º–Ω—ã—Ö –ò–ò-–º–æ–¥–µ–ª–µ–π"
}
[30.06.2025 22:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Fractional Reasoning dynamically adjusts reasoning depth during inference to enhance the performance of large language models across various tasks.  					AI-generated summary 				 Test-time compute has emerged as a powerful paradigm for improving the performance of large language models (LLMs), where generating multiple outputs or refining individual chains can significantly boost answer accuracy. However, existing methods like Best-of-N, majority voting, and self-reflection typically apply reasoning in a uniform way across inputs, overlooking the fact that different problems may require different levels of reasoning depth. In this work, we propose Fractional Reasoning, a training-free and model-agnostic framework that enables continuous control over reasoning intensity at inference time, going beyond the limitations of fixed instructional prompts. Our method operates by extracting the latent steering vector associated with deeper reasoning and reapplying it with a tunable scaling factor, allowing the model to tailor its reasoning process to the complexity of each input. This supports two key modes of test-time scaling: (1) improving output quality in breadth-based strategies (e.g., Best-of-N, majority voting), and (2) enhancing the correctness of individual reasoning chains in depth-based strategies (e.g., self-reflection). Experiments on GSM8K, MATH500, and GPQA demonstrate that Fractional Reasoning consistently improves performance across diverse reasoning tasks and models."

[30.06.2025 22:12] Response: ```python
['INFERENCE', 'TRAINING', 'MATH']
```
[30.06.2025 22:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Fractional Reasoning dynamically adjusts reasoning depth during inference to enhance the performance of large language models across various tasks.  					AI-generated summary 				 Test-time compute has emerged as a powerful paradigm for improving the performance of large language models (LLMs), where generating multiple outputs or refining individual chains can significantly boost answer accuracy. However, existing methods like Best-of-N, majority voting, and self-reflection typically apply reasoning in a uniform way across inputs, overlooking the fact that different problems may require different levels of reasoning depth. In this work, we propose Fractional Reasoning, a training-free and model-agnostic framework that enables continuous control over reasoning intensity at inference time, going beyond the limitations of fixed instructional prompts. Our method operates by extracting the latent steering vector associated with deeper reasoning and reapplying it with a tunable scaling factor, allowing the model to tailor its reasoning process to the complexity of each input. This supports two key modes of test-time scaling: (1) improving output quality in breadth-based strategies (e.g., Best-of-N, majority voting), and (2) enhancing the correctness of individual reasoning chains in depth-based strategies (e.g., self-reflection). Experiments on GSM8K, MATH500, and GPQA demonstrate that Fractional Reasoning consistently improves performance across diverse reasoning tasks and models."

[30.06.2025 22:12] Response: ```python
["REASONING"]
```
[30.06.2025 22:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Fractional Reasoning is a novel approach that allows large language models (LLMs) to adjust their reasoning depth dynamically during inference. This method recognizes that different tasks may require varying levels of reasoning complexity, rather than applying a one-size-fits-all strategy. By utilizing a latent steering vector and a tunable scaling factor, the model can enhance its output quality and correctness based on the specific demands of each input. Experiments show that this framework significantly boosts performance across multiple reasoning tasks and models, making it a versatile tool in the field of machine learning.","title":"Dynamic Depth for Enhanced Reasoning in Language Models"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Fractional Reasoning is a novel approach that allows large language models (LLMs) to adjust their reasoning depth dynamically during inference. This method recognizes that different tasks may require varying levels of reasoning complexity, rather than applying a one-size-fits-all strategy. By utilizing a latent steering vector and a tunable scaling factor, the model can enhance its output quality and correctness based on the specific demands of each input. Experiments show that this framework significantly boosts performance across multiple reasoning tasks and models, making it a versatile tool in the field of machine learning.', title='Dynamic Depth for Enhanced Reasoning in Language Models'))
[30.06.2025 22:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Fractional ReasoningÊòØ‰∏ÄÁßçÂä®ÊÄÅË∞ÉÊï¥Êé®ÁêÜÊ∑±Â∫¶ÁöÑÊñπÊ≥ïÔºåÊó®Âú®ÊèêÂçáÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®ÂêÑÁßç‰ªªÂä°‰∏≠ÁöÑË°®Áé∞„ÄÇËØ•ÊñπÊ≥ïÂÖÅËÆ∏Âú®Êé®ÁêÜÊó∂ËøûÁª≠ÊéßÂà∂Êé®ÁêÜÂº∫Â∫¶ÔºåË∂ÖË∂ä‰∫ÜÂõ∫ÂÆöÊåá‰ª§ÊèêÁ§∫ÁöÑÂ±ÄÈôêÊÄß„ÄÇÈÄöËøáÊèêÂèñ‰∏éÊõ¥Ê∑±Â±ÇÊé®ÁêÜÁõ∏ÂÖ≥ÁöÑÊΩúÂú®ÂºïÂØºÂêëÈáèÔºåÂπ∂‰ΩøÁî®ÂèØË∞ÉÁº©ÊîæÂõ†Â≠êÈáçÊñ∞Â∫îÁî®ÔºåÊ®°ÂûãËÉΩÂ§üÊ†πÊçÆÊØè‰∏™ËæìÂÖ•ÁöÑÂ§çÊùÇÊÄßÂÆöÂà∂Êé®ÁêÜËøáÁ®ã„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåFractional ReasoningÂú®GSM8K„ÄÅMATH500ÂíåGPQAÁ≠âÂ§öÁßçÊé®ÁêÜ‰ªªÂä°‰∏≠ÂùáËÉΩÊåÅÁª≠ÊèêÈ´òÊÄßËÉΩ„ÄÇ","title":"Âä®ÊÄÅË∞ÉÊï¥Êé®ÁêÜÊ∑±Â∫¶ÔºåÊèêÂçáÊ®°ÂûãË°®Áé∞"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Fractional ReasoningÊòØ‰∏ÄÁßçÂä®ÊÄÅË∞ÉÊï¥Êé®ÁêÜÊ∑±Â∫¶ÁöÑÊñπÊ≥ïÔºåÊó®Âú®ÊèêÂçáÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®ÂêÑÁßç‰ªªÂä°‰∏≠ÁöÑË°®Áé∞„ÄÇËØ•ÊñπÊ≥ïÂÖÅËÆ∏Âú®Êé®ÁêÜÊó∂ËøûÁª≠ÊéßÂà∂Êé®ÁêÜÂº∫Â∫¶ÔºåË∂ÖË∂ä‰∫ÜÂõ∫ÂÆöÊåá‰ª§ÊèêÁ§∫ÁöÑÂ±ÄÈôêÊÄß„ÄÇÈÄöËøáÊèêÂèñ‰∏éÊõ¥Ê∑±Â±ÇÊé®ÁêÜÁõ∏ÂÖ≥ÁöÑÊΩúÂú®ÂºïÂØºÂêëÈáèÔºåÂπ∂‰ΩøÁî®ÂèØË∞ÉÁº©ÊîæÂõ†Â≠êÈáçÊñ∞Â∫îÁî®ÔºåÊ®°ÂûãËÉΩÂ§üÊ†πÊçÆÊØè‰∏™ËæìÂÖ•ÁöÑÂ§çÊùÇÊÄßÂÆöÂà∂Êé®ÁêÜËøáÁ®ã„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåFractional ReasoningÂú®GSM8K„ÄÅMATH500ÂíåGPQAÁ≠âÂ§öÁßçÊé®ÁêÜ‰ªªÂä°‰∏≠ÂùáËÉΩÊåÅÁª≠ÊèêÈ´òÊÄßËÉΩ„ÄÇ', title='Âä®ÊÄÅË∞ÉÊï¥Êé®ÁêÜÊ∑±Â∫¶ÔºåÊèêÂçáÊ®°ÂûãË°®Áé∞'))
[30.06.2025 22:12] Using data from previous issue: {"categories": ["#training", "#architecture", "#optimization"], "emoji": "üìä", "ru": {"title": "GPAS: –£–ª—É—á—à–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ Gradient-Preserving Activation Scaling (GPAS) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ —Å –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π –Ω–æ—Ä–º–∞–ª
[30.06.2025 22:12] Querying the API.
[30.06.2025 22:12] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

TAPAS integrates LLMs with symbolic planning to dynamically adapt and generate domain models, initial states, and goals for complex tasks, achieving strong performance in various environments and with real-world robots.  					AI-generated summary 				 We introduce TAPAS (Task-based Adaptation and Planning using AgentS), a multi-agent framework that integrates Large Language Models (LLMs) with symbolic planning to solve complex tasks without the need for manually defined environment models. TAPAS employs specialized LLM-based agents that collaboratively generate and adapt domain models, initial states, and goal specifications as needed using structured tool-calling mechanisms. Through this tool-based interaction, downstream agents can request modifications from upstream agents, enabling adaptation to novel attributes and constraints without manual domain redefinition. A ReAct (Reason+Act)-style execution agent, coupled with natural language plan translation, bridges the gap between dynamically generated plans and real-world robot capabilities. TAPAS demonstrates strong performance in benchmark planning domains and in the VirtualHome simulated real-world environment.
[30.06.2025 22:12] Response: {
  "desc": "TAPAS - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∏–π –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (LLM) —Å —Å–∏–º–≤–æ–ª–∏—á–µ—Å–∫–∏–º –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ–º –¥–ª—è —Ä–µ—à–µ–Ω–∏—è —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –º–æ–¥–µ–ª–µ–π –ø—Ä–µ–¥–º–µ—Ç–Ω–æ–π –æ–±–ª–∞—Å—Ç–∏, –Ω–∞—á–∞–ª—å–Ω—ã—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π –∏ —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–π —Ü–µ–ª–µ–π. TAPAS –ø–æ–∑–≤–æ–ª—è–µ—Ç –∞–≥–µ–Ω—Ç–∞–º –∑–∞–ø—Ä–∞—à–∏–≤–∞—Ç—å –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏ —É –¥—Ä—É–≥–∏—Ö –∞–≥–µ–Ω—Ç–æ–≤, —á—Ç–æ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –∞–¥–∞–ø—Ç–∞—Ü–∏—é –∫ –Ω–æ–≤—ã–º –∞—Ç—Ä–∏–±—É—Ç–∞–º –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è–º –±–µ–∑ —Ä—É—á–Ω–æ–≥–æ –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–æ–º–µ–Ω–∞. –§—Ä–µ–π–º–≤–æ—Ä–∫ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –≤—ã—Å–æ–∫—É—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∫–∞–∫ –≤ —ç—Ç–∞–ª–æ–Ω–Ω—ã—Ö –¥–æ–º–µ–Ω–∞—Ö –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è, —Ç–∞–∫ –∏ –≤ —Å–∏–º—É–ª–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Å—Ä–µ–¥–µ —Ä–µ–∞–ª—å–Ω–æ–≥–æ –º–∏—Ä–∞ VirtualHome.",
  "emoji": "ü§ñ",
  "title": "–ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–¥–∞—á —Å –ø–æ–º–æ—â—å—é LLM –∏ —Å–∏–º–≤–æ–ª–∏—á–µ—Å–∫–∏—Ö –º–µ—Ç–æ–¥–æ–≤"
}
[30.06.2025 22:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"TAPAS integrates LLMs with symbolic planning to dynamically adapt and generate domain models, initial states, and goals for complex tasks, achieving strong performance in various environments and with real-world robots.  					AI-generated summary 				 We introduce TAPAS (Task-based Adaptation and Planning using AgentS), a multi-agent framework that integrates Large Language Models (LLMs) with symbolic planning to solve complex tasks without the need for manually defined environment models. TAPAS employs specialized LLM-based agents that collaboratively generate and adapt domain models, initial states, and goal specifications as needed using structured tool-calling mechanisms. Through this tool-based interaction, downstream agents can request modifications from upstream agents, enabling adaptation to novel attributes and constraints without manual domain redefinition. A ReAct (Reason+Act)-style execution agent, coupled with natural language plan translation, bridges the gap between dynamically generated plans and real-world robot capabilities. TAPAS demonstrates strong performance in benchmark planning domains and in the VirtualHome simulated real-world environment."

[30.06.2025 22:12] Response: ```python
['AGENTS', 'MULTIMODAL', 'BENCHMARK', 'ROBOTICS']
```
[30.06.2025 22:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"TAPAS integrates LLMs with symbolic planning to dynamically adapt and generate domain models, initial states, and goals for complex tasks, achieving strong performance in various environments and with real-world robots.  					AI-generated summary 				 We introduce TAPAS (Task-based Adaptation and Planning using AgentS), a multi-agent framework that integrates Large Language Models (LLMs) with symbolic planning to solve complex tasks without the need for manually defined environment models. TAPAS employs specialized LLM-based agents that collaboratively generate and adapt domain models, initial states, and goal specifications as needed using structured tool-calling mechanisms. Through this tool-based interaction, downstream agents can request modifications from upstream agents, enabling adaptation to novel attributes and constraints without manual domain redefinition. A ReAct (Reason+Act)-style execution agent, coupled with natural language plan translation, bridges the gap between dynamically generated plans and real-world robot capabilities. TAPAS demonstrates strong performance in benchmark planning domains and in the VirtualHome simulated real-world environment."

[30.06.2025 22:12] Response: ```python
['REASONING', 'AGI']
```
[30.06.2025 22:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"TAPAS is a framework that combines Large Language Models (LLMs) with symbolic planning to handle complex tasks efficiently. It allows agents to create and modify domain models, initial states, and goals dynamically, eliminating the need for pre-defined models. The system uses structured tool-calling mechanisms for agents to communicate and adapt to new requirements. TAPAS has shown impressive results in various planning scenarios and simulated real-world environments, showcasing its effectiveness in robotic applications.","title":"Dynamic Task Adaptation with TAPAS"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='TAPAS is a framework that combines Large Language Models (LLMs) with symbolic planning to handle complex tasks efficiently. It allows agents to create and modify domain models, initial states, and goals dynamically, eliminating the need for pre-defined models. The system uses structured tool-calling mechanisms for agents to communicate and adapt to new requirements. TAPAS has shown impressive results in various planning scenarios and simulated real-world environments, showcasing its effectiveness in robotic applications.', title='Dynamic Task Adaptation with TAPAS'))
[30.06.2025 22:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"TAPASÊòØ‰∏ÄÁßçÂ§öÊô∫ËÉΩ‰ΩìÊ°ÜÊû∂ÔºåÁªìÂêà‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂíåÁ¨¶Âè∑ËßÑÂàíÔºåËÉΩÂ§üÂä®ÊÄÅÈÄÇÂ∫îÂíåÁîüÊàêÂ§çÊùÇ‰ªªÂä°ÁöÑÈ¢ÜÂüüÊ®°Âûã„ÄÅÂàùÂßãÁä∂ÊÄÅÂíåÁõÆÊ†á„ÄÇËØ•Á≥ªÁªüÈÄöËøá‰∏ìÈó®ÁöÑLLM‰ª£ÁêÜÂçè‰ΩúÁîüÊàêÂíåË∞ÉÊï¥ÊâÄÈúÄÁöÑÊ®°ÂûãÂíåÁõÆÊ†áÔºåÈÅøÂÖç‰∫ÜÊâãÂä®ÂÆö‰πâÁéØÂ¢ÉÊ®°ÂûãÁöÑÈúÄÊ±Ç„ÄÇÈÄöËøáÂ∑•ÂÖ∑Ë∞ÉÁî®Êú∫Âà∂Ôºå‰∏ãÊ∏∏‰ª£ÁêÜÂèØ‰ª•Âêë‰∏äÊ∏∏‰ª£ÁêÜËØ∑Ê±Ç‰øÆÊîπÔºå‰ªéËÄåÈÄÇÂ∫îÊñ∞ÁöÑÂ±ûÊÄßÂíåÁ∫¶Êùü„ÄÇTAPASÂú®Âü∫ÂáÜËßÑÂàíÈ¢ÜÂüüÂíåVirtualHomeÊ®°ÊãüÁúüÂÆûÁéØÂ¢É‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂Âú®ÂÆûÈôÖÊú∫Âô®‰∫∫ËÉΩÂäõ‰∏éÂä®ÊÄÅÁîüÊàêËÆ°Âàí‰πãÈó¥ÁöÑÊ°•Ê¢Å‰ΩúÁî®„ÄÇ","title":"TAPASÔºöÊô∫ËÉΩ‰ΩìÂçè‰ΩúÔºåÂä®ÊÄÅÈÄÇÂ∫îÂ§çÊùÇ‰ªªÂä°"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='TAPASÊòØ‰∏ÄÁßçÂ§öÊô∫ËÉΩ‰ΩìÊ°ÜÊû∂ÔºåÁªìÂêà‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂíåÁ¨¶Âè∑ËßÑÂàíÔºåËÉΩÂ§üÂä®ÊÄÅÈÄÇÂ∫îÂíåÁîüÊàêÂ§çÊùÇ‰ªªÂä°ÁöÑÈ¢ÜÂüüÊ®°Âûã„ÄÅÂàùÂßãÁä∂ÊÄÅÂíåÁõÆÊ†á„ÄÇËØ•Á≥ªÁªüÈÄöËøá‰∏ìÈó®ÁöÑLLM‰ª£ÁêÜÂçè‰ΩúÁîüÊàêÂíåË∞ÉÊï¥ÊâÄÈúÄÁöÑÊ®°ÂûãÂíåÁõÆÊ†áÔºåÈÅøÂÖç‰∫ÜÊâãÂä®ÂÆö‰πâÁéØÂ¢ÉÊ®°ÂûãÁöÑÈúÄÊ±Ç„ÄÇÈÄöËøáÂ∑•ÂÖ∑Ë∞ÉÁî®Êú∫Âà∂Ôºå‰∏ãÊ∏∏‰ª£ÁêÜÂèØ‰ª•Âêë‰∏äÊ∏∏‰ª£ÁêÜËØ∑Ê±Ç‰øÆÊîπÔºå‰ªéËÄåÈÄÇÂ∫îÊñ∞ÁöÑÂ±ûÊÄßÂíåÁ∫¶Êùü„ÄÇTAPASÂú®Âü∫ÂáÜËßÑÂàíÈ¢ÜÂüüÂíåVirtualHomeÊ®°ÊãüÁúüÂÆûÁéØÂ¢É‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂Âú®ÂÆûÈôÖÊú∫Âô®‰∫∫ËÉΩÂäõ‰∏éÂä®ÊÄÅÁîüÊàêËÆ°Âàí‰πãÈó¥ÁöÑÊ°•Ê¢Å‰ΩúÁî®„ÄÇ', title='TAPASÔºöÊô∫ËÉΩ‰ΩìÂçè‰ΩúÔºåÂä®ÊÄÅÈÄÇÂ∫îÂ§çÊùÇ‰ªªÂä°'))
[30.06.2025 22:12] Renaming data file.
[30.06.2025 22:12] Renaming previous data. hf_papers.json to ./d/2025-06-30.json
[30.06.2025 22:12] Saving new data file.
[30.06.2025 22:12] Generating page.
[30.06.2025 22:12] Renaming previous page.
[30.06.2025 22:12] Renaming previous data. index.html to ./d/2025-06-30.html
[30.06.2025 22:12] Writing result.
[30.06.2025 22:12] Renaming log file.
[30.06.2025 22:12] Renaming previous data. log.txt to ./logs/2025-06-30_last_log.txt
