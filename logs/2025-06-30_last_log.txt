[30.06.2025 03:52] Read previous papers.
[30.06.2025 03:52] Generating top page (month).
[30.06.2025 03:52] Writing top page (month).
[30.06.2025 04:27] Read previous papers.
[30.06.2025 04:27] Get feed.
[30.06.2025 04:27] Get page data from previous paper. URL: https://huggingface.co/papers/2506.21862
[30.06.2025 04:27] Extract page data from URL. URL: https://huggingface.co/papers/2506.17450
[30.06.2025 04:27] Get page data from previous paper. URL: https://huggingface.co/papers/2506.21656
[30.06.2025 04:27] Get page data from previous paper. URL: https://huggingface.co/papers/2506.22434
[30.06.2025 04:27] Get page data from previous paper. URL: https://huggingface.co/papers/2506.19741
[30.06.2025 04:27] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[30.06.2025 04:27] No deleted papers detected.
[30.06.2025 04:27] Downloading and parsing papers (pdf, html). Total: 5.
[30.06.2025 04:27] Downloading and parsing paper https://huggingface.co/papers/2506.21862.
[30.06.2025 04:27] Extra JSON file exists (./assets/json/2506.21862.json), skip PDF parsing.
[30.06.2025 04:27] Paper image links file exists (./assets/img_data/2506.21862.json), skip HTML parsing.
[30.06.2025 04:27] Success.
[30.06.2025 04:27] Downloading and parsing paper https://huggingface.co/papers/2506.17450.
[30.06.2025 04:27] Downloading paper 2506.17450 from http://arxiv.org/pdf/2506.17450v2...
[30.06.2025 04:27] Extracting affiliations from text.
[30.06.2025 04:27] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 6 2 ] . [ 2 0 5 4 7 1 . 6 0 5 2 : r 2025-6-27 BlenderFusion: 3D-Grounded Visual Editing and Generative Compositing Jiacheng Chen(cid:113),1,2, Ramin Mehran1, Xuhui Jia1, Saining Xie1,3 and Sanghyun Woo1 1Google DeepMind, 2Simon Fraser University, 3New York University, (cid:113)Work done during an internship at Google Deepmind We present BlenderFusion, generative visual compositing framework that synthesizes new scenes by recomposing objects, camera, and background. It follows layering-editing-compositing pipeline: (i) segmenting and converting visual inputs into editable 3D entities (layering), (ii) editing them in Blender with 3D-grounded control (editing), and (iii) fusing them into coherent scene using generative compositor (compositing). Our generative compositor extends pre-trained diffusion model to process both the original (source) and edited (target) scenes in parallel. It is fine-tuned on video frames with two key training strategies: (i) source masking, enabling flexible modifications like background replacement; (ii) simulated object jittering, facilitating disentangled control over objects and camera. BlenderFusion significantly outperforms prior methods in complex compositional scene editing tasks. See the project page for demos and more results: blenderfusion.github.io 1. Introduction Figure 1 BlenderFusion integrates the 3D-grounded editing capabilities of graphics software into the strong synthesis abilities of diffusion models. Despite fine-tuned only on video frames of simple object transformations with entangled camera motion, it learns precise object control, inherits Blenders rich editing functionalities (e.g., attribute modification, deformation, novel asset insertion), and generalizes to highly fine-grained multi-object editing and scene composition tasks (Figure 6). Visual compositing is the process of constructing scene by extracting objects from multiple images, manipulating their appearance or spatial configuration, inserting them"
[30.06.2025 04:27] Response: ```python
["Google DeepMind", "Simon Fraser University", "New York University"]
```
[30.06.2025 04:27] Deleting PDF ./assets/pdf/2506.17450.pdf.
[30.06.2025 04:27] Success.
[30.06.2025 04:27] Downloading and parsing paper https://huggingface.co/papers/2506.21656.
[30.06.2025 04:27] Extra JSON file exists (./assets/json/2506.21656.json), skip PDF parsing.
[30.06.2025 04:27] Paper image links file exists (./assets/img_data/2506.21656.json), skip HTML parsing.
[30.06.2025 04:27] Success.
[30.06.2025 04:27] Downloading and parsing paper https://huggingface.co/papers/2506.22434.
[30.06.2025 04:27] Extra JSON file exists (./assets/json/2506.22434.json), skip PDF parsing.
[30.06.2025 04:27] Paper image links file exists (./assets/img_data/2506.22434.json), skip HTML parsing.
[30.06.2025 04:27] Success.
[30.06.2025 04:27] Downloading and parsing paper https://huggingface.co/papers/2506.19741.
[30.06.2025 04:27] Extra JSON file exists (./assets/json/2506.19741.json), skip PDF parsing.
[30.06.2025 04:27] Paper image links file exists (./assets/img_data/2506.19741.json), skip HTML parsing.
[30.06.2025 04:27] Success.
[30.06.2025 04:27] Enriching papers with extra data.
[30.06.2025 04:27] ********************************************************************************
[30.06.2025 04:27] Abstract 0. LLaVA-Scissor, a token compression strategy for video multimodal large language models, uses Semantic Connected Components to compress tokens effectively while maintaining semantic coverage and outperforming other methods.  					AI-generated summary 				 In this paper, we present LLaVA-Scissor, a tr...
[30.06.2025 04:27] ********************************************************************************
[30.06.2025 04:27] Abstract 1. A generative visual compositing framework using a diffusion model for scene editing and composition with source masking and simulated object jittering.  					AI-generated summary 				 We present BlenderFusion, a generative visual compositing framework that synthesizes new scenes by recomposing objec...
[30.06.2025 04:27] ********************************************************************************
[30.06.2025 04:27] Abstract 2. SpatialReasoner-R1, a vision-language reasoning model, uses Multi-Model Monte Carlo Tree Search and fine-grained Direct Preference Optimization to improve spatial reasoning, setting a new state-of-the-art on SPATIALRGPT-Bench.  					AI-generated summary 				 Current Vision-Language Models (VLMs) str...
[30.06.2025 04:27] ********************************************************************************
[30.06.2025 04:27] Abstract 3. Self-supervised learning using image triplets enhances the reasoning ability of Vision-Language Models (VLMs) on multi-image tasks without the need for human-annotated question-answer pairs.  					AI-generated summary 				 This work explores enabling Chain-of-Thought (CoT) reasoning to link visual c...
[30.06.2025 04:27] ********************************************************************************
[30.06.2025 04:27] Abstract 4. A novel Noise Consistency Training approach integrates new control signals into pre-trained one-step generators efficiently without retraining, outperforming existing methods in quality and computational efficiency.  					AI-generated summary 				 The pursuit of efficient and controllable high-quali...
[30.06.2025 04:27] Read previous papers.
[30.06.2025 04:27] Generating reviews via LLM API.
[30.06.2025 04:27] Using data from previous issue: {"categories": ["#training", "#benchmark", "#multimodal", "#long_context", "#dataset", "#video"], "emoji": "‚úÇÔ∏è", "ru": {"title": "–£–º–Ω–æ–µ —Å–∂–∞—Ç–∏–µ –¥–ª—è —É–º–Ω—ã—Ö –≤–∏–¥–µ–æ–º–æ–¥–µ–ª–µ–π", "desc": "LLaVA-Scissor - —ç—Ç–æ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è —Å–∂–∞—Ç–∏—è —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –≤–∏–¥–µ–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–µ—Ç–æ–¥ –°–µ–º–∞–Ω
[30.06.2025 04:27] Querying the API.
[30.06.2025 04:27] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A generative visual compositing framework using a diffusion model for scene editing and composition with source masking and simulated object jittering.  					AI-generated summary 				 We present BlenderFusion, a generative visual compositing framework that synthesizes new scenes by recomposing objects, camera, and background. It follows a layering-editing-compositing pipeline: (i) segmenting and converting visual inputs into editable 3D entities (layering), (ii) editing them in Blender with 3D-grounded control (editing), and (iii) fusing them into a coherent scene using a generative compositor (compositing). Our generative compositor extends a pre-trained diffusion model to process both the original (source) and edited (target) scenes in parallel. It is fine-tuned on video frames with two key training strategies: (i) source masking, enabling flexible modifications like background replacement; (ii) simulated object jittering, facilitating disentangled control over objects and camera. BlenderFusion significantly outperforms prior methods in complex compositional scene editing tasks.
[30.06.2025 04:27] Error getting data: Error code: 529 - {'type': 'error', 'error': {'type': 'overloaded_error', 'message': 'Overloaded'}}
[30.06.2025 04:27] Using data from previous issue: {"categories": ["#training", "#architecture", "#optimization", "#multimodal", "#rlhf", "#cv", "#reasoning"], "emoji": "üß†", "ru": {"title": "–ü—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ –ò–ò –≤—ã—Ö–æ–¥–∏—Ç –Ω–∞ –Ω–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å", "desc": "SpatialReasoner-R1 - —ç—Ç–æ –Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å –∑—Ä–∏—Ç–µ–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä–∞—è —É–ª—É—á—à–∞–µ—Ç –ø—Ä–æ—Å—Ç—Ä–∞
[30.06.2025 04:27] Using data from previous issue: {"categories": ["#optimization", "#rl", "#benchmark", "#cv", "#dataset", "#reasoning"], "emoji": "üîç", "ru": {"title": "–°–∞–º–æ–æ–±—É—á–µ–Ω–∏–µ –ò–ò –≤–∏–∑—É–∞–ª—å–Ω—ã–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º –±–µ–∑ —É—á–∞—Å—Ç–∏—è —á–µ–ª–æ–≤–µ–∫–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ (VLM) –¥–ª—è
[30.06.2025 04:27] Using data from previous issue: {"categories": ["#cv", "#training", "#diffusion", "#optimization"], "emoji": "üéõÔ∏è", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ Noise Consistency Training (NCT) –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –Ω–æ–≤—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –≤ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –æ–¥–Ω–æ—à–∞–≥
[30.06.2025 04:27] Renaming data file.
[30.06.2025 04:27] Renaming previous data. hf_papers.json to ./d/2025-06-30.json
[30.06.2025 04:27] Saving new data file.
[30.06.2025 04:27] Generating page.
[30.06.2025 04:27] Renaming previous page.
[30.06.2025 04:27] Renaming previous data. index.html to ./d/2025-06-30.html
[30.06.2025 04:27] Writing result.
[30.06.2025 04:27] Renaming log file.
[30.06.2025 04:27] Renaming previous data. log.txt to ./logs/2025-06-30_last_log.txt
