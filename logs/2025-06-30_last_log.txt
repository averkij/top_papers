[30.06.2025 02:55] Read previous papers.
[30.06.2025 02:55] Generating top page (month).
[30.06.2025 02:55] Writing top page (month).
[30.06.2025 03:51] Read previous papers.
[30.06.2025 03:51] Get feed.
[30.06.2025 03:51] Extract page data from URL. URL: https://huggingface.co/papers/2506.21862
[30.06.2025 03:51] Extract page data from URL. URL: https://huggingface.co/papers/2506.21656
[30.06.2025 03:51] Extract page data from URL. URL: https://huggingface.co/papers/2506.22434
[30.06.2025 03:51] Extract page data from URL. URL: https://huggingface.co/papers/2506.19741
[30.06.2025 03:51] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[30.06.2025 03:51] No deleted papers detected.
[30.06.2025 03:51] Downloading and parsing papers (pdf, html). Total: 4.
[30.06.2025 03:51] Downloading and parsing paper https://huggingface.co/papers/2506.21862.
[30.06.2025 03:51] Extra JSON file exists (./assets/json/2506.21862.json), skip PDF parsing.
[30.06.2025 03:51] Paper image links file exists (./assets/img_data/2506.21862.json), skip HTML parsing.
[30.06.2025 03:51] Success.
[30.06.2025 03:51] Downloading and parsing paper https://huggingface.co/papers/2506.21656.
[30.06.2025 03:51] Extra JSON file exists (./assets/json/2506.21656.json), skip PDF parsing.
[30.06.2025 03:51] Paper image links file exists (./assets/img_data/2506.21656.json), skip HTML parsing.
[30.06.2025 03:51] Success.
[30.06.2025 03:51] Downloading and parsing paper https://huggingface.co/papers/2506.22434.
[30.06.2025 03:51] Downloading paper 2506.22434 from http://arxiv.org/pdf/2506.22434v1...
[30.06.2025 03:51] Extracting affiliations from text.
[30.06.2025 03:51] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 7 2 ] . [ 1 4 3 4 2 2 . 6 0 5 2 : r MiCo: Multi-image Contrast for Reinforcement Visual Reasoning Xi Chen1 Mingkang Zhu3 Shaoteng Liu3 Xiaoyang Wu1 Xiaogang Xu3 Yu Liu2 Xiang Bai4 Hengshuang Zhao1 1HKU 2 Tongyi Lab, Alibaba Group 3CUHK 4HUST "
[30.06.2025 03:51] Response: ```python
["HKU", "Tongyi Lab, Alibaba Group", "CUHK", "HUST"]
```
[30.06.2025 03:51] Deleting PDF ./assets/pdf/2506.22434.pdf.
[30.06.2025 03:51] Success.
[30.06.2025 03:51] Downloading and parsing paper https://huggingface.co/papers/2506.19741.
[30.06.2025 03:51] Downloading paper 2506.19741 from http://arxiv.org/pdf/2506.19741v1...
[30.06.2025 03:51] Extracting affiliations from text.
[30.06.2025 03:51] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 4 2 ] . [ 1 1 4 7 9 1 . 6 0 5 2 : r Noise Consistency Training: Native Approach for One-Step Generator in Learning Additional Controls Yihong Luo1, Shuchen Xue2, Tianyang Hu3, Jing Tang4,1 1HKUST 2UCAS 3NUS 4HKUST(GZ) "
[30.06.2025 03:51] Response: ```python
["HKUST", "UCAS", "NUS", "HKUST(GZ)"]
```
[30.06.2025 03:51] Deleting PDF ./assets/pdf/2506.19741.pdf.
[30.06.2025 03:51] Success.
[30.06.2025 03:51] Enriching papers with extra data.
[30.06.2025 03:51] ********************************************************************************
[30.06.2025 03:51] Abstract 0. LLaVA-Scissor, a token compression strategy for video multimodal large language models, uses Semantic Connected Components to compress tokens effectively while maintaining semantic coverage and outperforming other methods.  					AI-generated summary 				 In this paper, we present LLaVA-Scissor, a tr...
[30.06.2025 03:51] ********************************************************************************
[30.06.2025 03:51] Abstract 1. SpatialReasoner-R1, a vision-language reasoning model, uses Multi-Model Monte Carlo Tree Search and fine-grained Direct Preference Optimization to improve spatial reasoning, setting a new state-of-the-art on SPATIALRGPT-Bench.  					AI-generated summary 				 Current Vision-Language Models (VLMs) str...
[30.06.2025 03:51] ********************************************************************************
[30.06.2025 03:51] Abstract 2. Self-supervised learning using image triplets enhances the reasoning ability of Vision-Language Models (VLMs) on multi-image tasks without the need for human-annotated question-answer pairs.  					AI-generated summary 				 This work explores enabling Chain-of-Thought (CoT) reasoning to link visual c...
[30.06.2025 03:51] ********************************************************************************
[30.06.2025 03:51] Abstract 3. A novel Noise Consistency Training approach integrates new control signals into pre-trained one-step generators efficiently without retraining, outperforming existing methods in quality and computational efficiency.  					AI-generated summary 				 The pursuit of efficient and controllable high-quali...
[30.06.2025 03:51] Read previous papers.
[30.06.2025 03:51] Generating reviews via LLM API.
[30.06.2025 03:51] Querying the API.
[30.06.2025 03:51] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

LLaVA-Scissor, a token compression strategy for video multimodal large language models, uses Semantic Connected Components to compress tokens effectively while maintaining semantic coverage and outperforming other methods.  					AI-generated summary 				 In this paper, we present LLaVA-Scissor, a training-free token compression strategy designed for video multimodal large language models. Previous methods mostly attempt to compress tokens based on attention scores, but fail to effectively capture all semantic regions and often lead to token redundancy. Differently, we propose to leverage the Semantic Connected Components (SCC) approach that assigns tokens to distinct semantic regions within the token set, ensuring comprehensive semantic coverage. The outcome is a two-step spatio-temporal token compression strategy that utilizes SCC in both spatial and temporal domains. This strategy can effectively compress tokens by representing the entire video with a set of non-overlapping semantic tokens. We conduct extensive evaluations of the token compression capabilities of LLaVA-Scissor across diverse video understanding benchmarks, including video question answering, long video understanding, and comprehensive multi-choices benchmarks. Experimental results show that the proposed LLaVA-Scissor outperforms other token compression methods, achieving superior performance in various video understanding benchmarks, particularly at low token retention ratios. Project page: https://github.com/HumanMLLM/LLaVA-Scissor.
[30.06.2025 03:51] Response: {
  "desc": "LLaVA-Scissor - это стратегия сжатия токенов для видео мультимодальных больших языковых моделей. Она использует метод Семантически Связанных Компонентов (SCC) для эффективного сжатия токенов, сохраняя при этом семантическое покрытие. LLaVA-Scissor применяет двухэтапный подход к пространственно-временному сжатию токенов, используя SCC как в пространственной, так и во временной областях. Экспериментальные результаты показывают, что LLaVA-Scissor превосходит другие методы сжатия токенов в различных задачах понимания видео.",
  "emoji": "✂️",
  "title": "Умное сжатие для умных видеомоделей"
}
[30.06.2025 03:51] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"LLaVA-Scissor, a token compression strategy for video multimodal large language models, uses Semantic Connected Components to compress tokens effectively while maintaining semantic coverage and outperforming other methods.  					AI-generated summary 				 In this paper, we present LLaVA-Scissor, a training-free token compression strategy designed for video multimodal large language models. Previous methods mostly attempt to compress tokens based on attention scores, but fail to effectively capture all semantic regions and often lead to token redundancy. Differently, we propose to leverage the Semantic Connected Components (SCC) approach that assigns tokens to distinct semantic regions within the token set, ensuring comprehensive semantic coverage. The outcome is a two-step spatio-temporal token compression strategy that utilizes SCC in both spatial and temporal domains. This strategy can effectively compress tokens by representing the entire video with a set of non-overlapping semantic tokens. We conduct extensive evaluations of the token compression capabilities of LLaVA-Scissor across diverse video understanding benchmarks, including video question answering, long video understanding, and comprehensive multi-choices benchmarks. Experimental results show that the proposed LLaVA-Scissor outperforms other token compression methods, achieving superior performance in various video understanding benchmarks, particularly at low token retention ratios. Project page: https://github.com/HumanMLLM/LLaVA-Scissor."

[30.06.2025 03:51] Response: ```python
['DATASET', 'MULTIMODAL', 'VIDEO', 'TRAINING', 'BENCHMARK']
```
[30.06.2025 03:51] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"LLaVA-Scissor, a token compression strategy for video multimodal large language models, uses Semantic Connected Components to compress tokens effectively while maintaining semantic coverage and outperforming other methods.  					AI-generated summary 				 In this paper, we present LLaVA-Scissor, a training-free token compression strategy designed for video multimodal large language models. Previous methods mostly attempt to compress tokens based on attention scores, but fail to effectively capture all semantic regions and often lead to token redundancy. Differently, we propose to leverage the Semantic Connected Components (SCC) approach that assigns tokens to distinct semantic regions within the token set, ensuring comprehensive semantic coverage. The outcome is a two-step spatio-temporal token compression strategy that utilizes SCC in both spatial and temporal domains. This strategy can effectively compress tokens by representing the entire video with a set of non-overlapping semantic tokens. We conduct extensive evaluations of the token compression capabilities of LLaVA-Scissor across diverse video understanding benchmarks, including video question answering, long video understanding, and comprehensive multi-choices benchmarks. Experimental results show that the proposed LLaVA-Scissor outperforms other token compression methods, achieving superior performance in various video understanding benchmarks, particularly at low token retention ratios. Project page: https://github.com/HumanMLLM/LLaVA-Scissor."

[30.06.2025 03:51] Response: ```python
["LONG_CONTEXT"]
```
[30.06.2025 03:51] Response: ParsedChatCompletionMessage[Article](content='{"desc":"LLaVA-Scissor is a novel token compression strategy specifically designed for video multimodal large language models. It utilizes Semantic Connected Components (SCC) to effectively group tokens into distinct semantic regions, which helps in reducing redundancy and maintaining semantic integrity. Unlike previous methods that rely on attention scores, LLaVA-Scissor compresses tokens in both spatial and temporal dimensions, ensuring comprehensive coverage of the video\'s content. Extensive evaluations demonstrate that this approach significantly outperforms existing token compression techniques, especially when retaining fewer tokens during video understanding tasks.","title":"Efficient Video Understanding with Semantic Token Compression"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="LLaVA-Scissor is a novel token compression strategy specifically designed for video multimodal large language models. It utilizes Semantic Connected Components (SCC) to effectively group tokens into distinct semantic regions, which helps in reducing redundancy and maintaining semantic integrity. Unlike previous methods that rely on attention scores, LLaVA-Scissor compresses tokens in both spatial and temporal dimensions, ensuring comprehensive coverage of the video's content. Extensive evaluations demonstrate that this approach significantly outperforms existing token compression techniques, especially when retaining fewer tokens during video understanding tasks.", title='Efficient Video Understanding with Semantic Token Compression'))
[30.06.2025 03:51] Response: ParsedChatCompletionMessage[Article](content='{"desc":"LLaVA-Scissor是一种针对视频多模态大语言模型的令牌压缩策略。它利用语义连通组件（SCC）方法，有效地将令牌分配到不同的语义区域，从而确保全面的语义覆盖。与以往基于注意力分数的压缩方法不同，LLaVA-Scissor能够减少令牌冗余，并在空间和时间域中进行两步压缩。实验结果表明，该方法在视频理解基准测试中表现优异，尤其是在低令牌保留比率下。","title":"LLaVA-Scissor：高效的视频令牌压缩策略"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='LLaVA-Scissor是一种针对视频多模态大语言模型的令牌压缩策略。它利用语义连通组件（SCC）方法，有效地将令牌分配到不同的语义区域，从而确保全面的语义覆盖。与以往基于注意力分数的压缩方法不同，LLaVA-Scissor能够减少令牌冗余，并在空间和时间域中进行两步压缩。实验结果表明，该方法在视频理解基准测试中表现优异，尤其是在低令牌保留比率下。', title='LLaVA-Scissor：高效的视频令牌压缩策略'))
[30.06.2025 03:51] Querying the API.
[30.06.2025 03:51] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

SpatialReasoner-R1, a vision-language reasoning model, uses Multi-Model Monte Carlo Tree Search and fine-grained Direct Preference Optimization to improve spatial reasoning, setting a new state-of-the-art on SPATIALRGPT-Bench.  					AI-generated summary 				 Current Vision-Language Models (VLMs) struggle with fine-grained spatial reasoning, particularly when multi-step logic and precise spatial alignment are required. In this work, we introduce SpatialReasoner-R1, a vision-language reasoning model designed to address these limitations. To construct high-quality supervision for spatial reasoning, we design a Multi-Model Monte Carlo Tree Search (M3CTS) method that generates diverse, logically consistent Long Chain-of-Thought (LongCoT) reasoning trajectories. In addition, we propose fine-grained Direct Preference Optimization (fDPO), which introduces segment-specific preference granularity for descriptive grounding and logical reasoning, guided by a spatial reward mechanism that evaluates candidate responses based on visual consistency, spatial grounding, and logical coherence. Experimental results demonstrate that fDPO achieves an average improvement of 4.1% over standard DPO across spatial quality tasks, and a 9.0% gain in spatial quantity tasks. SpatialReasoner-R1, trained with fDPO, sets a new SoTA on SPATIALRGPT-Bench, outperforming the strongest baseline by 9.8% in average accuracy, while maintaining competitive performance on general vision-language tasks.
[30.06.2025 03:51] Response: {
  "desc": "SpatialReasoner-R1 - это новая модель зрительно-языкового рассуждения, которая улучшает пространственное мышление с помощью мультимодельного поиска Монте-Карло по дереву и оптимизации прямых предпочтений. Модель генерирует длинные цепочки рассуждений и использует сегментированную оптимизацию предпочтений для улучшения визуальной и логической согласованности. SpatialReasoner-R1 достигает нового уровня производительности на бенчмарке SPATIALRGPT-Bench, превосходя базовые модели на 9.8% по средней точности. При этом модель сохраняет конкурентоспособность в общих задачах компьютерного зрения и обработки естественного языка.",

  "emoji": "🧠",

  "title": "Пространственное мышление ИИ выходит на новый уровень"
}
[30.06.2025 03:51] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"SpatialReasoner-R1, a vision-language reasoning model, uses Multi-Model Monte Carlo Tree Search and fine-grained Direct Preference Optimization to improve spatial reasoning, setting a new state-of-the-art on SPATIALRGPT-Bench.  					AI-generated summary 				 Current Vision-Language Models (VLMs) struggle with fine-grained spatial reasoning, particularly when multi-step logic and precise spatial alignment are required. In this work, we introduce SpatialReasoner-R1, a vision-language reasoning model designed to address these limitations. To construct high-quality supervision for spatial reasoning, we design a Multi-Model Monte Carlo Tree Search (M3CTS) method that generates diverse, logically consistent Long Chain-of-Thought (LongCoT) reasoning trajectories. In addition, we propose fine-grained Direct Preference Optimization (fDPO), which introduces segment-specific preference granularity for descriptive grounding and logical reasoning, guided by a spatial reward mechanism that evaluates candidate responses based on visual consistency, spatial grounding, and logical coherence. Experimental results demonstrate that fDPO achieves an average improvement of 4.1% over standard DPO across spatial quality tasks, and a 9.0% gain in spatial quantity tasks. SpatialReasoner-R1, trained with fDPO, sets a new SoTA on SPATIALRGPT-Bench, outperforming the strongest baseline by 9.8% in average accuracy, while maintaining competitive performance on general vision-language tasks."

[30.06.2025 03:51] Response: ```python
['CV', 'RLHF', 'MULTIMODAL', 'TRAINING', 'ARCHITECTURE']
```
[30.06.2025 03:51] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"SpatialReasoner-R1, a vision-language reasoning model, uses Multi-Model Monte Carlo Tree Search and fine-grained Direct Preference Optimization to improve spatial reasoning, setting a new state-of-the-art on SPATIALRGPT-Bench.  					AI-generated summary 				 Current Vision-Language Models (VLMs) struggle with fine-grained spatial reasoning, particularly when multi-step logic and precise spatial alignment are required. In this work, we introduce SpatialReasoner-R1, a vision-language reasoning model designed to address these limitations. To construct high-quality supervision for spatial reasoning, we design a Multi-Model Monte Carlo Tree Search (M3CTS) method that generates diverse, logically consistent Long Chain-of-Thought (LongCoT) reasoning trajectories. In addition, we propose fine-grained Direct Preference Optimization (fDPO), which introduces segment-specific preference granularity for descriptive grounding and logical reasoning, guided by a spatial reward mechanism that evaluates candidate responses based on visual consistency, spatial grounding, and logical coherence. Experimental results demonstrate that fDPO achieves an average improvement of 4.1% over standard DPO across spatial quality tasks, and a 9.0% gain in spatial quantity tasks. SpatialReasoner-R1, trained with fDPO, sets a new SoTA on SPATIALRGPT-Bench, outperforming the strongest baseline by 9.8% in average accuracy, while maintaining competitive performance on general vision-language tasks."

[30.06.2025 03:52] Response: ```python
["REASONING", "OPTIMIZATION"]
```
[30.06.2025 03:52] Response: ParsedChatCompletionMessage[Article](content='{"desc":"SpatialReasoner-R1 is a vision-language reasoning model that enhances spatial reasoning capabilities in AI. It employs Multi-Model Monte Carlo Tree Search (M3CTS) to create diverse reasoning paths that are logically consistent, improving the model\'s ability to handle complex spatial tasks. Additionally, it introduces fine-grained Direct Preference Optimization (fDPO), which refines the model\'s decision-making by focusing on specific segments of reasoning and using a spatial reward mechanism. The model achieves state-of-the-art performance on the SPATIALRGPT-Bench, significantly outperforming previous models in both spatial quality and quantity tasks.","title":"Elevating Spatial Reasoning with SpatialReasoner-R1"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="SpatialReasoner-R1 is a vision-language reasoning model that enhances spatial reasoning capabilities in AI. It employs Multi-Model Monte Carlo Tree Search (M3CTS) to create diverse reasoning paths that are logically consistent, improving the model's ability to handle complex spatial tasks. Additionally, it introduces fine-grained Direct Preference Optimization (fDPO), which refines the model's decision-making by focusing on specific segments of reasoning and using a spatial reward mechanism. The model achieves state-of-the-art performance on the SPATIALRGPT-Bench, significantly outperforming previous models in both spatial quality and quantity tasks.", title='Elevating Spatial Reasoning with SpatialReasoner-R1'))
[30.06.2025 03:52] Response: ParsedChatCompletionMessage[Article](content='{"desc":"SpatialReasoner-R1是一种视觉-语言推理模型，旨在解决当前视觉-语言模型在细粒度空间推理方面的不足。该模型采用多模型蒙特卡洛树搜索（M3CTS）方法，生成多样且逻辑一致的长链思维推理轨迹，以构建高质量的空间推理监督。除此之外，SpatialReasoner-R1还引入了细粒度直接偏好优化（fDPO），通过空间奖励机制对候选响应进行评估，从而提高描述性基础和逻辑推理的准确性。实验结果表明，SpatialReasoner-R1在SPATIALRGPT-Bench上设定了新的最先进水平，平均准确率比最强基线提高了9.8%。","title":"空间推理的新突破"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='SpatialReasoner-R1是一种视觉-语言推理模型，旨在解决当前视觉-语言模型在细粒度空间推理方面的不足。该模型采用多模型蒙特卡洛树搜索（M3CTS）方法，生成多样且逻辑一致的长链思维推理轨迹，以构建高质量的空间推理监督。除此之外，SpatialReasoner-R1还引入了细粒度直接偏好优化（fDPO），通过空间奖励机制对候选响应进行评估，从而提高描述性基础和逻辑推理的准确性。实验结果表明，SpatialReasoner-R1在SPATIALRGPT-Bench上设定了新的最先进水平，平均准确率比最强基线提高了9.8%。', title='空间推理的新突破'))
[30.06.2025 03:52] Querying the API.
[30.06.2025 03:52] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Self-supervised learning using image triplets enhances the reasoning ability of Vision-Language Models (VLMs) on multi-image tasks without the need for human-annotated question-answer pairs.  					AI-generated summary 				 This work explores enabling Chain-of-Thought (CoT) reasoning to link visual cues across multiple images. A straightforward solution is to adapt rule-based reinforcement learning for Vision-Language Models (VLMs). However, such methods typically rely on manually curated question-answer pairs, which can be particularly challenging when dealing with fine grained visual details and complex logic across images. Inspired by self-supervised visual representation learning, we observe that images contain inherent constraints that can serve as supervision. Based on this insight, we construct image triplets comprising two augmented views of the same image and a third, similar but distinct image. During training, the model is prompted to generate a reasoning process to compare these images (i.e., determine same or different). Then we optimize the model with rule-based reinforcement learning. Due to the high visual similarity and the presence of augmentations, the model must attend to subtle visual changes and perform logical reasoning to succeed. Experiments show that, although trained solely on visual comparison tasks, the learned reasoning ability generalizes effectively to a wide range of questions. Without relying on any human-annotated question-answer pairs, our method achieves significant improvements on multi-image reasoning benchmarks and shows strong performance on general vision tasks.
[30.06.2025 03:52] Response: {
  "desc": "Статья представляет метод самообучения моделей компьютерного зрения и обработки естественного языка (VLM) для улучшения их способности рассуждать о нескольких изображениях. Авторы используют триплеты изображений и обучение с подкреплением, чтобы научить модель сравнивать тонкие визуальные детали. Этот подход не требует размеченных человеком пар вопрос-ответ и позволяет генерировать цепочки рассуждений. Эксперименты показывают, что полученные навыки обобщаются на широкий спектр задач визуального анализа.",
  "emoji": "🔍",
  "title": "Самообучение ИИ визуальным рассуждениям без участия человека"
}
[30.06.2025 03:52] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Self-supervised learning using image triplets enhances the reasoning ability of Vision-Language Models (VLMs) on multi-image tasks without the need for human-annotated question-answer pairs.  					AI-generated summary 				 This work explores enabling Chain-of-Thought (CoT) reasoning to link visual cues across multiple images. A straightforward solution is to adapt rule-based reinforcement learning for Vision-Language Models (VLMs). However, such methods typically rely on manually curated question-answer pairs, which can be particularly challenging when dealing with fine grained visual details and complex logic across images. Inspired by self-supervised visual representation learning, we observe that images contain inherent constraints that can serve as supervision. Based on this insight, we construct image triplets comprising two augmented views of the same image and a third, similar but distinct image. During training, the model is prompted to generate a reasoning process to compare these images (i.e., determine same or different). Then we optimize the model with rule-based reinforcement learning. Due to the high visual similarity and the presence of augmentations, the model must attend to subtle visual changes and perform logical reasoning to succeed. Experiments show that, although trained solely on visual comparison tasks, the learned reasoning ability generalizes effectively to a wide range of questions. Without relying on any human-annotated question-answer pairs, our method achieves significant improvements on multi-image reasoning benchmarks and shows strong performance on general vision tasks."

[30.06.2025 03:52] Response: ```python
['DATASET', 'RL', 'CV', 'BENCHMARK']
```
[30.06.2025 03:52] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Self-supervised learning using image triplets enhances the reasoning ability of Vision-Language Models (VLMs) on multi-image tasks without the need for human-annotated question-answer pairs.  					AI-generated summary 				 This work explores enabling Chain-of-Thought (CoT) reasoning to link visual cues across multiple images. A straightforward solution is to adapt rule-based reinforcement learning for Vision-Language Models (VLMs). However, such methods typically rely on manually curated question-answer pairs, which can be particularly challenging when dealing with fine grained visual details and complex logic across images. Inspired by self-supervised visual representation learning, we observe that images contain inherent constraints that can serve as supervision. Based on this insight, we construct image triplets comprising two augmented views of the same image and a third, similar but distinct image. During training, the model is prompted to generate a reasoning process to compare these images (i.e., determine same or different). Then we optimize the model with rule-based reinforcement learning. Due to the high visual similarity and the presence of augmentations, the model must attend to subtle visual changes and perform logical reasoning to succeed. Experiments show that, although trained solely on visual comparison tasks, the learned reasoning ability generalizes effectively to a wide range of questions. Without relying on any human-annotated question-answer pairs, our method achieves significant improvements on multi-image reasoning benchmarks and shows strong performance on general vision tasks."

[30.06.2025 03:52] Response: ```python
['REASONING', 'OPTIMIZATION']
```
[30.06.2025 03:52] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents a method for enhancing the reasoning capabilities of Vision-Language Models (VLMs) using self-supervised learning with image triplets. The approach involves creating pairs of augmented images along with a distinct image, allowing the model to learn to compare and reason about visual differences. By training the model to determine whether images are the same or different, it develops a reasoning process that generalizes to various tasks without needing human-annotated data. The results demonstrate that this method significantly improves performance on multi-image reasoning benchmarks and general vision tasks.","title":"Empowering VLMs with Self-Supervised Image Triplet Learning"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper presents a method for enhancing the reasoning capabilities of Vision-Language Models (VLMs) using self-supervised learning with image triplets. The approach involves creating pairs of augmented images along with a distinct image, allowing the model to learn to compare and reason about visual differences. By training the model to determine whether images are the same or different, it develops a reasoning process that generalizes to various tasks without needing human-annotated data. The results demonstrate that this method significantly improves performance on multi-image reasoning benchmarks and general vision tasks.', title='Empowering VLMs with Self-Supervised Image Triplet Learning'))
[30.06.2025 03:52] Response: ParsedChatCompletionMessage[Article](content='{"desc":"这篇论文探讨了如何通过使用图像三元组的自监督学习来增强视觉语言模型（VLM）在多图像任务上的推理能力，而无需人工标注的问题-答案对。研究者们构建了由同一图像的两个增强视图和一个相似但不同的图像组成的图像三元组。在训练过程中，模型被要求生成推理过程，以比较这些图像（即判断相同或不同）。实验表明，尽管模型仅在视觉比较任务上训练，但其学习到的推理能力能够有效地推广到各种问题上。","title":"自监督学习提升视觉语言模型推理能力"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='这篇论文探讨了如何通过使用图像三元组的自监督学习来增强视觉语言模型（VLM）在多图像任务上的推理能力，而无需人工标注的问题-答案对。研究者们构建了由同一图像的两个增强视图和一个相似但不同的图像组成的图像三元组。在训练过程中，模型被要求生成推理过程，以比较这些图像（即判断相同或不同）。实验表明，尽管模型仅在视觉比较任务上训练，但其学习到的推理能力能够有效地推广到各种问题上。', title='自监督学习提升视觉语言模型推理能力'))
[30.06.2025 03:52] Querying the API.
[30.06.2025 03:52] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A novel Noise Consistency Training approach integrates new control signals into pre-trained one-step generators efficiently without retraining, outperforming existing methods in quality and computational efficiency.  					AI-generated summary 				 The pursuit of efficient and controllable high-quality content generation remains a central challenge in artificial intelligence-generated content (AIGC). While one-step generators, enabled by diffusion distillation techniques, offer excellent generation quality and computational efficiency, adapting them to new control conditions--such as structural constraints, semantic guidelines, or external inputs--poses a significant challenge. Conventional approaches often necessitate computationally expensive modifications to the base model and subsequent diffusion distillation. This paper introduces Noise Consistency Training (NCT), a novel and lightweight approach to directly integrate new control signals into pre-trained one-step generators without requiring access to original training images or retraining the base diffusion model. NCT operates by introducing an adapter module and employs a noise consistency loss in the noise space of the generator. This loss aligns the adapted model's generation behavior across noises that are conditionally dependent to varying degrees, implicitly guiding it to adhere to the new control. Theoretically, this training objective can be understood as minimizing the distributional distance between the adapted generator and the conditional distribution induced by the new conditions. NCT is modular, data-efficient, and easily deployable, relying only on the pre-trained one-step generator and a control signal model. Extensive experiments demonstrate that NCT achieves state-of-the-art controllable generation in a single forward pass, surpassing existing multi-step and distillation-based methods in both generation quality and computational efficiency. Code is available at https://github.com/Luo-Yihong/NCT
[30.06.2025 03:52] Response: {
  "desc": "Статья представляет новый метод Noise Consistency Training (NCT) для интеграции новых сигналов управления в предобученные одношаговые генераторы без необходимости переобучения. NCT использует адаптерный модуль и функцию потерь согласованности шума в пространстве шума генератора. Этот подход позволяет эффективно адаптировать модели к новым условиям, таким как структурные ограничения или семантические указания. Эксперименты показывают, что NCT превосходит существующие методы по качеству генерации и вычислительной эффективности.",
  "emoji": "🎛️",
  "title": "Эффективная адаптация генеративных моделей без переобучения"
}
[30.06.2025 03:52] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A novel Noise Consistency Training approach integrates new control signals into pre-trained one-step generators efficiently without retraining, outperforming existing methods in quality and computational efficiency.  					AI-generated summary 				 The pursuit of efficient and controllable high-quality content generation remains a central challenge in artificial intelligence-generated content (AIGC). While one-step generators, enabled by diffusion distillation techniques, offer excellent generation quality and computational efficiency, adapting them to new control conditions--such as structural constraints, semantic guidelines, or external inputs--poses a significant challenge. Conventional approaches often necessitate computationally expensive modifications to the base model and subsequent diffusion distillation. This paper introduces Noise Consistency Training (NCT), a novel and lightweight approach to directly integrate new control signals into pre-trained one-step generators without requiring access to original training images or retraining the base diffusion model. NCT operates by introducing an adapter module and employs a noise consistency loss in the noise space of the generator. This loss aligns the adapted model's generation behavior across noises that are conditionally dependent to varying degrees, implicitly guiding it to adhere to the new control. Theoretically, this training objective can be understood as minimizing the distributional distance between the adapted generator and the conditional distribution induced by the new conditions. NCT is modular, data-efficient, and easily deployable, relying only on the pre-trained one-step generator and a control signal model. Extensive experiments demonstrate that NCT achieves state-of-the-art controllable generation in a single forward pass, surpassing existing multi-step and distillation-based methods in both generation quality and computational efficiency. Code is available at https://github.com/Luo-Yihong/NCT"

[30.06.2025 03:52] Response: ```python
["TRAINING", "CV"]
```
[30.06.2025 03:52] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A novel Noise Consistency Training approach integrates new control signals into pre-trained one-step generators efficiently without retraining, outperforming existing methods in quality and computational efficiency.  					AI-generated summary 				 The pursuit of efficient and controllable high-quality content generation remains a central challenge in artificial intelligence-generated content (AIGC). While one-step generators, enabled by diffusion distillation techniques, offer excellent generation quality and computational efficiency, adapting them to new control conditions--such as structural constraints, semantic guidelines, or external inputs--poses a significant challenge. Conventional approaches often necessitate computationally expensive modifications to the base model and subsequent diffusion distillation. This paper introduces Noise Consistency Training (NCT), a novel and lightweight approach to directly integrate new control signals into pre-trained one-step generators without requiring access to original training images or retraining the base diffusion model. NCT operates by introducing an adapter module and employs a noise consistency loss in the noise space of the generator. This loss aligns the adapted model's generation behavior across noises that are conditionally dependent to varying degrees, implicitly guiding it to adhere to the new control. Theoretically, this training objective can be understood as minimizing the distributional distance between the adapted generator and the conditional distribution induced by the new conditions. NCT is modular, data-efficient, and easily deployable, relying only on the pre-trained one-step generator and a control signal model. Extensive experiments demonstrate that NCT achieves state-of-the-art controllable generation in a single forward pass, surpassing existing multi-step and distillation-based methods in both generation quality and computational efficiency. Code is available at https://github.com/Luo-Yihong/NCT"

[30.06.2025 03:52] Response: ```python
["DIFFUSION", "OPTIMIZATION"]
```
[30.06.2025 03:52] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents a new method called Noise Consistency Training (NCT) that enhances pre-trained one-step generators for content generation without the need for retraining. NCT efficiently integrates new control signals, such as structural or semantic guidelines, into the generator by using an adapter module and a noise consistency loss. This approach allows the generator to produce high-quality outputs while maintaining computational efficiency, outperforming traditional methods that require extensive modifications. The results show that NCT achieves superior controllable generation in a single forward pass, making it a significant advancement in the field of artificial intelligence-generated content.","title":"Efficient Control in AI Content Generation with Noise Consistency Training"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper presents a new method called Noise Consistency Training (NCT) that enhances pre-trained one-step generators for content generation without the need for retraining. NCT efficiently integrates new control signals, such as structural or semantic guidelines, into the generator by using an adapter module and a noise consistency loss. This approach allows the generator to produce high-quality outputs while maintaining computational efficiency, outperforming traditional methods that require extensive modifications. The results show that NCT achieves superior controllable generation in a single forward pass, making it a significant advancement in the field of artificial intelligence-generated content.', title='Efficient Control in AI Content Generation with Noise Consistency Training'))
[30.06.2025 03:52] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本文提出了一种新颖的噪声一致性训练（NCT）方法，能够高效地将新的控制信号整合到预训练的一步生成器中，而无需重新训练。传统方法通常需要对基础模型进行昂贵的修改，而NCT通过引入适配模块和噪声一致性损失，在生成器的噪声空间中直接进行调整。该方法在生成质量和计算效率上超越了现有的多步和蒸馏方法，展示了其在可控生成方面的优越性。NCT的模块化设计使其在数据使用上更加高效，易于部署。","title":"噪声一致性训练：高效可控生成的新方法"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本文提出了一种新颖的噪声一致性训练（NCT）方法，能够高效地将新的控制信号整合到预训练的一步生成器中，而无需重新训练。传统方法通常需要对基础模型进行昂贵的修改，而NCT通过引入适配模块和噪声一致性损失，在生成器的噪声空间中直接进行调整。该方法在生成质量和计算效率上超越了现有的多步和蒸馏方法，展示了其在可控生成方面的优越性。NCT的模块化设计使其在数据使用上更加高效，易于部署。', title='噪声一致性训练：高效可控生成的新方法'))
[30.06.2025 03:52] Renaming data file.
[30.06.2025 03:52] Renaming previous data. hf_papers.json to ./d/2025-06-30.json
[30.06.2025 03:52] Saving new data file.
[30.06.2025 03:52] Generating page.
[30.06.2025 03:52] Renaming previous page.
[30.06.2025 03:52] Renaming previous data. index.html to ./d/2025-06-30.html
[30.06.2025 03:52] Writing result.
[30.06.2025 03:52] Renaming log file.
[30.06.2025 03:52] Renaming previous data. log.txt to ./logs/2025-06-30_last_log.txt
