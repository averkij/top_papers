[04.02.2026 12:41] Read previous papers.
[04.02.2026 12:41] Generating top page (month).
[04.02.2026 12:41] Writing top page (month).
[04.02.2026 13:59] Read previous papers.
[04.02.2026 13:59] Get feed.
[04.02.2026 13:59] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01785
[04.02.2026 13:59] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02103
[04.02.2026 13:59] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03786
[04.02.2026 13:59] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01630
[04.02.2026 13:59] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02619
[04.02.2026 13:59] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03796
[04.02.2026 13:59] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02660
[04.02.2026 13:59] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03048
[04.02.2026 13:59] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03139
[04.02.2026 13:59] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03419
[04.02.2026 13:59] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03411
[04.02.2026 13:59] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03619
[04.02.2026 13:59] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03845
[04.02.2026 13:59] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02380
[04.02.2026 13:59] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02636
[04.02.2026 13:59] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21244
[04.02.2026 13:59] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03798
[04.02.2026 13:59] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03216
[04.02.2026 13:59] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01362
[04.02.2026 13:59] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02676
[04.02.2026 13:59] Get page data from previous paper. URL: https://huggingface.co/papers/2602.00747
[04.02.2026 13:59] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03709
[04.02.2026 13:59] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01053
[04.02.2026 13:59] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02537
[04.02.2026 13:59] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03677
[04.02.2026 13:59] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03647
[04.02.2026 13:59] Get page data from previous paper. URL: https://huggingface.co/papers/2601.19103
[04.02.2026 13:59] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03806
[04.02.2026 13:59] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02419
[04.02.2026 13:59] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01753
[04.02.2026 13:59] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03837
[04.02.2026 13:59] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03454
[04.02.2026 13:59] Extract page data from URL. URL: https://huggingface.co/papers/2602.03295
[04.02.2026 13:59] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02914
[04.02.2026 13:59] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03238
[04.02.2026 13:59] Extract page data from URL. URL: https://huggingface.co/papers/2602.02494
[04.02.2026 13:59] Extract page data from URL. URL: https://huggingface.co/papers/2602.02220
[04.02.2026 13:59] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01212
[04.02.2026 13:59] Get page data from previous paper. URL: https://huggingface.co/papers/2602.00682
[04.02.2026 13:59] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03817
[04.02.2026 13:59] Extract page data from URL. URL: https://huggingface.co/papers/2602.03320
[04.02.2026 13:59] Extract page data from URL. URL: https://huggingface.co/papers/2602.01519
[04.02.2026 13:59] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[04.02.2026 13:59] No deleted papers detected.
[04.02.2026 13:59] Downloading and parsing papers (pdf, html). Total: 42.
[04.02.2026 13:59] Downloading and parsing paper https://huggingface.co/papers/2602.01785.
[04.02.2026 13:59] Extra JSON file exists (./assets/json/2602.01785.json), skip PDF parsing.
[04.02.2026 13:59] Paper image links file exists (./assets/img_data/2602.01785.json), skip HTML parsing.
[04.02.2026 13:59] Success.
[04.02.2026 13:59] Downloading and parsing paper https://huggingface.co/papers/2602.02103.
[04.02.2026 13:59] Extra JSON file exists (./assets/json/2602.02103.json), skip PDF parsing.
[04.02.2026 13:59] Paper image links file exists (./assets/img_data/2602.02103.json), skip HTML parsing.
[04.02.2026 13:59] Success.
[04.02.2026 13:59] Downloading and parsing paper https://huggingface.co/papers/2602.03786.
[04.02.2026 13:59] Extra JSON file exists (./assets/json/2602.03786.json), skip PDF parsing.
[04.02.2026 13:59] Paper image links file exists (./assets/img_data/2602.03786.json), skip HTML parsing.
[04.02.2026 13:59] Success.
[04.02.2026 13:59] Downloading and parsing paper https://huggingface.co/papers/2602.01630.
[04.02.2026 13:59] Extra JSON file exists (./assets/json/2602.01630.json), skip PDF parsing.
[04.02.2026 13:59] Paper image links file exists (./assets/img_data/2602.01630.json), skip HTML parsing.
[04.02.2026 13:59] Success.
[04.02.2026 13:59] Downloading and parsing paper https://huggingface.co/papers/2602.02619.
[04.02.2026 13:59] Extra JSON file exists (./assets/json/2602.02619.json), skip PDF parsing.
[04.02.2026 13:59] Paper image links file exists (./assets/img_data/2602.02619.json), skip HTML parsing.
[04.02.2026 13:59] Success.
[04.02.2026 13:59] Downloading and parsing paper https://huggingface.co/papers/2602.03796.
[04.02.2026 13:59] Extra JSON file exists (./assets/json/2602.03796.json), skip PDF parsing.
[04.02.2026 13:59] Paper image links file exists (./assets/img_data/2602.03796.json), skip HTML parsing.
[04.02.2026 13:59] Success.
[04.02.2026 13:59] Downloading and parsing paper https://huggingface.co/papers/2602.02660.
[04.02.2026 13:59] Extra JSON file exists (./assets/json/2602.02660.json), skip PDF parsing.
[04.02.2026 13:59] Paper image links file exists (./assets/img_data/2602.02660.json), skip HTML parsing.
[04.02.2026 13:59] Success.
[04.02.2026 13:59] Downloading and parsing paper https://huggingface.co/papers/2602.03048.
[04.02.2026 13:59] Extra JSON file exists (./assets/json/2602.03048.json), skip PDF parsing.
[04.02.2026 13:59] Paper image links file exists (./assets/img_data/2602.03048.json), skip HTML parsing.
[04.02.2026 13:59] Success.
[04.02.2026 13:59] Downloading and parsing paper https://huggingface.co/papers/2602.03139.
[04.02.2026 13:59] Extra JSON file exists (./assets/json/2602.03139.json), skip PDF parsing.
[04.02.2026 13:59] Paper image links file exists (./assets/img_data/2602.03139.json), skip HTML parsing.
[04.02.2026 13:59] Success.
[04.02.2026 13:59] Downloading and parsing paper https://huggingface.co/papers/2602.03419.
[04.02.2026 13:59] Extra JSON file exists (./assets/json/2602.03419.json), skip PDF parsing.
[04.02.2026 13:59] Paper image links file exists (./assets/img_data/2602.03419.json), skip HTML parsing.
[04.02.2026 13:59] Success.
[04.02.2026 13:59] Downloading and parsing paper https://huggingface.co/papers/2602.03411.
[04.02.2026 13:59] Extra JSON file exists (./assets/json/2602.03411.json), skip PDF parsing.
[04.02.2026 13:59] Paper image links file exists (./assets/img_data/2602.03411.json), skip HTML parsing.
[04.02.2026 13:59] Success.
[04.02.2026 13:59] Downloading and parsing paper https://huggingface.co/papers/2602.03619.
[04.02.2026 13:59] Extra JSON file exists (./assets/json/2602.03619.json), skip PDF parsing.
[04.02.2026 13:59] Paper image links file exists (./assets/img_data/2602.03619.json), skip HTML parsing.
[04.02.2026 13:59] Success.
[04.02.2026 13:59] Downloading and parsing paper https://huggingface.co/papers/2602.03845.
[04.02.2026 13:59] Extra JSON file exists (./assets/json/2602.03845.json), skip PDF parsing.
[04.02.2026 13:59] Paper image links file exists (./assets/img_data/2602.03845.json), skip HTML parsing.
[04.02.2026 13:59] Success.
[04.02.2026 13:59] Downloading and parsing paper https://huggingface.co/papers/2602.02380.
[04.02.2026 13:59] Extra JSON file exists (./assets/json/2602.02380.json), skip PDF parsing.
[04.02.2026 13:59] Paper image links file exists (./assets/img_data/2602.02380.json), skip HTML parsing.
[04.02.2026 13:59] Success.
[04.02.2026 13:59] Downloading and parsing paper https://huggingface.co/papers/2602.02636.
[04.02.2026 13:59] Extra JSON file exists (./assets/json/2602.02636.json), skip PDF parsing.
[04.02.2026 13:59] Paper image links file exists (./assets/img_data/2602.02636.json), skip HTML parsing.
[04.02.2026 13:59] Success.
[04.02.2026 13:59] Downloading and parsing paper https://huggingface.co/papers/2601.21244.
[04.02.2026 13:59] Extra JSON file exists (./assets/json/2601.21244.json), skip PDF parsing.
[04.02.2026 13:59] Paper image links file exists (./assets/img_data/2601.21244.json), skip HTML parsing.
[04.02.2026 13:59] Success.
[04.02.2026 13:59] Downloading and parsing paper https://huggingface.co/papers/2602.03798.
[04.02.2026 13:59] Extra JSON file exists (./assets/json/2602.03798.json), skip PDF parsing.
[04.02.2026 13:59] Paper image links file exists (./assets/img_data/2602.03798.json), skip HTML parsing.
[04.02.2026 13:59] Success.
[04.02.2026 13:59] Downloading and parsing paper https://huggingface.co/papers/2602.03216.
[04.02.2026 13:59] Extra JSON file exists (./assets/json/2602.03216.json), skip PDF parsing.
[04.02.2026 13:59] Paper image links file exists (./assets/img_data/2602.03216.json), skip HTML parsing.
[04.02.2026 13:59] Success.
[04.02.2026 13:59] Downloading and parsing paper https://huggingface.co/papers/2602.01362.
[04.02.2026 13:59] Extra JSON file exists (./assets/json/2602.01362.json), skip PDF parsing.
[04.02.2026 13:59] Paper image links file exists (./assets/img_data/2602.01362.json), skip HTML parsing.
[04.02.2026 13:59] Success.
[04.02.2026 13:59] Downloading and parsing paper https://huggingface.co/papers/2602.02676.
[04.02.2026 13:59] Extra JSON file exists (./assets/json/2602.02676.json), skip PDF parsing.
[04.02.2026 13:59] Paper image links file exists (./assets/img_data/2602.02676.json), skip HTML parsing.
[04.02.2026 13:59] Success.
[04.02.2026 13:59] Downloading and parsing paper https://huggingface.co/papers/2602.00747.
[04.02.2026 13:59] Extra JSON file exists (./assets/json/2602.00747.json), skip PDF parsing.
[04.02.2026 13:59] Paper image links file exists (./assets/img_data/2602.00747.json), skip HTML parsing.
[04.02.2026 13:59] Success.
[04.02.2026 13:59] Downloading and parsing paper https://huggingface.co/papers/2602.03709.
[04.02.2026 13:59] Extra JSON file exists (./assets/json/2602.03709.json), skip PDF parsing.
[04.02.2026 13:59] Paper image links file exists (./assets/img_data/2602.03709.json), skip HTML parsing.
[04.02.2026 13:59] Success.
[04.02.2026 13:59] Downloading and parsing paper https://huggingface.co/papers/2602.01053.
[04.02.2026 13:59] Extra JSON file exists (./assets/json/2602.01053.json), skip PDF parsing.
[04.02.2026 13:59] Paper image links file exists (./assets/img_data/2602.01053.json), skip HTML parsing.
[04.02.2026 13:59] Success.
[04.02.2026 13:59] Downloading and parsing paper https://huggingface.co/papers/2602.02537.
[04.02.2026 13:59] Extra JSON file exists (./assets/json/2602.02537.json), skip PDF parsing.
[04.02.2026 13:59] Paper image links file exists (./assets/img_data/2602.02537.json), skip HTML parsing.
[04.02.2026 13:59] Success.
[04.02.2026 13:59] Downloading and parsing paper https://huggingface.co/papers/2602.03677.
[04.02.2026 13:59] Extra JSON file exists (./assets/json/2602.03677.json), skip PDF parsing.
[04.02.2026 13:59] Paper image links file exists (./assets/img_data/2602.03677.json), skip HTML parsing.
[04.02.2026 13:59] Success.
[04.02.2026 13:59] Downloading and parsing paper https://huggingface.co/papers/2602.03647.
[04.02.2026 13:59] Extra JSON file exists (./assets/json/2602.03647.json), skip PDF parsing.
[04.02.2026 13:59] Paper image links file exists (./assets/img_data/2602.03647.json), skip HTML parsing.
[04.02.2026 13:59] Success.
[04.02.2026 13:59] Downloading and parsing paper https://huggingface.co/papers/2601.19103.
[04.02.2026 13:59] Extra JSON file exists (./assets/json/2601.19103.json), skip PDF parsing.
[04.02.2026 13:59] Paper image links file exists (./assets/img_data/2601.19103.json), skip HTML parsing.
[04.02.2026 13:59] Success.
[04.02.2026 13:59] Downloading and parsing paper https://huggingface.co/papers/2602.03806.
[04.02.2026 13:59] Extra JSON file exists (./assets/json/2602.03806.json), skip PDF parsing.
[04.02.2026 13:59] Paper image links file exists (./assets/img_data/2602.03806.json), skip HTML parsing.
[04.02.2026 13:59] Success.
[04.02.2026 13:59] Downloading and parsing paper https://huggingface.co/papers/2602.02419.
[04.02.2026 13:59] Extra JSON file exists (./assets/json/2602.02419.json), skip PDF parsing.
[04.02.2026 13:59] Paper image links file exists (./assets/img_data/2602.02419.json), skip HTML parsing.
[04.02.2026 13:59] Success.
[04.02.2026 13:59] Downloading and parsing paper https://huggingface.co/papers/2602.01753.
[04.02.2026 13:59] Extra JSON file exists (./assets/json/2602.01753.json), skip PDF parsing.
[04.02.2026 13:59] Paper image links file exists (./assets/img_data/2602.01753.json), skip HTML parsing.
[04.02.2026 13:59] Success.
[04.02.2026 13:59] Downloading and parsing paper https://huggingface.co/papers/2602.03837.
[04.02.2026 13:59] Extra JSON file exists (./assets/json/2602.03837.json), skip PDF parsing.
[04.02.2026 13:59] Paper image links file exists (./assets/img_data/2602.03837.json), skip HTML parsing.
[04.02.2026 13:59] Success.
[04.02.2026 13:59] Downloading and parsing paper https://huggingface.co/papers/2602.03454.
[04.02.2026 13:59] Extra JSON file exists (./assets/json/2602.03454.json), skip PDF parsing.
[04.02.2026 13:59] Paper image links file exists (./assets/img_data/2602.03454.json), skip HTML parsing.
[04.02.2026 13:59] Success.
[04.02.2026 13:59] Downloading and parsing paper https://huggingface.co/papers/2602.03295.
[04.02.2026 13:59] Downloading paper 2602.03295 from https://arxiv.org/pdf/2602.03295v1...
[04.02.2026 13:59] Extracting affiliations from text.
[04.02.2026 13:59] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"POP: Prefill-Only Pruning for Efficient Large Model Inference Junhui He1,2 Zhihui Fu2 Jun Wang2 Qingan Li1 * 1Wuhan University 2OPPO Research Institute 6 2 0 F 3 ] . [ 1 5 9 2 3 0 . 2 0 6 2 : r a "
[04.02.2026 13:59] Response: ```python
["Wuhan University", "OPPO Research Institute"]
```
[04.02.2026 13:59] Deleting PDF ./assets/pdf/2602.03295.pdf.
[04.02.2026 13:59] Success.
[04.02.2026 13:59] Downloading and parsing paper https://huggingface.co/papers/2602.02914.
[04.02.2026 13:59] Extra JSON file exists (./assets/json/2602.02914.json), skip PDF parsing.
[04.02.2026 13:59] Paper image links file exists (./assets/img_data/2602.02914.json), skip HTML parsing.
[04.02.2026 13:59] Success.
[04.02.2026 13:59] Downloading and parsing paper https://huggingface.co/papers/2602.03238.
[04.02.2026 13:59] Extra JSON file exists (./assets/json/2602.03238.json), skip PDF parsing.
[04.02.2026 13:59] Paper image links file exists (./assets/img_data/2602.03238.json), skip HTML parsing.
[04.02.2026 13:59] Success.
[04.02.2026 13:59] Downloading and parsing paper https://huggingface.co/papers/2602.02494.
[04.02.2026 13:59] Downloading paper 2602.02494 from https://arxiv.org/pdf/2602.02494v1...
[04.02.2026 13:59] Extracting affiliations from text.
[04.02.2026 13:59] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"MEG-XL: Data-Efficient Brain-to-Text via Long-Context Pre-Training Dulhan Jayalath Oiwi Parker Jones PNPL , University of Oxford {dulhan,oiwi}@robots.ox.ac.uk 6 2 0 2 2 ] . [ 1 4 9 4 2 0 . 2 0 6 2 : r Figure 1. MEG-XL introduces long-context MEG pre-training. When fine-tuned, this approach generalises to decoding words in brain-to-text with less labelled subject data than required by the supervised state-of-the-art (SOTA) and brain foundation models (FMs). Abstract Clinical brain-to-text interfaces are designed for paralysed patients who cannot provide extensive training recordings. Pre-training improves dataefficient generalisation by learning statistical priors across subjects, but these priors critically depend on context. While natural speech might unfold gradually over minutes, most methods pretrain with only few seconds of context. Thus, we propose MEG-XL, model pre-trained with 2.5 minutes of MEG context per sample, 5-300 longer than prior work, and equivalent to 191k tokens, capturing extended neural context. Finetuning on the task of word decoding from brain data, MEG-XL matches supervised performance with fraction of the data (e.g. 1hr vs 50hrs) and outperforms brain foundation models. We find that models pre-trained with longer contexts learn representations that transfer better to word decoding. Our results indicate that long-context pre-training helps exploit extended neural context that other methods unnecessarily discard. Code, model weights, and instructions are available at https://github.com/ neural-processing-lab/MEG-XL. 1. Introduction Across modalities in deep learning, extending context has unlocked capabilities that short contexts could not provide. For example, task performance has improved by pre-training with longer, un-fragmented documents in language models (Dai et al., 2019; Beltagy et al., 2020), and by including more task examples in context at inference time (Brown et al., 2020). In audio modelling, dilated convolutions have extended "
[04.02.2026 13:59] Response: ```python
["PNPL, University of Oxford"]
```
[04.02.2026 13:59] Deleting PDF ./assets/pdf/2602.02494.pdf.
[04.02.2026 13:59] Success.
[04.02.2026 13:59] Downloading and parsing paper https://huggingface.co/papers/2602.02220.
[04.02.2026 13:59] Downloading paper 2602.02220 from https://arxiv.org/pdf/2602.02220v1...
[04.02.2026 13:59] Extracting affiliations from text.
[04.02.2026 13:59] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"LangMap: Hierarchical Benchmark for Open-Vocabulary Goal Navigation Bo Miao1, Weijia Liu2, Jun Luo3, Lachlan Shinnick1, Jian Liu3, Thomas Hamilton-Smith1, Yuhe Yang4, Zijie Wu5, Vanja Videnovic6, Feras Dayoub1, Anton van den Hengel1 1AIML, Adelaide University 2East China Normal University 3NERC-RVC, Hunan University 4University Western Australia 5Singapore University of Technology and Design 6Breaker Industries Project page: bo-miao.github.io/LangMap 6 2 0 2 2 ] . [ 1 0 2 2 2 0 . 2 0 6 2 : r a "
[04.02.2026 13:59] Response: ```python
[
    "AIML, Adelaide University",
    "East China Normal University",
    "NERC-RVC, Hunan University",
    "University Western Australia",
    "Singapore University of Technology and Design",
    "Breaker Industries"
]
```
[04.02.2026 13:59] Deleting PDF ./assets/pdf/2602.02220.pdf.
[04.02.2026 13:59] Success.
[04.02.2026 13:59] Downloading and parsing paper https://huggingface.co/papers/2602.01212.
[04.02.2026 13:59] Extra JSON file exists (./assets/json/2602.01212.json), skip PDF parsing.
[04.02.2026 13:59] Paper image links file exists (./assets/img_data/2602.01212.json), skip HTML parsing.
[04.02.2026 13:59] Success.
[04.02.2026 13:59] Downloading and parsing paper https://huggingface.co/papers/2602.00682.
[04.02.2026 13:59] Extra JSON file exists (./assets/json/2602.00682.json), skip PDF parsing.
[04.02.2026 13:59] Paper image links file exists (./assets/img_data/2602.00682.json), skip HTML parsing.
[04.02.2026 13:59] Success.
[04.02.2026 13:59] Downloading and parsing paper https://huggingface.co/papers/2602.03817.
[04.02.2026 13:59] Extra JSON file exists (./assets/json/2602.03817.json), skip PDF parsing.
[04.02.2026 13:59] Paper image links file exists (./assets/img_data/2602.03817.json), skip HTML parsing.
[04.02.2026 13:59] Success.
[04.02.2026 13:59] Downloading and parsing paper https://huggingface.co/papers/2602.03320.
[04.02.2026 13:59] Downloading paper 2602.03320 from https://arxiv.org/pdf/2602.03320v1...
[04.02.2026 14:00] Extracting affiliations from text.
[04.02.2026 14:00] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"MedSAM-Agent: Empowering Interactive Medical Image Segmentation with Multi-turn Agentic Reinforcement Learning Shengyuan Liu 1 Liuxin Bao 1 Qi Yang 2 3 Wanting Geng 2 4 Boyun Zheng 1 Chenxin Li 1 Wenting Chen 5 Houwen Peng 2 Yixuan Yuan 1 6 2 0 2 3 ] . [ 1 0 2 3 3 0 . 2 0 6 2 : r a "
[04.02.2026 14:00] Response: ```python
[]
```
[04.02.2026 14:00] Extracting affiliations from text.
[04.02.2026 14:00] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"MedSAM-Agent: Empowering Interactive Medical Image Segmentation with Multi-turn Agentic Reinforcement Learning Shengyuan Liu 1 Liuxin Bao 1 Qi Yang 2 3 Wanting Geng 2 4 Boyun Zheng 1 Chenxin Li 1 Wenting Chen 5 Houwen Peng 2 Yixuan Yuan 1 6 2 0 2 3 ] . [ 1 0 2 3 3 0 . 2 0 6 2 : r aMedical image segmentation is evolving from task-specific models toward generalizable frameworks. Recent research leverages Multi-modal Large Language Models (MLLMs) as autonomous agents, employing reinforcement learning with verifiable reward (RLVR) to orchestrate specialized tools like the Segment Anything Model (SAM). However, these approaches often rely on single-turn, rigid interaction strategies and lack process-level supervision during training, which hinders their ability to fully exploit the dynamic potential of interactive tools and leads to redundant actions. To bridge this gap, we propose MedSAM-Agent, framework that reformulates interactive segmentation as multi-step autonomous decision-making process. First, we introduce hybrid prompting strategy for expertcurated trajectory generation, enabling the model to internalize human-like decision heuristics and adaptive refinement strategies. Furthermore, we develop two-stage training pipeline that integrates multi-turn, end-to-end outcome verification with clinical-fidelity process reward design to promote interaction parsimony and decision efficiency. Extensive experiments across 6 medical modalities and 21 datasets demonstrate that MedSAM-Agent achieves state-of-the-art performance, effectively unifying autonomous medical reasoning with robust, iterative optimization. Code is available here. 1Chinese University of Hong Kong, Hong Kong SAR, China 2Hunyuan Group, Tencent 3Institute of Automation, the Chinese Academy of Sciences, Beijing, China 4Dalian University of Technology, Dalian, China 5Stanford University, Stanford, USA. Correspondence to: Yixuan Yuan <yxyuan@ee.cuhk.edu.hk>, Houwen Peng <henryllpeng@tencent.com>. Figure 1. Comparison of medical image segmentation paradigms. (a) SAM-based models (e.g., SAM, MedSAM) require continuous manual prompting via points or bounding boxes. (b) MLLMdriven models (e.g., LISA, UniBioMed) employs MLLM with specialized seg decoders and <seg> tokens. (c) Ours MedSAMAgent functions as an autonomous visual agent that performs multiturn refinement through iterative feedback and tool interaction, emulating the professional decision-making process. 1. Introduction Medical image segmentation stands as foundational task in clinical computer vision, underpinning critical applications such as early disease diagnosis, surgical planning, and treatment response assessment. Traditional AI-driven segmentation approaches, including UNet (Ronneberger et al., 2015) and its variants (Chen et al., 2021; Isensee et al., 2021; Li et al., 2025a; Zhou et al., 2018), have demonstrated impressive performance in medical segmentation scenarios. However, these methods are primarily tailored to specific tasks or imaging modalities, posing significant challenges when generalizing to new tasks that were not encountered during model training. The emergence of the Segment Anything Model (SAM) (Kirillov et al., 2023; Ravi et al., 2024) represented pivotal breakthrough, enabling high-quality segmentation through interactive human prompts (e.g., points, bounding boxes, or masks) (Ma et al., 2024; Wei et al., 2024b; Konwer et al., 2025; Ma et al., 2025b; Wu et al., 2025a; Gong et al., 2024). Nevertheless, MedSAM-Agent: Empowering Interactive Medical Image Segmentation with Multi-turn Agentic Reinforcement Learning these SAM-derived interactive models remain inherently dependent on expert prompting (Fig. 1a), which prevents them from achieving autonomous, generalized segmentation without manual intervention. Meanwhile, Multi-modal Large Language Models (MLLMs) (Sellergren et al., 2025; Xu et al., 2025b; Jiang et al., 2025a; Chen et al., 2024a) have shown remarkable perception and reasoning abilities in the medical domain, especially in visual question answering (Hu et al., 2024; Ye et al., 2024; Liu et al., 2021; Lau et al., 2018; He et al., 2020; Liu et al., 2025a) and report generation (Li et al., 2025b; Zambrano Chaves et al., 2025; Ma et al., 2025a; Chen et al., 2024b; Ru et al., 2026) tasks. While early attempts (Wu et al., 2025b; Huang et al., 2025b; Wang et al., 2025a) integrate MLLMs for segmentation via implicit tokens and additional pixel decoders (Fig. 1b), these methods often compromise semantic generalization by altering the original output space and shifting away from language-based outputs. Recent advancements (Li et al., 2024; Nath et al., 2025; Fathi et al., 2025) have shifted toward employing MLLMs as agents for invoking SAM tools, utilizing Reinforcement Learning from Verifiable Rewards (RLVR) during the post-training stage to integrate the high-level reasoning of MLLMs with the robust interactive segmentation of SAM by internalizing the autonomous tool-using. However, existing methods still suffer from two major limitations: First, interaction strategies remain simplistic and inefficient. Current studies either rely on single-turn prompts (You & Wu, 2025; Liu et al., 2025b; Huang et al., 2025a; Li et al., 2024), treating SAM as static segmentor rather than an iterative agent, or adhere to rigid, point-only trajectories (Zhu et al., 2025; Jiang et al., 2026). These pointcentric paradigms lack the spatial flexibility to adaptively encompass morphological heterogeneity. In voluminous or ambiguously bounded cases, such methods fail to leverage bounding boxes as human-like anchor for global context, leading to suboptimal step-wise refinement and underutilized iterative potential. Second, the reinforcement learning process lacks process-level supervision. Existing RLVR frameworks (You & Wu, 2025; Huang et al., 2025a; Liu et al., 2025b; Jiang et al., 2025b; Liu et al., 2025c; Su et al., 2025a) focus predominantly on terminal outcomes, such as the final segmentation accuracy (e.g., Dice or IoU), while neglecting the efficiency and logical coherence of intermediate actions. Without specific rewards to incentivize action parsimony and per-step effectiveness, the agent may take unnecessary steps that do not contribute to mask refinement, which ultimately compromises both training convergence efficiency and final inference performance. To address these limitations, we propose MedSAM-Agent, framework that enables MLLMs to emulate hum"
[04.02.2026 14:00] Mistral response. {"id": "da767e4bf4134e599ab0c63d3a3c022c", "created": 1770213601, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1653, "total_tokens": 1724, "completion_tokens": 71, "num_cached_tokens": 1652}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\n    \"Chinese University of Hong Kong, Hong Kong SAR, China\",\n    \"Hunyuan Group, Tencent\",\n    \"Institute of Automation, the Chinese Academy of Sciences, Beijing, China\",\n    \"Dalian University of Technology, Dalian, China\",\n    \"Stanford University, Stanford, USA\"\n]\n```"}}]}
[04.02.2026 14:00] Response: ```python
[
    "Chinese University of Hong Kong, Hong Kong SAR, China",
    "Hunyuan Group, Tencent",
    "Institute of Automation, the Chinese Academy of Sciences, Beijing, China",
    "Dalian University of Technology, Dalian, China",
    "Stanford University, Stanford, USA"
]
```
[04.02.2026 14:00] Deleting PDF ./assets/pdf/2602.03320.pdf.
[04.02.2026 14:00] Success.
[04.02.2026 14:00] Downloading and parsing paper https://huggingface.co/papers/2602.01519.
[04.02.2026 14:00] Downloading paper 2602.01519 from https://arxiv.org/pdf/2602.01519v1...
[04.02.2026 14:00] Extracting affiliations from text.
[04.02.2026 14:00] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"You Need an Encoder for Native Position-Independent Caching Shiju Zhao, Junhao Hu, Jiaqi Zheng, Guihai Chen State Key Laboratory for Novel Software Technology, Nanjing University, China School of Computer Science, Peking University, China 6 2 0 2 2 ] . [ 1 9 1 5 1 0 . 2 0 6 2 : r a "
[04.02.2026 14:00] Response: ```python
[
    "State Key Laboratory for Novel Software Technology, Nanjing University, China",
    "School of Computer Science, Peking University, China"
]
```
[04.02.2026 14:00] Deleting PDF ./assets/pdf/2602.01519.pdf.
[04.02.2026 14:00] Success.
[04.02.2026 14:00] Enriching papers with extra data.
[04.02.2026 14:00] ********************************************************************************
[04.02.2026 14:00] Abstract 0. Multimodal large language models can effectively understand source code when represented as compressed images, achieving significant token reduction while maintaining or improving performance on code comprehension tasks.  					AI-generated summary 				 Large Language Models (LLMs) have achieved rema...
[04.02.2026 14:00] ********************************************************************************
[04.02.2026 14:00] Abstract 1. Research investigates latent planning dynamics in large language models through a probing method called Tele-Lens, revealing limited global planning and enabling improved uncertainty estimation and CoT bypass recognition.  					AI-generated summary 				 This work stems from prior complementary obser...
[04.02.2026 14:00] ********************************************************************************
[04.02.2026 14:00] Abstract 2. AOrchestra is a framework-agnostic agentic system that uses a tuple-based abstraction to dynamically create specialized task executors, achieving improved performance on complex benchmarks through automated agent creation and resource management.  					AI-generated summary 				 Language agents have ...
[04.02.2026 14:00] ********************************************************************************
[04.02.2026 14:00] Abstract 3. Current world models lack unified frameworks despite task-specific advances, necessitating a comprehensive approach integrating interaction, perception, symbolic reasoning, and spatial representation.  					AI-generated summary 				 World models have emerged as a critical frontier in AI research, ai...
[04.02.2026 14:00] ********************************************************************************
[04.02.2026 14:00] Abstract 4. Large language models face challenges in long-horizon agentic workflows due to lack of authentic long-dependency training data, which is addressed by leveraging pull request sequences for structured supervision through progressive decomposition, consistency enforcement, and refinement from bug-fix h...
[04.02.2026 14:00] ********************************************************************************
[04.02.2026 14:00] Abstract 5. 3DiMo enables view-agnostic human motion control in video generation by training a motion encoder alongside a pretrained video generator to distill driving frames into compact motion tokens that align with the generator's spatial priors.  					AI-generated summary 				 Existing methods for human mot...
[04.02.2026 14:00] ********************************************************************************
[04.02.2026 14:00] Abstract 6. MARS is a modular AI research automation framework that uses budget-aware planning, modular construction, and reflective memory to achieve state-of-the-art performance in autonomous machine learning research.  					AI-generated summary 				 Automating AI research differs from general software engine...
[04.02.2026 14:00] ********************************************************************************
[04.02.2026 14:00] Abstract 7. CoBA-RL adapts rollout budget allocation for LLM training by evaluating sample training value and optimizing resource distribution through a capability-oriented value function and greedy strategy.  					AI-generated summary 				 Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a ...
[04.02.2026 14:00] ********************************************************************************
[04.02.2026 14:00] Abstract 8. A novel distillation framework called DP-DMD is introduced that preserves sample diversity in text-to-image generation by separating the roles of distilled steps, using v-prediction for diversity and standard DMD loss for quality refinement without additional computational overhead.  					AI-generat...
[04.02.2026 14:00] ********************************************************************************
[04.02.2026 14:00] Abstract 9. A Docker-free framework replaces physical execution environments with learned surrogates for training software engineering agents, enabling efficient training and test-time scaling without costly container setup.  					AI-generated summary 				 Recent advances in large language models (LLMs) have en...
[04.02.2026 14:00] ********************************************************************************
[04.02.2026 14:00] Abstract 10. SWE-Master presents a reproducible framework for developing software engineering agents through systematic optimization across multiple stages of agent development, achieving superior performance on software task resolution benchmarks.  					AI-generated summary 				 In this technical report, we pre...
[04.02.2026 14:00] ********************************************************************************
[04.02.2026 14:00] Abstract 11. DeepResearch report generation is improved through human-preference-aligned rubric generators trained via reinforcement learning with hybrid rewards and enhanced with multi-agent Markov-state workflows.  					AI-generated summary 				 Nowadays, training and evaluating DeepResearch-generated reports ...
[04.02.2026 14:00] ********************************************************************************
[04.02.2026 14:00] Abstract 12. Parallel-Probe is a training-free controller that optimizes parallel thinking by using consensus-based early stopping and deviation-based branch pruning to reduce computational costs while maintaining accuracy.  					AI-generated summary 				 Parallel thinking has emerged as a promising paradigm for...
[04.02.2026 14:00] ********************************************************************************
[04.02.2026 14:00] Abstract 13. UnifiedReward-Flex combines reward modeling with flexible, context-adaptive reasoning to improve visual generation by dynamically constructing hierarchical assessments based on semantic intent and visual evidence.  					AI-generated summary 				 Recent advancements in multimodal reward models (RMs) ...
[04.02.2026 14:00] ********************************************************************************
[04.02.2026 14:00] Abstract 14. Wide Research advances search intelligence through a dedicated benchmark and multi-agent architecture that enables parallel information retrieval under complex constraints.  					AI-generated summary 				 Search intelligence is evolving from Deep Research to Wide Research, a paradigm essential for r...
[04.02.2026 14:00] ********************************************************************************
[04.02.2026 14:00] Abstract 15. LENS framework improves reinforcement learning with verifiable rewards by identifying and removing interference tokens to enhance exploration efficiency and training stability.  					AI-generated summary 				 Reinforcement Learning with Verifiable Rewards (RLVR) has advanced LLM reasoning, but remai...
[04.02.2026 14:00] ********************************************************************************
[04.02.2026 14:00] Abstract 16. A unified agent system called FullStack-Agent is introduced to assist non-expert users in developing complex interactive websites by addressing full-stack development challenges through enhanced planning, code editing, and self-improving capabilities.  					AI-generated summary 				 Assisting non-ex...
[04.02.2026 14:00] ********************************************************************************
[04.02.2026 14:00] Abstract 17. Token Sparse Attention enables efficient long-context inference by dynamically compressing and decompressing attention tensors at the token level, achieving significant speedup with minimal accuracy loss.  					AI-generated summary 				 The quadratic complexity of attention remains the central bottl...
[04.02.2026 14:00] ********************************************************************************
[04.02.2026 14:00] Abstract 18. XDLM unifies Masked Diffusion Language Models and Uniform-noise Diffusion Language Models through a stationary noise kernel, achieving improved performance in both semantic understanding and generation quality.  					AI-generated summary 				 In discrete generative modeling, two dominant paradigms d...
[04.02.2026 14:00] ********************************************************************************
[04.02.2026 14:00] Abstract 19. AdaptMMBench presents a comprehensive benchmark for evaluating adaptive multimodal reasoning in Vision-Language Models, measuring reasoning mode selection rationality through dynamic difficulty assessment and multi-dimensional process evaluation.  					AI-generated summary 				 Adaptive multimodal r...
[04.02.2026 14:00] ********************************************************************************
[04.02.2026 14:00] Abstract 20. DeMix is a framework that uses model merging to predict optimal data ratios for LLM pre-training, decoupling search from training costs to improve mixture discovery efficiency.  					AI-generated summary 				 Determining an effective data mixture is a key factor in Large Language Model (LLM) pre-tra...
[04.02.2026 14:00] ********************************************************************************
[04.02.2026 14:00] Abstract 21. Multi-hop question answering dataset ID-MoCQA assesses cultural understanding in large language models through Indonesian traditions with diverse reasoning chains.  					AI-generated summary 				 Understanding culture requires reasoning across context, tradition, and implicit social knowledge, far b...
[04.02.2026 14:00] ********************************************************************************
[04.02.2026 14:00] Abstract 22. LRAgent is a KV cache sharing framework for multi-LoRA agents that decomposes cache into shared and adapter-dependent components, reducing memory and compute overhead while maintaining accuracy.  					AI-generated summary 				 Role specialization in multi-LLM agent systems is often realized via mult...
[04.02.2026 14:00] ********************************************************************************
[04.02.2026 14:00] Abstract 23. WorldVQA is a benchmark for evaluating the visual world knowledge of multimodal large language models by separating visual knowledge retrieval from reasoning to measure memorized facts.  					AI-generated summary 				 We introduce WorldVQA, a benchmark designed to evaluate the atomic visual world kn...
[04.02.2026 14:00] ********************************************************************************
[04.02.2026 14:00] Abstract 24. Research reveals that instruction tokens act as structural anchors in multimodal large language models, with shallow layers performing non-selective information transfer and deep layers resolving modality competition guided by instruction intent.  					AI-generated summary 				 Modality following se...
[04.02.2026 14:00] ********************************************************************************
[04.02.2026 14:00] Abstract 25. Search-R2 framework improves language agent reasoning through Actor-Refiner collaboration with targeted interventions and fine-grained reward supervision for better credit assignment in reinforcement learning.  					AI-generated summary 				 Search-integrated reasoning enables language agents to tra...
[04.02.2026 14:00] ********************************************************************************
[04.02.2026 14:00] Abstract 26. A reinforcement learning framework with glance and focus models improves pan-cancer screening in CT scans by addressing foreground-background imbalance and reducing false positives through group relative learning.  					AI-generated summary 				 Pan-cancer screening in large-scale CT scans remains c...
[04.02.2026 14:00] ********************************************************************************
[04.02.2026 14:00] Abstract 27. Offline reinforcement learning method combines contextual bandit learning with partial trajectories to improve multi-turn code generation performance while reducing training costs.  					AI-generated summary 				 Recently, there have been significant research interests in training large language mod...
[04.02.2026 14:00] ********************************************************************************
[04.02.2026 14:00] Abstract 28. SafeGround is a uncertainty-aware framework for GUI grounding models that uses distribution-aware uncertainty quantification and calibration to enable risk-aware predictions with controlled false discovery rates.  					AI-generated summary 				 Graphical User Interface (GUI) grounding aims to transl...
[04.02.2026 14:00] ********************************************************************************
[04.02.2026 14:00] Abstract 29. ObjEmbed is a novel multimodal language-model embedding approach that decomposes images into regional embeddings for improved object-level visual understanding and retrieval tasks.  					AI-generated summary 				 Aligning objects with corresponding textual descriptions is a fundamental challenge and...
[04.02.2026 14:00] ********************************************************************************
[04.02.2026 14:00] Abstract 30. Advanced AI models demonstrate capability in supporting expert-level mathematical discovery and scientific research through collaborative approaches involving proof verification and automated code execution.  					AI-generated summary 				 Recent advances in large language models (LLMs) have opened ...
[04.02.2026 14:00] ********************************************************************************
[04.02.2026 14:00] Abstract 31. CoViP addresses contextualized visual personalization by treating personalized image captioning as a core task and improving capabilities through reinforcement-learning-based post-training and caption-augmented generation.  					AI-generated summary 				 Despite recent progress in vision-language mo...
[04.02.2026 14:00] ********************************************************************************
[04.02.2026 14:00] Abstract 32. Stage-aware pruning method for large language and vision-language models that improves efficiency by selectively removing layers during different processing phases while maintaining accuracy.  					AI-generated summary 				 Large Language Models (LLMs) and Vision-Language Models (VLMs) have demonstr...
[04.02.2026 14:00] ********************************************************************************
[04.02.2026 14:00] Abstract 33. FaceLinkGen attack demonstrates that current privacy-preserving face recognition methods fail to protect identity information despite pixel-level distortion metrics suggesting adequate protection.  					AI-generated summary 				 Transformation-based privacy-preserving face recognition (PPFR) aims to...
[04.02.2026 14:00] ********************************************************************************
[04.02.2026 14:00] Abstract 34. Large Language Models have advanced general-purpose agents, but current evaluation benchmarks suffer from confounding factors and lack of standardization, necessitating a unified framework for rigorous assessment.  					AI-generated summary 				 With the advent of Large Language Models (LLMs), gener...
[04.02.2026 14:00] ********************************************************************************
[04.02.2026 14:00] Abstract 35. MEG-XL demonstrates improved brain-to-text decoding performance through extended pre-training with 2.5-minute MEG context, significantly outperforming previous models with less contextual data.  					AI-generated summary 				 Clinical brain-to-text interfaces are designed for paralysed patients who ...
[04.02.2026 14:00] ********************************************************************************
[04.02.2026 14:00] Abstract 36. HieraNav presents a multi-granularity, open-vocabulary navigation task with LangMap benchmark that enables agents to follow natural language instructions across different semantic levels in 3D environments.  					AI-generated summary 				 The relationships between objects and language are fundamenta...
[04.02.2026 14:00] ********************************************************************************
[04.02.2026 14:00] Abstract 37. SimpleNorm normalization strategy stabilizes activation scales and enables larger stable learning rates in Transformer models by reducing Hessian spectral norm, leading to improved training performance.  					AI-generated summary 				 In this work, we revisit Transformer optimization through the len...
[04.02.2026 14:00] ********************************************************************************
[04.02.2026 14:00] Abstract 38. A novel dual semantic alignment framework for LLM-enhanced multimodal recommendation that addresses representational divergence between large models and recommendation systems through graph attention networks and cross-modal contrastive learning.  					AI-generated summary 				 Multimodal recommenda...
[04.02.2026 14:00] ********************************************************************************
[04.02.2026 14:00] Abstract 39. A fusion framework called FINCH combines audio and spatiotemporal predictors for bioacoustic classification by adaptively weighting evidence based on reliability estimates, outperforming fixed-weight methods and audio-only approaches.  					AI-generated summary 				 Many machine learning systems hav...
[04.02.2026 14:00] ********************************************************************************
[04.02.2026 14:00] Abstract 40. MedSAM-Agent reformulates medical image segmentation as a multi-step decision-making process using hybrid prompting and a two-stage training pipeline with process rewards to improve autonomous reasoning and optimization.  					AI-generated summary 				 Medical image segmentation is evolving from tas...
[04.02.2026 14:00] ********************************************************************************
[04.02.2026 14:00] Abstract 41. Native position-independent caching enhances LLM inference efficiency by reintroducing encoders and developing a caching system that reduces latency while maintaining accuracy.  					AI-generated summary 				 The Key-Value (KV) cache of Large Language Models (LLMs) is prefix-based, making it highly ...
[04.02.2026 14:00] Read previous papers.
[04.02.2026 14:00] Generating reviews via LLM API.
[04.02.2026 14:00] Using data from previous issue: {"categories": ["#plp", "#multimodal", "#inference"], "emoji": "", "ru": {"title": "  :   ", "desc": "        (MLLM)    ,       
[04.02.2026 14:00] Using data from previous issue: {"categories": ["#reasoning", "#training", "#open_source", "#interpretability", "#benchmark"], "emoji": "", "ru": {"title": "   LLM:    ", "desc": "        ,   
[04.02.2026 14:00] Using data from previous issue: {"categories": [], "emoji": "", "ru": {"title": "     ", "desc": "AOrchestra      ,       (, , , )    
[04.02.2026 14:00] Using data from previous issue: {"categories": [], "emoji": "", "ru": {"title": "        ", "desc": "         ,       
[04.02.2026 14:00] Using data from previous issue: {"categories": ["#training", "#synthetic", "#alignment", "#data", "#long_context", "#reasoning", "#agents"], "emoji": "", "ru": {"title": "   :      ", "desc": "         
[04.02.2026 14:00] Using data from previous issue: {"categories": ["#video", "#training", "#architecture", "#multimodal", "#optimization", "#3d"], "emoji": "", "ru": {"title": " 3D         ", "desc": "3DiMo          ,  
[04.02.2026 14:00] Using data from previous issue: {"categories": ["#training", "#agents", "#open_source", "#science", "#benchmark"], "emoji": "", "ru": {"title": "        ML-", "desc": "MARS           
[04.02.2026 14:00] Using data from previous issue: {"categories": ["#reasoning", "#rl", "#optimization", "#training"], "emoji": "", "ru": {"title": "        ", "desc": "CoBA-RL      ,     
[04.02.2026 14:00] Using data from previous issue: {"categories": ["#multimodal", "#training", "#inference", "#diffusion", "#optimization"], "emoji": "", "ru": {"title": "       ", "desc": "      DP-DMD     , 
[04.02.2026 14:00] Using data from previous issue: {"categories": ["#training", "#benchmark", "#rl", "#plp", "#agents"], "emoji": "", "ru": {"title": "   Docker     ", "desc": "   SWE-World    Docker,       
[04.02.2026 14:00] Using data from previous issue: {"categories": ["#open_source", "#training", "#benchmark", "#long_context", "#rl", "#plp", "#reasoning", "#agents", "#optimization"], "emoji": "", "ru": {"title": "       ", "desc": "SWE-Master    
[04.02.2026 14:00] Using data from previous issue: {"categories": ["#rlhf", "#benchmark", "#dataset", "#agents"], "emoji": "", "ru": {"title": "         ", "desc": "       ,     
[04.02.2026 14:00] Using data from previous issue: {"categories": ["#reasoning", "#optimization"], "emoji": "", "ru": {"title": "       ", "desc": "   Parallel-Probe            .   2D-
[04.02.2026 14:00] Using data from previous issue: {"categories": ["#video", "#reasoning", "#multimodal", "#alignment", "#cv", "#rlhf"], "emoji": "", "ru": {"title": "         ", "desc": "   UnifiedReward-Flex    
[04.02.2026 14:00] Using data from previous issue: {"categories": ["#training", "#agents", "#rl", "#benchmark", "#dataset"], "emoji": "", "ru": {"title": " :      ", "desc": "  Wide Research     ,    
[04.02.2026 14:00] Using data from previous issue: {"categories": ["#reasoning", "#rl", "#optimization", "#training"], "emoji": "", "ru": {"title": "        ", "desc": "   LENS -        ,    
[04.02.2026 14:00] Using data from previous issue: {"categories": ["#benchmark", "#dataset", "#agents", "#training", "#plp"], "emoji": "", "ru": {"title": "     -", "desc": " FullStack-Agent -   ,    
[04.02.2026 14:00] Using data from previous issue: {"categories": ["#inference", "#long_context", "#architecture", "#optimization"], "emoji": "", "ru": {"title": "     inference   ", "desc": "  Token Sparse Attention       
[04.02.2026 14:00] Using data from previous issue: {"categories": ["#open_source", "#diffusion", "#optimization"], "emoji": "", "ru": {"title": " :       ", "desc": "   XDLM   ,       
[04.02.2026 14:00] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#cv"], "emoji": "", "ru": {"title": " :        ", "desc": "AdaptMMBench           Vision-Language Mod
[04.02.2026 14:00] Using data from previous issue: {"categories": ["#training", "#optimization", "#data", "#open_source", "#dataset"], "emoji": "", "ru": {"title": "        ", "desc": "DeMix           LLM,  
[04.02.2026 14:00] Using data from previous issue: {"categories": ["#reasoning", "#dataset", "#benchmark", "#low_resource", "#open_source", "#multilingual"], "emoji": "", "ru": {"title": "        ", "desc": "   ID-MoCQA      
[04.02.2026 14:00] Using data from previous issue: {"categories": ["#agents", "#inference", "#architecture"], "emoji": "", "ru": {"title": "        LoRA ", "desc": "LRAgent       KV         LoRA 
[04.02.2026 14:00] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#cv"], "emoji": "", "ru": {"title": " ,       ", "desc": "  WorldVQA          .   
[04.02.2026 14:00] Using data from previous issue: {"categories": ["#multimodal", "#interpretability", "#architecture"], "emoji": "", "ru": {"title": "  :      ", "desc": "           (MLLM)   
[04.02.2026 14:00] Using data from previous issue: {"categories": ["#reasoning", "#training", "#optimization", "#agents", "#rl", "#rag"], "emoji": "", "ru": {"title": "         ", "desc": "    Search-R2,     
[04.02.2026 14:00] Using data from previous issue: {"categories": ["#rl", "#cv", "#healthcare"], "emoji": "", "ru": {"title": "         ", "desc": "  GF-Screen          -,    
[04.02.2026 14:00] Using data from previous issue: {"categories": ["#training", "#rl", "#plp"], "emoji": "", "ru": {"title": "  LLM      ", "desc": "    Cobalt,            
[04.02.2026 14:00] Using data from previous issue: {"categories": ["#benchmark", "#inference", "#agents", "#security"], "emoji": "", "ru": {"title": "     ", "desc": "SafeGround         ,    
[04.02.2026 14:00] Using data from previous issue: {"categories": ["#interpretability", "#multimodal", "#benchmark", "#cv"], "emoji": "", "ru": {"title": "      ", "desc": "ObjEmbed           ,   
[04.02.2026 14:00] Using data from previous issue: {"categories": ["#reasoning", "#science", "#transfer_learning"], "emoji": "", "ru": {"title": "AI   :       ", "desc": "          ,  
[04.02.2026 14:00] Using data from previous issue: {"categories": ["#rl", "#multimodal", "#benchmark", "#cv"], "emoji": "", "ru": {"title": "      vision-language", "desc": "    CoViP      -.   
[04.02.2026 14:00] Querying the API.
[04.02.2026 14:00] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Stage-aware pruning method for large language and vision-language models that improves efficiency by selectively removing layers during different processing phases while maintaining accuracy.  					AI-generated summary 				 Large Language Models (LLMs) and Vision-Language Models (VLMs) have demonstrated remarkable capabilities. However, their deployment is hindered by significant computational costs. Existing structured pruning methods, while hardware-efficient, often suffer from significant accuracy degradation. In this paper, we argue that this failure stems from a stage-agnostic pruning approach that overlooks the asymmetric roles between the prefill and decode stages. By introducing a virtual gate mechanism, our importance analysis reveals that deep layers are critical for next-token prediction (decode) but largely redundant for context encoding (prefill). Leveraging this insight, we propose Prefill-Only Pruning (POP), a stage-aware inference strategy that safely omits deep layers during the computationally intensive prefill stage while retaining the full model for the sensitive decode stage. To enable the transition between stages, we introduce independent Key-Value (KV) projections to maintain cache integrity, and a boundary handling strategy to ensure the accuracy of the first generated token. Extensive experiments on Llama-3.1, Qwen3-VL, and Gemma-3 across diverse modalities demonstrate that POP achieves up to 1.37times speedup in prefill latency with minimal performance loss, effectively overcoming the accuracy-efficiency trade-off limitations of existing structured pruning methods.
[04.02.2026 14:00] Response: ```json
{
  "desc": "            ,         .  ,        ,     .    Prefill-Only Pruning,           ,       .    Llama-3.1  Qwen3-VL    1.37     .",
  "emoji": "",
  "title": "       "
}
```
[04.02.2026 14:00] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Stage-aware pruning method for large language and vision-language models that improves efficiency by selectively removing layers during different processing phases while maintaining accuracy.  					AI-generated summary 				 Large Language Models (LLMs) and Vision-Language Models (VLMs) have demonstrated remarkable capabilities. However, their deployment is hindered by significant computational costs. Existing structured pruning methods, while hardware-efficient, often suffer from significant accuracy degradation. In this paper, we argue that this failure stems from a stage-agnostic pruning approach that overlooks the asymmetric roles between the prefill and decode stages. By introducing a virtual gate mechanism, our importance analysis reveals that deep layers are critical for next-token prediction (decode) but largely redundant for context encoding (prefill). Leveraging this insight, we propose Prefill-Only Pruning (POP), a stage-aware inference strategy that safely omits deep layers during the computationally intensive prefill stage while retaining the full model for the sensitive decode stage. To enable the transition between stages, we introduce independent Key-Value (KV) projections to maintain cache integrity, and a boundary handling strategy to ensure the accuracy of the first generated token. Extensive experiments on Llama-3.1, Qwen3-VL, and Gemma-3 across diverse modalities demonstrate that POP achieves up to 1.37times speedup in prefill latency with minimal performance loss, effectively overcoming the accuracy-efficiency trade-off limitations of existing structured pruning methods."

[04.02.2026 14:00] Response: ```python
["INFERENCE", "MULTIMODAL", "ARCHITECTURE"]
```
[04.02.2026 14:00] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Stage-aware pruning method for large language and vision-language models that improves efficiency by selectively removing layers during different processing phases while maintaining accuracy.  					AI-generated summary 				 Large Language Models (LLMs) and Vision-Language Models (VLMs) have demonstrated remarkable capabilities. However, their deployment is hindered by significant computational costs. Existing structured pruning methods, while hardware-efficient, often suffer from significant accuracy degradation. In this paper, we argue that this failure stems from a stage-agnostic pruning approach that overlooks the asymmetric roles between the prefill and decode stages. By introducing a virtual gate mechanism, our importance analysis reveals that deep layers are critical for next-token prediction (decode) but largely redundant for context encoding (prefill). Leveraging this insight, we propose Prefill-Only Pruning (POP), a stage-aware inference strategy that safely omits deep layers during the computationally intensive prefill stage while retaining the full model for the sensitive decode stage. To enable the transition between stages, we introduce independent Key-Value (KV) projections to maintain cache integrity, and a boundary handling strategy to ensure the accuracy of the first generated token. Extensive experiments on Llama-3.1, Qwen3-VL, and Gemma-3 across diverse modalities demonstrate that POP achieves up to 1.37times speedup in prefill latency with minimal performance loss, effectively overcoming the accuracy-efficiency trade-off limitations of existing structured pruning methods."

[04.02.2026 14:00] Response: ```python
['OPTIMIZATION']
```
[04.02.2026 14:00] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents a novel stage-aware pruning method for large language models (LLMs) and vision-language models (VLMs) that enhances computational efficiency without sacrificing accuracy. The authors identify that traditional pruning techniques often degrade performance because they do not consider the different roles of model layers during the prefill and decode stages. By implementing a Prefill-Only Pruning (POP) strategy, they selectively remove deep layers during the prefill phase, which is less sensitive to accuracy, while keeping the full model intact for the decode phase, where precision is crucial. Their experiments show that this approach can significantly speed up prefill latency by up to 1.37 times, demonstrating a successful balance between efficiency and model performance.","title":"Efficient Layer Pruning for Enhanced Model Performance"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper presents a novel stage-aware pruning method for large language models (LLMs) and vision-language models (VLMs) that enhances computational efficiency without sacrificing accuracy. The authors identify that traditional pruning techniques often degrade performance because they do not consider the different roles of model layers during the prefill and decode stages. By implementing a Prefill-Only Pruning (POP) strategy, they selectively remove deep layers during the prefill phase, which is less sensitive to accuracy, while keeping the full model intact for the decode phase, where precision is crucial. Their experiments show that this approach can significantly speed up prefill latency by up to 1.37 times, demonstrating a successful balance between efficiency and model performance.', title='Efficient Layer Pruning for Enhanced Model Performance'))
[04.02.2026 14:00] Response: ParsedChatCompletionMessage[Article](content='{"desc":"1.37","title":""}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='1.37', title=''))
[04.02.2026 14:00] Using data from previous issue: {"categories": ["#ethics", "#benchmark", "#cv", "#security"], "emoji": "", "ru": {"title": "     ", "desc": "    FaceLinkGen,          -   
[04.02.2026 14:00] Using data from previous issue: {"categories": ["#agents", "#reasoning", "#benchmark"], "emoji": "", "ru": {"title": "        LLM", "desc": "            .  ,   
[04.02.2026 14:00] Querying the API.
[04.02.2026 14:00] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

MEG-XL demonstrates improved brain-to-text decoding performance through extended pre-training with 2.5-minute MEG context, significantly outperforming previous models with less contextual data.  					AI-generated summary 				 Clinical brain-to-text interfaces are designed for paralysed patients who cannot provide extensive training recordings. Pre-training improves data-efficient generalisation by learning statistical priors across subjects, but these priors critically depend on context. While natural speech might unfold gradually over minutes, most methods pre-train with only a few seconds of context. Thus, we propose MEG-XL, a model pre-trained with 2.5 minutes of MEG context per sample, 5-300x longer than prior work, and equivalent to 191k tokens, capturing extended neural context. Fine-tuning on the task of word decoding from brain data, MEG-XL matches supervised performance with a fraction of the data (e.g. 1hr vs 50hrs) and outperforms brain foundation models. We find that models pre-trained with longer contexts learn representations that transfer better to word decoding. Our results indicate that long-context pre-training helps exploit extended neural context that other methods unnecessarily discard. Code, model weights, and instructions are available at https://github.com/neural-processing-lab/MEG-XL .
[04.02.2026 14:00] Response: ```json
{
  "desc": "MEG-XL         ,       2.5   ,   5-300  ,    .                      .          MEG-XL   1     50 ,   .  ,  ,       ,          -.",
  "emoji": "",
  "title": "       "
}
```
[04.02.2026 14:00] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"MEG-XL demonstrates improved brain-to-text decoding performance through extended pre-training with 2.5-minute MEG context, significantly outperforming previous models with less contextual data.  					AI-generated summary 				 Clinical brain-to-text interfaces are designed for paralysed patients who cannot provide extensive training recordings. Pre-training improves data-efficient generalisation by learning statistical priors across subjects, but these priors critically depend on context. While natural speech might unfold gradually over minutes, most methods pre-train with only a few seconds of context. Thus, we propose MEG-XL, a model pre-trained with 2.5 minutes of MEG context per sample, 5-300x longer than prior work, and equivalent to 191k tokens, capturing extended neural context. Fine-tuning on the task of word decoding from brain data, MEG-XL matches supervised performance with a fraction of the data (e.g. 1hr vs 50hrs) and outperforms brain foundation models. We find that models pre-trained with longer contexts learn representations that transfer better to word decoding. Our results indicate that long-context pre-training helps exploit extended neural context that other methods unnecessarily discard. Code, model weights, and instructions are available at https://github.com/neural-processing-lab/MEG-XL ."

[04.02.2026 14:00] Response: ```python
["HEALTHCARE", "TRAINING", "MULTIMODAL"]
```

**Justification:**

- **HEALTHCARE**: The paper explicitly addresses clinical applications for paralyzed patients, positioning this as a medical/healthcare application of ML.
- **TRAINING**: The paper focuses on improving model training and fine-tuning methods, specifically through extended pre-training with longer context windows and demonstrating improved data-efficient generalization.
- **MULTIMODAL**: The work involves brain signals (MEG data) as input and text as output, combining multiple modalities (neural/biological signals with language).
[04.02.2026 14:00] Error. Failed to parse JSON from LLM. ["HEALTHCARE", "TRAINING", "MULTIMODAL"]


**Justification:**

- **HEALTHCARE**: The paper explicitly addresses clinical applications for paralyzed patients, positioning this as a medical/healthcare application of ML.
- **TRAINING**: The paper focuses on improving model training and fine-tuning methods, specifically through extended pre-training with longer context windows and demonstrating improved data-efficient generalization.
- **MULTIMODAL**: The work involves brain signals (MEG data) as input and text as output, combining multiple modalities (neural/biological signals with language).
[04.02.2026 14:00] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"MEG-XL demonstrates improved brain-to-text decoding performance through extended pre-training with 2.5-minute MEG context, significantly outperforming previous models with less contextual data.  					AI-generated summary 				 Clinical brain-to-text interfaces are designed for paralysed patients who cannot provide extensive training recordings. Pre-training improves data-efficient generalisation by learning statistical priors across subjects, but these priors critically depend on context. While natural speech might unfold gradually over minutes, most methods pre-train with only a few seconds of context. Thus, we propose MEG-XL, a model pre-trained with 2.5 minutes of MEG context per sample, 5-300x longer than prior work, and equivalent to 191k tokens, capturing extended neural context. Fine-tuning on the task of word decoding from brain data, MEG-XL matches supervised performance with a fraction of the data (e.g. 1hr vs 50hrs) and outperforms brain foundation models. We find that models pre-trained with longer contexts learn representations that transfer better to word decoding. Our results indicate that long-context pre-training helps exploit extended neural context that other methods unnecessarily discard. Code, model weights, and instructions are available at https://github.com/neural-processing-lab/MEG-XL ."

[04.02.2026 14:00] Response: ```python
['LONG_CONTEXT', 'TRANSFER_LEARNING', 'OPEN_SOURCE', 'SCIENCE']
```
[04.02.2026 14:00] Response: ParsedChatCompletionMessage[Article](content='{"desc":"MEG-XL is a machine learning model that enhances brain-to-text decoding by using a longer context of 2.5 minutes of MEG data for pre-training. This approach allows the model to learn better statistical patterns across different subjects, which is crucial for decoding natural speech that occurs over extended periods. By fine-tuning on word decoding tasks, MEG-XL achieves performance comparable to models trained on much larger datasets, demonstrating its efficiency. The findings suggest that utilizing longer contexts during pre-training significantly improves the model\'s ability to generalize and transfer knowledge to specific tasks.","title":"Unlocking Speech with Extended Neural Context"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="MEG-XL is a machine learning model that enhances brain-to-text decoding by using a longer context of 2.5 minutes of MEG data for pre-training. This approach allows the model to learn better statistical patterns across different subjects, which is crucial for decoding natural speech that occurs over extended periods. By fine-tuning on word decoding tasks, MEG-XL achieves performance comparable to models trained on much larger datasets, demonstrating its efficiency. The findings suggest that utilizing longer contexts during pre-training significantly improves the model's ability to generalize and transfer knowledge to specific tasks.", title='Unlocking Speech with Extended Neural Context'))
[04.02.2026 14:00] Response: ParsedChatCompletionMessage[Article](content='{"desc":"MEG-XL-2.5MEGMEG-XL150","title":"-"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='MEG-XL-2.5MEGMEG-XL150', title='-'))
[04.02.2026 14:00] Querying the API.
[04.02.2026 14:00] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

HieraNav presents a multi-granularity, open-vocabulary navigation task with LangMap benchmark that enables agents to follow natural language instructions across different semantic levels in 3D environments.  					AI-generated summary 				 The relationships between objects and language are fundamental to meaningful communication between humans and AI, and to practically useful embodied intelligence. We introduce HieraNav, a multi-granularity, open-vocabulary goal navigation task where agents interpret natural language instructions to reach targets at four semantic levels: scene, room, region, and instance. To this end, we present Language as a Map (LangMap), a large-scale benchmark built on real-world 3D indoor scans with comprehensive human-verified annotations and tasks spanning these levels. LangMap provides region labels, discriminative region descriptions, discriminative instance descriptions covering 414 object categories, and over 18K navigation tasks. Each target features both concise and detailed descriptions, enabling evaluation across different instruction styles. LangMap achieves superior annotation quality, outperforming GOAT-Bench by 23.8% in discriminative accuracy using four times fewer words. Comprehensive evaluations of zero-shot and supervised models on LangMap reveal that richer context and memory improve success, while long-tailed, small, context-dependent, and distant goals, as well as multi-goal completion, remain challenging. HieraNav and LangMap establish a rigorous testbed for advancing language-driven embodied navigation. Project: https://bo-miao.github.io/LangMap
[04.02.2026 14:00] Response: ```json
{
  "desc": "    HieraNav     3D-        : , ,    .     LangMap         ,   414     18K  .                   .  ,         ,      .",
  "emoji": "",
  "title": "        3D-"
}
```
[04.02.2026 14:00] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"HieraNav presents a multi-granularity, open-vocabulary navigation task with LangMap benchmark that enables agents to follow natural language instructions across different semantic levels in 3D environments.  					AI-generated summary 				 The relationships between objects and language are fundamental to meaningful communication between humans and AI, and to practically useful embodied intelligence. We introduce HieraNav, a multi-granularity, open-vocabulary goal navigation task where agents interpret natural language instructions to reach targets at four semantic levels: scene, room, region, and instance. To this end, we present Language as a Map (LangMap), a large-scale benchmark built on real-world 3D indoor scans with comprehensive human-verified annotations and tasks spanning these levels. LangMap provides region labels, discriminative region descriptions, discriminative instance descriptions covering 414 object categories, and over 18K navigation tasks. Each target features both concise and detailed descriptions, enabling evaluation across different instruction styles. LangMap achieves superior annotation quality, outperforming GOAT-Bench by 23.8% in discriminative accuracy using four times fewer words. Comprehensive evaluations of zero-shot and supervised models on LangMap reveal that richer context and memory improve success, while long-tailed, small, context-dependent, and distant goals, as well as multi-goal completion, remain challenging. HieraNav and LangMap establish a rigorous testbed for advancing language-driven embodied navigation. Project: https://bo-miao.github.io/LangMap"

[04.02.2026 14:00] Response: ```python
['AGENTS', 'BENCHMARK', 'DATASET', '3D', 'MULTIMODAL']
```
[04.02.2026 14:00] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"HieraNav presents a multi-granularity, open-vocabulary navigation task with LangMap benchmark that enables agents to follow natural language instructions across different semantic levels in 3D environments.  					AI-generated summary 				 The relationships between objects and language are fundamental to meaningful communication between humans and AI, and to practically useful embodied intelligence. We introduce HieraNav, a multi-granularity, open-vocabulary goal navigation task where agents interpret natural language instructions to reach targets at four semantic levels: scene, room, region, and instance. To this end, we present Language as a Map (LangMap), a large-scale benchmark built on real-world 3D indoor scans with comprehensive human-verified annotations and tasks spanning these levels. LangMap provides region labels, discriminative region descriptions, discriminative instance descriptions covering 414 object categories, and over 18K navigation tasks. Each target features both concise and detailed descriptions, enabling evaluation across different instruction styles. LangMap achieves superior annotation quality, outperforming GOAT-Bench by 23.8% in discriminative accuracy using four times fewer words. Comprehensive evaluations of zero-shot and supervised models on LangMap reveal that richer context and memory improve success, while long-tailed, small, context-dependent, and distant goals, as well as multi-goal completion, remain challenging. HieraNav and LangMap establish a rigorous testbed for advancing language-driven embodied navigation. Project: https://bo-miao.github.io/LangMap"

[04.02.2026 14:00] Response: ```python
['OPEN_SOURCE']
```
[04.02.2026 14:00] Response: ParsedChatCompletionMessage[Article](content='{"desc":"HieraNav introduces a new navigation task that allows AI agents to understand and follow natural language instructions at different levels of detail in 3D spaces. The LangMap benchmark provides a rich dataset with various semantic levels, including scene, room, region, and instance, enabling comprehensive evaluation of navigation tasks. It features high-quality annotations for over 414 object categories and includes more than 18,000 tasks, enhancing the agents\' ability to interpret instructions. The study shows that while context and memory improve navigation success, challenges remain with complex goals and multi-goal scenarios.","title":"Navigating Language: HieraNav and LangMap for AI Agents"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="HieraNav introduces a new navigation task that allows AI agents to understand and follow natural language instructions at different levels of detail in 3D spaces. The LangMap benchmark provides a rich dataset with various semantic levels, including scene, room, region, and instance, enabling comprehensive evaluation of navigation tasks. It features high-quality annotations for over 414 object categories and includes more than 18,000 tasks, enhancing the agents' ability to interpret instructions. The study shows that while context and memory improve navigation success, challenges remain with complex goals and multi-goal scenarios.", title='Navigating Language: HieraNav and LangMap for AI Agents'))
[04.02.2026 14:00] Response: ParsedChatCompletionMessage[Article](content='{"desc":"HieraNav3DLangMapLangMap3D","title":"HieraNav"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='HieraNav3DLangMapLangMap3D', title='HieraNav'))
[04.02.2026 14:00] Using data from previous issue: {"categories": ["#training", "#open_source", "#architecture", "#optimization", "#math"], "emoji": "", "ru": {"title": "   :        ", "desc": "  SimpleNorm    ,  
[04.02.2026 14:00] Using data from previous issue: {"categories": ["#benchmark", "#multimodal"], "emoji": "", "ru": {"title": "        ", "desc": "  RecGOAT            . 
[04.02.2026 14:00] Using data from previous issue: {"categories": ["#audio", "#multimodal", "#benchmark"], "emoji": "", "ru": {"title": "       ", "desc": "  FINCH         ,   
[04.02.2026 14:00] Querying the API.
[04.02.2026 14:00] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

MedSAM-Agent reformulates medical image segmentation as a multi-step decision-making process using hybrid prompting and a two-stage training pipeline with process rewards to improve autonomous reasoning and optimization.  					AI-generated summary 				 Medical image segmentation is evolving from task-specific models toward generalizable frameworks. Recent research leverages Multi-modal Large Language Models (MLLMs) as autonomous agents, employing reinforcement learning with verifiable reward (RLVR) to orchestrate specialized tools like the Segment Anything Model (SAM). However, these approaches often rely on single-turn, rigid interaction strategies and lack process-level supervision during training, which hinders their ability to fully exploit the dynamic potential of interactive tools and leads to redundant actions. To bridge this gap, we propose MedSAM-Agent, a framework that reformulates interactive segmentation as a multi-step autonomous decision-making process. First, we introduce a hybrid prompting strategy for expert-curated trajectory generation, enabling the model to internalize human-like decision heuristics and adaptive refinement strategies. Furthermore, we develop a two-stage training pipeline that integrates multi-turn, end-to-end outcome verification with a clinical-fidelity process reward design to promote interaction parsimony and decision efficiency. Extensive experiments across 6 medical modalities and 21 datasets demonstrate that MedSAM-Agent achieves state-of-the-art performance, effectively unifying autonomous medical reasoning with robust, iterative optimization. Code is available https://github.com/CUHK-AIM-Group/MedSAM-Agent{here}.
[04.02.2026 14:00] Response: ```json
{
  "desc": "MedSAM-Agent              .        ,         .                  .    21    6     state-of-the-art     .",
  "emoji": "",
  "title": "      "
}
```
[04.02.2026 14:00] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"MedSAM-Agent reformulates medical image segmentation as a multi-step decision-making process using hybrid prompting and a two-stage training pipeline with process rewards to improve autonomous reasoning and optimization.  					AI-generated summary 				 Medical image segmentation is evolving from task-specific models toward generalizable frameworks. Recent research leverages Multi-modal Large Language Models (MLLMs) as autonomous agents, employing reinforcement learning with verifiable reward (RLVR) to orchestrate specialized tools like the Segment Anything Model (SAM). However, these approaches often rely on single-turn, rigid interaction strategies and lack process-level supervision during training, which hinders their ability to fully exploit the dynamic potential of interactive tools and leads to redundant actions. To bridge this gap, we propose MedSAM-Agent, a framework that reformulates interactive segmentation as a multi-step autonomous decision-making process. First, we introduce a hybrid prompting strategy for expert-curated trajectory generation, enabling the model to internalize human-like decision heuristics and adaptive refinement strategies. Furthermore, we develop a two-stage training pipeline that integrates multi-turn, end-to-end outcome verification with a clinical-fidelity process reward design to promote interaction parsimony and decision efficiency. Extensive experiments across 6 medical modalities and 21 datasets demonstrate that MedSAM-Agent achieves state-of-the-art performance, effectively unifying autonomous medical reasoning with robust, iterative optimization. Code is available https://github.com/CUHK-AIM-Group/MedSAM-Agent{here}."

[04.02.2026 14:00] Response: ```python
["AGENTS", "HEALTHCARE", "RL", "RLHF", "CV", "TRAINING"]
```
[04.02.2026 14:00] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"MedSAM-Agent reformulates medical image segmentation as a multi-step decision-making process using hybrid prompting and a two-stage training pipeline with process rewards to improve autonomous reasoning and optimization.  					AI-generated summary 				 Medical image segmentation is evolving from task-specific models toward generalizable frameworks. Recent research leverages Multi-modal Large Language Models (MLLMs) as autonomous agents, employing reinforcement learning with verifiable reward (RLVR) to orchestrate specialized tools like the Segment Anything Model (SAM). However, these approaches often rely on single-turn, rigid interaction strategies and lack process-level supervision during training, which hinders their ability to fully exploit the dynamic potential of interactive tools and leads to redundant actions. To bridge this gap, we propose MedSAM-Agent, a framework that reformulates interactive segmentation as a multi-step autonomous decision-making process. First, we introduce a hybrid prompting strategy for expert-curated trajectory generation, enabling the model to internalize human-like decision heuristics and adaptive refinement strategies. Furthermore, we develop a two-stage training pipeline that integrates multi-turn, end-to-end outcome verification with a clinical-fidelity process reward design to promote interaction parsimony and decision efficiency. Extensive experiments across 6 medical modalities and 21 datasets demonstrate that MedSAM-Agent achieves state-of-the-art performance, effectively unifying autonomous medical reasoning with robust, iterative optimization. Code is available https://github.com/CUHK-AIM-Group/MedSAM-Agent{here}."

[04.02.2026 14:00] Response: ```python
['REASONING', 'OPTIMIZATION', 'OPEN_SOURCE', 'SCIENCE']
```
[04.02.2026 14:01] Response: ParsedChatCompletionMessage[Article](content='{"desc":"MedSAM-Agent transforms medical image segmentation into a multi-step decision-making process, enhancing the model\'s ability to reason and optimize autonomously. It utilizes a hybrid prompting strategy to generate expert-curated trajectories, allowing the model to learn human-like decision-making and refinement techniques. The framework features a two-stage training pipeline that combines multi-turn outcome verification with a clinical-fidelity process reward system, improving interaction efficiency and reducing redundant actions. Extensive testing across various medical modalities shows that MedSAM-Agent achieves leading performance, effectively integrating autonomous reasoning with iterative optimization.","title":"Revolutionizing Medical Image Segmentation with Autonomous Decision-Making"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="MedSAM-Agent transforms medical image segmentation into a multi-step decision-making process, enhancing the model's ability to reason and optimize autonomously. It utilizes a hybrid prompting strategy to generate expert-curated trajectories, allowing the model to learn human-like decision-making and refinement techniques. The framework features a two-stage training pipeline that combines multi-turn outcome verification with a clinical-fidelity process reward system, improving interaction efficiency and reducing redundant actions. Extensive testing across various medical modalities shows that MedSAM-Agent achieves leading performance, effectively integrating autonomous reasoning with iterative optimization.", title='Revolutionizing Medical Image Segmentation with Autonomous Decision-Making'))
[04.02.2026 14:01] Response: ParsedChatCompletionMessage[Article](content='{"desc":"MedSAM-AgentMedSAM-Agent21","title":"MedSAM-Agent"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='MedSAM-AgentMedSAM-Agent21', title='MedSAM-Agent'))
[04.02.2026 14:01] Querying the API.
[04.02.2026 14:01] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Native position-independent caching enhances LLM inference efficiency by reintroducing encoders and developing a caching system that reduces latency while maintaining accuracy.  					AI-generated summary 				 The Key-Value (KV) cache of Large Language Models (LLMs) is prefix-based, making it highly inefficient for processing contexts retrieved in arbitrary order. Position-Independent Caching (PIC) has been proposed to enable KV reuse without positional constraints; however, existing approaches often incur substantial accuracy degradation, limiting their practical adoption. To address this issue, we propose native PIC by reintroducing the encoder to prevalent decoder-only LLMs and explicitly training it to support PIC. We further develop COMB, a PIC-aware caching system that integrates seamlessly with existing inference frameworks. Experimental results show that COMB reduces Time-to-First-Token (TTFT) by 51-94% and increases throughput by 3times with comparable accuracy. Furthermore, the quality improvement when using DeepSeek-V2-Lite-Chat demonstrates the applicability of COMB to other types of decoder-only LLMs. Our code is available at https://github.com/shijuzhao/Comb.
[04.02.2026 14:01] Response: ```json
{
  "desc": "    -  (PIC)    ,    Key-Value       .   -only             PIC.    COMB,             .         51-94%         .",
  "emoji": "",
  "title": "-       "
}
```
[04.02.2026 14:01] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Native position-independent caching enhances LLM inference efficiency by reintroducing encoders and developing a caching system that reduces latency while maintaining accuracy.  					AI-generated summary 				 The Key-Value (KV) cache of Large Language Models (LLMs) is prefix-based, making it highly inefficient for processing contexts retrieved in arbitrary order. Position-Independent Caching (PIC) has been proposed to enable KV reuse without positional constraints; however, existing approaches often incur substantial accuracy degradation, limiting their practical adoption. To address this issue, we propose native PIC by reintroducing the encoder to prevalent decoder-only LLMs and explicitly training it to support PIC. We further develop COMB, a PIC-aware caching system that integrates seamlessly with existing inference frameworks. Experimental results show that COMB reduces Time-to-First-Token (TTFT) by 51-94% and increases throughput by 3times with comparable accuracy. Furthermore, the quality improvement when using DeepSeek-V2-Lite-Chat demonstrates the applicability of COMB to other types of decoder-only LLMs. Our code is available at https://github.com/shijuzhao/Comb."

[04.02.2026 14:01] Response: ```python
["INFERENCE", "ARCHITECTURE", "RAG"]
```

**Justification:**

1. **INFERENCE**: The paper explicitly focuses on optimizing model deployment and inference efficiency, specifically addressing KV cache optimization, reducing latency (TTFT), and increasing throughput during inference.

2. **ARCHITECTURE**: The paper proposes novel architectural modifications by reintroducing encoders to decoder-only LLMs and developing a new caching system (COMB), which represents a novel neural architecture component.

3. **RAG**: The paper addresses processing "contexts retrieved in arbitrary order," which is directly related to retrieval-augmented generation scenarios where retrieved contexts need to be efficiently cached and processed.
[04.02.2026 14:01] Error. Failed to parse JSON from LLM. ["INFERENCE", "ARCHITECTURE", "RAG"]


**Justification:**

1. **INFERENCE**: The paper explicitly focuses on optimizing model deployment and inference efficiency, specifically addressing KV cache optimization, reducing latency (TTFT), and increasing throughput during inference.

2. **ARCHITECTURE**: The paper proposes novel architectural modifications by reintroducing encoders to decoder-only LLMs and developing a new caching system (COMB), which represents a novel neural architecture component.

3. **RAG**: The paper addresses processing "contexts retrieved in arbitrary order," which is directly related to retrieval-augmented generation scenarios where retrieved contexts need to be efficiently cached and processed.
[04.02.2026 14:01] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Native position-independent caching enhances LLM inference efficiency by reintroducing encoders and developing a caching system that reduces latency while maintaining accuracy.  					AI-generated summary 				 The Key-Value (KV) cache of Large Language Models (LLMs) is prefix-based, making it highly inefficient for processing contexts retrieved in arbitrary order. Position-Independent Caching (PIC) has been proposed to enable KV reuse without positional constraints; however, existing approaches often incur substantial accuracy degradation, limiting their practical adoption. To address this issue, we propose native PIC by reintroducing the encoder to prevalent decoder-only LLMs and explicitly training it to support PIC. We further develop COMB, a PIC-aware caching system that integrates seamlessly with existing inference frameworks. Experimental results show that COMB reduces Time-to-First-Token (TTFT) by 51-94% and increases throughput by 3times with comparable accuracy. Furthermore, the quality improvement when using DeepSeek-V2-Lite-Chat demonstrates the applicability of COMB to other types of decoder-only LLMs. Our code is available at https://github.com/shijuzhao/Comb."

[04.02.2026 14:01] Response: ```python
["OPTIMIZATION", "OPEN_SOURCE"]
```

**Justification:**

- **OPTIMIZATION**: The paper focuses on enhancing LLM inference efficiency through a caching system (COMB) that reduces latency and improves throughput metrics (51-94% reduction in TTFT, 3x throughput increase). This directly addresses training and inference optimization methods.

- **OPEN_SOURCE**: The paper explicitly states "Our code is available at https://github.com/shijuzhao/Comb," indicating the authors are releasing their code/framework to the public.
[04.02.2026 14:01] Error. Failed to parse JSON from LLM. ["OPTIMIZATION", "OPEN_SOURCE"]


**Justification:**

- **OPTIMIZATION**: The paper focuses on enhancing LLM inference efficiency through a caching system (COMB) that reduces latency and improves throughput metrics (51-94% reduction in TTFT, 3x throughput increase). This directly addresses training and inference optimization methods.

- **OPEN_SOURCE**: The paper explicitly states "Our code is available at https://github.com/shijuzhao/Comb," indicating the authors are releasing their code/framework to the public.
[04.02.2026 14:01] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents a novel approach to improve the efficiency of Large Language Models (LLMs) during inference by implementing native position-independent caching (PIC). The authors reintroduce encoders into decoder-only LLMs, allowing for key-value (KV) cache reuse without being limited by positional constraints. They develop a new caching system called COMB, which integrates with existing frameworks and significantly reduces latency while maintaining accuracy. Experimental results indicate that COMB can decrease Time-to-First-Token (TTFT) by up to 94% and triple throughput, demonstrating its effectiveness across various decoder-only LLMs.","title":"Boosting LLM Efficiency with Native Position-Independent Caching"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper presents a novel approach to improve the efficiency of Large Language Models (LLMs) during inference by implementing native position-independent caching (PIC). The authors reintroduce encoders into decoder-only LLMs, allowing for key-value (KV) cache reuse without being limited by positional constraints. They develop a new caching system called COMB, which integrates with existing frameworks and significantly reduces latency while maintaining accuracy. Experimental results indicate that COMB can decrease Time-to-First-Token (TTFT) by up to 94% and triple throughput, demonstrating its effectiveness across various decoder-only LLMs.', title='Boosting LLM Efficiency with Native Position-Independent Caching'))
[04.02.2026 14:01] Response: ParsedChatCompletionMessage[Article](content='{"desc":"PICLLMCOMBCOMB","title":"LLM"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='PICLLMCOMBCOMB', title='LLM'))
[04.02.2026 14:01] Renaming data file.
[04.02.2026 14:01] Renaming previous data. hf_papers.json to ./d/2026-02-04.json
[04.02.2026 14:01] Saving new data file.
[04.02.2026 14:01] Generating page.
[04.02.2026 14:01] Renaming previous page.
[04.02.2026 14:01] Renaming previous data. index.html to ./d/2026-02-04.html
[04.02.2026 14:01] Writing result.
[04.02.2026 14:01] Renaming log file.
[04.02.2026 14:01] Renaming previous data. log.txt to ./logs/2026-02-04_last_log.txt
