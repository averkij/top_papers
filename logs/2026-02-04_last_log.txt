[04.02.2026 15:37] Read previous papers.
[04.02.2026 15:37] Generating top page (month).
[04.02.2026 15:37] Writing top page (month).
[04.02.2026 16:38] Read previous papers.
[04.02.2026 16:38] Get feed.
[04.02.2026 16:38] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01785
[04.02.2026 16:38] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03786
[04.02.2026 16:38] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02103
[04.02.2026 16:38] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02619
[04.02.2026 16:38] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01630
[04.02.2026 16:38] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03796
[04.02.2026 16:38] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02660
[04.02.2026 16:38] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03048
[04.02.2026 16:38] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03139
[04.02.2026 16:38] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03419
[04.02.2026 16:38] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03411
[04.02.2026 16:38] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03845
[04.02.2026 16:38] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03619
[04.02.2026 16:38] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02380
[04.02.2026 16:38] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02444
[04.02.2026 16:38] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02636
[04.02.2026 16:38] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21244
[04.02.2026 16:38] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01362
[04.02.2026 16:38] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03798
[04.02.2026 16:38] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03216
[04.02.2026 16:38] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03086
[04.02.2026 16:38] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02676
[04.02.2026 16:38] Get page data from previous paper. URL: https://huggingface.co/papers/2602.00747
[04.02.2026 16:38] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03747
[04.02.2026 16:38] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03709
[04.02.2026 16:38] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03677
[04.02.2026 16:38] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03647
[04.02.2026 16:38] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01053
[04.02.2026 16:38] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02537
[04.02.2026 16:38] Get page data from previous paper. URL: https://huggingface.co/papers/2601.19103
[04.02.2026 16:38] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03806
[04.02.2026 16:38] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03295
[04.02.2026 16:38] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02419
[04.02.2026 16:38] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01753
[04.02.2026 16:38] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03837
[04.02.2026 16:38] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03454
[04.02.2026 16:38] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02914
[04.02.2026 16:38] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01212
[04.02.2026 16:38] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03238
[04.02.2026 16:38] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02494
[04.02.2026 16:38] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02405
[04.02.2026 16:38] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02220
[04.02.2026 16:38] Extract page data from URL. URL: https://huggingface.co/papers/2602.01405
[04.02.2026 16:38] Get page data from previous paper. URL: https://huggingface.co/papers/2602.00682
[04.02.2026 16:38] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03817
[04.02.2026 16:38] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03320
[04.02.2026 16:38] Extract page data from URL. URL: https://huggingface.co/papers/2602.02751
[04.02.2026 16:38] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01519
[04.02.2026 16:38] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[04.02.2026 16:38] No deleted papers detected.
[04.02.2026 16:38] Downloading and parsing papers (pdf, html). Total: 48.
[04.02.2026 16:38] Downloading and parsing paper https://huggingface.co/papers/2602.01785.
[04.02.2026 16:38] Extra JSON file exists (./assets/json/2602.01785.json), skip PDF parsing.
[04.02.2026 16:38] Paper image links file exists (./assets/img_data/2602.01785.json), skip HTML parsing.
[04.02.2026 16:38] Success.
[04.02.2026 16:38] Downloading and parsing paper https://huggingface.co/papers/2602.03786.
[04.02.2026 16:38] Extra JSON file exists (./assets/json/2602.03786.json), skip PDF parsing.
[04.02.2026 16:38] Paper image links file exists (./assets/img_data/2602.03786.json), skip HTML parsing.
[04.02.2026 16:38] Success.
[04.02.2026 16:38] Downloading and parsing paper https://huggingface.co/papers/2602.02103.
[04.02.2026 16:38] Extra JSON file exists (./assets/json/2602.02103.json), skip PDF parsing.
[04.02.2026 16:38] Paper image links file exists (./assets/img_data/2602.02103.json), skip HTML parsing.
[04.02.2026 16:38] Success.
[04.02.2026 16:38] Downloading and parsing paper https://huggingface.co/papers/2602.02619.
[04.02.2026 16:38] Extra JSON file exists (./assets/json/2602.02619.json), skip PDF parsing.
[04.02.2026 16:38] Paper image links file exists (./assets/img_data/2602.02619.json), skip HTML parsing.
[04.02.2026 16:38] Success.
[04.02.2026 16:38] Downloading and parsing paper https://huggingface.co/papers/2602.01630.
[04.02.2026 16:38] Extra JSON file exists (./assets/json/2602.01630.json), skip PDF parsing.
[04.02.2026 16:38] Paper image links file exists (./assets/img_data/2602.01630.json), skip HTML parsing.
[04.02.2026 16:38] Success.
[04.02.2026 16:38] Downloading and parsing paper https://huggingface.co/papers/2602.03796.
[04.02.2026 16:38] Extra JSON file exists (./assets/json/2602.03796.json), skip PDF parsing.
[04.02.2026 16:38] Paper image links file exists (./assets/img_data/2602.03796.json), skip HTML parsing.
[04.02.2026 16:38] Success.
[04.02.2026 16:38] Downloading and parsing paper https://huggingface.co/papers/2602.02660.
[04.02.2026 16:38] Extra JSON file exists (./assets/json/2602.02660.json), skip PDF parsing.
[04.02.2026 16:38] Paper image links file exists (./assets/img_data/2602.02660.json), skip HTML parsing.
[04.02.2026 16:38] Success.
[04.02.2026 16:38] Downloading and parsing paper https://huggingface.co/papers/2602.03048.
[04.02.2026 16:38] Extra JSON file exists (./assets/json/2602.03048.json), skip PDF parsing.
[04.02.2026 16:38] Paper image links file exists (./assets/img_data/2602.03048.json), skip HTML parsing.
[04.02.2026 16:38] Success.
[04.02.2026 16:38] Downloading and parsing paper https://huggingface.co/papers/2602.03139.
[04.02.2026 16:38] Extra JSON file exists (./assets/json/2602.03139.json), skip PDF parsing.
[04.02.2026 16:38] Paper image links file exists (./assets/img_data/2602.03139.json), skip HTML parsing.
[04.02.2026 16:38] Success.
[04.02.2026 16:38] Downloading and parsing paper https://huggingface.co/papers/2602.03419.
[04.02.2026 16:38] Extra JSON file exists (./assets/json/2602.03419.json), skip PDF parsing.
[04.02.2026 16:38] Paper image links file exists (./assets/img_data/2602.03419.json), skip HTML parsing.
[04.02.2026 16:38] Success.
[04.02.2026 16:38] Downloading and parsing paper https://huggingface.co/papers/2602.03411.
[04.02.2026 16:38] Extra JSON file exists (./assets/json/2602.03411.json), skip PDF parsing.
[04.02.2026 16:38] Paper image links file exists (./assets/img_data/2602.03411.json), skip HTML parsing.
[04.02.2026 16:38] Success.
[04.02.2026 16:38] Downloading and parsing paper https://huggingface.co/papers/2602.03845.
[04.02.2026 16:38] Extra JSON file exists (./assets/json/2602.03845.json), skip PDF parsing.
[04.02.2026 16:38] Paper image links file exists (./assets/img_data/2602.03845.json), skip HTML parsing.
[04.02.2026 16:38] Success.
[04.02.2026 16:38] Downloading and parsing paper https://huggingface.co/papers/2602.03619.
[04.02.2026 16:38] Extra JSON file exists (./assets/json/2602.03619.json), skip PDF parsing.
[04.02.2026 16:38] Paper image links file exists (./assets/img_data/2602.03619.json), skip HTML parsing.
[04.02.2026 16:38] Success.
[04.02.2026 16:38] Downloading and parsing paper https://huggingface.co/papers/2602.02380.
[04.02.2026 16:38] Extra JSON file exists (./assets/json/2602.02380.json), skip PDF parsing.
[04.02.2026 16:38] Paper image links file exists (./assets/img_data/2602.02380.json), skip HTML parsing.
[04.02.2026 16:38] Success.
[04.02.2026 16:38] Downloading and parsing paper https://huggingface.co/papers/2602.02444.
[04.02.2026 16:38] Extra JSON file exists (./assets/json/2602.02444.json), skip PDF parsing.
[04.02.2026 16:38] Paper image links file exists (./assets/img_data/2602.02444.json), skip HTML parsing.
[04.02.2026 16:38] Success.
[04.02.2026 16:38] Downloading and parsing paper https://huggingface.co/papers/2602.02636.
[04.02.2026 16:38] Extra JSON file exists (./assets/json/2602.02636.json), skip PDF parsing.
[04.02.2026 16:38] Paper image links file exists (./assets/img_data/2602.02636.json), skip HTML parsing.
[04.02.2026 16:38] Success.
[04.02.2026 16:38] Downloading and parsing paper https://huggingface.co/papers/2601.21244.
[04.02.2026 16:38] Extra JSON file exists (./assets/json/2601.21244.json), skip PDF parsing.
[04.02.2026 16:38] Paper image links file exists (./assets/img_data/2601.21244.json), skip HTML parsing.
[04.02.2026 16:38] Success.
[04.02.2026 16:38] Downloading and parsing paper https://huggingface.co/papers/2602.01362.
[04.02.2026 16:38] Extra JSON file exists (./assets/json/2602.01362.json), skip PDF parsing.
[04.02.2026 16:38] Paper image links file exists (./assets/img_data/2602.01362.json), skip HTML parsing.
[04.02.2026 16:38] Success.
[04.02.2026 16:38] Downloading and parsing paper https://huggingface.co/papers/2602.03798.
[04.02.2026 16:38] Extra JSON file exists (./assets/json/2602.03798.json), skip PDF parsing.
[04.02.2026 16:38] Paper image links file exists (./assets/img_data/2602.03798.json), skip HTML parsing.
[04.02.2026 16:38] Success.
[04.02.2026 16:38] Downloading and parsing paper https://huggingface.co/papers/2602.03216.
[04.02.2026 16:38] Extra JSON file exists (./assets/json/2602.03216.json), skip PDF parsing.
[04.02.2026 16:38] Paper image links file exists (./assets/img_data/2602.03216.json), skip HTML parsing.
[04.02.2026 16:38] Success.
[04.02.2026 16:38] Downloading and parsing paper https://huggingface.co/papers/2602.03086.
[04.02.2026 16:38] Extra JSON file exists (./assets/json/2602.03086.json), skip PDF parsing.
[04.02.2026 16:38] Paper image links file exists (./assets/img_data/2602.03086.json), skip HTML parsing.
[04.02.2026 16:38] Success.
[04.02.2026 16:38] Downloading and parsing paper https://huggingface.co/papers/2602.02676.
[04.02.2026 16:38] Extra JSON file exists (./assets/json/2602.02676.json), skip PDF parsing.
[04.02.2026 16:38] Paper image links file exists (./assets/img_data/2602.02676.json), skip HTML parsing.
[04.02.2026 16:38] Success.
[04.02.2026 16:38] Downloading and parsing paper https://huggingface.co/papers/2602.00747.
[04.02.2026 16:38] Extra JSON file exists (./assets/json/2602.00747.json), skip PDF parsing.
[04.02.2026 16:38] Paper image links file exists (./assets/img_data/2602.00747.json), skip HTML parsing.
[04.02.2026 16:38] Success.
[04.02.2026 16:38] Downloading and parsing paper https://huggingface.co/papers/2602.03747.
[04.02.2026 16:38] Extra JSON file exists (./assets/json/2602.03747.json), skip PDF parsing.
[04.02.2026 16:38] Paper image links file exists (./assets/img_data/2602.03747.json), skip HTML parsing.
[04.02.2026 16:38] Success.
[04.02.2026 16:38] Downloading and parsing paper https://huggingface.co/papers/2602.03709.
[04.02.2026 16:38] Extra JSON file exists (./assets/json/2602.03709.json), skip PDF parsing.
[04.02.2026 16:38] Paper image links file exists (./assets/img_data/2602.03709.json), skip HTML parsing.
[04.02.2026 16:38] Success.
[04.02.2026 16:38] Downloading and parsing paper https://huggingface.co/papers/2602.03677.
[04.02.2026 16:38] Extra JSON file exists (./assets/json/2602.03677.json), skip PDF parsing.
[04.02.2026 16:38] Paper image links file exists (./assets/img_data/2602.03677.json), skip HTML parsing.
[04.02.2026 16:38] Success.
[04.02.2026 16:38] Downloading and parsing paper https://huggingface.co/papers/2602.03647.
[04.02.2026 16:38] Extra JSON file exists (./assets/json/2602.03647.json), skip PDF parsing.
[04.02.2026 16:38] Paper image links file exists (./assets/img_data/2602.03647.json), skip HTML parsing.
[04.02.2026 16:38] Success.
[04.02.2026 16:38] Downloading and parsing paper https://huggingface.co/papers/2602.01053.
[04.02.2026 16:38] Extra JSON file exists (./assets/json/2602.01053.json), skip PDF parsing.
[04.02.2026 16:38] Paper image links file exists (./assets/img_data/2602.01053.json), skip HTML parsing.
[04.02.2026 16:38] Success.
[04.02.2026 16:38] Downloading and parsing paper https://huggingface.co/papers/2602.02537.
[04.02.2026 16:38] Extra JSON file exists (./assets/json/2602.02537.json), skip PDF parsing.
[04.02.2026 16:38] Paper image links file exists (./assets/img_data/2602.02537.json), skip HTML parsing.
[04.02.2026 16:38] Success.
[04.02.2026 16:38] Downloading and parsing paper https://huggingface.co/papers/2601.19103.
[04.02.2026 16:38] Extra JSON file exists (./assets/json/2601.19103.json), skip PDF parsing.
[04.02.2026 16:38] Paper image links file exists (./assets/img_data/2601.19103.json), skip HTML parsing.
[04.02.2026 16:38] Success.
[04.02.2026 16:38] Downloading and parsing paper https://huggingface.co/papers/2602.03806.
[04.02.2026 16:38] Extra JSON file exists (./assets/json/2602.03806.json), skip PDF parsing.
[04.02.2026 16:38] Paper image links file exists (./assets/img_data/2602.03806.json), skip HTML parsing.
[04.02.2026 16:38] Success.
[04.02.2026 16:38] Downloading and parsing paper https://huggingface.co/papers/2602.03295.
[04.02.2026 16:38] Extra JSON file exists (./assets/json/2602.03295.json), skip PDF parsing.
[04.02.2026 16:38] Paper image links file exists (./assets/img_data/2602.03295.json), skip HTML parsing.
[04.02.2026 16:38] Success.
[04.02.2026 16:38] Downloading and parsing paper https://huggingface.co/papers/2602.02419.
[04.02.2026 16:38] Extra JSON file exists (./assets/json/2602.02419.json), skip PDF parsing.
[04.02.2026 16:38] Paper image links file exists (./assets/img_data/2602.02419.json), skip HTML parsing.
[04.02.2026 16:38] Success.
[04.02.2026 16:38] Downloading and parsing paper https://huggingface.co/papers/2602.01753.
[04.02.2026 16:38] Extra JSON file exists (./assets/json/2602.01753.json), skip PDF parsing.
[04.02.2026 16:38] Paper image links file exists (./assets/img_data/2602.01753.json), skip HTML parsing.
[04.02.2026 16:38] Success.
[04.02.2026 16:38] Downloading and parsing paper https://huggingface.co/papers/2602.03837.
[04.02.2026 16:38] Extra JSON file exists (./assets/json/2602.03837.json), skip PDF parsing.
[04.02.2026 16:38] Paper image links file exists (./assets/img_data/2602.03837.json), skip HTML parsing.
[04.02.2026 16:38] Success.
[04.02.2026 16:38] Downloading and parsing paper https://huggingface.co/papers/2602.03454.
[04.02.2026 16:38] Extra JSON file exists (./assets/json/2602.03454.json), skip PDF parsing.
[04.02.2026 16:38] Paper image links file exists (./assets/img_data/2602.03454.json), skip HTML parsing.
[04.02.2026 16:38] Success.
[04.02.2026 16:38] Downloading and parsing paper https://huggingface.co/papers/2602.02914.
[04.02.2026 16:38] Extra JSON file exists (./assets/json/2602.02914.json), skip PDF parsing.
[04.02.2026 16:38] Paper image links file exists (./assets/img_data/2602.02914.json), skip HTML parsing.
[04.02.2026 16:38] Success.
[04.02.2026 16:38] Downloading and parsing paper https://huggingface.co/papers/2602.01212.
[04.02.2026 16:38] Extra JSON file exists (./assets/json/2602.01212.json), skip PDF parsing.
[04.02.2026 16:38] Paper image links file exists (./assets/img_data/2602.01212.json), skip HTML parsing.
[04.02.2026 16:38] Success.
[04.02.2026 16:38] Downloading and parsing paper https://huggingface.co/papers/2602.03238.
[04.02.2026 16:38] Extra JSON file exists (./assets/json/2602.03238.json), skip PDF parsing.
[04.02.2026 16:38] Paper image links file exists (./assets/img_data/2602.03238.json), skip HTML parsing.
[04.02.2026 16:38] Success.
[04.02.2026 16:38] Downloading and parsing paper https://huggingface.co/papers/2602.02494.
[04.02.2026 16:38] Extra JSON file exists (./assets/json/2602.02494.json), skip PDF parsing.
[04.02.2026 16:38] Paper image links file exists (./assets/img_data/2602.02494.json), skip HTML parsing.
[04.02.2026 16:38] Success.
[04.02.2026 16:38] Downloading and parsing paper https://huggingface.co/papers/2602.02405.
[04.02.2026 16:38] Extra JSON file exists (./assets/json/2602.02405.json), skip PDF parsing.
[04.02.2026 16:38] Paper image links file exists (./assets/img_data/2602.02405.json), skip HTML parsing.
[04.02.2026 16:38] Success.
[04.02.2026 16:38] Downloading and parsing paper https://huggingface.co/papers/2602.02220.
[04.02.2026 16:38] Extra JSON file exists (./assets/json/2602.02220.json), skip PDF parsing.
[04.02.2026 16:38] Paper image links file exists (./assets/img_data/2602.02220.json), skip HTML parsing.
[04.02.2026 16:38] Success.
[04.02.2026 16:38] Downloading and parsing paper https://huggingface.co/papers/2602.01405.
[04.02.2026 16:38] Downloading paper 2602.01405 from https://arxiv.org/pdf/2602.01405v1...
[04.02.2026 16:38] Extracting affiliations from text.
[04.02.2026 16:38] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 1 ] . [ 1 5 0 4 1 0 . 2 0 6 2 : r Feedback by Design: Understanding and Overcoming User Feedback Barriers in Conversational Agents Nikhil Sharma nsharm27@jhu.edu Johns Hopkins University Baltimore, MD, USA Namita Krishnan namitak@adobe.com Adobe Inc. San Jose, CA, USA Daniel Lee dlee1@adobe.com Adobe Inc. San Jose, CA, USA Ziang Xiao ziang.xiao@jhu.edu Johns Hopkins University Baltimore, MD, USA Zheng Zhang zhengzhang@adobe.com Adobe Inc. San Jose, CA, USA Guang-Jie Ren gren@adobe.com Adobe Inc. San Jose, CA, USA Yunyao Li yunyaol@adobe.com Adobe Inc. San Jose, CA, USA Abstract High-quality feedback is essential for effective humanAI interaction. It bridges knowledge gaps, corrects digressions, and shapes system behavior; both during interaction and throughout model development. Yet despite its importance, human feedback to AI is often infrequent and low quality. This gap motivates critical examination of human feedback during interactions with AIs. To understand and overcome the challenges preventing users from giving high-quality feedback, we conducted two studies examining feedback dynamics between humans and conversational agents (CAs). Our formative study, through the lens of Grices maxims, identified four Feedback BarriersCommon Ground, Verifiability, Communication, and Informativenessthat prevent high-quality feedback by users. Building on these findings, we derive three design desiderata and show that systems incorporating scaffolds aligned with these desiderata enabled users to provide higher-quality feedback. Finally, we detail call for action to the broader AI community for advances in Large Language Models capabilities to overcome Feedback Barriers. CCS Concepts Human-centered computing Human computer interaction (HCI); HCI design and evaluation methods; User studies; Empirical studies in HCI; Collaborative and social computing; Information systems; This work was done during the authors internship at Adobe This work is licensed under Creative Com"
[04.02.2026 16:38] Response: ```python
[
    "Johns Hopkins University",
    "Adobe Inc."
]
```
[04.02.2026 16:38] Deleting PDF ./assets/pdf/2602.01405.pdf.
[04.02.2026 16:38] Success.
[04.02.2026 16:38] Downloading and parsing paper https://huggingface.co/papers/2602.00682.
[04.02.2026 16:38] Extra JSON file exists (./assets/json/2602.00682.json), skip PDF parsing.
[04.02.2026 16:38] Paper image links file exists (./assets/img_data/2602.00682.json), skip HTML parsing.
[04.02.2026 16:38] Success.
[04.02.2026 16:38] Downloading and parsing paper https://huggingface.co/papers/2602.03817.
[04.02.2026 16:38] Extra JSON file exists (./assets/json/2602.03817.json), skip PDF parsing.
[04.02.2026 16:38] Paper image links file exists (./assets/img_data/2602.03817.json), skip HTML parsing.
[04.02.2026 16:38] Success.
[04.02.2026 16:38] Downloading and parsing paper https://huggingface.co/papers/2602.03320.
[04.02.2026 16:38] Extra JSON file exists (./assets/json/2602.03320.json), skip PDF parsing.
[04.02.2026 16:38] Paper image links file exists (./assets/img_data/2602.03320.json), skip HTML parsing.
[04.02.2026 16:38] Success.
[04.02.2026 16:38] Downloading and parsing paper https://huggingface.co/papers/2602.02751.
[04.02.2026 16:38] Downloading paper 2602.02751 from https://arxiv.org/pdf/2602.02751v1...
[04.02.2026 16:38] Extracting affiliations from text.
[04.02.2026 16:38] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 2 ] . [ 1 1 5 7 2 0 . 2 0 6 2 : r Scaling Small Agents Through Strategy Auctions Lisa Alazraki,2, William F. Shen,3, Yoram Bachrach1, Akhil Mathur1 1Meta Superintelligence Labs, 2Imperial College London, 3University of Cambridge Work done at Meta Small language models are increasingly viewed as promising, cost-effective approach to agentic AI, with proponents claiming they are sufficiently capable for agentic workflows. However, while smaller agents can closely match larger ones on simple tasks, it remains unclear how their performance scales with task complexity, when large models become necessary, and how to better leverage small agents for long-horizon workloads. In this work, we empirically show that small agents performance fails to scale with task complexity on deep search and coding tasks, and we introduce Strategy Auctions for Workload Efficiency (sale), an agent framework inspired by freelancer marketplaces. In sale, agents bid with short strategic plans, which are scored by systematic costvalue mechanism and refined via shared auction memory, enabling per-task routing and continual self-improvement without training separate router or running all models to completion. Across deep search and coding tasks of varying complexity, sale reduces reliance on the largest agent by 53%, lowers overall cost by 35%, and consistently improves upon the largest agents pass@1 with only negligible overhead beyond executing the final trace. In contrast, established routers that rely on task descriptions either underperform the largest agent or fail to reduce costoften bothunderscoring their poor fit for agentic workflows. These results suggest that while small agents may be insufficient for complex workloads, they can be effectively scaled up through coordinated task allocation and test-time self-improvement. More broadly, they motivate systems-level view of agentic AI in which performance gains come less from ever-larger individual models and more from market-inspire"
[04.02.2026 16:38] Failed to download and parse paper https://huggingface.co/papers/2602.02751: Error code: 500 - {'type': 'error', 'error': {'type': 'api_error', 'message': 'Internal server error'}, 'request_id': 'req_011CXoMjdobqg1EVP8kFrDiV'}
[04.02.2026 16:38] Downloading and parsing paper https://huggingface.co/papers/2602.01519.
[04.02.2026 16:38] Extra JSON file exists (./assets/json/2602.01519.json), skip PDF parsing.
[04.02.2026 16:38] Paper image links file exists (./assets/img_data/2602.01519.json), skip HTML parsing.
[04.02.2026 16:38] Success.
[04.02.2026 16:38] Enriching papers with extra data.
[04.02.2026 16:38] ********************************************************************************
[04.02.2026 16:38] Abstract 0. Multimodal large language models can effectively understand source code when represented as compressed images, achieving significant token reduction while maintaining or improving performance on code comprehension tasks.  					AI-generated summary 				 Large Language Models (LLMs) have achieved rema...
[04.02.2026 16:38] ********************************************************************************
[04.02.2026 16:38] Abstract 1. AOrchestra is a framework-agnostic agentic system that uses a tuple-based abstraction to dynamically create specialized task executors, achieving improved performance on complex benchmarks through automated agent creation and resource management.  					AI-generated summary 				 Language agents have ...
[04.02.2026 16:38] ********************************************************************************
[04.02.2026 16:38] Abstract 2. Research investigates latent planning dynamics in large language models through a probing method called Tele-Lens, revealing limited global planning and enabling improved uncertainty estimation and CoT bypass recognition.  					AI-generated summary 				 This work stems from prior complementary obser...
[04.02.2026 16:38] ********************************************************************************
[04.02.2026 16:38] Abstract 3. Large language models face challenges in long-horizon agentic workflows due to lack of authentic long-dependency training data, which is addressed by leveraging pull request sequences for structured supervision through progressive decomposition, consistency enforcement, and refinement from bug-fix h...
[04.02.2026 16:38] ********************************************************************************
[04.02.2026 16:38] Abstract 4. Current world models lack unified frameworks despite task-specific advances, necessitating a comprehensive approach integrating interaction, perception, symbolic reasoning, and spatial representation.  					AI-generated summary 				 World models have emerged as a critical frontier in AI research, ai...
[04.02.2026 16:38] ********************************************************************************
[04.02.2026 16:38] Abstract 5. 3DiMo enables view-agnostic human motion control in video generation by training a motion encoder alongside a pretrained video generator to distill driving frames into compact motion tokens that align with the generator's spatial priors.  					AI-generated summary 				 Existing methods for human mot...
[04.02.2026 16:38] ********************************************************************************
[04.02.2026 16:38] Abstract 6. MARS is a modular AI research automation framework that uses budget-aware planning, modular construction, and reflective memory to achieve state-of-the-art performance in autonomous machine learning research.  					AI-generated summary 				 Automating AI research differs from general software engine...
[04.02.2026 16:38] ********************************************************************************
[04.02.2026 16:38] Abstract 7. CoBA-RL adapts rollout budget allocation for LLM training by evaluating sample training value and optimizing resource distribution through a capability-oriented value function and greedy strategy.  					AI-generated summary 				 Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a ...
[04.02.2026 16:38] ********************************************************************************
[04.02.2026 16:38] Abstract 8. A novel distillation framework called DP-DMD is introduced that preserves sample diversity in text-to-image generation by separating the roles of distilled steps, using v-prediction for diversity and standard DMD loss for quality refinement without additional computational overhead.  					AI-generat...
[04.02.2026 16:38] ********************************************************************************
[04.02.2026 16:38] Abstract 9. A Docker-free framework replaces physical execution environments with learned surrogates for training software engineering agents, enabling efficient training and test-time scaling without costly container setup.  					AI-generated summary 				 Recent advances in large language models (LLMs) have en...
[04.02.2026 16:38] ********************************************************************************
[04.02.2026 16:38] Abstract 10. SWE-Master presents a reproducible framework for developing software engineering agents through systematic optimization across multiple stages of agent development, achieving superior performance on software task resolution benchmarks.  					AI-generated summary 				 In this technical report, we pre...
[04.02.2026 16:38] ********************************************************************************
[04.02.2026 16:38] Abstract 11. Parallel-Probe is a training-free controller that optimizes parallel thinking by using consensus-based early stopping and deviation-based branch pruning to reduce computational costs while maintaining accuracy.  					AI-generated summary 				 Parallel thinking has emerged as a promising paradigm for...
[04.02.2026 16:38] ********************************************************************************
[04.02.2026 16:38] Abstract 12. DeepResearch report generation is improved through human-preference-aligned rubric generators trained via reinforcement learning with hybrid rewards and enhanced with multi-agent Markov-state workflows.  					AI-generated summary 				 Nowadays, training and evaluating DeepResearch-generated reports ...
[04.02.2026 16:38] ********************************************************************************
[04.02.2026 16:38] Abstract 13. UnifiedReward-Flex combines reward modeling with flexible, context-adaptive reasoning to improve visual generation by dynamically constructing hierarchical assessments based on semantic intent and visual evidence.  					AI-generated summary 				 Recent advancements in multimodal reward models (RMs) ...
[04.02.2026 16:38] ********************************************************************************
[04.02.2026 16:38] Abstract 14. RANKVIDEO is a reasoning-based video retrieval system that improves upon traditional two-stage frameworks through explicit query-video pair analysis and a multi-objective training approach.  					AI-generated summary 				 Reranking is a critical component of modern retrieval systems, which typically...
[04.02.2026 16:38] ********************************************************************************
[04.02.2026 16:38] Abstract 15. Wide Research advances search intelligence through a dedicated benchmark and multi-agent architecture that enables parallel information retrieval under complex constraints.  					AI-generated summary 				 Search intelligence is evolving from Deep Research to Wide Research, a paradigm essential for r...
[04.02.2026 16:38] ********************************************************************************
[04.02.2026 16:38] Abstract 16. LENS framework improves reinforcement learning with verifiable rewards by identifying and removing interference tokens to enhance exploration efficiency and training stability.  					AI-generated summary 				 Reinforcement Learning with Verifiable Rewards (RLVR) has advanced LLM reasoning, but remai...
[04.02.2026 16:38] ********************************************************************************
[04.02.2026 16:38] Abstract 17. XDLM unifies Masked Diffusion Language Models and Uniform-noise Diffusion Language Models through a stationary noise kernel, achieving improved performance in both semantic understanding and generation quality.  					AI-generated summary 				 In discrete generative modeling, two dominant paradigms d...
[04.02.2026 16:38] ********************************************************************************
[04.02.2026 16:38] Abstract 18. A unified agent system called FullStack-Agent is introduced to assist non-expert users in developing complex interactive websites by addressing full-stack development challenges through enhanced planning, code editing, and self-improving capabilities.  					AI-generated summary 				 Assisting non-ex...
[04.02.2026 16:38] ********************************************************************************
[04.02.2026 16:38] Abstract 19. Token Sparse Attention enables efficient long-context inference by dynamically compressing and decompressing attention tensors at the token level, achieving significant speedup with minimal accuracy loss.  					AI-generated summary 				 The quadratic complexity of attention remains the central bottl...
[04.02.2026 16:38] ********************************************************************************
[04.02.2026 16:38] Abstract 20. Neural Predictor-Corrector framework unifies homotopy methods across multiple domains and outperforms classical approaches through learned policies and amortized training.  					AI-generated summary 				 The Homotopy paradigm, a general principle for solving challenging problems, appears across dive...
[04.02.2026 16:38] ********************************************************************************
[04.02.2026 16:38] Abstract 21. AdaptMMBench presents a comprehensive benchmark for evaluating adaptive multimodal reasoning in Vision-Language Models, measuring reasoning mode selection rationality through dynamic difficulty assessment and multi-dimensional process evaluation.  					AI-generated summary 				 Adaptive multimodal r...
[04.02.2026 16:38] ********************************************************************************
[04.02.2026 16:38] Abstract 22. DeMix is a framework that uses model merging to predict optimal data ratios for LLM pre-training, decoupling search from training costs to improve mixture discovery efficiency.  					AI-generated summary 				 Determining an effective data mixture is a key factor in Large Language Model (LLM) pre-tra...
[04.02.2026 16:38] ********************************************************************************
[04.02.2026 16:38] Abstract 23. LIVE is a long-horizon video world model that uses cycle-consistency and diffusion loss to control error accumulation during extended video generation.  					AI-generated summary 				 Autoregressive video world models predict future visual observations conditioned on actions. While effective over sh...
[04.02.2026 16:38] ********************************************************************************
[04.02.2026 16:38] Abstract 24. Multi-hop question answering dataset ID-MoCQA assesses cultural understanding in large language models through Indonesian traditions with diverse reasoning chains.  					AI-generated summary 				 Understanding culture requires reasoning across context, tradition, and implicit social knowledge, far b...
[04.02.2026 16:38] ********************************************************************************
[04.02.2026 16:38] Abstract 25. Research reveals that instruction tokens act as structural anchors in multimodal large language models, with shallow layers performing non-selective information transfer and deep layers resolving modality competition guided by instruction intent.  					AI-generated summary 				 Modality following se...
[04.02.2026 16:38] ********************************************************************************
[04.02.2026 16:38] Abstract 26. Search-R2 framework improves language agent reasoning through Actor-Refiner collaboration with targeted interventions and fine-grained reward supervision for better credit assignment in reinforcement learning.  					AI-generated summary 				 Search-integrated reasoning enables language agents to tra...
[04.02.2026 16:38] ********************************************************************************
[04.02.2026 16:38] Abstract 27. LRAgent is a KV cache sharing framework for multi-LoRA agents that decomposes cache into shared and adapter-dependent components, reducing memory and compute overhead while maintaining accuracy.  					AI-generated summary 				 Role specialization in multi-LLM agent systems is often realized via mult...
[04.02.2026 16:38] ********************************************************************************
[04.02.2026 16:38] Abstract 28. WorldVQA is a benchmark for evaluating the visual world knowledge of multimodal large language models by separating visual knowledge retrieval from reasoning to measure memorized facts.  					AI-generated summary 				 We introduce WorldVQA, a benchmark designed to evaluate the atomic visual world kn...
[04.02.2026 16:38] ********************************************************************************
[04.02.2026 16:38] Abstract 29. A reinforcement learning framework with glance and focus models improves pan-cancer screening in CT scans by addressing foreground-background imbalance and reducing false positives through group relative learning.  					AI-generated summary 				 Pan-cancer screening in large-scale CT scans remains c...
[04.02.2026 16:38] ********************************************************************************
[04.02.2026 16:38] Abstract 30. Offline reinforcement learning method combines contextual bandit learning with partial trajectories to improve multi-turn code generation performance while reducing training costs.  					AI-generated summary 				 Recently, there have been significant research interests in training large language mod...
[04.02.2026 16:38] ********************************************************************************
[04.02.2026 16:38] Abstract 31. Stage-aware pruning method for large language and vision-language models that improves efficiency by selectively removing layers during different processing phases while maintaining accuracy.  					AI-generated summary 				 Large Language Models (LLMs) and Vision-Language Models (VLMs) have demonstr...
[04.02.2026 16:38] ********************************************************************************
[04.02.2026 16:38] Abstract 32. SafeGround is a uncertainty-aware framework for GUI grounding models that uses distribution-aware uncertainty quantification and calibration to enable risk-aware predictions with controlled false discovery rates.  					AI-generated summary 				 Graphical User Interface (GUI) grounding aims to transl...
[04.02.2026 16:38] ********************************************************************************
[04.02.2026 16:38] Abstract 33. ObjEmbed is a novel multimodal language-model embedding approach that decomposes images into regional embeddings for improved object-level visual understanding and retrieval tasks.  					AI-generated summary 				 Aligning objects with corresponding textual descriptions is a fundamental challenge and...
[04.02.2026 16:38] ********************************************************************************
[04.02.2026 16:38] Abstract 34. Advanced AI models demonstrate capability in supporting expert-level mathematical discovery and scientific research through collaborative approaches involving proof verification and automated code execution.  					AI-generated summary 				 Recent advances in large language models (LLMs) have opened ...
[04.02.2026 16:38] ********************************************************************************
[04.02.2026 16:38] Abstract 35. CoViP addresses contextualized visual personalization by treating personalized image captioning as a core task and improving capabilities through reinforcement-learning-based post-training and caption-augmented generation.  					AI-generated summary 				 Despite recent progress in vision-language mo...
[04.02.2026 16:38] ********************************************************************************
[04.02.2026 16:38] Abstract 36. FaceLinkGen attack demonstrates that current privacy-preserving face recognition methods fail to protect identity information despite pixel-level distortion metrics suggesting adequate protection.  					AI-generated summary 				 Transformation-based privacy-preserving face recognition (PPFR) aims to...
[04.02.2026 16:38] ********************************************************************************
[04.02.2026 16:38] Abstract 37. SimpleNorm normalization strategy stabilizes activation scales and enables larger stable learning rates in Transformer models by reducing Hessian spectral norm, leading to improved training performance.  					AI-generated summary 				 In this work, we revisit Transformer optimization through the len...
[04.02.2026 16:38] ********************************************************************************
[04.02.2026 16:38] Abstract 38. Large Language Models have advanced general-purpose agents, but current evaluation benchmarks suffer from confounding factors and lack of standardization, necessitating a unified framework for rigorous assessment.  					AI-generated summary 				 With the advent of Large Language Models (LLMs), gener...
[04.02.2026 16:38] ********************************************************************************
[04.02.2026 16:38] Abstract 39. MEG-XL demonstrates improved brain-to-text decoding performance through extended pre-training with 2.5-minute MEG context, significantly outperforming previous models with less contextual data.  					AI-generated summary 				 Clinical brain-to-text interfaces are designed for paralysed patients who ...
[04.02.2026 16:38] ********************************************************************************
[04.02.2026 16:38] Abstract 40. Distribution Aligned Imitation Learning (DAIL) improves LLM reasoning by transforming expert solutions into in-distribution traces and using contrastive learning to focus on expert methodologies, achieving significant performance gains with minimal expert data.  					AI-generated summary 				 Improv...
[04.02.2026 16:38] ********************************************************************************
[04.02.2026 16:38] Abstract 41. HieraNav presents a multi-granularity, open-vocabulary navigation task with LangMap benchmark that enables agents to follow natural language instructions across different semantic levels in 3D environments.  					AI-generated summary 				 The relationships between objects and language are fundamenta...
[04.02.2026 16:38] ********************************************************************************
[04.02.2026 16:38] Abstract 42. High-quality feedback is essential for effective human-AI interaction. It bridges knowledge gaps, corrects digressions, and shapes system behavior; both during interaction and throughout model development. Yet despite its importance, human feedback to AI is often infrequent and low quality. This gap...
[04.02.2026 16:38] ********************************************************************************
[04.02.2026 16:38] Abstract 43. A novel dual semantic alignment framework for LLM-enhanced multimodal recommendation that addresses representational divergence between large models and recommendation systems through graph attention networks and cross-modal contrastive learning.  					AI-generated summary 				 Multimodal recommenda...
[04.02.2026 16:38] ********************************************************************************
[04.02.2026 16:38] Abstract 44. A fusion framework called FINCH combines audio and spatiotemporal predictors for bioacoustic classification by adaptively weighting evidence based on reliability estimates, outperforming fixed-weight methods and audio-only approaches.  					AI-generated summary 				 Many machine learning systems hav...
[04.02.2026 16:38] ********************************************************************************
[04.02.2026 16:38] Abstract 45. MedSAM-Agent reformulates medical image segmentation as a multi-step decision-making process using hybrid prompting and a two-stage training pipeline with process rewards to improve autonomous reasoning and optimization.  					AI-generated summary 				 Medical image segmentation is evolving from tas...
[04.02.2026 16:38] ********************************************************************************
[04.02.2026 16:38] Abstract 46. Small language models struggle with complex tasks but can be effectively coordinated through a marketplace-inspired framework that reduces costs and improves performance through strategic bidding and self-improvement mechanisms.  					AI-generated summary 				 Small language models are increasingly ...
[04.02.2026 16:38] ********************************************************************************
[04.02.2026 16:38] Abstract 47. Native position-independent caching enhances LLM inference efficiency by reintroducing encoders and developing a caching system that reduces latency while maintaining accuracy.  					AI-generated summary 				 The Key-Value (KV) cache of Large Language Models (LLMs) is prefix-based, making it highly ...
[04.02.2026 16:38] Read previous papers.
[04.02.2026 16:38] Generating reviews via LLM API.
[04.02.2026 16:38] Using data from previous issue: {"categories": ["#plp", "#multimodal", "#inference"], "emoji": "üñºÔ∏è", "ru": {"title": "–ö–æ–¥ –∫–∞–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: —Å–∂–∞—Ç–∏–µ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å", "desc": "–í —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –º–Ω–æ–≥–æ–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (MLLM) –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –∫–æ–¥–∞, –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –≤ –≤–∏–¥–µ —Å–∂–∞—Ç—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤–º–µ—Å—Ç–æ —Ç
[04.02.2026 16:38] Using data from previous issue: {"categories": [], "emoji": "üéº", "ru": {"title": "–î–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ü–∏—è –∞–≥–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ –∫–æ—Ä—Ç–µ–∂–Ω—É—é –∞–±—Å—Ç—Ä–∞–∫—Ü–∏—é", "desc": "AOrchestra ‚Äî —ç—Ç–æ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º, –∫–æ—Ç–æ—Ä–∞—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∞–±—Å—Ç—Ä–∞–∫—Ü–∏—é –≤ –≤–∏–¥–µ –∫–æ—Ä—Ç–µ–∂–∞ (–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è, –∫–æ–Ω—Ç–µ–∫—Å—Ç, –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã, –º–æ–¥–µ–ª—å) –¥–ª—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ —Å–æ–∑–¥–∞–Ω–∏—è —Å–ø
[04.02.2026 16:38] Using data from previous issue: {"categories": ["#reasoning", "#training", "#open_source", "#interpretability", "#benchmark"], "emoji": "üîç", "ru": {"title": "–°–∫—Ä—ã—Ç–æ–µ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ LLM: –ª–æ–∫–∞–ª—å–Ω–æ—Å—Ç—å –≤–º–µ—Å—Ç–æ –≥–ª–æ–±–∞–ª—å–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏", "desc": "–†–∞–±–æ—Ç–∞ –∏—Å—Å–ª–µ–¥—É–µ—Ç —Å–∫—Ä—ã—Ç—ã–µ –º–µ—Ö–∞–Ω–∏–∑–º—ã –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö, –∏—Å–ø–æ–ª—å–∑—É—è –º–µ—Ç–æ–¥ –∑–æ–Ω
[04.02.2026 16:38] Using data from previous issue: {"categories": ["#training", "#synthetic", "#alignment", "#data", "#long_context", "#reasoning", "#agents"], "emoji": "üîÑ", "ru": {"title": "–û—Ç –∫–æ–¥–∞ –∫ –∞–≥–µ–Ω—Ç–∞–º: —Å–∏–Ω—Ç–µ–∑ –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏–∑ –∏—Å—Ç–æ—Ä–∏–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏", "desc": "–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã—Ö –∞
[04.02.2026 16:38] Using data from previous issue: {"categories": [], "emoji": "üåç", "ru": {"title": "–£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–∏—Ä–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ü–µ–ª–æ—Å—Ç–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –æ–∫—Ä—É–∂–∞—é—â–µ–π —Å—Ä–µ–¥—ã", "desc": "–í —Å—Ç–∞—Ç—å–µ –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç—Å—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞ –∫ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ –º–∏—Ä–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–µ —Å–µ–≥–æ–¥–Ω—è —Ä–∞–∑—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç—Å—è –¥–ª—è –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á –ø—Ä–µ–¥—Å–∫–∞–∑–∞
[04.02.2026 16:38] Using data from previous issue: {"categories": ["#video", "#training", "#architecture", "#multimodal", "#optimization", "#3d"], "emoji": "üé¨", "ru": {"title": "–ù–µ—è–≤–Ω–æ–µ 3D –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –¥–≤–∏–∂–µ–Ω–∏—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç —É–≥–ª–∞ –æ–±–∑–æ—Ä–∞", "desc": "3DiMo ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥ –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–≤–∏–∂–µ–Ω–∏–µ–º —á–µ–ª–æ–≤–µ–∫–∞ –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ, –∫–æ—Ç–æ—Ä—ã–π –æ–±—É
[04.02.2026 16:38] Using data from previous issue: {"categories": ["#training", "#agents", "#open_source", "#science", "#benchmark"], "emoji": "üî¨", "ru": {"title": "–ú–æ–¥—É–ª—å–Ω—ã–π –∞–≥–µ–Ω—Ç —Å —Ä–µ—Ñ–ª–µ–∫—Å–∏–≤–Ω–æ–π –ø–∞–º—è—Ç—å—é –¥–ª—è —ç–∫–æ–Ω–æ–º–Ω–æ–≥–æ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ ML-–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è", "desc": "MARS ‚Äî —ç—Ç–æ –º–æ–¥—É–ª—å–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –≤ –æ–±–ª–∞—Å—Ç–∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É
[04.02.2026 16:38] Using data from previous issue: {"categories": ["#reasoning", "#rl", "#optimization", "#training"], "emoji": "‚öñÔ∏è", "ru": {"title": "–£–º–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "CoBA-RL ‚Äî —ç—Ç–æ –∞–ª–≥–æ—Ä–∏—Ç–º –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –∫–æ—Ç–æ—Ä—ã–π –∞–¥–∞–ø—Ç–∏–≤–Ω–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—è–µ—Ç –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã
[04.02.2026 16:38] Using data from previous issue: {"categories": ["#multimodal", "#training", "#inference", "#diffusion", "#optimization"], "emoji": "üé®", "ru": {"title": "–î–∏—Å—Ç–∏–ª–ª—è—Ü–∏—è —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ DP-DMD –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ç–µ–∫—Å—Ç—É, –∫–æ—Ç–æ—Ä
[04.02.2026 16:38] Using data from previous issue: {"categories": ["#training", "#benchmark", "#rl", "#plp", "#agents"], "emoji": "üöÄ", "ru": {"title": "–í–∏—Ä—Ç—É–∞–ª—å–Ω–æ–µ –æ–∫—Ä—É–∂–µ–Ω–∏–µ –≤–º–µ—Å—Ç–æ Docker –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤ –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–π –∏–Ω–∂–µ–Ω–µ—Ä–∏–∏", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω SWE-World ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –±–µ–∑ Docker, –∫–æ—Ç–æ—Ä—ã–π –∑–∞–º–µ–Ω—è–µ—Ç —Ñ–∏–∑–∏—á–µ—Å–∫–∏–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –Ω–∞ –æ–±—É—á
[04.02.2026 16:38] Using data from previous issue: {"categories": ["#open_source", "#training", "#benchmark", "#long_context", "#rl", "#plp", "#reasoning", "#agents", "#optimization"], "emoji": "üõ†Ô∏è", "ru": {"title": "–°–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∞–≥–µ–Ω—Ç–æ–≤ –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–π –∏–Ω–∂–µ–Ω–µ—Ä–∏–∏", "desc": "SWE-Master –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –æ—Ç–∫—Ä—ã—Ç—É—é –∏ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏
[04.02.2026 16:38] Using data from previous issue: {"categories": ["#reasoning", "#optimization"], "emoji": "üå≥", "ru": {"title": "–£–º–Ω–∞—è –æ–±—Ä–µ–∑–∫–∞ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —á–µ—Ä–µ–∑ –∫–æ–Ω—Å–µ–Ω—Å—É—Å –∏ –¥–µ–≤–∏–∞—Ü–∏—é", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω Parallel-Probe ‚Äî –∫–æ–Ω—Ç—Ä–æ–ª–ª–µ—Ä –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö. –ê–≤—Ç–æ—Ä—ã –≤–≤–æ–¥—è—Ç 2D-
[04.02.2026 16:38] Using data from previous issue: {"categories": ["#rlhf", "#benchmark", "#dataset", "#agents"], "emoji": "üìã", "ru": {"title": "–í—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–æ–≤ –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤ –æ—Ü–µ–Ω–∫–∏ —Å —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º–∏ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ—Ç—á—ë—Ç–æ–≤", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–æ–≤ –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤ –æ—Ü–µ–Ω–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –≤—ã—Ä–∞–≤–Ω–µ–Ω—ã —Å –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º–∏ —á
[04.02.2026 16:38] Using data from previous issue: {"categories": ["#video", "#reasoning", "#multimodal", "#alignment", "#cv", "#rlhf"], "emoji": "üé®", "ru": {"title": "–ì–∏–±–∫–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–π –∞–¥–∞–ø—Ç–∞—Ü–∏–µ–π –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ UnifiedReward-Flex ‚Äî —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –≤–æ–∑–Ω–∞–≥—Ä–∞
[04.02.2026 16:38] Using data from previous issue: {"categories": ["#training", "#benchmark", "#video", "#reasoning", "#synthetic", "#multimodal"], "emoji": "üé¨", "ru": {"title": "–£–º–Ω—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤–∏–¥–µ–æ ‚Äî –ø–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–æ–≤–æ–≥–æ –ø–æ–∫–æ–ª–µ–Ω–∏—è", "desc": "RANKVIDEO ‚Äî —ç—Ç–æ —Å–∏—Å—Ç–µ–º–∞ –ø–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è –≤–∏–¥–µ–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä–∞—è –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç 
[04.02.2026 16:38] Using data from previous issue: {"categories": ["#training", "#agents", "#rl", "#benchmark", "#dataset"], "emoji": "üîç", "ru": {"title": "–®–∏—Ä–æ–∫–∏–π –ø–æ–∏—Å–∫: –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Wide Research ‚Äî –Ω–æ–≤—É—é –ø–∞—Ä–∞–¥–∏–≥–º—É –ø–æ–∏—Å–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –∏–∑
[04.02.2026 16:38] Using data from previous issue: {"categories": ["#reasoning", "#rl", "#optimization", "#training"], "emoji": "üßπ", "ru": {"title": "–û—á–∏—Å—Ç–∫–∞ –ø—Ä–æ–º–ø—Ç–æ–≤ –æ—Ç –ø–æ–º–µ—Ö –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ LENS - —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º —Å –≤–µ—Ä–∏—Ñ–∏—Ü–∏—Ä—É–µ–º—ã–º–∏ –Ω–∞–≥—Ä–∞–¥–∞–º–∏, –∫–æ—Ç–æ—Ä—ã–π —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É 
[04.02.2026 16:38] Using data from previous issue: {"categories": ["#open_source", "#diffusion", "#optimization"], "emoji": "üîÄ", "ru": {"title": "–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ø–∞—Ä–∞–¥–∏–≥–º: –µ–¥–∏–Ω–∞—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω XDLM ‚Äî –µ–¥–∏–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∏–π –¥–≤–µ –∫–æ–Ω–∫—É—Ä–∏—Ä—É—é—â–∏–µ –ø–∞—Ä–∞–¥–∏–≥–º—ã –¥–∏—Å–∫—Ä–µ—Ç–Ω–æ–≥–æ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ
[04.02.2026 16:38] Using data from previous issue: {"categories": ["#benchmark", "#dataset", "#agents", "#training", "#plp"], "emoji": "üèóÔ∏è", "ru": {"title": "–ü–æ–ª–Ω–æ—Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –∞–≥–µ–Ω—Ç –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –≤–µ–±-–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ FullStack-Agent - —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–º–æ–≥–∞–µ—Ç –Ω–µ–ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–º —Ä–∞–∑—Ä–∞
[04.02.2026 16:38] Using data from previous issue: {"categories": ["#inference", "#long_context", "#architecture", "#optimization"], "emoji": "‚ö°", "ru": {"title": "–î–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è —Å–ø–∞—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ inference –Ω–∞ –¥–ª–∏–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞—Ö", "desc": "–†–∞–±–æ—Ç–∞ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç Token Sparse Attention ‚Äî –º–µ—Ö–∞–Ω–∏–∑–º –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π —Å–ø–∞—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏ –≤–Ω–∏–º–∞–Ω–∏—è –Ω–∞ —É—Ä
[04.02.2026 16:38] Using data from previous issue: {"categories": ["#architecture", "#rl", "#training"], "emoji": "üîÑ", "ru": {"title": "–ù–µ–π—Ä–æ—Å–µ—Ç—å –≤–º–µ—Å—Ç–æ —ç–≤—Ä–∏—Å—Ç–∏–∫: —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–æ–º–æ—Ç–æ–ø–∏—á–µ—Å–∫–∏–º –º–µ—Ç–æ–¥–∞–º", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è Neural Predictor-Corrector (NPC) - –µ–¥–∏–Ω–∞—è –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤–∞—èÊ°ÜÊû∂–¥–ª—è —Ä–µ—à–µ–Ω–∏—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏, –æ—Å–Ω–æ–≤–∞–Ω–Ω–∞—è
[04.02.2026 16:38] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#cv"], "emoji": "üß†", "ru": {"title": "–£–º–Ω–æ–µ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ: –æ—Ü–µ–Ω–∫–∞ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –≤—ã–±–æ—Ä–∞ —Ä–µ–∂–∏–º–æ–≤ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "AdaptMMBench ‚Äî —ç—Ç–æ –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ Vision-Language Mod
[04.02.2026 16:38] Using data from previous issue: {"categories": ["#training", "#optimization", "#data", "#open_source", "#dataset"], "emoji": "üß©", "ru": {"title": "–ü–æ–∏—Å–∫ –∏–¥–µ–∞–ª—å–Ω–æ–π —Å–º–µ—Å–∏ –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ —Å–ª–∏—è–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è", "desc": "DeMix ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Å–º–µ—Å–∏ –¥–∞–Ω–Ω—ã—Ö –ø—Ä–∏ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏ LLM, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –æ–±—ä–µ–¥–∏–Ω–µ
[04.02.2026 16:38] Using data from previous issue: {"categories": ["#training", "#benchmark", "#diffusion", "#long_context", "#optimization", "#video"], "emoji": "üîÑ", "ru": {"title": "–¶–∏–∫–ª–∏—á–Ω–æ—Å—Ç—å –ø—Ä–æ—Ç–∏–≤ –æ—à–∏–±–æ–∫: –≤–∏–¥–µ–æ–º–æ–¥–µ–ª—å –¥–ª—è –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–≥–æ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è", "desc": "LIVE ‚Äî —ç—Ç–æ –≤–∏–¥–µ–æ–º–æ–¥–µ–ª—å –º–∏—Ä–∞ –¥–ª—è –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä–∞—è —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É
[04.02.2026 16:38] Using data from previous issue: {"categories": ["#reasoning", "#dataset", "#benchmark", "#low_resource", "#open_source", "#multilingual"], "emoji": "üåè", "ru": {"title": "–ú–Ω–æ–≥–æ—Ö–æ–¥–æ–≤–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –¥–ª—è –≥–ª—É–±–æ–∫–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∫—É–ª—å—Ç—É—Ä—ã –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω ID-MoCQA ‚Äî –ø–µ—Ä–≤—ã–π –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç —Å –º–Ω–æ–≥–æ—Ö–æ–¥
[04.02.2026 16:38] Using data from previous issue: {"categories": ["#multimodal", "#interpretability", "#architecture"], "emoji": "üß≠", "ru": {"title": "–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –∫–∞–∫ —è–∫–æ—Ä—è: –∫–∞–∫ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≤—ã–±–∏—Ä–∞—é—Ç –Ω—É–∂–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ", "desc": "–í —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –º–µ—Ö–∞–Ω–∏–∑–º —Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç—è–º –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (MLLM) —á–µ—Ä–µ–∑ –∞–Ω–∞–ª–∏–∑ 
[04.02.2026 16:38] Using data from previous issue: {"categories": ["#reasoning", "#training", "#optimization", "#agents", "#rl", "#rag"], "emoji": "üîç", "ru": {"title": "–î–≤—É—Ö—É—Ä–æ–≤–Ω–µ–≤–∞—è —Ä–µ—Ñ–∏–Ω–∏—Ä–æ–≤–∫–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫—Ä–µ–¥–∏—Ç–∞ –≤ –∞–≥–µ–Ω—Ç–∞—Ö –ø–æ–∏—Å–∫–∞", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ Search-R2, –∫–æ—Ç–æ—Ä–∞—è —É–ª—É—á—à–∞–µ—Ç —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –∞
[04.02.2026 16:38] Using data from previous issue: {"categories": ["#agents", "#inference", "#architecture"], "emoji": "‚ö°", "ru": {"title": "–£–º–Ω–æ–µ —Å–æ–≤–º–µ—Å—Ç–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∫—ç—à–∞ –¥–ª—è –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º —Å LoRA –∞–¥–∞–ø—Ç–µ—Ä–∞–º–∏", "desc": "LRAgent ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è KV –∫—ç—à–∞ –≤ —Å–∏—Å—Ç–µ–º–∞—Ö —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –∞–≥–µ–Ω—Ç–∞–º–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ LoRA –∞–¥–∞–ø—Ç–µ—Ä–æ–≤
[04.02.2026 16:38] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#cv"], "emoji": "üåç", "ru": {"title": "–ò–∑–º–µ—Ä–µ–Ω–∏–µ —Ç–æ–≥–æ, —á—Ç–æ –º–æ–¥–µ–ª—å –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –∑–∞–ø–æ–º–Ω–∏–ª–∞ –æ –≤–∏–∑—É–∞–ª—å–Ω–æ–º –º–∏—Ä–µ", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç WorldVQA ‚Äî –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –∑–Ω–∞–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ì–ª–∞–≤–Ω–∞—è –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç—å –∑–∞–∫–ª
[04.02.2026 16:38] Using data from previous issue: {"categories": ["#rl", "#cv", "#healthcare"], "emoji": "üîç", "ru": {"title": "–î–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–π —Å–∫—Ä–∏–Ω–∏–Ω–≥ —Ä–∞–∫–∞ —á–µ—Ä–µ–∑ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –∏ –≥—Ä—É–ø–ø–æ–≤–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ GF-Screen –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —Å–∫—Ä–∏–Ω–∏–Ω–≥–∞ —Ä–∞–∫–∞ –ø–æ –ö–¢-—Å–Ω–∏–º–∫–∞–º, –∫–æ—Ç–æ—Ä–∞—è —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –¥–∏—Å–±–∞–ª–∞–Ω—Å
[04.02.2026 16:38] Using data from previous issue: {"categories": ["#training", "#rl", "#plp"], "emoji": "üíª", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ LLM —á–µ—Ä–µ–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ –±–∞–Ω–¥–∏—Ç—ã –∏ –æ—Ñ—Ñ–ª–∞–π–Ω —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –º–µ—Ç–æ–¥ Cobalt, –∫–æ—Ç–æ—Ä—ã–π –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –±–∞–Ω–¥–∏—Ç –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º–Ω–æ–≥–æ—à–∞–≥–æ–≤–æ–≥–æ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–∏
[04.02.2026 16:38] Using data from previous issue: {"categories": ["#inference", "#optimization", "#architecture", "#multimodal"], "emoji": "‚ö°", "ru": {"title": "–ò–∑–±–∏—Ä–∞—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–µ–∑–∫–∞ —Å–ª–æ–µ–≤ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —ç—Ç–∞–ø–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–æ–π –æ–±—Ä–µ–∑–∫–∏ –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–π —É—á–∏—Ç—ã–≤–∞–µ—Ç 
[04.02.2026 16:38] Using data from previous issue: {"categories": ["#benchmark", "#inference", "#agents", "#security"], "emoji": "üõ°Ô∏è", "ru": {"title": "–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–º —á–µ—Ä–µ–∑ –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º—É—é –Ω–µ–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ—Å—Ç—å", "desc": "SafeGround ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –º–æ–¥–µ–ª–µ–π –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏—Ö –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–≤, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–µ—Ç–æ–¥—ã –∫–æ–ª–∏
[04.02.2026 16:38] Using data from previous issue: {"categories": ["#interpretability", "#multimodal", "#benchmark", "#cv"], "emoji": "üéØ", "ru": {"title": "–û–±—ä–µ–∫—Ç–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –¥–µ—Ç–∞–ª–µ–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "ObjEmbed ‚Äî —ç—Ç–æ –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –≤—Å—Ç—Ä–∞–∏–≤–∞–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏, –∫–æ—Ç–æ—Ä–∞—è —Ä–∞–∑–±–∏–≤–∞–µ—Ç –∏–∑–æ–±
[04.02.2026 16:38] Using data from previous issue: {"categories": ["#reasoning", "#science", "#transfer_learning"], "emoji": "ü§ù", "ru": {"title": "AI –∫–∞–∫ –Ω–∞—É—á–Ω—ã–π –ø–∞—Ä—Ç–Ω—ë—Ä: –æ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –∫ –ø–æ–¥–ª–∏–Ω–Ω–æ–º—É —Å–æ—Ç–≤–æ—Ä—á–µ—Å—Ç–≤—É –≤ –º–∞—Ç–µ–º–∞—Ç–∏–∫–µ", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã –ø—Ä–∏–º–µ—Ä—ã —É—Å–ø–µ—à–Ω–æ–≥–æ —Å–æ—Ç—Ä—É–¥–Ω–∏—á–µ—Å—Ç–≤–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–µ–π —Å –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–º–∏ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏, —Ç–∞–∫–∏–º–∏ –∫
[04.02.2026 16:38] Using data from previous issue: {"categories": ["#rl", "#multimodal", "#benchmark", "#cv"], "emoji": "üì∏", "ru": {"title": "–ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏—è –∑—Ä–µ–Ω–∏—è —á–µ—Ä–µ–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π vision-language", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ CoViP –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –≤–∏–∑—É–∞–ª—å–Ω–æ–π –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏ –º–æ–¥–µ–ª–µ–π –∑—Ä–µ–Ω–∏—è-—è–∑—ã–∫–∞. –û—Å–Ω–æ–≤–Ω–∞—è –∏–¥–µ—è –∑–∞
[04.02.2026 16:38] Using data from previous issue: {"categories": ["#ethics", "#benchmark", "#cv", "#security"], "emoji": "üîì", "ru": {"title": "–ü–∏–∫—Å–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –Ω–µ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É—é—Ç –ø—Ä–∏–≤–∞—Ç–Ω–æ—Å—Ç—å –ª–∏—Ü", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –∞—Ç–∞–∫–∞ FaceLinkGen, –∫–æ—Ç–æ—Ä–∞—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ–∫ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤ –ø—Ä–∏–≤–∞—Ç–Ω–æ–π —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –ª–∏—Ü - –æ–Ω–∏ –Ω–µ –∑–∞—â
[04.02.2026 16:38] Using data from previous issue: {"categories": ["#training", "#open_source", "#architecture", "#optimization", "#math"], "emoji": "üìà", "ru": {"title": "–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —á–µ—Ä–µ–∑ —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑: –ø—É—Ç—å –∫ —Å—Ç–∞–±–∏–ª—å–Ω–æ–º—É –∏ –±—ã—Å—Ç—Ä–æ–º—É –æ–±—É—á–µ–Ω–∏—é –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –≤–≤–æ–¥–∏—Ç SimpleNorm ‚Äî –ø—Ä–æ—Å—Ç—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏, –∫–æ—Ç–æ—Ä–∞—è —Å—Ç–∞–±–∏–ª–∏
[04.02.2026 16:38] Using data from previous issue: {"categories": ["#agents", "#reasoning", "#benchmark"], "emoji": "‚öñÔ∏è", "ru": {"title": "–ï–¥–∏–Ω—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è —á–µ—Å—Ç–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM", "desc": "–í —Ä–∞–±–æ—Ç–µ –æ–±—Å—É–∂–¥–∞–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ –æ—Ü–µ–Ω–∫–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –∞–≤—Ç–æ–Ω–æ–º–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã –æ—Ç–º–µ—á–∞—é—Ç, —á—Ç–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –±–µ–Ω—á–º
[04.02.2026 16:38] Using data from previous issue: {"categories": ["#science", "#transfer_learning", "#long_context", "#open_source"], "emoji": "üß†", "ru": {"title": "–î–æ–ª–≥–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç ‚Äî –∫–ª—é—á –∫ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–º—É –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—é –º–æ–∑–≥–∞", "desc": "MEG-XL ‚Äî —ç—Ç–æ –º–æ–¥–µ–ª—å –¥–ª—è –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–µ—á–∏ –∏–∑ —Å–∏–≥–Ω–∞–ª–æ–≤ –º–æ–∑–≥–∞, –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω–∞—è –Ω–∞ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –≤ 2.
[04.02.2026 16:38] Using data from previous issue: {"categories": ["#rlhf", "#training", "#optimization", "#reasoning"], "emoji": "üéØ", "ru": {"title": "–í—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –Ω–∞ —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–∞—Ö", "desc": "–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç –º–µ—Ç–æ–¥ Distribution Aligned Imitation Learning (DAIL), –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –±–æ–ª—å
[04.02.2026 16:38] Using data from previous issue: {"categories": ["#3d", "#multimodal", "#benchmark", "#dataset", "#agents", "#open_source"], "emoji": "üó∫Ô∏è", "ru": {"title": "–ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è –Ω–∞–≤–∏–≥–∞—Ü–∏—è –∞–≥–µ–Ω—Ç–æ–≤ –ø–æ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–º—É —è–∑—ã–∫—É –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö 3D-—Å—Ü–µ–Ω–∞—Ö", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –∑–∞–¥–∞—á–∞ HieraNav –¥–ª—è –Ω–∞–≤–∏–≥–∞—Ü–∏–∏ –∞–≥–µ–Ω—Ç–æ–≤ –≤ 3D-–æ–∫—Ä—É–∂–µ–Ω–∏—è—Ö –Ω–∞ –æ—Å–Ω–æ–≤–µ –µ—Å—Ç
[04.02.2026 16:38] Querying the API.
[04.02.2026 16:38] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

High-quality feedback is essential for effective human-AI interaction. It bridges knowledge gaps, corrects digressions, and shapes system behavior; both during interaction and throughout model development. Yet despite its importance, human feedback to AI is often infrequent and low quality. This gap motivates a critical examination of human feedback during interactions with AIs. To understand and overcome the challenges preventing users from giving high-quality feedback, we conducted two studies examining feedback dynamics between humans and conversational agents (CAs). Our formative study, through the lens of Grice's maxims, identified four Feedback Barriers -- Common Ground, Verifiability, Communication, and Informativeness -- that prevent high-quality feedback by users. Building on these findings, we derive three design desiderata and show that systems incorporating scaffolds aligned with these desiderata enabled users to provide higher-quality feedback. Finally, we detail a call for action to the broader AI community for advances in Large Language Models capabilities to overcome Feedback Barriers.
[04.02.2026 16:39] Error getting data: Error code: 500 - {'type': 'error', 'error': {'type': 'api_error', 'message': 'Internal server error'}, 'request_id': 'req_011CXoMjpXX7B4mVJePiosmo'}
[04.02.2026 16:39] Using data from previous issue: {"categories": ["#benchmark", "#multimodal"], "emoji": "üéØ", "ru": {"title": "–ì–∞—Ä–º–æ–Ω–∏–∑–∞—Ü–∏—è –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π —á–µ—Ä–µ–∑ –¥–≤–æ–π–Ω–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏–∫–∏", "desc": "–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ RecGOAT ‚Äî –Ω–æ–≤–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —Å–∏—Å—Ç–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç
[04.02.2026 16:39] Using data from previous issue: {"categories": ["#audio", "#multimodal", "#benchmark"], "emoji": "üê¶", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ —Å–ª–∏—è–Ω–∏–µ –∑–≤—É–∫–∞ –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Å –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–π –Ω–∞–¥—ë–∂–Ω–æ—Å—Ç—å—é", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç FINCH ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ —Å–ª–∏—è–Ω–∏—è –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –±–∏–æ–∞–∫—É—Å—Ç–∏—á–µ—Å–∫–∏—Ö —Å–∏–≥–Ω–∞–ª–æ–≤, –∫–æ—Ç–æ—Ä—ã–π –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –ø
[04.02.2026 16:39] Using data from previous issue: {"categories": ["#optimization", "#healthcare", "#rl", "#cv", "#rlhf", "#science", "#training", "#agents", "#reasoning", "#open_source"], "emoji": "üè•", "ru": {"title": "–ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –∞–≥–µ–Ω—Ç –¥–ª—è –ø–æ—à–∞–≥–æ–≤–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "MedSAM-Agent –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä—É–µ—Ç –∑–∞–¥–∞—á—É —Å–µ–≥–º–µ–Ω
[04.02.2026 16:39] Querying the API.
[04.02.2026 16:39] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Small language models struggle with complex tasks but can be effectively coordinated through a marketplace-inspired framework that reduces costs and improves performance through strategic bidding and self-improvement mechanisms.  					AI-generated summary 				 Small language models are increasingly viewed as a promising, cost-effective approach to agentic AI, with proponents claiming they are sufficiently capable for agentic workflows. However, while smaller agents can closely match larger ones on simple tasks, it remains unclear how their performance scales with task complexity, when large models become necessary, and how to better leverage small agents for long-horizon workloads. In this work, we empirically show that small agents' performance fails to scale with task complexity on deep search and coding tasks, and we introduce Strategy Auctions for Workload Efficiency (SALE), an agent framework inspired by freelancer marketplaces. In SALE, agents bid with short strategic plans, which are scored by a systematic cost-value mechanism and refined via a shared auction memory, enabling per-task routing and continual self-improvement without training a separate router or running all models to completion. Across deep search and coding tasks of varying complexity, SALE reduces reliance on the largest agent by 53%, lowers overall cost by 35%, and consistently improves upon the largest agent's pass@1 with only a negligible overhead beyond executing the final trace. In contrast, established routers that rely on task descriptions either underperform the largest agent or fail to reduce cost -- often both -- underscoring their poor fit for agentic workflows. These results suggest that while small agents may be insufficient for complex workloads, they can be effectively "scaled up" through coordinated task allocation and test-time self-improvement. More broadly, they motivate a systems-level view of agentic AI in which performance gains come less from ever-larger individual models and more from market-inspired coordination mechanisms that organize heterogeneous agents into efficient, adaptive ecosystems.
[04.02.2026 16:39] Error getting data: Error code: 500 - {'type': 'error', 'error': {'type': 'api_error', 'message': 'Internal server error'}, 'request_id': 'req_011CXoMkCHDWPTen3iVtx2GG'}
[04.02.2026 16:39] Using data from previous issue: {"categories": [], "emoji": "‚ö°", "ru": {"title": "–ü–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ-–Ω–µ–∑–∞–≤–∏—Å–∏–º–æ–µ –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –º–µ—Ç–æ–¥ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ-–Ω–µ–∑–∞–≤–∏—Å–∏–º–æ–≥–æ –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è (PIC) –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Key-Value –∫–µ—à –±–µ–∑ —É—á
[04.02.2026 16:39] Renaming data file.
[04.02.2026 16:39] Renaming previous data. hf_papers.json to ./d/2026-02-04.json
[04.02.2026 16:39] Saving new data file.
[04.02.2026 16:39] Generating page.
[04.02.2026 16:39] Renaming previous page.
[04.02.2026 16:39] Renaming previous data. index.html to ./d/2026-02-04.html
[04.02.2026 16:39] Writing result.
[04.02.2026 16:39] Renaming log file.
[04.02.2026 16:39] Renaming previous data. log.txt to ./logs/2026-02-04_last_log.txt
