[04.02.2026 01:17] Read previous papers.
[04.02.2026 01:17] Generating top page (month).
[04.02.2026 01:17] Writing top page (month).
[04.02.2026 04:06] Read previous papers.
[04.02.2026 04:06] Get feed.
[04.02.2026 04:06] Extract page data from URL. URL: https://huggingface.co/papers/2602.01785
[04.02.2026 04:06] Extract page data from URL. URL: https://huggingface.co/papers/2602.01630
[04.02.2026 04:06] Extract page data from URL. URL: https://huggingface.co/papers/2602.03619
[04.02.2026 04:06] Extract page data from URL. URL: https://huggingface.co/papers/2602.02380
[04.02.2026 04:06] Extract page data from URL. URL: https://huggingface.co/papers/2602.03845
[04.02.2026 04:06] Extract page data from URL. URL: https://huggingface.co/papers/2602.02676
[04.02.2026 04:06] Extract page data from URL. URL: https://huggingface.co/papers/2602.02636
[04.02.2026 04:06] Extract page data from URL. URL: https://huggingface.co/papers/2601.21244
[04.02.2026 04:06] Extract page data from URL. URL: https://huggingface.co/papers/2602.02103
[04.02.2026 04:06] Extract page data from URL. URL: https://huggingface.co/papers/2602.00747
[04.02.2026 04:06] Extract page data from URL. URL: https://huggingface.co/papers/2602.01362
[04.02.2026 04:06] Extract page data from URL. URL: https://huggingface.co/papers/2601.19103
[04.02.2026 04:06] Extract page data from URL. URL: https://huggingface.co/papers/2602.03454
[04.02.2026 04:06] Extract page data from URL. URL: https://huggingface.co/papers/2602.03048
[04.02.2026 04:06] Extract page data from URL. URL: https://huggingface.co/papers/2602.02914
[04.02.2026 04:06] Extract page data from URL. URL: https://huggingface.co/papers/2602.02537
[04.02.2026 04:06] Extract page data from URL. URL: https://huggingface.co/papers/2602.03837
[04.02.2026 04:06] Extract page data from URL. URL: https://huggingface.co/papers/2602.03796
[04.02.2026 04:06] Extract page data from URL. URL: https://huggingface.co/papers/2602.01753
[04.02.2026 04:06] Extract page data from URL. URL: https://huggingface.co/papers/2602.03817
[04.02.2026 04:06] Extract page data from URL. URL: https://huggingface.co/papers/2602.03647
[04.02.2026 04:06] Extract page data from URL. URL: https://huggingface.co/papers/2602.02660
[04.02.2026 04:06] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[04.02.2026 04:06] Downloading and parsing papers (pdf, html). Total: 22.
[04.02.2026 04:06] Downloading and parsing paper https://huggingface.co/papers/2602.01785.
[04.02.2026 04:06] Downloading paper 2602.01785 from https://arxiv.org/pdf/2602.01785v1...
[04.02.2026 04:07] Extracting affiliations from text.
[04.02.2026 04:07] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 2 ] . [ 1 5 8 7 1 0 . 2 0 6 2 : r CodeOCR: On the Effectiveness of Vision Language Models in Code Understanding Yuling Shi1, Chaoxiang Xie2, Zhensu Sun3, Yeheng Chen4, Chenxu Zhang5, Longfei Yun6, Chengcheng Wan7,8, Hongyu Zhang9, David Lo3, Xiaodong Gu1 1Shanghai Jiao Tong University, 2Hohai University, 3Singapore Management University, 4Beijing Institute of Technology, Zhuhai, 5Imperial College London, 6UC San Diego, 7East China Normal University, 8Shanghai Innovation Institute, 9Chongqing University "
[04.02.2026 04:07] Response: ```python
[
    "Shanghai Jiao Tong University",
    "Hohai University",
    "Singapore Management University",
    "Beijing Institute of Technology",
    "Imperial College London",
    "UC San Diego",
    "East China Normal University",
    "Shanghai Innovation Institute",
    "Chongqing University"
]
```
[04.02.2026 04:07] Deleting PDF ./assets/pdf/2602.01785.pdf.
[04.02.2026 04:07] Success.
[04.02.2026 04:07] Downloading and parsing paper https://huggingface.co/papers/2602.01630.
[04.02.2026 04:07] Downloading paper 2602.01630 from https://arxiv.org/pdf/2602.01630v1...
[04.02.2026 04:07] Extracting affiliations from text.
[04.02.2026 04:07] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Bohan Zeng * 1 Kaixin Zhu * 1 Daili Hua * 1 Bozhou Li * 1 Chengzhuo Tong * 1 Yuran Wang * 1 Xinyi Huang * 1 Yifan Dai * 2 Zixiang Zhang * 1 Yifan Yang * 1 Zhou Liu 1 Hao Liang 1 Xiaochen Ma 3 Ruichuan An 1 Tianyi Bai 3 Hongcheng Gao 4 Junbo Niu 1 Yang Shi 1 Xinlong Chen 5 Yue Ding 5 Minglei Shi 4 Kai Zeng 1 Yiwen Tang 1 Yuanxing Zhang 6 Pengfei Wan 6 Xintao Wang 6 Wentao Zhang 1 6 2 0 2 2 ] . [ 1 0 3 6 1 0 . 2 0 6 2 : r a "
[04.02.2026 04:07] Response: ```python
[]
```
[04.02.2026 04:07] Extracting affiliations from text.
[04.02.2026 04:07] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Bohan Zeng * 1 Kaixin Zhu * 1 Daili Hua * 1 Bozhou Li * 1 Chengzhuo Tong * 1 Yuran Wang * 1 Xinyi Huang * 1 Yifan Dai * 2 Zixiang Zhang * 1 Yifan Yang * 1 Zhou Liu 1 Hao Liang 1 Xiaochen Ma 3 Ruichuan An 1 Tianyi Bai 3 Hongcheng Gao 4 Junbo Niu 1 Yang Shi 1 Xinlong Chen 5 Yue Ding 5 Minglei Shi 4 Kai Zeng 1 Yiwen Tang 1 Yuanxing Zhang 6 Pengfei Wan 6 Xintao Wang 6 Wentao Zhang 1 6 2 0 2 2 ] . [ 1 0 3 6 1 0 . 2 0 6 2 : r aWorld models have emerged as critical frontier in AI research, aiming to enhance large models by infusing them with physical dynamics and world knowledge. The core objective is to enable agents to understand, predict, and interact with complex environments. However, current research landscape remains fragmented, with approaches predominantly focused on injecting world knowledge into isolated tasks, such as visual prediction, 3D estimation, or symbol grounding, rather than establishing unified definition or framework. While these task-specific integrations yield performance gains, they often lack the systematic coherence required for holistic world understanding. In this paper, we analyze the limitations of such fragmented approaches and propose unified design specification for world models. We suggest that robust world model should not be loose collection of capabilities but normative framework that integrally incorporates interaction, perception, symbolic reasoning, and spatial representation. This work aims to provide structured perspective to guide future research toward more general, robust, and principled models of the world. 1. Introduction With the explosive growth of internet data and continuous advances in neural network model training, existing *Equal contribution 1Peking University 2Shanghai Jiao Tong University 3HKUST 4Tsinghua University 5School of Artificial Intelligence, University of Chinese Academy of Sciences 6Kling Team, Kuaishou Technology. Correspondence to: Bohan Zeng <bhzeng25@stu.pku.edu.cn>, Wentao Zhang <wentao.zhang@pku.edu.cn>. Preprint. February 3, 2026. large models (Achiam et al., 2023; Bai et al., 2023; Yang et al., 2025a; Liu et al., 2024; Bai et al., 2025a; Chen et al., 2024b; Team et al., 2024; Lu et al., 2024) and diffusion models (Liu et al., 2022; Lipman et al., 2022; Labs, 2024; Peebles & Xie, 2023) have achieved remarkable results in various fields. However, as model performance further improves, the bottleneck in data quality has become increasingly difficult to overcome, hindering further progress, especially in multimodal domains requiring precise analysis, such as multimodal reasoning, chemical formula recognition, 3D scene generation, and specific professional areas like healthcare (Tang et al., 2024; 2025a; Tochilkin et al., 2024; Xiang et al., 2025b;a; Cheng et al., 2025). To break through the traditional token-prediction paradigm of large models, researchers have begun to focus on the study of world models. The concept of World Model was first introduced by (Ha & Schmidhuber, 2018), which proposed strategy of constructing an interactive system between agents and the world to handle complex visual input environments. With the rapid development of large models and various multimodal generation methods, recent work (Zhu et al., 2024) has further expanded the notion of world models, viewing video generation and 3D generation as intelligent systems that simulate the real world. Researchers are considering world models as the next-generation paradigm to replace token-predicting large language models (Yang et al., 2025c). As world models attract growing interest, numerous research fields have begun to incorporate world knowledge to empower models to perform tasks that require an understanding of physical and contextual rules (Yu et al., 2025a; Team et al., 2025b; Liu et al., 2025c; Zhu et al., 2025). This trend is evident in diverse applications, including image editing (Zeng et al., 2025a; Lin et al., 2025a; Chen et al., 2025c), multimodal spatial reasoning (Chen et al., 2024a), autonomous driving (Tu et al., 2025; Zeng et al., 2025b), and even mobile communication methods such as MobileWorld (Kong et al., 2025). Several studies (Hu et al., 2025) have further provided systematic categorizations and summaries of Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks advocates for shift from task-specific adaptations to comprehensive system design. Specifically, the main contents and contributions of this work are organized as follows: We provide detailed review of recent progress in World Models, categorizing existing approaches into reasoning, content generation, and interactive agents. We examine how these fields currently incorporate world knowledge to enhance performance. We critically analyze the shortcomings of current methods that rely on injecting knowledge into isolated tasks. Through case studies in LLMs, video generation, and embodied AI, we demonstrate that these approaches often fail to achieve genuine physical understanding and long-term consistency. We propose unified and standardized World Model Framework. We define the essential components, including Interaction, Reasoning, Memory, Environment, and Multimodal Generation, and articulate how they should be integrally designed to support robust world simulation. We identify critical directions for future breakthroughs, such as physically-grounded spatiotemporal representation, embodied interaction control, and autonomous modular evolution, to guide the community toward more general and principled models. 2. Background Understanding world knowledge is crucial for enhancing the ability of artificial intelligence systems to handle complex physical environments. Existing research can be broadly categorized into three classes based on the proactivity of their interaction with the environment and their approach to knowledge integration. Although these methods have made progress in their respective fields, they collectively highlight an urgent need for unified and proactive world modeling framework in current research. 2.1. Reasoning with World Knowledge First, since Large Language Models and Vision-Language Models (LLM/VLM) have demonstrated powerful reasoning and generalization capabilities, some studies have built upon this foundation to further enhance models reasoning abilities concerning complex physical worlds and challenging logical concepts. This category of work primarily includes: general multimodal reasoning represented by"
[04.02.2026 04:07] Mistral response. {"id": "c92a0490e22a486398f978b30a6d4140", "created": 1770178040, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1536, "total_tokens": 1596, "completion_tokens": 60, "num_cached_tokens": 1535}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\n    \"Peking University\",\n    \"Shanghai Jiao Tong University\",\n    \"HKUST\",\n    \"Tsinghua University\",\n    \"School of Artificial Intelligence, University of Chinese Academy of Sciences\",\n    \"Kling Team, Kuaishou Technology\"\n]\n```"}}]}
[04.02.2026 04:07] Response: ```python
[
    "Peking University",
    "Shanghai Jiao Tong University",
    "HKUST",
    "Tsinghua University",
    "School of Artificial Intelligence, University of Chinese Academy of Sciences",
    "Kling Team, Kuaishou Technology"
]
```
[04.02.2026 04:07] Deleting PDF ./assets/pdf/2602.01630.pdf.
[04.02.2026 04:07] Success.
[04.02.2026 04:07] Downloading and parsing paper https://huggingface.co/papers/2602.03619.
[04.02.2026 04:07] Downloading paper 2602.03619 from https://arxiv.org/pdf/2602.03619v1...
[04.02.2026 04:07] Extracting affiliations from text.
[04.02.2026 04:07] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Learning Query-Specific Rubrics from Human Preferences for DeepResearch Report Generation Changze Lv 1 2 Jie Zhou 1 Wentao Zhao 1 Jingwen Xu 2 Zisu Huang 2 Muzhao Tian 2 Shihan Dou 2 Tao Gui 2 Le Tian 1 Xiao Zhou 1 Xiaoqing Zheng 2 Xuanjing Huang 2 Jie Zhou 1 6 2 0 2 3 ] . [ 1 9 1 6 3 0 . 2 0 6 2 : r a "
[04.02.2026 04:07] Response: ```python
[]
```
[04.02.2026 04:07] Extracting affiliations from text.
[04.02.2026 04:07] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Learning Query-Specific Rubrics from Human Preferences for DeepResearch Report Generation Changze Lv 1 2 Jie Zhou 1 Wentao Zhao 1 Jingwen Xu 2 Zisu Huang 2 Muzhao Tian 2 Shihan Dou 2 Tao Gui 2 Le Tian 1 Xiao Zhou 1 Xiaoqing Zheng 2 Xuanjing Huang 2 Jie Zhou 1 6 2 0 2 3 ] . [ 1 9 1 6 3 0 . 2 0 6 2 : r aNowadays, training and evaluating DeepResearchgenerated reports remain challenging due to the lack of verifiable reward signals. Accordingly, rubric-based evaluation has become common practice. However, existing approaches either rely on coarse, pre-defined rubrics that lack sufficient granularity, or depend on manually constructed query-specific rubrics that are costly and difficult to scale. In this paper, we propose pipeline to train human-preference-aligned query-specific rubric generators tailored for DeepResearch report generation. We first construct dataset of DeepResearch-style queries annotated with human preferences over paired reports, and train rubric generators via reinforcement learning with hybrid reward combining human preference supervision and LLM-based rubric evaluation. To better handle long-horizon reasoning, we further introduce Multi-agent Markov-state (MaMs) workflow for report generation. We empirically show that our proposed rubric generators deliver more discriminative and better human-aligned supervision than existing rubric design strategies. Moreover, when integrated into the MaMs training framework, DeepResearch systems equipped with our rubric generators consistently outperform all open-source baselines on the DeepResearch Bench and achieve performance comparable to that of leading closed-source models. 1. Introduction Large language models (LLMs) (Achiam et al., 2023; Guo et al., 2025; Yang et al., 2025) have recently enThe work was conducted during the internship of Changze Lv (czlv24@m.fudan.edu.cn) at Tencent. 1Pattern Recognition Center, WeChat AI, Tencent Inc. 2College of Computer Science and Artificial Intelligence, Fudan University. Correspondence to: Xiaoqing Zheng <zhengxq@fudan.edu.cn>. Preprint. abled DeepResearch systems (Qwen Team, 2025; Google, 2025; OpenAI, 2025; Anthropic, 2025) that can synthesize evidence from large-scale document collections and produce long-form analytical reports for complex, openended queries. Unlike short-form DeepResearch tasks like BrowseComp (Wei et al., 2025; Zhou et al., 2025), GAIA (Mialon et al., 2023), and HLE (Phan et al., 2025), report generation requires models to reason, retrieve, and integrate over diverse sources and multiple turns, while presenting results in coherent and well-structured manner. However, training and evaluating DeepResearch report generators remain fundamentally challenging. key difficulty lies in the absence of verifiable rewards. Human evaluation, while reliable, is costly and difficult to scale (Krishna et al., 2021; Xu et al., 2023; Shao et al., 2025b), motivating the widespread adoption of rubric-based evaluation (Gunjal et al., 2025; Huang et al., 2025; Viswanathan et al., 2025) as practical alternative. In principle, expert-designed, queryspecific rubrics, like ResearchRubrics (Sharma et al., 2025), could serve as faithful proxy for human judgments when evaluating DeepResearch reports. However, authoring such rubrics requires substantial domain expertise and effort for each query, making this approach difficult to scale to large and diverse training corpora. Prior work has explored pre-defined generic rubrics (Que et al., 2024; Hashemi et al., 2024; Shao et al., 2024a) or LLM-generated query-specific rubrics (Xie et al., 2025; Du et al., 2025) to provide structured feedback for report generation tasks. However, those methods suffer from two limitations: First, pre-defined rubrics are necessarily generic and lack the granularity needed to distinguish subtle quality differences across diverse research queries. Second, LLMgenerated query-specific rubrics are typically not grounded in human preference data, making them prone to misalignment with how humans actually compare and judge research reports. These issues may lead to weak supervision signals, reward hacking, and inefficient learning dynamics. To address these limitations, it is crucial to reconsider the source of supervision for evaluating DeepResearch reports. We argue that one of the most direct supervision signals for assessing report quality is human preference (Dai Learning Query-Specific Rubrics from Human Preferences for DeepResearch Report Generation et al., 2023; Zheng et al., 2023b; Wang et al., 2024a; Liu et al., 2024b) over candidate reports. Given these limitations, natural question arises: rather than applying generic or human-annotated rubrics, can we learn to evaluate reports in way that is both scalable on large training data and aligned with human preferences? In this paper, we propose pipeline to effectively train human-preference-aligned query-specific rubric generators tailored for DeepResearch report generation. We construct preference dataset of over 5, 000 DeepResearch-style queries, each paired with two candidate reports and annotated with human preference judgments. Then we train the rubric generator using Group Relative Policy Optimization (GRPO) (Shao et al., 2024b) with hybrid reward that integrates two complementary signals. The first component is the preference consistency reward, which encourages the generated rubric to be discriminative by correctly ranking human-preferred reports above less-preferred ones, using the annotated preference dataset to guide scoring. For the second reward, we leverage LLMs to assess rubric quality, providing feedback on whether the generated criteria are coherent, applicable, and discriminative. Once trained, the rubric generator is integrated into the training of DeepResearch systems. For each input query, it automatically produces query-level rubrics that are used to evaluate rollout samples from the policy model, assigning fine-grained reward scores that guide the optimization process. Whats more, to address the long-context dependencies in the ReAct workflow (Yao et al., 2022), we further propose the Multi-agent Markov-state (MaMs) workflow. Empirically, we demonstrate that our proposed rubric generators deliver more discriminative and human-aligned supervision signals than pre-defined or LLM-generated alternatives. Furthermore, when employed as training signals for DeepResearch agents, these generated rubrics consistently enhance performance across m"
[04.02.2026 04:07] Mistral response. {"id": "8b03ccbfccb04f7c9003303aabcd5a47", "created": 1770178047, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1508, "total_tokens": 1539, "completion_tokens": 31, "num_cached_tokens": 1507}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\"Pattern Recognition Center, WeChat AI, Tencent Inc.\", \"College of Computer Science and Artificial Intelligence, Fudan University\"]\n```"}}]}
[04.02.2026 04:07] Response: ```python
["Pattern Recognition Center, WeChat AI, Tencent Inc.", "College of Computer Science and Artificial Intelligence, Fudan University"]
```
[04.02.2026 04:07] Deleting PDF ./assets/pdf/2602.03619.pdf.
[04.02.2026 04:07] Success.
[04.02.2026 04:07] Downloading and parsing paper https://huggingface.co/papers/2602.02380.
[04.02.2026 04:07] Downloading paper 2602.02380 from https://arxiv.org/pdf/2602.02380v1...
[04.02.2026 04:07] Extracting affiliations from text.
[04.02.2026 04:07] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Yibin Wang1,2, Yuhang Zang4, Feng Han1,2, Yujie Zhou3,4, Jiazi Bu3,4, Cheng Jin1,2, Jiaqi Wang2 1Fudan University 3Shanghai Jiaotong University 2Shanghai Innovation Institute 4Shanghai AI Lab "
[04.02.2026 04:07] Response: ```python
[
    "Fudan University",
    "Shanghai Innovation Institute",
    "Shanghai Jiaotong University",
    "Shanghai AI Lab"
]
```
[04.02.2026 04:07] Deleting PDF ./assets/pdf/2602.02380.pdf.
[04.02.2026 04:07] Success.
[04.02.2026 04:07] Downloading and parsing paper https://huggingface.co/papers/2602.03845.
[04.02.2026 04:07] Downloading paper 2602.03845 from https://arxiv.org/pdf/2602.03845v1...
[04.02.2026 04:07] Extracting affiliations from text.
[04.02.2026 04:07] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Parallel-Probe: Towards Efficient Parallel Thinking via 2D Probing Tong Zheng 1 * Chengsong Huang 2 * Runpeng Dai 3 * Yun He 1 Rui Liu 1 Xin Ni 4 Huiwen Bao 5 Kaishen Wang 1 Hongtu Zhu 3 Jiaxin Huang 2 Furong Huang 1 Heng Huang 1 Code: https://github.com/zhengkid/Parallel-Probe Online Judge Platform: Efficient Reasoning Online Judge "
[04.02.2026 04:07] Response: ```python
[]
```
[04.02.2026 04:07] Extracting affiliations from text.
[04.02.2026 04:07] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Parallel-Probe: Towards Efficient Parallel Thinking via 2D Probing Tong Zheng 1 * Chengsong Huang 2 * Runpeng Dai 3 * Yun He 1 Rui Liu 1 Xin Ni 4 Huiwen Bao 5 Kaishen Wang 1 Hongtu Zhu 3 Jiaxin Huang 2 Furong Huang 1 Heng Huang 1 Code: https://github.com/zhengkid/Parallel-Probe Online Judge Platform: Efficient Reasoning Online JudgeParallel thinking has emerged as promising paradigm for reasoning, yet it imposes significant computational burdens. Existing efficiency methods primarily rely on local, per-trajectory signals and lack principled mechanisms to exploit global dynamics across parallel branches. We introduce 2D probing, an interface that exposes the widthdepth dynamics of parallel thinking by periodically eliciting intermediate answers from all branches. Our analysis reveals three key insights: non-monotonic scaling across widthdepth allocations, heterogeneous reasoning branch lengths, and early stabilization of global consensus. Guided by these insights, we introduce Parallel-Probe, training-free controller designed to optimize online parallel thinking. Parallel-Probe employs consensus-based early stopping to regulate reasoning depth and deviation-based branch pruning to dynamically adjust width. Extensive experiments across three benchmarks and multiple models demonstrate that Parallel-Probe establishes superior Pareto frontier for test-time scaling. Compared to standard majority voting, it reduces sequential tokens by up to 35.8% and total token cost by over 25.8% while maintaining competitive accuracy. 6 2 0 2 3 ] . [ 1 5 4 8 3 0 . 2 0 6 2 : r 1. Introduction Parallel thinking has emerged as promising paradigm for improving LLM reasoning by exploring multiple reasoning trajectories in parallel and aggregating them (e.g., via voting, selection, or summarization) (Comanici et al., 2025; Zheng *Equal contribution Core Contributors 1Department of Computer Science, University of Maryland, College, Park 2Washington University in St. Louis 3University of North Carolina at Chapel Hill 4Tongji University 5City University of Hong Kong. Correspondence to: Tong Zheng <tzheng24@umd.edu>. Preprint. February 4, 2026. 1 et al., 2025; Wen et al., 2025). By maintaining multiple candidate reasoning trajectories, it reduces the brittleness of single-chain reasoning, where early mistakes can easily compromise the entire reasoning process (Wang et al., 2022a; Zheng et al., 2025). Moreover, parallel thinking is also hardware-friendly: it naturally aligns with modern GPU parallelism, enabling high-throughput batched decoding (Rodionov et al., 2025; Hsu et al., 2025; Yang et al., 2025c). However, this paradigm often requires massive token generation (Fu et al., 2025b), e.g., token usage nearly scales with the number of parallel branches, thereby posing significant challenges to efficiency. To improve efficiency, previous work studies efficient reasoning at test time. The majority of the research investigates early-stopping strategies for sequential generation (e.g., extended Chain-of-Thought), leveraging signals such as confidence (Fu et al., 2025b), hidden states (Li et al., 2026), or answer convergence (Liu & Wang, 2025; Zhang et al., 2025b). Since these approaches focus on the internal state of individual trajectories, they ignore critical global information across branches (e.g., consensus), making them sub-optimal in parallel thinking settings. Meanwhile, several studies have explored adaptive sampling to reduce the inference cost of self-consistency (Mao et al., 2025; Aggarwal et al., 2023; Wan et al., 2025; Fu et al., 2025b; Huang et al., 2025). Since these methods rely on sequential control loops, they transform parallel sampling into semi-sequential process. Consequently, even though sample efficiency is improved, the increased latency cancels out the speed advantage. Efficient parallel thinking in an online setting has received limited attention, particularly the simultaneous launch of multiple paths. The fundamental challenge lies the intrinsic independence of parallel decoding threads, where each branch evolves without regard for the progression of others. This isolation leads to suboptimal resource allocation and decoding of redundant trajectories. This raises pivotal question: Can we introduce lightweight global signals to facilitate efficient, hardware-friendly parallel thinking? To bridge this gap, we introduce 2D Probing, black-box Parallel-Probe: Towards Efficient Parallel Thinking via 2D Probing Figure 1. Overview of the Parallel-Probe framework. It monitors parallel reasoning branches via continuous 2D probing. (1) Divergence Pruning: Outlying trajectories that drift from the global majority (e.g., Branch 4) are aggressively pruned to save compute. (2) Stability Stopping: The global controller halts the entire ensemble once the consensus stabilizes, preventing the execution of redundant post-convergence steps (dashed area). Crucially, Parallel-Probe is model-agnostic and compatible with various off-the-shelf LLMs. We evaluate Performance, Cost Efficiency, and Latency Efficiency across 0.6B and 1.7B models. Values are averaged across all datasets and normalized such that the best-performing method on each axis equals 1.0. Parallel-Probe (blue) achieves the largest coverage area, demonstrating superior balance between high accuracy and computational efficiency compared to SC and ESC methods. interface that periodically injects an end-of-think token to elicit intermediate answers from each branch during decoding. This constructs 2D probing matrix with intermediate answers, defined by branch index (width) and probing period (depth). Such probing matrix enables fine-grained monitoring of reasoning trajectories. To analyze these dynamics, we develop SCOUT (Sequential & Concurrent Offline Utilization Testbed), an evaluation platform designed to rapidly assess different strategies using pre-sampled data. Using SCOUT, we discover three simple but important insights that explain why standard per-trajectory early stopping is suboptimal for online parallel thinking: (i) Scaling is non-monotonic: Accuracy depends heavily on how width and depth are balanced, not just the total token budget (Figure 2 (a)); (ii) Lengths of reasoning branches are highly uneven (Figure 2 (b) and Figure 7). (iii) Consensus stabilizes early: Early majority votes are often unstable and inaccurate, but they converge to reliable consensus long before all branches terminate (Figure 2 (c)). Guided by these insig"
[04.02.2026 04:07] Mistral response. {"id": "4320306258044469bca052a023c247df", "created": 1770178063, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1485, "total_tokens": 1541, "completion_tokens": 56, "num_cached_tokens": 1484}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\n    \"Department of Computer Science, University of Maryland, College, Park\",\n    \"Washington University in St. Louis\",\n    \"University of North Carolina at Chapel Hill\",\n    \"Tongji University\",\n    \"City University of Hong Kong\"\n]\n```"}}]}
[04.02.2026 04:07] Response: ```python
[
    "Department of Computer Science, University of Maryland, College, Park",
    "Washington University in St. Louis",
    "University of North Carolina at Chapel Hill",
    "Tongji University",
    "City University of Hong Kong"
]
```
[04.02.2026 04:07] Deleting PDF ./assets/pdf/2602.03845.pdf.
[04.02.2026 04:07] Success.
[04.02.2026 04:07] Downloading and parsing paper https://huggingface.co/papers/2602.02676.
[04.02.2026 04:07] Downloading paper 2602.02676 from https://arxiv.org/pdf/2602.02676v1...
[04.02.2026 04:07] Extracting affiliations from text.
[04.02.2026 04:07] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 2 ] . [ 1 6 7 6 2 0 . 2 0 6 2 : r AdaptMMBench: Benchmarking Adaptive Multimodal Reasoning for Mode Selection and Reasoning Process Xintong Zhang1,2, Xiaowen Zhang2,3, Jongrong Wu2, Zhi Gao1,2,4,, Shilin Yan5, Zhenxin Diao1, Kunpeng Gao1, Xuanyan Chen1, Yuwei Wu1,4, Yunde Jia4 Qing Li2, 1Beijing Key Laboratory of Intelligent Information Technology, School of Computer Science & Technology, Beijing Institute of Technology 2State Key Laboratory of General Artificial Intelligence, BIGAI 3Xidian University 4Guangdong Laboratory of Machine Perception and Intelligent Computing, Shenzhen MSU-BIT University 5Alibaba Group Core contribution, Project supervisor, Equal contribution, Corresponding authors Project Page: https://adaptmmbench.github.io/ "
[04.02.2026 04:07] Response: ```python
[
    "Beijing Key Laboratory of Intelligent Information Technology, School of Computer Science & Technology, Beijing Institute of Technology",
    "State Key Laboratory of General Artificial Intelligence, BIGAI",
    "Xidian University",
    "Guangdong Laboratory of Machine Perception and Intelligent Computing, Shenzhen MSU-BIT University",
    "Alibaba Group"
]
```
[04.02.2026 04:07] Deleting PDF ./assets/pdf/2602.02676.pdf.
[04.02.2026 04:07] Success.
[04.02.2026 04:07] Downloading and parsing paper https://huggingface.co/papers/2602.02636.
[04.02.2026 04:07] Downloading paper 2602.02636 from https://arxiv.org/pdf/2602.02636v1...
[04.02.2026 04:08] Extracting affiliations from text.
[04.02.2026 04:08] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"WideSeek: Advancing Wide Research via Multi-Agent Scaling Ziyang Huang * 1 2 Haolin Ren * 1 2 Xiaowei Yuan 1 2 Jiawei Wang 3 Zhongtao Jiang Kun Xu Shizhu He 1 2 Jun Zhao 1 2 Kang Liu 1 2 https://wideseek-ai.github.io 6 2 0 2 2 ] . [ 1 6 3 6 2 0 . 2 0 6 2 : r a "
[04.02.2026 04:08] Response: ```python
[]
```
[04.02.2026 04:08] Extracting affiliations from text.
[04.02.2026 04:08] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"WideSeek: Advancing Wide Research via Multi-Agent Scaling Ziyang Huang * 1 2 Haolin Ren * 1 2 Xiaowei Yuan 1 2 Jiawei Wang 3 Zhongtao Jiang Kun Xu Shizhu He 1 2 Jun Zhao 1 2 Kang Liu 1 2 https://wideseek-ai.github.io 6 2 0 2 2 ] . [ 1 6 3 6 2 0 . 2 0 6 2 : r aSearch intelligence is evolving from Deep Research to Wide Research, paradigm essential for retrieving and synthesizing comprehensive information under complex constraints in parallel. However, progress in this field is impeded by the lack of dedicated benchmarks and optimization methodologies for search breadth. To address these challenges, we take deep dive into Wide Research from two perspectives: Data Pipeline and Agent Optimization. First, we produce WideSeekBench, General Broad Information Seeking (GBIS) benchmark constructed via rigorous multi-phase data pipeline to ensure diversity across the target information volume, logical constraints, and domains. Second, we introduce WideSeek, dynamic hierarchical multi-agent architecture that can autonomously fork parallel sub-agents based on task requirements. Furthermore, we design unified training framework that linearizes multi-agent trajectories and optimizes the system using end-to-end RL. Experimental results demonstrate the effectiveness of WideSeek and multi-agent RL, highlighting that scaling the number of agents is promising direction for advancing the Wide Research paradigm. 1. Introduction Search Intelligence constitutes the cornerstone of Agentic AI (Shi et al., 2025; Abou Ali et al., 2025). Moving beyond mere substitute for conventional search engines, it serves as an essential module for complex, real-world applications, including repository-level code generation (Jimenez et al., 2024), enterprise data intelligence (Lei et al., 2025), and Equal contribution. 1Institute of Automation, Chinese Academy of Sciences 2University of Chinese Academy of Sciences 3University of Science and Technology of China. Corresponding author: Kang Liu . Preprint. Emali: huangzy0312@gmail.com 1 Figure 1. Deep Research paradigm vs. Wide Research paradigm. general GUI manipulation (Xie et al., 2024). Existing research has predominantly focused on Deep Research (Wei et al., 2025), which employs complex, multistep reasoning and action sequences to locate single hard-to-find piece of information. As AI enters its Second Half (Yao, 2025), the research community is increasingly shifting its focus toward real-world and utility scenarios. This transition necessitates move toward Wide Research (Manus, 2025), as shown in Figure 1, which replaces sequential reasoning with parallel orchestration paradigm. By prioritizing high-breadth synthesis and structural comprehensiveness, Wide Research enhances productivity and scales the effectiveness of industrial AI deployment. Wide Research focuses on systematic retrieval across expansive search spaces, transitioning from deep-but-narrow chains to high-breadth parallelized frameworks. Aligning with Kimi Agent-Swarm (Moonshot AI, 2026), this WideSeek: Advancing Wide Research via Multi-Agent Scaling paradigm employs sophisticated orchestrator to decompose complex global objectives into granular, parallel subtasks, which are then concurrently executed by autonomous agents capable of iterative deep research and mutual crossvalidation. representative application is the generation of Competitor Analysis Tables, as exemplified by systems such as Manus (Manus, 2025), which synthesize information from thousands of sources into comprehensive comparative tables, substantially reducing labor costs of Human Data Analyst while enhancing productivity at scale. Despite its promise, the advancement of Wide Research is hindered by three primary challenges: (1) Limitations in Benchmarks: Existing benchmarks (Wong et al., 2025; Lan et al., 2025) are largely constructed by human experts, which limits their scale, diversity, and categorization depth. Furthermore, they typically provide only test sets, lacking the training data necessary for model optimization; (2) Deficiencies in Data Synthesis: Current data synthesis methods for search agents focus on sampling complex graph topologies to simulate multi-step reasoning paths (Li et al., 2025; Tao et al., 2025). While these approaches effectively optimize for search depth, they lack the capacity to efficiently synthesize large scale of atomic information under complex constraint, which is critical for search width; and (3) Optimization Gaps: Previous approaches often rely on closed-source models within static multi-agent frameworks (Roucher et al., 2025), or concentrates on enhancing the depth of single-agent reasoning (Lu et al., 2025). There is notable lack of exploration into the end-to-end optimization of systems capable of autonomously broadening their search paths. To address these challenges, we investigate the Wide Research paradigm through two perspectives: data pipeline construction and agent optimization. Data Pipeline & Benchmark. While conventional methods construct information graphs from web pages to emulate reasoning paths toward single answer, our approach utilizes large-scale Knowledge Graphs (KGs) (Schmelzeisen et al., 2021) to extract clusters of interconnected world knowledge. Specifically, we initialize the process with seed entities and set of sampled seed constraints. By applying formal set operations (including intersection, union, and difference), we construct complex constraints that resolve into target entity set. Simultaneously, we sample high-coverage attributes of these entities to define the target attribute set. Next, we fetch all atomic information from Knowledge Graph to form the answer table and construct the input task based on the complex constraints. For convenient evaluation, this pipeline produces column-wise rubrics for reward system. To ensure the quality of data, all tasks will be evaluated by hybrid filtering system. Based on this pipeline, we introduce WideSeekBench, benchmark for General Broad Information Seeking (GBIS) comprising both training and test sets. To ensure rigorous and multi-dimensional evaluation, the test set is strictly sampled and balanced across target information volume, operator complexity, and domains. Agent Optimization. The Wide Research paradigm requires agents to acquire and synthesize target information from large volume of sources. This necessitates reasoning architecture that supports both parallel and serial execution, typically involving ultra-long-"
[04.02.2026 04:08] Mistral response. {"id": "fab1e94949b947b09925c1927166647f", "created": 1770178101, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1372, "total_tokens": 1410, "completion_tokens": 38, "num_cached_tokens": 1371}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\n    \"Institute of Automation, Chinese Academy of Sciences\",\n    \"University of Chinese Academy of Sciences\",\n    \"University of Science and Technology of China\"\n]\n```"}}]}
[04.02.2026 04:08] Response: ```python
[
    "Institute of Automation, Chinese Academy of Sciences",
    "University of Chinese Academy of Sciences",
    "University of Science and Technology of China"
]
```
[04.02.2026 04:08] Deleting PDF ./assets/pdf/2602.02636.pdf.
[04.02.2026 04:08] Success.
[04.02.2026 04:08] Downloading and parsing paper https://huggingface.co/papers/2601.21244.
[04.02.2026 04:08] Downloading paper 2601.21244 from https://arxiv.org/pdf/2601.21244v2...
[04.02.2026 04:08] Extracting affiliations from text.
[04.02.2026 04:08] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Less Noise, More Voice: Reinforcement Learning for Reasoning via Instruction Purification , Tianyi Hu (cid:13) (cid:11) , Zexu Sun> , Yankai Lin (cid:11) (cid:66) (cid:11) Gaoling School of Artificial Intelligence, Renmin University of China Department of Computer Science, Aarhus University > Baidu Inc. (cid:66){yijuguo, yankailin}@ruc.edu.cn (cid:13) 6 2 0 2 2 ] . [ 2 4 4 2 1 2 . 1 0 6 2 : r a "
[04.02.2026 04:08] Response: ```python
[
    "Gaoling School of Artificial Intelligence, Renmin University of China",
    "Department of Computer Science, Aarhus University",
    "Baidu Inc."
]
```
[04.02.2026 04:08] Deleting PDF ./assets/pdf/2601.21244.pdf.
[04.02.2026 04:08] Success.
[04.02.2026 04:08] Downloading and parsing paper https://huggingface.co/papers/2602.02103.
[04.02.2026 04:08] Downloading paper 2602.02103 from https://arxiv.org/pdf/2602.02103v1...
[04.02.2026 04:09] Extracting affiliations from text.
[04.02.2026 04:09] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"No Global Plan in Chain-of-Thought: Uncover the Latent Planning Horizon of LLMs Liyan Xu 1 Mo Yu 1 Fandong Meng 1 Jie Zhou 1 6 2 0 2 2 ] . [ 1 3 0 1 2 0 . 2 0 6 2 : r a "
[04.02.2026 04:09] Response: ```python
[]
```
[04.02.2026 04:09] Extracting affiliations from text.
[04.02.2026 04:09] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"No Global Plan in Chain-of-Thought: Uncover the Latent Planning Horizon of LLMs Liyan Xu 1 Mo Yu 1 Fandong Meng 1 Jie Zhou 1 6 2 0 2 2 ] . [ 1 3 0 1 2 0 . 2 0 6 2 : r aThis work stems from prior complementary observations on the dynamics of Chain-of-Thought (CoT): Large Language Models (LLMs) is shown latent planning of subsequent reasoning prior to CoT emergence, thereby diminishing the significance of explicit CoT; whereas CoT remains critical for tasks requiring multi-step reasoning. To deepen the understanding between LLMs internal states and its verbalized reasoning trajectories, we investigate the latent planning strength of LLMs, through our probing method, Tele-Lens, applying to hidden states across diverse task domains. Our empirical results indicate that LLMs exhibit myopic horizon, primarily conducting incremental transitions without precise global planning. Leveraging this characteristic, we propose hypothesis on enhancing uncertainty estimation of CoT, which we validate that small subset of CoT positions can effectively represent the uncertainty of the entire path. We further underscore the significance of exploiting CoT dynamics, and demonstrate that automatic recognition of CoT bypass can be achieved without performance degradation. Our code, data and models are released at https: //github.com/lxucs/tele-lens. 1. Introduction Chain-of-Thought (CoT) (Nye et al., 2021; Wei et al., 2022) has fundamentally reshaped problem-solving in natural language processing, marking shift from traditional patternmatching approaches, e.g. encoder-based classification (Devlin et al., 2019; Liu et al., 2019), toward prompt-based reasoning articulated explicitly in natural language (Zhou et al., 2023; Dong et al., 2024; Sahoo et al., 2025). The capacity of CoT is further amplified through extensive thinking emanated from reinforcement learning, characterized by recent models such as DeepSeek-R1 (DeepSeek-AI, 2025). 1WeChat AI, Tencent Inc. (cid:66): <liyanlxu@tencent.com> Preprint. February 3, 2026. While CoT is widely perceived as the de facto reasoning paradigm, however, recent studies on Large Language Models (LLMs) have revealed complementary, and at times seemingly conflicting perspectives. On the one hand, LLMs have been shown to exhibit internal planning on the reasoning trace prior to the explicit emergence of CoT. Dong et al. (2025) observes that hidden states at the beginning of CoT can reliably predict the total reasoning steps and key attributes with high correlation to the realized trajectories. Similarly, other studies also suggest that earlier hidden states already carry the information of subsequent generation (Pal et al., 2023), to the extent where the initial stages of CoT effectively plan the final answers (Azaria & Mitchell, 2023; Gottesman & Geva, 2024; Afzal et al., 2025). The internal planning capabilities of LLMs appear to diminish the necessity of CoT, raising the question of whether the thinking process is just echoing pre-determined paths already encoded in the prior internal states. On the other hand, theoretical analyses state that CoT is indispensable due to the limited expressivity of Transformers bounded by its architectures (Bhattamishra et al., 2023; Merrill & Sabharwal, 2023; Li et al., 2024), and only intermediate steps of CoT can derive length generalization (Anil et al., 2022; Xiao & Liu, 2025) and compositional reasoning (Wies et al., 2023; Abbe et al., 2024; Zubic et al., 2025). Therefore, the manifestation of pre-calculated trajectories appear unlikely via internal planning before the onset of CoT. Nonetheless, the relationship between the models internal representations and its verbalized reasoning tokens largely remains opaque. In this work, we investigate the internal dynamics of CoT, and target the following questions concerning the latent planning horizon: To what extent do hidden states encode global plan for the reasoning roadmap, as opposed to supporting rather local, incremental state transitions? And how does the scope of planning horizon further imply other CoT characteristics? Towards this objective, we derive empirical insights by examining the synergy between explicit CoT steps and its latent planning horizon. Building on the observations, we then highlight the significance of leveraging CoT dynamics No Global Plan in Chain-of-Thought: Uncover the Latent Planning Horizon of LLMs on estimating CoTs uncertainty and necessity. To answer the first question, Section 2 presents series of probing experiments designed for dissecting LLM hidden states, aiming to evaluate the internal planning strengths with respect to future reasoning trajectories. We first introduce our probing method, termed Tele-Lens, which employs trained low-rank adapter (Houlsby et al., 2019) that transforms each hidden state within CoT steps to predict Teleological information along multiple dimensions, including subsequent tokens, final answers, reasoning lengths, etc. Importantly, unlike prior works that primarily address singledomain tasks, we conduct probing experiments across 12 diverse datasets spanning different classes and domains, ranging from straightforward knowledge question answering to classic hard problems, e.g. Parity (counting the number of digits as even or odd), canonical challenge for Transformers (Chiang & Cholak, 2022; Hahn & Rofin, 2024). By empirical results, we observe sharply contrasting behaviors across probing dimensions and task domains, as detailed in Section 2.5. For instance, in terms of probing subsequent reasoning paths, hidden states can be indeed predictive on tasks with more structured solutions, such as algorithmic tasks, but they generally fail to predict on tasks with natural language tone, such as document comprehension. In terms of predicting final answers, hidden states exhibit limited planning horizon; for compositional tasks especially, they can only reliably capture the precise answer only one or two steps away from the reasoning completion. Interestingly, at the early stage of CoT, our results suggest that hidden states can encode predictive signals of the final answer for easier problems, reflecting coarse answer gist, which echos prior observations (Gottesman & Geva, 2024; Afzal et al., 2025). However, for harder tasks requiring explicit multi-step, the initial prediction drops to near-flat. Overall, our probing results bring unified view of the prior complementary beliefs from previous works: LLMs exhibit myopic pla"
[04.02.2026 04:09] Mistral response. {"id": "aa5cebbd32ce495b852dcb7588bea6e0", "created": 1770178153, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1484, "total_tokens": 1499, "completion_tokens": 15, "num_cached_tokens": 1483}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\"WeChat AI, Tencent Inc.\"]\n```"}}]}
[04.02.2026 04:09] Response: ```python
["WeChat AI, Tencent Inc."]
```
[04.02.2026 04:09] Deleting PDF ./assets/pdf/2602.02103.pdf.
[04.02.2026 04:09] Success.
[04.02.2026 04:09] Downloading and parsing paper https://huggingface.co/papers/2602.00747.
[04.02.2026 04:09] Downloading paper 2602.00747 from https://arxiv.org/pdf/2602.00747v1...
[04.02.2026 04:09] Extracting affiliations from text.
[04.02.2026 04:09] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Decouple Searching from Training: Scaling Data Mixing via Model Merging for Large Language Model Pre-training Shengrui Li 1 Fei Zhao 1 Kaiyan Zhao 1 Jieying Ye 1 Haifeng Liu 1 Fangcheng Shi 1 Zheyong Xie 1 Yao Hu 1 Shaosheng Cao 1 6 2 0 2 1 3 ] . [ 1 7 4 7 0 0 . 2 0 6 2 : r Abstract Determining an effective data mixture is key factor in Large Language Model (LLM) pre-training, where models must balance general competence with proficiency on hard tasks such as math and code. However, identifying an optimal mixture remains an open challenge, as existing approaches either rely on unreliable tiny-scale proxy experiments or require prohibitively expensive largescale exploration. To address this, we propose Decouple Searching from Training Mix (DeMix), novel framework that leverages model merging to predict optimal data ratios. Instead of training proxy models for every sampled mixture, DeMix trains component models on candidate datasets at scale and derives data mixture proxies via weighted model merging. This paradigm decouples search from training costs, enabling evaluation of unlimited sampled mixtures without extra training burden and thus facilitating better mixture discovery through more search trials. Extensive experiments demonstrate that DeMix breaks the trade-off between sufficiency, accuracy and efficiency, obtaining the optimal mixture with higher benchmark performance at lower search cost. Additionally, we release the DeMix Corpora, comprehensive 22T-token dataset comprising high-quality pre-training data with validated mixtures to facilitate open research. Our code and DeMix Corpora is available at https: //github.com/Lucius-lsr/DeMix. 1. Introduction Large Language Models (LLMs) have achieved remarkable success across wide range of domains (Shao et al., 2024; Guo et al., 2025; Kimi Team et al., 2025; Bai et al., 2025), 1NLP Team, Xiaohongshu Inc. Shanghai, China. <caoshaosheng@xiaohongshu.com>. Correspondence to: Huangpu District, Shaosheng Cao Preprint. F"
[04.02.2026 04:09] Response: ```python
["NLP Team, Xiaohongshu Inc. Shanghai, China"]
```
[04.02.2026 04:09] Deleting PDF ./assets/pdf/2602.00747.pdf.
[04.02.2026 04:09] Success.
[04.02.2026 04:09] Downloading and parsing paper https://huggingface.co/papers/2602.01362.
[04.02.2026 04:09] Downloading paper 2602.01362 from https://arxiv.org/pdf/2602.01362v1...
[04.02.2026 04:09] Extracting affiliations from text.
[04.02.2026 04:09] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Yue Liu 1 Yuzhong Zhao 1 Zheyong Xie 2 Qixiang Ye 1 Jianbin Jiao 1 Yao Hu 2 Shaosheng Cao 2 Yunfan Liu 1 6 2 0 2 1 ] . [ 1 2 6 3 1 0 . 2 0 6 2 : r a "
[04.02.2026 04:09] Response: ```python
[]
```
[04.02.2026 04:09] Extracting affiliations from text.
[04.02.2026 04:09] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Yue Liu 1 Yuzhong Zhao 1 Zheyong Xie 2 Qixiang Ye 1 Jianbin Jiao 1 Yao Hu 2 Shaosheng Cao 2 Yunfan Liu 1 6 2 0 2 1 ] . [ 1 2 6 3 1 0 . 2 0 6 2 : r aIn discrete generative modeling, two dominant paradigms demonstrate divergent capabilities: Masked Diffusion Language Models (MDLM) excel at semantic understanding and zero-shot generalization, whereas Uniform-noise Diffusion Language Models (UDLM) achieve strong fewstep generation quality, yet neither attains balanced performance across both dimensions. To address this, we propose XDLM, which bridges the two paradigms via stationary noise kernel. XDLM offers two key contributions: (1) it provides principled theoretical unification of MDLM and UDLM, recovering each paradigm as special case; and (2) an alleviated memory bottleneck enabled by an algebraic simplification of the posterior probabilities. Experiments demonstrate that XDLM advances the Pareto frontier between understanding capability and generation quality. Quantitatively, XDLM surpasses UDLM by 5.4 points on zero-shot text benchmarks and outperforms MDLM in few-step image generation (FID 54.1 vs. 80.8). When scaled to tune an 8B-parameter large language model, XDLM achieves 15.0 MBPP in just 32 steps, effectively doubling the baseline performance. Finally, analysis of training dynamics reveals XDLMs superior potential for long-term scaling. Code is available at this http URL. 1. Introduction Diffusion models have achieved remarkable success in continuous domains, particularly in image and audio generation (Ho et al., 2020; Dhariwal & Nichol, 2021; Rombach et al., 2022; Kong et al., 2020). Inspired by this potential, Discrete Denoising Diffusion Probabilistic Model (D3PM) has emerged as promising paradigm shift to the discrete state space (Austin et al., 2021). Notably, these Discrete 1UCAS 2Xiaohongshu Inc.. Yunfan <caoshaosheng@xiaohongshu.com>. <liuyunfan@ucas.ac.cn>, Liu Preprint. February 3, 2026. Correspondence to: Shaosheng Cao Diffusion Models (DDMs) are now demonstrating strong performance in language modeling, field long dominated by auto-regressive (AR) architectures. Within this landscape, DDM studies have diverged into two distinct branches: Masked Diffusion Language Models (MDLMs) (Sahoo et al., 2024) and Uniform-noise Diffusion Language Models (UDLMs) (Schiff et al., 2024). While MDLMs achieve superior performance in likelihood modeling and zero-shot generalization, they often struggle to generate coherent, contextually consistent outputs with limited inference steps. Conversely, UDLMs excel at low-step generation but frequently lag behind MDLMs when the number of inference steps increases. Despite their individual merits, neither achieves balanced performance across these dimensions. This imbalance carries significant practical implications. In domains like image synthesis, the required inference steps are typically far fewer than the total number of patches. In these few-step regimes, UDLM (and its hidden Gaussian counterparts (Schiff et al., 2024; Sahoo et al., 2025)) consistently outperforms the masked paradigm. Concretely, our experiments demonstrate that UDLM outperforms MDLM by 17.6% when generating ImageNet-1K images in an 8step regime. Consequently, balancing the robust few-step generation of UDLMs with the superior semantic understanding of MDLMs remains critical open challenge. To this end, we propose miXed Diffusion Language Modeling (XDLM), balanced theoretical formulation that establishes rigorous bridge between uniform and masked noise distributions. Unlike GIDD (von Rtte et al., 2025), which relies on computationally expensive time-inhomogeneous process where noise characteristics shift at every timestep, XDLM enforces stationary noise kernel where incremental noise structurally matches the marginal noise, as shown in Fig. 1 (left). This consistency allows our training objective to be efficiently factorized into static constants and dynamic schedules, avoiding the complex re-computation of noise distributions required by GIDD. In addition, we prove that MDLM and UDLM are limiting cases of our formulation. Furthermore, XDLM enables memory-efficient implementation by algebraically simplifying posterior calculations, allowing XDLM to scale to large vocabulary sizes without prohibitive computational costs. Balancing Understanding and Generation in Discrete Diffusion Models Figure 1. Left: XDLM combines the noise kernel of UDLM (u) and MDLM (m) to achieve favorable trade-off between the two methods. [NORMAL] denotes normal tokens, while [MASK] represents the mask token. Right: The trade-off between understanding capability (zero-shot perplexity; lower is better) and generation capability (generation perplexity in 32 sampling steps; lower is better). The proposed XDLM with mixing ratio of = 0.1 achieves the optimal balance, labeled as the Sweet Spot. Experimental results demonstrate that XDLM consistently achieves superior performance across diverse modalities, validating the efficacy of our balanced discrete diffusion approach. As illustrated in Fig. 1 (right), XDLM advances the Pareto frontier between understanding capability and generative quality. Notably, as the interpolation parameters shift toward their extremes, XDLM matches the performance of UDLM and MDLM respectively, confirming the models theoretical foundation. Concretely, in zero-shot language benchmarks, XDLM surpasses UDLM by 5.4 points in averaged metrics and trails MDLM by only 0.45. In conditional image generation, XDLM excels in both efficiency and quality: it significantly outperforms MDLM in few-step generation (reducing FID from 80.8 to 54.1 at 4 steps) and ranks first at 16 steps, surpassing UDLM by 0.4 points. Furthermore, large-scale continual pretraining on 8B-parameter LLMs (Nie et al., 2025) yields an MBPP score of 15.0 in only 32 sampling steps, improving upon the vanilla LLaDA baseline by over 120% (15.0 vs. 6.8). Finally, analysis of training dynamics reveals distinct performance crossover: while masked baselines converge quickly but plateau early, XDLM sustains steady improvement throughout training, demonstrating superior long-term scaling. 2. Preliminary Discrete Diffusion Models (DDMs) are latent generative models defined by two Markov processes: forward process and reverse process. Considering single categorical variable x0, represented as one-hot vector in {0, 1}N , where = denotes the vocabulary size. Let zt represent the latent state at time [0, 1], with z0 "
[04.02.2026 04:09] Mistral response. {"id": "798ebd4faa684245a95d6a1a0dce855a", "created": 1770178170, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1558, "total_tokens": 1577, "completion_tokens": 19, "num_cached_tokens": 1557}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\"UCAS\", \"Xiaohongshu Inc.\"]\n```"}}]}
[04.02.2026 04:09] Response: ```python
["UCAS", "Xiaohongshu Inc."]
```
[04.02.2026 04:09] Deleting PDF ./assets/pdf/2602.01362.pdf.
[04.02.2026 04:09] Success.
[04.02.2026 04:09] Downloading and parsing paper https://huggingface.co/papers/2601.19103.
[04.02.2026 04:09] Downloading paper 2601.19103 from https://arxiv.org/pdf/2601.19103v2...
[04.02.2026 04:09] Extracting affiliations from text.
[04.02.2026 04:09] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 2 ] . [ 2 3 0 1 9 1 . 1 0 6 2 : r Accepted as conference paper at ICLR GLANCE AND FOCUS REINFORCEMENT FOR PAN- Linshan Wu, Jiaxin Zhuang & Hao Chen Department of Computer Science and Engineering The Hong Kong University of Science and Technology Hong Kong, China {linshan.wu,jzhuangad}@connect.ust.hk, jhc@cse.ust.hk "
[04.02.2026 04:09] Response: ```python
["The Hong Kong University of Science and Technology"]
```
[04.02.2026 04:09] Deleting PDF ./assets/pdf/2601.19103.pdf.
[04.02.2026 04:09] Success.
[04.02.2026 04:09] Downloading and parsing paper https://huggingface.co/papers/2602.03454.
[04.02.2026 04:09] Downloading paper 2602.03454 from https://arxiv.org/pdf/2602.03454v1...
[04.02.2026 04:09] Extracting affiliations from text.
[04.02.2026 04:09] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Contextualized Visual Personalization in Vision-Language Models Yeongtak Oh * 1 Sangwon Yu * 1 Junsung Park 1 Han Cheol Moon 2 Jisoo Mok 3 Sungroh Yoon 1 4 6 2 0 2 3 ] . [ 1 4 5 4 3 0 . 2 0 6 2 : r a "
[04.02.2026 04:09] Response: ```python
[]
```
[04.02.2026 04:09] Extracting affiliations from text.
[04.02.2026 04:09] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Contextualized Visual Personalization in Vision-Language Models Yeongtak Oh * 1 Sangwon Yu * 1 Junsung Park 1 Han Cheol Moon 2 Jisoo Mok 3 Sungroh Yoon 1 4 6 2 0 2 3 ] . [ 1 4 5 4 3 0 . 2 0 6 2 : r aDespite recent progress in vision-language models (VLMs), existing approaches often fail to generate personalized responses based on the users specific experiences, as they lack the ability to associate visual inputs with users accumulated visual-textual context. We newly formalize this challenge as contextualized visual personalization, which requires the visual recognition and textual retrieval of personalized visual experiences by VLMs when interpreting new images. To address this issue, we propose CoViP, unified framework that treats personalized image captioning as core task for contextualized visual personalization and improves this capability through reinforcement-learning-based post-training and caption-augmented generation. We further introduce diagnostic evaluations that explicitly rule out textual shortcut solutions and verify whether VLMs truly leverage visual context. Extensive experiments demonstrate that existing open-source and proprietary VLMs exhibit substantial limitations, while CoViP not only improves personalized image captioning but also yields holistic gains across downstream personalization tasks. These results highlight CoViP as crucial stage for enabling robust and generalizable contextualized visual personalization. Project Page 1. Introduction Recent advances in vision-language models (VLMs) (Liu et al., 2023; Li et al., 2024; Chen et al., 2024; Bai et al., 2025) have demonstrated impressive performance across wide range of vision-language tasks, including image cap- *Equal contribution 1Department of Electrical and Computer Engineering, Seoul National University, Seoul, South Korea 2Samsung Electronics, South Korea 3Daegu Gyeongbuk Institute of Science and Technology, Daegu, South Korea 4Interdisciplinary Program in Artificial Intelligence, Seoul National University, Seoul, Korea. Correspondence to: Sungroh Yoon <sryoon@snu.ac.kr>. Preprint. February 4, 2026. 1 tioning, visual question answering, and open-ended visual dialogue. Despite these advances, however, current VLMs remain limited in their ability to personalize visual understanding based on user-specific context (Wang et al., 2025; Team, 2025). For example, while VLM may correctly recognize person in an image as man wearing black suit, it typically fails to identify that the person corresponds to the users brother mentioned in prior interactions. This issue indicates that existing VLMs do not yet possess genuine capability for visual personalization. To address this limitation, growing body of work has explored methods for enhancing visual personalization in VLMs (Alaluf et al., 2024; Nguyen et al., 2024; Kim et al., 2025a). While existing methods successfully enable personalization over simple attributes or identities, they remain limited in scope. In particular, they do not account for personalization grounded in rich, experience-level user context, such as past interactions or episodic memories, which remains largely underexplored in existing literature (Hao et al., 2025; Nguyen et al., 2025; Hong et al., 2025; Oh et al., 2025; Doveh et al., 2025). In this work, we define such visual experiences as contextual history that integrates previously observed images with associated personal textual information. We consider setting in which VLMs maintain users past interactions in their context and are expected to leverage this history when interpreting new visual input. We refer to this realistic setting as contextualized visual personalization. Figure 1 illustrates the example of the use-case for contextual visual personalization in VLMs. In practice, personalization in real-world settings is inherently diverse and open-ended, as it depends on implicit user intent and fine-grained contextual cues. Consequently, relying solely on task-specific post-training is both insufficient and inefficient for achieving robust personalization, since it does not scale to the long tail of personalized, contextdependent behaviors in real-world interactions. To address this challenge, we propose unified approach to contextualized visual personalization that targets shared underlying process common across downstream tasks. We argue that this process naturally aligns with the objective of personalized image captioning, which focuses on grounding visual inputs in user-specific contextual knowledge, without reContextualized Visual Personalization in Vision-Language Models Figure 1. Qualitative example of the use-case for contextual visual personalization in VLMs. Note that our CoViP effectively responds to the question while integrating the mentioned personal details from the given multimodal contexts. quiring additional task-specific processing. Accordingly, we leverage personalized image captioning as proxy task to effectively model and learn this shared process. Building on this insight about personalization, we introduce CoViP (Contextualized Visual Personalization via Image Captioning), unified framework that enables holistic personalization by explicitly modeling this underlying process. CoViP formulates contextualized visual personalization as personalized image captioning task, in which VLM recognizes relevant visual concepts in query image, retrieves the corresponding user-specific context, and directly incorporates the retrieved information in the generated caption. Within this framework, we first construct novel and challenging personalized image captioning benchmark that faithfully captures the complexities of contextualized visual personalization. We then adopt reinforcement learning (RL)-based post-training strategy to optimize VLMs for producing personalized captions through our captioning benchmark. At inference time, we further introduce caption-augmented generation (CAG), where the VLMs own generated caption is reused as an explicit conditioning signal to guide personalized response generation. Beyond modeling and training, we place particular emphasis on evaluation, as contextualized visual personalization should avoid spurious textual shortcuts (e.g., directly retrieving query-related hints from context without recognition) that allow VLMs to answer questions while bypassing visual understanding. To this end, we design suite of diagnostic downstream tasks that explicitly assess whether VLM correctly "
[04.02.2026 04:09] Mistral response. {"id": "73ec4a07b649414c94e2f0854f1a36d3", "created": 1770178193, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1344, "total_tokens": 1418, "completion_tokens": 74, "num_cached_tokens": 1343}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\n    \"Department of Electrical and Computer Engineering, Seoul National University, Seoul, South Korea\",\n    \"Samsung Electronics, South Korea\",\n    \"Daegu Gyeongbuk Institute of Science and Technology, Daegu, South Korea\",\n    \"Interdisciplinary Program in Artificial Intelligence, Seoul National University, Seoul, Korea\"\n]\n```"}}]}
[04.02.2026 04:09] Response: ```python
[
    "Department of Electrical and Computer Engineering, Seoul National University, Seoul, South Korea",
    "Samsung Electronics, South Korea",
    "Daegu Gyeongbuk Institute of Science and Technology, Daegu, South Korea",
    "Interdisciplinary Program in Artificial Intelligence, Seoul National University, Seoul, Korea"
]
```
[04.02.2026 04:09] Deleting PDF ./assets/pdf/2602.03454.pdf.
[04.02.2026 04:09] Success.
[04.02.2026 04:09] Downloading and parsing paper https://huggingface.co/papers/2602.03048.
[04.02.2026 04:09] Downloading paper 2602.03048 from https://arxiv.org/pdf/2602.03048v1...
[04.02.2026 04:10] Extracting affiliations from text.
[04.02.2026 04:10] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"CoBA-RL: Capability-Oriented Budget Allocation for Reinforcement Learning in LLMs Zhiyuan Yao 1 4 Yi-Kai Zhang 2 4 Yuxin Chen 3 4 Yueqing Sun 4 Zishan Xu 5 Yu Yang 4 Tianhao Hu 4 Qi Gu 4 Hui Su 4 Xunliang Cai "
[04.02.2026 04:10] Response: ```python
[]
```
[04.02.2026 04:10] Extracting affiliations from text.
[04.02.2026 04:10] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"CoBA-RL: Capability-Oriented Budget Allocation for Reinforcement Learning in LLMs Zhiyuan Yao 1 4 Yi-Kai Zhang 2 4 Yuxin Chen 3 4 Yueqing Sun 4 Zishan Xu 5 Yu Yang 4 Tianhao Hu 4 Qi Gu 4 Hui Su 4 Xunliang CaiReinforcement Learning with Verifiable Rewards (RLVR) has emerged as key approach for enhancing LLM reasoning.However, standard frameworks like Group Relative Policy Optimization (GRPO) typically employ uniform rollout budget, leading to resource inefficiency. Moreover, existing adaptive methods often rely on instance-level metrics, such as task pass rates, failing to capture the models dynamic learning state. To address these limitations, we propose CoBA-RL, reinforcement learning algorithm designed to adaptively allocate rollout budgets based on the models evolving capability. Specifically, CoBA-RL utilizes Capability-Oriented Value function to map tasks to their potential training gains and employs heap-based greedy strategy to efficiently self-calibrate the distribution of computational resources to samples with high training value. Extensive experiments demonstrate that our approach effectively orchestrates the trade-off between exploration and exploitation, delivering consistent generalization improvements across multiple challenging benchmarks. These findings underscore that quantifying sample training value and optimizing budget allocation are pivotal for advancing LLM post-training efficiency. Our code is available at https: //github.com/Within-yao/CoBA-RL. 6 2 0 2 3 ] . [ 1 8 4 0 3 0 . 2 0 6 2 : r 1. Introduction Reinforcement Learning with Verifiable Rewards (RLVR) (Lambert et al., 2024) has established itself as cornerstone for elevating LLM reasoning in agentic, coding, and mathematical domains (Guo et al., 2025; Work done during an internship at Meituan. 1Zhejiang University 2Nanjing University 3National University of Singapore 4Meituan 5Shanghai Jiao Tong University. Correspondence to: Zhiyuan Yao, Qi Gu. Preprint. February 4, 2026. 1 Figure 1. Comparison between GRPO-based methods and CoBA-RL. (a) GRPO employs uniform strategy independent of training progress. (b) CoBA-RL dynamically self-calibrates the allocation strategy throughout the training process. It autonomously directs the rollout budget toward instances with high training value, aligned with the models evolving capability. In this visualization, pi denotes the pass rate corresponding to the task instance xi. Yang et al., 2025; Comanici et al., 2025; Bai et al., 2025). Within this landscape, Group Relative Policy Optimization (GRPO) (Shao et al., 2024) and its variants (Yu et al., 2025; Zheng et al., 2025a; Liu et al., 2026; Shrivastava et al., 2025) have risen to prominence. By assigning uniform budget of rollouts to every prompt to compute group-relative advantages, GRPO eliminates the need for separate value network. However, ideally, the rollout budgets allocated to each instance should be commensurate with its training value.Intuitively, complex samples often harbor higher training value and demand extensive exploration, whereas simple instances require minimal resources. In practice, vanilla GRPO overlooks the critical impact of sample difficulty on training value and the corresponding rollout budget. While recent studies have begun to tailor budgets using instance-level metrics related to task difficulty, such as historical pass rates (Li et al., 2025b), these approaches typically rely on static value functions. They operate on the fixed assumption that harder samples inherently offer superior training value than simpler ones and that this relationship remains constant throughout the entire training CoBA-RL: Capability-Oriented Budget Allocation for Reinforcement Learning in LLMs process. This perspective neglects crucial reality: the true training value of sample is inextricably linked to the policy models real-time capabilities (Wang et al., 2025b; Zhang et al., 2025b; Wang et al., 2025a). As the models capabilities evolve during training, the set of samples holding the highest training value constantly shifts (Hu et al., 2025). To accommodate these variations, the allocation strategy must continuously calibrate the tradeoff between exploitation and exploration. Specifically, exploitation involves consolidating mastery over instances where the model already succeeds, whereas exploration requires allocating resources to sample diverse trajectories on challenging queries, thereby expanding the search space to discover potential solutions (Yang et al., 2026; Chen et al., 2025c; Hou et al., 2025; Cui et al., 2025). Consequently, it is imperative to quantify model capability to facilitate policy self-calibration, ensuring the rollout budget is continually re-aligned with the samples most suitable for the current training phase. To tackle the aforementioned challenges, We propose CoBARL, reinforcement learning algorithm that dynamically allocates the rollout budget in accordance with the models evolving capability. Specifically, we introduce CapabilityOriented Value function, modeled as Beta distribution, to map individual instances to their potential training value. By continuously monitoring the global failure rate of the current training batch, we quantify the models global capability and dynamically calibrate the shape of the value function, as shown in Figure 1. This mechanism autonomously orchestrates the exploration-exploitation trade-off: the value function shifts its high-density regions in real-time to prioritize either consolidating established knowledge or exploring high-uncertainty frontiers based on the models current competence. Finally, to operationalize this theoretical distribution, we present an Efficient Allocation Optimization algorithm. By formulating the allocation as constrained maximization problem, we employ heap-based greedy strategy that iteratively assigns budget to samples offering the highest marginal gain, thereby maximizing the aggregate value of the training batch. To validate the efficacy of our approach, we conducted extensive experiments using Qwen2.5-7B-Base, Qwen2.5-7BInstruct, and Qwen3-1.7B/4B-Base models. Empirical results demonstrate that our method significantly outperforms strong baselines across multiple challenging mathematical benchmarks. These findings suggest that, empowered by the Capability-Oriented Value function, CoBA-RL effectively identifies samples holding the high training value at the current training step, thereby achieving superior tra"
[04.02.2026 04:10] Mistral response. {"id": "f768e3ae7fbb4bd99815a537c048ae6f", "created": 1770178202, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1451, "total_tokens": 1492, "completion_tokens": 41, "num_cached_tokens": 1450}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\n    \"Zhejiang University\",\n    \"Nanjing University\",\n    \"National University of Singapore\",\n    \"Meituan\",\n    \"Shanghai Jiao Tong University\"\n]\n```"}}]}
[04.02.2026 04:10] Response: ```python
[
    "Zhejiang University",
    "Nanjing University",
    "National University of Singapore",
    "Meituan",
    "Shanghai Jiao Tong University"
]
```
[04.02.2026 04:10] Deleting PDF ./assets/pdf/2602.03048.pdf.
[04.02.2026 04:10] Success.
[04.02.2026 04:10] Downloading and parsing paper https://huggingface.co/papers/2602.02914.
[04.02.2026 04:10] Downloading paper 2602.02914 from https://arxiv.org/pdf/2602.02914v1...
[04.02.2026 04:10] Extracting affiliations from text.
[04.02.2026 04:10] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"FaceLinkGen: Rethinking Identity Leakage in Privacy-Preserving Face Recognition with Identity Extraction Wenqi Guo 1 2 Shan Du 1 6 2 0 2 2 ] . [ 1 4 1 9 2 0 . 2 0 6 2 : r Abstract Transformation-based privacy-preserving face recognition (PPFR) aims to verify identities while hiding facial data from attackers and malicious service providers. Existing evaluations mostly treat privacy as resistance to pixel-level reconstruction, measured by PSNR and SSIM. We show that this reconstruction-centric view fails. We present FaceLinkGen, an identity extraction attack that performs linkage/matching and face regeneration directly from protected templates without recovering original pixels. On three recent PPFR systems, FaceLinkGen reaches over 98.5% matching accuracy and above 96% regeneration success, and still exceeds 92% matching and 94% regeneration in near zero knowledge setting. These results expose structural gap between pixel distortion metrics, which are widely used in PPFR evaluation, and real privacy. We show that visual obfuscation leaves identity information broadly exposed to both external intruders and untrusted service providers. 1. Introduction and Related Works The fundamental promise of transformation-based Privacy Preserving Face Recognition (PPFR) systems is compelling: verify users identity without ever exposing their raw facial data to potential attackers (Dai et al., 2025; Mi et al., 2023; 2024; 2022; Jin et al., 2024). Originally, the threat model was for curious or malicious recognition service provider (Erkin et al., 2009; Ji et al., 2022), but it now also includes, or shifted towards, external attackers by wiretapping (Mi et al., 2024) or leaked databases. We argue that robust leakage analysis must address both the service provider who inherently accesses the templates and the external intruder who obtains the templates by wiretapping or database leakage. 1Department of CMPS, University of British Columbia, Kelowna, Canada 2Weathon Software, Canada. "
[04.02.2026 04:10] Response: ```python
[
    "Department of CMPS, University of British Columbia, Kelowna, Canada",
    "Weathon Software, Canada"
]
```
[04.02.2026 04:10] Deleting PDF ./assets/pdf/2602.02914.pdf.
[04.02.2026 04:10] Success.
[04.02.2026 04:10] Downloading and parsing paper https://huggingface.co/papers/2602.02537.
[04.02.2026 04:10] Downloading paper 2602.02537 from https://arxiv.org/pdf/2602.02537v1...
[04.02.2026 04:10] Extracting affiliations from text.
[04.02.2026 04:10] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"WORLDVQA: MEASURING ATOMIC WORLD KNOWLEDGE IN MULTIMODAL LARGE LANGUAGE MODELS Runjie Zhou1 Youbo Shao1 Haoyu Lu1 Bowei Xing1 Tongtong Bai1 Yujie Chen1 Jie Zhao1 Lin Sui1 Haotian Yao1 Zijia Zhao1 Hao Yang1 Haoning Wu1 Zaida Zhou1 Jinguo Zhu1 Zhiqi Huang1 Yiping Bao1 Yangyang Liu1 Y.Charles1 Xinyu Zhou1 1 Moonshot AI "
[04.02.2026 04:10] Response: ```python
["Moonshot AI"]
```
[04.02.2026 04:10] Deleting PDF ./assets/pdf/2602.02537.pdf.
[04.02.2026 04:10] Success.
[04.02.2026 04:10] Downloading and parsing paper https://huggingface.co/papers/2602.03837.
[04.02.2026 04:10] Downloading paper 2602.03837 from https://arxiv.org/pdf/2602.03837v1...
[04.02.2026 04:10] Extracting affiliations from text.
[04.02.2026 04:10] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 3 ] . [ 1 7 3 8 3 0 . 2 0 6 2 : r Accelerating Scientific Research with Gemini: Case Studies and Common Techniques David P. Woodruff*, , 1,2, Vincent Cohen-Addad, 1, Lalit Jain1, Jieming Mao1, Song Zuo, 1, MohammadHossein Bateni1, Simina Brnzei3,1, Michael P. Brenner1,5, Lin Chen1, Ying Feng6, Lance Fortnow7, Gang Fu1, Ziyi Guan13, Zahra Hadizadeh10, Mohammad T. Hajiaghayi1,14, Mahdi JafariRaviz14, Adel Javanmard1,4, Karthik C. S.8, Ken-ichi Kawarabayashi12, Ravi Kumar1, Silvio Lattanzi1, Euiwoong Lee9, Yi Li15, Ioannis Panageas10, Dimitris Paparas1, Benjamin Przybocki2, Bernardo Subercaseaux2, Ola Svensson13, Shayan Taherijam10, Xuan Wu15, Eylon Yogev 16, Morteza Zadimoghaddam1, Samson Zhou11, and Vahab Mirrokni*, , 1Google Research 2Carnegie Mellon University 3Purdue University 4University of Southern California 5Harvard University 6MIT 7Illinois Institute of Technology 8Rutgers University 9University of Michigan 10University of California, Irvine 11Texas A&M University 12National Institute of Informatics, Tokyo and The University of Tokyo 13EPFL 14University of Maryland, College Park 15Nanyang Technological University 16Bar-Ilan University Abstract Recent advances in large language models (LLMs) have opened new avenues for accelerating scientific research. While models are increasingly capable of assisting with routine tasks, their ability to contribute to novel, expert-level mathematical discovery is less understood. We present collection of case studies demonstrating how researchers have successfully collaborated with advanced AI models, specifically Googles Gemini-based models (in particular Gemini Deep Think and its advanced variants), to solve open problems, refute conjectures, and generate new proofs across diverse areas in theoretical computer science, as well as other areas such as economics, optimization, and physics. Based on these experiences, we extract common techniques for effective human-AI collaboration in theoretical research, such as it"
[04.02.2026 04:10] Response: ```python
[
    "Google Research",
    "Carnegie Mellon University",
    "Purdue University",
    "University of Southern California",
    "Harvard University",
    "MIT",
    "Illinois Institute of Technology",
    "Rutgers University",
    "University of Michigan",
    "University of California, Irvine",
    "Texas A&M University",
    "National Institute of Informatics, Tokyo",
    "The University of Tokyo",
    "EPFL",
    "University of Maryland, College Park",
    "Nanyang Technological University",
    "Bar-Ilan University"
]
```
[04.02.2026 04:10] Deleting PDF ./assets/pdf/2602.03837.pdf.
[04.02.2026 04:10] Success.
[04.02.2026 04:10] Downloading and parsing paper https://huggingface.co/papers/2602.03796.
[04.02.2026 04:10] Downloading paper 2602.03796 from https://arxiv.org/pdf/2602.03796v1...
[04.02.2026 04:10] Extracting affiliations from text.
[04.02.2026 04:10] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"3D-Aware Implicit Motion Control for View-Adaptive Human Video Generation Zhixue Fang1, Xu He2, Songlin Tang1, Haoxian Zhang1,,(cid:66) Qingfeng Li3 Xiaoqiang Liu1 1Kling Team, Kuaishou Technology Pengfei Wan1 Kun Gai1 2Tsinghua University 3CASIA Equal contribution Project leader https://hjrphoebus.github.io/3DiMo (cid:66)Corresponding author 6 2 0 2 3 ] . [ 1 6 9 7 3 0 . 2 0 6 2 : r Figure 1. 3DiMo can faithfully reproduce the 3D spatial motion from 2D driving video, supporting flexible text-guided camera control. "
[04.02.2026 04:10] Response: ```python
[
    "Kling Team, Kuaishou Technology",
    "Tsinghua University",
    "CASIA"
]
```
[04.02.2026 04:10] Deleting PDF ./assets/pdf/2602.03796.pdf.
[04.02.2026 04:10] Success.
[04.02.2026 04:10] Downloading and parsing paper https://huggingface.co/papers/2602.01753.
[04.02.2026 04:10] Downloading paper 2602.01753 from https://arxiv.org/pdf/2602.01753v2...
[04.02.2026 04:10] Extracting affiliations from text.
[04.02.2026 04:10] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"ObjEmbed: Towards Universal Multimodal Object Embeddings Shenghao Fu 1 2 Yukun Su 3 Fengyun Rao 3 Jing LYU 3 Xiaohua Xie 1 4 5 6 Wei-Shi Zheng 1 2 4 "
[04.02.2026 04:10] Response: ```python
[]
```
[04.02.2026 04:10] Extracting affiliations from text.
[04.02.2026 04:10] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"ObjEmbed: Towards Universal Multimodal Object Embeddings Shenghao Fu 1 2 Yukun Su 3 Fengyun Rao 3 Jing LYU 3 Xiaohua Xie 1 4 5 6 Wei-Shi Zheng 1 2 4Aligning objects with corresponding textual descriptions is fundamental challenge and realistic requirement in vision-language understanding. While recent multimodal embedding models excel at global image-text alignment, they often struggle with fine-grained alignment between image regions and specific phrases. In this work, we present ObjEmbed, novel MLLM embedding model that decomposes the input image into multiple regional embeddings, each corresponding to an individual object, along with global embeddings. It supports wide range of visual understanding tasks like visual grounding, local image retrieval, and global image retrieval. ObjEmbed enjoys three key properties: (1) ObjectOriented Representation: It captures both semantic and spatial aspects of objects by generating two complementary embeddings for each region: an object embedding for semantic matching and an IoU embedding that predicts localization quality. The final object matching score combines semantic similarity with the predicted IoU, enabling more accurate retrieval. (2) Versatility: It seamlessly handles both region-level and image-level tasks. (3) Efficient Encoding: All objects in an image, along with the full image, are encoded in single forward pass for high efficiency. Superior performance on 18 diverse benchmarks demonstrates its strong semantic discrimination. Code is available at https: //github.com/WeChatCV/ObjEmbed. 6 2 0 2 3 ] . [ 2 3 5 7 1 0 . 2 0 6 2 : r 1School of Computer Science and Engineering, Sun Yat-sen University, China 2Peng Cheng Laboratory, China 3Independent Researcher 4Key Laboratory of Machine Intelligence and Advanced Computing, Ministry of Education, China 5Guangdong Province Key Laboratory of Information Security Technology, China 6Pazhou Laboratory (Huangpu), China. Correspondence to: Xiaohua Xie <xiexiaoh6@mail.sysu.edu.cn>, Wei-Shi Zheng <wszheng@ieee.org>. Preprint. February 4, 2026. 1 Figure 1. ObjEmbed achieves balanced and superior performance across wide span of benchmarks. 1. Introduction Multimodal embedding models have emerged as cornerstone in bridging heterogeneous data modalities, such as vision, language, and audio, into unified semantic space, enabling rich cross-modal understanding, retrieval, and reasoning. Recent advances in large-scale image-text contrastive learning (Radford et al., 2021; Zhai et al., 2023; Xie et al., 2025b) have led to significant progress in multimodal representation learning, especially in aligning images and corresponding captions. Powered by large multimodal models (Bai et al., 2025b;a), embedding models (Zhang et al., 2024; 2025; Jiang et al., 2025c; Lan et al., 2025) can generate task-specific embeddings following user instructions, generate cross-modal embeddings with arbitrary modality combinations, and even perform high-level summary and deep reasoning through chain-of-thought. However, encoding objects within images and aligning them with text queries are still challenging for recent embedding ObjEmbed: Towards Universal Multimodal Object Embeddings models. Such capabilities are often critical in real-world applications such as autonomous driving (e.g., distant traffic signs), robotics (e.g., small parts manipulation), and digital content safety moderation. Reliable retrieval and representation of objects demand precise localization and strong semantic discrimination, yet remain under-addressed in current frameworks. While FG-CLIP (Xie et al., 2025b;a) improves regional alignment by combining regional and global contrastive objectives, its object embeddings lack explicit modeling of bounding box quality, limiting their reliability in localization-sensitive tasks. Another line of research, open-vocabulary object detection (Liu et al., 2024b; Fu et al., 2025c;b), directly aligning text embeddings with regions of interest, can accurately localize objects and perform openvocabulary recognition. However, due to limited training data, their generalization ability is largely constrained. To encode objects with high semantic discrimination and precise localization awareness, we introduce ObjEmbed, an MLLM-based object embedding model that encodes all objects within an image as embeddings. Given an input image, regions of interest (RoIs) are first extracted using an off-theshelf proposal generator and then encoded as sequence of tokens. Each object is represented by two special tokens: (1) an object token to capture fine-grained semantic content; and (2) an IoU token to predict the quality of the corresponding bounding box by regressing its IoU score with the ground truth. The object and IoU tokens, along with global image tokens, are processed in parallel by large language model (LLM) to ensure efficiency. The final-layer hidden states of these tokens serve as the object embeddings, IoU embeddings, and image embeddings, respectively. Similarly, text queries are independently encoded into text embeddings through the same LLM backbone, enabling seamless crossmodal alignment. With this novel architecture, ObjEmbed produces object-centric representations that jointly encode semantic meaning and localization confidence, enabling accurate recognition, precise localization, and robust crossmodal retrieval. Equipped with this object-centric design, ObjEmbed supports wide range of downstream applications in unified framework: (1) Object detection and referring expression comprehension: Class names or natural language expressions are encoded as text embeddings and matched against all object embeddings in the image. The final object matching score combines both semantic similarity (between object and text embeddings) and predicted localization quality (from the IoU embedding), computed as their product, effectively balancing semantic relevance and spatial accuracy. (2) Local image retrieval: When the query describes only specific region or object, we compute the image-level relevance as the maximum matching score across all detected objects. This strategy enables fine-grained, part-aware retrieval even when the target occupies small portion of the image. (3) Global image retrieval: Since ObjEmbed also retains global image embeddings, they can be directly used for standard image-text retrieval tasks, ensuring compatibility with conventional benchmarks and applications. After training on 1.3M samples, ObjE"
[04.02.2026 04:10] Mistral response. {"id": "b81e5188c7c041ed8b49d02dfe78d716", "created": 1770178255, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1410, "total_tokens": 1494, "completion_tokens": 84, "num_cached_tokens": 1409}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\n    \"School of Computer Science and Engineering, Sun Yat-sen University, China\",\n    \"Peng Cheng Laboratory, China\",\n    \"Independent Researcher\",\n    \"Key Laboratory of Machine Intelligence and Advanced Computing, Ministry of Education, China\",\n    \"Guangdong Province Key Laboratory of Information Security Technology, China\",\n    \"Pazhou Laboratory (Huangpu), China\"\n]\n```"}}]}
[04.02.2026 04:10] Response: ```python
[
    "School of Computer Science and Engineering, Sun Yat-sen University, China",
    "Peng Cheng Laboratory, China",
    "Independent Researcher",
    "Key Laboratory of Machine Intelligence and Advanced Computing, Ministry of Education, China",
    "Guangdong Province Key Laboratory of Information Security Technology, China",
    "Pazhou Laboratory (Huangpu), China"
]
```
[04.02.2026 04:10] Deleting PDF ./assets/pdf/2602.01753.pdf.
[04.02.2026 04:10] Success.
[04.02.2026 04:10] Downloading and parsing paper https://huggingface.co/papers/2602.03817.
[04.02.2026 04:10] Downloading paper 2602.03817 from https://arxiv.org/pdf/2602.03817v1...
[04.02.2026 04:11] Extracting affiliations from text.
[04.02.2026 04:11] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Adaptive Evidence Weighting for Audio-Spatiotemporal Fusion Oscar Ovanger 1 Levi Harris 2 Timothy H. Keitt "
[04.02.2026 04:11] Response: ```python
[]
```
[04.02.2026 04:11] Extracting affiliations from text.
[04.02.2026 04:11] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Adaptive Evidence Weighting for Audio-Spatiotemporal Fusion Oscar Ovanger 1 Levi Harris 2 Timothy H. KeittMany machine learning systems have access to multiple sources of evidence for the same prediction target, yet these sources often differ in reliability and informativeness across inputs. In bioacoustic classification, species identity may be inferred both from the acoustic signal and from spatiotemporal context such as location and season; while Bayesian inference motivates multiplicative evidence combination, in practice we typically only have access to discriminative predictors rather than calibrated generative models. We introduce Fusion under INdependent Conditional Hypotheses (FINCH), an adaptive log-linear evidence fusion framework that integrates pretrained audio classifier with structured spatiotemporal predictor. FINCH learns per-sample gating function that estimates the reliability of contextual information from uncertainty and informativeness statistics. The resulting fusion family contains the audio-only classifier as special case and explicitly bounds the influence of contextual evidence, yielding risk-contained hypothesis class with an interpretable audio-only fallback. Across benchmarks, FINCH consistently outperforms fixed-weight fusion and audio-only baselines, improving robustness and error trade-offs even when contextual information is weak in isolation. We achieve state-of-the-art performance on CBI and competitive or improved performance on several subsets of BirdSet using lightweight, interpretable, evidence-based approach. Code is available: anonymous-repository 6 2 0 2 3 ] . [ 1 7 1 8 3 0 . 2 0 6 2 : r 1. Introduction Ensemble learning studies how multiple predictive models for the same target variable can be combined to improve performance, robustness, or calibration relative to any single model (Kuncheva, 2004; Kittler et al., 1998). Classical ensemble methods such as bagging, boosting, and stacking typically assume that all predictors are trained on the same underlying data distribution and that ensemble weights or 1 Figure 1. Haemorhous mexicanus (North American House Finch), the namesake of our model. combination rules can be learned jointly with model parameters. In many modern applications, however, predictive models are pre-trained, heterogeneous, and fixed at inference time. These models may rely on different sources of evidence, be trained on distinct datasets, and operate under incompatible modeling assumptions. As result, joint retraining or finetuning is often infeasible. Instead, the problem becomes one of combining the outputs of fixed predictors in principled manner. We consider the common setting in which multiple predictive models provide complementary evidence about shared latent target. Let denote class label, and let and denote two sources of evidence. Conditional independence, y, (1) provides useful idealization that motivates multiplicative evidence fusion. If the full generative distributions were available, Bayesian inference would yield p(y x, s) p(x y) p(s y) p(y). (2) FINCH adopts the corresponding log-linear fusion form while remaining applicable in regimes where this independence assumption holds only approximately, and deviations can be mitigated through adaptive, bounded weighting. In practice, however, generative models are often unavailable. Instead, one typically has access only to discriminative predictors p(y x) and p(y s), trained independently. In this case, the posterior can be expressed as p(y x, s) p(y x) p(x) p(y s) p(s) p(y) , (3) Adaptive Evidence Weighting for Audio-Spatiotemporal Fusion where the marginal distributions p(x), p(s), and the implied prior p(y) are generally unknown. As result, the true posterior is not directly computable from the discriminative models alone. common approximation in this setting is to combine discriminative predictors using log-linear models or productof-experts formulations (Hinton, 2002; Genest & Zidek, 1986). These models operate directly on posterior outputs and define fused distribution of the form log p(y x, s) = log p(y x)+log p(y s)log Z(x, s), (4) where Z(x, s) is normalization constant ensuring (cid:80) p(y x, s) = 1. Throughout the paper, we write fusion rules in unnormalized log-space; the normalization constant is implicitly handled by the final softmax and is therefore omitted when it does not affect comparisons across classes. Log-linear fusion is decision-theoretically justified as logarithmic opinion pool under mild axioms (Heskes, 1998) and provides practical surrogate when only discriminative models are available. Existing approaches typically employ fixed or globally learned weights when combining predictors. This implicitly assumes that the relative reliability of each evidence source is constant across the input space. In many applications, this assumption does not hold. The informativeness of given evidence source may vary substantially across samples, leading to degraded performance or pathological dominance when fixed weights are used. In this work, we introduce an adaptive log-linear fusion framework that preserves the structure and interpretability of classical product-of-experts models while allowing persample modulation of evidence strength. We assume that the constituent predictors are fixed and discriminative, and we do not retrain or recalibrate their parameters. Instead, we learn gating function that estimates the reliability of contextual evidence on per-sample basis. The resulting fused posterior is defined (up to normalization) as log p(y x, s) = log p(y x) + (x, s) log p(y s), (5) where (x, s) 0 is learned weighting function. The normalized posterior p(y x, s) is obtained by applying softmax over y. This formulation recovers the first model classifier when (x, s) = 0, bounds the influence of contextual evidence, and enables adaptive fusion without retraining the base models. We evaluate this framework in the context of bioacoustic species classification by combining state-of-the-art acoustic classifier with structured spatiotemporal prior derived from large-scale observational data. Experiments show that adaptive weighting consistently outperforms fixed-weight fusion and audio-only baselines, particularly in regimes where contextual information is heterogeneous or weak in isolation. 2. Related Work We organize related work into two themes: (i) theory and mechanisms for combining probabilistic predictors (loglinear pooling, p"
[04.02.2026 04:11] Mistral response. {"id": "616f12b63bde4fc0bdcec2384adae507", "created": 1770178260, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1369, "total_tokens": 1384, "completion_tokens": 15, "num_cached_tokens": 1368}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\n    \"1\",\n    \"2\"\n]\n```"}}]}
[04.02.2026 04:11] Response: ```python
[
    "1",
    "2"
]
```
[04.02.2026 04:11] Deleting PDF ./assets/pdf/2602.03817.pdf.
[04.02.2026 04:11] Success.
[04.02.2026 04:11] Downloading and parsing paper https://huggingface.co/papers/2602.03647.
[04.02.2026 04:11] Downloading paper 2602.03647 from https://arxiv.org/pdf/2602.03647v1...
[04.02.2026 04:11] Extracting affiliations from text.
[04.02.2026 04:11] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 3 ] . [ 1 7 4 6 3 0 . 2 0 6 2 : r 2026-02Search-R2: Enhancing Search-Integrated Reasoning via Actor-Refiner Collaboration Bowei He, Minda Hu, Zenan Xu, Hongru Wang, Licheng Zong, Yankai Chen Chen Ma, Xue Liu, Pluto Zhou, Irwin King The Chinese University of Hong Kong LLM Department, Tencent Mohamed bin Zayed University of Artificial Intelligence McGill University City University of Hong Kong The University of Edinburgh "
[04.02.2026 04:11] Response: ```python
[
    "The Chinese University of Hong Kong",
    "LLM Department, Tencent",
    "Mohamed bin Zayed University of Artificial Intelligence",
    "McGill University",
    "City University of Hong Kong",
    "The University of Edinburgh"
]
```
[04.02.2026 04:11] Deleting PDF ./assets/pdf/2602.03647.pdf.
[04.02.2026 04:11] Success.
[04.02.2026 04:11] Downloading and parsing paper https://huggingface.co/papers/2602.02660.
[04.02.2026 04:11] Downloading paper 2602.02660 from https://arxiv.org/pdf/2602.02660v1...
[04.02.2026 04:11] Extracting affiliations from text.
[04.02.2026 04:11] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"2026-2-4 MARS: Modular Agent with Reflective Search for Automated AI Research Jiefeng Chen1, Bhavana Dalvi Mishra1, Jaehyun Nam1, Rui Meng1, Tomas Pfister1 and Jinsung Yoon1 1Google Cloud AI Research Automating AI research differs from general software engineering due to computationally expensive evaluation (e.g., model training) and opaque performance attribution. Current LLM-based agents struggle here, often generating monolithic scripts that ignore execution costs and causal factors. We introduce MARS (Modular Agent with Reflective Search), framework optimized for autonomous AI research. MARS relies on three pillars: (1) Budget-Aware Planning via cost-constrained Monte Carlo Tree Search (MCTS) to explicitly balance performance with execution expense; (2) Modular Construction, employing Design-Decompose-Implement pipeline to manage complex research repositories; and (3) Comparative Reflective Memory, which addresses credit assignment by analyzing solution differences to distill high-signal insights. MARS achieves state-of-the-art performance among open-source frameworks on MLE-Bench under comparable settings, maintaining competitiveness with the global leaderboards top methods. Furthermore, the system exhibits qualitative Aha! moments, where 63% of all utilized lessons originate from cross-branch transfer, demonstrating that the agent effectively generalizes insights across search paths. 1. Introduction The integration of Large Language Models (LLMs) into software engineering has fundamentally transformed code generation, evolving from simple auto-completion to autonomous agents capable of resolving GitHub issues Jimenez et al. (2023); Yang et al. (2024) and generating functional scripts (Jiang et al., 2025; Li et al., 2022; Wang et al., 2024). However, while current agents excel at general software maintenance tasks such as patching bugs or writing unit tests they face significant hurdles when applied to the domain of Automating AI Research (Chan et al., 2024; St"
[04.02.2026 04:11] Response: ```python
["Google Cloud AI Research"]
```
[04.02.2026 04:11] Deleting PDF ./assets/pdf/2602.02660.pdf.
[04.02.2026 04:11] Success.
[04.02.2026 04:11] Enriching papers with extra data.
[04.02.2026 04:11] ********************************************************************************
[04.02.2026 04:11] Abstract 0. Multimodal large language models can effectively understand source code when represented as compressed images, achieving significant token reduction while maintaining or improving performance on code comprehension tasks.  					AI-generated summary 				 Large Language Models (LLMs) have achieved rema...
[04.02.2026 04:11] ********************************************************************************
[04.02.2026 04:11] Abstract 1. Current world models lack unified frameworks despite task-specific advances, necessitating a comprehensive approach integrating interaction, perception, symbolic reasoning, and spatial representation.  					AI-generated summary 				 World models have emerged as a critical frontier in AI research, ai...
[04.02.2026 04:11] ********************************************************************************
[04.02.2026 04:11] Abstract 2. DeepResearch report generation is improved through human-preference-aligned rubric generators trained via reinforcement learning with hybrid rewards and enhanced with multi-agent Markov-state workflows.  					AI-generated summary 				 Nowadays, training and evaluating DeepResearch-generated reports ...
[04.02.2026 04:11] ********************************************************************************
[04.02.2026 04:11] Abstract 3. UnifiedReward-Flex combines reward modeling with flexible, context-adaptive reasoning to improve visual generation by dynamically constructing hierarchical assessments based on semantic intent and visual evidence.  					AI-generated summary 				 Recent advancements in multimodal reward models (RMs) ...
[04.02.2026 04:11] ********************************************************************************
[04.02.2026 04:11] Abstract 4. Parallel-Probe is a training-free controller that optimizes parallel thinking by using consensus-based early stopping and deviation-based branch pruning to reduce computational costs while maintaining accuracy.  					AI-generated summary 				 Parallel thinking has emerged as a promising paradigm for...
[04.02.2026 04:11] ********************************************************************************
[04.02.2026 04:11] Abstract 5. AdaptMMBench presents a comprehensive benchmark for evaluating adaptive multimodal reasoning in Vision-Language Models, measuring reasoning mode selection rationality through dynamic difficulty assessment and multi-dimensional process evaluation.  					AI-generated summary 				 Adaptive multimodal r...
[04.02.2026 04:11] ********************************************************************************
[04.02.2026 04:11] Abstract 6. Wide Research advances search intelligence through a dedicated benchmark and multi-agent architecture that enables parallel information retrieval under complex constraints.  					AI-generated summary 				 Search intelligence is evolving from Deep Research to Wide Research, a paradigm essential for r...
[04.02.2026 04:11] ********************************************************************************
[04.02.2026 04:11] Abstract 7. LENS framework improves reinforcement learning with verifiable rewards by identifying and removing interference tokens to enhance exploration efficiency and training stability.  					AI-generated summary 				 Reinforcement Learning with Verifiable Rewards (RLVR) has advanced LLM reasoning, but remai...
[04.02.2026 04:11] ********************************************************************************
[04.02.2026 04:11] Abstract 8. Research investigates latent planning dynamics in large language models through a probing method called Tele-Lens, revealing limited global planning and enabling improved uncertainty estimation and CoT bypass recognition.  					AI-generated summary 				 This work stems from prior complementary obser...
[04.02.2026 04:11] ********************************************************************************
[04.02.2026 04:11] Abstract 9. DeMix is a framework that uses model merging to predict optimal data ratios for LLM pre-training, decoupling search from training costs to improve mixture discovery efficiency.  					AI-generated summary 				 Determining an effective data mixture is a key factor in Large Language Model (LLM) pre-tra...
[04.02.2026 04:11] ********************************************************************************
[04.02.2026 04:11] Abstract 10. XDLM unifies Masked Diffusion Language Models and Uniform-noise Diffusion Language Models through a stationary noise kernel, achieving improved performance in both semantic understanding and generation quality.  					AI-generated summary 				 In discrete generative modeling, two dominant paradigms d...
[04.02.2026 04:11] ********************************************************************************
[04.02.2026 04:11] Abstract 11. A reinforcement learning framework with glance and focus models improves pan-cancer screening in CT scans by addressing foreground-background imbalance and reducing false positives through group relative learning.  					AI-generated summary 				 Pan-cancer screening in large-scale CT scans remains c...
[04.02.2026 04:11] ********************************************************************************
[04.02.2026 04:11] Abstract 12. CoViP addresses contextualized visual personalization by treating personalized image captioning as a core task and improving capabilities through reinforcement-learning-based post-training and caption-augmented generation.  					AI-generated summary 				 Despite recent progress in vision-language mo...
[04.02.2026 04:11] ********************************************************************************
[04.02.2026 04:11] Abstract 13. CoBA-RL adapts rollout budget allocation for LLM training by evaluating sample training value and optimizing resource distribution through a capability-oriented value function and greedy strategy.  					AI-generated summary 				 Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a ...
[04.02.2026 04:11] ********************************************************************************
[04.02.2026 04:11] Abstract 14. FaceLinkGen attack demonstrates that current privacy-preserving face recognition methods fail to protect identity information despite pixel-level distortion metrics suggesting adequate protection.  					AI-generated summary 				 Transformation-based privacy-preserving face recognition (PPFR) aims to...
[04.02.2026 04:11] ********************************************************************************
[04.02.2026 04:11] Abstract 15. WorldVQA is a benchmark for evaluating the visual world knowledge of multimodal large language models by separating visual knowledge retrieval from reasoning to measure memorized facts.  					AI-generated summary 				 We introduce WorldVQA, a benchmark designed to evaluate the atomic visual world kn...
[04.02.2026 04:11] ********************************************************************************
[04.02.2026 04:11] Abstract 16. Advanced AI models demonstrate capability in supporting expert-level mathematical discovery and scientific research through collaborative approaches involving proof verification and automated code execution.  					AI-generated summary 				 Recent advances in large language models (LLMs) have opened ...
[04.02.2026 04:11] ********************************************************************************
[04.02.2026 04:11] Abstract 17. 3DiMo enables view-agnostic human motion control in video generation by training a motion encoder alongside a pretrained video generator to distill driving frames into compact motion tokens that align with the generator's spatial priors.  					AI-generated summary 				 Existing methods for human mot...
[04.02.2026 04:11] ********************************************************************************
[04.02.2026 04:11] Abstract 18. ObjEmbed is a novel multimodal language-model embedding approach that decomposes images into regional embeddings for improved object-level visual understanding and retrieval tasks.  					AI-generated summary 				 Aligning objects with corresponding textual descriptions is a fundamental challenge and...
[04.02.2026 04:11] ********************************************************************************
[04.02.2026 04:11] Abstract 19. A fusion framework called FINCH combines audio and spatiotemporal predictors for bioacoustic classification by adaptively weighting evidence based on reliability estimates, outperforming fixed-weight methods and audio-only approaches.  					AI-generated summary 				 Many machine learning systems hav...
[04.02.2026 04:11] ********************************************************************************
[04.02.2026 04:11] Abstract 20. Search-R2 framework improves language agent reasoning through Actor-Refiner collaboration with targeted interventions and fine-grained reward supervision for better credit assignment in reinforcement learning.  					AI-generated summary 				 Search-integrated reasoning enables language agents to tra...
[04.02.2026 04:11] ********************************************************************************
[04.02.2026 04:11] Abstract 21. MARS is a modular AI research automation framework that uses budget-aware planning, modular construction, and reflective memory to achieve state-of-the-art performance in autonomous machine learning research.  					AI-generated summary 				 Automating AI research differs from general software engine...
[04.02.2026 04:11] Read previous papers.
[04.02.2026 04:11] Generating reviews via LLM API.
[04.02.2026 04:11] Querying the API.
[04.02.2026 04:11] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Multimodal large language models can effectively understand source code when represented as compressed images, achieving significant token reduction while maintaining or improving performance on code comprehension tasks.  					AI-generated summary 				 Large Language Models (LLMs) have achieved remarkable success in source code understanding, yet as software systems grow in scale, computational efficiency has become a critical bottleneck. Currently, these models rely on a text-based paradigm that treats source code as a linear sequence of tokens, which leads to a linear increase in context length and associated computational costs. The rapid advancement of Multimodal LLMs (MLLMs) introduces an opportunity to optimize efficiency by representing source code as rendered images. Unlike text, which is difficult to compress without losing semantic meaning, the image modality is inherently suitable for compression. By adjusting resolution, images can be scaled to a fraction of their original token cost while remaining recognizable to vision-capable models. To explore the feasibility of this approach, we conduct the first systematic study on the effectiveness of MLLMs for code understanding. Our experiments reveal that: (1) MLLMs can effectively understand code with substantial token reduction, achieving up to 8x compression; (2) MLLMs can effectively leverage visual cues such as syntax highlighting, improving code completion performance under 4x compression; and (3) Code-understanding tasks like clone detection exhibit exceptional resilience to visual compression, with some compression ratios even slightly outperforming raw text inputs. Our findings highlight both the potential and current limitations of MLLMs in code understanding, which points out a shift toward image-modality code representation as a pathway to more efficient inference.
[04.02.2026 04:11] Response: ```json
{
  "desc": "        (MLLM)    ,       .  ,          8    ,        ,    .  ,                 .               .",
  "emoji": "",
  "title": "  :   "
}
```
[04.02.2026 04:11] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Multimodal large language models can effectively understand source code when represented as compressed images, achieving significant token reduction while maintaining or improving performance on code comprehension tasks.  					AI-generated summary 				 Large Language Models (LLMs) have achieved remarkable success in source code understanding, yet as software systems grow in scale, computational efficiency has become a critical bottleneck. Currently, these models rely on a text-based paradigm that treats source code as a linear sequence of tokens, which leads to a linear increase in context length and associated computational costs. The rapid advancement of Multimodal LLMs (MLLMs) introduces an opportunity to optimize efficiency by representing source code as rendered images. Unlike text, which is difficult to compress without losing semantic meaning, the image modality is inherently suitable for compression. By adjusting resolution, images can be scaled to a fraction of their original token cost while remaining recognizable to vision-capable models. To explore the feasibility of this approach, we conduct the first systematic study on the effectiveness of MLLMs for code understanding. Our experiments reveal that: (1) MLLMs can effectively understand code with substantial token reduction, achieving up to 8x compression; (2) MLLMs can effectively leverage visual cues such as syntax highlighting, improving code completion performance under 4x compression; and (3) Code-understanding tasks like clone detection exhibit exceptional resilience to visual compression, with some compression ratios even slightly outperforming raw text inputs. Our findings highlight both the potential and current limitations of MLLMs in code understanding, which points out a shift toward image-modality code representation as a pathway to more efficient inference."

[04.02.2026 04:11] Response: ```python
["MULTIMODAL", "PLP", "INFERENCE"]
```
[04.02.2026 04:11] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Multimodal large language models can effectively understand source code when represented as compressed images, achieving significant token reduction while maintaining or improving performance on code comprehension tasks.  					AI-generated summary 				 Large Language Models (LLMs) have achieved remarkable success in source code understanding, yet as software systems grow in scale, computational efficiency has become a critical bottleneck. Currently, these models rely on a text-based paradigm that treats source code as a linear sequence of tokens, which leads to a linear increase in context length and associated computational costs. The rapid advancement of Multimodal LLMs (MLLMs) introduces an opportunity to optimize efficiency by representing source code as rendered images. Unlike text, which is difficult to compress without losing semantic meaning, the image modality is inherently suitable for compression. By adjusting resolution, images can be scaled to a fraction of their original token cost while remaining recognizable to vision-capable models. To explore the feasibility of this approach, we conduct the first systematic study on the effectiveness of MLLMs for code understanding. Our experiments reveal that: (1) MLLMs can effectively understand code with substantial token reduction, achieving up to 8x compression; (2) MLLMs can effectively leverage visual cues such as syntax highlighting, improving code completion performance under 4x compression; and (3) Code-understanding tasks like clone detection exhibit exceptional resilience to visual compression, with some compression ratios even slightly outperforming raw text inputs. Our findings highlight both the potential and current limitations of MLLMs in code understanding, which points out a shift toward image-modality code representation as a pathway to more efficient inference."

[04.02.2026 04:11] Response: ```python
['OPTIMIZATION', 'LONG_CONTEXT']
```

**Justification:**

- **OPTIMIZATION**: The paper focuses on improving computational efficiency in LLMs by reducing token count through image compression of source code, achieving up to 8x compression. This directly addresses training and inference optimization.

- **LONG_CONTEXT**: The paper addresses the problem of linear increase in context length and associated computational costs as a critical bottleneck, proposing image-based representation as a solution to handle code more efficiently.
[04.02.2026 04:11] Error. Failed to parse JSON from LLM. ["OPTIMIZATION", "LONG_CONTEXT"]


**Justification:**

- **OPTIMIZATION**: The paper focuses on improving computational efficiency in LLMs by reducing token count through image compression of source code, achieving up to 8x compression. This directly addresses training and inference optimization.

- **LONG_CONTEXT**: The paper addresses the problem of linear increase in context length and associated computational costs as a critical bottleneck, proposing image-based representation as a solution to handle code more efficiently.
[04.02.2026 04:11] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper explores the use of Multimodal Large Language Models (MLLMs) for understanding source code by representing it as compressed images instead of traditional text. By converting code into images, the models can achieve significant token reduction, with up to 8x compression, while still maintaining or improving performance on code comprehension tasks. The study shows that MLLMs can utilize visual features like syntax highlighting to enhance code completion, even under high compression. Overall, the research suggests that using image representation for code can lead to more efficient processing and opens new avenues for optimizing code understanding in large software systems.","title":"Revolutionizing Code Understanding with Image-Based Compression"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper explores the use of Multimodal Large Language Models (MLLMs) for understanding source code by representing it as compressed images instead of traditional text. By converting code into images, the models can achieve significant token reduction, with up to 8x compression, while still maintaining or improving performance on code comprehension tasks. The study shows that MLLMs can utilize visual features like syntax highlighting to enhance code completion, even under high compression. Overall, the research suggests that using image representation for code can lead to more efficient processing and opens new avenues for optimizing code understanding in large software systems.', title='Revolutionizing Code Understanding with Image-Based Compression'))
[04.02.2026 04:11] Response: ParsedChatCompletionMessage[Article](content='{"desc":"MLLMsMLLMs","title":""}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='MLLMsMLLMs', title=''))
[04.02.2026 04:11] Querying the API.
[04.02.2026 04:11] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Current world models lack unified frameworks despite task-specific advances, necessitating a comprehensive approach integrating interaction, perception, symbolic reasoning, and spatial representation.  					AI-generated summary 				 World models have emerged as a critical frontier in AI research, aiming to enhance large models by infusing them with physical dynamics and world knowledge. The core objective is to enable agents to understand, predict, and interact with complex environments. However, current research landscape remains fragmented, with approaches predominantly focused on injecting world knowledge into isolated tasks, such as visual prediction, 3D estimation, or symbol grounding, rather than establishing a unified definition or framework. While these task-specific integrations yield performance gains, they often lack the systematic coherence required for holistic world understanding. In this paper, we analyze the limitations of such fragmented approaches and propose a unified design specification for world models. We suggest that a robust world model should not be a loose collection of capabilities but a normative framework that integrally incorporates interaction, perception, symbolic reasoning, and spatial representation. This work aims to provide a structured perspective to guide future research toward more general, robust, and principled models of the world.
[04.02.2026 04:11] Response: ```json
{
  "desc": "         ,           .        ,      , ,        .         ,    ,        .           ,     .",
  "emoji": "",
  "title": "        "
}
```
[04.02.2026 04:11] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Current world models lack unified frameworks despite task-specific advances, necessitating a comprehensive approach integrating interaction, perception, symbolic reasoning, and spatial representation.  					AI-generated summary 				 World models have emerged as a critical frontier in AI research, aiming to enhance large models by infusing them with physical dynamics and world knowledge. The core objective is to enable agents to understand, predict, and interact with complex environments. However, current research landscape remains fragmented, with approaches predominantly focused on injecting world knowledge into isolated tasks, such as visual prediction, 3D estimation, or symbol grounding, rather than establishing a unified definition or framework. While these task-specific integrations yield performance gains, they often lack the systematic coherence required for holistic world understanding. In this paper, we analyze the limitations of such fragmented approaches and propose a unified design specification for world models. We suggest that a robust world model should not be a loose collection of capabilities but a normative framework that integrally incorporates interaction, perception, symbolic reasoning, and spatial representation. This work aims to provide a structured perspective to guide future research toward more general, robust, and principled models of the world."

[04.02.2026 04:11] Response: ```python
['3D', 'MULTIMODAL', 'ARCHITECTURE']
```

**Justification:**

- **3D**: The paper explicitly mentions "3D estimation" as one of the task-specific approaches in world models.

- **MULTIMODAL**: The paper discusses integrating multiple modalities including "interaction, perception, symbolic reasoning, and spatial representation" - combining different types of information processing.

- **ARCHITECTURE**: The paper proposes "a unified design specification for world models" and discusses establishing a "normative framework" for world model architecture, which is fundamentally about proposing novel architectural approaches.
[04.02.2026 04:11] Error. Failed to parse JSON from LLM. ["3D", "MULTIMODAL", "ARCHITECTURE"]


**Justification:**

- **3D**: The paper explicitly mentions "3D estimation" as one of the task-specific approaches in world models.

- **MULTIMODAL**: The paper discusses integrating multiple modalities including "interaction, perception, symbolic reasoning, and spatial representation" - combining different types of information processing.

- **ARCHITECTURE**: The paper proposes "a unified design specification for world models" and discusses establishing a "normative framework" for world model architecture, which is fundamentally about proposing novel architectural approaches.
[04.02.2026 04:11] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Current world models lack unified frameworks despite task-specific advances, necessitating a comprehensive approach integrating interaction, perception, symbolic reasoning, and spatial representation.  					AI-generated summary 				 World models have emerged as a critical frontier in AI research, aiming to enhance large models by infusing them with physical dynamics and world knowledge. The core objective is to enable agents to understand, predict, and interact with complex environments. However, current research landscape remains fragmented, with approaches predominantly focused on injecting world knowledge into isolated tasks, such as visual prediction, 3D estimation, or symbol grounding, rather than establishing a unified definition or framework. While these task-specific integrations yield performance gains, they often lack the systematic coherence required for holistic world understanding. In this paper, we analyze the limitations of such fragmented approaches and propose a unified design specification for world models. We suggest that a robust world model should not be a loose collection of capabilities but a normative framework that integrally incorporates interaction, perception, symbolic reasoning, and spatial representation. This work aims to provide a structured perspective to guide future research toward more general, robust, and principled models of the world."

[04.02.2026 04:11] Response: ```python
["REASONING", "SURVEY"]
```

**Justification:**

- **REASONING**: The paper emphasizes "symbolic reasoning" as a core component of world models and discusses the need for logical understanding and prediction of complex environments, which relates to enhancing reasoning capabilities.

- **SURVEY**: The paper explicitly analyzes the current research landscape, discusses limitations of existing approaches, and proposes a unified framework by reviewing and synthesizing task-specific advances in world modeling. This comprehensive review and analysis of the field aligns with survey paper characteristics.
[04.02.2026 04:11] Error. Failed to parse JSON from LLM. ["REASONING", "SURVEY"]


**Justification:**

- **REASONING**: The paper emphasizes "symbolic reasoning" as a core component of world models and discusses the need for logical understanding and prediction of complex environments, which relates to enhancing reasoning capabilities.

- **SURVEY**: The paper explicitly analyzes the current research landscape, discusses limitations of existing approaches, and proposes a unified framework by reviewing and synthesizing task-specific advances in world modeling. This comprehensive review and analysis of the field aligns with survey paper characteristics.
[04.02.2026 04:11] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper discusses the need for a unified framework in world models within AI, which currently focus on specific tasks like visual prediction or 3D estimation. It highlights that while these task-specific models improve performance, they often lack a cohesive structure for understanding the world as a whole. The authors propose a comprehensive design specification that integrates key components such as interaction, perception, symbolic reasoning, and spatial representation. The goal is to guide future research towards developing more robust and generalizable world models that can effectively understand and interact with complex environments.","title":"Towards a Unified Framework for World Models in AI"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper discusses the need for a unified framework in world models within AI, which currently focus on specific tasks like visual prediction or 3D estimation. It highlights that while these task-specific models improve performance, they often lack a cohesive structure for understanding the world as a whole. The authors propose a comprehensive design specification that integrates key components such as interaction, perception, symbolic reasoning, and spatial representation. The goal is to guide future research towards developing more robust and generalizable world models that can effectively understand and interact with complex environments.', title='Towards a Unified Framework for World Models in AI'))
[04.02.2026 04:11] Response: ParsedChatCompletionMessage[Article](content='{"desc":"","title":""}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='', title=''))
[04.02.2026 04:11] Querying the API.
[04.02.2026 04:11] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

DeepResearch report generation is improved through human-preference-aligned rubric generators trained via reinforcement learning with hybrid rewards and enhanced with multi-agent Markov-state workflows.  					AI-generated summary 				 Nowadays, training and evaluating DeepResearch-generated reports remain challenging due to the lack of verifiable reward signals. Accordingly, rubric-based evaluation has become a common practice. However, existing approaches either rely on coarse, pre-defined rubrics that lack sufficient granularity, or depend on manually constructed query-specific rubrics that are costly and difficult to scale. In this paper, we propose a pipeline to train human-preference-aligned query-specific rubric generators tailored for DeepResearch report generation. We first construct a dataset of DeepResearch-style queries annotated with human preferences over paired reports, and train rubric generators via reinforcement learning with a hybrid reward combining human preference supervision and LLM-based rubric evaluation. To better handle long-horizon reasoning, we further introduce a Multi-agent Markov-state (MaMs) workflow for report generation. We empirically show that our proposed rubric generators deliver more discriminative and better human-aligned supervision than existing rubric design strategies. Moreover, when integrated into the MaMs training framework, DeepResearch systems equipped with our rubric generators consistently outperform all open-source baselines on the DeepResearch Bench and achieve performance comparable to that of leading closed-source models.
[04.02.2026 04:11] Response: ```json
{
  "desc": "       ,        ,    .         ,       ,    .               .   ,                  .",
  "emoji": "",
  "title": "         "
}
```
[04.02.2026 04:11] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"DeepResearch report generation is improved through human-preference-aligned rubric generators trained via reinforcement learning with hybrid rewards and enhanced with multi-agent Markov-state workflows.  					AI-generated summary 				 Nowadays, training and evaluating DeepResearch-generated reports remain challenging due to the lack of verifiable reward signals. Accordingly, rubric-based evaluation has become a common practice. However, existing approaches either rely on coarse, pre-defined rubrics that lack sufficient granularity, or depend on manually constructed query-specific rubrics that are costly and difficult to scale. In this paper, we propose a pipeline to train human-preference-aligned query-specific rubric generators tailored for DeepResearch report generation. We first construct a dataset of DeepResearch-style queries annotated with human preferences over paired reports, and train rubric generators via reinforcement learning with a hybrid reward combining human preference supervision and LLM-based rubric evaluation. To better handle long-horizon reasoning, we further introduce a Multi-agent Markov-state (MaMs) workflow for report generation. We empirically show that our proposed rubric generators deliver more discriminative and better human-aligned supervision than existing rubric design strategies. Moreover, when integrated into the MaMs training framework, DeepResearch systems equipped with our rubric generators consistently outperform all open-source baselines on the DeepResearch Bench and achieve performance comparable to that of leading closed-source models."

[04.02.2026 04:11] Response: ```python
["DATASET", "RLHF", "AGENTS", "BENCHMARK"]
```
[04.02.2026 04:11] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"DeepResearch report generation is improved through human-preference-aligned rubric generators trained via reinforcement learning with hybrid rewards and enhanced with multi-agent Markov-state workflows.  					AI-generated summary 				 Nowadays, training and evaluating DeepResearch-generated reports remain challenging due to the lack of verifiable reward signals. Accordingly, rubric-based evaluation has become a common practice. However, existing approaches either rely on coarse, pre-defined rubrics that lack sufficient granularity, or depend on manually constructed query-specific rubrics that are costly and difficult to scale. In this paper, we propose a pipeline to train human-preference-aligned query-specific rubric generators tailored for DeepResearch report generation. We first construct a dataset of DeepResearch-style queries annotated with human preferences over paired reports, and train rubric generators via reinforcement learning with a hybrid reward combining human preference supervision and LLM-based rubric evaluation. To better handle long-horizon reasoning, we further introduce a Multi-agent Markov-state (MaMs) workflow for report generation. We empirically show that our proposed rubric generators deliver more discriminative and better human-aligned supervision than existing rubric design strategies. Moreover, when integrated into the MaMs training framework, DeepResearch systems equipped with our rubric generators consistently outperform all open-source baselines on the DeepResearch Bench and achieve performance comparable to that of leading closed-source models."

[04.02.2026 04:11] Response: ```python
["ALIGNMENT", "REASONING", "OPTIMIZATION"]
```

**Justification:**

1. **ALIGNMENT**: The paper explicitly focuses on "human-preference-aligned rubric generators" and training systems to align with human preferences, which is a core alignment concern.

2. **REASONING**: The paper mentions "long-horizon reasoning" as a key challenge being addressed through the Multi-agent Markov-state workflow, directly relating to reasoning capabilities.

3. **OPTIMIZATION**: The paper describes training rubric generators via reinforcement learning with hybrid rewards, which is an optimization method for improving model training and evaluation.
[04.02.2026 04:11] Error. Failed to parse JSON from LLM. ["ALIGNMENT", "REASONING", "OPTIMIZATION"]


**Justification:**

1. **ALIGNMENT**: The paper explicitly focuses on "human-preference-aligned rubric generators" and training systems to align with human preferences, which is a core alignment concern.

2. **REASONING**: The paper mentions "long-horizon reasoning" as a key challenge being addressed through the Multi-agent Markov-state workflow, directly relating to reasoning capabilities.

3. **OPTIMIZATION**: The paper describes training rubric generators via reinforcement learning with hybrid rewards, which is an optimization method for improving model training and evaluation.
[04.02.2026 04:11] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents a novel approach to improve the generation of reports in DeepResearch by using human-preference-aligned rubric generators. These generators are trained through reinforcement learning, utilizing a hybrid reward system that combines human preferences and evaluations from large language models (LLMs). The authors introduce a Multi-agent Markov-state (MaMs) workflow to enhance the reasoning capabilities of the report generation process. The results demonstrate that their method provides superior supervision and performance compared to existing rubric strategies, achieving results on par with top closed-source models.","title":"Enhancing DeepResearch Reports with Human-Aligned Rubrics"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper presents a novel approach to improve the generation of reports in DeepResearch by using human-preference-aligned rubric generators. These generators are trained through reinforcement learning, utilizing a hybrid reward system that combines human preferences and evaluations from large language models (LLMs). The authors introduce a Multi-agent Markov-state (MaMs) workflow to enhance the reasoning capabilities of the report generation process. The results demonstrate that their method provides superior supervision and performance compared to existing rubric strategies, achieving results on par with top closed-source models.', title='Enhancing DeepResearch Reports with Human-Aligned Rubrics'))
[04.02.2026 04:11] Response: ParsedChatCompletionMessage[Article](content='{"desc":"DeepResearchLLMDeepResearch","title":"DeepResearch"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='DeepResearchLLMDeepResearch', title='DeepResearch'))
[04.02.2026 04:11] Querying the API.
[04.02.2026 04:11] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

UnifiedReward-Flex combines reward modeling with flexible, context-adaptive reasoning to improve visual generation by dynamically constructing hierarchical assessments based on semantic intent and visual evidence.  					AI-generated summary 				 Recent advancements in multimodal reward models (RMs) have significantly propelled the development of visual generation. Existing frameworks typically adopt Bradley-Terry-style preference modeling or leverage generative VLMs as judges, and subsequently optimize visual generation models via reinforcement learning. However, current RMs suffer from inherent limitations: they often follow a one-size-fits-all paradigm that assumes a monolithic preference distribution or relies on fixed evaluation rubrics. As a result, they are insensitive to content-specific visual cues, leading to systematic misalignment with subjective and context-dependent human preferences. To this end, inspired by human assessment, we propose UnifiedReward-Flex, a unified personalized reward model for vision generation that couples reward modeling with flexible and context-adaptive reasoning. Specifically, given a prompt and the generated visual content, it first interprets the semantic intent and grounds on visual evidence, then dynamically constructs a hierarchical assessment by instantiating fine-grained criteria under both predefined and self-generated high-level dimensions. Our training pipeline follows a two-stage process: (1) we first distill structured, high-quality reasoning traces from advanced closed-source VLMs to bootstrap SFT, equipping the model with flexible and context-adaptive reasoning behaviors; (2) we then perform direct preference optimization (DPO) on carefully curated preference pairs to further strengthen reasoning fidelity and discriminative alignment. To validate the effectiveness, we integrate UnifiedReward-Flex into the GRPO framework for image and video synthesis, and extensive results demonstrate its superiority.
[04.02.2026 04:11] Response: ```json
{
  "desc": "   UnifiedReward-Flex       ,        - .        ,         .     :            ,       .      GRPO       .",
  "emoji": "",
  "title": "         "
}
```
[04.02.2026 04:11] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"UnifiedReward-Flex combines reward modeling with flexible, context-adaptive reasoning to improve visual generation by dynamically constructing hierarchical assessments based on semantic intent and visual evidence.  					AI-generated summary 				 Recent advancements in multimodal reward models (RMs) have significantly propelled the development of visual generation. Existing frameworks typically adopt Bradley-Terry-style preference modeling or leverage generative VLMs as judges, and subsequently optimize visual generation models via reinforcement learning. However, current RMs suffer from inherent limitations: they often follow a one-size-fits-all paradigm that assumes a monolithic preference distribution or relies on fixed evaluation rubrics. As a result, they are insensitive to content-specific visual cues, leading to systematic misalignment with subjective and context-dependent human preferences. To this end, inspired by human assessment, we propose UnifiedReward-Flex, a unified personalized reward model for vision generation that couples reward modeling with flexible and context-adaptive reasoning. Specifically, given a prompt and the generated visual content, it first interprets the semantic intent and grounds on visual evidence, then dynamically constructs a hierarchical assessment by instantiating fine-grained criteria under both predefined and self-generated high-level dimensions. Our training pipeline follows a two-stage process: (1) we first distill structured, high-quality reasoning traces from advanced closed-source VLMs to bootstrap SFT, equipping the model with flexible and context-adaptive reasoning behaviors; (2) we then perform direct preference optimization (DPO) on carefully curated preference pairs to further strengthen reasoning fidelity and discriminative alignment. To validate the effectiveness, we integrate UnifiedReward-Flex into the GRPO framework for image and video synthesis, and extensive results demonstrate its superiority."

[04.02.2026 04:11] Response: ```python
["MULTIMODAL", "RLHF", "CV", "VIDEO"]
```
[04.02.2026 04:11] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"UnifiedReward-Flex combines reward modeling with flexible, context-adaptive reasoning to improve visual generation by dynamically constructing hierarchical assessments based on semantic intent and visual evidence.  					AI-generated summary 				 Recent advancements in multimodal reward models (RMs) have significantly propelled the development of visual generation. Existing frameworks typically adopt Bradley-Terry-style preference modeling or leverage generative VLMs as judges, and subsequently optimize visual generation models via reinforcement learning. However, current RMs suffer from inherent limitations: they often follow a one-size-fits-all paradigm that assumes a monolithic preference distribution or relies on fixed evaluation rubrics. As a result, they are insensitive to content-specific visual cues, leading to systematic misalignment with subjective and context-dependent human preferences. To this end, inspired by human assessment, we propose UnifiedReward-Flex, a unified personalized reward model for vision generation that couples reward modeling with flexible and context-adaptive reasoning. Specifically, given a prompt and the generated visual content, it first interprets the semantic intent and grounds on visual evidence, then dynamically constructs a hierarchical assessment by instantiating fine-grained criteria under both predefined and self-generated high-level dimensions. Our training pipeline follows a two-stage process: (1) we first distill structured, high-quality reasoning traces from advanced closed-source VLMs to bootstrap SFT, equipping the model with flexible and context-adaptive reasoning behaviors; (2) we then perform direct preference optimization (DPO) on carefully curated preference pairs to further strengthen reasoning fidelity and discriminative alignment. To validate the effectiveness, we integrate UnifiedReward-Flex into the GRPO framework for image and video synthesis, and extensive results demonstrate its superiority."

[04.02.2026 04:11] Response: ```python
['REASONING', 'ALIGNMENT']
```
[04.02.2026 04:11] Response: ParsedChatCompletionMessage[Article](content='{"desc":"UnifiedReward-Flex is a novel approach that enhances visual generation by integrating reward modeling with adaptable reasoning techniques. It addresses the limitations of traditional reward models, which often use a fixed evaluation system that does not account for specific visual details or human preferences. By dynamically creating hierarchical assessments based on both semantic intent and visual evidence, it allows for more personalized and context-sensitive evaluations. The model is trained through a two-stage process that improves its reasoning capabilities and aligns it more closely with human-like preferences, leading to better performance in generating images and videos.","title":"Dynamic Context-Aware Reward Modeling for Enhanced Visual Generation"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='UnifiedReward-Flex is a novel approach that enhances visual generation by integrating reward modeling with adaptable reasoning techniques. It addresses the limitations of traditional reward models, which often use a fixed evaluation system that does not account for specific visual details or human preferences. By dynamically creating hierarchical assessments based on both semantic intent and visual evidence, it allows for more personalized and context-sensitive evaluations. The model is trained through a two-stage process that improves its reasoning capabilities and aligns it more closely with human-like preferences, leading to better performance in generating images and videos.', title='Dynamic Context-Aware Reward Modeling for Enhanced Visual Generation'))
[04.02.2026 04:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"UnifiedReward-Flex UnifiedReward-Flex ","title":""}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='UnifiedReward-Flex UnifiedReward-Flex ', title=''))
[04.02.2026 04:12] Querying the API.
[04.02.2026 04:12] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Parallel-Probe is a training-free controller that optimizes parallel thinking by using consensus-based early stopping and deviation-based branch pruning to reduce computational costs while maintaining accuracy.  					AI-generated summary 				 Parallel thinking has emerged as a promising paradigm for reasoning, yet it imposes significant computational burdens. Existing efficiency methods primarily rely on local, per-trajectory signals and lack principled mechanisms to exploit global dynamics across parallel branches. We introduce 2D probing, an interface that exposes the width-depth dynamics of parallel thinking by periodically eliciting intermediate answers from all branches. Our analysis reveals three key insights: non-monotonic scaling across width-depth allocations, heterogeneous reasoning branch lengths, and early stabilization of global consensus. Guided by these insights, we introduce Parallel-Probe, a training-free controller designed to optimize online parallel thinking. Parallel-Probe employs consensus-based early stopping to regulate reasoning depth and deviation-based branch pruning to dynamically adjust width. Extensive experiments across three benchmarks and multiple models demonstrate that Parallel-Probe establishes a superior Pareto frontier for test-time scaling. Compared to standard majority voting, it reduces sequential tokens by up to 35.8% and total token cost by over 25.8% while maintaining competitive accuracy.
[04.02.2026 04:12] Response: ```json
{
  "desc": "   Parallel-Probe            .   2D-,             .       : -                .        25.8%    .",
  "emoji": "",
  "title": "       "
}
```
[04.02.2026 04:12] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Parallel-Probe is a training-free controller that optimizes parallel thinking by using consensus-based early stopping and deviation-based branch pruning to reduce computational costs while maintaining accuracy.  					AI-generated summary 				 Parallel thinking has emerged as a promising paradigm for reasoning, yet it imposes significant computational burdens. Existing efficiency methods primarily rely on local, per-trajectory signals and lack principled mechanisms to exploit global dynamics across parallel branches. We introduce 2D probing, an interface that exposes the width-depth dynamics of parallel thinking by periodically eliciting intermediate answers from all branches. Our analysis reveals three key insights: non-monotonic scaling across width-depth allocations, heterogeneous reasoning branch lengths, and early stabilization of global consensus. Guided by these insights, we introduce Parallel-Probe, a training-free controller designed to optimize online parallel thinking. Parallel-Probe employs consensus-based early stopping to regulate reasoning depth and deviation-based branch pruning to dynamically adjust width. Extensive experiments across three benchmarks and multiple models demonstrate that Parallel-Probe establishes a superior Pareto frontier for test-time scaling. Compared to standard majority voting, it reduces sequential tokens by up to 35.8% and total token cost by over 25.8% while maintaining competitive accuracy."

[04.02.2026 04:12] Response: ```python
["INFERENCE", "BENCHMARK"]
```

**Justification:**

1. **INFERENCE**: The paper focuses on optimizing model deployment and computational efficiency through techniques like early stopping and branch pruning to reduce computational costs during inference (test-time scaling).

2. **BENCHMARK**: The paper evaluates the proposed method "across three benchmarks and multiple models," indicating the use of benchmark evaluation frameworks to assess performance.
[04.02.2026 04:12] Error. Failed to parse JSON from LLM. ["INFERENCE", "BENCHMARK"]


**Justification:**

1. **INFERENCE**: The paper focuses on optimizing model deployment and computational efficiency through techniques like early stopping and branch pruning to reduce computational costs during inference (test-time scaling).

2. **BENCHMARK**: The paper evaluates the proposed method "across three benchmarks and multiple models," indicating the use of benchmark evaluation frameworks to assess performance.
[04.02.2026 04:12] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Parallel-Probe is a training-free controller that optimizes parallel thinking by using consensus-based early stopping and deviation-based branch pruning to reduce computational costs while maintaining accuracy.  					AI-generated summary 				 Parallel thinking has emerged as a promising paradigm for reasoning, yet it imposes significant computational burdens. Existing efficiency methods primarily rely on local, per-trajectory signals and lack principled mechanisms to exploit global dynamics across parallel branches. We introduce 2D probing, an interface that exposes the width-depth dynamics of parallel thinking by periodically eliciting intermediate answers from all branches. Our analysis reveals three key insights: non-monotonic scaling across width-depth allocations, heterogeneous reasoning branch lengths, and early stabilization of global consensus. Guided by these insights, we introduce Parallel-Probe, a training-free controller designed to optimize online parallel thinking. Parallel-Probe employs consensus-based early stopping to regulate reasoning depth and deviation-based branch pruning to dynamically adjust width. Extensive experiments across three benchmarks and multiple models demonstrate that Parallel-Probe establishes a superior Pareto frontier for test-time scaling. Compared to standard majority voting, it reduces sequential tokens by up to 35.8% and total token cost by over 25.8% while maintaining competitive accuracy."

[04.02.2026 04:12] Response: ```python
['REASONING', 'OPTIMIZATION']
```
[04.02.2026 04:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Parallel-Probe is a novel controller that enhances parallel thinking in machine learning without requiring prior training. It utilizes consensus-based early stopping to determine when to halt reasoning and deviation-based branch pruning to optimize the number of branches used. This approach allows for efficient computation by balancing the depth and width of reasoning processes, leading to significant reductions in token usage while preserving accuracy. Experiments show that Parallel-Probe outperforms traditional methods, achieving better efficiency in resource usage during model inference.","title":"Optimizing Parallel Thinking with Efficiency and Accuracy"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Parallel-Probe is a novel controller that enhances parallel thinking in machine learning without requiring prior training. It utilizes consensus-based early stopping to determine when to halt reasoning and deviation-based branch pruning to optimize the number of branches used. This approach allows for efficient computation by balancing the depth and width of reasoning processes, leading to significant reductions in token usage while preserving accuracy. Experiments show that Parallel-Probe outperforms traditional methods, achieving better efficiency in resource usage during model inference.', title='Optimizing Parallel Thinking with Efficiency and Accuracy'))
[04.02.2026 04:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Parallel-Probe2D-Parallel-Probe","title":""}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Parallel-Probe2D-Parallel-Probe', title=''))
[04.02.2026 04:12] Querying the API.
[04.02.2026 04:12] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

AdaptMMBench presents a comprehensive benchmark for evaluating adaptive multimodal reasoning in Vision-Language Models, measuring reasoning mode selection rationality through dynamic difficulty assessment and multi-dimensional process evaluation.  					AI-generated summary 				 Adaptive multimodal reasoning has emerged as a promising frontier in Vision-Language Models (VLMs), aiming to dynamically modulate between tool-augmented visual reasoning and text reasoning to enhance both effectiveness and efficiency. However, existing evaluations rely on static difficulty labels and simplistic metrics, which fail to capture the dynamic nature of difficulty relative to varying model capacities. Consequently, they obscure the distinction between adaptive mode selection and general performance while neglecting fine-grained process analyses. In this paper, we propose AdaptMMBench, a comprehensive benchmark for adaptive multimodal reasoning across five domains: real-world, OCR, GUI, knowledge, and math, encompassing both direct perception and complex reasoning tasks. AdaptMMBench utilizes a Matthews Correlation Coefficient (MCC) metric to evaluate the selection rationality of different reasoning modes, isolating this meta-cognition ability by dynamically identifying task difficulties based on models' capability boundaries. Moreover, AdaptMMBench facilitates multi-dimensional process evaluation across key step coverage, tool effectiveness, and computational efficiency. Our evaluation reveals that while adaptive mode selection scales with model capacity, it notably decouples from final accuracy. Conversely, key step coverage aligns with performance, though tool effectiveness remains highly inconsistent across model architectures.
[04.02.2026 04:12] Response: ```json
{
  "desc": "AdaptMMBench           Vision-Language Models,               .    ,    Matthews Correlation Coefficient      ,         .     ( , OCR, GUI,   )     :   ,     .  ,         ,      ,         .",
  "emoji": "",
  "title": " :        "
}
```
[04.02.2026 04:12] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"AdaptMMBench presents a comprehensive benchmark for evaluating adaptive multimodal reasoning in Vision-Language Models, measuring reasoning mode selection rationality through dynamic difficulty assessment and multi-dimensional process evaluation.  					AI-generated summary 				 Adaptive multimodal reasoning has emerged as a promising frontier in Vision-Language Models (VLMs), aiming to dynamically modulate between tool-augmented visual reasoning and text reasoning to enhance both effectiveness and efficiency. However, existing evaluations rely on static difficulty labels and simplistic metrics, which fail to capture the dynamic nature of difficulty relative to varying model capacities. Consequently, they obscure the distinction between adaptive mode selection and general performance while neglecting fine-grained process analyses. In this paper, we propose AdaptMMBench, a comprehensive benchmark for adaptive multimodal reasoning across five domains: real-world, OCR, GUI, knowledge, and math, encompassing both direct perception and complex reasoning tasks. AdaptMMBench utilizes a Matthews Correlation Coefficient (MCC) metric to evaluate the selection rationality of different reasoning modes, isolating this meta-cognition ability by dynamically identifying task difficulties based on models' capability boundaries. Moreover, AdaptMMBench facilitates multi-dimensional process evaluation across key step coverage, tool effectiveness, and computational efficiency. Our evaluation reveals that while adaptive mode selection scales with model capacity, it notably decouples from final accuracy. Conversely, key step coverage aligns with performance, though tool effectiveness remains highly inconsistent across model architectures."

[04.02.2026 04:12] Response: ```python
["BENCHMARK", "MULTIMODAL", "CV"]
```
[04.02.2026 04:12] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"AdaptMMBench presents a comprehensive benchmark for evaluating adaptive multimodal reasoning in Vision-Language Models, measuring reasoning mode selection rationality through dynamic difficulty assessment and multi-dimensional process evaluation.  					AI-generated summary 				 Adaptive multimodal reasoning has emerged as a promising frontier in Vision-Language Models (VLMs), aiming to dynamically modulate between tool-augmented visual reasoning and text reasoning to enhance both effectiveness and efficiency. However, existing evaluations rely on static difficulty labels and simplistic metrics, which fail to capture the dynamic nature of difficulty relative to varying model capacities. Consequently, they obscure the distinction between adaptive mode selection and general performance while neglecting fine-grained process analyses. In this paper, we propose AdaptMMBench, a comprehensive benchmark for adaptive multimodal reasoning across five domains: real-world, OCR, GUI, knowledge, and math, encompassing both direct perception and complex reasoning tasks. AdaptMMBench utilizes a Matthews Correlation Coefficient (MCC) metric to evaluate the selection rationality of different reasoning modes, isolating this meta-cognition ability by dynamically identifying task difficulties based on models' capability boundaries. Moreover, AdaptMMBench facilitates multi-dimensional process evaluation across key step coverage, tool effectiveness, and computational efficiency. Our evaluation reveals that while adaptive mode selection scales with model capacity, it notably decouples from final accuracy. Conversely, key step coverage aligns with performance, though tool effectiveness remains highly inconsistent across model architectures."

[04.02.2026 04:12] Response: ```python
["REASONING", "INTERPRETABILITY"]
```

**Justification:**

1. **REASONING**: The paper explicitly focuses on "adaptive multimodal reasoning" in Vision-Language Models, measuring how models dynamically select between different reasoning modes (tool-augmented visual reasoning vs. text reasoning). This directly addresses enhancing logical reasoning capabilities.

2. **INTERPRETABILITY**: The paper analyzes model behavior through "multi-dimensional process evaluation" including "key step coverage, tool effectiveness, and computational efficiency." It also evaluates "selection rationality" of reasoning modes and examines the distinction between adaptive mode selection and general performance, which involves understanding and explaining model decision-making processes.
[04.02.2026 04:12] Error. Failed to parse JSON from LLM. ["REASONING", "INTERPRETABILITY"]


**Justification:**

1. **REASONING**: The paper explicitly focuses on "adaptive multimodal reasoning" in Vision-Language Models, measuring how models dynamically select between different reasoning modes (tool-augmented visual reasoning vs. text reasoning). This directly addresses enhancing logical reasoning capabilities.

2. **INTERPRETABILITY**: The paper analyzes model behavior through "multi-dimensional process evaluation" including "key step coverage, tool effectiveness, and computational efficiency." It also evaluates "selection rationality" of reasoning modes and examines the distinction between adaptive mode selection and general performance, which involves understanding and explaining model decision-making processes.
[04.02.2026 04:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The paper introduces AdaptMMBench, a new benchmark designed to evaluate adaptive multimodal reasoning in Vision-Language Models (VLMs). It addresses the limitations of existing evaluations that use static difficulty labels and simplistic metrics, which do not reflect the dynamic nature of task difficulty. AdaptMMBench assesses reasoning mode selection rationality using the Matthews Correlation Coefficient (MCC) and evaluates models across five domains, focusing on both direct perception and complex reasoning tasks. The findings indicate that while adaptive mode selection improves with model capacity, it does not necessarily correlate with final accuracy, highlighting the need for a nuanced understanding of model performance.","title":"AdaptMMBench: Elevating Adaptive Reasoning in Vision-Language Models"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='The paper introduces AdaptMMBench, a new benchmark designed to evaluate adaptive multimodal reasoning in Vision-Language Models (VLMs). It addresses the limitations of existing evaluations that use static difficulty labels and simplistic metrics, which do not reflect the dynamic nature of task difficulty. AdaptMMBench assesses reasoning mode selection rationality using the Matthews Correlation Coefficient (MCC) and evaluates models across five domains, focusing on both direct perception and complex reasoning tasks. The findings indicate that while adaptive mode selection improves with model capacity, it does not necessarily correlate with final accuracy, highlighting the need for a nuanced understanding of model performance.', title='AdaptMMBench: Elevating Adaptive Reasoning in Vision-Language Models'))
[04.02.2026 04:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"AdaptMMBench -OCRGUI","title":""}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='AdaptMMBench -OCRGUI', title=''))
[04.02.2026 04:12] Querying the API.
[04.02.2026 04:12] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Wide Research advances search intelligence through a dedicated benchmark and multi-agent architecture that enables parallel information retrieval under complex constraints.  					AI-generated summary 				 Search intelligence is evolving from Deep Research to Wide Research, a paradigm essential for retrieving and synthesizing comprehensive information under complex constraints in parallel. However, progress in this field is impeded by the lack of dedicated benchmarks and optimization methodologies for search breadth. To address these challenges, we take a deep dive into Wide Research from two perspectives: Data Pipeline and Agent Optimization. First, we produce WideSeekBench, a General Broad Information Seeking (GBIS) benchmark constructed via a rigorous multi-phase data pipeline to ensure diversity across the target information volume, logical constraints, and domains. Second, we introduce WideSeek, a dynamic hierarchical multi-agent architecture that can autonomously fork parallel sub-agents based on task requirements. Furthermore, we design a unified training framework that linearizes multi-agent trajectories and optimizes the system using end-to-end RL. Experimental results demonstrate the effectiveness of WideSeek and multi-agent RL, highlighting that scaling the number of agents is a promising direction for advancing the Wide Research paradigm.
[04.02.2026 04:12] Response: ```json
{
  "desc": "  Wide Research     ,            .   WideSeekBench        ,   WideSeek    ,     .        (RL)     .   ,         .",
  "emoji": "",
  "title": " :      "
}
```
[04.02.2026 04:12] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Wide Research advances search intelligence through a dedicated benchmark and multi-agent architecture that enables parallel information retrieval under complex constraints.  					AI-generated summary 				 Search intelligence is evolving from Deep Research to Wide Research, a paradigm essential for retrieving and synthesizing comprehensive information under complex constraints in parallel. However, progress in this field is impeded by the lack of dedicated benchmarks and optimization methodologies for search breadth. To address these challenges, we take a deep dive into Wide Research from two perspectives: Data Pipeline and Agent Optimization. First, we produce WideSeekBench, a General Broad Information Seeking (GBIS) benchmark constructed via a rigorous multi-phase data pipeline to ensure diversity across the target information volume, logical constraints, and domains. Second, we introduce WideSeek, a dynamic hierarchical multi-agent architecture that can autonomously fork parallel sub-agents based on task requirements. Furthermore, we design a unified training framework that linearizes multi-agent trajectories and optimizes the system using end-to-end RL. Experimental results demonstrate the effectiveness of WideSeek and multi-agent RL, highlighting that scaling the number of agents is a promising direction for advancing the Wide Research paradigm."

[04.02.2026 04:12] Response: ```python
['DATASET', 'BENCHMARK', 'AGENTS', 'RL', 'TRAINING']
```
[04.02.2026 04:12] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Wide Research advances search intelligence through a dedicated benchmark and multi-agent architecture that enables parallel information retrieval under complex constraints.  					AI-generated summary 				 Search intelligence is evolving from Deep Research to Wide Research, a paradigm essential for retrieving and synthesizing comprehensive information under complex constraints in parallel. However, progress in this field is impeded by the lack of dedicated benchmarks and optimization methodologies for search breadth. To address these challenges, we take a deep dive into Wide Research from two perspectives: Data Pipeline and Agent Optimization. First, we produce WideSeekBench, a General Broad Information Seeking (GBIS) benchmark constructed via a rigorous multi-phase data pipeline to ensure diversity across the target information volume, logical constraints, and domains. Second, we introduce WideSeek, a dynamic hierarchical multi-agent architecture that can autonomously fork parallel sub-agents based on task requirements. Furthermore, we design a unified training framework that linearizes multi-agent trajectories and optimizes the system using end-to-end RL. Experimental results demonstrate the effectiveness of WideSeek and multi-agent RL, highlighting that scaling the number of agents is a promising direction for advancing the Wide Research paradigm."

[04.02.2026 04:12] Response: ```python
['REASONING', 'OPTIMIZATION', 'OPEN_SOURCE']
```

**Justification:**

- **REASONING**: The paper addresses logical reasoning capabilities through "complex constraints" and "logical constraints" in information retrieval and synthesis tasks, which relates to enhancing reasoning abilities.

- **OPTIMIZATION**: The paper explicitly discusses "optimization methodologies" and "optimizes the system using end-to-end RL" (reinforcement learning), which directly relates to advancing training optimization methods.

- **OPEN_SOURCE**: The paper introduces "WideSeekBench" (a benchmark) and "WideSeek" (an architecture), which appear to be research contributions that would typically be released publicly as open-source resources for the community.
[04.02.2026 04:12] Error. Failed to parse JSON from LLM. ["REASONING", "OPTIMIZATION", "OPEN_SOURCE"]


**Justification:**

- **REASONING**: The paper addresses logical reasoning capabilities through "complex constraints" and "logical constraints" in information retrieval and synthesis tasks, which relates to enhancing reasoning abilities.

- **OPTIMIZATION**: The paper explicitly discusses "optimization methodologies" and "optimizes the system using end-to-end RL" (reinforcement learning), which directly relates to advancing training optimization methods.

- **OPEN_SOURCE**: The paper introduces "WideSeekBench" (a benchmark) and "WideSeek" (an architecture), which appear to be research contributions that would typically be released publicly as open-source resources for the community.
[04.02.2026 04:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents advancements in search intelligence through a new framework called Wide Research, which focuses on retrieving information under complex constraints. It introduces WideSeekBench, a benchmark designed to evaluate General Broad Information Seeking (GBIS) using a multi-phase data pipeline that ensures diverse information retrieval. Additionally, the authors propose WideSeek, a multi-agent architecture that can dynamically create sub-agents to handle various tasks in parallel. The study shows that using reinforcement learning to optimize these agents can significantly enhance the efficiency and effectiveness of information retrieval processes.","title":"Revolutionizing Search Intelligence with Wide Research"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper presents advancements in search intelligence through a new framework called Wide Research, which focuses on retrieving information under complex constraints. It introduces WideSeekBench, a benchmark designed to evaluate General Broad Information Seeking (GBIS) using a multi-phase data pipeline that ensures diverse information retrieval. Additionally, the authors propose WideSeek, a multi-agent architecture that can dynamically create sub-agents to handle various tasks in parallel. The study shows that using reinforcement learning to optimize these agents can significantly enhance the efficiency and effectiveness of information retrieval processes.', title='Revolutionizing Search Intelligence with Wide Research'))
[04.02.2026 04:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Wide ResearchWideSeekBenchWideSeekWideSeek","title":""}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Wide ResearchWideSeekBenchWideSeekWideSeek', title=''))
[04.02.2026 04:12] Querying the API.
[04.02.2026 04:12] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

LENS framework improves reinforcement learning with verifiable rewards by identifying and removing interference tokens to enhance exploration efficiency and training stability.  					AI-generated summary 				 Reinforcement Learning with Verifiable Rewards (RLVR) has advanced LLM reasoning, but remains constrained by inefficient exploration under limited rollout budgets, leading to low sampling success and unstable training in complex tasks. We find that many exploration failures arise not from problem difficulty, but from a small number of prompt tokens that introduce interference. Building on this insight, we propose the Less Noise Sampling Framework (LENS), which first prompts by identifying and removing interference tokens. then transfers successful rollouts from the purification process to supervise policy optimization on the original noisy prompts, enabling the model to learn to ignore interference in the real-world, noisy prompting settings. Experimental results show that LENS significantly outperforms GRPO, delivering higher performance and faster convergence, with a 3.88% average gain and over 1.6times speedup. Our work highlights the critical role of pruning interference tokens in improving rollout efficiency, offering a new perspective for RLVR research.
[04.02.2026 04:12] Response: ```json
{
  "desc": "   LENS -        ,         .  ,          ,  ,    .         ,            .         GRPO:    3,88%    1,6 .",
  "emoji": "",
  "title": "        "
}
```
[04.02.2026 04:12] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"LENS framework improves reinforcement learning with verifiable rewards by identifying and removing interference tokens to enhance exploration efficiency and training stability.  					AI-generated summary 				 Reinforcement Learning with Verifiable Rewards (RLVR) has advanced LLM reasoning, but remains constrained by inefficient exploration under limited rollout budgets, leading to low sampling success and unstable training in complex tasks. We find that many exploration failures arise not from problem difficulty, but from a small number of prompt tokens that introduce interference. Building on this insight, we propose the Less Noise Sampling Framework (LENS), which first prompts by identifying and removing interference tokens. then transfers successful rollouts from the purification process to supervise policy optimization on the original noisy prompts, enabling the model to learn to ignore interference in the real-world, noisy prompting settings. Experimental results show that LENS significantly outperforms GRPO, delivering higher performance and faster convergence, with a 3.88% average gain and over 1.6times speedup. Our work highlights the critical role of pruning interference tokens in improving rollout efficiency, offering a new perspective for RLVR research."

[04.02.2026 04:12] Response: ```python
["RL", "TRAINING"]
```
[04.02.2026 04:12] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"LENS framework improves reinforcement learning with verifiable rewards by identifying and removing interference tokens to enhance exploration efficiency and training stability.  					AI-generated summary 				 Reinforcement Learning with Verifiable Rewards (RLVR) has advanced LLM reasoning, but remains constrained by inefficient exploration under limited rollout budgets, leading to low sampling success and unstable training in complex tasks. We find that many exploration failures arise not from problem difficulty, but from a small number of prompt tokens that introduce interference. Building on this insight, we propose the Less Noise Sampling Framework (LENS), which first prompts by identifying and removing interference tokens. then transfers successful rollouts from the purification process to supervise policy optimization on the original noisy prompts, enabling the model to learn to ignore interference in the real-world, noisy prompting settings. Experimental results show that LENS significantly outperforms GRPO, delivering higher performance and faster convergence, with a 3.88% average gain and over 1.6times speedup. Our work highlights the critical role of pruning interference tokens in improving rollout efficiency, offering a new perspective for RLVR research."

[04.02.2026 04:12] Response: ```python
["REASONING", "OPTIMIZATION"]
```
[04.02.2026 04:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The LENS framework enhances reinforcement learning by focusing on verifiable rewards and improving exploration efficiency. It identifies and removes interference tokens that hinder the learning process, leading to more stable training outcomes. By purifying the prompts, LENS allows the model to learn effectively from successful rollouts, even in noisy environments. Experimental results demonstrate that LENS achieves better performance and faster convergence compared to traditional methods, highlighting the importance of reducing noise in reinforcement learning tasks.","title":"Enhancing Reinforcement Learning by Reducing Interference"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='The LENS framework enhances reinforcement learning by focusing on verifiable rewards and improving exploration efficiency. It identifies and removes interference tokens that hinder the learning process, leading to more stable training outcomes. By purifying the prompts, LENS allows the model to learn effectively from successful rollouts, even in noisy environments. Experimental results demonstrate that LENS achieves better performance and faster convergence compared to traditional methods, highlighting the importance of reducing noise in reinforcement learning tasks.', title='Enhancing Reinforcement Learning by Reducing Interference'))
[04.02.2026 04:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"LENSLENSLENSGRPO","title":""}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='LENSLENSLENSGRPO', title=''))
[04.02.2026 04:12] Querying the API.
[04.02.2026 04:12] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Research investigates latent planning dynamics in large language models through a probing method called Tele-Lens, revealing limited global planning and enabling improved uncertainty estimation and CoT bypass recognition.  					AI-generated summary 				 This work stems from prior complementary observations on the dynamics of Chain-of-Thought (CoT): Large Language Models (LLMs) is shown latent planning of subsequent reasoning prior to CoT emergence, thereby diminishing the significance of explicit CoT; whereas CoT remains critical for tasks requiring multi-step reasoning. To deepen the understanding between LLM's internal states and its verbalized reasoning trajectories, we investigate the latent planning strength of LLMs, through our probing method, Tele-Lens, applying to hidden states across diverse task domains. Our empirical results indicate that LLMs exhibit a myopic horizon, primarily conducting incremental transitions without precise global planning. Leveraging this characteristic, we propose a hypothesis on enhancing uncertainty estimation of CoT, which we validate that a small subset of CoT positions can effectively represent the uncertainty of the entire path. We further underscore the significance of exploiting CoT dynamics, and demonstrate that automatic recognition of CoT bypass can be achieved without performance degradation. Our code, data and models are released at https://github.com/lxucs/tele-lens.
[04.02.2026 04:12] Response: ```json
{
  "desc": "        ,    Tele-Lens.  ,  LLM             .            ,         .  , ,           .",
  "emoji": "",
  "title": "   LLM:    "
}
```
[04.02.2026 04:12] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Research investigates latent planning dynamics in large language models through a probing method called Tele-Lens, revealing limited global planning and enabling improved uncertainty estimation and CoT bypass recognition.  					AI-generated summary 				 This work stems from prior complementary observations on the dynamics of Chain-of-Thought (CoT): Large Language Models (LLMs) is shown latent planning of subsequent reasoning prior to CoT emergence, thereby diminishing the significance of explicit CoT; whereas CoT remains critical for tasks requiring multi-step reasoning. To deepen the understanding between LLM's internal states and its verbalized reasoning trajectories, we investigate the latent planning strength of LLMs, through our probing method, Tele-Lens, applying to hidden states across diverse task domains. Our empirical results indicate that LLMs exhibit a myopic horizon, primarily conducting incremental transitions without precise global planning. Leveraging this characteristic, we propose a hypothesis on enhancing uncertainty estimation of CoT, which we validate that a small subset of CoT positions can effectively represent the uncertainty of the entire path. We further underscore the significance of exploiting CoT dynamics, and demonstrate that automatic recognition of CoT bypass can be achieved without performance degradation. Our code, data and models are released at https://github.com/lxucs/tele-lens."

[04.02.2026 04:12] Response: ```python
["BENCHMARK", "TRAINING"]
```
[04.02.2026 04:12] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Research investigates latent planning dynamics in large language models through a probing method called Tele-Lens, revealing limited global planning and enabling improved uncertainty estimation and CoT bypass recognition.  					AI-generated summary 				 This work stems from prior complementary observations on the dynamics of Chain-of-Thought (CoT): Large Language Models (LLMs) is shown latent planning of subsequent reasoning prior to CoT emergence, thereby diminishing the significance of explicit CoT; whereas CoT remains critical for tasks requiring multi-step reasoning. To deepen the understanding between LLM's internal states and its verbalized reasoning trajectories, we investigate the latent planning strength of LLMs, through our probing method, Tele-Lens, applying to hidden states across diverse task domains. Our empirical results indicate that LLMs exhibit a myopic horizon, primarily conducting incremental transitions without precise global planning. Leveraging this characteristic, we propose a hypothesis on enhancing uncertainty estimation of CoT, which we validate that a small subset of CoT positions can effectively represent the uncertainty of the entire path. We further underscore the significance of exploiting CoT dynamics, and demonstrate that automatic recognition of CoT bypass can be achieved without performance degradation. Our code, data and models are released at https://github.com/lxucs/tele-lens."

[04.02.2026 04:12] Response: ```python
['INTERPRETABILITY', 'REASONING', 'OPEN_SOURCE']
```
[04.02.2026 04:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper explores how large language models (LLMs) plan their reasoning processes using a method called Tele-Lens. It finds that LLMs often lack comprehensive global planning, instead relying on short-term, incremental reasoning steps. The study highlights the importance of Chain-of-Thought (CoT) in tasks that require multi-step reasoning, while also suggesting that a few key CoT positions can effectively estimate uncertainty in reasoning paths. Additionally, the research shows that it is possible to recognize when CoT is bypassed without losing performance, enhancing our understanding of LLM dynamics.","title":"Unveiling Latent Planning in Language Models with Tele-Lens"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper explores how large language models (LLMs) plan their reasoning processes using a method called Tele-Lens. It finds that LLMs often lack comprehensive global planning, instead relying on short-term, incremental reasoning steps. The study highlights the importance of Chain-of-Thought (CoT) in tasks that require multi-step reasoning, while also suggesting that a few key CoT positions can effectively estimate uncertainty in reasoning paths. Additionally, the research shows that it is possible to recognize when CoT is bypassed without losing performance, enhancing our understanding of LLM dynamics.', title='Unveiling Latent Planning in Language Models with Tele-Lens'))
[04.02.2026 04:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Tele-LensLLMsLLMsCoTCoTCoT","title":""}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Tele-LensLLMsLLMsCoTCoTCoT', title=''))
[04.02.2026 04:12] Querying the API.
[04.02.2026 04:12] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

DeMix is a framework that uses model merging to predict optimal data ratios for LLM pre-training, decoupling search from training costs to improve mixture discovery efficiency.  					AI-generated summary 				 Determining an effective data mixture is a key factor in Large Language Model (LLM) pre-training, where models must balance general competence with proficiency on hard tasks such as math and code. However, identifying an optimal mixture remains an open challenge, as existing approaches either rely on unreliable tiny-scale proxy experiments or require prohibitively expensive large-scale exploration. To address this, we propose Decouple Searching from Training Mix (DeMix), a novel framework that leverages model merging to predict optimal data ratios. Instead of training proxy models for every sampled mixture, DeMix trains component models on candidate datasets at scale and derives data mixture proxies via weighted model merging. This paradigm decouples search from training costs, enabling evaluation of unlimited sampled mixtures without extra training burden and thus facilitating better mixture discovery through more search trials. Extensive experiments demonstrate that DeMix breaks the trade-off between sufficiency, accuracy and efficiency, obtaining the optimal mixture with higher benchmark performance at lower search cost. Additionally, we release the DeMix Corpora, a comprehensive 22T-token dataset comprising high-quality pre-training data with validated mixtures to facilitate open research. Our code and DeMix Corpora is available at https://github.com/Lucius-lsr/DeMix.
[04.02.2026 04:12] Response: ```json
{
  "desc": "DeMix           LLM,        -.        ,                   .          ,          .    DeMix Corpora     22         .",
  "emoji": "",
  "title": "        "
}
```
[04.02.2026 04:12] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"DeMix is a framework that uses model merging to predict optimal data ratios for LLM pre-training, decoupling search from training costs to improve mixture discovery efficiency.  					AI-generated summary 				 Determining an effective data mixture is a key factor in Large Language Model (LLM) pre-training, where models must balance general competence with proficiency on hard tasks such as math and code. However, identifying an optimal mixture remains an open challenge, as existing approaches either rely on unreliable tiny-scale proxy experiments or require prohibitively expensive large-scale exploration. To address this, we propose Decouple Searching from Training Mix (DeMix), a novel framework that leverages model merging to predict optimal data ratios. Instead of training proxy models for every sampled mixture, DeMix trains component models on candidate datasets at scale and derives data mixture proxies via weighted model merging. This paradigm decouples search from training costs, enabling evaluation of unlimited sampled mixtures without extra training burden and thus facilitating better mixture discovery through more search trials. Extensive experiments demonstrate that DeMix breaks the trade-off between sufficiency, accuracy and efficiency, obtaining the optimal mixture with higher benchmark performance at lower search cost. Additionally, we release the DeMix Corpora, a comprehensive 22T-token dataset comprising high-quality pre-training data with validated mixtures to facilitate open research. Our code and DeMix Corpora is available at https://github.com/Lucius-lsr/DeMix."

[04.02.2026 04:12] Response: ```python
["DATASET", "TRAINING", "DATA"]
```
[04.02.2026 04:12] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"DeMix is a framework that uses model merging to predict optimal data ratios for LLM pre-training, decoupling search from training costs to improve mixture discovery efficiency.  					AI-generated summary 				 Determining an effective data mixture is a key factor in Large Language Model (LLM) pre-training, where models must balance general competence with proficiency on hard tasks such as math and code. However, identifying an optimal mixture remains an open challenge, as existing approaches either rely on unreliable tiny-scale proxy experiments or require prohibitively expensive large-scale exploration. To address this, we propose Decouple Searching from Training Mix (DeMix), a novel framework that leverages model merging to predict optimal data ratios. Instead of training proxy models for every sampled mixture, DeMix trains component models on candidate datasets at scale and derives data mixture proxies via weighted model merging. This paradigm decouples search from training costs, enabling evaluation of unlimited sampled mixtures without extra training burden and thus facilitating better mixture discovery through more search trials. Extensive experiments demonstrate that DeMix breaks the trade-off between sufficiency, accuracy and efficiency, obtaining the optimal mixture with higher benchmark performance at lower search cost. Additionally, we release the DeMix Corpora, a comprehensive 22T-token dataset comprising high-quality pre-training data with validated mixtures to facilitate open research. Our code and DeMix Corpora is available at https://github.com/Lucius-lsr/DeMix."

[04.02.2026 04:12] Response: ```python
['OPTIMIZATION', 'OPEN_SOURCE']
```
[04.02.2026 04:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"DeMix is a framework designed to enhance the efficiency of discovering optimal data mixtures for pre-training Large Language Models (LLMs). It achieves this by using model merging techniques to predict the best data ratios, which allows for extensive mixture evaluation without the need for costly training of proxy models. This decoupling of search from training costs enables researchers to explore a wider range of data mixtures, leading to improved performance on challenging tasks. The framework also introduces the DeMix Corpora, a large dataset that supports further research in this area by providing validated data mixtures.","title":"Decoupling Search from Training for Optimal Data Mixtures in LLMs"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='DeMix is a framework designed to enhance the efficiency of discovering optimal data mixtures for pre-training Large Language Models (LLMs). It achieves this by using model merging techniques to predict the best data ratios, which allows for extensive mixture evaluation without the need for costly training of proxy models. This decoupling of search from training costs enables researchers to explore a wider range of data mixtures, leading to improved performance on challenging tasks. The framework also introduces the DeMix Corpora, a large dataset that supports further research in this area by providing validated data mixtures.', title='Decoupling Search from Training for Optimal Data Mixtures in LLMs'))
[04.02.2026 04:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"DeMixLLMDeMixDeMix","title":""}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='DeMixLLMDeMixDeMix', title=''))
[04.02.2026 04:13] Querying the API.
[04.02.2026 04:13] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

XDLM unifies Masked Diffusion Language Models and Uniform-noise Diffusion Language Models through a stationary noise kernel, achieving improved performance in both semantic understanding and generation quality.  					AI-generated summary 				 In discrete generative modeling, two dominant paradigms demonstrate divergent capabilities: Masked Diffusion Language Models (MDLM) excel at semantic understanding and zero-shot generalization, whereas Uniform-noise Diffusion Language Models (UDLM) achieve strong few-step generation quality, yet neither attains balanced performance across both dimensions. To address this, we propose XDLM, which bridges the two paradigms via a stationary noise kernel. XDLM offers two key contributions: (1) it provides a principled theoretical unification of MDLM and UDLM, recovering each paradigm as a special case; and (2) an alleviated memory bottleneck enabled by an algebraic simplification of the posterior probabilities. Experiments demonstrate that XDLM advances the Pareto frontier between understanding capability and generation quality. Quantitatively, XDLM surpasses UDLM by 5.4 points on zero-shot text benchmarks and outperforms MDLM in few-step image generation (FID 54.1 vs. 80.8). When scaled to tune an 8B-parameter large language model, XDLM achieves 15.0 MBPP in just 32 steps, effectively doubling the baseline performance. Finally, analysis of training dynamics reveals XDLM's superior potential for long-term scaling. Code is available at https://github.com/MzeroMiko/XDLM
[04.02.2026 04:13] Response: ```json
{
  "desc": "   XDLM   ,           .  ,  Masked Diffusion Language Models  Uniform-noise Diffusion Language Models     ,        . XDLM          ,     ,       .           ,       32 .)",
  "emoji": "",
  "title": " :       "
}
```
[04.02.2026 04:13] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"XDLM unifies Masked Diffusion Language Models and Uniform-noise Diffusion Language Models through a stationary noise kernel, achieving improved performance in both semantic understanding and generation quality.  					AI-generated summary 				 In discrete generative modeling, two dominant paradigms demonstrate divergent capabilities: Masked Diffusion Language Models (MDLM) excel at semantic understanding and zero-shot generalization, whereas Uniform-noise Diffusion Language Models (UDLM) achieve strong few-step generation quality, yet neither attains balanced performance across both dimensions. To address this, we propose XDLM, which bridges the two paradigms via a stationary noise kernel. XDLM offers two key contributions: (1) it provides a principled theoretical unification of MDLM and UDLM, recovering each paradigm as a special case; and (2) an alleviated memory bottleneck enabled by an algebraic simplification of the posterior probabilities. Experiments demonstrate that XDLM advances the Pareto frontier between understanding capability and generation quality. Quantitatively, XDLM surpasses UDLM by 5.4 points on zero-shot text benchmarks and outperforms MDLM in few-step image generation (FID 54.1 vs. 80.8). When scaled to tune an 8B-parameter large language model, XDLM achieves 15.0 MBPP in just 32 steps, effectively doubling the baseline performance. Finally, analysis of training dynamics reveals XDLM's superior potential for long-term scaling. Code is available at https://github.com/MzeroMiko/XDLM"

[04.02.2026 04:13] Response: ```python
["ARCHITECTURE", "TRAINING", "MULTIMODAL"]
```

**Justification:**

- **ARCHITECTURE**: The paper proposes XDLM, a novel neural architecture that unifies two different diffusion language model paradigms (MDLM and UDLM) through a stationary noise kernel. This is a core architectural contribution.

- **TRAINING**: The paper focuses on improving model training methods, including theoretical unification of different training paradigms and an algebraic simplification that alleviates memory bottlenecks during training.

- **MULTIMODAL**: The paper demonstrates applications across both text and image modalities (zero-shot text benchmarks and few-step image generation), indicating multimodal capabilities.
[04.02.2026 04:13] Error. Failed to parse JSON from LLM. ["ARCHITECTURE", "TRAINING", "MULTIMODAL"]


**Justification:**

- **ARCHITECTURE**: The paper proposes XDLM, a novel neural architecture that unifies two different diffusion language model paradigms (MDLM and UDLM) through a stationary noise kernel. This is a core architectural contribution.

- **TRAINING**: The paper focuses on improving model training methods, including theoretical unification of different training paradigms and an algebraic simplification that alleviates memory bottlenecks during training.

- **MULTIMODAL**: The paper demonstrates applications across both text and image modalities (zero-shot text benchmarks and few-step image generation), indicating multimodal capabilities.
[04.02.2026 04:13] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"XDLM unifies Masked Diffusion Language Models and Uniform-noise Diffusion Language Models through a stationary noise kernel, achieving improved performance in both semantic understanding and generation quality.  					AI-generated summary 				 In discrete generative modeling, two dominant paradigms demonstrate divergent capabilities: Masked Diffusion Language Models (MDLM) excel at semantic understanding and zero-shot generalization, whereas Uniform-noise Diffusion Language Models (UDLM) achieve strong few-step generation quality, yet neither attains balanced performance across both dimensions. To address this, we propose XDLM, which bridges the two paradigms via a stationary noise kernel. XDLM offers two key contributions: (1) it provides a principled theoretical unification of MDLM and UDLM, recovering each paradigm as a special case; and (2) an alleviated memory bottleneck enabled by an algebraic simplification of the posterior probabilities. Experiments demonstrate that XDLM advances the Pareto frontier between understanding capability and generation quality. Quantitatively, XDLM surpasses UDLM by 5.4 points on zero-shot text benchmarks and outperforms MDLM in few-step image generation (FID 54.1 vs. 80.8). When scaled to tune an 8B-parameter large language model, XDLM achieves 15.0 MBPP in just 32 steps, effectively doubling the baseline performance. Finally, analysis of training dynamics reveals XDLM's superior potential for long-term scaling. Code is available at https://github.com/MzeroMiko/XDLM"

[04.02.2026 04:13] Response: ```python
["DIFFUSION", "OPTIMIZATION", "OPEN_SOURCE"]
```
[04.02.2026 04:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"XDLM is a novel framework that combines the strengths of Masked Diffusion Language Models (MDLM) and Uniform-noise Diffusion Language Models (UDLM) using a stationary noise kernel. This unification allows XDLM to enhance both semantic understanding and generation quality, addressing the limitations of each individual model. The framework not only theoretically connects MDLM and UDLM but also simplifies memory usage through algebraic adjustments in posterior probabilities. Experimental results show that XDLM significantly improves performance on zero-shot text tasks and few-step image generation, demonstrating its effectiveness in balancing understanding and generation capabilities.","title":"XDLM: Bridging Understanding and Generation in Language Models"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='XDLM is a novel framework that combines the strengths of Masked Diffusion Language Models (MDLM) and Uniform-noise Diffusion Language Models (UDLM) using a stationary noise kernel. This unification allows XDLM to enhance both semantic understanding and generation quality, addressing the limitations of each individual model. The framework not only theoretically connects MDLM and UDLM but also simplifies memory usage through algebraic adjustments in posterior probabilities. Experimental results show that XDLM significantly improves performance on zero-shot text tasks and few-step image generation, demonstrating its effectiveness in balancing understanding and generation capabilities.', title='XDLM: Bridging Understanding and Generation in Language Models'))
[04.02.2026 04:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"XDLMMDLMUDLMXDLMMDLMUDLMXDLM","title":"XDLM"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='XDLMMDLMUDLMXDLMMDLMUDLMXDLM', title='XDLM'))
[04.02.2026 04:13] Querying the API.
[04.02.2026 04:13] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A reinforcement learning framework with glance and focus models improves pan-cancer screening in CT scans by addressing foreground-background imbalance and reducing false positives through group relative learning.  					AI-generated summary 				 Pan-cancer screening in large-scale CT scans remains challenging for existing AI methods, primarily due to the difficulty of localizing diverse types of tiny lesions in large CT volumes. The extreme foreground-background imbalance significantly hinders models from focusing on diseased regions, while redundant focus on healthy regions not only decreases the efficiency but also increases false positives. Inspired by radiologists' glance and focus diagnostic strategy, we introduce GF-Screen, a Glance and Focus reinforcement learning framework for pan-cancer screening. GF-Screen employs a Glance model to localize the diseased regions and a Focus model to precisely segment the lesions, where segmentation results of the Focus model are leveraged to reward the Glance model via Reinforcement Learning (RL). Specifically, the Glance model crops a group of sub-volumes from the entire CT volume and learns to select the sub-volumes with lesions for the Focus model to segment. Given that the selecting operation is non-differentiable for segmentation training, we propose to employ the segmentation results to reward the Glance model. To optimize the Glance model, we introduce a novel group relative learning paradigm, which employs group relative comparison to prioritize high-advantage predictions and discard low-advantage predictions within sub-volume groups, not only improving efficiency but also reducing false positives. In this way, for the first time, we effectively extend cutting-edge RL techniques to tackle the specific challenges in pan-cancer screening. Extensive experiments on 16 internal and 7 external datasets across 9 lesion types demonstrated the effectiveness of GF-Screen. Notably, GF-Screen leads the public validation leaderboard of MICCAI FLARE25 pan-cancer challenge, surpassing the FLARE24 champion solution by a large margin (+25.6% DSC and +28.2% NSD).
[04.02.2026 04:13] Response: ```json
{
  "desc": "  GF-Screen          -,         .    :  \"\"    ,   \"\"   .   \"\"      \"\"    .      ,      ,       .",
  "emoji": "",
  "title": "         "
}
```
[04.02.2026 04:13] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A reinforcement learning framework with glance and focus models improves pan-cancer screening in CT scans by addressing foreground-background imbalance and reducing false positives through group relative learning.  					AI-generated summary 				 Pan-cancer screening in large-scale CT scans remains challenging for existing AI methods, primarily due to the difficulty of localizing diverse types of tiny lesions in large CT volumes. The extreme foreground-background imbalance significantly hinders models from focusing on diseased regions, while redundant focus on healthy regions not only decreases the efficiency but also increases false positives. Inspired by radiologists' glance and focus diagnostic strategy, we introduce GF-Screen, a Glance and Focus reinforcement learning framework for pan-cancer screening. GF-Screen employs a Glance model to localize the diseased regions and a Focus model to precisely segment the lesions, where segmentation results of the Focus model are leveraged to reward the Glance model via Reinforcement Learning (RL). Specifically, the Glance model crops a group of sub-volumes from the entire CT volume and learns to select the sub-volumes with lesions for the Focus model to segment. Given that the selecting operation is non-differentiable for segmentation training, we propose to employ the segmentation results to reward the Glance model. To optimize the Glance model, we introduce a novel group relative learning paradigm, which employs group relative comparison to prioritize high-advantage predictions and discard low-advantage predictions within sub-volume groups, not only improving efficiency but also reducing false positives. In this way, for the first time, we effectively extend cutting-edge RL techniques to tackle the specific challenges in pan-cancer screening. Extensive experiments on 16 internal and 7 external datasets across 9 lesion types demonstrated the effectiveness of GF-Screen. Notably, GF-Screen leads the public validation leaderboard of MICCAI FLARE25 pan-cancer challenge, surpassing the FLARE24 champion solution by a large margin (+25.6% DSC and +28.2% NSD)."

[04.02.2026 04:13] Response: ```python
['RL', 'HEALTHCARE', 'CV']
```
[04.02.2026 04:13] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A reinforcement learning framework with glance and focus models improves pan-cancer screening in CT scans by addressing foreground-background imbalance and reducing false positives through group relative learning.  					AI-generated summary 				 Pan-cancer screening in large-scale CT scans remains challenging for existing AI methods, primarily due to the difficulty of localizing diverse types of tiny lesions in large CT volumes. The extreme foreground-background imbalance significantly hinders models from focusing on diseased regions, while redundant focus on healthy regions not only decreases the efficiency but also increases false positives. Inspired by radiologists' glance and focus diagnostic strategy, we introduce GF-Screen, a Glance and Focus reinforcement learning framework for pan-cancer screening. GF-Screen employs a Glance model to localize the diseased regions and a Focus model to precisely segment the lesions, where segmentation results of the Focus model are leveraged to reward the Glance model via Reinforcement Learning (RL). Specifically, the Glance model crops a group of sub-volumes from the entire CT volume and learns to select the sub-volumes with lesions for the Focus model to segment. Given that the selecting operation is non-differentiable for segmentation training, we propose to employ the segmentation results to reward the Glance model. To optimize the Glance model, we introduce a novel group relative learning paradigm, which employs group relative comparison to prioritize high-advantage predictions and discard low-advantage predictions within sub-volume groups, not only improving efficiency but also reducing false positives. In this way, for the first time, we effectively extend cutting-edge RL techniques to tackle the specific challenges in pan-cancer screening. Extensive experiments on 16 internal and 7 external datasets across 9 lesion types demonstrated the effectiveness of GF-Screen. Notably, GF-Screen leads the public validation leaderboard of MICCAI FLARE25 pan-cancer challenge, surpassing the FLARE24 champion solution by a large margin (+25.6% DSC and +28.2% NSD)."

[04.02.2026 04:13] Response: ```python
['OPTIMIZATION', 'SCIENCE']
```

**Justification:**

- **OPTIMIZATION**: The paper introduces a novel reinforcement learning framework with a "group relative learning paradigm" designed to optimize model training and improve efficiency while reducing false positives. This is fundamentally about advancing training optimization methods.

- **SCIENCE**: The paper applies machine learning to a scientific/medical application - pan-cancer screening in CT scans. It addresses a specific scientific problem in medical imaging and demonstrates results on medical datasets, which falls under scientific applications of AI/ML.
[04.02.2026 04:13] Error. Failed to parse JSON from LLM. ["OPTIMIZATION", "SCIENCE"]


**Justification:**

- **OPTIMIZATION**: The paper introduces a novel reinforcement learning framework with a "group relative learning paradigm" designed to optimize model training and improve efficiency while reducing false positives. This is fundamentally about advancing training optimization methods.

- **SCIENCE**: The paper applies machine learning to a scientific/medical application - pan-cancer screening in CT scans. It addresses a specific scientific problem in medical imaging and demonstrates results on medical datasets, which falls under scientific applications of AI/ML.
[04.02.2026 04:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents GF-Screen, a novel reinforcement learning framework designed to enhance pan-cancer screening in CT scans. It addresses the challenge of foreground-background imbalance by utilizing a Glance model to identify regions with lesions and a Focus model to accurately segment these lesions. The framework employs group relative learning to optimize the Glance model, allowing it to prioritize the most promising sub-volumes for analysis while minimizing false positives. Extensive testing on multiple datasets shows that GF-Screen significantly outperforms existing methods, achieving top results in a major pan-cancer challenge.","title":"Enhancing Pan-Cancer Screening with Glance and Focus Models"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper presents GF-Screen, a novel reinforcement learning framework designed to enhance pan-cancer screening in CT scans. It addresses the challenge of foreground-background imbalance by utilizing a Glance model to identify regions with lesions and a Focus model to accurately segment these lesions. The framework employs group relative learning to optimize the Glance model, allowing it to prioritize the most promising sub-volumes for analysis while minimizing false positives. Extensive testing on multiple datasets shows that GF-Screen significantly outperforms existing methods, achieving top results in a major pan-cancer challenge.', title='Enhancing Pan-Cancer Screening with Glance and Focus Models'))
[04.02.2026 04:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"GF-ScreenCTGF-ScreenGF-Screen","title":"GF-ScreenCT"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='GF-ScreenCTGF-ScreenGF-Screen', title='GF-ScreenCT'))
[04.02.2026 04:13] Querying the API.
[04.02.2026 04:13] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

CoViP addresses contextualized visual personalization by treating personalized image captioning as a core task and improving capabilities through reinforcement-learning-based post-training and caption-augmented generation.  					AI-generated summary 				 Despite recent progress in vision-language models (VLMs), existing approaches often fail to generate personalized responses based on the user's specific experiences, as they lack the ability to associate visual inputs with a user's accumulated visual-textual context. We newly formalize this challenge as contextualized visual personalization, which requires the visual recognition and textual retrieval of personalized visual experiences by VLMs when interpreting new images. To address this issue, we propose CoViP, a unified framework that treats personalized image captioning as a core task for contextualized visual personalization and improves this capability through reinforcement-learning-based post-training and caption-augmented generation. We further introduce diagnostic evaluations that explicitly rule out textual shortcut solutions and verify whether VLMs truly leverage visual context. Extensive experiments demonstrate that existing open-source and proprietary VLMs exhibit substantial limitations, while CoViP not only improves personalized image captioning but also yields holistic gains across downstream personalization tasks. These results highlight CoViP as a crucial stage for enabling robust and generalizable contextualized visual personalization.
[04.02.2026 04:13] Response: ```json
{
  "desc": "    CoViP      -.     ,            ,       .              ,      ,   .  ,         ,       .",
  "emoji": "",
  "title": "      vision-language"
}
```
[04.02.2026 04:13] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"CoViP addresses contextualized visual personalization by treating personalized image captioning as a core task and improving capabilities through reinforcement-learning-based post-training and caption-augmented generation.  					AI-generated summary 				 Despite recent progress in vision-language models (VLMs), existing approaches often fail to generate personalized responses based on the user's specific experiences, as they lack the ability to associate visual inputs with a user's accumulated visual-textual context. We newly formalize this challenge as contextualized visual personalization, which requires the visual recognition and textual retrieval of personalized visual experiences by VLMs when interpreting new images. To address this issue, we propose CoViP, a unified framework that treats personalized image captioning as a core task for contextualized visual personalization and improves this capability through reinforcement-learning-based post-training and caption-augmented generation. We further introduce diagnostic evaluations that explicitly rule out textual shortcut solutions and verify whether VLMs truly leverage visual context. Extensive experiments demonstrate that existing open-source and proprietary VLMs exhibit substantial limitations, while CoViP not only improves personalized image captioning but also yields holistic gains across downstream personalization tasks. These results highlight CoViP as a crucial stage for enabling robust and generalizable contextualized visual personalization."

[04.02.2026 04:13] Response: ```python
["CV", "MULTIMODAL", "RL", "BENCHMARK"]
```
[04.02.2026 04:13] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"CoViP addresses contextualized visual personalization by treating personalized image captioning as a core task and improving capabilities through reinforcement-learning-based post-training and caption-augmented generation.  					AI-generated summary 				 Despite recent progress in vision-language models (VLMs), existing approaches often fail to generate personalized responses based on the user's specific experiences, as they lack the ability to associate visual inputs with a user's accumulated visual-textual context. We newly formalize this challenge as contextualized visual personalization, which requires the visual recognition and textual retrieval of personalized visual experiences by VLMs when interpreting new images. To address this issue, we propose CoViP, a unified framework that treats personalized image captioning as a core task for contextualized visual personalization and improves this capability through reinforcement-learning-based post-training and caption-augmented generation. We further introduce diagnostic evaluations that explicitly rule out textual shortcut solutions and verify whether VLMs truly leverage visual context. Extensive experiments demonstrate that existing open-source and proprietary VLMs exhibit substantial limitations, while CoViP not only improves personalized image captioning but also yields holistic gains across downstream personalization tasks. These results highlight CoViP as a crucial stage for enabling robust and generalizable contextualized visual personalization."

[04.02.2026 04:13] Response: ```python
['OPTIMIZATION', 'ALIGNMENT']
```

**Justification:**

- **OPTIMIZATION**: The paper uses "reinforcement-learning-based post-training" to improve model capabilities, which is an optimization method for training.

- **ALIGNMENT**: The paper addresses aligning vision-language models to generate personalized responses based on user-specific experiences and preferences, which relates to aligning models with human values and intended behavior (personalization according to user context).
[04.02.2026 04:13] Error. Failed to parse JSON from LLM. ["OPTIMIZATION", "ALIGNMENT"]


**Justification:**

- **OPTIMIZATION**: The paper uses "reinforcement-learning-based post-training" to improve model capabilities, which is an optimization method for training.

- **ALIGNMENT**: The paper addresses aligning vision-language models to generate personalized responses based on user-specific experiences and preferences, which relates to aligning models with human values and intended behavior (personalization according to user context).
[04.02.2026 04:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"CoViP is a framework designed to enhance personalized image captioning by integrating contextualized visual personalization. It addresses the limitations of current vision-language models (VLMs) that struggle to generate responses tailored to individual user experiences. By employing reinforcement learning for post-training and augmenting generation with captions, CoViP improves the model\'s ability to connect visual inputs with a user\'s unique context. The framework also includes diagnostic evaluations to ensure that VLMs effectively utilize visual context rather than relying on shortcuts, demonstrating significant improvements in personalized tasks.","title":"Enhancing Personalization in Image Captioning with CoViP"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="CoViP is a framework designed to enhance personalized image captioning by integrating contextualized visual personalization. It addresses the limitations of current vision-language models (VLMs) that struggle to generate responses tailored to individual user experiences. By employing reinforcement learning for post-training and augmenting generation with captions, CoViP improves the model's ability to connect visual inputs with a user's unique context. The framework also includes diagnostic evaluations to ensure that VLMs effectively utilize visual context rather than relying on shortcuts, demonstrating significant improvements in personalized tasks.", title='Enhancing Personalization in Image Captioning with CoViP'))
[04.02.2026 04:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"CoViPCoViP","title":"CoViP"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='CoViPCoViP', title='CoViP'))
[04.02.2026 04:13] Querying the API.
[04.02.2026 04:13] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

CoBA-RL adapts rollout budget allocation for LLM training by evaluating sample training value and optimizing resource distribution through a capability-oriented value function and greedy strategy.  					AI-generated summary 				 Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a key approach for enhancing LLM reasoning.However, standard frameworks like Group Relative Policy Optimization (GRPO) typically employ a uniform rollout budget, leading to resource inefficiency. Moreover, existing adaptive methods often rely on instance-level metrics, such as task pass rates, failing to capture the model's dynamic learning state. To address these limitations, we propose CoBA-RL, a reinforcement learning algorithm designed to adaptively allocate rollout budgets based on the model's evolving capability. Specifically, CoBA-RL utilizes a Capability-Oriented Value function to map tasks to their potential training gains and employs a heap-based greedy strategy to efficiently self-calibrate the distribution of computational resources to samples with high training value. Extensive experiments demonstrate that our approach effectively orchestrates the trade-off between exploration and exploitation, delivering consistent generalization improvements across multiple challenging benchmarks. These findings underscore that quantifying sample training value and optimizing budget allocation are pivotal for advancing LLM post-training efficiency.
[04.02.2026 04:13] Response: ```json
{
  "desc": "CoBA-RL      ,               .     ,    ,         .                   .        ,        LLM  .",
  "emoji": "",
  "title": "        "
}
```
[04.02.2026 04:13] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"CoBA-RL adapts rollout budget allocation for LLM training by evaluating sample training value and optimizing resource distribution through a capability-oriented value function and greedy strategy.  					AI-generated summary 				 Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a key approach for enhancing LLM reasoning.However, standard frameworks like Group Relative Policy Optimization (GRPO) typically employ a uniform rollout budget, leading to resource inefficiency. Moreover, existing adaptive methods often rely on instance-level metrics, such as task pass rates, failing to capture the model's dynamic learning state. To address these limitations, we propose CoBA-RL, a reinforcement learning algorithm designed to adaptively allocate rollout budgets based on the model's evolving capability. Specifically, CoBA-RL utilizes a Capability-Oriented Value function to map tasks to their potential training gains and employs a heap-based greedy strategy to efficiently self-calibrate the distribution of computational resources to samples with high training value. Extensive experiments demonstrate that our approach effectively orchestrates the trade-off between exploration and exploitation, delivering consistent generalization improvements across multiple challenging benchmarks. These findings underscore that quantifying sample training value and optimizing budget allocation are pivotal for advancing LLM post-training efficiency."

[04.02.2026 04:13] Response: ```python
["RL", "TRAINING"]
```
[04.02.2026 04:13] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"CoBA-RL adapts rollout budget allocation for LLM training by evaluating sample training value and optimizing resource distribution through a capability-oriented value function and greedy strategy.  					AI-generated summary 				 Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a key approach for enhancing LLM reasoning.However, standard frameworks like Group Relative Policy Optimization (GRPO) typically employ a uniform rollout budget, leading to resource inefficiency. Moreover, existing adaptive methods often rely on instance-level metrics, such as task pass rates, failing to capture the model's dynamic learning state. To address these limitations, we propose CoBA-RL, a reinforcement learning algorithm designed to adaptively allocate rollout budgets based on the model's evolving capability. Specifically, CoBA-RL utilizes a Capability-Oriented Value function to map tasks to their potential training gains and employs a heap-based greedy strategy to efficiently self-calibrate the distribution of computational resources to samples with high training value. Extensive experiments demonstrate that our approach effectively orchestrates the trade-off between exploration and exploitation, delivering consistent generalization improvements across multiple challenging benchmarks. These findings underscore that quantifying sample training value and optimizing budget allocation are pivotal for advancing LLM post-training efficiency."

[04.02.2026 04:13] Response: ```python
["OPTIMIZATION", "REASONING"]
```
[04.02.2026 04:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"CoBA-RL is a novel reinforcement learning algorithm that optimizes the allocation of rollout budgets during the training of large language models (LLMs). It introduces a Capability-Oriented Value function to assess the potential training gains of different tasks, allowing for a more efficient distribution of computational resources. Unlike traditional methods that use a uniform budget, CoBA-RL employs a greedy strategy to focus on samples that offer the highest training value. The results show that this adaptive approach significantly enhances the model\'s generalization performance across various benchmarks by effectively balancing exploration and exploitation.","title":"Optimizing Training Budgets for Smarter LLMs"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="CoBA-RL is a novel reinforcement learning algorithm that optimizes the allocation of rollout budgets during the training of large language models (LLMs). It introduces a Capability-Oriented Value function to assess the potential training gains of different tasks, allowing for a more efficient distribution of computational resources. Unlike traditional methods that use a uniform budget, CoBA-RL employs a greedy strategy to focus on samples that offer the highest training value. The results show that this adaptive approach significantly enhances the model's generalization performance across various benchmarks by effectively balancing exploration and exploitation.", title='Optimizing Training Budgets for Smarter LLMs'))
[04.02.2026 04:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"CoBA-RLCoBA-RL","title":""}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='CoBA-RLCoBA-RL', title=''))
[04.02.2026 04:13] Querying the API.
[04.02.2026 04:13] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

FaceLinkGen attack demonstrates that current privacy-preserving face recognition methods fail to protect identity information despite pixel-level distortion metrics suggesting adequate protection.  					AI-generated summary 				 Transformation-based privacy-preserving face recognition (PPFR) aims to verify identities while hiding facial data from attackers and malicious service providers. Existing evaluations mostly treat privacy as resistance to pixel-level reconstruction, measured by PSNR and SSIM. We show that this reconstruction-centric view fails. We present FaceLinkGen, an identity extraction attack that performs linkage/matching and face regeneration directly from protected templates without recovering original pixels. On three recent PPFR systems, FaceLinkGen reaches over 98.5\% matching accuracy and above 96\% regeneration success, and still exceeds 92\% matching and 94\% regeneration in a near zero knowledge setting. These results expose a structural gap between pixel distortion metrics, which are widely used in PPFR evaluation, and real privacy. We show that visual obfuscation leaves identity information broadly exposed to both external intruders and untrusted service providers.
[04.02.2026 04:13] Response: ```json
{
  "desc": "    FaceLinkGen,          -     ,       .  ,             (PSNR, SSIM),     .                ,   98.5%  .           , ,         .",
  "emoji": "",
  "title": "     "
}
```
[04.02.2026 04:13] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"FaceLinkGen attack demonstrates that current privacy-preserving face recognition methods fail to protect identity information despite pixel-level distortion metrics suggesting adequate protection.  					AI-generated summary 				 Transformation-based privacy-preserving face recognition (PPFR) aims to verify identities while hiding facial data from attackers and malicious service providers. Existing evaluations mostly treat privacy as resistance to pixel-level reconstruction, measured by PSNR and SSIM. We show that this reconstruction-centric view fails. We present FaceLinkGen, an identity extraction attack that performs linkage/matching and face regeneration directly from protected templates without recovering original pixels. On three recent PPFR systems, FaceLinkGen reaches over 98.5\% matching accuracy and above 96\% regeneration success, and still exceeds 92\% matching and 94\% regeneration in a near zero knowledge setting. These results expose a structural gap between pixel distortion metrics, which are widely used in PPFR evaluation, and real privacy. We show that visual obfuscation leaves identity information broadly exposed to both external intruders and untrusted service providers."

[04.02.2026 04:13] Response: ```python
['CV', 'BENCHMARK']
```
[04.02.2026 04:13] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"FaceLinkGen attack demonstrates that current privacy-preserving face recognition methods fail to protect identity information despite pixel-level distortion metrics suggesting adequate protection.  					AI-generated summary 				 Transformation-based privacy-preserving face recognition (PPFR) aims to verify identities while hiding facial data from attackers and malicious service providers. Existing evaluations mostly treat privacy as resistance to pixel-level reconstruction, measured by PSNR and SSIM. We show that this reconstruction-centric view fails. We present FaceLinkGen, an identity extraction attack that performs linkage/matching and face regeneration directly from protected templates without recovering original pixels. On three recent PPFR systems, FaceLinkGen reaches over 98.5\% matching accuracy and above 96\% regeneration success, and still exceeds 92\% matching and 94\% regeneration in a near zero knowledge setting. These results expose a structural gap between pixel distortion metrics, which are widely used in PPFR evaluation, and real privacy. We show that visual obfuscation leaves identity information broadly exposed to both external intruders and untrusted service providers."

[04.02.2026 04:13] Response: ```python
['SECURITY', 'ETHICS']
```
[04.02.2026 04:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The FaceLinkGen attack reveals that current privacy-preserving face recognition (PPFR) methods do not adequately protect individual identities, despite appearing effective based on pixel-level distortion metrics like PSNR and SSIM. This paper introduces FaceLinkGen, an attack that can extract and regenerate identities from protected face templates without needing to reconstruct the original pixel data. The results demonstrate that FaceLinkGen achieves over 98.5% matching accuracy and more than 96% regeneration success across various PPFR systems. This highlights a significant disconnect between traditional pixel distortion evaluations and actual privacy protection, showing that visual obfuscation techniques still leave identity information vulnerable to unauthorized access.","title":"FaceLinkGen: Unmasking the Flaws in Privacy-Preserving Face Recognition"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='The FaceLinkGen attack reveals that current privacy-preserving face recognition (PPFR) methods do not adequately protect individual identities, despite appearing effective based on pixel-level distortion metrics like PSNR and SSIM. This paper introduces FaceLinkGen, an attack that can extract and regenerate identities from protected face templates without needing to reconstruct the original pixel data. The results demonstrate that FaceLinkGen achieves over 98.5% matching accuracy and more than 96% regeneration success across various PPFR systems. This highlights a significant disconnect between traditional pixel distortion evaluations and actual privacy protection, showing that visual obfuscation techniques still leave identity information vulnerable to unauthorized access.', title='FaceLinkGen: Unmasking the Flaws in Privacy-Preserving Face Recognition'))
[04.02.2026 04:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"FaceLinkGenPSNRSSIMFaceLinkGen/FaceLinkGen98.5%96%92%94%","title":""}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='FaceLinkGenPSNRSSIMFaceLinkGen/FaceLinkGen98.5%96%92%94%', title=''))
[04.02.2026 04:13] Querying the API.
[04.02.2026 04:13] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

WorldVQA is a benchmark for evaluating the visual world knowledge of multimodal large language models by separating visual knowledge retrieval from reasoning to measure memorized facts.  					AI-generated summary 				 We introduce WorldVQA, a benchmark designed to evaluate the atomic visual world knowledge of Multimodal Large Language Models (MLLMs). Unlike current evaluations, which often conflate visual knowledge retrieval with reasoning, WorldVQA decouples these capabilities to strictly measure "what the model memorizes." The benchmark assesses the atomic capability of grounding and naming visual entities across a stratified taxonomy, spanning from common head-class objects to long-tail rarities. We expect WorldVQA to serve as a rigorous test for visual factuality, thereby establishing a standard for assessing the encyclopedic breadth and hallucination rates of current and next-generation frontier models.
[04.02.2026 04:14] Response: ```json
{
  "desc": "  WorldVQA          .           ,      .                    .        ,         .",
  "emoji": "",
  "title": " ,       "
}
```
[04.02.2026 04:14] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"WorldVQA is a benchmark for evaluating the visual world knowledge of multimodal large language models by separating visual knowledge retrieval from reasoning to measure memorized facts.  					AI-generated summary 				 We introduce WorldVQA, a benchmark designed to evaluate the atomic visual world knowledge of Multimodal Large Language Models (MLLMs). Unlike current evaluations, which often conflate visual knowledge retrieval with reasoning, WorldVQA decouples these capabilities to strictly measure "what the model memorizes." The benchmark assesses the atomic capability of grounding and naming visual entities across a stratified taxonomy, spanning from common head-class objects to long-tail rarities. We expect WorldVQA to serve as a rigorous test for visual factuality, thereby establishing a standard for assessing the encyclopedic breadth and hallucination rates of current and next-generation frontier models."

[04.02.2026 04:14] Response: ```python
["BENCHMARK", "MULTIMODAL", "CV"]
```
[04.02.2026 04:14] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"WorldVQA is a benchmark for evaluating the visual world knowledge of multimodal large language models by separating visual knowledge retrieval from reasoning to measure memorized facts.  					AI-generated summary 				 We introduce WorldVQA, a benchmark designed to evaluate the atomic visual world knowledge of Multimodal Large Language Models (MLLMs). Unlike current evaluations, which often conflate visual knowledge retrieval with reasoning, WorldVQA decouples these capabilities to strictly measure "what the model memorizes." The benchmark assesses the atomic capability of grounding and naming visual entities across a stratified taxonomy, spanning from common head-class objects to long-tail rarities. We expect WorldVQA to serve as a rigorous test for visual factuality, thereby establishing a standard for assessing the encyclopedic breadth and hallucination rates of current and next-generation frontier models."

[04.02.2026 04:14] Response: ```python
['HALLUCINATIONS', 'INTERPRETABILITY']
```

**Reasoning:**
- **HALLUCINATIONS**: The paper explicitly mentions "hallucination rates" and aims to measure "visual factuality" by assessing what models actually memorize versus what they might fabricate.
- **INTERPRETABILITY**: The paper focuses on analyzing and understanding model behavior by decoupling visual knowledge retrieval from reasoning to measure specific capabilities ("what the model memorizes"), which relates to understanding how models work.
[04.02.2026 04:14] Error. Failed to parse JSON from LLM. ["HALLUCINATIONS", "INTERPRETABILITY"]


**Reasoning:**
- **HALLUCINATIONS**: The paper explicitly mentions "hallucination rates" and aims to measure "visual factuality" by assessing what models actually memorize versus what they might fabricate.
- **INTERPRETABILITY**: The paper focuses on analyzing and understanding model behavior by decoupling visual knowledge retrieval from reasoning to measure specific capabilities ("what the model memorizes"), which relates to understanding how models work.
[04.02.2026 04:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"WorldVQA is a new benchmark aimed at evaluating the visual knowledge of Multimodal Large Language Models (MLLMs). It distinguishes between the retrieval of visual knowledge and reasoning, allowing for a clearer assessment of what the model has memorized. The benchmark tests the model\'s ability to identify and name visual entities across a wide range of categories, from common objects to rare items. By doing so, WorldVQA aims to provide a standard for measuring the accuracy and comprehensiveness of visual knowledge in AI models.","title":"WorldVQA: Measuring Visual Knowledge in AI Models"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="WorldVQA is a new benchmark aimed at evaluating the visual knowledge of Multimodal Large Language Models (MLLMs). It distinguishes between the retrieval of visual knowledge and reasoning, allowing for a clearer assessment of what the model has memorized. The benchmark tests the model's ability to identify and name visual entities across a wide range of categories, from common objects to rare items. By doing so, WorldVQA aims to provide a standard for measuring the accuracy and comprehensiveness of visual knowledge in AI models.", title='WorldVQA: Measuring Visual Knowledge in AI Models'))
[04.02.2026 04:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"WorldVQAWorldVQA","title":"WorldVQA"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='WorldVQAWorldVQA', title='WorldVQA'))
[04.02.2026 04:14] Querying the API.
[04.02.2026 04:14] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Advanced AI models demonstrate capability in supporting expert-level mathematical discovery and scientific research through collaborative approaches involving proof verification and automated code execution.  					AI-generated summary 				 Recent advances in large language models (LLMs) have opened new avenues for accelerating scientific research. While models are increasingly capable of assisting with routine tasks, their ability to contribute to novel, expert-level mathematical discovery is less understood. We present a collection of case studies demonstrating how researchers have successfully collaborated with advanced AI models, specifically Google's Gemini-based models (in particular Gemini Deep Think and its advanced variants), to solve open problems, refute conjectures, and generate new proofs across diverse areas in theoretical computer science, as well as other areas such as economics, optimization, and physics. Based on these experiences, we extract common techniques for effective human-AI collaboration in theoretical research, such as iterative refinement, problem decomposition, and cross-disciplinary knowledge transfer. While the majority of our results stem from this interactive, conversational methodology, we also highlight specific instances that push beyond standard chat interfaces. These include deploying the model as a rigorous adversarial reviewer to detect subtle flaws in existing proofs, and embedding it within a "neuro-symbolic" loop that autonomously writes and executes code to verify complex derivations. Together, these examples highlight the potential of AI not just as a tool for automation, but as a versatile, genuine partner in the creative process of scientific discovery.
[04.02.2026 04:14] Response: ```json
{
  "desc": "          ,   Google Gemini,              .      - ,   ,       .       ,                      .             ,     .",
  "emoji": "",
  "title": "AI   :       "
}
```
[04.02.2026 04:14] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Advanced AI models demonstrate capability in supporting expert-level mathematical discovery and scientific research through collaborative approaches involving proof verification and automated code execution.  					AI-generated summary 				 Recent advances in large language models (LLMs) have opened new avenues for accelerating scientific research. While models are increasingly capable of assisting with routine tasks, their ability to contribute to novel, expert-level mathematical discovery is less understood. We present a collection of case studies demonstrating how researchers have successfully collaborated with advanced AI models, specifically Google's Gemini-based models (in particular Gemini Deep Think and its advanced variants), to solve open problems, refute conjectures, and generate new proofs across diverse areas in theoretical computer science, as well as other areas such as economics, optimization, and physics. Based on these experiences, we extract common techniques for effective human-AI collaboration in theoretical research, such as iterative refinement, problem decomposition, and cross-disciplinary knowledge transfer. While the majority of our results stem from this interactive, conversational methodology, we also highlight specific instances that push beyond standard chat interfaces. These include deploying the model as a rigorous adversarial reviewer to detect subtle flaws in existing proofs, and embedding it within a "neuro-symbolic" loop that autonomously writes and executes code to verify complex derivations. Together, these examples highlight the potential of AI not just as a tool for automation, but as a versatile, genuine partner in the creative process of scientific discovery."

[04.02.2026 04:14] Response: ```python
["MATH", "AGENTS"]
```

**Justification:**

- **MATH**: The paper is explicitly focused on mathematical discovery, proof generation, proof verification, and solving open problems across theoretical computer science and related fields. Mathematical theory and algorithms are central to the work.

- **AGENTS**: The paper describes autonomous systems where the AI model operates beyond standard chat interfaces, including instances where it "autonomously writes and executes code" within a "neuro-symbolic loop" and acts as an "adversarial reviewer." This demonstrates agent-based autonomous behavior and decision-making.
[04.02.2026 04:14] Error. Failed to parse JSON from LLM. ["MATH", "AGENTS"]


**Justification:**

- **MATH**: The paper is explicitly focused on mathematical discovery, proof generation, proof verification, and solving open problems across theoretical computer science and related fields. Mathematical theory and algorithms are central to the work.

- **AGENTS**: The paper describes autonomous systems where the AI model operates beyond standard chat interfaces, including instances where it "autonomously writes and executes code" within a "neuro-symbolic loop" and acts as an "adversarial reviewer." This demonstrates agent-based autonomous behavior and decision-making.
[04.02.2026 04:14] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Advanced AI models demonstrate capability in supporting expert-level mathematical discovery and scientific research through collaborative approaches involving proof verification and automated code execution.  					AI-generated summary 				 Recent advances in large language models (LLMs) have opened new avenues for accelerating scientific research. While models are increasingly capable of assisting with routine tasks, their ability to contribute to novel, expert-level mathematical discovery is less understood. We present a collection of case studies demonstrating how researchers have successfully collaborated with advanced AI models, specifically Google's Gemini-based models (in particular Gemini Deep Think and its advanced variants), to solve open problems, refute conjectures, and generate new proofs across diverse areas in theoretical computer science, as well as other areas such as economics, optimization, and physics. Based on these experiences, we extract common techniques for effective human-AI collaboration in theoretical research, such as iterative refinement, problem decomposition, and cross-disciplinary knowledge transfer. While the majority of our results stem from this interactive, conversational methodology, we also highlight specific instances that push beyond standard chat interfaces. These include deploying the model as a rigorous adversarial reviewer to detect subtle flaws in existing proofs, and embedding it within a "neuro-symbolic" loop that autonomously writes and executes code to verify complex derivations. Together, these examples highlight the potential of AI not just as a tool for automation, but as a versatile, genuine partner in the creative process of scientific discovery."

[04.02.2026 04:14] Response: ```python
['SCIENCE', 'REASONING', 'TRANSFER_LEARNING']
```
[04.02.2026 04:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper explores how advanced AI models, particularly Google\'s Gemini, can assist researchers in making significant mathematical discoveries and conducting scientific research. It presents case studies where AI has helped solve open problems and generate new proofs in fields like theoretical computer science, economics, and physics. The authors identify effective collaboration techniques between humans and AI, such as iterative refinement and problem decomposition. Additionally, they showcase innovative uses of AI, including its role as a rigorous reviewer and its ability to autonomously verify complex proofs through code execution.","title":"AI: A Collaborative Partner in Scientific Discovery"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="This paper explores how advanced AI models, particularly Google's Gemini, can assist researchers in making significant mathematical discoveries and conducting scientific research. It presents case studies where AI has helped solve open problems and generate new proofs in fields like theoretical computer science, economics, and physics. The authors identify effective collaboration techniques between humans and AI, such as iterative refinement and problem decomposition. Additionally, they showcase innovative uses of AI, including its role as a rigorous reviewer and its ability to autonomously verify complex proofs through code execution.", title='AI: A Collaborative Partner in Scientific Discovery'))
[04.02.2026 04:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"GeminiAI","title":"AI"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='GeminiAI', title='AI'))
[04.02.2026 04:14] Querying the API.
[04.02.2026 04:14] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

3DiMo enables view-agnostic human motion control in video generation by training a motion encoder alongside a pretrained video generator to distill driving frames into compact motion tokens that align with the generator's spatial priors.  					AI-generated summary 				 Existing methods for human motion control in video generation typically rely on either 2D poses or explicit 3D parametric models (e.g., SMPL) as control signals. However, 2D poses rigidly bind motion to the driving viewpoint, precluding novel-view synthesis. Explicit 3D models, though structurally informative, suffer from inherent inaccuracies (e.g., depth ambiguity and inaccurate dynamics) which, when used as a strong constraint, override the powerful intrinsic 3D awareness of large-scale video generators. In this work, we revisit motion control from a 3D-aware perspective, advocating for an implicit, view-agnostic motion representation that naturally aligns with the generator's spatial priors rather than depending on externally reconstructed constraints. We introduce 3DiMo, which jointly trains a motion encoder with a pretrained video generator to distill driving frames into compact, view-agnostic motion tokens, injected semantically via cross-attention. To foster 3D awareness, we train with view-rich supervision (i.e., single-view, multi-view, and moving-camera videos), forcing motion consistency across diverse viewpoints. Additionally, we use auxiliary geometric supervision that leverages SMPL only for early initialization and is annealed to zero, enabling the model to transition from external 3D guidance to learning genuine 3D spatial motion understanding from the data and the generator's priors. Experiments confirm that 3DiMo faithfully reproduces driving motions with flexible, text-driven camera control, significantly surpassing existing methods in both motion fidelity and visual quality.
[04.02.2026 04:14] Response: ```json
{
  "desc": "3DiMo          ,         .   2D    3D  ,         ,             .        ,       3D  ,         .  ,         ,         .",
  "emoji": "",
  "title": " 3D         "
}
```
[04.02.2026 04:14] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"3DiMo enables view-agnostic human motion control in video generation by training a motion encoder alongside a pretrained video generator to distill driving frames into compact motion tokens that align with the generator's spatial priors.  					AI-generated summary 				 Existing methods for human motion control in video generation typically rely on either 2D poses or explicit 3D parametric models (e.g., SMPL) as control signals. However, 2D poses rigidly bind motion to the driving viewpoint, precluding novel-view synthesis. Explicit 3D models, though structurally informative, suffer from inherent inaccuracies (e.g., depth ambiguity and inaccurate dynamics) which, when used as a strong constraint, override the powerful intrinsic 3D awareness of large-scale video generators. In this work, we revisit motion control from a 3D-aware perspective, advocating for an implicit, view-agnostic motion representation that naturally aligns with the generator's spatial priors rather than depending on externally reconstructed constraints. We introduce 3DiMo, which jointly trains a motion encoder with a pretrained video generator to distill driving frames into compact, view-agnostic motion tokens, injected semantically via cross-attention. To foster 3D awareness, we train with view-rich supervision (i.e., single-view, multi-view, and moving-camera videos), forcing motion consistency across diverse viewpoints. Additionally, we use auxiliary geometric supervision that leverages SMPL only for early initialization and is annealed to zero, enabling the model to transition from external 3D guidance to learning genuine 3D spatial motion understanding from the data and the generator's priors. Experiments confirm that 3DiMo faithfully reproduces driving motions with flexible, text-driven camera control, significantly surpassing existing methods in both motion fidelity and visual quality."

[04.02.2026 04:14] Response: ```python
['VIDEO', 'MULTIMODAL', '3D', 'ARCHITECTURE', 'TRAINING']
```
[04.02.2026 04:14] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"3DiMo enables view-agnostic human motion control in video generation by training a motion encoder alongside a pretrained video generator to distill driving frames into compact motion tokens that align with the generator's spatial priors.  					AI-generated summary 				 Existing methods for human motion control in video generation typically rely on either 2D poses or explicit 3D parametric models (e.g., SMPL) as control signals. However, 2D poses rigidly bind motion to the driving viewpoint, precluding novel-view synthesis. Explicit 3D models, though structurally informative, suffer from inherent inaccuracies (e.g., depth ambiguity and inaccurate dynamics) which, when used as a strong constraint, override the powerful intrinsic 3D awareness of large-scale video generators. In this work, we revisit motion control from a 3D-aware perspective, advocating for an implicit, view-agnostic motion representation that naturally aligns with the generator's spatial priors rather than depending on externally reconstructed constraints. We introduce 3DiMo, which jointly trains a motion encoder with a pretrained video generator to distill driving frames into compact, view-agnostic motion tokens, injected semantically via cross-attention. To foster 3D awareness, we train with view-rich supervision (i.e., single-view, multi-view, and moving-camera videos), forcing motion consistency across diverse viewpoints. Additionally, we use auxiliary geometric supervision that leverages SMPL only for early initialization and is annealed to zero, enabling the model to transition from external 3D guidance to learning genuine 3D spatial motion understanding from the data and the generator's priors. Experiments confirm that 3DiMo faithfully reproduces driving motions with flexible, text-driven camera control, significantly surpassing existing methods in both motion fidelity and visual quality."

[04.02.2026 04:14] Response: ```python
["OPTIMIZATION"]
```
[04.02.2026 04:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"3DiMo is a novel approach for controlling human motion in video generation that does not depend on specific viewpoints. It trains a motion encoder alongside a video generator to create compact motion tokens that align with the generator\'s understanding of space. This method avoids the limitations of 2D poses and explicit 3D models by using a view-agnostic representation, allowing for more flexible and accurate motion synthesis. The model is trained with diverse video inputs to ensure consistent motion across different perspectives, leading to improved motion fidelity and visual quality in generated videos.","title":"View-Agnostic Motion Control for Enhanced Video Generation"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="3DiMo is a novel approach for controlling human motion in video generation that does not depend on specific viewpoints. It trains a motion encoder alongside a video generator to create compact motion tokens that align with the generator's understanding of space. This method avoids the limitations of 2D poses and explicit 3D models by using a view-agnostic representation, allowing for more flexible and accurate motion synthesis. The model is trained with diverse video inputs to ensure consistent motion across different perspectives, leading to improved motion fidelity and visual quality in generated videos.", title='View-Agnostic Motion Control for Enhanced Video Generation'))
[04.02.2026 04:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"3DiMo3DiMo2D3D3DiMo","title":"3DiMo"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='3DiMo3DiMo2D3D3DiMo', title='3DiMo'))
[04.02.2026 04:14] Querying the API.
[04.02.2026 04:14] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

ObjEmbed is a novel multimodal language-model embedding approach that decomposes images into regional embeddings for improved object-level visual understanding and retrieval tasks.  					AI-generated summary 				 Aligning objects with corresponding textual descriptions is a fundamental challenge and a realistic requirement in vision-language understanding. While recent multimodal embedding models excel at global image-text alignment, they often struggle with fine-grained alignment between image regions and specific phrases. In this work, we present ObjEmbed, a novel MLLM embedding model that decomposes the input image into multiple regional embeddings, each corresponding to an individual object, along with global embeddings. It supports a wide range of visual understanding tasks like visual grounding, local image retrieval, and global image retrieval. ObjEmbed enjoys three key properties: (1) Object-Oriented Representation: It captures both semantic and spatial aspects of objects by generating two complementary embeddings for each region: an object embedding for semantic matching and an IoU embedding that predicts localization quality. The final object matching score combines semantic similarity with the predicted IoU, enabling more accurate retrieval. (2) Versatility: It seamlessly handles both region-level and image-level tasks. (3) Efficient Encoding: All objects in an image, along with the full image, are encoded in a single forward pass for high efficiency. Superior performance on 18 diverse benchmarks demonstrates its strong semantic discrimination.
[04.02.2026 04:14] Response: ```json
{
  "desc": "ObjEmbed           ,             .        :       IoU-    .       ,  visual grounding,     ,      forward pass   .    18       .",
  "emoji": "",
  "title": "      "
}
```
[04.02.2026 04:14] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"ObjEmbed is a novel multimodal language-model embedding approach that decomposes images into regional embeddings for improved object-level visual understanding and retrieval tasks.  					AI-generated summary 				 Aligning objects with corresponding textual descriptions is a fundamental challenge and a realistic requirement in vision-language understanding. While recent multimodal embedding models excel at global image-text alignment, they often struggle with fine-grained alignment between image regions and specific phrases. In this work, we present ObjEmbed, a novel MLLM embedding model that decomposes the input image into multiple regional embeddings, each corresponding to an individual object, along with global embeddings. It supports a wide range of visual understanding tasks like visual grounding, local image retrieval, and global image retrieval. ObjEmbed enjoys three key properties: (1) Object-Oriented Representation: It captures both semantic and spatial aspects of objects by generating two complementary embeddings for each region: an object embedding for semantic matching and an IoU embedding that predicts localization quality. The final object matching score combines semantic similarity with the predicted IoU, enabling more accurate retrieval. (2) Versatility: It seamlessly handles both region-level and image-level tasks. (3) Efficient Encoding: All objects in an image, along with the full image, are encoded in a single forward pass for high efficiency. Superior performance on 18 diverse benchmarks demonstrates its strong semantic discrimination."

[04.02.2026 04:14] Response: ```python
["MULTIMODAL", "CV", "BENCHMARK"]
```
[04.02.2026 04:14] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"ObjEmbed is a novel multimodal language-model embedding approach that decomposes images into regional embeddings for improved object-level visual understanding and retrieval tasks.  					AI-generated summary 				 Aligning objects with corresponding textual descriptions is a fundamental challenge and a realistic requirement in vision-language understanding. While recent multimodal embedding models excel at global image-text alignment, they often struggle with fine-grained alignment between image regions and specific phrases. In this work, we present ObjEmbed, a novel MLLM embedding model that decomposes the input image into multiple regional embeddings, each corresponding to an individual object, along with global embeddings. It supports a wide range of visual understanding tasks like visual grounding, local image retrieval, and global image retrieval. ObjEmbed enjoys three key properties: (1) Object-Oriented Representation: It captures both semantic and spatial aspects of objects by generating two complementary embeddings for each region: an object embedding for semantic matching and an IoU embedding that predicts localization quality. The final object matching score combines semantic similarity with the predicted IoU, enabling more accurate retrieval. (2) Versatility: It seamlessly handles both region-level and image-level tasks. (3) Efficient Encoding: All objects in an image, along with the full image, are encoded in a single forward pass for high efficiency. Superior performance on 18 diverse benchmarks demonstrates its strong semantic discrimination."

[04.02.2026 04:14] Response: ```python
['INTERPRETABILITY']
```
[04.02.2026 04:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"ObjEmbed is a new approach in multimodal language modeling that enhances the understanding of images by breaking them down into regional embeddings. This method addresses the challenge of aligning specific image regions with their corresponding textual descriptions, which is crucial for tasks like visual grounding and image retrieval. By generating both object embeddings for semantic matching and IoU embeddings for localization quality, ObjEmbed improves the accuracy of object retrieval. Its efficient encoding allows for simultaneous processing of all objects and the full image, leading to superior performance across various benchmarks.","title":"Enhancing Visual Understanding with Regional Embeddings"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='ObjEmbed is a new approach in multimodal language modeling that enhances the understanding of images by breaking them down into regional embeddings. This method addresses the challenge of aligning specific image regions with their corresponding textual descriptions, which is crucial for tasks like visual grounding and image retrieval. By generating both object embeddings for semantic matching and IoU embeddings for localization quality, ObjEmbed improves the accuracy of object retrieval. Its efficient encoding allows for simultaneous processing of all objects and the full image, leading to superior performance across various benchmarks.', title='Enhancing Visual Understanding with Regional Embeddings'))
[04.02.2026 04:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"ObjEmbedObjEmbed18","title":"ObjEmbed"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='ObjEmbedObjEmbed18', title='ObjEmbed'))
[04.02.2026 04:14] Querying the API.
[04.02.2026 04:14] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A fusion framework called FINCH combines audio and spatiotemporal predictors for bioacoustic classification by adaptively weighting evidence based on reliability estimates, outperforming fixed-weight methods and audio-only approaches.  					AI-generated summary 				 Many machine learning systems have access to multiple sources of evidence for the same prediction target, yet these sources often differ in reliability and informativeness across inputs. In bioacoustic classification, species identity may be inferred both from the acoustic signal and from spatiotemporal context such as location and season; while Bayesian inference motivates multiplicative evidence combination, in practice we typically only have access to discriminative predictors rather than calibrated generative models. We introduce Fusion under INdependent Conditional Hypotheses (FINCH), an adaptive log-linear evidence fusion framework that integrates a pre-trained audio classifier with a structured spatiotemporal predictor. FINCH learns a per-sample gating function that estimates the reliability of contextual information from uncertainty and informativeness statistics. The resulting fusion family contains the audio-only classifier as a special case and explicitly bounds the influence of contextual evidence, yielding a risk-contained hypothesis class with an interpretable audio-only fallback. Across benchmarks, FINCH consistently outperforms fixed-weight fusion and audio-only baselines, improving robustness and error trade-offs even when contextual information is weak in isolation. We achieve state-of-the-art performance on CBI and competitive or improved performance on several subsets of BirdSet using a lightweight, interpretable, evidence-based approach. Code is available: \href{https://anonymous.4open.science/r/birdnoise-85CD/README.md{anonymous-repository}}
[04.02.2026 04:14] Response: ```json
{
  "desc": "  FINCH         ,      - .       ,              .   log-          ,       .   ,  FINCH          ,      CBI  BirdSet.",
  "emoji": "",
  "title": "       "
}
```
[04.02.2026 04:14] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A fusion framework called FINCH combines audio and spatiotemporal predictors for bioacoustic classification by adaptively weighting evidence based on reliability estimates, outperforming fixed-weight methods and audio-only approaches.  					AI-generated summary 				 Many machine learning systems have access to multiple sources of evidence for the same prediction target, yet these sources often differ in reliability and informativeness across inputs. In bioacoustic classification, species identity may be inferred both from the acoustic signal and from spatiotemporal context such as location and season; while Bayesian inference motivates multiplicative evidence combination, in practice we typically only have access to discriminative predictors rather than calibrated generative models. We introduce Fusion under INdependent Conditional Hypotheses (FINCH), an adaptive log-linear evidence fusion framework that integrates a pre-trained audio classifier with a structured spatiotemporal predictor. FINCH learns a per-sample gating function that estimates the reliability of contextual information from uncertainty and informativeness statistics. The resulting fusion family contains the audio-only classifier as a special case and explicitly bounds the influence of contextual evidence, yielding a risk-contained hypothesis class with an interpretable audio-only fallback. Across benchmarks, FINCH consistently outperforms fixed-weight fusion and audio-only baselines, improving robustness and error trade-offs even when contextual information is weak in isolation. We achieve state-of-the-art performance on CBI and competitive or improved performance on several subsets of BirdSet using a lightweight, interpretable, evidence-based approach. Code is available: \href{https://anonymous.4open.science/r/birdnoise-85CD/README.md{anonymous-repository}}"

[04.02.2026 04:14] Response: ```python
["AUDIO", "MULTIMODAL", "BENCHMARK"]
```
[04.02.2026 04:14] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A fusion framework called FINCH combines audio and spatiotemporal predictors for bioacoustic classification by adaptively weighting evidence based on reliability estimates, outperforming fixed-weight methods and audio-only approaches.  					AI-generated summary 				 Many machine learning systems have access to multiple sources of evidence for the same prediction target, yet these sources often differ in reliability and informativeness across inputs. In bioacoustic classification, species identity may be inferred both from the acoustic signal and from spatiotemporal context such as location and season; while Bayesian inference motivates multiplicative evidence combination, in practice we typically only have access to discriminative predictors rather than calibrated generative models. We introduce Fusion under INdependent Conditional Hypotheses (FINCH), an adaptive log-linear evidence fusion framework that integrates a pre-trained audio classifier with a structured spatiotemporal predictor. FINCH learns a per-sample gating function that estimates the reliability of contextual information from uncertainty and informativeness statistics. The resulting fusion family contains the audio-only classifier as a special case and explicitly bounds the influence of contextual evidence, yielding a risk-contained hypothesis class with an interpretable audio-only fallback. Across benchmarks, FINCH consistently outperforms fixed-weight fusion and audio-only baselines, improving robustness and error trade-offs even when contextual information is weak in isolation. We achieve state-of-the-art performance on CBI and competitive or improved performance on several subsets of BirdSet using a lightweight, interpretable, evidence-based approach. Code is available: \href{https://anonymous.4open.science/r/birdnoise-85CD/README.md{anonymous-repository}}"

[04.02.2026 04:14] Response: ```python
['INTERPRETABILITY']
```

The paper is classified as INTERPRETABILITY because it explicitly emphasizes interpretability as a key feature of the proposed FINCH framework. The text states that the method yields "an interpretable audio-only fallback" and describes it as "a lightweight, interpretable, evidence-based approach." The framework is designed to be interpretable by learning per-sample gating functions that estimate reliability and explicitly bound the influence of contextual evidence.
[04.02.2026 04:14] Error. Failed to parse JSON from LLM. ["INTERPRETABILITY"]


The paper is classified as INTERPRETABILITY because it explicitly emphasizes interpretability as a key feature of the proposed FINCH framework. The text states that the method yields "an interpretable audio-only fallback" and describes it as "a lightweight, interpretable, evidence-based approach." The framework is designed to be interpretable by learning per-sample gating functions that estimate reliability and explicitly bound the influence of contextual evidence.
[04.02.2026 04:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The paper presents FINCH, a novel framework for bioacoustic classification that combines audio data with spatiotemporal predictors. It adaptively weighs the evidence from these sources based on their reliability, improving upon traditional fixed-weight methods. By using a gating function that assesses the uncertainty and informativeness of contextual information, FINCH enhances the robustness of predictions. The framework not only outperforms audio-only classifiers but also provides a clear fallback option, making it interpretable and effective even when contextual data is weak.","title":"Adaptive Fusion for Enhanced Bioacoustic Classification"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='The paper presents FINCH, a novel framework for bioacoustic classification that combines audio data with spatiotemporal predictors. It adaptively weighs the evidence from these sources based on their reliability, improving upon traditional fixed-weight methods. By using a gating function that assesses the uncertainty and informativeness of contextual information, FINCH enhances the robustness of predictions. The framework not only outperforms audio-only classifiers but also provides a clear fallback option, making it interpretable and effective even when contextual data is weak.', title='Adaptive Fusion for Enhanced Bioacoustic Classification'))
[04.02.2026 04:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"FINCHFINCHFINCH","title":""}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='FINCHFINCHFINCH', title=''))
[04.02.2026 04:14] Querying the API.
[04.02.2026 04:14] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Search-R2 framework improves language agent reasoning through Actor-Refiner collaboration with targeted interventions and fine-grained reward supervision for better credit assignment in reinforcement learning.  					AI-generated summary 				 Search-integrated reasoning enables language agents to transcend static parametric knowledge by actively querying external sources. However, training these agents via reinforcement learning is hindered by the multi-scale credit assignment problem: existing methods typically rely on sparse, trajectory-level rewards that fail to distinguish between high-quality reasoning and fortuitous guesses, leading to redundant or misleading search behaviors. To address this, we propose Search-R2, a novel Actor-Refiner collaboration framework that enhances reasoning through targeted intervention, with both components jointly optimized during training. Our approach decomposes the generation process into an Actor, which produces initial reasoning trajectories, and a Meta-Refiner, which selectively diagnoses and repairs flawed steps via a 'cut-and-regenerate' mechanism. To provide fine-grained supervision, we introduce a hybrid reward design that couples outcome correctness with a dense process reward quantifying the information density of retrieved evidence. Theoretically, we formalize the Actor-Refiner interaction as a smoothed mixture policy, proving that selective correction yields strict performance gains over strong baselines. Extensive experiments across various general and multi-hop QA datasets demonstrate that Search-R2 consistently outperforms strong RAG and RL-based baselines across model scales, achieving superior reasoning accuracy with minimal overhead.
[04.02.2026 04:14] Response: ```json
{
  "desc": "    Search-R2,        Actor  Refiner    .            ,             .     ,         ,     .   ,  Search-R2       -      .",
  "emoji": "",
  "title": "         "
}
```
[04.02.2026 04:14] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Search-R2 framework improves language agent reasoning through Actor-Refiner collaboration with targeted interventions and fine-grained reward supervision for better credit assignment in reinforcement learning.  					AI-generated summary 				 Search-integrated reasoning enables language agents to transcend static parametric knowledge by actively querying external sources. However, training these agents via reinforcement learning is hindered by the multi-scale credit assignment problem: existing methods typically rely on sparse, trajectory-level rewards that fail to distinguish between high-quality reasoning and fortuitous guesses, leading to redundant or misleading search behaviors. To address this, we propose Search-R2, a novel Actor-Refiner collaboration framework that enhances reasoning through targeted intervention, with both components jointly optimized during training. Our approach decomposes the generation process into an Actor, which produces initial reasoning trajectories, and a Meta-Refiner, which selectively diagnoses and repairs flawed steps via a 'cut-and-regenerate' mechanism. To provide fine-grained supervision, we introduce a hybrid reward design that couples outcome correctness with a dense process reward quantifying the information density of retrieved evidence. Theoretically, we formalize the Actor-Refiner interaction as a smoothed mixture policy, proving that selective correction yields strict performance gains over strong baselines. Extensive experiments across various general and multi-hop QA datasets demonstrate that Search-R2 consistently outperforms strong RAG and RL-based baselines across model scales, achieving superior reasoning accuracy with minimal overhead."

[04.02.2026 04:14] Response: ```python
["AGENTS", "RL", "RAG", "TRAINING"]
```
[04.02.2026 04:14] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Search-R2 framework improves language agent reasoning through Actor-Refiner collaboration with targeted interventions and fine-grained reward supervision for better credit assignment in reinforcement learning.  					AI-generated summary 				 Search-integrated reasoning enables language agents to transcend static parametric knowledge by actively querying external sources. However, training these agents via reinforcement learning is hindered by the multi-scale credit assignment problem: existing methods typically rely on sparse, trajectory-level rewards that fail to distinguish between high-quality reasoning and fortuitous guesses, leading to redundant or misleading search behaviors. To address this, we propose Search-R2, a novel Actor-Refiner collaboration framework that enhances reasoning through targeted intervention, with both components jointly optimized during training. Our approach decomposes the generation process into an Actor, which produces initial reasoning trajectories, and a Meta-Refiner, which selectively diagnoses and repairs flawed steps via a 'cut-and-regenerate' mechanism. To provide fine-grained supervision, we introduce a hybrid reward design that couples outcome correctness with a dense process reward quantifying the information density of retrieved evidence. Theoretically, we formalize the Actor-Refiner interaction as a smoothed mixture policy, proving that selective correction yields strict performance gains over strong baselines. Extensive experiments across various general and multi-hop QA datasets demonstrate that Search-R2 consistently outperforms strong RAG and RL-based baselines across model scales, achieving superior reasoning accuracy with minimal overhead."

[04.02.2026 04:14] Response: ```python
['REASONING', 'OPTIMIZATION']
```
[04.02.2026 04:15] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The Search-R2 framework enhances the reasoning capabilities of language agents by utilizing a collaborative approach between an Actor and a Meta-Refiner. This method addresses the multi-scale credit assignment problem in reinforcement learning by providing targeted interventions and fine-grained reward supervision. The Actor generates initial reasoning paths, while the Meta-Refiner corrects errors through a \'cut-and-regenerate\' process, improving the overall reasoning quality. Experimental results show that Search-R2 outperforms existing models in reasoning accuracy across various question-answering datasets, demonstrating its effectiveness in optimizing agent performance.","title":"Enhancing Language Agent Reasoning with Search-R2 Framework"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="The Search-R2 framework enhances the reasoning capabilities of language agents by utilizing a collaborative approach between an Actor and a Meta-Refiner. This method addresses the multi-scale credit assignment problem in reinforcement learning by providing targeted interventions and fine-grained reward supervision. The Actor generates initial reasoning paths, while the Meta-Refiner corrects errors through a 'cut-and-regenerate' process, improving the overall reasoning quality. Experimental results show that Search-R2 outperforms existing models in reasoning accuracy across various question-answering datasets, demonstrating its effectiveness in optimizing agent performance.", title='Enhancing Language Agent Reasoning with Search-R2 Framework'))
[04.02.2026 04:15] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Search-R2--Search-R2","title":"-"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Search-R2--Search-R2', title='-'))
[04.02.2026 04:15] Querying the API.
[04.02.2026 04:15] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

MARS is a modular AI research automation framework that uses budget-aware planning, modular construction, and reflective memory to achieve state-of-the-art performance in autonomous machine learning research.  					AI-generated summary 				 Automating AI research differs from general software engineering due to computationally expensive evaluation (e.g., model training) and opaque performance attribution. Current LLM-based agents struggle here, often generating monolithic scripts that ignore execution costs and causal factors. We introduce MARS (Modular Agent with Reflective Search), a framework optimized for autonomous AI research. MARS relies on three pillars: (1) Budget-Aware Planning via cost-constrained Monte Carlo Tree Search (MCTS) to explicitly balance performance with execution expense; (2) Modular Construction, employing a "Design-Decompose-Implement" pipeline to manage complex research repositories; and (3) Comparative Reflective Memory, which addresses credit assignment by analyzing solution differences to distill high-signal insights. MARS achieves state-of-the-art performance among open-source frameworks on MLE-Bench under comparable settings, maintaining competitiveness with the global leaderboard's top methods. Furthermore, the system exhibits qualitative "Aha!" moments, where 63% of all utilized lessons originate from cross-branch transfer, demonstrating that the agent effectively generalizes insights across search paths.
[04.02.2026 04:15] Response: ```json
{
  "desc": "MARS           ,     :          -,     --,        .         ML-,            . MARS         MLE-Bench         .  ,  63%          ,        .",
  "emoji": "",
  "title": "        ML-"
}
```
[04.02.2026 04:15] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"MARS is a modular AI research automation framework that uses budget-aware planning, modular construction, and reflective memory to achieve state-of-the-art performance in autonomous machine learning research.  					AI-generated summary 				 Automating AI research differs from general software engineering due to computationally expensive evaluation (e.g., model training) and opaque performance attribution. Current LLM-based agents struggle here, often generating monolithic scripts that ignore execution costs and causal factors. We introduce MARS (Modular Agent with Reflective Search), a framework optimized for autonomous AI research. MARS relies on three pillars: (1) Budget-Aware Planning via cost-constrained Monte Carlo Tree Search (MCTS) to explicitly balance performance with execution expense; (2) Modular Construction, employing a "Design-Decompose-Implement" pipeline to manage complex research repositories; and (3) Comparative Reflective Memory, which addresses credit assignment by analyzing solution differences to distill high-signal insights. MARS achieves state-of-the-art performance among open-source frameworks on MLE-Bench under comparable settings, maintaining competitiveness with the global leaderboard's top methods. Furthermore, the system exhibits qualitative "Aha!" moments, where 63% of all utilized lessons originate from cross-branch transfer, demonstrating that the agent effectively generalizes insights across search paths."

[04.02.2026 04:15] Response: ```python
["AGENTS", "BENCHMARK", "TRAINING"]
```
[04.02.2026 04:15] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"MARS is a modular AI research automation framework that uses budget-aware planning, modular construction, and reflective memory to achieve state-of-the-art performance in autonomous machine learning research.  					AI-generated summary 				 Automating AI research differs from general software engineering due to computationally expensive evaluation (e.g., model training) and opaque performance attribution. Current LLM-based agents struggle here, often generating monolithic scripts that ignore execution costs and causal factors. We introduce MARS (Modular Agent with Reflective Search), a framework optimized for autonomous AI research. MARS relies on three pillars: (1) Budget-Aware Planning via cost-constrained Monte Carlo Tree Search (MCTS) to explicitly balance performance with execution expense; (2) Modular Construction, employing a "Design-Decompose-Implement" pipeline to manage complex research repositories; and (3) Comparative Reflective Memory, which addresses credit assignment by analyzing solution differences to distill high-signal insights. MARS achieves state-of-the-art performance among open-source frameworks on MLE-Bench under comparable settings, maintaining competitiveness with the global leaderboard's top methods. Furthermore, the system exhibits qualitative "Aha!" moments, where 63% of all utilized lessons originate from cross-branch transfer, demonstrating that the agent effectively generalizes insights across search paths."

[04.02.2026 04:15] Response: ```python
['SCIENCE', 'OPEN_SOURCE']
```
[04.02.2026 04:15] Response: ParsedChatCompletionMessage[Article](content='{"desc":"MARS is a framework designed to automate AI research by optimizing the planning and execution of machine learning tasks. It uses budget-aware planning to balance the performance of models with the costs of training them, ensuring efficient resource use. The framework is modular, allowing researchers to break down complex tasks into manageable parts, and it incorporates reflective memory to improve learning from past experiences. MARS has shown to outperform other open-source frameworks in benchmarks, highlighting its effectiveness in generating valuable insights through cross-branch learning.","title":"MARS: Optimizing AI Research with Smart Planning and Modular Design"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='MARS is a framework designed to automate AI research by optimizing the planning and execution of machine learning tasks. It uses budget-aware planning to balance the performance of models with the costs of training them, ensuring efficient resource use. The framework is modular, allowing researchers to break down complex tasks into manageable parts, and it incorporates reflective memory to improve learning from past experiences. MARS has shown to outperform other open-source frameworks in benchmarks, highlighting its effectiveness in generating valuable insights through cross-branch learning.', title='MARS: Optimizing AI Research with Smart Planning and Modular Design'))
[04.02.2026 04:15] Response: ParsedChatCompletionMessage[Article](content='{"desc":"MARSMCTSMARS--MLE-Bench63%","title":"MARS"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='MARSMCTSMARS--MLE-Bench63%', title='MARS'))
[04.02.2026 04:15] Renaming data file.
[04.02.2026 04:15] Renaming previous data. hf_papers.json to ./d/2026-02-04.json
[04.02.2026 04:15] Saving new data file.
[04.02.2026 04:15] Generating page.
[04.02.2026 04:15] Renaming previous page.
[04.02.2026 04:15] Renaming previous data. index.html to ./d/2026-02-04.html
[04.02.2026 04:15] Writing result.
[04.02.2026 04:15] Renaming log file.
[04.02.2026 04:15] Renaming previous data. log.txt to ./logs/2026-02-04_last_log.txt
