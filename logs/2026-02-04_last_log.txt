[04.02.2026 06:42] Read previous papers.
[04.02.2026 06:42] Generating top page (month).
[04.02.2026 06:42] Writing top page (month).
[04.02.2026 07:40] Read previous papers.
[04.02.2026 07:40] Get feed.
[04.02.2026 07:40] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01785
[04.02.2026 07:40] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02619
[04.02.2026 07:40] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01630
[04.02.2026 07:40] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02660
[04.02.2026 07:40] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03796
[04.02.2026 07:40] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02103
[04.02.2026 07:40] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03419
[04.02.2026 07:40] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03619
[04.02.2026 07:40] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03411
[04.02.2026 07:40] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03139
[04.02.2026 07:40] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03786
[04.02.2026 07:40] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03845
[04.02.2026 07:40] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03048
[04.02.2026 07:40] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02380
[04.02.2026 07:40] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02636
[04.02.2026 07:40] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21244
[04.02.2026 07:40] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02676
[04.02.2026 07:40] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01362
[04.02.2026 07:40] Get page data from previous paper. URL: https://huggingface.co/papers/2602.00747
[04.02.2026 07:40] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03798
[04.02.2026 07:40] Extract page data from URL. URL: https://huggingface.co/papers/2602.03216
[04.02.2026 07:40] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02537
[04.02.2026 07:40] Get page data from previous paper. URL: https://huggingface.co/papers/2601.19103
[04.02.2026 07:40] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02419
[04.02.2026 07:40] Extract page data from URL. URL: https://huggingface.co/papers/2602.01053
[04.02.2026 07:40] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03837
[04.02.2026 07:40] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03806
[04.02.2026 07:40] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03454
[04.02.2026 07:40] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02914
[04.02.2026 07:40] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03647
[04.02.2026 07:40] Extract page data from URL. URL: https://huggingface.co/papers/2602.03238
[04.02.2026 07:40] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01753
[04.02.2026 07:40] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03817
[04.02.2026 07:40] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[04.02.2026 07:40] No deleted papers detected.
[04.02.2026 07:40] Downloading and parsing papers (pdf, html). Total: 33.
[04.02.2026 07:40] Downloading and parsing paper https://huggingface.co/papers/2602.01785.
[04.02.2026 07:40] Extra JSON file exists (./assets/json/2602.01785.json), skip PDF parsing.
[04.02.2026 07:40] Paper image links file exists (./assets/img_data/2602.01785.json), skip HTML parsing.
[04.02.2026 07:40] Success.
[04.02.2026 07:40] Downloading and parsing paper https://huggingface.co/papers/2602.02619.
[04.02.2026 07:40] Extra JSON file exists (./assets/json/2602.02619.json), skip PDF parsing.
[04.02.2026 07:40] Paper image links file exists (./assets/img_data/2602.02619.json), skip HTML parsing.
[04.02.2026 07:40] Success.
[04.02.2026 07:40] Downloading and parsing paper https://huggingface.co/papers/2602.01630.
[04.02.2026 07:40] Extra JSON file exists (./assets/json/2602.01630.json), skip PDF parsing.
[04.02.2026 07:40] Paper image links file exists (./assets/img_data/2602.01630.json), skip HTML parsing.
[04.02.2026 07:40] Success.
[04.02.2026 07:40] Downloading and parsing paper https://huggingface.co/papers/2602.02660.
[04.02.2026 07:40] Extra JSON file exists (./assets/json/2602.02660.json), skip PDF parsing.
[04.02.2026 07:40] Paper image links file exists (./assets/img_data/2602.02660.json), skip HTML parsing.
[04.02.2026 07:40] Success.
[04.02.2026 07:40] Downloading and parsing paper https://huggingface.co/papers/2602.03796.
[04.02.2026 07:40] Extra JSON file exists (./assets/json/2602.03796.json), skip PDF parsing.
[04.02.2026 07:40] Paper image links file exists (./assets/img_data/2602.03796.json), skip HTML parsing.
[04.02.2026 07:40] Success.
[04.02.2026 07:40] Downloading and parsing paper https://huggingface.co/papers/2602.02103.
[04.02.2026 07:40] Extra JSON file exists (./assets/json/2602.02103.json), skip PDF parsing.
[04.02.2026 07:40] Paper image links file exists (./assets/img_data/2602.02103.json), skip HTML parsing.
[04.02.2026 07:40] Success.
[04.02.2026 07:40] Downloading and parsing paper https://huggingface.co/papers/2602.03419.
[04.02.2026 07:40] Extra JSON file exists (./assets/json/2602.03419.json), skip PDF parsing.
[04.02.2026 07:40] Paper image links file exists (./assets/img_data/2602.03419.json), skip HTML parsing.
[04.02.2026 07:40] Success.
[04.02.2026 07:40] Downloading and parsing paper https://huggingface.co/papers/2602.03619.
[04.02.2026 07:40] Extra JSON file exists (./assets/json/2602.03619.json), skip PDF parsing.
[04.02.2026 07:40] Paper image links file exists (./assets/img_data/2602.03619.json), skip HTML parsing.
[04.02.2026 07:40] Success.
[04.02.2026 07:40] Downloading and parsing paper https://huggingface.co/papers/2602.03411.
[04.02.2026 07:40] Extra JSON file exists (./assets/json/2602.03411.json), skip PDF parsing.
[04.02.2026 07:40] Paper image links file exists (./assets/img_data/2602.03411.json), skip HTML parsing.
[04.02.2026 07:40] Success.
[04.02.2026 07:40] Downloading and parsing paper https://huggingface.co/papers/2602.03139.
[04.02.2026 07:40] Extra JSON file exists (./assets/json/2602.03139.json), skip PDF parsing.
[04.02.2026 07:40] Paper image links file exists (./assets/img_data/2602.03139.json), skip HTML parsing.
[04.02.2026 07:40] Success.
[04.02.2026 07:40] Downloading and parsing paper https://huggingface.co/papers/2602.03786.
[04.02.2026 07:40] Extra JSON file exists (./assets/json/2602.03786.json), skip PDF parsing.
[04.02.2026 07:40] Paper image links file exists (./assets/img_data/2602.03786.json), skip HTML parsing.
[04.02.2026 07:40] Success.
[04.02.2026 07:40] Downloading and parsing paper https://huggingface.co/papers/2602.03845.
[04.02.2026 07:40] Extra JSON file exists (./assets/json/2602.03845.json), skip PDF parsing.
[04.02.2026 07:40] Paper image links file exists (./assets/img_data/2602.03845.json), skip HTML parsing.
[04.02.2026 07:40] Success.
[04.02.2026 07:40] Downloading and parsing paper https://huggingface.co/papers/2602.03048.
[04.02.2026 07:40] Extra JSON file exists (./assets/json/2602.03048.json), skip PDF parsing.
[04.02.2026 07:40] Paper image links file exists (./assets/img_data/2602.03048.json), skip HTML parsing.
[04.02.2026 07:40] Success.
[04.02.2026 07:40] Downloading and parsing paper https://huggingface.co/papers/2602.02380.
[04.02.2026 07:40] Extra JSON file exists (./assets/json/2602.02380.json), skip PDF parsing.
[04.02.2026 07:40] Paper image links file exists (./assets/img_data/2602.02380.json), skip HTML parsing.
[04.02.2026 07:40] Success.
[04.02.2026 07:40] Downloading and parsing paper https://huggingface.co/papers/2602.02636.
[04.02.2026 07:40] Extra JSON file exists (./assets/json/2602.02636.json), skip PDF parsing.
[04.02.2026 07:40] Paper image links file exists (./assets/img_data/2602.02636.json), skip HTML parsing.
[04.02.2026 07:40] Success.
[04.02.2026 07:40] Downloading and parsing paper https://huggingface.co/papers/2601.21244.
[04.02.2026 07:40] Extra JSON file exists (./assets/json/2601.21244.json), skip PDF parsing.
[04.02.2026 07:40] Paper image links file exists (./assets/img_data/2601.21244.json), skip HTML parsing.
[04.02.2026 07:40] Success.
[04.02.2026 07:40] Downloading and parsing paper https://huggingface.co/papers/2602.02676.
[04.02.2026 07:40] Extra JSON file exists (./assets/json/2602.02676.json), skip PDF parsing.
[04.02.2026 07:40] Paper image links file exists (./assets/img_data/2602.02676.json), skip HTML parsing.
[04.02.2026 07:40] Success.
[04.02.2026 07:40] Downloading and parsing paper https://huggingface.co/papers/2602.01362.
[04.02.2026 07:40] Extra JSON file exists (./assets/json/2602.01362.json), skip PDF parsing.
[04.02.2026 07:40] Paper image links file exists (./assets/img_data/2602.01362.json), skip HTML parsing.
[04.02.2026 07:40] Success.
[04.02.2026 07:40] Downloading and parsing paper https://huggingface.co/papers/2602.00747.
[04.02.2026 07:40] Extra JSON file exists (./assets/json/2602.00747.json), skip PDF parsing.
[04.02.2026 07:40] Paper image links file exists (./assets/img_data/2602.00747.json), skip HTML parsing.
[04.02.2026 07:40] Success.
[04.02.2026 07:40] Downloading and parsing paper https://huggingface.co/papers/2602.03798.
[04.02.2026 07:40] Extra JSON file exists (./assets/json/2602.03798.json), skip PDF parsing.
[04.02.2026 07:40] Paper image links file exists (./assets/img_data/2602.03798.json), skip HTML parsing.
[04.02.2026 07:40] Success.
[04.02.2026 07:40] Downloading and parsing paper https://huggingface.co/papers/2602.03216.
[04.02.2026 07:40] Downloading paper 2602.03216 from https://arxiv.org/pdf/2602.03216v1...
[04.02.2026 07:40] Extracting affiliations from text.
[04.02.2026 07:40] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Token Sparse Attention: Efficient Long-Context Inference with Interleaved Token Selection Dongwon Jo 1 Beomseok Kang 1 Jiwon Song 1 Jae-Joon Kim 1 https://github.com/dongwonjo/Token-Sparse-Attention "
[04.02.2026 07:40] Response: ```python
[]
```
[04.02.2026 07:40] Extracting affiliations from text.
[04.02.2026 07:40] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Token Sparse Attention: Efficient Long-Context Inference with Interleaved Token Selection Dongwon Jo 1 Beomseok Kang 1 Jiwon Song 1 Jae-Joon Kim 1 https://github.com/dongwonjo/Token-Sparse-AttentionThe quadratic complexity of attention remains the central bottleneck in long-context inference for large language models. Prior acceleration methods either sparsify the attention map with structured patterns or permanently evict tokens at specific layers, which can retain irrelevant tokens or rely on irreversible early decisions despite the layer-/head-wise dynamics of token importance. In this paper, we propose Token Sparse Attention, lightweight and dynamic token-level sparsification mechanism that compresses per-head Q, K, to reduced token set during attention and then decompresses the output back to the original sequence, enabling token information to be reconsidered in subsequent layers. Furthermore, Token Sparse Attention exposes new design point at the intersection of token selection and sparse attention. Our approach is fully compatible with dense attention implementations, including Flash Attention, and can be seamlessly composed with existing sparse attention kernels. Experimental results show that Token Sparse Attention consistently improves accuracylatency trade-off, achieving up to 3.23 attention speedup at 128K context with less than 1% accuracy degradation. These results demonstrate that dynamic and interleaved token-level sparsification is complementary and effective strategy for scalable longcontext inference. 6 2 0 2 3 ] . [ 1 6 1 2 3 0 . 2 0 6 2 : r 1. Introduction The capability to process long contexts has become defining feature of modern Large Language Models (LLMs) (Achiam et al., 2023; Comanici et al., 2025; An1Department of Electrical and Computer Engineering, Seoul National University, Seoul, South Korea. Correspondence to: JaeJoon Kim <kimjaejoon@snu.ac.kr>. Preprint. February 4, 2026. 1 Figure 1. Speedups with Token Sparse Attention. Attention acceleration ratios obtained by applying the proposed Token Sparse Attention (ours) to existing attention acceleration methods. τ denotes the sparsity level (i.e., higher the sparser). thropic, 2024), enabling applications ranging from long document summarization to multi-turn reasoning and code generation (Yi et al., 2024; Laban et al., 2023; Rando et al., 2025). However, as context lengths (L) increase, the complexity of attention mechanism grows quadratically (O(L2)) during the prefill stage, creating fundamental bottleneck in inference. While hardware-aware optimizations such as FlashAttention (Dao, 2023) significantly reduce memory I/O overhead, the inherent quadratic complexity remains the same, necessitating algorithmic innovations to scale practical context lengths beyond 100K. Recent state-of-the-art (SOTA) methods for accelerating prefill predominantly rely on sparse attention (Lai et al., 2025; Jiang et al., 2024a), where computations for low-importance regions in the attention map (QK ) are bypassed. While these methods approximate attention weights effectively, they typically impose block-level sparsity to stay compatible with the execution patterns of attention kernels. As result, less informative tokens may still be preserved when grouped with salient tokens in the same block, potentially leading to sub-optimal sparsity. few studies have explored token-level sparsity during prefill (Jo et al., 2025; Shi et al., 2024). These methods identify subset of important tokens in the early layers and permanently discard the rest in deeper layers. However, once tokens are evicted, they cannot be reconsidered even if they become significant in Token Sparse Attention: Efficient Long-Context Inference with Interleaved Token Selection Figure 2. Dynamics of Token Importance. (a) Layer-wise overlap of top-k important tokens, showing that token importance shifts significantly across layers. (b) Head-wise token importance rankings within the same layer, illustrating that different attention heads prioritize different subsets of tokens. subsequent layers. This naturally enforces the same set of tokens to persist across layers and fails to capture more selective patterns such as layeror/and head-specific shifts in token importance. Consequently, eviction-based approaches often exhibit weaker accuracy-speedup trade-offs, falling short of fully leveraging the benefits of operating at finer (token-level) granularity. In this paper, we introduce Token Sparse Attention, novel token-level attention mechanism for accelerating prefill in long-context inference. At each head, the method selects small subset of informative tokens (L<L) and performs attention efficiently within the resulting compressed space (RLL ), allowing the selected subset to differ across heads. However, naıvely applying this selection prevents the remaining tokens (LL) from being re-visited in subsequent layers, inevitably evicting them. To address this issue, we interleave the attention output (RLd) back into the original sequence dimension (RLd). This Compress and then Decompress design enables each head to repeatedly select its own significant tokens from the full sequence (L), reaping the benefits of head-specific token selection. In addition, because the full sequence dimension is restored (LL) at every layer, our design naturally supports layerwise adaptive sparsity budgets. Consequently, our method dynamically determines the sparsity level on-the-fly during inference, providing flexibility that eviction-based approaches inherently lack. While improving performance on its own, Token Sparse Attentions core strength lies in being complementary to existing sparse attention methods rather than replacing them. This arises from its strong compatibility: irrelevant tokens can be pruned prior to any sparse computations, e.g., blocksparse or Λ-shape. Because our method essentially performs dense attention within the compressed space (RLL ), it integrates seamlessly with implementations like FlashAttention and sparse attention kernels. As result, we demonstrate heterogeneous granularity (see Figure 1) as an effective new strategy for enhancing sparse attention, e.g., combining FlexPrefill (Lai et al., 2025) (block-sparse) with ours (token-sparse) reaches 87.3% accuracy with 2.8 speedup than FlashAttention, whereas standard FlexPrefill achieves the same accuracy (87.3%) but 2.4 speedup (see Table 1). 2. Method 2.1. Motivation Prior token-sparse techniques (Jo et a"
[04.02.2026 07:40] Mistral response. {"id": "feca28f89f004e8eb0f324b0dfe22924", "created": 1770190833, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1435, "total_tokens": 1457, "completion_tokens": 22, "num_cached_tokens": 1434}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\"Department of Electrical and Computer Engineering, Seoul National University, Seoul, South Korea\"]\n```"}}]}
[04.02.2026 07:40] Response: ```python
["Department of Electrical and Computer Engineering, Seoul National University, Seoul, South Korea"]
```
[04.02.2026 07:40] Deleting PDF ./assets/pdf/2602.03216.pdf.
[04.02.2026 07:40] Success.
[04.02.2026 07:40] Downloading and parsing paper https://huggingface.co/papers/2602.02537.
[04.02.2026 07:40] Extra JSON file exists (./assets/json/2602.02537.json), skip PDF parsing.
[04.02.2026 07:40] Paper image links file exists (./assets/img_data/2602.02537.json), skip HTML parsing.
[04.02.2026 07:40] Success.
[04.02.2026 07:40] Downloading and parsing paper https://huggingface.co/papers/2601.19103.
[04.02.2026 07:40] Extra JSON file exists (./assets/json/2601.19103.json), skip PDF parsing.
[04.02.2026 07:40] Paper image links file exists (./assets/img_data/2601.19103.json), skip HTML parsing.
[04.02.2026 07:40] Success.
[04.02.2026 07:40] Downloading and parsing paper https://huggingface.co/papers/2602.02419.
[04.02.2026 07:40] Extra JSON file exists (./assets/json/2602.02419.json), skip PDF parsing.
[04.02.2026 07:40] Paper image links file exists (./assets/img_data/2602.02419.json), skip HTML parsing.
[04.02.2026 07:40] Success.
[04.02.2026 07:40] Downloading and parsing paper https://huggingface.co/papers/2602.01053.
[04.02.2026 07:40] Downloading paper 2602.01053 from https://arxiv.org/pdf/2602.01053v1...
[04.02.2026 07:40] Extracting affiliations from text.
[04.02.2026 07:40] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"LRAgent: Efficient KV Cache Sharing for Multi-LoRA LLM Agents Hyesung Jeon 1 Hyeongju Ha 1 Jae-Joon Kim "
[04.02.2026 07:40] Response: ```python
[]
```
[04.02.2026 07:40] Extracting affiliations from text.
[04.02.2026 07:40] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"LRAgent: Efficient KV Cache Sharing for Multi-LoRA LLM Agents Hyesung Jeon 1 Hyeongju Ha 1 Jae-Joon Kim1. Introduction 6 2 0 2 1 ] . [ 1 3 5 0 1 0 . 2 0 6 2 : r Role specialization in multi-LLM agent systems is often realized via multi-LoRA, where agents share pretrained backbone and differ only through lightweight adapters. Despite sharing base model weights, each agent independently builds and stores its own KV cache for the same long, tool-augmented trajectories, incurring substantial memory and compute overhead. Existing KV cache sharing methods largely overlook this multi-LoRA setting. We observe that, across agents, cache differences are dominated by adapter outputs, while activations from the shared pretrained backbone remain highly similar. Based on this observation, we propose LRAgent, KV cache sharing framework for multiLoRA agents that decomposes the cache into shared base component from the pretrained weights and an adapter-dependent component from LoRA weights. LRAgent reduces memory overhead by sharing the base component and storing the adapter component in its inherent lowrank form, and further reduces compute overhead, enabled by shared-A multi-LoRA architectures, by also sharing the low-rank cache and avoiding redundant computations for contexts already processed by other agents. To efficiently reconstruct adapter contributions at runtime, we introduce Flash-LoRA-Attention, kernel that reorders attention computation to avoid materializing the low-rank cache to full dimension. LRAgent achieves throughput and time-to-first-token latency close to fully shared caching, while preserving accuracy near the non-shared caching baseline across agentic question-answering benchmarks. Code available at https://github. com/hjeon2k/LRAgent. 1Department of Electrical and Computer Engineering, Seoul National University, Seoul, Korea. Correspondence to: Jae-Joon Kim <kimjaejoon@snu.ac.kr>. Preprint. February 3, 2026. 1 Recently, LLMs have been widely adopted in agent systems due to their long-context understanding ability (Dubey et al., 2024; Jiang et al., 2023; Yang et al., 2025a), reasoning ability (Wei et al., 2022; Yao et al., 2023a; Snell et al., 2024), and external tool interaction capabilities (Yao et al., 2023b; Shen et al., 2024; Qin et al., 2024). In particular, multi-LLM agent systems have gained increasing attention for their ability to assign specialized roles to multiple agents that collaboratively decompose and solve complex tasks (Talebirad & Nadiri, 2023; Wu et al., 2024; Rasal, 2024; Zhang et al., 2025). These agents retrieve information from external tools, augment it with generated outputs, and pass the accumulated context, referred to as trajectories, to other agents for subsequent steps. To improve accuracy, common approach is to fine-tune pretrained model separately for each agent role. These fine-tuned models are typically trained on pre-generated trajectories that reflect role-specific behavior and tool usage patterns (Shinn et al., 2023; Bo et al., 2024; Liu et al., 2025a; Bai et al., 2025; Fu et al., 2025a). Parameter-efficient fine-tuning (PEFT) methods, such as low-rank adaptation (LoRA) (Hu et al., 2022), further enhance scalability by reducing the number of trainable parameters from the full model to pair of low-rank matrices. As result, multi-LoRA architectures enable agents to share the large pretrained backbone during inference while retraining lightweight, role-specific adapters (Wang et al., 2023; Xia et al., 2024). This design has proven effective in practice, consistently outperforming single-model agents and non-fine-tuned baselines in agentic tasks (Qiao et al., 2024; Yu et al., 2024; Liu et al., 2025b; Li et al., 2025). Due to the long trajectories in LLM agent systems, KV cache overhead and compute overhead become more severe in multi-agent systems than in single-agent settings, because each agent maintains its own KV cache and redundant prefills occur even though large portion of the context is shared. This redundancy increases both memory usage and inference latency. To mitigate the memory issue, recent work has explored KV cache sharing across agents. However, existing approaches either require architectural modifications and additional training for cache fusion (Woo et al., 2025; Fu et al., 2025b), focus mainly on handling positional misalignment caused by agent-specific prefixes (Yang et al., 2025b; Pan et al., 2025; Ye et al., 2025), or rely on selective LRAgent: Efficient KV Cache Sharing for Multi-LoRA LLM Agents recomputation of certain tokens or layers (Yao et al., 2025; Liu et al., 2026). Furthermore, these works focus primarily on memory reduction but still incur redundant computation to build hidden state for context that has already been processed by other agents. Importantly, KV cache sharing schemes that explicitly exploit the multi-LoRA architecture remain largely unexplored. In this work, we make key observation that, for the same context, cache discrepancies across agents are dominated by task-specific LoRA-induced outputs, while activations produced by the shared pretrained backbone remain highly similar. Motivated by this observation, we propose LRAGENT, KV cache sharing framework tailored to multi-LoRA agent systems. We decompose the cache into two parts: shared component computed from the pretrained weights, which we call the base cache, and an agent-dependent component induced by the LoRA weights, which we call the adapter outputs. The next key property we exploit is that the adapter output naturally admits low-rank representation. Specifically, we store the intermediate activations produced right after the LoRA down-projection, which have small rank dimension. We refer to these activations as the LR cache. At runtime, we reconstruct the full-dimension adapter contribution from LR cache by multiplying it with the LoRA up-projection matrix only when needed. As result, we compress multiple KV caches into single shared base cache with lightweight LR caches. Based on this concept, we introduce two cache sharing schemes. BaseShared shares the base cache across all agents while maintaining separate LR cache per agent, substantially reducing KV cache memory. Furthermore, motivated by recent multi-LoRA variants that share the down-projection matrix across tasks (Tian et al., 2024; Yang et al., 2025c), we extend our idea to BaseLRShared, which also shares the LR cache as well as the base cache by aligning agents to use com"
[04.02.2026 07:40] Mistral response. {"id": "ad13ddd73bac4ac9870b5fa726cf94d6", "created": 1770190839, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1567, "total_tokens": 1588, "completion_tokens": 21, "num_cached_tokens": 1566}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\"Department of Electrical and Computer Engineering, Seoul National University, Seoul, Korea\"]\n```"}}]}
[04.02.2026 07:40] Response: ```python
["Department of Electrical and Computer Engineering, Seoul National University, Seoul, Korea"]
```
[04.02.2026 07:40] Deleting PDF ./assets/pdf/2602.01053.pdf.
[04.02.2026 07:40] Success.
[04.02.2026 07:40] Downloading and parsing paper https://huggingface.co/papers/2602.03837.
[04.02.2026 07:40] Extra JSON file exists (./assets/json/2602.03837.json), skip PDF parsing.
[04.02.2026 07:40] Paper image links file exists (./assets/img_data/2602.03837.json), skip HTML parsing.
[04.02.2026 07:40] Success.
[04.02.2026 07:40] Downloading and parsing paper https://huggingface.co/papers/2602.03806.
[04.02.2026 07:40] Extra JSON file exists (./assets/json/2602.03806.json), skip PDF parsing.
[04.02.2026 07:40] Paper image links file exists (./assets/img_data/2602.03806.json), skip HTML parsing.
[04.02.2026 07:40] Success.
[04.02.2026 07:40] Downloading and parsing paper https://huggingface.co/papers/2602.03454.
[04.02.2026 07:40] Extra JSON file exists (./assets/json/2602.03454.json), skip PDF parsing.
[04.02.2026 07:40] Paper image links file exists (./assets/img_data/2602.03454.json), skip HTML parsing.
[04.02.2026 07:40] Success.
[04.02.2026 07:40] Downloading and parsing paper https://huggingface.co/papers/2602.02914.
[04.02.2026 07:40] Extra JSON file exists (./assets/json/2602.02914.json), skip PDF parsing.
[04.02.2026 07:40] Paper image links file exists (./assets/img_data/2602.02914.json), skip HTML parsing.
[04.02.2026 07:40] Success.
[04.02.2026 07:40] Downloading and parsing paper https://huggingface.co/papers/2602.03647.
[04.02.2026 07:40] Extra JSON file exists (./assets/json/2602.03647.json), skip PDF parsing.
[04.02.2026 07:40] Paper image links file exists (./assets/img_data/2602.03647.json), skip HTML parsing.
[04.02.2026 07:40] Success.
[04.02.2026 07:40] Downloading and parsing paper https://huggingface.co/papers/2602.03238.
[04.02.2026 07:40] Downloading paper 2602.03238 from https://arxiv.org/pdf/2602.03238v1...
[04.02.2026 07:40] Extracting affiliations from text.
[04.02.2026 07:40] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"The Necessity of Unified Framework for LLM-Based Agent Evaluation Pengyu Zhu 1 Li Sun 1 Philip S. Yu 2 Sen Su 1 3 6 2 0 2 3 ] . [ 1 8 3 2 3 0 . 2 0 6 2 : r a "
[04.02.2026 07:40] Response: ```python
[]
```
[04.02.2026 07:40] Extracting affiliations from text.
[04.02.2026 07:40] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"The Necessity of Unified Framework for LLM-Based Agent Evaluation Pengyu Zhu 1 Li Sun 1 Philip S. Yu 2 Sen Su 1 3 6 2 0 2 3 ] . [ 1 8 3 2 3 0 . 2 0 6 2 : r aWith the advent of Large Language Models (LLMs), general-purpose agents have seen fundamental advancements. However, evaluating these agents presents unique challenges that distinguish them from static QA benchmarks. We observe that current agent benchmarks are heavily confounded by extraneous factors, including system prompts, toolset configurations, and environmental dynamics. Existing evaluations often rely on fragmented, researcher-specific frameworks where the prompt engineering for reasoning and tool usage varies significantly, making it difficult to attribute performance gains to the model itself. Additionally, the lack of standardized environmental data leads to untraceable errors and non-reproducible results. This lack of standardization introduces substantial unfairness and opacity into the field. We propose that unified evaluation framework is essential for the rigorous advancement of agent evaluation. To this end, we introduce proposal aimed at standardizing agent evaluation. 1. Introduction As large language model (LLM)based agents become increasingly prevalent (Xi et al., 2025), evaluating agentic capability has emerged as central criterion for assessing modern LLMs (OpenAI et al., 2024; Team et al., 2025; Anthropic, 2025). Unlike traditional LLMs that exhibit primarily static inputoutput behavior (Ni et al., 2025), agents operate as integrated systems with internal mechanisms for planning and memory, acting upon external environments through tool interactions(Xi et al., 2025). This system-level nature fundamentally expands the evaluation space and introduces challenges to the consistency, completeness, and 1Beijing University of Posts and Telecommunications 2University of Illinois Chicago 3Chongqing University of Posts and Telecommunications. Correspondence to: Pengyu Zhu <whfelingyu_zhupengyu@bupt.edu.cn>. Preprint. February 4, 2026. fairness of agent evaluation, as evaluation outcomes now depend on how the surrounding system components are instantiated. To address the paradigm shift, growing number of agent benchmarks have been proposed, aiming to measure autonomous decision-making (Mialon et al., 2024), tool invocation (Patil et al., 2025b), and task execution (Yao et al., 2022) in interactive environments. However, despite their rapid proliferation, existing agent benchmarks are almost exclusively developed within isolated, researcher-designed frameworks for agent evaluation, with benchmark outcomes tightly coupled to distinct environment designs, system prompts, tool abstractions, and memory mechanisms, along with other specific configurations. As result, benchmark outcomes are deeply intertwined with agent specific design choices, obscuring whether reported performance gains reflect genuine improvements in agentic capability. Without unified evaluation standard, benchmarks remain incomparable, and improvements cannot be reliably attributed to the agentic capability. Collectively, these discrepancies reveal systemic structural flaw in current evaluation practices, rather than isolated limitations of individual benchmarks. Without shared evaluation standard, it remains unclear whether the reported performance gains reflect genuine improvements in agentic capability or artifacts of framework-specific design choices. We therefore take the position that unified framework for LLM-based agent evaluation is not optional, but necessary. In this paper, we first provide background on agent evaluation to establish the context for our position. We then analyze the major components of agent evaluation frameworks, examining how variations in each component influence evaluation outcomes and contribute to inconsistency. Building on this analysis, we decompose the evaluation framework into two core elementsthe sandbox and the evaluation methodologyand articulate the properties each must satisfy to support reliable and meaningful evaluation. Finally, we present our proposed approach toward unified framework for LLM-based agents and address alternative viewpoints to demonstrate the robustness of our position. The Necessity of Unified Framework for LLM-Based Agent Evaluation ical agent integrates LLM with auxiliary modules for planning, memory, and tools, enabling it to reason, act, and maintain state across multiple interaction steps. To operationalize these components, agents must be instantiated within an execution system that coordinates model inference, action execution, and environment interaction. this instantiation is commonly realIn practice, ized using open-source agent plantforms such as LangChain (Mavroudis, 2024), LangGraph (LangChain AI, 2024), AutoGPT (Significant Gravitas, 2023), and smolagents (Roucher et al., 2025), or through benchmark-specific implementations. These plantforms define how the agents components are orchestrated at runtime, transforming abstract agent designs into runnable systems rather than isolated model calls. As result, the concrete behavior of an agent is shaped not only by the underlying model, but also by the design and configuration of its execution plantforms, which governs planning loops, memory updates, and tool invocation procedures. Figure 1. Overview of LLM-based agent evaluation, where correctness is assessed over trajectories, final answer, and environment state changes rather than single output. 2. Background on Agent Evaluation 2.1. From LLM Evaluation to Agent Evaluation Classical LLM benchmarks evaluate models as static input output systems, where performance is assessed by the correctness or quality of the response for fixed prompt (Ni et al., 2025). Such benchmarks are effective for measuring foundational capabilities, including language understanding (Hendrycks et al., 2021a), code generation (Chen et al., 2021), mathematical reasoning (Hendrycks et al., 2021b), logical problem solving (Rein et al., 2024), and sentiment analysis (Chen et al., 2024). These tasks typically admit deterministic ground-truth answers or well-defined evaluation criteria, enabling models to be assessed independently across samples. As result, traditional LLM benchmarks characterize models as static mappings. This static evaluation paradigm becomes insufficient once LLMs are deployed as agents. Unlike passive responders to prompts, agents operate through multi-step decision-making trajectorie"
[04.02.2026 07:40] Mistral response. {"id": "1bbda663ee294a5cb278bc10b6a92839", "created": 1770190846, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1351, "total_tokens": 1389, "completion_tokens": 38, "num_cached_tokens": 1350}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\n    \"Beijing University of Posts and Telecommunications\",\n    \"University of Illinois Chicago\",\n    \"Chongqing University of Posts and Telecommunications\"\n]\n```"}}]}
[04.02.2026 07:40] Response: ```python
[
    "Beijing University of Posts and Telecommunications",
    "University of Illinois Chicago",
    "Chongqing University of Posts and Telecommunications"
]
```
[04.02.2026 07:40] Deleting PDF ./assets/pdf/2602.03238.pdf.
[04.02.2026 07:40] Success.
[04.02.2026 07:40] Downloading and parsing paper https://huggingface.co/papers/2602.01753.
[04.02.2026 07:40] Extra JSON file exists (./assets/json/2602.01753.json), skip PDF parsing.
[04.02.2026 07:40] Paper image links file exists (./assets/img_data/2602.01753.json), skip HTML parsing.
[04.02.2026 07:40] Success.
[04.02.2026 07:40] Downloading and parsing paper https://huggingface.co/papers/2602.03817.
[04.02.2026 07:40] Extra JSON file exists (./assets/json/2602.03817.json), skip PDF parsing.
[04.02.2026 07:40] Paper image links file exists (./assets/img_data/2602.03817.json), skip HTML parsing.
[04.02.2026 07:40] Success.
[04.02.2026 07:40] Enriching papers with extra data.
[04.02.2026 07:40] ********************************************************************************
[04.02.2026 07:40] Abstract 0. Multimodal large language models can effectively understand source code when represented as compressed images, achieving significant token reduction while maintaining or improving performance on code comprehension tasks.  					AI-generated summary 				 Large Language Models (LLMs) have achieved rema...
[04.02.2026 07:40] ********************************************************************************
[04.02.2026 07:40] Abstract 1. Large language models face challenges in long-horizon agentic workflows due to lack of authentic long-dependency training data, which is addressed by leveraging pull request sequences for structured supervision through progressive decomposition, consistency enforcement, and refinement from bug-fix h...
[04.02.2026 07:40] ********************************************************************************
[04.02.2026 07:40] Abstract 2. Current world models lack unified frameworks despite task-specific advances, necessitating a comprehensive approach integrating interaction, perception, symbolic reasoning, and spatial representation.  					AI-generated summary 				 World models have emerged as a critical frontier in AI research, ai...
[04.02.2026 07:40] ********************************************************************************
[04.02.2026 07:40] Abstract 3. MARS is a modular AI research automation framework that uses budget-aware planning, modular construction, and reflective memory to achieve state-of-the-art performance in autonomous machine learning research.  					AI-generated summary 				 Automating AI research differs from general software engine...
[04.02.2026 07:40] ********************************************************************************
[04.02.2026 07:40] Abstract 4. 3DiMo enables view-agnostic human motion control in video generation by training a motion encoder alongside a pretrained video generator to distill driving frames into compact motion tokens that align with the generator's spatial priors.  					AI-generated summary 				 Existing methods for human mot...
[04.02.2026 07:40] ********************************************************************************
[04.02.2026 07:40] Abstract 5. Research investigates latent planning dynamics in large language models through a probing method called Tele-Lens, revealing limited global planning and enabling improved uncertainty estimation and CoT bypass recognition.  					AI-generated summary 				 This work stems from prior complementary obser...
[04.02.2026 07:40] ********************************************************************************
[04.02.2026 07:40] Abstract 6. A Docker-free framework replaces physical execution environments with learned surrogates for training software engineering agents, enabling efficient training and test-time scaling without costly container setup.  					AI-generated summary 				 Recent advances in large language models (LLMs) have en...
[04.02.2026 07:40] ********************************************************************************
[04.02.2026 07:40] Abstract 7. DeepResearch report generation is improved through human-preference-aligned rubric generators trained via reinforcement learning with hybrid rewards and enhanced with multi-agent Markov-state workflows.  					AI-generated summary 				 Nowadays, training and evaluating DeepResearch-generated reports ...
[04.02.2026 07:40] ********************************************************************************
[04.02.2026 07:40] Abstract 8. SWE-Master presents a reproducible framework for developing software engineering agents through systematic optimization across multiple stages of agent development, achieving superior performance on software task resolution benchmarks.  					AI-generated summary 				 In this technical report, we pre...
[04.02.2026 07:40] ********************************************************************************
[04.02.2026 07:40] Abstract 9. A novel distillation framework called DP-DMD is introduced that preserves sample diversity in text-to-image generation by separating the roles of distilled steps, using v-prediction for diversity and standard DMD loss for quality refinement without additional computational overhead.  					AI-generat...
[04.02.2026 07:40] ********************************************************************************
[04.02.2026 07:40] Abstract 10. AOrchestra is a framework-agnostic agentic system that uses a tuple-based abstraction to dynamically create specialized task executors, achieving improved performance on complex benchmarks through automated agent creation and resource management.  					AI-generated summary 				 Language agents have ...
[04.02.2026 07:40] ********************************************************************************
[04.02.2026 07:40] Abstract 11. Parallel-Probe is a training-free controller that optimizes parallel thinking by using consensus-based early stopping and deviation-based branch pruning to reduce computational costs while maintaining accuracy.  					AI-generated summary 				 Parallel thinking has emerged as a promising paradigm for...
[04.02.2026 07:40] ********************************************************************************
[04.02.2026 07:40] Abstract 12. CoBA-RL adapts rollout budget allocation for LLM training by evaluating sample training value and optimizing resource distribution through a capability-oriented value function and greedy strategy.  					AI-generated summary 				 Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a ...
[04.02.2026 07:40] ********************************************************************************
[04.02.2026 07:40] Abstract 13. UnifiedReward-Flex combines reward modeling with flexible, context-adaptive reasoning to improve visual generation by dynamically constructing hierarchical assessments based on semantic intent and visual evidence.  					AI-generated summary 				 Recent advancements in multimodal reward models (RMs) ...
[04.02.2026 07:40] ********************************************************************************
[04.02.2026 07:40] Abstract 14. Wide Research advances search intelligence through a dedicated benchmark and multi-agent architecture that enables parallel information retrieval under complex constraints.  					AI-generated summary 				 Search intelligence is evolving from Deep Research to Wide Research, a paradigm essential for r...
[04.02.2026 07:40] ********************************************************************************
[04.02.2026 07:40] Abstract 15. LENS framework improves reinforcement learning with verifiable rewards by identifying and removing interference tokens to enhance exploration efficiency and training stability.  					AI-generated summary 				 Reinforcement Learning with Verifiable Rewards (RLVR) has advanced LLM reasoning, but remai...
[04.02.2026 07:40] ********************************************************************************
[04.02.2026 07:40] Abstract 16. AdaptMMBench presents a comprehensive benchmark for evaluating adaptive multimodal reasoning in Vision-Language Models, measuring reasoning mode selection rationality through dynamic difficulty assessment and multi-dimensional process evaluation.  					AI-generated summary 				 Adaptive multimodal r...
[04.02.2026 07:40] ********************************************************************************
[04.02.2026 07:40] Abstract 17. XDLM unifies Masked Diffusion Language Models and Uniform-noise Diffusion Language Models through a stationary noise kernel, achieving improved performance in both semantic understanding and generation quality.  					AI-generated summary 				 In discrete generative modeling, two dominant paradigms d...
[04.02.2026 07:40] ********************************************************************************
[04.02.2026 07:40] Abstract 18. DeMix is a framework that uses model merging to predict optimal data ratios for LLM pre-training, decoupling search from training costs to improve mixture discovery efficiency.  					AI-generated summary 				 Determining an effective data mixture is a key factor in Large Language Model (LLM) pre-tra...
[04.02.2026 07:40] ********************************************************************************
[04.02.2026 07:40] Abstract 19. A unified agent system called FullStack-Agent is introduced to assist non-expert users in developing complex interactive websites by addressing full-stack development challenges through enhanced planning, code editing, and self-improving capabilities.  					AI-generated summary 				 Assisting non-ex...
[04.02.2026 07:40] ********************************************************************************
[04.02.2026 07:40] Abstract 20. Token Sparse Attention enables efficient long-context inference by dynamically compressing and decompressing attention tensors at the token level, achieving significant speedup with minimal accuracy loss.  					AI-generated summary 				 The quadratic complexity of attention remains the central bottl...
[04.02.2026 07:40] ********************************************************************************
[04.02.2026 07:40] Abstract 21. WorldVQA is a benchmark for evaluating the visual world knowledge of multimodal large language models by separating visual knowledge retrieval from reasoning to measure memorized facts.  					AI-generated summary 				 We introduce WorldVQA, a benchmark designed to evaluate the atomic visual world kn...
[04.02.2026 07:40] ********************************************************************************
[04.02.2026 07:40] Abstract 22. A reinforcement learning framework with glance and focus models improves pan-cancer screening in CT scans by addressing foreground-background imbalance and reducing false positives through group relative learning.  					AI-generated summary 				 Pan-cancer screening in large-scale CT scans remains c...
[04.02.2026 07:40] ********************************************************************************
[04.02.2026 07:40] Abstract 23. SafeGround is a uncertainty-aware framework for GUI grounding models that uses distribution-aware uncertainty quantification and calibration to enable risk-aware predictions with controlled false discovery rates.  					AI-generated summary 				 Graphical User Interface (GUI) grounding aims to transl...
[04.02.2026 07:40] ********************************************************************************
[04.02.2026 07:40] Abstract 24. LRAgent is a KV cache sharing framework for multi-LoRA agents that decomposes cache into shared and adapter-dependent components, reducing memory and compute overhead while maintaining accuracy.  					AI-generated summary 				 Role specialization in multi-LLM agent systems is often realized via mult...
[04.02.2026 07:40] ********************************************************************************
[04.02.2026 07:40] Abstract 25. Advanced AI models demonstrate capability in supporting expert-level mathematical discovery and scientific research through collaborative approaches involving proof verification and automated code execution.  					AI-generated summary 				 Recent advances in large language models (LLMs) have opened ...
[04.02.2026 07:40] ********************************************************************************
[04.02.2026 07:40] Abstract 26. Offline reinforcement learning method combines contextual bandit learning with partial trajectories to improve multi-turn code generation performance while reducing training costs.  					AI-generated summary 				 Recently, there have been significant research interests in training large language mod...
[04.02.2026 07:40] ********************************************************************************
[04.02.2026 07:40] Abstract 27. CoViP addresses contextualized visual personalization by treating personalized image captioning as a core task and improving capabilities through reinforcement-learning-based post-training and caption-augmented generation.  					AI-generated summary 				 Despite recent progress in vision-language mo...
[04.02.2026 07:40] ********************************************************************************
[04.02.2026 07:40] Abstract 28. FaceLinkGen attack demonstrates that current privacy-preserving face recognition methods fail to protect identity information despite pixel-level distortion metrics suggesting adequate protection.  					AI-generated summary 				 Transformation-based privacy-preserving face recognition (PPFR) aims to...
[04.02.2026 07:40] ********************************************************************************
[04.02.2026 07:40] Abstract 29. Search-R2 framework improves language agent reasoning through Actor-Refiner collaboration with targeted interventions and fine-grained reward supervision for better credit assignment in reinforcement learning.  					AI-generated summary 				 Search-integrated reasoning enables language agents to tra...
[04.02.2026 07:40] ********************************************************************************
[04.02.2026 07:40] Abstract 30. Large Language Models have advanced general-purpose agents, but current evaluation benchmarks suffer from confounding factors and lack of standardization, necessitating a unified framework for rigorous assessment.  					AI-generated summary 				 With the advent of Large Language Models (LLMs), gener...
[04.02.2026 07:40] ********************************************************************************
[04.02.2026 07:40] Abstract 31. ObjEmbed is a novel multimodal language-model embedding approach that decomposes images into regional embeddings for improved object-level visual understanding and retrieval tasks.  					AI-generated summary 				 Aligning objects with corresponding textual descriptions is a fundamental challenge and...
[04.02.2026 07:40] ********************************************************************************
[04.02.2026 07:40] Abstract 32. A fusion framework called FINCH combines audio and spatiotemporal predictors for bioacoustic classification by adaptively weighting evidence based on reliability estimates, outperforming fixed-weight methods and audio-only approaches.  					AI-generated summary 				 Many machine learning systems hav...
[04.02.2026 07:40] Read previous papers.
[04.02.2026 07:40] Generating reviews via LLM API.
[04.02.2026 07:40] Using data from previous issue: {"categories": ["#plp", "#multimodal", "#inference"], "emoji": "🖼️", "ru": {"title": "Код как изображение: сжатие и эффективность", "desc": "В статье исследуется применение многомодальных больших языковых моделей (MLLM) для понимания исходного кода, представленного в виде сжатых изображений вместо т
[04.02.2026 07:40] Using data from previous issue: {"categories": ["#training", "#synthetic", "#alignment", "#data", "#long_context", "#reasoning", "#agents"], "emoji": "🔄", "ru": {"title": "От кода к агентам: синтез долгосрочного обучения из истории разработки", "desc": "Статья описывает проблему обучения больших языковых моделей для долгосрочных а
[04.02.2026 07:40] Using data from previous issue: {"categories": [], "emoji": "🌍", "ru": {"title": "Унифицированная архитектура мировых моделей для целостного понимания окружающей среды", "desc": "В статье анализируются ограничения фрагментированного подхода к разработке мировых моделей, которые сегодня разрабатываются для отдельных задач предсказа
[04.02.2026 07:40] Using data from previous issue: {"categories": ["#training", "#agents", "#open_source", "#science", "#benchmark"], "emoji": "🔬", "ru": {"title": "Модульный агент с рефлексивной памятью для экономного автоматизированного ML-исследования", "desc": "MARS — это модульный фреймворк для автоматизации исследований в области машинного обу
[04.02.2026 07:40] Using data from previous issue: {"categories": ["#video", "#training", "#architecture", "#multimodal", "#optimization", "#3d"], "emoji": "🎬", "ru": {"title": "Неявное 3D представление движения для генерации видео независимо от угла обзора", "desc": "3DiMo — это метод для управления движением человека в генерации видео, который обу
[04.02.2026 07:40] Using data from previous issue: {"categories": ["#reasoning", "#training", "#open_source", "#interpretability", "#benchmark"], "emoji": "🔍", "ru": {"title": "Скрытое планирование в LLM: локальность вместо глобальной стратегии", "desc": "Работа исследует скрытые механизмы планирования в больших языковых моделях, используя метод зон
[04.02.2026 07:40] Using data from previous issue: {"categories": ["#training", "#benchmark", "#rl", "#plp", "#agents"], "emoji": "🚀", "ru": {"title": "Виртуальное окружение вместо Docker для обучения агентов программной инженерии", "desc": "В работе предложен SWE-World — фреймворк без Docker, который заменяет физические окружения выполнения на обуч
[04.02.2026 07:40] Using data from previous issue: {"categories": ["#rlhf", "#benchmark", "#dataset", "#agents"], "emoji": "📋", "ru": {"title": "Выравнивание генераторов критериев оценки с человеческими предпочтениями для улучшения отчётов", "desc": "В статье предложен метод обучения генераторов критериев оценки, которые выравнены с предпочтениями ч
[04.02.2026 07:40] Using data from previous issue: {"categories": ["#open_source", "#training", "#benchmark", "#long_context", "#rl", "#plp", "#reasoning", "#agents", "#optimization"], "emoji": "🛠️", "ru": {"title": "Систематическая оптимизация агентов для решения задач программной инженерии", "desc": "SWE-Master представляет открытую и воспроизводи
[04.02.2026 07:40] Using data from previous issue: {"categories": ["#multimodal", "#training", "#inference", "#diffusion", "#optimization"], "emoji": "🎨", "ru": {"title": "Дистилляция с сохранением разнообразия для быстрой генерации изображений", "desc": "В работе предлагается новый метод дистилляции DP-DMD для генерации изображений по тексту, котор
[04.02.2026 07:40] Using data from previous issue: {"categories": [], "emoji": "🎼", "ru": {"title": "Динамическая оркестрация агентов через кортежную абстракцию", "desc": "AOrchestra — это универсальная система многоагентных систем, которая использует абстракцию в виде кортежа (инструкция, контекст, инструменты, модель) для динамического создания сп
[04.02.2026 07:40] Using data from previous issue: {"categories": ["#reasoning", "#optimization"], "emoji": "🌳", "ru": {"title": "Умная обрезка параллельных рассуждений через консенсус и девиацию", "desc": "В статье предложен Parallel-Probe — контроллер без обучения для оптимизации параллельного мышления в больших языковых моделях. Авторы вводят 2D-
[04.02.2026 07:40] Using data from previous issue: {"categories": ["#reasoning", "#rl", "#optimization", "#training"], "emoji": "⚖️", "ru": {"title": "Умное распределение вычислительных ресурсов для эффективного обучения языковых моделей", "desc": "CoBA-RL — это алгоритм обучения с подкреплением, который адаптивно распределяет вычислительные ресурсы
[04.02.2026 07:40] Using data from previous issue: {"categories": ["#video", "#reasoning", "#multimodal", "#alignment", "#cv", "#rlhf"], "emoji": "🎨", "ru": {"title": "Гибкое моделирование вознаграждения с контекстной адаптацией для качественной генерации изображений", "desc": "В работе предложена UnifiedReward-Flex — унифицированная модель вознагра
[04.02.2026 07:40] Using data from previous issue: {"categories": ["#training", "#agents", "#rl", "#benchmark", "#dataset"], "emoji": "🔍", "ru": {"title": "Широкий поиск: многоагентная архитектура для параллельного извлечения информации", "desc": "Статья представляет Wide Research — новую парадигму поиска информации, которая позволяет параллельно из
[04.02.2026 07:40] Using data from previous issue: {"categories": ["#reasoning", "#rl", "#optimization", "#training"], "emoji": "🧹", "ru": {"title": "Очистка промптов от помех для эффективного обучения языковых моделей", "desc": "В статье представлена LENS - фреймворк для обучения с подкреплением с верифицируемыми наградами, который решает проблему 
[04.02.2026 07:40] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#cv"], "emoji": "🧠", "ru": {"title": "Умное переключение: оценка адаптивного выбора режимов рассуждения в мультимодальных моделях", "desc": "AdaptMMBench — это комплексный бенчмарк для оценки адаптивного мультимодального рассуждения в Vision-Language Mod
[04.02.2026 07:40] Using data from previous issue: {"categories": ["#open_source", "#diffusion", "#optimization"], "emoji": "🔀", "ru": {"title": "Объединение парадигм: единая диффузионная модель для понимания и генерации", "desc": "В работе предложен XDLM — единый фреймворк, объединяющий две конкурирующие парадигмы дискретного генеративного моделиро
[04.02.2026 07:40] Using data from previous issue: {"categories": ["#training", "#optimization", "#data", "#open_source", "#dataset"], "emoji": "🧩", "ru": {"title": "Поиск идеальной смеси данных через слияние моделей без переобучения", "desc": "DeMix — это фреймворк для оптимизации смеси данных при предварительном обучении LLM, использующий объедине
[04.02.2026 07:40] Using data from previous issue: {"categories": ["#benchmark", "#dataset", "#agents", "#training", "#plp"], "emoji": "🏗️", "ru": {"title": "Полнофункциональный агент для автоматизации разработки веб-приложений", "desc": "Представлена FullStack-Agent - унифицированная мультиагентная система, которая помогает непрофессиональным разра
[04.02.2026 07:40] Querying the API.
[04.02.2026 07:40] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Token Sparse Attention enables efficient long-context inference by dynamically compressing and decompressing attention tensors at the token level, achieving significant speedup with minimal accuracy loss.  					AI-generated summary 				 The quadratic complexity of attention remains the central bottleneck in long-context inference for large language models. Prior acceleration methods either sparsify the attention map with structured patterns or permanently evict tokens at specific layers, which can retain irrelevant tokens or rely on irreversible early decisions despite the layer-/head-wise dynamics of token importance. In this paper, we propose Token Sparse Attention, a lightweight and dynamic token-level sparsification mechanism that compresses per-head Q, K, V to a reduced token set during attention and then decompresses the output back to the original sequence, enabling token information to be reconsidered in subsequent layers. Furthermore, Token Sparse Attention exposes a new design point at the intersection of token selection and sparse attention. Our approach is fully compatible with dense attention implementations, including Flash Attention, and can be seamlessly composed with existing sparse attention kernels. Experimental results show that Token Sparse Attention consistently improves accuracy-latency trade-off, achieving up to times3.23 attention speedup at 128K context with less than 1% accuracy degradation. These results demonstrate that dynamic and interleaved token-level sparsification is a complementary and effective strategy for scalable long-context inference.
[04.02.2026 07:40] Response: ```json
{
  "desc": "Работа предлагает Token Sparse Attention — механизм динамической спарификации внимания на уровне токенов, который решает проблему квадратичной сложности механизма внимания при обработке длинных контекстов. Метод компрессирует матрицы Q, K, V для каждой головки внимания, отбирая только релевантные токены, а затем восстанавливает полный выход для последующих слоёв, позволяя модели переосмыслить важность токенов на разных этапах. Предложенный подход полностью совместим с существующими реализациями, включая Flash Attention, и достигает ускорения в 3.23 раза на контексте из 128K токенов с потерей точности менее 1%. Этот результат показывает, что динамическая и чередующаяся спарификация на уровне токенов является эффективной стратегией для масштабируемого inference с длинными контекстами.",
  "emoji": "⚡",
  "title": "Динамическая спарификация внимания для эффективного inference на длинных контекстах"
}
```
[04.02.2026 07:40] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Token Sparse Attention enables efficient long-context inference by dynamically compressing and decompressing attention tensors at the token level, achieving significant speedup with minimal accuracy loss.  					AI-generated summary 				 The quadratic complexity of attention remains the central bottleneck in long-context inference for large language models. Prior acceleration methods either sparsify the attention map with structured patterns or permanently evict tokens at specific layers, which can retain irrelevant tokens or rely on irreversible early decisions despite the layer-/head-wise dynamics of token importance. In this paper, we propose Token Sparse Attention, a lightweight and dynamic token-level sparsification mechanism that compresses per-head Q, K, V to a reduced token set during attention and then decompresses the output back to the original sequence, enabling token information to be reconsidered in subsequent layers. Furthermore, Token Sparse Attention exposes a new design point at the intersection of token selection and sparse attention. Our approach is fully compatible with dense attention implementations, including Flash Attention, and can be seamlessly composed with existing sparse attention kernels. Experimental results show that Token Sparse Attention consistently improves accuracy-latency trade-off, achieving up to times3.23 attention speedup at 128K context with less than 1% accuracy degradation. These results demonstrate that dynamic and interleaved token-level sparsification is a complementary and effective strategy for scalable long-context inference."

[04.02.2026 07:40] Response: ```python
["INFERENCE", "ARCHITECTURE"]
```
[04.02.2026 07:40] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Token Sparse Attention enables efficient long-context inference by dynamically compressing and decompressing attention tensors at the token level, achieving significant speedup with minimal accuracy loss.  					AI-generated summary 				 The quadratic complexity of attention remains the central bottleneck in long-context inference for large language models. Prior acceleration methods either sparsify the attention map with structured patterns or permanently evict tokens at specific layers, which can retain irrelevant tokens or rely on irreversible early decisions despite the layer-/head-wise dynamics of token importance. In this paper, we propose Token Sparse Attention, a lightweight and dynamic token-level sparsification mechanism that compresses per-head Q, K, V to a reduced token set during attention and then decompresses the output back to the original sequence, enabling token information to be reconsidered in subsequent layers. Furthermore, Token Sparse Attention exposes a new design point at the intersection of token selection and sparse attention. Our approach is fully compatible with dense attention implementations, including Flash Attention, and can be seamlessly composed with existing sparse attention kernels. Experimental results show that Token Sparse Attention consistently improves accuracy-latency trade-off, achieving up to times3.23 attention speedup at 128K context with less than 1% accuracy degradation. These results demonstrate that dynamic and interleaved token-level sparsification is a complementary and effective strategy for scalable long-context inference."

[04.02.2026 07:40] Response: ```python
['LONG_CONTEXT', 'OPTIMIZATION']
```
[04.02.2026 07:40] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces Token Sparse Attention, a method designed to improve the efficiency of long-context inference in large language models by dynamically compressing and decompressing attention tensors at the token level. Traditional attention mechanisms face challenges due to their quadratic complexity, which limits performance when processing long sequences. Token Sparse Attention addresses this by selectively reducing the number of tokens considered during attention calculations, allowing for a more flexible and efficient use of resources. The results show that this approach can significantly speed up attention processes while maintaining high accuracy, making it a valuable advancement in the field of machine learning.","title":"Dynamic Token Sparsification for Efficient Long-Context Inference"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper introduces Token Sparse Attention, a method designed to improve the efficiency of long-context inference in large language models by dynamically compressing and decompressing attention tensors at the token level. Traditional attention mechanisms face challenges due to their quadratic complexity, which limits performance when processing long sequences. Token Sparse Attention addresses this by selectively reducing the number of tokens considered during attention calculations, allowing for a more flexible and efficient use of resources. The results show that this approach can significantly speed up attention processes while maintaining high accuracy, making it a valuable advancement in the field of machine learning.', title='Dynamic Token Sparsification for Efficient Long-Context Inference'))
[04.02.2026 07:41] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本文提出了一种名为Token Sparse Attention的机制，旨在提高长上下文推理的效率。该方法通过在令牌级别动态压缩和解压注意力张量，显著加快了计算速度，同时保持了较小的准确性损失。与以往的加速方法不同，Token Sparse Attention允许在后续层中重新考虑令牌信息，从而提高了模型的灵活性和性能。实验结果表明，该方法在128K上下文中实现了最高3.23倍的注意力加速，且准确性下降不到1%。","title":"动态令牌稀疏注意力，提升长上下文推理效率"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本文提出了一种名为Token Sparse Attention的机制，旨在提高长上下文推理的效率。该方法通过在令牌级别动态压缩和解压注意力张量，显著加快了计算速度，同时保持了较小的准确性损失。与以往的加速方法不同，Token Sparse Attention允许在后续层中重新考虑令牌信息，从而提高了模型的灵活性和性能。实验结果表明，该方法在128K上下文中实现了最高3.23倍的注意力加速，且准确性下降不到1%。', title='动态令牌稀疏注意力，提升长上下文推理效率'))
[04.02.2026 07:41] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#cv"], "emoji": "🌍", "ru": {"title": "Измерение того, что модель действительно запомнила о визуальном мире", "desc": "Авторы представляют WorldVQA — бенчмарк для оценки визуального знания мультимодальных больших языковых моделей. Главная особенность закл
[04.02.2026 07:41] Using data from previous issue: {"categories": ["#rl", "#cv", "#healthcare"], "emoji": "🔍", "ru": {"title": "Двухэтапный скрининг рака через обучение с подкреплением и групповое сравнение", "desc": "Представлена система GF-Screen на основе обучения с подкреплением для скрининга рака по КТ-снимкам, которая решает проблему дисбаланс
[04.02.2026 07:41] Using data from previous issue: {"categories": ["#benchmark", "#inference", "#agents", "#security"], "emoji": "🛡️", "ru": {"title": "Безопасное управление интерфейсом через контролируемую неопределённость", "desc": "SafeGround — это фреймворк для моделей визуального понимания графических интерфейсов, который использует методы коли
[04.02.2026 07:41] Querying the API.
[04.02.2026 07:41] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

LRAgent is a KV cache sharing framework for multi-LoRA agents that decomposes cache into shared and adapter-dependent components, reducing memory and compute overhead while maintaining accuracy.  					AI-generated summary 				 Role specialization in multi-LLM agent systems is often realized via multi-LoRA, where agents share a pretrained backbone and differ only through lightweight adapters. Despite sharing base model weights, each agent independently builds and stores its own KV cache for the same long, tool-augmented trajectories, incurring substantial memory and compute overhead. Existing KV cache sharing methods largely overlook this multi-LoRA setting. We observe that, across agents, cache differences are dominated by adapter outputs, while activations from the shared pretrained backbone remain highly similar. Based on this observation, we propose LRAgent, a KV cache sharing framework for multi-LoRA agents that decomposes the cache into a shared base component from the pretrained weights and an adapter-dependent component from LoRA weights. LRAgent reduces memory overhead by sharing the base component and storing the adapter component in its inherent low-rank form, and further reduces compute overhead, enabled by shared-A multi-LoRA architectures, by also sharing the low-rank cache and avoiding redundant computations for contexts already processed by other agents. To efficiently reconstruct adapter contributions at runtime, we introduce Flash-LoRA-Attention, a kernel that reorders attention computation to avoid materializing the low-rank cache to full dimension. LRAgent achieves throughput and time-to-first-token latency close to fully shared caching, while preserving accuracy near the non-shared caching baseline across agentic question-answering benchmarks.
[04.02.2026 07:41] Response: ```json
{
  "desc": "LRAgent — это фреймворк для совместного использования KV кэша в системах с несколькими агентами на основе LoRA адаптеров. Основная идея заключается в декомпозиции кэша на два компонента: общую часть, полученную из предварительно обученной модели, и зависящую от адаптера часть, которая хранится в низкорангвой форме. Авторы вводят Flash-LoRA-Attention — специальное ядро, которое переупорядочивает вычисление внимания, избегая необходимости развёртывания низкорангового кэша в полную размерность. Предложенный подход достигает пропускной способности и скорости отклика, близких к полностью общему кэшированию, при этом сохраняя точность на уровне независимого кэширования.",
  "emoji": "⚡",
  "title": "Умное совместное использование кэша для многоагентных систем с LoRA адаптерами"
}
```
[04.02.2026 07:41] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"LRAgent is a KV cache sharing framework for multi-LoRA agents that decomposes cache into shared and adapter-dependent components, reducing memory and compute overhead while maintaining accuracy.  					AI-generated summary 				 Role specialization in multi-LLM agent systems is often realized via multi-LoRA, where agents share a pretrained backbone and differ only through lightweight adapters. Despite sharing base model weights, each agent independently builds and stores its own KV cache for the same long, tool-augmented trajectories, incurring substantial memory and compute overhead. Existing KV cache sharing methods largely overlook this multi-LoRA setting. We observe that, across agents, cache differences are dominated by adapter outputs, while activations from the shared pretrained backbone remain highly similar. Based on this observation, we propose LRAgent, a KV cache sharing framework for multi-LoRA agents that decomposes the cache into a shared base component from the pretrained weights and an adapter-dependent component from LoRA weights. LRAgent reduces memory overhead by sharing the base component and storing the adapter component in its inherent low-rank form, and further reduces compute overhead, enabled by shared-A multi-LoRA architectures, by also sharing the low-rank cache and avoiding redundant computations for contexts already processed by other agents. To efficiently reconstruct adapter contributions at runtime, we introduce Flash-LoRA-Attention, a kernel that reorders attention computation to avoid materializing the low-rank cache to full dimension. LRAgent achieves throughput and time-to-first-token latency close to fully shared caching, while preserving accuracy near the non-shared caching baseline across agentic question-answering benchmarks."

[04.02.2026 07:41] Response: ```python
['AGENTS', 'INFERENCE', 'ARCHITECTURE']
```
[04.02.2026 07:41] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"LRAgent is a KV cache sharing framework for multi-LoRA agents that decomposes cache into shared and adapter-dependent components, reducing memory and compute overhead while maintaining accuracy.  					AI-generated summary 				 Role specialization in multi-LLM agent systems is often realized via multi-LoRA, where agents share a pretrained backbone and differ only through lightweight adapters. Despite sharing base model weights, each agent independently builds and stores its own KV cache for the same long, tool-augmented trajectories, incurring substantial memory and compute overhead. Existing KV cache sharing methods largely overlook this multi-LoRA setting. We observe that, across agents, cache differences are dominated by adapter outputs, while activations from the shared pretrained backbone remain highly similar. Based on this observation, we propose LRAgent, a KV cache sharing framework for multi-LoRA agents that decomposes the cache into a shared base component from the pretrained weights and an adapter-dependent component from LoRA weights. LRAgent reduces memory overhead by sharing the base component and storing the adapter component in its inherent low-rank form, and further reduces compute overhead, enabled by shared-A multi-LoRA architectures, by also sharing the low-rank cache and avoiding redundant computations for contexts already processed by other agents. To efficiently reconstruct adapter contributions at runtime, we introduce Flash-LoRA-Attention, a kernel that reorders attention computation to avoid materializing the low-rank cache to full dimension. LRAgent achieves throughput and time-to-first-token latency close to fully shared caching, while preserving accuracy near the non-shared caching baseline across agentic question-answering benchmarks."

[04.02.2026 07:41] Response: ```python
["OPTIMIZATION", "LONG_CONTEXT"]
```

**Justification:**

1. **OPTIMIZATION**: The paper focuses on optimizing training and inference efficiency through KV cache sharing, reducing memory and compute overhead while maintaining accuracy. This is a core optimization contribution.

2. **LONG_CONTEXT**: The paper explicitly mentions handling "long, tool-augmented trajectories" and addresses the challenges of processing long contexts efficiently in multi-agent systems, which is directly related to long context handling.
[04.02.2026 07:41] Error. Failed to parse JSON from LLM. ["OPTIMIZATION", "LONG_CONTEXT"]


**Justification:**

1. **OPTIMIZATION**: The paper focuses on optimizing training and inference efficiency through KV cache sharing, reducing memory and compute overhead while maintaining accuracy. This is a core optimization contribution.

2. **LONG_CONTEXT**: The paper explicitly mentions handling "long, tool-augmented trajectories" and addresses the challenges of processing long contexts efficiently in multi-agent systems, which is directly related to long context handling.
[04.02.2026 07:41] Response: ParsedChatCompletionMessage[Article](content='{"desc":"LRAgent is a framework designed to optimize memory and computational efficiency in multi-LoRA agent systems by sharing key-value (KV) caches. It separates the cache into two parts: a shared component derived from the pretrained backbone and an adapter-dependent component specific to each agent. This approach minimizes memory usage by allowing agents to share the base cache while storing only the necessary low-rank adapter components. Additionally, LRAgent employs a novel kernel called Flash-LoRA-Attention to streamline attention computations, resulting in faster processing times without sacrificing accuracy.","title":"Efficient Cache Sharing for Multi-LoRA Agents"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='LRAgent is a framework designed to optimize memory and computational efficiency in multi-LoRA agent systems by sharing key-value (KV) caches. It separates the cache into two parts: a shared component derived from the pretrained backbone and an adapter-dependent component specific to each agent. This approach minimizes memory usage by allowing agents to share the base cache while storing only the necessary low-rank adapter components. Additionally, LRAgent employs a novel kernel called Flash-LoRA-Attention to streamline attention computations, resulting in faster processing times without sacrificing accuracy.', title='Efficient Cache Sharing for Multi-LoRA Agents'))
[04.02.2026 07:41] Response: ParsedChatCompletionMessage[Article](content='{"desc":"LRAgent是一个用于多LoRA代理的KV缓存共享框架，它将缓存分解为共享组件和适配器依赖组件，从而减少内存和计算开销，同时保持准确性。该框架利用共享的预训练基础模型权重，降低了内存占用，并通过以低秩形式存储适配器组件，进一步减少计算开销。LRAgent还引入了Flash-LoRA-Attention内核，优化了注意力计算，避免了将低秩缓存扩展到全维度。通过这些创新，LRAgent在代理问答基准测试中实现了接近完全共享缓存的吞吐量和首次响应延迟，同时保持了接近非共享缓存的准确性。","title":"LRAgent：高效的多LoRA代理缓存共享框架"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='LRAgent是一个用于多LoRA代理的KV缓存共享框架，它将缓存分解为共享组件和适配器依赖组件，从而减少内存和计算开销，同时保持准确性。该框架利用共享的预训练基础模型权重，降低了内存占用，并通过以低秩形式存储适配器组件，进一步减少计算开销。LRAgent还引入了Flash-LoRA-Attention内核，优化了注意力计算，避免了将低秩缓存扩展到全维度。通过这些创新，LRAgent在代理问答基准测试中实现了接近完全共享缓存的吞吐量和首次响应延迟，同时保持了接近非共享缓存的准确性。', title='LRAgent：高效的多LoRA代理缓存共享框架'))
[04.02.2026 07:41] Using data from previous issue: {"categories": ["#reasoning", "#science", "#transfer_learning"], "emoji": "🤝", "ru": {"title": "AI как научный партнёр: от автоматизации к подлинному сотворчеству в математике", "desc": "В статье представлены примеры успешного сотрудничества исследователей с продвинутыми языковыми моделями, такими к
[04.02.2026 07:41] Using data from previous issue: {"categories": ["#training", "#rl", "#plp"], "emoji": "💻", "ru": {"title": "Эффективное обучение LLM через контекстные бандиты и оффлайн траектории", "desc": "В статье предлагается метод Cobalt, который объединяет обучение с подкреплением и контекстный бандит для улучшения многошагового генерировани
[04.02.2026 07:41] Using data from previous issue: {"categories": ["#rl", "#multimodal", "#benchmark", "#cv"], "emoji": "📸", "ru": {"title": "Персонализация зрения через контекстное обучение моделей vision-language", "desc": "В работе предложена система CoViP для контекстуализированной визуальной персонализации моделей зрения-языка. Основная идея за
[04.02.2026 07:41] Using data from previous issue: {"categories": ["#ethics", "#benchmark", "#cv", "#security"], "emoji": "🔓", "ru": {"title": "Пиксельные метрики не гарантируют приватность лиц", "desc": "В статье представлена атака FaceLinkGen, которая демонстрирует критический недостаток современных методов приватной распознавания лиц - они не защ
[04.02.2026 07:41] Using data from previous issue: {"categories": ["#reasoning", "#training", "#optimization", "#agents", "#rl", "#rag"], "emoji": "🔍", "ru": {"title": "Двухуровневая рефинировка рассуждений для точного распределения кредита в агентах поиска", "desc": "В статье представлена фреймворк Search-R2, которая улучшает рассуждение языковых а
[04.02.2026 07:41] Querying the API.
[04.02.2026 07:41] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Large Language Models have advanced general-purpose agents, but current evaluation benchmarks suffer from confounding factors and lack of standardization, necessitating a unified framework for rigorous assessment.  					AI-generated summary 				 With the advent of Large Language Models (LLMs), general-purpose agents have seen fundamental advancements. However, evaluating these agents presents unique challenges that distinguish them from static QA benchmarks. We observe that current agent benchmarks are heavily confounded by extraneous factors, including system prompts, toolset configurations, and environmental dynamics. Existing evaluations often rely on fragmented, researcher-specific frameworks where the prompt engineering for reasoning and tool usage varies significantly, making it difficult to attribute performance gains to the model itself. Additionally, the lack of standardized environmental data leads to untraceable errors and non-reproducible results. This lack of standardization introduces substantial unfairness and opacity into the field. We propose that a unified evaluation framework is essential for the rigorous advancement of agent evaluation. To this end, we introduce a proposal aimed at standardizing agent evaluation.
[04.02.2026 07:41] Response: ```json
{
  "desc": "В работе обсуждается проблема оценки возможностей автономных агентов на основе больших языковых моделей. Авторы отмечают, что существующие бенчмарки для оценки агентов содержат множество посторонних факторов, таких как системные промты, конфигурация инструментов и динамика окружения, что затрудняет объективное сравнение моделей. Текущие методики оценки используют разрозненные, специфичные для каждого исследователя фреймворки с различными подходами к инжинирингу промтов для рассуждений и использования инструментов, что приводит к невоспроизводимым результатам. Авторы предлагают создать унифицированный стандартизированный фреймворк для объективной и справедливой оценки производительности агентов на основе LLM.",
  "emoji": "⚖️",
  "title": "Единый стандарт для честной оценки агентов на основе LLM"
}
```
[04.02.2026 07:41] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Large Language Models have advanced general-purpose agents, but current evaluation benchmarks suffer from confounding factors and lack of standardization, necessitating a unified framework for rigorous assessment.  					AI-generated summary 				 With the advent of Large Language Models (LLMs), general-purpose agents have seen fundamental advancements. However, evaluating these agents presents unique challenges that distinguish them from static QA benchmarks. We observe that current agent benchmarks are heavily confounded by extraneous factors, including system prompts, toolset configurations, and environmental dynamics. Existing evaluations often rely on fragmented, researcher-specific frameworks where the prompt engineering for reasoning and tool usage varies significantly, making it difficult to attribute performance gains to the model itself. Additionally, the lack of standardized environmental data leads to untraceable errors and non-reproducible results. This lack of standardization introduces substantial unfairness and opacity into the field. We propose that a unified evaluation framework is essential for the rigorous advancement of agent evaluation. To this end, we introduce a proposal aimed at standardizing agent evaluation."

[04.02.2026 07:41] Response: ```python
["AGENTS", "BENCHMARK"]
```
[04.02.2026 07:41] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Large Language Models have advanced general-purpose agents, but current evaluation benchmarks suffer from confounding factors and lack of standardization, necessitating a unified framework for rigorous assessment.  					AI-generated summary 				 With the advent of Large Language Models (LLMs), general-purpose agents have seen fundamental advancements. However, evaluating these agents presents unique challenges that distinguish them from static QA benchmarks. We observe that current agent benchmarks are heavily confounded by extraneous factors, including system prompts, toolset configurations, and environmental dynamics. Existing evaluations often rely on fragmented, researcher-specific frameworks where the prompt engineering for reasoning and tool usage varies significantly, making it difficult to attribute performance gains to the model itself. Additionally, the lack of standardized environmental data leads to untraceable errors and non-reproducible results. This lack of standardization introduces substantial unfairness and opacity into the field. We propose that a unified evaluation framework is essential for the rigorous advancement of agent evaluation. To this end, we introduce a proposal aimed at standardizing agent evaluation."

[04.02.2026 07:41] Response: ```python
['REASONING']
```
[04.02.2026 07:41] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper discusses the challenges in evaluating Large Language Models (LLMs) used as general-purpose agents. It highlights that current benchmarks are affected by various confounding factors, such as different prompts and tool configurations, which complicate the assessment of model performance. The authors argue that the lack of a standardized evaluation framework leads to unfairness and non-reproducible results in the field. To address these issues, they propose a unified framework for rigorous and consistent evaluation of agent performance.","title":"Standardizing Evaluation for Fair AI Agents"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper discusses the challenges in evaluating Large Language Models (LLMs) used as general-purpose agents. It highlights that current benchmarks are affected by various confounding factors, such as different prompts and tool configurations, which complicate the assessment of model performance. The authors argue that the lack of a standardized evaluation framework leads to unfairness and non-reproducible results in the field. To address these issues, they propose a unified framework for rigorous and consistent evaluation of agent performance.', title='Standardizing Evaluation for Fair AI Agents'))
[04.02.2026 07:41] Response: ParsedChatCompletionMessage[Article](content='{"desc":"大型语言模型（LLMs）推动了通用智能体的进步，但当前的评估基准存在混淆因素和缺乏标准化的问题。这些评估面临的挑战与静态问答基准不同，主要受到系统提示、工具配置和环境动态等外部因素的影响。现有的评估方法往往依赖于分散的、研究者特定的框架，导致难以将性能提升归因于模型本身。此外，缺乏标准化的环境数据使得错误难以追踪，结果也难以重复。","title":"统一评估框架，推动智能体评估的严格进展"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='大型语言模型（LLMs）推动了通用智能体的进步，但当前的评估基准存在混淆因素和缺乏标准化的问题。这些评估面临的挑战与静态问答基准不同，主要受到系统提示、工具配置和环境动态等外部因素的影响。现有的评估方法往往依赖于分散的、研究者特定的框架，导致难以将性能提升归因于模型本身。此外，缺乏标准化的环境数据使得错误难以追踪，结果也难以重复。', title='统一评估框架，推动智能体评估的严格进展'))
[04.02.2026 07:41] Using data from previous issue: {"categories": ["#interpretability", "#multimodal", "#benchmark", "#cv"], "emoji": "🎯", "ru": {"title": "Объектные эмбеддинги для точного понимания деталей изображений", "desc": "ObjEmbed — это инновационная мультимодальная модель встраивания на основе большой языковой модели, которая разбивает изоб
[04.02.2026 07:41] Using data from previous issue: {"categories": ["#audio", "#multimodal", "#benchmark"], "emoji": "🐦", "ru": {"title": "Адаптивное слияние звука и контекста с контролируемой надёжностью", "desc": "Авторы представляют FINCH — фреймворк адаптивного слияния доказательств для классификации биоакустических сигналов, который объединяет п
[04.02.2026 07:41] Renaming data file.
[04.02.2026 07:41] Renaming previous data. hf_papers.json to ./d/2026-02-04.json
[04.02.2026 07:41] Saving new data file.
[04.02.2026 07:41] Generating page.
[04.02.2026 07:41] Renaming previous page.
[04.02.2026 07:41] Renaming previous data. index.html to ./d/2026-02-04.html
[04.02.2026 07:41] Writing result.
[04.02.2026 07:41] Renaming log file.
[04.02.2026 07:41] Renaming previous data. log.txt to ./logs/2026-02-04_last_log.txt
