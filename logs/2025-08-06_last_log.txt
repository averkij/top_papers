[06.08.2025 04:38] Read previous papers.
[06.08.2025 04:38] Generating top page (month).
[06.08.2025 04:38] Writing top page (month).
[06.08.2025 05:24] Read previous papers.
[06.08.2025 05:24] Get feed.
[06.08.2025 05:24] Get page data from previous paper. URL: https://huggingface.co/papers/2508.03694
[06.08.2025 05:24] Get page data from previous paper. URL: https://huggingface.co/papers/2508.03320
[06.08.2025 05:24] Get page data from previous paper. URL: https://huggingface.co/papers/2508.02193
[06.08.2025 05:24] Get page data from previous paper. URL: https://huggingface.co/papers/2508.03012
[06.08.2025 05:24] Get page data from previous paper. URL: https://huggingface.co/papers/2508.01780
[06.08.2025 05:24] Extract page data from URL. URL: https://huggingface.co/papers/2508.03050
[06.08.2025 05:24] Get page data from previous paper. URL: https://huggingface.co/papers/2508.02079
[06.08.2025 05:24] Get page data from previous paper. URL: https://huggingface.co/papers/2508.02063
[06.08.2025 05:24] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[06.08.2025 05:24] No deleted papers detected.
[06.08.2025 05:24] Downloading and parsing papers (pdf, html). Total: 8.
[06.08.2025 05:24] Downloading and parsing paper https://huggingface.co/papers/2508.03694.
[06.08.2025 05:24] Extra JSON file exists (./assets/json/2508.03694.json), skip PDF parsing.
[06.08.2025 05:24] Paper image links file exists (./assets/img_data/2508.03694.json), skip HTML parsing.
[06.08.2025 05:24] Success.
[06.08.2025 05:24] Downloading and parsing paper https://huggingface.co/papers/2508.03320.
[06.08.2025 05:24] Extra JSON file exists (./assets/json/2508.03320.json), skip PDF parsing.
[06.08.2025 05:24] Paper image links file exists (./assets/img_data/2508.03320.json), skip HTML parsing.
[06.08.2025 05:24] Success.
[06.08.2025 05:24] Downloading and parsing paper https://huggingface.co/papers/2508.02193.
[06.08.2025 05:24] Extra JSON file exists (./assets/json/2508.02193.json), skip PDF parsing.
[06.08.2025 05:24] Paper image links file exists (./assets/img_data/2508.02193.json), skip HTML parsing.
[06.08.2025 05:24] Success.
[06.08.2025 05:24] Downloading and parsing paper https://huggingface.co/papers/2508.03012.
[06.08.2025 05:24] Extra JSON file exists (./assets/json/2508.03012.json), skip PDF parsing.
[06.08.2025 05:24] Paper image links file exists (./assets/img_data/2508.03012.json), skip HTML parsing.
[06.08.2025 05:24] Success.
[06.08.2025 05:24] Downloading and parsing paper https://huggingface.co/papers/2508.01780.
[06.08.2025 05:24] Extra JSON file exists (./assets/json/2508.01780.json), skip PDF parsing.
[06.08.2025 05:24] Paper image links file exists (./assets/img_data/2508.01780.json), skip HTML parsing.
[06.08.2025 05:24] Success.
[06.08.2025 05:24] Downloading and parsing paper https://huggingface.co/papers/2508.03050.
[06.08.2025 05:24] Downloading paper 2508.03050 from http://arxiv.org/pdf/2508.03050v1...
[06.08.2025 05:24] Extracting affiliations from text.
[06.08.2025 05:24] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 5 ] . [ 1 0 5 0 3 0 . 8 0 5 2 : r Multi-human Interactive Talking Dataset Zeyu Zhu, Weijia Wu, Mike Zheng Shou(Q) Show Lab, National University of Singapore "
[06.08.2025 05:24] Response: ```python
["Show Lab, National University of Singapore"]
```
[06.08.2025 05:24] Deleting PDF ./assets/pdf/2508.03050.pdf.
[06.08.2025 05:24] Success.
[06.08.2025 05:24] Downloading and parsing paper https://huggingface.co/papers/2508.02079.
[06.08.2025 05:24] Extra JSON file exists (./assets/json/2508.02079.json), skip PDF parsing.
[06.08.2025 05:24] Paper image links file exists (./assets/img_data/2508.02079.json), skip HTML parsing.
[06.08.2025 05:24] Success.
[06.08.2025 05:24] Downloading and parsing paper https://huggingface.co/papers/2508.02063.
[06.08.2025 05:24] Extra JSON file exists (./assets/json/2508.02063.json), skip PDF parsing.
[06.08.2025 05:24] Paper image links file exists (./assets/img_data/2508.02063.json), skip HTML parsing.
[06.08.2025 05:24] Success.
[06.08.2025 05:24] Enriching papers with extra data.
[06.08.2025 05:24] ********************************************************************************
[06.08.2025 05:24] Abstract 0. LongVie, an end-to-end autoregressive framework, addresses temporal consistency and visual degradation in ultra-long video generation through unified noise initialization, global control signal normalization, multi-modal control, and degradation-aware training.  					AI-generated summary 				 Contro...
[06.08.2025 05:24] ********************************************************************************
[06.08.2025 05:24] Abstract 1. Skywork UniPic, a 1.5 billion-parameter autoregressive model, unifies image understanding, text-to-image generation, and image editing with state-of-the-art performance on commodity hardware.  					AI-generated summary 				 We introduce Skywork UniPic, a 1.5 billion-parameter autoregressive model th...
[06.08.2025 05:24] ********************************************************************************
[06.08.2025 05:24] Abstract 2. Seed Diffusion Preview, a discrete-state diffusion language model, achieves fast inference speeds through parallel generation, outperforming Mercury and Gemini Diffusion in speed and quality.  					AI-generated summary 				 We present Seed Diffusion Preview, a large-scale language model based on dis...
[06.08.2025 05:24] ********************************************************************************
[06.08.2025 05:24] Abstract 3. ToolTrain, a two-stage training framework combining supervised fine-tuning and reinforcement learning, enhances LLMs for issue localization by integrating repository retrieval tools, achieving state-of-the-art performance.  					AI-generated summary 				 Issue localization, the process of identifyin...
[06.08.2025 05:24] ********************************************************************************
[06.08.2025 05:24] Abstract 4. LiveMCPBench provides a comprehensive benchmark for evaluating LLM agents across a diverse set of real-world tasks in the MCP ecosystem, using a scalable evaluation pipeline and adaptive judging framework.  					AI-generated summary 				 With the rapid development of Model Context Protocol (MCP), th...
[06.08.2025 05:24] ********************************************************************************
[06.08.2025 05:24] Abstract 5. MIT, a large-scale dataset for multi-human talking video generation, includes fine-grained annotations and is used to demonstrate CovOG, a baseline model integrating a Multi-Human Pose Encoder and an Interactive Audio Driver.  					AI-generated summary 				 Existing studies on talking video generati...
[06.08.2025 05:24] ********************************************************************************
[06.08.2025 05:24] Abstract 6. AlignGuard-LoRA (AGL) is a framework that preserves alignment during fine-tuning of large language models by introducing regularization techniques and a diagnostic benchmark to mitigate alignment drift.  					AI-generated summary 				 Low-rank adaptation (LoRA) has become a standard tool for efficie...
[06.08.2025 05:24] ********************************************************************************
[06.08.2025 05:24] Abstract 7. TraceAlign is a framework that identifies and mitigates alignment drift in LLMs by tracing unsafe completions to their training sources and applying interventions to reduce drift while maintaining utility.  					AI-generated summary 				 Large Language Models (LLMs) fine-tuned to align with human va...
[06.08.2025 05:24] Read previous papers.
[06.08.2025 05:24] Generating reviews via LLM API.
[06.08.2025 05:24] Using data from previous issue: {"categories": ["#benchmark", "#synthetic", "#multimodal", "#long_context", "#video"], "emoji": "üé¨", "ru": {"title": "LongVie: –ø—Ä–æ—Ä—ã–≤ –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–≤–µ—Ä—Ö–¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∫–∞—á–µ—Å—Ç–≤–∞", "desc": "LongVie - —ç—Ç–æ –Ω–æ–≤–∞—è –∞–≤—Ç–æ—Ä–µ–≥—Ä–∞–¥–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–≤–µ—Ä—Ö–¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ. –û–Ω–∞ —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã –≤—Ä–µ–º–µ–Ω
[06.08.2025 05:24] Using data from previous issue: {"categories": ["#dataset", "#training", "#multimodal", "#architecture", "#open_source"], "emoji": "üñºÔ∏è", "ru": {"title": "–ï–¥–∏–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è, —Å–æ–∑–¥–∞–Ω–∏—è –∏ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "Skywork UniPic - —ç—Ç–æ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å —Å 1,5 –º–∏–ª–ª–∏–∞—Ä–¥–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∞—è –ø–æ–Ω–∏–º–∞–Ω–∏–µ –∏
[06.08.2025 05:24] Using data from previous issue: {"categories": ["#diffusion", "#inference", "#benchmark", "#architecture", "#optimization"], "emoji": "üöÄ", "ru": {"title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ —Å–∫–æ—Ä–æ—Å—Ç–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞", "desc": "Seed Diffusion Preview - —ç—Ç–æ –Ω–æ–≤–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å, –æ—Å–Ω–æ–≤–∞–Ω–Ω–∞—è –Ω–∞ –¥–∏—Å–∫—Ä–µ—Ç–Ω–æ–π –¥–∏—Ñ—Ñ—É–∑–∏–∏. –û–Ω–∞ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –æ—á–µ
[06.08.2025 05:24] Using data from previous issue: {"categories": ["#rl", "#training", "#reasoning", "#agents", "#optimization"], "emoji": "üîç", "ru": {"title": "ToolTrain: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–±–ª–µ–º –≤ –∫–æ–¥–µ —Å –ø–æ–º–æ—â—å—é –æ–±—É—á–µ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "ToolTrain - —ç—Ç–æ –¥–≤—É—Ö—ç—Ç–∞–ø–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∞—è –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º—É—é —Ç–æ–Ω–∫—É—é –Ω–∞—Å—Ç—Ä–æ–π–∫—É –∏
[06.08.2025 05:24] Using data from previous issue: {"categories": ["#open_source", "#survey", "#agents", "#benchmark"], "emoji": "ü§ñ", "ru": {"title": "LiveMCPBench: –ù–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –æ—Ü–µ–Ω–∫–∏ LLM-–∞–≥–µ–Ω—Ç–æ–≤ –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö MCP-—Å—Ä–µ–¥–∞—Ö", "desc": "LiveMCPBench –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≤ —Ä–µ–∞–ª
[06.08.2025 05:24] Querying the API.
[06.08.2025 05:24] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

MIT, a large-scale dataset for multi-human talking video generation, includes fine-grained annotations and is used to demonstrate CovOG, a baseline model integrating a Multi-Human Pose Encoder and an Interactive Audio Driver.  					AI-generated summary 				 Existing studies on talking video generation have predominantly focused on single-person monologues or isolated facial animations, limiting their applicability to realistic multi-human interactions. To bridge this gap, we introduce MIT, a large-scale dataset specifically designed for multi-human talking video generation. To this end, we develop an automatic pipeline that collects and annotates multi-person conversational videos. The resulting dataset comprises 12 hours of high-resolution footage, each featuring two to four speakers, with fine-grained annotations of body poses and speech interactions. It captures natural conversational dynamics in multi-speaker scenario, offering a rich resource for studying interactive visual behaviors. To demonstrate the potential of MIT, we furthur propose CovOG, a baseline model for this novel task. It integrates a Multi-Human Pose Encoder (MPE) to handle varying numbers of speakers by aggregating individual pose embeddings, and an Interactive Audio Driver (IAD) to modulate head dynamics based on speaker-specific audio features. Together, these components showcase the feasibility and challenges of generating realistic multi-human talking videos, establishing MIT as a valuable benchmark for future research. The code is avalibale at: https://github.com/showlab/Multi-human-Talking-Video-Dataset.
[06.08.2025 05:24] Response: {
  "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ MIT - –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ —Å —Ä–∞–∑–≥–æ–≤–æ—Ä–∞–º–∏ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ª—é–¥–µ–π. –≠—Ç–æ—Ç –¥–∞—Ç–∞—Å–µ—Ç –≤–∫–ª—é—á–∞–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—ã–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ CovOG - –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏, –æ–±—ä–µ–¥–∏–Ω—è—é—â–µ–π –∫–æ–¥–∏—Ä–æ–≤—â–∏–∫ –ø–æ–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ª—é–¥–µ–π –∏ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π –∞—É–¥–∏–æ–¥—Ä–∞–π–≤–µ—Ä. MIT —Å–æ–¥–µ—Ä–∂–∏—Ç 12 —á–∞—Å–æ–≤ –≤–∏–¥–µ–æ –≤—ã—Å–æ–∫–æ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è —Å 2-4 –≥–æ–≤–æ—Ä—è—â–∏–º–∏ –∏ –¥–µ—Ç–∞–ª—å–Ω—ã–º–∏ –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è–º–∏ –ø–æ–∑ —Ç–µ–ª–∞ –∏ —Ä–µ—á–µ–≤—ã—Ö –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π. CovOG –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∞–≥—Ä–µ–≥–∞—Ü–∏—é –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –ø–æ–∑ –∏ –º–æ–¥—É–ª—è—Ü–∏—é –¥–∏–Ω–∞–º–∏–∫–∏ –≥–æ–ª–æ–≤—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞—É–¥–∏–æ–ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∫–∞–∂–¥–æ–≥–æ –≥–æ–≤–æ—Ä—è—â–µ–≥–æ.",
  "emoji": "üó£Ô∏è",
  "title": "–ù–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç –∏ –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ —Å —Ä–∞–∑–≥–æ–≤–æ—Ä–∞–º–∏ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ª—é–¥–µ–π"
}
[06.08.2025 05:24] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"MIT, a large-scale dataset for multi-human talking video generation, includes fine-grained annotations and is used to demonstrate CovOG, a baseline model integrating a Multi-Human Pose Encoder and an Interactive Audio Driver.  					AI-generated summary 				 Existing studies on talking video generation have predominantly focused on single-person monologues or isolated facial animations, limiting their applicability to realistic multi-human interactions. To bridge this gap, we introduce MIT, a large-scale dataset specifically designed for multi-human talking video generation. To this end, we develop an automatic pipeline that collects and annotates multi-person conversational videos. The resulting dataset comprises 12 hours of high-resolution footage, each featuring two to four speakers, with fine-grained annotations of body poses and speech interactions. It captures natural conversational dynamics in multi-speaker scenario, offering a rich resource for studying interactive visual behaviors. To demonstrate the potential of MIT, we furthur propose CovOG, a baseline model for this novel task. It integrates a Multi-Human Pose Encoder (MPE) to handle varying numbers of speakers by aggregating individual pose embeddings, and an Interactive Audio Driver (IAD) to modulate head dynamics based on speaker-specific audio features. Together, these components showcase the feasibility and challenges of generating realistic multi-human talking videos, establishing MIT as a valuable benchmark for future research. The code is avalibale at: https://github.com/showlab/Multi-human-Talking-Video-Dataset."

[06.08.2025 05:24] Response: ```python
['DATASET', 'BENCHMARK', 'CV']
```
[06.08.2025 05:24] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"MIT, a large-scale dataset for multi-human talking video generation, includes fine-grained annotations and is used to demonstrate CovOG, a baseline model integrating a Multi-Human Pose Encoder and an Interactive Audio Driver.  					AI-generated summary 				 Existing studies on talking video generation have predominantly focused on single-person monologues or isolated facial animations, limiting their applicability to realistic multi-human interactions. To bridge this gap, we introduce MIT, a large-scale dataset specifically designed for multi-human talking video generation. To this end, we develop an automatic pipeline that collects and annotates multi-person conversational videos. The resulting dataset comprises 12 hours of high-resolution footage, each featuring two to four speakers, with fine-grained annotations of body poses and speech interactions. It captures natural conversational dynamics in multi-speaker scenario, offering a rich resource for studying interactive visual behaviors. To demonstrate the potential of MIT, we furthur propose CovOG, a baseline model for this novel task. It integrates a Multi-Human Pose Encoder (MPE) to handle varying numbers of speakers by aggregating individual pose embeddings, and an Interactive Audio Driver (IAD) to modulate head dynamics based on speaker-specific audio features. Together, these components showcase the feasibility and challenges of generating realistic multi-human talking videos, establishing MIT as a valuable benchmark for future research. The code is avalibale at: https://github.com/showlab/Multi-human-Talking-Video-Dataset."

[06.08.2025 05:24] Response: ```python
[]
```
[06.08.2025 05:24] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The paper introduces MIT, a large-scale dataset aimed at generating multi-human talking videos, which includes detailed annotations for body poses and speech interactions. This dataset addresses the limitations of previous studies that focused mainly on single-person videos, making it more applicable to real-life conversations. To utilize this dataset, the authors propose CovOG, a baseline model that combines a Multi-Human Pose Encoder to manage multiple speakers and an Interactive Audio Driver to synchronize head movements with audio cues. This work not only demonstrates the potential of generating realistic multi-human interactions but also sets a new benchmark for future research in this area.","title":"MIT: Pioneering Multi-Human Talking Video Generation"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='The paper introduces MIT, a large-scale dataset aimed at generating multi-human talking videos, which includes detailed annotations for body poses and speech interactions. This dataset addresses the limitations of previous studies that focused mainly on single-person videos, making it more applicable to real-life conversations. To utilize this dataset, the authors propose CovOG, a baseline model that combines a Multi-Human Pose Encoder to manage multiple speakers and an Interactive Audio Driver to synchronize head movements with audio cues. This work not only demonstrates the potential of generating realistic multi-human interactions but also sets a new benchmark for future research in this area.', title='MIT: Pioneering Multi-Human Talking Video Generation'))
[06.08.2025 05:24] Response: ParsedChatCompletionMessage[Article](content='{"desc":"MITÊòØ‰∏Ä‰∏™Â§ßËßÑÊ®°ÁöÑÊï∞ÊçÆÈõÜÔºå‰∏ìÈó®Áî®‰∫éÂ§ö‰∫∫ÁöÑÂØπËØùËßÜÈ¢ëÁîüÊàêÔºåÂåÖÂê´ÁªÜËá¥ÁöÑÊ≥®Èáä‰ø°ÊÅØ„ÄÇÁé∞ÊúâÁöÑÁ†îÁ©∂‰∏ªË¶ÅÈõÜ‰∏≠Âú®Âçï‰∫∫Áã¨ÁôΩÊàñÂ≠§Á´ãÁöÑÈù¢ÈÉ®Âä®Áîª‰∏äÔºåÈôêÂà∂‰∫ÜÂÖ∂Âú®ÁúüÂÆûÂ§ö‰∫∫ÁöÑ‰∫íÂä®‰∏≠ÁöÑÂ∫îÁî®„ÄÇÊàë‰ª¨ÂºÄÂèë‰∫Ü‰∏Ä‰∏™Ëá™Âä®ÂåñÊµÅÁ®ãÔºåÊî∂ÈõÜÂíåÊ≥®ÈáäÂ§ö‰∫∫ÁöÑÂØπËØùËßÜÈ¢ëÔºåÊï∞ÊçÆÈõÜÂåÖÂê´12Â∞èÊó∂ÁöÑÈ´òÂàÜËæ®ÁéáËßÜÈ¢ëÔºåÂ±ïÁ§∫‰∫ÜËá™ÁÑ∂ÁöÑÂØπËØùÂä®ÊÄÅ„ÄÇ‰∏∫‰∫ÜÂ±ïÁ§∫MITÁöÑÊΩúÂäõÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜCovOGÊ®°ÂûãÔºåÁªìÂêà‰∫ÜÂ§ö‰∫∫ÁöÑÂßøÊÄÅÁºñÁ†ÅÂô®Âíå‰∫íÂä®Èü≥È¢ëÈ©±Âä®Âô®ÔºåÂ±ïÁ§∫‰∫ÜÁîüÊàêÁúüÂÆûÂ§ö‰∫∫ÁöÑÂØπËØùËßÜÈ¢ëÁöÑÂèØË°åÊÄßÂíåÊåëÊàò„ÄÇ","title":"MITÔºöÂ§ö‰∫∫‰∫∫ÂØπËØùËßÜÈ¢ëÁîüÊàêÁöÑÊñ∞Âü∫ÂáÜ"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='MITÊòØ‰∏Ä‰∏™Â§ßËßÑÊ®°ÁöÑÊï∞ÊçÆÈõÜÔºå‰∏ìÈó®Áî®‰∫éÂ§ö‰∫∫ÁöÑÂØπËØùËßÜÈ¢ëÁîüÊàêÔºåÂåÖÂê´ÁªÜËá¥ÁöÑÊ≥®Èáä‰ø°ÊÅØ„ÄÇÁé∞ÊúâÁöÑÁ†îÁ©∂‰∏ªË¶ÅÈõÜ‰∏≠Âú®Âçï‰∫∫Áã¨ÁôΩÊàñÂ≠§Á´ãÁöÑÈù¢ÈÉ®Âä®Áîª‰∏äÔºåÈôêÂà∂‰∫ÜÂÖ∂Âú®ÁúüÂÆûÂ§ö‰∫∫ÁöÑ‰∫íÂä®‰∏≠ÁöÑÂ∫îÁî®„ÄÇÊàë‰ª¨ÂºÄÂèë‰∫Ü‰∏Ä‰∏™Ëá™Âä®ÂåñÊµÅÁ®ãÔºåÊî∂ÈõÜÂíåÊ≥®ÈáäÂ§ö‰∫∫ÁöÑÂØπËØùËßÜÈ¢ëÔºåÊï∞ÊçÆÈõÜÂåÖÂê´12Â∞èÊó∂ÁöÑÈ´òÂàÜËæ®ÁéáËßÜÈ¢ëÔºåÂ±ïÁ§∫‰∫ÜËá™ÁÑ∂ÁöÑÂØπËØùÂä®ÊÄÅ„ÄÇ‰∏∫‰∫ÜÂ±ïÁ§∫MITÁöÑÊΩúÂäõÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜCovOGÊ®°ÂûãÔºåÁªìÂêà‰∫ÜÂ§ö‰∫∫ÁöÑÂßøÊÄÅÁºñÁ†ÅÂô®Âíå‰∫íÂä®Èü≥È¢ëÈ©±Âä®Âô®ÔºåÂ±ïÁ§∫‰∫ÜÁîüÊàêÁúüÂÆûÂ§ö‰∫∫ÁöÑÂØπËØùËßÜÈ¢ëÁöÑÂèØË°åÊÄßÂíåÊåëÊàò„ÄÇ', title='MITÔºöÂ§ö‰∫∫‰∫∫ÂØπËØùËßÜÈ¢ëÁîüÊàêÁöÑÊñ∞Âü∫ÂáÜ'))
[06.08.2025 05:24] Using data from previous issue: {"categories": ["#benchmark", "#dataset", "#alignment", "#open_source", "#training"], "emoji": "üõ°Ô∏è", "ru": {"title": "–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –ø—Ä–∏ –¥–æ–æ–±—É—á–µ–Ω–∏–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "AlignGuard-LoRA (AGL) - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –ø—Ä–∏ –¥–æ–æ–±—É—á–µ–Ω–∏–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –û–Ω –≤–≤–æ–¥–∏
[06.08.2025 05:24] Using data from previous issue: {"categories": ["#security", "#benchmark", "#rlhf", "#alignment", "#open_source", "#training"], "emoji": "üõ°Ô∏è", "ru": {"title": "TraceAlign: –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –∏ —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –¥—Ä–µ–π—Ñ–∞ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "TraceAlign - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è –∏ —Å–Ω–∏–∂–µ–Ω–∏—è –¥—Ä–µ–π—Ñ–∞ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –≤ –±–æ
[06.08.2025 05:24] Renaming data file.
[06.08.2025 05:24] Renaming previous data. hf_papers.json to ./d/2025-08-06.json
[06.08.2025 05:24] Saving new data file.
[06.08.2025 05:24] Generating page.
[06.08.2025 05:24] Renaming previous page.
[06.08.2025 05:24] Renaming previous data. index.html to ./d/2025-08-06.html
[06.08.2025 05:24] Writing result.
[06.08.2025 05:24] Renaming log file.
[06.08.2025 05:24] Renaming previous data. log.txt to ./logs/2025-08-06_last_log.txt
