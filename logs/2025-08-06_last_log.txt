[05.08.2025 23:12] Read previous papers.
[05.08.2025 23:12] Generating top page (month).
[05.08.2025 23:12] Writing top page (month).
[06.08.2025 01:01] Read previous papers.
[06.08.2025 01:01] Get feed.
[06.08.2025 01:01] Get page data from previous paper. URL: https://huggingface.co/papers/2508.02324
[06.08.2025 01:01] Get page data from previous paper. URL: https://huggingface.co/papers/2508.01959
[06.08.2025 01:01] Get page data from previous paper. URL: https://huggingface.co/papers/2508.02276
[06.08.2025 01:01] Get page data from previous paper. URL: https://huggingface.co/papers/2508.02150
[06.08.2025 01:01] Get page data from previous paper. URL: https://huggingface.co/papers/2508.01059
[06.08.2025 01:01] Get page data from previous paper. URL: https://huggingface.co/papers/2508.02317
[06.08.2025 01:01] Get page data from previous paper. URL: https://huggingface.co/papers/2508.02137
[06.08.2025 01:01] Get page data from previous paper. URL: https://huggingface.co/papers/2507.17520
[06.08.2025 01:01] Get page data from previous paper. URL: https://huggingface.co/papers/2508.02271
[06.08.2025 01:01] Get page data from previous paper. URL: https://huggingface.co/papers/2508.01548
[06.08.2025 01:01] Get page data from previous paper. URL: https://huggingface.co/papers/2508.01691
[06.08.2025 01:01] Get page data from previous paper. URL: https://huggingface.co/papers/2508.01151
[06.08.2025 01:01] Get page data from previous paper. URL: https://huggingface.co/papers/2508.02558
[06.08.2025 01:01] Get page data from previous paper. URL: https://huggingface.co/papers/2508.01415
[06.08.2025 01:01] Get page data from previous paper. URL: https://huggingface.co/papers/2508.01408
[06.08.2025 01:01] Get page data from previous paper. URL: https://huggingface.co/papers/2508.01287
[06.08.2025 01:01] Get page data from previous paper. URL: https://huggingface.co/papers/2508.00910
[06.08.2025 01:01] Get page data from previous paper. URL: https://huggingface.co/papers/2508.00024
[06.08.2025 01:01] Get page data from previous paper. URL: https://huggingface.co/papers/2508.00890
[06.08.2025 01:01] Get page data from previous paper. URL: https://huggingface.co/papers/2508.02605
[06.08.2025 01:01] Get page data from previous paper. URL: https://huggingface.co/papers/2508.02268
[06.08.2025 01:01] Get page data from previous paper. URL: https://huggingface.co/papers/2508.01109
[06.08.2025 01:01] Get page data from previous paper. URL: https://huggingface.co/papers/2508.01773
[06.08.2025 01:01] Get page data from previous paper. URL: https://huggingface.co/papers/2507.16290
[06.08.2025 01:01] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[06.08.2025 01:01] No deleted papers detected.
[06.08.2025 01:01] Downloading and parsing papers (pdf, html). Total: 24.
[06.08.2025 01:01] Downloading and parsing paper https://huggingface.co/papers/2508.02324.
[06.08.2025 01:01] Extra JSON file exists (./assets/json/2508.02324.json), skip PDF parsing.
[06.08.2025 01:01] Paper image links file exists (./assets/img_data/2508.02324.json), skip HTML parsing.
[06.08.2025 01:01] Success.
[06.08.2025 01:01] Downloading and parsing paper https://huggingface.co/papers/2508.01959.
[06.08.2025 01:01] Extra JSON file exists (./assets/json/2508.01959.json), skip PDF parsing.
[06.08.2025 01:01] Paper image links file exists (./assets/img_data/2508.01959.json), skip HTML parsing.
[06.08.2025 01:01] Success.
[06.08.2025 01:01] Downloading and parsing paper https://huggingface.co/papers/2508.02276.
[06.08.2025 01:01] Extra JSON file exists (./assets/json/2508.02276.json), skip PDF parsing.
[06.08.2025 01:01] Paper image links file exists (./assets/img_data/2508.02276.json), skip HTML parsing.
[06.08.2025 01:01] Success.
[06.08.2025 01:01] Downloading and parsing paper https://huggingface.co/papers/2508.02150.
[06.08.2025 01:01] Extra JSON file exists (./assets/json/2508.02150.json), skip PDF parsing.
[06.08.2025 01:01] Paper image links file exists (./assets/img_data/2508.02150.json), skip HTML parsing.
[06.08.2025 01:01] Success.
[06.08.2025 01:01] Downloading and parsing paper https://huggingface.co/papers/2508.01059.
[06.08.2025 01:01] Extra JSON file exists (./assets/json/2508.01059.json), skip PDF parsing.
[06.08.2025 01:01] Paper image links file exists (./assets/img_data/2508.01059.json), skip HTML parsing.
[06.08.2025 01:01] Success.
[06.08.2025 01:01] Downloading and parsing paper https://huggingface.co/papers/2508.02317.
[06.08.2025 01:01] Extra JSON file exists (./assets/json/2508.02317.json), skip PDF parsing.
[06.08.2025 01:01] Paper image links file exists (./assets/img_data/2508.02317.json), skip HTML parsing.
[06.08.2025 01:01] Success.
[06.08.2025 01:01] Downloading and parsing paper https://huggingface.co/papers/2508.02137.
[06.08.2025 01:01] Extra JSON file exists (./assets/json/2508.02137.json), skip PDF parsing.
[06.08.2025 01:01] Paper image links file exists (./assets/img_data/2508.02137.json), skip HTML parsing.
[06.08.2025 01:01] Success.
[06.08.2025 01:01] Downloading and parsing paper https://huggingface.co/papers/2507.17520.
[06.08.2025 01:01] Extra JSON file exists (./assets/json/2507.17520.json), skip PDF parsing.
[06.08.2025 01:01] Paper image links file exists (./assets/img_data/2507.17520.json), skip HTML parsing.
[06.08.2025 01:01] Success.
[06.08.2025 01:01] Downloading and parsing paper https://huggingface.co/papers/2508.02271.
[06.08.2025 01:01] Extra JSON file exists (./assets/json/2508.02271.json), skip PDF parsing.
[06.08.2025 01:01] Paper image links file exists (./assets/img_data/2508.02271.json), skip HTML parsing.
[06.08.2025 01:01] Success.
[06.08.2025 01:01] Downloading and parsing paper https://huggingface.co/papers/2508.01548.
[06.08.2025 01:01] Extra JSON file exists (./assets/json/2508.01548.json), skip PDF parsing.
[06.08.2025 01:01] Paper image links file exists (./assets/img_data/2508.01548.json), skip HTML parsing.
[06.08.2025 01:01] Success.
[06.08.2025 01:01] Downloading and parsing paper https://huggingface.co/papers/2508.01691.
[06.08.2025 01:01] Extra JSON file exists (./assets/json/2508.01691.json), skip PDF parsing.
[06.08.2025 01:01] Paper image links file exists (./assets/img_data/2508.01691.json), skip HTML parsing.
[06.08.2025 01:01] Success.
[06.08.2025 01:01] Downloading and parsing paper https://huggingface.co/papers/2508.01151.
[06.08.2025 01:01] Extra JSON file exists (./assets/json/2508.01151.json), skip PDF parsing.
[06.08.2025 01:01] Paper image links file exists (./assets/img_data/2508.01151.json), skip HTML parsing.
[06.08.2025 01:01] Success.
[06.08.2025 01:01] Downloading and parsing paper https://huggingface.co/papers/2508.02558.
[06.08.2025 01:01] Extra JSON file exists (./assets/json/2508.02558.json), skip PDF parsing.
[06.08.2025 01:01] Paper image links file exists (./assets/img_data/2508.02558.json), skip HTML parsing.
[06.08.2025 01:01] Success.
[06.08.2025 01:01] Downloading and parsing paper https://huggingface.co/papers/2508.01415.
[06.08.2025 01:01] Extra JSON file exists (./assets/json/2508.01415.json), skip PDF parsing.
[06.08.2025 01:01] Paper image links file exists (./assets/img_data/2508.01415.json), skip HTML parsing.
[06.08.2025 01:01] Success.
[06.08.2025 01:01] Downloading and parsing paper https://huggingface.co/papers/2508.01408.
[06.08.2025 01:01] Extra JSON file exists (./assets/json/2508.01408.json), skip PDF parsing.
[06.08.2025 01:01] Paper image links file exists (./assets/img_data/2508.01408.json), skip HTML parsing.
[06.08.2025 01:01] Success.
[06.08.2025 01:01] Downloading and parsing paper https://huggingface.co/papers/2508.01287.
[06.08.2025 01:01] Extra JSON file exists (./assets/json/2508.01287.json), skip PDF parsing.
[06.08.2025 01:01] Paper image links file exists (./assets/img_data/2508.01287.json), skip HTML parsing.
[06.08.2025 01:01] Success.
[06.08.2025 01:01] Downloading and parsing paper https://huggingface.co/papers/2508.00910.
[06.08.2025 01:01] Extra JSON file exists (./assets/json/2508.00910.json), skip PDF parsing.
[06.08.2025 01:01] Paper image links file exists (./assets/img_data/2508.00910.json), skip HTML parsing.
[06.08.2025 01:01] Success.
[06.08.2025 01:01] Downloading and parsing paper https://huggingface.co/papers/2508.00024.
[06.08.2025 01:01] Extra JSON file exists (./assets/json/2508.00024.json), skip PDF parsing.
[06.08.2025 01:01] Paper image links file exists (./assets/img_data/2508.00024.json), skip HTML parsing.
[06.08.2025 01:01] Success.
[06.08.2025 01:01] Downloading and parsing paper https://huggingface.co/papers/2508.00890.
[06.08.2025 01:01] Extra JSON file exists (./assets/json/2508.00890.json), skip PDF parsing.
[06.08.2025 01:01] Paper image links file exists (./assets/img_data/2508.00890.json), skip HTML parsing.
[06.08.2025 01:01] Success.
[06.08.2025 01:01] Downloading and parsing paper https://huggingface.co/papers/2508.02605.
[06.08.2025 01:01] Extra JSON file exists (./assets/json/2508.02605.json), skip PDF parsing.
[06.08.2025 01:01] Paper image links file exists (./assets/img_data/2508.02605.json), skip HTML parsing.
[06.08.2025 01:01] Success.
[06.08.2025 01:01] Downloading and parsing paper https://huggingface.co/papers/2508.02268.
[06.08.2025 01:01] Extra JSON file exists (./assets/json/2508.02268.json), skip PDF parsing.
[06.08.2025 01:01] Paper image links file exists (./assets/img_data/2508.02268.json), skip HTML parsing.
[06.08.2025 01:01] Success.
[06.08.2025 01:01] Downloading and parsing paper https://huggingface.co/papers/2508.01109.
[06.08.2025 01:01] Extra JSON file exists (./assets/json/2508.01109.json), skip PDF parsing.
[06.08.2025 01:01] Paper image links file exists (./assets/img_data/2508.01109.json), skip HTML parsing.
[06.08.2025 01:01] Success.
[06.08.2025 01:01] Downloading and parsing paper https://huggingface.co/papers/2508.01773.
[06.08.2025 01:01] Extra JSON file exists (./assets/json/2508.01773.json), skip PDF parsing.
[06.08.2025 01:01] Paper image links file exists (./assets/img_data/2508.01773.json), skip HTML parsing.
[06.08.2025 01:01] Success.
[06.08.2025 01:01] Downloading and parsing paper https://huggingface.co/papers/2507.16290.
[06.08.2025 01:01] Extra JSON file exists (./assets/json/2507.16290.json), skip PDF parsing.
[06.08.2025 01:01] Paper image links file exists (./assets/img_data/2507.16290.json), skip HTML parsing.
[06.08.2025 01:01] Success.
[06.08.2025 01:01] Enriching papers with extra data.
[06.08.2025 01:01] ********************************************************************************
[06.08.2025 01:01] Abstract 0. Qwen-Image, an image generation model, advances text rendering and image editing through a comprehensive data pipeline, progressive training, and dual-encoding mechanism.  					AI-generated summary 				 We present Qwen-Image, an image generation foundation model in the Qwen series that achieves sign...
[06.08.2025 01:01] ********************************************************************************
[06.08.2025 01:01] Abstract 1. A new training paradigm and situated embedding models (SitEmb) enhance retrieval performance by conditioning short text chunks on broader context windows, outperforming state-of-the-art models with fewer parameters.  					AI-generated summary 				 Retrieval-augmented generation (RAG) over long docum...
[06.08.2025 01:01] ********************************************************************************
[06.08.2025 01:01] Abstract 2. CellForge, an agentic system using a multi-agent framework, transforms raw single-cell multi-omics data into optimized computational models for virtual cells, outperforming state-of-the-art methods in single-cell perturbation prediction.  					AI-generated summary 				 Virtual cell modeling represen...
[06.08.2025 01:01] ********************************************************************************
[06.08.2025 01:01] Abstract 3. A self-supervised RL framework enhances instruction following in reasoning models without external supervision, maintaining reasoning performance and offering scalability and cost-effectiveness.  					AI-generated summary 				 Reasoning models excel in complex problem solving but exhibit a concernin...
[06.08.2025 01:01] ********************************************************************************
[06.08.2025 01:01] Abstract 4. Foundation-Sec-8B-Instruct is a cybersecurity-focused LLM designed for chat-style interactions and instruction-following, outperforming other models in cybersecurity tasks while matching their instruction-following capabilities.  					AI-generated summary 				 Large language models (LLMs) have shown...
[06.08.2025 01:01] ********************************************************************************
[06.08.2025 01:01] Abstract 5. A modular training framework accelerates the development of omni-modal LLMs through efficient 3D parallelism and flexible configuration.  					AI-generated summary 				 Recent advances in large language models (LLMs) have driven impressive progress in omni-modal understanding and generation. However...
[06.08.2025 01:01] ********************************************************************************
[06.08.2025 01:01] Abstract 6. AuroBind is a scalable virtual screening framework that fine-tunes atomic-level structural models to predict ligand-bound structures and binding fitness, achieving high hit rates in prospective screens across disease-relevant targets.  					AI-generated summary 				 Most human proteins remain undrug...
[06.08.2025 01:01] ********************************************************************************
[06.08.2025 01:01] Abstract 7. InstructVLA is an end-to-end vision-language-action model that enhances manipulation performance while preserving vision-language reasoning through multimodal training and mixture-of-experts adaptation.  					AI-generated summary 				 To operate effectively in the real world, robots must integrate m...
[06.08.2025 01:01] ********************************************************************************
[06.08.2025 01:01] Abstract 8. A framework called Dynaword and its implementation Danish Dynaword enable community-driven, open, and continuously updated large-scale natural language datasets.  					AI-generated summary 				 Large-scale datasets are foundational for research and development in natural language processing. However...
[06.08.2025 01:01] ********************************************************************************
[06.08.2025 01:01] Abstract 9. A dynamic pruning framework, GlimpsePrune, improves efficiency in Large Vision-Language Models by adaptively removing irrelevant visual tokens without degrading performance.  					AI-generated summary 				 Visual token compression is critical for Large Vision-Language Models (LVLMs) to efficiently p...
[06.08.2025 01:01] ********************************************************************************
[06.08.2025 01:01] Abstract 10. Voxlect is a benchmark for evaluating speech foundation models on dialect classification and downstream applications across multiple languages and dialects.  					AI-generated summary 				 We present Voxlect, a novel benchmark for modeling dialects and regional languages worldwide using speech found...
[06.08.2025 01:01] ********************************************************************************
[06.08.2025 01:01] Abstract 11. A personalized safety alignment framework integrates user-specific profiles into text-to-image diffusion models to better align generated content with individual safety preferences.  					AI-generated summary 				 Text-to-image diffusion models have revolutionized visual content generation, but curr...
[06.08.2025 01:01] ********************************************************************************
[06.08.2025 01:01] Abstract 12. Sparse-dLLM improves the efficiency of diffusion large language models by implementing dynamic cache eviction and sparse attention, enhancing throughput without compromising performance.  					AI-generated summary 				 Diffusion Large Language Models (dLLMs) enable breakthroughs in reasoning and par...
[06.08.2025 01:01] ********************************************************************************
[06.08.2025 01:01] Abstract 13. RoboMemory, a brain-inspired multi-memory framework, enhances lifelong learning in physical robots by integrating cognitive neuroscience principles and achieving state-of-the-art performance in real-world tasks.  					AI-generated summary 				 We present RoboMemory, a brain-inspired multi-memory fra...
[06.08.2025 01:01] ********************************************************************************
[06.08.2025 01:01] Abstract 14. State-of-the-art vision language models struggle with accurately attributing artists and distinguishing AI-generated images, highlighting the need for improvement to prevent misinformation.  					AI-generated summary 				 The attribution of artworks in general and of paintings in particular has alwa...
[06.08.2025 01:01] ********************************************************************************
[06.08.2025 01:01] Abstract 15. Meta-reinforcement learning agents can exhibit exploratory behavior when trained with a greedy objective, provided the environment has recurring structure, the agent has memory, and long-horizon credit assignment is possible.  					AI-generated summary 				 Ensuring sufficient exploration is a centr...
[06.08.2025 01:01] ********************************************************************************
[06.08.2025 01:01] Abstract 16. Cyber-Zero synthesizes agent trajectories from CTF writeups to train runtime-free cybersecurity LLMs, achieving state-of-the-art performance on benchmarks.  					AI-generated summary 				 Large Language Models (LLMs) have achieved remarkable success in software engineering tasks when trained with ex...
[06.08.2025 01:01] ********************************************************************************
[06.08.2025 01:01] Abstract 17. Combining Vision Transformer embeddings with quantum-classical pipelines achieves quantum advantage in classification tasks, demonstrating the importance of embedding choice in quantum machine learning.  					AI-generated summary 				 Quantum Support Vector Machines face scalability challenges due t...
[06.08.2025 01:01] ********************************************************************************
[06.08.2025 01:01] Abstract 18. AgentTTS, an LLM-agent-based framework, optimizes compute allocation for multi-stage complex tasks, improving performance and robustness compared to traditional methods.  					AI-generated summary 				 Test-time scaling (TTS) enhances the performance of large language models (LLMs) by allocating add...
[06.08.2025 01:01] ********************************************************************************
[06.08.2025 01:01] Abstract 19. ReMoMask, a unified framework, addresses limitations in text-to-motion generation by integrating a Bidirectional Momentum Text-Motion Model, Semantic Spatio-temporal Attention, and RAG-Classier-Free Guidance, achieving state-of-the-art performance on HumanML3D and KIT-ML benchmarks.  					AI-generat...
[06.08.2025 01:01] ********************************************************************************
[06.08.2025 01:01] Abstract 20. A bidirectional machine translation system, SHAMI-MT, bridges the gap between Modern Standard Arabic and the Syrian dialect using AraT5v2-base-1024 architecture, achieving high-quality translations.  					AI-generated summary 				 The rich linguistic landscape of the Arab world is characterized by a...
[06.08.2025 01:01] ********************************************************************************
[06.08.2025 01:01] Abstract 21. A multimodal framework using satellite imagery and text data outperforms vision-only models in predicting household wealth, with LLM-generated text proving more effective than agent-retrieved text.  					AI-generated summary 				 We investigate whether socio-economic indicators like household wealth...
[06.08.2025 01:01] ********************************************************************************
[06.08.2025 01:01] Abstract 22. An uncertainty-driven framework for automated process reward data construction and aggregation methods improves the effectiveness and efficiency of Process-Level Reward Models in mathematical reasoning tasks.  					AI-generated summary 				 Large language models have demonstrated remarkable capabili...
[06.08.2025 01:01] ********************************************************************************
[06.08.2025 01:01] Abstract 23. Dens3R is a 3D foundation model that jointly predicts multiple geometric quantities using a two-stage training framework, enhancing consistency and performance in dense 3D reconstruction tasks.  					AI-generated summary 				 Recent advances in dense 3D reconstruction have led to significant progres...
[06.08.2025 01:01] Read previous papers.
[06.08.2025 01:01] Generating reviews via LLM API.
[06.08.2025 01:01] Using data from previous issue: {"categories": ["#optimization", "#dataset", "#data", "#games", "#cv", "#training", "#multimodal", "#benchmark"], "emoji": "üé®", "ru": {"title": "Qwen-Image: –Ω–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å —Ç–µ–∫—Å—Ç–æ–º", "desc": "Qwen-Image - —ç—Ç–æ –º–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä–∞—è –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –∑–Ω–∞—á
[06.08.2025 01:01] Using data from previous issue: {"categories": ["#dataset", "#multilingual", "#rag", "#long_context", "#optimization", "#benchmark", "#training"], "emoji": "üîç", "ru": {"title": "–ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–º–µ–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ: —Å–∏—Ç—É–∞—Ç–∏–≤–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ
[06.08.2025 01:01] Using data from previous issue: {"categories": ["#dataset", "#training", "#science", "#open_source", "#architecture", "#agents", "#multimodal"], "emoji": "üß¨", "ru": {"title": "CellForge: –ò–ò-–∞–≥–µ–Ω—Ç—ã —Å–æ–∑–¥–∞—é—Ç –≤–∏—Ä—Ç—É–∞–ª—å–Ω—ã–µ –∫–ª–µ—Ç–∫–∏ –∏–∑ –æ–¥–Ω–æ–∫–ª–µ—Ç–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö", "desc": "CellForge - —ç—Ç–æ –∞–≥–µ–Ω—Ç–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥
[06.08.2025 01:01] Using data from previous issue: {"categories": ["#rl", "#training", "#reasoning", "#optimization"], "emoji": "üß†", "ru": {"title": "–°–∞–º–æ–æ–±—É—á–µ–Ω–∏–µ –ò–ò –¥–ª—è –ª—É—á—à–µ–≥–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å–ª–µ–¥–æ–≤–∞—Ç—å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º –±–µ–∑ –≤–Ω–µ—à–Ω–µ–≥–æ –∫–æ–Ω—Ç—Ä–æ–ª—è. –û–Ω–∏ —Ä–∞
[06.08.2025 01:01] Using data from previous issue: {"categories": ["#dataset", "#alignment", "#security", "#training", "#multimodal"], "emoji": "üõ°Ô∏è", "ru": {"title": "–ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫ –ø–æ –∫–∏–±–µ—Ä–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –Ω–∞ –±–∞–∑–µ –ò–ò", "desc": "Foundation-Sec-8B-Instruct - —ç—Ç–æ —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å, —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä—É—é—â–∞—è—Å—è –Ω–∞ –∫–∏–±–µ—Ä–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –∏ –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–Ω–∞—è –¥–ª—è –¥–∏–∞
[06.08.2025 01:01] Using data from previous issue: {"categories": ["#architecture", "#optimization", "#training", "#multimodal"], "emoji": "üöÄ", "ru": {"title": "–£—Å–∫–æ—Ä—è–µ–º –æ–±—É—á–µ–Ω–∏–µ –æ–º–Ω–∏-–º–æ–¥–∞–ª—å–Ω—ã—Ö LLM —Å –ø–æ–º–æ—â—å—é –º–æ–¥—É–ª—å–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –º–æ–¥—É–ª—å–Ω—É—é —Å–∏—Å—Ç–µ–º—É –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –æ–º–Ω–∏-–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –°–∏—Å—Ç–µ–º–∞ –ø—Ä
[06.08.2025 01:01] Using data from previous issue: {"categories": ["#healthcare", "#data", "#optimization", "#science", "#dataset", "#benchmark"], "emoji": "üß¨", "ru": {"title": "AuroBind: –ø—Ä–æ—Ä—ã–≤ –≤ –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–º —Å–∫—Ä–∏–Ω–∏–Ω–≥–µ –ª–µ–∫–∞—Ä—Å—Ç–≤ –Ω–∞ –∞—Ç–æ–º–∞—Ä–Ω–æ–º —É—Ä–æ–≤–Ω–µ", "desc": "AuroBind - —ç—Ç–æ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–∞—è —Å–∏—Å—Ç–µ–º–∞ –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–≥–æ —Å–∫—Ä–∏–Ω–∏–Ω–≥–∞, –∫–æ—Ç–æ—Ä–∞—è –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω
[06.08.2025 01:01] Using data from previous issue: {"categories": ["#optimization", "#robotics", "#training", "#reasoning", "#multimodal", "#benchmark"], "emoji": "ü§ñ", "ru": {"title": "InstructVLA: –ú–æ—Å—Ç –º–µ–∂–¥—É –∏–Ω—Ç—É–∏—Ç–∏–≤–Ω—ã–º —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º —Ä–æ–±–æ—Ç–∞–º–∏ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–º –æ–±—É—á–µ–Ω–∏–µ–º", "desc": "InstructVLA - —ç—Ç–æ –º–æ–¥–µ–ª—å –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑—Ä–µ–Ω–∏—è, —è–∑—ã–∫–∞ –∏ –¥–µ–π—Å—Ç–≤–∏–π, –∫–æ—Ç–æ—Ä–∞—è —É–ª—É—á—à
[06.08.2025 01:01] Using data from previous issue: {"categories": ["#dataset", "#survey", "#open_source", "#data"], "emoji": "üå±", "ru": {"title": "–û—Ç–∫—Ä—ã—Ç—ã–µ –∏ —Ä–∞–∑–≤–∏–≤–∞—é—â–∏–µ—Å—è –¥–∞—Ç–∞—Å–µ—Ç—ã –¥–ª—è NLP —Å–∏–ª–∞–º–∏ —Å–æ–æ–±—â–µ—Å—Ç–≤–∞", "desc": "Dynaword - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –±–æ–ª—å—à–∏—Ö –æ—Ç–∫—Ä—ã—Ç—ã—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–º —è–∑—ã–∫–µ —Å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å—é –ø–æ—Å—Ç–æ—è–Ω–Ω–æ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Å–æ–æ
[06.08.2025 01:01] Using data from previous issue: {"categories": ["#inference", "#cv", "#training", "#optimization"], "emoji": "‚úÇÔ∏è", "ru": {"title": "–£–º–Ω–∞—è –æ–±—Ä–µ–∑–∫–∞ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "GlimpsePrune - —ç—Ç–æ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ–±—Ä–µ–∑–∫–∏, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–≤—ã—à–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∫—Ä—É–ø–Ω—ã—Ö –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LVLM). –û–Ω–∞ –∞–¥–∞–ø—Ç
[06.08.2025 01:01] Using data from previous issue: {"categories": ["#multilingual", "#science", "#audio", "#benchmark", "#open_source"], "emoji": "üó£Ô∏è", "ru": {"title": "Voxlect: –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–∏–∞–ª–µ–∫—Ç–æ–≤ –≤ —Ä–µ—á–µ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "Voxlect - —ç—Ç–æ –Ω–æ–≤—ã–π —ç—Ç–∞–ª–æ–Ω–Ω—ã–π —Ç–µ—Å—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ä–µ—á–µ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –∑–∞–¥–∞—á–∞—Ö –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –¥–∏–∞–ª–µ–∫—Ç–æ–≤ –∏
[06.08.2025 01:01] Using data from previous issue: {"categories": ["#cv", "#dataset", "#alignment", "#diffusion", "#open_source", "#multimodal"], "emoji": "üõ°Ô∏è", "ru": {"title": "–ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –≤ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ Personalized Safety Alignment (PSA) –¥–ª—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ
[06.08.2025 01:01] Using data from previous issue: {"categories": ["#architecture", "#optimization", "#diffusion", "#training", "#inference", "#long_context"], "emoji": "üöÄ", "ru": {"title": "–£—Å–∫–æ—Ä–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞", "desc": "Sparse-dLLM - —ç—Ç–æ –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (d
[06.08.2025 01:01] Using data from previous issue: {"categories": ["#agents", "#optimization", "#open_source", "#training", "#agi", "#robotics"], "emoji": "ü§ñ", "ru": {"title": "RoboMemory: –ú–æ–∑–≥–æ–ø–æ–¥–æ–±–Ω–∞—è –ø–∞–º—è—Ç—å –¥–ª—è –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Ä–æ–±–æ—Ç–æ–≤", "desc": "RoboMemory - —ç—Ç–æ –º—É–ª—å—Ç–∏–º–æ–¥—É–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –ø–∞–º—è—Ç–∏ –¥–ª—è –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Ä–æ–±–æ—Ç–æ–≤, –≤–¥–æ—Ö–Ω–æ–≤–ª–µ–Ω–Ω–∞—è
[06.08.2025 01:01] Using data from previous issue: {"categories": ["#cv", "#dataset", "#ethics", "#hallucinations", "#multimodal"], "emoji": "üé®", "ru": {"title": "–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç vs –ò—Å–∫—É—Å—Å—Ç–≤–æ: –ø—Ä–æ–±–ª–µ–º—ã –∞—Ç—Ä–∏–±—É—Ü–∏–∏ –≤ —ç–ø–æ—Ö—É –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π", "desc": "–°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ (VLM) –∏—Å–ø—ã—Ç—ã–≤–∞—é—Ç —Ç—Ä—É–¥–Ω–æ—Å—Ç–∏ —Å 
[06.08.2025 01:01] Using data from previous issue: {"categories": ["#agents", "#optimization", "#games", "#reasoning", "#rl"], "emoji": "üîç", "ru": {"title": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –º–æ–∂–µ—Ç –≤–æ–∑–Ω–∏–∫–Ω—É—Ç—å –∏–∑ —á–∏—Å—Ç–æ–π —ç–∫—Å–ø–ª—É–∞—Ç–∞—Ü–∏–∏", "desc": "–≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –∞–≥–µ–Ω—Ç—ã –º–µ—Ç–∞-–æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –º–æ–≥—É—Ç –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞—Ç—å –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ –ø—Ä–∏ –æ–±—É—á–µ
[06.08.2025 01:01] Using data from previous issue: {"categories": ["#agents", "#dataset", "#synthetic", "#benchmark", "#open_source"], "emoji": "üõ°Ô∏è", "ru": {"title": "–°–∏–Ω—Ç–µ–∑ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π –±–µ–∑ —Å—Ä–µ–¥—ã –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –ø–µ—Ä–µ–¥–æ–≤—ã—Ö LLM –≤ –∫–∏–±–µ—Ä–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏", "desc": "Cyber-Zero - —ç—Ç–æ –ø–µ—Ä–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π –∞–≥–µ–Ω—Ç–æ–≤ –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Å—Ä–µ–¥—ã 
[06.08.2025 01:01] Using data from previous issue: {"categories": ["#cv", "#games", "#architecture", "#optimization"], "emoji": "üß†", "ru": {"title": "–ö–≤–∞–Ω—Ç–æ–≤–æ–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ —á–µ—Ä–µ–∑ —Å–∏–Ω–µ—Ä–≥–∏—é —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –∏ –∫–≤–∞–Ω—Ç–æ–≤—ã—Ö –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∫–≤–∞–Ω—Ç–æ–≤–æ–º—É –º–∞—à–∏–Ω–Ω–æ–º—É –æ–±—É—á–µ–Ω–∏—é, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∏–π —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ Vision Transfo
[06.08.2025 01:01] Using data from previous issue: {"categories": ["#interpretability", "#rl", "#agents", "#optimization", "#inference"], "emoji": "ü§ñ", "ru": {"title": "–£–º–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–µ—Å—É—Ä—Å–æ–≤ –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞", "desc": "AgentTTS - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM-–∞–≥–µ–Ω—Ç–æ–≤ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å
[06.08.2025 01:01] Using data from previous issue: {"categories": ["#diffusion", "#games", "#benchmark", "#rag", "#video", "#multimodal"], "emoji": "ü§ñ", "ru": {"title": "ReMoMask: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–≤–∏–∂–µ–Ω–∏–π –∏–∑ —Ç–µ–∫—Å—Ç–∞", "desc": "ReMoMask - —ç—Ç–æ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–≤–∏–∂–µ–Ω–∏–π –ø–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–º—É –æ–ø–∏—Å–∞–Ω–∏—é, —Ä–µ—à–∞—é—â–∞—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö
[06.08.2025 01:01] Using data from previous issue: {"categories": ["#training", "#low_resource", "#multilingual", "#dataset", "#machine_translation"], "emoji": "üåâ", "ru": {"title": "–ú–æ—Å—Ç –º–µ–∂–¥—É –∞—Ä–∞–±—Å–∫–∏–º —è–∑—ã–∫–æ–º –∏ –¥–∏–∞–ª–µ–∫—Ç–æ–º: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ –º–∞—à–∏–Ω–Ω–æ–º –ø–µ—Ä–µ–≤–æ–¥–µ", "desc": "SHAMI-MT - —ç—Ç–æ —Å–∏—Å—Ç–µ–º–∞ –º–∞—à–∏–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –¥–ª—è –ø—Ä–µ–æ–¥–æ–ª–µ–Ω–∏—è —Ä–∞–∑—Ä—ã–≤–∞ –º–µ–∂–¥—É —Å
[06.08.2025 01:01] Using data from previous issue: {"categories": ["#open_source", "#multimodal", "#dataset", "#science"], "emoji": "üõ∞Ô∏è", "ru": {"title": "–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≤ –æ—Ü–µ–Ω–∫–µ –±–ª–∞–≥–æ—Å–æ—Å—Ç–æ—è–Ω–∏—è", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –∫–æ–º–±–∏–Ω–∞—Ü–∏—è —Å–ø—É—Ç–Ω–∏–∫–æ–≤—ã—Ö —Å–Ω–∏–º–∫–æ–≤ –∏ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –º–æ–¥–µ–ª–∏, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã
[06.08.2025 01:01] Using data from previous issue: {"categories": ["#math", "#training", "#dataset", "#reasoning", "#optimization", "#data"], "emoji": "üßÆ", "ru": {"title": "–£–ª—É—á—à–µ–Ω–∏–µ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –ò–ò —á–µ—Ä–µ–∑ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç—å", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—é –∏ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –Ω–∞ 
[06.08.2025 01:01] Using data from previous issue: {"categories": ["#3d"], "emoji": "üßä", "ru": {"title": "–ï–¥–∏–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Ç–æ—á–Ω–æ–π 3D-—Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏", "desc": "Dens3R - —ç—Ç–æ –º–æ–¥–µ–ª—å –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–Ω–æ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ –≤ –∑–∞–¥–∞—á–∞—Ö –ø–ª–æ—Ç–Ω–æ–π 3D-—Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—É—é —Å—Ö–µ–º—É –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω
[06.08.2025 01:01] Renaming data file.
[06.08.2025 01:01] Renaming previous data. hf_papers.json to ./d/2025-08-06.json
[06.08.2025 01:01] Saving new data file.
[06.08.2025 01:01] Generating page.
[06.08.2025 01:01] Renaming previous page.
[06.08.2025 01:01] Renaming previous data. index.html to ./d/2025-08-06.html
[06.08.2025 01:01] Writing result.
[06.08.2025 01:01] Renaming log file.
[06.08.2025 01:01] Renaming previous data. log.txt to ./logs/2025-08-06_last_log.txt
