[15.10.2025 13:27] Read previous papers.
[15.10.2025 13:27] Generating top page (month).
[15.10.2025 13:27] Writing top page (month).
[15.10.2025 14:13] Read previous papers.
[15.10.2025 14:13] Get feed.
[15.10.2025 14:13] Get page data from previous paper. URL: https://huggingface.co/papers/2510.12276
[15.10.2025 14:13] Get page data from previous paper. URL: https://huggingface.co/papers/2510.12586
[15.10.2025 14:13] Get page data from previous paper. URL: https://huggingface.co/papers/2510.09116
[15.10.2025 14:13] Get page data from previous paper. URL: https://huggingface.co/papers/2510.11693
[15.10.2025 14:13] Get page data from previous paper. URL: https://huggingface.co/papers/2510.12798
[15.10.2025 14:13] Get page data from previous paper. URL: https://huggingface.co/papers/2510.12399
[15.10.2025 14:13] Get page data from previous paper. URL: https://huggingface.co/papers/2510.12747
[15.10.2025 14:13] Get page data from previous paper. URL: https://huggingface.co/papers/2510.11057
[15.10.2025 14:13] Get page data from previous paper. URL: https://huggingface.co/papers/2510.12773
[15.10.2025 14:13] Get page data from previous paper. URL: https://huggingface.co/papers/2510.12403
[15.10.2025 14:13] Get page data from previous paper. URL: https://huggingface.co/papers/2510.12693
[15.10.2025 14:13] Get page data from previous paper. URL: https://huggingface.co/papers/2510.12784
[15.10.2025 14:13] Get page data from previous paper. URL: https://huggingface.co/papers/2510.12789
[15.10.2025 14:13] Get page data from previous paper. URL: https://huggingface.co/papers/2510.12635
[15.10.2025 14:13] Get page data from previous paper. URL: https://huggingface.co/papers/2510.11683
[15.10.2025 14:13] Get page data from previous paper. URL: https://huggingface.co/papers/2510.12709
[15.10.2025 14:13] Get page data from previous paper. URL: https://huggingface.co/papers/2510.12225
[15.10.2025 14:13] Get page data from previous paper. URL: https://huggingface.co/papers/2510.12801
[15.10.2025 14:13] Get page data from previous paper. URL: https://huggingface.co/papers/2510.11919
[15.10.2025 14:13] Extract page data from URL. URL: https://huggingface.co/papers/2510.01171
[15.10.2025 14:13] Get page data from previous paper. URL: https://huggingface.co/papers/2510.12777
[15.10.2025 14:13] Get page data from previous paper. URL: https://huggingface.co/papers/2510.12793
[15.10.2025 14:13] Extract page data from URL. URL: https://huggingface.co/papers/2510.12323
[15.10.2025 14:13] Get page data from previous paper. URL: https://huggingface.co/papers/2510.12088
[15.10.2025 14:13] Get page data from previous paper. URL: https://huggingface.co/papers/2510.11892
[15.10.2025 14:13] Extract page data from URL. URL: https://huggingface.co/papers/2510.11851
[15.10.2025 14:13] Get page data from previous paper. URL: https://huggingface.co/papers/2510.11606
[15.10.2025 14:13] Extract page data from URL. URL: https://huggingface.co/papers/2510.11570
[15.10.2025 14:13] Get page data from previous paper. URL: https://huggingface.co/papers/2510.11545
[15.10.2025 14:13] Get page data from previous paper. URL: https://huggingface.co/papers/2510.09263
[15.10.2025 14:13] Get page data from previous paper. URL: https://huggingface.co/papers/2510.09062
[15.10.2025 14:13] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08783
[15.10.2025 14:13] Get page data from previous paper. URL: https://huggingface.co/papers/2510.12269
[15.10.2025 14:13] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[15.10.2025 14:13] No deleted papers detected.
[15.10.2025 14:13] Downloading and parsing papers (pdf, html). Total: 33.
[15.10.2025 14:13] Downloading and parsing paper https://huggingface.co/papers/2510.12276.
[15.10.2025 14:13] Extra JSON file exists (./assets/json/2510.12276.json), skip PDF parsing.
[15.10.2025 14:13] Paper image links file exists (./assets/img_data/2510.12276.json), skip HTML parsing.
[15.10.2025 14:13] Success.
[15.10.2025 14:13] Downloading and parsing paper https://huggingface.co/papers/2510.12586.
[15.10.2025 14:13] Extra JSON file exists (./assets/json/2510.12586.json), skip PDF parsing.
[15.10.2025 14:13] Paper image links file exists (./assets/img_data/2510.12586.json), skip HTML parsing.
[15.10.2025 14:13] Success.
[15.10.2025 14:13] Downloading and parsing paper https://huggingface.co/papers/2510.09116.
[15.10.2025 14:13] Extra JSON file exists (./assets/json/2510.09116.json), skip PDF parsing.
[15.10.2025 14:13] Paper image links file exists (./assets/img_data/2510.09116.json), skip HTML parsing.
[15.10.2025 14:13] Success.
[15.10.2025 14:13] Downloading and parsing paper https://huggingface.co/papers/2510.11693.
[15.10.2025 14:13] Extra JSON file exists (./assets/json/2510.11693.json), skip PDF parsing.
[15.10.2025 14:13] Paper image links file exists (./assets/img_data/2510.11693.json), skip HTML parsing.
[15.10.2025 14:13] Success.
[15.10.2025 14:13] Downloading and parsing paper https://huggingface.co/papers/2510.12798.
[15.10.2025 14:13] Extra JSON file exists (./assets/json/2510.12798.json), skip PDF parsing.
[15.10.2025 14:13] Paper image links file exists (./assets/img_data/2510.12798.json), skip HTML parsing.
[15.10.2025 14:13] Success.
[15.10.2025 14:13] Downloading and parsing paper https://huggingface.co/papers/2510.12399.
[15.10.2025 14:13] Extra JSON file exists (./assets/json/2510.12399.json), skip PDF parsing.
[15.10.2025 14:13] Paper image links file exists (./assets/img_data/2510.12399.json), skip HTML parsing.
[15.10.2025 14:13] Success.
[15.10.2025 14:13] Downloading and parsing paper https://huggingface.co/papers/2510.12747.
[15.10.2025 14:13] Extra JSON file exists (./assets/json/2510.12747.json), skip PDF parsing.
[15.10.2025 14:13] Paper image links file exists (./assets/img_data/2510.12747.json), skip HTML parsing.
[15.10.2025 14:13] Success.
[15.10.2025 14:13] Downloading and parsing paper https://huggingface.co/papers/2510.11057.
[15.10.2025 14:13] Extra JSON file exists (./assets/json/2510.11057.json), skip PDF parsing.
[15.10.2025 14:13] Paper image links file exists (./assets/img_data/2510.11057.json), skip HTML parsing.
[15.10.2025 14:13] Success.
[15.10.2025 14:13] Downloading and parsing paper https://huggingface.co/papers/2510.12773.
[15.10.2025 14:13] Extra JSON file exists (./assets/json/2510.12773.json), skip PDF parsing.
[15.10.2025 14:13] Paper image links file exists (./assets/img_data/2510.12773.json), skip HTML parsing.
[15.10.2025 14:13] Success.
[15.10.2025 14:13] Downloading and parsing paper https://huggingface.co/papers/2510.12403.
[15.10.2025 14:13] Extra JSON file exists (./assets/json/2510.12403.json), skip PDF parsing.
[15.10.2025 14:13] Paper image links file exists (./assets/img_data/2510.12403.json), skip HTML parsing.
[15.10.2025 14:13] Success.
[15.10.2025 14:13] Downloading and parsing paper https://huggingface.co/papers/2510.12693.
[15.10.2025 14:13] Extra JSON file exists (./assets/json/2510.12693.json), skip PDF parsing.
[15.10.2025 14:13] Paper image links file exists (./assets/img_data/2510.12693.json), skip HTML parsing.
[15.10.2025 14:13] Success.
[15.10.2025 14:13] Downloading and parsing paper https://huggingface.co/papers/2510.12784.
[15.10.2025 14:13] Extra JSON file exists (./assets/json/2510.12784.json), skip PDF parsing.
[15.10.2025 14:13] Paper image links file exists (./assets/img_data/2510.12784.json), skip HTML parsing.
[15.10.2025 14:13] Success.
[15.10.2025 14:13] Downloading and parsing paper https://huggingface.co/papers/2510.12789.
[15.10.2025 14:13] Extra JSON file exists (./assets/json/2510.12789.json), skip PDF parsing.
[15.10.2025 14:13] Paper image links file exists (./assets/img_data/2510.12789.json), skip HTML parsing.
[15.10.2025 14:13] Success.
[15.10.2025 14:13] Downloading and parsing paper https://huggingface.co/papers/2510.12635.
[15.10.2025 14:13] Extra JSON file exists (./assets/json/2510.12635.json), skip PDF parsing.
[15.10.2025 14:13] Paper image links file exists (./assets/img_data/2510.12635.json), skip HTML parsing.
[15.10.2025 14:13] Success.
[15.10.2025 14:13] Downloading and parsing paper https://huggingface.co/papers/2510.11683.
[15.10.2025 14:13] Extra JSON file exists (./assets/json/2510.11683.json), skip PDF parsing.
[15.10.2025 14:13] Paper image links file exists (./assets/img_data/2510.11683.json), skip HTML parsing.
[15.10.2025 14:13] Success.
[15.10.2025 14:13] Downloading and parsing paper https://huggingface.co/papers/2510.12709.
[15.10.2025 14:13] Extra JSON file exists (./assets/json/2510.12709.json), skip PDF parsing.
[15.10.2025 14:13] Paper image links file exists (./assets/img_data/2510.12709.json), skip HTML parsing.
[15.10.2025 14:13] Success.
[15.10.2025 14:13] Downloading and parsing paper https://huggingface.co/papers/2510.12225.
[15.10.2025 14:13] Extra JSON file exists (./assets/json/2510.12225.json), skip PDF parsing.
[15.10.2025 14:13] Paper image links file exists (./assets/img_data/2510.12225.json), skip HTML parsing.
[15.10.2025 14:13] Success.
[15.10.2025 14:13] Downloading and parsing paper https://huggingface.co/papers/2510.12801.
[15.10.2025 14:13] Extra JSON file exists (./assets/json/2510.12801.json), skip PDF parsing.
[15.10.2025 14:13] Paper image links file exists (./assets/img_data/2510.12801.json), skip HTML parsing.
[15.10.2025 14:13] Success.
[15.10.2025 14:13] Downloading and parsing paper https://huggingface.co/papers/2510.11919.
[15.10.2025 14:13] Extra JSON file exists (./assets/json/2510.11919.json), skip PDF parsing.
[15.10.2025 14:13] Paper image links file exists (./assets/img_data/2510.11919.json), skip HTML parsing.
[15.10.2025 14:13] Success.
[15.10.2025 14:13] Downloading and parsing paper https://huggingface.co/papers/2510.01171.
[15.10.2025 14:13] Downloading paper 2510.01171 from http://arxiv.org/pdf/2510.01171v3...
[15.10.2025 14:13] Extracting affiliations from text.
[15.10.2025 14:13] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Preprint VERBALIZED SAMPLING: HOW TO MITIGATE MODE COLLAPSE AND UNLOCK LLM DIVERSITY Jiayi Zhang1, Simon Yu1, Derek Chong2, Anthony Sicilia3 Michael R. Tomz2, Christopher D. Manning2, Weiyan Shi1 Northeastern University1 {zhang.jiayi12, yu.chi, we.shi}@northeastern.edu {derekch, tomz, manning}@stanford.edu, anthony.sicilia@mail.wvu.edu (cid:128) Website [ Blog Code Stanford University2 West Virginia University "
[15.10.2025 14:13] Response: ```python
["Northeastern University", "Stanford University", "West Virginia University"]
```
[15.10.2025 14:13] Deleting PDF ./assets/pdf/2510.01171.pdf.
[15.10.2025 14:13] Success.
[15.10.2025 14:13] Downloading and parsing paper https://huggingface.co/papers/2510.12777.
[15.10.2025 14:13] Extra JSON file exists (./assets/json/2510.12777.json), skip PDF parsing.
[15.10.2025 14:13] Paper image links file exists (./assets/img_data/2510.12777.json), skip HTML parsing.
[15.10.2025 14:13] Success.
[15.10.2025 14:13] Downloading and parsing paper https://huggingface.co/papers/2510.12793.
[15.10.2025 14:13] Extra JSON file exists (./assets/json/2510.12793.json), skip PDF parsing.
[15.10.2025 14:13] Paper image links file exists (./assets/img_data/2510.12793.json), skip HTML parsing.
[15.10.2025 14:13] Success.
[15.10.2025 14:13] Downloading and parsing paper https://huggingface.co/papers/2510.12323.
[15.10.2025 14:13] Downloading paper 2510.12323 from http://arxiv.org/pdf/2510.12323v1...
[15.10.2025 14:13] Extracting affiliations from text.
[15.10.2025 14:13] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 4 1 ] . [ 1 3 2 3 2 1 . 0 1 5 2 : r RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK Zirui Guo, Xubin Ren, Lingrui Xu, Jiahao Zhang, Chao Huang The University of Hong Kong zrguo101@hku.hk xubinrencs@gmail.com chaohuang75@gmail.com "
[15.10.2025 14:13] Response: ```python
["The University of Hong Kong"]
```
[15.10.2025 14:13] Deleting PDF ./assets/pdf/2510.12323.pdf.
[15.10.2025 14:13] Success.
[15.10.2025 14:13] Downloading and parsing paper https://huggingface.co/papers/2510.12088.
[15.10.2025 14:13] Extra JSON file exists (./assets/json/2510.12088.json), skip PDF parsing.
[15.10.2025 14:13] Paper image links file exists (./assets/img_data/2510.12088.json), skip HTML parsing.
[15.10.2025 14:13] Success.
[15.10.2025 14:13] Downloading and parsing paper https://huggingface.co/papers/2510.11892.
[15.10.2025 14:13] Extra JSON file exists (./assets/json/2510.11892.json), skip PDF parsing.
[15.10.2025 14:13] Paper image links file exists (./assets/img_data/2510.11892.json), skip HTML parsing.
[15.10.2025 14:13] Success.
[15.10.2025 14:13] Downloading and parsing paper https://huggingface.co/papers/2510.11851.
[15.10.2025 14:13] Downloading paper 2510.11851 from http://arxiv.org/pdf/2510.11851v1...
[15.10.2025 14:13] Extracting affiliations from text.
[15.10.2025 14:13] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 3 1 ] . [ 1 1 5 8 1 1 . 0 1 5 2 : r a Shuo Chen1,2,5 Zonggen Li3,5,6 Zhen Han4 Bailan He1,5 Tong Liu1,5 Haokun Chen1,2 Georg Groh3 Philip Torr7 Volker Tresp1,2 Jindong Gu7 1LMU Munich 2Munich Center for Machine Learning (MCML) 3Technical University of Munich (TUM) 4AWS AI 5Konrad Zuse School of Excellence in Reliable AI (relAI) 6University of Hong Kong (HKU) 7University of Oxford "
[15.10.2025 14:13] Response: ```python
[
    "LMU Munich",
    "Munich Center for Machine Learning (MCML)",
    "Technical University of Munich (TUM)",
    "AWS AI",
    "Konrad Zuse School of Excellence in Reliable AI (relAI)",
    "University of Hong Kong (HKU)",
    "University of Oxford"
]
```
[15.10.2025 14:13] Deleting PDF ./assets/pdf/2510.11851.pdf.
[15.10.2025 14:13] Success.
[15.10.2025 14:13] Downloading and parsing paper https://huggingface.co/papers/2510.11606.
[15.10.2025 14:13] Extra JSON file exists (./assets/json/2510.11606.json), skip PDF parsing.
[15.10.2025 14:13] Paper image links file exists (./assets/img_data/2510.11606.json), skip HTML parsing.
[15.10.2025 14:13] Success.
[15.10.2025 14:13] Downloading and parsing paper https://huggingface.co/papers/2510.11570.
[15.10.2025 14:13] Downloading paper 2510.11570 from http://arxiv.org/pdf/2510.11570v1...
[15.10.2025 14:13] Extracting affiliations from text.
[15.10.2025 14:13] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 3 1 ] . [ 1 0 7 5 1 1 . 0 1 5 2 : r Bag of Tricks for Subverting Reasoning-Based Safety Guardrails BAG OF TRICKS FOR SUBVERTING REASONING-BASED SAFETY GUARDRAILS Shuo Chen1,2,4 Zhen Han6 Haokun Chen1,2 Bailan He1,4 Shengyun Si3,5 Jingpei Wu1,4 Philip Torr7 Volker Tresp1,2 Jindong Gu7 1LMU Munich 2Munich Center for Machine Learning (MCML) 3Technical University of Berlin 4Konrad Zuse School of Excellence in Reliable AI (relAI) 5DFKI 6AWS AI 7University of Oxford "
[15.10.2025 14:13] Response: ```python
[
    "LMU Munich",
    "Munich Center for Machine Learning (MCML)",
    "Technical University of Berlin",
    "Konrad Zuse School of Excellence in Reliable AI (relAI)",
    "DFKI",
    "AWS AI",
    "University of Oxford"
]
```
[15.10.2025 14:13] Deleting PDF ./assets/pdf/2510.11570.pdf.
[15.10.2025 14:13] Success.
[15.10.2025 14:13] Downloading and parsing paper https://huggingface.co/papers/2510.11545.
[15.10.2025 14:13] Extra JSON file exists (./assets/json/2510.11545.json), skip PDF parsing.
[15.10.2025 14:13] Paper image links file exists (./assets/img_data/2510.11545.json), skip HTML parsing.
[15.10.2025 14:13] Success.
[15.10.2025 14:13] Downloading and parsing paper https://huggingface.co/papers/2510.09263.
[15.10.2025 14:13] Extra JSON file exists (./assets/json/2510.09263.json), skip PDF parsing.
[15.10.2025 14:13] Paper image links file exists (./assets/img_data/2510.09263.json), skip HTML parsing.
[15.10.2025 14:13] Success.
[15.10.2025 14:13] Downloading and parsing paper https://huggingface.co/papers/2510.09062.
[15.10.2025 14:13] Extra JSON file exists (./assets/json/2510.09062.json), skip PDF parsing.
[15.10.2025 14:13] Paper image links file exists (./assets/img_data/2510.09062.json), skip HTML parsing.
[15.10.2025 14:13] Success.
[15.10.2025 14:13] Downloading and parsing paper https://huggingface.co/papers/2510.08783.
[15.10.2025 14:13] Extra JSON file exists (./assets/json/2510.08783.json), skip PDF parsing.
[15.10.2025 14:13] Paper image links file exists (./assets/img_data/2510.08783.json), skip HTML parsing.
[15.10.2025 14:13] Success.
[15.10.2025 14:13] Downloading and parsing paper https://huggingface.co/papers/2510.12269.
[15.10.2025 14:13] Extra JSON file exists (./assets/json/2510.12269.json), skip PDF parsing.
[15.10.2025 14:13] Paper image links file exists (./assets/img_data/2510.12269.json), skip HTML parsing.
[15.10.2025 14:13] Success.
[15.10.2025 14:13] Enriching papers with extra data.
[15.10.2025 14:13] ********************************************************************************
[15.10.2025 14:13] Abstract 0. Vision-language-action (VLA) models have recently shown strong potential in enabling robots to follow language instructions and execute precise actions. However, most VLAs are built upon vision-language models pretrained solely on 2D data, which lack accurate spatial awareness and hinder their abili...
[15.10.2025 14:13] ********************************************************************************
[15.10.2025 14:13] Abstract 1. Pixel-space generative models are often more difficult to train and generally underperform compared to their latent-space counterparts, leaving a persistent performance and efficiency gap. In this paper, we introduce a novel two-stage training framework that closes this gap for pixel-space diffusion...
[15.10.2025 14:13] ********************************************************************************
[15.10.2025 14:13] Abstract 2. A new evaluation framework, DITING, and a reasoning-driven multi-agent evaluation framework, AgentEval, are introduced to assess the quality of web novel translations, revealing that Chinese-trained LLMs outperform larger foreign models.  					AI-generated summary 				 Large language models (LLMs) h...
[15.10.2025 14:13] ********************************************************************************
[15.10.2025 14:13] Abstract 3. Recent multimodal embedding approaches leveraging multimodal large language models (MLLMs) fine-tuned with contrastive learning (CL) have shown promising results, yet the underlying reasons behind their superiority remain underexplored. This work argues that a crucial advantage of MLLM-based approac...
[15.10.2025 14:13] ********************************************************************************
[15.10.2025 14:13] Abstract 4. Object detection has long been dominated by traditional coordinate regression-based models, such as YOLO, DETR, and Grounding DINO. Although recent efforts have attempted to leverage MLLMs to tackle this task, they face challenges like low recall rate, duplicate predictions, coordinate misalignment,...
[15.10.2025 14:13] ********************************************************************************
[15.10.2025 14:13] Abstract 5. The advancement of large language models (LLMs) has catalyzed a paradigm shift from code generation assistance to autonomous coding agents, enabling a novel development methodology termed "Vibe Coding" where developers validate AI-generated implementations through outcome observation rather than lin...
[15.10.2025 14:13] ********************************************************************************
[15.10.2025 14:13] Abstract 6. Diffusion models have recently advanced video restoration, but applying them to real-world video super-resolution (VSR) remains challenging due to high latency, prohibitive computation, and poor generalization to ultra-high resolutions. Our goal in this work is to make diffusion-based VSR practical ...
[15.10.2025 14:13] ********************************************************************************
[15.10.2025 14:13] Abstract 7. Diffusion models have achieved remarkable success as generative models. However, even a well-trained model can accumulate errors throughout the generation process. These errors become particularly problematic when arbitrary guidance is applied to steer samples toward desired properties, which often ...
[15.10.2025 14:13] ********************************************************************************
[15.10.2025 14:13] Abstract 8. Large Language Models (LLMs) process every token through all layers of a transformer stack, causing wasted computation on simple queries and insufficient flexibility for harder ones that need deeper reasoning. Adaptive-depth methods can improve efficiency, but prior approaches rely on costly inferen...
[15.10.2025 14:13] ********************************************************************************
[15.10.2025 14:13] Abstract 9. Robot learning transitions from model-based to data-driven methods, leveraging reinforcement learning and behavioral cloning to develop versatile, language-conditioned models for diverse tasks and robot types.  					AI-generated summary 				 Robot learning is at an inflection point, driven by rapid ...
[15.10.2025 14:13] ********************************************************************************
[15.10.2025 14:13] Abstract 10. Recent advances in embodied AI highlight the potential of vision language models (VLMs) as agents capable of perception, reasoning, and interaction in complex environments. However, top-performing systems rely on large-scale models that are costly to deploy, while smaller VLMs lack the necessary kno...
[15.10.2025 14:13] ********************************************************************************
[15.10.2025 14:13] Abstract 11. Recently, remarkable progress has been made in Unified Multimodal Models (UMMs), which integrate vision-language generation and understanding capabilities within a single framework. However, a significant gap exists where a model's strong visual understanding often fails to transfer to its visual ge...
[15.10.2025 14:13] ********************************************************************************
[15.10.2025 14:13] Abstract 12. Although recent advances in visual generation have been remarkable, most existing architectures still depend on distinct encoders for images and text. This separation constrains diffusion models' ability to perform cross-modal reasoning and knowledge transfer. Prior attempts to bridge this gap often...
[15.10.2025 14:13] ********************************************************************************
[15.10.2025 14:13] Abstract 13. Large Language Models face challenges in long-horizon agentic tasks as their constrained memory is easily overwhelmed by distracting or irrelevant context. Existing working memory methods typically rely on external, heuristic mechanisms that are decoupled from the agent's core policy. In this work, ...
[15.10.2025 14:13] ********************************************************************************
[15.10.2025 14:13] Abstract 14. Boundary-Guided Policy Optimization (BGPO) improves reinforcement learning for diffusion large language models by efficiently approximating likelihoods with a memory-efficient lower bound, enhancing performance in tasks like math problem solving, code generation, and planning.  					AI-generated sum...
[15.10.2025 14:13] ********************************************************************************
[15.10.2025 14:13] Abstract 15. Multimodal embedding models aim to yield informative unified representations that empower diverse cross-modal tasks. Despite promising developments in the evolution from CLIP-based dual-tower architectures to large vision-language models, prior works still face unavoidable challenges in real-world a...
[15.10.2025 14:13] ********************************************************************************
[15.10.2025 14:13] Abstract 16. Recent advances in vision-language models (VLMs) have made them highly effective at reasoning tasks. However, the principles underlying the construction of performant VL reasoning training datasets remain poorly understood. In this work, we introduce several data curation approaches and study their ...
[15.10.2025 14:13] ********************************************************************************
[15.10.2025 14:13] Abstract 17. Multimodal Large Language Models (MLLMs) in real-world applications require access to external knowledge sources and must remain responsive to the dynamic and ever-changing real-world information in order to address information-seeking and knowledge-intensive user queries. Existing approaches, such ...
[15.10.2025 14:13] ********************************************************************************
[15.10.2025 14:13] Abstract 18. Large reasoning models (LRMs) have led to new possibilities in terms of problem-solving, through the devising of a natural language thought process prior to answering a query. While their capabilities are well known across mathematics and coding tasks, their impact on the task of machine translation...
[15.10.2025 14:13] ********************************************************************************
[15.10.2025 14:13] Abstract 19. Typicality bias in preference data causes mode collapse in LLMs, and Verbalized Sampling is introduced as a prompting strategy to enhance diversity without compromising accuracy or safety.  					AI-generated summary 				 Post-training alignment often reduces LLM diversity, leading to a phenomenon kn...
[15.10.2025 14:13] ********************************************************************************
[15.10.2025 14:13] Abstract 20. Understanding the dynamics of a physical scene involves reasoning about the diverse ways it can potentially change, especially as a result of local interactions. We present the Flow Poke Transformer (FPT), a novel framework for directly predicting the distribution of local motion, conditioned on spa...
[15.10.2025 14:13] ********************************************************************************
[15.10.2025 14:13] Abstract 21. Existing Multimodal Large Language Models (MLLMs) suffer from increased inference costs due to the additional vision tokens introduced by image inputs. In this work, we propose Visual Consistency Learning (ViCO), a novel training algorithm that enables the model to represent images of varying semant...
[15.10.2025 14:13] ********************************************************************************
[15.10.2025 14:13] Abstract 22. RAG-Anything is a unified framework that enhances multimodal knowledge retrieval by integrating cross-modal relationships and semantic matching, outperforming existing methods on complex benchmarks.  					AI-generated summary 				 Retrieval-Augmented Generation (RAG) has emerged as a fundamental par...
[15.10.2025 14:13] ********************************************************************************
[15.10.2025 14:13] Abstract 23. OneLife framework models complex, stochastic environments using conditionally-activated programmatic laws within a probabilistic programming framework, enabling learning from minimal, unguided interaction and outperforming baselines in state ranking and fidelity.  					AI-generated summary 				 Symb...
[15.10.2025 14:13] ********************************************************************************
[15.10.2025 14:13] Abstract 24. LLMs can enhance decision-making in digital environments but struggle with long-horizon simulations due to hallucination and static knowledge. R-WoM improves performance by integrating external, up-to-date knowledge.  					AI-generated summary 				 Large Language Models (LLMs) can serve as world mod...
[15.10.2025 14:13] ********************************************************************************
[15.10.2025 14:13] Abstract 25. DR agents based on LLMs can generate detailed reports from harmful queries, highlighting alignment failures and the need for specialized safety measures.  					AI-generated summary 				 Deep Research (DR) agents built on Large Language Models (LLMs) can perform complex, multi-step research by decomp...
[15.10.2025 14:13] ********************************************************************************
[15.10.2025 14:13] Abstract 26. ExpVid, a new benchmark for evaluating multimodal large language models on scientific experiment videos, highlights gaps in fine-grained perception, procedural understanding, and scientific reasoning.  					AI-generated summary 				 Multimodal Large Language Models (MLLMs) hold promise for accelerat...
[15.10.2025 14:13] ********************************************************************************
[15.10.2025 14:13] Abstract 27. Reasoning-based safety guardrails in Large Reasoning Models are vulnerable to subtle prompt manipulations, leading to high attack success rates across various benchmarks.  					AI-generated summary 				 Recent reasoning-based safety guardrails for Large Reasoning Models (LRMs), such as deliberative ...
[15.10.2025 14:13] ********************************************************************************
[15.10.2025 14:13] Abstract 28. PART reformulates reasoning traces to preserve information while disrupting unauthorized distillation in Large Language Models.  					AI-generated summary 				 Recent advances in Large Language Models (LLMs) show that extending the length of reasoning chains significantly improves performance on com...
[15.10.2025 14:13] ********************************************************************************
[15.10.2025 14:13] Abstract 29. SynthID-Image, a deep learning system for watermarking AI-generated imagery, demonstrates state-of-the-art performance in visual quality and robustness, and is deployed across Google's services.  					AI-generated summary 				 We introduce SynthID-Image, a deep learning-based system for invisibly wa...
[15.10.2025 14:13] ********************************************************************************
[15.10.2025 14:13] Abstract 30. ReFIne, a new training framework, enhances the trustworthiness of reasoning models by improving interpretability, faithfulness, and reliability through structured traces, decisive information disclosure, and confidence estimates.  					AI-generated summary 				 Recent advances in long chain-of-thoug...
[15.10.2025 14:13] ********************************************************************************
[15.10.2025 14:13] Abstract 31. In an ideal design pipeline, user interface (UI) design is intertwined with user research to validate decisions, yet studies are often resource-constrained during early exploration. Recent advances in multimodal large language models (MLLMs) offer a promising opportunity to act as early evaluators, ...
[15.10.2025 14:13] ********************************************************************************
[15.10.2025 14:13] Abstract 32. Tensor logic unifies neural and symbolic AI by using tensor equations, enabling scalable, learnable, and transparent AI systems.  					AI-generated summary 				 Progress in AI is hindered by the lack of a programming language with all the requisite features. Libraries like PyTorch and TensorFlow pro...
[15.10.2025 14:13] Read previous papers.
[15.10.2025 14:13] Generating reviews via LLM API.
[15.10.2025 14:13] Using data from previous issue: {"categories": ["#3d", "#optimization", "#training", "#agents", "#alignment"], "emoji": "ü§ñ", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ —Ä–æ–±–æ—Ç–æ–≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–º—É –º—ã—à–ª–µ–Ω–∏—é –±–µ–∑ 3D —Å–µ–Ω—Å–æ—Ä–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Spatial Forcing (SF) ‚Äî –º–µ—Ç–æ–¥ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è vision-language-action (VLA) –º–æ–¥–µ–ª–µ–π –≤ —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–µ. –ü—Ä
[15.10.2025 14:13] Using data from previous issue: {"categories": ["#training", "#diffusion", "#cv", "#optimization"], "emoji": "üéØ", "ru": {"title": "–î–≤—É—Ö—ç—Ç–∞–ø–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∑–∞–∫—Ä—ã–≤–∞–µ—Ç —Ä–∞–∑—Ä—ã–≤ –º–µ–∂–¥—É –ø–∏–∫—Å–µ–ª—å–Ω—ã–º–∏ –∏ –ª–∞—Ç–µ–Ω—Ç–Ω—ã–º–∏ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –Ω–æ–≤—ã–π –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∏ consistency 
[15.10.2025 14:13] Using data from previous issue: {"categories": ["#machine_translation", "#open_source", "#reasoning", "#dataset", "#multilingual", "#benchmark"], "emoji": "üìö", "ru": {"title": "–ö–∏—Ç–∞–π—Å–∫–∏–µ LLM –ø–æ–±–µ–∂–¥–∞—é—Ç –≤ –ø–µ—Ä–µ–≤–æ–¥–µ –≤–µ–±-—Ä–æ–º–∞–Ω–æ–≤", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ DITING ‚Äî –ø–µ—Ä–≤—É—é –∫–æ–º–ø–ª–µ–∫—Å–Ω—É—é —Å–∏—Å—Ç–µ–º—É –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –ø–µ—Ä–µ–≤–æ–¥–∞ –≤–µ–±-—Ä–æ–º–∞–Ω–æ
[15.10.2025 14:13] Using data from previous issue: {"categories": ["#low_resource", "#data", "#benchmark", "#multimodal", "#transfer_learning", "#training", "#alignment"], "emoji": "üîó", "ru": {"title": "–ì–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–µ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ –∫–∞–∫ –∫–ª—é—á –∫ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–º –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö
[15.10.2025 14:13] Using data from previous issue: {"categories": ["#cv", "#multimodal", "#training", "#optimization", "#rl"], "emoji": "üîç", "ru": {"title": "Rex-Omni: –Ω–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å –≤ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–∏ –æ–±—ä–µ–∫—Ç–æ–≤", "desc": "–í —Å—Ç–∞—Ç—å–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å Rex-Omni, –∫–æ—Ç–æ—Ä–∞—è —É–ª—É—á—à–∞–µ—Ç –∑–∞–¥–∞—á–∏ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –æ–±—ä–µ–∫—Ç–æ–≤, –∏—Å–ø–æ–ª—å–∑—É—è –ø–æ–¥—Ö–æ–¥—ã LLM. Rex-Omni –¥–æ—Å—Ç–∏–≥–∞–µ—Ç
[15.10.2025 14:13] Using data from previous issue: {"categories": ["#multimodal", "#training", "#survey", "#agi", "#agents", "#rl"], "emoji": "üéµ", "ru": {"title": "Vibe Coding: –∫–æ–≥–¥–∞ —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫ –ø—Ä–æ–≤–µ—Ä—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç, –∞ –Ω–µ —á–∏—Ç–∞–µ—Ç –∫–æ–¥", "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –Ω–æ–≤—É—é –ø–∞—Ä–∞–¥–∏–≥–º—É —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º \"Vibe Coding\", –≥–¥–µ —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∏ –ø—Ä–æ–≤–µ—Ä—è—é—Ç —Ä–∞–±–æ—Ç–æ
[15.10.2025 14:13] Using data from previous issue: {"categories": ["#video", "#open_source", "#inference", "#training", "#dataset", "#diffusion"], "emoji": "‚ö°", "ru": {"title": "–î–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–æ–µ –≤–∏–¥–µ–æ super-resolution –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏", "desc": "FlashVSR ‚Äî –ø–µ—Ä–≤–∞—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –≤–∏–¥–µ–æ super-resolution –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏, —Ä–∞–±–æ—Ç–∞—é—â–∞—è —Å–æ —Å–∫–æ—Ä–æ—Å—Ç—å
[15.10.2025 14:13] Using data from previous issue: {"categories": ["#training", "#optimization", "#diffusion", "#multimodal", "#cv"], "emoji": "üéØ", "ru": {"title": "–í—Ä–µ–º–µ–Ω–Ω–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –¥–ª—è —Ç–æ—á–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–î–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –æ—Ç–ª–∏—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, –Ω–æ –Ω–∞–∫–∞–ø–ª–∏–≤–∞—é—Ç –æ—à–∏–±–∫–∏ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ —Ä–∞–±–æ—Ç—ã, –æ—Å–æ
[15.10.2025 14:13] Using data from previous issue: {"categories": ["#reasoning", "#architecture", "#optimization", "#training", "#inference"], "emoji": "üß≠", "ru": {"title": "–£–º–Ω–∞—è –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—è —Å–ª–æ—ë–≤: LLM —É—á–∞—Ç—Å—è –ø—Ä–æ–ø—É—Å–∫–∞—Ç—å –Ω–µ–Ω—É–∂–Ω—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω Dr.LLM ‚Äî –º–µ—Ç–æ–¥ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–∏ —Å–ª–æ—ë–≤ –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π
[15.10.2025 14:13] Using data from previous issue: {"categories": ["#rl", "#reasoning", "#optimization", "#games", "#agents", "#robotics"], "emoji": "ü§ñ", "ru": {"title": "–û—Ç –∫–ª–∞—Å—Å–∏–∫–∏ –∫ –¥–∞–Ω–Ω—ã–º: –Ω–æ–≤–∞—è —ç—Ä–∞ –æ–±—É—á–µ–Ω–∏—è —Ä–æ–±–æ—Ç–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π —É—á–µ–±–Ω–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–º—É –º–∞—à–∏–Ω–Ω–æ–º—É –æ–±—É—á–µ–Ω–∏—é –¥–ª—è —Ä–æ–±–æ—Ç–æ–≤. –ê–≤—Ç–æ—Ä—ã –æ–ø–∏—Å—ã–≤–∞—é—Ç –ø–µ—Ä–µ—Ö–æ–¥ –æ
[15.10.2025 14:13] Using data from previous issue: {"categories": ["#reasoning", "#training", "#optimization", "#transfer_learning", "#agents", "#rl"], "emoji": "ü§ñ", "ru": {"title": "–ú–∞–ª–µ–Ω—å–∫–∏–µ –º–æ–¥–µ–ª–∏ —É—á–∞—Ç—Å—è –¥–µ–π—Å—Ç–≤–æ–≤–∞—Ç—å –∫–∞–∫ –±–æ–ª—å—à–∏–µ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç ERA ‚Äî –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∫–æ–º–ø–∞–∫—Ç–Ω—ã—Ö vision language models –¥–µ–π—Å—Ç–≤–æ–≤–∞—Ç—å –≤ 
[15.10.2025 14:13] Using data from previous issue: {"categories": ["#training", "#optimization", "#multimodal", "#transfer_learning"], "emoji": "üîÑ", "ru": {"title": "–°–∞–º–æ–æ–±—É—á–µ–Ω–∏–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –≤–Ω—É—Ç—Ä–µ–Ω–Ω—é—é —Å–∞–º–æ–æ—Ü–µ–Ω–∫—É", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏ –ø–∞—Ä–∞–¥–æ–∫—Å: –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –º–æ–≥—É—Ç —Ö–æ—Ä–æ—à–æ –ø–æ–Ω–∏–º–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, –Ω–æ –ø–ª–æ—Ö–æ –∏—Ö –≥–µ–Ω–µ
[15.10.2025 14:13] Using data from previous issue: {"categories": ["#cv", "#multimodal", "#reasoning", "#training", "#transfer_learning", "#diffusion"], "emoji": "üîó", "ru": {"title": "–ï–¥–∏–Ω—ã–π —ç–Ω–∫–æ–¥–µ—Ä –¥–ª—è —Ç–µ–∫—Å—Ç–∞ –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "UniFusion ‚Äî —ç—Ç–æ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä–∞—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∑–∞–º–æ—Ä–æ–∂–µ–Ω–Ω—É
[15.10.2025 14:13] Using data from previous issue: {"categories": ["#long_context", "#reasoning", "#training", "#optimization", "#agents", "#rl"], "emoji": "üß†", "ru": {"title": "–ü–∞–º—è—Ç—å –∫–∞–∫ –¥–µ–π—Å—Ç–≤–∏–µ: LLM —É—á–∞—Ç—Å—è —É–ø—Ä–∞–≤–ª—è—Ç—å —Å–≤–æ–∏–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º", "desc": "–ë–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ —Å—Ç–∞–ª–∫–∏–≤–∞—é—Ç—Å—è —Å –ø—Ä–æ–±–ª–µ–º–æ–π –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–π –ø–∞–º—è—Ç–∏ –ø—Ä–∏ —Ä–µ—à–µ–Ω–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –∑–∞–¥–∞—á, –∫–æ–≥–¥–∞ –∫
[15.10.2025 14:13] Using data from previous issue: {"categories": ["#games", "#rlhf", "#training", "#optimization", "#rl"], "emoji": "üéØ", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –ø–æ–º–æ—â—å—é –ª–∏–Ω–µ–π–Ω–æ–π –∞–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ BGPO –¥–ª—è reinforcement learning –≤ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö LLM, –∫–æ—Ç–æ—Ä—ã–µ —Å—Ç–∞–ª–∫–∏–≤–∞—é—Ç—Å—è —Å 
[15.10.2025 14:13] Using data from previous issue: {"categories": ["#training", "#optimization", "#multimodal"], "emoji": "‚õµ", "ru": {"title": "SAIL-Embedding: –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –ø–æ–∏—Å–∫–∞ –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç SAIL-Embedding ‚Äî –æ–º–Ω–∏–º–æ–¥–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤, –∫–æ—Ç–æ—Ä–∞—è —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–π –ø–æ–¥–¥–µ—Ä–∂–∫–∏
[15.10.2025 14:13] Using data from previous issue: {"categories": ["#cv", "#multimodal", "#reasoning", "#dataset", "#benchmark", "#data"], "emoji": "üêù", "ru": {"title": "HoneyBee: –ö–∞–∫ –ø—Ä–∞–≤–∏–ª—å–Ω–æ –≥–æ—Ç–æ–≤–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –∏–∑—É—á–∏–ª–∏ –ø—Ä–∏–Ω—Ü–∏–ø—ã —Å–æ–∑–¥–∞–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è vision-language –º–æ–¥–µ–ª
[15.10.2025 14:13] Using data from previous issue: {"categories": ["#multimodal", "#reasoning", "#training", "#dataset", "#agi", "#optimization", "#rag", "#benchmark"], "emoji": "üîç", "ru": {"title": "–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ —Å –æ–±—É—á–µ–Ω–∏–µ–º —á–µ—Ä–µ–∑ –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç DeepMMSearch-R1 ‚Äî –ø–µ—Ä–≤—É—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—É—é LLM, —Å–ø–æ—Å–æ–±–Ω—É—é
[15.10.2025 14:13] Using data from previous issue: {"categories": ["#low_resource", "#reasoning", "#training", "#multilingual", "#machine_translation", "#synthetic"], "emoji": "ü§î", "ru": {"title": "–†–∞–∑–º—ã—à–ª–µ–Ω–∏—è –Ω–µ –ø–æ–º–æ–≥–∞—é—Ç LLM –ª—É—á—à–µ –ø–µ—Ä–µ–≤–æ–¥–∏—Ç—å", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –∏–∑—É—á–∏–ª–∏, –ø–æ–º–æ–≥–∞—é—Ç –ª–∏ \"—Ç–æ–∫–µ–Ω—ã —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏–π\" (thinking tokens) –±–æ–ª—å—à–∏–º reasoning –º–æ–¥
[15.10.2025 14:13] Querying the API.
[15.10.2025 14:13] Claude request. Model: claude-sonnet-4-5-20250929. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Typicality bias in preference data causes mode collapse in LLMs, and Verbalized Sampling is introduced as a prompting strategy to enhance diversity without compromising accuracy or safety.  					AI-generated summary 				 Post-training alignment often reduces LLM diversity, leading to a phenomenon known as mode collapse. Unlike prior work that attributes this effect to algorithmic limitations, we identify a fundamental, pervasive data-level driver: typicality bias in preference data, whereby annotators systematically favor familiar text as a result of well-established findings in cognitive psychology. We formalize this bias theoretically, verify it on preference datasets empirically, and show that it plays a central role in mode collapse. Motivated by this analysis, we introduce Verbalized Sampling, a simple, training-free prompting strategy to circumvent mode collapse. VS prompts the model to verbalize a probability distribution over a set of responses (e.g., ``Generate 5 jokes about coffee and their corresponding probabilities''). Comprehensive experiments show that VS significantly improves performance across creative writing (poems, stories, jokes), dialogue simulation, open-ended QA, and synthetic data generation, without sacrificing factual accuracy and safety. For instance, in creative writing, VS increases diversity by 1.6-2.1x over direct prompting. We further observe an emergent trend that more capable models benefit more from VS. In sum, our work provides a new data-centric perspective on mode collapse and a practical inference-time remedy that helps unlock pre-trained generative diversity.
[15.10.2025 14:14] Response: ```json
{
  "title": "–†–∞–∑–±–ª–æ–∫–∏—Ä–æ–≤–∫–∞ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è LLM —á–µ—Ä–µ–∑ –≤–µ—Ä–±–∞–ª–∏–∑–∞—Ü–∏—é –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π",
  "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ —Ä–µ–∂–∏–º–Ω—ã–π –∫–æ–ª–ª–∞–ø—Å –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö –≤—ã–∑–≤–∞–Ω —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç—å—é –≤ –¥–∞–Ω–Ω—ã—Ö –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π: –∞–Ω–Ω–æ—Ç–∞—Ç–æ—Ä—ã –≤—ã–±–∏—Ä–∞—é—Ç –∑–Ω–∞–∫–æ–º—ã–µ, —Ç–∏–ø–∏—á–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã –∏–∑-–∑–∞ –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã—Ö –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–µ–π —á–µ–ª–æ–≤–µ–∫–∞. –ê–≤—Ç–æ—Ä—ã —Ñ–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–ª–∏ —ç—Ç—É –ø—Ä–æ–±–ª–µ–º—É —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏ –∏ –ø–æ–¥—Ç–≤–µ—Ä–¥–∏–ª–∏ –µ—ë —ç–º–ø–∏—Ä–∏—á–µ—Å–∫–∏ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π. –î–ª—è —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º—ã –ø—Ä–µ–¥–ª–æ–∂–µ–Ω Verbalized Sampling ‚Äî –ø—Ä–æ—Å—Ç–∞—è prompting-—Å—Ç—Ä–∞—Ç–µ–≥–∏—è –±–µ–∑ –¥–æ–æ–±—É—á–µ–Ω–∏—è, –≥–¥–µ –º–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –æ—Ç–≤–µ—Ç–æ–≤ —Å –∏—Ö –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—è–º–∏. –ú–µ—Ç–æ–¥ —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤ 1.6-2.1 —Ä–∞–∑–∞ –≤ —Ç–≤–æ—Ä—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á–∞—Ö, —Å–æ—Ö—Ä–∞–Ω—è—è —Ç–æ—á–Ω–æ—Å—Ç—å –∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å, –ø—Ä–∏—á—ë–º –±–æ–ª–µ–µ –º–æ—â–Ω—ã–µ –º–æ–¥–µ–ª–∏ –ø–æ–ª—É—á–∞—é—Ç –±–æ–ª—å—à—É—é –≤—ã–≥–æ–¥—É –æ—Ç —ç—Ç–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞.",
  "emoji": "üé≤"
}
```
[15.10.2025 14:14] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Typicality bias in preference data causes mode collapse in LLMs, and Verbalized Sampling is introduced as a prompting strategy to enhance diversity without compromising accuracy or safety.  					AI-generated summary 				 Post-training alignment often reduces LLM diversity, leading to a phenomenon known as mode collapse. Unlike prior work that attributes this effect to algorithmic limitations, we identify a fundamental, pervasive data-level driver: typicality bias in preference data, whereby annotators systematically favor familiar text as a result of well-established findings in cognitive psychology. We formalize this bias theoretically, verify it on preference datasets empirically, and show that it plays a central role in mode collapse. Motivated by this analysis, we introduce Verbalized Sampling, a simple, training-free prompting strategy to circumvent mode collapse. VS prompts the model to verbalize a probability distribution over a set of responses (e.g., ``Generate 5 jokes about coffee and their corresponding probabilities''). Comprehensive experiments show that VS significantly improves performance across creative writing (poems, stories, jokes), dialogue simulation, open-ended QA, and synthetic data generation, without sacrificing factual accuracy and safety. For instance, in creative writing, VS increases diversity by 1.6-2.1x over direct prompting. We further observe an emergent trend that more capable models benefit more from VS. In sum, our work provides a new data-centric perspective on mode collapse and a practical inference-time remedy that helps unlock pre-trained generative diversity."

[15.10.2025 14:14] Response: ```python
['DATA', 'TRAINING', 'INFERENCE']
```
[15.10.2025 14:14] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Typicality bias in preference data causes mode collapse in LLMs, and Verbalized Sampling is introduced as a prompting strategy to enhance diversity without compromising accuracy or safety.  					AI-generated summary 				 Post-training alignment often reduces LLM diversity, leading to a phenomenon known as mode collapse. Unlike prior work that attributes this effect to algorithmic limitations, we identify a fundamental, pervasive data-level driver: typicality bias in preference data, whereby annotators systematically favor familiar text as a result of well-established findings in cognitive psychology. We formalize this bias theoretically, verify it on preference datasets empirically, and show that it plays a central role in mode collapse. Motivated by this analysis, we introduce Verbalized Sampling, a simple, training-free prompting strategy to circumvent mode collapse. VS prompts the model to verbalize a probability distribution over a set of responses (e.g., ``Generate 5 jokes about coffee and their corresponding probabilities''). Comprehensive experiments show that VS significantly improves performance across creative writing (poems, stories, jokes), dialogue simulation, open-ended QA, and synthetic data generation, without sacrificing factual accuracy and safety. For instance, in creative writing, VS increases diversity by 1.6-2.1x over direct prompting. We further observe an emergent trend that more capable models benefit more from VS. In sum, our work provides a new data-centric perspective on mode collapse and a practical inference-time remedy that helps unlock pre-trained generative diversity."

[15.10.2025 14:14] Response: ```python
['ALIGNMENT', 'HALLUCINATIONS', 'STORY_GENERATION', 'SYNTHETIC']
```
[15.10.2025 14:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper addresses the issue of mode collapse in large language models (LLMs), which occurs when models generate repetitive outputs due to typicality bias in preference data. Typicality bias leads annotators to prefer familiar text, limiting the diversity of generated responses. The authors propose a new prompting strategy called Verbalized Sampling (VS), which encourages models to express a range of possible outputs along with their probabilities. Experiments demonstrate that VS significantly enhances creative writing and other tasks by increasing output diversity while maintaining accuracy and safety.","title":"Unlocking Diversity in LLMs with Verbalized Sampling"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper addresses the issue of mode collapse in large language models (LLMs), which occurs when models generate repetitive outputs due to typicality bias in preference data. Typicality bias leads annotators to prefer familiar text, limiting the diversity of generated responses. The authors propose a new prompting strategy called Verbalized Sampling (VS), which encourages models to express a range of possible outputs along with their probabilities. Experiments demonstrate that VS significantly enhances creative writing and other tasks by increasing output diversity while maintaining accuracy and safety.', title='Unlocking Diversity in LLMs with Verbalized Sampling'))
[15.10.2025 14:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"ËøôÁØáËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÂÅèÂ•ΩÊï∞ÊçÆ‰∏≠ÁöÑÂÖ∏ÂûãÊÄßÂÅèËßÅÂ¶Ç‰ΩïÂØºËá¥Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂá∫Áé∞Ê®°ÂºèÂ¥©Ê∫É„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÊ≥®ÈáäËÄÖÂÄæÂêë‰∫éÈÄâÊã©ÁÜüÊÇâÁöÑÊñáÊú¨ÔºåËøôÁßçÁé∞Ë±°Ê∫ê‰∫éËÆ§Áü•ÂøÉÁêÜÂ≠¶ÁöÑÁ†îÁ©∂„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊèêÁ§∫Á≠ñÁï•‚Äî‚ÄîÂè£Â§¥ÈááÊ†∑ÔºàVerbalized SamplingÔºâÔºåÂèØ‰ª•Âú®‰∏çÂΩ±ÂìçÂáÜÁ°ÆÊÄßÂíåÂÆâÂÖ®ÊÄßÁöÑÊÉÖÂÜµ‰∏ãÂ¢ûÂº∫ÁîüÊàêÂÜÖÂÆπÁöÑÂ§öÊ†∑ÊÄß„ÄÇÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåÂè£Â§¥ÈááÊ†∑Âú®ÂàõÊÑèÂÜô‰Ωú„ÄÅÂØπËØùÊ®°ÊãüÂíåÂºÄÊîæÂºèÈóÆÁ≠îÁ≠â‰ªªÂä°‰∏≠ÊòæËëóÊèêÈ´ò‰∫ÜÊ®°ÂûãÁöÑË°®Áé∞„ÄÇ","title":"ÊâìÁ†¥Ê®°ÂºèÂ¥©Ê∫ÉÔºåÊèêÂçáÁîüÊàêÂ§öÊ†∑ÊÄßÔºÅ"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='ËøôÁØáËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÂÅèÂ•ΩÊï∞ÊçÆ‰∏≠ÁöÑÂÖ∏ÂûãÊÄßÂÅèËßÅÂ¶Ç‰ΩïÂØºËá¥Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂá∫Áé∞Ê®°ÂºèÂ¥©Ê∫É„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÊ≥®ÈáäËÄÖÂÄæÂêë‰∫éÈÄâÊã©ÁÜüÊÇâÁöÑÊñáÊú¨ÔºåËøôÁßçÁé∞Ë±°Ê∫ê‰∫éËÆ§Áü•ÂøÉÁêÜÂ≠¶ÁöÑÁ†îÁ©∂„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊèêÁ§∫Á≠ñÁï•‚Äî‚ÄîÂè£Â§¥ÈááÊ†∑ÔºàVerbalized SamplingÔºâÔºåÂèØ‰ª•Âú®‰∏çÂΩ±ÂìçÂáÜÁ°ÆÊÄßÂíåÂÆâÂÖ®ÊÄßÁöÑÊÉÖÂÜµ‰∏ãÂ¢ûÂº∫ÁîüÊàêÂÜÖÂÆπÁöÑÂ§öÊ†∑ÊÄß„ÄÇÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåÂè£Â§¥ÈááÊ†∑Âú®ÂàõÊÑèÂÜô‰Ωú„ÄÅÂØπËØùÊ®°ÊãüÂíåÂºÄÊîæÂºèÈóÆÁ≠îÁ≠â‰ªªÂä°‰∏≠ÊòæËëóÊèêÈ´ò‰∫ÜÊ®°ÂûãÁöÑË°®Áé∞„ÄÇ', title='ÊâìÁ†¥Ê®°ÂºèÂ¥©Ê∫ÉÔºåÊèêÂçáÁîüÊàêÂ§öÊ†∑ÊÄßÔºÅ'))
[15.10.2025 14:14] Using data from previous issue: {"categories": ["#cv", "#interpretability", "#open_source", "#reasoning", "#training", "#optimization", "#synthetic"], "emoji": "üîÑ", "ru": {"title": "Flow Poke Transformer: –Ω–æ–≤–∞—è —ç—Ä–∞ –≤ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–∏ –¥–≤–∏–∂–µ–Ω–∏—è", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å Flow Poke Transformer (FPT), –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤
[15.10.2025 14:14] Using data from previous issue: {"categories": ["#cv", "#training", "#inference", "#interpretability", "#agi", "#optimization", "#multimodal"], "emoji": "üéØ", "ru": {"title": "–£–º–Ω–æ–µ —Å–∂–∞—Ç–∏–µ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –ø–æ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –º–µ—Ç–æ–¥ Visual Consistency Learning (ViCO), –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç mult
[15.10.2025 14:14] Querying the API.
[15.10.2025 14:14] Claude request. Model: claude-sonnet-4-5-20250929. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

RAG-Anything is a unified framework that enhances multimodal knowledge retrieval by integrating cross-modal relationships and semantic matching, outperforming existing methods on complex benchmarks.  					AI-generated summary 				 Retrieval-Augmented Generation (RAG) has emerged as a fundamental paradigm for expanding Large Language Models beyond their static training limitations. However, a critical misalignment exists between current RAG capabilities and real-world information environments. Modern knowledge repositories are inherently multimodal, containing rich combinations of textual content, visual elements, structured tables, and mathematical expressions. Yet existing RAG frameworks are limited to textual content, creating fundamental gaps when processing multimodal documents. We present RAG-Anything, a unified framework that enables comprehensive knowledge retrieval across all modalities. Our approach reconceptualizes multimodal content as interconnected knowledge entities rather than isolated data types. The framework introduces dual-graph construction to capture both cross-modal relationships and textual semantics within a unified representation. We develop cross-modal hybrid retrieval that combines structural knowledge navigation with semantic matching. This enables effective reasoning over heterogeneous content where relevant evidence spans multiple modalities. RAG-Anything demonstrates superior performance on challenging multimodal benchmarks, achieving significant improvements over state-of-the-art methods. Performance gains become particularly pronounced on long documents where traditional approaches fail. Our framework establishes a new paradigm for multimodal knowledge access, eliminating the architectural fragmentation that constrains current systems. Our framework is open-sourced at: https://github.com/HKUDS/RAG-Anything.
[15.10.2025 14:14] Response: ```json
{
  "desc": "RAG-Anything ‚Äî —ç—Ç–æ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è Retrieval-Augmented Generation (RAG), –∫–æ—Ç–æ—Ä–∞—è —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏, –≤–∫–ª—é—á–∞—è —Ç–µ–∫—Å—Ç, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, —Ç–∞–±–ª–∏—Ü—ã –∏ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è. –§—Ä–µ–π–º–≤–æ—Ä–∫ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–≤–æ–π–Ω—É—é –≥—Ä–∞—Ñ–æ–≤—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–ª—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Å–≤—è–∑–µ–π –º–µ–∂–¥—É —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç—è–º–∏ –∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –æ—Ç–Ω–æ—à–µ–Ω–∏–π –≤ –µ–¥–∏–Ω–æ–º –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–∏. –°–∏—Å—Ç–µ–º–∞ –∫–æ–º–±–∏–Ω–∏—Ä—É–µ—Ç –Ω–∞–≤–∏–≥–∞—Ü–∏—é –ø–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–º –∑–Ω–∞–Ω–∏—è–º —Å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–º –ø–æ–∏—Å–∫–æ–º, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –Ω–∞—Ö–æ–¥–∏—Ç—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ –≥–µ—Ç–µ—Ä–æ–≥–µ–Ω–Ω–æ–º –∫–æ–Ω—Ç–µ–Ω—Ç–µ. RAG-Anything –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ –Ω–∞–¥ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏ –Ω–∞ —Å–ª–æ–∂–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö, –æ—Å–æ–±–µ–Ω–Ω–æ –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å –¥–ª–∏–Ω–Ω—ã–º–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏.",
  "emoji": "üéØ",
  "title": "–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π RAG –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –ª—é–±—ã–º–∏ —Ç–∏–ø–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö"
}
```
[15.10.2025 14:14] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"RAG-Anything is a unified framework that enhances multimodal knowledge retrieval by integrating cross-modal relationships and semantic matching, outperforming existing methods on complex benchmarks.  					AI-generated summary 				 Retrieval-Augmented Generation (RAG) has emerged as a fundamental paradigm for expanding Large Language Models beyond their static training limitations. However, a critical misalignment exists between current RAG capabilities and real-world information environments. Modern knowledge repositories are inherently multimodal, containing rich combinations of textual content, visual elements, structured tables, and mathematical expressions. Yet existing RAG frameworks are limited to textual content, creating fundamental gaps when processing multimodal documents. We present RAG-Anything, a unified framework that enables comprehensive knowledge retrieval across all modalities. Our approach reconceptualizes multimodal content as interconnected knowledge entities rather than isolated data types. The framework introduces dual-graph construction to capture both cross-modal relationships and textual semantics within a unified representation. We develop cross-modal hybrid retrieval that combines structural knowledge navigation with semantic matching. This enables effective reasoning over heterogeneous content where relevant evidence spans multiple modalities. RAG-Anything demonstrates superior performance on challenging multimodal benchmarks, achieving significant improvements over state-of-the-art methods. Performance gains become particularly pronounced on long documents where traditional approaches fail. Our framework establishes a new paradigm for multimodal knowledge access, eliminating the architectural fragmentation that constrains current systems. Our framework is open-sourced at: https://github.com/HKUDS/RAG-Anything."

[15.10.2025 14:14] Response: ```python
['RAG', 'MULTIMODAL', 'BENCHMARK']
```
[15.10.2025 14:14] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"RAG-Anything is a unified framework that enhances multimodal knowledge retrieval by integrating cross-modal relationships and semantic matching, outperforming existing methods on complex benchmarks.  					AI-generated summary 				 Retrieval-Augmented Generation (RAG) has emerged as a fundamental paradigm for expanding Large Language Models beyond their static training limitations. However, a critical misalignment exists between current RAG capabilities and real-world information environments. Modern knowledge repositories are inherently multimodal, containing rich combinations of textual content, visual elements, structured tables, and mathematical expressions. Yet existing RAG frameworks are limited to textual content, creating fundamental gaps when processing multimodal documents. We present RAG-Anything, a unified framework that enables comprehensive knowledge retrieval across all modalities. Our approach reconceptualizes multimodal content as interconnected knowledge entities rather than isolated data types. The framework introduces dual-graph construction to capture both cross-modal relationships and textual semantics within a unified representation. We develop cross-modal hybrid retrieval that combines structural knowledge navigation with semantic matching. This enables effective reasoning over heterogeneous content where relevant evidence spans multiple modalities. RAG-Anything demonstrates superior performance on challenging multimodal benchmarks, achieving significant improvements over state-of-the-art methods. Performance gains become particularly pronounced on long documents where traditional approaches fail. Our framework establishes a new paradigm for multimodal knowledge access, eliminating the architectural fragmentation that constrains current systems. Our framework is open-sourced at: https://github.com/HKUDS/RAG-Anything."

[15.10.2025 14:14] Response: ```python
['AGI', 'REASONING', 'OPEN_SOURCE', 'LONG_CONTEXT']
```
[15.10.2025 14:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"RAG-Anything is a new framework designed to improve how we retrieve knowledge from different types of information, like text, images, and tables. It addresses the limitations of existing methods that only focus on text, allowing for better understanding and processing of complex documents that contain various formats. By using a dual-graph structure, it captures relationships between different types of data and enhances semantic matching. This leads to better performance on challenging tasks, especially with long documents, making it a significant advancement in multimodal knowledge retrieval.","title":"Unlocking Multimodal Knowledge Retrieval with RAG-Anything"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='RAG-Anything is a new framework designed to improve how we retrieve knowledge from different types of information, like text, images, and tables. It addresses the limitations of existing methods that only focus on text, allowing for better understanding and processing of complex documents that contain various formats. By using a dual-graph structure, it captures relationships between different types of data and enhances semantic matching. This leads to better performance on challenging tasks, especially with long documents, making it a significant advancement in multimodal knowledge retrieval.', title='Unlocking Multimodal Knowledge Retrieval with RAG-Anything'))
[15.10.2025 14:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"RAG-AnythingÊòØ‰∏Ä‰∏™Áªü‰∏ÄÊ°ÜÊû∂ÔºåÊó®Âú®ÈÄöËøáÊï¥ÂêàË∑®Ê®°ÊÄÅÂÖ≥Á≥ªÂíåËØ≠‰πâÂåπÈÖçÊù•Â¢ûÂº∫Â§öÊ®°ÊÄÅÁü•ËØÜÊ£ÄÁ¥¢„ÄÇÂÆÉËß£ÂÜ≥‰∫ÜÁé∞ÊúâRAGÊñπÊ≥ïÂú®Â§ÑÁêÜÂ§çÊùÇÂü∫ÂáÜÊó∂ÁöÑ‰∏çË∂≥ÔºåÁâπÂà´ÊòØÂú®ÈïøÊñáÊ°£‰∏äË°®Áé∞Âá∫Ëâ≤„ÄÇËØ•Ê°ÜÊû∂Â∞ÜÂ§öÊ®°ÊÄÅÂÜÖÂÆπËßÜ‰∏∫Áõ∏‰∫íÂÖ≥ËÅîÁöÑÁü•ËØÜÂÆû‰ΩìÔºåËÄå‰∏çÊòØÂ≠§Á´ãÁöÑÊï∞ÊçÆÁ±ªÂûãÔºå‰ªéËÄåÂÆûÁé∞ÂÖ®Èù¢ÁöÑÁü•ËØÜÊ£ÄÁ¥¢„ÄÇÈÄöËøáÂèåÂõæÊûÑÂª∫ÂíåË∑®Ê®°ÊÄÅÊ∑∑ÂêàÊ£ÄÁ¥¢ÔºåRAG-AnythingËÉΩÂ§üÊúâÊïàÂú∞Â§ÑÁêÜÂºÇÊûÑÂÜÖÂÆπÔºåÊèêÂçá‰∫ÜÂ§öÊ®°ÊÄÅÁü•ËØÜËÆøÈóÆÁöÑËÉΩÂäõ„ÄÇ","title":"RAG-AnythingÔºöÂ§öÊ®°ÊÄÅÁü•ËØÜÊ£ÄÁ¥¢ÁöÑÊñ∞ËåÉÂºè"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='RAG-AnythingÊòØ‰∏Ä‰∏™Áªü‰∏ÄÊ°ÜÊû∂ÔºåÊó®Âú®ÈÄöËøáÊï¥ÂêàË∑®Ê®°ÊÄÅÂÖ≥Á≥ªÂíåËØ≠‰πâÂåπÈÖçÊù•Â¢ûÂº∫Â§öÊ®°ÊÄÅÁü•ËØÜÊ£ÄÁ¥¢„ÄÇÂÆÉËß£ÂÜ≥‰∫ÜÁé∞ÊúâRAGÊñπÊ≥ïÂú®Â§ÑÁêÜÂ§çÊùÇÂü∫ÂáÜÊó∂ÁöÑ‰∏çË∂≥ÔºåÁâπÂà´ÊòØÂú®ÈïøÊñáÊ°£‰∏äË°®Áé∞Âá∫Ëâ≤„ÄÇËØ•Ê°ÜÊû∂Â∞ÜÂ§öÊ®°ÊÄÅÂÜÖÂÆπËßÜ‰∏∫Áõ∏‰∫íÂÖ≥ËÅîÁöÑÁü•ËØÜÂÆû‰ΩìÔºåËÄå‰∏çÊòØÂ≠§Á´ãÁöÑÊï∞ÊçÆÁ±ªÂûãÔºå‰ªéËÄåÂÆûÁé∞ÂÖ®Èù¢ÁöÑÁü•ËØÜÊ£ÄÁ¥¢„ÄÇÈÄöËøáÂèåÂõæÊûÑÂª∫ÂíåË∑®Ê®°ÊÄÅÊ∑∑ÂêàÊ£ÄÁ¥¢ÔºåRAG-AnythingËÉΩÂ§üÊúâÊïàÂú∞Â§ÑÁêÜÂºÇÊûÑÂÜÖÂÆπÔºåÊèêÂçá‰∫ÜÂ§öÊ®°ÊÄÅÁü•ËØÜËÆøÈóÆÁöÑËÉΩÂäõ„ÄÇ', title='RAG-AnythingÔºöÂ§öÊ®°ÊÄÅÁü•ËØÜÊ£ÄÁ¥¢ÁöÑÊñ∞ËåÉÂºè'))
[15.10.2025 14:14] Using data from previous issue: {"categories": ["#rl", "#benchmark", "#reasoning", "#optimization", "#games", "#agents"], "emoji": "üéÆ", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –º–∏—Ä–∞ –∑–∞ –æ–¥–Ω—É –∂–∏–∑–Ω—å –±–µ–∑ –ø–æ–¥—Å–∫–∞–∑–æ–∫", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç OneLife ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Å–∏–º–≤–æ–ª—å–Ω–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∏–Ω–∞–º–∏–∫–∏ —Å–ª–æ–∂–Ω—ã—Ö —Å—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–∏—Ö —Å—Ä–µ–¥ —á–µ—Ä–µ–∑ –ø—Ä–æ–≥—Ä–∞
[15.10.2025 14:14] Using data from previous issue: {"categories": ["#hallucinations", "#agents", "#long_context", "#rag"], "emoji": "üîÆ", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ AI –∞–≥–µ–Ω—Ç–æ–≤ —Å –≤–Ω–µ—à–Ω–∏–º–∏ –∑–Ω–∞–Ω–∏—è–º–∏ –≤–º–µ—Å—Ç–æ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π", "desc": "LLM –º–æ–≥—É—Ç —Å–ª—É–∂–∏—Ç—å –º–æ–¥–µ–ª—è–º–∏ –º–∏—Ä–∞ –¥–ª—è –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –¥–µ–π—Å—Ç–≤–∏–π AI-–∞–≥–µ–Ω—Ç–æ–≤ –≤ —Ü–∏—Ñ—Ä–æ–≤—ã—Ö —Å—Ä–µ–¥–∞—Ö, –Ω–æ —Å—Ç—Ä–∞–¥–∞—é—Ç –æ—Ç –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π –∏ —É—Å—Ç–∞—Ä–µ–≤—à
[15.10.2025 14:14] Querying the API.
[15.10.2025 14:14] Claude request. Model: claude-sonnet-4-5-20250929. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

DR agents based on LLMs can generate detailed reports from harmful queries, highlighting alignment failures and the need for specialized safety measures.  					AI-generated summary 				 Deep Research (DR) agents built on Large Language Models (LLMs) can perform complex, multi-step research by decomposing tasks, retrieving online information, and synthesizing detailed reports. However, the misuse of LLMs with such powerful capabilities can lead to even greater risks. This is especially concerning in high-stakes and knowledge-intensive domains such as biosecurity, where DR can generate a professional report containing detailed forbidden knowledge. Unfortunately, we have found such risks in practice: simply submitting a harmful query, which a standalone LLM directly rejects, can elicit a detailed and dangerous report from DR agents. This highlights the elevated risks and underscores the need for a deeper safety analysis. Yet, jailbreak methods designed for LLMs fall short in exposing such unique risks, as they do not target the research ability of DR agents. To address this gap, we propose two novel jailbreak strategies: Plan Injection, which injects malicious sub-goals into the agent's plan; and Intent Hijack, which reframes harmful queries as academic research questions. We conducted extensive experiments across different LLMs and various safety benchmarks, including general and biosecurity forbidden prompts. These experiments reveal 3 key findings: (1) Alignment of the LLMs often fail in DR agents, where harmful prompts framed in academic terms can hijack agent intent; (2) Multi-step planning and execution weaken the alignment, revealing systemic vulnerabilities that prompt-level safeguards cannot address; (3) DR agents not only bypass refusals but also produce more coherent, professional, and dangerous content, compared with standalone LLMs. These results demonstrate a fundamental misalignment in DR agents and call for better alignment techniques tailored to DR agents. Code and datasets are available at https://chenxshuo.github.io/deeper-harm.
[15.10.2025 14:14] Response: ```json
{
  "title": "–û–ø–∞—Å–Ω–æ—Å—Ç—å –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö AI-–∞–≥–µ–Ω—Ç–æ–≤: –∫–æ–≥–¥–∞ —É–º–Ω—ã–µ –ø–æ–º–æ—â–Ω–∏–∫–∏ –æ–±—Ö–æ–¥—è—Ç –∑–∞—â–∏—Ç—É",
  "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏ –∫—Ä–∏—Ç–∏—á–µ—Å–∫—É—é —É—è–∑–≤–∏–º–æ—Å—Ç—å –≤ Deep Research –∞–≥–µ–Ω—Ç–∞—Ö –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –¥–µ—Ç–∞–ª—å–Ω—ã–µ –æ–ø–∞—Å–Ω—ã–µ –æ—Ç—á—ë—Ç—ã –ø–æ –≤—Ä–µ–¥–æ–Ω–æ—Å–Ω—ã–º –∑–∞–ø—Ä–æ—Å–∞–º, –¥–∞–∂–µ –µ—Å–ª–∏ –æ–±—ã—á–Ω–∞—è –º–æ–¥–µ–ª—å –∏—Ö –±–ª–æ–∫–∏—Ä—É–µ—Ç. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω—ã –¥–≤–µ –Ω–æ–≤—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –¥–∂–µ–π–ª–±—Ä–µ–π–∫–∞: Plan Injection (–≤–Ω–µ–¥—Ä–µ–Ω–∏–µ –≤—Ä–µ–¥–æ–Ω–æ—Å–Ω—ã—Ö –ø–æ–¥—Ü–µ–ª–µ–π –≤ –ø–ª–∞–Ω –∞–≥–µ–Ω—Ç–∞) –∏ Intent Hijack (–º–∞—Å–∫–∏—Ä–æ–≤–∫–∞ –æ–ø–∞—Å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –ø–æ–¥ –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è). –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –º–Ω–æ–≥–æ—à–∞–≥–æ–≤–æ–µ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–¥–∞—á –æ—Å–ª–∞–±–ª—è–µ—Ç alignment –º–æ–¥–µ–ª–∏, –∞ DR-–∞–≥–µ–Ω—Ç—ã —Å–æ–∑–¥–∞—é—Ç –±–æ–ª–µ–µ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –∏ –æ–ø–∞—Å–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç, —á–µ–º standalone LLM. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—É—é –ø—Ä–æ–±–ª–µ–º—É –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ DR-–∞–≥–µ–Ω—Ç–æ–≤ –∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤ alignment –¥–ª—è —Ç–∞–∫–∏—Ö —Å–∏—Å—Ç–µ–º.",
  "emoji": "üî¨"
}
```
[15.10.2025 14:14] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"DR agents based on LLMs can generate detailed reports from harmful queries, highlighting alignment failures and the need for specialized safety measures.  					AI-generated summary 				 Deep Research (DR) agents built on Large Language Models (LLMs) can perform complex, multi-step research by decomposing tasks, retrieving online information, and synthesizing detailed reports. However, the misuse of LLMs with such powerful capabilities can lead to even greater risks. This is especially concerning in high-stakes and knowledge-intensive domains such as biosecurity, where DR can generate a professional report containing detailed forbidden knowledge. Unfortunately, we have found such risks in practice: simply submitting a harmful query, which a standalone LLM directly rejects, can elicit a detailed and dangerous report from DR agents. This highlights the elevated risks and underscores the need for a deeper safety analysis. Yet, jailbreak methods designed for LLMs fall short in exposing such unique risks, as they do not target the research ability of DR agents. To address this gap, we propose two novel jailbreak strategies: Plan Injection, which injects malicious sub-goals into the agent's plan; and Intent Hijack, which reframes harmful queries as academic research questions. We conducted extensive experiments across different LLMs and various safety benchmarks, including general and biosecurity forbidden prompts. These experiments reveal 3 key findings: (1) Alignment of the LLMs often fail in DR agents, where harmful prompts framed in academic terms can hijack agent intent; (2) Multi-step planning and execution weaken the alignment, revealing systemic vulnerabilities that prompt-level safeguards cannot address; (3) DR agents not only bypass refusals but also produce more coherent, professional, and dangerous content, compared with standalone LLMs. These results demonstrate a fundamental misalignment in DR agents and call for better alignment techniques tailored to DR agents. Code and datasets are available at https://chenxshuo.github.io/deeper-harm."

[15.10.2025 14:14] Response: ```python
['AGENTS', 'RLHF', 'BENCHMARK', 'DATASET']
```
[15.10.2025 14:14] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"DR agents based on LLMs can generate detailed reports from harmful queries, highlighting alignment failures and the need for specialized safety measures.  					AI-generated summary 				 Deep Research (DR) agents built on Large Language Models (LLMs) can perform complex, multi-step research by decomposing tasks, retrieving online information, and synthesizing detailed reports. However, the misuse of LLMs with such powerful capabilities can lead to even greater risks. This is especially concerning in high-stakes and knowledge-intensive domains such as biosecurity, where DR can generate a professional report containing detailed forbidden knowledge. Unfortunately, we have found such risks in practice: simply submitting a harmful query, which a standalone LLM directly rejects, can elicit a detailed and dangerous report from DR agents. This highlights the elevated risks and underscores the need for a deeper safety analysis. Yet, jailbreak methods designed for LLMs fall short in exposing such unique risks, as they do not target the research ability of DR agents. To address this gap, we propose two novel jailbreak strategies: Plan Injection, which injects malicious sub-goals into the agent's plan; and Intent Hijack, which reframes harmful queries as academic research questions. We conducted extensive experiments across different LLMs and various safety benchmarks, including general and biosecurity forbidden prompts. These experiments reveal 3 key findings: (1) Alignment of the LLMs often fail in DR agents, where harmful prompts framed in academic terms can hijack agent intent; (2) Multi-step planning and execution weaken the alignment, revealing systemic vulnerabilities that prompt-level safeguards cannot address; (3) DR agents not only bypass refusals but also produce more coherent, professional, and dangerous content, compared with standalone LLMs. These results demonstrate a fundamental misalignment in DR agents and call for better alignment techniques tailored to DR agents. Code and datasets are available at https://chenxshuo.github.io/deeper-harm."

[15.10.2025 14:14] Response: ```python
['ALIGNMENT', 'SECURITY', 'ETHICS']
```
[15.10.2025 14:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper discusses the risks associated with Deep Research (DR) agents that utilize Large Language Models (LLMs) to generate detailed reports from complex queries. It highlights how these agents can produce harmful content when given seemingly academic prompts, revealing a failure in alignment with safety protocols. The authors propose two new jailbreak strategies, Plan Injection and Intent Hijack, to expose these vulnerabilities in DR agents. The findings indicate that the multi-step capabilities of DR agents can lead to more coherent and dangerous outputs, necessitating improved alignment techniques for safety.","title":"Unmasking Risks: Enhancing Safety in Deep Research Agents"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper discusses the risks associated with Deep Research (DR) agents that utilize Large Language Models (LLMs) to generate detailed reports from complex queries. It highlights how these agents can produce harmful content when given seemingly academic prompts, revealing a failure in alignment with safety protocols. The authors propose two new jailbreak strategies, Plan Injection and Intent Hijack, to expose these vulnerabilities in DR agents. The findings indicate that the multi-step capabilities of DR agents can lead to more coherent and dangerous outputs, necessitating improved alignment techniques for safety.', title='Unmasking Risks: Enhancing Safety in Deep Research Agents'))
[15.10.2025 14:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Âü∫‰∫éÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÁöÑÊ∑±Â∫¶Á†îÁ©∂ÔºàDRÔºâ‰ª£ÁêÜÂèØ‰ª•‰ªéÊúâÂÆ≥Êü•ËØ¢‰∏≠ÁîüÊàêËØ¶ÁªÜÊä•ÂëäÔºåÊè≠Á§∫‰∫ÜÂØπÈΩêÂ§±Ë¥•ÂíåÈúÄË¶Å‰∏ìÈó®ÂÆâÂÖ®Êé™ÊñΩÁöÑÂøÖË¶ÅÊÄß„ÄÇÂ∞ΩÁÆ°LLMsÂÖ∑ÊúâÂº∫Â§ßÁöÑËÉΩÂäõÔºå‰ΩÜÂÖ∂ËØØÁî®ÂèØËÉΩÂØºËá¥Êõ¥Â§ßÁöÑÈ£éÈô©ÔºåÂ∞§ÂÖ∂ÊòØÂú®ÁîüÁâ©ÂÆâÂÖ®Á≠âÈ´òÈ£éÈô©È¢ÜÂüü„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÁÆÄÂçïÁöÑÊúâÂÆ≥Êü•ËØ¢ÂèØ‰ª•ÂºïÂèëDR‰ª£ÁêÜÁîüÊàêÂç±Èô©ÁöÑÊä•ÂëäÔºåËøôÁ™ÅÊòæ‰∫ÜÂÆâÂÖ®ÂàÜÊûêÁöÑÂøÖË¶ÅÊÄß„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏ÄÈóÆÈ¢òÔºåÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏§ÁßçÊñ∞ÁöÑË∂äÁã±Á≠ñÁï•Ôºå‰ª•Â∫îÂØπDR‰ª£ÁêÜÁöÑÁã¨ÁâπÈ£éÈô©„ÄÇ","title":"Ê∑±Â∫¶Á†îÁ©∂‰ª£ÁêÜÁöÑÂÆâÂÖ®ÈöêÊÇ£‰∏éÂØπÁ≠ñ"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Âü∫‰∫éÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÁöÑÊ∑±Â∫¶Á†îÁ©∂ÔºàDRÔºâ‰ª£ÁêÜÂèØ‰ª•‰ªéÊúâÂÆ≥Êü•ËØ¢‰∏≠ÁîüÊàêËØ¶ÁªÜÊä•ÂëäÔºåÊè≠Á§∫‰∫ÜÂØπÈΩêÂ§±Ë¥•ÂíåÈúÄË¶Å‰∏ìÈó®ÂÆâÂÖ®Êé™ÊñΩÁöÑÂøÖË¶ÅÊÄß„ÄÇÂ∞ΩÁÆ°LLMsÂÖ∑ÊúâÂº∫Â§ßÁöÑËÉΩÂäõÔºå‰ΩÜÂÖ∂ËØØÁî®ÂèØËÉΩÂØºËá¥Êõ¥Â§ßÁöÑÈ£éÈô©ÔºåÂ∞§ÂÖ∂ÊòØÂú®ÁîüÁâ©ÂÆâÂÖ®Á≠âÈ´òÈ£éÈô©È¢ÜÂüü„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÁÆÄÂçïÁöÑÊúâÂÆ≥Êü•ËØ¢ÂèØ‰ª•ÂºïÂèëDR‰ª£ÁêÜÁîüÊàêÂç±Èô©ÁöÑÊä•ÂëäÔºåËøôÁ™ÅÊòæ‰∫ÜÂÆâÂÖ®ÂàÜÊûêÁöÑÂøÖË¶ÅÊÄß„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏ÄÈóÆÈ¢òÔºåÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏§ÁßçÊñ∞ÁöÑË∂äÁã±Á≠ñÁï•Ôºå‰ª•Â∫îÂØπDR‰ª£ÁêÜÁöÑÁã¨ÁâπÈ£éÈô©„ÄÇ', title='Ê∑±Â∫¶Á†îÁ©∂‰ª£ÁêÜÁöÑÂÆâÂÖ®ÈöêÊÇ£‰∏éÂØπÁ≠ñ'))
[15.10.2025 14:14] Using data from previous issue: {"categories": ["#cv", "#science", "#benchmark", "#open_source", "#reasoning", "#multimodal"], "emoji": "üî¨", "ru": {"title": "–ù–∞—É—á–Ω—ã–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã ‚Äî —Å–ª–∞–±–æ–µ –º–µ—Å—Ç–æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö AI", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ ExpVid ‚Äî –ø–µ—Ä–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö LLM –Ω–∞ –≤–∏–¥–µ–æ –Ω–∞—É—á–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤
[15.10.2025 14:14] Querying the API.
[15.10.2025 14:14] Claude request. Model: claude-sonnet-4-5-20250929. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Reasoning-based safety guardrails in Large Reasoning Models are vulnerable to subtle prompt manipulations, leading to high attack success rates across various benchmarks.  					AI-generated summary 				 Recent reasoning-based safety guardrails for Large Reasoning Models (LRMs), such as deliberative alignment, have shown strong defense against jailbreak attacks. By leveraging LRMs' reasoning ability, these guardrails help the models to assess the safety of user inputs before generating final responses. The powerful reasoning ability can analyze the intention of the input query and will refuse to assist once it detects the harmful intent hidden by the jailbreak methods. Such guardrails have shown a significant boost in defense, such as the near-perfect refusal rates on the open-source gpt-oss series. Unfortunately, we find that these powerful reasoning-based guardrails can be extremely vulnerable to subtle manipulation of the input prompts, and once hijacked, can lead to even more harmful results. Specifically, we first uncover a surprisingly fragile aspect of these guardrails: simply adding a few template tokens to the input prompt can successfully bypass the seemingly powerful guardrails and lead to explicit and harmful responses. To explore further, we introduce a bag of jailbreak methods that subvert the reasoning-based guardrails. Our attacks span white-, gray-, and black-box settings and range from effortless template manipulations to fully automated optimization. Along with the potential for scalable implementation, these methods also achieve alarmingly high attack success rates (e.g., exceeding 90% across 5 different benchmarks on gpt-oss series on both local host models and online API services). Evaluations across various leading open-source LRMs confirm that these vulnerabilities are systemic, underscoring the urgent need for stronger alignment techniques for open-sourced LRMs to prevent malicious misuse. Code is open-sourced at https://chenxshuo.github.io/bag-of-tricks.
[15.10.2025 14:14] Response: ```json
{
  "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏ –∫—Ä–∏—Ç–∏—á–µ—Å–∫—É—é —É—è–∑–≤–∏–º–æ—Å—Ç—å –≤ —Å–∏—Å—Ç–µ–º–∞—Ö –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ–º (LRM), –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã—Ö –Ω–∞ deliberative alignment. –û–∫–∞–∑–∞–ª–æ—Å—å, —á—Ç–æ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤—Å–µ–≥–æ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –≤ –ø—Ä–æ–º–ø—Ç –º–æ–∂–µ—Ç –ø–æ–ª–Ω–æ—Å—Ç—å—é –æ–±–æ–π—Ç–∏ –∑–∞—â–∏—Ç—É, –∫–æ—Ç–æ—Ä–∞—è –∫–∞–∑–∞–ª–∞—Å—å –ø–æ—á—Ç–∏ –∏–¥–µ–∞–ª—å–Ω–æ–π. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã –∞—Ç–∞–∫ —Ä–∞–±–æ—Ç–∞—é—Ç –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ä–µ–∂–∏–º–∞—Ö (white-box, gray-box, black-box) –∏ –¥–æ—Å—Ç–∏–≥–∞—é—Ç –±–æ–ª–µ–µ 90% —É—Å–ø–µ—Ö–∞ –Ω–∞ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–¥—á—ë—Ä–∫–∏–≤–∞—é—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –±–æ–ª–µ–µ –Ω–∞–¥—ë–∂–Ω—ã—Ö —Ç–µ—Ö–Ω–∏–∫ alignment –¥–ª—è –æ—Ç–∫—Ä—ã—Ç—ã—Ö LRM, —á—Ç–æ–±—ã –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—Ç–∏—Ç—å –∑–ª–æ–Ω–∞–º–µ—Ä–µ–Ω–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ.",
  "emoji": "üîì",
  "title": "–•—Ä—É–ø–∫–∞—è –±—Ä–æ–Ω—è: –∫–∞–∫ –ø—Ä–æ—Å—Ç—ã–µ —Ç–æ–∫–µ–Ω—ã –ª–æ–º–∞—é—Ç –∑–∞—â–∏—Ç—É reasoning-–º–æ–¥–µ–ª–µ–π"
}
```
[15.10.2025 14:14] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Reasoning-based safety guardrails in Large Reasoning Models are vulnerable to subtle prompt manipulations, leading to high attack success rates across various benchmarks.  					AI-generated summary 				 Recent reasoning-based safety guardrails for Large Reasoning Models (LRMs), such as deliberative alignment, have shown strong defense against jailbreak attacks. By leveraging LRMs' reasoning ability, these guardrails help the models to assess the safety of user inputs before generating final responses. The powerful reasoning ability can analyze the intention of the input query and will refuse to assist once it detects the harmful intent hidden by the jailbreak methods. Such guardrails have shown a significant boost in defense, such as the near-perfect refusal rates on the open-source gpt-oss series. Unfortunately, we find that these powerful reasoning-based guardrails can be extremely vulnerable to subtle manipulation of the input prompts, and once hijacked, can lead to even more harmful results. Specifically, we first uncover a surprisingly fragile aspect of these guardrails: simply adding a few template tokens to the input prompt can successfully bypass the seemingly powerful guardrails and lead to explicit and harmful responses. To explore further, we introduce a bag of jailbreak methods that subvert the reasoning-based guardrails. Our attacks span white-, gray-, and black-box settings and range from effortless template manipulations to fully automated optimization. Along with the potential for scalable implementation, these methods also achieve alarmingly high attack success rates (e.g., exceeding 90% across 5 different benchmarks on gpt-oss series on both local host models and online API services). Evaluations across various leading open-source LRMs confirm that these vulnerabilities are systemic, underscoring the urgent need for stronger alignment techniques for open-sourced LRMs to prevent malicious misuse. Code is open-sourced at https://chenxshuo.github.io/bag-of-tricks."

[15.10.2025 14:14] Response: ```python
['BENCHMARK', 'RLHF', 'ARCHITECTURE']
```
[15.10.2025 14:14] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Reasoning-based safety guardrails in Large Reasoning Models are vulnerable to subtle prompt manipulations, leading to high attack success rates across various benchmarks.  					AI-generated summary 				 Recent reasoning-based safety guardrails for Large Reasoning Models (LRMs), such as deliberative alignment, have shown strong defense against jailbreak attacks. By leveraging LRMs' reasoning ability, these guardrails help the models to assess the safety of user inputs before generating final responses. The powerful reasoning ability can analyze the intention of the input query and will refuse to assist once it detects the harmful intent hidden by the jailbreak methods. Such guardrails have shown a significant boost in defense, such as the near-perfect refusal rates on the open-source gpt-oss series. Unfortunately, we find that these powerful reasoning-based guardrails can be extremely vulnerable to subtle manipulation of the input prompts, and once hijacked, can lead to even more harmful results. Specifically, we first uncover a surprisingly fragile aspect of these guardrails: simply adding a few template tokens to the input prompt can successfully bypass the seemingly powerful guardrails and lead to explicit and harmful responses. To explore further, we introduce a bag of jailbreak methods that subvert the reasoning-based guardrails. Our attacks span white-, gray-, and black-box settings and range from effortless template manipulations to fully automated optimization. Along with the potential for scalable implementation, these methods also achieve alarmingly high attack success rates (e.g., exceeding 90% across 5 different benchmarks on gpt-oss series on both local host models and online API services). Evaluations across various leading open-source LRMs confirm that these vulnerabilities are systemic, underscoring the urgent need for stronger alignment techniques for open-sourced LRMs to prevent malicious misuse. Code is open-sourced at https://chenxshuo.github.io/bag-of-tricks."

[15.10.2025 14:14] Response: ```python
['REASONING', 'ALIGNMENT', 'SECURITY', 'OPEN_SOURCE']
```
[15.10.2025 14:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper discusses the vulnerabilities of reasoning-based safety guardrails in Large Reasoning Models (LRMs) against subtle prompt manipulations. Although these guardrails, like deliberative alignment, are designed to enhance safety by analyzing user inputs for harmful intent, they can be easily bypassed with minor changes to the input prompts. The authors demonstrate that simple template modifications can lead to high attack success rates, revealing a significant flaw in the guardrails\' effectiveness. The study emphasizes the need for improved alignment techniques to safeguard LRMs from malicious exploitation.","title":"Strengthening Safety: Guardrails Are Not Foolproof!"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="This paper discusses the vulnerabilities of reasoning-based safety guardrails in Large Reasoning Models (LRMs) against subtle prompt manipulations. Although these guardrails, like deliberative alignment, are designed to enhance safety by analyzing user inputs for harmful intent, they can be easily bypassed with minor changes to the input prompts. The authors demonstrate that simple template modifications can lead to high attack success rates, revealing a significant flaw in the guardrails' effectiveness. The study emphasizes the need for improved alignment techniques to safeguard LRMs from malicious exploitation.", title='Strengthening Safety: Guardrails Are Not Foolproof!'))
[15.10.2025 14:15] Response: ParsedChatCompletionMessage[Article](content='{"desc":"ËøôÁØáËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÂ§ßÂûãÊé®ÁêÜÊ®°Âûã‰∏≠ÁöÑÂü∫‰∫éÊé®ÁêÜÁöÑÂÆâÂÖ®Èò≤Êä§Êé™ÊñΩÁöÑËÑÜÂº±ÊÄß„ÄÇÂ∞ΩÁÆ°Ëøô‰∫õÈò≤Êä§Êé™ÊñΩÂú®ÊäµÂæ°Ë∂äÁã±ÊîªÂáªÊñπÈù¢Ë°®Áé∞ËâØÂ•ΩÔºå‰ΩÜÁ†îÁ©∂ÂèëÁé∞ÔºåËæìÂÖ•ÊèêÁ§∫ÁöÑÂæÆÂ∞èÊìçÊéßÂèØ‰ª•ËΩªÊòìÁªïËøáËøô‰∫õÈò≤Êä§„ÄÇ‰ΩúËÄÖÊèêÂá∫‰∫Ü‰∏ÄÁ≥ªÂàóË∂äÁã±ÊñπÊ≥ïÔºåËøô‰∫õÊñπÊ≥ïÂú®‰∏çÂêåÁöÑÁéØÂ¢É‰∏ãÈÉΩËÉΩÊàêÂäüÊîªÂáªÔºåÊàêÂäüÁéáÈ´òËææ90%‰ª•‰∏ä„ÄÇËÆ∫ÊñáÂº∫Ë∞É‰∫ÜËø´ÂàáÈúÄË¶ÅÊõ¥Âº∫ÁöÑÂØπÈΩêÊäÄÊúØÔºå‰ª•Èò≤Ê≠¢ÂºÄÊ∫êÂ§ßÂûãÊé®ÁêÜÊ®°ÂûãÁöÑÊÅ∂ÊÑèÊª•Áî®„ÄÇ","title":"Â¢ûÂº∫Â§ßÂûãÊé®ÁêÜÊ®°ÂûãÁöÑÂÆâÂÖ®Èò≤Êä§ÔºåÊäµÂæ°ÂæÆÂ¶ôÊîªÂáªÔºÅ"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='ËøôÁØáËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÂ§ßÂûãÊé®ÁêÜÊ®°Âûã‰∏≠ÁöÑÂü∫‰∫éÊé®ÁêÜÁöÑÂÆâÂÖ®Èò≤Êä§Êé™ÊñΩÁöÑËÑÜÂº±ÊÄß„ÄÇÂ∞ΩÁÆ°Ëøô‰∫õÈò≤Êä§Êé™ÊñΩÂú®ÊäµÂæ°Ë∂äÁã±ÊîªÂáªÊñπÈù¢Ë°®Áé∞ËâØÂ•ΩÔºå‰ΩÜÁ†îÁ©∂ÂèëÁé∞ÔºåËæìÂÖ•ÊèêÁ§∫ÁöÑÂæÆÂ∞èÊìçÊéßÂèØ‰ª•ËΩªÊòìÁªïËøáËøô‰∫õÈò≤Êä§„ÄÇ‰ΩúËÄÖÊèêÂá∫‰∫Ü‰∏ÄÁ≥ªÂàóË∂äÁã±ÊñπÊ≥ïÔºåËøô‰∫õÊñπÊ≥ïÂú®‰∏çÂêåÁöÑÁéØÂ¢É‰∏ãÈÉΩËÉΩÊàêÂäüÊîªÂáªÔºåÊàêÂäüÁéáÈ´òËææ90%‰ª•‰∏ä„ÄÇËÆ∫ÊñáÂº∫Ë∞É‰∫ÜËø´ÂàáÈúÄË¶ÅÊõ¥Âº∫ÁöÑÂØπÈΩêÊäÄÊúØÔºå‰ª•Èò≤Ê≠¢ÂºÄÊ∫êÂ§ßÂûãÊé®ÁêÜÊ®°ÂûãÁöÑÊÅ∂ÊÑèÊª•Áî®„ÄÇ', title='Â¢ûÂº∫Â§ßÂûãÊé®ÁêÜÊ®°ÂûãÁöÑÂÆâÂÖ®Èò≤Êä§ÔºåÊäµÂæ°ÂæÆÂ¶ôÊîªÂáªÔºÅ'))
[15.10.2025 14:15] Using data from previous issue: {"categories": ["#hallucinations", "#reasoning", "#benchmark", "#data", "#training"], "emoji": "üõ°Ô∏è", "ru": {"title": "–ó–∞—â–∏—Ç–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π LLM –æ—Ç –∫—Ä–∞–∂–∏ —á–µ—Ä–µ–∑ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞–Ω–∏–µ", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –º–µ—Ç–æ–¥ PART –¥–ª—è –∑–∞—â–∏—Ç—ã –¥–µ—Ç–∞–ª—å–Ω—ã—Ö reasoning-—Ü–µ–ø–æ—á–µ–∫ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –æ—Ç –Ω–µ—Å–∞–Ω–∫—Ü–∏–æ–Ω–∏
[15.10.2025 14:15] Using data from previous issue: {"categories": ["#optimization", "#security", "#multimodal", "#benchmark", "#cv"], "emoji": "üîè", "ru": {"title": "SynthID-Image: –Ω–µ–≤–∏–¥–∏–º—ã–µ –≤–æ–¥—è–Ω—ã–µ –∑–Ω–∞–∫–∏ –¥–ª—è AI-–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ –º–∞—Å—à—Ç–∞–±–µ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ SynthID-Image ‚Äî —Å–∏—Å—Ç–µ–º—É –Ω–∞ –æ—Å–Ω–æ–≤–µ –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –Ω–µ–≤–∏–¥–∏–º–æ–π –≤–æ–¥
[15.10.2025 14:15] Using data from previous issue: {"categories": ["#optimization", "#reasoning", "#math", "#training", "#interpretability"], "emoji": "üîç", "ru": {"title": "–ü—Ä–æ–∑—Ä–∞—á–Ω—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è: –∫–∞–∫ –Ω–∞—É—á–∏—Ç—å AI –æ–±—ä—è—Å–Ω—è—Ç—å —Å–≤–æ–∏ —Ä–µ—à–µ–Ω–∏—è", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω ReFIne ‚Äî –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π –¥–µ–ª–∞–µ—Ç reasoning –º–æ–¥–µ–ª–∏ –±–æ–ª–µ–µ –Ω–∞–¥—ë–∂–Ω—ã–º–∏ –∏ –ø–æ–Ω
[15.10.2025 14:15] Using data from previous issue: {"categories": ["#alignment", "#interpretability", "#multimodal", "#benchmark"], "emoji": "üé®", "ru": {"title": "LLM –∫–∞–∫ —Ä–∞–Ω–Ω–∏–µ –æ—Ü–µ–Ω—â–∏–∫–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–≤", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–≤–µ—Ä—è–µ—Ç, –º–æ–≥—É—Ç –ª–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ LLM (–±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏) –æ—Ü–µ–Ω–∏–≤–∞—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å—ã —Ç–∞–∫ –∂–µ,
[15.10.2025 14:15] Using data from previous issue: {"categories": ["#architecture", "#reasoning", "#interpretability", "#plp", "#agi"], "emoji": "üîó", "ru": {"title": "–¢–µ–Ω–∑–æ—Ä–Ω–∞—è –ª–æ–≥–∏–∫–∞: –º–æ—Å—Ç –º–µ–∂–¥—É –Ω–µ–π—Ä–æ–Ω–Ω—ã–º –∏ —Å–∏–º–≤–æ–ª—å–Ω—ã–º AI", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç tensor logic ‚Äî –Ω–æ–≤—ã–π —è–∑—ã–∫ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ –∏ —Å–∏–º–≤–æ–ª—å–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã –≤ AI —á
[15.10.2025 14:15] Renaming data file.
[15.10.2025 14:15] Renaming previous data. hf_papers.json to ./d/2025-10-15.json
[15.10.2025 14:15] Saving new data file.
[15.10.2025 14:15] Generating page.
[15.10.2025 14:15] Renaming previous page.
[15.10.2025 14:15] Renaming previous data. index.html to ./d/2025-10-15.html
[15.10.2025 14:15] Writing result.
[15.10.2025 14:15] Renaming log file.
[15.10.2025 14:15] Renaming previous data. log.txt to ./logs/2025-10-15_last_log.txt
