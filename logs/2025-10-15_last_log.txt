[15.10.2025 22:10] Read previous papers.
[15.10.2025 22:10] Generating top page (month).
[15.10.2025 22:10] Writing top page (month).
[15.10.2025 23:10] Read previous papers.
[15.10.2025 23:10] Get feed.
[15.10.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.12276
[15.10.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.09116
[15.10.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.12586
[15.10.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.11693
[15.10.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.12403
[15.10.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.12798
[15.10.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.12399
[15.10.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.12747
[15.10.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.12773
[15.10.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.11057
[15.10.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.12693
[15.10.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.12784
[15.10.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.12789
[15.10.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.12635
[15.10.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.11683
[15.10.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.11602
[15.10.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.01171
[15.10.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.12225
[15.10.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.12709
[15.10.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.12801
[15.10.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.11892
[15.10.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.11919
[15.10.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.11000
[15.10.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.12777
[15.10.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.09782
[15.10.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08783
[15.10.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.12402
[15.10.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.12088
[15.10.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.11661
[15.10.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.11606
[15.10.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.09259
[15.10.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.12793
[15.10.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.12497
[15.10.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.12323
[15.10.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.12117
[15.10.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.11851
[15.10.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.11570
[15.10.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.11545
[15.10.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.11330
[15.10.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.09776
[15.10.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.09263
[15.10.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.09062
[15.10.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08532
[15.10.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.12269
[15.10.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08666
[15.10.2025 23:10] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[15.10.2025 23:10] No deleted papers detected.
[15.10.2025 23:10] Downloading and parsing papers (pdf, html). Total: 45.
[15.10.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2510.12276.
[15.10.2025 23:10] Extra JSON file exists (./assets/json/2510.12276.json), skip PDF parsing.
[15.10.2025 23:10] Paper image links file exists (./assets/img_data/2510.12276.json), skip HTML parsing.
[15.10.2025 23:10] Success.
[15.10.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2510.09116.
[15.10.2025 23:10] Extra JSON file exists (./assets/json/2510.09116.json), skip PDF parsing.
[15.10.2025 23:10] Paper image links file exists (./assets/img_data/2510.09116.json), skip HTML parsing.
[15.10.2025 23:10] Success.
[15.10.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2510.12586.
[15.10.2025 23:10] Extra JSON file exists (./assets/json/2510.12586.json), skip PDF parsing.
[15.10.2025 23:10] Paper image links file exists (./assets/img_data/2510.12586.json), skip HTML parsing.
[15.10.2025 23:10] Success.
[15.10.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2510.11693.
[15.10.2025 23:10] Extra JSON file exists (./assets/json/2510.11693.json), skip PDF parsing.
[15.10.2025 23:10] Paper image links file exists (./assets/img_data/2510.11693.json), skip HTML parsing.
[15.10.2025 23:10] Success.
[15.10.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2510.12403.
[15.10.2025 23:10] Extra JSON file exists (./assets/json/2510.12403.json), skip PDF parsing.
[15.10.2025 23:10] Paper image links file exists (./assets/img_data/2510.12403.json), skip HTML parsing.
[15.10.2025 23:10] Success.
[15.10.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2510.12798.
[15.10.2025 23:10] Extra JSON file exists (./assets/json/2510.12798.json), skip PDF parsing.
[15.10.2025 23:10] Paper image links file exists (./assets/img_data/2510.12798.json), skip HTML parsing.
[15.10.2025 23:10] Success.
[15.10.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2510.12399.
[15.10.2025 23:10] Extra JSON file exists (./assets/json/2510.12399.json), skip PDF parsing.
[15.10.2025 23:10] Paper image links file exists (./assets/img_data/2510.12399.json), skip HTML parsing.
[15.10.2025 23:10] Success.
[15.10.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2510.12747.
[15.10.2025 23:10] Extra JSON file exists (./assets/json/2510.12747.json), skip PDF parsing.
[15.10.2025 23:10] Paper image links file exists (./assets/img_data/2510.12747.json), skip HTML parsing.
[15.10.2025 23:10] Success.
[15.10.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2510.12773.
[15.10.2025 23:10] Extra JSON file exists (./assets/json/2510.12773.json), skip PDF parsing.
[15.10.2025 23:10] Paper image links file exists (./assets/img_data/2510.12773.json), skip HTML parsing.
[15.10.2025 23:10] Success.
[15.10.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2510.11057.
[15.10.2025 23:10] Extra JSON file exists (./assets/json/2510.11057.json), skip PDF parsing.
[15.10.2025 23:10] Paper image links file exists (./assets/img_data/2510.11057.json), skip HTML parsing.
[15.10.2025 23:10] Success.
[15.10.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2510.12693.
[15.10.2025 23:10] Extra JSON file exists (./assets/json/2510.12693.json), skip PDF parsing.
[15.10.2025 23:10] Paper image links file exists (./assets/img_data/2510.12693.json), skip HTML parsing.
[15.10.2025 23:10] Success.
[15.10.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2510.12784.
[15.10.2025 23:10] Extra JSON file exists (./assets/json/2510.12784.json), skip PDF parsing.
[15.10.2025 23:10] Paper image links file exists (./assets/img_data/2510.12784.json), skip HTML parsing.
[15.10.2025 23:10] Success.
[15.10.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2510.12789.
[15.10.2025 23:10] Extra JSON file exists (./assets/json/2510.12789.json), skip PDF parsing.
[15.10.2025 23:10] Paper image links file exists (./assets/img_data/2510.12789.json), skip HTML parsing.
[15.10.2025 23:10] Success.
[15.10.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2510.12635.
[15.10.2025 23:10] Extra JSON file exists (./assets/json/2510.12635.json), skip PDF parsing.
[15.10.2025 23:10] Paper image links file exists (./assets/img_data/2510.12635.json), skip HTML parsing.
[15.10.2025 23:10] Success.
[15.10.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2510.11683.
[15.10.2025 23:10] Extra JSON file exists (./assets/json/2510.11683.json), skip PDF parsing.
[15.10.2025 23:10] Paper image links file exists (./assets/img_data/2510.11683.json), skip HTML parsing.
[15.10.2025 23:10] Success.
[15.10.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2510.11602.
[15.10.2025 23:10] Extra JSON file exists (./assets/json/2510.11602.json), skip PDF parsing.
[15.10.2025 23:10] Paper image links file exists (./assets/img_data/2510.11602.json), skip HTML parsing.
[15.10.2025 23:10] Success.
[15.10.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2510.01171.
[15.10.2025 23:10] Extra JSON file exists (./assets/json/2510.01171.json), skip PDF parsing.
[15.10.2025 23:10] Paper image links file exists (./assets/img_data/2510.01171.json), skip HTML parsing.
[15.10.2025 23:10] Success.
[15.10.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2510.12225.
[15.10.2025 23:10] Extra JSON file exists (./assets/json/2510.12225.json), skip PDF parsing.
[15.10.2025 23:10] Paper image links file exists (./assets/img_data/2510.12225.json), skip HTML parsing.
[15.10.2025 23:10] Success.
[15.10.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2510.12709.
[15.10.2025 23:10] Extra JSON file exists (./assets/json/2510.12709.json), skip PDF parsing.
[15.10.2025 23:10] Paper image links file exists (./assets/img_data/2510.12709.json), skip HTML parsing.
[15.10.2025 23:10] Success.
[15.10.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2510.12801.
[15.10.2025 23:10] Extra JSON file exists (./assets/json/2510.12801.json), skip PDF parsing.
[15.10.2025 23:10] Paper image links file exists (./assets/img_data/2510.12801.json), skip HTML parsing.
[15.10.2025 23:10] Success.
[15.10.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2510.11892.
[15.10.2025 23:10] Extra JSON file exists (./assets/json/2510.11892.json), skip PDF parsing.
[15.10.2025 23:10] Paper image links file exists (./assets/img_data/2510.11892.json), skip HTML parsing.
[15.10.2025 23:10] Success.
[15.10.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2510.11919.
[15.10.2025 23:10] Extra JSON file exists (./assets/json/2510.11919.json), skip PDF parsing.
[15.10.2025 23:10] Paper image links file exists (./assets/img_data/2510.11919.json), skip HTML parsing.
[15.10.2025 23:10] Success.
[15.10.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2510.11000.
[15.10.2025 23:10] Downloading paper 2510.11000 from http://arxiv.org/pdf/2510.11000v1...
[15.10.2025 23:10] Failed to download and parse paper https://huggingface.co/papers/2510.11000: 'LTChar' object is not iterable
[15.10.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2510.12777.
[15.10.2025 23:10] Extra JSON file exists (./assets/json/2510.12777.json), skip PDF parsing.
[15.10.2025 23:10] Paper image links file exists (./assets/img_data/2510.12777.json), skip HTML parsing.
[15.10.2025 23:10] Success.
[15.10.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2510.09782.
[15.10.2025 23:10] Extra JSON file exists (./assets/json/2510.09782.json), skip PDF parsing.
[15.10.2025 23:10] Paper image links file exists (./assets/img_data/2510.09782.json), skip HTML parsing.
[15.10.2025 23:10] Success.
[15.10.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2510.08783.
[15.10.2025 23:10] Extra JSON file exists (./assets/json/2510.08783.json), skip PDF parsing.
[15.10.2025 23:10] Paper image links file exists (./assets/img_data/2510.08783.json), skip HTML parsing.
[15.10.2025 23:10] Success.
[15.10.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2510.12402.
[15.10.2025 23:10] Extra JSON file exists (./assets/json/2510.12402.json), skip PDF parsing.
[15.10.2025 23:10] Paper image links file exists (./assets/img_data/2510.12402.json), skip HTML parsing.
[15.10.2025 23:10] Success.
[15.10.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2510.12088.
[15.10.2025 23:10] Extra JSON file exists (./assets/json/2510.12088.json), skip PDF parsing.
[15.10.2025 23:10] Paper image links file exists (./assets/img_data/2510.12088.json), skip HTML parsing.
[15.10.2025 23:10] Success.
[15.10.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2510.11661.
[15.10.2025 23:10] Extra JSON file exists (./assets/json/2510.11661.json), skip PDF parsing.
[15.10.2025 23:10] Paper image links file exists (./assets/img_data/2510.11661.json), skip HTML parsing.
[15.10.2025 23:10] Success.
[15.10.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2510.11606.
[15.10.2025 23:10] Extra JSON file exists (./assets/json/2510.11606.json), skip PDF parsing.
[15.10.2025 23:10] Paper image links file exists (./assets/img_data/2510.11606.json), skip HTML parsing.
[15.10.2025 23:10] Success.
[15.10.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2510.09259.
[15.10.2025 23:10] Extra JSON file exists (./assets/json/2510.09259.json), skip PDF parsing.
[15.10.2025 23:10] Paper image links file exists (./assets/img_data/2510.09259.json), skip HTML parsing.
[15.10.2025 23:10] Success.
[15.10.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2510.12793.
[15.10.2025 23:10] Extra JSON file exists (./assets/json/2510.12793.json), skip PDF parsing.
[15.10.2025 23:10] Paper image links file exists (./assets/img_data/2510.12793.json), skip HTML parsing.
[15.10.2025 23:10] Success.
[15.10.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2510.12497.
[15.10.2025 23:10] Extra JSON file exists (./assets/json/2510.12497.json), skip PDF parsing.
[15.10.2025 23:10] Paper image links file exists (./assets/img_data/2510.12497.json), skip HTML parsing.
[15.10.2025 23:10] Success.
[15.10.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2510.12323.
[15.10.2025 23:10] Extra JSON file exists (./assets/json/2510.12323.json), skip PDF parsing.
[15.10.2025 23:10] Paper image links file exists (./assets/img_data/2510.12323.json), skip HTML parsing.
[15.10.2025 23:10] Success.
[15.10.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2510.12117.
[15.10.2025 23:10] Extra JSON file exists (./assets/json/2510.12117.json), skip PDF parsing.
[15.10.2025 23:10] Paper image links file exists (./assets/img_data/2510.12117.json), skip HTML parsing.
[15.10.2025 23:10] Success.
[15.10.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2510.11851.
[15.10.2025 23:10] Extra JSON file exists (./assets/json/2510.11851.json), skip PDF parsing.
[15.10.2025 23:10] Paper image links file exists (./assets/img_data/2510.11851.json), skip HTML parsing.
[15.10.2025 23:10] Success.
[15.10.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2510.11570.
[15.10.2025 23:10] Extra JSON file exists (./assets/json/2510.11570.json), skip PDF parsing.
[15.10.2025 23:10] Paper image links file exists (./assets/img_data/2510.11570.json), skip HTML parsing.
[15.10.2025 23:10] Success.
[15.10.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2510.11545.
[15.10.2025 23:10] Extra JSON file exists (./assets/json/2510.11545.json), skip PDF parsing.
[15.10.2025 23:10] Paper image links file exists (./assets/img_data/2510.11545.json), skip HTML parsing.
[15.10.2025 23:10] Success.
[15.10.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2510.11330.
[15.10.2025 23:10] Extra JSON file exists (./assets/json/2510.11330.json), skip PDF parsing.
[15.10.2025 23:10] Paper image links file exists (./assets/img_data/2510.11330.json), skip HTML parsing.
[15.10.2025 23:10] Success.
[15.10.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2510.09776.
[15.10.2025 23:10] Extra JSON file exists (./assets/json/2510.09776.json), skip PDF parsing.
[15.10.2025 23:10] Paper image links file exists (./assets/img_data/2510.09776.json), skip HTML parsing.
[15.10.2025 23:10] Success.
[15.10.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2510.09263.
[15.10.2025 23:10] Extra JSON file exists (./assets/json/2510.09263.json), skip PDF parsing.
[15.10.2025 23:10] Paper image links file exists (./assets/img_data/2510.09263.json), skip HTML parsing.
[15.10.2025 23:10] Success.
[15.10.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2510.09062.
[15.10.2025 23:10] Extra JSON file exists (./assets/json/2510.09062.json), skip PDF parsing.
[15.10.2025 23:10] Paper image links file exists (./assets/img_data/2510.09062.json), skip HTML parsing.
[15.10.2025 23:10] Success.
[15.10.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2510.08532.
[15.10.2025 23:10] Extra JSON file exists (./assets/json/2510.08532.json), skip PDF parsing.
[15.10.2025 23:10] Paper image links file exists (./assets/img_data/2510.08532.json), skip HTML parsing.
[15.10.2025 23:10] Success.
[15.10.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2510.12269.
[15.10.2025 23:10] Extra JSON file exists (./assets/json/2510.12269.json), skip PDF parsing.
[15.10.2025 23:10] Paper image links file exists (./assets/img_data/2510.12269.json), skip HTML parsing.
[15.10.2025 23:10] Success.
[15.10.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2510.08666.
[15.10.2025 23:10] Extra JSON file exists (./assets/json/2510.08666.json), skip PDF parsing.
[15.10.2025 23:10] Paper image links file exists (./assets/img_data/2510.08666.json), skip HTML parsing.
[15.10.2025 23:10] Success.
[15.10.2025 23:10] Enriching papers with extra data.
[15.10.2025 23:10] ********************************************************************************
[15.10.2025 23:10] Abstract 0. Vision-language-action (VLA) models have recently shown strong potential in enabling robots to follow language instructions and execute precise actions. However, most VLAs are built upon vision-language models pretrained solely on 2D data, which lack accurate spatial awareness and hinder their abili...
[15.10.2025 23:10] ********************************************************************************
[15.10.2025 23:10] Abstract 1. A new evaluation framework, DITING, and a reasoning-driven multi-agent evaluation framework, AgentEval, are introduced to assess the quality of web novel translations, revealing that Chinese-trained LLMs outperform larger foreign models.  					AI-generated summary 				 Large language models (LLMs) h...
[15.10.2025 23:10] ********************************************************************************
[15.10.2025 23:10] Abstract 2. Pixel-space generative models are often more difficult to train and generally underperform compared to their latent-space counterparts, leaving a persistent performance and efficiency gap. In this paper, we introduce a novel two-stage training framework that closes this gap for pixel-space diffusion...
[15.10.2025 23:10] ********************************************************************************
[15.10.2025 23:10] Abstract 3. Recent multimodal embedding approaches leveraging multimodal large language models (MLLMs) fine-tuned with contrastive learning (CL) have shown promising results, yet the underlying reasons behind their superiority remain underexplored. This work argues that a crucial advantage of MLLM-based approac...
[15.10.2025 23:10] ********************************************************************************
[15.10.2025 23:10] Abstract 4. Robot learning transitions from model-based to data-driven methods, leveraging reinforcement learning and behavioral cloning to develop versatile, language-conditioned models for diverse tasks and robot types.  					AI-generated summary 				 Robot learning is at an inflection point, driven by rapid ...
[15.10.2025 23:10] ********************************************************************************
[15.10.2025 23:10] Abstract 5. Object detection has long been dominated by traditional coordinate regression-based models, such as YOLO, DETR, and Grounding DINO. Although recent efforts have attempted to leverage MLLMs to tackle this task, they face challenges like low recall rate, duplicate predictions, coordinate misalignment,...
[15.10.2025 23:10] ********************************************************************************
[15.10.2025 23:10] Abstract 6. The advancement of large language models (LLMs) has catalyzed a paradigm shift from code generation assistance to autonomous coding agents, enabling a novel development methodology termed "Vibe Coding" where developers validate AI-generated implementations through outcome observation rather than lin...
[15.10.2025 23:10] ********************************************************************************
[15.10.2025 23:10] Abstract 7. Diffusion models have recently advanced video restoration, but applying them to real-world video super-resolution (VSR) remains challenging due to high latency, prohibitive computation, and poor generalization to ultra-high resolutions. Our goal in this work is to make diffusion-based VSR practical ...
[15.10.2025 23:10] ********************************************************************************
[15.10.2025 23:10] Abstract 8. Large Language Models (LLMs) process every token through all layers of a transformer stack, causing wasted computation on simple queries and insufficient flexibility for harder ones that need deeper reasoning. Adaptive-depth methods can improve efficiency, but prior approaches rely on costly inferen...
[15.10.2025 23:10] ********************************************************************************
[15.10.2025 23:10] Abstract 9. Diffusion models have achieved remarkable success as generative models. However, even a well-trained model can accumulate errors throughout the generation process. These errors become particularly problematic when arbitrary guidance is applied to steer samples toward desired properties, which often ...
[15.10.2025 23:10] ********************************************************************************
[15.10.2025 23:10] Abstract 10. Recent advances in embodied AI highlight the potential of vision language models (VLMs) as agents capable of perception, reasoning, and interaction in complex environments. However, top-performing systems rely on large-scale models that are costly to deploy, while smaller VLMs lack the necessary kno...
[15.10.2025 23:10] ********************************************************************************
[15.10.2025 23:10] Abstract 11. Recently, remarkable progress has been made in Unified Multimodal Models (UMMs), which integrate vision-language generation and understanding capabilities within a single framework. However, a significant gap exists where a model's strong visual understanding often fails to transfer to its visual ge...
[15.10.2025 23:10] ********************************************************************************
[15.10.2025 23:10] Abstract 12. Although recent advances in visual generation have been remarkable, most existing architectures still depend on distinct encoders for images and text. This separation constrains diffusion models' ability to perform cross-modal reasoning and knowledge transfer. Prior attempts to bridge this gap often...
[15.10.2025 23:10] ********************************************************************************
[15.10.2025 23:10] Abstract 13. Large Language Models face challenges in long-horizon agentic tasks as their constrained memory is easily overwhelmed by distracting or irrelevant context. Existing working memory methods typically rely on external, heuristic mechanisms that are decoupled from the agent's core policy. In this work, ...
[15.10.2025 23:10] ********************************************************************************
[15.10.2025 23:10] Abstract 14. Boundary-Guided Policy Optimization (BGPO) improves reinforcement learning for diffusion large language models by efficiently approximating likelihoods with a memory-efficient lower bound, enhancing performance in tasks like math problem solving, code generation, and planning.  					AI-generated sum...
[15.10.2025 23:10] ********************************************************************************
[15.10.2025 23:10] Abstract 15. Systematic analysis of attention mechanisms in Transformer models shows that token mixing is essential, while other aspects like sequence dependency and mathematical form can be relaxed or interleaved to maintain performance.  					AI-generated summary 				 The success of Transformer language models...
[15.10.2025 23:10] ********************************************************************************
[15.10.2025 23:10] Abstract 16. Typicality bias in preference data causes mode collapse in LLMs, and Verbalized Sampling is introduced as a prompting strategy to enhance diversity without compromising accuracy or safety.  					AI-generated summary 				 Post-training alignment often reduces LLM diversity, leading to a phenomenon kn...
[15.10.2025 23:10] ********************************************************************************
[15.10.2025 23:10] Abstract 17. Recent advances in vision-language models (VLMs) have made them highly effective at reasoning tasks. However, the principles underlying the construction of performant VL reasoning training datasets remain poorly understood. In this work, we introduce several data curation approaches and study their ...
[15.10.2025 23:10] ********************************************************************************
[15.10.2025 23:10] Abstract 18. Multimodal embedding models aim to yield informative unified representations that empower diverse cross-modal tasks. Despite promising developments in the evolution from CLIP-based dual-tower architectures to large vision-language models, prior works still face unavoidable challenges in real-world a...
[15.10.2025 23:10] ********************************************************************************
[15.10.2025 23:10] Abstract 19. Multimodal Large Language Models (MLLMs) in real-world applications require access to external knowledge sources and must remain responsive to the dynamic and ever-changing real-world information in order to address information-seeking and knowledge-intensive user queries. Existing approaches, such ...
[15.10.2025 23:10] ********************************************************************************
[15.10.2025 23:10] Abstract 20. LLMs can enhance decision-making in digital environments but struggle with long-horizon simulations due to hallucination and static knowledge. R-WoM improves performance by integrating external, up-to-date knowledge.  					AI-generated summary 				 Large Language Models (LLMs) can serve as world mod...
[15.10.2025 23:10] ********************************************************************************
[15.10.2025 23:10] Abstract 21. Large reasoning models (LRMs) have led to new possibilities in terms of problem-solving, through the devising of a natural language thought process prior to answering a query. While their capabilities are well known across mathematics and coding tasks, their impact on the task of machine translation...
[15.10.2025 23:10] ********************************************************************************
[15.10.2025 23:10] Abstract 22. ContextGen, a Diffusion Transformer framework, enhances multi-instance image generation by integrating layout anchoring and identity consistency attention, achieving superior control and quality.  					AI-generated summary 				 Multi-instance image generation (MIG) remains a significant challenge fo...
[15.10.2025 23:10] ********************************************************************************
[15.10.2025 23:10] Abstract 23. Understanding the dynamics of a physical scene involves reasoning about the diverse ways it can potentially change, especially as a result of local interactions. We present the Flow Poke Transformer (FPT), a novel framework for directly predicting the distribution of local motion, conditioned on spa...
[15.10.2025 23:10] ********************************************************************************
[15.10.2025 23:10] Abstract 24. We study how large language models (LLMs) ``think'' through their representation space. We propose a novel geometric framework that models an LLM's reasoning as flows -- embedding trajectories evolving where logic goes. We disentangle logical structure from semantics by employing the same natural de...
[15.10.2025 23:10] ********************************************************************************
[15.10.2025 23:10] Abstract 25. In an ideal design pipeline, user interface (UI) design is intertwined with user research to validate decisions, yet studies are often resource-constrained during early exploration. Recent advances in multimodal large language models (MLLMs) offer a promising opportunity to act as early evaluators, ...
[15.10.2025 23:10] ********************************************************************************
[15.10.2025 23:10] Abstract 26. Cautious Weight Decay (CWD) enhances optimizer performance by applying weight decay selectively, improving accuracy and loss in large-scale models without additional tuning.  					AI-generated summary 				 We introduce Cautious Weight Decay (CWD), a one-line, optimizer-agnostic modification that app...
[15.10.2025 23:10] ********************************************************************************
[15.10.2025 23:10] Abstract 27. OneLife framework models complex, stochastic environments using conditionally-activated programmatic laws within a probabilistic programming framework, enabling learning from minimal, unguided interaction and outperforming baselines in state ranking and fidelity.  					AI-generated summary 				 Symb...
[15.10.2025 23:10] ********************************************************************************
[15.10.2025 23:10] Abstract 28. SR-Scientist, an autonomous AI framework, leverages LLMs to generate, implement, and optimize scientific equations, outperforming baselines across multiple disciplines.  					AI-generated summary 				 Recently, Large Language Models (LLMs) have been applied to scientific equation discovery, leveragi...
[15.10.2025 23:10] ********************************************************************************
[15.10.2025 23:10] Abstract 29. ExpVid, a new benchmark for evaluating multimodal large language models on scientific experiment videos, highlights gaps in fine-grained perception, procedural understanding, and scientific reasoning.  					AI-generated summary 				 Multimodal Large Language Models (MLLMs) hold promise for accelerat...
[15.10.2025 23:10] ********************************************************************************
[15.10.2025 23:10] Abstract 30. Self-Critique addresses data contamination in the RL post-training phase of LLMs by detecting policy collapse, outperforming existing methods with significant improvements in AUC.  					AI-generated summary 				 Data contamination poses a significant threat to the reliable evaluation of Large Langua...
[15.10.2025 23:10] ********************************************************************************
[15.10.2025 23:10] Abstract 31. Existing Multimodal Large Language Models (MLLMs) suffer from increased inference costs due to the additional vision tokens introduced by image inputs. In this work, we propose Visual Consistency Learning (ViCO), a novel training algorithm that enables the model to represent images of varying semant...
[15.10.2025 23:10] ********************************************************************************
[15.10.2025 23:10] Abstract 32. Noise Awareness Guidance (NAG) addresses noise shift in diffusion models by aligning sampling trajectories with the pre-defined noise schedule, improving generation quality.  					AI-generated summary 				 Existing denoising generative models rely on solving discretized reverse-time SDEs or ODEs. In...
[15.10.2025 23:10] ********************************************************************************
[15.10.2025 23:10] Abstract 33. RAG-Anything is a unified framework that enhances multimodal knowledge retrieval by integrating cross-modal relationships and semantic matching, outperforming existing methods on complex benchmarks.  					AI-generated summary 				 Retrieval-Augmented Generation (RAG) has emerged as a fundamental par...
[15.10.2025 23:10] ********************************************************************************
[15.10.2025 23:10] Abstract 34. Chatbot providers (e.g., OpenAI) rely on tiered subscription schemes to generate revenue, offering basic models for free users, and advanced models for paying subscribers. However, a finer-grained pay-to-unlock scheme for premium features (e.g., math, coding) is thought to be more economically viabl...
[15.10.2025 23:10] ********************************************************************************
[15.10.2025 23:10] Abstract 35. DR agents based on LLMs can generate detailed reports from harmful queries, highlighting alignment failures and the need for specialized safety measures.  					AI-generated summary 				 Deep Research (DR) agents built on Large Language Models (LLMs) can perform complex, multi-step research by decomp...
[15.10.2025 23:10] ********************************************************************************
[15.10.2025 23:10] Abstract 36. Reasoning-based safety guardrails in Large Reasoning Models are vulnerable to subtle prompt manipulations, leading to high attack success rates across various benchmarks.  					AI-generated summary 				 Recent reasoning-based safety guardrails for Large Reasoning Models (LRMs), such as deliberative ...
[15.10.2025 23:10] ********************************************************************************
[15.10.2025 23:10] Abstract 37. PART reformulates reasoning traces to preserve information while disrupting unauthorized distillation in Large Language Models.  					AI-generated summary 				 Recent advances in Large Language Models (LLMs) show that extending the length of reasoning chains significantly improves performance on com...
[15.10.2025 23:10] ********************************************************************************
[15.10.2025 23:10] Abstract 38. Diffusion-Link, a diffusion-based modality-bridging module, reduces the audio-text modality gap and enhances multimodal encoder-LLM coupling, achieving state-of-the-art performance in automatic audio captioning.  					AI-generated summary 				 Contrastive audio-language pretraining yields powerful j...
[15.10.2025 23:10] ********************************************************************************
[15.10.2025 23:10] Abstract 39. Theoretical analysis reveals that Transformers, particularly Linear Self-Attention models, have limitations in time series forecasting compared to classical linear models, with predictions collapsing to the mean under Chain-of-Thought inference.  					AI-generated summary 				 Time series forecastin...
[15.10.2025 23:10] ********************************************************************************
[15.10.2025 23:10] Abstract 40. SynthID-Image, a deep learning system for watermarking AI-generated imagery, demonstrates state-of-the-art performance in visual quality and robustness, and is deployed across Google's services.  					AI-generated summary 				 We introduce SynthID-Image, a deep learning-based system for invisibly wa...
[15.10.2025 23:10] ********************************************************************************
[15.10.2025 23:10] Abstract 41. ReFIne, a new training framework, enhances the trustworthiness of reasoning models by improving interpretability, faithfulness, and reliability through structured traces, decisive information disclosure, and confidence estimates.  					AI-generated summary 				 Recent advances in long chain-of-thoug...
[15.10.2025 23:10] ********************************************************************************
[15.10.2025 23:10] Abstract 42. Kontinuous Kontext is an instruction-driven image editing model that introduces a scalar edit strength for fine-grained control over the extent of edits, using a lightweight projector network and a synthesized dataset of image-edit-instruction-strength quadruplets.  					AI-generated summary 				 In...
[15.10.2025 23:10] ********************************************************************************
[15.10.2025 23:10] Abstract 43. Tensor logic unifies neural and symbolic AI by using tensor equations, enabling scalable, learnable, and transparent AI systems.  					AI-generated summary 				 Progress in AI is hindered by the lack of a programming language with all the requisite features. Libraries like PyTorch and TensorFlow pro...
[15.10.2025 23:10] ********************************************************************************
[15.10.2025 23:10] Abstract 44. dInfer is an efficient and extensible framework for diffusion-based large language model inference, achieving significant speedups over existing systems without compromising output quality.  					AI-generated summary 				 Diffusion-based large language models (dLLMs) have emerged as a promising alte...
[15.10.2025 23:10] Read previous papers.
[15.10.2025 23:10] Generating reviews via LLM API.
[15.10.2025 23:10] Using data from previous issue: {"categories": ["#3d", "#optimization", "#training", "#agents", "#alignment"], "emoji": "ü§ñ", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ —Ä–æ–±–æ—Ç–æ–≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–º—É –º—ã—à–ª–µ–Ω–∏—é –±–µ–∑ 3D —Å–µ–Ω—Å–æ—Ä–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Spatial Forcing (SF) ‚Äî –º–µ—Ç–æ–¥ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è vision-language-action (VLA) –º–æ–¥–µ–ª–µ–π –≤ —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–µ. –ü—Ä
[15.10.2025 23:10] Using data from previous issue: {"categories": ["#machine_translation", "#open_source", "#reasoning", "#dataset", "#multilingual", "#benchmark"], "emoji": "üìö", "ru": {"title": "–ö–∏—Ç–∞–π—Å–∫–∏–µ LLM –ø–æ–±–µ–∂–¥–∞—é—Ç –≤ –ø–µ—Ä–µ–≤–æ–¥–µ –≤–µ–±-—Ä–æ–º–∞–Ω–æ–≤", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ DITING ‚Äî –ø–µ—Ä–≤—É—é –∫–æ–º–ø–ª–µ–∫—Å–Ω—É—é —Å–∏—Å—Ç–µ–º—É –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –ø–µ—Ä–µ–≤–æ–¥–∞ –≤–µ–±-—Ä–æ–º–∞–Ω–æ
[15.10.2025 23:10] Using data from previous issue: {"categories": ["#training", "#diffusion", "#cv", "#optimization"], "emoji": "üéØ", "ru": {"title": "–î–≤—É—Ö—ç—Ç–∞–ø–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∑–∞–∫—Ä—ã–≤–∞–µ—Ç —Ä–∞–∑—Ä—ã–≤ –º–µ–∂–¥—É –ø–∏–∫—Å–µ–ª—å–Ω—ã–º–∏ –∏ –ª–∞—Ç–µ–Ω—Ç–Ω—ã–º–∏ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –Ω–æ–≤—ã–π –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∏ consistency 
[15.10.2025 23:10] Using data from previous issue: {"categories": ["#low_resource", "#data", "#benchmark", "#multimodal", "#transfer_learning", "#training", "#alignment"], "emoji": "üîó", "ru": {"title": "–ì–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–µ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ –∫–∞–∫ –∫–ª—é—á –∫ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–º –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö
[15.10.2025 23:10] Using data from previous issue: {"categories": ["#rl", "#reasoning", "#optimization", "#games", "#agents", "#robotics"], "emoji": "ü§ñ", "ru": {"title": "–û—Ç –∫–ª–∞—Å—Å–∏–∫–∏ –∫ –¥–∞–Ω–Ω—ã–º: –Ω–æ–≤–∞—è —ç—Ä–∞ –æ–±—É—á–µ–Ω–∏—è —Ä–æ–±–æ—Ç–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π —É—á–µ–±–Ω–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–º—É –º–∞—à–∏–Ω–Ω–æ–º—É –æ–±—É—á–µ–Ω–∏—é –¥–ª—è —Ä–æ–±–æ—Ç–æ–≤. –ê–≤—Ç–æ—Ä—ã –æ–ø–∏—Å—ã–≤–∞—é—Ç –ø–µ—Ä–µ—Ö–æ–¥ –æ
[15.10.2025 23:10] Using data from previous issue: {"categories": ["#cv", "#multimodal", "#training", "#optimization", "#rl"], "emoji": "üîç", "ru": {"title": "Rex-Omni: –Ω–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å –≤ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–∏ –æ–±—ä–µ–∫—Ç–æ–≤", "desc": "–í —Å—Ç–∞—Ç—å–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å Rex-Omni, –∫–æ—Ç–æ—Ä–∞—è —É–ª—É—á—à–∞–µ—Ç –∑–∞–¥–∞—á–∏ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –æ–±—ä–µ–∫—Ç–æ–≤, –∏—Å–ø–æ–ª—å–∑—É—è –ø–æ–¥—Ö–æ–¥—ã LLM. Rex-Omni –¥–æ—Å—Ç–∏–≥–∞–µ—Ç
[15.10.2025 23:10] Using data from previous issue: {"categories": ["#multimodal", "#training", "#survey", "#agi", "#agents", "#rl"], "emoji": "üéµ", "ru": {"title": "Vibe Coding: –∫–æ–≥–¥–∞ —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫ –ø—Ä–æ–≤–µ—Ä—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç, –∞ –Ω–µ —á–∏—Ç–∞–µ—Ç –∫–æ–¥", "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –Ω–æ–≤—É—é –ø–∞—Ä–∞–¥–∏–≥–º—É —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º \"Vibe Coding\", –≥–¥–µ —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∏ –ø—Ä–æ–≤–µ—Ä—è—é—Ç —Ä–∞–±–æ—Ç–æ
[15.10.2025 23:10] Using data from previous issue: {"categories": ["#video", "#open_source", "#inference", "#training", "#dataset", "#diffusion"], "emoji": "‚ö°", "ru": {"title": "–î–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–æ–µ –≤–∏–¥–µ–æ super-resolution –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏", "desc": "FlashVSR ‚Äî –ø–µ—Ä–≤–∞—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –≤–∏–¥–µ–æ super-resolution –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏, —Ä–∞–±–æ—Ç–∞—é—â–∞—è —Å–æ —Å–∫–æ—Ä–æ—Å—Ç—å
[15.10.2025 23:10] Using data from previous issue: {"categories": ["#reasoning", "#architecture", "#optimization", "#training", "#inference"], "emoji": "üß≠", "ru": {"title": "–£–º–Ω–∞—è –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—è —Å–ª–æ—ë–≤: LLM —É—á–∞—Ç—Å—è –ø—Ä–æ–ø—É—Å–∫–∞—Ç—å –Ω–µ–Ω—É–∂–Ω—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω Dr.LLM ‚Äî –º–µ—Ç–æ–¥ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–∏ —Å–ª–æ—ë–≤ –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π
[15.10.2025 23:10] Using data from previous issue: {"categories": ["#training", "#optimization", "#diffusion", "#multimodal", "#cv"], "emoji": "üéØ", "ru": {"title": "–í—Ä–µ–º–µ–Ω–Ω–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –¥–ª—è —Ç–æ—á–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–î–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –æ—Ç–ª–∏—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, –Ω–æ –Ω–∞–∫–∞–ø–ª–∏–≤–∞—é—Ç –æ—à–∏–±–∫–∏ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ —Ä–∞–±–æ—Ç—ã, –æ—Å–æ
[15.10.2025 23:10] Using data from previous issue: {"categories": ["#reasoning", "#training", "#optimization", "#transfer_learning", "#agents", "#rl"], "emoji": "ü§ñ", "ru": {"title": "–ú–∞–ª–µ–Ω—å–∫–∏–µ –º–æ–¥–µ–ª–∏ —É—á–∞—Ç—Å—è –¥–µ–π—Å—Ç–≤–æ–≤–∞—Ç—å –∫–∞–∫ –±–æ–ª—å—à–∏–µ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç ERA ‚Äî –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∫–æ–º–ø–∞–∫—Ç–Ω—ã—Ö vision language models –¥–µ–π—Å—Ç–≤–æ–≤–∞—Ç—å –≤ 
[15.10.2025 23:10] Using data from previous issue: {"categories": ["#training", "#optimization", "#multimodal", "#transfer_learning"], "emoji": "üîÑ", "ru": {"title": "–°–∞–º–æ–æ–±—É—á–µ–Ω–∏–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –≤–Ω—É—Ç—Ä–µ–Ω–Ω—é—é —Å–∞–º–æ–æ—Ü–µ–Ω–∫—É", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏ –ø–∞—Ä–∞–¥–æ–∫—Å: –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –º–æ–≥—É—Ç —Ö–æ—Ä–æ—à–æ –ø–æ–Ω–∏–º–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, –Ω–æ –ø–ª–æ—Ö–æ –∏—Ö –≥–µ–Ω–µ
[15.10.2025 23:10] Using data from previous issue: {"categories": ["#cv", "#multimodal", "#reasoning", "#training", "#transfer_learning", "#diffusion"], "emoji": "üîó", "ru": {"title": "–ï–¥–∏–Ω—ã–π —ç–Ω–∫–æ–¥–µ—Ä –¥–ª—è —Ç–µ–∫—Å—Ç–∞ –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "UniFusion ‚Äî —ç—Ç–æ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä–∞—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∑–∞–º–æ—Ä–æ–∂–µ–Ω–Ω—É
[15.10.2025 23:10] Using data from previous issue: {"categories": ["#long_context", "#reasoning", "#training", "#optimization", "#agents", "#rl"], "emoji": "üß†", "ru": {"title": "–ü–∞–º—è—Ç—å –∫–∞–∫ –¥–µ–π—Å—Ç–≤–∏–µ: LLM —É—á–∞—Ç—Å—è —É–ø—Ä–∞–≤–ª—è—Ç—å —Å–≤–æ–∏–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º", "desc": "–ë–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ —Å—Ç–∞–ª–∫–∏–≤–∞—é—Ç—Å—è —Å –ø—Ä–æ–±–ª–µ–º–æ–π –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–π –ø–∞–º—è—Ç–∏ –ø—Ä–∏ —Ä–µ—à–µ–Ω–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –∑–∞–¥–∞—á, –∫–æ–≥–¥–∞ –∫
[15.10.2025 23:10] Using data from previous issue: {"categories": ["#games", "#rlhf", "#training", "#optimization", "#rl"], "emoji": "üéØ", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –ø–æ–º–æ—â—å—é –ª–∏–Ω–µ–π–Ω–æ–π –∞–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ BGPO –¥–ª—è reinforcement learning –≤ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö LLM, –∫–æ—Ç–æ—Ä—ã–µ —Å—Ç–∞–ª–∫–∏–≤–∞—é—Ç—Å—è —Å 
[15.10.2025 23:10] Using data from previous issue: {"categories": ["#math", "#architecture", "#optimization", "#interpretability", "#training"], "emoji": "üîç", "ru": {"title": "–°–º–µ—à–∏–≤–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤ –≤–∞–∂–Ω–µ–µ, —á–µ–º —Ç–æ—á–Ω–∞—è —Ñ–æ—Ä–º—É–ª–∞ attention", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏–∑—É—á–∏–ª–∏ –º–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è –≤ Transformer –º–æ–¥–µ–ª—è—Ö, —Ä–∞–∑–±–∏—Ä–∞—è –µ–≥–æ –Ω–∞ —Å–æ—Å—Ç–∞–≤–Ω—ã–µ —á–∞—Å—Ç
[15.10.2025 23:10] Using data from previous issue: {"categories": ["#hallucinations", "#data", "#inference", "#training", "#alignment", "#story_generation", "#synthetic"], "emoji": "üé≤", "ru": {"title": "–†–∞–∑–±–ª–æ–∫–∏—Ä–æ–≤–∫–∞ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è LLM —á–µ—Ä–µ–∑ –≤–µ—Ä–±–∞–ª–∏–∑–∞—Ü–∏—é –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ —Ä–µ–∂–∏–º–Ω—ã–π –∫–æ–ª–ª–∞–ø—Å –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö –≤—ã–∑–≤–∞–Ω —Å
[15.10.2025 23:10] Using data from previous issue: {"categories": ["#cv", "#multimodal", "#reasoning", "#dataset", "#benchmark", "#data"], "emoji": "üêù", "ru": {"title": "HoneyBee: –ö–∞–∫ –ø—Ä–∞–≤–∏–ª—å–Ω–æ –≥–æ—Ç–æ–≤–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –∏–∑—É—á–∏–ª–∏ –ø—Ä–∏–Ω—Ü–∏–ø—ã —Å–æ–∑–¥–∞–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è vision-language –º–æ–¥–µ–ª
[15.10.2025 23:10] Using data from previous issue: {"categories": ["#training", "#optimization", "#multimodal"], "emoji": "‚õµ", "ru": {"title": "SAIL-Embedding: –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –ø–æ–∏—Å–∫–∞ –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç SAIL-Embedding ‚Äî –æ–º–Ω–∏–º–æ–¥–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤, –∫–æ—Ç–æ—Ä–∞—è —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–π –ø–æ–¥–¥–µ—Ä–∂–∫–∏
[15.10.2025 23:10] Using data from previous issue: {"categories": ["#multimodal", "#reasoning", "#training", "#dataset", "#agi", "#optimization", "#rag", "#benchmark"], "emoji": "üîç", "ru": {"title": "–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ —Å –æ–±—É—á–µ–Ω–∏–µ–º —á–µ—Ä–µ–∑ –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç DeepMMSearch-R1 ‚Äî –ø–µ—Ä–≤—É—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—É—é LLM, —Å–ø–æ—Å–æ–±–Ω—É—é
[15.10.2025 23:10] Using data from previous issue: {"categories": ["#hallucinations", "#agents", "#long_context", "#rag"], "emoji": "üîÆ", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ AI –∞–≥–µ–Ω—Ç–æ–≤ —Å –≤–Ω–µ—à–Ω–∏–º–∏ –∑–Ω–∞–Ω–∏—è–º–∏ –≤–º–µ—Å—Ç–æ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π", "desc": "LLM –º–æ–≥—É—Ç —Å–ª—É–∂–∏—Ç—å –º–æ–¥–µ–ª—è–º–∏ –º–∏—Ä–∞ –¥–ª—è –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –¥–µ–π—Å—Ç–≤–∏–π AI-–∞–≥–µ–Ω—Ç–æ–≤ –≤ —Ü–∏—Ñ—Ä–æ–≤—ã—Ö —Å—Ä–µ–¥–∞—Ö, –Ω–æ —Å—Ç—Ä–∞–¥–∞—é—Ç –æ—Ç –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π –∏ —É—Å—Ç–∞—Ä–µ–≤—à
[15.10.2025 23:10] Using data from previous issue: {"categories": ["#low_resource", "#reasoning", "#training", "#multilingual", "#machine_translation", "#synthetic"], "emoji": "ü§î", "ru": {"title": "–†–∞–∑–º—ã—à–ª–µ–Ω–∏—è –Ω–µ –ø–æ–º–æ–≥–∞—é—Ç LLM –ª—É—á—à–µ –ø–µ—Ä–µ–≤–æ–¥–∏—Ç—å", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –∏–∑—É—á–∏–ª–∏, –ø–æ–º–æ–≥–∞—é—Ç –ª–∏ \"—Ç–æ–∫–µ–Ω—ã —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏–π\" (thinking tokens) –±–æ–ª—å—à–∏–º reasoning –º–æ–¥
[15.10.2025 23:10] Using data from previous issue: {"categories": ["#cv", "#dataset", "#diffusion"], "emoji": "üéØ", "ru": {"title": "–ö–æ–Ω—Ç—Ä–æ–ª—å —Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω–∏—è –∏ –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç–∏ –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç ContextGen ‚Äî –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ Diffusion Transformer –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –æ–±—ä–µ–∫—Ç–∞–º–∏. –°–∏—Å—Ç–µ–º
[15.10.2025 23:10] Using data from previous issue: {"categories": ["#cv", "#interpretability", "#open_source", "#reasoning", "#training", "#optimization", "#synthetic"], "emoji": "üîÑ", "ru": {"title": "Flow Poke Transformer: –Ω–æ–≤–∞—è —ç—Ä–∞ –≤ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–∏ –¥–≤–∏–∂–µ–Ω–∏—è", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å Flow Poke Transformer (FPT), –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤
[15.10.2025 23:10] Using data from previous issue: {"categories": ["#interpretability", "#architecture", "#math", "#reasoning"], "emoji": "üåä", "ru": {"title": "–õ–æ–≥–∏–∫–∞ –∫–∞–∫ –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–π –ø–æ—Ç–æ–∫ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –Ω–æ–≤—ã–π –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–π –ø–æ–¥—Ö–æ–¥ –∫ –ø–æ–Ω–∏–º–∞–Ω–∏—é —Ç–æ–≥–æ, –∫–∞–∫ LLM ¬´—Ä–∞–∑–º—ã—à–ª—è—é—Ç¬ª –≤ —Å–≤–æ—ë–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –ø—Ä–µ–¥—Å—Ç
[15.10.2025 23:10] Using data from previous issue: {"categories": ["#alignment", "#interpretability", "#multimodal", "#benchmark"], "emoji": "üé®", "ru": {"title": "LLM –∫–∞–∫ —Ä–∞–Ω–Ω–∏–µ –æ—Ü–µ–Ω—â–∏–∫–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–≤", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–≤–µ—Ä—è–µ—Ç, –º–æ–≥—É—Ç –ª–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ LLM (–±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏) –æ—Ü–µ–Ω–∏–≤–∞—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å—ã —Ç–∞–∫ –∂–µ,
[15.10.2025 23:10] Using data from previous issue: {"categories": ["#training", "#architecture", "#optimization"], "emoji": "üéØ", "ru": {"title": "–û—Å—Ç–æ—Ä–æ–∂–Ω–æ–µ –∑–∞—Ç—É—Ö–∞–Ω–∏–µ –≤–µ—Å–æ–≤: —É–º–Ω—ã–π —Å–ø–æ—Å–æ–± —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –º–µ—Ç–æ–¥ Cautious Weight Decay (CWD) ‚Äî –ø—Ä–æ—Å—Ç–∞—è –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏—è –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–æ–≤, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–∏–º–µ–Ω—è–µ—Ç weight decay —Ç–æ–ª—å–∫
[15.10.2025 23:10] Using data from previous issue: {"categories": ["#rl", "#benchmark", "#reasoning", "#optimization", "#games", "#agents"], "emoji": "üéÆ", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –º–∏—Ä–∞ –∑–∞ –æ–¥–Ω—É –∂–∏–∑–Ω—å –±–µ–∑ –ø–æ–¥—Å–∫–∞–∑–æ–∫", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç OneLife ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Å–∏–º–≤–æ–ª—å–Ω–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∏–Ω–∞–º–∏–∫–∏ —Å–ª–æ–∂–Ω—ã—Ö —Å—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–∏—Ö —Å—Ä–µ–¥ —á–µ—Ä–µ–∑ –ø—Ä–æ–≥—Ä–∞
[15.10.2025 23:10] Using data from previous issue: {"categories": ["#agents", "#rl", "#science", "#dataset", "#optimization"], "emoji": "üî¨", "ru": {"title": "AI-—É—á—ë–Ω—ã–π, –∫–æ—Ç–æ—Ä—ã–π —Å–∞–º –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç –Ω–∞—É—á–Ω—ã–µ —É—Ä–∞–≤–Ω–µ–Ω–∏—è", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω SR-Scientist ‚Äî –∞–≤—Ç–æ–Ω–æ–º–Ω—ã–π AI-—Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç LLM –¥–ª—è –æ—Ç–∫—Ä—ã—Ç–∏—è –Ω–∞—É—á–Ω—ã—Ö —É—Ä–∞–≤–Ω–µ–Ω–∏–π. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Å—É
[15.10.2025 23:10] Using data from previous issue: {"categories": ["#cv", "#science", "#benchmark", "#open_source", "#reasoning", "#multimodal"], "emoji": "üî¨", "ru": {"title": "–ù–∞—É—á–Ω—ã–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã ‚Äî —Å–ª–∞–±–æ–µ –º–µ—Å—Ç–æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö AI", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ ExpVid ‚Äî –ø–µ—Ä–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö LLM –Ω–∞ –≤–∏–¥–µ–æ –Ω–∞—É—á–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤
[15.10.2025 23:10] Using data from previous issue: {"categories": ["#reasoning", "#rl", "#security", "#hallucinations", "#training", "#benchmark"], "emoji": "üîç", "ru": {"title": "–î–µ—Ç–µ–∫—Ç–æ—Ä –∑–∞–≥—Ä—è–∑–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ –∫–æ–ª–ª–∞–ø—Å –ø–æ–ª–∏—Ç–∏–∫–∏ –≤ RL", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ –º–µ—Ç–æ–¥ Self-Critique –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∑–∞–≥—Ä—è–∑–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –Ω–∞ —ç—Ç–∞–ø–µ RL –ø–æ—Å—Ç-—Ç—Ä–µ–Ω–∏–Ω–≥
[15.10.2025 23:10] Using data from previous issue: {"categories": ["#cv", "#training", "#inference", "#interpretability", "#agi", "#optimization", "#multimodal"], "emoji": "üéØ", "ru": {"title": "–£–º–Ω–æ–µ —Å–∂–∞—Ç–∏–µ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –ø–æ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –º–µ—Ç–æ–¥ Visual Consistency Learning (ViCO), –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç mult
[15.10.2025 23:10] Using data from previous issue: {"categories": ["#cv", "#diffusion", "#training", "#optimization", "#data"], "emoji": "üéØ", "ru": {"title": "–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —à—É–º–∞ –≤ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏ –ø—Ä–æ–±–ª–µ–º—É –≤ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö: –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –º–µ–∂–¥—É –∑–∞—Ä–∞–Ω–µ–µ –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã–º —É—Ä–æ–≤–Ω–µ–º —à—É–º–∞ –∏ —Ñ–∞
[15.10.2025 23:10] Using data from previous issue: {"categories": ["#long_context", "#multimodal", "#open_source", "#reasoning", "#agi", "#benchmark", "#rag"], "emoji": "üéØ", "ru": {"title": "–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π RAG –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –ª—é–±—ã–º–∏ —Ç–∏–ø–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö", "desc": "RAG-Anything ‚Äî —ç—Ç–æ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è Retrieval-Augmented Generation (RAG), –∫–æ
[15.10.2025 23:10] Using data from previous issue: {"categories": ["#security", "#architecture", "#training", "#optimization"], "emoji": "üîê", "ru": {"title": "Locket: –ú–æ–Ω–µ—Ç–∏–∑–∞—Ü–∏—è AI —á–µ—Ä–µ–∑ –±–ª–æ–∫–∏—Ä–æ–≤–∫—É —Ñ—É–Ω–∫—Ü–∏–π –ø–æ –ø–æ–¥–ø–∏—Å–∫–µ", "desc": "–ü—Ä–æ–≤–∞–π–¥–µ—Ä—ã —á–∞—Ç-–±–æ—Ç–æ–≤ –∏—Å–ø–æ–ª—å–∑—É—é—Ç –º–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤—ã–µ –ø–æ–¥–ø–∏—Å–∫–∏, –Ω–æ –±–æ–ª–µ–µ –≤—ã–≥–æ–¥–Ω–æ–π –º–æ–∂–µ—Ç –±—ã—Ç—å —Å—Ö–µ–º–∞ –æ–ø–ª–∞—Ç—ã –∑–∞ –æ—Ç–¥–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ (
[15.10.2025 23:10] Using data from previous issue: {"categories": ["#dataset", "#agents", "#ethics", "#rlhf", "#security", "#alignment", "#benchmark"], "emoji": "üî¨", "ru": {"title": "–û–ø–∞—Å–Ω–æ—Å—Ç—å –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö AI-–∞–≥–µ–Ω—Ç–æ–≤: –∫–æ–≥–¥–∞ —É–º–Ω—ã–µ –ø–æ–º–æ—â–Ω–∏–∫–∏ –æ–±—Ö–æ–¥—è—Ç –∑–∞—â–∏—Ç—É", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏ –∫—Ä–∏—Ç–∏—á–µ—Å–∫—É—é —É—è–∑–≤–∏–º–æ—Å—Ç—å –≤ Deep Research –∞–≥–µ–Ω—Ç–∞—Ö –Ω–∞ –æ—Å–Ω–æ–≤
[15.10.2025 23:10] Using data from previous issue: {"categories": ["#open_source", "#reasoning", "#rlhf", "#security", "#alignment", "#benchmark", "#architecture"], "emoji": "üîì", "ru": {"title": "–•—Ä—É–ø–∫–∞—è –±—Ä–æ–Ω—è: –∫–∞–∫ –ø—Ä–æ—Å—Ç—ã–µ —Ç–æ–∫–µ–Ω—ã –ª–æ–º–∞—é—Ç –∑–∞—â–∏—Ç—É reasoning-–º–æ–¥–µ–ª–µ–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏ –∫—Ä–∏—Ç–∏—á–µ—Å–∫—É—é —É—è–∑–≤–∏–º–æ—Å—Ç—å –≤ —Å–∏—Å—Ç–µ–º–∞—Ö –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏
[15.10.2025 23:10] Using data from previous issue: {"categories": ["#hallucinations", "#reasoning", "#benchmark", "#data", "#training"], "emoji": "üõ°Ô∏è", "ru": {"title": "–ó–∞—â–∏—Ç–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π LLM –æ—Ç –∫—Ä–∞–∂–∏ —á–µ—Ä–µ–∑ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞–Ω–∏–µ", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –º–µ—Ç–æ–¥ PART –¥–ª—è –∑–∞—â–∏—Ç—ã –¥–µ—Ç–∞–ª—å–Ω—ã—Ö reasoning-—Ü–µ–ø–æ—á–µ–∫ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –æ—Ç –Ω–µ—Å–∞–Ω–∫—Ü–∏–æ–Ω–∏
[15.10.2025 23:10] Using data from previous issue: {"categories": ["#diffusion", "#audio", "#games", "#multimodal"], "emoji": "üéµ", "ru": {"title": "–î–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–π –º–æ—Å—Ç –º–µ–∂–¥—É –∑–≤—É–∫–æ–º –∏ —Ç–µ–∫—Å—Ç–æ–º –¥–ª—è LLM", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Diffusion-Link ‚Äî –º–æ–¥—É–ª—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è –∞—É–¥–∏–æ –∏ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π. –ú–æ–¥—É–ª—å –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ –ø—Ä
[15.10.2025 23:10] Using data from previous issue: {"categories": ["#optimization", "#architecture", "#math", "#reasoning", "#training"], "emoji": "üìâ", "ru": {"title": "–ü–æ—á–µ–º—É –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã –ø—Ä–æ–∏–≥—Ä—ã–≤–∞—é—Ç –ª–∏–Ω–µ–π–Ω—ã–º –º–æ–¥–µ–ª—è–º –≤ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏ –¥–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ Transformer-–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã, –æ—Å–æ–±–µ–Ω–Ω–æ –º–æ–¥–µ–ª–∏ —Å Lin
[15.10.2025 23:10] Using data from previous issue: {"categories": ["#optimization", "#security", "#multimodal", "#benchmark", "#cv"], "emoji": "üîè", "ru": {"title": "SynthID-Image: –Ω–µ–≤–∏–¥–∏–º—ã–µ –≤–æ–¥—è–Ω—ã–µ –∑–Ω–∞–∫–∏ –¥–ª—è AI-–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ –º–∞—Å—à—Ç–∞–±–µ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ SynthID-Image ‚Äî —Å–∏—Å—Ç–µ–º—É –Ω–∞ –æ—Å–Ω–æ–≤–µ –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –Ω–µ–≤–∏–¥–∏–º–æ–π –≤–æ–¥
[15.10.2025 23:10] Using data from previous issue: {"categories": ["#optimization", "#reasoning", "#math", "#training", "#interpretability"], "emoji": "üîç", "ru": {"title": "–ü—Ä–æ–∑—Ä–∞—á–Ω—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è: –∫–∞–∫ –Ω–∞—É—á–∏—Ç—å AI –æ–±—ä—è—Å–Ω—è—Ç—å —Å–≤–æ–∏ —Ä–µ—à–µ–Ω–∏—è", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω ReFIne ‚Äî –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π –¥–µ–ª–∞–µ—Ç reasoning –º–æ–¥–µ–ª–∏ –±–æ–ª–µ–µ –Ω–∞–¥—ë–∂–Ω—ã–º–∏ –∏ –ø–æ–Ω
[15.10.2025 23:10] Using data from previous issue: {"categories": ["#data", "#cv", "#optimization", "#dataset", "#synthetic", "#training"], "emoji": "üéöÔ∏è", "ru": {"title": "–ü–ª–∞–≤–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Å–∏–ª–æ–π —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —á–µ—Ä–µ–∑ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Kontinuous Kontext ‚Äî –º–æ–¥–µ–ª—å –¥–ª—è —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ 
[15.10.2025 23:10] Using data from previous issue: {"categories": ["#architecture", "#reasoning", "#interpretability", "#plp", "#agi"], "emoji": "üîó", "ru": {"title": "–¢–µ–Ω–∑–æ—Ä–Ω–∞—è –ª–æ–≥–∏–∫–∞: –º–æ—Å—Ç –º–µ–∂–¥—É –Ω–µ–π—Ä–æ–Ω–Ω—ã–º –∏ —Å–∏–º–≤–æ–ª—å–Ω—ã–º AI", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç tensor logic ‚Äî –Ω–æ–≤—ã–π —è–∑—ã–∫ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ –∏ —Å–∏–º–≤–æ–ª—å–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã –≤ AI —á
[15.10.2025 23:10] Using data from previous issue: {"categories": ["#open_source", "#inference", "#diffusion", "#architecture"], "emoji": "‚ö°", "ru": {"title": "–î–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ LLM –≤ 10 —Ä–∞–∑ –±—ã—Å—Ç—Ä–µ–µ —Å dInfer", "desc": "dInfer - —ç—Ç–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (dLLM), –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—É —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–º –∞–≤—Ç
[15.10.2025 23:10] Renaming data file.
[15.10.2025 23:10] Renaming previous data. hf_papers.json to ./d/2025-10-15.json
[15.10.2025 23:10] Saving new data file.
[15.10.2025 23:10] Generating page.
[15.10.2025 23:10] Renaming previous page.
[15.10.2025 23:10] Renaming previous data. index.html to ./d/2025-10-15.html
[15.10.2025 23:10] Writing result.
[15.10.2025 23:10] Renaming log file.
[15.10.2025 23:10] Renaming previous data. log.txt to ./logs/2025-10-15_last_log.txt
