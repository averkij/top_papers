[15.10.2025 03:34] Read previous papers.
[15.10.2025 03:34] Generating top page (month).
[15.10.2025 03:34] Writing top page (month).
[15.10.2025 04:14] Read previous papers.
[15.10.2025 04:14] Get feed.
[15.10.2025 04:14] Get page data from previous paper. URL: https://huggingface.co/papers/2510.12586
[15.10.2025 04:14] Get page data from previous paper. URL: https://huggingface.co/papers/2510.09116
[15.10.2025 04:14] Get page data from previous paper. URL: https://huggingface.co/papers/2510.11693
[15.10.2025 04:14] Get page data from previous paper. URL: https://huggingface.co/papers/2510.12747
[15.10.2025 04:14] Get page data from previous paper. URL: https://huggingface.co/papers/2510.12798
[15.10.2025 04:14] Get page data from previous paper. URL: https://huggingface.co/papers/2510.12399
[15.10.2025 04:14] Get page data from previous paper. URL: https://huggingface.co/papers/2510.12693
[15.10.2025 04:14] Get page data from previous paper. URL: https://huggingface.co/papers/2510.12276
[15.10.2025 04:14] Get page data from previous paper. URL: https://huggingface.co/papers/2510.12773
[15.10.2025 04:14] Get page data from previous paper. URL: https://huggingface.co/papers/2510.11683
[15.10.2025 04:14] Get page data from previous paper. URL: https://huggingface.co/papers/2510.12784
[15.10.2025 04:14] Get page data from previous paper. URL: https://huggingface.co/papers/2510.12635
[15.10.2025 04:14] Get page data from previous paper. URL: https://huggingface.co/papers/2510.12709
[15.10.2025 04:14] Get page data from previous paper. URL: https://huggingface.co/papers/2510.12801
[15.10.2025 04:14] Get page data from previous paper. URL: https://huggingface.co/papers/2510.12789
[15.10.2025 04:14] Get page data from previous paper. URL: https://huggingface.co/papers/2510.12777
[15.10.2025 04:14] Get page data from previous paper. URL: https://huggingface.co/papers/2510.12225
[15.10.2025 04:14] Get page data from previous paper. URL: https://huggingface.co/papers/2510.11919
[15.10.2025 04:14] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[15.10.2025 04:14] No deleted papers detected.
[15.10.2025 04:14] Downloading and parsing papers (pdf, html). Total: 18.
[15.10.2025 04:14] Downloading and parsing paper https://huggingface.co/papers/2510.12586.
[15.10.2025 04:14] Extra JSON file exists (./assets/json/2510.12586.json), skip PDF parsing.
[15.10.2025 04:14] Paper image links file exists (./assets/img_data/2510.12586.json), skip HTML parsing.
[15.10.2025 04:14] Success.
[15.10.2025 04:14] Downloading and parsing paper https://huggingface.co/papers/2510.09116.
[15.10.2025 04:14] Extra JSON file exists (./assets/json/2510.09116.json), skip PDF parsing.
[15.10.2025 04:14] Paper image links file exists (./assets/img_data/2510.09116.json), skip HTML parsing.
[15.10.2025 04:14] Success.
[15.10.2025 04:14] Downloading and parsing paper https://huggingface.co/papers/2510.11693.
[15.10.2025 04:14] Extra JSON file exists (./assets/json/2510.11693.json), skip PDF parsing.
[15.10.2025 04:14] Paper image links file exists (./assets/img_data/2510.11693.json), skip HTML parsing.
[15.10.2025 04:14] Success.
[15.10.2025 04:14] Downloading and parsing paper https://huggingface.co/papers/2510.12747.
[15.10.2025 04:14] Extra JSON file exists (./assets/json/2510.12747.json), skip PDF parsing.
[15.10.2025 04:14] Paper image links file exists (./assets/img_data/2510.12747.json), skip HTML parsing.
[15.10.2025 04:14] Success.
[15.10.2025 04:14] Downloading and parsing paper https://huggingface.co/papers/2510.12798.
[15.10.2025 04:14] Extra JSON file exists (./assets/json/2510.12798.json), skip PDF parsing.
[15.10.2025 04:14] Paper image links file exists (./assets/img_data/2510.12798.json), skip HTML parsing.
[15.10.2025 04:14] Success.
[15.10.2025 04:14] Downloading and parsing paper https://huggingface.co/papers/2510.12399.
[15.10.2025 04:14] Extra JSON file exists (./assets/json/2510.12399.json), skip PDF parsing.
[15.10.2025 04:14] Paper image links file exists (./assets/img_data/2510.12399.json), skip HTML parsing.
[15.10.2025 04:14] Success.
[15.10.2025 04:14] Downloading and parsing paper https://huggingface.co/papers/2510.12693.
[15.10.2025 04:14] Extra JSON file exists (./assets/json/2510.12693.json), skip PDF parsing.
[15.10.2025 04:14] Paper image links file exists (./assets/img_data/2510.12693.json), skip HTML parsing.
[15.10.2025 04:14] Success.
[15.10.2025 04:14] Downloading and parsing paper https://huggingface.co/papers/2510.12276.
[15.10.2025 04:14] Extra JSON file exists (./assets/json/2510.12276.json), skip PDF parsing.
[15.10.2025 04:14] Paper image links file exists (./assets/img_data/2510.12276.json), skip HTML parsing.
[15.10.2025 04:14] Success.
[15.10.2025 04:14] Downloading and parsing paper https://huggingface.co/papers/2510.12773.
[15.10.2025 04:14] Extra JSON file exists (./assets/json/2510.12773.json), skip PDF parsing.
[15.10.2025 04:14] Paper image links file exists (./assets/img_data/2510.12773.json), skip HTML parsing.
[15.10.2025 04:14] Success.
[15.10.2025 04:14] Downloading and parsing paper https://huggingface.co/papers/2510.11683.
[15.10.2025 04:14] Extra JSON file exists (./assets/json/2510.11683.json), skip PDF parsing.
[15.10.2025 04:14] Paper image links file exists (./assets/img_data/2510.11683.json), skip HTML parsing.
[15.10.2025 04:14] Success.
[15.10.2025 04:14] Downloading and parsing paper https://huggingface.co/papers/2510.12784.
[15.10.2025 04:14] Extra JSON file exists (./assets/json/2510.12784.json), skip PDF parsing.
[15.10.2025 04:14] Paper image links file exists (./assets/img_data/2510.12784.json), skip HTML parsing.
[15.10.2025 04:14] Success.
[15.10.2025 04:14] Downloading and parsing paper https://huggingface.co/papers/2510.12635.
[15.10.2025 04:14] Extra JSON file exists (./assets/json/2510.12635.json), skip PDF parsing.
[15.10.2025 04:14] Paper image links file exists (./assets/img_data/2510.12635.json), skip HTML parsing.
[15.10.2025 04:14] Success.
[15.10.2025 04:14] Downloading and parsing paper https://huggingface.co/papers/2510.12709.
[15.10.2025 04:14] Extra JSON file exists (./assets/json/2510.12709.json), skip PDF parsing.
[15.10.2025 04:14] Paper image links file exists (./assets/img_data/2510.12709.json), skip HTML parsing.
[15.10.2025 04:14] Success.
[15.10.2025 04:14] Downloading and parsing paper https://huggingface.co/papers/2510.12801.
[15.10.2025 04:14] Extra JSON file exists (./assets/json/2510.12801.json), skip PDF parsing.
[15.10.2025 04:14] Paper image links file exists (./assets/img_data/2510.12801.json), skip HTML parsing.
[15.10.2025 04:14] Success.
[15.10.2025 04:14] Downloading and parsing paper https://huggingface.co/papers/2510.12789.
[15.10.2025 04:14] Extra JSON file exists (./assets/json/2510.12789.json), skip PDF parsing.
[15.10.2025 04:14] Paper image links file exists (./assets/img_data/2510.12789.json), skip HTML parsing.
[15.10.2025 04:14] Success.
[15.10.2025 04:14] Downloading and parsing paper https://huggingface.co/papers/2510.12777.
[15.10.2025 04:14] Extra JSON file exists (./assets/json/2510.12777.json), skip PDF parsing.
[15.10.2025 04:14] Paper image links file exists (./assets/img_data/2510.12777.json), skip HTML parsing.
[15.10.2025 04:14] Success.
[15.10.2025 04:14] Downloading and parsing paper https://huggingface.co/papers/2510.12225.
[15.10.2025 04:14] Extra JSON file exists (./assets/json/2510.12225.json), skip PDF parsing.
[15.10.2025 04:14] Paper image links file exists (./assets/img_data/2510.12225.json), skip HTML parsing.
[15.10.2025 04:14] Success.
[15.10.2025 04:14] Downloading and parsing paper https://huggingface.co/papers/2510.11919.
[15.10.2025 04:14] Extra JSON file exists (./assets/json/2510.11919.json), skip PDF parsing.
[15.10.2025 04:14] Paper image links file exists (./assets/img_data/2510.11919.json), skip HTML parsing.
[15.10.2025 04:14] Success.
[15.10.2025 04:14] Enriching papers with extra data.
[15.10.2025 04:14] ********************************************************************************
[15.10.2025 04:14] Abstract 0. Pixel-space generative models are often more difficult to train and generally underperform compared to their latent-space counterparts, leaving a persistent performance and efficiency gap. In this paper, we introduce a novel two-stage training framework that closes this gap for pixel-space diffusion...
[15.10.2025 04:14] ********************************************************************************
[15.10.2025 04:14] Abstract 1. A new evaluation framework, DITING, and a reasoning-driven multi-agent evaluation framework, AgentEval, are introduced to assess the quality of web novel translations, revealing that Chinese-trained LLMs outperform larger foreign models.  					AI-generated summary 				 Large language models (LLMs) h...
[15.10.2025 04:14] ********************************************************************************
[15.10.2025 04:14] Abstract 2. Recent multimodal embedding approaches leveraging multimodal large language models (MLLMs) fine-tuned with contrastive learning (CL) have shown promising results, yet the underlying reasons behind their superiority remain underexplored. This work argues that a crucial advantage of MLLM-based approac...
[15.10.2025 04:14] ********************************************************************************
[15.10.2025 04:14] Abstract 3. Diffusion models have recently advanced video restoration, but applying them to real-world video super-resolution (VSR) remains challenging due to high latency, prohibitive computation, and poor generalization to ultra-high resolutions. Our goal in this work is to make diffusion-based VSR practical ...
[15.10.2025 04:14] ********************************************************************************
[15.10.2025 04:14] Abstract 4. Object detection has long been dominated by traditional coordinate regression-based models, such as YOLO, DETR, and Grounding DINO. Although recent efforts have attempted to leverage MLLMs to tackle this task, they face challenges like low recall rate, duplicate predictions, coordinate misalignment,...
[15.10.2025 04:14] ********************************************************************************
[15.10.2025 04:14] Abstract 5. The advancement of large language models (LLMs) has catalyzed a paradigm shift from code generation assistance to autonomous coding agents, enabling a novel development methodology termed "Vibe Coding" where developers validate AI-generated implementations through outcome observation rather than lin...
[15.10.2025 04:14] ********************************************************************************
[15.10.2025 04:14] Abstract 6. Recent advances in embodied AI highlight the potential of vision language models (VLMs) as agents capable of perception, reasoning, and interaction in complex environments. However, top-performing systems rely on large-scale models that are costly to deploy, while smaller VLMs lack the necessary kno...
[15.10.2025 04:14] ********************************************************************************
[15.10.2025 04:14] Abstract 7. Vision-language-action (VLA) models have recently shown strong potential in enabling robots to follow language instructions and execute precise actions. However, most VLAs are built upon vision-language models pretrained solely on 2D data, which lack accurate spatial awareness and hinder their abili...
[15.10.2025 04:14] ********************************************************************************
[15.10.2025 04:14] Abstract 8. Large Language Models (LLMs) process every token through all layers of a transformer stack, causing wasted computation on simple queries and insufficient flexibility for harder ones that need deeper reasoning. Adaptive-depth methods can improve efficiency, but prior approaches rely on costly inferen...
[15.10.2025 04:14] ********************************************************************************
[15.10.2025 04:14] Abstract 9. Boundary-Guided Policy Optimization (BGPO) improves reinforcement learning for diffusion large language models by efficiently approximating likelihoods with a memory-efficient lower bound, enhancing performance in tasks like math problem solving, code generation, and planning.  					AI-generated sum...
[15.10.2025 04:14] ********************************************************************************
[15.10.2025 04:14] Abstract 10. Recently, remarkable progress has been made in Unified Multimodal Models (UMMs), which integrate vision-language generation and understanding capabilities within a single framework. However, a significant gap exists where a model's strong visual understanding often fails to transfer to its visual ge...
[15.10.2025 04:14] ********************************************************************************
[15.10.2025 04:14] Abstract 11. Large Language Models face challenges in long-horizon agentic tasks as their constrained memory is easily overwhelmed by distracting or irrelevant context. Existing working memory methods typically rely on external, heuristic mechanisms that are decoupled from the agent's core policy. In this work, ...
[15.10.2025 04:14] ********************************************************************************
[15.10.2025 04:14] Abstract 12. Multimodal embedding models aim to yield informative unified representations that empower diverse cross-modal tasks. Despite promising developments in the evolution from CLIP-based dual-tower architectures to large vision-language models, prior works still face unavoidable challenges in real-world a...
[15.10.2025 04:14] ********************************************************************************
[15.10.2025 04:14] Abstract 13. Multimodal Large Language Models (MLLMs) in real-world applications require access to external knowledge sources and must remain responsive to the dynamic and ever-changing real-world information in order to address information-seeking and knowledge-intensive user queries. Existing approaches, such ...
[15.10.2025 04:14] ********************************************************************************
[15.10.2025 04:14] Abstract 14. Although recent advances in visual generation have been remarkable, most existing architectures still depend on distinct encoders for images and text. This separation constrains diffusion models' ability to perform cross-modal reasoning and knowledge transfer. Prior attempts to bridge this gap often...
[15.10.2025 04:14] ********************************************************************************
[15.10.2025 04:14] Abstract 15. Understanding the dynamics of a physical scene involves reasoning about the diverse ways it can potentially change, especially as a result of local interactions. We present the Flow Poke Transformer (FPT), a novel framework for directly predicting the distribution of local motion, conditioned on spa...
[15.10.2025 04:14] ********************************************************************************
[15.10.2025 04:14] Abstract 16. Recent advances in vision-language models (VLMs) have made them highly effective at reasoning tasks. However, the principles underlying the construction of performant VL reasoning training datasets remain poorly understood. In this work, we introduce several data curation approaches and study their ...
[15.10.2025 04:14] ********************************************************************************
[15.10.2025 04:14] Abstract 17. Large reasoning models (LRMs) have led to new possibilities in terms of problem-solving, through the devising of a natural language thought process prior to answering a query. While their capabilities are well known across mathematics and coding tasks, their impact on the task of machine translation...
[15.10.2025 04:14] Read previous papers.
[15.10.2025 04:14] Generating reviews via LLM API.
[15.10.2025 04:14] Using data from previous issue: {"categories": ["#training", "#diffusion", "#cv", "#optimization"], "emoji": "üéØ", "ru": {"title": "–î–≤—É—Ö—ç—Ç–∞–ø–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∑–∞–∫—Ä—ã–≤–∞–µ—Ç —Ä–∞–∑—Ä—ã–≤ –º–µ–∂–¥—É –ø–∏–∫—Å–µ–ª—å–Ω—ã–º–∏ –∏ –ª–∞—Ç–µ–Ω—Ç–Ω—ã–º–∏ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –Ω–æ–≤—ã–π –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∏ consistency 
[15.10.2025 04:14] Using data from previous issue: {"categories": ["#machine_translation", "#open_source", "#reasoning", "#dataset", "#multilingual", "#benchmark"], "emoji": "üìö", "ru": {"title": "–ö–∏—Ç–∞–π—Å–∫–∏–µ LLM –ø–æ–±–µ–∂–¥–∞—é—Ç –≤ –ø–µ—Ä–µ–≤–æ–¥–µ –≤–µ–±-—Ä–æ–º–∞–Ω–æ–≤", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ DITING ‚Äî –ø–µ—Ä–≤—É—é –∫–æ–º–ø–ª–µ–∫—Å–Ω—É—é —Å–∏—Å—Ç–µ–º—É –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –ø–µ—Ä–µ–≤–æ–¥–∞ –≤–µ–±-—Ä–æ–º–∞–Ω–æ
[15.10.2025 04:14] Using data from previous issue: {"categories": ["#low_resource", "#data", "#benchmark", "#multimodal", "#transfer_learning", "#training", "#alignment"], "emoji": "üîó", "ru": {"title": "–ì–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–µ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ –∫–∞–∫ –∫–ª—é—á –∫ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–º –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö
[15.10.2025 04:14] Using data from previous issue: {"categories": ["#video", "#open_source", "#inference", "#training", "#dataset", "#diffusion"], "emoji": "‚ö°", "ru": {"title": "–î–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–æ–µ –≤–∏–¥–µ–æ super-resolution –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏", "desc": "FlashVSR ‚Äî –ø–µ—Ä–≤–∞—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –≤–∏–¥–µ–æ super-resolution –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏, —Ä–∞–±–æ—Ç–∞—é—â–∞—è —Å–æ —Å–∫–æ—Ä–æ—Å—Ç—å
[15.10.2025 04:14] Using data from previous issue: {"categories": ["#cv", "#multimodal", "#training", "#optimization", "#rl"], "emoji": "üîç", "ru": {"title": "Rex-Omni: –Ω–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å –≤ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–∏ –æ–±—ä–µ–∫—Ç–æ–≤", "desc": "–í —Å—Ç–∞—Ç—å–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å Rex-Omni, –∫–æ—Ç–æ—Ä–∞—è —É–ª—É—á—à–∞–µ—Ç –∑–∞–¥–∞—á–∏ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –æ–±—ä–µ–∫—Ç–æ–≤, –∏—Å–ø–æ–ª—å–∑—É—è –ø–æ–¥—Ö–æ–¥—ã LLM. Rex-Omni –¥–æ—Å—Ç–∏–≥–∞–µ—Ç
[15.10.2025 04:14] Using data from previous issue: {"categories": ["#multimodal", "#training", "#survey", "#agi", "#agents", "#rl"], "emoji": "üéµ", "ru": {"title": "Vibe Coding: –∫–æ–≥–¥–∞ —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫ –ø—Ä–æ–≤–µ—Ä—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç, –∞ –Ω–µ —á–∏—Ç–∞–µ—Ç –∫–æ–¥", "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –Ω–æ–≤—É—é –ø–∞—Ä–∞–¥–∏–≥–º—É —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º \"Vibe Coding\", –≥–¥–µ —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∏ –ø—Ä–æ–≤–µ—Ä—è—é—Ç —Ä–∞–±–æ—Ç–æ
[15.10.2025 04:14] Using data from previous issue: {"categories": ["#reasoning", "#training", "#optimization", "#transfer_learning", "#agents", "#rl"], "emoji": "ü§ñ", "ru": {"title": "–ú–∞–ª–µ–Ω—å–∫–∏–µ –º–æ–¥–µ–ª–∏ —É—á–∞—Ç—Å—è –¥–µ–π—Å—Ç–≤–æ–≤–∞—Ç—å –∫–∞–∫ –±–æ–ª—å—à–∏–µ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç ERA ‚Äî –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∫–æ–º–ø–∞–∫—Ç–Ω—ã—Ö vision language models –¥–µ–π—Å—Ç–≤–æ–≤–∞—Ç—å –≤ 
[15.10.2025 04:14] Using data from previous issue: {"categories": ["#3d", "#optimization", "#training", "#agents", "#alignment"], "emoji": "ü§ñ", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ —Ä–æ–±–æ—Ç–æ–≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–º—É –º—ã—à–ª–µ–Ω–∏—é –±–µ–∑ 3D —Å–µ–Ω—Å–æ—Ä–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Spatial Forcing (SF) ‚Äî –º–µ—Ç–æ–¥ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è vision-language-action (VLA) –º–æ–¥–µ–ª–µ–π –≤ —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–µ. –ü—Ä
[15.10.2025 04:14] Using data from previous issue: {"categories": ["#reasoning", "#architecture", "#optimization", "#training", "#inference"], "emoji": "üß≠", "ru": {"title": "–£–º–Ω–∞—è –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—è —Å–ª–æ—ë–≤: LLM —É—á–∞—Ç—Å—è –ø—Ä–æ–ø—É—Å–∫–∞—Ç—å –Ω–µ–Ω—É–∂–Ω—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω Dr.LLM ‚Äî –º–µ—Ç–æ–¥ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–∏ —Å–ª–æ—ë–≤ –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π
[15.10.2025 04:14] Using data from previous issue: {"categories": ["#games", "#rlhf", "#training", "#optimization", "#rl"], "emoji": "üéØ", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –ø–æ–º–æ—â—å—é –ª–∏–Ω–µ–π–Ω–æ–π –∞–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ BGPO –¥–ª—è reinforcement learning –≤ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö LLM, –∫–æ—Ç–æ—Ä—ã–µ —Å—Ç–∞–ª–∫–∏–≤–∞—é—Ç—Å—è —Å 
[15.10.2025 04:14] Using data from previous issue: {"categories": ["#training", "#optimization", "#multimodal", "#transfer_learning"], "emoji": "üîÑ", "ru": {"title": "–°–∞–º–æ–æ–±—É—á–µ–Ω–∏–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –≤–Ω—É—Ç—Ä–µ–Ω–Ω—é—é —Å–∞–º–æ–æ—Ü–µ–Ω–∫—É", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏ –ø–∞—Ä–∞–¥–æ–∫—Å: –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –º–æ–≥—É—Ç —Ö–æ—Ä–æ—à–æ –ø–æ–Ω–∏–º–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, –Ω–æ –ø–ª–æ—Ö–æ –∏—Ö –≥–µ–Ω–µ
[15.10.2025 04:14] Using data from previous issue: {"categories": ["#long_context", "#reasoning", "#training", "#optimization", "#agents", "#rl"], "emoji": "üß†", "ru": {"title": "–ü–∞–º—è—Ç—å –∫–∞–∫ –¥–µ–π—Å—Ç–≤–∏–µ: LLM —É—á–∞—Ç—Å—è —É–ø—Ä–∞–≤–ª—è—Ç—å —Å–≤–æ–∏–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º", "desc": "–ë–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ —Å—Ç–∞–ª–∫–∏–≤–∞—é—Ç—Å—è —Å –ø—Ä–æ–±–ª–µ–º–æ–π –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–π –ø–∞–º—è—Ç–∏ –ø—Ä–∏ —Ä–µ—à–µ–Ω–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –∑–∞–¥–∞—á, –∫–æ–≥–¥–∞ –∫
[15.10.2025 04:14] Using data from previous issue: {"categories": ["#training", "#optimization", "#multimodal"], "emoji": "‚õµ", "ru": {"title": "SAIL-Embedding: –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –ø–æ–∏—Å–∫–∞ –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç SAIL-Embedding ‚Äî –æ–º–Ω–∏–º–æ–¥–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤, –∫–æ—Ç–æ—Ä–∞—è —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–π –ø–æ–¥–¥–µ—Ä–∂–∫–∏
[15.10.2025 04:14] Using data from previous issue: {"categories": ["#multimodal", "#reasoning", "#training", "#dataset", "#agi", "#optimization", "#rag", "#benchmark"], "emoji": "üîç", "ru": {"title": "–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ —Å –æ–±—É—á–µ–Ω–∏–µ–º —á–µ—Ä–µ–∑ –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç DeepMMSearch-R1 ‚Äî –ø–µ—Ä–≤—É—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—É—é LLM, —Å–ø–æ—Å–æ–±–Ω—É—é
[15.10.2025 04:14] Using data from previous issue: {"categories": ["#cv", "#multimodal", "#reasoning", "#training", "#transfer_learning", "#diffusion"], "emoji": "üîó", "ru": {"title": "–ï–¥–∏–Ω—ã–π —ç–Ω–∫–æ–¥–µ—Ä –¥–ª—è —Ç–µ–∫—Å—Ç–∞ –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "UniFusion ‚Äî —ç—Ç–æ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä–∞—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∑–∞–º–æ—Ä–æ–∂–µ–Ω–Ω—É
[15.10.2025 04:14] Using data from previous issue: {"categories": ["#cv", "#interpretability", "#open_source", "#reasoning", "#training", "#optimization", "#synthetic"], "emoji": "üîÑ", "ru": {"title": "Flow Poke Transformer: –Ω–æ–≤–∞—è —ç—Ä–∞ –≤ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–∏ –¥–≤–∏–∂–µ–Ω–∏—è", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å Flow Poke Transformer (FPT), –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤
[15.10.2025 04:14] Using data from previous issue: {"categories": ["#cv", "#multimodal", "#reasoning", "#dataset", "#benchmark", "#data"], "emoji": "üêù", "ru": {"title": "HoneyBee: –ö–∞–∫ –ø—Ä–∞–≤–∏–ª—å–Ω–æ –≥–æ—Ç–æ–≤–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –∏–∑—É—á–∏–ª–∏ –ø—Ä–∏–Ω—Ü–∏–ø—ã —Å–æ–∑–¥–∞–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è vision-language –º–æ–¥–µ–ª
[15.10.2025 04:14] Using data from previous issue: {"categories": ["#low_resource", "#reasoning", "#training", "#multilingual", "#machine_translation", "#synthetic"], "emoji": "ü§î", "ru": {"title": "–†–∞–∑–º—ã—à–ª–µ–Ω–∏—è –Ω–µ –ø–æ–º–æ–≥–∞—é—Ç LLM –ª—É—á—à–µ –ø–µ—Ä–µ–≤–æ–¥–∏—Ç—å", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –∏–∑—É—á–∏–ª–∏, –ø–æ–º–æ–≥–∞—é—Ç –ª–∏ \"—Ç–æ–∫–µ–Ω—ã —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏–π\" (thinking tokens) –±–æ–ª—å—à–∏–º reasoning –º–æ–¥
[15.10.2025 04:14] Renaming data file.
[15.10.2025 04:14] Renaming previous data. hf_papers.json to ./d/2025-10-15.json
[15.10.2025 04:14] Saving new data file.
[15.10.2025 04:14] Generating page.
[15.10.2025 04:14] Renaming previous page.
[15.10.2025 04:14] Renaming previous data. index.html to ./d/2025-10-15.html
[15.10.2025 04:14] Writing result.
[15.10.2025 04:14] Renaming log file.
[15.10.2025 04:14] Renaming previous data. log.txt to ./logs/2025-10-15_last_log.txt
