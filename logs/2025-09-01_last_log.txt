[01.09.2025 21:10] Read previous papers.
[01.09.2025 21:10] Generating top page (month).
[01.09.2025 21:10] Writing top page (month).
[01.09.2025 22:10] Read previous papers.
[01.09.2025 22:10] Get feed.
[01.09.2025 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2508.21113
[01.09.2025 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2508.21112
[01.09.2025 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2508.18106
[01.09.2025 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2508.20470
[01.09.2025 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2508.21148
[01.09.2025 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2508.13618
[01.09.2025 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2508.21365
[01.09.2025 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2508.17677
[01.09.2025 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2508.21767
[01.09.2025 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2508.21290
[01.09.2025 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2508.21376
[01.09.2025 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2508.21456
[01.09.2025 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2508.21188
[01.09.2025 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2508.20085
[01.09.2025 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2508.17380
[01.09.2025 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2508.14197
[01.09.2025 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2508.21172
[01.09.2025 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2508.19600
[01.09.2025 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2508.17008
[01.09.2025 22:10] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[01.09.2025 22:10] No deleted papers detected.
[01.09.2025 22:10] Downloading and parsing papers (pdf, html). Total: 19.
[01.09.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2508.21113.
[01.09.2025 22:10] Extra JSON file exists (./assets/json/2508.21113.json), skip PDF parsing.
[01.09.2025 22:10] Paper image links file exists (./assets/img_data/2508.21113.json), skip HTML parsing.
[01.09.2025 22:10] Success.
[01.09.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2508.21112.
[01.09.2025 22:10] Extra JSON file exists (./assets/json/2508.21112.json), skip PDF parsing.
[01.09.2025 22:10] Paper image links file exists (./assets/img_data/2508.21112.json), skip HTML parsing.
[01.09.2025 22:10] Success.
[01.09.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2508.18106.
[01.09.2025 22:10] Extra JSON file exists (./assets/json/2508.18106.json), skip PDF parsing.
[01.09.2025 22:10] Paper image links file exists (./assets/img_data/2508.18106.json), skip HTML parsing.
[01.09.2025 22:10] Success.
[01.09.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2508.20470.
[01.09.2025 22:10] Extra JSON file exists (./assets/json/2508.20470.json), skip PDF parsing.
[01.09.2025 22:10] Paper image links file exists (./assets/img_data/2508.20470.json), skip HTML parsing.
[01.09.2025 22:10] Success.
[01.09.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2508.21148.
[01.09.2025 22:10] Extra JSON file exists (./assets/json/2508.21148.json), skip PDF parsing.
[01.09.2025 22:10] Paper image links file exists (./assets/img_data/2508.21148.json), skip HTML parsing.
[01.09.2025 22:10] Success.
[01.09.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2508.13618.
[01.09.2025 22:10] Extra JSON file exists (./assets/json/2508.13618.json), skip PDF parsing.
[01.09.2025 22:10] Paper image links file exists (./assets/img_data/2508.13618.json), skip HTML parsing.
[01.09.2025 22:10] Success.
[01.09.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2508.21365.
[01.09.2025 22:10] Extra JSON file exists (./assets/json/2508.21365.json), skip PDF parsing.
[01.09.2025 22:10] Paper image links file exists (./assets/img_data/2508.21365.json), skip HTML parsing.
[01.09.2025 22:10] Success.
[01.09.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2508.17677.
[01.09.2025 22:10] Extra JSON file exists (./assets/json/2508.17677.json), skip PDF parsing.
[01.09.2025 22:10] Paper image links file exists (./assets/img_data/2508.17677.json), skip HTML parsing.
[01.09.2025 22:10] Success.
[01.09.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2508.21767.
[01.09.2025 22:10] Extra JSON file exists (./assets/json/2508.21767.json), skip PDF parsing.
[01.09.2025 22:10] Paper image links file exists (./assets/img_data/2508.21767.json), skip HTML parsing.
[01.09.2025 22:10] Success.
[01.09.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2508.21290.
[01.09.2025 22:10] Extra JSON file exists (./assets/json/2508.21290.json), skip PDF parsing.
[01.09.2025 22:10] Paper image links file exists (./assets/img_data/2508.21290.json), skip HTML parsing.
[01.09.2025 22:10] Success.
[01.09.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2508.21376.
[01.09.2025 22:10] Extra JSON file exists (./assets/json/2508.21376.json), skip PDF parsing.
[01.09.2025 22:10] Paper image links file exists (./assets/img_data/2508.21376.json), skip HTML parsing.
[01.09.2025 22:10] Success.
[01.09.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2508.21456.
[01.09.2025 22:10] Extra JSON file exists (./assets/json/2508.21456.json), skip PDF parsing.
[01.09.2025 22:10] Paper image links file exists (./assets/img_data/2508.21456.json), skip HTML parsing.
[01.09.2025 22:10] Success.
[01.09.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2508.21188.
[01.09.2025 22:10] Extra JSON file exists (./assets/json/2508.21188.json), skip PDF parsing.
[01.09.2025 22:10] Paper image links file exists (./assets/img_data/2508.21188.json), skip HTML parsing.
[01.09.2025 22:10] Success.
[01.09.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2508.20085.
[01.09.2025 22:10] Extra JSON file exists (./assets/json/2508.20085.json), skip PDF parsing.
[01.09.2025 22:10] Paper image links file exists (./assets/img_data/2508.20085.json), skip HTML parsing.
[01.09.2025 22:10] Success.
[01.09.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2508.17380.
[01.09.2025 22:10] Extra JSON file exists (./assets/json/2508.17380.json), skip PDF parsing.
[01.09.2025 22:10] Paper image links file exists (./assets/img_data/2508.17380.json), skip HTML parsing.
[01.09.2025 22:10] Success.
[01.09.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2508.14197.
[01.09.2025 22:10] Downloading paper 2508.14197 from http://arxiv.org/pdf/2508.14197v1...
[01.09.2025 22:10] Failed to download and parse paper https://huggingface.co/papers/2508.14197: 'LTChar' object is not iterable
[01.09.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2508.21172.
[01.09.2025 22:10] Extra JSON file exists (./assets/json/2508.21172.json), skip PDF parsing.
[01.09.2025 22:10] Paper image links file exists (./assets/img_data/2508.21172.json), skip HTML parsing.
[01.09.2025 22:10] Success.
[01.09.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2508.19600.
[01.09.2025 22:10] Extra JSON file exists (./assets/json/2508.19600.json), skip PDF parsing.
[01.09.2025 22:10] Paper image links file exists (./assets/img_data/2508.19600.json), skip HTML parsing.
[01.09.2025 22:10] Success.
[01.09.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2508.17008.
[01.09.2025 22:10] Extra JSON file exists (./assets/json/2508.17008.json), skip PDF parsing.
[01.09.2025 22:10] Paper image links file exists (./assets/img_data/2508.17008.json), skip HTML parsing.
[01.09.2025 22:10] Success.
[01.09.2025 22:10] Enriching papers with extra data.
[01.09.2025 22:10] ********************************************************************************
[01.09.2025 22:10] Abstract 0. R-4B, an auto-thinking multimodal large language model, uses bi-mode annealing and Bi-mode Policy Optimization to adaptively decide on problem-solving strategies, achieving state-of-the-art performance with lower computational cost.  					AI-generated summary 				 Multimodal Large Language Models (M...
[01.09.2025 22:10] ********************************************************************************
[01.09.2025 22:10] Abstract 1. EO-Robotics, comprising EO-1 model and EO-Data1.5M dataset, advances multimodal embodied reasoning and robot control through interleaved vision-text-action pre-training.  					AI-generated summary 				 The human ability to seamlessly perform multimodal reasoning and physical interaction in the open ...
[01.09.2025 22:10] ********************************************************************************
[01.09.2025 22:10] Abstract 2. A.S.E is a benchmark for evaluating the security of code generated by large language models using real-world repositories and expert-defined rules, revealing insights into model performance and decoding strategies.  					AI-generated summary 				 The increasing adoption of large language models (LLM...
[01.09.2025 22:10] ********************************************************************************
[01.09.2025 22:10] Abstract 3. Using video data to provide commonsense priors enhances 3D asset generation, enabling spatial consistency and semantic plausibility in 3D content creation.  					AI-generated summary 				 Scaling laws have validated the success and promise of large-data-trained models in creative generation across t...
[01.09.2025 22:10] ********************************************************************************
[01.09.2025 22:10] Abstract 4. Sci-LLMs are evolving through a co-development with scientific data, addressing unique challenges like multimodal and domain-specific information, and are moving towards autonomous, closed-loop systems in scientific research.  					AI-generated summary 				 Scientific Large Language Models (Sci-LLMs...
[01.09.2025 22:10] ********************************************************************************
[01.09.2025 22:10] Abstract 5. TalkVid, a large-scale, high-quality, and diverse dataset, improves audio-driven talking head synthesis by enhancing generalization across human diversity and revealing subgroup performance disparities.  					AI-generated summary 				 Audio-driven talking head synthesis has achieved remarkable photo...
[01.09.2025 22:10] ********************************************************************************
[01.09.2025 22:10] Abstract 6. Think in Games (TiG) framework enables large language models to develop procedural knowledge through interactive game environments, achieving competitive performance with reduced data and computational demands while providing transparent explanations.  					AI-generated summary 				 Large language m...
[01.09.2025 22:10] ********************************************************************************
[01.09.2025 22:10] Abstract 7. Dynamic adjustment of data mixture based on Group Influence metric improves language model performance by adapting to evolving learning preferences.  					AI-generated summary 				 The data mixture used in the pre-training of a language model is a cornerstone of its final performance. However, a sta...
[01.09.2025 22:10] ********************************************************************************
[01.09.2025 22:10] Abstract 8. UItron, an open-source foundational model for GUI agents, enhances visual understanding and task planning through advanced perception, grounding, and planning capabilities, achieving superior performance in Chinese app scenarios.  					AI-generated summary 				 GUI agent aims to enable automated ope...
[01.09.2025 22:10] ********************************************************************************
[01.09.2025 22:10] Abstract 9. Jina-code-embeddings uses an autoregressive backbone pre-trained on text and code to generate embeddings for code retrieval, question-answering, and similarity identification.  					AI-generated summary 				 jina-code-embeddings is a novel code embedding model suite designed to retrieve code from na...
[01.09.2025 22:10] ********************************************************************************
[01.09.2025 22:10] Abstract 10. AHELM is a comprehensive benchmark for audio-language models that evaluates multiple aspects including fairness, safety, and reasoning across various datasets and models.  					AI-generated summary 				 Evaluations of audio-language models (ALMs) -- multimodal models that take interleaved audio and ...
[01.09.2025 22:10] ********************************************************************************
[01.09.2025 22:10] Abstract 11. Morae, a UI agent, enhances accessibility for BLV users by involving them in decision-making processes during task execution, using large multimodal models to interpret user queries and UI elements.  					AI-generated summary 				 User interface (UI) agents promise to make inaccessible or complex UI...
[01.09.2025 22:10] ********************************************************************************
[01.09.2025 22:10] Abstract 12. Reinforcement learning applied to large language models shows counterintuitive results that depend on pre-existing model-task alignment, with standard RL methods remaining robust in challenging scenarios.  					AI-generated summary 				 Recent advances in applying reinforcement learning (RL) to larg...
[01.09.2025 22:10] ********************************************************************************
[01.09.2025 22:10] Abstract 13. HERMES is a human-to-robot learning framework that translates human hand motions into robotic behaviors, using reinforcement learning and sim2real transfer for versatile manipulation in diverse environments.  					AI-generated summary 				 Leveraging human motion data to impart robots with versatile...
[01.09.2025 22:10] ********************************************************************************
[01.09.2025 22:10] Abstract 14. VIPER-R1, a multimodal model combining visual perception, trajectory data, and symbolic reasoning, discovers physical laws with higher accuracy and interpretability than existing methods.  					AI-generated summary 				 Automated discovery of physical laws from observational data in the real world i...
[01.09.2025 22:10] ********************************************************************************
[01.09.2025 22:10] Abstract 15. CLIPSym, a vision-language model using CLIP, enhances symmetry detection through a rotation-equivariant decoder and semantic-aware prompting, outperforming existing methods on standard datasets.  					AI-generated summary 				 Symmetry is one of the most fundamental geometric cues in computer vision...
[01.09.2025 22:10] ********************************************************************************
[01.09.2025 22:10] Abstract 16. Deep Residual Echo State Networks (DeepResESNs) enhance long-term temporal modeling and memory capacity through hierarchical untrained residual layers, outperforming traditional shallow and deep reservoir computing methods.  					AI-generated summary 				 Echo State Networks (ESNs) are a particular ...
[01.09.2025 22:10] ********************************************************************************
[01.09.2025 22:10] Abstract 17. Post-training quantization of YOLO models is evaluated for robustness to real-world degradations, with a focus on the effectiveness of a degradation-aware calibration strategy for Static INT8 quantization.  					AI-generated summary 				 Post-training quantization (PTQ) is crucial for deploying effi...
[01.09.2025 22:10] ********************************************************************************
[01.09.2025 22:10] Abstract 18. EduRABSA is a public dataset and ASQE-DPT is a tool for aspect-based sentiment analysis in education reviews, addressing the lack of resources in this domain.  					AI-generated summary 				 Every year, most educational institutions seek and receive an enormous volume of text feedback from students ...
[01.09.2025 22:10] Read previous papers.
[01.09.2025 22:10] Generating reviews via LLM API.
[01.09.2025 22:10] Using data from previous issue: {"categories": ["#training", "#multimodal", "#reasoning", "#benchmark", "#dataset", "#optimization"], "emoji": "üß†", "ru": {"title": "R-4B: –ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á", "desc": "R-4B - —ç—Ç–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –±–æ–ª—å—à–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å —Å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å—é –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è. –û–Ω–∞ –∏—Å–ø
[01.09.2025 22:10] Using data from previous issue: {"categories": ["#architecture", "#training", "#agents", "#multimodal", "#agi", "#reasoning", "#dataset"], "emoji": "ü§ñ", "ru": {"title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–º –ò–ò –¥–ª—è —Ä–æ–±–æ—Ç–æ–≤: EO-Robotics –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –∑—Ä–µ–Ω–∏–µ, —Ç–µ–∫—Å—Ç –∏ –¥–µ–π—Å—Ç–≤–∏–µ", "desc": "EO-Robotics –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π —Å–∏—Å—Ç–µ–º—É, —Å–æ—Å—Ç–æ—è—â—É—é –∏–∑ –º–æ–¥–µ–ª–∏
[01.09.2025 22:10] Using data from previous issue: {"categories": ["#open_source", "#security", "#benchmark", "#dataset"], "emoji": "üõ°Ô∏è", "ru": {"title": "A.S.E: –ù–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –æ—Ü–µ–Ω–∫–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –ò–ò-–≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º–æ–≥–æ –∫–æ–¥–∞", "desc": "A.S.E - —ç—Ç–æ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –∫–æ–¥–∞, –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º–æ–≥–æ –±–æ–ª—å—à–∏–º–∏ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ (LLM). –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ä
[01.09.2025 22:10] Using data from previous issue: {"categories": ["#dataset", "#3d", "#multimodal", "#open_source", "#synthetic"], "emoji": "üé•", "ru": {"title": "–í–∏–¥–µ–æ –∫–∞–∫ –∏—Å—Ç–æ—á–Ω–∏–∫ –∑–¥—Ä–∞–≤–æ–≥–æ —Å–º—ã—Å–ª–∞ –¥–ª—è 3D-–≥–µ–Ω–µ—Ä–∞—Ü–∏–∏", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –≤–∏–¥–µ–æ–¥–∞–Ω–Ω—ã—Ö –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ 3D-–æ–±—ä–µ–∫—Ç–æ–≤. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç –¥–∞—Ç–∞—Å–µ—Ç Droplet3D-4M —Å –∞
[01.09.2025 22:10] Using data from previous issue: {"categories": ["#benchmark", "#agents", "#survey", "#multimodal", "#data", "#dataset", "#science"], "emoji": "üß¨", "ru": {"title": "Sci-LLMs: —ç–≤–æ–ª—é—Ü–∏—è –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ –≤ –Ω–∞—É—á–Ω–æ–º –ø–æ–∑–Ω–∞–Ω–∏–∏", "desc": "–ù–∞—É—á–Ω—ã–µ –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (Sci-LLMs) —Ä–∞–∑–≤–∏–≤–∞—é—Ç—Å—è –≤ —Ç–µ—Å–Ω–æ–π —Å–≤—è–∑–∏ —Å –Ω–∞—É—á–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏, —Ä–µ—à
[01.09.2025 22:10] Using data from previous issue: {"categories": ["#benchmark", "#open_source", "#ethics", "#transfer_learning", "#dataset", "#cv", "#data"], "emoji": "üó£Ô∏è", "ru": {"title": "TalkVid: –±–æ–ª—å—à–æ–π –∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–∏–Ω—Ç–µ–∑–∞ –≥–æ–≤–æ—Ä—è—â–∏—Ö –≥–æ–ª–æ–≤", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö TalkVid –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–∏–Ω—Ç–µ–∑–∞ –≥–æ–≤–æ—Ä—è—â–∏—Ö
[01.09.2025 22:10] Using data from previous issue: {"categories": ["#training", "#games", "#optimization", "#reasoning", "#interpretability", "#multimodal", "#rl", "#rlhf"], "emoji": "üéÆ", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ –ò–ò —á–µ—Ä–µ–∑ –∏–≥—Ä—ã: –æ—Ç –∑–Ω–∞–Ω–∏–π –∫ —É–º–µ–Ω–∏—è–º", "desc": "–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ Think in Games (TiG), –ø–æ–∑–≤–æ–ª—è—é—â–∞—è –±–æ–ª—å—à–∏–º —è–∑—ã–∫–æ–≤—ã–º –º–æ–¥–µ–ª—è–º —Ä–∞–∑–≤–∏–≤–∞
[01.09.2025 22:10] Using data from previous issue: {"categories": ["#data", "#optimization", "#training"], "emoji": "üîÑ", "ru": {"title": "–î–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç TiKMiX - –º–µ—Ç–æ–¥ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∏ —Å–º–µ—Å–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã –≤–≤–æ–¥—è—Ç –º–µ—Ç
[01.09.2025 22:10] Using data from previous issue: {"categories": ["#benchmark", "#rl", "#open_source", "#training", "#optimization", "#reasoning", "#agi", "#dataset", "#data", "#agents"], "emoji": "ü§ñ", "ru": {"title": "UItron: –ò–ò-–∞–≥–µ–Ω—Ç –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏—Ö –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–≤", "desc": "UItron - —ç—Ç–æ –º–æ–¥–µ–ª—å –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å –æ—Ç–∫—Ä—ã—Ç—ã–º –∏—Å—Ö–æ–¥–Ω—ã–º –∫–æ
[01.09.2025 22:10] Using data from previous issue: {"categories": ["#multilingual", "#transfer_learning", "#data", "#dataset", "#games", "#plp", "#small_models", "#training"], "emoji": "üß†", "ru": {"title": "–£–º–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π —Ä–∞–±–æ—Ç—ã —Å –∫–æ–¥–æ–º", "desc": "Jina-code-embeddings - —ç—Ç–æ –Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å –≤—Å—Ç—Ä–∞–∏–≤–∞–Ω–∏—è –∫–æ–¥–∞, –æ—Å–Ω–æ–≤–∞–Ω–Ω–∞—è –Ω–∞ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ
[01.09.2025 22:10] Using data from previous issue: {"categories": ["#benchmark", "#open_source", "#ethics", "#reasoning", "#multimodal", "#audio", "#dataset"], "emoji": "üéß", "ru": {"title": "AHELM: –í—Å–µ—Å—Ç–æ—Ä–æ–Ω–Ω—è—è –æ—Ü–µ–Ω–∫–∞ –∞—É–¥–∏–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "AHELM - —ç—Ç–æ –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∞—É–¥–∏–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (ALM). –û–Ω –∏–∑–º–µ—Ä—è–µ—Ç 10 –∞—Å–ø–µ–∫—Ç–æ–≤, 
[01.09.2025 22:10] Using data from previous issue: {"categories": ["#ethics", "#agi", "#multimodal", "#healthcare", "#agents"], "emoji": "üëÅÔ∏è", "ru": {"title": "Morae: –ò–ò-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç, —Ä–∞—Å—à–∏—Ä—è—é—â–∏–π –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π —Å –Ω–∞—Ä—É—à–µ–Ω–∏—è–º–∏ –∑—Ä–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Morae - –∞–≥–µ–Ω—Ç–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –¥–ª—è –ø–æ
[01.09.2025 22:10] Using data from previous issue: {"categories": ["#training", "#rlhf", "#reasoning", "#alignment", "#rl"], "emoji": "üß†", "ru": {"title": "–ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–µ —ç—Ñ—Ñ–µ–∫—Ç—ã –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –≤ –ò–ò –∑–∞–≤–∏—Å—è—Ç –æ—Ç –Ω–∞—á–∞–ª—å–Ω–æ–π –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –º–æ–¥–µ–ª–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –∫ –±–æ–ª—å—à–∏–º —è–∑—ã–∫–æ–≤—ã–º –º–æ–¥–µ–ª—è–º –≤—ã—è–≤–∏–ª–æ –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–µ
[01.09.2025 22:10] Using data from previous issue: {"categories": ["#rl", "#agents", "#optimization", "#transfer_learning", "#robotics"], "emoji": "ü§ñ", "ru": {"title": "–û—Ç –¥–≤–∏–∂–µ–Ω–∏–π —á–µ–ª–æ–≤–µ–∫–∞ –∫ —É–º–µ–ª—ã–º —Ä—É–∫–∞–º —Ä–æ–±–æ—Ç–∞", "desc": "HERMES - —ç—Ç–æ —Å–∏—Å—Ç–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è —Ä–æ–±–æ—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∞–Ω–Ω—ã—Ö –æ –¥–≤–∏–∂–µ–Ω–∏—è—Ö —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏—Ö —Ä—É–∫. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –∏ –ø
[01.09.2025 22:10] Using data from previous issue: {"categories": ["#agents", "#reasoning", "#rl", "#multimodal", "#interpretability", "#dataset", "#science"], "emoji": "üî¨", "ru": {"title": "VIPER-R1: –ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –ò–ò –¥–ª—è –æ—Ç–∫—Ä—ã—Ç–∏—è –∑–∞–∫–æ–Ω–æ–≤ —Ñ–∏–∑–∏–∫–∏", "desc": "VIPER-R1 - —ç—Ç–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ—Ç–∫—Ä—ã—Ç–∏—è —Ñ–∏–∑–∏—á–µ—Å–∫–∏—Ö –∑–∞–∫–æ–Ω–æ–≤, —Å–æ—á–µ—Ç
[01.09.2025 22:10] Using data from previous issue: {"categories": ["#dataset", "#architecture", "#optimization", "#multimodal", "#cv", "#games"], "emoji": "üîç", "ru": {"title": "CLIPSym: –£–ª—É—á—à–µ–Ω–Ω–æ–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —Å–∏–º–º–µ—Ç—Ä–∏–∏ —Å –ø–æ–º–æ—â—å—é —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏", "desc": "CLIPSym - —ç—Ç–æ –Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è —Å–∏–º–º–µ—Ç—Ä–∏–∏, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å CLIP. –û
[01.09.2025 22:10] Using data from previous issue: {"categories": ["#long_context", "#math", "#optimization", "#architecture", "#training"], "emoji": "üåä", "ru": {"title": "–ì–ª—É–±–æ–∫–∏–µ –æ—Å—Ç–∞—Ç–æ—á–Ω—ã–µ —Å–µ—Ç–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤", "desc": "–í —ç—Ç–æ–π —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã –ì–ª—É–±–æ–∫–∏–µ –û—Å—Ç–∞—Ç–æ—á–Ω—ã–µ –≠—Ö–æ-–ì–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã–µ –°–µ—Ç–∏ (DeepResESNs), –Ω–æ–≤—ã–π –∫–ª–∞—Å—Å –≥–ª—É
[01.09.2025 22:10] Using data from previous issue: {"categories": ["#benchmark", "#security", "#optimization", "#inference"], "emoji": "üî¨", "ru": {"title": "–ö–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è YOLO: –±–∞–ª–∞–Ω—Å –º–µ–∂–¥—É —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å—é –∏ —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å—é", "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –≤–ª–∏—è–Ω–∏–µ –ø–æ—Å—Ç-—Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–π –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ –º–æ–¥–µ–ª–µ–π YOLO –Ω–∞ –∏—Ö —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∫ –∏—Å–∫–∞–∂–µ–Ω–∏—è–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ —Ä–µ–∞–ª—å–Ω
[01.09.2025 22:10] Using data from previous issue: {"categories": ["#open_source", "#dataset", "#low_resource", "#data"], "emoji": "üéì", "ru": {"title": "–ù–æ–≤—ã–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –º–Ω–µ–Ω–∏–π –≤ –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –æ—Ç–∑—ã–≤–∞—Ö", "desc": "EduRABSA - —ç—Ç–æ –ø—É–±–ª–∏—á–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –ø–æ –∞—Å–ø–µ–∫—Ç–∞–º –≤ –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –æ—Ç–∑—ã–≤–∞—Ö. ASQE-DPT - –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è —Ä–∞
[01.09.2025 22:10] Renaming data file.
[01.09.2025 22:10] Renaming previous data. hf_papers.json to ./d/2025-09-01.json
[01.09.2025 22:10] Saving new data file.
[01.09.2025 22:10] Generating page.
[01.09.2025 22:10] Renaming previous page.
[01.09.2025 22:10] Renaming previous data. index.html to ./d/2025-09-01.html
[01.09.2025 22:10] Writing result.
[01.09.2025 22:10] Renaming log file.
[01.09.2025 22:10] Renaming previous data. log.txt to ./logs/2025-09-01_last_log.txt
