[01.09.2025 04:24] Read previous papers.
[01.09.2025 04:24] Generating top page (month).
[01.09.2025 04:24] Writing top page (month).
[01.09.2025 05:12] Read previous papers.
[01.09.2025 05:12] Get feed.
[01.09.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2508.18106
[01.09.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2508.21112
[01.09.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2508.21113
[01.09.2025 05:12] Extract page data from URL. URL: https://huggingface.co/papers/2508.21290
[01.09.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2508.17677
[01.09.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2508.13618
[01.09.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2508.21767
[01.09.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2508.21456
[01.09.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2508.21376
[01.09.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2508.21365
[01.09.2025 05:12] Extract page data from URL. URL: https://huggingface.co/papers/2508.20085
[01.09.2025 05:12] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[01.09.2025 05:12] No deleted papers detected.
[01.09.2025 05:12] Downloading and parsing papers (pdf, html). Total: 11.
[01.09.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2508.18106.
[01.09.2025 05:12] Extra JSON file exists (./assets/json/2508.18106.json), skip PDF parsing.
[01.09.2025 05:12] Paper image links file exists (./assets/img_data/2508.18106.json), skip HTML parsing.
[01.09.2025 05:12] Success.
[01.09.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2508.21112.
[01.09.2025 05:12] Extra JSON file exists (./assets/json/2508.21112.json), skip PDF parsing.
[01.09.2025 05:12] Paper image links file exists (./assets/img_data/2508.21112.json), skip HTML parsing.
[01.09.2025 05:12] Success.
[01.09.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2508.21113.
[01.09.2025 05:12] Extra JSON file exists (./assets/json/2508.21113.json), skip PDF parsing.
[01.09.2025 05:12] Paper image links file exists (./assets/img_data/2508.21113.json), skip HTML parsing.
[01.09.2025 05:12] Success.
[01.09.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2508.21290.
[01.09.2025 05:12] Downloading paper 2508.21290 from http://arxiv.org/pdf/2508.21290v1...
[01.09.2025 05:12] Extracting affiliations from text.
[01.09.2025 05:12] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Daria Kryvosheieva1,2 Saba Sturua2 Michael Günther2 Scott Martens2 Han Xiao2 1Massachusetts Institute of Technology 2Jina AI GmbH Prinzessinnenstraße 19, 10969, Berlin, Germany research@jina.ai "
[01.09.2025 05:12] Response: ```python
["Massachusetts Institute of Technology", "Jina AI GmbH"]
```
[01.09.2025 05:12] Deleting PDF ./assets/pdf/2508.21290.pdf.
[01.09.2025 05:12] Success.
[01.09.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2508.17677.
[01.09.2025 05:12] Extra JSON file exists (./assets/json/2508.17677.json), skip PDF parsing.
[01.09.2025 05:12] Paper image links file exists (./assets/img_data/2508.17677.json), skip HTML parsing.
[01.09.2025 05:12] Success.
[01.09.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2508.13618.
[01.09.2025 05:12] Extra JSON file exists (./assets/json/2508.13618.json), skip PDF parsing.
[01.09.2025 05:12] Paper image links file exists (./assets/img_data/2508.13618.json), skip HTML parsing.
[01.09.2025 05:12] Success.
[01.09.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2508.21767.
[01.09.2025 05:12] Extra JSON file exists (./assets/json/2508.21767.json), skip PDF parsing.
[01.09.2025 05:12] Paper image links file exists (./assets/img_data/2508.21767.json), skip HTML parsing.
[01.09.2025 05:12] Success.
[01.09.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2508.21456.
[01.09.2025 05:12] Extra JSON file exists (./assets/json/2508.21456.json), skip PDF parsing.
[01.09.2025 05:12] Paper image links file exists (./assets/img_data/2508.21456.json), skip HTML parsing.
[01.09.2025 05:12] Success.
[01.09.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2508.21376.
[01.09.2025 05:12] Extra JSON file exists (./assets/json/2508.21376.json), skip PDF parsing.
[01.09.2025 05:12] Paper image links file exists (./assets/img_data/2508.21376.json), skip HTML parsing.
[01.09.2025 05:12] Success.
[01.09.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2508.21365.
[01.09.2025 05:12] Extra JSON file exists (./assets/json/2508.21365.json), skip PDF parsing.
[01.09.2025 05:12] Paper image links file exists (./assets/img_data/2508.21365.json), skip HTML parsing.
[01.09.2025 05:12] Success.
[01.09.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2508.20085.
[01.09.2025 05:12] Downloading paper 2508.20085 from http://arxiv.org/pdf/2508.20085v2...
[01.09.2025 05:13] Extracting affiliations from text.
[01.09.2025 05:13] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"HERMES: Human-to-Robot Embodied Learning from Multi-SouRce Zhecheng Yuan1,2, Tianming Wei1,2, Langzhe Gu1,2, Pu Hua1,2, Tianhai Liang1,2, Yuanpei Chen3, Huazhe Xu1,2 1 5 2 0 2 8 2 ] . [ 2 5 8 0 0 2 . 8 0 5 2 : r AbstractLeveraging human motion data to impart robots with versatile manipulation skills has emerged as promising paradigm in robotic manipulation. Nevertheless, translating multi-source human hand motions into feasible robot behaviors remains challenging, particularly for robots equipped with multi-fingered dexterous hands characterized by complex, highdimensional action spaces. Moreover, existing approaches often struggle to produce policies capable of adapting to diverse environmental conditions. In this paper, we introduce HERMES, human-to-robot learning framework for mobile bimanual dexterous manipulation. First, HERMES formulates unified reinforcement learning approach capable of seamlessly transforming heterogeneous human hand motions from multiple sources into physically plausible robotic behaviors. Subsequently, to mitigate the sim2real gap, we devise an end-to-end, depth image-based sim2real transfer method for improved generalization to realworld scenarios. Furthermore, to enable autonomous operation in varied and unstructured environments, we augment the navigation foundation model with closed-loop Perspective-n-Point (PnP) localization mechanism, ensuring precise alignment of visual goals and effectively bridging autonomous navigation and dexterous manipulation. Extensive experimental results demonstrate that HERMES consistently exhibits generalizable behaviors across diverse, in-the-wild scenarios, successfully performing numerous complex mobile bimanual dexterous manipulation tasks. Project Page:https://gemcollector.github.io/HERMES/. Index TermsBimanual dexterous manipulation, Mobile manipulation, Sim2real, Reinforcement learning, Learning from human motion. I. INTRODUCTION CHIEVING human-level dexterity for robots has long been central chall"
[01.09.2025 05:13] Response: ```python
[]
```
[01.09.2025 05:13] Extracting affiliations from text.
[01.09.2025 05:13] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"HERMES: Human-to-Robot Embodied Learning from Multi-SouRceZhecheng Yuan1,2, Tianming Wei1,2, Langzhe Gu1,2, Pu Hua1,2, Tianhai Liang1,2, Yuanpei Chen3, Huazhe Xu1,2 1 5 2 0 2 8 2 ] . [ 2 5 8 0 0 2 . 8 0 5 2 : r AbstractLeveraging human motion data to impart robots with versatile manipulation skills has emerged as promising paradigm in robotic manipulation. Nevertheless, translating multi-source human hand motions into feasible robot behaviors remains challenging, particularly for robots equipped with multi-fingered dexterous hands characterized by complex, highdimensional action spaces. Moreover, existing approaches often struggle to produce policies capable of adapting to diverse environmental conditions. In this paper, we introduce HERMES, human-to-robot learning framework for mobile bimanual dexterous manipulation. First, HERMES formulates unified reinforcement learning approach capable of seamlessly transforming heterogeneous human hand motions from multiple sources into physically plausible robotic behaviors. Subsequently, to mitigate the sim2real gap, we devise an end-to-end, depth image-based sim2real transfer method for improved generalization to realworld scenarios. Furthermore, to enable autonomous operation in varied and unstructured environments, we augment the navigation foundation model with closed-loop Perspective-n-Point (PnP) localization mechanism, ensuring precise alignment of visual goals and effectively bridging autonomous navigation and dexterous manipulation. Extensive experimental results demonstrate that HERMES consistently exhibits generalizable behaviors across diverse, in-the-wild scenarios, successfully performing numerous complex mobile bimanual dexterous manipulation tasks. Project Page:https://gemcollector.github.io/HERMES/. Index TermsBimanual dexterous manipulation, Mobile manipulation, Sim2real, Reinforcement learning, Learning from human motion. I. INTRODUCTION CHIEVING human-level dexterity for robots has long been central challenge in robotic research. The prospect of bimanual robotic systems with dexterous hands that mirror human sensorimotor dexterity holds the promise of seamlessly integrating robots into daily human activities and environments. Despite notable progress, how to capitalize on the abundance of available human data and develop algorithms suited to intricate and high-precision dexterous manipulation remains underexplored. Humans continuously generate diverse bimanual manipulation data, inherently serving as natural guidance for robots to emulate human-like behaviors. Several previous studies [1] [5] have attempted to extract trajectories of human hands and manipulated objects from video data, subsequently applying them to robotic manipulation tasks. Nevertheless, these methods have predominantly targeted robots equipped with simple parallel gripper end effectors, failing to generalize indicates equal contribution. 1 Tsinghua University 2 Shanghai Qi Zhi Institute 3 Peking University Corresponding to huazhe xu@mail.tsinghua.edu.cn. effectively to dexterous hands due to the vastly greater complexity of action space. Despite recent advances that utilize kinematic retargeting approaches to produce humanlike robotic motions [6][10], these approaches still fall short in achieving physically-aware pose retargeting and bridging the embodiment gap to derive feasible robot actions capable of successfully accomplishing the intended tasks. critical limitation lies in omitting the modeling of interactions between robotic hands and manipulated objects, fundamental component of manipulation tasks. Consequently, neglecting these interactions undermines the robots ability to fully understand and adapt to the dynamics of manipulation scenarios. Therefore, in an attempt to address the aforementioned challenges, recent approaches have begun leveraging Reinforcement Learning (RL) paradigms [11][13], allowing robots to autonomously explore feasible motion strategies under the guidance of kinematic reference trajectories. These methods commonly design general reward functions encompassing object tracking, hand configurations, and collision dynamics. Maximizing such rewards drives the robot toward successful execution of complex manipulation tasks. Nonetheless, existing works [11][13] typically draw on limited human motion data sources and some have not transferred the trained robot behaviors to the physical world. Such limitations not only hinder the evaluation of whether the learned policies exhibit behaviorally plausible performance in the real world, but also preclude the integration of sim2real methodologies necessary for robust policy control and for deployment across various environmental conditions. Furthermore, current methods for sim2real transfer of bimanual dexterous manipulation predominantly rely on explicit extraction of object and robot state information [11], [14][16], thus failing to achieve end-toend visual learning. This limitation inherently confines learned policies to specific fixed setups, significantly hindering their adaptability to diverse scenarios. Motivated by these challenges, we propose HERMES, versatile human-to-robot embodied learning framework tailored for mobile bimanual dexterous hand manipulation. HERMES offers the following three advantages: Diverse sources of human motion: Our framework supports several human motion sources, including teleoperated simulation data, motion capture (mocap) data, and raw human videos. We also provide corresponding approaches for data acquisition, enabling HERMES to efficiently transform varied human motion data into robot-feasible behaviors through RL. Furthermore, these tasks share uniform set of reward terms, obviating the necessity of designing intricate and task-specific reward 2 Fig. 1. HERMES exhibits rich spectrum of mobile bimanual dexterous manipulation skills. The robot is able to navigate over extended distances in both indoor and outdoor environments, and effectively execute variety of complex manipulation tasks in unstructured, real-world scenarios, drawing upon behaviors learned from only one-shot human motion. functions. In contrast to the methods that depend on collecting large amount of demonstrations, we can achieve generalizable policy by augmenting single reference human motion trajectory coupling with RL training. End-to-end vision-based sim2real transfer: HERMES facilitates robust vision-based sim2real transfer by employing DAgger distillation, which convert"
[01.09.2025 05:13] Mistral response. {"id": "7c7da5195da240c1b34c9f579a617554", "created": 1756703594, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1285, "total_tokens": 1314, "completion_tokens": 29}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\n    \"Tsinghua University\",\n    \"Shanghai Qi Zhi Institute\",\n    \"Peking University\"\n]\n```"}}]}
[01.09.2025 05:13] Response: ```python
[
    "Tsinghua University",
    "Shanghai Qi Zhi Institute",
    "Peking University"
]
```
[01.09.2025 05:13] Deleting PDF ./assets/pdf/2508.20085.pdf.
[01.09.2025 05:13] Success.
[01.09.2025 05:13] Enriching papers with extra data.
[01.09.2025 05:13] ********************************************************************************
[01.09.2025 05:13] Abstract 0. A.S.E is a benchmark for evaluating the security of code generated by large language models using real-world repositories and expert-defined rules, revealing insights into model performance and decoding strategies.  					AI-generated summary 				 The increasing adoption of large language models (LLM...
[01.09.2025 05:13] ********************************************************************************
[01.09.2025 05:13] Abstract 1. EO-Robotics, comprising EO-1 model and EO-Data1.5M dataset, advances multimodal embodied reasoning and robot control through interleaved vision-text-action pre-training.  					AI-generated summary 				 The human ability to seamlessly perform multimodal reasoning and physical interaction in the open ...
[01.09.2025 05:13] ********************************************************************************
[01.09.2025 05:13] Abstract 2. R-4B, an auto-thinking multimodal large language model, uses bi-mode annealing and Bi-mode Policy Optimization to adaptively decide on problem-solving strategies, achieving state-of-the-art performance with lower computational cost.  					AI-generated summary 				 Multimodal Large Language Models (M...
[01.09.2025 05:13] ********************************************************************************
[01.09.2025 05:13] Abstract 3. Jina-code-embeddings uses an autoregressive backbone pre-trained on text and code to generate embeddings for code retrieval, question-answering, and similarity identification.  					AI-generated summary 				 jina-code-embeddings is a novel code embedding model suite designed to retrieve code from na...
[01.09.2025 05:13] ********************************************************************************
[01.09.2025 05:13] Abstract 4. Dynamic adjustment of data mixture based on Group Influence metric improves language model performance by adapting to evolving learning preferences.  					AI-generated summary 				 The data mixture used in the pre-training of a language model is a cornerstone of its final performance. However, a sta...
[01.09.2025 05:13] ********************************************************************************
[01.09.2025 05:13] Abstract 5. TalkVid, a large-scale, high-quality, and diverse dataset, improves audio-driven talking head synthesis by enhancing generalization across human diversity and revealing subgroup performance disparities.  					AI-generated summary 				 Audio-driven talking head synthesis has achieved remarkable photo...
[01.09.2025 05:13] ********************************************************************************
[01.09.2025 05:13] Abstract 6. UItron, an open-source foundational model for GUI agents, enhances visual understanding and task planning through advanced perception, grounding, and planning capabilities, achieving superior performance in Chinese app scenarios.  					AI-generated summary 				 GUI agent aims to enable automated ope...
[01.09.2025 05:13] ********************************************************************************
[01.09.2025 05:13] Abstract 7. Morae, a UI agent, enhances accessibility for BLV users by involving them in decision-making processes during task execution, using large multimodal models to interpret user queries and UI elements.  					AI-generated summary 				 User interface (UI) agents promise to make inaccessible or complex UI...
[01.09.2025 05:13] ********************************************************************************
[01.09.2025 05:13] Abstract 8. AHELM is a comprehensive benchmark for audio-language models that evaluates multiple aspects including fairness, safety, and reasoning across various datasets and models.  					AI-generated summary 				 Evaluations of audio-language models (ALMs) -- multimodal models that take interleaved audio and ...
[01.09.2025 05:13] ********************************************************************************
[01.09.2025 05:13] Abstract 9. Think in Games (TiG) framework enables large language models to develop procedural knowledge through interactive game environments, achieving competitive performance with reduced data and computational demands while providing transparent explanations.  					AI-generated summary 				 Large language m...
[01.09.2025 05:13] ********************************************************************************
[01.09.2025 05:13] Abstract 10. HERMES is a human-to-robot learning framework that translates human hand motions into robotic behaviors, using reinforcement learning and sim2real transfer for versatile manipulation in diverse environments.  					AI-generated summary 				 Leveraging human motion data to impart robots with versatile...
[01.09.2025 05:13] Read previous papers.
[01.09.2025 05:13] Generating reviews via LLM API.
[01.09.2025 05:13] Using data from previous issue: {"categories": ["#open_source", "#security", "#benchmark", "#dataset"], "emoji": "🛡️", "ru": {"title": "A.S.E: Новый стандарт оценки безопасности ИИ-генерируемого кода", "desc": "A.S.E - это новый бенчмарк для оценки безопасности кода, генерируемого большими языковыми моделями (LLM). Он использует р
[01.09.2025 05:13] Using data from previous issue: {"categories": ["#architecture", "#training", "#agents", "#multimodal", "#agi", "#reasoning", "#dataset"], "emoji": "🤖", "ru": {"title": "Революция в мультимодальном ИИ для роботов: EO-Robotics объединяет зрение, текст и действие", "desc": "EO-Robotics представляет собой систему, состоящую из модели
[01.09.2025 05:13] Using data from previous issue: {"categories": ["#training", "#multimodal", "#reasoning", "#benchmark", "#dataset", "#optimization"], "emoji": "🧠", "ru": {"title": "R-4B: Адаптивное мышление для эффективного решения задач", "desc": "R-4B - это мультимодальная большая языковая модель с возможностью автоматического мышления. Она исп
[01.09.2025 05:13] Querying the API.
[01.09.2025 05:13] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Jina-code-embeddings uses an autoregressive backbone pre-trained on text and code to generate embeddings for code retrieval, question-answering, and similarity identification.  					AI-generated summary 				 jina-code-embeddings is a novel code embedding model suite designed to retrieve code from natural language queries, perform technical question-answering, and identify semantically similar code snippets across programming languages. It makes innovative use of an autoregressive backbone pre-trained on both text and code, generating embeddings via last-token pooling. We outline the training recipe and demonstrate state-of-the-art performance despite the relatively small size of the models, validating this approach to code embedding model construction.
[01.09.2025 05:13] Response: {
  "desc": "Jina-code-embeddings - это новая модель встраивания кода, основанная на авторегрессионной архитектуре, предобученной на тексте и коде. Модель генерирует эмбеддинги для поиска кода, ответов на вопросы и определения семантически похожих фрагментов кода. Несмотря на относительно небольшой размер, модель демонстрирует передовые результаты в различных задачах, связанных с кодом. Авторы описывают методику обучения и валидируют эффективность данного подхода к созданию моделей встраивания кода.",
  "emoji": "🧠",
  "title": "Умные эмбеддинги для эффективной работы с кодом"
}
[01.09.2025 05:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Jina-code-embeddings uses an autoregressive backbone pre-trained on text and code to generate embeddings for code retrieval, question-answering, and similarity identification.  					AI-generated summary 				 jina-code-embeddings is a novel code embedding model suite designed to retrieve code from natural language queries, perform technical question-answering, and identify semantically similar code snippets across programming languages. It makes innovative use of an autoregressive backbone pre-trained on both text and code, generating embeddings via last-token pooling. We outline the training recipe and demonstrate state-of-the-art performance despite the relatively small size of the models, validating this approach to code embedding model construction."

[01.09.2025 05:13] Response: ```python
['DATASET', 'DATA', 'MULTILINGUAL', 'PLP', 'TRAINING', 'SMALL_MODELS']
```
[01.09.2025 05:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Jina-code-embeddings uses an autoregressive backbone pre-trained on text and code to generate embeddings for code retrieval, question-answering, and similarity identification.  					AI-generated summary 				 jina-code-embeddings is a novel code embedding model suite designed to retrieve code from natural language queries, perform technical question-answering, and identify semantically similar code snippets across programming languages. It makes innovative use of an autoregressive backbone pre-trained on both text and code, generating embeddings via last-token pooling. We outline the training recipe and demonstrate state-of-the-art performance despite the relatively small size of the models, validating this approach to code embedding model construction."

[01.09.2025 05:13] Response: ```python
["GAMES", "TRANSFER_LEARNING"]
```
[01.09.2025 05:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Jina-code-embeddings is a new model that creates embeddings for code, which helps in finding code based on natural language questions and identifying similar code snippets. It uses an autoregressive backbone that has been trained on both text and code, allowing it to understand and generate relevant embeddings effectively. The model employs a technique called last-token pooling to produce these embeddings, which enhances its performance. Despite being smaller in size compared to other models, it achieves state-of-the-art results in code retrieval and question-answering tasks.","title":"Revolutionizing Code Retrieval with Smart Embeddings"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Jina-code-embeddings is a new model that creates embeddings for code, which helps in finding code based on natural language questions and identifying similar code snippets. It uses an autoregressive backbone that has been trained on both text and code, allowing it to understand and generate relevant embeddings effectively. The model employs a technique called last-token pooling to produce these embeddings, which enhances its performance. Despite being smaller in size compared to other models, it achieves state-of-the-art results in code retrieval and question-answering tasks.', title='Revolutionizing Code Retrieval with Smart Embeddings'))
[01.09.2025 05:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Jina-code-embeddings 是一种新型的代码嵌入模型，旨在通过自然语言查询检索代码、进行技术问答以及识别不同编程语言中语义相似的代码片段。该模型创新性地使用了一个在文本和代码上预训练的自回归骨干网络，通过最后一个标记的池化生成嵌入。我们详细介绍了训练方法，并展示了尽管模型相对较小，但仍能实现最先进的性能。这验证了这种代码嵌入模型构建方法的有效性。","title":"创新代码嵌入模型，提升代码检索与问答能力"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Jina-code-embeddings 是一种新型的代码嵌入模型，旨在通过自然语言查询检索代码、进行技术问答以及识别不同编程语言中语义相似的代码片段。该模型创新性地使用了一个在文本和代码上预训练的自回归骨干网络，通过最后一个标记的池化生成嵌入。我们详细介绍了训练方法，并展示了尽管模型相对较小，但仍能实现最先进的性能。这验证了这种代码嵌入模型构建方法的有效性。', title='创新代码嵌入模型，提升代码检索与问答能力'))
[01.09.2025 05:13] Using data from previous issue: {"categories": ["#data", "#optimization", "#training"], "emoji": "🔄", "ru": {"title": "Динамическая оптимизация данных для эффективного обучения языковых моделей", "desc": "Статья представляет TiKMiX - метод динамической корректировки смеси данных для предобучения языковых моделей. Авторы вводят мет
[01.09.2025 05:13] Using data from previous issue: {"categories": ["#benchmark", "#open_source", "#ethics", "#transfer_learning", "#dataset", "#cv", "#data"], "emoji": "🗣️", "ru": {"title": "TalkVid: большой и разнообразный датасет для улучшения синтеза говорящих голов", "desc": "Представлен новый набор данных TalkVid для улучшения синтеза говорящих
[01.09.2025 05:13] Using data from previous issue: {"categories": ["#benchmark", "#rl", "#open_source", "#training", "#optimization", "#reasoning", "#agi", "#dataset", "#data", "#agents"], "emoji": "🤖", "ru": {"title": "UItron: ИИ-агент для автоматизации графических интерфейсов", "desc": "UItron - это модель машинного обучения с открытым исходным ко
[01.09.2025 05:13] Using data from previous issue: {"categories": ["#ethics", "#agi", "#multimodal", "#healthcare", "#agents"], "emoji": "👁️", "ru": {"title": "Morae: ИИ-ассистент, расширяющий возможности пользователей с нарушениями зрения", "desc": "Статья представляет Morae - агента пользовательского интерфейса, который улучшает доступность для по
[01.09.2025 05:13] Using data from previous issue: {"categories": ["#benchmark", "#open_source", "#ethics", "#reasoning", "#multimodal", "#audio", "#dataset"], "emoji": "🎧", "ru": {"title": "AHELM: Всесторонняя оценка аудио-языковых моделей", "desc": "AHELM - это комплексный бенчмарк для оценки аудио-языковых моделей (ALM). Он измеряет 10 аспектов, 
[01.09.2025 05:13] Using data from previous issue: {"categories": ["#training", "#games", "#optimization", "#reasoning", "#interpretability", "#multimodal", "#rl", "#rlhf"], "emoji": "🎮", "ru": {"title": "Обучение ИИ через игры: от знаний к умениям", "desc": "Предложена новая система Think in Games (TiG), позволяющая большим языковым моделям развива
[01.09.2025 05:13] Querying the API.
[01.09.2025 05:13] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

HERMES is a human-to-robot learning framework that translates human hand motions into robotic behaviors, using reinforcement learning and sim2real transfer for versatile manipulation in diverse environments.  					AI-generated summary 				 Leveraging human motion data to impart robots with versatile manipulation skills has emerged as a promising paradigm in robotic manipulation. Nevertheless, translating multi-source human hand motions into feasible robot behaviors remains challenging, particularly for robots equipped with multi-fingered dexterous hands characterized by complex, high-dimensional action spaces. Moreover, existing approaches often struggle to produce policies capable of adapting to diverse environmental conditions. In this paper, we introduce HERMES, a human-to-robot learning framework for mobile bimanual dexterous manipulation. First, HERMES formulates a unified reinforcement learning approach capable of seamlessly transforming heterogeneous human hand motions from multiple sources into physically plausible robotic behaviors. Subsequently, to mitigate the sim2real gap, we devise an end-to-end, depth image-based sim2real transfer method for improved generalization to real-world scenarios. Furthermore, to enable autonomous operation in varied and unstructured environments, we augment the navigation foundation model with a closed-loop Perspective-n-Point (PnP) localization mechanism, ensuring precise alignment of visual goals and effectively bridging autonomous navigation and dexterous manipulation. Extensive experimental results demonstrate that HERMES consistently exhibits generalizable behaviors across diverse, in-the-wild scenarios, successfully performing numerous complex mobile bimanual dexterous manipulation tasks. Project Page:https://gemcollector.github.io/HERMES/.
[01.09.2025 05:13] Response: {
  "desc": "HERMES - это система обучения роботов на основе данных о движениях человеческих рук. Она использует обучение с подкреплением и перенос из симуляции в реальность для создания универсальных манипуляционных навыков. HERMES способна адаптировать движения к различным условиям окружающей среды. Система включает в себя навигационную модель с механизмом локализации для автономной работы в неструктурированных средах.",
  "emoji": "🤖",
  "title": "От движений человека к умелым рукам робота"
}
[01.09.2025 05:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"HERMES is a human-to-robot learning framework that translates human hand motions into robotic behaviors, using reinforcement learning and sim2real transfer for versatile manipulation in diverse environments.  					AI-generated summary 				 Leveraging human motion data to impart robots with versatile manipulation skills has emerged as a promising paradigm in robotic manipulation. Nevertheless, translating multi-source human hand motions into feasible robot behaviors remains challenging, particularly for robots equipped with multi-fingered dexterous hands characterized by complex, high-dimensional action spaces. Moreover, existing approaches often struggle to produce policies capable of adapting to diverse environmental conditions. In this paper, we introduce HERMES, a human-to-robot learning framework for mobile bimanual dexterous manipulation. First, HERMES formulates a unified reinforcement learning approach capable of seamlessly transforming heterogeneous human hand motions from multiple sources into physically plausible robotic behaviors. Subsequently, to mitigate the sim2real gap, we devise an end-to-end, depth image-based sim2real transfer method for improved generalization to real-world scenarios. Furthermore, to enable autonomous operation in varied and unstructured environments, we augment the navigation foundation model with a closed-loop Perspective-n-Point (PnP) localization mechanism, ensuring precise alignment of visual goals and effectively bridging autonomous navigation and dexterous manipulation. Extensive experimental results demonstrate that HERMES consistently exhibits generalizable behaviors across diverse, in-the-wild scenarios, successfully performing numerous complex mobile bimanual dexterous manipulation tasks. Project Page:https://gemcollector.github.io/HERMES/."

[01.09.2025 05:13] Response: ```python
['AGENTS', 'RL', 'ROBOTICS']
```
[01.09.2025 05:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"HERMES is a human-to-robot learning framework that translates human hand motions into robotic behaviors, using reinforcement learning and sim2real transfer for versatile manipulation in diverse environments.  					AI-generated summary 				 Leveraging human motion data to impart robots with versatile manipulation skills has emerged as a promising paradigm in robotic manipulation. Nevertheless, translating multi-source human hand motions into feasible robot behaviors remains challenging, particularly for robots equipped with multi-fingered dexterous hands characterized by complex, high-dimensional action spaces. Moreover, existing approaches often struggle to produce policies capable of adapting to diverse environmental conditions. In this paper, we introduce HERMES, a human-to-robot learning framework for mobile bimanual dexterous manipulation. First, HERMES formulates a unified reinforcement learning approach capable of seamlessly transforming heterogeneous human hand motions from multiple sources into physically plausible robotic behaviors. Subsequently, to mitigate the sim2real gap, we devise an end-to-end, depth image-based sim2real transfer method for improved generalization to real-world scenarios. Furthermore, to enable autonomous operation in varied and unstructured environments, we augment the navigation foundation model with a closed-loop Perspective-n-Point (PnP) localization mechanism, ensuring precise alignment of visual goals and effectively bridging autonomous navigation and dexterous manipulation. Extensive experimental results demonstrate that HERMES consistently exhibits generalizable behaviors across diverse, in-the-wild scenarios, successfully performing numerous complex mobile bimanual dexterous manipulation tasks. Project Page:https://gemcollector.github.io/HERMES/."

[01.09.2025 05:13] Response: ```python
["TRANSFER_LEARNING", "OPTIMIZATION"]
```
[01.09.2025 05:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"HERMES is a framework that helps robots learn to mimic human hand movements for better manipulation tasks. It uses reinforcement learning to convert various human motions into actions that robots can perform, even with complex hand structures. The framework also addresses the challenge of transferring learned behaviors from simulated environments to real-world applications. By incorporating advanced localization techniques, HERMES enables robots to operate effectively in diverse and unpredictable settings.","title":"Bridging Human Motion and Robotic Dexterity with HERMES"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='HERMES is a framework that helps robots learn to mimic human hand movements for better manipulation tasks. It uses reinforcement learning to convert various human motions into actions that robots can perform, even with complex hand structures. The framework also addresses the challenge of transferring learned behaviors from simulated environments to real-world applications. By incorporating advanced localization techniques, HERMES enables robots to operate effectively in diverse and unpredictable settings.', title='Bridging Human Motion and Robotic Dexterity with HERMES'))
[01.09.2025 05:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"HERMES是一个人机学习框架，旨在将人类手部动作转化为机器人行为。该框架利用强化学习和仿真到现实的转移技术，帮助机器人在多样化环境中进行灵活的操作。HERMES能够将来自多个来源的人类手部动作统一转化为可行的机器人行为，并通过深度图像实现更好的现实适应性。实验结果表明，HERMES在各种复杂的移动双手灵巧操作任务中表现出色，具有良好的泛化能力。","title":"HERMES：人机协作的灵巧操作新框架"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='HERMES是一个人机学习框架，旨在将人类手部动作转化为机器人行为。该框架利用强化学习和仿真到现实的转移技术，帮助机器人在多样化环境中进行灵活的操作。HERMES能够将来自多个来源的人类手部动作统一转化为可行的机器人行为，并通过深度图像实现更好的现实适应性。实验结果表明，HERMES在各种复杂的移动双手灵巧操作任务中表现出色，具有良好的泛化能力。', title='HERMES：人机协作的灵巧操作新框架'))
[01.09.2025 05:13] Renaming data file.
[01.09.2025 05:13] Renaming previous data. hf_papers.json to ./d/2025-09-01.json
[01.09.2025 05:13] Saving new data file.
[01.09.2025 05:13] Generating page.
[01.09.2025 05:13] Renaming previous page.
[01.09.2025 05:13] Renaming previous data. index.html to ./d/2025-09-01.html
[01.09.2025 05:13] Writing result.
[01.09.2025 05:13] Renaming log file.
[01.09.2025 05:13] Renaming previous data. log.txt to ./logs/2025-09-01_last_log.txt
