[17.07.2025 12:23] Read previous papers.
[17.07.2025 12:23] Generating top page (month).
[17.07.2025 12:23] Writing top page (month).
[17.07.2025 13:31] Read previous papers.
[17.07.2025 13:31] Get feed.
[17.07.2025 13:31] Get page data from previous paper. URL: https://huggingface.co/papers/2507.09477
[17.07.2025 13:31] Get page data from previous paper. URL: https://huggingface.co/papers/2507.12465
[17.07.2025 13:31] Get page data from previous paper. URL: https://huggingface.co/papers/2507.11949
[17.07.2025 13:31] Get page data from previous paper. URL: https://huggingface.co/papers/2507.12463
[17.07.2025 13:31] Get page data from previous paper. URL: https://huggingface.co/papers/2507.12415
[17.07.2025 13:31] Get page data from previous paper. URL: https://huggingface.co/papers/2507.11527
[17.07.2025 13:31] Extract page data from URL. URL: https://huggingface.co/papers/2507.11412
[17.07.2025 13:31] Get page data from previous paper. URL: https://huggingface.co/papers/2507.12462
[17.07.2025 13:31] Get page data from previous paper. URL: https://huggingface.co/papers/2507.02857
[17.07.2025 13:31] Extract page data from URL. URL: https://huggingface.co/papers/2507.11764
[17.07.2025 13:31] Get page data from previous paper. URL: https://huggingface.co/papers/2507.09025
[17.07.2025 13:31] Get page data from previous paper. URL: https://huggingface.co/papers/2507.07451
[17.07.2025 13:31] Get page data from previous paper. URL: https://huggingface.co/papers/2507.05065
[17.07.2025 13:31] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[17.07.2025 13:31] No deleted papers detected.
[17.07.2025 13:31] Downloading and parsing papers (pdf, html). Total: 13.
[17.07.2025 13:31] Downloading and parsing paper https://huggingface.co/papers/2507.09477.
[17.07.2025 13:31] Extra JSON file exists (./assets/json/2507.09477.json), skip PDF parsing.
[17.07.2025 13:31] Paper image links file exists (./assets/img_data/2507.09477.json), skip HTML parsing.
[17.07.2025 13:31] Success.
[17.07.2025 13:31] Downloading and parsing paper https://huggingface.co/papers/2507.12465.
[17.07.2025 13:31] Extra JSON file exists (./assets/json/2507.12465.json), skip PDF parsing.
[17.07.2025 13:31] Paper image links file exists (./assets/img_data/2507.12465.json), skip HTML parsing.
[17.07.2025 13:31] Success.
[17.07.2025 13:31] Downloading and parsing paper https://huggingface.co/papers/2507.11949.
[17.07.2025 13:31] Extra JSON file exists (./assets/json/2507.11949.json), skip PDF parsing.
[17.07.2025 13:31] Paper image links file exists (./assets/img_data/2507.11949.json), skip HTML parsing.
[17.07.2025 13:31] Success.
[17.07.2025 13:31] Downloading and parsing paper https://huggingface.co/papers/2507.12463.
[17.07.2025 13:31] Extra JSON file exists (./assets/json/2507.12463.json), skip PDF parsing.
[17.07.2025 13:31] Paper image links file exists (./assets/img_data/2507.12463.json), skip HTML parsing.
[17.07.2025 13:31] Success.
[17.07.2025 13:31] Downloading and parsing paper https://huggingface.co/papers/2507.12415.
[17.07.2025 13:31] Extra JSON file exists (./assets/json/2507.12415.json), skip PDF parsing.
[17.07.2025 13:31] Paper image links file exists (./assets/img_data/2507.12415.json), skip HTML parsing.
[17.07.2025 13:31] Success.
[17.07.2025 13:31] Downloading and parsing paper https://huggingface.co/papers/2507.11527.
[17.07.2025 13:31] Extra JSON file exists (./assets/json/2507.11527.json), skip PDF parsing.
[17.07.2025 13:31] Paper image links file exists (./assets/img_data/2507.11527.json), skip HTML parsing.
[17.07.2025 13:31] Success.
[17.07.2025 13:31] Downloading and parsing paper https://huggingface.co/papers/2507.11412.
[17.07.2025 13:31] Downloading paper 2507.11412 from http://arxiv.org/pdf/2507.11412v1...
[17.07.2025 13:31] Extracting affiliations from text.
[17.07.2025 13:31] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Seq vs Seq: An Open Suite of Paired Encoders and Decoders Orion Weller Î¹ Kathryn Ricci Î¹ Marc Marone Î¹ Antoine Chaffin Î± Dawn Lawrie Î¹ Benjamin Van Durme Î¹ 5 2 0 2 J 5 1 ] . [ 1 2 1 4 1 1 . 7 0 5 2 : r Î¹ Johns Hopkins University Î± LightOn "
[17.07.2025 13:31] Response: ```python
["Johns Hopkins University", "LightOn"]
```
[17.07.2025 13:31] Deleting PDF ./assets/pdf/2507.11412.pdf.
[17.07.2025 13:31] Success.
[17.07.2025 13:31] Downloading and parsing paper https://huggingface.co/papers/2507.12462.
[17.07.2025 13:31] Extra JSON file exists (./assets/json/2507.12462.json), skip PDF parsing.
[17.07.2025 13:31] Paper image links file exists (./assets/img_data/2507.12462.json), skip HTML parsing.
[17.07.2025 13:31] Success.
[17.07.2025 13:31] Downloading and parsing paper https://huggingface.co/papers/2507.02857.
[17.07.2025 13:31] Extra JSON file exists (./assets/json/2507.02857.json), skip PDF parsing.
[17.07.2025 13:31] Paper image links file exists (./assets/img_data/2507.02857.json), skip HTML parsing.
[17.07.2025 13:31] Success.
[17.07.2025 13:31] Downloading and parsing paper https://huggingface.co/papers/2507.11764.
[17.07.2025 13:31] Downloading paper 2507.11764 from http://arxiv.org/pdf/2507.11764v1...
[17.07.2025 13:31] Extracting affiliations from text.
[17.07.2025 13:31] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"AI Wizards at CheckThat! 2025: Enhancing Transformer-Based Embeddings with Sentiment for Subjectivity Detection in News Articles Notebook for the CheckThat! Lab at CLEF 2025 Matteo Fasulo1,*,, Luca Babboni1, and Luca Tedeschini1, 1Department of Computer Science and Engineering (DISI) - University of Bologna Abstract This paper presents AI Wizards participation in the CLEF 2025 CheckThat! Lab Task 1: Subjectivity Detection in News Articles, classifying sentences as subjective/objective in monolingual, multilingual, and zero-shot settings. Training/development datasets were provided for Arabic, German, English, Italian, and Bulgarian; final evaluation included additional unseen languages (e.g., Greek, Romanian, Polish, Ukrainian) to assess generalization. Our primary strategy enhanced transformer-based classifiers by integrating sentiment scores, derived from an auxiliary model, with sentence representations, aiming to improve upon standard fine-tuning. We explored this sentimentaugmented architecture with mDeBERTaV3-base, ModernBERT-base (English), and Llama3.2-1B. To address class imbalance, prevalent across languages, we employed decision threshold calibration optimized on the development set. Our experiments show sentiment feature integration significantly boosts performance, especially subjective F1 score. This framework led to high rankings, notably 1st for Greek (Macro F1 = 0.51). Keywords subjectivity detection, transformers, multilinguality, sentiment-based features, threshold calibration 1. Introduction Our work addresses subjectivity detection as defined in Task 1 [1] of the CLEF 2025 CheckThat! Lab. An overview of the CheckThat! Lab and its constituent tasks can be found in [2, 3]. Specifically, Task 1 challenges systems to classify sentences from news articles as subjective (SUBJ) or objective (OBJ). This capability is vital for efforts to combat misinformation and improve fact-checking, as the ability to separate opinion from factual claims is essential,"
[17.07.2025 13:31] Response: ```python
["Department of Computer Science and Engineering (DISI) - University of Bologna"]
```
[17.07.2025 13:31] Deleting PDF ./assets/pdf/2507.11764.pdf.
[17.07.2025 13:31] Success.
[17.07.2025 13:31] Downloading and parsing paper https://huggingface.co/papers/2507.09025.
[17.07.2025 13:31] Extra JSON file exists (./assets/json/2507.09025.json), skip PDF parsing.
[17.07.2025 13:31] Paper image links file exists (./assets/img_data/2507.09025.json), skip HTML parsing.
[17.07.2025 13:31] Success.
[17.07.2025 13:31] Downloading and parsing paper https://huggingface.co/papers/2507.07451.
[17.07.2025 13:31] Extra JSON file exists (./assets/json/2507.07451.json), skip PDF parsing.
[17.07.2025 13:31] Paper image links file exists (./assets/img_data/2507.07451.json), skip HTML parsing.
[17.07.2025 13:31] Success.
[17.07.2025 13:31] Downloading and parsing paper https://huggingface.co/papers/2507.05065.
[17.07.2025 13:31] Extra JSON file exists (./assets/json/2507.05065.json), skip PDF parsing.
[17.07.2025 13:31] Paper image links file exists (./assets/img_data/2507.05065.json), skip HTML parsing.
[17.07.2025 13:31] Success.
[17.07.2025 13:31] Enriching papers with extra data.
[17.07.2025 13:31] ********************************************************************************
[17.07.2025 13:31] Abstract 0. This survey integrates reasoning and retrieval in Large Language Models to improve factuality and multi-step inference, highlighting Synergized RAG-Reasoning frameworks and outlining future research directions.  					AI-generated summary 				 Retrieval-Augmented Generation (RAG) lifts the factuality...
[17.07.2025 13:31] ********************************************************************************
[17.07.2025 13:31] Abstract 1. PhysX addresses the lack of physical properties in 3D generative models by introducing PhysXNet, a physics-annotated dataset, and PhysXGen, a framework that integrates physical knowledge into 3D asset generation.  					AI-generated summary 				 3D modeling is moving from virtual to physical. Existin...
[17.07.2025 13:31] ********************************************************************************
[17.07.2025 13:31] Abstract 2. A diffusion-based generative framework, MOSPA, is introduced to model human motion in response to spatial audio, achieving state-of-the-art performance using the newly created SAM dataset.  					AI-generated summary 				 Enabling virtual humans to dynamically and realistically respond to diverse aud...
[17.07.2025 13:31] ********************************************************************************
[17.07.2025 13:31] Abstract 3. A large-scale benchmark, MMHU, is proposed for human behavior analysis in autonomous driving, featuring rich annotations and diverse data sources, and benchmarking multiple tasks including motion prediction and behavior question answering.  					AI-generated summary 				 Humans are integral componen...
[17.07.2025 13:31] ********************************************************************************
[17.07.2025 13:31] Abstract 4. SWE-Perf is a benchmark for evaluating Large Language Models in code performance optimization using real-world repository data.  					AI-generated summary 				 Code performance optimization is paramount in real-world software engineering and critical for production-level systems. While Large Languag...
[17.07.2025 13:31] ********************************************************************************
[17.07.2025 13:31] Abstract 5. DrafterBench is an open-source benchmark for evaluating LLM agents in technical drawing revision, assessing their capabilities in structured data comprehension, function execution, instruction following, and critical reasoning.  					AI-generated summary 				 Large Language Model (LLM) agents have s...
[17.07.2025 13:31] ********************************************************************************
[17.07.2025 13:31] Abstract 6. The large language model (LLM) community focuses almost exclusively on decoder-only language models, since they are easier to use for text generation. However, a large subset of the community still uses encoder-only models for tasks such as classification or retrieval. Previous work has attempted to...
[17.07.2025 13:31] ********************************************************************************
[17.07.2025 13:31] Abstract 7. SpatialTrackerV2 is a feed-forward 3D point tracking method for monocular videos that integrates point tracking, monocular depth, and camera pose estimation into a unified, end-to-end architecture, achieving high performance and speed.  					AI-generated summary 				 We present SpatialTrackerV2, a f...
[17.07.2025 13:31] ********************************************************************************
[17.07.2025 13:31] Abstract 8. AnyI2V is a training-free framework that animates conditional images with user-defined motion trajectories, supporting various data types and enabling flexible video generation.  					AI-generated summary 				 Recent advancements in video generation, particularly in diffusion models, have driven not...
[17.07.2025 13:31] ********************************************************************************
[17.07.2025 13:31] Abstract 9. Sentiment-augmented transformer-based classifiers improve subjectivity detection in multilingual and zero-shot settings, achieving high performance and ranking first for Greek.  					AI-generated summary 				 This paper presents AI Wizards' participation in the CLEF 2025 CheckThat! Lab Task 1: Subje...
[17.07.2025 13:31] ********************************************************************************
[17.07.2025 13:31] Abstract 10. Lizard is a linearization framework that transforms Transformer-based LLMs into subquadratic architectures for efficient infinite-context generation, using a hybrid attention mechanism and hardware-aware training.  					AI-generated summary 				 We propose Lizard, a linearization framework that tran...
[17.07.2025 13:31] ********************************************************************************
[17.07.2025 13:31] Abstract 11. RLEP, a reinforcement learning framework with experience replay, enhances large language model training by focusing on high-quality examples, leading to faster convergence and improved performance on math-related benchmarks.  					AI-generated summary 				 Reinforcement learning (RL) for large langu...
[17.07.2025 13:31] ********************************************************************************
[17.07.2025 13:31] Abstract 12. A new approach formats tokens as a multi-turn interaction trace with a stateful tool for training Large Language Models, enabling faster sampling and denser reward signals for tasks like repairing Python code.  					AI-generated summary 				 Recent advances have established a new machine learning pa...
[17.07.2025 13:31] Read previous papers.
[17.07.2025 13:31] Generating reviews via LLM API.
[17.07.2025 13:31] Using data from previous issue: {"categories": ["#reasoning", "#multimodal", "#benchmark", "#survey", "#rag"], "emoji": "ğŸ§ ", "ru": {"title": "ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¼Ğ¾Ñ‰Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹", "desc": "Ğ­Ñ‚Ğ¾ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LL
[17.07.2025 13:31] Using data from previous issue: {"categories": ["#architecture", "#synthetic", "#games", "#3d", "#open_source", "#dataset"], "emoji": "ğŸ§±", "ru": {"title": "Ğ¤Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ´Ğ¾ÑÑ‚Ğ¾Ğ²ĞµÑ€Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ²", "desc": "PhysX Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¸Ñ… Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Phys
[17.07.2025 13:31] Using data from previous issue: {"categories": ["#multimodal", "#diffusion", "#benchmark", "#open_source", "#dataset"], "emoji": "ğŸ§", "ru": {"title": "Ğ ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ°Ñ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾Ğ´ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ°ÑƒĞ´Ğ¸Ğ¾", "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ MOSPA - Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ² Ğ¾
[17.07.2025 13:31] Using data from previous issue: {"categories": ["#benchmark", "#agents", "#dataset"], "emoji": "ğŸš—", "ru": {"title": "MMHU: ĞšĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ´Ğ»Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾Ğ³Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ", "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº MMHU Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ³
[17.07.2025 13:31] Using data from previous issue: {"categories": ["#optimization", "#benchmark", "#dataset"], "emoji": "ğŸš€", "ru": {"title": "SWE-Perf: ĞĞ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ LLM", "desc": "SWE-Perf - ÑÑ‚Ğ¾ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ¾Ğ´Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… 
[17.07.2025 13:31] Using data from previous issue: {"categories": ["#reasoning", "#benchmark", "#open_source", "#long_context", "#agents"], "emoji": "ğŸ“", "ru": {"title": "DrafterBench: ĞšĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸", "desc": "DrafterBench - ÑÑ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM)
[17.07.2025 13:31] Querying the API.
[17.07.2025 13:31] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

The large language model (LLM) community focuses almost exclusively on decoder-only language models, since they are easier to use for text generation. However, a large subset of the community still uses encoder-only models for tasks such as classification or retrieval. Previous work has attempted to compare these architectures, but is forced to make comparisons with models that have different numbers of parameters, training techniques, and datasets. We introduce the SOTA open-data Ettin suite of models: paired encoder-only and decoder-only models ranging from 17 million parameters to 1 billion, trained on up to 2 trillion tokens. Using the same recipe for both encoder-only and decoder-only models produces SOTA recipes in both categories for their respective sizes, beating ModernBERT as an encoder and Llama 3.2 and SmolLM2 as decoders. Like previous work, we find that encoder-only models excel at classification and retrieval tasks while decoders excel at generative tasks. However, we show that adapting a decoder model to encoder tasks (and vice versa) through continued training is subpar compared to using only the reverse objective (i.e. a 400M encoder outperforms a 1B decoder on MNLI, and vice versa for generative tasks). We open-source all artifacts of this study including training data, training order segmented by checkpoint, and 200+ checkpoints to allow future work to analyze or extend all aspects of training.
[17.07.2025 13:32] Response: {
  "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ettin, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€-Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¸ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€-Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ¾Ñ‚ 17 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ´Ğ¾ 1 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ° Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ñ‹ Ğ½Ğ° 2 Ñ‚Ñ€Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸ Ğ² ÑĞ²Ğ¾Ğ¸Ñ… ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸ÑÑ…. ĞŸĞ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ñ‹ Ğ»ÑƒÑ‡ÑˆĞµ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ°, Ğ° Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€Ñ‹ - Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸. ĞŸĞ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€Ğ° Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ° (Ğ¸ Ğ½Ğ°Ğ¾Ğ±Ğ¾Ñ€Ğ¾Ñ‚) Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑƒÑÑ‚ÑƒĞ¿Ğ°ĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹.",
  "emoji": "ğŸ¤–",
  "title": "Ettin: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ´Ğ»Ñ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ² Ğ¸ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ² Ğ² Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸"
}
[17.07.2025 13:32] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"The large language model (LLM) community focuses almost exclusively on decoder-only language models, since they are easier to use for text generation. However, a large subset of the community still uses encoder-only models for tasks such as classification or retrieval. Previous work has attempted to compare these architectures, but is forced to make comparisons with models that have different numbers of parameters, training techniques, and datasets. We introduce the SOTA open-data Ettin suite of models: paired encoder-only and decoder-only models ranging from 17 million parameters to 1 billion, trained on up to 2 trillion tokens. Using the same recipe for both encoder-only and decoder-only models produces SOTA recipes in both categories for their respective sizes, beating ModernBERT as an encoder and Llama 3.2 and SmolLM2 as decoders. Like previous work, we find that encoder-only models excel at classification and retrieval tasks while decoders excel at generative tasks. However, we show that adapting a decoder model to encoder tasks (and vice versa) through continued training is subpar compared to using only the reverse objective (i.e. a 400M encoder outperforms a 1B decoder on MNLI, and vice versa for generative tasks). We open-source all artifacts of this study including training data, training order segmented by checkpoint, and 200+ checkpoints to allow future work to analyze or extend all aspects of training."

[17.07.2025 13:32] Response: ```python
['DATASET', 'TRAINING', 'ARCHITECTURE']
```
[17.07.2025 13:32] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"The large language model (LLM) community focuses almost exclusively on decoder-only language models, since they are easier to use for text generation. However, a large subset of the community still uses encoder-only models for tasks such as classification or retrieval. Previous work has attempted to compare these architectures, but is forced to make comparisons with models that have different numbers of parameters, training techniques, and datasets. We introduce the SOTA open-data Ettin suite of models: paired encoder-only and decoder-only models ranging from 17 million parameters to 1 billion, trained on up to 2 trillion tokens. Using the same recipe for both encoder-only and decoder-only models produces SOTA recipes in both categories for their respective sizes, beating ModernBERT as an encoder and Llama 3.2 and SmolLM2 as decoders. Like previous work, we find that encoder-only models excel at classification and retrieval tasks while decoders excel at generative tasks. However, we show that adapting a decoder model to encoder tasks (and vice versa) through continued training is subpar compared to using only the reverse objective (i.e. a 400M encoder outperforms a 1B decoder on MNLI, and vice versa for generative tasks). We open-source all artifacts of this study including training data, training order segmented by checkpoint, and 200+ checkpoints to allow future work to analyze or extend all aspects of training."

[17.07.2025 13:32] Response: ```python
['OPEN_SOURCE', 'OPTIMIZATION']
```
[17.07.2025 13:32] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper discusses the comparison between encoder-only and decoder-only language models in the context of machine learning. It introduces the Ettin suite, which consists of paired models of both types, ensuring they are trained under the same conditions for a fair comparison. The findings reveal that encoder-only models are better suited for classification and retrieval tasks, while decoder-only models excel in text generation. Additionally, the study shows that adapting models for different tasks through continued training is less effective than using models specifically designed for those tasks.","title":"Unlocking the Power of Encoder and Decoder Models in NLP"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper discusses the comparison between encoder-only and decoder-only language models in the context of machine learning. It introduces the Ettin suite, which consists of paired models of both types, ensuring they are trained under the same conditions for a fair comparison. The findings reveal that encoder-only models are better suited for classification and retrieval tasks, while decoder-only models excel in text generation. Additionally, the study shows that adapting models for different tasks through continued training is less effective than using models specifically designed for those tasks.', title='Unlocking the Power of Encoder and Decoder Models in NLP'))
[17.07.2025 13:32] Response: ParsedChatCompletionMessage[Article](content='{"desc":"æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æ¨¡å‹å¥—ä»¶Ettinï¼ŒåŒ…å«é…å¯¹çš„ç¼–ç å™¨å’Œè§£ç å™¨æ¨¡å‹ï¼Œå‚æ•°èŒƒå›´ä»1700ä¸‡åˆ°10äº¿ï¼Œè®­ç»ƒæ•°æ®è¾¾åˆ°2ä¸‡äº¿ä¸ªæ ‡è®°ã€‚ç ”ç©¶è¡¨æ˜ï¼Œç¼–ç å™¨æ¨¡å‹åœ¨åˆ†ç±»å’Œæ£€ç´¢ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè€Œè§£ç å™¨æ¨¡å‹åœ¨ç”Ÿæˆä»»åŠ¡ä¸­æ›´å…·ä¼˜åŠ¿ã€‚é€šè¿‡ç›¸åŒçš„è®­ç»ƒæ–¹æ³•ï¼ŒEttinæ¨¡å‹åœ¨å„è‡ªçš„ç±»åˆ«ä¸­è¾¾åˆ°äº†æœ€æ–°çš„æ€§èƒ½ï¼Œè¶…è¶Šäº†ç°ä»£çš„BERTå’ŒLlama 3.2ç­‰æ¨¡å‹ã€‚æˆ‘ä»¬è¿˜å‘ç°ï¼Œç»§ç»­è®­ç»ƒè§£ç å™¨æ¨¡å‹ä»¥é€‚åº”ç¼–ç å™¨ä»»åŠ¡çš„æ•ˆæœä¸å¦‚ç›´æ¥ä½¿ç”¨ç¼–ç å™¨æ¨¡å‹ã€‚","title":"Ettinæ¨¡å‹ï¼šç¼–ç å™¨ä¸è§£ç å™¨çš„å®Œç¾ç»“åˆ"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æ¨¡å‹å¥—ä»¶Ettinï¼ŒåŒ…å«é…å¯¹çš„ç¼–ç å™¨å’Œè§£ç å™¨æ¨¡å‹ï¼Œå‚æ•°èŒƒå›´ä»1700ä¸‡åˆ°10äº¿ï¼Œè®­ç»ƒæ•°æ®è¾¾åˆ°2ä¸‡äº¿ä¸ªæ ‡è®°ã€‚ç ”ç©¶è¡¨æ˜ï¼Œç¼–ç å™¨æ¨¡å‹åœ¨åˆ†ç±»å’Œæ£€ç´¢ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè€Œè§£ç å™¨æ¨¡å‹åœ¨ç”Ÿæˆä»»åŠ¡ä¸­æ›´å…·ä¼˜åŠ¿ã€‚é€šè¿‡ç›¸åŒçš„è®­ç»ƒæ–¹æ³•ï¼ŒEttinæ¨¡å‹åœ¨å„è‡ªçš„ç±»åˆ«ä¸­è¾¾åˆ°äº†æœ€æ–°çš„æ€§èƒ½ï¼Œè¶…è¶Šäº†ç°ä»£çš„BERTå’ŒLlama 3.2ç­‰æ¨¡å‹ã€‚æˆ‘ä»¬è¿˜å‘ç°ï¼Œç»§ç»­è®­ç»ƒè§£ç å™¨æ¨¡å‹ä»¥é€‚åº”ç¼–ç å™¨ä»»åŠ¡çš„æ•ˆæœä¸å¦‚ç›´æ¥ä½¿ç”¨ç¼–ç å™¨æ¨¡å‹ã€‚', title='Ettinæ¨¡å‹ï¼šç¼–ç å™¨ä¸è§£ç å™¨çš„å®Œç¾ç»“åˆ'))
[17.07.2025 13:32] Using data from previous issue: {"categories": ["#architecture", "#3d"], "emoji": "ğŸ¥", "ru": {"title": "Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ 3D-Ñ‚Ñ€ĞµĞºĞ¸Ğ½Ğ³Ğ° Ğ² Ğ¼Ğ¾Ğ½Ğ¾ĞºÑƒĞ»ÑÑ€Ğ½Ğ¾Ğ¼ Ğ²Ğ¸Ğ´ĞµĞ¾", "desc": "SpatialTrackerV2 - ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ 3D Ñ‚Ğ¾Ñ‡ĞµĞº Ğ² Ğ¼Ğ¾Ğ½Ğ¾ĞºÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ¹ ÑĞ²ÑĞ·ÑŒÑ. ĞĞ½ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡ĞµĞº, Ğ¼Ğ¾Ğ½Ğ¾ĞºÑƒĞ»ÑÑ€Ğ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ
[17.07.2025 13:32] Using data from previous issue: {"categories": ["#optimization", "#diffusion", "#multimodal", "#video"], "emoji": "ğŸ¬", "ru": {"title": "Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ", "desc": "AnyI2V - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğ¼Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ÑĞ¼Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ½
[17.07.2025 13:32] Querying the API.
[17.07.2025 13:32] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Sentiment-augmented transformer-based classifiers improve subjectivity detection in multilingual and zero-shot settings, achieving high performance and ranking first for Greek.  					AI-generated summary 				 This paper presents AI Wizards' participation in the CLEF 2025 CheckThat! Lab Task 1: Subjectivity Detection in News Articles, classifying sentences as subjective/objective in monolingual, multilingual, and zero-shot settings. Training/development datasets were provided for Arabic, German, English, Italian, and Bulgarian; final evaluation included additional unseen languages (e.g., Greek, Romanian, Polish, Ukrainian) to assess generalization. Our primary strategy enhanced transformer-based classifiers by integrating sentiment scores, derived from an auxiliary model, with sentence representations, aiming to improve upon standard fine-tuning. We explored this sentiment-augmented architecture with mDeBERTaV3-base, ModernBERT-base (English), and Llama3.2-1B. To address class imbalance, prevalent across languages, we employed decision threshold calibration optimized on the development set. Our experiments show sentiment feature integration significantly boosts performance, especially subjective F1 score. This framework led to high rankings, notably 1st for Greek (Macro F1 = 0.51).
[17.07.2025 13:32] Response: {
  "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ¸ zero-shot ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞµĞ½Ñ‚Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°, Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾Ñ‚ Ğ²ÑĞ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ² Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ F1-Ğ¼ĞµÑ€Ñƒ Ğ´Ğ»Ñ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ»Ğ°ÑÑĞ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ» Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² ÑĞ¾Ñ€ĞµĞ²Ğ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ CLEF 2025 CheckThat!, Ğ·Ğ°Ğ½ÑĞ² Ğ¿ĞµÑ€Ğ²Ğ¾Ğµ Ğ¼ĞµÑÑ‚Ğ¾ Ğ´Ğ»Ñ Ğ³Ñ€ĞµÑ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°.",
  "emoji": "ğŸŒ",
  "title": "Ğ¡ĞµĞ½Ñ‚Ğ¸Ğ¼ĞµĞ½Ñ‚-ÑƒÑĞ¸Ğ»ĞµĞ½Ğ½Ñ‹Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ñ‹ Ğ¿Ğ¾ĞºĞ¾Ñ€ÑÑÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ"
}
[17.07.2025 13:32] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Sentiment-augmented transformer-based classifiers improve subjectivity detection in multilingual and zero-shot settings, achieving high performance and ranking first for Greek.  					AI-generated summary 				 This paper presents AI Wizards' participation in the CLEF 2025 CheckThat! Lab Task 1: Subjectivity Detection in News Articles, classifying sentences as subjective/objective in monolingual, multilingual, and zero-shot settings. Training/development datasets were provided for Arabic, German, English, Italian, and Bulgarian; final evaluation included additional unseen languages (e.g., Greek, Romanian, Polish, Ukrainian) to assess generalization. Our primary strategy enhanced transformer-based classifiers by integrating sentiment scores, derived from an auxiliary model, with sentence representations, aiming to improve upon standard fine-tuning. We explored this sentiment-augmented architecture with mDeBERTaV3-base, ModernBERT-base (English), and Llama3.2-1B. To address class imbalance, prevalent across languages, we employed decision threshold calibration optimized on the development set. Our experiments show sentiment feature integration significantly boosts performance, especially subjective F1 score. This framework led to high rankings, notably 1st for Greek (Macro F1 = 0.51)."

[17.07.2025 13:32] Response: ```python
['MULTILINGUAL', 'TRAINING', 'ARCHITECTURE']
```
[17.07.2025 13:32] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Sentiment-augmented transformer-based classifiers improve subjectivity detection in multilingual and zero-shot settings, achieving high performance and ranking first for Greek.  					AI-generated summary 				 This paper presents AI Wizards' participation in the CLEF 2025 CheckThat! Lab Task 1: Subjectivity Detection in News Articles, classifying sentences as subjective/objective in monolingual, multilingual, and zero-shot settings. Training/development datasets were provided for Arabic, German, English, Italian, and Bulgarian; final evaluation included additional unseen languages (e.g., Greek, Romanian, Polish, Ukrainian) to assess generalization. Our primary strategy enhanced transformer-based classifiers by integrating sentiment scores, derived from an auxiliary model, with sentence representations, aiming to improve upon standard fine-tuning. We explored this sentiment-augmented architecture with mDeBERTaV3-base, ModernBERT-base (English), and Llama3.2-1B. To address class imbalance, prevalent across languages, we employed decision threshold calibration optimized on the development set. Our experiments show sentiment feature integration significantly boosts performance, especially subjective F1 score. This framework led to high rankings, notably 1st for Greek (Macro F1 = 0.51)."

[17.07.2025 13:32] Response: ```python
['TRANSLATION', 'LOW_RESOURCE']
```
[17.07.2025 13:32] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper discusses a method to improve the detection of subjective and objective sentences in news articles using advanced transformer-based classifiers. The authors enhanced these classifiers by incorporating sentiment scores from an auxiliary model, which helped in better understanding the context of sentences. They tested their approach on multiple languages, including unseen ones, to ensure the model\'s ability to generalize. The results showed that this sentiment-augmented method significantly improved performance, particularly in identifying subjective content, achieving top rankings for Greek.","title":"Boosting Subjectivity Detection with Sentiment-Enhanced Transformers"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="This paper discusses a method to improve the detection of subjective and objective sentences in news articles using advanced transformer-based classifiers. The authors enhanced these classifiers by incorporating sentiment scores from an auxiliary model, which helped in better understanding the context of sentences. They tested their approach on multiple languages, including unseen ones, to ensure the model's ability to generalize. The results showed that this sentiment-augmented method significantly improved performance, particularly in identifying subjective content, achieving top rankings for Greek.", title='Boosting Subjectivity Detection with Sentiment-Enhanced Transformers'))
[17.07.2025 13:32] Response: ParsedChatCompletionMessage[Article](content='{"desc":"æœ¬æ–‡ä»‹ç»äº†AI Wizardsåœ¨CLEF 2025 CheckThat! Lab Task 1ä¸­çš„å‚ä¸ï¼Œæ—¨åœ¨å¯¹æ–°é—»æ–‡ç« ä¸­çš„å¥å­è¿›è¡Œä¸»è§‚æ€§æ£€æµ‹ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§å¢å¼ºå‹çš„å˜æ¢å™¨åˆ†ç±»å™¨ï¼Œé€šè¿‡å°†æƒ…æ„Ÿåˆ†æ•°ä¸å¥å­è¡¨ç¤ºç»“åˆï¼Œæ¥æé«˜æ¨¡å‹åœ¨å•è¯­ã€å¤šè¯­å’Œé›¶æ ·æœ¬è®¾ç½®ä¸‹çš„è¡¨ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæƒ…æ„Ÿç‰¹å¾çš„æ•´åˆæ˜¾è‘—æå‡äº†ä¸»è§‚æ€§F1åˆ†æ•°ï¼Œå°¤å…¶æ˜¯åœ¨å¸Œè…Šè¯­ä¸­å–å¾—äº†ç¬¬ä¸€åçš„ä¼˜å¼‚æˆç»©ã€‚æˆ‘ä»¬è¿˜é€šè¿‡ä¼˜åŒ–å†³ç­–é˜ˆå€¼æ¥è§£å†³å„è¯­è¨€é—´çš„ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ã€‚","title":"æƒ…æ„Ÿå¢å¼ºå˜æ¢å™¨æå‡å¤šè¯­è¨€ä¸»è§‚æ€§æ£€æµ‹"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='æœ¬æ–‡ä»‹ç»äº†AI Wizardsåœ¨CLEF 2025 CheckThat! Lab Task 1ä¸­çš„å‚ä¸ï¼Œæ—¨åœ¨å¯¹æ–°é—»æ–‡ç« ä¸­çš„å¥å­è¿›è¡Œä¸»è§‚æ€§æ£€æµ‹ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§å¢å¼ºå‹çš„å˜æ¢å™¨åˆ†ç±»å™¨ï¼Œé€šè¿‡å°†æƒ…æ„Ÿåˆ†æ•°ä¸å¥å­è¡¨ç¤ºç»“åˆï¼Œæ¥æé«˜æ¨¡å‹åœ¨å•è¯­ã€å¤šè¯­å’Œé›¶æ ·æœ¬è®¾ç½®ä¸‹çš„è¡¨ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæƒ…æ„Ÿç‰¹å¾çš„æ•´åˆæ˜¾è‘—æå‡äº†ä¸»è§‚æ€§F1åˆ†æ•°ï¼Œå°¤å…¶æ˜¯åœ¨å¸Œè…Šè¯­ä¸­å–å¾—äº†ç¬¬ä¸€åçš„ä¼˜å¼‚æˆç»©ã€‚æˆ‘ä»¬è¿˜é€šè¿‡ä¼˜åŒ–å†³ç­–é˜ˆå€¼æ¥è§£å†³å„è¯­è¨€é—´çš„ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ã€‚', title='æƒ…æ„Ÿå¢å¼ºå˜æ¢å™¨æå‡å¤šè¯­è¨€ä¸»è§‚æ€§æ£€æµ‹'))
[17.07.2025 13:32] Using data from previous issue: {"categories": ["#benchmark", "#architecture", "#optimization", "#training", "#long_context"], "emoji": "ğŸ¦", "ru": {"title": "Lizard: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ±ĞµÑĞºĞ¾Ğ½ĞµÑ‡Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼", "desc": "Lizard - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ»Ğ¸Ğ½ĞµĞ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² ÑÑƒĞ±ĞºĞ²Ğ°Ğ´Ñ€Ğ°Ñ‚Ğ¸Ñ‡
[17.07.2025 13:32] Using data from previous issue: {"categories": ["#reasoning", "#open_source", "#optimization", "#dataset", "#rl", "#training"], "emoji": "ğŸ§ ", "ru": {"title": "RLEP: Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ¿Ñ‹Ñ‚Ğ°", "desc": "RLEP - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ğ½Ğ¾Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´Ğµ
[17.07.2025 13:32] Using data from previous issue: {"categories": ["#rl", "#optimization", "#plp", "#training", "#transfer_learning", "#benchmark"], "emoji": "ğŸ› ï¸", "ru": {"title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ LLM Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM), Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ñ€ÑƒÑ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ ĞºĞ°Ğº Ğ¼
[17.07.2025 13:32] Renaming data file.
[17.07.2025 13:32] Renaming previous data. hf_papers.json to ./d/2025-07-17.json
[17.07.2025 13:32] Saving new data file.
[17.07.2025 13:32] Generating page.
[17.07.2025 13:32] Renaming previous page.
[17.07.2025 13:32] Renaming previous data. index.html to ./d/2025-07-17.html
[17.07.2025 13:32] Writing result.
[17.07.2025 13:32] Renaming log file.
[17.07.2025 13:32] Renaming previous data. log.txt to ./logs/2025-07-17_last_log.txt
