[17.07.2025 05:18] Read previous papers.
[17.07.2025 05:18] Generating top page (month).
[17.07.2025 05:18] Writing top page (month).
[17.07.2025 06:18] Read previous papers.
[17.07.2025 06:18] Get feed.
[17.07.2025 06:18] Get page data from previous paper. URL: https://huggingface.co/papers/2507.12465
[17.07.2025 06:18] Get page data from previous paper. URL: https://huggingface.co/papers/2507.09477
[17.07.2025 06:18] Extract page data from URL. URL: https://huggingface.co/papers/2507.12463
[17.07.2025 06:18] Get page data from previous paper. URL: https://huggingface.co/papers/2507.11527
[17.07.2025 06:18] Get page data from previous paper. URL: https://huggingface.co/papers/2507.11949
[17.07.2025 06:18] Extract page data from URL. URL: https://huggingface.co/papers/2507.12415
[17.07.2025 06:18] Get page data from previous paper. URL: https://huggingface.co/papers/2507.09025
[17.07.2025 06:18] Extract page data from URL. URL: https://huggingface.co/papers/2507.02857
[17.07.2025 06:18] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[17.07.2025 06:18] No deleted papers detected.
[17.07.2025 06:18] Downloading and parsing papers (pdf, html). Total: 8.
[17.07.2025 06:18] Downloading and parsing paper https://huggingface.co/papers/2507.12465.
[17.07.2025 06:18] Extra JSON file exists (./assets/json/2507.12465.json), skip PDF parsing.
[17.07.2025 06:18] Paper image links file exists (./assets/img_data/2507.12465.json), skip HTML parsing.
[17.07.2025 06:18] Success.
[17.07.2025 06:18] Downloading and parsing paper https://huggingface.co/papers/2507.09477.
[17.07.2025 06:18] Extra JSON file exists (./assets/json/2507.09477.json), skip PDF parsing.
[17.07.2025 06:18] Paper image links file exists (./assets/img_data/2507.09477.json), skip HTML parsing.
[17.07.2025 06:18] Success.
[17.07.2025 06:18] Downloading and parsing paper https://huggingface.co/papers/2507.12463.
[17.07.2025 06:18] Downloading paper 2507.12463 from http://arxiv.org/pdf/2507.12463v1...
[17.07.2025 06:18] Extracting affiliations from text.
[17.07.2025 06:18] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"MMHU: Massive-Scale Multimodal Benchmark for Human Behavior Understanding Renjie Li1, Ruijie Ye2, Mingyang Wu1, Hao Frank Yang3, Zhiwen Fan4, Hezhen Hu4, Zhengzhong Tu1 1Texas A&M University 2Brown University 3Johns Hopkins University 4UT Austin 5 2 0 2 6 1 ] . [ 1 3 6 4 2 1 . 7 0 5 2 : r Project Page: https://MMHU-Benchmark.github.io Figure 1: We propose MMHU, large-scale dataset for human behavior understanding. We collected 57k human instances with diverse behaviors such as playing mobile phone, holding object, or using mobility devices, from diverse scenes such as in the city, school, park, and alley. We provide rich annotations including motion and trajectory, text descriptions for human motions, and recognize the behaviors that are critical to driving safety. "
[17.07.2025 06:18] Response: ```python
["Texas A&M University", "Brown University", "Johns Hopkins University", "UT Austin"]
```
[17.07.2025 06:18] Deleting PDF ./assets/pdf/2507.12463.pdf.
[17.07.2025 06:18] Success.
[17.07.2025 06:18] Downloading and parsing paper https://huggingface.co/papers/2507.11527.
[17.07.2025 06:18] Extra JSON file exists (./assets/json/2507.11527.json), skip PDF parsing.
[17.07.2025 06:18] Paper image links file exists (./assets/img_data/2507.11527.json), skip HTML parsing.
[17.07.2025 06:18] Success.
[17.07.2025 06:18] Downloading and parsing paper https://huggingface.co/papers/2507.11949.
[17.07.2025 06:18] Extra JSON file exists (./assets/json/2507.11949.json), skip PDF parsing.
[17.07.2025 06:18] Paper image links file exists (./assets/img_data/2507.11949.json), skip HTML parsing.
[17.07.2025 06:18] Success.
[17.07.2025 06:18] Downloading and parsing paper https://huggingface.co/papers/2507.12415.
[17.07.2025 06:18] Downloading paper 2507.12415 from http://arxiv.org/pdf/2507.12415v1...
[17.07.2025 06:18] Extracting affiliations from text.
[17.07.2025 06:18] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 6 1 ] . [ 1 5 1 4 2 1 . 7 0 5 2 : r SWE-Perf: Can Language Models Optimize Code Performance on Real-World Repositories? Xinyi He1, Qian Liu2, Mingzhe Du3, Lin Yan2, Zhijie Fan2, Yiming Huang4 Zejian Yuan1, Zejun Ma2 1Xian Jiaotong University, 2TikTok 3National University of Singapore, 4University of California San Diego Code performance optimization is paramount in real-world software engineering and critical for productionlevel systems. While Large Language Models (LLMs) have demonstrated impressive capabilities in code generation and bug fixing, their proficiency in enhancing code performance at the repository level remains largely unexplored. To address this gap, we introduce SWE-Perf, the first benchmark specifically designed to systematically evaluate LLMs on code performance optimization tasks within authentic repository contexts. SWE-Perf comprises 140 carefully curated instances, each derived from performance-improving pull requests from popular GitHub repositories. Each benchmark instance includes the relevant codebase, target functions, performance-related tests, expert-authored patches, and executable environments. Through comprehensive evaluation of representative methods that span file-level and repo-level approaches (e.g., Agentless and OpenHands), we reveal substantial capability gap between existing LLMs and expert-level optimization performance, highlighting critical research opportunities in this emerging field. Date: July 16, 2025 Home: https://swe-perf.github.io Correspondence: Qian Liu (qian.liu@tiktok.com) 1. Introduction Recent advances in Large Language Models (LLMs) have significantly enhanced automated code generation and software development assistance, exemplified by tools like GitHub Copilot (Microsoft, 2025) and Cursor (Cursor, 2025). This progress has spurred growing interest in repository-level software engineering challenges in real-world settings (Jimenez et al., 2024). Recent work has introduced multiple benchmarks for eval"
[17.07.2025 06:18] Response: ```python
[
    "Xian Jiaotong University",
    "TikTok",
    "National University of Singapore",
    "University of California San Diego"
]
```
[17.07.2025 06:18] Deleting PDF ./assets/pdf/2507.12415.pdf.
[17.07.2025 06:18] Success.
[17.07.2025 06:18] Downloading and parsing paper https://huggingface.co/papers/2507.09025.
[17.07.2025 06:18] Extra JSON file exists (./assets/json/2507.09025.json), skip PDF parsing.
[17.07.2025 06:18] Paper image links file exists (./assets/img_data/2507.09025.json), skip HTML parsing.
[17.07.2025 06:18] Success.
[17.07.2025 06:18] Downloading and parsing paper https://huggingface.co/papers/2507.02857.
[17.07.2025 06:18] Downloading paper 2507.02857 from http://arxiv.org/pdf/2507.02857v1...
[17.07.2025 06:18] Extracting affiliations from text.
[17.07.2025 06:18] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"AnyI2V: Animating Any Conditional Image with Motion Control Ziye Li1 Hao Luo2,3 Xincheng Shuai1 Henghui Ding1 (cid:0) 1Fudan University 2DAMO Academy, Alibaba group 3Hupan Lab https://henghuiding.com/AnyI2V/ 5 2 0 2 3 ] . [ 1 7 5 8 2 0 . 7 0 5 2 : r Figure 1. The first frame conditional control of our Training-Free architecture AnyI2V. (a) AnyI2V supports diverse types of conditional inputs, including those that are difficult to obtain construct pairs for training, such as mesh and point cloud data. The trajectories serve as input for motion control in subsequent frames. (b) AnyI2V can accept inputs with mixed conditional types, further increasing the flexibility of the input. (c) By using LoRA [13] or different text prompts, AnyI2V can achieve the editing effect of the original image. "
[17.07.2025 06:18] Response: ```python
["Fudan University", "DAMO Academy, Alibaba group", "Hupan Lab"]
```
[17.07.2025 06:18] Deleting PDF ./assets/pdf/2507.02857.pdf.
[17.07.2025 06:18] Success.
[17.07.2025 06:18] Enriching papers with extra data.
[17.07.2025 06:18] ********************************************************************************
[17.07.2025 06:18] Abstract 0. PhysX addresses the lack of physical properties in 3D generative models by introducing PhysXNet, a physics-annotated dataset, and PhysXGen, a framework that integrates physical knowledge into 3D asset generation.  					AI-generated summary 				 3D modeling is moving from virtual to physical. Existin...
[17.07.2025 06:18] ********************************************************************************
[17.07.2025 06:18] Abstract 1. This survey integrates reasoning and retrieval in Large Language Models to improve factuality and multi-step inference, highlighting Synergized RAG-Reasoning frameworks and outlining future research directions.  					AI-generated summary 				 Retrieval-Augmented Generation (RAG) lifts the factuality...
[17.07.2025 06:18] ********************************************************************************
[17.07.2025 06:18] Abstract 2. A large-scale benchmark, MMHU, is proposed for human behavior analysis in autonomous driving, featuring rich annotations and diverse data sources, and benchmarking multiple tasks including motion prediction and behavior question answering.  					AI-generated summary 				 Humans are integral componen...
[17.07.2025 06:18] ********************************************************************************
[17.07.2025 06:18] Abstract 3. DrafterBench is an open-source benchmark for evaluating LLM agents in technical drawing revision, assessing their capabilities in structured data comprehension, function execution, instruction following, and critical reasoning.  					AI-generated summary 				 Large Language Model (LLM) agents have s...
[17.07.2025 06:18] ********************************************************************************
[17.07.2025 06:18] Abstract 4. A diffusion-based generative framework, MOSPA, is introduced to model human motion in response to spatial audio, achieving state-of-the-art performance using the newly created SAM dataset.  					AI-generated summary 				 Enabling virtual humans to dynamically and realistically respond to diverse aud...
[17.07.2025 06:18] ********************************************************************************
[17.07.2025 06:18] Abstract 5. SWE-Perf is a benchmark for evaluating Large Language Models in code performance optimization using real-world repository data.  					AI-generated summary 				 Code performance optimization is paramount in real-world software engineering and critical for production-level systems. While Large Languag...
[17.07.2025 06:18] ********************************************************************************
[17.07.2025 06:18] Abstract 6. Lizard is a linearization framework that transforms Transformer-based LLMs into subquadratic architectures for efficient infinite-context generation, using a hybrid attention mechanism and hardware-aware training.  					AI-generated summary 				 We propose Lizard, a linearization framework that tran...
[17.07.2025 06:18] ********************************************************************************
[17.07.2025 06:18] Abstract 7. AnyI2V is a training-free framework that animates conditional images with user-defined motion trajectories, supporting various data types and enabling flexible video generation.  					AI-generated summary 				 Recent advancements in video generation, particularly in diffusion models, have driven not...
[17.07.2025 06:18] Read previous papers.
[17.07.2025 06:18] Generating reviews via LLM API.
[17.07.2025 06:18] Using data from previous issue: {"categories": ["#architecture", "#synthetic", "#games", "#3d", "#open_source", "#dataset"], "emoji": "🧱", "ru": {"title": "Физически достоверная генерация 3D-объектов", "desc": "PhysX представляет собой новый подход к генерации 3D-объектов с учетом их физических свойств. Авторы создали датасет Phys
[17.07.2025 06:18] Using data from previous issue: {"categories": ["#reasoning", "#multimodal", "#benchmark", "#survey", "#rag"], "emoji": "🧠", "ru": {"title": "Объединение извлечения информации и рассуждений для создания более мощных языковых моделей", "desc": "Это обзор интегрирует рассуждения и извлечение информации в больших языковых моделях (LL
[17.07.2025 06:18] Querying the API.
[17.07.2025 06:18] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A large-scale benchmark, MMHU, is proposed for human behavior analysis in autonomous driving, featuring rich annotations and diverse data sources, and benchmarking multiple tasks including motion prediction and behavior question answering.  					AI-generated summary 				 Humans are integral components of the transportation ecosystem, and understanding their behaviors is crucial to facilitating the development of safe driving systems. Although recent progress has explored various aspects of human behaviorx2014such as motion, trajectories, and intentionx2014a comprehensive benchmark for evaluating human behavior understanding in autonomous driving remains unavailable. In this work, we propose MMHU, a large-scale benchmark for human behavior analysis featuring rich annotations, such as human motion and trajectories, text description for human motions, human intention, and critical behavior labels relevant to driving safety. Our dataset encompasses 57k human motion clips and 1.73M frames gathered from diverse sources, including established driving datasets such as Waymo, in-the-wild videos from YouTube, and self-collected data. A human-in-the-loop annotation pipeline is developed to generate rich behavior captions. We provide a thorough dataset analysis and benchmark multiple tasksx2014ranging from motion prediction to motion generation and human behavior question answeringx2014thereby offering a broad evaluation suite. Project page : https://MMHU-Benchmark.github.io.
[17.07.2025 06:18] Response: {
  "desc": "Представлен крупномасштабный бенчмарк MMHU для анализа поведения человека в контексте автономного вождения. Он включает богатые аннотации и разнообразные источники данных, охватывающие 57 тысяч клипов с движениями людей и 1,73 миллиона кадров. MMHU позволяет оценивать различные задачи, включая прогнозирование движения и ответы на вопросы о поведении. Бенчмарк предоставляет комплексный набор для оценки понимания человеческого поведения в системах автономного вождения.",
  "emoji": "🚗",
  "title": "MMHU: Комплексный анализ поведения человека для безопасного автономного вождения"
}
[17.07.2025 06:18] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A large-scale benchmark, MMHU, is proposed for human behavior analysis in autonomous driving, featuring rich annotations and diverse data sources, and benchmarking multiple tasks including motion prediction and behavior question answering.  					AI-generated summary 				 Humans are integral components of the transportation ecosystem, and understanding their behaviors is crucial to facilitating the development of safe driving systems. Although recent progress has explored various aspects of human behaviorx2014such as motion, trajectories, and intentionx2014a comprehensive benchmark for evaluating human behavior understanding in autonomous driving remains unavailable. In this work, we propose MMHU, a large-scale benchmark for human behavior analysis featuring rich annotations, such as human motion and trajectories, text description for human motions, human intention, and critical behavior labels relevant to driving safety. Our dataset encompasses 57k human motion clips and 1.73M frames gathered from diverse sources, including established driving datasets such as Waymo, in-the-wild videos from YouTube, and self-collected data. A human-in-the-loop annotation pipeline is developed to generate rich behavior captions. We provide a thorough dataset analysis and benchmark multiple tasksx2014ranging from motion prediction to motion generation and human behavior question answeringx2014thereby offering a broad evaluation suite. Project page : https://MMHU-Benchmark.github.io."

[17.07.2025 06:19] Response: ```python
['DATASET', 'BENCHMARK', 'AGENTS']
```
[17.07.2025 06:19] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A large-scale benchmark, MMHU, is proposed for human behavior analysis in autonomous driving, featuring rich annotations and diverse data sources, and benchmarking multiple tasks including motion prediction and behavior question answering.  					AI-generated summary 				 Humans are integral components of the transportation ecosystem, and understanding their behaviors is crucial to facilitating the development of safe driving systems. Although recent progress has explored various aspects of human behaviorx2014such as motion, trajectories, and intentionx2014a comprehensive benchmark for evaluating human behavior understanding in autonomous driving remains unavailable. In this work, we propose MMHU, a large-scale benchmark for human behavior analysis featuring rich annotations, such as human motion and trajectories, text description for human motions, human intention, and critical behavior labels relevant to driving safety. Our dataset encompasses 57k human motion clips and 1.73M frames gathered from diverse sources, including established driving datasets such as Waymo, in-the-wild videos from YouTube, and self-collected data. A human-in-the-loop annotation pipeline is developed to generate rich behavior captions. We provide a thorough dataset analysis and benchmark multiple tasksx2014ranging from motion prediction to motion generation and human behavior question answeringx2014thereby offering a broad evaluation suite. Project page : https://MMHU-Benchmark.github.io."

[17.07.2025 06:19] Response: ```python
[]
```
[17.07.2025 06:19] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The paper introduces MMHU, a comprehensive benchmark designed for analyzing human behavior in the context of autonomous driving. It includes extensive annotations on human motion, trajectories, intentions, and safety-related behaviors, making it a valuable resource for researchers. The dataset consists of 57,000 motion clips and 1.73 million frames sourced from various platforms, including established driving datasets and real-world videos. By benchmarking multiple tasks such as motion prediction and behavior question answering, MMHU aims to enhance the understanding of human behavior in driving scenarios.","title":"MMHU: Advancing Human Behavior Analysis for Safer Autonomous Driving"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='The paper introduces MMHU, a comprehensive benchmark designed for analyzing human behavior in the context of autonomous driving. It includes extensive annotations on human motion, trajectories, intentions, and safety-related behaviors, making it a valuable resource for researchers. The dataset consists of 57,000 motion clips and 1.73 million frames sourced from various platforms, including established driving datasets and real-world videos. By benchmarking multiple tasks such as motion prediction and behavior question answering, MMHU aims to enhance the understanding of human behavior in driving scenarios.', title='MMHU: Advancing Human Behavior Analysis for Safer Autonomous Driving'))
[17.07.2025 06:19] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本文提出了一个名为MMHU的大规模基准，用于分析自动驾驶中的人类行为。该基准包含丰富的注释和多样的数据来源，涵盖了人类运动、轨迹、意图等多个方面。数据集包括57,000个运动片段和173万帧，来源于知名的驾驶数据集和YouTube等平台。我们还开发了一个人机协作的注释流程，以生成详细的行为描述，并对多个任务进行了基准测试，包括运动预测和行为问答。","title":"MMHU：自动驾驶人类行为分析的新基准"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本文提出了一个名为MMHU的大规模基准，用于分析自动驾驶中的人类行为。该基准包含丰富的注释和多样的数据来源，涵盖了人类运动、轨迹、意图等多个方面。数据集包括57,000个运动片段和173万帧，来源于知名的驾驶数据集和YouTube等平台。我们还开发了一个人机协作的注释流程，以生成详细的行为描述，并对多个任务进行了基准测试，包括运动预测和行为问答。', title='MMHU：自动驾驶人类行为分析的新基准'))
[17.07.2025 06:19] Using data from previous issue: {"categories": ["#reasoning", "#benchmark", "#open_source", "#long_context", "#agents"], "emoji": "📐", "ru": {"title": "DrafterBench: Комплексная оценка LLM-агентов в инженерном проектировании", "desc": "DrafterBench - это открытый бенчмарк для оценки агентов на основе больших языковых моделей (LLM)
[17.07.2025 06:19] Using data from previous issue: {"categories": ["#multimodal", "#diffusion", "#benchmark", "#open_source", "#dataset"], "emoji": "🎧", "ru": {"title": "Реалистичная анимация движений под пространственное аудио", "desc": "Исследователи представили MOSPA - генеративную модель на основе диффузии для моделирования движений человека в о
[17.07.2025 06:19] Querying the API.
[17.07.2025 06:19] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

SWE-Perf is a benchmark for evaluating Large Language Models in code performance optimization using real-world repository data.  					AI-generated summary 				 Code performance optimization is paramount in real-world software engineering and critical for production-level systems. While Large Language Models (LLMs) have demonstrated impressive capabilities in code generation and bug fixing, their proficiency in enhancing code performance at the repository level remains largely unexplored. To address this gap, we introduce SWE-Perf, the first benchmark specifically designed to systematically evaluate LLMs on code performance optimization tasks within authentic repository contexts. SWE-Perf comprises 140 carefully curated instances, each derived from performance-improving pull requests from popular GitHub repositories. Each benchmark instance includes the relevant codebase, target functions, performance-related tests, expert-authored patches, and executable environments. Through a comprehensive evaluation of representative methods that span file-level and repo-level approaches (e.g., Agentless and OpenHands), we reveal a substantial capability gap between existing LLMs and expert-level optimization performance, highlighting critical research opportunities in this emerging field.
[17.07.2025 06:19] Response: {
  "desc": "SWE-Perf - это бенчмарк для оценки способностей больших языковых моделей (LLM) в оптимизации производительности кода на основе реальных репозиториев. Он содержит 140 тщательно отобранных примеров из популярных GitHub-репозиториев, включающих кодовую базу, целевые функции, тесты производительности и экспертные патчи. Оценка существующих методов показала значительный разрыв между возможностями LLM и экспертным уровнем оптимизации. SWE-Perf открывает новые направления исследований в области применения LLM для повышения производительности кода.",
  "emoji": "🚀",
  "title": "SWE-Perf: Новый рубеж в оценке оптимизации кода с помощью LLM"
}
[17.07.2025 06:19] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"SWE-Perf is a benchmark for evaluating Large Language Models in code performance optimization using real-world repository data.  					AI-generated summary 				 Code performance optimization is paramount in real-world software engineering and critical for production-level systems. While Large Language Models (LLMs) have demonstrated impressive capabilities in code generation and bug fixing, their proficiency in enhancing code performance at the repository level remains largely unexplored. To address this gap, we introduce SWE-Perf, the first benchmark specifically designed to systematically evaluate LLMs on code performance optimization tasks within authentic repository contexts. SWE-Perf comprises 140 carefully curated instances, each derived from performance-improving pull requests from popular GitHub repositories. Each benchmark instance includes the relevant codebase, target functions, performance-related tests, expert-authored patches, and executable environments. Through a comprehensive evaluation of representative methods that span file-level and repo-level approaches (e.g., Agentless and OpenHands), we reveal a substantial capability gap between existing LLMs and expert-level optimization performance, highlighting critical research opportunities in this emerging field."

[17.07.2025 06:19] Response: ```python
['BENCHMARK', 'DATASET']
```
[17.07.2025 06:19] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"SWE-Perf is a benchmark for evaluating Large Language Models in code performance optimization using real-world repository data.  					AI-generated summary 				 Code performance optimization is paramount in real-world software engineering and critical for production-level systems. While Large Language Models (LLMs) have demonstrated impressive capabilities in code generation and bug fixing, their proficiency in enhancing code performance at the repository level remains largely unexplored. To address this gap, we introduce SWE-Perf, the first benchmark specifically designed to systematically evaluate LLMs on code performance optimization tasks within authentic repository contexts. SWE-Perf comprises 140 carefully curated instances, each derived from performance-improving pull requests from popular GitHub repositories. Each benchmark instance includes the relevant codebase, target functions, performance-related tests, expert-authored patches, and executable environments. Through a comprehensive evaluation of representative methods that span file-level and repo-level approaches (e.g., Agentless and OpenHands), we reveal a substantial capability gap between existing LLMs and expert-level optimization performance, highlighting critical research opportunities in this emerging field."

[17.07.2025 06:19] Response: ```python
["OPTIMIZATION"]
```
[17.07.2025 06:19] Response: ParsedChatCompletionMessage[Article](content='{"desc":"SWE-Perf is a new benchmark designed to evaluate how well Large Language Models (LLMs) can optimize code performance using real-world data from software repositories. It focuses on the important task of improving code efficiency, which is essential for high-quality software systems. The benchmark includes 140 instances based on actual performance-enhancing pull requests from GitHub, providing a realistic testing environment. The study reveals that current LLMs significantly lag behind expert-level optimization, indicating a need for further research in this area.","title":"Unlocking Code Efficiency: Evaluating LLMs with SWE-Perf"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='SWE-Perf is a new benchmark designed to evaluate how well Large Language Models (LLMs) can optimize code performance using real-world data from software repositories. It focuses on the important task of improving code efficiency, which is essential for high-quality software systems. The benchmark includes 140 instances based on actual performance-enhancing pull requests from GitHub, providing a realistic testing environment. The study reveals that current LLMs significantly lag behind expert-level optimization, indicating a need for further research in this area.', title='Unlocking Code Efficiency: Evaluating LLMs with SWE-Perf'))
[17.07.2025 06:19] Response: ParsedChatCompletionMessage[Article](content='{"desc":"SWE-Perf是一个基准测试，用于评估大型语言模型在代码性能优化方面的表现。该基准测试使用真实的代码库数据，专注于软件工程中的代码性能优化。虽然大型语言模型在代码生成和错误修复方面表现出色，但它们在提升代码性能方面的能力尚未得到充分探索。通过对140个精心挑选的实例进行评估，SWE-Perf揭示了现有大型语言模型与专家级优化性能之间的显著差距，指出了这一新兴领域中的重要研究机会。","title":"SWE-Perf：评估代码性能优化的新基准"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='SWE-Perf是一个基准测试，用于评估大型语言模型在代码性能优化方面的表现。该基准测试使用真实的代码库数据，专注于软件工程中的代码性能优化。虽然大型语言模型在代码生成和错误修复方面表现出色，但它们在提升代码性能方面的能力尚未得到充分探索。通过对140个精心挑选的实例进行评估，SWE-Perf揭示了现有大型语言模型与专家级优化性能之间的显著差距，指出了这一新兴领域中的重要研究机会。', title='SWE-Perf：评估代码性能优化的新基准'))
[17.07.2025 06:19] Using data from previous issue: {"categories": ["#benchmark", "#architecture", "#optimization", "#training", "#long_context"], "emoji": "🦎", "ru": {"title": "Lizard: эффективные языковые модели с бесконечным контекстом", "desc": "Lizard - это фреймворк линеаризации, который преобразует трансформерные языковые модели в субквадратич
[17.07.2025 06:19] Querying the API.
[17.07.2025 06:19] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

AnyI2V is a training-free framework that animates conditional images with user-defined motion trajectories, supporting various data types and enabling flexible video generation.  					AI-generated summary 				 Recent advancements in video generation, particularly in diffusion models, have driven notable progress in text-to-video (T2V) and image-to-video (I2V) synthesis. However, challenges remain in effectively integrating dynamic motion signals and flexible spatial constraints. Existing T2V methods typically rely on text prompts, which inherently lack precise control over the spatial layout of generated content. In contrast, I2V methods are limited by their dependence on real images, which restricts the editability of the synthesized content. Although some methods incorporate ControlNet to introduce image-based conditioning, they often lack explicit motion control and require computationally expensive training. To address these limitations, we propose AnyI2V, a training-free framework that animates any conditional images with user-defined motion trajectories. AnyI2V supports a broader range of modalities as the conditional image, including data types such as meshes and point clouds that are not supported by ControlNet, enabling more flexible and versatile video generation. Additionally, it supports mixed conditional inputs and enables style transfer and editing via LoRA and text prompts. Extensive experiments demonstrate that the proposed AnyI2V achieves superior performance and provides a new perspective in spatial- and motion-controlled video generation. Code is available at https://henghuiding.com/AnyI2V/.
[17.07.2025 06:19] Response: {
  "desc": "AnyI2V - это фреймворк для анимации условных изображений с пользовательскими траекториями движения без необходимости обучения. Он поддерживает различные типы данных, включая сетки и облака точек, что позволяет создавать более гибкие и универсальные видео. AnyI2V превосходит существующие методы текст-в-видео и изображение-в-видео, предоставляя точный контроль над пространственным расположением и движением генерируемого контента. Фреймворк также поддерживает смешанные условные входные данные и позволяет выполнять перенос стиля и редактирование с помощью LoRA и текстовых подсказок.",
  "emoji": "🎬",
  "title": "Универсальная анимация изображений с контролем движения"
}
[17.07.2025 06:19] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"AnyI2V is a training-free framework that animates conditional images with user-defined motion trajectories, supporting various data types and enabling flexible video generation.  					AI-generated summary 				 Recent advancements in video generation, particularly in diffusion models, have driven notable progress in text-to-video (T2V) and image-to-video (I2V) synthesis. However, challenges remain in effectively integrating dynamic motion signals and flexible spatial constraints. Existing T2V methods typically rely on text prompts, which inherently lack precise control over the spatial layout of generated content. In contrast, I2V methods are limited by their dependence on real images, which restricts the editability of the synthesized content. Although some methods incorporate ControlNet to introduce image-based conditioning, they often lack explicit motion control and require computationally expensive training. To address these limitations, we propose AnyI2V, a training-free framework that animates any conditional images with user-defined motion trajectories. AnyI2V supports a broader range of modalities as the conditional image, including data types such as meshes and point clouds that are not supported by ControlNet, enabling more flexible and versatile video generation. Additionally, it supports mixed conditional inputs and enables style transfer and editing via LoRA and text prompts. Extensive experiments demonstrate that the proposed AnyI2V achieves superior performance and provides a new perspective in spatial- and motion-controlled video generation. Code is available at https://henghuiding.com/AnyI2V/."

[17.07.2025 06:19] Response: ```python
['VIDEO', 'MULTIMODAL']
```
[17.07.2025 06:19] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"AnyI2V is a training-free framework that animates conditional images with user-defined motion trajectories, supporting various data types and enabling flexible video generation.  					AI-generated summary 				 Recent advancements in video generation, particularly in diffusion models, have driven notable progress in text-to-video (T2V) and image-to-video (I2V) synthesis. However, challenges remain in effectively integrating dynamic motion signals and flexible spatial constraints. Existing T2V methods typically rely on text prompts, which inherently lack precise control over the spatial layout of generated content. In contrast, I2V methods are limited by their dependence on real images, which restricts the editability of the synthesized content. Although some methods incorporate ControlNet to introduce image-based conditioning, they often lack explicit motion control and require computationally expensive training. To address these limitations, we propose AnyI2V, a training-free framework that animates any conditional images with user-defined motion trajectories. AnyI2V supports a broader range of modalities as the conditional image, including data types such as meshes and point clouds that are not supported by ControlNet, enabling more flexible and versatile video generation. Additionally, it supports mixed conditional inputs and enables style transfer and editing via LoRA and text prompts. Extensive experiments demonstrate that the proposed AnyI2V achieves superior performance and provides a new perspective in spatial- and motion-controlled video generation. Code is available at https://henghuiding.com/AnyI2V/."

[17.07.2025 06:19] Response: ```python
["DIFFUSION", "OPTIMIZATION"]
```
[17.07.2025 06:19] Response: ParsedChatCompletionMessage[Article](content='{"desc":"AnyI2V is a novel framework designed for animating images based on user-defined motion paths without the need for extensive training. It addresses the limitations of existing text-to-video and image-to-video methods by allowing for greater control over spatial layouts and dynamic motion signals. This framework supports various data types, including meshes and point clouds, which enhances its versatility in video generation. Through extensive testing, AnyI2V has shown to outperform previous methods, offering a fresh approach to generating videos with precise motion and spatial control.","title":"AnyI2V: Freedom in Motion-Controlled Video Generation"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='AnyI2V is a novel framework designed for animating images based on user-defined motion paths without the need for extensive training. It addresses the limitations of existing text-to-video and image-to-video methods by allowing for greater control over spatial layouts and dynamic motion signals. This framework supports various data types, including meshes and point clouds, which enhances its versatility in video generation. Through extensive testing, AnyI2V has shown to outperform previous methods, offering a fresh approach to generating videos with precise motion and spatial control.', title='AnyI2V: Freedom in Motion-Controlled Video Generation'))
[17.07.2025 06:19] Response: ParsedChatCompletionMessage[Article](content='{"desc":"AnyI2V是一个无需训练的框架，可以根据用户定义的运动轨迹为条件图像添加动画，支持多种数据类型，从而实现灵活的视频生成。该方法解决了现有文本到视频（T2V）和图像到视频（I2V）合成中的动态运动信号和空间约束整合问题。与传统方法不同，AnyI2V不依赖于真实图像，允许更高的可编辑性，并支持混合条件输入和风格转移。实验结果表明，AnyI2V在空间和运动控制的视频生成方面表现优越，提供了新的视角。","title":"AnyI2V：无训练的灵活视频生成框架"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='AnyI2V是一个无需训练的框架，可以根据用户定义的运动轨迹为条件图像添加动画，支持多种数据类型，从而实现灵活的视频生成。该方法解决了现有文本到视频（T2V）和图像到视频（I2V）合成中的动态运动信号和空间约束整合问题。与传统方法不同，AnyI2V不依赖于真实图像，允许更高的可编辑性，并支持混合条件输入和风格转移。实验结果表明，AnyI2V在空间和运动控制的视频生成方面表现优越，提供了新的视角。', title='AnyI2V：无训练的灵活视频生成框架'))
[17.07.2025 06:19] Renaming data file.
[17.07.2025 06:19] Renaming previous data. hf_papers.json to ./d/2025-07-17.json
[17.07.2025 06:19] Saving new data file.
[17.07.2025 06:19] Generating page.
[17.07.2025 06:19] Renaming previous page.
[17.07.2025 06:19] Renaming previous data. index.html to ./d/2025-07-17.html
[17.07.2025 06:19] Writing result.
[17.07.2025 06:19] Renaming log file.
[17.07.2025 06:19] Renaming previous data. log.txt to ./logs/2025-07-17_last_log.txt
