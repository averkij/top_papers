[17.07.2025 18:17] Read previous papers.
[17.07.2025 18:17] Generating top page (month).
[17.07.2025 18:17] Writing top page (month).
[17.07.2025 19:12] Read previous papers.
[17.07.2025 19:12] Get feed.
[17.07.2025 19:12] Get page data from previous paper. URL: https://huggingface.co/papers/2507.09477
[17.07.2025 19:12] Get page data from previous paper. URL: https://huggingface.co/papers/2507.12465
[17.07.2025 19:12] Get page data from previous paper. URL: https://huggingface.co/papers/2507.12463
[17.07.2025 19:12] Get page data from previous paper. URL: https://huggingface.co/papers/2507.12415
[17.07.2025 19:12] Get page data from previous paper. URL: https://huggingface.co/papers/2507.11949
[17.07.2025 19:12] Get page data from previous paper. URL: https://huggingface.co/papers/2507.11527
[17.07.2025 19:12] Get page data from previous paper. URL: https://huggingface.co/papers/2507.11412
[17.07.2025 19:12] Get page data from previous paper. URL: https://huggingface.co/papers/2507.02857
[17.07.2025 19:12] Get page data from previous paper. URL: https://huggingface.co/papers/2507.12462
[17.07.2025 19:12] Get page data from previous paper. URL: https://huggingface.co/papers/2507.09025
[17.07.2025 19:12] Get page data from previous paper. URL: https://huggingface.co/papers/2507.05065
[17.07.2025 19:12] Get page data from previous paper. URL: https://huggingface.co/papers/2507.11764
[17.07.2025 19:12] Get page data from previous paper. URL: https://huggingface.co/papers/2507.07451
[17.07.2025 19:12] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[17.07.2025 19:12] No deleted papers detected.
[17.07.2025 19:12] Downloading and parsing papers (pdf, html). Total: 13.
[17.07.2025 19:12] Downloading and parsing paper https://huggingface.co/papers/2507.09477.
[17.07.2025 19:12] Extra JSON file exists (./assets/json/2507.09477.json), skip PDF parsing.
[17.07.2025 19:12] Paper image links file exists (./assets/img_data/2507.09477.json), skip HTML parsing.
[17.07.2025 19:12] Success.
[17.07.2025 19:12] Downloading and parsing paper https://huggingface.co/papers/2507.12465.
[17.07.2025 19:12] Extra JSON file exists (./assets/json/2507.12465.json), skip PDF parsing.
[17.07.2025 19:12] Paper image links file exists (./assets/img_data/2507.12465.json), skip HTML parsing.
[17.07.2025 19:12] Success.
[17.07.2025 19:12] Downloading and parsing paper https://huggingface.co/papers/2507.12463.
[17.07.2025 19:12] Extra JSON file exists (./assets/json/2507.12463.json), skip PDF parsing.
[17.07.2025 19:12] Paper image links file exists (./assets/img_data/2507.12463.json), skip HTML parsing.
[17.07.2025 19:12] Success.
[17.07.2025 19:12] Downloading and parsing paper https://huggingface.co/papers/2507.12415.
[17.07.2025 19:12] Extra JSON file exists (./assets/json/2507.12415.json), skip PDF parsing.
[17.07.2025 19:12] Paper image links file exists (./assets/img_data/2507.12415.json), skip HTML parsing.
[17.07.2025 19:12] Success.
[17.07.2025 19:12] Downloading and parsing paper https://huggingface.co/papers/2507.11949.
[17.07.2025 19:12] Extra JSON file exists (./assets/json/2507.11949.json), skip PDF parsing.
[17.07.2025 19:12] Paper image links file exists (./assets/img_data/2507.11949.json), skip HTML parsing.
[17.07.2025 19:12] Success.
[17.07.2025 19:12] Downloading and parsing paper https://huggingface.co/papers/2507.11527.
[17.07.2025 19:12] Extra JSON file exists (./assets/json/2507.11527.json), skip PDF parsing.
[17.07.2025 19:12] Paper image links file exists (./assets/img_data/2507.11527.json), skip HTML parsing.
[17.07.2025 19:12] Success.
[17.07.2025 19:12] Downloading and parsing paper https://huggingface.co/papers/2507.11412.
[17.07.2025 19:12] Extra JSON file exists (./assets/json/2507.11412.json), skip PDF parsing.
[17.07.2025 19:12] Paper image links file exists (./assets/img_data/2507.11412.json), skip HTML parsing.
[17.07.2025 19:12] Success.
[17.07.2025 19:12] Downloading and parsing paper https://huggingface.co/papers/2507.02857.
[17.07.2025 19:12] Extra JSON file exists (./assets/json/2507.02857.json), skip PDF parsing.
[17.07.2025 19:12] Paper image links file exists (./assets/img_data/2507.02857.json), skip HTML parsing.
[17.07.2025 19:12] Success.
[17.07.2025 19:12] Downloading and parsing paper https://huggingface.co/papers/2507.12462.
[17.07.2025 19:12] Extra JSON file exists (./assets/json/2507.12462.json), skip PDF parsing.
[17.07.2025 19:12] Paper image links file exists (./assets/img_data/2507.12462.json), skip HTML parsing.
[17.07.2025 19:12] Success.
[17.07.2025 19:12] Downloading and parsing paper https://huggingface.co/papers/2507.09025.
[17.07.2025 19:12] Extra JSON file exists (./assets/json/2507.09025.json), skip PDF parsing.
[17.07.2025 19:12] Paper image links file exists (./assets/img_data/2507.09025.json), skip HTML parsing.
[17.07.2025 19:12] Success.
[17.07.2025 19:12] Downloading and parsing paper https://huggingface.co/papers/2507.05065.
[17.07.2025 19:12] Extra JSON file exists (./assets/json/2507.05065.json), skip PDF parsing.
[17.07.2025 19:12] Paper image links file exists (./assets/img_data/2507.05065.json), skip HTML parsing.
[17.07.2025 19:12] Success.
[17.07.2025 19:12] Downloading and parsing paper https://huggingface.co/papers/2507.11764.
[17.07.2025 19:12] Extra JSON file exists (./assets/json/2507.11764.json), skip PDF parsing.
[17.07.2025 19:12] Paper image links file exists (./assets/img_data/2507.11764.json), skip HTML parsing.
[17.07.2025 19:12] Success.
[17.07.2025 19:12] Downloading and parsing paper https://huggingface.co/papers/2507.07451.
[17.07.2025 19:12] Extra JSON file exists (./assets/json/2507.07451.json), skip PDF parsing.
[17.07.2025 19:12] Paper image links file exists (./assets/img_data/2507.07451.json), skip HTML parsing.
[17.07.2025 19:12] Success.
[17.07.2025 19:12] Enriching papers with extra data.
[17.07.2025 19:12] ********************************************************************************
[17.07.2025 19:12] Abstract 0. This survey integrates reasoning and retrieval in Large Language Models to improve factuality and multi-step inference, highlighting Synergized RAG-Reasoning frameworks and outlining future research directions.  					AI-generated summary 				 Retrieval-Augmented Generation (RAG) lifts the factuality...
[17.07.2025 19:12] ********************************************************************************
[17.07.2025 19:12] Abstract 1. PhysX addresses the lack of physical properties in 3D generative models by introducing PhysXNet, a physics-annotated dataset, and PhysXGen, a framework that integrates physical knowledge into 3D asset generation.  					AI-generated summary 				 3D modeling is moving from virtual to physical. Existin...
[17.07.2025 19:12] ********************************************************************************
[17.07.2025 19:12] Abstract 2. A large-scale benchmark, MMHU, is proposed for human behavior analysis in autonomous driving, featuring rich annotations and diverse data sources, and benchmarking multiple tasks including motion prediction and behavior question answering.  					AI-generated summary 				 Humans are integral componen...
[17.07.2025 19:12] ********************************************************************************
[17.07.2025 19:12] Abstract 3. SWE-Perf is a benchmark for evaluating Large Language Models in code performance optimization using real-world repository data.  					AI-generated summary 				 Code performance optimization is paramount in real-world software engineering and critical for production-level systems. While Large Languag...
[17.07.2025 19:12] ********************************************************************************
[17.07.2025 19:12] Abstract 4. A diffusion-based generative framework, MOSPA, is introduced to model human motion in response to spatial audio, achieving state-of-the-art performance using the newly created SAM dataset.  					AI-generated summary 				 Enabling virtual humans to dynamically and realistically respond to diverse aud...
[17.07.2025 19:12] ********************************************************************************
[17.07.2025 19:12] Abstract 5. DrafterBench is an open-source benchmark for evaluating LLM agents in technical drawing revision, assessing their capabilities in structured data comprehension, function execution, instruction following, and critical reasoning.  					AI-generated summary 				 Large Language Model (LLM) agents have s...
[17.07.2025 19:12] ********************************************************************************
[17.07.2025 19:12] Abstract 6. The large language model (LLM) community focuses almost exclusively on decoder-only language models, since they are easier to use for text generation. However, a large subset of the community still uses encoder-only models for tasks such as classification or retrieval. Previous work has attempted to...
[17.07.2025 19:12] ********************************************************************************
[17.07.2025 19:12] Abstract 7. AnyI2V is a training-free framework that animates conditional images with user-defined motion trajectories, supporting various data types and enabling flexible video generation.  					AI-generated summary 				 Recent advancements in video generation, particularly in diffusion models, have driven not...
[17.07.2025 19:12] ********************************************************************************
[17.07.2025 19:12] Abstract 8. SpatialTrackerV2 is a feed-forward 3D point tracking method for monocular videos that integrates point tracking, monocular depth, and camera pose estimation into a unified, end-to-end architecture, achieving high performance and speed.  					AI-generated summary 				 We present SpatialTrackerV2, a f...
[17.07.2025 19:12] ********************************************************************************
[17.07.2025 19:12] Abstract 9. Lizard is a linearization framework that transforms Transformer-based LLMs into subquadratic architectures for efficient infinite-context generation, using a hybrid attention mechanism and hardware-aware training.  					AI-generated summary 				 We propose Lizard, a linearization framework that tran...
[17.07.2025 19:12] ********************************************************************************
[17.07.2025 19:12] Abstract 10. A new approach formats tokens as a multi-turn interaction trace with a stateful tool for training Large Language Models, enabling faster sampling and denser reward signals for tasks like repairing Python code.  					AI-generated summary 				 Recent advances have established a new machine learning pa...
[17.07.2025 19:12] ********************************************************************************
[17.07.2025 19:12] Abstract 11. Sentiment-augmented transformer-based classifiers improve subjectivity detection in multilingual and zero-shot settings, achieving high performance and ranking first for Greek.  					AI-generated summary 				 This paper presents AI Wizards' participation in the CLEF 2025 CheckThat! Lab Task 1: Subje...
[17.07.2025 19:12] ********************************************************************************
[17.07.2025 19:12] Abstract 12. RLEP, a reinforcement learning framework with experience replay, enhances large language model training by focusing on high-quality examples, leading to faster convergence and improved performance on math-related benchmarks.  					AI-generated summary 				 Reinforcement learning (RL) for large langu...
[17.07.2025 19:12] Read previous papers.
[17.07.2025 19:12] Generating reviews via LLM API.
[17.07.2025 19:12] Using data from previous issue: {"categories": ["#reasoning", "#multimodal", "#benchmark", "#survey", "#rag"], "emoji": "üß†", "ru": {"title": "–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –±–æ–ª–µ–µ –º–æ—â–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–≠—Ç–æ –æ–±–∑–æ—Ä –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LL
[17.07.2025 19:12] Using data from previous issue: {"categories": ["#architecture", "#synthetic", "#games", "#3d", "#open_source", "#dataset"], "emoji": "üß±", "ru": {"title": "–§–∏–∑–∏—á–µ—Å–∫–∏ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è 3D-–æ–±—ä–µ–∫—Ç–æ–≤", "desc": "PhysX –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ 3D-–æ–±—ä–µ–∫—Ç–æ–≤ —Å —É—á–µ—Ç–æ–º –∏—Ö —Ñ–∏–∑–∏—á–µ—Å–∫–∏—Ö —Å–≤–æ–π—Å—Ç–≤. –ê–≤—Ç–æ—Ä—ã —Å–æ–∑–¥–∞–ª–∏ –¥–∞—Ç–∞—Å–µ—Ç Phys
[17.07.2025 19:12] Using data from previous issue: {"categories": ["#benchmark", "#agents", "#dataset"], "emoji": "üöó", "ru": {"title": "MMHU: –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø–æ–≤–µ–¥–µ–Ω–∏—è —á–µ–ª–æ–≤–µ–∫–∞ –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ–≥–æ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –≤–æ–∂–¥–µ–Ω–∏—è", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ MMHU –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ø–æ–≤–µ–¥–µ–Ω–∏—è —á–µ–ª–æ–≤–µ–∫–∞ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –≤–æ–∂–¥–µ–Ω–∏—è. –û–Ω –≤–∫–ª—é—á–∞–µ—Ç –±–æ–≥
[17.07.2025 19:12] Using data from previous issue: {"categories": ["#optimization", "#benchmark", "#dataset"], "emoji": "üöÄ", "ru": {"title": "SWE-Perf: –ù–æ–≤—ã–π —Ä—É–±–µ–∂ –≤ –æ—Ü–µ–Ω–∫–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∫–æ–¥–∞ —Å –ø–æ–º–æ—â—å—é LLM", "desc": "SWE-Perf - —ç—Ç–æ –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≤ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∫–æ–¥–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∞–ª—å–Ω—ã—Ö 
[17.07.2025 19:12] Using data from previous issue: {"categories": ["#multimodal", "#diffusion", "#benchmark", "#open_source", "#dataset"], "emoji": "üéß", "ru": {"title": "–†–µ–∞–ª–∏—Å—Ç–∏—á–Ω–∞—è –∞–Ω–∏–º–∞—Ü–∏—è –¥–≤–∏–∂–µ–Ω–∏–π –ø–æ–¥ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ –∞—É–¥–∏–æ", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ MOSPA - –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—É—é –º–æ–¥–µ–ª—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∏—Ñ—Ñ—É–∑–∏–∏ –¥–ª—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –¥–≤–∏–∂–µ–Ω–∏–π —á–µ–ª–æ–≤–µ–∫–∞ –≤ –æ
[17.07.2025 19:12] Using data from previous issue: {"categories": ["#reasoning", "#benchmark", "#open_source", "#long_context", "#agents"], "emoji": "üìê", "ru": {"title": "DrafterBench: –ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ LLM-–∞–≥–µ–Ω—Ç–æ–≤ –≤ –∏–Ω–∂–µ–Ω–µ—Ä–Ω–æ–º –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏", "desc": "DrafterBench - —ç—Ç–æ –æ—Ç–∫—Ä—ã—Ç—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM)
[17.07.2025 19:12] Using data from previous issue: {"categories": ["#optimization", "#architecture", "#open_source", "#training", "#dataset"], "emoji": "ü§ñ", "ru": {"title": "Ettin: –Ω–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è —ç–Ω–∫–æ–¥–µ—Ä–æ–≤ –∏ –¥–µ–∫–æ–¥–µ—Ä–æ–≤ –≤ –º–∞—à–∏–Ω–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ –Ω–∞–±–æ—Ä –º–æ–¥–µ–ª–µ–π Ettin, –≤–∫–ª—é—á–∞—é—â–∏–π —ç–Ω–∫–æ–¥–µ—Ä-—Ç–æ–ª—å–∫–æ –∏ –¥–µ–∫–æ–¥–µ—Ä-—Ç–æ–ª—å–∫–æ –∞—Ä—Ö–∏—Ç–µ–∫—Ç
[17.07.2025 19:12] Using data from previous issue: {"categories": ["#optimization", "#diffusion", "#multimodal", "#video"], "emoji": "üé¨", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –∞–Ω–∏–º–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –∫–æ–Ω—Ç—Ä–æ–ª–µ–º –¥–≤–∏–∂–µ–Ω–∏—è", "desc": "AnyI2V - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∞–Ω–∏–º–∞—Ü–∏–∏ —É—Å–ª–æ–≤–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–º–∏ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—è–º–∏ –¥–≤–∏–∂–µ–Ω–∏—è –±–µ–∑ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è. –û–Ω
[17.07.2025 19:12] Using data from previous issue: {"categories": ["#architecture", "#3d"], "emoji": "üé•", "ru": {"title": "–ï–¥–∏–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –∏ —Ç–æ—á–Ω–æ–≥–æ 3D-—Ç—Ä–µ–∫–∏–Ω–≥–∞ –≤ –º–æ–Ω–æ–∫—É–ª—è—Ä–Ω–æ–º –≤–∏–¥–µ–æ", "desc": "SpatialTrackerV2 - —ç—Ç–æ –º–µ—Ç–æ–¥ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è 3D —Ç–æ—á–µ–∫ –≤ –º–æ–Ω–æ–∫—É–ª—è—Ä–Ω—ã—Ö –≤–∏–¥–µ–æ —Å –ø—Ä—è–º–æ–π —Å–≤—è–∑—å—é. –û–Ω –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ —Ç–æ—á–µ–∫, –º–æ–Ω–æ–∫—É–ª—è—Ä–Ω—É—é –æ—Ü–µ–Ω–∫—É
[17.07.2025 19:12] Using data from previous issue: {"categories": ["#benchmark", "#architecture", "#optimization", "#training", "#long_context"], "emoji": "ü¶é", "ru": {"title": "Lizard: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ —Å –±–µ—Å–∫–æ–Ω–µ—á–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º", "desc": "Lizard - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –ª–∏–Ω–µ–∞—Ä–∏–∑–∞—Ü–∏–∏, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –≤ —Å—É–±–∫–≤–∞–¥—Ä–∞—Ç–∏—á
[17.07.2025 19:12] Using data from previous issue: {"categories": ["#rl", "#optimization", "#plp", "#training", "#transfer_learning", "#benchmark"], "emoji": "üõ†Ô∏è", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ LLM —á–µ—Ä–µ–∑ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ —Å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM), —Ñ–æ—Ä–º–∞—Ç–∏—Ä—É—è —Ç–æ–∫–µ–Ω—ã –∫–∞–∫ –º
[17.07.2025 19:12] Using data from previous issue: {"categories": ["#multilingual", "#machine_translation", "#architecture", "#low_resource", "#training"], "emoji": "üåê", "ru": {"title": "–°–µ–Ω—Ç–∏–º–µ–Ω—Ç-—É—Å–∏–ª–µ–Ω–Ω—ã–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã –ø–æ–∫–æ—Ä—è—é—Ç –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ—Å—Ç—å", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ —É–ª—É—á—à–µ–Ω–∏—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è —Å—É–±—ä
[17.07.2025 19:12] Using data from previous issue: {"categories": ["#reasoning", "#open_source", "#optimization", "#dataset", "#rl", "#training"], "emoji": "üß†", "ru": {"title": "RLEP: –£—Å–∫–æ—Ä–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ–≥–æ –æ–ø—ã—Ç–∞", "desc": "RLEP - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –ø–æ–≤—Ç–æ—Ä–Ω–æ–µ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ
[17.07.2025 19:12] Renaming data file.
[17.07.2025 19:12] Renaming previous data. hf_papers.json to ./d/2025-07-17.json
[17.07.2025 19:12] Saving new data file.
[17.07.2025 19:12] Generating page.
[17.07.2025 19:12] Renaming previous page.
[17.07.2025 19:12] Renaming previous data. index.html to ./d/2025-07-17.html
[17.07.2025 19:12] Writing result.
[17.07.2025 19:12] Renaming log file.
[17.07.2025 19:12] Renaming previous data. log.txt to ./logs/2025-07-17_last_log.txt
