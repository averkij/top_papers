[17.07.2025 06:19] Read previous papers.
[17.07.2025 06:19] Generating top page (month).
[17.07.2025 06:19] Writing top page (month).
[17.07.2025 07:15] Read previous papers.
[17.07.2025 07:15] Get feed.
[17.07.2025 07:15] Get page data from previous paper. URL: https://huggingface.co/papers/2507.09477
[17.07.2025 07:15] Get page data from previous paper. URL: https://huggingface.co/papers/2507.12465
[17.07.2025 07:15] Get page data from previous paper. URL: https://huggingface.co/papers/2507.12463
[17.07.2025 07:15] Get page data from previous paper. URL: https://huggingface.co/papers/2507.11527
[17.07.2025 07:15] Get page data from previous paper. URL: https://huggingface.co/papers/2507.12415
[17.07.2025 07:15] Get page data from previous paper. URL: https://huggingface.co/papers/2507.11949
[17.07.2025 07:15] Get page data from previous paper. URL: https://huggingface.co/papers/2507.09025
[17.07.2025 07:15] Get page data from previous paper. URL: https://huggingface.co/papers/2507.02857
[17.07.2025 07:15] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[17.07.2025 07:15] No deleted papers detected.
[17.07.2025 07:15] Downloading and parsing papers (pdf, html). Total: 8.
[17.07.2025 07:15] Downloading and parsing paper https://huggingface.co/papers/2507.09477.
[17.07.2025 07:15] Extra JSON file exists (./assets/json/2507.09477.json), skip PDF parsing.
[17.07.2025 07:15] Paper image links file exists (./assets/img_data/2507.09477.json), skip HTML parsing.
[17.07.2025 07:15] Success.
[17.07.2025 07:15] Downloading and parsing paper https://huggingface.co/papers/2507.12465.
[17.07.2025 07:15] Extra JSON file exists (./assets/json/2507.12465.json), skip PDF parsing.
[17.07.2025 07:15] Paper image links file exists (./assets/img_data/2507.12465.json), skip HTML parsing.
[17.07.2025 07:15] Success.
[17.07.2025 07:15] Downloading and parsing paper https://huggingface.co/papers/2507.12463.
[17.07.2025 07:15] Extra JSON file exists (./assets/json/2507.12463.json), skip PDF parsing.
[17.07.2025 07:15] Paper image links file exists (./assets/img_data/2507.12463.json), skip HTML parsing.
[17.07.2025 07:15] Success.
[17.07.2025 07:15] Downloading and parsing paper https://huggingface.co/papers/2507.11527.
[17.07.2025 07:15] Extra JSON file exists (./assets/json/2507.11527.json), skip PDF parsing.
[17.07.2025 07:15] Paper image links file exists (./assets/img_data/2507.11527.json), skip HTML parsing.
[17.07.2025 07:15] Success.
[17.07.2025 07:15] Downloading and parsing paper https://huggingface.co/papers/2507.12415.
[17.07.2025 07:15] Extra JSON file exists (./assets/json/2507.12415.json), skip PDF parsing.
[17.07.2025 07:15] Paper image links file exists (./assets/img_data/2507.12415.json), skip HTML parsing.
[17.07.2025 07:15] Success.
[17.07.2025 07:15] Downloading and parsing paper https://huggingface.co/papers/2507.11949.
[17.07.2025 07:15] Extra JSON file exists (./assets/json/2507.11949.json), skip PDF parsing.
[17.07.2025 07:15] Paper image links file exists (./assets/img_data/2507.11949.json), skip HTML parsing.
[17.07.2025 07:15] Success.
[17.07.2025 07:15] Downloading and parsing paper https://huggingface.co/papers/2507.09025.
[17.07.2025 07:15] Extra JSON file exists (./assets/json/2507.09025.json), skip PDF parsing.
[17.07.2025 07:15] Paper image links file exists (./assets/img_data/2507.09025.json), skip HTML parsing.
[17.07.2025 07:15] Success.
[17.07.2025 07:15] Downloading and parsing paper https://huggingface.co/papers/2507.02857.
[17.07.2025 07:15] Extra JSON file exists (./assets/json/2507.02857.json), skip PDF parsing.
[17.07.2025 07:15] Paper image links file exists (./assets/img_data/2507.02857.json), skip HTML parsing.
[17.07.2025 07:15] Success.
[17.07.2025 07:15] Enriching papers with extra data.
[17.07.2025 07:15] ********************************************************************************
[17.07.2025 07:15] Abstract 0. This survey integrates reasoning and retrieval in Large Language Models to improve factuality and multi-step inference, highlighting Synergized RAG-Reasoning frameworks and outlining future research directions.  					AI-generated summary 				 Retrieval-Augmented Generation (RAG) lifts the factuality...
[17.07.2025 07:15] ********************************************************************************
[17.07.2025 07:15] Abstract 1. PhysX addresses the lack of physical properties in 3D generative models by introducing PhysXNet, a physics-annotated dataset, and PhysXGen, a framework that integrates physical knowledge into 3D asset generation.  					AI-generated summary 				 3D modeling is moving from virtual to physical. Existin...
[17.07.2025 07:15] ********************************************************************************
[17.07.2025 07:15] Abstract 2. A large-scale benchmark, MMHU, is proposed for human behavior analysis in autonomous driving, featuring rich annotations and diverse data sources, and benchmarking multiple tasks including motion prediction and behavior question answering.  					AI-generated summary 				 Humans are integral componen...
[17.07.2025 07:15] ********************************************************************************
[17.07.2025 07:15] Abstract 3. DrafterBench is an open-source benchmark for evaluating LLM agents in technical drawing revision, assessing their capabilities in structured data comprehension, function execution, instruction following, and critical reasoning.  					AI-generated summary 				 Large Language Model (LLM) agents have s...
[17.07.2025 07:15] ********************************************************************************
[17.07.2025 07:15] Abstract 4. SWE-Perf is a benchmark for evaluating Large Language Models in code performance optimization using real-world repository data.  					AI-generated summary 				 Code performance optimization is paramount in real-world software engineering and critical for production-level systems. While Large Languag...
[17.07.2025 07:15] ********************************************************************************
[17.07.2025 07:15] Abstract 5. A diffusion-based generative framework, MOSPA, is introduced to model human motion in response to spatial audio, achieving state-of-the-art performance using the newly created SAM dataset.  					AI-generated summary 				 Enabling virtual humans to dynamically and realistically respond to diverse aud...
[17.07.2025 07:15] ********************************************************************************
[17.07.2025 07:15] Abstract 6. Lizard is a linearization framework that transforms Transformer-based LLMs into subquadratic architectures for efficient infinite-context generation, using a hybrid attention mechanism and hardware-aware training.  					AI-generated summary 				 We propose Lizard, a linearization framework that tran...
[17.07.2025 07:15] ********************************************************************************
[17.07.2025 07:15] Abstract 7. AnyI2V is a training-free framework that animates conditional images with user-defined motion trajectories, supporting various data types and enabling flexible video generation.  					AI-generated summary 				 Recent advancements in video generation, particularly in diffusion models, have driven not...
[17.07.2025 07:15] Read previous papers.
[17.07.2025 07:15] Generating reviews via LLM API.
[17.07.2025 07:15] Using data from previous issue: {"categories": ["#reasoning", "#multimodal", "#benchmark", "#survey", "#rag"], "emoji": "üß†", "ru": {"title": "–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –±–æ–ª–µ–µ –º–æ—â–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–≠—Ç–æ –æ–±–∑–æ—Ä –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LL
[17.07.2025 07:15] Using data from previous issue: {"categories": ["#architecture", "#synthetic", "#games", "#3d", "#open_source", "#dataset"], "emoji": "üß±", "ru": {"title": "–§–∏–∑–∏—á–µ—Å–∫–∏ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è 3D-–æ–±—ä–µ–∫—Ç–æ–≤", "desc": "PhysX –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ 3D-–æ–±—ä–µ–∫—Ç–æ–≤ —Å —É—á–µ—Ç–æ–º –∏—Ö —Ñ–∏–∑–∏—á–µ—Å–∫–∏—Ö —Å–≤–æ–π—Å—Ç–≤. –ê–≤—Ç–æ—Ä—ã —Å–æ–∑–¥–∞–ª–∏ –¥–∞—Ç–∞—Å–µ—Ç Phys
[17.07.2025 07:15] Using data from previous issue: {"categories": ["#benchmark", "#agents", "#dataset"], "emoji": "üöó", "ru": {"title": "MMHU: –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø–æ–≤–µ–¥–µ–Ω–∏—è —á–µ–ª–æ–≤–µ–∫–∞ –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ–≥–æ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –≤–æ–∂–¥–µ–Ω–∏—è", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ MMHU –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ø–æ–≤–µ–¥–µ–Ω–∏—è —á–µ–ª–æ–≤–µ–∫–∞ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –≤–æ–∂–¥–µ–Ω–∏—è. –û–Ω –≤–∫–ª—é—á–∞–µ—Ç –±–æ–≥
[17.07.2025 07:15] Using data from previous issue: {"categories": ["#reasoning", "#benchmark", "#open_source", "#long_context", "#agents"], "emoji": "üìê", "ru": {"title": "DrafterBench: –ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ LLM-–∞–≥–µ–Ω—Ç–æ–≤ –≤ –∏–Ω–∂–µ–Ω–µ—Ä–Ω–æ–º –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏", "desc": "DrafterBench - —ç—Ç–æ –æ—Ç–∫—Ä—ã—Ç—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM)
[17.07.2025 07:15] Using data from previous issue: {"categories": ["#optimization", "#benchmark", "#dataset"], "emoji": "üöÄ", "ru": {"title": "SWE-Perf: –ù–æ–≤—ã–π —Ä—É–±–µ–∂ –≤ –æ—Ü–µ–Ω–∫–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∫–æ–¥–∞ —Å –ø–æ–º–æ—â—å—é LLM", "desc": "SWE-Perf - —ç—Ç–æ –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≤ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∫–æ–¥–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∞–ª—å–Ω—ã—Ö 
[17.07.2025 07:15] Using data from previous issue: {"categories": ["#multimodal", "#diffusion", "#benchmark", "#open_source", "#dataset"], "emoji": "üéß", "ru": {"title": "–†–µ–∞–ª–∏—Å—Ç–∏—á–Ω–∞—è –∞–Ω–∏–º–∞—Ü–∏—è –¥–≤–∏–∂–µ–Ω–∏–π –ø–æ–¥ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ –∞—É–¥–∏–æ", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ MOSPA - –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—É—é –º–æ–¥–µ–ª—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∏—Ñ—Ñ—É–∑–∏–∏ –¥–ª—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –¥–≤–∏–∂–µ–Ω–∏–π —á–µ–ª–æ–≤–µ–∫–∞ –≤ –æ
[17.07.2025 07:15] Using data from previous issue: {"categories": ["#benchmark", "#architecture", "#optimization", "#training", "#long_context"], "emoji": "ü¶é", "ru": {"title": "Lizard: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ —Å –±–µ—Å–∫–æ–Ω–µ—á–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º", "desc": "Lizard - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –ª–∏–Ω–µ–∞—Ä–∏–∑–∞—Ü–∏–∏, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –≤ —Å—É–±–∫–≤–∞–¥—Ä–∞—Ç–∏—á
[17.07.2025 07:15] Using data from previous issue: {"categories": ["#optimization", "#diffusion", "#multimodal", "#video"], "emoji": "üé¨", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –∞–Ω–∏–º–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –∫–æ–Ω—Ç—Ä–æ–ª–µ–º –¥–≤–∏–∂–µ–Ω–∏—è", "desc": "AnyI2V - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∞–Ω–∏–º–∞—Ü–∏–∏ —É—Å–ª–æ–≤–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–º–∏ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—è–º–∏ –¥–≤–∏–∂–µ–Ω–∏—è –±–µ–∑ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è. –û–Ω
[17.07.2025 07:15] Renaming data file.
[17.07.2025 07:15] Renaming previous data. hf_papers.json to ./d/2025-07-17.json
[17.07.2025 07:15] Saving new data file.
[17.07.2025 07:15] Generating page.
[17.07.2025 07:15] Renaming previous page.
[17.07.2025 07:15] Renaming previous data. index.html to ./d/2025-07-17.html
[17.07.2025 07:15] Writing result.
[17.07.2025 07:15] Renaming log file.
[17.07.2025 07:15] Renaming previous data. log.txt to ./logs/2025-07-17_last_log.txt
