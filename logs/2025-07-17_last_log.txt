[17.07.2025 12:23] Read previous papers.
[17.07.2025 12:23] Generating top page (month).
[17.07.2025 12:23] Writing top page (month).
[17.07.2025 13:31] Read previous papers.
[17.07.2025 13:31] Get feed.
[17.07.2025 13:31] Get page data from previous paper. URL: https://huggingface.co/papers/2507.09477
[17.07.2025 13:31] Get page data from previous paper. URL: https://huggingface.co/papers/2507.12465
[17.07.2025 13:31] Get page data from previous paper. URL: https://huggingface.co/papers/2507.11949
[17.07.2025 13:31] Get page data from previous paper. URL: https://huggingface.co/papers/2507.12463
[17.07.2025 13:31] Get page data from previous paper. URL: https://huggingface.co/papers/2507.12415
[17.07.2025 13:31] Get page data from previous paper. URL: https://huggingface.co/papers/2507.11527
[17.07.2025 13:31] Extract page data from URL. URL: https://huggingface.co/papers/2507.11412
[17.07.2025 13:31] Get page data from previous paper. URL: https://huggingface.co/papers/2507.12462
[17.07.2025 13:31] Get page data from previous paper. URL: https://huggingface.co/papers/2507.02857
[17.07.2025 13:31] Extract page data from URL. URL: https://huggingface.co/papers/2507.11764
[17.07.2025 13:31] Get page data from previous paper. URL: https://huggingface.co/papers/2507.09025
[17.07.2025 13:31] Get page data from previous paper. URL: https://huggingface.co/papers/2507.07451
[17.07.2025 13:31] Get page data from previous paper. URL: https://huggingface.co/papers/2507.05065
[17.07.2025 13:31] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[17.07.2025 13:31] No deleted papers detected.
[17.07.2025 13:31] Downloading and parsing papers (pdf, html). Total: 13.
[17.07.2025 13:31] Downloading and parsing paper https://huggingface.co/papers/2507.09477.
[17.07.2025 13:31] Extra JSON file exists (./assets/json/2507.09477.json), skip PDF parsing.
[17.07.2025 13:31] Paper image links file exists (./assets/img_data/2507.09477.json), skip HTML parsing.
[17.07.2025 13:31] Success.
[17.07.2025 13:31] Downloading and parsing paper https://huggingface.co/papers/2507.12465.
[17.07.2025 13:31] Extra JSON file exists (./assets/json/2507.12465.json), skip PDF parsing.
[17.07.2025 13:31] Paper image links file exists (./assets/img_data/2507.12465.json), skip HTML parsing.
[17.07.2025 13:31] Success.
[17.07.2025 13:31] Downloading and parsing paper https://huggingface.co/papers/2507.11949.
[17.07.2025 13:31] Extra JSON file exists (./assets/json/2507.11949.json), skip PDF parsing.
[17.07.2025 13:31] Paper image links file exists (./assets/img_data/2507.11949.json), skip HTML parsing.
[17.07.2025 13:31] Success.
[17.07.2025 13:31] Downloading and parsing paper https://huggingface.co/papers/2507.12463.
[17.07.2025 13:31] Extra JSON file exists (./assets/json/2507.12463.json), skip PDF parsing.
[17.07.2025 13:31] Paper image links file exists (./assets/img_data/2507.12463.json), skip HTML parsing.
[17.07.2025 13:31] Success.
[17.07.2025 13:31] Downloading and parsing paper https://huggingface.co/papers/2507.12415.
[17.07.2025 13:31] Extra JSON file exists (./assets/json/2507.12415.json), skip PDF parsing.
[17.07.2025 13:31] Paper image links file exists (./assets/img_data/2507.12415.json), skip HTML parsing.
[17.07.2025 13:31] Success.
[17.07.2025 13:31] Downloading and parsing paper https://huggingface.co/papers/2507.11527.
[17.07.2025 13:31] Extra JSON file exists (./assets/json/2507.11527.json), skip PDF parsing.
[17.07.2025 13:31] Paper image links file exists (./assets/img_data/2507.11527.json), skip HTML parsing.
[17.07.2025 13:31] Success.
[17.07.2025 13:31] Downloading and parsing paper https://huggingface.co/papers/2507.11412.
[17.07.2025 13:31] Downloading paper 2507.11412 from http://arxiv.org/pdf/2507.11412v1...
[17.07.2025 13:31] Extracting affiliations from text.
[17.07.2025 13:31] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Seq vs Seq: An Open Suite of Paired Encoders and Decoders Orion Weller ι Kathryn Ricci ι Marc Marone ι Antoine Chaffin α Dawn Lawrie ι Benjamin Van Durme ι 5 2 0 2 J 5 1 ] . [ 1 2 1 4 1 1 . 7 0 5 2 : r ι Johns Hopkins University α LightOn "
[17.07.2025 13:31] Response: ```python
["Johns Hopkins University", "LightOn"]
```
[17.07.2025 13:31] Deleting PDF ./assets/pdf/2507.11412.pdf.
[17.07.2025 13:31] Success.
[17.07.2025 13:31] Downloading and parsing paper https://huggingface.co/papers/2507.12462.
[17.07.2025 13:31] Extra JSON file exists (./assets/json/2507.12462.json), skip PDF parsing.
[17.07.2025 13:31] Paper image links file exists (./assets/img_data/2507.12462.json), skip HTML parsing.
[17.07.2025 13:31] Success.
[17.07.2025 13:31] Downloading and parsing paper https://huggingface.co/papers/2507.02857.
[17.07.2025 13:31] Extra JSON file exists (./assets/json/2507.02857.json), skip PDF parsing.
[17.07.2025 13:31] Paper image links file exists (./assets/img_data/2507.02857.json), skip HTML parsing.
[17.07.2025 13:31] Success.
[17.07.2025 13:31] Downloading and parsing paper https://huggingface.co/papers/2507.11764.
[17.07.2025 13:31] Downloading paper 2507.11764 from http://arxiv.org/pdf/2507.11764v1...
[17.07.2025 13:31] Extracting affiliations from text.
[17.07.2025 13:31] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"AI Wizards at CheckThat! 2025: Enhancing Transformer-Based Embeddings with Sentiment for Subjectivity Detection in News Articles Notebook for the CheckThat! Lab at CLEF 2025 Matteo Fasulo1,*,, Luca Babboni1, and Luca Tedeschini1, 1Department of Computer Science and Engineering (DISI) - University of Bologna Abstract This paper presents AI Wizards participation in the CLEF 2025 CheckThat! Lab Task 1: Subjectivity Detection in News Articles, classifying sentences as subjective/objective in monolingual, multilingual, and zero-shot settings. Training/development datasets were provided for Arabic, German, English, Italian, and Bulgarian; final evaluation included additional unseen languages (e.g., Greek, Romanian, Polish, Ukrainian) to assess generalization. Our primary strategy enhanced transformer-based classifiers by integrating sentiment scores, derived from an auxiliary model, with sentence representations, aiming to improve upon standard fine-tuning. We explored this sentimentaugmented architecture with mDeBERTaV3-base, ModernBERT-base (English), and Llama3.2-1B. To address class imbalance, prevalent across languages, we employed decision threshold calibration optimized on the development set. Our experiments show sentiment feature integration significantly boosts performance, especially subjective F1 score. This framework led to high rankings, notably 1st for Greek (Macro F1 = 0.51). Keywords subjectivity detection, transformers, multilinguality, sentiment-based features, threshold calibration 1. Introduction Our work addresses subjectivity detection as defined in Task 1 [1] of the CLEF 2025 CheckThat! Lab. An overview of the CheckThat! Lab and its constituent tasks can be found in [2, 3]. Specifically, Task 1 challenges systems to classify sentences from news articles as subjective (SUBJ) or objective (OBJ). This capability is vital for efforts to combat misinformation and improve fact-checking, as the ability to separate opinion from factual claims is essential,"
[17.07.2025 13:31] Response: ```python
["Department of Computer Science and Engineering (DISI) - University of Bologna"]
```
[17.07.2025 13:31] Deleting PDF ./assets/pdf/2507.11764.pdf.
[17.07.2025 13:31] Success.
[17.07.2025 13:31] Downloading and parsing paper https://huggingface.co/papers/2507.09025.
[17.07.2025 13:31] Extra JSON file exists (./assets/json/2507.09025.json), skip PDF parsing.
[17.07.2025 13:31] Paper image links file exists (./assets/img_data/2507.09025.json), skip HTML parsing.
[17.07.2025 13:31] Success.
[17.07.2025 13:31] Downloading and parsing paper https://huggingface.co/papers/2507.07451.
[17.07.2025 13:31] Extra JSON file exists (./assets/json/2507.07451.json), skip PDF parsing.
[17.07.2025 13:31] Paper image links file exists (./assets/img_data/2507.07451.json), skip HTML parsing.
[17.07.2025 13:31] Success.
[17.07.2025 13:31] Downloading and parsing paper https://huggingface.co/papers/2507.05065.
[17.07.2025 13:31] Extra JSON file exists (./assets/json/2507.05065.json), skip PDF parsing.
[17.07.2025 13:31] Paper image links file exists (./assets/img_data/2507.05065.json), skip HTML parsing.
[17.07.2025 13:31] Success.
[17.07.2025 13:31] Enriching papers with extra data.
[17.07.2025 13:31] ********************************************************************************
[17.07.2025 13:31] Abstract 0. This survey integrates reasoning and retrieval in Large Language Models to improve factuality and multi-step inference, highlighting Synergized RAG-Reasoning frameworks and outlining future research directions.  					AI-generated summary 				 Retrieval-Augmented Generation (RAG) lifts the factuality...
[17.07.2025 13:31] ********************************************************************************
[17.07.2025 13:31] Abstract 1. PhysX addresses the lack of physical properties in 3D generative models by introducing PhysXNet, a physics-annotated dataset, and PhysXGen, a framework that integrates physical knowledge into 3D asset generation.  					AI-generated summary 				 3D modeling is moving from virtual to physical. Existin...
[17.07.2025 13:31] ********************************************************************************
[17.07.2025 13:31] Abstract 2. A diffusion-based generative framework, MOSPA, is introduced to model human motion in response to spatial audio, achieving state-of-the-art performance using the newly created SAM dataset.  					AI-generated summary 				 Enabling virtual humans to dynamically and realistically respond to diverse aud...
[17.07.2025 13:31] ********************************************************************************
[17.07.2025 13:31] Abstract 3. A large-scale benchmark, MMHU, is proposed for human behavior analysis in autonomous driving, featuring rich annotations and diverse data sources, and benchmarking multiple tasks including motion prediction and behavior question answering.  					AI-generated summary 				 Humans are integral componen...
[17.07.2025 13:31] ********************************************************************************
[17.07.2025 13:31] Abstract 4. SWE-Perf is a benchmark for evaluating Large Language Models in code performance optimization using real-world repository data.  					AI-generated summary 				 Code performance optimization is paramount in real-world software engineering and critical for production-level systems. While Large Languag...
[17.07.2025 13:31] ********************************************************************************
[17.07.2025 13:31] Abstract 5. DrafterBench is an open-source benchmark for evaluating LLM agents in technical drawing revision, assessing their capabilities in structured data comprehension, function execution, instruction following, and critical reasoning.  					AI-generated summary 				 Large Language Model (LLM) agents have s...
[17.07.2025 13:31] ********************************************************************************
[17.07.2025 13:31] Abstract 6. The large language model (LLM) community focuses almost exclusively on decoder-only language models, since they are easier to use for text generation. However, a large subset of the community still uses encoder-only models for tasks such as classification or retrieval. Previous work has attempted to...
[17.07.2025 13:31] ********************************************************************************
[17.07.2025 13:31] Abstract 7. SpatialTrackerV2 is a feed-forward 3D point tracking method for monocular videos that integrates point tracking, monocular depth, and camera pose estimation into a unified, end-to-end architecture, achieving high performance and speed.  					AI-generated summary 				 We present SpatialTrackerV2, a f...
[17.07.2025 13:31] ********************************************************************************
[17.07.2025 13:31] Abstract 8. AnyI2V is a training-free framework that animates conditional images with user-defined motion trajectories, supporting various data types and enabling flexible video generation.  					AI-generated summary 				 Recent advancements in video generation, particularly in diffusion models, have driven not...
[17.07.2025 13:31] ********************************************************************************
[17.07.2025 13:31] Abstract 9. Sentiment-augmented transformer-based classifiers improve subjectivity detection in multilingual and zero-shot settings, achieving high performance and ranking first for Greek.  					AI-generated summary 				 This paper presents AI Wizards' participation in the CLEF 2025 CheckThat! Lab Task 1: Subje...
[17.07.2025 13:31] ********************************************************************************
[17.07.2025 13:31] Abstract 10. Lizard is a linearization framework that transforms Transformer-based LLMs into subquadratic architectures for efficient infinite-context generation, using a hybrid attention mechanism and hardware-aware training.  					AI-generated summary 				 We propose Lizard, a linearization framework that tran...
[17.07.2025 13:31] ********************************************************************************
[17.07.2025 13:31] Abstract 11. RLEP, a reinforcement learning framework with experience replay, enhances large language model training by focusing on high-quality examples, leading to faster convergence and improved performance on math-related benchmarks.  					AI-generated summary 				 Reinforcement learning (RL) for large langu...
[17.07.2025 13:31] ********************************************************************************
[17.07.2025 13:31] Abstract 12. A new approach formats tokens as a multi-turn interaction trace with a stateful tool for training Large Language Models, enabling faster sampling and denser reward signals for tasks like repairing Python code.  					AI-generated summary 				 Recent advances have established a new machine learning pa...
[17.07.2025 13:31] Read previous papers.
[17.07.2025 13:31] Generating reviews via LLM API.
[17.07.2025 13:31] Using data from previous issue: {"categories": ["#reasoning", "#multimodal", "#benchmark", "#survey", "#rag"], "emoji": "🧠", "ru": {"title": "Объединение извлечения информации и рассуждений для создания более мощных языковых моделей", "desc": "Это обзор интегрирует рассуждения и извлечение информации в больших языковых моделях (LL
[17.07.2025 13:31] Using data from previous issue: {"categories": ["#architecture", "#synthetic", "#games", "#3d", "#open_source", "#dataset"], "emoji": "🧱", "ru": {"title": "Физически достоверная генерация 3D-объектов", "desc": "PhysX представляет собой новый подход к генерации 3D-объектов с учетом их физических свойств. Авторы создали датасет Phys
[17.07.2025 13:31] Using data from previous issue: {"categories": ["#multimodal", "#diffusion", "#benchmark", "#open_source", "#dataset"], "emoji": "🎧", "ru": {"title": "Реалистичная анимация движений под пространственное аудио", "desc": "Исследователи представили MOSPA - генеративную модель на основе диффузии для моделирования движений человека в о
[17.07.2025 13:31] Using data from previous issue: {"categories": ["#benchmark", "#agents", "#dataset"], "emoji": "🚗", "ru": {"title": "MMHU: Комплексный анализ поведения человека для безопасного автономного вождения", "desc": "Представлен крупномасштабный бенчмарк MMHU для анализа поведения человека в контексте автономного вождения. Он включает бог
[17.07.2025 13:31] Using data from previous issue: {"categories": ["#optimization", "#benchmark", "#dataset"], "emoji": "🚀", "ru": {"title": "SWE-Perf: Новый рубеж в оценке оптимизации кода с помощью LLM", "desc": "SWE-Perf - это бенчмарк для оценки способностей больших языковых моделей (LLM) в оптимизации производительности кода на основе реальных 
[17.07.2025 13:31] Using data from previous issue: {"categories": ["#reasoning", "#benchmark", "#open_source", "#long_context", "#agents"], "emoji": "📐", "ru": {"title": "DrafterBench: Комплексная оценка LLM-агентов в инженерном проектировании", "desc": "DrafterBench - это открытый бенчмарк для оценки агентов на основе больших языковых моделей (LLM)
[17.07.2025 13:31] Querying the API.
[17.07.2025 13:31] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

The large language model (LLM) community focuses almost exclusively on decoder-only language models, since they are easier to use for text generation. However, a large subset of the community still uses encoder-only models for tasks such as classification or retrieval. Previous work has attempted to compare these architectures, but is forced to make comparisons with models that have different numbers of parameters, training techniques, and datasets. We introduce the SOTA open-data Ettin suite of models: paired encoder-only and decoder-only models ranging from 17 million parameters to 1 billion, trained on up to 2 trillion tokens. Using the same recipe for both encoder-only and decoder-only models produces SOTA recipes in both categories for their respective sizes, beating ModernBERT as an encoder and Llama 3.2 and SmolLM2 as decoders. Like previous work, we find that encoder-only models excel at classification and retrieval tasks while decoders excel at generative tasks. However, we show that adapting a decoder model to encoder tasks (and vice versa) through continued training is subpar compared to using only the reverse objective (i.e. a 400M encoder outperforms a 1B decoder on MNLI, and vice versa for generative tasks). We open-source all artifacts of this study including training data, training order segmented by checkpoint, and 200+ checkpoints to allow future work to analyze or extend all aspects of training.
[17.07.2025 13:32] Response: {
  "desc": "Исследователи представили набор моделей Ettin, включающий энкодер-только и декодер-только архитектуры от 17 миллионов до 1 миллиарда параметров. Модели обучены на 2 триллионах токенов и превосходят существующие аналоги в своих категориях. Подтверждено, что энкодеры лучше справляются с задачами классификации и поиска, а декодеры - с генеративными задачами. Показано, что адаптация декодера к задачам энкодера (и наоборот) путем дообучения уступает использованию изначально правильной архитектуры.",
  "emoji": "🤖",
  "title": "Ettin: новый стандарт для энкодеров и декодеров в машинном обучении"
}
[17.07.2025 13:32] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"The large language model (LLM) community focuses almost exclusively on decoder-only language models, since they are easier to use for text generation. However, a large subset of the community still uses encoder-only models for tasks such as classification or retrieval. Previous work has attempted to compare these architectures, but is forced to make comparisons with models that have different numbers of parameters, training techniques, and datasets. We introduce the SOTA open-data Ettin suite of models: paired encoder-only and decoder-only models ranging from 17 million parameters to 1 billion, trained on up to 2 trillion tokens. Using the same recipe for both encoder-only and decoder-only models produces SOTA recipes in both categories for their respective sizes, beating ModernBERT as an encoder and Llama 3.2 and SmolLM2 as decoders. Like previous work, we find that encoder-only models excel at classification and retrieval tasks while decoders excel at generative tasks. However, we show that adapting a decoder model to encoder tasks (and vice versa) through continued training is subpar compared to using only the reverse objective (i.e. a 400M encoder outperforms a 1B decoder on MNLI, and vice versa for generative tasks). We open-source all artifacts of this study including training data, training order segmented by checkpoint, and 200+ checkpoints to allow future work to analyze or extend all aspects of training."

[17.07.2025 13:32] Response: ```python
['DATASET', 'TRAINING', 'ARCHITECTURE']
```
[17.07.2025 13:32] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"The large language model (LLM) community focuses almost exclusively on decoder-only language models, since they are easier to use for text generation. However, a large subset of the community still uses encoder-only models for tasks such as classification or retrieval. Previous work has attempted to compare these architectures, but is forced to make comparisons with models that have different numbers of parameters, training techniques, and datasets. We introduce the SOTA open-data Ettin suite of models: paired encoder-only and decoder-only models ranging from 17 million parameters to 1 billion, trained on up to 2 trillion tokens. Using the same recipe for both encoder-only and decoder-only models produces SOTA recipes in both categories for their respective sizes, beating ModernBERT as an encoder and Llama 3.2 and SmolLM2 as decoders. Like previous work, we find that encoder-only models excel at classification and retrieval tasks while decoders excel at generative tasks. However, we show that adapting a decoder model to encoder tasks (and vice versa) through continued training is subpar compared to using only the reverse objective (i.e. a 400M encoder outperforms a 1B decoder on MNLI, and vice versa for generative tasks). We open-source all artifacts of this study including training data, training order segmented by checkpoint, and 200+ checkpoints to allow future work to analyze or extend all aspects of training."

[17.07.2025 13:32] Response: ```python
['OPEN_SOURCE', 'OPTIMIZATION']
```
[17.07.2025 13:32] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper discusses the comparison between encoder-only and decoder-only language models in the context of machine learning. It introduces the Ettin suite, which consists of paired models of both types, ensuring they are trained under the same conditions for a fair comparison. The findings reveal that encoder-only models are better suited for classification and retrieval tasks, while decoder-only models excel in text generation. Additionally, the study shows that adapting models for different tasks through continued training is less effective than using models specifically designed for those tasks.","title":"Unlocking the Power of Encoder and Decoder Models in NLP"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper discusses the comparison between encoder-only and decoder-only language models in the context of machine learning. It introduces the Ettin suite, which consists of paired models of both types, ensuring they are trained under the same conditions for a fair comparison. The findings reveal that encoder-only models are better suited for classification and retrieval tasks, while decoder-only models excel in text generation. Additionally, the study shows that adapting models for different tasks through continued training is less effective than using models specifically designed for those tasks.', title='Unlocking the Power of Encoder and Decoder Models in NLP'))
[17.07.2025 13:32] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本文介绍了一种新的模型套件Ettin，包含配对的编码器和解码器模型，参数范围从1700万到10亿，训练数据达到2万亿个标记。研究表明，编码器模型在分类和检索任务中表现优异，而解码器模型在生成任务中更具优势。通过相同的训练方法，Ettin模型在各自的类别中达到了最新的性能，超越了现代的BERT和Llama 3.2等模型。我们还发现，继续训练解码器模型以适应编码器任务的效果不如直接使用编码器模型。","title":"Ettin模型：编码器与解码器的完美结合"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本文介绍了一种新的模型套件Ettin，包含配对的编码器和解码器模型，参数范围从1700万到10亿，训练数据达到2万亿个标记。研究表明，编码器模型在分类和检索任务中表现优异，而解码器模型在生成任务中更具优势。通过相同的训练方法，Ettin模型在各自的类别中达到了最新的性能，超越了现代的BERT和Llama 3.2等模型。我们还发现，继续训练解码器模型以适应编码器任务的效果不如直接使用编码器模型。', title='Ettin模型：编码器与解码器的完美结合'))
[17.07.2025 13:32] Using data from previous issue: {"categories": ["#architecture", "#3d"], "emoji": "🎥", "ru": {"title": "Единая архитектура для быстрого и точного 3D-трекинга в монокулярном видео", "desc": "SpatialTrackerV2 - это метод отслеживания 3D точек в монокулярных видео с прямой связью. Он объединяет отслеживание точек, монокулярную оценку
[17.07.2025 13:32] Using data from previous issue: {"categories": ["#optimization", "#diffusion", "#multimodal", "#video"], "emoji": "🎬", "ru": {"title": "Универсальная анимация изображений с контролем движения", "desc": "AnyI2V - это фреймворк для анимации условных изображений с пользовательскими траекториями движения без необходимости обучения. Он
[17.07.2025 13:32] Querying the API.
[17.07.2025 13:32] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Sentiment-augmented transformer-based classifiers improve subjectivity detection in multilingual and zero-shot settings, achieving high performance and ranking first for Greek.  					AI-generated summary 				 This paper presents AI Wizards' participation in the CLEF 2025 CheckThat! Lab Task 1: Subjectivity Detection in News Articles, classifying sentences as subjective/objective in monolingual, multilingual, and zero-shot settings. Training/development datasets were provided for Arabic, German, English, Italian, and Bulgarian; final evaluation included additional unseen languages (e.g., Greek, Romanian, Polish, Ukrainian) to assess generalization. Our primary strategy enhanced transformer-based classifiers by integrating sentiment scores, derived from an auxiliary model, with sentence representations, aiming to improve upon standard fine-tuning. We explored this sentiment-augmented architecture with mDeBERTaV3-base, ModernBERT-base (English), and Llama3.2-1B. To address class imbalance, prevalent across languages, we employed decision threshold calibration optimized on the development set. Our experiments show sentiment feature integration significantly boosts performance, especially subjective F1 score. This framework led to high rankings, notably 1st for Greek (Macro F1 = 0.51).
[17.07.2025 13:32] Response: {
  "desc": "Статья представляет метод улучшения классификаторов на основе трансформеров для обнаружения субъективности в многоязычных и zero-shot сценариях. Авторы интегрировали оценки сентимента, полученные от вспомогательной модели, в представления предложений. Эксперименты показали, что такой подход значительно повышает производительность, особенно F1-меру для субъективного класса. Метод продемонстрировал высокие результаты в соревновании CLEF 2025 CheckThat!, заняв первое место для греческого языка.",
  "emoji": "🌐",
  "title": "Сентимент-усиленные трансформеры покоряют многоязычность"
}
[17.07.2025 13:32] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Sentiment-augmented transformer-based classifiers improve subjectivity detection in multilingual and zero-shot settings, achieving high performance and ranking first for Greek.  					AI-generated summary 				 This paper presents AI Wizards' participation in the CLEF 2025 CheckThat! Lab Task 1: Subjectivity Detection in News Articles, classifying sentences as subjective/objective in monolingual, multilingual, and zero-shot settings. Training/development datasets were provided for Arabic, German, English, Italian, and Bulgarian; final evaluation included additional unseen languages (e.g., Greek, Romanian, Polish, Ukrainian) to assess generalization. Our primary strategy enhanced transformer-based classifiers by integrating sentiment scores, derived from an auxiliary model, with sentence representations, aiming to improve upon standard fine-tuning. We explored this sentiment-augmented architecture with mDeBERTaV3-base, ModernBERT-base (English), and Llama3.2-1B. To address class imbalance, prevalent across languages, we employed decision threshold calibration optimized on the development set. Our experiments show sentiment feature integration significantly boosts performance, especially subjective F1 score. This framework led to high rankings, notably 1st for Greek (Macro F1 = 0.51)."

[17.07.2025 13:32] Response: ```python
['MULTILINGUAL', 'TRAINING', 'ARCHITECTURE']
```
[17.07.2025 13:32] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Sentiment-augmented transformer-based classifiers improve subjectivity detection in multilingual and zero-shot settings, achieving high performance and ranking first for Greek.  					AI-generated summary 				 This paper presents AI Wizards' participation in the CLEF 2025 CheckThat! Lab Task 1: Subjectivity Detection in News Articles, classifying sentences as subjective/objective in monolingual, multilingual, and zero-shot settings. Training/development datasets were provided for Arabic, German, English, Italian, and Bulgarian; final evaluation included additional unseen languages (e.g., Greek, Romanian, Polish, Ukrainian) to assess generalization. Our primary strategy enhanced transformer-based classifiers by integrating sentiment scores, derived from an auxiliary model, with sentence representations, aiming to improve upon standard fine-tuning. We explored this sentiment-augmented architecture with mDeBERTaV3-base, ModernBERT-base (English), and Llama3.2-1B. To address class imbalance, prevalent across languages, we employed decision threshold calibration optimized on the development set. Our experiments show sentiment feature integration significantly boosts performance, especially subjective F1 score. This framework led to high rankings, notably 1st for Greek (Macro F1 = 0.51)."

[17.07.2025 13:32] Response: ```python
['TRANSLATION', 'LOW_RESOURCE']
```
[17.07.2025 13:32] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper discusses a method to improve the detection of subjective and objective sentences in news articles using advanced transformer-based classifiers. The authors enhanced these classifiers by incorporating sentiment scores from an auxiliary model, which helped in better understanding the context of sentences. They tested their approach on multiple languages, including unseen ones, to ensure the model\'s ability to generalize. The results showed that this sentiment-augmented method significantly improved performance, particularly in identifying subjective content, achieving top rankings for Greek.","title":"Boosting Subjectivity Detection with Sentiment-Enhanced Transformers"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="This paper discusses a method to improve the detection of subjective and objective sentences in news articles using advanced transformer-based classifiers. The authors enhanced these classifiers by incorporating sentiment scores from an auxiliary model, which helped in better understanding the context of sentences. They tested their approach on multiple languages, including unseen ones, to ensure the model's ability to generalize. The results showed that this sentiment-augmented method significantly improved performance, particularly in identifying subjective content, achieving top rankings for Greek.", title='Boosting Subjectivity Detection with Sentiment-Enhanced Transformers'))
[17.07.2025 13:32] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本文介绍了AI Wizards在CLEF 2025 CheckThat! Lab Task 1中的参与，旨在对新闻文章中的句子进行主观性检测。我们提出了一种增强型的变换器分类器，通过将情感分数与句子表示结合，来提高模型在单语、多语和零样本设置下的表现。实验结果表明，情感特征的整合显著提升了主观性F1分数，尤其是在希腊语中取得了第一名的优异成绩。我们还通过优化决策阈值来解决各语言间的类别不平衡问题。","title":"情感增强变换器提升多语言主观性检测"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本文介绍了AI Wizards在CLEF 2025 CheckThat! Lab Task 1中的参与，旨在对新闻文章中的句子进行主观性检测。我们提出了一种增强型的变换器分类器，通过将情感分数与句子表示结合，来提高模型在单语、多语和零样本设置下的表现。实验结果表明，情感特征的整合显著提升了主观性F1分数，尤其是在希腊语中取得了第一名的优异成绩。我们还通过优化决策阈值来解决各语言间的类别不平衡问题。', title='情感增强变换器提升多语言主观性检测'))
[17.07.2025 13:32] Using data from previous issue: {"categories": ["#benchmark", "#architecture", "#optimization", "#training", "#long_context"], "emoji": "🦎", "ru": {"title": "Lizard: эффективные языковые модели с бесконечным контекстом", "desc": "Lizard - это фреймворк линеаризации, который преобразует трансформерные языковые модели в субквадратич
[17.07.2025 13:32] Using data from previous issue: {"categories": ["#reasoning", "#open_source", "#optimization", "#dataset", "#rl", "#training"], "emoji": "🧠", "ru": {"title": "RLEP: Ускорение обучения языковых моделей через воспроизведение успешного опыта", "desc": "RLEP - это фреймворк обучения с подкреплением, использующий повторное воспроизведе
[17.07.2025 13:32] Using data from previous issue: {"categories": ["#rl", "#optimization", "#plp", "#training", "#transfer_learning", "#benchmark"], "emoji": "🛠️", "ru": {"title": "Эффективное обучение LLM через взаимодействие с инструментами", "desc": "Статья предлагает новый подход к обучению больших языковых моделей (LLM), форматируя токены как м
[17.07.2025 13:32] Renaming data file.
[17.07.2025 13:32] Renaming previous data. hf_papers.json to ./d/2025-07-17.json
[17.07.2025 13:32] Saving new data file.
[17.07.2025 13:32] Generating page.
[17.07.2025 13:32] Renaming previous page.
[17.07.2025 13:32] Renaming previous data. index.html to ./d/2025-07-17.html
[17.07.2025 13:32] Writing result.
[17.07.2025 13:32] Renaming log file.
[17.07.2025 13:32] Renaming previous data. log.txt to ./logs/2025-07-17_last_log.txt
