[18.11.2024 21:09] Read previous papers.
[18.11.2024 21:09] Generating top page (month).
[18.11.2024 21:09] Writing top page (month).
[18.11.2024 22:10] Read previous papers.
[18.11.2024 22:10] Get feed.
[18.11.2024 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2411.10440
[18.11.2024 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2411.06558
[18.11.2024 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2411.08033
[18.11.2024 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2411.10323
[18.11.2024 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2411.10332
[18.11.2024 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2411.10083
[18.11.2024 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2411.10438
[18.11.2024 22:10] Downloading and parsing papers (pdf, html). Total: 7.
[18.11.2024 22:10] Downloading and parsing paper https://huggingface.co/papers/2411.10440.
[18.11.2024 22:10] Extra JSON file exists (./assets/json/2411.10440.json), skip PDF parsing.
[18.11.2024 22:10] Paper image links file exists (./assets/img_data/2411.10440.json), skip HTML parsing.
[18.11.2024 22:10] Success.
[18.11.2024 22:10] Downloading and parsing paper https://huggingface.co/papers/2411.06558.
[18.11.2024 22:10] Extra JSON file exists (./assets/json/2411.06558.json), skip PDF parsing.
[18.11.2024 22:10] Paper image links file exists (./assets/img_data/2411.06558.json), skip HTML parsing.
[18.11.2024 22:10] Success.
[18.11.2024 22:10] Downloading and parsing paper https://huggingface.co/papers/2411.08033.
[18.11.2024 22:10] Extra JSON file exists (./assets/json/2411.08033.json), skip PDF parsing.
[18.11.2024 22:10] Paper image links file exists (./assets/img_data/2411.08033.json), skip HTML parsing.
[18.11.2024 22:10] Success.
[18.11.2024 22:10] Downloading and parsing paper https://huggingface.co/papers/2411.10323.
[18.11.2024 22:10] Extra JSON file exists (./assets/json/2411.10323.json), skip PDF parsing.
[18.11.2024 22:10] Paper image links file exists (./assets/img_data/2411.10323.json), skip HTML parsing.
[18.11.2024 22:10] Success.
[18.11.2024 22:10] Downloading and parsing paper https://huggingface.co/papers/2411.10332.
[18.11.2024 22:10] Extra JSON file exists (./assets/json/2411.10332.json), skip PDF parsing.
[18.11.2024 22:10] Paper image links file exists (./assets/img_data/2411.10332.json), skip HTML parsing.
[18.11.2024 22:10] Success.
[18.11.2024 22:10] Downloading and parsing paper https://huggingface.co/papers/2411.10083.
[18.11.2024 22:10] Extra JSON file exists (./assets/json/2411.10083.json), skip PDF parsing.
[18.11.2024 22:10] Paper image links file exists (./assets/img_data/2411.10083.json), skip HTML parsing.
[18.11.2024 22:10] Success.
[18.11.2024 22:10] Downloading and parsing paper https://huggingface.co/papers/2411.10438.
[18.11.2024 22:10] Extra JSON file exists (./assets/json/2411.10438.json), skip PDF parsing.
[18.11.2024 22:10] Paper image links file exists (./assets/img_data/2411.10438.json), skip HTML parsing.
[18.11.2024 22:10] Success.
[18.11.2024 22:10] Enriching papers with extra data.
[18.11.2024 22:10] ********************************************************************************
[18.11.2024 22:10] Abstract 0. Large language models have demonstrated substantial advancements in reasoning capabilities, particularly through inference-time scaling, as illustrated by models such as OpenAI's o1. However, current Vision-Language Models (VLMs) often struggle to perform systematic and structured reasoning, especia...
[18.11.2024 22:10] ********************************************************************************
[18.11.2024 22:10] Abstract 1. In this paper, we present RAG, a Regional-Aware text-to-image Generation method conditioned on regional descriptions for precise layout composition. Regional prompting, or compositional generation, which enables fine-grained spatial control, has gained increasing attention for its practicality in re...
[18.11.2024 22:10] ********************************************************************************
[18.11.2024 22:10] Abstract 2. While 3D content generation has advanced significantly, existing methods still face challenges with input formats, latent space design, and output representations. This paper introduces a novel 3D generation framework that addresses these challenges, offering scalable, high-quality 3D generation wit...
[18.11.2024 22:10] ********************************************************************************
[18.11.2024 22:10] Abstract 3. The recently released model, Claude 3.5 Computer Use, stands out as the first frontier AI model to offer computer use in public beta as a graphical user interface (GUI) agent. As an early beta, its capability in the real-world complex environment remains unknown. In this case study to explore Claude...
[18.11.2024 22:10] ********************************************************************************
[18.11.2024 22:10] Abstract 4. Video Large Language Models (Vid-LLMs) have made remarkable advancements in comprehending video content for QA dialogue. However, they struggle to extend this visual understanding to tasks requiring precise temporal localization, known as Video Temporal Grounding (VTG). To address this gap, we intro...
[18.11.2024 22:10] ********************************************************************************
[18.11.2024 22:10] Abstract 5. We introduce Xmodel-1.5, a novel 1-billion-parameter multilingual large model pretrained on approximately 2 trillion tokens. The model demonstrates strong performance across several languages, with particularly notable results in Thai, Arabic, and French, alongside its effectiveness in Chinese and E...
[18.11.2024 22:10] ********************************************************************************
[18.11.2024 22:10] Abstract 6. Training deep neural networks--and more recently, large models--demands efficient and scalable optimizers. Adaptive gradient algorithms like Adam, AdamW, and their variants have been central to this task. Despite the development of numerous variance reduction algorithms in the past decade aimed at a...
[18.11.2024 22:10] Read previous papers.
[18.11.2024 22:10] Generating reviews via LLM API.
[18.11.2024 22:10] Using data from previous issue: {"categories": ["#dataset", "#inference", "#reasoning", "#multimodal", "#benchmark"], "emoji": "üß†", "ru": {"title": "LLaVA-o1: –ü—Ä–æ—Ä—ã–≤ –≤ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–º –≤–∏–∑—É–∞–ª—å–Ω–æ–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–∏", "desc": "LLaVA-o1 - —ç—Ç–æ –Ω–æ–≤–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å, —Å–ø–æ—Å–æ–±–Ω–∞—è –∫ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–º—É –º–Ω–æ–≥–æ—Å—Ç—É–ø–µ–Ω—á–∞—Ç–æ–º—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é. –í –æ—Ç–ª–∏—á–∏–µ 
[18.11.2024 22:10] Using data from previous issue: {"categories": ["#rag", "#cv", "#multimodal", "#optimization", "#games"], "emoji": "üé®", "ru": {"title": "–¢–æ—á–Ω—ã–π –∫–æ–Ω—Ç—Ä–æ–ª—å –Ω–∞–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é —Ä–µ–≥–∏–æ–Ω–∞–ª—å–Ω—ã—Ö –æ–ø–∏—Å–∞–Ω–∏–π", "desc": "–í —ç—Ç–æ–π —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –º–µ—Ç–æ–¥ RAG (Regional-Aware Generation) –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–º—É –æ–ø–∏—Å–∞
[18.11.2024 22:10] Using data from previous issue: {"categories": ["#synthetic", "#3d", "#multimodal", "#diffusion"], "emoji": "üßä", "ru": {"title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ 3D-–≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: –æ—Ç —Ç–æ—á–µ–∫ –∫ —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ 3D-–∫–æ–Ω—Ç–µ–Ω—Ç–∞, –∫–æ—Ç–æ—Ä—ã–π —Ä–µ—à–∞–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø—Ä–æ–±–ª–µ–º—ã —Å —Ñ–æ—Ä–º–∞—Ç–∞–º–∏ –≤–≤–æ–¥–∞, –¥–∏–∑–∞–π–Ω–æ–º –ª–∞—Ç–µ–Ω—Ç–Ω–æ–≥–æ 
[18.11.2024 22:10] Using data from previous issue: {"categories": ["#dataset", "#agents"], "emoji": "üñ•Ô∏è", "ru": {"title": "Claude 3.5: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –∞–Ω–∞–ª–∏–∑ –º–æ–¥–µ–ª–∏ Claude 3.5 Computer Use - –ø–µ—Ä–≤–æ–≥–æ –ø—É–±–ª–∏—á–Ω–æ –¥–æ—Å—Ç—É–ø–Ω–æ–≥–æ –ò–ò-–∞–≥–µ–Ω—Ç–∞ —Å –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–º –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–º. –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ–≤–µ–ª–∏ —Ä—è–¥ —Ç–µ—Å—Ç–æ
[18.11.2024 22:10] Using data from previous issue: {"categories": ["#multimodal", "#training", "#optimization", "#video", "#games"], "emoji": "üé¨", "ru": {"title": "NumPro: –ü–æ–º–æ–≥–∞–µ–º –ò–ò –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å—Å—è –≤–æ –≤—Ä–µ–º–µ–Ω–∏ –≤–∏–¥–µ–æ", "desc": "Video-LLM –¥–æ—Å—Ç–∏–≥–ª–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã—Ö —É—Å–ø–µ—Ö–æ–≤ –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ –≤–∏–¥–µ–æ–∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–ª—è –¥–∏–∞–ª–æ–≥–æ–≤ –≤–æ–ø—Ä–æ—Å–æ–≤ –∏ –æ—Ç–≤–µ—Ç–æ–≤, –Ω–æ –∏—Å–ø—ã—Ç—ã–≤–∞—é—Ç —Ç—Ä—É–¥–Ω–æ—Å—Ç–∏ 
[18.11.2024 22:10] Using data from previous issue: {"categories": ["#multilingual", "#dataset", "#open_source", "#small_models", "#low_resource"], "emoji": "üåç", "ru": {"title": "Xmodel-1.5: –ú—É–ª—å—Ç–∏—è–∑—ã—á–Ω—ã–π –ò–ò –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –º–µ–∂–¥—É –∫—É–ª—å—Ç—É—Ä–∞–º–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ Xmodel-1.5 - –Ω–æ–≤—É—é –º—É–ª—å—Ç–∏—è–∑—ã—á–Ω—É—é –±–æ–ª—å—à—É—é —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å —Å 1 –º–∏–ª–ª–∏–∞—Ä
[18.11.2024 22:10] Using data from previous issue: {"categories": ["#training", "#optimization"], "emoji": "üöÄ", "ru": {"title": "MARS: –ù–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–æ–Ω–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ MARS, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∏–π –º–µ—Ç–æ–¥—ã –ø—Ä–µ–¥–æ–±—É—Å–ª–æ–≤–ª–µ–Ω–Ω–æ–≥–æ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ —Å —Ä–µ–¥—É–∫—Ü–∏–µ–π –¥–∏—Å–ø–µ—Ä—Å
[18.11.2024 22:10] Loading Chinese text from previous data.
[18.11.2024 22:10] Renaming data file.
[18.11.2024 22:10] Renaming previous data. hf_papers.json to ./d/2024-11-18.json
[18.11.2024 22:10] Saving new data file.
[18.11.2024 22:10] Generating page.
[18.11.2024 22:10] Renaming previous page.
[18.11.2024 22:10] Renaming previous data. index.html to ./d/2024-11-18.html
[18.11.2024 22:10] [Experimental] Generating Chinese page for reading.
[18.11.2024 22:10] Chinese vocab [{'word': 'ËßÜËßâËØ≠Ë®ÄÊ®°Âûã', 'pinyin': 'sh√¨ju√© y«îy√°n m√≥x√≠ng', 'trans': 'visual language model'}, {'word': 'Ëá™‰∏ª', 'pinyin': 'z√¨zh«î', 'trans': 'autonomous'}, {'word': 'Â§öÈò∂ÊÆµ', 'pinyin': 'du≈ç jiƒìdu√†n', 'trans': 'multi-stage'}, {'word': 'Êé®ÁêÜ', 'pinyin': 'tuƒ´l«ê', 'trans': 'reasoning'}, {'word': 'ÈìæÂºè', 'pinyin': 'li√†nsh√¨', 'trans': 'chain-like'}, {'word': 'ÊèêÁ§∫', 'pinyin': 't√≠sh√¨', 'trans': 'prompt'}, {'word': 'Áã¨Á´ã', 'pinyin': 'd√∫l√¨', 'trans': 'independent'}, {'word': 'ÊÄªÁªì', 'pinyin': 'z«íngji√©', 'trans': 'summary'}, {'word': 'ËßÜËßâËß£Èáä', 'pinyin': 'sh√¨ju√© jiƒõsh√¨', 'trans': 'visual explanation'}, {'word': 'ÈÄªËæëÊé®ÁêÜ', 'pinyin': 'lu√≥ji tuƒ´l«ê', 'trans': 'logical reasoning'}, {'word': 'ÁªìËÆ∫ÁîüÊàê', 'pinyin': 'ji√©l√πn shƒìngch√©ng', 'trans': 'conclusion generation'}, {'word': 'ÁªìÊûÑÂåñ', 'pinyin': 'ji√©g√≤uhu√†', 'trans': 'structured'}, {'word': 'ÊñπÊ≥ï', 'pinyin': 'fƒÅngf«é', 'trans': 'method'}, {'word': 'Á≤æÂ∫¶', 'pinyin': 'jƒ´ngd√π', 'trans': 'accuracy'}, {'word': 'ÊèêÂçá', 'pinyin': 't√≠shƒìng', 'trans': 'improvement'}, {'word': 'Á†îÁ©∂Âõ¢Èòü', 'pinyin': 'y√°nji≈´ tu√°ndu√¨', 'trans': 'research team'}, {'word': 'ÁºñÂà∂', 'pinyin': 'biƒÅnzh√¨', 'trans': 'compile'}, {'word': 'Êï∞ÊçÆÈõÜ', 'pinyin': 'sh√πj√πj√≠', 'trans': 'dataset'}, {'word': 'Êé®ÁêÜÊó∂', 'pinyin': 'tuƒ´l«ê sh√≠', 'trans': 'during reasoning'}, {'word': 'Èò∂ÊÆµÁ∫ß', 'pinyin': 'jiƒìdu√†n j√≠', 'trans': 'stage-level'}, {'word': 'ÊùüÊêúÁ¥¢', 'pinyin': 'sh√π s≈çusu«í', 'trans': 'beam search'}, {'word': 'Êâ©Â±ï', 'pinyin': 'ku√≤zh«én', 'trans': 'expansion'}, {'word': 'Â§öÊ®°ÊÄÅ', 'pinyin': 'du≈ç m√≥sh√¨', 'trans': 'multimodal'}, {'word': 'Âü∫ÂáÜÊµãËØï', 'pinyin': 'jƒ´zh«în c√®sh√¨', 'trans': 'benchmark test'}, {'word': 'Ë°®Áé∞', 'pinyin': 'bi«éoxi√†n', 'trans': 'performance'}, {'word': 'Âá∫Ëâ≤', 'pinyin': 'ch≈´s√®', 'trans': 'outstanding'}, {'word': 'Ë∂ÖË∂ä', 'pinyin': 'chƒÅoyu√®', 'trans': 'surpass'}, {'word': 'Â§ßÂûã', 'pinyin': 'd√†x√≠ng', 'trans': 'large-scale'}, {'word': 'Â∞ÅÈó≠Ê∫ê', 'pinyin': 'fƒìngb√¨ yu√°n', 'trans': 'closed-source'}]
[18.11.2024 22:10] Renaming previous Chinese page.
[18.11.2024 22:10] Renaming previous data. zh.html to ./d/2024-11-17_zh_reading_task.html
[18.11.2024 22:10] Writing Chinese reading task.
[18.11.2024 22:10] Writing result.
[18.11.2024 22:10] Renaming log file.
[18.11.2024 22:10] Renaming previous data. log.txt to ./logs/2024-11-18_last_log.txt
