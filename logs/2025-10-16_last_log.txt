[16.10.2025 04:14] Read previous papers.
[16.10.2025 04:14] Generating top page (month).
[16.10.2025 04:14] Writing top page (month).
[16.10.2025 05:12] Read previous papers.
[16.10.2025 05:12] Get feed.
[16.10.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.13554
[16.10.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.13678
[16.10.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.07944
[16.10.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.13747
[16.10.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.13795
[16.10.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.13804
[16.10.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.13802
[16.10.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.13626
[16.10.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.04767
[16.10.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.13344
[16.10.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.13621
[16.10.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.13515
[16.10.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.13759
[16.10.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.10921
[16.10.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.13809
[16.10.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.10977
[16.10.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.11958
[16.10.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.13602
[16.10.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.12560
[16.10.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.13744
[16.10.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.12831
[16.10.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.10611
[16.10.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.10581
[16.10.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.13778
[16.10.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.13586
[16.10.2025 05:12] Extract page data from URL. URL: https://huggingface.co/papers/2510.13282
[16.10.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.10274
[16.10.2025 05:12] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[16.10.2025 05:12] No deleted papers detected.
[16.10.2025 05:12] Downloading and parsing papers (pdf, html). Total: 27.
[16.10.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2510.13554.
[16.10.2025 05:12] Extra JSON file exists (./assets/json/2510.13554.json), skip PDF parsing.
[16.10.2025 05:12] Paper image links file exists (./assets/img_data/2510.13554.json), skip HTML parsing.
[16.10.2025 05:12] Success.
[16.10.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2510.13678.
[16.10.2025 05:12] Extra JSON file exists (./assets/json/2510.13678.json), skip PDF parsing.
[16.10.2025 05:12] Paper image links file exists (./assets/img_data/2510.13678.json), skip HTML parsing.
[16.10.2025 05:12] Success.
[16.10.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2510.07944.
[16.10.2025 05:12] Extra JSON file exists (./assets/json/2510.07944.json), skip PDF parsing.
[16.10.2025 05:12] Paper image links file exists (./assets/img_data/2510.07944.json), skip HTML parsing.
[16.10.2025 05:12] Success.
[16.10.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2510.13747.
[16.10.2025 05:12] Extra JSON file exists (./assets/json/2510.13747.json), skip PDF parsing.
[16.10.2025 05:12] Paper image links file exists (./assets/img_data/2510.13747.json), skip HTML parsing.
[16.10.2025 05:12] Success.
[16.10.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2510.13795.
[16.10.2025 05:12] Extra JSON file exists (./assets/json/2510.13795.json), skip PDF parsing.
[16.10.2025 05:12] Paper image links file exists (./assets/img_data/2510.13795.json), skip HTML parsing.
[16.10.2025 05:12] Success.
[16.10.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2510.13804.
[16.10.2025 05:12] Extra JSON file exists (./assets/json/2510.13804.json), skip PDF parsing.
[16.10.2025 05:12] Paper image links file exists (./assets/img_data/2510.13804.json), skip HTML parsing.
[16.10.2025 05:12] Success.
[16.10.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2510.13802.
[16.10.2025 05:12] Extra JSON file exists (./assets/json/2510.13802.json), skip PDF parsing.
[16.10.2025 05:12] Paper image links file exists (./assets/img_data/2510.13802.json), skip HTML parsing.
[16.10.2025 05:12] Success.
[16.10.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2510.13626.
[16.10.2025 05:12] Extra JSON file exists (./assets/json/2510.13626.json), skip PDF parsing.
[16.10.2025 05:12] Paper image links file exists (./assets/img_data/2510.13626.json), skip HTML parsing.
[16.10.2025 05:12] Success.
[16.10.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2510.04767.
[16.10.2025 05:12] Extra JSON file exists (./assets/json/2510.04767.json), skip PDF parsing.
[16.10.2025 05:12] Paper image links file exists (./assets/img_data/2510.04767.json), skip HTML parsing.
[16.10.2025 05:12] Success.
[16.10.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2510.13344.
[16.10.2025 05:12] Extra JSON file exists (./assets/json/2510.13344.json), skip PDF parsing.
[16.10.2025 05:12] Paper image links file exists (./assets/img_data/2510.13344.json), skip HTML parsing.
[16.10.2025 05:12] Success.
[16.10.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2510.13621.
[16.10.2025 05:12] Extra JSON file exists (./assets/json/2510.13621.json), skip PDF parsing.
[16.10.2025 05:12] Paper image links file exists (./assets/img_data/2510.13621.json), skip HTML parsing.
[16.10.2025 05:12] Success.
[16.10.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2510.13515.
[16.10.2025 05:12] Extra JSON file exists (./assets/json/2510.13515.json), skip PDF parsing.
[16.10.2025 05:12] Paper image links file exists (./assets/img_data/2510.13515.json), skip HTML parsing.
[16.10.2025 05:12] Success.
[16.10.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2510.13759.
[16.10.2025 05:12] Extra JSON file exists (./assets/json/2510.13759.json), skip PDF parsing.
[16.10.2025 05:12] Paper image links file exists (./assets/img_data/2510.13759.json), skip HTML parsing.
[16.10.2025 05:12] Success.
[16.10.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2510.10921.
[16.10.2025 05:12] Extra JSON file exists (./assets/json/2510.10921.json), skip PDF parsing.
[16.10.2025 05:12] Paper image links file exists (./assets/img_data/2510.10921.json), skip HTML parsing.
[16.10.2025 05:12] Success.
[16.10.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2510.13809.
[16.10.2025 05:12] Extra JSON file exists (./assets/json/2510.13809.json), skip PDF parsing.
[16.10.2025 05:12] Paper image links file exists (./assets/img_data/2510.13809.json), skip HTML parsing.
[16.10.2025 05:12] Success.
[16.10.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2510.10977.
[16.10.2025 05:12] Extra JSON file exists (./assets/json/2510.10977.json), skip PDF parsing.
[16.10.2025 05:12] Paper image links file exists (./assets/img_data/2510.10977.json), skip HTML parsing.
[16.10.2025 05:12] Success.
[16.10.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2510.11958.
[16.10.2025 05:12] Extra JSON file exists (./assets/json/2510.11958.json), skip PDF parsing.
[16.10.2025 05:12] Paper image links file exists (./assets/img_data/2510.11958.json), skip HTML parsing.
[16.10.2025 05:12] Success.
[16.10.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2510.13602.
[16.10.2025 05:12] Extra JSON file exists (./assets/json/2510.13602.json), skip PDF parsing.
[16.10.2025 05:12] Paper image links file exists (./assets/img_data/2510.13602.json), skip HTML parsing.
[16.10.2025 05:12] Success.
[16.10.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2510.12560.
[16.10.2025 05:12] Extra JSON file exists (./assets/json/2510.12560.json), skip PDF parsing.
[16.10.2025 05:12] Paper image links file exists (./assets/img_data/2510.12560.json), skip HTML parsing.
[16.10.2025 05:12] Success.
[16.10.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2510.13744.
[16.10.2025 05:12] Extra JSON file exists (./assets/json/2510.13744.json), skip PDF parsing.
[16.10.2025 05:12] Paper image links file exists (./assets/img_data/2510.13744.json), skip HTML parsing.
[16.10.2025 05:12] Success.
[16.10.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2510.12831.
[16.10.2025 05:12] Extra JSON file exists (./assets/json/2510.12831.json), skip PDF parsing.
[16.10.2025 05:12] Paper image links file exists (./assets/img_data/2510.12831.json), skip HTML parsing.
[16.10.2025 05:12] Success.
[16.10.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2510.10611.
[16.10.2025 05:12] Extra JSON file exists (./assets/json/2510.10611.json), skip PDF parsing.
[16.10.2025 05:12] Paper image links file exists (./assets/img_data/2510.10611.json), skip HTML parsing.
[16.10.2025 05:12] Success.
[16.10.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2510.10581.
[16.10.2025 05:12] Extra JSON file exists (./assets/json/2510.10581.json), skip PDF parsing.
[16.10.2025 05:12] Paper image links file exists (./assets/img_data/2510.10581.json), skip HTML parsing.
[16.10.2025 05:12] Success.
[16.10.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2510.13778.
[16.10.2025 05:12] Extra JSON file exists (./assets/json/2510.13778.json), skip PDF parsing.
[16.10.2025 05:12] Paper image links file exists (./assets/img_data/2510.13778.json), skip HTML parsing.
[16.10.2025 05:12] Success.
[16.10.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2510.13586.
[16.10.2025 05:12] Extra JSON file exists (./assets/json/2510.13586.json), skip PDF parsing.
[16.10.2025 05:12] Paper image links file exists (./assets/img_data/2510.13586.json), skip HTML parsing.
[16.10.2025 05:12] Success.
[16.10.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2510.13282.
[16.10.2025 05:12] Downloading paper 2510.13282 from http://arxiv.org/pdf/2510.13282v1...
[16.10.2025 05:12] Extracting affiliations from text.
[16.10.2025 05:12] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 1 Universal Image Restoration Pre-training via Masked Degradation Classification JiaKui Hu, Zhengjian Yao, Lujia Jin, Yinghao Chen, Yanye Lu 5 2 0 2 5 ] . [ 1 2 8 2 3 1 . 0 1 5 2 : r AbstractThis study introduces Masked Degradation Classification Pre-Training method (MaskDCPT), designed to facilitate the classification of degradation types in input images, leading to comprehensive image restoration pre-training. Unlike conventional pre-training methods, MaskDCPT uses the degradation type of the image as an extremely weak supervision, while simultaneously leveraging the image reconstruction to enhance performance and robustness. MaskDCPT includes an encoder and two decoders: the encoder extracts features from the masked low-quality input image. The classification decoder uses these features to identify the degradation type, whereas the reconstruction decoder aims to reconstruct corresponding high-quality image. This design allows the pre-training to benefit from both masked image modeling and contrastive learning, resulting in generalized representation suited for restoration tasks. Benefit from the straightforward yet potent MaskDCPT, the pre-trained encoder can be used to address universal image restoration and achieve outstanding performance. Implementing MaskDCPT significantly improves performance for both convolution neural networks (CNNs) and Transformers, with minimum increase in PSNR of 3.77 dB in the 5D all-in-one restoration task and 34.8% reduction in PIQE compared to baseline in real-world degradation scenarios. It also emergences strong generalization to previously unseen degradation types and levels. In addition, we curate and release the UIR-2.5M dataset, which includes 2.5 million paired restoration samples across 19 degradation types and over 200 degradation levels, incorporating both synthetic and real-world data. The dataset, source code, and models are available at https://github.com/MILab-"
[16.10.2025 05:12] Response: ```python
[]
```
[16.10.2025 05:12] Extracting affiliations from text.
[16.10.2025 05:12] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 1 Universal Image Restoration Pre-training via Masked Degradation Classification JiaKui Hu, Zhengjian Yao, Lujia Jin, Yinghao Chen, Yanye Lu 5 2 0 2 5 ] . [ 1 2 8 2 3 1 . 0 1 5 2 : r AbstractThis study introduces Masked Degradation Classification Pre-Training method (MaskDCPT), designed to facilitate the classification of degradation types in input images, leading to comprehensive image restoration pre-training. Unlike conventional pre-training methods, MaskDCPT uses the degradation type of the image as an extremely weak supervision, while simultaneously leveraging the image reconstruction to enhance performance and robustness. MaskDCPT includes an encoder and two decoders: the encoder extracts features from the masked low-quality input image. The classification decoder uses these features to identify the degradation type, whereas the reconstruction decoder aims to reconstruct corresponding high-quality image. This design allows the pre-training to benefit from both masked image modeling and contrastive learning, resulting in generalized representation suited for restoration tasks. Benefit from the straightforward yet potent MaskDCPT, the pre-trained encoder can be used to address universal image restoration and achieve outstanding performance. Implementing MaskDCPT significantly improves performance for both convolution neural networks (CNNs) and Transformers, with minimum increase in PSNR of 3.77 dB in the 5D all-in-one restoration task and 34.8% reduction in PIQE compared to baseline in real-world degradation scenarios. It also emergences strong generalization to previously unseen degradation types and levels. In addition, we curate and release the UIR-2.5M dataset, which includes 2.5 million paired restoration samples across 19 degradation types and over 200 degradation levels, incorporating both synthetic and real-world data. The dataset, source code, and models are available at https://github.com/MILab-PKU/MaskDCPT. Index TermsPre-training, Degradation classification, Universal image restoration. I. INTRODUCTIONa single model to transform low-quality (LQ) images affected by variable, mixed, and real-world degradation into high-quality (HQ) images. In recent work, deep learningbased methods [1, 2, 3, 4, 5, 6] have demonstrated superior performance and efficiency in solving universal image restoration compared to traditional techniques [7, 8]. The prevalent approaches employ degradation representations of LQ images as discriminative prompts for universal image restoration tasks, utilizing elements such as gradients [9], frequency [10], supplementary parameters [2], and features JiaKui Hu, Zhengjian Yao and Yanye Lu are with Institute of Medical Technology, Peking University Health Science Center, Peking University, Beijing, China, and also with Biomedical Engineering Department, College of Future Technology, Peking University, Beijing, China, and also with National Biomedical Imaging Center, Peking University, Beijing, China. Lujia Jin is with JIUTIAN Research, Beijing, China. Yinghao Chen is with the College of Electronic Engineering, National University of Defense Technology, Changsha, China. Corresponding Authors: Yanye Lu (yanye.lu@pku.edu.cn). Fig. 1: MaskDCPT achieves the state-of-the-art fidelity and perception in multiple restoration tasks, including all-in-one and real-world scenarios. compressed through large neural networks [1, 3, 4, 5, 11]. These degradation representations subsequently function as prompts for base restoration models, which are either fine-tuned or specifically trained for universal image restoration. Despite achieving high performance through the implementation of precise and effective prompts, these methods do not exploit the latent prior information inherent within the restoration models. Pre-training methods [12, 13, 14, 15, 16, 17, 18, 19] are adept at exploiting the latent prior information inherent within the restoration models themselves. They can activate latent discriminant information within neural networks, thereby facilitating the acquisition of universal representations and rendering the pre-trained model suitable for downstream tasks. Contrastive learning [20, 21] discovers representations by maximizing agreement across multiple augmented views of the same sample using contrastive loss [22], thus obtaining features with fine-grained discriminant information [17]. Masked Image Modeling (MIM) [18, 19, 23] extends BERTs [12] success from language to vision transformers and CNNs. MIM introduces challenging image reconstruction task through substantially high mask ratio, which requires the model to uncover the intrinsic distribution of images. Following GPTs [13, 14] success in language generation, related methods [24] are utilized in image generation. PURE [25] also successfully used pre-trained autoregressive MLLM to adapt to real-world super-resolution. However, pre-training in image restoration [26, 27, 28] is mainly confined to single-task applications or requires carefully designed fine-tuning methods. JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 2 This suggests that current approaches do not fully arouse the universal representations provided by extensive pre-training. It is imperative to develop pre-training framework for restoration models that can handle universal restoration tasks. In this paper, we assert that the ability for degradation classification constitutes frequently overlooked, yet salient, in restoration models. We discriminative feature inherent validate the effectiveness and robustness of neural networks in this capability. First, we examine the degradation classification capabilities of the classical [29, 30, 31] and all-in-one [2] image restoration architectures. Models with random initialization possess preliminary aptitude for degradation classification, which is subsequently refined through all-in-one restoration training, thus enabling better identification of previously unobserved degradation types. Further investigation reveals that this ability remains intact even when images are randomly masked. This observation indicates that image distribution learning based on masked modeling and degradation distribution learning based on degradation classification can coexist. Drawing upon this finding, we leverage this potential during the pre-training for universal image restoration tasks. By integrating degradation classification, restoration"
[16.10.2025 05:12] Mistral response. {"id": "0d7a332eaa7d42ad9cbc613064ee466a", "created": 1760591548, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1389, "total_tokens": 1483, "completion_tokens": 94}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\n    \"Institute of Medical Technology, Peking University Health Science Center, Peking University, Beijing, China\",\n    \"Biomedical Engineering Department, College of Future Technology, Peking University, Beijing, China\",\n    \"National Biomedical Imaging Center, Peking University, Beijing, China\",\n    \"JIUTIAN Research, Beijing, China\",\n    \"College of Electronic Engineering, National University of Defense Technology, Changsha, China\"\n]\n```"}}]}
[16.10.2025 05:12] Response: ```python
[
    "Institute of Medical Technology, Peking University Health Science Center, Peking University, Beijing, China",
    "Biomedical Engineering Department, College of Future Technology, Peking University, Beijing, China",
    "National Biomedical Imaging Center, Peking University, Beijing, China",
    "JIUTIAN Research, Beijing, China",
    "College of Electronic Engineering, National University of Defense Technology, Changsha, China"
]
```
[16.10.2025 05:12] Deleting PDF ./assets/pdf/2510.13282.pdf.
[16.10.2025 05:12] Success.
[16.10.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2510.10274.
[16.10.2025 05:12] Extra JSON file exists (./assets/json/2510.10274.json), skip PDF parsing.
[16.10.2025 05:12] Paper image links file exists (./assets/img_data/2510.10274.json), skip HTML parsing.
[16.10.2025 05:12] Success.
[16.10.2025 05:12] Enriching papers with extra data.
[16.10.2025 05:12] ********************************************************************************
[16.10.2025 05:12] Abstract 0. Attention mechanisms in LLMs are analyzed to reveal reasoning patterns, leading to novel RL strategies that improve performance by focusing on critical tokens.  					AI-generated summary 				 The reasoning pattern of Large language models (LLMs) remains opaque, and Reinforcement learning (RL) typica...
[16.10.2025 05:12] ********************************************************************************
[16.10.2025 05:12] Abstract 1. FlashWorld generates 3D scenes from single images or text prompts quickly and with high quality by combining MV-oriented and 3D-oriented generation methods.  					AI-generated summary 				 We propose FlashWorld, a generative model that produces 3D scenes from a single image or text prompt in seconds...
[16.10.2025 05:12] ********************************************************************************
[16.10.2025 05:12] Abstract 2. CVD-STORM, a cross-view video diffusion model with a spatial-temporal reconstruction VAE, enhances video generation quality and provides depth estimation for dynamic scenes.  					AI-generated summary 				 Generative models have been widely applied to world modeling for environment simulation and fu...
[16.10.2025 05:12] ********************************************************************************
[16.10.2025 05:12] Abstract 3. InteractiveOmni is a unified omni-modal large language model for audio-visual multi-turn interactions, offering comprehensive understanding and speech generation capabilities with efficient parameter usage.  					AI-generated summary 				 We introduce InteractiveOmni, a unified and open-source omni-...
[16.10.2025 05:12] ********************************************************************************
[16.10.2025 05:12] Abstract 4. A new dataset and pipeline for data curation improve the performance of fully open multimodal large language models, achieving state-of-the-art results competitive with semi-open models.  					AI-generated summary 				 Fully open multimodal large language models (MLLMs) currently lag behind propriet...
[16.10.2025 05:12] ********************************************************************************
[16.10.2025 05:12] Abstract 5. Generative Universal Verifier enhances multimodal reasoning by providing reliable visual verification through ViVerBench, OmniVerifier-7B, and OmniVerifier-TTS, improving generation and refinement capabilities.  					AI-generated summary 				 We introduce Generative Universal Verifier, a novel conce...
[16.10.2025 05:12] ********************************************************************************
[16.10.2025 05:12] Abstract 6. Trace Anything, a neural network, predicts video trajectories in a single pass, achieving state-of-the-art performance and demonstrating efficiency and emergent abilities like motion forecasting.  					AI-generated summary 				 Effective spatio-temporal representation is fundamental to modeling, und...
[16.10.2025 05:12] ********************************************************************************
[16.10.2025 05:12] Abstract 7. State-of-the-art Visual-Language-Action models show high benchmark scores but are brittle to various perturbations, particularly in camera viewpoints and robot initial states, and often ignore language instructions.  					AI-generated summary 				 Visual-Language-Action (VLA) models report impressiv...
[16.10.2025 05:12] ********************************************************************************
[16.10.2025 05:12] Abstract 8. Parallel decoding in diffusion LLMs degrades generation quality due to ignored token dependencies, highlighting the need for new decoding methods and benchmarks.  					AI-generated summary 				 While most autoregressive LLMs are constrained to one-by-one decoding, diffusion LLMs (dLLMs) have attract...
[16.10.2025 05:12] ********************************************************************************
[16.10.2025 05:12] Abstract 9. UniMoE-Audio, a unified speech and music generation model using a Dynamic-Capacity Mixture-of-Experts framework, addresses data imbalance and task conflicts, achieving state-of-the-art performance and enhanced cross-domain synergy.  					AI-generated summary 				 Recent advances in unified multimoda...
[16.10.2025 05:12] ********************************************************************************
[16.10.2025 05:12] Abstract 10. Increased computing resources are correlated with national funding and citations in foundation model research, but not with research environment, domain, or methodology.  					AI-generated summary 				 Cutting-edge research in Artificial Intelligence (AI) requires considerable resources, including G...
[16.10.2025 05:12] ********************************************************************************
[16.10.2025 05:12] Abstract 11. A novel Universal Multimodal Embedding (UniME-V2) model uses MLLMs to enhance representation learning by identifying diverse, high-quality hard negatives and improving discriminative capacity through soft semantic matching scores.  					AI-generated summary 				 Universal multimodal embedding models...
[16.10.2025 05:12] ********************************************************************************
[16.10.2025 05:12] Abstract 12. Uni-MMMU is a benchmark that evaluates the bidirectional synergy between visual understanding and generation across multiple domains, providing insights into their integration and performance.  					AI-generated summary 				 Unified multimodal models aim to jointly enable visual understanding and ge...
[16.10.2025 05:12] ********************************************************************************
[16.10.2025 05:12] Abstract 13. FG-CLIP 2, a bilingual vision-language model, enhances fine-grained alignment for English and Chinese through rich supervision and a new TIC loss, achieving state-of-the-art performance across multiple datasets and tasks.  					AI-generated summary 				 Fine-grained vision-language understanding req...
[16.10.2025 05:12] ********************************************************************************
[16.10.2025 05:12] Abstract 14. PhysMaster enhances video generation by integrating physical knowledge through PhysEncoder, using reinforcement learning and Direct Preference Optimization to improve physics-awareness.  					AI-generated summary 				 Video generation models nowadays are capable of generating visually realistic vide...
[16.10.2025 05:12] ********************************************************************************
[16.10.2025 05:12] Abstract 15. Model merging, typically on Instruct and Thinking models, has shown remarkable performance for efficient reasoning. In this paper, we systematically revisit the simplest merging method that interpolates two weights directly. Particularly, we observe that model interpolation follows a three-stage evo...
[16.10.2025 05:12] ********************************************************************************
[16.10.2025 05:12] Abstract 16. Direct Multi-Token Decoding (DMTD) accelerates large language model inference by using only late layers for token generation, achieving significant speedup with minimal performance loss.  					AI-generated summary 				 Decoder-only transformers have become the standard architecture for large languag...
[16.10.2025 05:12] ********************************************************************************
[16.10.2025 05:12] Abstract 17. NOSA, a trainable sparse attention framework, enhances decoding throughput by enabling efficient KV cache offloading without compromising performance.  					AI-generated summary 				 Trainable sparse attention has emerged as a promising solution to address the decoding efficiency bottleneck of LLMs ...
[16.10.2025 05:12] ********************************************************************************
[16.10.2025 05:12] Abstract 18. End-to-end autonomous driving models trained solely with imitation learning (IL) often suffer from poor generalization. In contrast, reinforcement learning (RL) promotes exploration through reward maximization but faces challenges such as sample inefficiency and unstable convergence. A natural solut...
[16.10.2025 05:12] ********************************************************************************
[16.10.2025 05:12] Abstract 19. Hard2Verify, a human-annotated benchmark, evaluates step-level verifiers for LLM-based mathematical reasoning systems, highlighting the challenges and performance gaps between open-source and closed-source models.  					AI-generated summary 				 Large language model (LLM)-based reasoning systems hav...
[16.10.2025 05:12] ********************************************************************************
[16.10.2025 05:12] Abstract 20. MTSQL-R1, an agentic training framework, improves multi-turn Text-to-SQL by treating it as an MDP with iterative propose-execute-verify-refine cycles, enhancing coherence and execution.  					AI-generated summary 				 Multi-turn Text-to-SQL aims to translate a user's conversational utterances into e...
[16.10.2025 05:12] ********************************************************************************
[16.10.2025 05:12] Abstract 21. HyperAgent, a hypergraph-based framework, optimizes communication topologies and captures group collaboration patterns, improving performance and efficiency in multi-agent systems.  					AI-generated summary 				 Recent advances in large language model-powered multi-agent systems have demonstrated r...
[16.10.2025 05:12] ********************************************************************************
[16.10.2025 05:12] Abstract 22. GraphTracer addresses multi-agent system failure attribution by constructing Information Dependency Graphs to trace information flow and improve debugging accuracy.  					AI-generated summary 				 Multi-agent systems powered by Large Language Models excel at complex tasks through coordinated collabo...
[16.10.2025 05:12] ********************************************************************************
[16.10.2025 05:12] Abstract 23. A unified framework for spatial grounding and robot control, InternVLA-M1, enhances instruction-following robots through spatially guided vision-language-action training, achieving significant improvements across various tasks and simulations.  					AI-generated summary 				 We introduce InternVLA-M...
[16.10.2025 05:12] ********************************************************************************
[16.10.2025 05:12] Abstract 24. Participants in the CPDC 2025 used lightweight prompting and fine-tuned large models to achieve high rankings in task-oriented and context-aware dialogue challenges.  					AI-generated summary 				 The emergence of large language models (LLMs) has opened new opportunities for cre- ating dynamic non-...
[16.10.2025 05:12] ********************************************************************************
[16.10.2025 05:12] Abstract 25. A Masked Degradation Classification Pre-Training method enhances image restoration by using degradation type classification and image reconstruction, improving performance across CNNs and Transformers.  					AI-generated summary 				 This study introduces a Masked Degradation Classification Pre-Trai...
[16.10.2025 05:12] ********************************************************************************
[16.10.2025 05:12] Abstract 26. A novel Soft Prompt approach enhances Vision-Language-Action models by using learnable embeddings for diverse robotic data, enabling superior performance across simulations and real-world robots.  					AI-generated summary 				 Successful generalist Vision-Language-Action (VLA) models rely on effect...
[16.10.2025 05:12] Read previous papers.
[16.10.2025 05:12] Generating reviews via LLM API.
[16.10.2025 05:12] Using data from previous issue: {"categories": ["#rl", "#reasoning", "#optimization", "#interpretability", "#training"], "emoji": "üîç", "ru": {"title": "–í–Ω–∏–º–∞–Ω–∏–µ –∫–∞–∫ –∫–∞—Ä—Ç–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π: –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —á–µ—Ä–µ–∑ –∫–ª—é—á–µ–≤—ã–µ —Ç–æ–∫–µ–Ω—ã", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç –º–µ—Ö–∞–Ω–∏–∑–º—ã attention –≤ LLM, —á—Ç–æ–±—ã –ø–æ–Ω—è—Ç—å, –∫–∞–∫ –º–æ–¥–µ–ª–∏ —Ä–∞—Å—Å—É–∂–¥–∞—é—Ç, –≤—ã—è
[16.10.2025 05:12] Using data from previous issue: {"categories": ["#3d", "#optimization", "#diffusion"], "emoji": "‚ö°", "ru": {"title": "–ú–≥–Ω–æ–≤–µ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è 3D-–º–∏—Ä–æ–≤: —Å–∫–æ—Ä–æ—Å—Ç—å –∏ –∫–∞—á–µ—Å—Ç–≤–æ –≤–º–µ—Å—Ç–µ", "desc": "FlashWorld ‚Äî —ç—Ç–æ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è —Å–æ–∑–¥–∞—ë—Ç 3D-—Å—Ü–µ–Ω—ã –∏–∑ –æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–ª–∏ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞ –∑–∞ —Å—á–∏—Ç–∞–Ω–Ω—ã–µ —Å–µ–∫—É–Ω–¥—ã, —Ä–∞–±–æ—Ç–∞—è –≤ 10-10
[16.10.2025 05:12] Using data from previous issue: {"categories": ["#architecture", "#3d", "#optimization", "#video", "#multimodal", "#diffusion"], "emoji": "üöó", "ru": {"title": "4D-—Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö —Å—Ü–µ–Ω —á–µ—Ä–µ–∑ –≤–∏–¥–µ–æ –¥–∏—Ñ—Ñ—É–∑–∏—é", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç CVD-STORM ‚Äî –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ä–∞–∫—É—Ä—Å–æ–≤, –∫–æ—Ç–æ—Ä–∞—è —Ç–∞–∫–∂–µ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç
[16.10.2025 05:12] Using data from previous issue: {"categories": ["#video", "#small_models", "#long_context", "#dataset", "#benchmark", "#multimodal", "#audio", "#open_source", "#training"], "emoji": "üé≠", "ru": {"title": "–õ–µ–≥–∫–æ–≤–µ—Å–Ω–∞—è –æ–º–Ω–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å —Å –¥–æ–ª–≥–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–π –ø–∞–º—è—Ç—å—é –¥–ª—è –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∞—É–¥–∏–æ–≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è", "desc": "Interacti
[16.10.2025 05:12] Using data from previous issue: {"categories": ["#data", "#transfer_learning", "#dataset", "#optimization", "#open_source"], "emoji": "üêù", "ru": {"title": "–ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ ‚Äî –∫–ª—é—á –∫ –æ—Ç–∫—Ä—ã—Ç—ã–º –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–º –º–æ–¥–µ–ª—è–º", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Å–æ–∑–¥–∞–ª–∏ –Ω–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç Honey-Data-15M –∏–∑ 15 –º–∏–ª–ª–∏–æ–Ω–æ–≤ –ø–∞—Ä –≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º—É–ª—å—Ç–∏
[16.10.2025 05:12] Using data from previous issue: {"categories": ["#reasoning", "#dataset", "#multimodal", "#optimization", "#benchmark"], "emoji": "üîç", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ç–æ—Ä –¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Generative Universal Verifier ‚Äî –Ω–æ–≤—É—é –∫–æ–Ω—Ü–µ–ø—Ü–∏—é –¥–ª—è 
[16.10.2025 05:12] Using data from previous issue: {"categories": ["#training", "#games", "#video", "#dataset", "#optimization", "#benchmark"], "emoji": "üéØ", "ru": {"title": "–û—Ç—Å–ª–µ–¥–∏—Ç—å –≤—Å—ë: –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π –≤—Å–µ—Ö –ø–∏–∫—Å–µ–ª–µ–π –≤–∏–¥–µ–æ –∑–∞ –æ–¥–∏–Ω –ø—Ä–æ—Ö–æ–¥", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Trace Anything ‚Äî –Ω–µ–π—Ä–æ—Å–µ—Ç—å, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –¥–≤–∏–∂–µ–Ω–∏—è –≤
[16.10.2025 05:12] Using data from previous issue: {"categories": ["#agents", "#benchmark", "#video", "#interpretability", "#security"], "emoji": "ü§ñ", "ru": {"title": "–•—Ä—É–ø–∫–æ—Å—Ç—å VLA –º–æ–¥–µ–ª–µ–π: –≤—ã—Å–æ–∫–∏–µ –±–µ–Ω—á–º–∞—Ä–∫–∏ —Å–∫—Ä—ã–≤–∞—é—Ç –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ —É—è–∑–≤–∏–º–æ—Å—Ç–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–æ–≤–µ–ª–∏ —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ —É—è–∑–≤–∏–º–æ—Å—Ç–µ–π —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö Visual-Language-Action (VLA) –º
[16.10.2025 05:12] Using data from previous issue: {"categories": ["#inference", "#diffusion", "#benchmark", "#optimization"], "emoji": "‚ö°", "ru": {"title": "–ü—Ä–æ–±–ª–µ–º–∞ —Å–∫–æ—Ä–æ—Å—Ç–∏ –ø—Ä–æ—Ç–∏–≤ –∫–∞—á–µ—Å—Ç–≤–∞ –≤ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–º –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–∏ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö LLM", "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –≤ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (dLLM), –∫–æ—Ç–æ—Ä—ã–µ 
[16.10.2025 05:12] Using data from previous issue: {"categories": ["#architecture", "#games", "#benchmark", "#optimization", "#multimodal", "#audio", "#training"], "emoji": "üéµ", "ru": {"title": "–ï–¥–∏–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Ä–µ—á–∏ –∏ –º—É–∑—ã–∫–∏ —á–µ—Ä–µ–∑ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫—É—é Mixture-of-Experts", "desc": "UniMoE-Audio ‚Äî —ç—Ç–æ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–µ—á–∏ –∏ –º—É–∑—ã–∫–∏, –æ—Å–Ω–æ
[16.10.2025 05:12] Using data from previous issue: {"categories": ["#open_source", "#ethics"], "emoji": "üí∞", "ru": {"title": "–î–µ–Ω—å–≥–∏ —Ä–µ—à–∞—é—Ç: –∫–∞–∫ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã –≤–ª–∏—è—é—Ç –Ω–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è foundation models", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å–≤—è–∑—å –º–µ–∂–¥—É –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏ –∏ –Ω–∞—É—á–Ω—ã–º –ø—Ä–æ–≥—Ä–µ—Å—Å–æ–º –≤ –æ–±–ª–∞—Å—Ç–∏ foundation models (–±–æ–ª—å—à–∏—Ö –±–∞–∑–æ–≤—ã—Ö –º–æ
[16.10.2025 05:12] Using data from previous issue: {"categories": ["#optimization", "#benchmark", "#training", "#multimodal"], "emoji": "üéØ", "ru": {"title": "MLLM –∫–∞–∫ —Å—É–¥—å—è –¥–ª—è —É–º–Ω–æ–≥–æ –º–∞–π–Ω–∏–Ω–≥–∞ hard negatives –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–∞—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç UniME-V2 ‚Äî —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—É—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤, –∫–æ—Ç–æ—Ä–∞—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º—É
[16.10.2025 05:12] Using data from previous issue: {"categories": ["#multimodal", "#survey", "#reasoning", "#benchmark"], "emoji": "üîÑ", "ru": {"title": "–î–≤—É—Å—Ç–æ—Ä–æ–Ω–Ω—è—è —Å–∏–Ω–µ—Ä–≥–∏—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "Uni-MMMU ‚Äî —ç—Ç–æ –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ unified –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–µ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –ø–æ–Ω–∏–º–∞—é—Ç –∏ –≥–µ–Ω–µ—Ä–∏—Ä—É—é—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è. –°—É—â–µ—Å—Ç–≤
[16.10.2025 05:12] Using data from previous issue: {"categories": ["#multilingual", "#dataset", "#benchmark", "#multimodal", "#alignment", "#open_source"], "emoji": "üîç", "ru": {"title": "–î–µ—Ç–∞–ª—å–Ω–æ–µ –¥–≤—É—è–∑—ã—á–Ω–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ —Ç–µ–∫—Å—Ç–∞", "desc": "FG-CLIP 2 ‚Äî —ç—Ç–æ –¥–≤—É—è–∑—ã—á–Ω–∞—è vision-language –º–æ–¥–µ–ª—å –¥–ª—è –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ –∏ –∫–∏—Ç–∞–π—Å–∫–æ–≥–æ —è–∑—ã–∫–æ–≤, –∫–æ—Ç–æ—Ä–∞—è —É–ª—É
[16.10.2025 05:12] Using data from previous issue: {"categories": ["#rlhf", "#games", "#rl", "#video", "#multimodal", "#optimization"], "emoji": "üéØ", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ –≤–∏–¥–µ–æ-–º–æ–¥–µ–ª–µ–π —Ñ–∏–∑–∏—á–µ—Å–∫–∏–º –∑–∞–∫–æ–Ω–∞–º —á–µ—Ä–µ–∑ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è", "desc": "PhysMaster —É–ª—É—á—à–∞–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –≤–∏–¥–µ–æ, –¥–æ–±–∞–≤–ª—è—è –ø–æ–Ω–∏–º–∞–Ω–∏–µ —Ñ–∏–∑–∏—á–µ—Å–∫–∏—Ö –∑–∞–∫–æ–Ω–æ–≤ —á–µ—Ä–µ–∑ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π PhysEncoder. –°–∏—Å—Ç–µ–º
[16.10.2025 05:12] Using data from previous issue: {"categories": ["#architecture", "#reasoning", "#training", "#optimization"], "emoji": "üîÄ", "ru": {"title": "–¢—Ä–∏ —Å—Ç–∞–¥–∏–∏ –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏–∏: –ø—Ä–æ—Å—Ç–æ–µ —Å–ª–∏—è–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –ø–æ–±–µ–∂–¥–∞–µ—Ç —Å–ª–æ–∂–Ω—ã–µ –º–µ—Ç–æ–¥—ã", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –∏–∑—É—á–∏–ª–∏ –ø—Ä–æ—Å—Ç–µ–π—à–∏–π –º–µ—Ç–æ–¥ —Å–ª–∏—è–Ω–∏—è –º–æ–¥–µ–ª–µ–π - –ø—Ä—è–º—É—é –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—é –≤–µ—Å–æ–≤ –º–µ–∂–¥—É Instruct –∏ Thinki
[16.10.2025 05:12] Using data from previous issue: {"categories": ["#architecture", "#training", "#inference", "#optimization"], "emoji": "‚ö°", "ru": {"title": "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤ –∑–∞ –æ–¥–∏–Ω –ø—Ä–æ—Ö–æ–¥ —á–µ—Ä–µ–∑ –ø–æ–∑–¥–Ω–∏–µ —Å–ª–æ–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –º–µ—Ç–æ–¥ Direct Multi-Token Decoding (DMTD), –∫–æ—Ç–æ—Ä—ã–π —É—Å–∫–æ—Ä—è–µ—Ç inference –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ
[16.10.2025 05:12] Using data from previous issue: {"categories": ["#architecture", "#inference", "#long_context", "#benchmark", "#optimization", "#training"], "emoji": "üöÄ", "ru": {"title": "–£—Å–∫–æ—Ä–µ–Ω–∏–µ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è LLM —á–µ—Ä–µ–∑ –ª–æ–∫–∞–ª—å–Ω–æ–µ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ", "desc": "NOSA ‚Äî —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–±—É—á–∞–µ–º–æ–≥–æ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è (sparse attention), –∫–æ—Ç–æ—Ä—ã–π —Ä–µ—à–∞–µ—Ç –ø—Ä
[16.10.2025 05:12] Using data from previous issue: {"categories": ["#rl", "#agents", "#games", "#optimization", "#training"], "emoji": "üèéÔ∏è", "ru": {"title": "–ö–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–≤—É—Ö –∞–≥–µ–Ω—Ç–æ–≤ –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ–≥–æ –∞–≤—Ç–æ–ø–∏–ª–æ—Ç–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–º—É –≤–æ–∂–¥–µ–Ω–∏—é –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º CoIRL-AD, –∫–æ—Ç–æ—Ä—ã–π –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç imitation learning –∏ re
[16.10.2025 05:12] Using data from previous issue: {"categories": ["#reasoning", "#math", "#benchmark", "#interpretability"], "emoji": "üîç", "ru": {"title": "–ü—Ä–æ–≤–µ—Ä–∫–∞ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π AI –Ω–∞ –ø—Ä–æ—á–Ω–æ—Å—Ç—å", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Å–æ–∑–¥–∞–ª–∏ Hard2Verify ‚Äî –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π LLM, —Å–æ–∑–¥–∞–Ω–Ω—ã–π —Å –ø–æ–º–æ—â—å—é 500 —á–∞—Å–æ–≤
[16.10.2025 05:12] Using data from previous issue: {"categories": ["#rl", "#agents", "#reasoning", "#optimization", "#survey"], "emoji": "üîÑ", "ru": {"title": "–ò—Ç–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è SQL —á–µ—Ä–µ–∑ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ —Å –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö", "desc": "MTSQL-R1 ‚Äî —ç—Ç–æ –∞–≥–µ–Ω—Ç–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å–∏—Å—Ç–µ–º –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –≤ SQL –≤ –º–Ω–æ–≥–æ—à–∞–≥–æ–≤—ã—Ö –¥–∏–∞–ª–æ–≥–∞—Ö. –ê–≤—Ç–æ—Ä—ã —Ñ–æ—Ä–º—É
[16.10.2025 05:12] Using data from previous issue: {"categories": ["#agents", "#optimization", "#graphs", "#games"], "emoji": "üï∏Ô∏è", "ru": {"title": "–ì–∏–ø–µ—Ä–≥—Ä–∞—Ñ—ã –¥–ª—è —É–º–Ω–æ–π –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏ AI-–∞–≥–µ–Ω—Ç–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç HyperAgent ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≥–∏–ø–µ—Ä–≥—Ä–∞—Ñ–æ–≤ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏ –≤ –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º–∞—Ö —Å LLM. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Ç—Ä–∞–¥–∏—Ü
[16.10.2025 05:12] Using data from previous issue: {"categories": ["#agents", "#synthetic", "#dataset", "#graphs", "#optimization", "#benchmark"], "emoji": "üï∏Ô∏è", "ru": {"title": "–ì—Ä–∞—Ñ—ã –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –¥–ª—è –ø–æ–∏—Å–∫–∞ –∫–æ—Ä–Ω–µ–≤—ã—Ö –ø—Ä–∏—á–∏–Ω –æ—à–∏–±–æ–∫ –≤ –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º–∞—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç GraphTracer ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ –æ—à–∏–±–æ–∫ –≤ –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω—ã
[16.10.2025 05:12] Using data from previous issue: {"categories": ["#science", "#agents", "#agi", "#optimization", "#robotics"], "emoji": "ü§ñ", "ru": {"title": "–†–æ–±–æ—Ç—ã —É—á–∞—Ç—Å—è –¥–µ–π—Å—Ç–≤–æ–≤–∞—Ç—å —á–µ—Ä–µ–∑ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞", "desc": "InternVLA-M1 ‚Äî —ç—Ç–æ –µ–¥–∏–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ä–æ–±–æ—Ç–∞–º–∏, –∫–æ—Ç–æ—Ä–∞—è —Å–≤—è–∑—ã–≤–∞–µ—Ç –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ —Å –¥–µ–π—Å—Ç–≤–∏—è–º–∏ —á–µ—Ä–µ–∑ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ –ø
[16.10.2025 05:12] Using data from previous issue: {"categories": ["#agents", "#optimization", "#training", "#games"], "emoji": "üéÆ", "ru": {"title": "–£–º–Ω—ã–µ –∏–≥—Ä–æ–≤—ã–µ –ø–µ—Ä—Å–æ–Ω–∞–∂–∏ —á–µ—Ä–µ–∑ –ø—Ä–æ–º–ø—Ç–∏–Ω–≥ –∏ —Ñ–∞–π–Ω-—Ç—é–Ω–∏–Ω–≥", "desc": "–ö–æ–º–∞–Ω–¥–∞ Tu_Character_lab —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∞ –º–µ—Ç–æ–¥—ã –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –¥–∏–∞–ª–æ–≥–æ–≤—ã—Ö NPC –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π –≤ –∏–≥—Ä–∞—Ö –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM. –í —Ä–∞–±–æ—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å –¥–≤–∞ –ø–æ
[16.10.2025 05:12] Querying the API.
[16.10.2025 05:12] Claude request. Model: claude-sonnet-4-5-20250929. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A Masked Degradation Classification Pre-Training method enhances image restoration by using degradation type classification and image reconstruction, improving performance across CNNs and Transformers.  					AI-generated summary 				 This study introduces a Masked Degradation Classification Pre-Training method (MaskDCPT), designed to facilitate the classification of degradation types in input images, leading to comprehensive image restoration pre-training. Unlike conventional pre-training methods, MaskDCPT uses the degradation type of the image as an extremely weak supervision, while simultaneously leveraging the image reconstruction to enhance performance and robustness. MaskDCPT includes an encoder and two decoders: the encoder extracts features from the masked low-quality input image. The classification decoder uses these features to identify the degradation type, whereas the reconstruction decoder aims to reconstruct a corresponding high-quality image. This design allows the pre-training to benefit from both masked image modeling and contrastive learning, resulting in a generalized representation suited for restoration tasks. Benefit from the straightforward yet potent MaskDCPT, the pre-trained encoder can be used to address universal image restoration and achieve outstanding performance. Implementing MaskDCPT significantly improves performance for both convolution neural networks (CNNs) and Transformers, with a minimum increase in PSNR of 3.77 dB in the 5D all-in-one restoration task and a 34.8% reduction in PIQE compared to baseline in real-world degradation scenarios. It also emergences strong generalization to previously unseen degradation types and levels. In addition, we curate and release the UIR-2.5M dataset, which includes 2.5 million paired restoration samples across 19 degradation types and over 200 degradation levels, incorporating both synthetic and real-world data. The dataset, source code, and models are available at https://github.com/MILab-PKU/MaskDCPT.
[16.10.2025 05:12] Response: ```json
{
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è MaskDCPT –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é —Ç–∏–ø–æ–≤ –¥–µ–≥—Ä–∞–¥–∞—Ü–∏–∏ –∫–∞–∫ —Å–ª–∞–±—ã–π –Ω–∞–¥–∑–æ—Ä –∏ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –≤—ã–ø–æ–ª–Ω—è–µ—Ç —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è. –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –≤–∫–ª—é—á–∞–µ—Ç —ç–Ω–∫–æ–¥–µ—Ä –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ –∑–∞–º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∏–∑–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ –∏ –¥–≤–∞ –¥–µ–∫–æ–¥–µ—Ä–∞: –æ–¥–∏–Ω –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç —Ç–∏–ø –¥–µ–≥—Ä–∞–¥–∞—Ü–∏–∏, –¥—Ä—É–≥–æ–π –≤–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ. –ú–µ—Ç–æ–¥ –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ masked image modeling –∏ contrastive learning, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–æ–∑–¥–∞—Ç—å —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –¥–ª—è –∑–∞–¥–∞—á restoration. MaskDCPT –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –¥–ª—è CNN –∏ Transformer –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä (–º–∏–Ω–∏–º—É–º +3.77 dB PSNR) –∏ –≤–∫–ª—é—á–∞–µ—Ç –Ω–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç UIR-2.5M —Å 2.5 –º–∏–ª–ª–∏–æ–Ω–∞–º–∏ –ø–∞—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å 19 —Ç–∏–ø–∞–º–∏ –¥–µ–≥—Ä–∞–¥–∞—Ü–∏–∏.",
  "emoji": "üé≠",
  "title": "–ü—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ —á–µ—Ä–µ–∑ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –¥–µ–≥—Ä–∞–¥–∞—Ü–∏–∏: —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"
}
```
[16.10.2025 05:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A Masked Degradation Classification Pre-Training method enhances image restoration by using degradation type classification and image reconstruction, improving performance across CNNs and Transformers.  					AI-generated summary 				 This study introduces a Masked Degradation Classification Pre-Training method (MaskDCPT), designed to facilitate the classification of degradation types in input images, leading to comprehensive image restoration pre-training. Unlike conventional pre-training methods, MaskDCPT uses the degradation type of the image as an extremely weak supervision, while simultaneously leveraging the image reconstruction to enhance performance and robustness. MaskDCPT includes an encoder and two decoders: the encoder extracts features from the masked low-quality input image. The classification decoder uses these features to identify the degradation type, whereas the reconstruction decoder aims to reconstruct a corresponding high-quality image. This design allows the pre-training to benefit from both masked image modeling and contrastive learning, resulting in a generalized representation suited for restoration tasks. Benefit from the straightforward yet potent MaskDCPT, the pre-trained encoder can be used to address universal image restoration and achieve outstanding performance. Implementing MaskDCPT significantly improves performance for both convolution neural networks (CNNs) and Transformers, with a minimum increase in PSNR of 3.77 dB in the 5D all-in-one restoration task and a 34.8% reduction in PIQE compared to baseline in real-world degradation scenarios. It also emergences strong generalization to previously unseen degradation types and levels. In addition, we curate and release the UIR-2.5M dataset, which includes 2.5 million paired restoration samples across 19 degradation types and over 200 degradation levels, incorporating both synthetic and real-world data. The dataset, source code, and models are available at https://github.com/MILab-PKU/MaskDCPT."

[16.10.2025 05:12] Response: ```python
['DATASET', 'DATA', 'CV']
```
[16.10.2025 05:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A Masked Degradation Classification Pre-Training method enhances image restoration by using degradation type classification and image reconstruction, improving performance across CNNs and Transformers.  					AI-generated summary 				 This study introduces a Masked Degradation Classification Pre-Training method (MaskDCPT), designed to facilitate the classification of degradation types in input images, leading to comprehensive image restoration pre-training. Unlike conventional pre-training methods, MaskDCPT uses the degradation type of the image as an extremely weak supervision, while simultaneously leveraging the image reconstruction to enhance performance and robustness. MaskDCPT includes an encoder and two decoders: the encoder extracts features from the masked low-quality input image. The classification decoder uses these features to identify the degradation type, whereas the reconstruction decoder aims to reconstruct a corresponding high-quality image. This design allows the pre-training to benefit from both masked image modeling and contrastive learning, resulting in a generalized representation suited for restoration tasks. Benefit from the straightforward yet potent MaskDCPT, the pre-trained encoder can be used to address universal image restoration and achieve outstanding performance. Implementing MaskDCPT significantly improves performance for both convolution neural networks (CNNs) and Transformers, with a minimum increase in PSNR of 3.77 dB in the 5D all-in-one restoration task and a 34.8% reduction in PIQE compared to baseline in real-world degradation scenarios. It also emergences strong generalization to previously unseen degradation types and levels. In addition, we curate and release the UIR-2.5M dataset, which includes 2.5 million paired restoration samples across 19 degradation types and over 200 degradation levels, incorporating both synthetic and real-world data. The dataset, source code, and models are available at https://github.com/MILab-PKU/MaskDCPT."

[16.10.2025 05:12] Response: ```python
["OPTIMIZATION", "SYNTHETIC", "OPEN_SOURCE"]
```
[16.10.2025 05:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The paper presents a novel Masked Degradation Classification Pre-Training method (MaskDCPT) that enhances image restoration by classifying degradation types and reconstructing images. This approach utilizes weak supervision from degradation classification while simultaneously improving image quality through reconstruction. The architecture consists of an encoder for feature extraction and two decoders for classification and reconstruction tasks, allowing for effective pre-training. Results show significant performance improvements in both CNNs and Transformers, with better generalization to new degradation types and a large dataset of 2.5 million samples for training and evaluation.","title":"Enhancing Image Restoration with Masked Degradation Classification"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='The paper presents a novel Masked Degradation Classification Pre-Training method (MaskDCPT) that enhances image restoration by classifying degradation types and reconstructing images. This approach utilizes weak supervision from degradation classification while simultaneously improving image quality through reconstruction. The architecture consists of an encoder for feature extraction and two decoders for classification and reconstruction tasks, allowing for effective pre-training. Results show significant performance improvements in both CNNs and Transformers, with better generalization to new degradation types and a large dataset of 2.5 million samples for training and evaluation.', title='Enhancing Image Restoration with Masked Degradation Classification'))
[16.10.2025 05:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫Masked Degradation Classification Pre-TrainingÔºàMaskDCPTÔºâÁöÑÊñπÊ≥ïÔºåÊó®Âú®ÈÄöËøáÂØπËæìÂÖ•ÂõæÂÉèÁöÑÈôçË¥®Á±ªÂûãËøõË°åÂàÜÁ±ªÊù•Â¢ûÂº∫ÂõæÂÉèÊÅ¢Â§çÁöÑÊïàÊûú„ÄÇ‰∏é‰º†ÁªüÁöÑÈ¢ÑËÆ≠ÁªÉÊñπÊ≥ï‰∏çÂêåÔºåMaskDCPTÂà©Áî®ÂõæÂÉèÁöÑÈôçË¥®Á±ªÂûã‰Ωú‰∏∫ÊûÅÂº±ÁöÑÁõëÁù£‰ø°Âè∑ÔºåÂêåÊó∂ÁªìÂêàÂõæÂÉèÈáçÂª∫Êù•ÊèêÈ´òÊÄßËÉΩÂíåÈ≤ÅÊ£íÊÄß„ÄÇËØ•ÊñπÊ≥ïÂåÖÊã¨‰∏Ä‰∏™ÁºñÁ†ÅÂô®Âíå‰∏§‰∏™Ëß£Á†ÅÂô®ÔºåÁºñÁ†ÅÂô®‰ªé‰ΩéË¥®ÈáèÁöÑËæìÂÖ•ÂõæÂÉè‰∏≠ÊèêÂèñÁâπÂæÅÔºåÂàÜÁ±ªËß£Á†ÅÂô®Áî®‰∫éËØÜÂà´ÈôçË¥®Á±ªÂûãÔºåËÄåÈáçÂª∫Ëß£Á†ÅÂô®ÂàôÊó®Âú®ÈáçÂª∫Áõ∏Â∫îÁöÑÈ´òË¥®ÈáèÂõæÂÉè„ÄÇÈÄöËøáËøôÁßçËÆæËÆ°ÔºåMaskDCPTËÉΩÂ§üÊúâÊïàÂú∞ËøõË°åÂõæÂÉèÊÅ¢Â§ç‰ªªÂä°ÔºåÂπ∂Âú®Âç∑ÁßØÁ•ûÁªèÁΩëÁªúÔºàCNNsÔºâÂíåÂèòÊç¢Âô®ÔºàTransformersÔºâ‰∏äÊòæËëóÊèêÂçáÊÄßËÉΩ„ÄÇ","title":"Masked Degradation ClassificationÔºöÊèêÂçáÂõæÂÉèÊÅ¢Â§çÁöÑÂàõÊñ∞ÊñπÊ≥ï"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫Masked Degradation Classification Pre-TrainingÔºàMaskDCPTÔºâÁöÑÊñπÊ≥ïÔºåÊó®Âú®ÈÄöËøáÂØπËæìÂÖ•ÂõæÂÉèÁöÑÈôçË¥®Á±ªÂûãËøõË°åÂàÜÁ±ªÊù•Â¢ûÂº∫ÂõæÂÉèÊÅ¢Â§çÁöÑÊïàÊûú„ÄÇ‰∏é‰º†ÁªüÁöÑÈ¢ÑËÆ≠ÁªÉÊñπÊ≥ï‰∏çÂêåÔºåMaskDCPTÂà©Áî®ÂõæÂÉèÁöÑÈôçË¥®Á±ªÂûã‰Ωú‰∏∫ÊûÅÂº±ÁöÑÁõëÁù£‰ø°Âè∑ÔºåÂêåÊó∂ÁªìÂêàÂõæÂÉèÈáçÂª∫Êù•ÊèêÈ´òÊÄßËÉΩÂíåÈ≤ÅÊ£íÊÄß„ÄÇËØ•ÊñπÊ≥ïÂåÖÊã¨‰∏Ä‰∏™ÁºñÁ†ÅÂô®Âíå‰∏§‰∏™Ëß£Á†ÅÂô®ÔºåÁºñÁ†ÅÂô®‰ªé‰ΩéË¥®ÈáèÁöÑËæìÂÖ•ÂõæÂÉè‰∏≠ÊèêÂèñÁâπÂæÅÔºåÂàÜÁ±ªËß£Á†ÅÂô®Áî®‰∫éËØÜÂà´ÈôçË¥®Á±ªÂûãÔºåËÄåÈáçÂª∫Ëß£Á†ÅÂô®ÂàôÊó®Âú®ÈáçÂª∫Áõ∏Â∫îÁöÑÈ´òË¥®ÈáèÂõæÂÉè„ÄÇÈÄöËøáËøôÁßçËÆæËÆ°ÔºåMaskDCPTËÉΩÂ§üÊúâÊïàÂú∞ËøõË°åÂõæÂÉèÊÅ¢Â§ç‰ªªÂä°ÔºåÂπ∂Âú®Âç∑ÁßØÁ•ûÁªèÁΩëÁªúÔºàCNNsÔºâÂíåÂèòÊç¢Âô®ÔºàTransformersÔºâ‰∏äÊòæËëóÊèêÂçáÊÄßËÉΩ„ÄÇ', title='Masked Degradation ClassificationÔºöÊèêÂçáÂõæÂÉèÊÅ¢Â§çÁöÑÂàõÊñ∞ÊñπÊ≥ï'))
[16.10.2025 05:12] Using data from previous issue: {"categories": ["#architecture", "#agi", "#agents", "#training", "#3d", "#optimization"], "emoji": "ü§ñ", "ru": {"title": "–ú—è–≥–∫–∏–µ –ø—Ä–æ–º–ø—Ç—ã –¥–ª—è —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã—Ö —Ä–æ–±–æ—Ç–æ–≤", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ Soft Prompt –¥–ª—è –æ–±—É—á–µ–Ω–∏—è Vision-Language-Action –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–µ —É–ø—Ä–∞–≤–ª—è—é—Ç —Ä–∞–∑–Ω—ã–º–∏ —Ä–æ–±–æ—Ç–∞
[16.10.2025 05:12] Renaming data file.
[16.10.2025 05:12] Renaming previous data. hf_papers.json to ./d/2025-10-16.json
[16.10.2025 05:12] Saving new data file.
[16.10.2025 05:12] Generating page.
[16.10.2025 05:12] Renaming previous page.
[16.10.2025 05:12] Renaming previous data. index.html to ./d/2025-10-16.html
[16.10.2025 05:12] Writing result.
[16.10.2025 05:12] Renaming log file.
[16.10.2025 05:12] Renaming previous data. log.txt to ./logs/2025-10-16_last_log.txt
