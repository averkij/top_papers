[16.10.2025 18:17] Read previous papers.
[16.10.2025 18:17] Generating top page (month).
[16.10.2025 18:17] Writing top page (month).
[16.10.2025 19:08] Read previous papers.
[16.10.2025 19:08] Get feed.
[16.10.2025 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2510.13344
[16.10.2025 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2510.13678
[16.10.2025 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2510.13554
[16.10.2025 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2510.13626
[16.10.2025 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2510.13795
[16.10.2025 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2510.13809
[16.10.2025 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2510.13747
[16.10.2025 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2510.07944
[16.10.2025 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2510.13804
[16.10.2025 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2510.13802
[16.10.2025 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2510.04767
[16.10.2025 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2510.13800
[16.10.2025 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2510.13778
[16.10.2025 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2510.13621
[16.10.2025 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2510.13515
[16.10.2025 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2510.11438
[16.10.2025 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2510.13759
[16.10.2025 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2510.13282
[16.10.2025 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2510.10274
[16.10.2025 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2510.10977
[16.10.2025 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2510.10921
[16.10.2025 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2510.11958
[16.10.2025 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2510.13786
[16.10.2025 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2510.13602
[16.10.2025 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2510.12560
[16.10.2025 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2510.11062
[16.10.2025 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2510.13744
[16.10.2025 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2510.10611
[16.10.2025 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2510.12866
[16.10.2025 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2510.12831
[16.10.2025 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2510.10581
[16.10.2025 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2510.13714
[16.10.2025 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2510.13586
[16.10.2025 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2510.13255
[16.10.2025 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2510.11715
[16.10.2025 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2510.11653
[16.10.2025 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2510.11170
[16.10.2025 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2510.10930
[16.10.2025 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2510.10494
[16.10.2025 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2510.07414
[16.10.2025 19:08] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[16.10.2025 19:08] No deleted papers detected.
[16.10.2025 19:08] Downloading and parsing papers (pdf, html). Total: 40.
[16.10.2025 19:08] Downloading and parsing paper https://huggingface.co/papers/2510.13344.
[16.10.2025 19:08] Extra JSON file exists (./assets/json/2510.13344.json), skip PDF parsing.
[16.10.2025 19:08] Paper image links file exists (./assets/img_data/2510.13344.json), skip HTML parsing.
[16.10.2025 19:08] Success.
[16.10.2025 19:08] Downloading and parsing paper https://huggingface.co/papers/2510.13678.
[16.10.2025 19:08] Extra JSON file exists (./assets/json/2510.13678.json), skip PDF parsing.
[16.10.2025 19:08] Paper image links file exists (./assets/img_data/2510.13678.json), skip HTML parsing.
[16.10.2025 19:08] Success.
[16.10.2025 19:08] Downloading and parsing paper https://huggingface.co/papers/2510.13554.
[16.10.2025 19:08] Extra JSON file exists (./assets/json/2510.13554.json), skip PDF parsing.
[16.10.2025 19:08] Paper image links file exists (./assets/img_data/2510.13554.json), skip HTML parsing.
[16.10.2025 19:08] Success.
[16.10.2025 19:08] Downloading and parsing paper https://huggingface.co/papers/2510.13626.
[16.10.2025 19:08] Extra JSON file exists (./assets/json/2510.13626.json), skip PDF parsing.
[16.10.2025 19:08] Paper image links file exists (./assets/img_data/2510.13626.json), skip HTML parsing.
[16.10.2025 19:08] Success.
[16.10.2025 19:08] Downloading and parsing paper https://huggingface.co/papers/2510.13795.
[16.10.2025 19:08] Extra JSON file exists (./assets/json/2510.13795.json), skip PDF parsing.
[16.10.2025 19:08] Paper image links file exists (./assets/img_data/2510.13795.json), skip HTML parsing.
[16.10.2025 19:08] Success.
[16.10.2025 19:08] Downloading and parsing paper https://huggingface.co/papers/2510.13809.
[16.10.2025 19:08] Extra JSON file exists (./assets/json/2510.13809.json), skip PDF parsing.
[16.10.2025 19:08] Paper image links file exists (./assets/img_data/2510.13809.json), skip HTML parsing.
[16.10.2025 19:08] Success.
[16.10.2025 19:08] Downloading and parsing paper https://huggingface.co/papers/2510.13747.
[16.10.2025 19:08] Extra JSON file exists (./assets/json/2510.13747.json), skip PDF parsing.
[16.10.2025 19:08] Paper image links file exists (./assets/img_data/2510.13747.json), skip HTML parsing.
[16.10.2025 19:08] Success.
[16.10.2025 19:08] Downloading and parsing paper https://huggingface.co/papers/2510.07944.
[16.10.2025 19:08] Extra JSON file exists (./assets/json/2510.07944.json), skip PDF parsing.
[16.10.2025 19:08] Paper image links file exists (./assets/img_data/2510.07944.json), skip HTML parsing.
[16.10.2025 19:08] Success.
[16.10.2025 19:08] Downloading and parsing paper https://huggingface.co/papers/2510.13804.
[16.10.2025 19:08] Extra JSON file exists (./assets/json/2510.13804.json), skip PDF parsing.
[16.10.2025 19:08] Paper image links file exists (./assets/img_data/2510.13804.json), skip HTML parsing.
[16.10.2025 19:08] Success.
[16.10.2025 19:08] Downloading and parsing paper https://huggingface.co/papers/2510.13802.
[16.10.2025 19:08] Extra JSON file exists (./assets/json/2510.13802.json), skip PDF parsing.
[16.10.2025 19:08] Paper image links file exists (./assets/img_data/2510.13802.json), skip HTML parsing.
[16.10.2025 19:08] Success.
[16.10.2025 19:08] Downloading and parsing paper https://huggingface.co/papers/2510.04767.
[16.10.2025 19:08] Extra JSON file exists (./assets/json/2510.04767.json), skip PDF parsing.
[16.10.2025 19:08] Paper image links file exists (./assets/img_data/2510.04767.json), skip HTML parsing.
[16.10.2025 19:08] Success.
[16.10.2025 19:08] Downloading and parsing paper https://huggingface.co/papers/2510.13800.
[16.10.2025 19:08] Extra JSON file exists (./assets/json/2510.13800.json), skip PDF parsing.
[16.10.2025 19:08] Paper image links file exists (./assets/img_data/2510.13800.json), skip HTML parsing.
[16.10.2025 19:08] Success.
[16.10.2025 19:08] Downloading and parsing paper https://huggingface.co/papers/2510.13778.
[16.10.2025 19:08] Extra JSON file exists (./assets/json/2510.13778.json), skip PDF parsing.
[16.10.2025 19:08] Paper image links file exists (./assets/img_data/2510.13778.json), skip HTML parsing.
[16.10.2025 19:08] Success.
[16.10.2025 19:08] Downloading and parsing paper https://huggingface.co/papers/2510.13621.
[16.10.2025 19:08] Extra JSON file exists (./assets/json/2510.13621.json), skip PDF parsing.
[16.10.2025 19:08] Paper image links file exists (./assets/img_data/2510.13621.json), skip HTML parsing.
[16.10.2025 19:08] Success.
[16.10.2025 19:08] Downloading and parsing paper https://huggingface.co/papers/2510.13515.
[16.10.2025 19:08] Extra JSON file exists (./assets/json/2510.13515.json), skip PDF parsing.
[16.10.2025 19:08] Paper image links file exists (./assets/img_data/2510.13515.json), skip HTML parsing.
[16.10.2025 19:08] Success.
[16.10.2025 19:08] Downloading and parsing paper https://huggingface.co/papers/2510.11438.
[16.10.2025 19:08] Extra JSON file exists (./assets/json/2510.11438.json), skip PDF parsing.
[16.10.2025 19:08] Paper image links file exists (./assets/img_data/2510.11438.json), skip HTML parsing.
[16.10.2025 19:08] Success.
[16.10.2025 19:08] Downloading and parsing paper https://huggingface.co/papers/2510.13759.
[16.10.2025 19:08] Extra JSON file exists (./assets/json/2510.13759.json), skip PDF parsing.
[16.10.2025 19:08] Paper image links file exists (./assets/img_data/2510.13759.json), skip HTML parsing.
[16.10.2025 19:08] Success.
[16.10.2025 19:08] Downloading and parsing paper https://huggingface.co/papers/2510.13282.
[16.10.2025 19:08] Extra JSON file exists (./assets/json/2510.13282.json), skip PDF parsing.
[16.10.2025 19:08] Paper image links file exists (./assets/img_data/2510.13282.json), skip HTML parsing.
[16.10.2025 19:08] Success.
[16.10.2025 19:08] Downloading and parsing paper https://huggingface.co/papers/2510.10274.
[16.10.2025 19:08] Extra JSON file exists (./assets/json/2510.10274.json), skip PDF parsing.
[16.10.2025 19:08] Paper image links file exists (./assets/img_data/2510.10274.json), skip HTML parsing.
[16.10.2025 19:08] Success.
[16.10.2025 19:08] Downloading and parsing paper https://huggingface.co/papers/2510.10977.
[16.10.2025 19:08] Extra JSON file exists (./assets/json/2510.10977.json), skip PDF parsing.
[16.10.2025 19:08] Paper image links file exists (./assets/img_data/2510.10977.json), skip HTML parsing.
[16.10.2025 19:08] Success.
[16.10.2025 19:08] Downloading and parsing paper https://huggingface.co/papers/2510.10921.
[16.10.2025 19:08] Extra JSON file exists (./assets/json/2510.10921.json), skip PDF parsing.
[16.10.2025 19:08] Paper image links file exists (./assets/img_data/2510.10921.json), skip HTML parsing.
[16.10.2025 19:08] Success.
[16.10.2025 19:08] Downloading and parsing paper https://huggingface.co/papers/2510.11958.
[16.10.2025 19:08] Extra JSON file exists (./assets/json/2510.11958.json), skip PDF parsing.
[16.10.2025 19:08] Paper image links file exists (./assets/img_data/2510.11958.json), skip HTML parsing.
[16.10.2025 19:08] Success.
[16.10.2025 19:08] Downloading and parsing paper https://huggingface.co/papers/2510.13786.
[16.10.2025 19:08] Extra JSON file exists (./assets/json/2510.13786.json), skip PDF parsing.
[16.10.2025 19:08] Paper image links file exists (./assets/img_data/2510.13786.json), skip HTML parsing.
[16.10.2025 19:08] Success.
[16.10.2025 19:08] Downloading and parsing paper https://huggingface.co/papers/2510.13602.
[16.10.2025 19:08] Extra JSON file exists (./assets/json/2510.13602.json), skip PDF parsing.
[16.10.2025 19:08] Paper image links file exists (./assets/img_data/2510.13602.json), skip HTML parsing.
[16.10.2025 19:08] Success.
[16.10.2025 19:08] Downloading and parsing paper https://huggingface.co/papers/2510.12560.
[16.10.2025 19:08] Extra JSON file exists (./assets/json/2510.12560.json), skip PDF parsing.
[16.10.2025 19:08] Paper image links file exists (./assets/img_data/2510.12560.json), skip HTML parsing.
[16.10.2025 19:08] Success.
[16.10.2025 19:08] Downloading and parsing paper https://huggingface.co/papers/2510.11062.
[16.10.2025 19:08] Extra JSON file exists (./assets/json/2510.11062.json), skip PDF parsing.
[16.10.2025 19:08] Paper image links file exists (./assets/img_data/2510.11062.json), skip HTML parsing.
[16.10.2025 19:08] Success.
[16.10.2025 19:08] Downloading and parsing paper https://huggingface.co/papers/2510.13744.
[16.10.2025 19:08] Extra JSON file exists (./assets/json/2510.13744.json), skip PDF parsing.
[16.10.2025 19:08] Paper image links file exists (./assets/img_data/2510.13744.json), skip HTML parsing.
[16.10.2025 19:08] Success.
[16.10.2025 19:08] Downloading and parsing paper https://huggingface.co/papers/2510.10611.
[16.10.2025 19:08] Extra JSON file exists (./assets/json/2510.10611.json), skip PDF parsing.
[16.10.2025 19:08] Paper image links file exists (./assets/img_data/2510.10611.json), skip HTML parsing.
[16.10.2025 19:08] Success.
[16.10.2025 19:08] Downloading and parsing paper https://huggingface.co/papers/2510.12866.
[16.10.2025 19:08] Extra JSON file exists (./assets/json/2510.12866.json), skip PDF parsing.
[16.10.2025 19:08] Paper image links file exists (./assets/img_data/2510.12866.json), skip HTML parsing.
[16.10.2025 19:08] Success.
[16.10.2025 19:08] Downloading and parsing paper https://huggingface.co/papers/2510.12831.
[16.10.2025 19:08] Extra JSON file exists (./assets/json/2510.12831.json), skip PDF parsing.
[16.10.2025 19:08] Paper image links file exists (./assets/img_data/2510.12831.json), skip HTML parsing.
[16.10.2025 19:08] Success.
[16.10.2025 19:08] Downloading and parsing paper https://huggingface.co/papers/2510.10581.
[16.10.2025 19:08] Extra JSON file exists (./assets/json/2510.10581.json), skip PDF parsing.
[16.10.2025 19:08] Paper image links file exists (./assets/img_data/2510.10581.json), skip HTML parsing.
[16.10.2025 19:08] Success.
[16.10.2025 19:08] Downloading and parsing paper https://huggingface.co/papers/2510.13714.
[16.10.2025 19:08] Extra JSON file exists (./assets/json/2510.13714.json), skip PDF parsing.
[16.10.2025 19:08] Paper image links file exists (./assets/img_data/2510.13714.json), skip HTML parsing.
[16.10.2025 19:08] Success.
[16.10.2025 19:08] Downloading and parsing paper https://huggingface.co/papers/2510.13586.
[16.10.2025 19:08] Extra JSON file exists (./assets/json/2510.13586.json), skip PDF parsing.
[16.10.2025 19:08] Paper image links file exists (./assets/img_data/2510.13586.json), skip HTML parsing.
[16.10.2025 19:08] Success.
[16.10.2025 19:08] Downloading and parsing paper https://huggingface.co/papers/2510.13255.
[16.10.2025 19:08] Extra JSON file exists (./assets/json/2510.13255.json), skip PDF parsing.
[16.10.2025 19:08] Paper image links file exists (./assets/img_data/2510.13255.json), skip HTML parsing.
[16.10.2025 19:08] Success.
[16.10.2025 19:08] Downloading and parsing paper https://huggingface.co/papers/2510.11715.
[16.10.2025 19:08] Extra JSON file exists (./assets/json/2510.11715.json), skip PDF parsing.
[16.10.2025 19:08] Paper image links file exists (./assets/img_data/2510.11715.json), skip HTML parsing.
[16.10.2025 19:08] Success.
[16.10.2025 19:08] Downloading and parsing paper https://huggingface.co/papers/2510.11653.
[16.10.2025 19:08] Extra JSON file exists (./assets/json/2510.11653.json), skip PDF parsing.
[16.10.2025 19:08] Paper image links file exists (./assets/img_data/2510.11653.json), skip HTML parsing.
[16.10.2025 19:08] Success.
[16.10.2025 19:08] Downloading and parsing paper https://huggingface.co/papers/2510.11170.
[16.10.2025 19:08] Extra JSON file exists (./assets/json/2510.11170.json), skip PDF parsing.
[16.10.2025 19:08] Paper image links file exists (./assets/img_data/2510.11170.json), skip HTML parsing.
[16.10.2025 19:08] Success.
[16.10.2025 19:08] Downloading and parsing paper https://huggingface.co/papers/2510.10930.
[16.10.2025 19:08] Extra JSON file exists (./assets/json/2510.10930.json), skip PDF parsing.
[16.10.2025 19:08] Paper image links file exists (./assets/img_data/2510.10930.json), skip HTML parsing.
[16.10.2025 19:08] Success.
[16.10.2025 19:08] Downloading and parsing paper https://huggingface.co/papers/2510.10494.
[16.10.2025 19:08] Extra JSON file exists (./assets/json/2510.10494.json), skip PDF parsing.
[16.10.2025 19:08] Paper image links file exists (./assets/img_data/2510.10494.json), skip HTML parsing.
[16.10.2025 19:08] Success.
[16.10.2025 19:08] Downloading and parsing paper https://huggingface.co/papers/2510.07414.
[16.10.2025 19:08] Extra JSON file exists (./assets/json/2510.07414.json), skip PDF parsing.
[16.10.2025 19:08] Paper image links file exists (./assets/img_data/2510.07414.json), skip HTML parsing.
[16.10.2025 19:08] Success.
[16.10.2025 19:08] Enriching papers with extra data.
[16.10.2025 19:08] ********************************************************************************
[16.10.2025 19:08] Abstract 0. UniMoE-Audio, a unified speech and music generation model using a Dynamic-Capacity Mixture-of-Experts framework, addresses data imbalance and task conflicts, achieving state-of-the-art performance and enhanced cross-domain synergy.  					AI-generated summary 				 Recent advances in unified multimoda...
[16.10.2025 19:08] ********************************************************************************
[16.10.2025 19:08] Abstract 1. FlashWorld generates 3D scenes from single images or text prompts quickly and with high quality by combining MV-oriented and 3D-oriented generation methods.  					AI-generated summary 				 We propose FlashWorld, a generative model that produces 3D scenes from a single image or text prompt in seconds...
[16.10.2025 19:08] ********************************************************************************
[16.10.2025 19:08] Abstract 2. Attention mechanisms in LLMs are analyzed to reveal reasoning patterns, leading to novel RL strategies that improve performance by focusing on critical tokens.  					AI-generated summary 				 The reasoning pattern of Large language models (LLMs) remains opaque, and Reinforcement learning (RL) typica...
[16.10.2025 19:08] ********************************************************************************
[16.10.2025 19:08] Abstract 3. State-of-the-art Visual-Language-Action models show high benchmark scores but are brittle to various perturbations, particularly in camera viewpoints and robot initial states, and often ignore language instructions.  					AI-generated summary 				 Visual-Language-Action (VLA) models report impressiv...
[16.10.2025 19:08] ********************************************************************************
[16.10.2025 19:08] Abstract 4. A new dataset and pipeline for data curation improve the performance of fully open multimodal large language models, achieving state-of-the-art results competitive with semi-open models.  					AI-generated summary 				 Fully open multimodal large language models (MLLMs) currently lag behind propriet...
[16.10.2025 19:08] ********************************************************************************
[16.10.2025 19:08] Abstract 5. PhysMaster enhances video generation by integrating physical knowledge through PhysEncoder, using reinforcement learning and Direct Preference Optimization to improve physics-awareness.  					AI-generated summary 				 Video generation models nowadays are capable of generating visually realistic vide...
[16.10.2025 19:08] ********************************************************************************
[16.10.2025 19:08] Abstract 6. InteractiveOmni is a unified omni-modal large language model for audio-visual multi-turn interactions, offering comprehensive understanding and speech generation capabilities with efficient parameter usage.  					AI-generated summary 				 We introduce InteractiveOmni, a unified and open-source omni-...
[16.10.2025 19:08] ********************************************************************************
[16.10.2025 19:08] Abstract 7. CVD-STORM, a cross-view video diffusion model with a spatial-temporal reconstruction VAE, enhances video generation quality and provides depth estimation for dynamic scenes.  					AI-generated summary 				 Generative models have been widely applied to world modeling for environment simulation and fu...
[16.10.2025 19:08] ********************************************************************************
[16.10.2025 19:08] Abstract 8. Generative Universal Verifier enhances multimodal reasoning by providing reliable visual verification through ViVerBench, OmniVerifier-7B, and OmniVerifier-TTS, improving generation and refinement capabilities.  					AI-generated summary 				 We introduce Generative Universal Verifier, a novel conce...
[16.10.2025 19:08] ********************************************************************************
[16.10.2025 19:08] Abstract 9. Trace Anything, a neural network, predicts video trajectories in a single pass, achieving state-of-the-art performance and demonstrating efficiency and emergent abilities like motion forecasting.  					AI-generated summary 				 Effective spatio-temporal representation is fundamental to modeling, und...
[16.10.2025 19:08] ********************************************************************************
[16.10.2025 19:08] Abstract 10. Parallel decoding in diffusion LLMs degrades generation quality due to ignored token dependencies, highlighting the need for new decoding methods and benchmarks.  					AI-generated summary 				 While most autoregressive LLMs are constrained to one-by-one decoding, diffusion LLMs (dLLMs) have attract...
[16.10.2025 19:08] ********************************************************************************
[16.10.2025 19:08] Abstract 11. GS-Reasoner, a 3D LLM with a dual-path pooling mechanism, achieves autoregressive grounding and state-of-the-art spatial reasoning without external modules.  					AI-generated summary 				 In this paper, we claim that 3D visual grounding is the cornerstone of spatial reasoning and introduce the Grou...
[16.10.2025 19:08] ********************************************************************************
[16.10.2025 19:08] Abstract 12. A unified framework for spatial grounding and robot control, InternVLA-M1, enhances instruction-following robots through spatially guided vision-language-action training, achieving significant improvements across various tasks and simulations.  					AI-generated summary 				 We introduce InternVLA-M...
[16.10.2025 19:08] ********************************************************************************
[16.10.2025 19:08] Abstract 13. Increased computing resources are correlated with national funding and citations in foundation model research, but not with research environment, domain, or methodology.  					AI-generated summary 				 Cutting-edge research in Artificial Intelligence (AI) requires considerable resources, including G...
[16.10.2025 19:08] ********************************************************************************
[16.10.2025 19:08] Abstract 14. A novel Universal Multimodal Embedding (UniME-V2) model uses MLLMs to enhance representation learning by identifying diverse, high-quality hard negatives and improving discriminative capacity through soft semantic matching scores.  					AI-generated summary 				 Universal multimodal embedding models...
[16.10.2025 19:08] ********************************************************************************
[16.10.2025 19:08] Abstract 15. AutoGEO, a framework for optimizing generative engines, learns and applies preference rules to enhance content traction and search utility using large language models.  					AI-generated summary 				 By employing large language models (LLMs) to retrieve documents and generate natural language respon...
[16.10.2025 19:08] ********************************************************************************
[16.10.2025 19:08] Abstract 16. Uni-MMMU is a benchmark that evaluates the bidirectional synergy between visual understanding and generation across multiple domains, providing insights into their integration and performance.  					AI-generated summary 				 Unified multimodal models aim to jointly enable visual understanding and ge...
[16.10.2025 19:08] ********************************************************************************
[16.10.2025 19:08] Abstract 17. A Masked Degradation Classification Pre-Training method enhances image restoration by using degradation type classification and image reconstruction, improving performance across CNNs and Transformers.  					AI-generated summary 				 This study introduces a Masked Degradation Classification Pre-Trai...
[16.10.2025 19:08] ********************************************************************************
[16.10.2025 19:08] Abstract 18. A novel Soft Prompt approach enhances Vision-Language-Action models by using learnable embeddings for diverse robotic data, enabling superior performance across simulations and real-world robots.  					AI-generated summary 				 Successful generalist Vision-Language-Action (VLA) models rely on effect...
[16.10.2025 19:08] ********************************************************************************
[16.10.2025 19:08] Abstract 19. Model merging, typically on Instruct and Thinking models, has shown remarkable performance for efficient reasoning. In this paper, we systematically revisit the simplest merging method that interpolates two weights directly. Particularly, we observe that model interpolation follows a three-stage evo...
[16.10.2025 19:08] ********************************************************************************
[16.10.2025 19:08] Abstract 20. FG-CLIP 2, a bilingual vision-language model, enhances fine-grained alignment for English and Chinese through rich supervision and a new TIC loss, achieving state-of-the-art performance across multiple datasets and tasks.  					AI-generated summary 				 Fine-grained vision-language understanding req...
[16.10.2025 19:08] ********************************************************************************
[16.10.2025 19:08] Abstract 21. Direct Multi-Token Decoding (DMTD) accelerates large language model inference by using only late layers for token generation, achieving significant speedup with minimal performance loss.  					AI-generated summary 				 Decoder-only transformers have become the standard architecture for large languag...
[16.10.2025 19:08] ********************************************************************************
[16.10.2025 19:08] Abstract 22. A systematic study defines a framework for analyzing and predicting reinforcement learning scaling in large language models, identifying key design choices that affect compute efficiency and proposing a best-practice recipe.  					AI-generated summary 				 Reinforcement learning (RL) has become cent...
[16.10.2025 19:08] ********************************************************************************
[16.10.2025 19:08] Abstract 23. NOSA, a trainable sparse attention framework, enhances decoding throughput by enabling efficient KV cache offloading without compromising performance.  					AI-generated summary 				 Trainable sparse attention has emerged as a promising solution to address the decoding efficiency bottleneck of LLMs ...
[16.10.2025 19:08] ********************************************************************************
[16.10.2025 19:08] Abstract 24. End-to-end autonomous driving models trained solely with imitation learning (IL) often suffer from poor generalization. In contrast, reinforcement learning (RL) promotes exploration through reward maximization but faces challenges such as sample inefficiency and unstable convergence. A natural solut...
[16.10.2025 19:08] ********************************************************************************
[16.10.2025 19:08] Abstract 25. AT-GRPO, a tailored RL algorithm for multi-agent systems, significantly enhances performance across various tasks by addressing unique challenges in on-policy RL.  					AI-generated summary 				 Multi-agent systems (MAS) and reinforcement learning (RL) are widely used to enhance the agentic capabili...
[16.10.2025 19:08] ********************************************************************************
[16.10.2025 19:08] Abstract 26. Hard2Verify, a human-annotated benchmark, evaluates step-level verifiers for LLM-based mathematical reasoning systems, highlighting the challenges and performance gaps between open-source and closed-source models.  					AI-generated summary 				 Large language model (LLM)-based reasoning systems hav...
[16.10.2025 19:08] ********************************************************************************
[16.10.2025 19:08] Abstract 27. HyperAgent, a hypergraph-based framework, optimizes communication topologies and captures group collaboration patterns, improving performance and efficiency in multi-agent systems.  					AI-generated summary 				 Recent advances in large language model-powered multi-agent systems have demonstrated r...
[16.10.2025 19:08] ********************************************************************************
[16.10.2025 19:08] Abstract 28. Robots can achieve generalizable grasping skills by learning from a small set of simple objects, using an object-centric visual representation, which outperforms state-of-the-art methods with less data.  					AI-generated summary 				 Robotic manipulation policies often struggle to generalize to nov...
[16.10.2025 19:08] ********************************************************************************
[16.10.2025 19:08] Abstract 29. MTSQL-R1, an agentic training framework, improves multi-turn Text-to-SQL by treating it as an MDP with iterative propose-execute-verify-refine cycles, enhancing coherence and execution.  					AI-generated summary 				 Multi-turn Text-to-SQL aims to translate a user's conversational utterances into e...
[16.10.2025 19:08] ********************************************************************************
[16.10.2025 19:08] Abstract 30. GraphTracer addresses multi-agent system failure attribution by constructing Information Dependency Graphs to trace information flow and improve debugging accuracy.  					AI-generated summary 				 Multi-agent systems powered by Large Language Models excel at complex tasks through coordinated collabo...
[16.10.2025 19:08] ********************************************************************************
[16.10.2025 19:08] Abstract 31. Dedelayed, a delay-corrective method, improves real-time semantic segmentation accuracy by fusing local and remote model outputs, mitigating communication network latency.  					AI-generated summary 				 Remote inference allows lightweight devices to leverage powerful cloud models. However, communic...
[16.10.2025 19:08] ********************************************************************************
[16.10.2025 19:08] Abstract 32. Participants in the CPDC 2025 used lightweight prompting and fine-tuned large models to achieve high rankings in task-oriented and context-aware dialogue challenges.  					AI-generated summary 				 The emergence of large language models (LLMs) has opened new opportunities for cre- ating dynamic non-...
[16.10.2025 19:08] ********************************************************************************
[16.10.2025 19:08] Abstract 33. Hierarchical Frequency Tagging Probe (HFTP) identifies neuron-wise components in LLMs and cortical regions encoding syntactic structures, revealing differences in how LLMs and the human brain process syntax.  					AI-generated summary 				 Large Language Models (LLMs) demonstrate human-level or even...
[16.10.2025 19:08] ********************************************************************************
[16.10.2025 19:08] Abstract 34. Pretrained video diffusion models can perform zero-shot point tracking by visually marking points and regenerating video frames, outperforming prior methods and handling occlusions.  					AI-generated summary 				 Trackers and video generators solve closely related problems: the former analyze motio...
[16.10.2025 19:08] ********************************************************************************
[16.10.2025 19:08] Abstract 35. MATH-Beyond is a benchmark designed to challenge existing reinforcement learning methods by requiring deeper reasoning capabilities beyond current model capabilities.  					AI-generated summary 				 With the advent of DeepSeek-R1, a new wave of reinforcement learning (RL) methods has emerged that se...
[16.10.2025 19:08] ********************************************************************************
[16.10.2025 19:08] Abstract 36. EAGer, a training-free method, uses token-wise entropy to optimize computational resources and improve performance on complex reasoning tasks.  					AI-generated summary 				 With the rise of reasoning language models and test-time scaling methods as a paradigm for improving model performance, subst...
[16.10.2025 19:08] ********************************************************************************
[16.10.2025 19:08] Abstract 37. Modern reasoning models are more aligned with human evaluations of games than non-reasoning models, but their performance can degrade as they approach game-theoretic optimality, especially for subjective assessments like funness.  					AI-generated summary 				 Reasoning is not just about solving pr...
[16.10.2025 19:08] ********************************************************************************
[16.10.2025 19:08] Abstract 38. Latent-Trajectory signals improve inference-time efficiency by predicting productive reasoning paths, reducing token usage and enhancing accuracy.  					AI-generated summary 				 Reasoning models improve their problem-solving ability through inference-time scaling, allocating more compute via longer...
[16.10.2025 19:08] ********************************************************************************
[16.10.2025 19:08] Abstract 39. HaystackCraft, a new benchmark using Wikipedia, evaluates long-context LLM robustness by simulating noisy retrieval and agentic workflows, revealing challenges in handling distractors and cascading errors.  					AI-generated summary 				 Modern long-context large language models (LLMs) perform well ...
[16.10.2025 19:08] Read previous papers.
[16.10.2025 19:08] Generating reviews via LLM API.
[16.10.2025 19:08] Using data from previous issue: {"categories": ["#architecture", "#games", "#benchmark", "#optimization", "#multimodal", "#audio", "#training"], "emoji": "üéµ", "ru": {"title": "–ï–¥–∏–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Ä–µ—á–∏ –∏ –º—É–∑—ã–∫–∏ —á–µ—Ä–µ–∑ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫—É—é Mixture-of-Experts", "desc": "UniMoE-Audio ‚Äî —ç—Ç–æ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–µ—á–∏ –∏ –º—É–∑—ã–∫–∏, –æ—Å–Ω–æ
[16.10.2025 19:08] Using data from previous issue: {"categories": ["#3d", "#optimization", "#diffusion"], "emoji": "‚ö°", "ru": {"title": "–ú–≥–Ω–æ–≤–µ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è 3D-–º–∏—Ä–æ–≤: —Å–∫–æ—Ä–æ—Å—Ç—å –∏ –∫–∞—á–µ—Å—Ç–≤–æ –≤–º–µ—Å—Ç–µ", "desc": "FlashWorld ‚Äî —ç—Ç–æ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è —Å–æ–∑–¥–∞—ë—Ç 3D-—Å—Ü–µ–Ω—ã –∏–∑ –æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–ª–∏ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞ –∑–∞ —Å—á–∏—Ç–∞–Ω–Ω—ã–µ —Å–µ–∫—É–Ω–¥—ã, —Ä–∞–±–æ—Ç–∞—è –≤ 10-10
[16.10.2025 19:08] Using data from previous issue: {"categories": ["#rl", "#reasoning", "#optimization", "#interpretability", "#training"], "emoji": "üîç", "ru": {"title": "–í–Ω–∏–º–∞–Ω–∏–µ –∫–∞–∫ –∫–∞—Ä—Ç–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π: –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —á–µ—Ä–µ–∑ –∫–ª—é—á–µ–≤—ã–µ —Ç–æ–∫–µ–Ω—ã", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç –º–µ—Ö–∞–Ω–∏–∑–º—ã attention –≤ LLM, —á—Ç–æ–±—ã –ø–æ–Ω—è—Ç—å, –∫–∞–∫ –º–æ–¥–µ–ª–∏ —Ä–∞—Å—Å—É–∂–¥–∞—é—Ç, –≤—ã—è
[16.10.2025 19:08] Using data from previous issue: {"categories": ["#agents", "#benchmark", "#video", "#interpretability", "#security"], "emoji": "ü§ñ", "ru": {"title": "–•—Ä—É–ø–∫–æ—Å—Ç—å VLA –º–æ–¥–µ–ª–µ–π: –≤—ã—Å–æ–∫–∏–µ –±–µ–Ω—á–º–∞—Ä–∫–∏ —Å–∫—Ä—ã–≤–∞—é—Ç –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ —É—è–∑–≤–∏–º–æ—Å—Ç–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–æ–≤–µ–ª–∏ —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ —É—è–∑–≤–∏–º–æ—Å—Ç–µ–π —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö Visual-Language-Action (VLA) –º
[16.10.2025 19:08] Using data from previous issue: {"categories": ["#data", "#transfer_learning", "#dataset", "#optimization", "#open_source"], "emoji": "üêù", "ru": {"title": "–ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ ‚Äî –∫–ª—é—á –∫ –æ—Ç–∫—Ä—ã—Ç—ã–º –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–º –º–æ–¥–µ–ª—è–º", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Å–æ–∑–¥–∞–ª–∏ –Ω–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç Honey-Data-15M –∏–∑ 15 –º–∏–ª–ª–∏–æ–Ω–æ–≤ –ø–∞—Ä –≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º—É–ª—å—Ç–∏
[16.10.2025 19:08] Using data from previous issue: {"categories": ["#rlhf", "#games", "#rl", "#video", "#multimodal", "#optimization"], "emoji": "üéØ", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ –≤–∏–¥–µ–æ-–º–æ–¥–µ–ª–µ–π —Ñ–∏–∑–∏—á–µ—Å–∫–∏–º –∑–∞–∫–æ–Ω–∞–º —á–µ—Ä–µ–∑ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è", "desc": "PhysMaster —É–ª—É—á—à–∞–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –≤–∏–¥–µ–æ, –¥–æ–±–∞–≤–ª—è—è –ø–æ–Ω–∏–º–∞–Ω–∏–µ —Ñ–∏–∑–∏—á–µ—Å–∫–∏—Ö –∑–∞–∫–æ–Ω–æ–≤ —á–µ—Ä–µ–∑ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π PhysEncoder. –°–∏—Å—Ç–µ–º
[16.10.2025 19:08] Using data from previous issue: {"categories": ["#video", "#small_models", "#long_context", "#dataset", "#benchmark", "#multimodal", "#audio", "#open_source", "#training"], "emoji": "üé≠", "ru": {"title": "–õ–µ–≥–∫–æ–≤–µ—Å–Ω–∞—è –æ–º–Ω–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å —Å –¥–æ–ª–≥–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–π –ø–∞–º—è—Ç—å—é –¥–ª—è –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∞—É–¥–∏–æ–≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è", "desc": "Interacti
[16.10.2025 19:08] Using data from previous issue: {"categories": ["#architecture", "#3d", "#optimization", "#video", "#multimodal", "#diffusion"], "emoji": "üöó", "ru": {"title": "4D-—Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö —Å—Ü–µ–Ω —á–µ—Ä–µ–∑ –≤–∏–¥–µ–æ –¥–∏—Ñ—Ñ—É–∑–∏—é", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç CVD-STORM ‚Äî –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ä–∞–∫—É—Ä—Å–æ–≤, –∫–æ—Ç–æ—Ä–∞—è —Ç–∞–∫–∂–µ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç
[16.10.2025 19:08] Using data from previous issue: {"categories": ["#reasoning", "#dataset", "#multimodal", "#optimization", "#benchmark"], "emoji": "üîç", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ç–æ—Ä –¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Generative Universal Verifier ‚Äî –Ω–æ–≤—É—é –∫–æ–Ω—Ü–µ–ø—Ü–∏—é –¥–ª—è 
[16.10.2025 19:08] Using data from previous issue: {"categories": ["#training", "#games", "#video", "#dataset", "#optimization", "#benchmark"], "emoji": "üéØ", "ru": {"title": "–û—Ç—Å–ª–µ–¥–∏—Ç—å –≤—Å—ë: –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π –≤—Å–µ—Ö –ø–∏–∫—Å–µ–ª–µ–π –≤–∏–¥–µ–æ –∑–∞ –æ–¥–∏–Ω –ø—Ä–æ—Ö–æ–¥", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Trace Anything ‚Äî –Ω–µ–π—Ä–æ—Å–µ—Ç—å, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –¥–≤–∏–∂–µ–Ω–∏—è –≤
[16.10.2025 19:08] Using data from previous issue: {"categories": ["#inference", "#diffusion", "#benchmark", "#optimization"], "emoji": "‚ö°", "ru": {"title": "–ü—Ä–æ–±–ª–µ–º–∞ —Å–∫–æ—Ä–æ—Å—Ç–∏ –ø—Ä–æ—Ç–∏–≤ –∫–∞—á–µ—Å—Ç–≤–∞ –≤ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–º –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–∏ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö LLM", "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –≤ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (dLLM), –∫–æ—Ç–æ—Ä—ã–µ 
[16.10.2025 19:08] Using data from previous issue: {"categories": ["#games", "#3d", "#dataset", "#cv", "#reasoning"], "emoji": "üéØ", "ru": {"title": "3D-reasoning —á–µ—Ä–µ–∑ –µ–¥–∏–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ: grounding –±–µ–∑ –≤–Ω–µ—à–Ω–∏—Ö –º–æ–¥—É–ª–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω GS-Reasoner ‚Äî 3D LLM —Å –º–µ—Ö–∞–Ω–∏–∑–º–æ–º –¥–≤—É—Ö–ø—É—Ç–µ–≤–æ–≥–æ pooling, –∫–æ—Ç–æ—Ä—ã–π –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é –∏ –≥–µ–æ–º–µ—Ç—Ä–∏
[16.10.2025 19:08] Using data from previous issue: {"categories": ["#science", "#agents", "#agi", "#optimization", "#robotics"], "emoji": "ü§ñ", "ru": {"title": "–†–æ–±–æ—Ç—ã —É—á–∞—Ç—Å—è –¥–µ–π—Å—Ç–≤–æ–≤–∞—Ç—å —á–µ—Ä–µ–∑ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞", "desc": "InternVLA-M1 ‚Äî —ç—Ç–æ –µ–¥–∏–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ä–æ–±–æ—Ç–∞–º–∏, –∫–æ—Ç–æ—Ä–∞—è —Å–≤—è–∑—ã–≤–∞–µ—Ç –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ —Å –¥–µ–π—Å—Ç–≤–∏—è–º–∏ —á–µ—Ä–µ–∑ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ –ø
[16.10.2025 19:08] Using data from previous issue: {"categories": ["#open_source", "#ethics"], "emoji": "üí∞", "ru": {"title": "–î–µ–Ω—å–≥–∏ —Ä–µ—à–∞—é—Ç: –∫–∞–∫ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã –≤–ª–∏—è—é—Ç –Ω–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è foundation models", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å–≤—è–∑—å –º–µ–∂–¥—É –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏ –∏ –Ω–∞—É—á–Ω—ã–º –ø—Ä–æ–≥—Ä–µ—Å—Å–æ–º –≤ –æ–±–ª–∞—Å—Ç–∏ foundation models (–±–æ–ª—å—à–∏—Ö –±–∞–∑–æ–≤—ã—Ö –º–æ
[16.10.2025 19:08] Using data from previous issue: {"categories": ["#optimization", "#benchmark", "#training", "#multimodal"], "emoji": "üéØ", "ru": {"title": "MLLM –∫–∞–∫ —Å—É–¥—å—è –¥–ª—è —É–º–Ω–æ–≥–æ –º–∞–π–Ω–∏–Ω–≥–∞ hard negatives –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–∞—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç UniME-V2 ‚Äî —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—É—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤, –∫–æ—Ç–æ—Ä–∞—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º—É
[16.10.2025 19:08] Using data from previous issue: {"categories": ["#multimodal", "#training", "#benchmark", "#optimization", "#dataset"], "emoji": "üîç", "ru": {"title": "–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–ª—è —ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ —Å –ø–æ–º–æ—â—å—é AI", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç AutoGEO ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –ø–æ–¥ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –ø–æ–∏—Å–∫–æ–≤—ã–µ —Å–∏—Å—Ç–µ–º—ã, —Ç–∞–∫–∏–µ 
[16.10.2025 19:08] Using data from previous issue: {"categories": ["#multimodal", "#survey", "#reasoning", "#benchmark"], "emoji": "üîÑ", "ru": {"title": "–î–≤—É—Å—Ç–æ—Ä–æ–Ω–Ω—è—è —Å–∏–Ω–µ—Ä–≥–∏—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "Uni-MMMU ‚Äî —ç—Ç–æ –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ unified –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–µ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –ø–æ–Ω–∏–º–∞—é—Ç –∏ –≥–µ–Ω–µ—Ä–∏—Ä—É—é—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è. –°—É—â–µ—Å—Ç–≤
[16.10.2025 19:08] Using data from previous issue: {"categories": ["#open_source", "#data", "#dataset", "#synthetic", "#optimization", "#cv"], "emoji": "üé≠", "ru": {"title": "–ü—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ —á–µ—Ä–µ–∑ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –¥–µ–≥—Ä–∞–¥–∞—Ü–∏–∏: —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è MaskDCPT –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –∏–∑–æ–±
[16.10.2025 19:08] Using data from previous issue: {"categories": ["#architecture", "#agi", "#agents", "#training", "#3d", "#optimization"], "emoji": "ü§ñ", "ru": {"title": "–ú—è–≥–∫–∏–µ –ø—Ä–æ–º–ø—Ç—ã –¥–ª—è —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã—Ö —Ä–æ–±–æ—Ç–æ–≤", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ Soft Prompt –¥–ª—è –æ–±—É—á–µ–Ω–∏—è Vision-Language-Action –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–µ —É–ø—Ä–∞–≤–ª—è—é—Ç —Ä–∞–∑–Ω—ã–º–∏ —Ä–æ–±–æ—Ç–∞
[16.10.2025 19:08] Using data from previous issue: {"categories": ["#architecture", "#reasoning", "#training", "#optimization"], "emoji": "üîÄ", "ru": {"title": "–¢—Ä–∏ —Å—Ç–∞–¥–∏–∏ –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏–∏: –ø—Ä–æ—Å—Ç–æ–µ —Å–ª–∏—è–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –ø–æ–±–µ–∂–¥–∞–µ—Ç —Å–ª–æ–∂–Ω—ã–µ –º–µ—Ç–æ–¥—ã", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –∏–∑—É—á–∏–ª–∏ –ø—Ä–æ—Å—Ç–µ–π—à–∏–π –º–µ—Ç–æ–¥ —Å–ª–∏—è–Ω–∏—è –º–æ–¥–µ–ª–µ–π - –ø—Ä—è–º—É—é –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—é –≤–µ—Å–æ–≤ –º–µ–∂–¥—É Instruct –∏ Thinki
[16.10.2025 19:08] Using data from previous issue: {"categories": ["#multilingual", "#dataset", "#benchmark", "#multimodal", "#alignment", "#open_source"], "emoji": "üîç", "ru": {"title": "–î–µ—Ç–∞–ª—å–Ω–æ–µ –¥–≤—É—è–∑—ã—á–Ω–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ —Ç–µ–∫—Å—Ç–∞", "desc": "FG-CLIP 2 ‚Äî —ç—Ç–æ –¥–≤—É—è–∑—ã—á–Ω–∞—è vision-language –º–æ–¥–µ–ª—å –¥–ª—è –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ –∏ –∫–∏—Ç–∞–π—Å–∫–æ–≥–æ —è–∑—ã–∫–æ–≤, –∫–æ—Ç–æ—Ä–∞—è —É–ª—É
[16.10.2025 19:08] Using data from previous issue: {"categories": ["#architecture", "#training", "#inference", "#optimization"], "emoji": "‚ö°", "ru": {"title": "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤ –∑–∞ –æ–¥–∏–Ω –ø—Ä–æ—Ö–æ–¥ —á–µ—Ä–µ–∑ –ø–æ–∑–¥–Ω–∏–µ —Å–ª–æ–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –º–µ—Ç–æ–¥ Direct Multi-Token Decoding (DMTD), –∫–æ—Ç–æ—Ä—ã–π —É—Å–∫–æ—Ä—è–µ—Ç inference –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ
[16.10.2025 19:08] Using data from previous issue: {"categories": ["#rl", "#optimization", "#science", "#training"], "emoji": "üîç", "ru": {"title": "–ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ RL: –æ—Ç –∞–Ω–∞–ª–∏–∑–∞ –∫ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—é", "desc": "–í —Å—Ç–∞—Ç—å–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —Å—Ç—Ä—É–∫
[16.10.2025 19:08] Using data from previous issue: {"categories": ["#architecture", "#inference", "#long_context", "#benchmark", "#optimization", "#training"], "emoji": "üöÄ", "ru": {"title": "–£—Å–∫–æ—Ä–µ–Ω–∏–µ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è LLM —á–µ—Ä–µ–∑ –ª–æ–∫–∞–ª—å–Ω–æ–µ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ", "desc": "NOSA ‚Äî —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–±—É—á–∞–µ–º–æ–≥–æ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è (sparse attention), –∫–æ—Ç–æ—Ä—ã–π —Ä–µ—à–∞–µ—Ç –ø—Ä
[16.10.2025 19:08] Using data from previous issue: {"categories": ["#rl", "#agents", "#games", "#optimization", "#training"], "emoji": "üèéÔ∏è", "ru": {"title": "–ö–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–≤—É—Ö –∞–≥–µ–Ω—Ç–æ–≤ –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ–≥–æ –∞–≤—Ç–æ–ø–∏–ª–æ—Ç–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–º—É –≤–æ–∂–¥–µ–Ω–∏—é –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º CoIRL-AD, –∫–æ—Ç–æ—Ä—ã–π –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç imitation learning –∏ re
[16.10.2025 19:08] Using data from previous issue: {"categories": ["#agents", "#rl", "#optimization", "#reasoning", "#games", "#training"], "emoji": "ü§ù", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –∫–æ–º–∞–Ω–¥—ã AI-–∞–≥–µ–Ω—Ç–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç AT-GRPO ‚Äî –∞–ª–≥–æ—Ä–∏—Ç–º –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π –¥–ª—è –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º –Ω–∞ –±–∞
[16.10.2025 19:08] Using data from previous issue: {"categories": ["#reasoning", "#math", "#benchmark", "#interpretability"], "emoji": "üîç", "ru": {"title": "–ü—Ä–æ–≤–µ—Ä–∫–∞ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π AI –Ω–∞ –ø—Ä–æ—á–Ω–æ—Å—Ç—å", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Å–æ–∑–¥–∞–ª–∏ Hard2Verify ‚Äî –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π LLM, —Å–æ–∑–¥–∞–Ω–Ω—ã–π —Å –ø–æ–º–æ—â—å—é 500 —á–∞—Å–æ–≤
[16.10.2025 19:08] Using data from previous issue: {"categories": ["#agents", "#optimization", "#graphs", "#games"], "emoji": "üï∏Ô∏è", "ru": {"title": "–ì–∏–ø–µ—Ä–≥—Ä–∞—Ñ—ã –¥–ª—è —É–º–Ω–æ–π –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏ AI-–∞–≥–µ–Ω—Ç–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç HyperAgent ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≥–∏–ø–µ—Ä–≥—Ä–∞—Ñ–æ–≤ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏ –≤ –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º–∞—Ö —Å LLM. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Ç—Ä–∞–¥–∏—Ü
[16.10.2025 19:08] Using data from previous issue: {"categories": ["#cv", "#open_source", "#robotics", "#transfer_learning", "#dataset"], "emoji": "üß∏", "ru": {"title": "–£—á–∏–º—Å—è –∑–∞—Ö–≤–∞—Ç—ã–≤–∞—Ç—å —Å–ª–æ–∂–Ω–æ–µ —á–µ—Ä–µ–∑ –ø—Ä–æ—Å—Ç–æ–µ: —Ä–æ–±–æ—Ç—ã –æ—Å–≤–∞–∏–≤–∞—é—Ç –º–∏—Ä —Å –ø–æ–º–æ—â—å—é –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–º–∏—Ç–∏–≤–æ–≤", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ —Ä–æ–±–æ—Ç—ã –º–æ–≥—É—Ç –Ω–∞—É—á–∏—Ç—å—Å—è —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–º –Ω–∞–≤—ã–∫–∞–º
[16.10.2025 19:08] Using data from previous issue: {"categories": ["#rl", "#agents", "#reasoning", "#optimization", "#survey"], "emoji": "üîÑ", "ru": {"title": "–ò—Ç–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è SQL —á–µ—Ä–µ–∑ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ —Å –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö", "desc": "MTSQL-R1 ‚Äî —ç—Ç–æ –∞–≥–µ–Ω—Ç–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å–∏—Å—Ç–µ–º –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –≤ SQL –≤ –º–Ω–æ–≥–æ—à–∞–≥–æ–≤—ã—Ö –¥–∏–∞–ª–æ–≥–∞—Ö. –ê–≤—Ç–æ—Ä—ã —Ñ–æ—Ä–º—É
[16.10.2025 19:08] Using data from previous issue: {"categories": ["#agents", "#synthetic", "#dataset", "#graphs", "#optimization", "#benchmark"], "emoji": "üï∏Ô∏è", "ru": {"title": "–ì—Ä–∞—Ñ—ã –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –¥–ª—è –ø–æ–∏—Å–∫–∞ –∫–æ—Ä–Ω–µ–≤—ã—Ö –ø—Ä–∏—á–∏–Ω –æ—à–∏–±–æ–∫ –≤ –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º–∞—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç GraphTracer ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ –æ—à–∏–±–æ–∫ –≤ –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω—ã
[16.10.2025 19:08] Using data from previous issue: {"categories": ["#video", "#optimization", "#inference"], "emoji": "‚ö°", "ru": {"title": "–ö–æ–º–ø–µ–Ω—Å–∞—Ü–∏—è –∑–∞–¥–µ—Ä–∂–µ–∫ –ø—Ä–∏ –æ–±–ª–∞—á–Ω–æ–º –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–µ –¥–ª—è —Ä–µ–∞–ª—å–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ Dedelayed –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º—ã –∑–∞–¥–µ—Ä–∂–µ–∫ –ø—Ä–∏ —É–¥–∞–ª—ë–Ω–Ω–æ–º –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–µ –Ω–∞ –æ–±–ª–∞—á–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö. –õ—ë–≥–∫–∞—è –ª–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å
[16.10.2025 19:08] Using data from previous issue: {"categories": ["#agents", "#optimization", "#training", "#games"], "emoji": "üéÆ", "ru": {"title": "–£–º–Ω—ã–µ –∏–≥—Ä–æ–≤—ã–µ –ø–µ—Ä—Å–æ–Ω–∞–∂–∏ —á–µ—Ä–µ–∑ –ø—Ä–æ–º–ø—Ç–∏–Ω–≥ –∏ —Ñ–∞–π–Ω-—Ç—é–Ω–∏–Ω–≥", "desc": "–ö–æ–º–∞–Ω–¥–∞ Tu_Character_lab —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∞ –º–µ—Ç–æ–¥—ã –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –¥–∏–∞–ª–æ–≥–æ–≤—ã—Ö NPC –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π –≤ –∏–≥—Ä–∞—Ö –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM. –í —Ä–∞–±–æ—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å –¥–≤–∞ –ø–æ
[16.10.2025 19:08] Using data from previous issue: {"categories": ["#alignment", "#interpretability", "#multilingual", "#architecture"], "emoji": "üß†", "ru": {"title": "–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤ LLM –∏ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–º –º–æ–∑–≥–µ —á–µ—Ä–µ–∑ —á–∞—Å—Ç–æ—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –º–µ—Ç–æ–¥ Hierarchical Frequency Tagging Probe (HFTP), –∫–æ—Ç–æ—Ä—ã–π –∏—Å
[16.10.2025 19:08] Using data from previous issue: {"categories": ["#video", "#cv", "#games", "#diffusion"], "emoji": "üéØ", "ru": {"title": "–¢—Ä–µ–∫–∏–Ω–≥ —Ç–æ—á–µ–∫ —á–µ—Ä–µ–∑ –≤–∏–¥–µ–æ-–¥–∏—Ñ—Ñ—É–∑–∏—é –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –≤–∏–¥–µ–æ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –º–æ–≥—É—Ç –≤—ã–ø–æ–ª–Ω—è—Ç—å —Ç—Ä–µ–∫–∏–Ω–≥ —Ç–æ—á–µ–∫ –≤ zero-shot —Ä–µ–∂–∏–º–µ, –ø—Ä–æ—Å—Ç–æ –≤–∏–∑—É–∞–ª—å–Ω–æ –ø–æ–º–µ—á–∞—è —Ç–æ—á–∫–∏ –Ω
[16.10.2025 19:08] Using data from previous issue: {"categories": ["#training", "#benchmark", "#reasoning", "#rl", "#open_source"], "emoji": "üßÆ", "ru": {"title": "MATH-Beyond: –±–µ–Ω—á–º–∞—Ä–∫, —Ç—Ä–µ–±—É—é—â–∏–π –Ω–∞—Å—Ç–æ—è—â–µ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –æ—Ç RL-–º–æ–¥–µ–ª–µ–π", "desc": "MATH-Beyond ‚Äî —ç—Ç–æ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ —Ä–∞–∑—Ä–∞
[16.10.2025 19:08] Using data from previous issue: {"categories": ["#optimization", "#benchmark", "#reasoning", "#training"], "emoji": "üåø", "ru": {"title": "–£–º–Ω–æ–µ –≤–µ—Ç–≤–ª–µ–Ω–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –ø–æ —ç–Ω—Ç—Ä–æ–ø–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤", "desc": "EAGer ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã –ø—Ä–∏ —Ä–µ—à–µ–Ω–∏–∏ —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. –í–º–µ—Å—Ç–æ –≤—ã–¥
[16.10.2025 19:08] Using data from previous issue: {"categories": ["#games", "#benchmark", "#reasoning", "#rl", "#dataset"], "emoji": "üé≤", "ru": {"title": "–ö–æ–≥–¥–∞ AI –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –∏–≥—Ä—ã: reasoning-–º–æ–¥–µ–ª–∏ –±–ª–∏–∂–µ –∫ –ª—é–¥—è–º, –Ω–æ –Ω–µ –∏–¥–µ–∞–ª—å–Ω—ã", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –Ω–æ–≤—É—é –ø–∞—Ä–∞–¥–∏–≥–º—É –æ—Ü–µ–Ω–∫–∏ AI-—Å–∏—Å—Ç–µ–º: –≤–º–µ—Å—Ç–æ —Ç–æ–≥–æ —á—Ç–æ–±—ã –æ—Ü–µ–Ω–∏–≤–∞—Ç—å, –∫–∞–∫ –º–æ–¥–µ–ª–∏ —Ä–µ—à–∞—é—Ç –∑–∞–¥–∞—á–∏
[16.10.2025 19:08] Using data from previous issue: {"categories": ["#interpretability", "#reasoning", "#inference", "#math"], "emoji": "üéØ", "ru": {"title": "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —É—Å–ø–µ—à–Ω—ã—Ö –ø—É—Ç–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —á–µ—Ä–µ–∑ —Å–∫—Ä—ã—Ç—ã–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –º–µ—Ç–æ–¥ Latent-Trajectory signals, –∫–æ—Ç–æ—Ä—ã–π –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –º–æ–¥–µ–ª
[16.10.2025 19:08] Using data from previous issue: {"categories": ["#games", "#benchmark", "#long_context", "#multimodal", "#agents"], "emoji": "üîç", "ru": {"title": "–ö–æ–≥–¥–∞ LLM —Ç–µ—Ä—è—é—Ç—Å—è –≤ —à—É–º–Ω–æ–º —Å—Ç–æ–≥–µ —Å–µ–Ω–∞", "desc": "HaystackCraft ‚Äî —ç—Ç–æ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏ LLM –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º, –ø–æ—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–≥–ª–∏–π—Å–∫–æ–π –í–∏–∫–∏–ø–µ
[16.10.2025 19:08] Renaming data file.
[16.10.2025 19:08] Renaming previous data. hf_papers.json to ./d/2025-10-16.json
[16.10.2025 19:08] Saving new data file.
[16.10.2025 19:08] Generating page.
[16.10.2025 19:08] Renaming previous page.
[16.10.2025 19:08] Renaming previous data. index.html to ./d/2025-10-16.html
[16.10.2025 19:08] Writing result.
[16.10.2025 19:08] Renaming log file.
[16.10.2025 19:08] Renaming previous data. log.txt to ./logs/2025-10-16_last_log.txt
