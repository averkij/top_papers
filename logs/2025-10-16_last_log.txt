[16.10.2025 10:13] Read previous papers.
[16.10.2025 10:13] Generating top page (month).
[16.10.2025 10:13] Writing top page (month).
[16.10.2025 11:10] Read previous papers.
[16.10.2025 11:10] Get feed.
[16.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.13344
[16.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.13554
[16.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.13678
[16.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.13795
[16.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.13747
[16.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.13809
[16.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.13626
[16.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.07944
[16.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.13804
[16.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.04767
[16.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.13802
[16.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.13800
[16.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.13778
[16.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.13621
[16.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.13515
[16.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.13759
[16.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.13282
[16.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.10977
[16.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.10921
[16.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.10274
[16.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.11958
[16.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.12560
[16.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.13602
[16.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.10611
[16.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.13744
[16.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.11062
[16.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.12831
[16.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.10581
[16.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.13586
[16.10.2025 11:10] Extract page data from URL. URL: https://huggingface.co/papers/2510.13255
[16.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.11715
[16.10.2025 11:10] Extract page data from URL. URL: https://huggingface.co/papers/2510.11653
[16.10.2025 11:10] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[16.10.2025 11:10] No deleted papers detected.
[16.10.2025 11:10] Downloading and parsing papers (pdf, html). Total: 32.
[16.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.13344.
[16.10.2025 11:10] Extra JSON file exists (./assets/json/2510.13344.json), skip PDF parsing.
[16.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.13344.json), skip HTML parsing.
[16.10.2025 11:10] Success.
[16.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.13554.
[16.10.2025 11:10] Extra JSON file exists (./assets/json/2510.13554.json), skip PDF parsing.
[16.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.13554.json), skip HTML parsing.
[16.10.2025 11:10] Success.
[16.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.13678.
[16.10.2025 11:10] Extra JSON file exists (./assets/json/2510.13678.json), skip PDF parsing.
[16.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.13678.json), skip HTML parsing.
[16.10.2025 11:10] Success.
[16.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.13795.
[16.10.2025 11:10] Extra JSON file exists (./assets/json/2510.13795.json), skip PDF parsing.
[16.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.13795.json), skip HTML parsing.
[16.10.2025 11:10] Success.
[16.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.13747.
[16.10.2025 11:10] Extra JSON file exists (./assets/json/2510.13747.json), skip PDF parsing.
[16.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.13747.json), skip HTML parsing.
[16.10.2025 11:10] Success.
[16.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.13809.
[16.10.2025 11:10] Extra JSON file exists (./assets/json/2510.13809.json), skip PDF parsing.
[16.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.13809.json), skip HTML parsing.
[16.10.2025 11:10] Success.
[16.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.13626.
[16.10.2025 11:10] Extra JSON file exists (./assets/json/2510.13626.json), skip PDF parsing.
[16.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.13626.json), skip HTML parsing.
[16.10.2025 11:10] Success.
[16.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.07944.
[16.10.2025 11:10] Extra JSON file exists (./assets/json/2510.07944.json), skip PDF parsing.
[16.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.07944.json), skip HTML parsing.
[16.10.2025 11:10] Success.
[16.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.13804.
[16.10.2025 11:10] Extra JSON file exists (./assets/json/2510.13804.json), skip PDF parsing.
[16.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.13804.json), skip HTML parsing.
[16.10.2025 11:10] Success.
[16.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.04767.
[16.10.2025 11:10] Extra JSON file exists (./assets/json/2510.04767.json), skip PDF parsing.
[16.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.04767.json), skip HTML parsing.
[16.10.2025 11:10] Success.
[16.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.13802.
[16.10.2025 11:10] Extra JSON file exists (./assets/json/2510.13802.json), skip PDF parsing.
[16.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.13802.json), skip HTML parsing.
[16.10.2025 11:10] Success.
[16.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.13800.
[16.10.2025 11:10] Extra JSON file exists (./assets/json/2510.13800.json), skip PDF parsing.
[16.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.13800.json), skip HTML parsing.
[16.10.2025 11:10] Success.
[16.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.13778.
[16.10.2025 11:10] Extra JSON file exists (./assets/json/2510.13778.json), skip PDF parsing.
[16.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.13778.json), skip HTML parsing.
[16.10.2025 11:10] Success.
[16.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.13621.
[16.10.2025 11:10] Extra JSON file exists (./assets/json/2510.13621.json), skip PDF parsing.
[16.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.13621.json), skip HTML parsing.
[16.10.2025 11:10] Success.
[16.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.13515.
[16.10.2025 11:10] Extra JSON file exists (./assets/json/2510.13515.json), skip PDF parsing.
[16.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.13515.json), skip HTML parsing.
[16.10.2025 11:10] Success.
[16.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.13759.
[16.10.2025 11:10] Extra JSON file exists (./assets/json/2510.13759.json), skip PDF parsing.
[16.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.13759.json), skip HTML parsing.
[16.10.2025 11:10] Success.
[16.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.13282.
[16.10.2025 11:10] Extra JSON file exists (./assets/json/2510.13282.json), skip PDF parsing.
[16.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.13282.json), skip HTML parsing.
[16.10.2025 11:10] Success.
[16.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.10977.
[16.10.2025 11:10] Extra JSON file exists (./assets/json/2510.10977.json), skip PDF parsing.
[16.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.10977.json), skip HTML parsing.
[16.10.2025 11:10] Success.
[16.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.10921.
[16.10.2025 11:10] Extra JSON file exists (./assets/json/2510.10921.json), skip PDF parsing.
[16.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.10921.json), skip HTML parsing.
[16.10.2025 11:10] Success.
[16.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.10274.
[16.10.2025 11:10] Extra JSON file exists (./assets/json/2510.10274.json), skip PDF parsing.
[16.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.10274.json), skip HTML parsing.
[16.10.2025 11:10] Success.
[16.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.11958.
[16.10.2025 11:10] Extra JSON file exists (./assets/json/2510.11958.json), skip PDF parsing.
[16.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.11958.json), skip HTML parsing.
[16.10.2025 11:10] Success.
[16.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.12560.
[16.10.2025 11:10] Extra JSON file exists (./assets/json/2510.12560.json), skip PDF parsing.
[16.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.12560.json), skip HTML parsing.
[16.10.2025 11:10] Success.
[16.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.13602.
[16.10.2025 11:10] Extra JSON file exists (./assets/json/2510.13602.json), skip PDF parsing.
[16.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.13602.json), skip HTML parsing.
[16.10.2025 11:10] Success.
[16.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.10611.
[16.10.2025 11:10] Extra JSON file exists (./assets/json/2510.10611.json), skip PDF parsing.
[16.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.10611.json), skip HTML parsing.
[16.10.2025 11:10] Success.
[16.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.13744.
[16.10.2025 11:10] Extra JSON file exists (./assets/json/2510.13744.json), skip PDF parsing.
[16.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.13744.json), skip HTML parsing.
[16.10.2025 11:10] Success.
[16.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.11062.
[16.10.2025 11:10] Extra JSON file exists (./assets/json/2510.11062.json), skip PDF parsing.
[16.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.11062.json), skip HTML parsing.
[16.10.2025 11:10] Success.
[16.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.12831.
[16.10.2025 11:10] Extra JSON file exists (./assets/json/2510.12831.json), skip PDF parsing.
[16.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.12831.json), skip HTML parsing.
[16.10.2025 11:10] Success.
[16.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.10581.
[16.10.2025 11:10] Extra JSON file exists (./assets/json/2510.10581.json), skip PDF parsing.
[16.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.10581.json), skip HTML parsing.
[16.10.2025 11:10] Success.
[16.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.13586.
[16.10.2025 11:10] Extra JSON file exists (./assets/json/2510.13586.json), skip PDF parsing.
[16.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.13586.json), skip HTML parsing.
[16.10.2025 11:10] Success.
[16.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.13255.
[16.10.2025 11:10] Downloading paper 2510.13255 from http://arxiv.org/pdf/2510.13255v1...
[16.10.2025 11:10] Extracting affiliations from text.
[16.10.2025 11:10] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 5 1 ] . [ 1 5 5 2 3 1 . 0 1 5 2 : r Hierarchical Frequency Tagging Probe (HFTP): Unified Approach to Investigate Syntactic Structure Representations in Large Language Models and the Human Brain Jingmin An1, Yilong Song1, Ruolin Yang1, Nai Ding2, Lingxi Lu3 Yuxuan Wang1, Wei Wang4, Chu Zhuang1, Qian Wang1,, Fang Fang1, 1Peking University, 2Zhejiang University, 3Beijing Language and Culture University, 4Beijing Institute for General Artificial Intelligence anjm@stu.pku.edu.cn, {wangqianpsy, ffang}@pku.edu.cn "
[16.10.2025 11:10] Response: ```python
[
    "Peking University",
    "Zhejiang University",
    "Beijing Language and Culture University",
    "Beijing Institute for General Artificial Intelligence"
]
```
[16.10.2025 11:10] Deleting PDF ./assets/pdf/2510.13255.pdf.
[16.10.2025 11:10] Success.
[16.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.11715.
[16.10.2025 11:10] Extra JSON file exists (./assets/json/2510.11715.json), skip PDF parsing.
[16.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.11715.json), skip HTML parsing.
[16.10.2025 11:10] Success.
[16.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.11653.
[16.10.2025 11:10] Downloading paper 2510.11653 from http://arxiv.org/pdf/2510.11653v1...
[16.10.2025 11:11] Extracting affiliations from text.
[16.10.2025 11:11] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 3 1 ] . [ 1 3 5 6 1 1 . 0 1 5 2 : r a MATH-BEYOND: BENCHMARK FOR RL TO EXPAND BEYOND THE BASE MODEL Prasanna Mayilvahanan1,2,3 Ricardo Dominguez-Olmedo2,3 Thadd√§us Wiedemer1,2,3 Wieland Brendel2,3, "
[16.10.2025 11:11] Response: ```python
[]
```
[16.10.2025 11:11] Extracting affiliations from text.
[16.10.2025 11:11] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 3 1 ] . [ 1 3 5 6 1 1 . 0 1 5 2 : r aMATH-BEYOND: BENCHMARK FOR RL TO EXPAND BEYOND THE BASE MODEL Prasanna Mayilvahanan1,2,3 Ricardo Dominguez-Olmedo2,3 Thadd√§us Wiedemer1,2,3 Wieland Brendel2,3,With the advent of DeepSeek-R1, new wave of reinforcement learning (RL) methods has emerged that seem to unlock stronger mathematical reasoning. However, closer look at the open-source ecosystem reveals critical limitation: with sufficiently many draws (e.g., pass@1024), many existing base models already solve nearly all questions on widely used math benchmarks such as MATH-500 and AIME 2024. This suggests that the RL fine-tuning methods prevalent in the LLM reasoning literature largely sharpen existing solution modes rather than discovering entirely new ones. Such sharpening stands in contrast to the broader promise of RL: to foster exploration and to acquire new skills. To move beyond this plateau, we introduce MATH-Beyond (MATH-B), benchmark deliberately constructed to defeat common open-source models of up to 8B parameters even under large sampling budgets. Improving performance on our benchmark via RL requires methods that learn to reason in ways that go beyond base model capabilities in repeated sampling. Since the problems are drawn from subsets of DAPOMath-17K and DeepScaleR datasets, they remain topically equivalent to standard high-school math. Validating our premise, RL fine-tuned models such as NemotronResearch-Reasoning-Qwen-1.5B and DeepScaleR-1.5B-Preview perform poorly on MATH-B at pass@1024, showing how existing approaches fall short on tackling harder instances. We hope MATH-B will catalyze exploration-driven RL approaches that elicit deeper reasoning capabilities. We release MATH-B at https: //huggingface.co/datasets/brendel-group/MATH-Beyond. Figure 1: MATH-Beyond: Benchmark Construction and Difficulty. Left: Schematic of the MATH-B creation process. large set of problems from DAPO-Math-17K and DeepScaleR is first refined through quality filters to ensure answer correctness and verifiability. This is followed by evaluation against gauntlet of open-source base models ( 8B, e.g., Qwen3, Qwen2.5 (-Math), DeepSeek-R1-Distill) at pass@1024 budget to isolate problems that lie beyond their limits. The filtering yields the MATH-B suite of benchmarks: 41-problem intersection set (unsolved by all base models) for evaluating universal difficulty, and larger 181-problem union set (unsolved by at least one model) with model-specific splits for targeted analysis. This suite provides rigorous testbed to drive the development of exploration methods for RL. Right: An illustration of MATH-Bs significant difficulty compared to common test sets like AIME24. Representative open-source models like Qwen2.5 achieve near-zero pass@1024 scores on MATH-B, highlighting its difficulty. Qwen2.5 results are from Yue et al. (2025). 1University of T√ºbingen, 2T√ºbingen AI Center, 3Max-Planck-Institute for Intelligent Systems, T√ºbingen, 4ELLIS Institute T√ºbingen. Contact: prasanna.mayilvahanan@uni-tuebingen.de. Code available at https://brendel-group.github.io/math-beyond/.In the 2010s, deep reinforcement learning showcased its power through striking demonstrations of exploration and skill acquisition (Mnih et al., 2013). Atari agents, starting from random play, mastered complex games by discovering strategies unreachable to base policies, guided by exploration incentives and intrinsic rewards (Ladosz et al., 2022; Amin et al., 2021). Methods such as count-based exploration (Bellemare et al., 2016) and later Go-Explore (Ecoffet et al., 2021) drove dramatic jumps from inept play to expertise, highlighting RLs ability to uncover new capabilities. Around the same time, AlphaGo (Silver et al., 2016) and AlphaZero (Silver et al., 2017) extended this promise to board games like Go, Chess, and Shogi, where self-play took agents from scratch to superhuman mastery, revealing novel strategies along the way. Against this backdrop, academic progress in reasoning-focused LLMs has taken very different path. Community-trained models often show improved accuracy on popular benchmarks such as MATH or AIME24 (Liu et al., 2025b; Song et al., 2025; Chen et al., 2025; Cheng et al., 2025; Cui et al., 2025; Shao et al., 2025; Wang et al., 2025; Yu et al., 2025b). However, these RL models typically succeed only on problems that their corresponding base models could already solve given realistic sampling budgets (Wu et al., 2025; Yue et al., 2025). This is far cry from earlier RL successes, where base policies were incapable of solving tasks outright and progress required genuine exploration and skill acquisition. This disconnect between RLs exploratory promise and its current application reflects substantial blindspot in the current open-source evaluation ecosystem. Because several open-source base models already achieve nearly 100% pass@1024 on several popular benchmarks (Yue et al., 2025), test sets in their current form are fundamentally inadequate for measuringor encouraginggenuine progress in reasoning beyond the base models reach. To address this gap, we introduce MATH-Beyond (MATH-B), new benchmark of highschoollevel competition math problems, specifically constructed so that popular open-weight base models are unlikely to solve even with 1024 attempts. As result, progress on MATH-B necessarily requires expanding the reasoning capabilities of base models, making it an ideal target for academic research. MATH-B is constructed by filtering mathematical reasoning datasets (DAPO-Math-17K (Yu et al., 2025b) and DeepScaleR (Luo et al., 2025)), resulting in problems that are topically indistinguishable from those in standard benchmarks. While constructing the dataset, we also uncovered and addressed several non-obvious failure modes in programmatic verification, which informed our final benchmark design. To ensure correctness, all problems are additionally verified against stronger reasoning models such as GPT-5-Mini or o4-mini-high, which reliably solve them. We further confirm that leading community RL models, including Nemotron-Research-Reasoning-Qwen-1.5B (Liu et al., 2025a) and DeepScaleR-1.5B, perform poorly on MATH-B, underscoring the limitations of current approaches and the need for methods that extend reasoning capabilities. In summary, our contributions are as follows: New benchmark suite: We construct MATH-Beyond, benchmark suite derived from the failures of large and diverse set of base models (pass@1024"
[16.10.2025 11:11] Mistral response. {"id": "00c3f31658484a5da20bcfece6b39559", "created": 1760613062, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1649, "total_tokens": 1695, "completion_tokens": 46}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\n    \"University of T\u00fcbingen\",\n    \"T\u00fcbingen AI Center\",\n    \"Max-Planck-Institute for Intelligent Systems, T\u00fcbingen\",\n    \"ELLIS Institute T\u00fcbingen\"\n]\n```"}}]}
[16.10.2025 11:11] Response: ```python
[
    "University of T√ºbingen",
    "T√ºbingen AI Center",
    "Max-Planck-Institute for Intelligent Systems, T√ºbingen",
    "ELLIS Institute T√ºbingen"
]
```
[16.10.2025 11:11] Deleting PDF ./assets/pdf/2510.11653.pdf.
[16.10.2025 11:11] Success.
[16.10.2025 11:11] Enriching papers with extra data.
[16.10.2025 11:11] ********************************************************************************
[16.10.2025 11:11] Abstract 0. UniMoE-Audio, a unified speech and music generation model using a Dynamic-Capacity Mixture-of-Experts framework, addresses data imbalance and task conflicts, achieving state-of-the-art performance and enhanced cross-domain synergy.  					AI-generated summary 				 Recent advances in unified multimoda...
[16.10.2025 11:11] ********************************************************************************
[16.10.2025 11:11] Abstract 1. Attention mechanisms in LLMs are analyzed to reveal reasoning patterns, leading to novel RL strategies that improve performance by focusing on critical tokens.  					AI-generated summary 				 The reasoning pattern of Large language models (LLMs) remains opaque, and Reinforcement learning (RL) typica...
[16.10.2025 11:11] ********************************************************************************
[16.10.2025 11:11] Abstract 2. FlashWorld generates 3D scenes from single images or text prompts quickly and with high quality by combining MV-oriented and 3D-oriented generation methods.  					AI-generated summary 				 We propose FlashWorld, a generative model that produces 3D scenes from a single image or text prompt in seconds...
[16.10.2025 11:11] ********************************************************************************
[16.10.2025 11:11] Abstract 3. A new dataset and pipeline for data curation improve the performance of fully open multimodal large language models, achieving state-of-the-art results competitive with semi-open models.  					AI-generated summary 				 Fully open multimodal large language models (MLLMs) currently lag behind propriet...
[16.10.2025 11:11] ********************************************************************************
[16.10.2025 11:11] Abstract 4. InteractiveOmni is a unified omni-modal large language model for audio-visual multi-turn interactions, offering comprehensive understanding and speech generation capabilities with efficient parameter usage.  					AI-generated summary 				 We introduce InteractiveOmni, a unified and open-source omni-...
[16.10.2025 11:11] ********************************************************************************
[16.10.2025 11:11] Abstract 5. PhysMaster enhances video generation by integrating physical knowledge through PhysEncoder, using reinforcement learning and Direct Preference Optimization to improve physics-awareness.  					AI-generated summary 				 Video generation models nowadays are capable of generating visually realistic vide...
[16.10.2025 11:11] ********************************************************************************
[16.10.2025 11:11] Abstract 6. State-of-the-art Visual-Language-Action models show high benchmark scores but are brittle to various perturbations, particularly in camera viewpoints and robot initial states, and often ignore language instructions.  					AI-generated summary 				 Visual-Language-Action (VLA) models report impressiv...
[16.10.2025 11:11] ********************************************************************************
[16.10.2025 11:11] Abstract 7. CVD-STORM, a cross-view video diffusion model with a spatial-temporal reconstruction VAE, enhances video generation quality and provides depth estimation for dynamic scenes.  					AI-generated summary 				 Generative models have been widely applied to world modeling for environment simulation and fu...
[16.10.2025 11:11] ********************************************************************************
[16.10.2025 11:11] Abstract 8. Generative Universal Verifier enhances multimodal reasoning by providing reliable visual verification through ViVerBench, OmniVerifier-7B, and OmniVerifier-TTS, improving generation and refinement capabilities.  					AI-generated summary 				 We introduce Generative Universal Verifier, a novel conce...
[16.10.2025 11:11] ********************************************************************************
[16.10.2025 11:11] Abstract 9. Parallel decoding in diffusion LLMs degrades generation quality due to ignored token dependencies, highlighting the need for new decoding methods and benchmarks.  					AI-generated summary 				 While most autoregressive LLMs are constrained to one-by-one decoding, diffusion LLMs (dLLMs) have attract...
[16.10.2025 11:11] ********************************************************************************
[16.10.2025 11:11] Abstract 10. Trace Anything, a neural network, predicts video trajectories in a single pass, achieving state-of-the-art performance and demonstrating efficiency and emergent abilities like motion forecasting.  					AI-generated summary 				 Effective spatio-temporal representation is fundamental to modeling, und...
[16.10.2025 11:11] ********************************************************************************
[16.10.2025 11:11] Abstract 11. GS-Reasoner, a 3D LLM with a dual-path pooling mechanism, achieves autoregressive grounding and state-of-the-art spatial reasoning without external modules.  					AI-generated summary 				 In this paper, we claim that 3D visual grounding is the cornerstone of spatial reasoning and introduce the Grou...
[16.10.2025 11:11] ********************************************************************************
[16.10.2025 11:11] Abstract 12. A unified framework for spatial grounding and robot control, InternVLA-M1, enhances instruction-following robots through spatially guided vision-language-action training, achieving significant improvements across various tasks and simulations.  					AI-generated summary 				 We introduce InternVLA-M...
[16.10.2025 11:11] ********************************************************************************
[16.10.2025 11:11] Abstract 13. Increased computing resources are correlated with national funding and citations in foundation model research, but not with research environment, domain, or methodology.  					AI-generated summary 				 Cutting-edge research in Artificial Intelligence (AI) requires considerable resources, including G...
[16.10.2025 11:11] ********************************************************************************
[16.10.2025 11:11] Abstract 14. A novel Universal Multimodal Embedding (UniME-V2) model uses MLLMs to enhance representation learning by identifying diverse, high-quality hard negatives and improving discriminative capacity through soft semantic matching scores.  					AI-generated summary 				 Universal multimodal embedding models...
[16.10.2025 11:11] ********************************************************************************
[16.10.2025 11:11] Abstract 15. Uni-MMMU is a benchmark that evaluates the bidirectional synergy between visual understanding and generation across multiple domains, providing insights into their integration and performance.  					AI-generated summary 				 Unified multimodal models aim to jointly enable visual understanding and ge...
[16.10.2025 11:11] ********************************************************************************
[16.10.2025 11:11] Abstract 16. A Masked Degradation Classification Pre-Training method enhances image restoration by using degradation type classification and image reconstruction, improving performance across CNNs and Transformers.  					AI-generated summary 				 This study introduces a Masked Degradation Classification Pre-Trai...
[16.10.2025 11:11] ********************************************************************************
[16.10.2025 11:11] Abstract 17. Model merging, typically on Instruct and Thinking models, has shown remarkable performance for efficient reasoning. In this paper, we systematically revisit the simplest merging method that interpolates two weights directly. Particularly, we observe that model interpolation follows a three-stage evo...
[16.10.2025 11:11] ********************************************************************************
[16.10.2025 11:11] Abstract 18. FG-CLIP 2, a bilingual vision-language model, enhances fine-grained alignment for English and Chinese through rich supervision and a new TIC loss, achieving state-of-the-art performance across multiple datasets and tasks.  					AI-generated summary 				 Fine-grained vision-language understanding req...
[16.10.2025 11:11] ********************************************************************************
[16.10.2025 11:11] Abstract 19. A novel Soft Prompt approach enhances Vision-Language-Action models by using learnable embeddings for diverse robotic data, enabling superior performance across simulations and real-world robots.  					AI-generated summary 				 Successful generalist Vision-Language-Action (VLA) models rely on effect...
[16.10.2025 11:11] ********************************************************************************
[16.10.2025 11:11] Abstract 20. Direct Multi-Token Decoding (DMTD) accelerates large language model inference by using only late layers for token generation, achieving significant speedup with minimal performance loss.  					AI-generated summary 				 Decoder-only transformers have become the standard architecture for large languag...
[16.10.2025 11:11] ********************************************************************************
[16.10.2025 11:11] Abstract 21. End-to-end autonomous driving models trained solely with imitation learning (IL) often suffer from poor generalization. In contrast, reinforcement learning (RL) promotes exploration through reward maximization but faces challenges such as sample inefficiency and unstable convergence. A natural solut...
[16.10.2025 11:11] ********************************************************************************
[16.10.2025 11:11] Abstract 22. NOSA, a trainable sparse attention framework, enhances decoding throughput by enabling efficient KV cache offloading without compromising performance.  					AI-generated summary 				 Trainable sparse attention has emerged as a promising solution to address the decoding efficiency bottleneck of LLMs ...
[16.10.2025 11:11] ********************************************************************************
[16.10.2025 11:11] Abstract 23. HyperAgent, a hypergraph-based framework, optimizes communication topologies and captures group collaboration patterns, improving performance and efficiency in multi-agent systems.  					AI-generated summary 				 Recent advances in large language model-powered multi-agent systems have demonstrated r...
[16.10.2025 11:11] ********************************************************************************
[16.10.2025 11:11] Abstract 24. Hard2Verify, a human-annotated benchmark, evaluates step-level verifiers for LLM-based mathematical reasoning systems, highlighting the challenges and performance gaps between open-source and closed-source models.  					AI-generated summary 				 Large language model (LLM)-based reasoning systems hav...
[16.10.2025 11:11] ********************************************************************************
[16.10.2025 11:11] Abstract 25. AT-GRPO, a tailored RL algorithm for multi-agent systems, significantly enhances performance across various tasks by addressing unique challenges in on-policy RL.  					AI-generated summary 				 Multi-agent systems (MAS) and reinforcement learning (RL) are widely used to enhance the agentic capabili...
[16.10.2025 11:11] ********************************************************************************
[16.10.2025 11:11] Abstract 26. MTSQL-R1, an agentic training framework, improves multi-turn Text-to-SQL by treating it as an MDP with iterative propose-execute-verify-refine cycles, enhancing coherence and execution.  					AI-generated summary 				 Multi-turn Text-to-SQL aims to translate a user's conversational utterances into e...
[16.10.2025 11:11] ********************************************************************************
[16.10.2025 11:11] Abstract 27. GraphTracer addresses multi-agent system failure attribution by constructing Information Dependency Graphs to trace information flow and improve debugging accuracy.  					AI-generated summary 				 Multi-agent systems powered by Large Language Models excel at complex tasks through coordinated collabo...
[16.10.2025 11:11] ********************************************************************************
[16.10.2025 11:11] Abstract 28. Participants in the CPDC 2025 used lightweight prompting and fine-tuned large models to achieve high rankings in task-oriented and context-aware dialogue challenges.  					AI-generated summary 				 The emergence of large language models (LLMs) has opened new opportunities for cre- ating dynamic non-...
[16.10.2025 11:11] ********************************************************************************
[16.10.2025 11:11] Abstract 29. Hierarchical Frequency Tagging Probe (HFTP) identifies neuron-wise components in LLMs and cortical regions encoding syntactic structures, revealing differences in how LLMs and the human brain process syntax.  					AI-generated summary 				 Large Language Models (LLMs) demonstrate human-level or even...
[16.10.2025 11:11] ********************************************************************************
[16.10.2025 11:11] Abstract 30. Pretrained video diffusion models can perform zero-shot point tracking by visually marking points and regenerating video frames, outperforming prior methods and handling occlusions.  					AI-generated summary 				 Trackers and video generators solve closely related problems: the former analyze motio...
[16.10.2025 11:11] ********************************************************************************
[16.10.2025 11:11] Abstract 31. MATH-Beyond is a benchmark designed to challenge existing reinforcement learning methods by requiring deeper reasoning capabilities beyond current model capabilities.  					AI-generated summary 				 With the advent of DeepSeek-R1, a new wave of reinforcement learning (RL) methods has emerged that se...
[16.10.2025 11:11] Read previous papers.
[16.10.2025 11:11] Generating reviews via LLM API.
[16.10.2025 11:11] Using data from previous issue: {"categories": ["#architecture", "#games", "#benchmark", "#optimization", "#multimodal", "#audio", "#training"], "emoji": "üéµ", "ru": {"title": "–ï–¥–∏–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Ä–µ—á–∏ –∏ –º—É–∑—ã–∫–∏ —á–µ—Ä–µ–∑ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫—É—é Mixture-of-Experts", "desc": "UniMoE-Audio ‚Äî —ç—Ç–æ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–µ—á–∏ –∏ –º—É–∑—ã–∫–∏, –æ—Å–Ω–æ
[16.10.2025 11:11] Using data from previous issue: {"categories": ["#rl", "#reasoning", "#optimization", "#interpretability", "#training"], "emoji": "üîç", "ru": {"title": "–í–Ω–∏–º–∞–Ω–∏–µ –∫–∞–∫ –∫–∞—Ä—Ç–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π: –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —á–µ—Ä–µ–∑ –∫–ª—é—á–µ–≤—ã–µ —Ç–æ–∫–µ–Ω—ã", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç –º–µ—Ö–∞–Ω–∏–∑–º—ã attention –≤ LLM, —á—Ç–æ–±—ã –ø–æ–Ω—è—Ç—å, –∫–∞–∫ –º–æ–¥–µ–ª–∏ —Ä–∞—Å—Å—É–∂–¥–∞—é—Ç, –≤—ã—è
[16.10.2025 11:11] Using data from previous issue: {"categories": ["#3d", "#optimization", "#diffusion"], "emoji": "‚ö°", "ru": {"title": "–ú–≥–Ω–æ–≤–µ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è 3D-–º–∏—Ä–æ–≤: —Å–∫–æ—Ä–æ—Å—Ç—å –∏ –∫–∞—á–µ—Å—Ç–≤–æ –≤–º–µ—Å—Ç–µ", "desc": "FlashWorld ‚Äî —ç—Ç–æ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è —Å–æ–∑–¥–∞—ë—Ç 3D-—Å—Ü–µ–Ω—ã –∏–∑ –æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–ª–∏ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞ –∑–∞ —Å—á–∏—Ç–∞–Ω–Ω—ã–µ —Å–µ–∫—É–Ω–¥—ã, —Ä–∞–±–æ—Ç–∞—è –≤ 10-10
[16.10.2025 11:11] Using data from previous issue: {"categories": ["#data", "#transfer_learning", "#dataset", "#optimization", "#open_source"], "emoji": "üêù", "ru": {"title": "–ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ ‚Äî –∫–ª—é—á –∫ –æ—Ç–∫—Ä—ã—Ç—ã–º –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–º –º–æ–¥–µ–ª—è–º", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Å–æ–∑–¥–∞–ª–∏ –Ω–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç Honey-Data-15M –∏–∑ 15 –º–∏–ª–ª–∏–æ–Ω–æ–≤ –ø–∞—Ä –≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º—É–ª—å—Ç–∏
[16.10.2025 11:11] Using data from previous issue: {"categories": ["#video", "#small_models", "#long_context", "#dataset", "#benchmark", "#multimodal", "#audio", "#open_source", "#training"], "emoji": "üé≠", "ru": {"title": "–õ–µ–≥–∫–æ–≤–µ—Å–Ω–∞—è –æ–º–Ω–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å —Å –¥–æ–ª–≥–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–π –ø–∞–º—è—Ç—å—é –¥–ª—è –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∞—É–¥–∏–æ–≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è", "desc": "Interacti
[16.10.2025 11:11] Using data from previous issue: {"categories": ["#rlhf", "#games", "#rl", "#video", "#multimodal", "#optimization"], "emoji": "üéØ", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ –≤–∏–¥–µ–æ-–º–æ–¥–µ–ª–µ–π —Ñ–∏–∑–∏—á–µ—Å–∫–∏–º –∑–∞–∫–æ–Ω–∞–º —á–µ—Ä–µ–∑ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è", "desc": "PhysMaster —É–ª—É—á—à–∞–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –≤–∏–¥–µ–æ, –¥–æ–±–∞–≤–ª—è—è –ø–æ–Ω–∏–º–∞–Ω–∏–µ —Ñ–∏–∑–∏—á–µ—Å–∫–∏—Ö –∑–∞–∫–æ–Ω–æ–≤ —á–µ—Ä–µ–∑ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π PhysEncoder. –°–∏—Å—Ç–µ–º
[16.10.2025 11:11] Using data from previous issue: {"categories": ["#agents", "#benchmark", "#video", "#interpretability", "#security"], "emoji": "ü§ñ", "ru": {"title": "–•—Ä—É–ø–∫–æ—Å—Ç—å VLA –º–æ–¥–µ–ª–µ–π: –≤—ã—Å–æ–∫–∏–µ –±–µ–Ω—á–º–∞—Ä–∫–∏ —Å–∫—Ä—ã–≤–∞—é—Ç –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ —É—è–∑–≤–∏–º–æ—Å—Ç–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–æ–≤–µ–ª–∏ —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ —É—è–∑–≤–∏–º–æ—Å—Ç–µ–π —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö Visual-Language-Action (VLA) –º
[16.10.2025 11:11] Using data from previous issue: {"categories": ["#architecture", "#3d", "#optimization", "#video", "#multimodal", "#diffusion"], "emoji": "üöó", "ru": {"title": "4D-—Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö —Å—Ü–µ–Ω —á–µ—Ä–µ–∑ –≤–∏–¥–µ–æ –¥–∏—Ñ—Ñ—É–∑–∏—é", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç CVD-STORM ‚Äî –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ä–∞–∫—É—Ä—Å–æ–≤, –∫–æ—Ç–æ—Ä–∞—è —Ç–∞–∫–∂–µ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç
[16.10.2025 11:11] Using data from previous issue: {"categories": ["#reasoning", "#dataset", "#multimodal", "#optimization", "#benchmark"], "emoji": "üîç", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ç–æ—Ä –¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Generative Universal Verifier ‚Äî –Ω–æ–≤—É—é –∫–æ–Ω—Ü–µ–ø—Ü–∏—é –¥–ª—è 
[16.10.2025 11:11] Using data from previous issue: {"categories": ["#inference", "#diffusion", "#benchmark", "#optimization"], "emoji": "‚ö°", "ru": {"title": "–ü—Ä–æ–±–ª–µ–º–∞ —Å–∫–æ—Ä–æ—Å—Ç–∏ –ø—Ä–æ—Ç–∏–≤ –∫–∞—á–µ—Å—Ç–≤–∞ –≤ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–º –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–∏ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö LLM", "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –≤ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (dLLM), –∫–æ—Ç–æ—Ä—ã–µ 
[16.10.2025 11:11] Using data from previous issue: {"categories": ["#training", "#games", "#video", "#dataset", "#optimization", "#benchmark"], "emoji": "üéØ", "ru": {"title": "–û—Ç—Å–ª–µ–¥–∏—Ç—å –≤—Å—ë: –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π –≤—Å–µ—Ö –ø–∏–∫—Å–µ–ª–µ–π –≤–∏–¥–µ–æ –∑–∞ –æ–¥–∏–Ω –ø—Ä–æ—Ö–æ–¥", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Trace Anything ‚Äî –Ω–µ–π—Ä–æ—Å–µ—Ç—å, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –¥–≤–∏–∂–µ–Ω–∏—è –≤
[16.10.2025 11:11] Using data from previous issue: {"categories": ["#games", "#3d", "#dataset", "#cv", "#reasoning"], "emoji": "üéØ", "ru": {"title": "3D-reasoning —á–µ—Ä–µ–∑ –µ–¥–∏–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ: grounding –±–µ–∑ –≤–Ω–µ—à–Ω–∏—Ö –º–æ–¥—É–ª–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω GS-Reasoner ‚Äî 3D LLM —Å –º–µ—Ö–∞–Ω–∏–∑–º–æ–º –¥–≤—É—Ö–ø—É—Ç–µ–≤–æ–≥–æ pooling, –∫–æ—Ç–æ—Ä—ã–π –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é –∏ –≥–µ–æ–º–µ—Ç—Ä–∏
[16.10.2025 11:11] Using data from previous issue: {"categories": ["#science", "#agents", "#agi", "#optimization", "#robotics"], "emoji": "ü§ñ", "ru": {"title": "–†–æ–±–æ—Ç—ã —É—á–∞—Ç—Å—è –¥–µ–π—Å—Ç–≤–æ–≤–∞—Ç—å —á–µ—Ä–µ–∑ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞", "desc": "InternVLA-M1 ‚Äî —ç—Ç–æ –µ–¥–∏–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ä–æ–±–æ—Ç–∞–º–∏, –∫–æ—Ç–æ—Ä–∞—è —Å–≤—è–∑—ã–≤–∞–µ—Ç –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ —Å –¥–µ–π—Å—Ç–≤–∏—è–º–∏ —á–µ—Ä–µ–∑ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ –ø
[16.10.2025 11:11] Using data from previous issue: {"categories": ["#open_source", "#ethics"], "emoji": "üí∞", "ru": {"title": "–î–µ–Ω—å–≥–∏ —Ä–µ—à–∞—é—Ç: –∫–∞–∫ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã –≤–ª–∏—è—é—Ç –Ω–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è foundation models", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å–≤—è–∑—å –º–µ–∂–¥—É –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏ –∏ –Ω–∞—É—á–Ω—ã–º –ø—Ä–æ–≥—Ä–µ—Å—Å–æ–º –≤ –æ–±–ª–∞—Å—Ç–∏ foundation models (–±–æ–ª—å—à–∏—Ö –±–∞–∑–æ–≤—ã—Ö –º–æ
[16.10.2025 11:11] Using data from previous issue: {"categories": ["#optimization", "#benchmark", "#training", "#multimodal"], "emoji": "üéØ", "ru": {"title": "MLLM –∫–∞–∫ —Å—É–¥—å—è –¥–ª—è —É–º–Ω–æ–≥–æ –º–∞–π–Ω–∏–Ω–≥–∞ hard negatives –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–∞—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç UniME-V2 ‚Äî —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—É—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤, –∫–æ—Ç–æ—Ä–∞—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º—É
[16.10.2025 11:11] Using data from previous issue: {"categories": ["#multimodal", "#survey", "#reasoning", "#benchmark"], "emoji": "üîÑ", "ru": {"title": "–î–≤—É—Å—Ç–æ—Ä–æ–Ω–Ω—è—è —Å–∏–Ω–µ—Ä–≥–∏—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "Uni-MMMU ‚Äî —ç—Ç–æ –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ unified –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–µ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –ø–æ–Ω–∏–º–∞—é—Ç –∏ –≥–µ–Ω–µ—Ä–∏—Ä—É—é—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è. –°—É—â–µ—Å—Ç–≤
[16.10.2025 11:11] Using data from previous issue: {"categories": ["#open_source", "#data", "#dataset", "#synthetic", "#optimization", "#cv"], "emoji": "üé≠", "ru": {"title": "–ü—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ —á–µ—Ä–µ–∑ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –¥–µ–≥—Ä–∞–¥–∞—Ü–∏–∏: —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è MaskDCPT –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –∏–∑–æ–±
[16.10.2025 11:11] Using data from previous issue: {"categories": ["#architecture", "#reasoning", "#training", "#optimization"], "emoji": "üîÄ", "ru": {"title": "–¢—Ä–∏ —Å—Ç–∞–¥–∏–∏ –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏–∏: –ø—Ä–æ—Å—Ç–æ–µ —Å–ª–∏—è–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –ø–æ–±–µ–∂–¥–∞–µ—Ç —Å–ª–æ–∂–Ω—ã–µ –º–µ—Ç–æ–¥—ã", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –∏–∑—É—á–∏–ª–∏ –ø—Ä–æ—Å—Ç–µ–π—à–∏–π –º–µ—Ç–æ–¥ —Å–ª–∏—è–Ω–∏—è –º–æ–¥–µ–ª–µ–π - –ø—Ä—è–º—É—é –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—é –≤–µ—Å–æ–≤ –º–µ–∂–¥—É Instruct –∏ Thinki
[16.10.2025 11:11] Using data from previous issue: {"categories": ["#multilingual", "#dataset", "#benchmark", "#multimodal", "#alignment", "#open_source"], "emoji": "üîç", "ru": {"title": "–î–µ—Ç–∞–ª—å–Ω–æ–µ –¥–≤—É—è–∑—ã—á–Ω–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ —Ç–µ–∫—Å—Ç–∞", "desc": "FG-CLIP 2 ‚Äî —ç—Ç–æ –¥–≤—É—è–∑—ã—á–Ω–∞—è vision-language –º–æ–¥–µ–ª—å –¥–ª—è –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ –∏ –∫–∏—Ç–∞–π—Å–∫–æ–≥–æ —è–∑—ã–∫–æ–≤, –∫–æ—Ç–æ—Ä–∞—è —É–ª—É
[16.10.2025 11:11] Using data from previous issue: {"categories": ["#architecture", "#agi", "#agents", "#training", "#3d", "#optimization"], "emoji": "ü§ñ", "ru": {"title": "–ú—è–≥–∫–∏–µ –ø—Ä–æ–º–ø—Ç—ã –¥–ª—è —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã—Ö —Ä–æ–±–æ—Ç–æ–≤", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ Soft Prompt –¥–ª—è –æ–±—É—á–µ–Ω–∏—è Vision-Language-Action –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–µ —É–ø—Ä–∞–≤–ª—è—é—Ç —Ä–∞–∑–Ω—ã–º–∏ —Ä–æ–±–æ—Ç–∞
[16.10.2025 11:11] Using data from previous issue: {"categories": ["#architecture", "#training", "#inference", "#optimization"], "emoji": "‚ö°", "ru": {"title": "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤ –∑–∞ –æ–¥–∏–Ω –ø—Ä–æ—Ö–æ–¥ —á–µ—Ä–µ–∑ –ø–æ–∑–¥–Ω–∏–µ —Å–ª–æ–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –º–µ—Ç–æ–¥ Direct Multi-Token Decoding (DMTD), –∫–æ—Ç–æ—Ä—ã–π —É—Å–∫–æ—Ä—è–µ—Ç inference –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ
[16.10.2025 11:11] Using data from previous issue: {"categories": ["#rl", "#agents", "#games", "#optimization", "#training"], "emoji": "üèéÔ∏è", "ru": {"title": "–ö–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–≤—É—Ö –∞–≥–µ–Ω—Ç–æ–≤ –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ–≥–æ –∞–≤—Ç–æ–ø–∏–ª–æ—Ç–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–º—É –≤–æ–∂–¥–µ–Ω–∏—é –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º CoIRL-AD, –∫–æ—Ç–æ—Ä—ã–π –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç imitation learning –∏ re
[16.10.2025 11:11] Using data from previous issue: {"categories": ["#architecture", "#inference", "#long_context", "#benchmark", "#optimization", "#training"], "emoji": "üöÄ", "ru": {"title": "–£—Å–∫–æ—Ä–µ–Ω–∏–µ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è LLM —á–µ—Ä–µ–∑ –ª–æ–∫–∞–ª—å–Ω–æ–µ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ", "desc": "NOSA ‚Äî —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–±—É—á–∞–µ–º–æ–≥–æ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è (sparse attention), –∫–æ—Ç–æ—Ä—ã–π —Ä–µ—à–∞–µ—Ç –ø—Ä
[16.10.2025 11:11] Using data from previous issue: {"categories": ["#agents", "#optimization", "#graphs", "#games"], "emoji": "üï∏Ô∏è", "ru": {"title": "–ì–∏–ø–µ—Ä–≥—Ä–∞—Ñ—ã –¥–ª—è —É–º–Ω–æ–π –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏ AI-–∞–≥–µ–Ω—Ç–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç HyperAgent ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≥–∏–ø–µ—Ä–≥—Ä–∞—Ñ–æ–≤ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏ –≤ –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º–∞—Ö —Å LLM. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Ç—Ä–∞–¥–∏—Ü
[16.10.2025 11:11] Using data from previous issue: {"categories": ["#reasoning", "#math", "#benchmark", "#interpretability"], "emoji": "üîç", "ru": {"title": "–ü—Ä–æ–≤–µ—Ä–∫–∞ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π AI –Ω–∞ –ø—Ä–æ—á–Ω–æ—Å—Ç—å", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Å–æ–∑–¥–∞–ª–∏ Hard2Verify ‚Äî –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π LLM, —Å–æ–∑–¥–∞–Ω–Ω—ã–π —Å –ø–æ–º–æ—â—å—é 500 —á–∞—Å–æ–≤
[16.10.2025 11:11] Using data from previous issue: {"categories": ["#agents", "#rl", "#optimization", "#reasoning", "#games", "#training"], "emoji": "ü§ù", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –∫–æ–º–∞–Ω–¥—ã AI-–∞–≥–µ–Ω—Ç–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç AT-GRPO ‚Äî –∞–ª–≥–æ—Ä–∏—Ç–º –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π –¥–ª—è –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º –Ω–∞ –±–∞
[16.10.2025 11:11] Using data from previous issue: {"categories": ["#rl", "#agents", "#reasoning", "#optimization", "#survey"], "emoji": "üîÑ", "ru": {"title": "–ò—Ç–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è SQL —á–µ—Ä–µ–∑ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ —Å –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö", "desc": "MTSQL-R1 ‚Äî —ç—Ç–æ –∞–≥–µ–Ω—Ç–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å–∏—Å—Ç–µ–º –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –≤ SQL –≤ –º–Ω–æ–≥–æ—à–∞–≥–æ–≤—ã—Ö –¥–∏–∞–ª–æ–≥–∞—Ö. –ê–≤—Ç–æ—Ä—ã —Ñ–æ—Ä–º—É
[16.10.2025 11:11] Using data from previous issue: {"categories": ["#agents", "#synthetic", "#dataset", "#graphs", "#optimization", "#benchmark"], "emoji": "üï∏Ô∏è", "ru": {"title": "–ì—Ä–∞—Ñ—ã –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –¥–ª—è –ø–æ–∏—Å–∫–∞ –∫–æ—Ä–Ω–µ–≤—ã—Ö –ø—Ä–∏—á–∏–Ω –æ—à–∏–±–æ–∫ –≤ –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º–∞—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç GraphTracer ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ –æ—à–∏–±–æ–∫ –≤ –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω—ã
[16.10.2025 11:11] Using data from previous issue: {"categories": ["#agents", "#optimization", "#training", "#games"], "emoji": "üéÆ", "ru": {"title": "–£–º–Ω—ã–µ –∏–≥—Ä–æ–≤—ã–µ –ø–µ—Ä—Å–æ–Ω–∞–∂–∏ —á–µ—Ä–µ–∑ –ø—Ä–æ–º–ø—Ç–∏–Ω–≥ –∏ —Ñ–∞–π–Ω-—Ç—é–Ω–∏–Ω–≥", "desc": "–ö–æ–º–∞–Ω–¥–∞ Tu_Character_lab —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∞ –º–µ—Ç–æ–¥—ã –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –¥–∏–∞–ª–æ–≥–æ–≤—ã—Ö NPC –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π –≤ –∏–≥—Ä–∞—Ö –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM. –í —Ä–∞–±–æ—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å –¥–≤–∞ –ø–æ
[16.10.2025 11:11] Querying the API.
[16.10.2025 11:11] Claude request. Model: claude-sonnet-4-5-20250929. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Hierarchical Frequency Tagging Probe (HFTP) identifies neuron-wise components in LLMs and cortical regions encoding syntactic structures, revealing differences in how LLMs and the human brain process syntax.  					AI-generated summary 				 Large Language Models (LLMs) demonstrate human-level or even superior language abilities, effectively modeling syntactic structures, yet the specific computational modules responsible remain unclear. A key question is whether LLM behavioral capabilities stem from mechanisms akin to those in the human brain. To address these questions, we introduce the Hierarchical Frequency Tagging Probe (HFTP), a tool that utilizes frequency-domain analysis to identify neuron-wise components of LLMs (e.g., individual Multilayer Perceptron (MLP) neurons) and cortical regions (via intracranial recordings) encoding syntactic structures. Our results show that models such as GPT-2, Gemma, Gemma 2, Llama 2, Llama 3.1, and GLM-4 process syntax in analogous layers, while the human brain relies on distinct cortical regions for different syntactic levels. Representational similarity analysis reveals a stronger alignment between LLM representations and the left hemisphere of the brain (dominant in language processing). Notably, upgraded models exhibit divergent trends: Gemma 2 shows greater brain similarity than Gemma, while Llama 3.1 shows less alignment with the brain compared to Llama 2. These findings offer new insights into the interpretability of LLM behavioral improvements, raising questions about whether these advancements are driven by human-like or non-human-like mechanisms, and establish HFTP as a valuable tool bridging computational linguistics and cognitive neuroscience. This project is available at https://github.com/LilTiger/HFTP.
[16.10.2025 11:11] Response: ```json
{
  "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –º–µ—Ç–æ–¥ Hierarchical Frequency Tagging Probe (HFTP), –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —á–∞—Å—Ç–æ—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è –Ω–µ–π—Ä–æ–Ω–æ–≤ –≤ LLM –∏ –æ–±–ª–∞—Å—Ç–µ–π –º–æ–∑–≥–∞, –æ—Ç–≤–µ—á–∞—é—â–∏—Ö –∑–∞ –æ–±—Ä–∞–±–æ—Ç–∫—É —Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∏—Ö —Å—Ç—Ä—É–∫—Ç—É—Ä. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –º–æ–¥–µ–ª–∏ –≤—Ä–æ–¥–µ GPT-2, Llama –∏ Gemma –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç —Å–∏–Ω—Ç–∞–∫—Å–∏—Å –≤ –ø–æ—Ö–æ–∂–∏—Ö —Å–ª–æ—è—Ö, —Ç–æ–≥–¥–∞ –∫–∞–∫ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–π –º–æ–∑–≥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ä–∞–∑–Ω—ã–µ –∫–æ—Ä—Ç–∏–∫–∞–ª—å–Ω—ã–µ –æ–±–ª–∞—Å—Ç–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —É—Ä–æ–≤–Ω–µ–π —Å–∏–Ω—Ç–∞–∫—Å–∏—Å–∞. –ê–Ω–∞–ª–∏–∑ –≤—ã—è–≤–∏–ª –±–æ–ª–µ–µ —Å–∏–ª—å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –º–µ–∂–¥—É –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è–º–∏ LLM –∏ –ª–µ–≤—ã–º –ø–æ–ª—É—à–∞—Ä–∏–µ–º –º–æ–∑–≥–∞, –∫–æ—Ç–æ—Ä–æ–µ –¥–æ–º–∏–Ω–∏—Ä—É–µ—Ç –≤ –æ–±—Ä–∞–±–æ—Ç–∫–µ —è–∑—ã–∫–∞. –ò–Ω—Ç–µ—Ä–µ—Å–Ω–æ, —á—Ç–æ —É–ª—É—á—à–µ–Ω–Ω—ã–µ –≤–µ—Ä—Å–∏–∏ –º–æ–¥–µ–ª–µ–π –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç —Ä–∞–∑–Ω—ã–µ —Ç–µ–Ω–¥–µ–Ω—Ü–∏–∏: Gemma 2 —Å—Ç–∞–ª–∞ –±–ª–∏–∂–µ –∫ –º–æ–∑–≥—É, –∞ Llama 3.1 –æ—Ç–¥–∞–ª–∏–ª–∞—Å—å –æ—Ç –Ω–µ–≥–æ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º–∏ –≤–µ—Ä—Å–∏—è–º–∏.",
  "emoji": "üß†",
  "title": "–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤ LLM –∏ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–º –º–æ–∑–≥–µ —á–µ—Ä–µ–∑ —á–∞—Å—Ç–æ—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑"
}
```
[16.10.2025 11:11] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Hierarchical Frequency Tagging Probe (HFTP) identifies neuron-wise components in LLMs and cortical regions encoding syntactic structures, revealing differences in how LLMs and the human brain process syntax.  					AI-generated summary 				 Large Language Models (LLMs) demonstrate human-level or even superior language abilities, effectively modeling syntactic structures, yet the specific computational modules responsible remain unclear. A key question is whether LLM behavioral capabilities stem from mechanisms akin to those in the human brain. To address these questions, we introduce the Hierarchical Frequency Tagging Probe (HFTP), a tool that utilizes frequency-domain analysis to identify neuron-wise components of LLMs (e.g., individual Multilayer Perceptron (MLP) neurons) and cortical regions (via intracranial recordings) encoding syntactic structures. Our results show that models such as GPT-2, Gemma, Gemma 2, Llama 2, Llama 3.1, and GLM-4 process syntax in analogous layers, while the human brain relies on distinct cortical regions for different syntactic levels. Representational similarity analysis reveals a stronger alignment between LLM representations and the left hemisphere of the brain (dominant in language processing). Notably, upgraded models exhibit divergent trends: Gemma 2 shows greater brain similarity than Gemma, while Llama 3.1 shows less alignment with the brain compared to Llama 2. These findings offer new insights into the interpretability of LLM behavioral improvements, raising questions about whether these advancements are driven by human-like or non-human-like mechanisms, and establish HFTP as a valuable tool bridging computational linguistics and cognitive neuroscience. This project is available at https://github.com/LilTiger/HFTP."

[16.10.2025 11:11] Response: ```python
["MULTILINGUAL", "ARCHITECTURE"]
```
[16.10.2025 11:11] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Hierarchical Frequency Tagging Probe (HFTP) identifies neuron-wise components in LLMs and cortical regions encoding syntactic structures, revealing differences in how LLMs and the human brain process syntax.  					AI-generated summary 				 Large Language Models (LLMs) demonstrate human-level or even superior language abilities, effectively modeling syntactic structures, yet the specific computational modules responsible remain unclear. A key question is whether LLM behavioral capabilities stem from mechanisms akin to those in the human brain. To address these questions, we introduce the Hierarchical Frequency Tagging Probe (HFTP), a tool that utilizes frequency-domain analysis to identify neuron-wise components of LLMs (e.g., individual Multilayer Perceptron (MLP) neurons) and cortical regions (via intracranial recordings) encoding syntactic structures. Our results show that models such as GPT-2, Gemma, Gemma 2, Llama 2, Llama 3.1, and GLM-4 process syntax in analogous layers, while the human brain relies on distinct cortical regions for different syntactic levels. Representational similarity analysis reveals a stronger alignment between LLM representations and the left hemisphere of the brain (dominant in language processing). Notably, upgraded models exhibit divergent trends: Gemma 2 shows greater brain similarity than Gemma, while Llama 3.1 shows less alignment with the brain compared to Llama 2. These findings offer new insights into the interpretability of LLM behavioral improvements, raising questions about whether these advancements are driven by human-like or non-human-like mechanisms, and establish HFTP as a valuable tool bridging computational linguistics and cognitive neuroscience. This project is available at https://github.com/LilTiger/HFTP."

[16.10.2025 11:11] Response: ```python
["INTERPRETABILITY", "ALIGNMENT"]
```
[16.10.2025 11:11] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The Hierarchical Frequency Tagging Probe (HFTP) is a novel tool designed to analyze how Large Language Models (LLMs) and the human brain process syntactic structures. By using frequency-domain analysis, HFTP identifies neuron-wise components in both LLMs and human cortical regions, revealing that while LLMs like GPT-2 and Llama 2 process syntax in similar layers, the human brain utilizes distinct regions for different syntactic levels. The study finds a stronger alignment between LLM representations and the left hemisphere of the brain, which is crucial for language processing. These insights enhance our understanding of LLMs\' capabilities and their potential similarities or differences with human cognitive mechanisms.","title":"Bridging LLMs and Human Syntax Processing with HFTP"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="The Hierarchical Frequency Tagging Probe (HFTP) is a novel tool designed to analyze how Large Language Models (LLMs) and the human brain process syntactic structures. By using frequency-domain analysis, HFTP identifies neuron-wise components in both LLMs and human cortical regions, revealing that while LLMs like GPT-2 and Llama 2 process syntax in similar layers, the human brain utilizes distinct regions for different syntactic levels. The study finds a stronger alignment between LLM representations and the left hemisphere of the brain, which is crucial for language processing. These insights enhance our understanding of LLMs' capabilities and their potential similarities or differences with human cognitive mechanisms.", title='Bridging LLMs and Human Syntax Processing with HFTP'))
[16.10.2025 11:11] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Â±ÇÊ¨°È¢ëÁéáÊ†áËÆ∞Êé¢ÈíàÔºàHFTPÔºâÁî®‰∫éËØÜÂà´Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂíå‰∫∫ËÑëÁöÆÂ±Ç‰∏≠‰∏éÂè•Ê≥ïÁªìÊûÑÁõ∏ÂÖ≥ÁöÑÁ•ûÁªèÂÖÉÁªÑ‰ª∂„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåLLMsÂú®Â§ÑÁêÜÂè•Ê≥ïÊó∂‰ΩøÁî®Á±ª‰ººÁöÑÂ±ÇÊ¨°ÁªìÊûÑÔºåËÄå‰∫∫ËÑëÂàô‰æùËµñ‰∏çÂêåÁöÑÁöÆÂ±ÇÂå∫Âüü„ÄÇÈÄöËøáË°®Á§∫Áõ∏‰ººÊÄßÂàÜÊûêÔºåÂèëÁé∞LLMsÁöÑË°®Á§∫‰∏éÂ§ßËÑëÂ∑¶ÂçäÁêÉÁöÑÂØπÈΩêÁ®ãÂ∫¶Êõ¥È´òÔºåËøô‰∏ÄÂçäÁêÉÂú®ËØ≠Ë®ÄÂ§ÑÁêÜ‰∏äÂç†‰∏ªÂØºÂú∞‰Ωç„ÄÇHFTP‰∏∫ÁêÜËß£LLMsÁöÑË°å‰∏∫ÊîπËøõÊèê‰æõ‰∫ÜÊñ∞ËßÜËßíÔºåÂπ∂Êé¢ËÆ®‰∫ÜËøô‰∫õËøõÊ≠•ÊòØÂê¶Ê∫ê‰∫éÁ±ª‰ºº‰∫∫Á±ªÁöÑÊú∫Âà∂„ÄÇ","title":"Êè≠Á§∫LLMs‰∏é‰∫∫ËÑëÂè•Ê≥ïÂ§ÑÁêÜÁöÑÂ∑ÆÂºÇ"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Â±ÇÊ¨°È¢ëÁéáÊ†áËÆ∞Êé¢ÈíàÔºàHFTPÔºâÁî®‰∫éËØÜÂà´Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂíå‰∫∫ËÑëÁöÆÂ±Ç‰∏≠‰∏éÂè•Ê≥ïÁªìÊûÑÁõ∏ÂÖ≥ÁöÑÁ•ûÁªèÂÖÉÁªÑ‰ª∂„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåLLMsÂú®Â§ÑÁêÜÂè•Ê≥ïÊó∂‰ΩøÁî®Á±ª‰ººÁöÑÂ±ÇÊ¨°ÁªìÊûÑÔºåËÄå‰∫∫ËÑëÂàô‰æùËµñ‰∏çÂêåÁöÑÁöÆÂ±ÇÂå∫Âüü„ÄÇÈÄöËøáË°®Á§∫Áõ∏‰ººÊÄßÂàÜÊûêÔºåÂèëÁé∞LLMsÁöÑË°®Á§∫‰∏éÂ§ßËÑëÂ∑¶ÂçäÁêÉÁöÑÂØπÈΩêÁ®ãÂ∫¶Êõ¥È´òÔºåËøô‰∏ÄÂçäÁêÉÂú®ËØ≠Ë®ÄÂ§ÑÁêÜ‰∏äÂç†‰∏ªÂØºÂú∞‰Ωç„ÄÇHFTP‰∏∫ÁêÜËß£LLMsÁöÑË°å‰∏∫ÊîπËøõÊèê‰æõ‰∫ÜÊñ∞ËßÜËßíÔºåÂπ∂Êé¢ËÆ®‰∫ÜËøô‰∫õËøõÊ≠•ÊòØÂê¶Ê∫ê‰∫éÁ±ª‰ºº‰∫∫Á±ªÁöÑÊú∫Âà∂„ÄÇ', title='Êè≠Á§∫LLMs‰∏é‰∫∫ËÑëÂè•Ê≥ïÂ§ÑÁêÜÁöÑÂ∑ÆÂºÇ'))
[16.10.2025 11:11] Using data from previous issue: {"categories": ["#video", "#cv", "#games", "#diffusion"], "emoji": "üéØ", "ru": {"title": "–¢—Ä–µ–∫–∏–Ω–≥ —Ç–æ—á–µ–∫ —á–µ—Ä–µ–∑ –≤–∏–¥–µ–æ-–¥–∏—Ñ—Ñ—É–∑–∏—é –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –≤–∏–¥–µ–æ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –º–æ–≥—É—Ç –≤—ã–ø–æ–ª–Ω—è—Ç—å —Ç—Ä–µ–∫–∏–Ω–≥ —Ç–æ—á–µ–∫ –≤ zero-shot —Ä–µ–∂–∏–º–µ, –ø—Ä–æ—Å—Ç–æ –≤–∏–∑—É–∞–ª—å–Ω–æ –ø–æ–º–µ—á–∞—è —Ç–æ—á–∫–∏ –Ω
[16.10.2025 11:11] Querying the API.
[16.10.2025 11:11] Claude request. Model: claude-sonnet-4-5-20250929. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

MATH-Beyond is a benchmark designed to challenge existing reinforcement learning methods by requiring deeper reasoning capabilities beyond current model capabilities.  					AI-generated summary 				 With the advent of DeepSeek-R1, a new wave of reinforcement learning (RL) methods has emerged that seem to unlock stronger mathematical reasoning. However, a closer look at the open-source ecosystem reveals a critical limitation: with sufficiently many draws (e.g., pass@1024), many existing base models already solve nearly all questions on widely used math benchmarks such as MATH-500 and AIME 2024. This suggests that the RL fine-tuning methods prevalent in the LLM reasoning literature largely sharpen existing solution modes rather than discovering entirely new ones. Such sharpening stands in contrast to the broader promise of RL: to foster exploration and to acquire new skills. To move beyond this plateau, we introduce MATH-Beyond (MATH-B), a benchmark deliberately constructed to defeat common open-source models of up to 8B parameters even under large sampling budgets. Improving performance on our benchmark via RL requires methods that learn to reason in ways that go beyond base model capabilities in repeated sampling. Since the problems are drawn from subsets of DAPO-Math-17K and DeepScaleR datasets, they remain topically equivalent to standard high-school math. Validating our premise, RL fine-tuned models such as Nemotron-Research-Reasoning-Qwen-1.5B and DeepScaleR-1.5B-Preview perform poorly on MATH-B at pass@1024, showing how existing approaches fall short on tackling harder instances. We hope MATH-B will catalyze exploration-driven RL approaches that elicit deeper reasoning capabilities. We release MATH-B at https://huggingface.co/datasets/brendel-group/MATH-Beyond.
[16.10.2025 11:11] Response: ```json
{
  "desc": "MATH-Beyond ‚Äî —ç—Ç–æ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π —Ç–∞–∫, —á—Ç–æ–±—ã —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–æ–¥–µ–ª–∏ –¥–æ 8B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –Ω–µ –º–æ–≥–ª–∏ —Ä–µ—à–∏—Ç—å –∑–∞–¥–∞—á–∏ –¥–∞–∂–µ –ø—Ä–∏ –±–æ–ª—å—à–æ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ –ø–æ–ø—ã—Ç–æ–∫. –ê–≤—Ç–æ—Ä—ã –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã reinforcement learning –≤ –æ—Å–Ω–æ–≤–Ω–æ–º ¬´–∑–∞—Ç–∞—á–∏–≤–∞—é—Ç¬ª —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Å–ø–æ—Å–æ–±—ã —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á, –≤–º–µ—Å—Ç–æ —Ç–æ–≥–æ —á—Ç–æ–±—ã –æ—Ç–∫—Ä—ã–≤–∞—Ç—å –ø—Ä–∏–Ω—Ü–∏–ø–∏–∞–ª—å–Ω–æ –Ω–æ–≤—ã–µ –ø–æ–¥—Ö–æ–¥—ã. –ó–∞–¥–∞—á–∏ –≤–∑—è—Ç—ã –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ DAPO-Math-17K –∏ DeepScaleR –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç —É—Ä–æ–≤–Ω—é —à–∫–æ–ª—å–Ω–æ–π –º–∞—Ç–µ–º–∞—Ç–∏–∫–∏, –Ω–æ —Ç—Ä–µ–±—É—é—Ç –±–æ–ª–µ–µ –≥–ª—É–±–æ–∫–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. –¶–µ–ª—å –±–µ–Ω—á–º–∞—Ä–∫–∞ ‚Äî —Å—Ç–∏–º—É–ª–∏—Ä–æ–≤–∞—Ç—å —Ä–∞–∑—Ä–∞–±–æ—Ç–∫—É RL-–º–µ—Ç–æ–¥–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ —Ä–∞–∑–≤–∏–≤–∞—é—Ç –Ω–æ–≤—ã–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫ –ª–æ–≥–∏—á–µ—Å–∫–æ–º—É –º—ã—à–ª–µ–Ω–∏—é —É LLM, –∞ –Ω–µ –ø—Ä–æ—Å—Ç–æ —É–ª—É—á—à–∞—é—Ç —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –∑–Ω–∞–∫–æ–º—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–∞—Ö.",
  "emoji": "üßÆ",
  "title": "MATH-Beyond: –±–µ–Ω—á–º–∞—Ä–∫, —Ç—Ä–µ–±—É—é—â–∏–π –Ω–∞—Å—Ç–æ—è—â–µ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –æ—Ç RL-–º–æ–¥–µ–ª–µ–π"
}
```
[16.10.2025 11:11] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"MATH-Beyond is a benchmark designed to challenge existing reinforcement learning methods by requiring deeper reasoning capabilities beyond current model capabilities.  					AI-generated summary 				 With the advent of DeepSeek-R1, a new wave of reinforcement learning (RL) methods has emerged that seem to unlock stronger mathematical reasoning. However, a closer look at the open-source ecosystem reveals a critical limitation: with sufficiently many draws (e.g., pass@1024), many existing base models already solve nearly all questions on widely used math benchmarks such as MATH-500 and AIME 2024. This suggests that the RL fine-tuning methods prevalent in the LLM reasoning literature largely sharpen existing solution modes rather than discovering entirely new ones. Such sharpening stands in contrast to the broader promise of RL: to foster exploration and to acquire new skills. To move beyond this plateau, we introduce MATH-Beyond (MATH-B), a benchmark deliberately constructed to defeat common open-source models of up to 8B parameters even under large sampling budgets. Improving performance on our benchmark via RL requires methods that learn to reason in ways that go beyond base model capabilities in repeated sampling. Since the problems are drawn from subsets of DAPO-Math-17K and DeepScaleR datasets, they remain topically equivalent to standard high-school math. Validating our premise, RL fine-tuned models such as Nemotron-Research-Reasoning-Qwen-1.5B and DeepScaleR-1.5B-Preview perform poorly on MATH-B at pass@1024, showing how existing approaches fall short on tackling harder instances. We hope MATH-B will catalyze exploration-driven RL approaches that elicit deeper reasoning capabilities. We release MATH-B at https://huggingface.co/datasets/brendel-group/MATH-Beyond."

[16.10.2025 11:11] Response: ```python
['BENCHMARK', 'RL', 'TRAINING']
```
[16.10.2025 11:11] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"MATH-Beyond is a benchmark designed to challenge existing reinforcement learning methods by requiring deeper reasoning capabilities beyond current model capabilities.  					AI-generated summary 				 With the advent of DeepSeek-R1, a new wave of reinforcement learning (RL) methods has emerged that seem to unlock stronger mathematical reasoning. However, a closer look at the open-source ecosystem reveals a critical limitation: with sufficiently many draws (e.g., pass@1024), many existing base models already solve nearly all questions on widely used math benchmarks such as MATH-500 and AIME 2024. This suggests that the RL fine-tuning methods prevalent in the LLM reasoning literature largely sharpen existing solution modes rather than discovering entirely new ones. Such sharpening stands in contrast to the broader promise of RL: to foster exploration and to acquire new skills. To move beyond this plateau, we introduce MATH-Beyond (MATH-B), a benchmark deliberately constructed to defeat common open-source models of up to 8B parameters even under large sampling budgets. Improving performance on our benchmark via RL requires methods that learn to reason in ways that go beyond base model capabilities in repeated sampling. Since the problems are drawn from subsets of DAPO-Math-17K and DeepScaleR datasets, they remain topically equivalent to standard high-school math. Validating our premise, RL fine-tuned models such as Nemotron-Research-Reasoning-Qwen-1.5B and DeepScaleR-1.5B-Preview perform poorly on MATH-B at pass@1024, showing how existing approaches fall short on tackling harder instances. We hope MATH-B will catalyze exploration-driven RL approaches that elicit deeper reasoning capabilities. We release MATH-B at https://huggingface.co/datasets/brendel-group/MATH-Beyond."

[16.10.2025 11:11] Response: ```python
["REASONING", "OPEN_SOURCE"]
```
[16.10.2025 11:11] Response: ParsedChatCompletionMessage[Article](content='{"desc":"MATH-Beyond is a new benchmark aimed at testing the limits of reinforcement learning (RL) methods in mathematical reasoning. It highlights that many existing models can solve standard math problems but struggle with more complex reasoning tasks. The benchmark is designed to challenge models with up to 8 billion parameters, requiring them to develop new reasoning strategies rather than just refining existing ones. By doing so, MATH-Beyond encourages the development of RL techniques that promote exploration and deeper understanding in mathematical problem-solving.","title":"MATH-Beyond: Elevating Reinforcement Learning to New Reasoning Heights"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='MATH-Beyond is a new benchmark aimed at testing the limits of reinforcement learning (RL) methods in mathematical reasoning. It highlights that many existing models can solve standard math problems but struggle with more complex reasoning tasks. The benchmark is designed to challenge models with up to 8 billion parameters, requiring them to develop new reasoning strategies rather than just refining existing ones. By doing so, MATH-Beyond encourages the development of RL techniques that promote exploration and deeper understanding in mathematical problem-solving.', title='MATH-Beyond: Elevating Reinforcement Learning to New Reasoning Heights'))
[16.10.2025 11:11] Response: ParsedChatCompletionMessage[Article](content='{"desc":"MATH-BeyondÊòØ‰∏Ä‰∏™Âü∫ÂáÜÊµãËØïÔºåÊó®Âú®ÊåëÊàòÁé∞ÊúâÁöÑÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ïÔºåË¶ÅÊ±ÇÊõ¥Ê∑±Â±ÇÊ¨°ÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇÂ∞ΩÁÆ°DeepSeek-R1Á≠âÊñ∞ÊñπÊ≥ï‰ºº‰πéÊèêÂçá‰∫ÜÊï∞Â≠¶Êé®ÁêÜËÉΩÂäõÔºå‰ΩÜËÆ∏Â§öÁé∞ÊúâÊ®°ÂûãÂú®Â∏∏Áî®Êï∞Â≠¶Âü∫ÂáÜ‰∏äÂá†‰πéÈÉΩËÉΩËß£ÂÜ≥ÊâÄÊúâÈóÆÈ¢òÔºåÊòæÁ§∫Âá∫Âº∫ÂåñÂ≠¶‰π†ÂæÆË∞ÉÊñπÊ≥ïÁöÑÂ±ÄÈôêÊÄß„ÄÇ‰∏∫‰∫ÜÁ™ÅÁ†¥Ëøô‰∏ÄÁì∂È¢àÔºåMATH-BeyondË¢´ËÆæËÆ°‰∏∫ËÉΩÂ§üÂáªË¥•Â∏∏ËßÅÁöÑÂºÄÊ∫êÊ®°ÂûãÔºåË¶ÅÊ±ÇÊ®°ÂûãÂú®ÈáçÂ§çÈááÊ†∑‰∏≠Â≠¶‰π†Ë∂ÖË∂äÂü∫Á°ÄÊ®°ÂûãËÉΩÂäõÁöÑÊé®ÁêÜÊñπÂºè„ÄÇÊàë‰ª¨Â∏åÊúõMATH-BËÉΩÂ§ü‰øÉËøõÊé¢Á¥¢È©±Âä®ÁöÑÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ïÔºåÊøÄÂèëÊõ¥Ê∑±Â±ÇÊ¨°ÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇ","title":"MATH-BeyondÔºöÊåëÊàòÂº∫ÂåñÂ≠¶‰π†ÁöÑÊé®ÁêÜÊûÅÈôê"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='MATH-BeyondÊòØ‰∏Ä‰∏™Âü∫ÂáÜÊµãËØïÔºåÊó®Âú®ÊåëÊàòÁé∞ÊúâÁöÑÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ïÔºåË¶ÅÊ±ÇÊõ¥Ê∑±Â±ÇÊ¨°ÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇÂ∞ΩÁÆ°DeepSeek-R1Á≠âÊñ∞ÊñπÊ≥ï‰ºº‰πéÊèêÂçá‰∫ÜÊï∞Â≠¶Êé®ÁêÜËÉΩÂäõÔºå‰ΩÜËÆ∏Â§öÁé∞ÊúâÊ®°ÂûãÂú®Â∏∏Áî®Êï∞Â≠¶Âü∫ÂáÜ‰∏äÂá†‰πéÈÉΩËÉΩËß£ÂÜ≥ÊâÄÊúâÈóÆÈ¢òÔºåÊòæÁ§∫Âá∫Âº∫ÂåñÂ≠¶‰π†ÂæÆË∞ÉÊñπÊ≥ïÁöÑÂ±ÄÈôêÊÄß„ÄÇ‰∏∫‰∫ÜÁ™ÅÁ†¥Ëøô‰∏ÄÁì∂È¢àÔºåMATH-BeyondË¢´ËÆæËÆ°‰∏∫ËÉΩÂ§üÂáªË¥•Â∏∏ËßÅÁöÑÂºÄÊ∫êÊ®°ÂûãÔºåË¶ÅÊ±ÇÊ®°ÂûãÂú®ÈáçÂ§çÈááÊ†∑‰∏≠Â≠¶‰π†Ë∂ÖË∂äÂü∫Á°ÄÊ®°ÂûãËÉΩÂäõÁöÑÊé®ÁêÜÊñπÂºè„ÄÇÊàë‰ª¨Â∏åÊúõMATH-BËÉΩÂ§ü‰øÉËøõÊé¢Á¥¢È©±Âä®ÁöÑÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ïÔºåÊøÄÂèëÊõ¥Ê∑±Â±ÇÊ¨°ÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇ', title='MATH-BeyondÔºöÊåëÊàòÂº∫ÂåñÂ≠¶‰π†ÁöÑÊé®ÁêÜÊûÅÈôê'))
[16.10.2025 11:11] Renaming data file.
[16.10.2025 11:11] Renaming previous data. hf_papers.json to ./d/2025-10-16.json
[16.10.2025 11:11] Saving new data file.
[16.10.2025 11:11] Generating page.
[16.10.2025 11:11] Renaming previous page.
[16.10.2025 11:11] Renaming previous data. index.html to ./d/2025-10-16.html
[16.10.2025 11:11] Writing result.
[16.10.2025 11:11] Renaming log file.
[16.10.2025 11:11] Renaming previous data. log.txt to ./logs/2025-10-16_last_log.txt
