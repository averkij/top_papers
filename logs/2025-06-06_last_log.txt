[06.06.2025 15:11] Read previous papers.
[06.06.2025 15:11] Generating top page (month).
[06.06.2025 15:11] Writing top page (month).
[06.06.2025 16:14] Read previous papers.
[06.06.2025 16:14] Get feed.
[06.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.05010
[06.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.05301
[06.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.04308
[06.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.05284
[06.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.05229
[06.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.02865
[06.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2505.23656
[06.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.05176
[06.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.05328
[06.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.05240
[06.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.05209
[06.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.04633
[06.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.03077
[06.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.05345
[06.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.05344
[06.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.05349
[06.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.05331
[06.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.05287
[06.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.05327
[06.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.02620
[06.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.04209
[06.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.01011
[06.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.04734
[06.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.05348
[06.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.04405
[06.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20914
[06.06.2025 16:14] Extract page data from URL. URL: https://huggingface.co/papers/2506.05334
[06.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.05282
[06.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.05278
[06.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.04956
[06.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.04598
[06.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.02751
[06.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.00830
[06.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.04245
[06.06.2025 16:14] Extract page data from URL. URL: https://huggingface.co/papers/2506.05313
[06.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.02587
[06.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2505.23115
[06.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.04996
[06.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.03643
[06.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.03238
[06.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.02444
[06.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.00981
[06.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.04462
[06.06.2025 16:14] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[06.06.2025 16:14] No deleted papers detected.
[06.06.2025 16:14] Downloading and parsing papers (pdf, html). Total: 43.
[06.06.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2506.05010.
[06.06.2025 16:14] Extra JSON file exists (./assets/json/2506.05010.json), skip PDF parsing.
[06.06.2025 16:14] Paper image links file exists (./assets/img_data/2506.05010.json), skip HTML parsing.
[06.06.2025 16:14] Success.
[06.06.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2506.05301.
[06.06.2025 16:14] Extra JSON file exists (./assets/json/2506.05301.json), skip PDF parsing.
[06.06.2025 16:14] Paper image links file exists (./assets/img_data/2506.05301.json), skip HTML parsing.
[06.06.2025 16:14] Success.
[06.06.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2506.04308.
[06.06.2025 16:14] Extra JSON file exists (./assets/json/2506.04308.json), skip PDF parsing.
[06.06.2025 16:14] Paper image links file exists (./assets/img_data/2506.04308.json), skip HTML parsing.
[06.06.2025 16:14] Success.
[06.06.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2506.05284.
[06.06.2025 16:14] Extra JSON file exists (./assets/json/2506.05284.json), skip PDF parsing.
[06.06.2025 16:14] Paper image links file exists (./assets/img_data/2506.05284.json), skip HTML parsing.
[06.06.2025 16:14] Success.
[06.06.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2506.05229.
[06.06.2025 16:14] Extra JSON file exists (./assets/json/2506.05229.json), skip PDF parsing.
[06.06.2025 16:14] Paper image links file exists (./assets/img_data/2506.05229.json), skip HTML parsing.
[06.06.2025 16:14] Success.
[06.06.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2506.02865.
[06.06.2025 16:14] Extra JSON file exists (./assets/json/2506.02865.json), skip PDF parsing.
[06.06.2025 16:14] Paper image links file exists (./assets/img_data/2506.02865.json), skip HTML parsing.
[06.06.2025 16:14] Success.
[06.06.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2505.23656.
[06.06.2025 16:14] Extra JSON file exists (./assets/json/2505.23656.json), skip PDF parsing.
[06.06.2025 16:14] Paper image links file exists (./assets/img_data/2505.23656.json), skip HTML parsing.
[06.06.2025 16:14] Success.
[06.06.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2506.05176.
[06.06.2025 16:14] Extra JSON file exists (./assets/json/2506.05176.json), skip PDF parsing.
[06.06.2025 16:14] Paper image links file exists (./assets/img_data/2506.05176.json), skip HTML parsing.
[06.06.2025 16:14] Success.
[06.06.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2506.05328.
[06.06.2025 16:14] Extra JSON file exists (./assets/json/2506.05328.json), skip PDF parsing.
[06.06.2025 16:14] Paper image links file exists (./assets/img_data/2506.05328.json), skip HTML parsing.
[06.06.2025 16:14] Success.
[06.06.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2506.05240.
[06.06.2025 16:14] Extra JSON file exists (./assets/json/2506.05240.json), skip PDF parsing.
[06.06.2025 16:14] Paper image links file exists (./assets/img_data/2506.05240.json), skip HTML parsing.
[06.06.2025 16:14] Success.
[06.06.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2506.05209.
[06.06.2025 16:14] Extra JSON file exists (./assets/json/2506.05209.json), skip PDF parsing.
[06.06.2025 16:14] Paper image links file exists (./assets/img_data/2506.05209.json), skip HTML parsing.
[06.06.2025 16:14] Success.
[06.06.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2506.04633.
[06.06.2025 16:14] Extra JSON file exists (./assets/json/2506.04633.json), skip PDF parsing.
[06.06.2025 16:14] Paper image links file exists (./assets/img_data/2506.04633.json), skip HTML parsing.
[06.06.2025 16:14] Success.
[06.06.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2506.03077.
[06.06.2025 16:14] Extra JSON file exists (./assets/json/2506.03077.json), skip PDF parsing.
[06.06.2025 16:14] Paper image links file exists (./assets/img_data/2506.03077.json), skip HTML parsing.
[06.06.2025 16:14] Success.
[06.06.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2506.05345.
[06.06.2025 16:14] Extra JSON file exists (./assets/json/2506.05345.json), skip PDF parsing.
[06.06.2025 16:14] Paper image links file exists (./assets/img_data/2506.05345.json), skip HTML parsing.
[06.06.2025 16:14] Success.
[06.06.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2506.05344.
[06.06.2025 16:14] Extra JSON file exists (./assets/json/2506.05344.json), skip PDF parsing.
[06.06.2025 16:14] Paper image links file exists (./assets/img_data/2506.05344.json), skip HTML parsing.
[06.06.2025 16:14] Success.
[06.06.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2506.05349.
[06.06.2025 16:14] Extra JSON file exists (./assets/json/2506.05349.json), skip PDF parsing.
[06.06.2025 16:14] Paper image links file exists (./assets/img_data/2506.05349.json), skip HTML parsing.
[06.06.2025 16:14] Success.
[06.06.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2506.05331.
[06.06.2025 16:14] Extra JSON file exists (./assets/json/2506.05331.json), skip PDF parsing.
[06.06.2025 16:14] Paper image links file exists (./assets/img_data/2506.05331.json), skip HTML parsing.
[06.06.2025 16:14] Success.
[06.06.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2506.05287.
[06.06.2025 16:14] Extra JSON file exists (./assets/json/2506.05287.json), skip PDF parsing.
[06.06.2025 16:14] Paper image links file exists (./assets/img_data/2506.05287.json), skip HTML parsing.
[06.06.2025 16:14] Success.
[06.06.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2506.05327.
[06.06.2025 16:14] Extra JSON file exists (./assets/json/2506.05327.json), skip PDF parsing.
[06.06.2025 16:14] Paper image links file exists (./assets/img_data/2506.05327.json), skip HTML parsing.
[06.06.2025 16:14] Success.
[06.06.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2506.02620.
[06.06.2025 16:14] Extra JSON file exists (./assets/json/2506.02620.json), skip PDF parsing.
[06.06.2025 16:14] Paper image links file exists (./assets/img_data/2506.02620.json), skip HTML parsing.
[06.06.2025 16:14] Success.
[06.06.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2506.04209.
[06.06.2025 16:14] Extra JSON file exists (./assets/json/2506.04209.json), skip PDF parsing.
[06.06.2025 16:14] Paper image links file exists (./assets/img_data/2506.04209.json), skip HTML parsing.
[06.06.2025 16:14] Success.
[06.06.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2506.01011.
[06.06.2025 16:14] Extra JSON file exists (./assets/json/2506.01011.json), skip PDF parsing.
[06.06.2025 16:14] Paper image links file exists (./assets/img_data/2506.01011.json), skip HTML parsing.
[06.06.2025 16:14] Success.
[06.06.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2506.04734.
[06.06.2025 16:14] Extra JSON file exists (./assets/json/2506.04734.json), skip PDF parsing.
[06.06.2025 16:14] Paper image links file exists (./assets/img_data/2506.04734.json), skip HTML parsing.
[06.06.2025 16:14] Success.
[06.06.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2506.05348.
[06.06.2025 16:14] Extra JSON file exists (./assets/json/2506.05348.json), skip PDF parsing.
[06.06.2025 16:14] Paper image links file exists (./assets/img_data/2506.05348.json), skip HTML parsing.
[06.06.2025 16:14] Success.
[06.06.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2506.04405.
[06.06.2025 16:14] Extra JSON file exists (./assets/json/2506.04405.json), skip PDF parsing.
[06.06.2025 16:14] Paper image links file exists (./assets/img_data/2506.04405.json), skip HTML parsing.
[06.06.2025 16:14] Success.
[06.06.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2505.20914.
[06.06.2025 16:14] Extra JSON file exists (./assets/json/2505.20914.json), skip PDF parsing.
[06.06.2025 16:14] Paper image links file exists (./assets/img_data/2505.20914.json), skip HTML parsing.
[06.06.2025 16:14] Success.
[06.06.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2506.05334.
[06.06.2025 16:14] Downloading paper 2506.05334 from http://arxiv.org/pdf/2506.05334v1...
[06.06.2025 16:14] Extracting affiliations from text.
[06.06.2025 16:14] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 5 ] . [ 1 4 3 3 5 0 . 6 0 5 2 : r Search Arena: Analyzing Search-Augmented LLMs Mihran Miroyan Tsung-Han Wu Logan King Tianle Li Jiayi Pan Xinyan Hu Wei-Lin Chiang Anastasios N. Angelopoulos Trevor Darrell Narges Norouzi Joseph E. Gonzalez "
[06.06.2025 16:14] Response: []
[06.06.2025 16:14] Extracting affiliations from text.
[06.06.2025 16:14] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 5 ] . [ 1 4 3 3 5 0 . 6 0 5 2 : r Search Arena: Analyzing Search-Augmented LLMs Mihran Miroyan Tsung-Han Wu Logan King Tianle Li Jiayi Pan Xinyan Hu Wei-Lin Chiang Anastasios N. Angelopoulos Trevor Darrell Narges Norouzi Joseph E. GonzalezSearch-augmented language models combine web search with Large Language Models (LLMs) to improve response groundedness and freshness. However, analyzing these systems remains challenging: existing datasets are limited in scale and narrow in scope, often constrained to static, single-turn, fact-checking questions. In this work, we introduce Search Arena, crowd-sourced, large-scale, human-preference dataset of over 24,000 paired multi-turn user interactions with search-augmented LLMs. The dataset spans diverse intents and languages, and contains full system traces with around 12,000 human preference votes. Our analysis reveals that user preferences are influenced by the number of citations, even when the cited content does not directly support the attributed claims, uncovering gap between perceived and actual credibility. Furthermore, user preferences vary across cited sources, revealing that community-driven platforms are generally preferred and static encyclopedic sources are not always appropriate and reliable. To assess performance across different settings, we conduct cross-arena analyses by testing search-augmented LLMs in general-purpose chat environment and conventional LLMs in search-intensive settings. We find that web search does not degrade and may even improve performance in non-search settings; however, the quality in search settings is significantly affected if solely relying on the models parametric knowledge. We open-sourced the dataset to support future research in this direction. Search Arena Code DatasetLarge Language Models (LLMs) have become popular interface for humanAI interaction, supporting information seeking and task assistance through natural, multi-turn dialogue. However, the capabilities of these models are constrained by their reliance on static training data, which prevents them from effectively handling time-sensitive questions, emerging topics, or niche domains. Search-augmented LLMs aim to bridge this gap by retrieving and using live web data during inference. Access to search enables LLMs to provide up-to-date, domain-specific, and factually verifiable responses [24, 33]. Recent developments [16, 41, 47] also reflect the growing interest in search-augmented LLMs. Despite rapid progress in developing search-augmented LLMs, our understanding of how users interact with these systemswhat they ask, how they engage in multi-turn dialogue, and what they expect in returnremains limited. Existing datasets capture interactions with either standalone LLMs [10, 70, 71] or traditional web search engines [8, 13]. However, search-augmented LLMs represent hybrid interface different from both: they not only retrieve information through web search but also rely on their reasoning and conversational capabilities. The most widely used datasets for evaluating these systems (SimpleQA [60] and BrowseComp [61]) primarily consist of single-turn, monolingual, fact-based queries and are relatively small in scale (typically 5k queries). As shown in Figure 1, 1*Equal contribution. Preprint. Table 1: Comparison of Search Datasets. Unlike prior datasets such as SimpleQA [60] and BrowseComp [61], which consist of static, English-only, single-turn fact-seeking queries, Search Arena evaluates models in diverse, open-ended, multilingual, and multi-turn settings. We release 24,069 conversations including 12,652 human preference votes. Further analyses are provided in Section 2. Dataset #Convs #Langs Multiturn Answer/Judge Conversation Properties Metadata SimpleQA 4,326 BrowseComp 1,266 1 (EN) No 1 (EN) No Short ground truth Expert-written short factual queries Verified supporting URLs, topic tags Short ground truth Expert-written challenging prompts Topic tags with detailed constraints Search Arena 24,069 71 Yes Human preference Open-ended, crowd-sourced prompts across diverse intents and topics Retrieved URLs, full model traces, user intent, and topic tags Figure 1: (Left) Nine intent categories with representative examples (truncated). In-the-wild user prompts are often ambiguous and require real-time web retrieval. (Right) Distribution of intents across user prompts. The majority of queries require more than simple factual lookup and range from information synthesis to creative content generation. The Other category is excluded from the visualizations. fact-checking accounts for only one-fifth of real-world user queries; the majority of user prompts, such as seeking analyses, recommendations, or problem-solving guidance, require combination of factual retrieval, reasoning, and open-ended dialogue. User expectations also extend beyond factual correctness: preferences can be shaped by the number, relevance, and credibility of citations, as well as the presentation style of responses. To address these gaps, we crowd-sourced the first large-scale human-preference dataset of in-the-wild user interactions with search-augmented LLMs. We developed Search Arena, an open evaluation and data collection platform that presents anonymized side-by-side model outputs in multi-turn settings and collects human votes. During the seven-week deployment period, we gathered and publicly released 24,069 conversations, along with 12,652 paired preference judgments. The dataset spans 11,650 users across 136 countries, 13 models, around 70 languages (including 11% multilingual prompts), and over 5,000 multi-turn interactions. We also introduce user intent taxonomy in the context of search-enabled human-AI interactions. As detailed in Table 1 and Section 2, Search Arena provides broad coverage across linguistic and intent features. We not only analyze user prompts to search-augmented LLMs, but also their preferences. We model user preferences through the BradleyTerry model [5, 10, 55] and study how different response characteristics interact with user judgments. We find that reasoning, larger search context window, and longer responses are positively associated with user preferences. Since citations are central to the trustworthiness of web-grounded outputs, we also examine citation features. Our results show that users prefer responses with higher number of cited sources (Figure 4). In addition, users prefer model responses citing tech-related platforms, commu"
[06.06.2025 16:15] Mistral response. {"id": "6d67c17e5c90442da36742d93c66c425", "object": "chat.completion", "created": 1749226500, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[]\n```"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1589, "total_tokens": 1597, "completion_tokens": 8}}
[06.06.2025 16:15] Response: ```python
[]
```
[06.06.2025 16:15] Deleting PDF ./assets/pdf/2506.05334.pdf.
[06.06.2025 16:15] Success.
[06.06.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2506.05282.
[06.06.2025 16:15] Extra JSON file exists (./assets/json/2506.05282.json), skip PDF parsing.
[06.06.2025 16:15] Paper image links file exists (./assets/img_data/2506.05282.json), skip HTML parsing.
[06.06.2025 16:15] Success.
[06.06.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2506.05278.
[06.06.2025 16:15] Extra JSON file exists (./assets/json/2506.05278.json), skip PDF parsing.
[06.06.2025 16:15] Paper image links file exists (./assets/img_data/2506.05278.json), skip HTML parsing.
[06.06.2025 16:15] Success.
[06.06.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2506.04956.
[06.06.2025 16:15] Extra JSON file exists (./assets/json/2506.04956.json), skip PDF parsing.
[06.06.2025 16:15] Paper image links file exists (./assets/img_data/2506.04956.json), skip HTML parsing.
[06.06.2025 16:15] Success.
[06.06.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2506.04598.
[06.06.2025 16:15] Extra JSON file exists (./assets/json/2506.04598.json), skip PDF parsing.
[06.06.2025 16:15] Paper image links file exists (./assets/img_data/2506.04598.json), skip HTML parsing.
[06.06.2025 16:15] Success.
[06.06.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2506.02751.
[06.06.2025 16:15] Extra JSON file exists (./assets/json/2506.02751.json), skip PDF parsing.
[06.06.2025 16:15] Paper image links file exists (./assets/img_data/2506.02751.json), skip HTML parsing.
[06.06.2025 16:15] Success.
[06.06.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2506.00830.
[06.06.2025 16:15] Extra JSON file exists (./assets/json/2506.00830.json), skip PDF parsing.
[06.06.2025 16:15] Paper image links file exists (./assets/img_data/2506.00830.json), skip HTML parsing.
[06.06.2025 16:15] Success.
[06.06.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2506.04245.
[06.06.2025 16:15] Extra JSON file exists (./assets/json/2506.04245.json), skip PDF parsing.
[06.06.2025 16:15] Paper image links file exists (./assets/img_data/2506.04245.json), skip HTML parsing.
[06.06.2025 16:15] Success.
[06.06.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2506.05313.
[06.06.2025 16:15] Downloading paper 2506.05313 from http://arxiv.org/pdf/2506.05313v1...
[06.06.2025 16:15] Extracting affiliations from text.
[06.06.2025 16:15] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"MARBLE: Material Recomposition and Blending in CLIP-Space Ta Ying Cheng* University of Oxford 5 2 0 2 5 ] . [ 1 3 1 3 5 0 . 6 0 5 2 : r Figure 1. Overview. We present MARBLE, method for performing various material editing in images such as material blending (top row) and parametric control of material properties (bottom row) leveraging CLIP-space and pre-trained generative models. Given two material exemplar images, we can achieve controllable blend of materials on the object by blending the material representation in CLIP-space. For parametric material attribute control, we learn shallow network using synthetic data to predict the direction in CLIP-space for changing specific material properties such as metallic. "
[06.06.2025 16:15] Response: ```python
["University of Oxford"]
```
[06.06.2025 16:15] Deleting PDF ./assets/pdf/2506.05313.pdf.
[06.06.2025 16:15] Success.
[06.06.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2506.02587.
[06.06.2025 16:15] Extra JSON file exists (./assets/json/2506.02587.json), skip PDF parsing.
[06.06.2025 16:15] Paper image links file exists (./assets/img_data/2506.02587.json), skip HTML parsing.
[06.06.2025 16:15] Success.
[06.06.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2505.23115.
[06.06.2025 16:15] Extra JSON file exists (./assets/json/2505.23115.json), skip PDF parsing.
[06.06.2025 16:15] Paper image links file exists (./assets/img_data/2505.23115.json), skip HTML parsing.
[06.06.2025 16:15] Success.
[06.06.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2506.04996.
[06.06.2025 16:15] Extra JSON file exists (./assets/json/2506.04996.json), skip PDF parsing.
[06.06.2025 16:15] Paper image links file exists (./assets/img_data/2506.04996.json), skip HTML parsing.
[06.06.2025 16:15] Success.
[06.06.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2506.03643.
[06.06.2025 16:15] Extra JSON file exists (./assets/json/2506.03643.json), skip PDF parsing.
[06.06.2025 16:15] Paper image links file exists (./assets/img_data/2506.03643.json), skip HTML parsing.
[06.06.2025 16:15] Success.
[06.06.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2506.03238.
[06.06.2025 16:15] Extra JSON file exists (./assets/json/2506.03238.json), skip PDF parsing.
[06.06.2025 16:15] Paper image links file exists (./assets/img_data/2506.03238.json), skip HTML parsing.
[06.06.2025 16:15] Success.
[06.06.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2506.02444.
[06.06.2025 16:15] Extra JSON file exists (./assets/json/2506.02444.json), skip PDF parsing.
[06.06.2025 16:15] Paper image links file exists (./assets/img_data/2506.02444.json), skip HTML parsing.
[06.06.2025 16:15] Success.
[06.06.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2506.00981.
[06.06.2025 16:15] Extra JSON file exists (./assets/json/2506.00981.json), skip PDF parsing.
[06.06.2025 16:15] Paper image links file exists (./assets/img_data/2506.00981.json), skip HTML parsing.
[06.06.2025 16:15] Success.
[06.06.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2506.04462.
[06.06.2025 16:15] Extra JSON file exists (./assets/json/2506.04462.json), skip PDF parsing.
[06.06.2025 16:15] Paper image links file exists (./assets/img_data/2506.04462.json), skip HTML parsing.
[06.06.2025 16:15] Success.
[06.06.2025 16:15] Enriching papers with extra data.
[06.06.2025 16:15] ********************************************************************************
[06.06.2025 16:15] Abstract 0. ComfyUI-Copilot uses a large language model and multi-agent system to enhance the usability and efficiency of the AI-driven art creation platform ComfyUI.  					AI-generated summary 				 We introduce ComfyUI-Copilot, a large language model-powered plugin designed to enhance the usability and efficie...
[06.06.2025 16:15] ********************************************************************************
[06.06.2025 16:15] Abstract 1. SeedVR2 is a one-step diffusion-based video restoration model that uses adaptive window attention and feature matching loss to achieve high visual quality with reduced computational cost compared to existing methods.  					AI-generated summary 				 Recent advances in diffusion-based video restoratio...
[06.06.2025 16:15] ********************************************************************************
[06.06.2025 16:15] Abstract 2. Spatial referring is a fundamental capability of embodied robots to interact with the 3D physical world. However, even with the powerful pretrained vision language models (VLMs), recent approaches are still not qualified to accurately understand the complex 3D scenes and dynamically reason about the...
[06.06.2025 16:15] ********************************************************************************
[06.06.2025 16:15] Abstract 3. A new framework enhances video world models' long-term consistency by integrating a geometry-grounded long-term spatial memory mechanism.  					AI-generated summary 				 Emerging world models autoregressively generate video frames in response to actions, such as camera movements and text prompts, am...
[06.06.2025 16:15] ********************************************************************************
[06.06.2025 16:15] Abstract 4. Transformer models struggle with long-context inference due to their quadratic time and linear memory complexity. Recurrent Memory Transformers (RMTs) offer a solution by reducing the asymptotic cost to linear time and constant memory usage. However, their memory update mechanism leads to sequential...
[06.06.2025 16:15] ********************************************************************************
[06.06.2025 16:15] Abstract 5. Surfer-H, paired with Holo1, an open-weight collection of Vision-Language Models, achieves top performance in web navigation tasks with high cost-efficiency.  					AI-generated summary 				 We present Surfer-H, a cost-efficient web agent that integrates Vision-Language Models (VLM) to perform user-d...
[06.06.2025 16:15] ********************************************************************************
[06.06.2025 16:15] Abstract 6. Recent advancements in text-to-video (T2V) diffusion models have enabled high-fidelity and realistic video synthesis. However, current T2V models often struggle to generate physically plausible content due to their limited inherent ability to accurately understand physics. We found that while the re...
[06.06.2025 16:15] ********************************************************************************
[06.06.2025 16:15] Abstract 7. In this work, we introduce the Qwen3 Embedding series, a significant advancement over its predecessor, the GTE-Qwen series, in text embedding and reranking capabilities, built upon the Qwen3 foundation models. Leveraging the Qwen3 LLMs' robust capabilities in multilingual text understanding and gene...
[06.06.2025 16:15] ********************************************************************************
[06.06.2025 16:15] Abstract 8. Despite progress in video understanding, current MLLMs struggle with counting tasks. Existing benchmarks are limited by short videos, close-set queries, lack of clue annotations, and weak multimodal coverage. In this paper, we introduce CG-AV-Counting, a manually-annotated clue-grounded counting ben...
[06.06.2025 16:15] ********************************************************************************
[06.06.2025 16:15] Abstract 9. This paper presents a novel framework for aligning learnable latent spaces to arbitrary target distributions by leveraging flow-based generative models as priors. Our method first pretrains a flow model on the target features to capture the underlying distribution. This fixed flow model subsequently...
[06.06.2025 16:15] ********************************************************************************
[06.06.2025 16:15] Abstract 10. The Common Pile v0.1 dataset, an 8-terabyte collection of openly licensed text, is used to train competitive 7 billion parameter LLMs.  					AI-generated summary 				 Large language models (LLMs) are typically trained on enormous quantities of unlicensed text, a practice that has led to scrutiny due...
[06.06.2025 16:15] ********************************************************************************
[06.06.2025 16:15] Abstract 11. Spatial cognition is essential for human intelligence, enabling problem-solving through visual simulations rather than solely relying on verbal reasoning. However, existing AI benchmarks primarily assess verbal reasoning, neglecting the complexities of non-verbal, multi-step visual simulation. We in...
[06.06.2025 16:15] ********************************************************************************
[06.06.2025 16:15] Abstract 12. StreamBP, a memory-efficient and exact backpropagation method, decomposes the chain rule to reduce memory costs, enabling longer sequence lengths and faster training speeds for language models compared to gradient checkpointing.  					AI-generated summary 				 Training language models on long sequen...
[06.06.2025 16:15] ********************************************************************************
[06.06.2025 16:15] Abstract 13. Inference-time hyper-scaling with Dynamic Memory Sparsification in Transformer LLMs allows for increased token generation within the same compute budget by compressing the key-value cache, thereby enhancing accuracy.  					AI-generated summary 				 Inference-time scaling trades efficiency for increa...
[06.06.2025 16:15] ********************************************************************************
[06.06.2025 16:15] Abstract 14. Multimodal Large Language Models (MLLMs) are commonly derived by extending pre-trained Large Language Models (LLMs) with visual capabilities. In this work, we investigate how MLLMs process visual inputs by analyzing their attention mechanisms. We reveal a surprising sparsity phenomenon: only a small...
[06.06.2025 16:15] ********************************************************************************
[06.06.2025 16:15] Abstract 15. VideoMathQA evaluates models' ability to perform temporally extended cross-modal reasoning across various mathematical domains in video settings, addressing direct problem solving, conceptual transfer, and deep instructional comprehension.  					AI-generated summary 				 Mathematical reasoning in re...
[06.06.2025 16:15] ********************************************************************************
[06.06.2025 16:15] Abstract 16. Chain-of-Thought (CoT) has widely enhanced mathematical reasoning in Large Language Models (LLMs), but it still remains challenging for extending it to multimodal domains. Existing works either adopt a similar textual reasoning for image input, or seek to interleave visual signals into mathematical ...
[06.06.2025 16:15] ********************************************************************************
[06.06.2025 16:15] Abstract 17. The emergence of multimodal large language models (MLLMs) has driven breakthroughs in egocentric vision applications. These applications necessitate persistent, context-aware understanding of objects, as users interact with tools in dynamic and cluttered environments. However, existing embodied benc...
[06.06.2025 16:15] ********************************************************************************
[06.06.2025 16:15] Abstract 18. Depth maps are widely used in feed-forward 3D Gaussian Splatting (3DGS) pipelines by unprojecting them into 3D point clouds for novel view synthesis. This approach offers advantages such as efficient training, the use of known camera poses, and accurate geometry estimation. However, depth discontinu...
[06.06.2025 16:15] ********************************************************************************
[06.06.2025 16:15] Abstract 19. FlexPainter, a novel texture generation pipeline, uses a shared conditional embedding space to enable flexible multi-modal guidance, ensuring high-quality and consistent texture map generation using image diffusion priors and a 3D-aware model.  					AI-generated summary 				 Texture map production i...
[06.06.2025 16:15] ********************************************************************************
[06.06.2025 16:15] Abstract 20. Learning Language-Image alignment with a Fixed Text encoder (LIFT) using pre-trained large language models effectively guides visual representation learning, outperforming joint training methods like CLIP in compositional understanding and long captions.  					AI-generated summary 				 Currently, th...
[06.06.2025 16:15] ********************************************************************************
[06.06.2025 16:15] Abstract 21. A novel watermarking technique, Lexical Bias Watermarking, enhances the security of autoregressive image generation models by embedding watermarks into token selection, demonstrating superior resistance to regeneration attacks.  					AI-generated summary 				 Autoregressive (AR) image generation mod...
[06.06.2025 16:15] ********************************************************************************
[06.06.2025 16:15] Abstract 22. Reasoning models represented by the Deepseek-R1-Distill series have been widely adopted by the open-source community due to their strong performance in mathematics, science, programming, and other domains. However, our study reveals that their benchmark evaluation results are subject to significant ...
[06.06.2025 16:15] ********************************************************************************
[06.06.2025 16:15] Abstract 23. A novel 4D representation, FreeTimeGS, enhances the modeling of dynamic 3D scenes by enabling Gaussian primitives to appear at arbitrary times and locations, improving rendering quality compared to existing methods.  					AI-generated summary 				 This paper addresses the challenge of reconstructing...
[06.06.2025 16:15] ********************************************************************************
[06.06.2025 16:15] Abstract 24. We introduce MedAgentGYM, the first publicly available training environment designed to enhance coding-based medical reasoning capabilities in large language model (LLM) agents. MedAgentGYM comprises 72,413 task instances across 129 categories derived from authentic real-world biomedical scenarios. ...
[06.06.2025 16:15] ********************************************************************************
[06.06.2025 16:15] Abstract 25. General object composition (GOC) aims to seamlessly integrate a target object into a background scene with desired geometric properties, while simultaneously preserving its fine-grained appearance details. Recent approaches derive semantic embeddings and integrate them into advanced diffusion models...
[06.06.2025 16:15] ********************************************************************************
[06.06.2025 16:15] Abstract 26. Search Arena is a large-scale human-preference dataset that analyzes user interactions with search-augmented language models, revealing insights into citation influence and source credibility.  					AI-generated summary 				 Search-augmented language models combine web search with Large Language Mod...
[06.06.2025 16:15] ********************************************************************************
[06.06.2025 16:15] Abstract 27. We introduce Rectified Point Flow, a unified parameterization that formulates pairwise point cloud registration and multi-part shape assembly as a single conditional generative problem. Given unposed point clouds, our method learns a continuous point-wise velocity field that transports noisy points ...
[06.06.2025 16:15] ********************************************************************************
[06.06.2025 16:15] Abstract 28. A framework called Micro-Act addresses Knowledge Conflicts in Retrieval-Augmented Generation by adaptively decomposing knowledge sources, leading to improved QA accuracy compared to existing methods.  					AI-generated summary 				 Retrieval-Augmented Generation (RAG) systems commonly suffer from Kn...
[06.06.2025 16:15] ********************************************************************************
[06.06.2025 16:15] Abstract 29. Synthesizing high-quality dynamic medical videos remains a significant challenge due to the need for modeling both spatial consistency and temporal dynamics. Existing Transformer-based approaches face critical limitations, including insufficient channel interactions, high computational complexity fr...
[06.06.2025 16:15] ********************************************************************************
[06.06.2025 16:15] Abstract 30. Scaling laws are derived for CLIP and MaMMUT to compare their performance and sample efficiency across different scales and datasets.  					AI-generated summary 				 In studies of transferable learning, scaling laws are obtained for various important foundation models to predict their properties and...
[06.06.2025 16:15] ********************************************************************************
[06.06.2025 16:15] Abstract 31. RobustSplat addresses artifacts in 3D Gaussian Splatting caused by transient objects through delayed Gaussian growth and scale-cascaded mask bootstrapping.  					AI-generated summary 				 3D Gaussian Splatting (3DGS) has gained significant attention for its real-time, photo-realistic rendering in no...
[06.06.2025 16:15] ********************************************************************************
[06.06.2025 16:15] Abstract 32. SkyReels-Audio is a unified framework using pretrained video diffusion transformers for generating high-fidelity and coherent audio-conditioned talking portrait videos, supported by a hybrid curriculum learning strategy and advanced loss mechanisms.  					AI-generated summary 				 The generation and...
[06.06.2025 16:15] ********************************************************************************
[06.06.2025 16:15] Abstract 33. As the era of autonomous agents making decisions on behalf of users unfolds, ensuring contextual integrity (CI) -- what is the appropriate information to share while carrying out a certain task -- becomes a central question to the field. We posit that CI demands a form of reasoning where the agent n...
[06.06.2025 16:15] ********************************************************************************
[06.06.2025 16:15] Abstract 34. MARBLE utilizes material embeddings in CLIP-space to control pre-trained text-to-image models for blending and recomposing material properties in images with parametric control over attributes.  					AI-generated summary 				 Editing materials of objects in images based on exemplar images is an acti...
[06.06.2025 16:15] ********************************************************************************
[06.06.2025 16:15] Abstract 35. BEVCALIB model uses bird's-eye view features for accurate LiDAR-camera calibration from raw data, demonstrating superior performance under various noise conditions.  					AI-generated summary 				 Accurate LiDAR-camera calibration is fundamental to fusing multi-modal perception in autonomous driving...
[06.06.2025 16:15] ********************************************************************************
[06.06.2025 16:15] Abstract 36. Accurately predicting 3D occupancy grids from visual inputs is critical for autonomous driving, but current discriminative methods struggle with noisy data, incomplete observations, and the complex structures inherent in 3D scenes. In this work, we reframe 3D occupancy prediction as a generative mod...
[06.06.2025 16:15] ********************************************************************************
[06.06.2025 16:15] Abstract 37. PATS is a novel temporal sampling method that enhances video analysis of athletic skills by ensuring complete movement patterns are captured, outperforming existing methods across various domains.  					AI-generated summary 				 Automated sports skill assessment requires capturing fundamental moveme...
[06.06.2025 16:15] ********************************************************************************
[06.06.2025 16:15] Abstract 38. Most existing vision encoders map images into a fixed-length sequence of tokens, overlooking the fact that different images contain varying amounts of information. For example, a visually complex image (e.g., a cluttered room) inherently carries more information and thus deserves more tokens than a ...
[06.06.2025 16:15] ********************************************************************************
[06.06.2025 16:15] Abstract 39. OminiAbnorm-CT, a model for automated interpretation of CT images, outperforms existing methods in localizing and describing abnormalities across different body regions using text queries and visual prompts.  					AI-generated summary 				 Automated interpretation of CT images-particularly localizin...
[06.06.2025 16:15] ********************************************************************************
[06.06.2025 16:15] Abstract 40. A framework combining visual priors and dynamic constraints within a synchronized diffusion process generates HOI video and motion simultaneously, enhancing video-motion consistency and generalization.  					AI-generated summary 				 Hand-Object Interaction (HOI) generation has significant applicati...
[06.06.2025 16:15] ********************************************************************************
[06.06.2025 16:15] Abstract 41. Self-supervised Wav2Vec2 models encode Dutch linguistic features more accurately when pre-trained exclusively on Dutch data, compared to similar amounts of English or multilingual data, as shown by clustering and classification probes, and demonstrated through improved Automatic Speech Recognition p...
[06.06.2025 16:15] ********************************************************************************
[06.06.2025 16:15] Abstract 42. Watermarking techniques in LLMs can degrade truthfulness, safety, and helpfulness, and alignment resampling is proposed to restore alignment while ensuring watermark detectability.  					AI-generated summary 				 Watermarking techniques for large language models (LLMs) can significantly impact outpu...
[06.06.2025 16:15] Read previous papers.
[06.06.2025 16:15] Generating reviews via LLM API.
[06.06.2025 16:15] Using data from previous issue: {"categories": ["#multimodal", "#agents", "#open_source"], "emoji": "üé®", "ru": {"title": "–ò–ò-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –¥–ª—è —Ç–≤–æ—Ä—á–µ—Å—Ç–≤–∞: ComfyUI-Copilot —É–ø—Ä–æ—â–∞–µ—Ç —Å–æ–∑–¥–∞–Ω–∏–µ —Ü–∏—Ñ—Ä–æ–≤–æ–≥–æ –∏—Å–∫—É—Å—Å—Ç–≤–∞", "desc": "ComfyUI-Copilot - —ç—Ç–æ –ø–ª–∞–≥–∏–Ω –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —É–¥–æ–±—Å—Ç–≤–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∏
[06.06.2025 16:15] Using data from previous issue: {"categories": ["#diffusion", "#architecture", "#training", "#video", "#optimization"], "emoji": "üé•", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–∏–¥–µ–æ –∑–∞ –æ–¥–∏–Ω —à–∞–≥", "desc": "SeedVR2 - —ç—Ç–æ –æ–¥–Ω–æ–ø—Ä–æ—Ö–æ–¥–Ω–∞—è –º–æ–¥–µ–ª—å –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –≤–∏–¥–µ–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∏—Ñ—Ñ—É–∑–∏–∏. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –æ–∫–æ–Ω–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –∏ —Ñ—É–Ω–∫
[06.06.2025 16:15] Using data from previous issue: {"categories": ["#optimization", "#benchmark", "#robotics", "#reasoning", "#dataset", "#3d", "#training", "#rl"], "emoji": "ü§ñ", "ru": {"title": "RoboRefer: –ü—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç –¥–ª—è —Ä–æ–±–æ—Ç–æ–≤ –Ω–æ–≤–æ–≥–æ –ø–æ–∫–æ–ª–µ–Ω–∏—è", "desc": "RoboRefer - —ç—Ç–æ –º–æ–¥–µ–ª—å –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –¥–ª—è —Ä–æ–±–æ—Ç–æ–≤, –æ—Å–Ω–æ–≤–∞–Ω–Ω–∞—è –Ω–∞
[06.06.2025 16:15] Using data from previous issue: {"categories": ["#video", "#dataset", "#3d", "#long_context"], "emoji": "üß†", "ru": {"title": "–£–ª—É—á—à–µ–Ω–∏–µ –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–π –ø–∞–º—è—Ç–∏ –≤ –≤–∏–¥–µ–æ-–º–æ–¥–µ–ª—è—Ö –º–∏—Ä–∞", "desc": "–ù–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ —É–ª—É—á—à–∞–µ—Ç –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—É—é —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –≤–∏–¥–µ–æ-–º–æ–¥–µ–ª–µ–π –º–∏—Ä–∞, –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É—è –º–µ—Ö–∞–Ω–∏–∑–º –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–π –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–π –ø–∞–º—è—Ç–∏, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ 
[06.06.2025 16:15] Using data from previous issue: {"categories": ["#architecture", "#inference", "#optimization", "#training", "#long_context"], "emoji": "‚ö°", "ru": {"title": "–£—Å–∫–æ—Ä–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –≤ —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω—ã—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –¥–ª—è —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Ç—Ä–∞–Ω—Å
[06.06.2025 16:15] Using data from previous issue: {"categories": ["#synthetic", "#dataset", "#open_source", "#agents", "#data", "#benchmark"], "emoji": "üèÑ", "ru": {"title": "Surfer-H –∏ Holo1: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –≤–µ–±-–Ω–∞–≤–∏–≥–∞—Ü–∏—è —Å –ø–æ–º–æ—â—å—é –ò–ò", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Surfer-H - —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –≤–µ–±-–∞–≥–µ–Ω—Ç–∞, –∏—Å–ø–æ–ª—å–∑—É—é—â–µ–≥–æ –º–æ–¥–µ–ª–∏ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è –∏ –æ–±—Ä–∞–±–æ—Ç–∫
[06.06.2025 16:15] Using data from previous issue: {"categories": ["#optimization", "#benchmark", "#training", "#video", "#diffusion"], "emoji": "üé•", "ru": {"title": "–§–∏–∑–∏—á–µ—Å–∫–∏ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ–µ –≤–∏–¥–µ–æ –∏–∑ —Ç–µ–∫—Å—Ç–∞: VideoREPA —É–ª—É—á—à–∞–µ—Ç –ø–æ–Ω–∏–º–∞–Ω–∏–µ —Ñ–∏–∑–∏–∫–∏ –≤ –º–æ–¥–µ–ª—è—Ö T2V", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ VideoREPA –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ñ–∏–∑–∏—á–µ—Å–∫–æ–π –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏ –≤–∏
[06.06.2025 16:15] Using data from previous issue: {"categories": ["#data", "#benchmark", "#dataset", "#multilingual", "#open_source", "#training", "#small_models", "#low_resource"], "emoji": "üîç", "ru": {"title": "Qwen3 Embedding: –ù–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Å–µ—Ä–∏—è –º–æ–¥–µ–ª–µ–π Qwen3 Embedding, —É–ª—É—á—à–∞
[06.06.2025 16:15] Using data from previous issue: {"categories": ["#optimization", "#benchmark", "#multimodal", "#reasoning", "#dataset", "#long_context", "#training", "#video", "#rl"], "emoji": "üßÆ", "ru": {"title": "–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –ø–æ–¥—Å—á–µ—Ç –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –ò–ò", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ CG-AV-Counting –¥–ª—è –∑
[06.06.2025 16:15] Using data from previous issue: {"categories": ["#optimization", "#alignment", "#training", "#cv", "#math", "#diffusion"], "emoji": "üîÑ", "ru": {"title": "–í—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –ª–∞—Ç–µ–Ω—Ç–Ω—ã—Ö –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤ —Å –ø–æ–º–æ—â—å—é –ø–æ—Ç–æ–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—é –æ–±—É—á–∞–µ–º—ã—Ö –ª–∞—Ç–µ–Ω—Ç–Ω—ã—Ö –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤ —Å –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã–º–∏ —Ü–µ–ª–µ–≤—ã–º–∏
[06.06.2025 16:15] Using data from previous issue: {"categories": ["#dataset", "#ethics", "#open_source", "#data"], "emoji": "üìö", "ru": {"title": "–û—Ç–∫—Ä—ã—Ç—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —ç—Ç–∏—á–Ω–æ–≥–æ –ò–ò", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö Common Pile v0.1 - 8-—Ç–µ—Ä–∞–±–∞–π—Ç–Ω—É—é –∫–æ–ª–ª–µ–∫—Ü–∏—é —Ç–µ–∫—Å—Ç–æ–≤ —Å –æ—Ç–∫—Ä—ã—Ç–æ–π –ª–∏—Ü–µ–Ω–∑–∏–µ–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ù–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–∏—Ö –¥–∞–Ω–Ω
[06.06.2025 16:15] Using data from previous issue: {"categories": ["#multimodal", "#3d", "#reasoning", "#benchmark"], "emoji": "üß†", "ru": {"title": "STARE: –ù–æ–≤—ã–π —Ä—É–±–µ–∂ –≤ –æ—Ü–µ–Ω–∫–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ –ò–ò", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ STARE –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –∑–∞–¥–∞—á–∞—Ö –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è –∏ –≤–∏–∑—É–∞–ª—å
[06.06.2025 16:15] Using data from previous issue: {"categories": ["#training", "#long_context", "#optimization"], "emoji": "üöÄ", "ru": {"title": "StreamBP: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –æ–±—É—á–µ–Ω–∏–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "StreamBP - —ç—Ç–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥ –æ–±—Ä–∞—Ç–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è –æ—à–∏–±–∫–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –û–Ω —Ä–∞–∑–ª–∞–≥–∞–µ—Ç –ø—Ä–∞–≤–∏–ª–æ —Ü–µ–ø–æ—á–∫–∏ –≤–¥–æ–ª—å –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω
[06.06.2025 16:15] Using data from previous issue: {"categories": ["#optimization", "#training", "#reasoning", "#inference"], "emoji": "üß†", "ru": {"title": "–ë–æ–ª—å—à–µ —Ç–æ–∫–µ–Ω–æ–≤, –≤—ã—à–µ —Ç–æ—á–Ω–æ—Å—Ç—å: —Å–∂–∞—Ç–∏–µ –ø–∞–º—è—Ç–∏ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –≤—ã–≤–æ–¥–∞ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞–∑—Ä–µ–∂–∏–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏ (Dynamic Memory Sparsification, DMS) 
[06.06.2025 16:15] Using data from previous issue: {"categories": ["#optimization", "#benchmark", "#multimodal", "#architecture", "#open_source", "#inference"], "emoji": "üîç", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –∑—Ä–µ–Ω–∏–µ: –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –ò–ò", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (ML
[06.06.2025 16:15] Using data from previous issue: {"categories": ["#math", "#multimodal", "#benchmark", "#transfer_learning", "#reasoning", "#video"], "emoji": "üßÆ", "ru": {"title": "VideoMathQA: –ù–æ–≤—ã–π —Ä—É–±–µ–∂ –≤ –æ—Ü–µ–Ω–∫–µ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –ò–ò –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–∏–¥–µ–æ", "desc": "VideoMathQA - —ç—Ç–æ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –≤—ã–ø–æ–ª–Ω—è—Ç—å –∫—Ä–æ
[06.06.2025 16:15] Using data from previous issue: {"categories": ["#training", "#multimodal", "#dataset", "#reasoning", "#math", "#games"], "emoji": "üßÆ", "ru": {"title": "–í–∏–∑—É–∞–ª—å–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –≤ –º–∞—Ç–µ–º–∞—Ç–∏–∫–µ: –Ω–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å —Å MINT-CoT", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç MINT-CoT - –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≤–∏–∑—É–∞–ª—å–Ω–æ–º—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é –≤ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á–∞—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤
[06.06.2025 16:15] Using data from previous issue: {"categories": ["#optimization", "#cv", "#multimodal", "#interpretability", "#benchmark", "#games"], "emoji": "üëÅÔ∏è", "ru": {"title": "EOC-Bench: –ù–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –ò–ò –≤ —ç–≥–æ—Ü–µ–Ω—Ç—Ä–∏—á–µ—Å–∫–æ–º –∑—Ä–µ–Ω–∏–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ EOC-Bench –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è 
[06.06.2025 16:15] Using data from previous issue: {"categories": ["#optimization", "#3d", "#architecture"], "emoji": "üîç", "ru": {"title": "–£–ª—É—á—à–µ–Ω–∏–µ 3D-—Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ —Å –ø–æ–º–æ—â—å—é —É–º–Ω–æ–π —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏ –≥—Ä–∞–Ω–∏—Ü –æ–±—ä–µ–∫—Ç–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ 3D-—Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –≤ —Å–∏—Å—Ç–µ–º–∞—Ö 3D Gaussian Splatting. –ê–≤—Ç–æ—Ä—ã 
[06.06.2025 16:15] Using data from previous issue: {"categories": ["#multimodal", "#3d", "#cv", "#diffusion"], "emoji": "üé®", "ru": {"title": "FlexPainter: –≥–∏–±–∫–∞—è –∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç—É—Ä —Å –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–º —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º", "desc": "FlexPainter - —ç—Ç–æ –Ω–æ–≤—ã–π –∫–æ–Ω–≤–µ–π–µ—Ä –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç—É—Ä, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –æ–±—â–µ–µ —É—Å–ª–æ–≤–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –≤–ª–æ–∂–µ–Ω–∏–π –¥–ª—è –≥–∏–±–∫–æ–≥–æ 
[06.06.2025 16:15] Using data from previous issue: {"categories": ["#benchmark", "#multimodal", "#long_context", "#alignment", "#cv"], "emoji": "üîç", "ru": {"title": "–§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —è–∑—ã–∫–æ–≤–æ–π —ç–Ω–∫–æ–¥–µ—Ä —É–ª—É—á—à–∞–µ—Ç –≤–∏–∑—É–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ LIFT –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ
[06.06.2025 16:15] Using data from previous issue: {"categories": ["#cv", "#security", "#video"], "emoji": "üîê", "ru": {"title": "–ó–∞—â–∏—Ç–∞ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é –ª–µ–∫—Å–∏—á–µ—Å–∫–∏—Ö –≤–æ–¥—è–Ω—ã—Ö –∑–Ω–∞–∫–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é —Ç–µ—Ö–Ω–∏–∫—É –≤–æ–¥—è–Ω—ã—Ö –∑–Ω–∞–∫–æ–≤ –¥–ª—è –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π - Lexical Bias Wat
[06.06.2025 16:15] Using data from previous issue: {"categories": ["#benchmark", "#reasoning", "#open_source", "#training", "#math"], "emoji": "üé¢", "ru": {"title": "–ù–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –æ—Ü–µ–Ω–æ–∫: –≤—ã–∑–æ–≤ –¥–ª—è –±–µ–Ω—á–º–∞—Ä–∫–∏–Ω–≥–∞ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π —Å–µ—Ä–∏–∏ Deepseek-R1-Distill –ø–æ–¥–≤–µ—Ä–∂–µ–Ω—ã –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–º –∫–æ–ª–µ–±–∞–Ω
[06.06.2025 16:15] Using data from previous issue: {"categories": ["#3d"], "emoji": "üîÑ", "ru": {"title": "–°–≤–æ–±–æ–¥–Ω–æ–µ –≤—Ä–µ–º—è –∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –¥–ª—è –≥–∞—É—Å—Å–æ–≤—ã—Ö –ø—Ä–∏–º–∏—Ç–∏–≤–æ–≤ –≤ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö 3D-—Å—Ü–µ–Ω–∞—Ö", "desc": "FreeTimeGS - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ 4D-–ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –¥–ª—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö 3D-—Å—Ü–µ–Ω. –û–Ω –ø–æ–∑–≤–æ–ª—è–µ—Ç –≥–∞—É—Å—Å–æ–≤—ã–º –ø—Ä–∏–º–∏—Ç–∏–≤–∞–º –ø–æ—è–≤–ª—è—Ç—å—Å—è –≤ –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –∏
[06.06.2025 16:15] Using data from previous issue: {"categories": ["#benchmark", "#reasoning", "#dataset", "#open_source", "#training", "#rl"], "emoji": "ü©∫", "ru": {"title": "MedAgentGYM: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –æ–±—É—á–µ–Ω–∏–∏ –ò–ò –¥–ª—è –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–≥–æ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è", "desc": "MedAgentGYM - —ç—Ç–æ –Ω–æ–≤–∞—è —Å—Ä–µ–¥–∞ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –Ω–∞–≤—ã–∫–æ–≤ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è —É –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ 
[06.06.2025 16:15] Using data from previous issue: {"categories": ["#cv", "#multimodal", "#benchmark", "#diffusion"], "emoji": "üé®", "ru": {"title": "–¢–æ—á–Ω–æ–µ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≥–µ–æ–º–µ—Ç—Ä–∏–∏ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–µ—Ç–∞–ª–µ–π –≤ –∫–æ–º–ø–æ–∑–∏—Ü–∏–∏ –æ–±—ä–µ–∫—Ç–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å DGAD (Disentangled Geometry-editable and Appearance-preserving Diffusion) –¥–ª—è –∫–æ–º–ø–æ
[06.06.2025 16:15] Querying the API.
[06.06.2025 16:15] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Search Arena is a large-scale human-preference dataset that analyzes user interactions with search-augmented language models, revealing insights into citation influence and source credibility.  					AI-generated summary 				 Search-augmented language models combine web search with Large Language Models (LLMs) to improve response groundedness and freshness. However, analyzing these systems remains challenging: existing datasets are limited in scale and narrow in scope, often constrained to static, single-turn, fact-checking questions. In this work, we introduce Search Arena, a crowd-sourced, large-scale, human-preference dataset of over 24,000 paired multi-turn user interactions with search-augmented LLMs. The dataset spans diverse intents and languages, and contains full system traces with around 12,000 human preference votes. Our analysis reveals that user preferences are influenced by the number of citations, even when the cited content does not directly support the attributed claims, uncovering a gap between perceived and actual credibility. Furthermore, user preferences vary across cited sources, revealing that community-driven platforms are generally preferred and static encyclopedic sources are not always appropriate and reliable. To assess performance across different settings, we conduct cross-arena analyses by testing search-augmented LLMs in a general-purpose chat environment and conventional LLMs in search-intensive settings. We find that web search does not degrade and may even improve performance in non-search settings; however, the quality in search settings is significantly affected if solely relying on the model's parametric knowledge. We open-sourced the dataset to support future research in this direction. Our dataset and code are available at: https://github.com/lmarena/search-arena.
[06.06.2025 16:15] Response: {
  "desc": "Search Arena - —ç—Ç–æ –º–∞—Å—à—Ç–∞–±–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –æ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –ø—Ä–∏ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–∏ —Å —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏, –¥–æ–ø–æ–ª–Ω–µ–Ω–Ω—ã–º–∏ –ø–æ–∏—Å–∫–æ–º. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ü–∏—Ç–∞—Ç –≤–ª–∏—è–µ—Ç –Ω–∞ –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏ –æ—Ç–≤–µ—Ç–æ–≤, –¥–∞–∂–µ –µ—Å–ª–∏ —Ü–∏—Ç–∏—Ä—É–µ–º—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–∞–ø—Ä—è–º—É—é –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è. –ê–Ω–∞–ª–∏–∑ –≤—ã—è–≤–∏–ª –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –∫ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, –ø—Ä–∏—á–µ–º –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞ —Å–æ–æ–±—â–µ—Å—Ç–≤–∞—Ö, –æ–∫–∞–∑–∞–ª–∏—Å—å –±–æ–ª–µ–µ –ø–æ–ø—É–ª—è—Ä–Ω—ã–º–∏. –ù–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –æ—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –Ω–∞–º–µ—Ä–µ–Ω–∏—è –∏ —è–∑—ã–∫–∏, —Å–æ–¥–µ—Ä–∂–∏—Ç –ø–æ–ª–Ω—ã–µ —Å–∏—Å—Ç–µ–º–Ω—ã–µ —Ç—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∏ –∏ –æ–∫–æ–ª–æ 12 000 –æ—Ü–µ–Ω–æ–∫ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π.",
  "emoji": "üîç",
  "title": "–ì–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π —Å –ò–ò-—Å–∏—Å—Ç–µ–º–∞–º–∏, –¥–æ–ø–æ–ª–Ω–µ–Ω–Ω—ã–º–∏ –ø–æ–∏—Å–∫–æ–º"
}
[06.06.2025 16:15] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Search Arena is a large-scale human-preference dataset that analyzes user interactions with search-augmented language models, revealing insights into citation influence and source credibility.  					AI-generated summary 				 Search-augmented language models combine web search with Large Language Models (LLMs) to improve response groundedness and freshness. However, analyzing these systems remains challenging: existing datasets are limited in scale and narrow in scope, often constrained to static, single-turn, fact-checking questions. In this work, we introduce Search Arena, a crowd-sourced, large-scale, human-preference dataset of over 24,000 paired multi-turn user interactions with search-augmented LLMs. The dataset spans diverse intents and languages, and contains full system traces with around 12,000 human preference votes. Our analysis reveals that user preferences are influenced by the number of citations, even when the cited content does not directly support the attributed claims, uncovering a gap between perceived and actual credibility. Furthermore, user preferences vary across cited sources, revealing that community-driven platforms are generally preferred and static encyclopedic sources are not always appropriate and reliable. To assess performance across different settings, we conduct cross-arena analyses by testing search-augmented LLMs in a general-purpose chat environment and conventional LLMs in search-intensive settings. We find that web search does not degrade and may even improve performance in non-search settings; however, the quality in search settings is significantly affected if solely relying on the model's parametric knowledge. We open-sourced the dataset to support future research in this direction. Our dataset and code are available at: https://github.com/lmarena/search-arena."

[06.06.2025 16:15] Response: ```python
['DATASET', 'DATA', 'MULTILINGUAL']
```
[06.06.2025 16:15] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Search Arena is a large-scale human-preference dataset that analyzes user interactions with search-augmented language models, revealing insights into citation influence and source credibility.  					AI-generated summary 				 Search-augmented language models combine web search with Large Language Models (LLMs) to improve response groundedness and freshness. However, analyzing these systems remains challenging: existing datasets are limited in scale and narrow in scope, often constrained to static, single-turn, fact-checking questions. In this work, we introduce Search Arena, a crowd-sourced, large-scale, human-preference dataset of over 24,000 paired multi-turn user interactions with search-augmented LLMs. The dataset spans diverse intents and languages, and contains full system traces with around 12,000 human preference votes. Our analysis reveals that user preferences are influenced by the number of citations, even when the cited content does not directly support the attributed claims, uncovering a gap between perceived and actual credibility. Furthermore, user preferences vary across cited sources, revealing that community-driven platforms are generally preferred and static encyclopedic sources are not always appropriate and reliable. To assess performance across different settings, we conduct cross-arena analyses by testing search-augmented LLMs in a general-purpose chat environment and conventional LLMs in search-intensive settings. We find that web search does not degrade and may even improve performance in non-search settings; however, the quality in search settings is significantly affected if solely relying on the model's parametric knowledge. We open-sourced the dataset to support future research in this direction. Our dataset and code are available at: https://github.com/lmarena/search-arena."

[06.06.2025 16:15] Response: ```python
['ALIGNMENT', 'OPEN_SOURCE']
```
[06.06.2025 16:15] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Search Arena is a comprehensive dataset designed to study user interactions with search-augmented language models (LLMs). It includes over 24,000 multi-turn interactions and 12,000 human preference votes, providing insights into how users perceive citation influence and source credibility. The findings indicate that user preferences are swayed by the number of citations, regardless of their relevance, highlighting a disconnect between perceived and actual credibility. Additionally, the dataset reveals that users favor community-driven sources over static encyclopedic ones, and it demonstrates that web search can enhance performance in various contexts, challenging the notion that LLMs should rely solely on their internal knowledge.","title":"Unveiling User Preferences in Search-Augmented Language Models"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Search Arena is a comprehensive dataset designed to study user interactions with search-augmented language models (LLMs). It includes over 24,000 multi-turn interactions and 12,000 human preference votes, providing insights into how users perceive citation influence and source credibility. The findings indicate that user preferences are swayed by the number of citations, regardless of their relevance, highlighting a disconnect between perceived and actual credibility. Additionally, the dataset reveals that users favor community-driven sources over static encyclopedic ones, and it demonstrates that web search can enhance performance in various contexts, challenging the notion that LLMs should rely solely on their internal knowledge.', title='Unveiling User Preferences in Search-Augmented Language Models'))
[06.06.2025 16:15] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Search ArenaÊòØ‰∏Ä‰∏™Â§ßËßÑÊ®°ÁöÑ‰∫∫Á±ªÂÅèÂ•ΩÊï∞ÊçÆÈõÜÔºåÂàÜÊûêÁî®Êà∑‰∏éÊêúÁ¥¢Â¢ûÂº∫ËØ≠Ë®ÄÊ®°ÂûãÁöÑ‰∫íÂä®ÔºåÊè≠Á§∫‰∫ÜÂºïÁî®ÂΩ±ÂìçÂíåÊù•Ê∫êÂèØ‰ø°Â∫¶ÁöÑËßÅËß£„ÄÇËØ•Êï∞ÊçÆÈõÜÂåÖÂê´Ë∂ÖËøá24,000ÂØπÂ§öËΩÆÁî®Êà∑‰∫§‰∫íÔºåÊ∂µÁõñÂ§öÁßçÊÑèÂõæÂíåËØ≠Ë®ÄÔºåÂπ∂Êèê‰æõ‰∫ÜÁ∫¶12,000‰∏™Áî®Êà∑ÂÅèÂ•ΩÊäïÁ•®ÁöÑÂÆåÊï¥Á≥ªÁªüËÆ∞ÂΩï„ÄÇÂàÜÊûêÁªìÊûúÊòæÁ§∫ÔºåÁî®Êà∑ÂÅèÂ•ΩÂèóÂà∞ÂºïÁî®Êï∞ÈáèÁöÑÂΩ±ÂìçÔºåÂç≥‰ΩøË¢´ÂºïÁî®ÁöÑÂÜÖÂÆπÂπ∂‰∏çÁõ¥Êé•ÊîØÊåÅÊâÄÂ£∞Áß∞ÁöÑËßÇÁÇπ„ÄÇÊ≠§Â§ñÔºåÁî®Êà∑ÂØπ‰∏çÂêåÂºïÁî®Êù•Ê∫êÁöÑÂÅèÂ•ΩÂ≠òÂú®Â∑ÆÂºÇÔºåË°®ÊòéÁ§æÂå∫È©±Âä®ÁöÑÂπ≥Âè∞ÈÄöÂ∏∏Êõ¥ÂèóÊ¨¢ËøéÔºåËÄåÈùôÊÄÅÁöÑÁôæÁßëÂÖ®‰π¶Êù•Ê∫êÂπ∂‰∏çÊÄªÊòØÂêàÈÄÇÂíåÂèØÈù†„ÄÇ","title":"Êè≠Á§∫ÊêúÁ¥¢Â¢ûÂº∫Ê®°Âûã‰∏≠ÁöÑÁî®Êà∑ÂÅèÂ•Ω‰∏éÂèØ‰ø°Â∫¶"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Search ArenaÊòØ‰∏Ä‰∏™Â§ßËßÑÊ®°ÁöÑ‰∫∫Á±ªÂÅèÂ•ΩÊï∞ÊçÆÈõÜÔºåÂàÜÊûêÁî®Êà∑‰∏éÊêúÁ¥¢Â¢ûÂº∫ËØ≠Ë®ÄÊ®°ÂûãÁöÑ‰∫íÂä®ÔºåÊè≠Á§∫‰∫ÜÂºïÁî®ÂΩ±ÂìçÂíåÊù•Ê∫êÂèØ‰ø°Â∫¶ÁöÑËßÅËß£„ÄÇËØ•Êï∞ÊçÆÈõÜÂåÖÂê´Ë∂ÖËøá24,000ÂØπÂ§öËΩÆÁî®Êà∑‰∫§‰∫íÔºåÊ∂µÁõñÂ§öÁßçÊÑèÂõæÂíåËØ≠Ë®ÄÔºåÂπ∂Êèê‰æõ‰∫ÜÁ∫¶12,000‰∏™Áî®Êà∑ÂÅèÂ•ΩÊäïÁ•®ÁöÑÂÆåÊï¥Á≥ªÁªüËÆ∞ÂΩï„ÄÇÂàÜÊûêÁªìÊûúÊòæÁ§∫ÔºåÁî®Êà∑ÂÅèÂ•ΩÂèóÂà∞ÂºïÁî®Êï∞ÈáèÁöÑÂΩ±ÂìçÔºåÂç≥‰ΩøË¢´ÂºïÁî®ÁöÑÂÜÖÂÆπÂπ∂‰∏çÁõ¥Êé•ÊîØÊåÅÊâÄÂ£∞Áß∞ÁöÑËßÇÁÇπ„ÄÇÊ≠§Â§ñÔºåÁî®Êà∑ÂØπ‰∏çÂêåÂºïÁî®Êù•Ê∫êÁöÑÂÅèÂ•ΩÂ≠òÂú®Â∑ÆÂºÇÔºåË°®ÊòéÁ§æÂå∫È©±Âä®ÁöÑÂπ≥Âè∞ÈÄöÂ∏∏Êõ¥ÂèóÊ¨¢ËøéÔºåËÄåÈùôÊÄÅÁöÑÁôæÁßëÂÖ®‰π¶Êù•Ê∫êÂπ∂‰∏çÊÄªÊòØÂêàÈÄÇÂíåÂèØÈù†„ÄÇ', title='Êè≠Á§∫ÊêúÁ¥¢Â¢ûÂº∫Ê®°Âûã‰∏≠ÁöÑÁî®Êà∑ÂÅèÂ•Ω‰∏éÂèØ‰ø°Â∫¶'))
[06.06.2025 16:15] Using data from previous issue: {"categories": ["#dataset", "#3d", "#benchmark"], "emoji": "üß©", "ru": {"title": "–ï–¥–∏–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏ –æ–±–ª–∞–∫–æ–≤ —Ç–æ—á–µ–∫ –∏ —Å–±–æ—Ä–∫–µ —Ñ–æ—Ä–º", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –º–µ—Ç–æ–¥ Rectified Point Flow, –∫–æ—Ç–æ—Ä—ã–π –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—é –æ–±–ª–∞–∫–æ–≤ —Ç–æ—á–µ–∫ –∏ —Å–±–æ—Ä–∫—É –º–Ω–æ–≥–æ–∫–æ–º–ø–æ–Ω–µ–Ω—Ç–Ω—ã—Ö —Ñ–æ—Ä–º –≤ –µ–¥–∏–Ω—É—é —É—Å–ª–æ–≤–Ω—É—é –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—É—é –∑–∞–¥–∞
[06.06.2025 16:15] Using data from previous issue: {"categories": ["#interpretability", "#rag", "#reasoning", "#benchmark"], "emoji": "üß†", "ru": {"title": "Micro-Act: —É–º–Ω–æ–µ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤ –∑–Ω–∞–Ω–∏–π –≤ RAG-—Å–∏—Å—Ç–µ–º–∞—Ö", "desc": "–§—Ä–µ–π–º–≤–æ—Ä–∫ Micro-Act —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤ –∑–Ω–∞–Ω–∏–π –≤ —Å–∏—Å—Ç–µ–º–∞—Ö –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å –¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ–º –∏–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ (RAG). –û–Ω –∞–¥–∞–ø—Ç–∏–≤–Ω–æ 
[06.06.2025 16:15] Using data from previous issue: {"categories": ["#video", "#architecture", "#open_source", "#optimization", "#training"], "emoji": "üè•", "ru": {"title": "FEAT: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –≤–∏–¥–µ–æ", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω FEAT - –Ω–æ–≤—ã–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –≤–∏–¥–µ–æ. FEAT –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø–æ
[06.06.2025 16:15] Using data from previous issue: {"categories": ["#data", "#optimization", "#benchmark", "#transfer_learning", "#training", "#dataset", "#open_source"], "emoji": "üìä", "ru": {"title": "–ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—ã–µ –∑–∞–∫–æ–Ω—ã –∫–∞–∫ –∫–ª—é—á –∫ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—ã–µ –∑–∞–∫–æ–Ω—ã –¥–ª—è –º–æ–¥–µ–ª–µ–π CLIP –∏ MaMMUT, 
[06.06.2025 16:15] Using data from previous issue: {"categories": ["#3d"], "emoji": "üåü", "ru": {"title": "–£—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤ –≤ 3D-—Å—Ü–µ–Ω–∞—Ö —Å –ø–æ–º–æ—â—å—é —É–º–Ω–æ–≥–æ —Ä–æ—Å—Ç–∞ –≥–∞—É—Å—Å–∏–∞–Ω–æ–≤", "desc": "RobustSplat - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –≤ –æ–±–ª–∞—Å—Ç–∏ 3D-–º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è, –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π –Ω–∞ —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤, –≤—ã–∑–≤–∞–Ω–Ω—ã—Ö –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –æ–±—ä–µ–∫—Ç–∞–º–∏ –≤ —Å—Ü–µ–Ω–∞—Ö. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –æ—Ç–ª–æ
[06.06.2025 16:15] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#multimodal", "#audio", "#diffusion", "#synthetic", "#video"], "emoji": "üé≠", "ru": {"title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ —Å–∏–Ω—Ç–µ–∑–µ –≥–æ–≤–æ—Ä—è—â–∏—Ö –ø–æ—Ä—Ç—Ä–µ—Ç–æ–≤: –æ—Ç –∞—É–¥–∏–æ –∫ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ–º—É –≤–∏–¥–µ–æ", "desc": "SkyReels-Audio - —ç—Ç–æ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –≤–∏–¥
[06.06.2025 16:15] Using data from previous issue: {"categories": ["#synthetic", "#agents", "#benchmark", "#reasoning", "#dataset", "#transfer_learning", "#rl", "#leakage"], "emoji": "üîê", "ru": {"title": "–†–∞–∑—É–º–Ω–æ–µ —Ä–∞—Å–∫—Ä—ã—Ç–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏: –æ–±—É—á–µ–Ω–∏–µ –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–π —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏", "desc": "–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –ø—Ä–æ–±–ª–µ–º–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–π —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏ (CI) –≤ —ç–ø–æ
[06.06.2025 16:15] Querying the API.
[06.06.2025 16:15] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

MARBLE utilizes material embeddings in CLIP-space to control pre-trained text-to-image models for blending and recomposing material properties in images with parametric control over attributes.  					AI-generated summary 				 Editing materials of objects in images based on exemplar images is an active area of research in computer vision and graphics. We propose MARBLE, a method for performing material blending and recomposing fine-grained material properties by finding material embeddings in CLIP-space and using that to control pre-trained text-to-image models. We improve exemplar-based material editing by finding a block in the denoising UNet responsible for material attribution. Given two material exemplar-images, we find directions in the CLIP-space for blending the materials. Further, we can achieve parametric control over fine-grained material attributes such as roughness, metallic, transparency, and glow using a shallow network to predict the direction for the desired material attribute change. We perform qualitative and quantitative analysis to demonstrate the efficacy of our proposed method. We also present the ability of our method to perform multiple edits in a single forward pass and applicability to painting.   Project Page: https://marblecontrol.github.io/
[06.06.2025 16:15] Response: {
  "desc": "MARBLE - —ç—Ç–æ –º–µ—Ç–æ–¥ –¥–ª—è —Å–º–µ—à–∏–≤–∞–Ω–∏—è –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤ –∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è –∏—Ö —Å–≤–æ–π—Å—Ç–≤ –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤—Å—Ç—Ä–∞–∏–≤–∞–Ω–∏–π –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ CLIP. –û–Ω –ø–æ–∑–≤–æ–ª—è–µ—Ç –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ text-to-image –¥–ª—è —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤ –æ–±—ä–µ–∫—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–∏–º–µ—Ä–æ–≤. MARBLE –Ω–∞—Ö–æ–¥–∏—Ç –±–ª–æ–∫ –≤ U-Net, –æ—Ç–≤–µ—á–∞—é—â–∏–π –∑–∞ –∞—Ç—Ä–∏–±—É—Ü–∏—é –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤, –∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–µ–≥–ª—É–±–æ–∫—É—é –Ω–µ–π—Ä–æ–Ω–Ω—É—é —Å–µ—Ç—å –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏—è –∞—Ç—Ä–∏–±—É—Ç–æ–≤. –ú–µ—Ç–æ–¥ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–π –∫–æ–Ω—Ç—Ä–æ–ª—å –Ω–∞–¥ —Ç–∞–∫–∏–º–∏ —Å–≤–æ–π—Å—Ç–≤–∞–º–∏ –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤ –∫–∞–∫ —à–µ—Ä–æ—Ö–æ–≤–∞—Ç–æ—Å—Ç—å, –º–µ—Ç–∞–ª–ª–∏—á–Ω–æ—Å—Ç—å, –ø—Ä–æ–∑—Ä–∞—á–Ω–æ—Å—Ç—å –∏ —Å–≤–µ—á–µ–Ω–∏–µ.",
  "emoji": "üé®",
  "title": "–£–º–Ω–æ–µ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤ –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö —Å –ø–æ–º–æ—â—å—é –ò–ò"
}
[06.06.2025 16:15] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"MARBLE utilizes material embeddings in CLIP-space to control pre-trained text-to-image models for blending and recomposing material properties in images with parametric control over attributes.  					AI-generated summary 				 Editing materials of objects in images based on exemplar images is an active area of research in computer vision and graphics. We propose MARBLE, a method for performing material blending and recomposing fine-grained material properties by finding material embeddings in CLIP-space and using that to control pre-trained text-to-image models. We improve exemplar-based material editing by finding a block in the denoising UNet responsible for material attribution. Given two material exemplar-images, we find directions in the CLIP-space for blending the materials. Further, we can achieve parametric control over fine-grained material attributes such as roughness, metallic, transparency, and glow using a shallow network to predict the direction for the desired material attribute change. We perform qualitative and quantitative analysis to demonstrate the efficacy of our proposed method. We also present the ability of our method to perform multiple edits in a single forward pass and applicability to painting.   Project Page: https://marblecontrol.github.io/"

[06.06.2025 16:15] Response: ```python
['CV']
```
[06.06.2025 16:15] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"MARBLE utilizes material embeddings in CLIP-space to control pre-trained text-to-image models for blending and recomposing material properties in images with parametric control over attributes.  					AI-generated summary 				 Editing materials of objects in images based on exemplar images is an active area of research in computer vision and graphics. We propose MARBLE, a method for performing material blending and recomposing fine-grained material properties by finding material embeddings in CLIP-space and using that to control pre-trained text-to-image models. We improve exemplar-based material editing by finding a block in the denoising UNet responsible for material attribution. Given two material exemplar-images, we find directions in the CLIP-space for blending the materials. Further, we can achieve parametric control over fine-grained material attributes such as roughness, metallic, transparency, and glow using a shallow network to predict the direction for the desired material attribute change. We perform qualitative and quantitative analysis to demonstrate the efficacy of our proposed method. We also present the ability of our method to perform multiple edits in a single forward pass and applicability to painting.   Project Page: https://marblecontrol.github.io/"

[06.06.2025 16:15] Response: ```python
["GAMES", "OPTIMIZATION"]
```
[06.06.2025 16:15] Response: ParsedChatCompletionMessage[Article](content='{"desc":"MARBLE is a novel method that enhances material editing in images by utilizing material embeddings in CLIP-space. It allows for the blending and recomposing of material properties in images through pre-trained text-to-image models. By identifying specific blocks in the denoising UNet that handle material attributes, MARBLE can manipulate fine-grained properties like roughness and transparency. The method also supports multiple edits in one pass, showcasing its efficiency and versatility in applications such as digital painting.","title":"Blend and Recompose Materials with MARBLE!"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='MARBLE is a novel method that enhances material editing in images by utilizing material embeddings in CLIP-space. It allows for the blending and recomposing of material properties in images through pre-trained text-to-image models. By identifying specific blocks in the denoising UNet that handle material attributes, MARBLE can manipulate fine-grained properties like roughness and transparency. The method also supports multiple edits in one pass, showcasing its efficiency and versatility in applications such as digital painting.', title='Blend and Recompose Materials with MARBLE!'))
[06.06.2025 16:15] Response: ParsedChatCompletionMessage[Article](content='{"desc":"MARBLEÊòØ‰∏ÄÁßçÂà©Áî®CLIPÁ©∫Èó¥‰∏≠ÁöÑÊùêÊñôÂµåÂÖ•Êù•ÊéßÂà∂È¢ÑËÆ≠ÁªÉÊñáÊú¨Âà∞ÂõæÂÉèÊ®°ÂûãÁöÑÊñπÊ≥ï„ÄÇÂÆÉÂèØ‰ª•ÂÆûÁé∞ÂõæÂÉè‰∏≠ÊùêÊñôÂ±ûÊÄßÁöÑÊ∑∑ÂêàÂíåÈáçÁªÑÔºåÂπ∂ÂØπÁªÜÁ≤íÂ∫¶ÊùêÊñôÂ±ûÊÄßËøõË°åÂèÇÊï∞ÂåñÊéßÂà∂„ÄÇÈÄöËøáÂú®ÂéªÂô™UNet‰∏≠ÊâæÂà∞‰∏éÊùêÊñôÂΩíÂ±ûÁõ∏ÂÖ≥ÁöÑÂùóÔºåMARBLEÊîπËøõ‰∫ÜÂü∫‰∫éÁ§∫‰æãÁöÑÊùêÊñôÁºñËæë„ÄÇËØ•ÊñπÊ≥ïËÉΩÂ§üÂú®‰∏ÄÊ¨°ÂâçÂêë‰º†ÈÄí‰∏≠ËøõË°åÂ§öÊ¨°ÁºñËæëÔºåÂπ∂ÈÄÇÁî®‰∫éÁªòÁîª„ÄÇ","title":"MARBLEÔºöÊô∫ËÉΩÊùêÊñôÁºñËæëÁöÑÊñ∞ÊñπÊ≥ï"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='MARBLEÊòØ‰∏ÄÁßçÂà©Áî®CLIPÁ©∫Èó¥‰∏≠ÁöÑÊùêÊñôÂµåÂÖ•Êù•ÊéßÂà∂È¢ÑËÆ≠ÁªÉÊñáÊú¨Âà∞ÂõæÂÉèÊ®°ÂûãÁöÑÊñπÊ≥ï„ÄÇÂÆÉÂèØ‰ª•ÂÆûÁé∞ÂõæÂÉè‰∏≠ÊùêÊñôÂ±ûÊÄßÁöÑÊ∑∑ÂêàÂíåÈáçÁªÑÔºåÂπ∂ÂØπÁªÜÁ≤íÂ∫¶ÊùêÊñôÂ±ûÊÄßËøõË°åÂèÇÊï∞ÂåñÊéßÂà∂„ÄÇÈÄöËøáÂú®ÂéªÂô™UNet‰∏≠ÊâæÂà∞‰∏éÊùêÊñôÂΩíÂ±ûÁõ∏ÂÖ≥ÁöÑÂùóÔºåMARBLEÊîπËøõ‰∫ÜÂü∫‰∫éÁ§∫‰æãÁöÑÊùêÊñôÁºñËæë„ÄÇËØ•ÊñπÊ≥ïËÉΩÂ§üÂú®‰∏ÄÊ¨°ÂâçÂêë‰º†ÈÄí‰∏≠ËøõË°åÂ§öÊ¨°ÁºñËæëÔºåÂπ∂ÈÄÇÁî®‰∫éÁªòÁîª„ÄÇ', title='MARBLEÔºöÊô∫ËÉΩÊùêÊñôÁºñËæëÁöÑÊñ∞ÊñπÊ≥ï'))
[06.06.2025 16:15] Using data from previous issue: {"categories": ["#robotics", "#cv", "#dataset", "#optimization", "#open_source"], "emoji": "üöó", "ru": {"title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –∫–∞–ª–∏–±—Ä–æ–≤–∫–µ LiDAR-–∫–∞–º–µ—Ä—ã —Å –ø–æ–º–æ—â—å—é –≤–∏–¥–∞ —Å–≤–µ—Ä—Ö—É", "desc": "BEVCALIB - —ç—Ç–æ –º–æ–¥–µ–ª—å, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è —Ñ—É–Ω–∫—Ü–∏–∏ –≤–∏–¥–∞ —Å–≤–µ—Ä—Ö—É –¥–ª—è —Ç–æ—á–Ω–æ–π –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏ LiDAR-–∫–∞–º–µ—Ä—ã –ø–æ –Ω–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–º –¥–∞–Ω–Ω—ã–º. –ú
[06.06.2025 16:15] Using data from previous issue: {"categories": ["#diffusion", "#agents", "#3d", "#cv"], "emoji": "üöó", "ru": {"title": "–î–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ —É–ª—É—á—à–∞—é—Ç 3D-–≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ –±–µ—Å–ø–∏–ª–æ—Ç–Ω—ã—Ö –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π", "desc": "–í —ç—Ç–æ–π —Ä–∞–±–æ—Ç–µ –∞–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Ç—Ä–µ—Ö–º–µ—Ä–Ω—ã—Ö –∫–∞—Ä—Ç –∑–∞–Ω—è—Ç–æ—Å—Ç–∏ –≤ –∑–∞–¥–∞—á–µ –∞–≤—Ç–æ–Ω
[06.06.2025 16:15] Using data from previous issue: {"categories": ["#benchmark", "#video"], "emoji": "üèãÔ∏è", "ru": {"title": "PATS: –£–º–Ω–∞—è –≤—Ä–µ–º–µ–Ω–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Ä—Ç–∏–≤–Ω–æ–≥–æ –º–∞—Å—Ç–µ—Ä—Å—Ç–≤–∞", "desc": "PATS (Proficiency-Aware Temporal Sampling) - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –≤–∏–¥–µ–æ —Å–ø–æ—Ä—Ç–∏–≤–Ω—ã—Ö –Ω–∞–≤—ã–∫–æ–≤. –û–Ω –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –∑–∞—Ö–≤–∞—Ç –ø–æ–ª–Ω—ã—Ö –ø–∞—Ç—Ç–µ
[06.06.2025 16:15] Using data from previous issue: {"categories": ["#cv", "#architecture", "#optimization", "#multimodal"], "emoji": "ü¶Ö", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: –±–æ–ª—å—à–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, –º–µ–Ω—å—à–µ —Ç–æ–∫–µ–Ω–æ–≤", "desc": "DOVE - —ç—Ç–æ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –∫–æ–¥–∏—Ä–æ–≤—â–∏–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä—ã–π —Å–æ–∑–¥–∞–µ—Ç –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è —Ä–µ–∫–æ–Ω—Å
[06.06.2025 16:15] Using data from previous issue: {"categories": ["#data", "#benchmark", "#dataset", "#healthcare", "#cv"], "emoji": "üî¨", "ru": {"title": "–ò–ò-—Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ –ö–¢-—Å–Ω–∏–º–∫–æ–≤", "desc": "–ú–æ–¥–µ–ª—å OminiAbnorm-CT –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–∞ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ –ö–¢-–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –û–Ω–∞ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã –≤ –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏ –∏ –æ–ø–∏
[06.06.2025 16:15] Using data from previous issue: {"categories": ["#architecture", "#video", "#multimodal", "#3d", "#games", "#diffusion"], "emoji": "ü§ñ", "ru": {"title": "–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∏–¥–µ–æ –∏ –¥–≤–∏–∂–µ–Ω–∏—è –¥–ª—è —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ–≥–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —á–µ–ª–æ–≤–µ–∫–∞ —Å –æ–±—ä–µ–∫—Ç–∞–º–∏", "desc": "–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –∏ –¥–≤–∏–∂–µ–Ω–∏—è –≤–∑–∞–∏–º–æ
[06.06.2025 16:15] Using data from previous issue: {"categories": ["#transfer_learning", "#multilingual", "#audio", "#low_resource"], "emoji": "üó£Ô∏è", "ru": {"title": "–Ø–∑—ã–∫–æ–≤–æ-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–æ–µ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ —É–ª—É—á—à–∞–µ—Ç –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Ä–µ—á–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –º–æ–¥–µ–ª–∏ Wav2Vec2, –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ –Ω–∞ –≥–æ–ª–ª–∞–Ω–¥—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö, –ª—É—á—à–µ –∫–æ–¥–∏—Ä—É—é—Ç –ª
[06.06.2025 16:15] Using data from previous issue: {"categories": ["#hallucinations", "#alignment", "#rlhf", "#inference"], "emoji": "üîí", "ru": {"title": "–ë–∞–ª–∞–Ω—Å –º–µ–∂–¥—É –≤–æ–¥—è–Ω—ã–º–∏ –∑–Ω–∞–∫–∞–º–∏ –∏ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ–º –≤ LLM", "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –≤–ª–∏—è–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤ –≤–æ–¥—è–Ω—ã—Ö –∑–Ω–∞–∫–æ–≤ –Ω–∞ –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (LLM), –≤—ã—è–≤–ª—è—è –∏—Ö –Ω–µ–≥–∞—Ç–∏–≤–Ω–æ–µ –≤–æ–∑–¥–µ–π—Å—Ç–≤–∏–µ –Ω–∞ –ø—Ä–∞–≤–¥–∏–≤–æ—Å—Ç—å, –±
[06.06.2025 16:15] Loading Chinese text from previous data.
[06.06.2025 16:15] Renaming data file.
[06.06.2025 16:15] Renaming previous data. hf_papers.json to ./d/2025-06-06.json
[06.06.2025 16:15] Saving new data file.
[06.06.2025 16:15] Generating page.
[06.06.2025 16:15] Renaming previous page.
[06.06.2025 16:15] Renaming previous data. index.html to ./d/2025-06-06.html
[06.06.2025 16:15] [Experimental] Generating Chinese page for reading.
[06.06.2025 16:15] Chinese vocab [{'word': '3DÊÑüÁü•ËßÜËßâËØ≠Ë®ÄÊ®°Âûã', 'pinyin': '3D g«énju√© sh√¨ju√© y«îy√°n m√≥x√≠ng', 'trans': '3D perception visual language model'}, {'word': 'Á≤æÁ°Æ', 'pinyin': 'jƒ´ngqu√®', 'trans': 'precise'}, {'word': 'ÁêÜËß£', 'pinyin': 'l«êjiƒõ', 'trans': 'understand'}, {'word': 'Â§çÊùÇ', 'pinyin': 'f√πz√°', 'trans': 'complex'}, {'word': '3DÂú∫ÊôØ', 'pinyin': '3D ch«éngj«êng', 'trans': '3D scene'}, {'word': 'Âä®ÊÄÅ', 'pinyin': 'd√≤ngt√†i', 'trans': 'dynamic'}, {'word': 'Êåá‰ª§', 'pinyin': 'zh«êl√¨ng', 'trans': 'instruction'}, {'word': 'Êé®ÁêÜ', 'pinyin': 'tuƒ´l«ê', 'trans': 'reasoning'}, {'word': 'ÁõëÁù£', 'pinyin': 'ji√†nd≈´', 'trans': 'supervised'}, {'word': 'ÂæÆË∞É', 'pinyin': 'wƒìiti√°o', 'trans': 'fine-tuning'}, {'word': 'Âº∫Âåñ', 'pinyin': 'qi√°nghu√†', 'trans': 'reinforcement'}, {'word': 'ÂÆûÁé∞', 'pinyin': 'sh√≠xi√†n', 'trans': 'achieve'}, {'word': 'ÂºïÂÖ•', 'pinyin': 'y«ênr√π', 'trans': 'introduce'}, {'word': 'Â§ßËßÑÊ®°', 'pinyin': 'd√†guƒ´m√≥', 'trans': 'large-scale'}, {'word': 'Êï∞ÊçÆÈõÜ', 'pinyin': 'sh√πj√πj√≠', 'trans': 'dataset'}, {'word': 'ËØÑ‰º∞', 'pinyin': 'p√≠ngg≈´', 'trans': 'evaluation'}, {'word': 'Âü∫ÂáÜ', 'pinyin': 'jƒ´zh«în', 'trans': 'benchmark'}, {'word': 'Á©∫Èó¥', 'pinyin': 'k≈çngjiƒÅn', 'trans': 'spatial'}, {'word': 'Â§öÊ≠•', 'pinyin': 'du≈çb√π', 'trans': 'multi-step'}, {'word': '‰ªªÂä°', 'pinyin': 'r√®nw√π', 'trans': 'task'}, {'word': 'Ë°®Áé∞', 'pinyin': 'bi«éoxi√†n', 'trans': 'performance'}, {'word': 'Âá∫Ëâ≤', 'pinyin': 'ch≈´s√®', 'trans': 'outstanding'}, {'word': 'Ë∂ÖË∂ä', 'pinyin': 'chƒÅoyu√®', 'trans': 'surpass'}]
[06.06.2025 16:15] Renaming previous Chinese page.
[06.06.2025 16:15] Renaming previous data. zh.html to ./d/2025-06-05_zh_reading_task.html
[06.06.2025 16:15] Writing Chinese reading task.
[06.06.2025 16:15] Writing result.
[06.06.2025 16:15] Renaming log file.
[06.06.2025 16:15] Renaming previous data. log.txt to ./logs/2025-06-06_last_log.txt
