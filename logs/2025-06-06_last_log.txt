[06.06.2025 04:19] Read previous papers.
[06.06.2025 04:19] Generating top page (month).
[06.06.2025 04:19] Writing top page (month).
[06.06.2025 05:12] Read previous papers.
[06.06.2025 05:12] Get feed.
[06.06.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.04308
[06.06.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.05240
[06.06.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.05010
[06.06.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.23656
[06.06.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.05176
[06.06.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.05327
[06.06.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.05344
[06.06.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.05287
[06.06.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.04209
[06.06.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.05331
[06.06.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.02620
[06.06.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.05328
[06.06.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.03077
[06.06.2025 05:12] Extract page data from URL. URL: https://huggingface.co/papers/2506.01011
[06.06.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.04633
[06.06.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.04405
[06.06.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.00830
[06.06.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.04245
[06.06.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20914
[06.06.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.05282
[06.06.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.04734
[06.06.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.03238
[06.06.2025 05:12] Extract page data from URL. URL: https://huggingface.co/papers/2506.05278
[06.06.2025 05:12] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[06.06.2025 05:12] No deleted papers detected.
[06.06.2025 05:12] Downloading and parsing papers (pdf, html). Total: 23.
[06.06.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2506.04308.
[06.06.2025 05:12] Extra JSON file exists (./assets/json/2506.04308.json), skip PDF parsing.
[06.06.2025 05:12] Paper image links file exists (./assets/img_data/2506.04308.json), skip HTML parsing.
[06.06.2025 05:12] Success.
[06.06.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2506.05240.
[06.06.2025 05:12] Extra JSON file exists (./assets/json/2506.05240.json), skip PDF parsing.
[06.06.2025 05:12] Paper image links file exists (./assets/img_data/2506.05240.json), skip HTML parsing.
[06.06.2025 05:12] Success.
[06.06.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2506.05010.
[06.06.2025 05:12] Extra JSON file exists (./assets/json/2506.05010.json), skip PDF parsing.
[06.06.2025 05:12] Paper image links file exists (./assets/img_data/2506.05010.json), skip HTML parsing.
[06.06.2025 05:12] Success.
[06.06.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2505.23656.
[06.06.2025 05:12] Extra JSON file exists (./assets/json/2505.23656.json), skip PDF parsing.
[06.06.2025 05:12] Paper image links file exists (./assets/img_data/2505.23656.json), skip HTML parsing.
[06.06.2025 05:12] Success.
[06.06.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2506.05176.
[06.06.2025 05:12] Extra JSON file exists (./assets/json/2506.05176.json), skip PDF parsing.
[06.06.2025 05:12] Paper image links file exists (./assets/img_data/2506.05176.json), skip HTML parsing.
[06.06.2025 05:12] Success.
[06.06.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2506.05327.
[06.06.2025 05:12] Extra JSON file exists (./assets/json/2506.05327.json), skip PDF parsing.
[06.06.2025 05:12] Paper image links file exists (./assets/img_data/2506.05327.json), skip HTML parsing.
[06.06.2025 05:12] Success.
[06.06.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2506.05344.
[06.06.2025 05:12] Extra JSON file exists (./assets/json/2506.05344.json), skip PDF parsing.
[06.06.2025 05:12] Paper image links file exists (./assets/img_data/2506.05344.json), skip HTML parsing.
[06.06.2025 05:12] Success.
[06.06.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2506.05287.
[06.06.2025 05:12] Extra JSON file exists (./assets/json/2506.05287.json), skip PDF parsing.
[06.06.2025 05:12] Paper image links file exists (./assets/img_data/2506.05287.json), skip HTML parsing.
[06.06.2025 05:12] Success.
[06.06.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2506.04209.
[06.06.2025 05:12] Extra JSON file exists (./assets/json/2506.04209.json), skip PDF parsing.
[06.06.2025 05:12] Paper image links file exists (./assets/img_data/2506.04209.json), skip HTML parsing.
[06.06.2025 05:12] Success.
[06.06.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2506.05331.
[06.06.2025 05:12] Extra JSON file exists (./assets/json/2506.05331.json), skip PDF parsing.
[06.06.2025 05:12] Paper image links file exists (./assets/img_data/2506.05331.json), skip HTML parsing.
[06.06.2025 05:12] Success.
[06.06.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2506.02620.
[06.06.2025 05:12] Extra JSON file exists (./assets/json/2506.02620.json), skip PDF parsing.
[06.06.2025 05:12] Paper image links file exists (./assets/img_data/2506.02620.json), skip HTML parsing.
[06.06.2025 05:12] Success.
[06.06.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2506.05328.
[06.06.2025 05:12] Extra JSON file exists (./assets/json/2506.05328.json), skip PDF parsing.
[06.06.2025 05:12] Paper image links file exists (./assets/img_data/2506.05328.json), skip HTML parsing.
[06.06.2025 05:12] Success.
[06.06.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2506.03077.
[06.06.2025 05:12] Extra JSON file exists (./assets/json/2506.03077.json), skip PDF parsing.
[06.06.2025 05:12] Paper image links file exists (./assets/img_data/2506.03077.json), skip HTML parsing.
[06.06.2025 05:12] Success.
[06.06.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2506.01011.
[06.06.2025 05:12] Downloading paper 2506.01011 from http://arxiv.org/pdf/2506.01011v1...
[06.06.2025 05:12] Extracting affiliations from text.
[06.06.2025 05:12] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 1 ] . [ 1 1 1 0 1 0 . 6 0 5 2 : r AUTOREGRESSIVE IMAGES WATERMARKING THROUGH LEXICAL BIASING: AN APPROACH RESISTANT TO REGENERATION ATTACK Siqi Hui Xian Jiaotong University huisiqi@stu.xjtu.edu.cn Yiren Song National University of Singapore songyiren725@gmail.com Sanping Zhou Xian Jiaotong University spzhou@xjtu.edu.cn Ye Deng Southwestern University of Finance and Economics dengye@stu.xjtu.edu.cn Wenli Huang Ningbo University of Technology huangwenwenlili@126.com Jinjun Wang Xian Jiaotong University jinjun@mail.xjtu.edu.cn June 3, "
[06.06.2025 05:13] Response: ```python
[
    "Xian Jiaotong University",
    "National University of Singapore",
    "Southwestern University of Finance and Economics",
    "Ningbo University of Technology"
]
```
[06.06.2025 05:13] Deleting PDF ./assets/pdf/2506.01011.pdf.
[06.06.2025 05:13] Success.
[06.06.2025 05:13] Downloading and parsing paper https://huggingface.co/papers/2506.04633.
[06.06.2025 05:13] Extra JSON file exists (./assets/json/2506.04633.json), skip PDF parsing.
[06.06.2025 05:13] Paper image links file exists (./assets/img_data/2506.04633.json), skip HTML parsing.
[06.06.2025 05:13] Success.
[06.06.2025 05:13] Downloading and parsing paper https://huggingface.co/papers/2506.04405.
[06.06.2025 05:13] Extra JSON file exists (./assets/json/2506.04405.json), skip PDF parsing.
[06.06.2025 05:13] Paper image links file exists (./assets/img_data/2506.04405.json), skip HTML parsing.
[06.06.2025 05:13] Success.
[06.06.2025 05:13] Downloading and parsing paper https://huggingface.co/papers/2506.00830.
[06.06.2025 05:13] Extra JSON file exists (./assets/json/2506.00830.json), skip PDF parsing.
[06.06.2025 05:13] Paper image links file exists (./assets/img_data/2506.00830.json), skip HTML parsing.
[06.06.2025 05:13] Success.
[06.06.2025 05:13] Downloading and parsing paper https://huggingface.co/papers/2506.04245.
[06.06.2025 05:13] Extra JSON file exists (./assets/json/2506.04245.json), skip PDF parsing.
[06.06.2025 05:13] Paper image links file exists (./assets/img_data/2506.04245.json), skip HTML parsing.
[06.06.2025 05:13] Success.
[06.06.2025 05:13] Downloading and parsing paper https://huggingface.co/papers/2505.20914.
[06.06.2025 05:13] Extra JSON file exists (./assets/json/2505.20914.json), skip PDF parsing.
[06.06.2025 05:13] Paper image links file exists (./assets/img_data/2505.20914.json), skip HTML parsing.
[06.06.2025 05:13] Success.
[06.06.2025 05:13] Downloading and parsing paper https://huggingface.co/papers/2506.05282.
[06.06.2025 05:13] Extra JSON file exists (./assets/json/2506.05282.json), skip PDF parsing.
[06.06.2025 05:13] Paper image links file exists (./assets/img_data/2506.05282.json), skip HTML parsing.
[06.06.2025 05:13] Success.
[06.06.2025 05:13] Downloading and parsing paper https://huggingface.co/papers/2506.04734.
[06.06.2025 05:13] Extra JSON file exists (./assets/json/2506.04734.json), skip PDF parsing.
[06.06.2025 05:13] Paper image links file exists (./assets/img_data/2506.04734.json), skip HTML parsing.
[06.06.2025 05:13] Success.
[06.06.2025 05:13] Downloading and parsing paper https://huggingface.co/papers/2506.03238.
[06.06.2025 05:13] Extra JSON file exists (./assets/json/2506.03238.json), skip PDF parsing.
[06.06.2025 05:13] Paper image links file exists (./assets/img_data/2506.03238.json), skip HTML parsing.
[06.06.2025 05:13] Success.
[06.06.2025 05:13] Downloading and parsing paper https://huggingface.co/papers/2506.05278.
[06.06.2025 05:13] Downloading paper 2506.05278 from http://arxiv.org/pdf/2506.05278v1...
[06.06.2025 05:13] Extracting affiliations from text.
[06.06.2025 05:13] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Micro-Act: Mitigate Knowledge Conflict in Question Answering via Actionable Self-Reasoning Nan Huo 1, Jinyang Li 1, Bowen Qin 2* , Ge Qu1, Xiaolong Li 1, Xiaodong Li3, Chenhao Ma 4, Reynold Cheng 1* 1The University of Hong Kong, 2 BAAI 3Xiamen University 4The Chinese University of Hong Kong, Shenzhen huonan@connect.hku.hk, bwqin@baai.ac.cn, ckcheng@cs.hku.hk 5 2 0 2 5 ] . [ 1 8 7 2 5 0 . 6 0 5 2 : r a "
[06.06.2025 05:13] Response: ```python
[
    "The University of Hong Kong",
    "BAAI",
    "Xiamen University",
    "The Chinese University of Hong Kong, Shenzhen"
]
```
[06.06.2025 05:13] Deleting PDF ./assets/pdf/2506.05278.pdf.
[06.06.2025 05:13] Success.
[06.06.2025 05:13] Enriching papers with extra data.
[06.06.2025 05:13] ********************************************************************************
[06.06.2025 05:13] Abstract 0. Spatial referring is a fundamental capability of embodied robots to interact with the 3D physical world. However, even with the powerful pretrained vision language models (VLMs), recent approaches are still not qualified to accurately understand the complex 3D scenes and dynamically reason about the...
[06.06.2025 05:13] ********************************************************************************
[06.06.2025 05:13] Abstract 1. This paper presents a novel framework for aligning learnable latent spaces to arbitrary target distributions by leveraging flow-based generative models as priors. Our method first pretrains a flow model on the target features to capture the underlying distribution. This fixed flow model subsequently...
[06.06.2025 05:13] ********************************************************************************
[06.06.2025 05:13] Abstract 2. ComfyUI-Copilot uses a large language model and multi-agent system to enhance the usability and efficiency of the AI-driven art creation platform ComfyUI.  					AI-generated summary 				 We introduce ComfyUI-Copilot, a large language model-powered plugin designed to enhance the usability and efficie...
[06.06.2025 05:13] ********************************************************************************
[06.06.2025 05:13] Abstract 3. Recent advancements in text-to-video (T2V) diffusion models have enabled high-fidelity and realistic video synthesis. However, current T2V models often struggle to generate physically plausible content due to their limited inherent ability to accurately understand physics. We found that while the re...
[06.06.2025 05:13] ********************************************************************************
[06.06.2025 05:13] Abstract 4. In this work, we introduce the Qwen3 Embedding series, a significant advancement over its predecessor, the GTE-Qwen series, in text embedding and reranking capabilities, built upon the Qwen3 foundation models. Leveraging the Qwen3 LLMs' robust capabilities in multilingual text understanding and gene...
[06.06.2025 05:13] ********************************************************************************
[06.06.2025 05:13] Abstract 5. Depth maps are widely used in feed-forward 3D Gaussian Splatting (3DGS) pipelines by unprojecting them into 3D point clouds for novel view synthesis. This approach offers advantages such as efficient training, the use of known camera poses, and accurate geometry estimation. However, depth discontinu...
[06.06.2025 05:13] ********************************************************************************
[06.06.2025 05:13] Abstract 6. Multimodal Large Language Models (MLLMs) are commonly derived by extending pre-trained Large Language Models (LLMs) with visual capabilities. In this work, we investigate how MLLMs process visual inputs by analyzing their attention mechanisms. We reveal a surprising sparsity phenomenon: only a small...
[06.06.2025 05:13] ********************************************************************************
[06.06.2025 05:13] Abstract 7. The emergence of multimodal large language models (MLLMs) has driven breakthroughs in egocentric vision applications. These applications necessitate persistent, context-aware understanding of objects, as users interact with tools in dynamic and cluttered environments. However, existing embodied benc...
[06.06.2025 05:13] ********************************************************************************
[06.06.2025 05:13] Abstract 8. Learning Language-Image alignment with a Fixed Text encoder (LIFT) using pre-trained large language models effectively guides visual representation learning, outperforming joint training methods like CLIP in compositional understanding and long captions.  					AI-generated summary 				 Currently, th...
[06.06.2025 05:13] ********************************************************************************
[06.06.2025 05:13] Abstract 9. Chain-of-Thought (CoT) has widely enhanced mathematical reasoning in Large Language Models (LLMs), but it still remains challenging for extending it to multimodal domains. Existing works either adopt a similar textual reasoning for image input, or seek to interleave visual signals into mathematical ...
[06.06.2025 05:13] ********************************************************************************
[06.06.2025 05:13] Abstract 10. FlexPainter, a novel texture generation pipeline, uses a shared conditional embedding space to enable flexible multi-modal guidance, ensuring high-quality and consistent texture map generation using image diffusion priors and a 3D-aware model.  					AI-generated summary 				 Texture map production i...
[06.06.2025 05:13] ********************************************************************************
[06.06.2025 05:13] Abstract 11. Despite progress in video understanding, current MLLMs struggle with counting tasks. Existing benchmarks are limited by short videos, close-set queries, lack of clue annotations, and weak multimodal coverage. In this paper, we introduce CG-AV-Counting, a manually-annotated clue-grounded counting ben...
[06.06.2025 05:13] ********************************************************************************
[06.06.2025 05:13] Abstract 12. StreamBP, a memory-efficient and exact backpropagation method, decomposes the chain rule to reduce memory costs, enabling longer sequence lengths and faster training speeds for language models compared to gradient checkpointing.  					AI-generated summary 				 Training language models on long sequen...
[06.06.2025 05:13] ********************************************************************************
[06.06.2025 05:13] Abstract 13. A novel watermarking technique, Lexical Bias Watermarking, enhances the security of autoregressive image generation models by embedding watermarks into token selection, demonstrating superior resistance to regeneration attacks.  					AI-generated summary 				 Autoregressive (AR) image generation mod...
[06.06.2025 05:13] ********************************************************************************
[06.06.2025 05:13] Abstract 14. Spatial cognition is essential for human intelligence, enabling problem-solving through visual simulations rather than solely relying on verbal reasoning. However, existing AI benchmarks primarily assess verbal reasoning, neglecting the complexities of non-verbal, multi-step visual simulation. We in...
[06.06.2025 05:13] ********************************************************************************
[06.06.2025 05:13] Abstract 15. We introduce MedAgentGYM, the first publicly available training environment designed to enhance coding-based medical reasoning capabilities in large language model (LLM) agents. MedAgentGYM comprises 72,413 task instances across 129 categories derived from authentic real-world biomedical scenarios. ...
[06.06.2025 05:13] ********************************************************************************
[06.06.2025 05:13] Abstract 16. SkyReels-Audio is a unified framework using pretrained video diffusion transformers for generating high-fidelity and coherent audio-conditioned talking portrait videos, supported by a hybrid curriculum learning strategy and advanced loss mechanisms.  					AI-generated summary 				 The generation and...
[06.06.2025 05:13] ********************************************************************************
[06.06.2025 05:13] Abstract 17. As the era of autonomous agents making decisions on behalf of users unfolds, ensuring contextual integrity (CI) -- what is the appropriate information to share while carrying out a certain task -- becomes a central question to the field. We posit that CI demands a form of reasoning where the agent n...
[06.06.2025 05:13] ********************************************************************************
[06.06.2025 05:13] Abstract 18. General object composition (GOC) aims to seamlessly integrate a target object into a background scene with desired geometric properties, while simultaneously preserving its fine-grained appearance details. Recent approaches derive semantic embeddings and integrate them into advanced diffusion models...
[06.06.2025 05:13] ********************************************************************************
[06.06.2025 05:13] Abstract 19. We introduce Rectified Point Flow, a unified parameterization that formulates pairwise point cloud registration and multi-part shape assembly as a single conditional generative problem. Given unposed point clouds, our method learns a continuous point-wise velocity field that transports noisy points ...
[06.06.2025 05:13] ********************************************************************************
[06.06.2025 05:13] Abstract 20. Reasoning models represented by the Deepseek-R1-Distill series have been widely adopted by the open-source community due to their strong performance in mathematics, science, programming, and other domains. However, our study reveals that their benchmark evaluation results are subject to significant ...
[06.06.2025 05:13] ********************************************************************************
[06.06.2025 05:13] Abstract 21. OminiAbnorm-CT, a model for automated interpretation of CT images, outperforms existing methods in localizing and describing abnormalities across different body regions using text queries and visual prompts.  					AI-generated summary 				 Automated interpretation of CT images-particularly localizin...
[06.06.2025 05:13] ********************************************************************************
[06.06.2025 05:13] Abstract 22. A framework called Micro-Act addresses Knowledge Conflicts in Retrieval-Augmented Generation by adaptively decomposing knowledge sources, leading to improved QA accuracy compared to existing methods.  					AI-generated summary 				 Retrieval-Augmented Generation (RAG) systems commonly suffer from Kn...
[06.06.2025 05:13] Read previous papers.
[06.06.2025 05:13] Generating reviews via LLM API.
[06.06.2025 05:13] Using data from previous issue: {"categories": ["#optimization", "#benchmark", "#robotics", "#reasoning", "#dataset", "#3d", "#training", "#rl"], "emoji": "ü§ñ", "ru": {"title": "RoboRefer: –ü—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç –¥–ª—è —Ä–æ–±–æ—Ç–æ–≤ –Ω–æ–≤–æ–≥–æ –ø–æ–∫–æ–ª–µ–Ω–∏—è", "desc": "RoboRefer - —ç—Ç–æ –º–æ–¥–µ–ª—å –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –¥–ª—è —Ä–æ–±–æ—Ç–æ–≤, –æ—Å–Ω–æ–≤–∞–Ω–Ω–∞—è –Ω–∞
[06.06.2025 05:13] Using data from previous issue: {"categories": ["#optimization", "#alignment", "#training", "#cv", "#math", "#diffusion"], "emoji": "üîÑ", "ru": {"title": "–í—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –ª–∞—Ç–µ–Ω—Ç–Ω—ã—Ö –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤ —Å –ø–æ–º–æ—â—å—é –ø–æ—Ç–æ–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—é –æ–±—É—á–∞–µ–º—ã—Ö –ª–∞—Ç–µ–Ω—Ç–Ω—ã—Ö –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤ —Å –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã–º–∏ —Ü–µ–ª–µ–≤—ã–º–∏
[06.06.2025 05:13] Using data from previous issue: {"categories": ["#multimodal", "#agents", "#open_source"], "emoji": "üé®", "ru": {"title": "–ò–ò-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –¥–ª—è —Ç–≤–æ—Ä—á–µ—Å—Ç–≤–∞: ComfyUI-Copilot —É–ø—Ä–æ—â–∞–µ—Ç —Å–æ–∑–¥–∞–Ω–∏–µ —Ü–∏—Ñ—Ä–æ–≤–æ–≥–æ –∏—Å–∫—É—Å—Å—Ç–≤–∞", "desc": "ComfyUI-Copilot - —ç—Ç–æ –ø–ª–∞–≥–∏–Ω –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —É–¥–æ–±—Å—Ç–≤–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∏
[06.06.2025 05:13] Using data from previous issue: {"categories": ["#optimization", "#benchmark", "#training", "#video", "#diffusion"], "emoji": "üé•", "ru": {"title": "–§–∏–∑–∏—á–µ—Å–∫–∏ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ–µ –≤–∏–¥–µ–æ –∏–∑ —Ç–µ–∫—Å—Ç–∞: VideoREPA —É–ª—É—á—à–∞–µ—Ç –ø–æ–Ω–∏–º–∞–Ω–∏–µ —Ñ–∏–∑–∏–∫–∏ –≤ –º–æ–¥–µ–ª—è—Ö T2V", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ VideoREPA –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ñ–∏–∑–∏—á–µ—Å–∫–æ–π –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏ –≤–∏
[06.06.2025 05:13] Using data from previous issue: {"categories": ["#data", "#benchmark", "#dataset", "#multilingual", "#open_source", "#training", "#small_models", "#low_resource"], "emoji": "üîç", "ru": {"title": "Qwen3 Embedding: –ù–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Å–µ—Ä–∏—è –º–æ–¥–µ–ª–µ–π Qwen3 Embedding, —É–ª—É—á—à–∞
[06.06.2025 05:13] Using data from previous issue: {"categories": ["#optimization", "#3d", "#architecture"], "emoji": "üîç", "ru": {"title": "–£–ª—É—á—à–µ–Ω–∏–µ 3D-—Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ —Å –ø–æ–º–æ—â—å—é —É–º–Ω–æ–π —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏ –≥—Ä–∞–Ω–∏—Ü –æ–±—ä–µ–∫—Ç–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ 3D-—Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –≤ —Å–∏—Å—Ç–µ–º–∞—Ö 3D Gaussian Splatting. –ê–≤—Ç–æ—Ä—ã 
[06.06.2025 05:13] Using data from previous issue: {"categories": ["#optimization", "#benchmark", "#multimodal", "#architecture", "#open_source", "#inference"], "emoji": "üîç", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –∑—Ä–µ–Ω–∏–µ: –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –ò–ò", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (ML
[06.06.2025 05:13] Using data from previous issue: {"categories": ["#optimization", "#cv", "#multimodal", "#interpretability", "#benchmark", "#games"], "emoji": "üëÅÔ∏è", "ru": {"title": "EOC-Bench: –ù–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –ò–ò –≤ —ç–≥–æ—Ü–µ–Ω—Ç—Ä–∏—á–µ—Å–∫–æ–º –∑—Ä–µ–Ω–∏–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ EOC-Bench –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è 
[06.06.2025 05:13] Using data from previous issue: {"categories": ["#benchmark", "#multimodal", "#long_context", "#alignment", "#cv"], "emoji": "üîç", "ru": {"title": "–§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —è–∑—ã–∫–æ–≤–æ–π —ç–Ω–∫–æ–¥–µ—Ä —É–ª—É—á—à–∞–µ—Ç –≤–∏–∑—É–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ LIFT –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ
[06.06.2025 05:13] Using data from previous issue: {"categories": ["#training", "#multimodal", "#dataset", "#reasoning", "#math", "#games"], "emoji": "üßÆ", "ru": {"title": "–í–∏–∑—É–∞–ª—å–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –≤ –º–∞—Ç–µ–º–∞—Ç–∏–∫–µ: –Ω–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å —Å MINT-CoT", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç MINT-CoT - –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≤–∏–∑—É–∞–ª—å–Ω–æ–º—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é –≤ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á–∞—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤
[06.06.2025 05:13] Using data from previous issue: {"categories": ["#multimodal", "#3d", "#cv", "#diffusion"], "emoji": "üé®", "ru": {"title": "FlexPainter: –≥–∏–±–∫–∞—è –∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç—É—Ä —Å –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–º —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º", "desc": "FlexPainter - —ç—Ç–æ –Ω–æ–≤—ã–π –∫–æ–Ω–≤–µ–π–µ—Ä –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç—É—Ä, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –æ–±—â–µ–µ —É—Å–ª–æ–≤–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –≤–ª–æ–∂–µ–Ω–∏–π –¥–ª—è –≥–∏–±–∫–æ–≥–æ 
[06.06.2025 05:13] Using data from previous issue: {"categories": ["#optimization", "#benchmark", "#multimodal", "#reasoning", "#dataset", "#long_context", "#training", "#video", "#rl"], "emoji": "üßÆ", "ru": {"title": "–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –ø–æ–¥—Å—á–µ—Ç –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –ò–ò", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ CG-AV-Counting –¥–ª—è –∑
[06.06.2025 05:13] Using data from previous issue: {"categories": ["#training", "#long_context", "#optimization"], "emoji": "üöÄ", "ru": {"title": "StreamBP: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –æ–±—É—á–µ–Ω–∏–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "StreamBP - —ç—Ç–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥ –æ–±—Ä–∞—Ç–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è –æ—à–∏–±–∫–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –û–Ω —Ä–∞–∑–ª–∞–≥–∞–µ—Ç –ø—Ä–∞–≤–∏–ª–æ —Ü–µ–ø–æ—á–∫–∏ –≤–¥–æ–ª—å –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω
[06.06.2025 05:13] Querying the API.
[06.06.2025 05:13] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A novel watermarking technique, Lexical Bias Watermarking, enhances the security of autoregressive image generation models by embedding watermarks into token selection, demonstrating superior resistance to regeneration attacks.  					AI-generated summary 				 Autoregressive (AR) image generation models have gained increasing attention for their breakthroughs in synthesis quality, highlighting the need for robust watermarking to prevent misuse. However, existing in-generation watermarking techniques are primarily designed for diffusion models, where watermarks are embedded within diffusion latent states. This design poses significant challenges for direct adaptation to AR models, which generate images sequentially through token prediction. Moreover, diffusion-based regeneration attacks can effectively erase such watermarks by perturbing diffusion latent states. To address these challenges, we propose Lexical Bias Watermarking (LBW), a novel framework designed for AR models that resists regeneration attacks. LBW embeds watermarks directly into token maps by biasing token selection toward a predefined green list during generation. This approach ensures seamless integration with existing AR models and extends naturally to post-hoc watermarking. To increase the security against white-box attacks, instead of using a single green list, the green list for each image is randomly sampled from a pool of green lists. Watermark detection is performed via quantization and statistical analysis of the token distribution. Extensive experiments demonstrate that LBW achieves superior watermark robustness, particularly in resisting regeneration attacks.
[06.06.2025 05:13] Response: {
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é —Ç–µ—Ö–Ω–∏–∫—É –≤–æ–¥—è–Ω—ã—Ö –∑–Ω–∞–∫–æ–≤ –¥–ª—è –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π - Lexical Bias Watermarking (LBW). LBW –≤—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç –≤–æ–¥—è–Ω—ã–µ –∑–Ω–∞–∫–∏ –Ω–µ–ø–æ—Å—Ä–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ –≤ –∫–∞—Ä—Ç—ã —Ç–æ–∫–µ–Ω–æ–≤, —Å–º–µ—â–∞—è –≤—ã–±–æ—Ä —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Å—Ç–æ—Ä–æ–Ω—É –ø—Ä–µ–¥–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–≥–æ '–∑–µ–ª–µ–Ω–æ–≥–æ —Å–ø–∏—Å–∫–∞' –≤–æ –≤—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –±–µ—Å—à–æ–≤–Ω—É—é –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ –∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ —Ä–∞—Å—à–∏—Ä—è–µ—Ç—Å—è –¥–æ –ø–æ—Å—Ç—Ñ–∞–∫—Ç—É–º–Ω–æ–π –≤—Å—Ç–∞–≤–∫–∏ –≤–æ–¥—è–Ω—ã—Ö –∑–Ω–∞–∫–æ–≤. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ LBW –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω–æ–π —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏ –≤–æ–¥—è–Ω—ã—Ö –∑–Ω–∞–∫–æ–≤, –æ—Å–æ–±–µ–Ω–Ω–æ –≤ –ø—Ä–æ—Ç–∏–≤–æ—Å—Ç–æ—è–Ω–∏–∏ –∞—Ç–∞–∫–∞–º —Ä–µ–≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.",
  "emoji": "üîê",
  "title": "–ó–∞—â–∏—Ç–∞ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é –ª–µ–∫—Å–∏—á–µ—Å–∫–∏—Ö –≤–æ–¥—è–Ω—ã—Ö –∑–Ω–∞–∫–æ–≤"
}
[06.06.2025 05:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A novel watermarking technique, Lexical Bias Watermarking, enhances the security of autoregressive image generation models by embedding watermarks into token selection, demonstrating superior resistance to regeneration attacks.  					AI-generated summary 				 Autoregressive (AR) image generation models have gained increasing attention for their breakthroughs in synthesis quality, highlighting the need for robust watermarking to prevent misuse. However, existing in-generation watermarking techniques are primarily designed for diffusion models, where watermarks are embedded within diffusion latent states. This design poses significant challenges for direct adaptation to AR models, which generate images sequentially through token prediction. Moreover, diffusion-based regeneration attacks can effectively erase such watermarks by perturbing diffusion latent states. To address these challenges, we propose Lexical Bias Watermarking (LBW), a novel framework designed for AR models that resists regeneration attacks. LBW embeds watermarks directly into token maps by biasing token selection toward a predefined green list during generation. This approach ensures seamless integration with existing AR models and extends naturally to post-hoc watermarking. To increase the security against white-box attacks, instead of using a single green list, the green list for each image is randomly sampled from a pool of green lists. Watermark detection is performed via quantization and statistical analysis of the token distribution. Extensive experiments demonstrate that LBW achieves superior watermark robustness, particularly in resisting regeneration attacks."

[06.06.2025 05:13] Response: ```python
["CV", "VIDEO"]
```
[06.06.2025 05:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A novel watermarking technique, Lexical Bias Watermarking, enhances the security of autoregressive image generation models by embedding watermarks into token selection, demonstrating superior resistance to regeneration attacks.  					AI-generated summary 				 Autoregressive (AR) image generation models have gained increasing attention for their breakthroughs in synthesis quality, highlighting the need for robust watermarking to prevent misuse. However, existing in-generation watermarking techniques are primarily designed for diffusion models, where watermarks are embedded within diffusion latent states. This design poses significant challenges for direct adaptation to AR models, which generate images sequentially through token prediction. Moreover, diffusion-based regeneration attacks can effectively erase such watermarks by perturbing diffusion latent states. To address these challenges, we propose Lexical Bias Watermarking (LBW), a novel framework designed for AR models that resists regeneration attacks. LBW embeds watermarks directly into token maps by biasing token selection toward a predefined green list during generation. This approach ensures seamless integration with existing AR models and extends naturally to post-hoc watermarking. To increase the security against white-box attacks, instead of using a single green list, the green list for each image is randomly sampled from a pool of green lists. Watermark detection is performed via quantization and statistical analysis of the token distribution. Extensive experiments demonstrate that LBW achieves superior watermark robustness, particularly in resisting regeneration attacks."

[06.06.2025 05:13] Response: ```python
["SECURITY"]
```
[06.06.2025 05:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The paper introduces Lexical Bias Watermarking (LBW), a new technique aimed at enhancing the security of autoregressive (AR) image generation models. Unlike traditional methods that embed watermarks in diffusion models, LBW integrates watermarks directly into the token selection process during image generation. This method not only improves the robustness of watermarks against regeneration attacks but also allows for easy adaptation to existing AR frameworks. The approach utilizes a randomized selection of green lists for watermarking, ensuring higher security and effective detection through statistical analysis of token distributions.","title":"Secure Your Images with Lexical Bias Watermarking!"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='The paper introduces Lexical Bias Watermarking (LBW), a new technique aimed at enhancing the security of autoregressive (AR) image generation models. Unlike traditional methods that embed watermarks in diffusion models, LBW integrates watermarks directly into the token selection process during image generation. This method not only improves the robustness of watermarks against regeneration attacks but also allows for easy adaptation to existing AR frameworks. The approach utilizes a randomized selection of green lists for watermarking, ensuring higher security and effective detection through statistical analysis of token distributions.', title='Secure Your Images with Lexical Bias Watermarking!'))
[06.06.2025 05:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÊ∞¥Âç∞ÊäÄÊúØÔºåÁß∞‰∏∫ËØçÊ±áÂÅèÁΩÆÊ∞¥Âç∞ÔºàLexical Bias WatermarkingÔºâÔºåÊó®Âú®Â¢ûÂº∫Ëá™ÂõûÂΩíÂõæÂÉèÁîüÊàêÊ®°ÂûãÁöÑÂÆâÂÖ®ÊÄß„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÂú®ÁîüÊàêËøáÁ®ã‰∏≠ÂÅèÂêëÈÄâÊã©È¢ÑÂÆö‰πâÁöÑÁªøËâ≤ÂàóË°®ÔºåÂ∞ÜÊ∞¥Âç∞ÂµåÂÖ•Âà∞‰ª§ÁâåÈÄâÊã©‰∏≠Ôºå‰ªéËÄåÊúâÊïàÊäµÂæ°ÂÜçÁîüÊîªÂáª„ÄÇ‰∏éÁé∞ÊúâÁöÑÊâ©Êï£Ê®°ÂûãÊ∞¥Âç∞ÊäÄÊúØ‰∏çÂêåÔºåLBWËÉΩÂ§üÁõ¥Êé•‰∏éËá™ÂõûÂΩíÊ®°ÂûãÈõÜÊàêÔºåÂπ∂ÊîØÊåÅÂêéÊúüÊ∞¥Âç∞Â§ÑÁêÜ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåLBWÂú®ÊäµÊäóÂÜçÁîüÊîªÂáªÊñπÈù¢Ë°®Áé∞Âá∫Ëâ≤ÔºåÊòæËëóÊèêÈ´ò‰∫ÜÊ∞¥Âç∞ÁöÑÈ≤ÅÊ£íÊÄß„ÄÇ","title":"Â¢ûÂº∫Ëá™ÂõûÂΩíÊ®°ÂûãÂÆâÂÖ®ÊÄßÁöÑÊ∞¥Âç∞ÊäÄÊúØ"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÊ∞¥Âç∞ÊäÄÊúØÔºåÁß∞‰∏∫ËØçÊ±áÂÅèÁΩÆÊ∞¥Âç∞ÔºàLexical Bias WatermarkingÔºâÔºåÊó®Âú®Â¢ûÂº∫Ëá™ÂõûÂΩíÂõæÂÉèÁîüÊàêÊ®°ÂûãÁöÑÂÆâÂÖ®ÊÄß„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÂú®ÁîüÊàêËøáÁ®ã‰∏≠ÂÅèÂêëÈÄâÊã©È¢ÑÂÆö‰πâÁöÑÁªøËâ≤ÂàóË°®ÔºåÂ∞ÜÊ∞¥Âç∞ÂµåÂÖ•Âà∞‰ª§ÁâåÈÄâÊã©‰∏≠Ôºå‰ªéËÄåÊúâÊïàÊäµÂæ°ÂÜçÁîüÊîªÂáª„ÄÇ‰∏éÁé∞ÊúâÁöÑÊâ©Êï£Ê®°ÂûãÊ∞¥Âç∞ÊäÄÊúØ‰∏çÂêåÔºåLBWËÉΩÂ§üÁõ¥Êé•‰∏éËá™ÂõûÂΩíÊ®°ÂûãÈõÜÊàêÔºåÂπ∂ÊîØÊåÅÂêéÊúüÊ∞¥Âç∞Â§ÑÁêÜ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåLBWÂú®ÊäµÊäóÂÜçÁîüÊîªÂáªÊñπÈù¢Ë°®Áé∞Âá∫Ëâ≤ÔºåÊòæËëóÊèêÈ´ò‰∫ÜÊ∞¥Âç∞ÁöÑÈ≤ÅÊ£íÊÄß„ÄÇ', title='Â¢ûÂº∫Ëá™ÂõûÂΩíÊ®°ÂûãÂÆâÂÖ®ÊÄßÁöÑÊ∞¥Âç∞ÊäÄÊúØ'))
[06.06.2025 05:13] Using data from previous issue: {"categories": ["#multimodal", "#3d", "#reasoning", "#benchmark"], "emoji": "üß†", "ru": {"title": "STARE: –ù–æ–≤—ã–π —Ä—É–±–µ–∂ –≤ –æ—Ü–µ–Ω–∫–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ –ò–ò", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ STARE –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –∑–∞–¥–∞—á–∞—Ö –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è –∏ –≤–∏–∑—É–∞–ª—å
[06.06.2025 05:13] Using data from previous issue: {"categories": ["#benchmark", "#reasoning", "#dataset", "#open_source", "#training", "#rl"], "emoji": "ü©∫", "ru": {"title": "MedAgentGYM: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –æ–±—É—á–µ–Ω–∏–∏ –ò–ò –¥–ª—è –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–≥–æ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è", "desc": "MedAgentGYM - —ç—Ç–æ –Ω–æ–≤–∞—è —Å—Ä–µ–¥–∞ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –Ω–∞–≤—ã–∫–æ–≤ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è —É –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ 
[06.06.2025 05:13] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#multimodal", "#audio", "#diffusion", "#synthetic", "#video"], "emoji": "üé≠", "ru": {"title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ —Å–∏–Ω—Ç–µ–∑–µ –≥–æ–≤–æ—Ä—è—â–∏—Ö –ø–æ—Ä—Ç—Ä–µ—Ç–æ–≤: –æ—Ç –∞—É–¥–∏–æ –∫ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ–º—É –≤–∏–¥–µ–æ", "desc": "SkyReels-Audio - —ç—Ç–æ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –≤–∏–¥
[06.06.2025 05:13] Using data from previous issue: {"categories": ["#synthetic", "#agents", "#benchmark", "#reasoning", "#dataset", "#transfer_learning", "#rl", "#leakage"], "emoji": "üîê", "ru": {"title": "–†–∞–∑—É–º–Ω–æ–µ —Ä–∞—Å–∫—Ä—ã—Ç–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏: –æ–±—É—á–µ–Ω–∏–µ –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–π —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏", "desc": "–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –ø—Ä–æ–±–ª–µ–º–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–π —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏ (CI) –≤ —ç–ø–æ
[06.06.2025 05:13] Using data from previous issue: {"categories": ["#cv", "#multimodal", "#benchmark", "#diffusion"], "emoji": "üé®", "ru": {"title": "–¢–æ—á–Ω–æ–µ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≥–µ–æ–º–µ—Ç—Ä–∏–∏ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–µ—Ç–∞–ª–µ–π –≤ –∫–æ–º–ø–æ–∑–∏—Ü–∏–∏ –æ–±—ä–µ–∫—Ç–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å DGAD (Disentangled Geometry-editable and Appearance-preserving Diffusion) –¥–ª—è –∫–æ–º–ø–æ
[06.06.2025 05:13] Using data from previous issue: {"categories": ["#dataset", "#3d", "#benchmark"], "emoji": "üß©", "ru": {"title": "–ï–¥–∏–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏ –æ–±–ª–∞–∫–æ–≤ —Ç–æ—á–µ–∫ –∏ —Å–±–æ—Ä–∫–µ —Ñ–æ—Ä–º", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –º–µ—Ç–æ–¥ Rectified Point Flow, –∫–æ—Ç–æ—Ä—ã–π –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—é –æ–±–ª–∞–∫–æ–≤ —Ç–æ—á–µ–∫ –∏ —Å–±–æ—Ä–∫—É –º–Ω–æ–≥–æ–∫–æ–º–ø–æ–Ω–µ–Ω—Ç–Ω—ã—Ö —Ñ–æ—Ä–º –≤ –µ–¥–∏–Ω—É—é —É—Å–ª–æ–≤–Ω—É—é –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—É—é –∑–∞–¥–∞
[06.06.2025 05:13] Using data from previous issue: {"categories": ["#benchmark", "#reasoning", "#open_source", "#training", "#math"], "emoji": "üé¢", "ru": {"title": "–ù–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –æ—Ü–µ–Ω–æ–∫: –≤—ã–∑–æ–≤ –¥–ª—è –±–µ–Ω—á–º–∞—Ä–∫–∏–Ω–≥–∞ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π —Å–µ—Ä–∏–∏ Deepseek-R1-Distill –ø–æ–¥–≤–µ—Ä–∂–µ–Ω—ã –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–º –∫–æ–ª–µ–±–∞–Ω
[06.06.2025 05:13] Using data from previous issue: {"categories": ["#data", "#benchmark", "#dataset", "#healthcare", "#cv"], "emoji": "üî¨", "ru": {"title": "–ò–ò-—Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ –ö–¢-—Å–Ω–∏–º–∫–æ–≤", "desc": "–ú–æ–¥–µ–ª—å OminiAbnorm-CT –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–∞ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ –ö–¢-–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –û–Ω–∞ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã –≤ –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏ –∏ –æ–ø–∏
[06.06.2025 05:13] Querying the API.
[06.06.2025 05:13] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A framework called Micro-Act addresses Knowledge Conflicts in Retrieval-Augmented Generation by adaptively decomposing knowledge sources, leading to improved QA accuracy compared to existing methods.  					AI-generated summary 				 Retrieval-Augmented Generation (RAG) systems commonly suffer from Knowledge Conflicts, where retrieved external knowledge contradicts the inherent, parametric knowledge of large language models (LLMs). It adversely affects performance on downstream tasks such as question answering (QA). Existing approaches often attempt to mitigate conflicts by directly comparing two knowledge sources in a side-by-side manner, but this can overwhelm LLMs with extraneous or lengthy contexts, ultimately hindering their ability to identify and mitigate inconsistencies. To address this issue, we propose Micro-Act a framework with a hierarchical action space that automatically perceives context complexity and adaptively decomposes each knowledge source into a sequence of fine-grained comparisons. These comparisons are represented as actionable steps, enabling reasoning beyond the superficial context. Through extensive experiments on five benchmark datasets, Micro-Act consistently achieves significant increase in QA accuracy over state-of-the-art baselines across all 5 datasets and 3 conflict types, especially in temporal and semantic types where all baselines fail significantly. More importantly, Micro-Act exhibits robust performance on non-conflict questions simultaneously, highlighting its practical value in real-world RAG applications.
[06.06.2025 05:13] Response: {
  "desc": "–§—Ä–µ–π–º–≤–æ—Ä–∫ Micro-Act —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤ –∑–Ω–∞–Ω–∏–π –≤ —Å–∏—Å—Ç–µ–º–∞—Ö –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å –¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ–º –∏–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ (RAG). –û–Ω –∞–¥–∞–ø—Ç–∏–≤–Ω–æ —Ä–∞–∑–±–∏–≤–∞–µ—Ç –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∑–Ω–∞–Ω–∏–π –Ω–∞ –º–µ–ª–∫–∏–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è, –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–µ –∫–∞–∫ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –¥–µ–π—Å—Ç–≤–∏–π. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Ä–∞—Å—Å—É–∂–¥–∞—Ç—å –∑–∞ –ø—Ä–µ–¥–µ–ª–∞–º–∏ –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ –ø–æ–≤—ã—à–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏.",
  "emoji": "üß†",
  "title": "Micro-Act: —É–º–Ω–æ–µ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤ –∑–Ω–∞–Ω–∏–π –≤ RAG-—Å–∏—Å—Ç–µ–º–∞—Ö"
}
[06.06.2025 05:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A framework called Micro-Act addresses Knowledge Conflicts in Retrieval-Augmented Generation by adaptively decomposing knowledge sources, leading to improved QA accuracy compared to existing methods.  					AI-generated summary 				 Retrieval-Augmented Generation (RAG) systems commonly suffer from Knowledge Conflicts, where retrieved external knowledge contradicts the inherent, parametric knowledge of large language models (LLMs). It adversely affects performance on downstream tasks such as question answering (QA). Existing approaches often attempt to mitigate conflicts by directly comparing two knowledge sources in a side-by-side manner, but this can overwhelm LLMs with extraneous or lengthy contexts, ultimately hindering their ability to identify and mitigate inconsistencies. To address this issue, we propose Micro-Act a framework with a hierarchical action space that automatically perceives context complexity and adaptively decomposes each knowledge source into a sequence of fine-grained comparisons. These comparisons are represented as actionable steps, enabling reasoning beyond the superficial context. Through extensive experiments on five benchmark datasets, Micro-Act consistently achieves significant increase in QA accuracy over state-of-the-art baselines across all 5 datasets and 3 conflict types, especially in temporal and semantic types where all baselines fail significantly. More importantly, Micro-Act exhibits robust performance on non-conflict questions simultaneously, highlighting its practical value in real-world RAG applications."

[06.06.2025 05:13] Response: ```python
['RAG', 'BENCHMARK']
```
[06.06.2025 05:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A framework called Micro-Act addresses Knowledge Conflicts in Retrieval-Augmented Generation by adaptively decomposing knowledge sources, leading to improved QA accuracy compared to existing methods.  					AI-generated summary 				 Retrieval-Augmented Generation (RAG) systems commonly suffer from Knowledge Conflicts, where retrieved external knowledge contradicts the inherent, parametric knowledge of large language models (LLMs). It adversely affects performance on downstream tasks such as question answering (QA). Existing approaches often attempt to mitigate conflicts by directly comparing two knowledge sources in a side-by-side manner, but this can overwhelm LLMs with extraneous or lengthy contexts, ultimately hindering their ability to identify and mitigate inconsistencies. To address this issue, we propose Micro-Act a framework with a hierarchical action space that automatically perceives context complexity and adaptively decomposes each knowledge source into a sequence of fine-grained comparisons. These comparisons are represented as actionable steps, enabling reasoning beyond the superficial context. Through extensive experiments on five benchmark datasets, Micro-Act consistently achieves significant increase in QA accuracy over state-of-the-art baselines across all 5 datasets and 3 conflict types, especially in temporal and semantic types where all baselines fail significantly. More importantly, Micro-Act exhibits robust performance on non-conflict questions simultaneously, highlighting its practical value in real-world RAG applications."

[06.06.2025 05:13] Response: ```python
["REASONING", "INTERPRETABILITY"]
```
[06.06.2025 05:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The paper introduces Micro-Act, a novel framework designed to tackle Knowledge Conflicts in Retrieval-Augmented Generation (RAG) systems. Knowledge Conflicts occur when external information contradicts the knowledge embedded in large language models, negatively impacting question answering (QA) tasks. Micro-Act improves QA accuracy by adaptively breaking down knowledge sources into manageable comparisons, allowing for deeper reasoning and better conflict resolution. Experimental results demonstrate that Micro-Act outperforms existing methods across various datasets and conflict types, while also maintaining strong performance on non-conflict questions.","title":"Micro-Act: Resolving Knowledge Conflicts for Better QA Accuracy"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='The paper introduces Micro-Act, a novel framework designed to tackle Knowledge Conflicts in Retrieval-Augmented Generation (RAG) systems. Knowledge Conflicts occur when external information contradicts the knowledge embedded in large language models, negatively impacting question answering (QA) tasks. Micro-Act improves QA accuracy by adaptively breaking down knowledge sources into manageable comparisons, allowing for deeper reasoning and better conflict resolution. Experimental results demonstrate that Micro-Act outperforms existing methods across various datasets and conflict types, while also maintaining strong performance on non-conflict questions.', title='Micro-Act: Resolving Knowledge Conflicts for Better QA Accuracy'))
[06.06.2025 05:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Micro-ActÊòØ‰∏Ä‰∏™Êñ∞Ê°ÜÊû∂ÔºåÊó®Âú®Ëß£ÂÜ≥Ê£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàêÔºàRAGÔºâ‰∏≠ÁöÑÁü•ËØÜÂÜ≤Á™ÅÈóÆÈ¢ò„ÄÇÂÆÉÈÄöËøáËá™ÈÄÇÂ∫îÂú∞ÂàÜËß£Áü•ËØÜÊ∫êÔºåÊîπÂñÑ‰∫ÜÈóÆÁ≠îÔºàQAÔºâÁöÑÂáÜÁ°ÆÊÄß„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ï‰∏çÂêåÔºåMicro-ActÈááÁî®ÂàÜÂ±ÇÁöÑË°åÂä®Á©∫Èó¥ÔºåËÉΩÂ§üËá™Âä®ÊÑüÁü•‰∏ä‰∏ãÊñáÁöÑÂ§çÊùÇÊÄßÔºåÂπ∂Â∞ÜÁü•ËØÜÊ∫êÁªÜÂàÜ‰∏∫‰∏ÄÁ≥ªÂàóÁ≤æÁªÜÁöÑÊØîËæÉÊ≠•È™§„ÄÇËøôÁßçÊñπÊ≥ïÂú®‰∫î‰∏™Âü∫ÂáÜÊï∞ÊçÆÈõÜ‰∏äÁöÑÂÆûÈ™å‰∏≠ÊòæÁ§∫Âá∫ÊòæËëóÁöÑQAÂáÜÁ°ÆÊÄßÊèêÂçáÔºåÂ∞§ÂÖ∂Âú®Êó∂Èó¥ÂíåËØ≠‰πâÁ±ªÂûãÁöÑÂÜ≤Á™Å‰∏≠Ë°®Áé∞‰ºòÂºÇ„ÄÇ","title":"Micro-ActÔºöËß£ÂÜ≥Áü•ËØÜÂÜ≤Á™ÅÁöÑÊô∫ËÉΩÊ°ÜÊû∂"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Micro-ActÊòØ‰∏Ä‰∏™Êñ∞Ê°ÜÊû∂ÔºåÊó®Âú®Ëß£ÂÜ≥Ê£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàêÔºàRAGÔºâ‰∏≠ÁöÑÁü•ËØÜÂÜ≤Á™ÅÈóÆÈ¢ò„ÄÇÂÆÉÈÄöËøáËá™ÈÄÇÂ∫îÂú∞ÂàÜËß£Áü•ËØÜÊ∫êÔºåÊîπÂñÑ‰∫ÜÈóÆÁ≠îÔºàQAÔºâÁöÑÂáÜÁ°ÆÊÄß„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ï‰∏çÂêåÔºåMicro-ActÈááÁî®ÂàÜÂ±ÇÁöÑË°åÂä®Á©∫Èó¥ÔºåËÉΩÂ§üËá™Âä®ÊÑüÁü•‰∏ä‰∏ãÊñáÁöÑÂ§çÊùÇÊÄßÔºåÂπ∂Â∞ÜÁü•ËØÜÊ∫êÁªÜÂàÜ‰∏∫‰∏ÄÁ≥ªÂàóÁ≤æÁªÜÁöÑÊØîËæÉÊ≠•È™§„ÄÇËøôÁßçÊñπÊ≥ïÂú®‰∫î‰∏™Âü∫ÂáÜÊï∞ÊçÆÈõÜ‰∏äÁöÑÂÆûÈ™å‰∏≠ÊòæÁ§∫Âá∫ÊòæËëóÁöÑQAÂáÜÁ°ÆÊÄßÊèêÂçáÔºåÂ∞§ÂÖ∂Âú®Êó∂Èó¥ÂíåËØ≠‰πâÁ±ªÂûãÁöÑÂÜ≤Á™Å‰∏≠Ë°®Áé∞‰ºòÂºÇ„ÄÇ', title='Micro-ActÔºöËß£ÂÜ≥Áü•ËØÜÂÜ≤Á™ÅÁöÑÊô∫ËÉΩÊ°ÜÊû∂'))
[06.06.2025 05:13] Loading Chinese text from previous data.
[06.06.2025 05:13] Renaming data file.
[06.06.2025 05:13] Renaming previous data. hf_papers.json to ./d/2025-06-06.json
[06.06.2025 05:13] Saving new data file.
[06.06.2025 05:13] Generating page.
[06.06.2025 05:13] Renaming previous page.
[06.06.2025 05:13] Renaming previous data. index.html to ./d/2025-06-06.html
[06.06.2025 05:13] [Experimental] Generating Chinese page for reading.
[06.06.2025 05:13] Chinese vocab [{'word': 'ÂºÄÊ∫ê', 'pinyin': 'kƒÅi yu√°n', 'trans': 'open source'}, {'word': 'ËßÜËßâ-ËØ≠Ë®ÄÊ®°Âûã', 'pinyin': 'sh√¨ ju√© y«î y√°n m√≥ x√≠ng', 'trans': 'vision-language model'}, {'word': 'Ë°®Áé∞Âá∫Ëâ≤', 'pinyin': 'bi«éo xi√†n ch≈´ s√®', 'trans': 'perform excellently'}, {'word': 'Â§öÊ®°ÊÄÅ', 'pinyin': 'du≈ç m√≥ t√†i', 'trans': 'multimodal'}, {'word': 'Êé®ÁêÜ', 'pinyin': 'tuƒ´ l«ê', 'trans': 'reasoning'}, {'word': 'Ë∂ÖË∂ä', 'pinyin': 'chƒÅo yu√®', 'trans': 'surpass'}, {'word': 'ÂèÇÊï∞Èáè', 'pinyin': 'cƒÅn sh«î li√†ng', 'trans': 'parameter quantity'}, {'word': 'GUI', 'pinyin': 'GUI', 'trans': 'Graphical User Interface'}, {'word': 'Â∫îÁî®', 'pinyin': 'y√¨ng y√≤ng', 'trans': 'application'}, {'word': '‰∏ìÈó®', 'pinyin': 'zhuƒÅn m√©n', 'trans': 'specialized'}, {'word': 'È¢ÑËÆ≠ÁªÉ', 'pinyin': 'y√π x√πn li√†n', 'trans': 'pre-training'}, {'word': 'Ê∑∑Âêà', 'pinyin': 'h√πn h√©', 'trans': 'hybrid'}, {'word': 'Á≠ñÁï•', 'pinyin': 'c√® l√º√®', 'trans': 'strategy'}, {'word': 'Âº∫ÂåñÂ≠¶‰π†', 'pinyin': 'qi√°ng hu√† xu√© x√≠', 'trans': 'reinforcement learning'}, {'word': 'ËØÑ‰º∞', 'pinyin': 'p√≠ng g≈´', 'trans': 'evaluation'}, {'word': 'Â•ó‰ª∂', 'pinyin': 't√†o ji√†n', 'trans': 'suite'}, {'word': 'ÂèØÈáçÂ§çÊÄß', 'pinyin': 'kƒõ ch√≥ng f√π x√¨ng', 'trans': 'reproducibility'}, {'word': 'Êé®Âä®', 'pinyin': 'tuƒ´ d√≤ng', 'trans': 'promote'}, {'word': 'È¢ÜÂüü', 'pinyin': 'l«êng y√π', 'trans': 'field'}, {'word': 'Ê£ÄÊü•ÁÇπ', 'pinyin': 'ji«én ch√° di«én', 'trans': 'checkpoint'}]
[06.06.2025 05:13] Renaming previous Chinese page.
[06.06.2025 05:13] Renaming previous data. zh.html to ./d/2025-06-05_zh_reading_task.html
[06.06.2025 05:13] Writing Chinese reading task.
[06.06.2025 05:13] Writing result.
[06.06.2025 05:13] Renaming log file.
[06.06.2025 05:13] Renaming previous data. log.txt to ./logs/2025-06-06_last_log.txt
