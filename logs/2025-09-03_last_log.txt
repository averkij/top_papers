[03.09.2025 07:12] Read previous papers.
[03.09.2025 07:12] Generating top page (month).
[03.09.2025 07:12] Writing top page (month).
[03.09.2025 08:15] Read previous papers.
[03.09.2025 08:15] Get feed.
[03.09.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2509.02547
[03.09.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2509.01215
[03.09.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2509.00676
[03.09.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2509.02479
[03.09.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2509.02544
[03.09.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2509.01055
[03.09.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2509.02208
[03.09.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2509.02522
[03.09.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2509.01563
[03.09.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2509.01363
[03.09.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2509.02534
[03.09.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2509.02460
[03.09.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2509.01644
[03.09.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2509.02040
[03.09.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2509.01360
[03.09.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2509.01052
[03.09.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2509.00244
[03.09.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2509.01984
[03.09.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2509.02379
[03.09.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2509.02133
[03.09.2025 08:15] Extract page data from URL. URL: https://huggingface.co/papers/2509.02046
[03.09.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2509.01610
[03.09.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2509.01250
[03.09.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2509.00581
[03.09.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2509.00578
[03.09.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2509.00404
[03.09.2025 08:15] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[03.09.2025 08:15] No deleted papers detected.
[03.09.2025 08:15] Downloading and parsing papers (pdf, html). Total: 26.
[03.09.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2509.02547.
[03.09.2025 08:15] Extra JSON file exists (./assets/json/2509.02547.json), skip PDF parsing.
[03.09.2025 08:15] Paper image links file exists (./assets/img_data/2509.02547.json), skip HTML parsing.
[03.09.2025 08:15] Success.
[03.09.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2509.01215.
[03.09.2025 08:15] Extra JSON file exists (./assets/json/2509.01215.json), skip PDF parsing.
[03.09.2025 08:15] Paper image links file exists (./assets/img_data/2509.01215.json), skip HTML parsing.
[03.09.2025 08:15] Success.
[03.09.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2509.00676.
[03.09.2025 08:15] Extra JSON file exists (./assets/json/2509.00676.json), skip PDF parsing.
[03.09.2025 08:15] Paper image links file exists (./assets/img_data/2509.00676.json), skip HTML parsing.
[03.09.2025 08:15] Success.
[03.09.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2509.02479.
[03.09.2025 08:15] Extra JSON file exists (./assets/json/2509.02479.json), skip PDF parsing.
[03.09.2025 08:15] Paper image links file exists (./assets/img_data/2509.02479.json), skip HTML parsing.
[03.09.2025 08:15] Success.
[03.09.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2509.02544.
[03.09.2025 08:15] Extra JSON file exists (./assets/json/2509.02544.json), skip PDF parsing.
[03.09.2025 08:15] Paper image links file exists (./assets/img_data/2509.02544.json), skip HTML parsing.
[03.09.2025 08:15] Success.
[03.09.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2509.01055.
[03.09.2025 08:15] Extra JSON file exists (./assets/json/2509.01055.json), skip PDF parsing.
[03.09.2025 08:15] Paper image links file exists (./assets/img_data/2509.01055.json), skip HTML parsing.
[03.09.2025 08:15] Success.
[03.09.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2509.02208.
[03.09.2025 08:15] Extra JSON file exists (./assets/json/2509.02208.json), skip PDF parsing.
[03.09.2025 08:15] Paper image links file exists (./assets/img_data/2509.02208.json), skip HTML parsing.
[03.09.2025 08:15] Success.
[03.09.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2509.02522.
[03.09.2025 08:15] Extra JSON file exists (./assets/json/2509.02522.json), skip PDF parsing.
[03.09.2025 08:15] Paper image links file exists (./assets/img_data/2509.02522.json), skip HTML parsing.
[03.09.2025 08:15] Success.
[03.09.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2509.01563.
[03.09.2025 08:15] Extra JSON file exists (./assets/json/2509.01563.json), skip PDF parsing.
[03.09.2025 08:15] Paper image links file exists (./assets/img_data/2509.01563.json), skip HTML parsing.
[03.09.2025 08:15] Success.
[03.09.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2509.01363.
[03.09.2025 08:15] Extra JSON file exists (./assets/json/2509.01363.json), skip PDF parsing.
[03.09.2025 08:15] Paper image links file exists (./assets/img_data/2509.01363.json), skip HTML parsing.
[03.09.2025 08:15] Success.
[03.09.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2509.02534.
[03.09.2025 08:15] Extra JSON file exists (./assets/json/2509.02534.json), skip PDF parsing.
[03.09.2025 08:15] Paper image links file exists (./assets/img_data/2509.02534.json), skip HTML parsing.
[03.09.2025 08:15] Success.
[03.09.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2509.02460.
[03.09.2025 08:15] Extra JSON file exists (./assets/json/2509.02460.json), skip PDF parsing.
[03.09.2025 08:15] Paper image links file exists (./assets/img_data/2509.02460.json), skip HTML parsing.
[03.09.2025 08:15] Success.
[03.09.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2509.01644.
[03.09.2025 08:15] Extra JSON file exists (./assets/json/2509.01644.json), skip PDF parsing.
[03.09.2025 08:15] Paper image links file exists (./assets/img_data/2509.01644.json), skip HTML parsing.
[03.09.2025 08:15] Success.
[03.09.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2509.02040.
[03.09.2025 08:15] Extra JSON file exists (./assets/json/2509.02040.json), skip PDF parsing.
[03.09.2025 08:15] Paper image links file exists (./assets/img_data/2509.02040.json), skip HTML parsing.
[03.09.2025 08:15] Success.
[03.09.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2509.01360.
[03.09.2025 08:15] Extra JSON file exists (./assets/json/2509.01360.json), skip PDF parsing.
[03.09.2025 08:15] Paper image links file exists (./assets/img_data/2509.01360.json), skip HTML parsing.
[03.09.2025 08:15] Success.
[03.09.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2509.01052.
[03.09.2025 08:15] Extra JSON file exists (./assets/json/2509.01052.json), skip PDF parsing.
[03.09.2025 08:15] Paper image links file exists (./assets/img_data/2509.01052.json), skip HTML parsing.
[03.09.2025 08:15] Success.
[03.09.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2509.00244.
[03.09.2025 08:15] Extra JSON file exists (./assets/json/2509.00244.json), skip PDF parsing.
[03.09.2025 08:15] Paper image links file exists (./assets/img_data/2509.00244.json), skip HTML parsing.
[03.09.2025 08:15] Success.
[03.09.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2509.01984.
[03.09.2025 08:15] Extra JSON file exists (./assets/json/2509.01984.json), skip PDF parsing.
[03.09.2025 08:15] Paper image links file exists (./assets/img_data/2509.01984.json), skip HTML parsing.
[03.09.2025 08:15] Success.
[03.09.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2509.02379.
[03.09.2025 08:15] Extra JSON file exists (./assets/json/2509.02379.json), skip PDF parsing.
[03.09.2025 08:15] Paper image links file exists (./assets/img_data/2509.02379.json), skip HTML parsing.
[03.09.2025 08:15] Success.
[03.09.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2509.02133.
[03.09.2025 08:15] Extra JSON file exists (./assets/json/2509.02133.json), skip PDF parsing.
[03.09.2025 08:15] Paper image links file exists (./assets/img_data/2509.02133.json), skip HTML parsing.
[03.09.2025 08:15] Success.
[03.09.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2509.02046.
[03.09.2025 08:15] Downloading paper 2509.02046 from http://arxiv.org/pdf/2509.02046v1...
[03.09.2025 08:15] Extracting affiliations from text.
[03.09.2025 08:15] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 2 ] . [ 1 6 4 0 2 0 . 9 0 5 2 : r a Kaiyue Wen Stanford University kaiyuew@stanford.edu David Hall Stanford University dlwh@cs.stanford.edu Tengyu Ma Stanford University tengyuma@stanford.edu Percy Liang Stanford University pliang@cs.stanford.edu September 3, 2025 Abstract AdamW has long been the dominant optimizer in language model pretraining, despite numerous claims that alternative optimizers offer 1.4 to 2 speedup. We posit that two methodological shortcomings have obscured fair comparisons and hindered practical adoption: (i) unequal hyperparameter tuning and (ii) limited or misleading evaluation setups. To address these two issues, we conduct systematic study of ten deep learning optimizers across four model scales (0.1B-1.2B parameters) and data-to-model ratios (18 the Chinchilla optimum). We find that fair and informative comparisons require rigorous hyperparameter tuning and evaluations across range of model scales and data-to-model ratios, performed at the end of training. First, optimal hyperparameters for one optimizer may be suboptimal for another, making blind hyperparameter transfer unfair. Second, the actual speedup of many proposed optimizers over well-tuned baselines is lower than claimed and decreases with model size to only 1.1 for 1.2B parameter models. Thirdly, comparing intermediate checkpoints before reaching the target training budgets can be misleading, as rankings between two optimizers can flip during training due to learning rate decay. Through our thorough investigation, we find that all the fastest optimizers such as Muon and Soap, use matrices as preconditioners multiplying gradients with matrices rather than entry-wise scalars. However, the speedup of matrix-based optimizers is inversely proportional to model scale, decreasing from 1.4 over AdamW for 0.1B parameter models to merely 1.1 for 1.2B parameter models. Pretraining has been the most computationally expensive component in the training pipeline for large language mode"
[03.09.2025 08:15] Response: ```python
["Stanford University"]
```
[03.09.2025 08:15] Deleting PDF ./assets/pdf/2509.02046.pdf.
[03.09.2025 08:15] Success.
[03.09.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2509.01610.
[03.09.2025 08:15] Extra JSON file exists (./assets/json/2509.01610.json), skip PDF parsing.
[03.09.2025 08:15] Paper image links file exists (./assets/img_data/2509.01610.json), skip HTML parsing.
[03.09.2025 08:15] Success.
[03.09.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2509.01250.
[03.09.2025 08:15] Extra JSON file exists (./assets/json/2509.01250.json), skip PDF parsing.
[03.09.2025 08:15] Paper image links file exists (./assets/img_data/2509.01250.json), skip HTML parsing.
[03.09.2025 08:15] Success.
[03.09.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2509.00581.
[03.09.2025 08:15] Extra JSON file exists (./assets/json/2509.00581.json), skip PDF parsing.
[03.09.2025 08:15] Paper image links file exists (./assets/img_data/2509.00581.json), skip HTML parsing.
[03.09.2025 08:15] Success.
[03.09.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2509.00578.
[03.09.2025 08:15] Extra JSON file exists (./assets/json/2509.00578.json), skip PDF parsing.
[03.09.2025 08:15] Paper image links file exists (./assets/img_data/2509.00578.json), skip HTML parsing.
[03.09.2025 08:15] Success.
[03.09.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2509.00404.
[03.09.2025 08:15] Extra JSON file exists (./assets/json/2509.00404.json), skip PDF parsing.
[03.09.2025 08:15] Paper image links file exists (./assets/img_data/2509.00404.json), skip HTML parsing.
[03.09.2025 08:15] Success.
[03.09.2025 08:15] Enriching papers with extra data.
[03.09.2025 08:15] ********************************************************************************
[03.09.2025 08:15] Abstract 0. Agentic reinforcement learning transforms large language models into autonomous decision-making agents by leveraging temporally extended POMDPs, enhancing capabilities like planning and reasoning through reinforcement learning.  					AI-generated summary 				 The emergence of agentic reinforcement l...
[03.09.2025 08:15] ********************************************************************************
[03.09.2025 08:15] Abstract 1. A framework for constructing high-quality document extraction datasets and models through synthetic data generation and iterative self-improvement outperforms existing models.  					AI-generated summary 				 High-quality labeled data is essential for training accurate document conversion models, par...
[03.09.2025 08:15] ********************************************************************************
[03.09.2025 08:15] Abstract 2. Reinforcement learning on preference-labeled critic datasets enhances a generative model's performance, creating a unified multimodal system that excels in both evaluation and generation.  					AI-generated summary 				 In vision-language modeling, critic models are typically trained to evaluate out...
[03.09.2025 08:15] ********************************************************************************
[03.09.2025 08:15] Abstract 3. SimpleTIR stabilizes multi-turn Tool-Integrated Reasoning training by filtering out void turns, achieving state-of-the-art performance on math reasoning benchmarks.  					AI-generated summary 				 Large Language Models (LLMs) can significantly improve their reasoning capabilities by interacting with...
[03.09.2025 08:15] ********************************************************************************
[03.09.2025 08:15] Abstract 4. UI-TARS-2, a native GUI-centered agent model, addresses challenges in data scalability, multi-turn reinforcement learning, and environment stability, achieving significant improvements over its predecessor and strong baselines across various benchmarks.  					AI-generated summary 				 The developmen...
[03.09.2025 08:15] ********************************************************************************
[03.09.2025 08:15] Abstract 5. VerlTool is a unified and modular framework for Agentic Reinforcement Learning with Tool use, addressing inefficiencies in existing approaches and providing competitive performance across multiple domains.  					AI-generated summary 				 Reinforcement Learning with Verifiable Rewards (RLVR) has demo...
[03.09.2025 08:15] ********************************************************************************
[03.09.2025 08:15] Abstract 6. A dynamic verification framework using reinforcement learning and a novel algorithm improves the performance of a large medical language model in real-world clinical decision-making.  					AI-generated summary 				 As large language models (LLMs) advance in conversational and reasoning capabilities,...
[03.09.2025 08:15] ********************************************************************************
[03.09.2025 08:15] Abstract 7. PACS, a novel RLVR framework, reformulates RLVR as a supervised learning task, improving stability and efficiency in training large language models for reasoning tasks.  					AI-generated summary 				 Recent advances in Reinforcement Learning with Verifiable Rewards (RLVR) have empowered large langu...
[03.09.2025 08:15] ********************************************************************************
[03.09.2025 08:15] Abstract 8. Keye-VL-1.5 enhances video understanding through a Slow-Fast encoding strategy, progressive pre-training, and post-training reasoning improvements, outperforming existing models on video tasks while maintaining general multimodal performance.  					AI-generated summary 				 In recent years, the deve...
[03.09.2025 08:15] ********************************************************************************
[03.09.2025 08:15] Abstract 9. Reasoning capabilities from reinforcement learning can be extracted as a task vector and transferred to other models to improve performance on diverse benchmarks.  					AI-generated summary 				 Large language models often require costly optimization, such as reinforcement learning, to master comple...
[03.09.2025 08:15] ********************************************************************************
[03.09.2025 08:15] Abstract 10. DARLING, a diversity-aware reinforcement learning framework, enhances both the quality and diversity of large language model outputs across various tasks.  					AI-generated summary 				 Post-training of Large Language Models (LMs) often prioritizes accuracy and helpfulness at the expense of diversi...
[03.09.2025 08:15] ********************************************************************************
[03.09.2025 08:15] Abstract 11. A novel Diffusion Transformer pipeline automates video compositing by adaptively injecting identity and motion information, maintaining consistency and enabling user customization.  					AI-generated summary 				 Video compositing combines live-action footage to create video production, serving as a...
[03.09.2025 08:15] ********************************************************************************
[03.09.2025 08:15] Abstract 12. OpenVision 2 simplifies the original architecture by removing the text encoder and contrastive loss, achieving competitive performance with reduced training time and memory usage, and enabling scaling to over 1 billion parameters.  					AI-generated summary 				 This paper provides a simplification ...
[03.09.2025 08:15] ********************************************************************************
[03.09.2025 08:15] Abstract 13. Genetic Prompt enhances synthetic data quality and diversity in NLP by combining genetic algorithms with LLMs, improving downstream model performance.  					AI-generated summary 				 Large Language Models (LLMs) excel at generating synthetic data, but ensuring its quality and diversity remains chall...
[03.09.2025 08:15] ********************************************************************************
[03.09.2025 08:15] Abstract 14. M3Ret, a unified visual encoder trained on a large-scale hybrid-modality dataset, achieves state-of-the-art zero-shot image-to-image retrieval and cross-modal alignment using generative and contrastive self-supervised learning paradigms.  					AI-generated summary 				 Medical image retrieval is ess...
[03.09.2025 08:15] ********************************************************************************
[03.09.2025 08:15] Abstract 15. FlashAdventure benchmark and COAST framework improve GUI agents' performance in completing full story arcs in Flash-based adventure games by addressing the observation-behavior gap.  					AI-generated summary 				 GUI agents powered by LLMs show promise in interacting with diverse digital environmen...
[03.09.2025 08:15] ********************************************************************************
[03.09.2025 08:15] Abstract 16. Universal Deep Research (UDR) is a flexible system that allows users to customize deep research strategies using any language model without additional training or fine-tuning.  					AI-generated summary 				 Deep research tools are among the most impactful and most commonly encountered agentic syste...
[03.09.2025 08:15] ********************************************************************************
[03.09.2025 08:15] Abstract 17. VARIN, a noise inversion-based editing technique for visual autoregressive models, enables precise image editing aligned with textual prompts while preserving original details.  					AI-generated summary 				 Visual autoregressive models (VAR) have recently emerged as a promising class of generative...
[03.09.2025 08:15] ********************************************************************************
[03.09.2025 08:15] Abstract 18. MedDINOv3, a framework adapting DINOv3 with multi-scale token aggregation, achieves state-of-the-art performance in medical image segmentation by overcoming challenges in domain adaptation and backbone performance.  					AI-generated summary 				 Accurate segmentation of organs and tumors in CT and ...
[03.09.2025 08:15] ********************************************************************************
[03.09.2025 08:15] Abstract 19. Ambekar framework uses a Constitution-Aware Decoding Layer to mitigate caste and religious biases in LLM outputs through speculative decoding without retraining.  					AI-generated summary 				 Large Language Models (LLMs) can inadvertently reflect societal biases present in their training data, lea...
[03.09.2025 08:15] ********************************************************************************
[03.09.2025 08:15] Abstract 20. A systematic study reveals that fair comparisons of deep learning optimizers require rigorous hyperparameter tuning and evaluations across various model scales and data-to-model ratios, showing that matrix-based optimizers like Muon and Soap offer diminishing speedups as model size increases.  					...
[03.09.2025 08:15] ********************************************************************************
[03.09.2025 08:15] Abstract 21. A Panel-of-Peers learning framework enhances Large Vision and Language Models by simulating peer reviews, improving performance without extensive human-labeled datasets.  					AI-generated summary 				 Traditional alignment methods for Large Vision and Language Models (LVLMs) primarily rely on human...
[03.09.2025 08:15] ********************************************************************************
[03.09.2025 08:15] Abstract 22. Point-PQAE, a two-view cross-reconstruction generative paradigm, enhances 3D self-supervised learning by introducing greater diversity and variance, outperforming single-view methods in point cloud reconstruction tasks.  					AI-generated summary 				 Point cloud learning, especially in a self-super...
[03.09.2025 08:15] ********************************************************************************
[03.09.2025 08:15] Abstract 23. A multi-agent framework decomposes the Text2SQL task into several components, using in-context learning and taxonomy-guided error modification to achieve state-of-the-art results.  					AI-generated summary 				 Converting natural language queries into SQL queries is a crucial challenge in both indu...
[03.09.2025 08:15] ********************************************************************************
[03.09.2025 08:15] Abstract 24. Context-Aware Fusion enhances DiffusionDet by integrating global scene context with local features using cross-attention, improving performance on fine-grained object detection tasks.  					AI-generated summary 				 Fine-grained object detection in challenging visual domains, such as vehicle damage ...
[03.09.2025 08:15] ********************************************************************************
[03.09.2025 08:15] Abstract 25. Metis addresses training instability in low-bit quantized large language models by using spectral decomposition, adaptive learning rates, and dual-range regularization to improve performance and stability.  					AI-generated summary 				 This work identifies anisotropic parameter distributions as a ...
[03.09.2025 08:15] Read previous papers.
[03.09.2025 08:15] Generating reviews via LLM API.
[03.09.2025 08:15] Using data from previous issue: {"categories": ["#benchmark", "#agents", "#reasoning", "#agi", "#survey", "#rl", "#open_source"], "emoji": "🤖", "ru": {"title": "Большие языковые модели становятся автономными агентами", "desc": "Агентное обучение с подкреплением (Agentic RL) трансформирует большие языковые модели в автономных агент
[03.09.2025 08:15] Using data from previous issue: {"categories": ["#dataset", "#data", "#optimization", "#synthetic", "#training"], "emoji": "📄", "ru": {"title": "Автоматическое создание высококачественных моделей для извлечения данных из документов", "desc": "Статья представляет новый фреймворк для создания высококачественных наборов данных и моде
[03.09.2025 08:15] Using data from previous issue: {"categories": ["#optimization", "#games", "#rlhf", "#multimodal", "#benchmark", "#reasoning", "#rl"], "emoji": "🤖", "ru": {"title": "Объединение критика и генератора: новый шаг в мультимодальном ИИ", "desc": "В этой статье предлагается новый подход к обучению мультимодальных языковых моделей, объед
[03.09.2025 08:15] Using data from previous issue: {"categories": ["#benchmark", "#reasoning", "#math", "#optimization", "#rl", "#training"], "emoji": "🧠", "ru": {"title": "Стабильное обучение AI рассуждать с инструментами", "desc": "Статья представляет алгоритм SimpleTIR, стабилизирующий обучение многоходовых моделей с интегрированными инструментам
[03.09.2025 08:15] Using data from previous issue: {"categories": ["#optimization", "#games", "#benchmark", "#training", "#reasoning", "#agents", "#rl"], "emoji": "🤖", "ru": {"title": "UI-TARS-2: Новый уровень GUI-агентов с улучшенным обучением и обобщением", "desc": "UI-TARS-2 - это модель агента для графических пользовательских интерфейсов, решающ
[03.09.2025 08:15] Using data from previous issue: {"categories": ["#architecture", "#rl", "#open_source", "#agi", "#agents", "#reasoning", "#rlhf", "#training", "#multimodal"], "emoji": "🛠️", "ru": {"title": "VerlTool: Универсальный фреймворк для обучения с подкреплением с использованием инструментов", "desc": "VerlTool - это унифицированный и моду
[03.09.2025 08:15] Using data from previous issue: {"categories": ["#benchmark", "#optimization", "#rl", "#open_source", "#reasoning", "#agents", "#alignment", "#training", "#healthcare"], "emoji": "🩺", "ru": {"title": "Динамическая верификация LLM для реальной клинической практики", "desc": "Эта статья представляет новую динамическую систему верифи
[03.09.2025 08:15] Using data from previous issue: {"categories": ["#benchmark", "#reasoning", "#optimization", "#rl", "#training", "#open_source"], "emoji": "🧠", "ru": {"title": "PACS: Эффективное обучение языковых моделей рассуждению через супервизорный RLVR", "desc": "PACS - это новый фреймворк для обучения с подкреплением с проверяемыми наградам
[03.09.2025 08:15] Using data from previous issue: {"categories": ["#multimodal", "#reasoning", "#video", "#long_context", "#rl", "#training", "#alignment"], "emoji": "🎥", "ru": {"title": "Революция в понимании видео: Keye-VL-1.5 объединяет эффективность и точность", "desc": "Keye-VL-1.5 - это новая модель машинного обучения для понимания видео. Она
[03.09.2025 08:15] Using data from previous issue: {"categories": ["#benchmark", "#training", "#rl", "#open_source", "#transfer_learning", "#optimization", "#reasoning"], "emoji": "🧠", "ru": {"title": "Передача способностей к рассуждению между языковыми моделями", "desc": "Исследование показывает, что способности к рассуждению, полученные с помощью 
[03.09.2025 08:15] Using data from previous issue: {"categories": ["#games", "#story_generation", "#rlhf", "#optimization", "#rl", "#training"], "emoji": "🌈", "ru": {"title": "DARLING: Качество и разнообразие в гармонии", "desc": "DARLING - это фреймворк обучения с подкреплением, который улучшает как качество, так и разнообразие выходных данных боль
[03.09.2025 08:15] Using data from previous issue: {"categories": ["#diffusion", "#dataset", "#video"], "emoji": "🎬", "ru": {"title": "Генеративный видеокомпозитинг: новый уровень автоматизации в производстве видео", "desc": "Статья представляет новый подход к автоматизации видеокомпозитинга с использованием генеративных моделей. Авторы разработали 
[03.09.2025 08:15] Using data from previous issue: {"categories": ["#optimization", "#multimodal", "#training", "#architecture"], "emoji": "🚀", "ru": {"title": "Упрощение архитектуры для эффективного обучения мультимодальных моделей", "desc": "OpenVision 2 представляет собой упрощенную версию архитектуры OpenVision, в которой удален текстовый энкоде
[03.09.2025 08:15] Using data from previous issue: {"categories": ["#optimization", "#data", "#synthetic", "#training", "#dataset"], "emoji": "🧬", "ru": {"title": "Генетический подход к созданию качественных синтетических данных для NLP", "desc": "Genetic Prompt - это новый метод улучшения качества и разнообразия синтетических данных в обработке ест
[03.09.2025 08:15] Using data from previous issue: {"categories": ["#cv", "#multimodal", "#dataset", "#transfer_learning", "#optimization", "#healthcare"], "emoji": "🏥", "ru": {"title": "Единый энкодер для мультимодальных медицинских изображений", "desc": "M3Ret - это унифицированный визуальный энкодер, обученный на крупномасштабном наборе данных с 
[03.09.2025 08:15] Using data from previous issue: {"categories": ["#benchmark", "#agents", "#optimization", "#games"], "emoji": "🕹️", "ru": {"title": "Преодолевая разрыв между наблюдением и действием в играх-квестах", "desc": "В статье представлен новый бенчмарк FlashAdventure для оценки агентов с графическим интерфейсом в Flash-играх жанра квест. 
[03.09.2025 08:15] Using data from previous issue: {"categories": ["#agents"], "emoji": "🔍", "ru": {"title": "Универсальный инструмент для настройки стратегий глубокого исследования", "desc": "Universal Deep Research (UDR) - это гибкая система, позволяющая пользователям настраивать стратегии глубокого исследования с использованием любой языковой мод
[03.09.2025 08:15] Using data from previous issue: {"categories": ["#optimization", "#cv", "#multimodal", "#diffusion"], "emoji": "🖌️", "ru": {"title": "VARIN: Точное редактирование изображений с помощью инверсии шума для VAR-моделей", "desc": "VARIN - это новый метод редактирования изображений для визуальных авторегрессионных моделей (VAR). Он испо
[03.09.2025 08:15] Using data from previous issue: {"categories": ["#cv", "#benchmark", "#dataset", "#architecture", "#transfer_learning", "#optimization", "#healthcare"], "emoji": "🏥", "ru": {"title": "MedDINOv3: Универсальный сегментатор медицинских изображений на основе фундаментальных моделей", "desc": "MedDINOv3 - это новая архитектура для сегм
[03.09.2025 08:15] Using data from previous issue: {"categories": ["#data", "#multilingual", "#ethics", "#open_source", "#alignment", "#inference"], "emoji": "🇮🇳", "ru": {"title": "Конституционное декодирование для справедливых языковых моделей", "desc": "Фреймворк AMBEDKAR предлагает новый подход к снижению предвзятости в выводах больших языковых м
[03.09.2025 08:15] Querying the API.
[03.09.2025 08:15] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A systematic study reveals that fair comparisons of deep learning optimizers require rigorous hyperparameter tuning and evaluations across various model scales and data-to-model ratios, showing that matrix-based optimizers like Muon and Soap offer diminishing speedups as model size increases.  					AI-generated summary 				 AdamW has long been the dominant optimizer in language model pretraining, despite numerous claims that alternative optimizers offer 1.4 to 2x speedup. We posit that two methodological shortcomings have obscured fair comparisons and hindered practical adoption: (i) unequal hyperparameter tuning and (ii) limited or misleading evaluation setups. To address these two issues, we conduct a systematic study of ten deep learning optimizers across four model scales (0.1B-1.2B parameters) and data-to-model ratios (1-8x the Chinchilla optimum). We find that fair and informative comparisons require rigorous hyperparameter tuning and evaluations across a range of model scales and data-to-model ratios, performed at the end of training. First, optimal hyperparameters for one optimizer may be suboptimal for another, making blind hyperparameter transfer unfair. Second, the actual speedup of many proposed optimizers over well-tuned baselines is lower than claimed and decreases with model size to only 1.1x for 1.2B parameter models. Thirdly, comparing intermediate checkpoints before reaching the target training budgets can be misleading, as rankings between two optimizers can flip during training due to learning rate decay. Through our thorough investigation, we find that all the fastest optimizers such as Muon and Soap, use matrices as preconditioners -- multiplying gradients with matrices rather than entry-wise scalars. However, the speedup of matrix-based optimizers is inversely proportional to model scale, decreasing from 1.4x over AdamW for 0.1B parameter models to merely 1.1x for 1.2B parameter models.
[03.09.2025 08:15] Response: {
  "desc": "Исследование показывает, что для справедливого сравнения оптимизаторов глубокого обучения необходима тщательная настройка гиперпараметров и оценка на различных масштабах моделей и соотношениях данных к модели. Выявлено, что матричные оптимизаторы, такие как Muon и Soap, дают уменьшающееся ускорение с ростом размера модели. Оптимальные гиперпараметры для одного оптимизатора могут быть неоптимальными для другого, что делает слепой перенос гиперпараметров некорректным. Исследование также показало, что сравнение промежуточных чекпоинтов может быть misleading, так как рейтинги оптимизаторов могут меняться в процессе обучения из-за decay скорости обучения.",
  "emoji": "🔬",
  "title": "Справедливое сравнение оптимизаторов требует тщательного анализа"
}
[03.09.2025 08:15] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A systematic study reveals that fair comparisons of deep learning optimizers require rigorous hyperparameter tuning and evaluations across various model scales and data-to-model ratios, showing that matrix-based optimizers like Muon and Soap offer diminishing speedups as model size increases.  					AI-generated summary 				 AdamW has long been the dominant optimizer in language model pretraining, despite numerous claims that alternative optimizers offer 1.4 to 2x speedup. We posit that two methodological shortcomings have obscured fair comparisons and hindered practical adoption: (i) unequal hyperparameter tuning and (ii) limited or misleading evaluation setups. To address these two issues, we conduct a systematic study of ten deep learning optimizers across four model scales (0.1B-1.2B parameters) and data-to-model ratios (1-8x the Chinchilla optimum). We find that fair and informative comparisons require rigorous hyperparameter tuning and evaluations across a range of model scales and data-to-model ratios, performed at the end of training. First, optimal hyperparameters for one optimizer may be suboptimal for another, making blind hyperparameter transfer unfair. Second, the actual speedup of many proposed optimizers over well-tuned baselines is lower than claimed and decreases with model size to only 1.1x for 1.2B parameter models. Thirdly, comparing intermediate checkpoints before reaching the target training budgets can be misleading, as rankings between two optimizers can flip during training due to learning rate decay. Through our thorough investigation, we find that all the fastest optimizers such as Muon and Soap, use matrices as preconditioners -- multiplying gradients with matrices rather than entry-wise scalars. However, the speedup of matrix-based optimizers is inversely proportional to model scale, decreasing from 1.4x over AdamW for 0.1B parameter models to merely 1.1x for 1.2B parameter models."

[03.09.2025 08:15] Response: ```python
['TRAINING']
```
[03.09.2025 08:15] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A systematic study reveals that fair comparisons of deep learning optimizers require rigorous hyperparameter tuning and evaluations across various model scales and data-to-model ratios, showing that matrix-based optimizers like Muon and Soap offer diminishing speedups as model size increases.  					AI-generated summary 				 AdamW has long been the dominant optimizer in language model pretraining, despite numerous claims that alternative optimizers offer 1.4 to 2x speedup. We posit that two methodological shortcomings have obscured fair comparisons and hindered practical adoption: (i) unequal hyperparameter tuning and (ii) limited or misleading evaluation setups. To address these two issues, we conduct a systematic study of ten deep learning optimizers across four model scales (0.1B-1.2B parameters) and data-to-model ratios (1-8x the Chinchilla optimum). We find that fair and informative comparisons require rigorous hyperparameter tuning and evaluations across a range of model scales and data-to-model ratios, performed at the end of training. First, optimal hyperparameters for one optimizer may be suboptimal for another, making blind hyperparameter transfer unfair. Second, the actual speedup of many proposed optimizers over well-tuned baselines is lower than claimed and decreases with model size to only 1.1x for 1.2B parameter models. Thirdly, comparing intermediate checkpoints before reaching the target training budgets can be misleading, as rankings between two optimizers can flip during training due to learning rate decay. Through our thorough investigation, we find that all the fastest optimizers such as Muon and Soap, use matrices as preconditioners -- multiplying gradients with matrices rather than entry-wise scalars. However, the speedup of matrix-based optimizers is inversely proportional to model scale, decreasing from 1.4x over AdamW for 0.1B parameter models to merely 1.1x for 1.2B parameter models."

[03.09.2025 08:15] Response: ```python
["OPTIMIZATION"]
```
[03.09.2025 08:15] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper investigates the effectiveness of various deep learning optimizers, particularly focusing on the popular AdamW. It highlights that fair comparisons of optimizers require careful hyperparameter tuning and evaluations across different model sizes and data ratios. The study reveals that matrix-based optimizers like Muon and Soap show diminishing returns in speedup as model size increases, with their advantages decreasing significantly for larger models. Ultimately, the findings suggest that many claims of speedup for alternative optimizers may be overstated, emphasizing the need for rigorous evaluation methods.","title":"Fair Comparisons of Deep Learning Optimizers: Tuning Matters!"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper investigates the effectiveness of various deep learning optimizers, particularly focusing on the popular AdamW. It highlights that fair comparisons of optimizers require careful hyperparameter tuning and evaluations across different model sizes and data ratios. The study reveals that matrix-based optimizers like Muon and Soap show diminishing returns in speedup as model size increases, with their advantages decreasing significantly for larger models. Ultimately, the findings suggest that many claims of speedup for alternative optimizers may be overstated, emphasizing the need for rigorous evaluation methods.', title='Fair Comparisons of Deep Learning Optimizers: Tuning Matters!'))
[03.09.2025 08:15] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本研究系统地探讨了深度学习优化器的公平比较，强调了超参数调优和模型规模、数据与模型比例的评估的重要性。我们发现，矩阵基础的优化器如Muon和Soap在模型规模增大时，速度提升逐渐减小。研究表明，盲目转移超参数会导致不公平的比较，而在训练结束时进行的严格评估才能提供真实的性能对比。最终结果显示，许多声称的速度提升在实际应用中往往低于预期，尤其是在大规模模型中。","title":"公平比较深度学习优化器的关键在于超参数调优"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本研究系统地探讨了深度学习优化器的公平比较，强调了超参数调优和模型规模、数据与模型比例的评估的重要性。我们发现，矩阵基础的优化器如Muon和Soap在模型规模增大时，速度提升逐渐减小。研究表明，盲目转移超参数会导致不公平的比较，而在训练结束时进行的严格评估才能提供真实的性能对比。最终结果显示，许多声称的速度提升在实际应用中往往低于预期，尤其是在大规模模型中。', title='公平比较深度学习优化器的关键在于超参数调优'))
[03.09.2025 08:15] Using data from previous issue: {"categories": ["#benchmark", "#multimodal", "#rlhf", "#hallucinations", "#alignment"], "emoji": "👥", "ru": {"title": "Коллективное обучение ИИ: имитация процесса рецензирования для улучшения языково-визуальных моделей", "desc": "Предложена новая методика обучения больших языково-визуальных моделей 
[03.09.2025 08:15] Using data from previous issue: {"categories": ["#optimization", "#3d", "#synthetic"], "emoji": "🔍", "ru": {"title": "Двухракурсное самообучение улучшает 3D реконструкцию облаков точек", "desc": "Статья представляет новый метод самообучения для трехмерных облаков точек под названием Point-PQAE. Этот подход использует двухракурсную
[03.09.2025 08:15] Using data from previous issue: {"categories": ["#data", "#optimization", "#reasoning", "#agents", "#dataset"], "emoji": "🤖", "ru": {"title": "Мультиагентный подход с обучением в контексте революционизирует Text2SQL", "desc": "Статья представляет новый подход к решению задачи Text2SQL, используя мультиагентную систему. Авторы пред
[03.09.2025 08:15] Using data from previous issue: {"categories": ["#diffusion", "#benchmark", "#architecture", "#optimization", "#cv"], "emoji": "🔍", "ru": {"title": "Контекстное слияние для точной детекции мелких объектов", "desc": "Статья представляет новый метод Context-Aware Fusion (CAF) для улучшения модели DiffusionDet в задаче детекции мелки
[03.09.2025 08:15] Using data from previous issue: {"categories": ["#low_resource", "#inference", "#training", "#optimization"], "emoji": "🧠", "ru": {"title": "Эффективное обучение LLM с низкой битностью", "desc": "Статья представляет Metis - фреймворк для обучения больших языковых моделей (LLM) с низкобитной квантизацией. Метод использует спектраль
[03.09.2025 08:15] Renaming data file.
[03.09.2025 08:15] Renaming previous data. hf_papers.json to ./d/2025-09-03.json
[03.09.2025 08:15] Saving new data file.
[03.09.2025 08:15] Generating page.
[03.09.2025 08:15] Renaming previous page.
[03.09.2025 08:15] Renaming previous data. index.html to ./d/2025-09-03.html
[03.09.2025 08:15] Writing result.
[03.09.2025 08:15] Renaming log file.
[03.09.2025 08:15] Renaming previous data. log.txt to ./logs/2025-09-03_last_log.txt
