[03.09.2025 12:21] Read previous papers.
[03.09.2025 12:21] Generating top page (month).
[03.09.2025 12:21] Writing top page (month).
[03.09.2025 13:22] Read previous papers.
[03.09.2025 13:22] Get feed.
[03.09.2025 13:22] Get page data from previous paper. URL: https://huggingface.co/papers/2509.02547
[03.09.2025 13:22] Get page data from previous paper. URL: https://huggingface.co/papers/2509.02479
[03.09.2025 13:22] Get page data from previous paper. URL: https://huggingface.co/papers/2509.02544
[03.09.2025 13:22] Get page data from previous paper. URL: https://huggingface.co/papers/2508.21496
[03.09.2025 13:22] Get page data from previous paper. URL: https://huggingface.co/papers/2509.00676
[03.09.2025 13:22] Get page data from previous paper. URL: https://huggingface.co/papers/2509.01055
[03.09.2025 13:22] Get page data from previous paper. URL: https://huggingface.co/papers/2509.01215
[03.09.2025 13:22] Get page data from previous paper. URL: https://huggingface.co/papers/2509.02208
[03.09.2025 13:22] Get page data from previous paper. URL: https://huggingface.co/papers/2509.01563
[03.09.2025 13:22] Get page data from previous paper. URL: https://huggingface.co/papers/2509.02522
[03.09.2025 13:22] Get page data from previous paper. URL: https://huggingface.co/papers/2509.02333
[03.09.2025 13:22] Get page data from previous paper. URL: https://huggingface.co/papers/2509.01363
[03.09.2025 13:22] Get page data from previous paper. URL: https://huggingface.co/papers/2509.02460
[03.09.2025 13:22] Get page data from previous paper. URL: https://huggingface.co/papers/2509.01644
[03.09.2025 13:22] Get page data from previous paper. URL: https://huggingface.co/papers/2509.02534
[03.09.2025 13:22] Get page data from previous paper. URL: https://huggingface.co/papers/2509.02040
[03.09.2025 13:22] Get page data from previous paper. URL: https://huggingface.co/papers/2509.01360
[03.09.2025 13:22] Get page data from previous paper. URL: https://huggingface.co/papers/2509.01440
[03.09.2025 13:22] Get page data from previous paper. URL: https://huggingface.co/papers/2509.00425
[03.09.2025 13:22] Get page data from previous paper. URL: https://huggingface.co/papers/2509.01052
[03.09.2025 13:22] Get page data from previous paper. URL: https://huggingface.co/papers/2509.02046
[03.09.2025 13:22] Get page data from previous paper. URL: https://huggingface.co/papers/2509.00244
[03.09.2025 13:22] Get page data from previous paper. URL: https://huggingface.co/papers/2509.01984
[03.09.2025 13:22] Get page data from previous paper. URL: https://huggingface.co/papers/2509.00581
[03.09.2025 13:22] Get page data from previous paper. URL: https://huggingface.co/papers/2509.00531
[03.09.2025 13:22] Get page data from previous paper. URL: https://huggingface.co/papers/2509.02379
[03.09.2025 13:22] Get page data from previous paper. URL: https://huggingface.co/papers/2509.02133
[03.09.2025 13:22] Get page data from previous paper. URL: https://huggingface.co/papers/2509.01610
[03.09.2025 13:22] Get page data from previous paper. URL: https://huggingface.co/papers/2509.01584
[03.09.2025 13:22] Get page data from previous paper. URL: https://huggingface.co/papers/2509.01250
[03.09.2025 13:22] Get page data from previous paper. URL: https://huggingface.co/papers/2509.00578
[03.09.2025 13:22] Get page data from previous paper. URL: https://huggingface.co/papers/2509.00404
[03.09.2025 13:22] Get page data from previous paper. URL: https://huggingface.co/papers/2508.20586
[03.09.2025 13:22] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[03.09.2025 13:22] No deleted papers detected.
[03.09.2025 13:22] Downloading and parsing papers (pdf, html). Total: 33.
[03.09.2025 13:22] Downloading and parsing paper https://huggingface.co/papers/2509.02547.
[03.09.2025 13:22] Extra JSON file exists (./assets/json/2509.02547.json), skip PDF parsing.
[03.09.2025 13:22] Paper image links file exists (./assets/img_data/2509.02547.json), skip HTML parsing.
[03.09.2025 13:22] Success.
[03.09.2025 13:22] Downloading and parsing paper https://huggingface.co/papers/2509.02479.
[03.09.2025 13:22] Extra JSON file exists (./assets/json/2509.02479.json), skip PDF parsing.
[03.09.2025 13:22] Paper image links file exists (./assets/img_data/2509.02479.json), skip HTML parsing.
[03.09.2025 13:22] Success.
[03.09.2025 13:22] Downloading and parsing paper https://huggingface.co/papers/2509.02544.
[03.09.2025 13:22] Extra JSON file exists (./assets/json/2509.02544.json), skip PDF parsing.
[03.09.2025 13:22] Paper image links file exists (./assets/img_data/2509.02544.json), skip HTML parsing.
[03.09.2025 13:22] Success.
[03.09.2025 13:22] Downloading and parsing paper https://huggingface.co/papers/2508.21496.
[03.09.2025 13:22] Extra JSON file exists (./assets/json/2508.21496.json), skip PDF parsing.
[03.09.2025 13:22] Paper image links file exists (./assets/img_data/2508.21496.json), skip HTML parsing.
[03.09.2025 13:22] Success.
[03.09.2025 13:22] Downloading and parsing paper https://huggingface.co/papers/2509.00676.
[03.09.2025 13:22] Extra JSON file exists (./assets/json/2509.00676.json), skip PDF parsing.
[03.09.2025 13:22] Paper image links file exists (./assets/img_data/2509.00676.json), skip HTML parsing.
[03.09.2025 13:22] Success.
[03.09.2025 13:22] Downloading and parsing paper https://huggingface.co/papers/2509.01055.
[03.09.2025 13:22] Extra JSON file exists (./assets/json/2509.01055.json), skip PDF parsing.
[03.09.2025 13:22] Paper image links file exists (./assets/img_data/2509.01055.json), skip HTML parsing.
[03.09.2025 13:22] Success.
[03.09.2025 13:22] Downloading and parsing paper https://huggingface.co/papers/2509.01215.
[03.09.2025 13:22] Extra JSON file exists (./assets/json/2509.01215.json), skip PDF parsing.
[03.09.2025 13:22] Paper image links file exists (./assets/img_data/2509.01215.json), skip HTML parsing.
[03.09.2025 13:22] Success.
[03.09.2025 13:22] Downloading and parsing paper https://huggingface.co/papers/2509.02208.
[03.09.2025 13:22] Extra JSON file exists (./assets/json/2509.02208.json), skip PDF parsing.
[03.09.2025 13:22] Paper image links file exists (./assets/img_data/2509.02208.json), skip HTML parsing.
[03.09.2025 13:22] Success.
[03.09.2025 13:22] Downloading and parsing paper https://huggingface.co/papers/2509.01563.
[03.09.2025 13:22] Extra JSON file exists (./assets/json/2509.01563.json), skip PDF parsing.
[03.09.2025 13:22] Paper image links file exists (./assets/img_data/2509.01563.json), skip HTML parsing.
[03.09.2025 13:22] Success.
[03.09.2025 13:22] Downloading and parsing paper https://huggingface.co/papers/2509.02522.
[03.09.2025 13:22] Extra JSON file exists (./assets/json/2509.02522.json), skip PDF parsing.
[03.09.2025 13:22] Paper image links file exists (./assets/img_data/2509.02522.json), skip HTML parsing.
[03.09.2025 13:22] Success.
[03.09.2025 13:22] Downloading and parsing paper https://huggingface.co/papers/2509.02333.
[03.09.2025 13:22] Extra JSON file exists (./assets/json/2509.02333.json), skip PDF parsing.
[03.09.2025 13:22] Paper image links file exists (./assets/img_data/2509.02333.json), skip HTML parsing.
[03.09.2025 13:22] Success.
[03.09.2025 13:22] Downloading and parsing paper https://huggingface.co/papers/2509.01363.
[03.09.2025 13:22] Extra JSON file exists (./assets/json/2509.01363.json), skip PDF parsing.
[03.09.2025 13:22] Paper image links file exists (./assets/img_data/2509.01363.json), skip HTML parsing.
[03.09.2025 13:22] Success.
[03.09.2025 13:22] Downloading and parsing paper https://huggingface.co/papers/2509.02460.
[03.09.2025 13:22] Extra JSON file exists (./assets/json/2509.02460.json), skip PDF parsing.
[03.09.2025 13:22] Paper image links file exists (./assets/img_data/2509.02460.json), skip HTML parsing.
[03.09.2025 13:22] Success.
[03.09.2025 13:22] Downloading and parsing paper https://huggingface.co/papers/2509.01644.
[03.09.2025 13:22] Extra JSON file exists (./assets/json/2509.01644.json), skip PDF parsing.
[03.09.2025 13:22] Paper image links file exists (./assets/img_data/2509.01644.json), skip HTML parsing.
[03.09.2025 13:22] Success.
[03.09.2025 13:22] Downloading and parsing paper https://huggingface.co/papers/2509.02534.
[03.09.2025 13:22] Extra JSON file exists (./assets/json/2509.02534.json), skip PDF parsing.
[03.09.2025 13:22] Paper image links file exists (./assets/img_data/2509.02534.json), skip HTML parsing.
[03.09.2025 13:22] Success.
[03.09.2025 13:22] Downloading and parsing paper https://huggingface.co/papers/2509.02040.
[03.09.2025 13:22] Extra JSON file exists (./assets/json/2509.02040.json), skip PDF parsing.
[03.09.2025 13:22] Paper image links file exists (./assets/img_data/2509.02040.json), skip HTML parsing.
[03.09.2025 13:22] Success.
[03.09.2025 13:22] Downloading and parsing paper https://huggingface.co/papers/2509.01360.
[03.09.2025 13:22] Extra JSON file exists (./assets/json/2509.01360.json), skip PDF parsing.
[03.09.2025 13:22] Paper image links file exists (./assets/img_data/2509.01360.json), skip HTML parsing.
[03.09.2025 13:22] Success.
[03.09.2025 13:22] Downloading and parsing paper https://huggingface.co/papers/2509.01440.
[03.09.2025 13:22] Extra JSON file exists (./assets/json/2509.01440.json), skip PDF parsing.
[03.09.2025 13:22] Paper image links file exists (./assets/img_data/2509.01440.json), skip HTML parsing.
[03.09.2025 13:22] Success.
[03.09.2025 13:22] Downloading and parsing paper https://huggingface.co/papers/2509.00425.
[03.09.2025 13:22] Extra JSON file exists (./assets/json/2509.00425.json), skip PDF parsing.
[03.09.2025 13:22] Paper image links file exists (./assets/img_data/2509.00425.json), skip HTML parsing.
[03.09.2025 13:22] Success.
[03.09.2025 13:22] Downloading and parsing paper https://huggingface.co/papers/2509.01052.
[03.09.2025 13:22] Extra JSON file exists (./assets/json/2509.01052.json), skip PDF parsing.
[03.09.2025 13:22] Paper image links file exists (./assets/img_data/2509.01052.json), skip HTML parsing.
[03.09.2025 13:22] Success.
[03.09.2025 13:22] Downloading and parsing paper https://huggingface.co/papers/2509.02046.
[03.09.2025 13:22] Extra JSON file exists (./assets/json/2509.02046.json), skip PDF parsing.
[03.09.2025 13:22] Paper image links file exists (./assets/img_data/2509.02046.json), skip HTML parsing.
[03.09.2025 13:22] Success.
[03.09.2025 13:22] Downloading and parsing paper https://huggingface.co/papers/2509.00244.
[03.09.2025 13:22] Extra JSON file exists (./assets/json/2509.00244.json), skip PDF parsing.
[03.09.2025 13:22] Paper image links file exists (./assets/img_data/2509.00244.json), skip HTML parsing.
[03.09.2025 13:22] Success.
[03.09.2025 13:22] Downloading and parsing paper https://huggingface.co/papers/2509.01984.
[03.09.2025 13:22] Extra JSON file exists (./assets/json/2509.01984.json), skip PDF parsing.
[03.09.2025 13:22] Paper image links file exists (./assets/img_data/2509.01984.json), skip HTML parsing.
[03.09.2025 13:22] Success.
[03.09.2025 13:22] Downloading and parsing paper https://huggingface.co/papers/2509.00581.
[03.09.2025 13:22] Extra JSON file exists (./assets/json/2509.00581.json), skip PDF parsing.
[03.09.2025 13:22] Paper image links file exists (./assets/img_data/2509.00581.json), skip HTML parsing.
[03.09.2025 13:22] Success.
[03.09.2025 13:22] Downloading and parsing paper https://huggingface.co/papers/2509.00531.
[03.09.2025 13:22] Extra JSON file exists (./assets/json/2509.00531.json), skip PDF parsing.
[03.09.2025 13:22] Paper image links file exists (./assets/img_data/2509.00531.json), skip HTML parsing.
[03.09.2025 13:22] Success.
[03.09.2025 13:22] Downloading and parsing paper https://huggingface.co/papers/2509.02379.
[03.09.2025 13:22] Extra JSON file exists (./assets/json/2509.02379.json), skip PDF parsing.
[03.09.2025 13:22] Paper image links file exists (./assets/img_data/2509.02379.json), skip HTML parsing.
[03.09.2025 13:22] Success.
[03.09.2025 13:22] Downloading and parsing paper https://huggingface.co/papers/2509.02133.
[03.09.2025 13:22] Extra JSON file exists (./assets/json/2509.02133.json), skip PDF parsing.
[03.09.2025 13:22] Paper image links file exists (./assets/img_data/2509.02133.json), skip HTML parsing.
[03.09.2025 13:22] Success.
[03.09.2025 13:22] Downloading and parsing paper https://huggingface.co/papers/2509.01610.
[03.09.2025 13:22] Extra JSON file exists (./assets/json/2509.01610.json), skip PDF parsing.
[03.09.2025 13:22] Paper image links file exists (./assets/img_data/2509.01610.json), skip HTML parsing.
[03.09.2025 13:22] Success.
[03.09.2025 13:22] Downloading and parsing paper https://huggingface.co/papers/2509.01584.
[03.09.2025 13:22] Extra JSON file exists (./assets/json/2509.01584.json), skip PDF parsing.
[03.09.2025 13:22] Paper image links file exists (./assets/img_data/2509.01584.json), skip HTML parsing.
[03.09.2025 13:22] Success.
[03.09.2025 13:22] Downloading and parsing paper https://huggingface.co/papers/2509.01250.
[03.09.2025 13:22] Extra JSON file exists (./assets/json/2509.01250.json), skip PDF parsing.
[03.09.2025 13:22] Paper image links file exists (./assets/img_data/2509.01250.json), skip HTML parsing.
[03.09.2025 13:22] Success.
[03.09.2025 13:22] Downloading and parsing paper https://huggingface.co/papers/2509.00578.
[03.09.2025 13:22] Extra JSON file exists (./assets/json/2509.00578.json), skip PDF parsing.
[03.09.2025 13:22] Paper image links file exists (./assets/img_data/2509.00578.json), skip HTML parsing.
[03.09.2025 13:22] Success.
[03.09.2025 13:22] Downloading and parsing paper https://huggingface.co/papers/2509.00404.
[03.09.2025 13:22] Extra JSON file exists (./assets/json/2509.00404.json), skip PDF parsing.
[03.09.2025 13:22] Paper image links file exists (./assets/img_data/2509.00404.json), skip HTML parsing.
[03.09.2025 13:22] Success.
[03.09.2025 13:22] Downloading and parsing paper https://huggingface.co/papers/2508.20586.
[03.09.2025 13:22] Extra JSON file exists (./assets/json/2508.20586.json), skip PDF parsing.
[03.09.2025 13:22] Paper image links file exists (./assets/img_data/2508.20586.json), skip HTML parsing.
[03.09.2025 13:22] Success.
[03.09.2025 13:22] Enriching papers with extra data.
[03.09.2025 13:22] ********************************************************************************
[03.09.2025 13:22] Abstract 0. Agentic reinforcement learning transforms large language models into autonomous decision-making agents by leveraging temporally extended POMDPs, enhancing capabilities like planning and reasoning through reinforcement learning.  					AI-generated summary 				 The emergence of agentic reinforcement l...
[03.09.2025 13:22] ********************************************************************************
[03.09.2025 13:22] Abstract 1. SimpleTIR stabilizes multi-turn Tool-Integrated Reasoning training by filtering out void turns, achieving state-of-the-art performance on math reasoning benchmarks.  					AI-generated summary 				 Large Language Models (LLMs) can significantly improve their reasoning capabilities by interacting with...
[03.09.2025 13:22] ********************************************************************************
[03.09.2025 13:22] Abstract 2. UI-TARS-2, a native GUI-centered agent model, addresses challenges in data scalability, multi-turn reinforcement learning, and environment stability, achieving significant improvements over its predecessor and strong baselines across various benchmarks.  					AI-generated summary 				 The developmen...
[03.09.2025 13:22] ********************************************************************************
[03.09.2025 13:22] Abstract 3. A benchmark for long-video hallucination identifies and investigates Semantic Aggregation Hallucination (SAH), showing its prevalence in complex and rapidly changing semantic contexts, and proposes strategies to mitigate it.  					AI-generated summary 				 Video multimodal large language models (Vid...
[03.09.2025 13:22] ********************************************************************************
[03.09.2025 13:22] Abstract 4. Reinforcement learning on preference-labeled critic datasets enhances a generative model's performance, creating a unified multimodal system that excels in both evaluation and generation.  					AI-generated summary 				 In vision-language modeling, critic models are typically trained to evaluate out...
[03.09.2025 13:22] ********************************************************************************
[03.09.2025 13:22] Abstract 5. VerlTool is a unified and modular framework for Agentic Reinforcement Learning with Tool use, addressing inefficiencies in existing approaches and providing competitive performance across multiple domains.  					AI-generated summary 				 Reinforcement Learning with Verifiable Rewards (RLVR) has demo...
[03.09.2025 13:22] ********************************************************************************
[03.09.2025 13:22] Abstract 6. A framework for constructing high-quality document extraction datasets and models through synthetic data generation and iterative self-improvement outperforms existing models.  					AI-generated summary 				 High-quality labeled data is essential for training accurate document conversion models, par...
[03.09.2025 13:22] ********************************************************************************
[03.09.2025 13:22] Abstract 7. A dynamic verification framework using reinforcement learning and a novel algorithm improves the performance of a large medical language model in real-world clinical decision-making.  					AI-generated summary 				 As large language models (LLMs) advance in conversational and reasoning capabilities,...
[03.09.2025 13:22] ********************************************************************************
[03.09.2025 13:22] Abstract 8. Keye-VL-1.5 enhances video understanding through a Slow-Fast encoding strategy, progressive pre-training, and post-training reasoning improvements, outperforming existing models on video tasks while maintaining general multimodal performance.  					AI-generated summary 				 In recent years, the deve...
[03.09.2025 13:22] ********************************************************************************
[03.09.2025 13:22] Abstract 9. PACS, a novel RLVR framework, reformulates RLVR as a supervised learning task, improving stability and efficiency in training large language models for reasoning tasks.  					AI-generated summary 				 Recent advances in Reinforcement Learning with Verifiable Rewards (RLVR) have empowered large langu...
[03.09.2025 13:22] ********************************************************************************
[03.09.2025 13:22] Abstract 10. DCPO, a novel reinforcement learning framework, enhances large language models by dynamically adjusting clipping bounds and standardizing rewards, leading to improved performance and efficiency.  					AI-generated summary 				 Reinforcement Learning from Verifiable Rewards (RLVR) has emerged as a pr...
[03.09.2025 13:22] ********************************************************************************
[03.09.2025 13:22] Abstract 11. Reasoning capabilities from reinforcement learning can be extracted as a task vector and transferred to other models to improve performance on diverse benchmarks.  					AI-generated summary 				 Large language models often require costly optimization, such as reinforcement learning, to master comple...
[03.09.2025 13:22] ********************************************************************************
[03.09.2025 13:22] Abstract 12. A novel Diffusion Transformer pipeline automates video compositing by adaptively injecting identity and motion information, maintaining consistency and enabling user customization.  					AI-generated summary 				 Video compositing combines live-action footage to create video production, serving as a...
[03.09.2025 13:22] ********************************************************************************
[03.09.2025 13:22] Abstract 13. OpenVision 2 simplifies the original architecture by removing the text encoder and contrastive loss, achieving competitive performance with reduced training time and memory usage, and enabling scaling to over 1 billion parameters.  					AI-generated summary 				 This paper provides a simplification ...
[03.09.2025 13:22] ********************************************************************************
[03.09.2025 13:22] Abstract 14. DARLING, a diversity-aware reinforcement learning framework, enhances both the quality and diversity of large language model outputs across various tasks.  					AI-generated summary 				 Post-training of Large Language Models (LMs) often prioritizes accuracy and helpfulness at the expense of diversi...
[03.09.2025 13:22] ********************************************************************************
[03.09.2025 13:22] Abstract 15. Genetic Prompt enhances synthetic data quality and diversity in NLP by combining genetic algorithms with LLMs, improving downstream model performance.  					AI-generated summary 				 Large Language Models (LLMs) excel at generating synthetic data, but ensuring its quality and diversity remains chall...
[03.09.2025 13:22] ********************************************************************************
[03.09.2025 13:22] Abstract 16. M3Ret, a unified visual encoder trained on a large-scale hybrid-modality dataset, achieves state-of-the-art zero-shot image-to-image retrieval and cross-modal alignment using generative and contrastive self-supervised learning paradigms.  					AI-generated summary 				 Medical image retrieval is ess...
[03.09.2025 13:22] ********************************************************************************
[03.09.2025 13:22] Abstract 17. A comprehensive evaluation of recent optimization techniques for Large Language Models provides guidance on selecting the best optimizer for different pretraining scenarios.  					AI-generated summary 				 The recent development of Large Language Models (LLMs) has been accompanied by an effervescenc...
[03.09.2025 13:22] ********************************************************************************
[03.09.2025 13:22] Abstract 18. Camlang, a constructed language, is used to evaluate whether LLMs can master unfamiliar languages through metalinguistic reasoning, revealing that current models lack systematic grammatical mastery compared to humans.  					AI-generated summary 				 Large Language Models (LLMs) achieve gold-medal pe...
[03.09.2025 13:22] ********************************************************************************
[03.09.2025 13:22] Abstract 19. FlashAdventure benchmark and COAST framework improve GUI agents' performance in completing full story arcs in Flash-based adventure games by addressing the observation-behavior gap.  					AI-generated summary 				 GUI agents powered by LLMs show promise in interacting with diverse digital environmen...
[03.09.2025 13:22] ********************************************************************************
[03.09.2025 13:22] Abstract 20. A systematic study reveals that fair comparisons of deep learning optimizers require rigorous hyperparameter tuning and evaluations across various model scales and data-to-model ratios, showing that matrix-based optimizers like Muon and Soap offer diminishing speedups as model size increases.  					...
[03.09.2025 13:22] ********************************************************************************
[03.09.2025 13:22] Abstract 21. Universal Deep Research (UDR) is a flexible system that allows users to customize deep research strategies using any language model without additional training or fine-tuning.  					AI-generated summary 				 Deep research tools are among the most impactful and most commonly encountered agentic syste...
[03.09.2025 13:22] ********************************************************************************
[03.09.2025 13:22] Abstract 22. VARIN, a noise inversion-based editing technique for visual autoregressive models, enables precise image editing aligned with textual prompts while preserving original details.  					AI-generated summary 				 Visual autoregressive models (VAR) have recently emerged as a promising class of generative...
[03.09.2025 13:22] ********************************************************************************
[03.09.2025 13:22] Abstract 23. A multi-agent framework decomposes the Text2SQL task into several components, using in-context learning and taxonomy-guided error modification to achieve state-of-the-art results.  					AI-generated summary 				 Converting natural language queries into SQL queries is a crucial challenge in both indu...
[03.09.2025 13:22] ********************************************************************************
[03.09.2025 13:22] Abstract 24. MobiAgent, a comprehensive mobile agent system, achieves state-of-the-art performance in real-world mobile scenarios through its MobiMind-series models, AgentRR framework, and MobiFlow benchmarking suite, while also reducing data annotation costs.  					AI-generated summary 				 With the rapid advan...
[03.09.2025 13:22] ********************************************************************************
[03.09.2025 13:22] Abstract 25. MedDINOv3, a framework adapting DINOv3 with multi-scale token aggregation, achieves state-of-the-art performance in medical image segmentation by overcoming challenges in domain adaptation and backbone performance.  					AI-generated summary 				 Accurate segmentation of organs and tumors in CT and ...
[03.09.2025 13:22] ********************************************************************************
[03.09.2025 13:22] Abstract 26. Ambekar framework uses a Constitution-Aware Decoding Layer to mitigate caste and religious biases in LLM outputs through speculative decoding without retraining.  					AI-generated summary 				 Large Language Models (LLMs) can inadvertently reflect societal biases present in their training data, lea...
[03.09.2025 13:22] ********************************************************************************
[03.09.2025 13:22] Abstract 27. A Panel-of-Peers learning framework enhances Large Vision and Language Models by simulating peer reviews, improving performance without extensive human-labeled datasets.  					AI-generated summary 				 Traditional alignment methods for Large Vision and Language Models (LVLMs) primarily rely on human...
[03.09.2025 13:22] ********************************************************************************
[03.09.2025 13:22] Abstract 28. ViSTA-SLAM is a real-time monocular SLAM system that uses a lightweight STA model for pose estimation and pointmap regression, and a Sim(3) pose graph for drift correction, achieving superior tracking and reconstruction.  					AI-generated summary 				 We present ViSTA-SLAM as a real-time monocular ...
[03.09.2025 13:22] ********************************************************************************
[03.09.2025 13:22] Abstract 29. Point-PQAE, a two-view cross-reconstruction generative paradigm, enhances 3D self-supervised learning by introducing greater diversity and variance, outperforming single-view methods in point cloud reconstruction tasks.  					AI-generated summary 				 Point cloud learning, especially in a self-super...
[03.09.2025 13:22] ********************************************************************************
[03.09.2025 13:22] Abstract 30. Context-Aware Fusion enhances DiffusionDet by integrating global scene context with local features using cross-attention, improving performance on fine-grained object detection tasks.  					AI-generated summary 				 Fine-grained object detection in challenging visual domains, such as vehicle damage ...
[03.09.2025 13:22] ********************************************************************************
[03.09.2025 13:22] Abstract 31. Metis addresses training instability in low-bit quantized large language models by using spectral decomposition, adaptive learning rates, and dual-range regularization to improve performance and stability.  					AI-generated summary 				 This work identifies anisotropic parameter distributions as a ...
[03.09.2025 13:22] ********************************************************************************
[03.09.2025 13:22] Abstract 32. FastFit, a high-speed virtual try-on framework using a cacheable diffusion architecture with a Semi-Attention mechanism, achieves significant speedup and maintains high fidelity in multi-reference outfit compositions.  					AI-generated summary 				 Despite its great potential, virtual try-on techno...
[03.09.2025 13:22] Read previous papers.
[03.09.2025 13:22] Generating reviews via LLM API.
[03.09.2025 13:22] Using data from previous issue: {"categories": ["#benchmark", "#agents", "#reasoning", "#agi", "#survey", "#rl", "#open_source"], "emoji": "ü§ñ", "ru": {"title": "–ë–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ —Å—Ç–∞–Ω–æ–≤—è—Ç—Å—è –∞–≤—Ç–æ–Ω–æ–º–Ω—ã–º–∏ –∞–≥–µ–Ω—Ç–∞–º–∏", "desc": "–ê–≥–µ–Ω—Ç–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º (Agentic RL) —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∏—Ä—É–µ—Ç –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –≤ –∞–≤—Ç–æ–Ω–æ–º–Ω—ã—Ö –∞–≥–µ–Ω—Ç
[03.09.2025 13:22] Using data from previous issue: {"categories": ["#benchmark", "#reasoning", "#math", "#optimization", "#rl", "#training"], "emoji": "üß†", "ru": {"title": "–°—Ç–∞–±–∏–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ AI —Ä–∞—Å—Å—É–∂–¥–∞—Ç—å —Å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∞–ª–≥–æ—Ä–∏—Ç–º SimpleTIR, —Å—Ç–∞–±–∏–ª–∏–∑–∏—Ä—É—é—â–∏–π –æ–±—É—á–µ–Ω–∏–µ –º–Ω–æ–≥–æ—Ö–æ–¥–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º
[03.09.2025 13:22] Using data from previous issue: {"categories": ["#optimization", "#games", "#benchmark", "#training", "#reasoning", "#agents", "#rl"], "emoji": "ü§ñ", "ru": {"title": "UI-TARS-2: –ù–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å GUI-–∞–≥–µ–Ω—Ç–æ–≤ —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º –æ–±—É—á–µ–Ω–∏–µ–º –∏ –æ–±–æ–±—â–µ–Ω–∏–µ–º", "desc": "UI-TARS-2 - —ç—Ç–æ –º–æ–¥–µ–ª—å –∞–≥–µ–Ω—Ç–∞ –¥–ª—è –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–≤, —Ä–µ—à–∞—é—â
[03.09.2025 13:22] Using data from previous issue: {"categories": ["#benchmark", "#long_context", "#hallucinations", "#dataset", "#video"], "emoji": "üé¨", "ru": {"title": "–ë–æ—Ä—å–±–∞ —Å –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏—è–º–∏ –≤ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ: –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ ELV-Halluc –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π –≤ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ. –ê–≤—Ç–æ—Ä—ã –≤–≤–æ–¥—è—Ç 
[03.09.2025 13:22] Using data from previous issue: {"categories": ["#optimization", "#games", "#rlhf", "#multimodal", "#benchmark", "#reasoning", "#rl"], "emoji": "ü§ñ", "ru": {"title": "–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –∫—Ä–∏—Ç–∏–∫–∞ –∏ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞: –Ω–æ–≤—ã–π —à–∞–≥ –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–º –ò–ò", "desc": "–í —ç—Ç–æ–π —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –æ–±—ä–µ–¥
[03.09.2025 13:22] Using data from previous issue: {"categories": ["#architecture", "#rl", "#open_source", "#agi", "#agents", "#reasoning", "#rlhf", "#training", "#multimodal"], "emoji": "üõ†Ô∏è", "ru": {"title": "VerlTool: –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤", "desc": "VerlTool - —ç—Ç–æ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∏ –º–æ–¥—É
[03.09.2025 13:22] Using data from previous issue: {"categories": ["#dataset", "#data", "#optimization", "#synthetic", "#training"], "emoji": "üìÑ", "ru": {"title": "–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö –∏ –º–æ–¥–µ
[03.09.2025 13:22] Using data from previous issue: {"categories": ["#benchmark", "#optimization", "#rl", "#open_source", "#reasoning", "#agents", "#alignment", "#training", "#healthcare"], "emoji": "ü©∫", "ru": {"title": "–î–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è LLM –¥–ª—è —Ä–µ–∞–ª—å–Ω–æ–π –∫–ª–∏–Ω–∏—á–µ—Å–∫–æ–π –ø—Ä–∞–∫—Ç–∏–∫–∏", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –¥–∏–Ω–∞–º–∏—á–µ—Å–∫—É—é —Å–∏—Å—Ç–µ–º—É –≤–µ—Ä–∏—Ñ–∏
[03.09.2025 13:22] Using data from previous issue: {"categories": ["#multimodal", "#reasoning", "#video", "#long_context", "#rl", "#training", "#alignment"], "emoji": "üé•", "ru": {"title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ –≤–∏–¥–µ–æ: Keye-VL-1.5 –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∏ —Ç–æ—á–Ω–æ—Å—Ç—å", "desc": "Keye-VL-1.5 - —ç—Ç–æ –Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –≤–∏–¥–µ–æ. –û–Ω–∞
[03.09.2025 13:22] Using data from previous issue: {"categories": ["#benchmark", "#reasoning", "#optimization", "#rl", "#training", "#open_source"], "emoji": "üß†", "ru": {"title": "PACS: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é —á–µ—Ä–µ–∑ —Å—É–ø–µ—Ä–≤–∏–∑–æ—Ä–Ω—ã–π RLVR", "desc": "PACS - —ç—Ç–æ –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º —Å –ø—Ä–æ–≤–µ—Ä—è–µ–º—ã–º–∏ –Ω–∞–≥—Ä–∞–¥–∞–º
[03.09.2025 13:22] Using data from previous issue: {"categories": ["#benchmark", "#reasoning", "#rlhf", "#training", "#optimization", "#rl"], "emoji": "üöÄ", "ru": {"title": "DCPO: –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è –º–æ—â–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "DCPO - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –û–Ω –¥–∏–Ω–∞–º–∏—á
[03.09.2025 13:22] Using data from previous issue: {"categories": ["#benchmark", "#training", "#rl", "#open_source", "#transfer_learning", "#optimization", "#reasoning"], "emoji": "üß†", "ru": {"title": "–ü–µ—Ä–µ–¥–∞—á–∞ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é –º–µ–∂–¥—É —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é, –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ —Å –ø–æ–º–æ—â—å—é 
[03.09.2025 13:22] Using data from previous issue: {"categories": ["#diffusion", "#dataset", "#video"], "emoji": "üé¨", "ru": {"title": "–ì–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–π –≤–∏–¥–µ–æ–∫–æ–º–ø–æ–∑–∏—Ç–∏–Ω–≥: –Ω–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –≤ –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–µ –≤–∏–¥–µ–æ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –≤–∏–¥–µ–æ–∫–æ–º–ø–æ–∑–∏—Ç–∏–Ω–≥–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ 
[03.09.2025 13:22] Using data from previous issue: {"categories": ["#optimization", "#multimodal", "#training", "#architecture"], "emoji": "üöÄ", "ru": {"title": "–£–ø—Ä–æ—â–µ–Ω–∏–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "OpenVision 2 –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π —É–ø—Ä–æ—â–µ–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã OpenVision, –≤ –∫–æ—Ç–æ—Ä–æ–π —É–¥–∞–ª–µ–Ω —Ç–µ–∫—Å—Ç–æ–≤—ã–π —ç–Ω–∫–æ–¥–µ
[03.09.2025 13:22] Using data from previous issue: {"categories": ["#games", "#story_generation", "#rlhf", "#optimization", "#rl", "#training"], "emoji": "üåà", "ru": {"title": "DARLING: –ö–∞—á–µ—Å—Ç–≤–æ –∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –≤ –≥–∞—Ä–º–æ–Ω–∏–∏", "desc": "DARLING - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç –∫–∞–∫ –∫–∞—á–µ—Å—Ç–≤–æ, —Ç–∞–∫ –∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –≤—ã—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –±–æ–ª—å
[03.09.2025 13:22] Using data from previous issue: {"categories": ["#optimization", "#data", "#synthetic", "#training", "#dataset"], "emoji": "üß¨", "ru": {"title": "–ì–µ–Ω–µ—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–æ–∑–¥–∞–Ω–∏—é –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è NLP", "desc": "Genetic Prompt - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –≤ –æ–±—Ä–∞–±–æ—Ç–∫–µ –µ—Å—Ç
[03.09.2025 13:22] Using data from previous issue: {"categories": ["#cv", "#multimodal", "#dataset", "#transfer_learning", "#optimization", "#healthcare"], "emoji": "üè•", "ru": {"title": "–ï–¥–∏–Ω—ã–π —ç–Ω–∫–æ–¥–µ—Ä –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "M3Ret - —ç—Ç–æ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≤–∏–∑—É–∞–ª—å–Ω—ã–π —ç–Ω–∫–æ–¥–µ—Ä, –æ–±—É—á–µ–Ω–Ω—ã–π –Ω–∞ –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω–æ–º –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö —Å 
[03.09.2025 13:22] Using data from previous issue: {"categories": ["#benchmark", "#open_source", "#optimization", "#training"], "emoji": "üî¨", "ru": {"title": "–û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä—ã –¥–ª—è LLM: –≤—Å–µ—Å—Ç–æ—Ä–æ–Ω–Ω–∏–π –∞–Ω–∞–ª–∏–∑ –∏ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∫–æ–º–ø–ª–µ–∫—Å–Ω—É—é –æ—Ü–µ–Ω–∫—É —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –ê–≤—Ç–æ
[03.09.2025 13:22] Using data from previous issue: {"categories": ["#dataset", "#multilingual", "#reasoning", "#long_context", "#benchmark"], "emoji": "üó£Ô∏è", "ru": {"title": "–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π —è–∑—ã–∫ —Ä–∞—Å–∫—Ä—ã–≤–∞–µ—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Å–æ–∑–¥–∞–ª–∏ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π —è–∑—ã–∫ Camlang –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –æ—Å–≤–∞
[03.09.2025 13:22] Using data from previous issue: {"categories": ["#benchmark", "#agents", "#optimization", "#games"], "emoji": "üïπÔ∏è", "ru": {"title": "–ü—Ä–µ–æ–¥–æ–ª–µ–≤–∞—è —Ä–∞–∑—Ä—ã–≤ –º–µ–∂–¥—É –Ω–∞–±–ª—é–¥–µ–Ω–∏–µ–º –∏ –¥–µ–π—Å—Ç–≤–∏–µ–º –≤ –∏–≥—Ä–∞—Ö-–∫–≤–µ—Å—Ç–∞—Ö", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ FlashAdventure –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∞–≥–µ–Ω—Ç–æ–≤ —Å –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–º –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–º –≤ Flash-–∏–≥—Ä–∞—Ö –∂–∞–Ω—Ä–∞ –∫–≤–µ—Å—Ç. 
[03.09.2025 13:22] Using data from previous issue: {"categories": ["#optimization", "#training"], "emoji": "üî¨", "ru": {"title": "–°–ø—Ä–∞–≤–µ–¥–ª–∏–≤–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–æ–≤ —Ç—Ä–µ–±—É–µ—Ç —Ç—â–∞—Ç–µ–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –¥–ª—è —Å–ø—Ä–∞–≤–µ–¥–ª–∏–≤–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–æ–≤ –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –Ω–µ–æ–±—Ö–æ–¥–∏–º–∞ —Ç—â–∞—Ç–µ–ª—å–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏ –æ—Ü–µ–Ω–∫–∞
[03.09.2025 13:22] Using data from previous issue: {"categories": ["#agents"], "emoji": "üîç", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –≥–ª—É–±–æ–∫–æ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è", "desc": "Universal Deep Research (UDR) - —ç—Ç–æ –≥–∏–±–∫–∞—è —Å–∏—Å—Ç–µ–º–∞, –ø–æ–∑–≤–æ–ª—è—é—â–∞—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞—Ç—å —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –≥–ª—É–±–æ–∫–æ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ª—é–±–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥
[03.09.2025 13:22] Using data from previous issue: {"categories": ["#optimization", "#cv", "#multimodal", "#diffusion"], "emoji": "üñåÔ∏è", "ru": {"title": "VARIN: –¢–æ—á–Ω–æ–µ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é –∏–Ω–≤–µ—Ä—Å–∏–∏ —à—É–º–∞ –¥–ª—è VAR-–º–æ–¥–µ–ª–µ–π", "desc": "VARIN - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π (VAR). –û–Ω –∏—Å–ø–æ
[03.09.2025 13:22] Using data from previous issue: {"categories": ["#data", "#optimization", "#reasoning", "#agents", "#dataset"], "emoji": "ü§ñ", "ru": {"title": "–ú—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω—ã–π –ø–æ–¥—Ö–æ–¥ —Å –æ–±—É—á–µ–Ω–∏–µ–º –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ —Ä–µ–≤–æ–ª—é—Ü–∏–æ–Ω–∏–∑–∏—Ä—É–µ—Ç Text2SQL", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ä–µ—à–µ–Ω–∏—é –∑–∞–¥–∞—á–∏ Text2SQL, –∏—Å–ø–æ–ª—å–∑—É—è –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω—É—é —Å–∏—Å—Ç–µ–º—É. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥
[03.09.2025 13:22] Using data from previous issue: {"categories": ["#dataset", "#agents", "#data", "#benchmark"], "emoji": "üì±", "ru": {"title": "MobiAgent: –ø–µ—Ä–µ–¥–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –º–æ–±–∏–ª—å–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ –¥–ª—è —Ä–µ–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á", "desc": "MobiAgent - —ç—Ç–æ –∫–æ–º–ø–ª–µ–∫—Å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –º–æ–±–∏–ª—å–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤, —Å–æ—Å—Ç–æ—è—â–∞—è –∏–∑ —Ç—Ä–µ—Ö –∫–ª—é—á–µ–≤—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤: —Å–µ—Ä–∏–∏ –º–æ–¥–µ–ª–µ–π MobiMind, —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞
[03.09.2025 13:22] Using data from previous issue: {"categories": ["#cv", "#benchmark", "#dataset", "#architecture", "#transfer_learning", "#optimization", "#healthcare"], "emoji": "üè•", "ru": {"title": "MedDINOv3: –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ç–æ—Ä –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "MedDINOv3 - —ç—Ç–æ –Ω–æ–≤–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¥–ª—è —Å–µ–≥–º
[03.09.2025 13:22] Using data from previous issue: {"categories": ["#data", "#multilingual", "#ethics", "#open_source", "#alignment", "#inference"], "emoji": "üáÆüá≥", "ru": {"title": "–ö–æ–Ω—Å—Ç–∏—Ç—É—Ü–∏–æ–Ω–Ω–æ–µ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è —Å–ø—Ä–∞–≤–µ–¥–ª–∏–≤—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–§—Ä–µ–π–º–≤–æ—Ä–∫ AMBEDKAR –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–Ω–∏–∂–µ–Ω–∏—é –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç–∏ –≤ –≤—ã–≤–æ–¥–∞—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º
[03.09.2025 13:22] Using data from previous issue: {"categories": ["#benchmark", "#multimodal", "#rlhf", "#hallucinations", "#alignment"], "emoji": "üë•", "ru": {"title": "–ö–æ–ª–ª–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –ò–ò: –∏–º–∏—Ç–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞ —Ä–µ—Ü–µ–Ω–∑–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —è–∑—ã–∫–æ–≤–æ-–≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –Ω–æ–≤–∞—è –º–µ—Ç–æ–¥–∏–∫–∞ –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤–æ-–≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π 
[03.09.2025 13:22] Using data from previous issue: {"categories": ["#architecture", "#3d", "#cv"], "emoji": "üó∫Ô∏è", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –º–æ–Ω–æ–∫—É–ª—è—Ä–Ω–∞—è SLAM-—Å–∏—Å—Ç–µ–º–∞ –±–µ–∑ –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏ –∫–∞–º–µ—Ä—ã", "desc": "ViSTA-SLAM - —ç—Ç–æ —Å–∏—Å—Ç–µ–º–∞ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–π –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏ –∏ –∫–∞—Ä—Ç–æ–≥—Ä–∞—Ñ–∏—Ä–æ–≤–∞–Ω–∏—è –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è –º–æ–Ω–æ–∫—É–ª—è—Ä–Ω—É—é –∫–∞–º–µ—Ä—É. –û–Ω–∞ –ø—Ä–∏–º–µ–Ω—è–µ—Ç –ª–µ–≥–∫–æ–≤–µ—Å–Ω—É
[03.09.2025 13:22] Using data from previous issue: {"categories": ["#optimization", "#3d", "#synthetic"], "emoji": "üîç", "ru": {"title": "–î–≤—É—Ö—Ä–∞–∫—É—Ä—Å–Ω–æ–µ —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏–µ —É–ª—É—á—à–∞–µ—Ç 3D —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—é –æ–±–ª–∞–∫–æ–≤ —Ç–æ—á–µ–∫", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏—è –¥–ª—è —Ç—Ä–µ—Ö–º–µ—Ä–Ω—ã—Ö –æ–±–ª–∞–∫–æ–≤ —Ç–æ—á–µ–∫ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Point-PQAE. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–≤—É—Ö—Ä–∞–∫—É—Ä—Å–Ω—É—é
[03.09.2025 13:22] Using data from previous issue: {"categories": ["#diffusion", "#benchmark", "#architecture", "#optimization", "#cv"], "emoji": "üîç", "ru": {"title": "–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ —Å–ª–∏—è–Ω–∏–µ –¥–ª—è —Ç–æ—á–Ω–æ–π –¥–µ—Ç–µ–∫—Ü–∏–∏ –º–µ–ª–∫–∏—Ö –æ–±—ä–µ–∫—Ç–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ Context-Aware Fusion (CAF) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ DiffusionDet –≤ –∑–∞–¥–∞—á–µ –¥–µ—Ç–µ–∫—Ü–∏–∏ –º–µ–ª–∫–∏
[03.09.2025 13:22] Using data from previous issue: {"categories": ["#low_resource", "#inference", "#training", "#optimization"], "emoji": "üß†", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ LLM —Å –Ω–∏–∑–∫–æ–π –±–∏—Ç–Ω–æ—Å—Ç—å—é", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Metis - —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) —Å –Ω–∏–∑–∫–æ–±–∏—Ç–Ω–æ–π –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–µ–π. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–ø–µ–∫—Ç—Ä–∞–ª—å
[03.09.2025 13:22] Using data from previous issue: {"categories": ["#benchmark", "#cv", "#inference", "#open_source", "#dataset", "#diffusion", "#data"], "emoji": "üëö", "ru": {"title": "–ë—ã—Å—Ç—Ä–∞—è –∏ —Ç–æ—á–Ω–∞—è –≤–∏—Ä—Ç—É–∞–ª—å–Ω–∞—è –ø—Ä–∏–º–µ—Ä–∫–∞ —Å FastFit", "desc": "FastFit - —ç—Ç–æ –Ω–æ–≤–∞—è –≤—ã—Å–æ–∫–æ—Å–∫–æ—Ä–æ—Å—Ç–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–π –ø—Ä–∏–º–µ—Ä–∫–∏ –æ–¥–µ–∂–¥—ã, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è –∫—ç—à–∏—Ä—É–µ–º—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É
[03.09.2025 13:22] Renaming data file.
[03.09.2025 13:22] Renaming previous data. hf_papers.json to ./d/2025-09-03.json
[03.09.2025 13:22] Saving new data file.
[03.09.2025 13:22] Generating page.
[03.09.2025 13:22] Renaming previous page.
[03.09.2025 13:22] Renaming previous data. index.html to ./d/2025-09-03.html
[03.09.2025 13:22] Writing result.
[03.09.2025 13:22] Renaming log file.
[03.09.2025 13:22] Renaming previous data. log.txt to ./logs/2025-09-03_last_log.txt
