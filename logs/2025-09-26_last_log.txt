[26.09.2025 10:11] Read previous papers.
[26.09.2025 10:11] Generating top page (month).
[26.09.2025 10:11] Writing top page (month).
[26.09.2025 11:09] Read previous papers.
[26.09.2025 11:09] Get feed.
[26.09.2025 11:09] Get page data from previous paper. URL: https://huggingface.co/papers/2509.21268
[26.09.2025 11:09] Get page data from previous paper. URL: https://huggingface.co/papers/2509.21320
[26.09.2025 11:09] Get page data from previous paper. URL: https://huggingface.co/papers/2509.21240
[26.09.2025 11:09] Get page data from previous paper. URL: https://huggingface.co/papers/2509.19803
[26.09.2025 11:09] Get page data from previous paper. URL: https://huggingface.co/papers/2509.20427
[26.09.2025 11:09] Get page data from previous paper. URL: https://huggingface.co/papers/2509.21245
[26.09.2025 11:09] Get page data from previous paper. URL: https://huggingface.co/papers/2509.21138
[26.09.2025 11:09] Get page data from previous paper. URL: https://huggingface.co/papers/2509.21117
[26.09.2025 11:09] Get page data from previous paper. URL: https://huggingface.co/papers/2509.20712
[26.09.2025 11:09] Get page data from previous paper. URL: https://huggingface.co/papers/2509.21278
[26.09.2025 11:09] Get page data from previous paper. URL: https://huggingface.co/papers/2509.21114
[26.09.2025 11:09] Get page data from previous paper. URL: https://huggingface.co/papers/2509.21072
[26.09.2025 11:09] Get page data from previous paper. URL: https://huggingface.co/papers/2509.20136
[26.09.2025 11:09] Get page data from previous paper. URL: https://huggingface.co/papers/2509.21317
[26.09.2025 11:09] Extract page data from URL. URL: https://huggingface.co/papers/2509.21106
[26.09.2025 11:09] Get page data from previous paper. URL: https://huggingface.co/papers/2509.19301
[26.09.2025 11:09] Get page data from previous paper. URL: https://huggingface.co/papers/2509.21318
[26.09.2025 11:09] Get page data from previous paper. URL: https://huggingface.co/papers/2509.21302
[26.09.2025 11:09] Get page data from previous paper. URL: https://huggingface.co/papers/2509.20186
[26.09.2025 11:09] Get page data from previous paper. URL: https://huggingface.co/papers/2509.20414
[26.09.2025 11:09] Get page data from previous paper. URL: https://huggingface.co/papers/2509.21070
[26.09.2025 11:09] Get page data from previous paper. URL: https://huggingface.co/papers/2509.21042
[26.09.2025 11:09] Get page data from previous paper. URL: https://huggingface.co/papers/2509.14662
[26.09.2025 11:09] Get page data from previous paper. URL: https://huggingface.co/papers/2509.20878
[26.09.2025 11:09] Get page data from previous paper. URL: https://huggingface.co/papers/2509.21113
[26.09.2025 11:09] Get page data from previous paper. URL: https://huggingface.co/papers/2509.20868
[26.09.2025 11:09] Extract page data from URL. URL: https://huggingface.co/papers/2509.20293
[26.09.2025 11:09] Get page data from previous paper. URL: https://huggingface.co/papers/2509.20109
[26.09.2025 11:09] Get page data from previous paper. URL: https://huggingface.co/papers/2509.20394
[26.09.2025 11:09] Get page data from previous paper. URL: https://huggingface.co/papers/2509.20706
[26.09.2025 11:09] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[26.09.2025 11:09] No deleted papers detected.
[26.09.2025 11:09] Downloading and parsing papers (pdf, html). Total: 30.
[26.09.2025 11:09] Downloading and parsing paper https://huggingface.co/papers/2509.21268.
[26.09.2025 11:09] Extra JSON file exists (./assets/json/2509.21268.json), skip PDF parsing.
[26.09.2025 11:09] Paper image links file exists (./assets/img_data/2509.21268.json), skip HTML parsing.
[26.09.2025 11:09] Success.
[26.09.2025 11:09] Downloading and parsing paper https://huggingface.co/papers/2509.21320.
[26.09.2025 11:09] Extra JSON file exists (./assets/json/2509.21320.json), skip PDF parsing.
[26.09.2025 11:09] Paper image links file exists (./assets/img_data/2509.21320.json), skip HTML parsing.
[26.09.2025 11:09] Success.
[26.09.2025 11:09] Downloading and parsing paper https://huggingface.co/papers/2509.21240.
[26.09.2025 11:09] Extra JSON file exists (./assets/json/2509.21240.json), skip PDF parsing.
[26.09.2025 11:09] Paper image links file exists (./assets/img_data/2509.21240.json), skip HTML parsing.
[26.09.2025 11:09] Success.
[26.09.2025 11:09] Downloading and parsing paper https://huggingface.co/papers/2509.19803.
[26.09.2025 11:09] Extra JSON file exists (./assets/json/2509.19803.json), skip PDF parsing.
[26.09.2025 11:09] Paper image links file exists (./assets/img_data/2509.19803.json), skip HTML parsing.
[26.09.2025 11:09] Success.
[26.09.2025 11:09] Downloading and parsing paper https://huggingface.co/papers/2509.20427.
[26.09.2025 11:09] Extra JSON file exists (./assets/json/2509.20427.json), skip PDF parsing.
[26.09.2025 11:09] Paper image links file exists (./assets/img_data/2509.20427.json), skip HTML parsing.
[26.09.2025 11:09] Success.
[26.09.2025 11:09] Downloading and parsing paper https://huggingface.co/papers/2509.21245.
[26.09.2025 11:09] Extra JSON file exists (./assets/json/2509.21245.json), skip PDF parsing.
[26.09.2025 11:09] Paper image links file exists (./assets/img_data/2509.21245.json), skip HTML parsing.
[26.09.2025 11:09] Success.
[26.09.2025 11:09] Downloading and parsing paper https://huggingface.co/papers/2509.21138.
[26.09.2025 11:09] Extra JSON file exists (./assets/json/2509.21138.json), skip PDF parsing.
[26.09.2025 11:09] Paper image links file exists (./assets/img_data/2509.21138.json), skip HTML parsing.
[26.09.2025 11:09] Success.
[26.09.2025 11:09] Downloading and parsing paper https://huggingface.co/papers/2509.21117.
[26.09.2025 11:09] Extra JSON file exists (./assets/json/2509.21117.json), skip PDF parsing.
[26.09.2025 11:09] Paper image links file exists (./assets/img_data/2509.21117.json), skip HTML parsing.
[26.09.2025 11:09] Success.
[26.09.2025 11:09] Downloading and parsing paper https://huggingface.co/papers/2509.20712.
[26.09.2025 11:09] Extra JSON file exists (./assets/json/2509.20712.json), skip PDF parsing.
[26.09.2025 11:09] Paper image links file exists (./assets/img_data/2509.20712.json), skip HTML parsing.
[26.09.2025 11:09] Success.
[26.09.2025 11:09] Downloading and parsing paper https://huggingface.co/papers/2509.21278.
[26.09.2025 11:09] Extra JSON file exists (./assets/json/2509.21278.json), skip PDF parsing.
[26.09.2025 11:09] Paper image links file exists (./assets/img_data/2509.21278.json), skip HTML parsing.
[26.09.2025 11:09] Success.
[26.09.2025 11:09] Downloading and parsing paper https://huggingface.co/papers/2509.21114.
[26.09.2025 11:09] Extra JSON file exists (./assets/json/2509.21114.json), skip PDF parsing.
[26.09.2025 11:09] Paper image links file exists (./assets/img_data/2509.21114.json), skip HTML parsing.
[26.09.2025 11:09] Success.
[26.09.2025 11:09] Downloading and parsing paper https://huggingface.co/papers/2509.21072.
[26.09.2025 11:09] Extra JSON file exists (./assets/json/2509.21072.json), skip PDF parsing.
[26.09.2025 11:09] Paper image links file exists (./assets/img_data/2509.21072.json), skip HTML parsing.
[26.09.2025 11:09] Success.
[26.09.2025 11:09] Downloading and parsing paper https://huggingface.co/papers/2509.20136.
[26.09.2025 11:09] Extra JSON file exists (./assets/json/2509.20136.json), skip PDF parsing.
[26.09.2025 11:09] Paper image links file exists (./assets/img_data/2509.20136.json), skip HTML parsing.
[26.09.2025 11:09] Success.
[26.09.2025 11:09] Downloading and parsing paper https://huggingface.co/papers/2509.21317.
[26.09.2025 11:09] Extra JSON file exists (./assets/json/2509.21317.json), skip PDF parsing.
[26.09.2025 11:09] Paper image links file exists (./assets/img_data/2509.21317.json), skip HTML parsing.
[26.09.2025 11:09] Success.
[26.09.2025 11:09] Downloading and parsing paper https://huggingface.co/papers/2509.21106.
[26.09.2025 11:09] Downloading paper 2509.21106 from http://arxiv.org/pdf/2509.21106v1...
[26.09.2025 11:09] Extracting affiliations from text.
[26.09.2025 11:09] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 5 2 ] . [ 1 6 0 1 1 2 . 9 0 5 2 : r Preprint. BESPOKE: BENCHMARK FOR SEARCH-AUGMENTED LARGE LANGUAGE MODEL PERSONALIZATION VIA DIAGNOSTIC FEEDBACK Hyunseo Kim, Sangam Lee, Kwangwook Seo, Dongha Lee Yonsei University {hyunseo00, salee, tommy2130, donalee}@yonsei.ac.kr "
[26.09.2025 11:09] Response: ```python
["Yonsei University"]
```
[26.09.2025 11:09] Deleting PDF ./assets/pdf/2509.21106.pdf.
[26.09.2025 11:09] Success.
[26.09.2025 11:09] Downloading and parsing paper https://huggingface.co/papers/2509.19301.
[26.09.2025 11:09] Extra JSON file exists (./assets/json/2509.19301.json), skip PDF parsing.
[26.09.2025 11:09] Paper image links file exists (./assets/img_data/2509.19301.json), skip HTML parsing.
[26.09.2025 11:09] Success.
[26.09.2025 11:09] Downloading and parsing paper https://huggingface.co/papers/2509.21318.
[26.09.2025 11:09] Extra JSON file exists (./assets/json/2509.21318.json), skip PDF parsing.
[26.09.2025 11:09] Paper image links file exists (./assets/img_data/2509.21318.json), skip HTML parsing.
[26.09.2025 11:09] Success.
[26.09.2025 11:09] Downloading and parsing paper https://huggingface.co/papers/2509.21302.
[26.09.2025 11:09] Extra JSON file exists (./assets/json/2509.21302.json), skip PDF parsing.
[26.09.2025 11:09] Paper image links file exists (./assets/img_data/2509.21302.json), skip HTML parsing.
[26.09.2025 11:09] Success.
[26.09.2025 11:09] Downloading and parsing paper https://huggingface.co/papers/2509.20186.
[26.09.2025 11:09] Extra JSON file exists (./assets/json/2509.20186.json), skip PDF parsing.
[26.09.2025 11:09] Paper image links file exists (./assets/img_data/2509.20186.json), skip HTML parsing.
[26.09.2025 11:09] Success.
[26.09.2025 11:09] Downloading and parsing paper https://huggingface.co/papers/2509.20414.
[26.09.2025 11:09] Extra JSON file exists (./assets/json/2509.20414.json), skip PDF parsing.
[26.09.2025 11:09] Paper image links file exists (./assets/img_data/2509.20414.json), skip HTML parsing.
[26.09.2025 11:09] Success.
[26.09.2025 11:09] Downloading and parsing paper https://huggingface.co/papers/2509.21070.
[26.09.2025 11:09] Extra JSON file exists (./assets/json/2509.21070.json), skip PDF parsing.
[26.09.2025 11:09] Paper image links file exists (./assets/img_data/2509.21070.json), skip HTML parsing.
[26.09.2025 11:09] Success.
[26.09.2025 11:09] Downloading and parsing paper https://huggingface.co/papers/2509.21042.
[26.09.2025 11:09] Extra JSON file exists (./assets/json/2509.21042.json), skip PDF parsing.
[26.09.2025 11:09] Paper image links file exists (./assets/img_data/2509.21042.json), skip HTML parsing.
[26.09.2025 11:09] Success.
[26.09.2025 11:09] Downloading and parsing paper https://huggingface.co/papers/2509.14662.
[26.09.2025 11:09] Extra JSON file exists (./assets/json/2509.14662.json), skip PDF parsing.
[26.09.2025 11:09] Paper image links file exists (./assets/img_data/2509.14662.json), skip HTML parsing.
[26.09.2025 11:09] Success.
[26.09.2025 11:09] Downloading and parsing paper https://huggingface.co/papers/2509.20878.
[26.09.2025 11:09] Extra JSON file exists (./assets/json/2509.20878.json), skip PDF parsing.
[26.09.2025 11:09] Paper image links file exists (./assets/img_data/2509.20878.json), skip HTML parsing.
[26.09.2025 11:09] Success.
[26.09.2025 11:09] Downloading and parsing paper https://huggingface.co/papers/2509.21113.
[26.09.2025 11:09] Extra JSON file exists (./assets/json/2509.21113.json), skip PDF parsing.
[26.09.2025 11:09] Paper image links file exists (./assets/img_data/2509.21113.json), skip HTML parsing.
[26.09.2025 11:09] Success.
[26.09.2025 11:09] Downloading and parsing paper https://huggingface.co/papers/2509.20868.
[26.09.2025 11:09] Extra JSON file exists (./assets/json/2509.20868.json), skip PDF parsing.
[26.09.2025 11:09] Paper image links file exists (./assets/img_data/2509.20868.json), skip HTML parsing.
[26.09.2025 11:09] Success.
[26.09.2025 11:09] Downloading and parsing paper https://huggingface.co/papers/2509.20293.
[26.09.2025 11:09] Downloading paper 2509.20293 from http://arxiv.org/pdf/2509.20293v1...
[26.09.2025 11:09] Extracting affiliations from text.
[26.09.2025 11:09] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 4 2 ] . [ 1 3 9 2 0 2 . 9 0 5 2 : r WHEN JUDGMENT BECOMES NOISE: HOW DESIGN FAILURES IN LLM JUDGE BENCHMARKS SILENTLY UNDERMINE VALIDITY Benjamin Feuer, Chiung-Yi Tseng, Astitwa Sarthak Lathe, Oussama Elachqar, John Dickerson "
[26.09.2025 11:09] Response: []
[26.09.2025 11:09] Extracting affiliations from text.
[26.09.2025 11:09] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 4 2 ] . [ 1 3 9 2 0 2 . 9 0 5 2 : r WHEN JUDGMENT BECOMES NOISE: HOW DESIGN FAILURES IN LLM JUDGE BENCHMARKS SILENTLY UNDERMINE VALIDITY Benjamin Feuer, Chiung-Yi Tseng, Astitwa Sarthak Lathe, Oussama Elachqar, John DickersonLLM-judged benchmarks are increasingly used to evaluate complex model behaviors, yet their design introduces failure modes absent in conventional, groundtruthbased benchmarks. We argue that, without tight objectives and verifiable constructions, benchmark rankings can produce high-confidence rankings that are in fact largely noise. We introduce two mechanisms to diagnose these issues. Schematic adherence quantifies how much of judges overall verdict is explained by the explicit evaluation schema, revealing unexplained variance when judges deviate from their own rubric. Psychometric validity aggregates internal consistency and discriminant validity signals to quantify irreducible uncertainty in any benchmarking run. Applying these tools to Arena-Hard Auto, we find severe schema incoherence and factor collapse across popular judges: e.g., unexplained variance exceeding 90% for DeepSeek-R1-32B and factor correlations above 0.93 for most criteria. We also show that the ELO-style aggregation used by Arena-Hard Auto collapses and masks genuine ranking uncertainty. Our results highlight design failures that undermine validity and offer actionable principles for building better-scoped, reliability-aware LLM-judged benchmarks. We release our code at https://anonymous.4open.science/r/judgment-to-noise-947D/README.mdAs the world grows increasingly saturated with AI-supplemented and AI-generated content, traditional evaluation and judgment mechanisms are struggling to keep up, leading some to propose AI as the solution to its own problem Gillespie (2020). LLM judges promise rapid, scalable evaluation of complex, open-ended tasks; far from being purely theoretical concern, the deployment of LLMs in real settings with real stakes is well underway. For instance, the 2026 AAAI Conference added an LLM judge (AI-Powered Peer Review System) to the panel of reviewers for its scientific submissions, with mixed results AAAI (2025). But how far can we trust LLM judges? Are they actually capable of delivering on their promise? Notwithstanding their complexity, these questions are ones that the benchmarking and evaluation research communities are duty-bound to address, and in recent years, there have been many attempts to do so Santurkar et al. (2024); Mazeika et al. (2024); Wu et al. (2024); Zhou et al. (2024). Because ground truth in open-ended judgments is difficult and expensive to obtain, many benchmarks of LLM judges utilizes LLM judges themselves. In the last few years, such benchmarks have been widely deployed in the alignment and reinforcement learning literature in particular, and their scores have been used as guiding signal for numerous accepted conference submissions Feuer et al. (2024). From these observations, we can deduce that one promising avenue for evaluating LLM judges is conducting meta-analyses on the benchmarks that utilize them. One particular design choice, the selection and technical deployment of the LLM judge, has been extensively critiqued and analyzed, which has in turn lead to important reforms and revisions in best practices Santurkar et al. (2024); Wu et al. (2023; 2024); Feuer et al. (2024). Other key design decisions in these benchmarks, however, remain understudied. What makes judgment rubric valid? What kind of metrics can benchmark designers employ to ensure that their judgment criteria are meaningfully distinct in benchmark space? Are the questions and baseline models in the benchmark suitably selected to produce meaningful comparisons? If the questions 1 and rubric are inadvertently mismatched, the measure may not be meaningful for some criteria. Last but not least, what metrics should we use to reliably score LLM-judged benchmarks? Numeric scores are an option, but they tend to transfer unreliably between judges without calibration. ELOstyle comparisons enforce transitive preferences and exaggerate distinctions, but in many real-world cases, model preference is non-transitive and the best we can hope for is to know what we dont know, appropriately grounding benchmarks in uncertainty. Figure 1: The majority of true judgment variance has no known cause. On the Arena-Hard-Auto benchmark, with rubric specifying 5 judgment criteria, we find that across four judges and two settings (different cohorts of models to be compared), approximately 55% of variance, on average, is unexplained either by linear or taylor-series polynomial factor analysis on the rubric criteria. After ELO transformation, the linear model explains 100% of observed variance, indicating that, by enforcing transitivity, ELO hides true latent uncertainty in multi-factor analysis. In this paper, we propose set of novel diagnostic metrics which aim to automate the assessment of common confounds in LLM-judged benchmarks, such as whether the data selected and models surveyed form an adequate artifact for meaningful analysis, and whether the selected judgment metrics effectively capture uncertainty. We then conduct large-scale empirical analysis on popular public benchmark using our metrics. From this, we discover that popular LLM-judged benchmarks contain severe and previously undocumented failure modes; many widely used LLM judges do not faithfully implement assigned schemas, popular benchmarks cannot produce statistically or practically significant measures the things their rubrics claim, and post-processing (e.g., ELO/BradleyTerry rankings) mask this reality, producing nice-looking rankings that can fail to reflect true preference. We hope that this work induces transition toward reliability-aware benchmark design that foreground validity rather than appearance of stability.Mechanisms: We introduce two novel diagnostic metrics for LLM judge benchmarks (1) Schematic adherence quantifies how well overall verdicts derive from factorwise rubric scores. (2) Psychometric validity aggregates internal consistency and discriminant validity to quantify the degree to which benchmarks design fits with its judgment rubric. Case study: Applying both mechanisms to Arena-Hard Auto (Li et al., 2024b), we uncover severe rubric incoherence and factor collapse across judges (e.g., 90% unexplained variance for DeepSeek-R1-32B; factor correlations 0.93 across criteria), and show that ELO-style "
[26.09.2025 11:10] Mistral response. {"id": "25443b975d0d4a9391cb545576a166e6", "created": 1758884999, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1443, "total_tokens": 1473, "completion_tokens": 30}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\n    \"University of Maryland, College Park\",\n    \"Google DeepMind\",\n    \"University of California, Berkeley\"\n]\n```"}}]}
[26.09.2025 11:10] Response: ```python
[
    "University of Maryland, College Park",
    "Google DeepMind",
    "University of California, Berkeley"
]
```
[26.09.2025 11:10] Deleting PDF ./assets/pdf/2509.20293.pdf.
[26.09.2025 11:10] Success.
[26.09.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2509.20109.
[26.09.2025 11:10] Extra JSON file exists (./assets/json/2509.20109.json), skip PDF parsing.
[26.09.2025 11:10] Paper image links file exists (./assets/img_data/2509.20109.json), skip HTML parsing.
[26.09.2025 11:10] Success.
[26.09.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2509.20394.
[26.09.2025 11:10] Extra JSON file exists (./assets/json/2509.20394.json), skip PDF parsing.
[26.09.2025 11:10] Paper image links file exists (./assets/img_data/2509.20394.json), skip HTML parsing.
[26.09.2025 11:10] Success.
[26.09.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2509.20706.
[26.09.2025 11:10] Extra JSON file exists (./assets/json/2509.20706.json), skip PDF parsing.
[26.09.2025 11:10] Paper image links file exists (./assets/img_data/2509.20706.json), skip HTML parsing.
[26.09.2025 11:10] Success.
[26.09.2025 11:10] Enriching papers with extra data.
[26.09.2025 11:10] ********************************************************************************
[26.09.2025 11:10] Abstract 0. Variance-Aware Sampling and large-scale CoT data improve multimodal reasoning models by stabilizing RL fine-tuning and enhancing performance on benchmarks.  					AI-generated summary 				 Large multimodal reasoning models have achieved rapid progress, but their advancement is constrained by two majo...
[26.09.2025 11:10] ********************************************************************************
[26.09.2025 11:10] Abstract 1. A scientific reasoning foundation model pre-trained on diverse scientific data supports multiple tasks and enhances cross-domain generalization and fidelity through specialized training techniques.  					AI-generated summary 				 We present a scientific reasoning foundation model that aligns natural...
[26.09.2025 11:10] ********************************************************************************
[26.09.2025 11:10] Abstract 2. Tree-based Group Relative Policy Optimization (Tree-GRPO) enhances reinforcement learning for large language models by using tree search to improve rollouts and estimate grouped relative advantages, outperforming chain-based methods.  					AI-generated summary 				 Recent advances in reinforcement l...
[26.09.2025 11:10] ********************************************************************************
[26.09.2025 11:10] Abstract 3. A curriculum reinforcement learning framework dynamically adjusts training sample difficulty based on reward variance, improving LLM performance on mathematical reasoning tasks.  					AI-generated summary 				 Policy-based reinforcement learning currently plays an important role in improving LLMs on...
[26.09.2025 11:10] ********************************************************************************
[26.09.2025 11:10] Abstract 4. Seedream 4.0 is a high-performance multimodal image generation system that integrates text-to-image synthesis, image editing, and multi-image composition using a diffusion transformer and VAE, achieving state-of-the-art results with efficient training and inference.  					AI-generated summary 				 W...
[26.09.2025 11:10] ********************************************************************************
[26.09.2025 11:10] Abstract 5. Hunyuan3D-Omni is a unified 3D asset generation framework that accepts multiple conditioning signals, improving controllability and robustness in production workflows.  					AI-generated summary 				 Recent advances in 3D-native generative models have accelerated asset creation for games, film, and ...
[26.09.2025 11:10] ********************************************************************************
[26.09.2025 11:10] Abstract 6. AutoIntent is an automated machine learning tool for text classification that offers end-to-end automation, including embedding model selection, classifier optimization, and decision threshold tuning, and supports multi-label classification and out-of-scope detection.  					AI-generated summary 				...
[26.09.2025 11:10] ********************************************************************************
[26.09.2025 11:10] Abstract 7. TrustJudge, a probabilistic framework, addresses inconsistencies in LLM-as-a-judge evaluation by using distribution-sensitive scoring and likelihood-aware aggregation, improving accuracy and reliability.  					AI-generated summary 				 The adoption of Large Language Models (LLMs) as automated evalua...
[26.09.2025 11:10] ********************************************************************************
[26.09.2025 11:10] Abstract 8. A novel reinforcement learning algorithm, CE-GPPO, reintroduces gradients from clipped tokens to improve the exploration-exploitation balance in training large language models.  					AI-generated summary 				 Reinforcement learning (RL) has become a powerful paradigm for optimizing large language mo...
[26.09.2025 11:10] ********************************************************************************
[26.09.2025 11:10] Abstract 9. SHINE is a training-free framework that uses manifold-steered anchor loss and pretrained customization adapters to seamlessly insert objects into new scenes with high fidelity, addressing challenges like complex lighting and diverse inputs.  					AI-generated summary 				 Image composition aims to s...
[26.09.2025 11:10] ********************************************************************************
[26.09.2025 11:10] Abstract 10. CHARM uses a control-point-based parameterization and autoregressive transformer to generate high-fidelity anime hairstyles efficiently.  					AI-generated summary 				 We present CHARM, a novel parametric representation and generative framework for anime hairstyle modeling. While traditional hair m...
[26.09.2025 11:10] ********************************************************************************
[26.09.2025 11:10] Abstract 11. Recon-Act, a self-evolving multi-agent framework, improves adaptability and performance on long-horizon web tasks by generating and utilizing generalized tools through reconnaissance and action teams.  					AI-generated summary 				 Recent years, multimodal models have made remarkable strides and pa...
[26.09.2025 11:10] ********************************************************************************
[26.09.2025 11:10] Abstract 12. V-GameGym is a comprehensive benchmark for evaluating code generation in game development, focusing on multimodal evaluation including playability, visual aesthetics, and user engagement.  					AI-generated summary 				 Code large language models have demonstrated remarkable capabilities in programm...
[26.09.2025 11:10] ********************************************************************************
[26.09.2025 11:10] Abstract 13. IRF, a new recommendation system using natural language commands, improves user satisfaction and business outcomes through a dual-agent architecture and simulation-augmented knowledge distillation.  					AI-generated summary 				 Traditional recommender systems rely on passive feedback mechanisms th...
[26.09.2025 11:10] ********************************************************************************
[26.09.2025 11:10] Abstract 14. BESPOKE is a benchmark for evaluating personalization in search-augmented LLMs using authentic user data and detailed feedback.  					AI-generated summary 				 Search-augmented large language models (LLMs) have advanced information-seeking tasks by integrating retrieval into generation, reducing use...
[26.09.2025 11:10] ********************************************************************************
[26.09.2025 11:10] Abstract 15. A residual learning framework combines behavior cloning and reinforcement learning to improve manipulation policies on high-degree-of-freedom systems using sparse binary rewards.  					AI-generated summary 				 Recent advances in behavior cloning (BC) have enabled impressive visuomotor control polic...
[26.09.2025 11:10] ********************************************************************************
[26.09.2025 11:10] Abstract 16. SD3.5-Flash is an efficient few-step distillation framework that enhances image generation on consumer devices using rectified flow models with innovations like timestep sharing and split-timestep fine-tuning.  					AI-generated summary 				 We present SD3.5-Flash, an efficient few-step distillation...
[26.09.2025 11:10] ********************************************************************************
[26.09.2025 11:10] Abstract 17. QuantVGGT, a quantization framework for Visual Geometry Grounded Transformers, achieves state-of-the-art results with memory reduction and acceleration while maintaining high reconstruction accuracy.  					AI-generated summary 				 Learning-based 3D reconstruction models, represented by Visual Geome...
[26.09.2025 11:10] ********************************************************************************
[26.09.2025 11:10] Abstract 18. Thinking augmented pre-training improves data efficiency and performance of large language models by augmenting text with automatically generated thinking trajectories.  					AI-generated summary 				 This paper introduces a simple and scalable approach to improve the data efficiency of large langua...
[26.09.2025 11:10] ********************************************************************************
[26.09.2025 11:10] Abstract 19. SceneWeaver, a reflective agentic framework, uses a language model-based planner to iteratively refine 3D scene synthesis, achieving high physical, visual, and semantic quality across diverse instructions.  					AI-generated summary 				 Indoor scene synthesis has become increasingly important with ...
[26.09.2025 11:10] ********************************************************************************
[26.09.2025 11:10] Abstract 20. ScaleDiff uses an adaptive thinking model to identify and generate difficult mathematical problems, improving the performance of large reasoning models with cost-efficient training.  					AI-generated summary 				 Large Reasoning Models (LRMs) have shown impressive capabilities in complex problem-so...
[26.09.2025 11:10] ********************************************************************************
[26.09.2025 11:10] Abstract 21. The causal mask in Transformer decoders induces position-dependent attention patterns, which can interact with explicit positional encodings like RoPE, affecting their relative attention score patterns.  					AI-generated summary 				 While explicit positional encodings such as RoPE are a primary so...
[26.09.2025 11:10] ********************************************************************************
[26.09.2025 11:10] Abstract 22. A novel framework using Schoenfeld's Episode Theory is introduced to analyze the reasoning patterns of Large Reasoning Models in solving math problems, providing a benchmark for machine reasoning.  					AI-generated summary 				 While Large Reasoning Models (LRMs) generate extensive chain-of-thought...
[26.09.2025 11:10] ********************************************************************************
[26.09.2025 11:10] Abstract 23. The study reveals an asymmetry between perceptual optimization and image quality assessment, showing that effective IQA metrics are not always suitable for perceptual optimization, especially under adversarial training, and highlights the importance of discriminator design in optimization.  					AI-...
[26.09.2025 11:10] ********************************************************************************
[26.09.2025 11:10] Abstract 24. MOSS-ChatV, a reinforcement learning framework with a DTW-based reward, improves video reasoning consistency and performance across various benchmarks.  					AI-generated summary 				 Video reasoning has emerged as a critical capability for multimodal large language models (MLLMs), requiring models ...
[26.09.2025 11:10] ********************************************************************************
[26.09.2025 11:10] Abstract 25. StyleBench evaluates various reasoning styles across tasks and models, revealing that strategy efficacy depends on model scale and task type.  					AI-generated summary 				 The effectiveness of Large Language Models (LLMs) is heavily influenced by the reasoning strategies, or styles of thought, emp...
[26.09.2025 11:10] ********************************************************************************
[26.09.2025 11:10] Abstract 26. LLM-judged benchmarks can produce unreliable rankings due to schema incoherence and factor collapse, which are diagnosed using schematic adherence and psychometric validity.  					AI-generated summary 				 LLM-judged benchmarks are increasingly used to evaluate complex model behaviors, yet their des...
[26.09.2025 11:10] ********************************************************************************
[26.09.2025 11:10] Abstract 27. ReflectDrive uses a reflection mechanism with discrete diffusion and pre-trained Diffusion Language Models to generate safe trajectories for autonomous driving systems.  					AI-generated summary 				 End-to-End (E2E) solutions have emerged as a mainstream approach for autonomous driving systems, wi...
[26.09.2025 11:10] ********************************************************************************
[26.09.2025 11:10] Abstract 28. The Hazard-Aware System Card (HASC) enhances AI system safety and accountability by integrating security and safety identifiers into a standardized framework.  					AI-generated summary 				 This paper introduces the Hazard-Aware System Card (HASC), a novel framework designed to enhance transparency...
[26.09.2025 11:10] ********************************************************************************
[26.09.2025 11:10] Abstract 29. MI-Fuse, a denoised label fusion framework, enhances speech emotion recognition in target domains using an API-only LALM and a source-domain SER classifier, achieving better performance than the LALM and other baselines.  					AI-generated summary 				 Large audio-language models (LALMs) show strong...
[26.09.2025 11:10] Read previous papers.
[26.09.2025 11:10] Generating reviews via LLM API.
[26.09.2025 11:10] Using data from previous issue: {"categories": ["#dataset", "#training", "#architecture", "#reasoning", "#benchmark", "#rl", "#optimization", "#multimodal", "#data", "#open_source"], "emoji": "🎯", "ru": {"title": "Стабилизация RL-обучения через управление дисперсией вознаграждений", "desc": "Исследователи предложили метод Variance
[26.09.2025 11:10] Using data from previous issue: {"categories": ["#dataset", "#training", "#reasoning", "#transfer_learning", "#multimodal", "#data", "#science", "#open_source"], "emoji": "🔬", "ru": {"title": "Универсальный AI для научных рассуждений во всех дисциплинах", "desc": "Исследователи создали foundation модель для научных рассуждений, ко
[26.09.2025 11:10] Using data from previous issue: {"categories": ["#reasoning", "#rlhf", "#rl", "#optimization"], "emoji": "🌳", "ru": {"title": "Древовидный поиск для умного обучения AI-агентов", "desc": "Исследователи предложили Tree-GRPO - новый метод обучения с подкреплением для больших языковых моделей, основанный на поиске по дереву. Метод реш
[26.09.2025 11:10] Using data from previous issue: {"categories": ["#math", "#training", "#rl", "#optimization", "#reasoning"], "emoji": "📈", "ru": {"title": "Обучение через дисперсию: адаптивная сложность для математического мышления LLM", "desc": "Исследователи предложили VCRL - новый подход к обучению языковых моделей решению математических задач
[26.09.2025 11:10] Using data from previous issue: {"categories": ["#inference", "#training", "#games", "#multimodal", "#cv", "#diffusion"], "emoji": "🎨", "ru": {"title": "Универсальная система для генерации и редактирования изображений нового поколения", "desc": "Seedream 4.0 — это высокопроизводительная мультимодальная система генерации изображени
[26.09.2025 11:10] Using data from previous issue: {"categories": ["#3d", "#training", "#architecture", "#games", "#synthetic", "#multimodal"], "emoji": "🎮", "ru": {"title": "Многомодальный контроль 3D-генерации для игровой индустрии", "desc": "Исследователи представили Hunyuan3D-Omni — единую систему для генерации 3D-объектов с множественным контро
[26.09.2025 11:10] Using data from previous issue: {"categories": ["#dataset", "#optimization", "#training", "#data"], "emoji": "🎯", "ru": {"title": "Полная автоматизация классификации текстов от эмбеддингов до решений", "desc": "AutoIntent представляет собой инструмент автоматизированного машинного обучения для классификации текстов. Система обеспе
[26.09.2025 11:10] Using data from previous issue: {"categories": ["#architecture", "#alignment", "#interpretability", "#data", "#benchmark"], "emoji": "⚖️", "ru": {"title": "Делаем LLM-судей честными: вероятностный подход против противоречий в оценках", "desc": "Исследователи выявили критические проблемы в системах оценки, где LLM выступают в роли 
[26.09.2025 11:10] Using data from previous issue: {"categories": ["#training", "#reasoning", "#optimization", "#rl"], "emoji": "⚖️", "ru": {"title": "Сохраняем градиенты для лучшего баланса в обучении LLM", "desc": "В работе предлагается новый алгоритм обучения с подкреплением CE-GPPO для оптимизации больших языковых моделей. Основная проблема суще
[26.09.2025 11:10] Using data from previous issue: {"categories": ["#training", "#optimization", "#diffusion", "#benchmark", "#cv", "#open_source"], "emoji": "✨", "ru": {"title": "Безупречная вставка объектов в сцены без переобучения", "desc": "Статья представляет SHINE - фреймворк без обучения для бесшовной вставки объектов в новые сцены с высокой 
[26.09.2025 11:10] Using data from previous issue: {"categories": ["#games", "#synthetic", "#training", "#cv", "#architecture", "#dataset"], "emoji": "💇", "ru": {"title": "AI создаёт аниме-причёски через \"язык волос\"", "desc": "В статье представлена CHARM - новая система для создания аниме-причёсок с помощью AI. Вместо традиционных методов моделир
[26.09.2025 11:10] Using data from previous issue: {"categories": ["#dataset", "#agents", "#multimodal", "#optimization", "#agi"], "emoji": "🕵️", "ru": {"title": "Разведка и действие: самообучающиеся агенты для веб-задач", "desc": "Статья представляет Recon-Act - самоэволюционирующий мульти-агентный фреймворк для решения долгосрочных задач в веб-бра
[26.09.2025 11:10] Using data from previous issue: {"categories": ["#optimization", "#benchmark", "#games", "#multimodal", "#dataset"], "emoji": "🎮", "ru": {"title": "Новый стандарт оценки AI в геймдеве: от кода к играбельности", "desc": "V-GameGym - это комплексный бенчмарк для оценки генерации кода в разработке игр, который включает мультимодальну
[26.09.2025 11:10] Using data from previous issue: {"categories": ["#training", "#architecture", "#reasoning", "#optimization", "#multimodal", "#agents"], "emoji": "🗣️", "ru": {"title": "Управляй рекомендациями голосом - говори системе, что хочешь увидеть", "desc": "Исследователи представили Interactive Recommendation Feed (IRF) - новую парадигму ре
[26.09.2025 11:10] Querying the API.
[26.09.2025 11:10] Claude request. Model: claude-sonnet-4-20250514. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

BESPOKE is a benchmark for evaluating personalization in search-augmented LLMs using authentic user data and detailed feedback.  					AI-generated summary 				 Search-augmented large language models (LLMs) have advanced information-seeking tasks by integrating retrieval into generation, reducing users' cognitive burden compared to traditional search systems. Yet they remain insufficient for fully addressing diverse user needs, which requires recognizing how the same query can reflect different intents across users and delivering information in preferred forms. While recent systems such as ChatGPT and Gemini attempt personalization by leveraging user histories, systematic evaluation of such personalization is under-explored. To address this gap, we propose BESPOKE, the realistic benchmark for evaluating personalization in search-augmented LLMs. BESPOKE is designed to be both realistic, by collecting authentic chat and search histories directly from humans, and diagnostic, by pairing responses with fine-grained preference scores and feedback. The benchmark is constructed through long-term, deeply engaged human annotation, where human annotators contributed their own histories, authored queries with detailed information needs, and evaluated responses with scores and diagnostic feedback. Leveraging BESPOKE, we conduct systematic analyses that reveal key requirements for effective personalization in information-seeking tasks, providing a foundation for fine-grained evaluation of personalized search-augmented LLMs. Our code and data are available at https://augustinlib.github.io/BESPOKE/.
[26.09.2025 11:10] Response: ```json
{
  "desc": "Исследователи создали бенчмарк BESPOKE для оценки персонализации в поисковых LLM, которые используют извлечение информации для генерации ответов. Существующие системы как ChatGPT и Gemini пытаются персонализировать ответы на основе истории пользователей, но систематическая оценка такой персонализации была недостаточно изучена. Бенчмарк построен на реальных данных чатов и поисковых запросов людей с детальной обратной связью и оценками предпочтений. Исследование выявляет ключевые требования для эффективной персонализации в задачах поиска информации.",
  "emoji": "🎯",
  "title": "Персонализация поисковых LLM под каждого пользователя"
}
```
[26.09.2025 11:10] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"BESPOKE is a benchmark for evaluating personalization in search-augmented LLMs using authentic user data and detailed feedback.  					AI-generated summary 				 Search-augmented large language models (LLMs) have advanced information-seeking tasks by integrating retrieval into generation, reducing users' cognitive burden compared to traditional search systems. Yet they remain insufficient for fully addressing diverse user needs, which requires recognizing how the same query can reflect different intents across users and delivering information in preferred forms. While recent systems such as ChatGPT and Gemini attempt personalization by leveraging user histories, systematic evaluation of such personalization is under-explored. To address this gap, we propose BESPOKE, the realistic benchmark for evaluating personalization in search-augmented LLMs. BESPOKE is designed to be both realistic, by collecting authentic chat and search histories directly from humans, and diagnostic, by pairing responses with fine-grained preference scores and feedback. The benchmark is constructed through long-term, deeply engaged human annotation, where human annotators contributed their own histories, authored queries with detailed information needs, and evaluated responses with scores and diagnostic feedback. Leveraging BESPOKE, we conduct systematic analyses that reveal key requirements for effective personalization in information-seeking tasks, providing a foundation for fine-grained evaluation of personalized search-augmented LLMs. Our code and data are available at https://augustinlib.github.io/BESPOKE/."

[26.09.2025 11:10] Response: ```python
['BENCHMARK', 'DATASET', 'MULTIMODAL']
```
[26.09.2025 11:10] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"BESPOKE is a benchmark for evaluating personalization in search-augmented LLMs using authentic user data and detailed feedback.  					AI-generated summary 				 Search-augmented large language models (LLMs) have advanced information-seeking tasks by integrating retrieval into generation, reducing users' cognitive burden compared to traditional search systems. Yet they remain insufficient for fully addressing diverse user needs, which requires recognizing how the same query can reflect different intents across users and delivering information in preferred forms. While recent systems such as ChatGPT and Gemini attempt personalization by leveraging user histories, systematic evaluation of such personalization is under-explored. To address this gap, we propose BESPOKE, the realistic benchmark for evaluating personalization in search-augmented LLMs. BESPOKE is designed to be both realistic, by collecting authentic chat and search histories directly from humans, and diagnostic, by pairing responses with fine-grained preference scores and feedback. The benchmark is constructed through long-term, deeply engaged human annotation, where human annotators contributed their own histories, authored queries with detailed information needs, and evaluated responses with scores and diagnostic feedback. Leveraging BESPOKE, we conduct systematic analyses that reveal key requirements for effective personalization in information-seeking tasks, providing a foundation for fine-grained evaluation of personalized search-augmented LLMs. Our code and data are available at https://augustinlib.github.io/BESPOKE/."

[26.09.2025 11:10] Response: ```python
["ALIGNMENT", "SURVEY"]
```
[26.09.2025 11:10] Response: ParsedChatCompletionMessage[Article](content='{"desc":"BESPOKE is a new benchmark designed to evaluate how well search-augmented large language models (LLMs) personalize responses based on user data and feedback. It addresses the challenge of understanding different user intents behind the same query and aims to improve the relevance of information provided. By collecting real user chat and search histories, BESPOKE allows for a detailed analysis of how well these models meet individual preferences. This systematic evaluation helps identify essential factors for effective personalization in information-seeking tasks, paving the way for advancements in personalized AI systems.","title":"BESPOKE: Personalization Benchmark for Search-Augmented LLMs"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='BESPOKE is a new benchmark designed to evaluate how well search-augmented large language models (LLMs) personalize responses based on user data and feedback. It addresses the challenge of understanding different user intents behind the same query and aims to improve the relevance of information provided. By collecting real user chat and search histories, BESPOKE allows for a detailed analysis of how well these models meet individual preferences. This systematic evaluation helps identify essential factors for effective personalization in information-seeking tasks, paving the way for advancements in personalized AI systems.', title='BESPOKE: Personalization Benchmark for Search-Augmented LLMs'))
[26.09.2025 11:10] Response: ParsedChatCompletionMessage[Article](content='{"desc":"BESPOKE是一个用于评估搜索增强型大语言模型（LLMs）个性化效果的基准，使用真实用户数据和详细反馈。该基准旨在通过收集真实的聊天和搜索历史，帮助识别用户在相同查询下的不同意图，并提供用户偏好的信息形式。尽管现有系统如ChatGPT和Gemini尝试通过用户历史实现个性化，但对这种个性化的系统评估仍然不足。BESPOKE通过长期的人工注释，结合用户的历史和反馈，提供了一个有效的个性化评估基础。","title":"BESPOKE：个性化搜索的评估新基准"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='BESPOKE是一个用于评估搜索增强型大语言模型（LLMs）个性化效果的基准，使用真实用户数据和详细反馈。该基准旨在通过收集真实的聊天和搜索历史，帮助识别用户在相同查询下的不同意图，并提供用户偏好的信息形式。尽管现有系统如ChatGPT和Gemini尝试通过用户历史实现个性化，但对这种个性化的系统评估仍然不足。BESPOKE通过长期的人工注释，结合用户的历史和反馈，提供了一个有效的个性化评估基础。', title='BESPOKE：个性化搜索的评估新基准'))
[26.09.2025 11:10] Using data from previous issue: {"categories": ["#games", "#optimization", "#rl", "#robotics", "#training"], "emoji": "🤖", "ru": {"title": "Остаточное обучение: от имитации к автономности", "desc": "Исследователи предложили новый подход, который объединяет behavior cloning и reinforcement learning через остаточное обучение. Метод 
[26.09.2025 11:10] Using data from previous issue: {"categories": ["#dataset", "#training", "#inference", "#optimization", "#data", "#cv", "#diffusion"], "emoji": "⚡", "ru": {"title": "Быстрая генерация изображений для всех устройств", "desc": "SD3.5-Flash представляет эффективный фреймворк дистилляции для генерации изображений за несколько шагов на
[26.09.2025 11:10] Using data from previous issue: {"categories": ["#benchmark", "#3d", "#optimization", "#inference"], "emoji": "🧊", "ru": {"title": "Квантизация трансформеров для 3D реконструкции с минимальной потерей качества", "desc": "В статье представлен QuantVGGT - первый фреймворк для квантизации Visual Geometry Grounded Transformers, которы
[26.09.2025 11:10] Using data from previous issue: {"categories": ["#data", "#training", "#reasoning", "#optimization"], "emoji": "🧠", "ru": {"title": "Думающие данные: как траектории рассуждений ускоряют обучение LLM в три раза", "desc": "В статье представлена методология Thinking augmented Pre-Training (TPT), которая улучшает эффективность обучени
[26.09.2025 11:10] Using data from previous issue: {"categories": ["#3d", "#reasoning", "#games", "#alignment", "#agents"], "emoji": "🏠", "ru": {"title": "Умный архитектор: AI-агент создает реалистичные 3D интерьеры через саморефлексию", "desc": "SceneWeaver - это агентная система для синтеза 3D сцен, которая использует языковую модель-планировщик д
[26.09.2025 11:10] Using data from previous issue: {"categories": ["#math", "#training", "#optimization", "#transfer_learning", "#reasoning", "#dataset"], "emoji": "🧮", "ru": {"title": "Масштабируемая генерация сложных математических задач для обучения моделей рассуждения", "desc": "ScaleDiff представляет новый подход для создания сложных математиче
[26.09.2025 11:10] Using data from previous issue: {"categories": ["#math", "#optimization", "#architecture", "#interpretability"], "emoji": "🎭", "ru": {"title": "Каузальная маска как скрытый источник позиционной информации", "desc": "Исследователи доказали, что каузальная маска в декодерах Transformer создаёт зависящие от позиции паттерны внимания 
[26.09.2025 11:10] Using data from previous issue: {"categories": ["#math", "#reasoning", "#interpretability", "#benchmark"], "emoji": "🧠", "ru": {"title": "Картография мышления AI через призму человеческого познания", "desc": "Исследователи применили теорию эпизодов Шёнфельда, классическую когнитивную модель решения математических задач человеком, 
[26.09.2025 11:10] Using data from previous issue: {"categories": ["#training", "#cv", "#optimization"], "emoji": "🔄", "ru": {"title": "Асимметрия между оптимизацией и оценкой качества изображений", "desc": "Исследование выявляет асимметрию между перцептуальной оптимизацией и оценкой качества изображений. Метрики IQA, которые хорошо работают для оце
[26.09.2025 11:10] Using data from previous issue: {"categories": ["#benchmark", "#reasoning", "#video", "#rl", "#interpretability", "#multimodal"], "emoji": "🎬", "ru": {"title": "Согласованные рассуждения о видео через обучение с подкреплением", "desc": "В статье представлена MOSS-ChatV — фреймворк обучения с подкреплением для улучшения рассуждений
[26.09.2025 11:10] Using data from previous issue: {"categories": ["#reasoning", "#open_source", "#benchmark", "#multimodal"], "emoji": "🧠", "ru": {"title": "Размер модели решает, какой стиль рассуждений работает лучше", "desc": "Исследование представляет StyleBench - бенчмарк для оценки различных стилей рассуждений в больших языковых моделях. Автор
[26.09.2025 11:10] Querying the API.
[26.09.2025 11:10] Claude request. Model: claude-sonnet-4-20250514. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

LLM-judged benchmarks can produce unreliable rankings due to schema incoherence and factor collapse, which are diagnosed using schematic adherence and psychometric validity.  					AI-generated summary 				 LLM-judged benchmarks are increasingly used to evaluate complex model behaviors, yet their design introduces failure modes absent in conventional ground-truth based benchmarks. We argue that without tight objectives and verifiable constructions, benchmark rankings can produce high-confidence rankings that are in fact largely noise. We introduce two mechanisms to diagnose these issues. Schematic adherence quantifies how much of a judge's overall verdict is explained by the explicit evaluation schema, revealing unexplained variance when judges deviate from their own rubric. Psychometric validity aggregates internal consistency and discriminant validity signals to quantify irreducible uncertainty in any benchmarking run. Applying these tools to Arena-Hard Auto, we find severe schema incoherence and factor collapse across popular judges: for example, unexplained variance exceeding 90 percent for DeepSeek-R1-32B and factor correlations above 0.93 for most criteria. We also show that the ELO-style aggregation used by Arena-Hard Auto collapses and masks genuine ranking uncertainty. Our results highlight design failures that undermine validity and offer actionable principles for building better-scoped, reliability-aware LLM-judged benchmarks. We release our code at https://anonymous.4open.science/r/judgment-to-noise-947D/README.md
[26.09.2025 11:10] Response: ```json
{
  "desc": "Исследователи обнаружили серьезные проблемы в бенчмарках, где LLM выступают в роли судей для оценки других моделей. Они выявили два ключевых недостатка: схематическую несогласованность (когда судьи отклоняются от собственных критериев оценки) и коллапс факторов (когда разные критерии оценки становятся неразличимыми). Для диагностики этих проблем предложены два метода: измерение схематического соответствия и психометрической валидности. На примере Arena-Hard Auto показано, что популярные LLM-судьи демонстрируют крайне высокую необъяснимую дисперсию (свыше 90% для DeepSeek-R1-32B) и сильную корреляцию между критериями (выше 0.93), что делает их оценки ненадежными.",
  "emoji": "⚖️",
  "title": "LLM-судьи дают ненадежные оценки из-за отклонения от собственных критериев"
}
```
[26.09.2025 11:10] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"LLM-judged benchmarks can produce unreliable rankings due to schema incoherence and factor collapse, which are diagnosed using schematic adherence and psychometric validity.  					AI-generated summary 				 LLM-judged benchmarks are increasingly used to evaluate complex model behaviors, yet their design introduces failure modes absent in conventional ground-truth based benchmarks. We argue that without tight objectives and verifiable constructions, benchmark rankings can produce high-confidence rankings that are in fact largely noise. We introduce two mechanisms to diagnose these issues. Schematic adherence quantifies how much of a judge's overall verdict is explained by the explicit evaluation schema, revealing unexplained variance when judges deviate from their own rubric. Psychometric validity aggregates internal consistency and discriminant validity signals to quantify irreducible uncertainty in any benchmarking run. Applying these tools to Arena-Hard Auto, we find severe schema incoherence and factor collapse across popular judges: for example, unexplained variance exceeding 90 percent for DeepSeek-R1-32B and factor correlations above 0.93 for most criteria. We also show that the ELO-style aggregation used by Arena-Hard Auto collapses and masks genuine ranking uncertainty. Our results highlight design failures that undermine validity and offer actionable principles for building better-scoped, reliability-aware LLM-judged benchmarks. We release our code at https://anonymous.4open.science/r/judgment-to-noise-947D/README.md"

[26.09.2025 11:10] Response: ```python
["BENCHMARK"]
```
[26.09.2025 11:10] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"LLM-judged benchmarks can produce unreliable rankings due to schema incoherence and factor collapse, which are diagnosed using schematic adherence and psychometric validity.  					AI-generated summary 				 LLM-judged benchmarks are increasingly used to evaluate complex model behaviors, yet their design introduces failure modes absent in conventional ground-truth based benchmarks. We argue that without tight objectives and verifiable constructions, benchmark rankings can produce high-confidence rankings that are in fact largely noise. We introduce two mechanisms to diagnose these issues. Schematic adherence quantifies how much of a judge's overall verdict is explained by the explicit evaluation schema, revealing unexplained variance when judges deviate from their own rubric. Psychometric validity aggregates internal consistency and discriminant validity signals to quantify irreducible uncertainty in any benchmarking run. Applying these tools to Arena-Hard Auto, we find severe schema incoherence and factor collapse across popular judges: for example, unexplained variance exceeding 90 percent for DeepSeek-R1-32B and factor correlations above 0.93 for most criteria. We also show that the ELO-style aggregation used by Arena-Hard Auto collapses and masks genuine ranking uncertainty. Our results highlight design failures that undermine validity and offer actionable principles for building better-scoped, reliability-aware LLM-judged benchmarks. We release our code at https://anonymous.4open.science/r/judgment-to-noise-947D/README.md"

[26.09.2025 11:10] Response: ```python
["INTERPRETABILITY", "HALLUCINATIONS"]
```
[26.09.2025 11:10] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper discusses the problems with benchmarks that use large language models (LLMs) for evaluation, highlighting issues like schema incoherence and factor collapse. It introduces two diagnostic tools: schematic adherence, which measures how well judges follow their own evaluation criteria, and psychometric validity, which assesses the reliability of the benchmark results. The authors demonstrate that many popular judges exhibit high levels of unexplained variance and strong correlations between criteria, indicating unreliable rankings. They propose guidelines for creating more reliable benchmarks that can better reflect model performance without the noise introduced by current methods.","title":"Improving Reliability in LLM-Judged Benchmarks"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper discusses the problems with benchmarks that use large language models (LLMs) for evaluation, highlighting issues like schema incoherence and factor collapse. It introduces two diagnostic tools: schematic adherence, which measures how well judges follow their own evaluation criteria, and psychometric validity, which assesses the reliability of the benchmark results. The authors demonstrate that many popular judges exhibit high levels of unexplained variance and strong correlations between criteria, indicating unreliable rankings. They propose guidelines for creating more reliable benchmarks that can better reflect model performance without the noise introduced by current methods.', title='Improving Reliability in LLM-Judged Benchmarks'))
[26.09.2025 11:10] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本文探讨了使用大型语言模型（LLM）评估基准时可能出现的不可靠排名问题，主要由于评估框架不一致和因素崩溃。我们提出了两种机制来诊断这些问题：一是通过评估框架的一致性来量化评审结果的解释程度，二是通过心理测量有效性来评估基准测试中的不确定性。研究发现，流行评审者在评估时存在严重的不一致性，导致高达90%的未解释方差。我们的结果强调了设计缺陷，并提供了构建更可靠的LLM评估基准的可行原则。","title":"提升LLM评估基准的可靠性"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本文探讨了使用大型语言模型（LLM）评估基准时可能出现的不可靠排名问题，主要由于评估框架不一致和因素崩溃。我们提出了两种机制来诊断这些问题：一是通过评估框架的一致性来量化评审结果的解释程度，二是通过心理测量有效性来评估基准测试中的不确定性。研究发现，流行评审者在评估时存在严重的不一致性，导致高达90%的未解释方差。我们的结果强调了设计缺陷，并提供了构建更可靠的LLM评估基准的可行原则。', title='提升LLM评估基准的可靠性'))
[26.09.2025 11:10] Using data from previous issue: {"categories": ["#optimization", "#agents", "#diffusion", "#benchmark", "#multimodal", "#rl"], "emoji": "🚗", "ru": {"title": "Безопасное автономное вождение через рефлексию и дискретную диффузию", "desc": "ReflectDrive представляет новый подход для автономного вождения, использующий механизм рефлекс
[26.09.2025 11:10] Using data from previous issue: {"categories": ["#dataset", "#architecture", "#ethics", "#data", "#security", "#benchmark"], "emoji": "🛡️", "ru": {"title": "Стандартизация безопасности AI через карточки систем с идентификаторами угроз", "desc": "В статье представлена новая система Hazard-Aware System Card (HASC), которая расширяет
[26.09.2025 11:10] Using data from previous issue: {"categories": ["#optimization", "#audio", "#multimodal", "#transfer_learning", "#training"], "emoji": "🎭", "ru": {"title": "Слияние знаний для распознавания эмоций без исходных данных", "desc": "В статье предлагается фреймворк MI-Fuse для адаптации моделей распознавания эмоций в речи к новому домен
[26.09.2025 11:10] Renaming data file.
[26.09.2025 11:10] Renaming previous data. hf_papers.json to ./d/2025-09-26.json
[26.09.2025 11:10] Saving new data file.
[26.09.2025 11:10] Generating page.
[26.09.2025 11:10] Renaming previous page.
[26.09.2025 11:10] Renaming previous data. index.html to ./d/2025-09-26.html
[26.09.2025 11:10] Writing result.
[26.09.2025 11:10] Renaming log file.
[26.09.2025 11:10] Renaming previous data. log.txt to ./logs/2025-09-26_last_log.txt
