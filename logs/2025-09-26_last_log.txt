[26.09.2025 02:22] Read previous papers.
[26.09.2025 02:22] Generating top page (month).
[26.09.2025 02:22] Writing top page (month).
[26.09.2025 03:26] Read previous papers.
[26.09.2025 03:26] Get feed.
[26.09.2025 03:26] Get page data from previous paper. URL: https://huggingface.co/papers/2509.21268
[26.09.2025 03:26] Get page data from previous paper. URL: https://huggingface.co/papers/2509.21320
[26.09.2025 03:26] Get page data from previous paper. URL: https://huggingface.co/papers/2509.20712
[26.09.2025 03:26] Get page data from previous paper. URL: https://huggingface.co/papers/2509.20427
[26.09.2025 03:26] Extract page data from URL. URL: https://huggingface.co/papers/2509.21114
[26.09.2025 03:26] Get page data from previous paper. URL: https://huggingface.co/papers/2509.21317
[26.09.2025 03:26] Extract page data from URL. URL: https://huggingface.co/papers/2509.20186
[26.09.2025 03:27] Get page data from previous paper. URL: https://huggingface.co/papers/2509.21318
[26.09.2025 03:27] Get page data from previous paper. URL: https://huggingface.co/papers/2509.21245
[26.09.2025 03:27] Extract page data from URL. URL: https://huggingface.co/papers/2509.21240
[26.09.2025 03:27] Extract page data from URL. URL: https://huggingface.co/papers/2509.21070
[26.09.2025 03:27] Get page data from previous paper. URL: https://huggingface.co/papers/2509.20868
[26.09.2025 03:27] Get page data from previous paper. URL: https://huggingface.co/papers/2509.20414
[26.09.2025 03:27] Extract page data from URL. URL: https://huggingface.co/papers/2509.19803
[26.09.2025 03:27] Get page data from previous paper. URL: https://huggingface.co/papers/2509.14662
[26.09.2025 03:27] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[26.09.2025 03:27] No deleted papers detected.
[26.09.2025 03:27] Downloading and parsing papers (pdf, html). Total: 15.
[26.09.2025 03:27] Downloading and parsing paper https://huggingface.co/papers/2509.21268.
[26.09.2025 03:27] Extra JSON file exists (./assets/json/2509.21268.json), skip PDF parsing.
[26.09.2025 03:27] Paper image links file exists (./assets/img_data/2509.21268.json), skip HTML parsing.
[26.09.2025 03:27] Success.
[26.09.2025 03:27] Downloading and parsing paper https://huggingface.co/papers/2509.21320.
[26.09.2025 03:27] Extra JSON file exists (./assets/json/2509.21320.json), skip PDF parsing.
[26.09.2025 03:27] Paper image links file exists (./assets/img_data/2509.21320.json), skip HTML parsing.
[26.09.2025 03:27] Success.
[26.09.2025 03:27] Downloading and parsing paper https://huggingface.co/papers/2509.20712.
[26.09.2025 03:27] Extra JSON file exists (./assets/json/2509.20712.json), skip PDF parsing.
[26.09.2025 03:27] Paper image links file exists (./assets/img_data/2509.20712.json), skip HTML parsing.
[26.09.2025 03:27] Success.
[26.09.2025 03:27] Downloading and parsing paper https://huggingface.co/papers/2509.20427.
[26.09.2025 03:27] Downloading paper 2509.20427 from http://arxiv.org/pdf/2509.20427v1...
[26.09.2025 03:27] Extracting affiliations from text.
[26.09.2025 03:27] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Seedream 4.0: Toward Next-generation Multimodal Image Generation "
[26.09.2025 03:27] Response: []
[26.09.2025 03:27] Extracting affiliations from text.
[26.09.2025 03:27] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Seedream 4.0: Toward Next-generation Multimodal Image GenerationWe introduce Seedream 4.0, an efficient and high-performance multimodal image generation system that unifies text-to-image (T2I) synthesis, image editing, and multi-image composition within single framework. We develop highly efficient diffusion transformer with powerful VAE which also can reduce the number of image tokens considerably. This allows for efficient training of our model, and enables it to fast generate native high-resolution images (e.g., 1K-4K). Seedream 4.0 is pretrained on billions of textimage pairs spanning diverse taxonomies and knowledgecentric concepts. Comprehensive data collection across hundreds of vertical scenarios, coupled with optimized strategies, ensures stable and large-scale training, with strong generalization. By incorporating carefully fine-tuned VLM model, we perform multi-modal post-training for training both T2I and image editing tasks jointly. For inference acceleration, we integrate adversarial distillation, distribution matching, and quantization, as well as speculative decoding. It achieves an inference time of up to 1.4 seconds for generating 2K image (without LLM/VLM as PE model). Comprehensive evaluations reveal that Seedream 4.0 can achieve state-of-the-art results on both T2I and multimodal image editing. In particular, it demonstrates exceptional multimodal capabilities in complex tasks, including precise image editing and in-context reasoning, and also allows for multi-image reference, and can generate multiple output images. This extends traditional T2I systems into an more interactive and multidimensional creative tool, pushing the boundary of generative AI for both creativity and professional applications. Seedream 4.0 is now accessible on Volcano Engineα. Official Page: https://seed.bytedance.com/seedream4_0 αModel ID: Doubao-Seedream-4.0 5 2 0 2 4 2 ] . [ 1 7 2 4 0 2 . 9 0 5 2 : r Figure 1 Overall evaluation. Left: Text-to-Image results; Right: Image-Editing results. The Elo scores are obtained from the Artificial Analysis Arena. Seedream 4.0 ranks first in both T2I and image-editing leaderboards, by September 18, 2025. 1 Figure 2 Seedream 4.0 visualization.Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 2 Data, Model Training and Acceleration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.1 Model Pre-training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.2 Model Post-training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.3 Model Acceleration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 Model Performance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.1 Comprehensive Human Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.1.1 Text-to-Image . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.1.2 Single-Image Editing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.1.3 Multi-Image Editing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2 Automatic Evaluation with DreamEval . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Inspire Creativity via Seedream 4.0 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.3 3.3.1 Precise Editing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.3.2 Flexible Reference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.3.3 Visual Signal Controllable Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.3.4 In-Context Reasoning Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.3.5 Multi-Image Reference Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.3.6 Multi-Image Output . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.3.7 Advanced Text Rendering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.3.8 Adaptive Aspect Ratio and 4K Generation . . . . . . . . . . . . . . . . . . . . . . . . 4 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Contributions and Acknowledgments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.1 Core Contributors A.2 Contributors 4 5 5 6 6 7 7 7 8 9 9 10 10 11 11 12 12 12 13 13 19 19 19Diffusion models have ushered in new era in generative AI, enabling the synthesis of images with remarkable fidelity and diversity. Building on recent advances in diffusion transformers (DiTs), state-of-the-art open-source and commercial systems have emerged, such as Stable Diffusion [18], FLUX series [7, 8], Seedream models [3, 4, 21], GPT-4o image generation [15] and Gemini 2.5 flash [5]. However, as the demand for higher image quality, greater controllability, and strong multimodal capabilities (e.g., text-to-image (T2I) synthesis and image editing) increases, current models often have critical scalability bottleneck. In this paper, we introduce Seedream 4.0, powerful multimodal generative model engineered for scalability and efficiency. We develop an efficient and scalable DiT backbone, which substantially increases the model capacity while reducing the training and inference FLOPs considerably. To further enhance model efficiency, especially for high-resolution image generation, we have developed an efficient Variational Autoencoder (VAE) with high compression ratio, significantly reducing the number of image tokens in latent space. This architectural design (including our DiT and VAE) makes our model highly efficient, easily scalable, and hardware-friendly in both training and inference. Our training strategy is meticulously designed to unlock the full potential of our architecture, achieving more than 10 inference acceleration compared to Seedream 3.0 [3], while having significantly better performance. This allows the model to be trained effectively on billions of textimage pairs at native image resolutions ranging from 1K to 4K, covering wide range of taxonomy and knowledge-centric concepts. In the post-training stage, we incorporat"
[26.09.2025 03:27] Mistral response. {"id": "0fb5cb7ff643499d88c3a7b900043a22", "created": 1758857230, "model": "mistral-large-latest", "usage": {"prompt_tokens": 2071, "total_tokens": 2089, "completion_tokens": 18}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\n    \"Volcano Engine\",\n    \"ByteDance\"\n]\n```"}}]}
[26.09.2025 03:27] Response: ```python
[
    "Volcano Engine",
    "ByteDance"
]
```
[26.09.2025 03:27] Deleting PDF ./assets/pdf/2509.20427.pdf.
[26.09.2025 03:27] Success.
[26.09.2025 03:27] Downloading and parsing paper https://huggingface.co/papers/2509.21114.
[26.09.2025 03:27] Downloading paper 2509.21114 from http://arxiv.org/pdf/2509.21114v1...
[26.09.2025 03:27] Extracting affiliations from text.
[26.09.2025 03:27] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"CHARM: Control-point-based 3D Anime Hairstyle Auto-Regressive Modeling YUZE HE, Tsinghua University and Tencent AIPD, China YANNING ZHOU, Tencent AIPD, China WANG ZHAO, Tsinghua University, China JINGWEN YE, Tencent AIPD, China YUSHI BAI, Tsinghua University, China KAIWEN XIAO, Tencent AIPD, China YONG-JIN LIU, Tsinghua University, China ZHONGQIAN SUN and WEI YANG, Tencent AIPD, China Fig. 1. Based on novel parametric representation, CHARM provides generative framework for 3D anime hairstyle modeling. We present CHARM, novel parametric representation and generative framework for anime hairstyle modeling. While traditional hair modeling methods focus on realistic hair using strand-based or volumetric representations, anime hairstyle exhibits highly stylized, piecewise-structured geometry that challenges existing techniques. Existing works often rely on dense mesh modeling or hand-crafted spline curves, making them inefficient for editing and unsuitable for scalable learning. CHARM introduces compact, invertible control-point-based parameterization, where sequence of control points represents each hair card, and each point is encoded with only five geometric parameters. This efficient and accurate representation supports both artist-friendly design and learning-based generation. Built upon this representation, CHARM introduces an autoregressive generative framework that effectively generates anime hairstyles from input images or point clouds. By interpreting anime hairstyles as sequential hair language, our autoregressive transformer captures both local geometry and global hairstyle topology, resulting in high-fidelity anime hairstyle creation. To facilitate both training and evaluation of anime hairstyle generation, we construct AnimeHair, large-scale dataset of 37K high-quality anime hairstyles with separated hair cards and processed mesh data. Extensive experiments demonstrate state-of-the-art performance of CHARM in both reconstruction accuracy and generation qual"
[26.09.2025 03:27] Response: ```python
[
    "Tsinghua University and Tencent AIPD, China",
    "Tencent AIPD, China",
    "Tsinghua University, China",
    "Tencent AIPD, China",
    "Tsinghua University, China",
    "Tencent AIPD, China",
    "Tsinghua University, China",
    "Tencent AIPD, China"
]
```
[26.09.2025 03:27] Deleting PDF ./assets/pdf/2509.21114.pdf.
[26.09.2025 03:27] Success.
[26.09.2025 03:27] Downloading and parsing paper https://huggingface.co/papers/2509.21317.
[26.09.2025 03:27] Extra JSON file exists (./assets/json/2509.21317.json), skip PDF parsing.
[26.09.2025 03:27] Paper image links file exists (./assets/img_data/2509.21317.json), skip HTML parsing.
[26.09.2025 03:27] Success.
[26.09.2025 03:27] Downloading and parsing paper https://huggingface.co/papers/2509.20186.
[26.09.2025 03:27] Downloading paper 2509.20186 from http://arxiv.org/pdf/2509.20186v2...
[26.09.2025 03:27] Extracting affiliations from text.
[26.09.2025 03:27] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Work in progress THINKING AUGMENTED PRE-TRAINING Liang Wang, Nan Yang, Shaohan Huang, Li Dong, Furu Wei Microsoft Research https://aka.ms/GeneralAI "
[26.09.2025 03:27] Response: ```python
["Microsoft Research"]
```
[26.09.2025 03:27] Deleting PDF ./assets/pdf/2509.20186.pdf.
[26.09.2025 03:27] Success.
[26.09.2025 03:27] Downloading and parsing paper https://huggingface.co/papers/2509.21318.
[26.09.2025 03:27] Extra JSON file exists (./assets/json/2509.21318.json), skip PDF parsing.
[26.09.2025 03:27] Paper image links file exists (./assets/img_data/2509.21318.json), skip HTML parsing.
[26.09.2025 03:27] Success.
[26.09.2025 03:27] Downloading and parsing paper https://huggingface.co/papers/2509.21245.
[26.09.2025 03:27] Downloading paper 2509.21245 from http://arxiv.org/pdf/2509.21245v1...
[26.09.2025 03:27] Extracting affiliations from text.
[26.09.2025 03:27] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Tencent Hunyuan Hunyuan3D-Omni: Unified Framework for Controllable Generation of 3D Assets Tencent Hunyuan3D https://3d.hunyuan.tencent.com https://huggingface.co/tencent/Hunyuan3D-Omni https://github.com/Tencent-Hunyuan/Hunyuan3D-Omni 5 2 0 2 5 ] . [ 1 5 4 2 1 2 . 9 0 5 2 : r Figure 1: Hunyuan3D-Omni is unified framework for supporting controllable generation based on point cloud, bounding box, voxel, and skeleton. "
[26.09.2025 03:27] Response: ```python
["Tencent Hunyuan"]
```
[26.09.2025 03:27] Deleting PDF ./assets/pdf/2509.21245.pdf.
[26.09.2025 03:27] Success.
[26.09.2025 03:27] Downloading and parsing paper https://huggingface.co/papers/2509.21240.
[26.09.2025 03:27] Downloading paper 2509.21240 from http://arxiv.org/pdf/2509.21240v1...
[26.09.2025 03:27] Extracting affiliations from text.
[26.09.2025 03:27] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 5 2 ] . [ 1 0 4 2 1 2 . 9 0 5 2 : r Preprint. Yuxiang Ji1,2, Ziyu Ma2, Yong Wang2, Guanhua Chen3, Xiangxiang Chu2, Liaoni Wu1 1Xiamen University 2AMAP, Alibaba Group 3Southern University of Science and Technology (cid:135) GitHub: https://github.com/AMAP-ML/Tree-GRPO "
[26.09.2025 03:27] Response: ```python
["Xiamen University", "AMAP, Alibaba Group", "Southern University of Science and Technology"]
```
[26.09.2025 03:27] Deleting PDF ./assets/pdf/2509.21240.pdf.
[26.09.2025 03:27] Success.
[26.09.2025 03:27] Downloading and parsing paper https://huggingface.co/papers/2509.21070.
[26.09.2025 03:27] Downloading paper 2509.21070 from http://arxiv.org/pdf/2509.21070v1...
[26.09.2025 03:27] Extracting affiliations from text.
[26.09.2025 03:27] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 5 2 ] . [ 1 0 7 0 1 2 . 9 0 5 2 : r ScaleDiff: Scaling Difficult Problems for Advanced Mathematical Reasoning Qizhi Pei1,2,, Zhuoshi Pan2,3,, Honglin Lin2 Xin Gao2 Yu Li2 Zinan Tang2 Conghui He2, Rui Yan2,4, Lijun Wu2, 1Gaoling School of Artificial Intelligence, Renmin University of China, 2OpenDataLab, Shanghai Artificial Intelligence Laboratory, 3Tsinghua University, 4School of Artificial Intelligence, Wuhan University Large Reasoning Models (LRMs) have shown impressive capabilities in complex problemsolving, often benefiting from training on difficult mathematical problems that stimulate intricate reasoning. Recent efforts have explored automated synthesis of mathematical problems by prompting proprietary models or large-scale open-source models from seed data or inherent mathematical concepts. However, scaling up these methods remains challenging due to their high computational/API cost, complexity of prompting, and limited difficulty level of the generated problems. To overcome these limitations, we propose ScaleDiff, simple yet effective pipeline designed to scale the creation of difficult problems. We efficiently identify difficult problems from existing datasets with only single forward pass using an adaptive thinking model, which can perceive problem difficulty and automatically switch between Thinking and NoThinking modes. We then train specialized difficult problem generator (DiffGen-8B) on this filtered difficult data, which can produce new difficult problems in large scale, eliminating the need for complex, per-instance prompting and its associated high API costs. Fine-tuning Qwen2.5-Math-7B-Instruct on the ScaleDiff-Math dataset yields substantial performance increase of 11.3% compared to the original dataset and achieves 65.9% average accuracy on AIME24, AIME25, HMMT-Feb25, BRUMO25, and MATH500, outperforming recent strong LRMs like OpenThinker3. Notably, this performance is achieved using the costefficient Qwen3-8B model as teacher, demonstra"
[26.09.2025 03:27] Response: ```python
[
    "Gaoling School of Artificial Intelligence, Renmin University of China",
    "OpenDataLab, Shanghai Artificial Intelligence Laboratory",
    "Tsinghua University",
    "School of Artificial Intelligence, Wuhan University"
]
```
[26.09.2025 03:27] Deleting PDF ./assets/pdf/2509.21070.pdf.
[26.09.2025 03:27] Success.
[26.09.2025 03:27] Downloading and parsing paper https://huggingface.co/papers/2509.20868.
[26.09.2025 03:27] Extra JSON file exists (./assets/json/2509.20868.json), skip PDF parsing.
[26.09.2025 03:27] Paper image links file exists (./assets/img_data/2509.20868.json), skip HTML parsing.
[26.09.2025 03:27] Success.
[26.09.2025 03:27] Downloading and parsing paper https://huggingface.co/papers/2509.20414.
[26.09.2025 03:27] Extra JSON file exists (./assets/json/2509.20414.json), skip PDF parsing.
[26.09.2025 03:27] Paper image links file exists (./assets/img_data/2509.20414.json), skip HTML parsing.
[26.09.2025 03:27] Success.
[26.09.2025 03:27] Downloading and parsing paper https://huggingface.co/papers/2509.19803.
[26.09.2025 03:27] Downloading paper 2509.19803 from http://arxiv.org/pdf/2509.19803v1...
[26.09.2025 03:27] Extracting affiliations from text.
[26.09.2025 03:27] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 4 2 ] . [ 1 3 0 8 9 1 . 9 0 5 2 : r a VCRL: VARIANCE-BASED CURRICULUM REINFORCEMENT LEARNING FOR LARGE LANGUAGE MODELS Guochao Jiang, Wenfeng Feng, Guofeng Quan, Chuzhan Hao, Yuewei Zhang, Guohua Liu, Hao Wang Alibaba Cloud Computing anyue.jgc@alibaba-inc.com cashenry@126.com "
[26.09.2025 03:27] Response: ```python
["Alibaba Cloud Computing"]
```
[26.09.2025 03:27] Deleting PDF ./assets/pdf/2509.19803.pdf.
[26.09.2025 03:27] Success.
[26.09.2025 03:27] Downloading and parsing paper https://huggingface.co/papers/2509.14662.
[26.09.2025 03:27] Extra JSON file exists (./assets/json/2509.14662.json), skip PDF parsing.
[26.09.2025 03:27] Paper image links file exists (./assets/img_data/2509.14662.json), skip HTML parsing.
[26.09.2025 03:27] Success.
[26.09.2025 03:27] Enriching papers with extra data.
[26.09.2025 03:27] ********************************************************************************
[26.09.2025 03:27] Abstract 0. Variance-Aware Sampling and large-scale CoT data improve multimodal reasoning models by stabilizing RL fine-tuning and enhancing performance on benchmarks.  					AI-generated summary 				 Large multimodal reasoning models have achieved rapid progress, but their advancement is constrained by two majo...
[26.09.2025 03:27] ********************************************************************************
[26.09.2025 03:27] Abstract 1. A scientific reasoning foundation model pre-trained on diverse scientific data supports multiple tasks and enhances cross-domain generalization and fidelity through specialized training techniques.  					AI-generated summary 				 We present a scientific reasoning foundation model that aligns natural...
[26.09.2025 03:27] ********************************************************************************
[26.09.2025 03:27] Abstract 2. A novel reinforcement learning algorithm, CE-GPPO, reintroduces gradients from clipped tokens to improve the exploration-exploitation balance in training large language models.  					AI-generated summary 				 Reinforcement learning (RL) has become a powerful paradigm for optimizing large language mo...
[26.09.2025 03:27] ********************************************************************************
[26.09.2025 03:27] Abstract 3. Seedream 4.0 is a high-performance multimodal image generation system that integrates text-to-image synthesis, image editing, and multi-image composition using a diffusion transformer and VAE, achieving state-of-the-art results with efficient training and inference.  					AI-generated summary 				 W...
[26.09.2025 03:27] ********************************************************************************
[26.09.2025 03:27] Abstract 4. CHARM uses a control-point-based parameterization and autoregressive transformer to generate high-fidelity anime hairstyles efficiently.  					AI-generated summary 				 We present CHARM, a novel parametric representation and generative framework for anime hairstyle modeling. While traditional hair m...
[26.09.2025 03:27] ********************************************************************************
[26.09.2025 03:27] Abstract 5. IRF, a new recommendation system using natural language commands, improves user satisfaction and business outcomes through a dual-agent architecture and simulation-augmented knowledge distillation.  					AI-generated summary 				 Traditional recommender systems rely on passive feedback mechanisms th...
[26.09.2025 03:27] ********************************************************************************
[26.09.2025 03:27] Abstract 6. Thinking augmented pre-training improves data efficiency and performance of large language models by augmenting text with automatically generated thinking trajectories.  					AI-generated summary 				 This paper introduces a simple and scalable approach to improve the data efficiency of large langua...
[26.09.2025 03:27] ********************************************************************************
[26.09.2025 03:27] Abstract 7. SD3.5-Flash is an efficient few-step distillation framework that enhances image generation on consumer devices using rectified flow models with innovations like timestep sharing and split-timestep fine-tuning.  					AI-generated summary 				 We present SD3.5-Flash, an efficient few-step distillation...
[26.09.2025 03:27] ********************************************************************************
[26.09.2025 03:27] Abstract 8. Hunyuan3D-Omni is a unified 3D asset generation framework that accepts multiple conditioning signals, improving controllability and robustness in production workflows.  					AI-generated summary 				 Recent advances in 3D-native generative models have accelerated asset creation for games, film, and ...
[26.09.2025 03:27] ********************************************************************************
[26.09.2025 03:27] Abstract 9. Tree-based Group Relative Policy Optimization (Tree-GRPO) enhances reinforcement learning for large language models by using tree search to improve rollouts and estimate grouped relative advantages, outperforming chain-based methods.  					AI-generated summary 				 Recent advances in reinforcement l...
[26.09.2025 03:27] ********************************************************************************
[26.09.2025 03:27] Abstract 10. ScaleDiff uses an adaptive thinking model to identify and generate difficult mathematical problems, improving the performance of large reasoning models with cost-efficient training.  					AI-generated summary 				 Large Reasoning Models (LRMs) have shown impressive capabilities in complex problem-so...
[26.09.2025 03:27] ********************************************************************************
[26.09.2025 03:27] Abstract 11. StyleBench evaluates various reasoning styles across tasks and models, revealing that strategy efficacy depends on model scale and task type.  					AI-generated summary 				 The effectiveness of Large Language Models (LLMs) is heavily influenced by the reasoning strategies, or styles of thought, emp...
[26.09.2025 03:27] ********************************************************************************
[26.09.2025 03:27] Abstract 12. SceneWeaver, a reflective agentic framework, uses a language model-based planner to iteratively refine 3D scene synthesis, achieving high physical, visual, and semantic quality across diverse instructions.  					AI-generated summary 				 Indoor scene synthesis has become increasingly important with ...
[26.09.2025 03:27] ********************************************************************************
[26.09.2025 03:27] Abstract 13. A curriculum reinforcement learning framework dynamically adjusts training sample difficulty based on reward variance, improving LLM performance on mathematical reasoning tasks.  					AI-generated summary 				 Policy-based reinforcement learning currently plays an important role in improving LLMs on...
[26.09.2025 03:27] ********************************************************************************
[26.09.2025 03:27] Abstract 14. A novel framework using Schoenfeld's Episode Theory is introduced to analyze the reasoning patterns of Large Reasoning Models in solving math problems, providing a benchmark for machine reasoning.  					AI-generated summary 				 While Large Reasoning Models (LRMs) generate extensive chain-of-thought...
[26.09.2025 03:27] Read previous papers.
[26.09.2025 03:27] Generating reviews via LLM API.
[26.09.2025 03:27] Using data from previous issue: {"categories": ["#dataset", "#training", "#architecture", "#reasoning", "#benchmark", "#rl", "#optimization", "#multimodal", "#data", "#open_source"], "emoji": "🎯", "ru": {"title": "Стабилизация RL-обучения через управление дисперсией вознаграждений", "desc": "Исследователи предложили метод Variance
[26.09.2025 03:27] Using data from previous issue: {"categories": ["#dataset", "#training", "#reasoning", "#transfer_learning", "#multimodal", "#data", "#science", "#open_source"], "emoji": "🔬", "ru": {"title": "Универсальный AI для научных рассуждений во всех дисциплинах", "desc": "Исследователи создали foundation модель для научных рассуждений, ко
[26.09.2025 03:27] Using data from previous issue: {"categories": ["#training", "#reasoning", "#optimization", "#rl"], "emoji": "⚖️", "ru": {"title": "Сохраняем градиенты для лучшего баланса в обучении LLM", "desc": "В работе предлагается новый алгоритм обучения с подкреплением CE-GPPO для оптимизации больших языковых моделей. Основная проблема суще
[26.09.2025 03:27] Using data from previous issue: {"categories": ["#inference", "#training", "#games", "#multimodal", "#cv", "#diffusion"], "emoji": "🎨", "ru": {"title": "Универсальная система для генерации и редактирования изображений нового поколения", "desc": "Seedream 4.0 — это высокопроизводительная мультимодальная система генерации изображени
[26.09.2025 03:27] Querying the API.
[26.09.2025 03:27] Claude request. Model: claude-sonnet-4-20250514. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

CHARM uses a control-point-based parameterization and autoregressive transformer to generate high-fidelity anime hairstyles efficiently.  					AI-generated summary 				 We present CHARM, a novel parametric representation and generative framework for anime hairstyle modeling. While traditional hair modeling methods focus on realistic hair using strand-based or volumetric representations, anime hairstyle exhibits highly stylized, piecewise-structured geometry that challenges existing techniques. Existing works often rely on dense mesh modeling or hand-crafted spline curves, making them inefficient for editing and unsuitable for scalable learning. CHARM introduces a compact, invertible control-point-based parameterization, where a sequence of control points represents each hair card, and each point is encoded with only five geometric parameters. This efficient and accurate representation supports both artist-friendly design and learning-based generation. Built upon this representation, CHARM introduces an autoregressive generative framework that effectively generates anime hairstyles from input images or point clouds. By interpreting anime hairstyles as a sequential "hair language", our autoregressive transformer captures both local geometry and global hairstyle topology, resulting in high-fidelity anime hairstyle creation. To facilitate both training and evaluation of anime hairstyle generation, we construct AnimeHair, a large-scale dataset of 37K high-quality anime hairstyles with separated hair cards and processed mesh data. Extensive experiments demonstrate state-of-the-art performance of CHARM in both reconstruction accuracy and generation quality, offering an expressive and scalable solution for anime hairstyle modeling. Project page: https://hyzcluster.github.io/charm/
[26.09.2025 03:28] Response: ```json
{
  "desc": "В статье представлена CHARM - новая система для создания аниме-причёсок с помощью AI. Вместо традиционных методов моделирования волос используется компактная параметризация на основе контрольных точек, где каждая прядь волос представлена последовательностью точек с пятью геометрическими параметрами. Авторегрессивный трансформер обрабатывает причёски как \"язык волос\", захватывая как локальную геометрию, так и глобальную топологию. Для обучения и тестирования создан датасет AnimeHair с 37 тысячами высококачественных аниме-причёсок.",
  "emoji": "💇",
  "title": "AI создаёт аниме-причёски через \"язык волос\""
}
```
[26.09.2025 03:28] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"CHARM uses a control-point-based parameterization and autoregressive transformer to generate high-fidelity anime hairstyles efficiently.  					AI-generated summary 				 We present CHARM, a novel parametric representation and generative framework for anime hairstyle modeling. While traditional hair modeling methods focus on realistic hair using strand-based or volumetric representations, anime hairstyle exhibits highly stylized, piecewise-structured geometry that challenges existing techniques. Existing works often rely on dense mesh modeling or hand-crafted spline curves, making them inefficient for editing and unsuitable for scalable learning. CHARM introduces a compact, invertible control-point-based parameterization, where a sequence of control points represents each hair card, and each point is encoded with only five geometric parameters. This efficient and accurate representation supports both artist-friendly design and learning-based generation. Built upon this representation, CHARM introduces an autoregressive generative framework that effectively generates anime hairstyles from input images or point clouds. By interpreting anime hairstyles as a sequential "hair language", our autoregressive transformer captures both local geometry and global hairstyle topology, resulting in high-fidelity anime hairstyle creation. To facilitate both training and evaluation of anime hairstyle generation, we construct AnimeHair, a large-scale dataset of 37K high-quality anime hairstyles with separated hair cards and processed mesh data. Extensive experiments demonstrate state-of-the-art performance of CHARM in both reconstruction accuracy and generation quality, offering an expressive and scalable solution for anime hairstyle modeling. Project page: https://hyzcluster.github.io/charm/"

[26.09.2025 03:28] Response: ```python
['DATASET', 'CV', 'ARCHITECTURE', 'TRAINING']
```
[26.09.2025 03:28] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"CHARM uses a control-point-based parameterization and autoregressive transformer to generate high-fidelity anime hairstyles efficiently.  					AI-generated summary 				 We present CHARM, a novel parametric representation and generative framework for anime hairstyle modeling. While traditional hair modeling methods focus on realistic hair using strand-based or volumetric representations, anime hairstyle exhibits highly stylized, piecewise-structured geometry that challenges existing techniques. Existing works often rely on dense mesh modeling or hand-crafted spline curves, making them inefficient for editing and unsuitable for scalable learning. CHARM introduces a compact, invertible control-point-based parameterization, where a sequence of control points represents each hair card, and each point is encoded with only five geometric parameters. This efficient and accurate representation supports both artist-friendly design and learning-based generation. Built upon this representation, CHARM introduces an autoregressive generative framework that effectively generates anime hairstyles from input images or point clouds. By interpreting anime hairstyles as a sequential "hair language", our autoregressive transformer captures both local geometry and global hairstyle topology, resulting in high-fidelity anime hairstyle creation. To facilitate both training and evaluation of anime hairstyle generation, we construct AnimeHair, a large-scale dataset of 37K high-quality anime hairstyles with separated hair cards and processed mesh data. Extensive experiments demonstrate state-of-the-art performance of CHARM in both reconstruction accuracy and generation quality, offering an expressive and scalable solution for anime hairstyle modeling. Project page: https://hyzcluster.github.io/charm/"

[26.09.2025 03:28] Response: ```python
["GAMES", "SYNTHETIC"]
```
[26.09.2025 03:28] Response: ParsedChatCompletionMessage[Article](content='{"desc":"CHARM is a new method for creating anime hairstyles using a unique control-point-based system and an autoregressive transformer. Unlike traditional methods that use complex mesh or spline techniques, CHARM simplifies the process by representing hairstyles with a few control points, each defined by just five parameters. This makes it easier for artists to design and for machines to learn from, allowing for efficient generation of high-quality hairstyles. The framework is supported by a large dataset of 37,000 anime hairstyles, ensuring that the generated styles are both accurate and visually appealing.","title":"Efficient Anime Hairstyle Generation with CHARM"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='CHARM is a new method for creating anime hairstyles using a unique control-point-based system and an autoregressive transformer. Unlike traditional methods that use complex mesh or spline techniques, CHARM simplifies the process by representing hairstyles with a few control points, each defined by just five parameters. This makes it easier for artists to design and for machines to learn from, allowing for efficient generation of high-quality hairstyles. The framework is supported by a large dataset of 37,000 anime hairstyles, ensuring that the generated styles are both accurate and visually appealing.', title='Efficient Anime Hairstyle Generation with CHARM'))
[26.09.2025 03:28] Response: ParsedChatCompletionMessage[Article](content='{"desc":"CHARM是一种新颖的参数化表示和生成框架，专门用于动漫发型建模。与传统的发型建模方法不同，CHARM采用基于控制点的紧凑表示，能够高效地生成高保真度的动漫发型。该方法通过自回归变换器捕捉局部几何和全局发型拓扑，支持从输入图像或点云生成发型。我们还构建了一个包含37K高质量动漫发型的大规模数据集，以促进发型生成的训练和评估。","title":"高效生成动漫发型的创新框架"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='CHARM是一种新颖的参数化表示和生成框架，专门用于动漫发型建模。与传统的发型建模方法不同，CHARM采用基于控制点的紧凑表示，能够高效地生成高保真度的动漫发型。该方法通过自回归变换器捕捉局部几何和全局发型拓扑，支持从输入图像或点云生成发型。我们还构建了一个包含37K高质量动漫发型的大规模数据集，以促进发型生成的训练和评估。', title='高效生成动漫发型的创新框架'))
[26.09.2025 03:28] Using data from previous issue: {"categories": ["#training", "#architecture", "#reasoning", "#optimization", "#multimodal", "#agents"], "emoji": "🗣️", "ru": {"title": "Управляй рекомендациями голосом - говори системе, что хочешь увидеть", "desc": "Исследователи представили Interactive Recommendation Feed (IRF) - новую парадигму ре
[26.09.2025 03:28] Querying the API.
[26.09.2025 03:28] Claude request. Model: claude-sonnet-4-20250514. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Thinking augmented pre-training improves data efficiency and performance of large language models by augmenting text with automatically generated thinking trajectories.  					AI-generated summary 				 This paper introduces a simple and scalable approach to improve the data efficiency of large language model (LLM) training by augmenting existing text data with thinking trajectories. The compute for pre-training LLMs has been growing at an unprecedented rate, while the availability of high-quality data remains limited. Consequently, maximizing the utility of available data constitutes a significant research challenge. A primary impediment is that certain high-quality tokens are difficult to learn given a fixed model capacity, as the underlying rationale for a single token can be exceptionally complex and deep. To address this issue, we propose Thinking augmented Pre-Training (TPT), a universal methodology that augments text with automatically generated thinking trajectories. Such augmentation effectively increases the volume of the training data and makes high-quality tokens more learnable through step-by-step reasoning and decomposition. We apply TPT across diverse training configurations up to 100B tokens, encompassing pre-training with both constrained and abundant data, as well as mid-training from strong open-source checkpoints. Experimental results indicate that our method substantially improves the performance of LLMs across various model sizes and families. Notably, TPT enhances the data efficiency of LLM pre-training by a factor of 3. For a 3B parameter model, it improves the post-training performance by over 10% on several challenging reasoning benchmarks.
[26.09.2025 03:28] Response: ```json
{
  "desc": "В статье представлена методология Thinking augmented Pre-Training (TPT), которая улучшает эффективность обучения больших языковых моделей путем дополнения текстовых данных автоматически сгенерированными траекториями рассуждений. Подход помогает модели лучше усваивать сложные токены через пошаговое рассуждение и декомпозицию. Эксперименты показали, что TPT повышает эффективность использования данных в 3 раза и улучшает производительность модели на 10% на сложных задачах рассуждения. Методология применима к различным конфигурациям обучения до 100B токенов и разным размерам моделей.",
  "emoji": "🧠",
  "title": "Думающие данные: как траектории рассуждений ускоряют обучение LLM в три раза"
}
```
[26.09.2025 03:28] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Thinking augmented pre-training improves data efficiency and performance of large language models by augmenting text with automatically generated thinking trajectories.  					AI-generated summary 				 This paper introduces a simple and scalable approach to improve the data efficiency of large language model (LLM) training by augmenting existing text data with thinking trajectories. The compute for pre-training LLMs has been growing at an unprecedented rate, while the availability of high-quality data remains limited. Consequently, maximizing the utility of available data constitutes a significant research challenge. A primary impediment is that certain high-quality tokens are difficult to learn given a fixed model capacity, as the underlying rationale for a single token can be exceptionally complex and deep. To address this issue, we propose Thinking augmented Pre-Training (TPT), a universal methodology that augments text with automatically generated thinking trajectories. Such augmentation effectively increases the volume of the training data and makes high-quality tokens more learnable through step-by-step reasoning and decomposition. We apply TPT across diverse training configurations up to 100B tokens, encompassing pre-training with both constrained and abundant data, as well as mid-training from strong open-source checkpoints. Experimental results indicate that our method substantially improves the performance of LLMs across various model sizes and families. Notably, TPT enhances the data efficiency of LLM pre-training by a factor of 3. For a 3B parameter model, it improves the post-training performance by over 10% on several challenging reasoning benchmarks."

[26.09.2025 03:28] Response: ```python
['DATA', 'TRAINING']
```
[26.09.2025 03:28] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Thinking augmented pre-training improves data efficiency and performance of large language models by augmenting text with automatically generated thinking trajectories.  					AI-generated summary 				 This paper introduces a simple and scalable approach to improve the data efficiency of large language model (LLM) training by augmenting existing text data with thinking trajectories. The compute for pre-training LLMs has been growing at an unprecedented rate, while the availability of high-quality data remains limited. Consequently, maximizing the utility of available data constitutes a significant research challenge. A primary impediment is that certain high-quality tokens are difficult to learn given a fixed model capacity, as the underlying rationale for a single token can be exceptionally complex and deep. To address this issue, we propose Thinking augmented Pre-Training (TPT), a universal methodology that augments text with automatically generated thinking trajectories. Such augmentation effectively increases the volume of the training data and makes high-quality tokens more learnable through step-by-step reasoning and decomposition. We apply TPT across diverse training configurations up to 100B tokens, encompassing pre-training with both constrained and abundant data, as well as mid-training from strong open-source checkpoints. Experimental results indicate that our method substantially improves the performance of LLMs across various model sizes and families. Notably, TPT enhances the data efficiency of LLM pre-training by a factor of 3. For a 3B parameter model, it improves the post-training performance by over 10% on several challenging reasoning benchmarks."

[26.09.2025 03:28] Response: ```python
["OPTIMIZATION", "REASONING"]
```
[26.09.2025 03:28] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents a novel approach called Thinking augmented Pre-Training (TPT) to enhance the data efficiency and performance of large language models (LLMs). By augmenting existing text data with automatically generated thinking trajectories, TPT increases the volume of training data and facilitates the learning of complex tokens through structured reasoning. The methodology addresses the challenge of limited high-quality data by making it easier for models to learn intricate concepts. Experimental results demonstrate that TPT significantly boosts LLM performance, achieving a threefold increase in data efficiency and over 10% improvement in reasoning tasks for a 3B parameter model.","title":"Boosting Language Models with Thinking Trajectories"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper presents a novel approach called Thinking augmented Pre-Training (TPT) to enhance the data efficiency and performance of large language models (LLMs). By augmenting existing text data with automatically generated thinking trajectories, TPT increases the volume of training data and facilitates the learning of complex tokens through structured reasoning. The methodology addresses the challenge of limited high-quality data by making it easier for models to learn intricate concepts. Experimental results demonstrate that TPT significantly boosts LLM performance, achieving a threefold increase in data efficiency and over 10% improvement in reasoning tasks for a 3B parameter model.', title='Boosting Language Models with Thinking Trajectories'))
[26.09.2025 03:28] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本文提出了一种简单且可扩展的方法，通过自动生成的思维轨迹来增强现有文本数据，从而提高大型语言模型（LLM）训练的数据效率。随着LLM预训练计算需求的急剧增长，高质量数据的可用性却依然有限，因此如何最大化现有数据的利用成为了一个重要的研究挑战。我们提出的思维增强预训练（TPT）方法，通过逐步推理和分解，使得高质量的标记更易于学习，从而有效增加训练数据的量。实验结果表明，TPT在不同模型规模和类型上显著提升了LLM的性能，数据效率提高了三倍。","title":"思维增强预训练：提升大型语言模型的数据效率与性能"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本文提出了一种简单且可扩展的方法，通过自动生成的思维轨迹来增强现有文本数据，从而提高大型语言模型（LLM）训练的数据效率。随着LLM预训练计算需求的急剧增长，高质量数据的可用性却依然有限，因此如何最大化现有数据的利用成为了一个重要的研究挑战。我们提出的思维增强预训练（TPT）方法，通过逐步推理和分解，使得高质量的标记更易于学习，从而有效增加训练数据的量。实验结果表明，TPT在不同模型规模和类型上显著提升了LLM的性能，数据效率提高了三倍。', title='思维增强预训练：提升大型语言模型的数据效率与性能'))
[26.09.2025 03:28] Using data from previous issue: {"categories": ["#dataset", "#training", "#inference", "#optimization", "#data", "#cv", "#diffusion"], "emoji": "⚡", "ru": {"title": "Быстрая генерация изображений для всех устройств", "desc": "SD3.5-Flash представляет эффективный фреймворк дистилляции для генерации изображений за несколько шагов на
[26.09.2025 03:28] Using data from previous issue: {"categories": ["#3d", "#training", "#architecture", "#games", "#synthetic", "#multimodal"], "emoji": "🎮", "ru": {"title": "Многомодальный контроль 3D-генерации для игровой индустрии", "desc": "Исследователи представили Hunyuan3D-Omni — единую систему для генерации 3D-объектов с множественным контро
[26.09.2025 03:28] Querying the API.
[26.09.2025 03:28] Claude request. Model: claude-sonnet-4-20250514. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Tree-based Group Relative Policy Optimization (Tree-GRPO) enhances reinforcement learning for large language models by using tree search to improve rollouts and estimate grouped relative advantages, outperforming chain-based methods.  					AI-generated summary 				 Recent advances in reinforcement learning (RL) have significantly enhanced the agentic capabilities of large language models (LLMs). In long-term and multi-turn agent tasks, existing approaches driven solely by outcome rewards often suffer from the problem of sparse supervision. To address the challenge, we propose Tree-based Group Relative Policy Optimization (Tree-GRPO), a grouped agent RL method based on tree search, where each tree node represents the complete agent interaction step. By sharing common prefixes, the tree search sampling increases the number of rollouts achievable within a fixed budget of tokens or tool calls. Moreover, we find that the tree-structured trajectory naturally allows the construction of step-wise process supervised signals even using only the outcome reward. Based on this, Tree-GRPO estimates the grouped relative advantages both on intra-tree and inter-tree levels. Through theoretical analysis, we demonstrate that the objective of intra-tree level group relative policy optimization is equivalent to that of step-level direct preference learning. Experiments across 11 datasets and 3 types of QA tasks demonstrate the superiority of the proposed tree-based RL over the chain-based RL method.
[26.09.2025 03:28] Response: ```json
{
  "desc": "Исследователи предложили Tree-GRPO - новый метод обучения с подкреплением для больших языковых моделей, основанный на поиске по дереву. Метод решает проблему разреженного обучающего сигнала в долгосрочных агентских задачах, создавая древовидные траектории взаимодействия. Tree-GRPO оценивает групповые относительные преимущества как внутри дерева, так и между деревьями, что эквивалентно пошаговому обучению предпочтениям. Эксперименты на 11 датасетах показали превосходство древовидного подхода над цепочечными методами RL.",
  "emoji": "🌳",
  "title": "Древовидный поиск для умного обучения AI-агентов"
}
```
[26.09.2025 03:28] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Tree-based Group Relative Policy Optimization (Tree-GRPO) enhances reinforcement learning for large language models by using tree search to improve rollouts and estimate grouped relative advantages, outperforming chain-based methods.  					AI-generated summary 				 Recent advances in reinforcement learning (RL) have significantly enhanced the agentic capabilities of large language models (LLMs). In long-term and multi-turn agent tasks, existing approaches driven solely by outcome rewards often suffer from the problem of sparse supervision. To address the challenge, we propose Tree-based Group Relative Policy Optimization (Tree-GRPO), a grouped agent RL method based on tree search, where each tree node represents the complete agent interaction step. By sharing common prefixes, the tree search sampling increases the number of rollouts achievable within a fixed budget of tokens or tool calls. Moreover, we find that the tree-structured trajectory naturally allows the construction of step-wise process supervised signals even using only the outcome reward. Based on this, Tree-GRPO estimates the grouped relative advantages both on intra-tree and inter-tree levels. Through theoretical analysis, we demonstrate that the objective of intra-tree level group relative policy optimization is equivalent to that of step-level direct preference learning. Experiments across 11 datasets and 3 types of QA tasks demonstrate the superiority of the proposed tree-based RL over the chain-based RL method."

[26.09.2025 03:28] Response: ```python
['RL', 'RLHF']
```
[26.09.2025 03:28] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Tree-based Group Relative Policy Optimization (Tree-GRPO) enhances reinforcement learning for large language models by using tree search to improve rollouts and estimate grouped relative advantages, outperforming chain-based methods.  					AI-generated summary 				 Recent advances in reinforcement learning (RL) have significantly enhanced the agentic capabilities of large language models (LLMs). In long-term and multi-turn agent tasks, existing approaches driven solely by outcome rewards often suffer from the problem of sparse supervision. To address the challenge, we propose Tree-based Group Relative Policy Optimization (Tree-GRPO), a grouped agent RL method based on tree search, where each tree node represents the complete agent interaction step. By sharing common prefixes, the tree search sampling increases the number of rollouts achievable within a fixed budget of tokens or tool calls. Moreover, we find that the tree-structured trajectory naturally allows the construction of step-wise process supervised signals even using only the outcome reward. Based on this, Tree-GRPO estimates the grouped relative advantages both on intra-tree and inter-tree levels. Through theoretical analysis, we demonstrate that the objective of intra-tree level group relative policy optimization is equivalent to that of step-level direct preference learning. Experiments across 11 datasets and 3 types of QA tasks demonstrate the superiority of the proposed tree-based RL over the chain-based RL method."

[26.09.2025 03:28] Response: ```python
["REASONING", "OPTIMIZATION"]
```
[26.09.2025 03:28] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Tree-based Group Relative Policy Optimization (Tree-GRPO) is a novel approach in reinforcement learning that enhances the performance of large language models by utilizing tree search techniques. This method improves the efficiency of rollouts and allows for better estimation of grouped relative advantages, addressing the issue of sparse supervision in long-term tasks. By structuring the agent\'s interactions in a tree format, Tree-GRPO can generate more rollouts within a limited resource budget, leading to improved learning signals. Experimental results show that Tree-GRPO outperforms traditional chain-based methods across various datasets and question-answering tasks.","title":"Optimizing Language Models with Tree-Based Reinforcement Learning"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="Tree-based Group Relative Policy Optimization (Tree-GRPO) is a novel approach in reinforcement learning that enhances the performance of large language models by utilizing tree search techniques. This method improves the efficiency of rollouts and allows for better estimation of grouped relative advantages, addressing the issue of sparse supervision in long-term tasks. By structuring the agent's interactions in a tree format, Tree-GRPO can generate more rollouts within a limited resource budget, leading to improved learning signals. Experimental results show that Tree-GRPO outperforms traditional chain-based methods across various datasets and question-answering tasks.", title='Optimizing Language Models with Tree-Based Reinforcement Learning'))
[26.09.2025 03:28] Response: ParsedChatCompletionMessage[Article](content='{"desc":"树基组相对策略优化（Tree-GRPO）通过树搜索来增强强化学习，特别是针对大型语言模型的应用。该方法通过共享公共前缀，增加了在固定预算内可实现的回合数，从而提高了学习效率。Tree-GRPO能够在树内和树间层面估计分组相对优势，解决了传统方法中稀疏监督的问题。实验结果表明，Tree-GRPO在多个数据集和问答任务中优于基于链的方法。","title":"树基优化，提升强化学习效率"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='树基组相对策略优化（Tree-GRPO）通过树搜索来增强强化学习，特别是针对大型语言模型的应用。该方法通过共享公共前缀，增加了在固定预算内可实现的回合数，从而提高了学习效率。Tree-GRPO能够在树内和树间层面估计分组相对优势，解决了传统方法中稀疏监督的问题。实验结果表明，Tree-GRPO在多个数据集和问答任务中优于基于链的方法。', title='树基优化，提升强化学习效率'))
[26.09.2025 03:28] Querying the API.
[26.09.2025 03:28] Claude request. Model: claude-sonnet-4-20250514. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

ScaleDiff uses an adaptive thinking model to identify and generate difficult mathematical problems, improving the performance of large reasoning models with cost-efficient training.  					AI-generated summary 				 Large Reasoning Models (LRMs) have shown impressive capabilities in complex problem-solving, often benefiting from training on difficult mathematical problems that stimulate intricate reasoning. Recent efforts have explored automated synthesis of mathematical problems by prompting proprietary models or large-scale open-source models from seed data or inherent mathematical concepts. However, scaling up these methods remains challenging due to their high computational/API cost, complexity of prompting, and limited difficulty level of the generated problems. To overcome these limitations, we propose ScaleDiff, a simple yet effective pipeline designed to scale the creation of difficult problems. We efficiently identify difficult problems from existing datasets with only a single forward pass using an adaptive thinking model, which can perceive problem difficulty and automatically switch between "Thinking" and "NoThinking" modes. We then train a specialized difficult problem generator (DiffGen-8B) on this filtered difficult data, which can produce new difficult problems in large scale, eliminating the need for complex, per-instance prompting and its associated high API costs. Fine-tuning Qwen2.5-Math-7B-Instruct on the ScaleDiff-Math dataset yields a substantial performance increase of 11.3% compared to the original dataset and achieves a 65.9% average accuracy on AIME'24, AIME'25, HMMT-Feb'25, BRUMO'25, and MATH500, outperforming recent strong LRMs like OpenThinker3. Notably, this performance is achieved using the cost-efficient Qwen3-8B model as a teacher, demonstrating that our pipeline can effectively transfer advanced reasoning capabilities without relying on larger, more expensive teacher models. Furthermore, we observe a clear scaling phenomenon in model performance on difficult benchmarks as the quantity of difficult problems increases. Code: https://github.com/QizhiPei/ScaleDiff.
[26.09.2025 03:28] Response: ```json
{
  "desc": "ScaleDiff представляет новый подход для создания сложных математических задач с целью улучшения способностей больших языковых моделей к рассуждению. Система использует адаптивную модель мышления, которая может автоматически определять сложность задач и переключаться между режимами «Thinking» и «NoThinking». Затем обученный генератор сложных задач DiffGen-8B создает новые трудные задачи в большом масштабе без необходимости дорогостоящих API запросов. Дообучение модели Qwen2.5-Math-7B-Instruct на датасете ScaleDiff-Math показало увеличение производительности на 11.3% и достижение 65.9% точности на сложных математических бенчмарках.",
  "emoji": "🧮",
  "title": "Масштабируемая генерация сложных математических задач для обучения моделей рассуждения"
}
```
[26.09.2025 03:28] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"ScaleDiff uses an adaptive thinking model to identify and generate difficult mathematical problems, improving the performance of large reasoning models with cost-efficient training.  					AI-generated summary 				 Large Reasoning Models (LRMs) have shown impressive capabilities in complex problem-solving, often benefiting from training on difficult mathematical problems that stimulate intricate reasoning. Recent efforts have explored automated synthesis of mathematical problems by prompting proprietary models or large-scale open-source models from seed data or inherent mathematical concepts. However, scaling up these methods remains challenging due to their high computational/API cost, complexity of prompting, and limited difficulty level of the generated problems. To overcome these limitations, we propose ScaleDiff, a simple yet effective pipeline designed to scale the creation of difficult problems. We efficiently identify difficult problems from existing datasets with only a single forward pass using an adaptive thinking model, which can perceive problem difficulty and automatically switch between "Thinking" and "NoThinking" modes. We then train a specialized difficult problem generator (DiffGen-8B) on this filtered difficult data, which can produce new difficult problems in large scale, eliminating the need for complex, per-instance prompting and its associated high API costs. Fine-tuning Qwen2.5-Math-7B-Instruct on the ScaleDiff-Math dataset yields a substantial performance increase of 11.3% compared to the original dataset and achieves a 65.9% average accuracy on AIME'24, AIME'25, HMMT-Feb'25, BRUMO'25, and MATH500, outperforming recent strong LRMs like OpenThinker3. Notably, this performance is achieved using the cost-efficient Qwen3-8B model as a teacher, demonstrating that our pipeline can effectively transfer advanced reasoning capabilities without relying on larger, more expensive teacher models. Furthermore, we observe a clear scaling phenomenon in model performance on difficult benchmarks as the quantity of difficult problems increases. Code: https://github.com/QizhiPei/ScaleDiff."

[26.09.2025 03:28] Response: ```python
['DATASET', 'MATH', 'TRAINING']
```
[26.09.2025 03:28] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"ScaleDiff uses an adaptive thinking model to identify and generate difficult mathematical problems, improving the performance of large reasoning models with cost-efficient training.  					AI-generated summary 				 Large Reasoning Models (LRMs) have shown impressive capabilities in complex problem-solving, often benefiting from training on difficult mathematical problems that stimulate intricate reasoning. Recent efforts have explored automated synthesis of mathematical problems by prompting proprietary models or large-scale open-source models from seed data or inherent mathematical concepts. However, scaling up these methods remains challenging due to their high computational/API cost, complexity of prompting, and limited difficulty level of the generated problems. To overcome these limitations, we propose ScaleDiff, a simple yet effective pipeline designed to scale the creation of difficult problems. We efficiently identify difficult problems from existing datasets with only a single forward pass using an adaptive thinking model, which can perceive problem difficulty and automatically switch between "Thinking" and "NoThinking" modes. We then train a specialized difficult problem generator (DiffGen-8B) on this filtered difficult data, which can produce new difficult problems in large scale, eliminating the need for complex, per-instance prompting and its associated high API costs. Fine-tuning Qwen2.5-Math-7B-Instruct on the ScaleDiff-Math dataset yields a substantial performance increase of 11.3% compared to the original dataset and achieves a 65.9% average accuracy on AIME'24, AIME'25, HMMT-Feb'25, BRUMO'25, and MATH500, outperforming recent strong LRMs like OpenThinker3. Notably, this performance is achieved using the cost-efficient Qwen3-8B model as a teacher, demonstrating that our pipeline can effectively transfer advanced reasoning capabilities without relying on larger, more expensive teacher models. Furthermore, we observe a clear scaling phenomenon in model performance on difficult benchmarks as the quantity of difficult problems increases. Code: https://github.com/QizhiPei/ScaleDiff."

[26.09.2025 03:28] Response: ```python
['REASONING', 'TRANSFER_LEARNING', 'OPTIMIZATION']
```
[26.09.2025 03:28] Response: ParsedChatCompletionMessage[Article](content='{"desc":"ScaleDiff introduces an adaptive thinking model that identifies and generates challenging mathematical problems, enhancing the training of large reasoning models (LRMs) in a cost-effective manner. By efficiently filtering difficult problems from existing datasets with a single forward pass, it simplifies the problem generation process, avoiding complex prompting methods. The specialized generator, DiffGen-8B, produces new difficult problems at scale, leading to significant performance improvements in models fine-tuned on this data. This approach not only reduces computational costs but also demonstrates a clear scaling effect in model performance as the number of difficult problems increases.","title":"Revolutionizing Problem Generation for Enhanced AI Reasoning"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='ScaleDiff introduces an adaptive thinking model that identifies and generates challenging mathematical problems, enhancing the training of large reasoning models (LRMs) in a cost-effective manner. By efficiently filtering difficult problems from existing datasets with a single forward pass, it simplifies the problem generation process, avoiding complex prompting methods. The specialized generator, DiffGen-8B, produces new difficult problems at scale, leading to significant performance improvements in models fine-tuned on this data. This approach not only reduces computational costs but also demonstrates a clear scaling effect in model performance as the number of difficult problems increases.', title='Revolutionizing Problem Generation for Enhanced AI Reasoning'))
[26.09.2025 03:28] Response: ParsedChatCompletionMessage[Article](content='{"desc":"ScaleDiff 是一种自适应思维模型，旨在识别和生成困难的数学问题，从而提高大型推理模型的性能，并降低训练成本。该方法通过单次前向传播有效识别现有数据集中困难问题，自动切换思维模式，简化了问题生成过程。我们训练的专门困难问题生成器 DiffGen-8B 能够大规模生成新问题，避免了复杂的逐实例提示和高昂的 API 成本。通过在 ScaleDiff-Math 数据集上微调 Qwen2.5-Math-7B-Instruct，模型性能显著提升，展示了在困难基准上随着问题数量增加而提升的明显效果。","title":"ScaleDiff：高效生成困难数学问题的解决方案"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='ScaleDiff 是一种自适应思维模型，旨在识别和生成困难的数学问题，从而提高大型推理模型的性能，并降低训练成本。该方法通过单次前向传播有效识别现有数据集中困难问题，自动切换思维模式，简化了问题生成过程。我们训练的专门困难问题生成器 DiffGen-8B 能够大规模生成新问题，避免了复杂的逐实例提示和高昂的 API 成本。通过在 ScaleDiff-Math 数据集上微调 Qwen2.5-Math-7B-Instruct，模型性能显著提升，展示了在困难基准上随着问题数量增加而提升的明显效果。', title='ScaleDiff：高效生成困难数学问题的解决方案'))
[26.09.2025 03:28] Using data from previous issue: {"categories": ["#reasoning", "#open_source", "#benchmark", "#multimodal"], "emoji": "🧠", "ru": {"title": "Размер модели решает, какой стиль рассуждений работает лучше", "desc": "Исследование представляет StyleBench - бенчмарк для оценки различных стилей рассуждений в больших языковых моделях. Автор
[26.09.2025 03:28] Using data from previous issue: {"categories": ["#3d", "#reasoning", "#games", "#alignment", "#agents"], "emoji": "🏠", "ru": {"title": "Умный архитектор: AI-агент создает реалистичные 3D интерьеры через саморефлексию", "desc": "SceneWeaver - это агентная система для синтеза 3D сцен, которая использует языковую модель-планировщик д
[26.09.2025 03:28] Querying the API.
[26.09.2025 03:28] Claude request. Model: claude-sonnet-4-20250514. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A curriculum reinforcement learning framework dynamically adjusts training sample difficulty based on reward variance, improving LLM performance on mathematical reasoning tasks.  					AI-generated summary 				 Policy-based reinforcement learning currently plays an important role in improving LLMs on mathematical reasoning tasks. However, existing rollout-based reinforcement learning methods (GRPO, DAPO, GSPO, etc.) fail to explicitly consider LLMs' learning ability for samples of different difficulty levels, which is contrary to the human cognitive process of mathematical reasoning tasks from easy to difficult. Intuitively, we find that the variance of the rollout group's reward in RLVR partly reflects the difficulty of the current sample for LLMs. Samples that are too easy or too difficult have a lower variance, while samples with moderate difficulty have a higher variance. Based on this, we propose VCRL, a curriculum reinforcement learning framework that dynamically controls the difficulty of training samples based on the variance of group rewards. Experiments on five mathematical benchmarks and two models reveal the advantages of VCRL over the current LLM RL baselines.
[26.09.2025 03:29] Response: ```json
{
  "desc": "Исследователи предложили VCRL - новый подход к обучению языковых моделей решению математических задач с использованием curriculum reinforcement learning. Метод динамически регулирует сложность обучающих примеров на основе дисперсии наград, что имитирует человеческий подход к изучению математики от простого к сложному. Авторы обнаружили, что дисперсия наград группы отражает сложность задачи для модели: слишком легкие и слишком сложные задачи имеют низкую дисперсию, а задачи умеренной сложности - высокую. Эксперименты на пяти математических benchmarks показали преимущества VCRL над существующими методами reinforcement learning для LLM.",
  "emoji": "📈",
  "title": "Обучение через дисперсию: адаптивная сложность для математического мышления LLM"
}
```
[26.09.2025 03:29] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A curriculum reinforcement learning framework dynamically adjusts training sample difficulty based on reward variance, improving LLM performance on mathematical reasoning tasks.  					AI-generated summary 				 Policy-based reinforcement learning currently plays an important role in improving LLMs on mathematical reasoning tasks. However, existing rollout-based reinforcement learning methods (GRPO, DAPO, GSPO, etc.) fail to explicitly consider LLMs' learning ability for samples of different difficulty levels, which is contrary to the human cognitive process of mathematical reasoning tasks from easy to difficult. Intuitively, we find that the variance of the rollout group's reward in RLVR partly reflects the difficulty of the current sample for LLMs. Samples that are too easy or too difficult have a lower variance, while samples with moderate difficulty have a higher variance. Based on this, we propose VCRL, a curriculum reinforcement learning framework that dynamically controls the difficulty of training samples based on the variance of group rewards. Experiments on five mathematical benchmarks and two models reveal the advantages of VCRL over the current LLM RL baselines."

[26.09.2025 03:29] Response: ```python
['RL', 'TRAINING', 'MATH']
```
[26.09.2025 03:29] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A curriculum reinforcement learning framework dynamically adjusts training sample difficulty based on reward variance, improving LLM performance on mathematical reasoning tasks.  					AI-generated summary 				 Policy-based reinforcement learning currently plays an important role in improving LLMs on mathematical reasoning tasks. However, existing rollout-based reinforcement learning methods (GRPO, DAPO, GSPO, etc.) fail to explicitly consider LLMs' learning ability for samples of different difficulty levels, which is contrary to the human cognitive process of mathematical reasoning tasks from easy to difficult. Intuitively, we find that the variance of the rollout group's reward in RLVR partly reflects the difficulty of the current sample for LLMs. Samples that are too easy or too difficult have a lower variance, while samples with moderate difficulty have a higher variance. Based on this, we propose VCRL, a curriculum reinforcement learning framework that dynamically controls the difficulty of training samples based on the variance of group rewards. Experiments on five mathematical benchmarks and two models reveal the advantages of VCRL over the current LLM RL baselines."

[26.09.2025 03:29] Response: ```python
['REASONING', 'OPTIMIZATION']
```
[26.09.2025 03:29] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces a new framework called VCRL, which stands for Variance-Controlled Reinforcement Learning, aimed at enhancing the performance of large language models (LLMs) in mathematical reasoning tasks. The framework adjusts the difficulty of training samples based on the variance of rewards received during training, aligning with how humans typically learn from easier to harder problems. By focusing on samples with moderate difficulty, which show higher reward variance, VCRL helps LLMs learn more effectively. Experiments demonstrate that VCRL outperforms existing reinforcement learning methods in improving LLM capabilities on various mathematical benchmarks.","title":"Dynamic Difficulty for Smarter Learning in LLMs"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper introduces a new framework called VCRL, which stands for Variance-Controlled Reinforcement Learning, aimed at enhancing the performance of large language models (LLMs) in mathematical reasoning tasks. The framework adjusts the difficulty of training samples based on the variance of rewards received during training, aligning with how humans typically learn from easier to harder problems. By focusing on samples with moderate difficulty, which show higher reward variance, VCRL helps LLMs learn more effectively. Experiments demonstrate that VCRL outperforms existing reinforcement learning methods in improving LLM capabilities on various mathematical benchmarks.', title='Dynamic Difficulty for Smarter Learning in LLMs'))
[26.09.2025 03:29] Response: ParsedChatCompletionMessage[Article](content='{"desc":"这篇论文提出了一种课程强化学习框架，称为VCRL，旨在根据奖励方差动态调整训练样本的难度，以提高大型语言模型（LLM）在数学推理任务上的表现。现有的基于回合的强化学习方法未能充分考虑LLM对不同难度样本的学习能力，这与人类在解决数学问题时从易到难的认知过程相悖。研究发现，回合组奖励的方差可以部分反映当前样本的难度，适中难度的样本具有较高的方差，而过于简单或困难的样本则方差较低。通过在五个数学基准和两个模型上的实验，VCRL显示出相较于现有LLM强化学习基线的优势。","title":"动态调整样本难度，提升数学推理能力"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='这篇论文提出了一种课程强化学习框架，称为VCRL，旨在根据奖励方差动态调整训练样本的难度，以提高大型语言模型（LLM）在数学推理任务上的表现。现有的基于回合的强化学习方法未能充分考虑LLM对不同难度样本的学习能力，这与人类在解决数学问题时从易到难的认知过程相悖。研究发现，回合组奖励的方差可以部分反映当前样本的难度，适中难度的样本具有较高的方差，而过于简单或困难的样本则方差较低。通过在五个数学基准和两个模型上的实验，VCRL显示出相较于现有LLM强化学习基线的优势。', title='动态调整样本难度，提升数学推理能力'))
[26.09.2025 03:29] Using data from previous issue: {"categories": ["#math", "#reasoning", "#interpretability", "#benchmark"], "emoji": "🧠", "ru": {"title": "Картография мышления AI через призму человеческого познания", "desc": "Исследователи применили теорию эпизодов Шёнфельда, классическую когнитивную модель решения математических задач человеком, 
[26.09.2025 03:29] Renaming data file.
[26.09.2025 03:29] Renaming previous data. hf_papers.json to ./d/2025-09-26.json
[26.09.2025 03:29] Saving new data file.
[26.09.2025 03:29] Generating page.
[26.09.2025 03:29] Renaming previous page.
[26.09.2025 03:29] Renaming previous data. index.html to ./d/2025-09-26.html
[26.09.2025 03:29] Writing result.
[26.09.2025 03:29] Renaming log file.
[26.09.2025 03:29] Renaming previous data. log.txt to ./logs/2025-09-26_last_log.txt
