[09.02.2026 09:57] Read previous papers.
[09.02.2026 09:57] Generating top page (month).
[09.02.2026 09:57] Writing top page (month).
[09.02.2026 10:51] Read previous papers.
[09.02.2026 10:51] Get feed.
[09.02.2026 10:51] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06570
[09.02.2026 10:51] Get page data from previous paper. URL: https://huggingface.co/papers/2602.05843
[09.02.2026 10:51] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03392
[09.02.2026 10:51] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06717
[09.02.2026 10:51] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01734
[09.02.2026 10:51] Get page data from previous paper. URL: https://huggingface.co/papers/2602.05940
[09.02.2026 10:51] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06949
[09.02.2026 10:51] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06291
[09.02.2026 10:51] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06391
[09.02.2026 10:51] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06075
[09.02.2026 10:51] Get page data from previous paper. URL: https://huggingface.co/papers/2602.05281
[09.02.2026 10:51] Get page data from previous paper. URL: https://huggingface.co/papers/2602.05027
[09.02.2026 10:51] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06079
[09.02.2026 10:51] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06960
[09.02.2026 10:51] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06663
[09.02.2026 10:51] Get page data from previous paper. URL: https://huggingface.co/papers/2602.05367
[09.02.2026 10:51] Get page data from previous paper. URL: https://huggingface.co/papers/2602.05711
[09.02.2026 10:51] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06854
[09.02.2026 10:51] Get page data from previous paper. URL: https://huggingface.co/papers/2602.04837
[09.02.2026 10:51] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06554
[09.02.2026 10:51] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06471
[09.02.2026 10:51] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06724
[09.02.2026 10:51] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06139
[09.02.2026 10:51] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06129
[09.02.2026 10:51] Get page data from previous paper. URL: https://huggingface.co/papers/2602.04454
[09.02.2026 10:51] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02581
[09.02.2026 10:51] Get page data from previous paper. URL: https://huggingface.co/papers/2601.23039
[09.02.2026 10:51] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06883
[09.02.2026 10:51] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01064
[09.02.2026 10:51] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[09.02.2026 10:51] No deleted papers detected.
[09.02.2026 10:51] Downloading and parsing papers (pdf, html). Total: 29.
[09.02.2026 10:51] Downloading and parsing paper https://huggingface.co/papers/2602.06570.
[09.02.2026 10:51] Extra JSON file exists (./assets/json/2602.06570.json), skip PDF parsing.
[09.02.2026 10:51] Paper image links file exists (./assets/img_data/2602.06570.json), skip HTML parsing.
[09.02.2026 10:51] Success.
[09.02.2026 10:51] Downloading and parsing paper https://huggingface.co/papers/2602.05843.
[09.02.2026 10:51] Extra JSON file exists (./assets/json/2602.05843.json), skip PDF parsing.
[09.02.2026 10:51] Paper image links file exists (./assets/img_data/2602.05843.json), skip HTML parsing.
[09.02.2026 10:51] Success.
[09.02.2026 10:51] Downloading and parsing paper https://huggingface.co/papers/2602.03392.
[09.02.2026 10:51] Extra JSON file exists (./assets/json/2602.03392.json), skip PDF parsing.
[09.02.2026 10:51] Paper image links file exists (./assets/img_data/2602.03392.json), skip HTML parsing.
[09.02.2026 10:51] Success.
[09.02.2026 10:51] Downloading and parsing paper https://huggingface.co/papers/2602.06717.
[09.02.2026 10:51] Extra JSON file exists (./assets/json/2602.06717.json), skip PDF parsing.
[09.02.2026 10:51] Paper image links file exists (./assets/img_data/2602.06717.json), skip HTML parsing.
[09.02.2026 10:51] Success.
[09.02.2026 10:51] Downloading and parsing paper https://huggingface.co/papers/2602.01734.
[09.02.2026 10:51] Extra JSON file exists (./assets/json/2602.01734.json), skip PDF parsing.
[09.02.2026 10:51] Paper image links file exists (./assets/img_data/2602.01734.json), skip HTML parsing.
[09.02.2026 10:51] Success.
[09.02.2026 10:51] Downloading and parsing paper https://huggingface.co/papers/2602.05940.
[09.02.2026 10:51] Extra JSON file exists (./assets/json/2602.05940.json), skip PDF parsing.
[09.02.2026 10:51] Paper image links file exists (./assets/img_data/2602.05940.json), skip HTML parsing.
[09.02.2026 10:51] Success.
[09.02.2026 10:51] Downloading and parsing paper https://huggingface.co/papers/2602.06949.
[09.02.2026 10:51] Extra JSON file exists (./assets/json/2602.06949.json), skip PDF parsing.
[09.02.2026 10:51] Paper image links file exists (./assets/img_data/2602.06949.json), skip HTML parsing.
[09.02.2026 10:51] Success.
[09.02.2026 10:51] Downloading and parsing paper https://huggingface.co/papers/2602.06291.
[09.02.2026 10:51] Extra JSON file exists (./assets/json/2602.06291.json), skip PDF parsing.
[09.02.2026 10:51] Paper image links file exists (./assets/img_data/2602.06291.json), skip HTML parsing.
[09.02.2026 10:51] Success.
[09.02.2026 10:51] Downloading and parsing paper https://huggingface.co/papers/2602.06391.
[09.02.2026 10:51] Extra JSON file exists (./assets/json/2602.06391.json), skip PDF parsing.
[09.02.2026 10:51] Paper image links file exists (./assets/img_data/2602.06391.json), skip HTML parsing.
[09.02.2026 10:51] Success.
[09.02.2026 10:51] Downloading and parsing paper https://huggingface.co/papers/2602.06075.
[09.02.2026 10:51] Extra JSON file exists (./assets/json/2602.06075.json), skip PDF parsing.
[09.02.2026 10:51] Paper image links file exists (./assets/img_data/2602.06075.json), skip HTML parsing.
[09.02.2026 10:51] Success.
[09.02.2026 10:51] Downloading and parsing paper https://huggingface.co/papers/2602.05281.
[09.02.2026 10:51] Extra JSON file exists (./assets/json/2602.05281.json), skip PDF parsing.
[09.02.2026 10:51] Paper image links file exists (./assets/img_data/2602.05281.json), skip HTML parsing.
[09.02.2026 10:51] Success.
[09.02.2026 10:51] Downloading and parsing paper https://huggingface.co/papers/2602.05027.
[09.02.2026 10:51] Extra JSON file exists (./assets/json/2602.05027.json), skip PDF parsing.
[09.02.2026 10:51] Paper image links file exists (./assets/img_data/2602.05027.json), skip HTML parsing.
[09.02.2026 10:51] Success.
[09.02.2026 10:51] Downloading and parsing paper https://huggingface.co/papers/2602.06079.
[09.02.2026 10:51] Extra JSON file exists (./assets/json/2602.06079.json), skip PDF parsing.
[09.02.2026 10:51] Paper image links file exists (./assets/img_data/2602.06079.json), skip HTML parsing.
[09.02.2026 10:51] Success.
[09.02.2026 10:51] Downloading and parsing paper https://huggingface.co/papers/2602.06960.
[09.02.2026 10:51] Extra JSON file exists (./assets/json/2602.06960.json), skip PDF parsing.
[09.02.2026 10:51] Paper image links file exists (./assets/img_data/2602.06960.json), skip HTML parsing.
[09.02.2026 10:51] Success.
[09.02.2026 10:51] Downloading and parsing paper https://huggingface.co/papers/2602.06663.
[09.02.2026 10:51] Extra JSON file exists (./assets/json/2602.06663.json), skip PDF parsing.
[09.02.2026 10:51] Paper image links file exists (./assets/img_data/2602.06663.json), skip HTML parsing.
[09.02.2026 10:51] Success.
[09.02.2026 10:51] Downloading and parsing paper https://huggingface.co/papers/2602.05367.
[09.02.2026 10:51] Extra JSON file exists (./assets/json/2602.05367.json), skip PDF parsing.
[09.02.2026 10:51] Paper image links file exists (./assets/img_data/2602.05367.json), skip HTML parsing.
[09.02.2026 10:51] Success.
[09.02.2026 10:51] Downloading and parsing paper https://huggingface.co/papers/2602.05711.
[09.02.2026 10:51] Extra JSON file exists (./assets/json/2602.05711.json), skip PDF parsing.
[09.02.2026 10:51] Paper image links file exists (./assets/img_data/2602.05711.json), skip HTML parsing.
[09.02.2026 10:51] Success.
[09.02.2026 10:51] Downloading and parsing paper https://huggingface.co/papers/2602.06854.
[09.02.2026 10:51] Extra JSON file exists (./assets/json/2602.06854.json), skip PDF parsing.
[09.02.2026 10:51] Paper image links file exists (./assets/img_data/2602.06854.json), skip HTML parsing.
[09.02.2026 10:51] Success.
[09.02.2026 10:51] Downloading and parsing paper https://huggingface.co/papers/2602.04837.
[09.02.2026 10:51] Extra JSON file exists (./assets/json/2602.04837.json), skip PDF parsing.
[09.02.2026 10:51] Paper image links file exists (./assets/img_data/2602.04837.json), skip HTML parsing.
[09.02.2026 10:51] Success.
[09.02.2026 10:51] Downloading and parsing paper https://huggingface.co/papers/2602.06554.
[09.02.2026 10:51] Extra JSON file exists (./assets/json/2602.06554.json), skip PDF parsing.
[09.02.2026 10:51] Paper image links file exists (./assets/img_data/2602.06554.json), skip HTML parsing.
[09.02.2026 10:51] Success.
[09.02.2026 10:51] Downloading and parsing paper https://huggingface.co/papers/2602.06471.
[09.02.2026 10:51] Extra JSON file exists (./assets/json/2602.06471.json), skip PDF parsing.
[09.02.2026 10:51] Paper image links file exists (./assets/img_data/2602.06471.json), skip HTML parsing.
[09.02.2026 10:51] Success.
[09.02.2026 10:51] Downloading and parsing paper https://huggingface.co/papers/2602.06724.
[09.02.2026 10:51] Extra JSON file exists (./assets/json/2602.06724.json), skip PDF parsing.
[09.02.2026 10:51] Paper image links file exists (./assets/img_data/2602.06724.json), skip HTML parsing.
[09.02.2026 10:51] Success.
[09.02.2026 10:51] Downloading and parsing paper https://huggingface.co/papers/2602.06139.
[09.02.2026 10:51] Extra JSON file exists (./assets/json/2602.06139.json), skip PDF parsing.
[09.02.2026 10:51] Paper image links file exists (./assets/img_data/2602.06139.json), skip HTML parsing.
[09.02.2026 10:51] Success.
[09.02.2026 10:51] Downloading and parsing paper https://huggingface.co/papers/2602.06129.
[09.02.2026 10:51] Extra JSON file exists (./assets/json/2602.06129.json), skip PDF parsing.
[09.02.2026 10:51] Paper image links file exists (./assets/img_data/2602.06129.json), skip HTML parsing.
[09.02.2026 10:51] Success.
[09.02.2026 10:51] Downloading and parsing paper https://huggingface.co/papers/2602.04454.
[09.02.2026 10:51] Extra JSON file exists (./assets/json/2602.04454.json), skip PDF parsing.
[09.02.2026 10:51] Paper image links file exists (./assets/img_data/2602.04454.json), skip HTML parsing.
[09.02.2026 10:51] Success.
[09.02.2026 10:51] Downloading and parsing paper https://huggingface.co/papers/2602.02581.
[09.02.2026 10:51] Extra JSON file exists (./assets/json/2602.02581.json), skip PDF parsing.
[09.02.2026 10:51] Paper image links file exists (./assets/img_data/2602.02581.json), skip HTML parsing.
[09.02.2026 10:51] Success.
[09.02.2026 10:51] Downloading and parsing paper https://huggingface.co/papers/2601.23039.
[09.02.2026 10:51] Extra JSON file exists (./assets/json/2601.23039.json), skip PDF parsing.
[09.02.2026 10:51] Paper image links file exists (./assets/img_data/2601.23039.json), skip HTML parsing.
[09.02.2026 10:51] Success.
[09.02.2026 10:51] Downloading and parsing paper https://huggingface.co/papers/2602.06883.
[09.02.2026 10:51] Extra JSON file exists (./assets/json/2602.06883.json), skip PDF parsing.
[09.02.2026 10:51] Paper image links file exists (./assets/img_data/2602.06883.json), skip HTML parsing.
[09.02.2026 10:51] Success.
[09.02.2026 10:51] Downloading and parsing paper https://huggingface.co/papers/2602.01064.
[09.02.2026 10:51] Extra JSON file exists (./assets/json/2602.01064.json), skip PDF parsing.
[09.02.2026 10:51] Paper image links file exists (./assets/img_data/2602.01064.json), skip HTML parsing.
[09.02.2026 10:51] Success.
[09.02.2026 10:51] Enriching papers with extra data.
[09.02.2026 10:51] ********************************************************************************
[09.02.2026 10:51] Abstract 0. Baichuan-M3 is a medical-enhanced large language model designed for clinical decision support with capabilities in proactive information gathering, long-horizon reasoning, and hallucination suppression.  					AI-generated summary 				 We introduce Baichuan-M3, a medical-enhanced large language model...
[09.02.2026 10:51] ********************************************************************************
[09.02.2026 10:51] Abstract 1. OdysseyArena presents a new framework for evaluating large language models on long-horizon, inductive agent tasks that emphasize autonomous discovery of environmental transition laws.  					AI-generated summary 				 The rapid advancement of Large Language Models (LLMs) has catalyzed the development ...
[09.02.2026 10:51] ********************************************************************************
[09.02.2026 10:51] Abstract 2. The paper establishes a theoretical framework for analyzing entropy dynamics in reinforcement fine-tuning of large language models, deriving expressions for entropy change and proposing entropy control methods based on discriminant analysis.  					AI-generated summary 				 Entropy serves as a critic...
[09.02.2026 10:51] ********************************************************************************
[09.02.2026 10:51] Abstract 3. RLVR methods using group sampling suffer from bias toward likely trajectories and missed rare-correct ones; a difficulty-aware advantage scaling technique improves performance on benchmarks without increasing computational cost.  					AI-generated summary 				 Reinforcement Learning with Verifiable ...
[09.02.2026 10:51] ********************************************************************************
[09.02.2026 10:51] Abstract 4. Training instability in large language models is linked to weight matrix stable rank decline and Jacobian alignment, which MSign addresses through matrix sign operations to prevent gradient explosions.  					AI-generated summary 				 Training instability remains a critical challenge in large languag...
[09.02.2026 10:51] ********************************************************************************
[09.02.2026 10:51] Abstract 5. TRIT framework improves multilingual reasoning by jointly training translation and reasoning components, enhancing question understanding and response generation across languages.  					AI-generated summary 				 Long reasoning models often struggle in multilingual settings: they tend to reason in En...
[09.02.2026 10:51] ********************************************************************************
[09.02.2026 10:51] Abstract 6. DreamDojo is a foundation world model trained on 44k hours of egocentric human videos that enables efficient simulation of dexterous robotic tasks through continuous latent actions and real-time distillation.  					AI-generated summary 				 Being able to simulate the outcomes of actions in varied en...
[09.02.2026 10:51] ********************************************************************************
[09.02.2026 10:51] Abstract 7. Consequence-Based Utility evaluates mathematical solutions by testing their effectiveness as exemplars for related problems, outperforming reward models and LLM judges in ranking quality and correct-wrong separation.  					AI-generated summary 				 Recent progress in reasoning models suggests that g...
[09.02.2026 10:51] ********************************************************************************
[09.02.2026 10:51] Abstract 8. GUI agents for automated digital tasks rely on vision-language models with enhanced grounding capabilities, achieved through refined data engineering, improved training strategies, and reinforcement learning with verifiable rewards.  					AI-generated summary 				 The rapid advancement of vision-lan...
[09.02.2026 10:51] ********************************************************************************
[09.02.2026 10:51] Abstract 9. A comprehensive memory-focused benchmark for mobile GUI agents reveals significant memory capability gaps and provides systematic evaluation methods and design insights.  					AI-generated summary 				 Current mobile GUI agent benchmarks systematically fail to assess memory capabilities, with only 5...
[09.02.2026 10:51] ********************************************************************************
[09.02.2026 10:51] Abstract 10. A novel reinforcement learning approach called ARM addresses entropy collapse in LLM reasoning by equilibrating confidence levels across correct responses through dynamic reward shaping.  					AI-generated summary 				 Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as an indispens...
[09.02.2026 10:51] ********************************************************************************
[09.02.2026 10:51] Abstract 11. Sparse Autoencoders trained on Whisper and HuBERT models demonstrate stable feature extraction and effective disentanglement of acoustic and semantic information, showing practical applications in audio processing and correlation with human neural activity.  					AI-generated summary 				 Sparse Aut...
[09.02.2026 10:51] ********************************************************************************
[09.02.2026 10:51] Abstract 12. Canzona presents a unified asynchronous framework that addresses the conflict between matrix-based optimizers and distributed tensor fragmentation in LLM training, improving efficiency and reducing latency.  					AI-generated summary 				 The scaling of Large Language Models (LLMs) drives interest i...
[09.02.2026 10:51] ********************************************************************************
[09.02.2026 10:51] Abstract 13. InftyThink+ uses reinforcement learning to optimize iterative reasoning processes, improving accuracy and efficiency in large language models.  					AI-generated summary 				 Large reasoning models achieve strong performance by scaling inference-time chain-of-thought, but this paradigm suffers from ...
[09.02.2026 10:51] ********************************************************************************
[09.02.2026 10:51] Abstract 14. PlanViz benchmark evaluates unified multimodal models' capabilities in computer-use planning tasks through route planning, work diagramming, and web&UI displaying sub-tasks with a task-adaptive scoring system.  					AI-generated summary 				 Unified multimodal models (UMMs) have shown impressive cap...
[09.02.2026 10:51] ********************************************************************************
[09.02.2026 10:51] Abstract 15. Residual binarization framework RaBiT addresses feature co-adaptation in quantized LLMs through hierarchical path derivation and robust initialization, achieving superior accuracy-efficiency trade-offs.  					AI-generated summary 				 Efficient deployment of large language models (LLMs) requires ext...
[09.02.2026 10:51] ********************************************************************************
[09.02.2026 10:51] Abstract 16. OmniMoE presents a system-algorithm co-designed framework that achieves fine-grained expert specialization in Mixture-of-Experts architectures through vector-level atomic experts and optimized routing and scheduling mechanisms.  					AI-generated summary 				 Mixture-of-Experts (MoE) architectures a...
[09.02.2026 10:51] ********************************************************************************
[09.02.2026 10:51] Abstract 17. A novel framework called SEMA is introduced that effectively trains multi-turn attackers for large language models without relying on existing strategies or external data, achieving state-of-the-art attack success rates while being compact, reproducible, and transferable across different models and ...
[09.02.2026 10:51] ********************************************************************************
[09.02.2026 10:51] Abstract 18. Group-Evolving Agents enable open-ended self-improvement by treating groups of agents as evolutionary units, allowing efficient experience sharing and reuse to enhance coding performance and robustness.  					AI-generated summary 				 Open-ended self-improving agents can autonomously modify their ow...
[09.02.2026 10:51] ********************************************************************************
[09.02.2026 10:51] Abstract 19. SeeUPO is a critic-free reinforcement learning method that ensures convergence guarantees in multi-turn agent interactions by modeling sequential decision-making as multi-agent bandit problems and using backward induction for policy updates.  					AI-generated summary 				 Reinforcement learning (RL...
[09.02.2026 10:51] ********************************************************************************
[09.02.2026 10:51] Abstract 20. Replacing conventional feed-forward networks with hourglass-shaped MLPs in Transformers improves model efficiency and performance by enabling better parameter utilization and competitive scaling.  					AI-generated summary 				 Dense Transformer language models have largely adhered to one consistent...
[09.02.2026 10:51] ********************************************************************************
[09.02.2026 10:51] Abstract 21. Table-as-Search framework reformulates information seeking tasks as table completion problems, improving long-horizon search robustness through structured state management.  					AI-generated summary 				 Current Information Seeking (InfoSeeking) agents struggle to maintain focus and coherence durin...
[09.02.2026 10:51] ********************************************************************************
[09.02.2026 10:51] Abstract 22. Multi-modal large language models struggle to jointly understand audio and visual signals in egocentric videos, but a new scalable data engine and dataset significantly improve their performance through targeted fine-tuning.  					AI-generated summary 				 Understanding egocentric videos plays a vit...
[09.02.2026 10:51] ********************************************************************************
[09.02.2026 10:51] Abstract 23. A diffusion-transformer framework integrates spatio-temporal urban data to predict building-level climate risks while incorporating transportation network structures for emergency response applications.  					AI-generated summary 				 Climate hazards increasingly disrupt urban transportation and eme...
[09.02.2026 10:51] ********************************************************************************
[09.02.2026 10:51] Abstract 24. Seg-ReSearch introduces a novel segmentation approach that combines interleaved reasoning with external search to overcome limitations of frozen MLLM knowledge, using hierarchical reward design for training and demonstrating superior performance on video object segmentation benchmarks.  					AI-gene...
[09.02.2026 10:51] ********************************************************************************
[09.02.2026 10:51] Abstract 25. QuantLRM uses weight update magnitude signals from fine-tuning to improve quantization of Large Reasoning Models, achieving better performance than traditional methods through channel importance estimation.  					AI-generated summary 				 Weight-only quantization is important for compressing Large L...
[09.02.2026 10:51] ********************************************************************************
[09.02.2026 10:51] Abstract 26. Researchers identify and address premature mode collapse in optimal transport-based structural prediction models through an adaptive stability control algorithm that prevents gradient explosions during large-scale training.  					AI-generated summary 				 Differentiable matching layers and residual ...
[09.02.2026 10:51] ********************************************************************************
[09.02.2026 10:51] Abstract 27. Vision transformer components exhibit varying plasticity levels that correlate with finetuning performance, challenging the assumption that smoothness is always beneficial.  					AI-generated summary 				 The smoothness of the transformer architecture has been extensively studied in the context of g...
[09.02.2026 10:51] ********************************************************************************
[09.02.2026 10:51] Abstract 28. Knowledge purification techniques consolidate rationales from multiple teacher LLMs to reduce conflicts and improve efficiency in distillation processes.  					AI-generated summary 				 Knowledge distillation has emerged as a pivotal technique for transferring knowledge from stronger large language ...
[09.02.2026 10:51] Read previous papers.
[09.02.2026 10:51] Generating reviews via LLM API.
[09.02.2026 10:51] Using data from previous issue: {"categories": ["#open_source", "#science", "#hallucinations", "#reasoning"], "emoji": "‚öïÔ∏è", "ru": {"title": "–û—Ç –ø–∞—Å—Å–∏–≤–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤ –∫ –∞–∫—Ç–∏–≤–Ω–æ–π –≤—Ä–∞—á–µ–±–Ω–æ–π –ø–æ–¥–¥–µ—Ä–∂–∫–µ", "desc": "Baichuan-M3 ‚Äî —ç—Ç–æ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∞—è LLM, –∫–æ—Ç–æ—Ä–∞—è –ø–µ—Ä–µ–≤–æ–¥–∏—Ç –ø–∞—Ä–∞–¥–∏–≥–º—É –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å –ø–∞—Ü–∏–µ–Ω—Ç–∞–º–∏ –æ—Ç –ø–∞—Å—Å–∏–≤–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –∫ –∞–∫
[09.02.2026 10:51] Using data from previous issue: {"categories": ["#agents", "#benchmark", "#reasoning", "#long_context", "#open_source", "#dataset"], "emoji": "üß≠", "ru": {"title": "–û—Ü–µ–Ω–∫–∞ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –æ—Ç–∫—Ä—ã—Ç–∏—è –∑–∞–∫–æ–Ω–æ–≤ –ø—Ä–∏—Ä–æ–¥—ã –≤ –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã—Ö –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è—Ö –∞–≥–µ–Ω—Ç–æ–≤", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω OdysseyArena ‚Äî –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫
[09.02.2026 10:51] Using data from previous issue: {"categories": ["#rlhf", "#math", "#training", "#rl"], "emoji": "‚öñÔ∏è", "ru": {"title": "–¢–µ–æ—Ä–∏—è —ç–Ω—Ç—Ä–æ–ø–∏–∏ –≤ —É—Å–∏–ª–µ–Ω–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–∏–Ω–∞–º–∏–∫–∏ —ç–Ω—Ç—Ä–æ–ø–∏–∏ –ø—Ä–∏ —É—Å–∏–ª–µ–Ω–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã –≤—ã–≤–æ–¥—è—Ç –≤—ã—Ä–∞–∂–µ–Ω–∏—è 
[09.02.2026 10:51] Using data from previous issue: {"categories": [], "emoji": "üéØ", "ru": {"title": "–°–ø—Ä–∞–≤–µ–¥–ª–∏–≤–æ–µ –æ–±—É—á–µ–Ω–∏–µ: –∫–∞–∫ –Ω–∞–π—Ç–∏ —Ä–µ–¥–∫–∏–µ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è", "desc": "–í —Å—Ç–∞—Ç—å–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ —Å–º–µ—â–µ–Ω–∏—è –≤ –º–µ—Ç–æ–¥–∞—Ö –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –∏ –ø—Ä–æ–≤–µ—Ä—è–µ–º—ã–º–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è–º–∏ (RLVR), –∫–æ—Ç–æ—Ä—ã–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç –≥—Ä—É–ø–ø–æ–≤—É—é –≤—ã–±–æ—Ä–∫—É –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤. –ê
[09.02.2026 10:51] Using data from previous issue: {"categories": ["#training", "#architecture", "#optimization"], "emoji": "‚ö°", "ru": {"title": "MSign: —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ —Ä–∞–Ω–≥–∞ –º–∞—Ç—Ä–∏—Ü", "desc": "–í —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–æ—è–≤–ª—è–µ—Ç—Å—è –≤ –≤–∏–¥–µ –≤–∑—Ä—ã–≤–Ω–æ–≥
[09.02.2026 10:51] Using data from previous issue: {"categories": ["#machine_translation", "#transfer_learning", "#multilingual", "#training", "#reasoning"], "emoji": "üåç", "ru": {"title": "–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ø–µ—Ä–µ–≤–æ–¥–∞ –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ TRIT, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ
[09.02.2026 10:51] Using data from previous issue: {"categories": ["#inference", "#open_source", "#synthetic", "#transfer_learning", "#video", "#training", "#robotics", "#dataset"], "emoji": "ü§ñ", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –º–∏—Ä–∞ –¥–ª—è –ª–æ–≤–∫–æ–≥–æ —Ä–æ–±–æ—É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∏–∑ –≤–∏–¥–µ–æ —á–µ–ª–æ–≤–µ–∫–∞", "desc": "DreamDojo ‚Äî —ç—Ç–æ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –º–∏—Ä–∞, –æ–±—É—á–µ–Ω–Ω–∞—è –Ω
[09.02.2026 10:51] Using data from previous issue: {"categories": ["#benchmark", "#reasoning", "#science", "#math", "#dataset"], "emoji": "üßÆ", "ru": {"title": "–û—Ü–µ–Ω–∫–∞ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–µ—à–µ–Ω–∏–π —á–µ—Ä–µ–∑ –∏—Ö –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫—É—é –ø–æ–ª–µ–∑–Ω–æ—Å—Ç—å –Ω–∞ —Å–æ—Å–µ–¥–Ω–∏—Ö –∑–∞–¥–∞—á–∞—Ö", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –º–µ—Ç–æ–¥ Consequence-Based Utility –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–µ—à–µ–Ω–∏–π 
[09.02.2026 10:51] Using data from previous issue: {"categories": ["#multimodal", "#rl", "#benchmark", "#agents", "#training", "#cv", "#data"], "emoji": "üñ±Ô∏è", "ru": {"title": "–¢–æ—á–Ω–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ GUI —á–µ—Ä–µ–∑ –∏–Ω–∂–µ–Ω–µ—Ä–∏—é –¥–∞–Ω–Ω—ã—Ö –∏ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –º–æ–¥–µ–ª—å POINTS-GUI-G-8B –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –∑–∞–¥–∞—á —Å –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏
[09.02.2026 10:51] Using data from previous issue: {"categories": [], "emoji": "üß†", "ru": {"title": "–ü–∞–º—è—Ç—å –∫–∞–∫ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ–∫ –º–æ–±–∏–ª—å–Ω—ã—Ö GUI-–∞–≥–µ–Ω—Ç–æ–≤: –∫–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∏ —Ä–µ—à–µ–Ω–∏—è", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ MemGUI-Bench ‚Äî –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–±–∏–ª—å–Ω—ã—Ö GUI-–∞–≥–µ–Ω—Ç–æ–≤ –∫ –∑–∞–ø–æ–º–∏–Ω–∞–Ω–∏—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏ –æ–±—É—á–µ–Ω–∏—é –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥
[09.02.2026 10:51] Using data from previous issue: {"categories": ["#training", "#reasoning", "#optimization", "#rl"], "emoji": "‚öñÔ∏è", "ru": {"title": "–£—Ä–∞–≤–Ω–æ–≤–µ—à–∏–≤–∞–Ω–∏–µ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –¥–ª—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ ARM (Advantage Re-weighting Mechanism) –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º—ã –∫–æ–ª–ª–∞–ø—Å–∞ —ç–Ω—Ç
[09.02.2026 10:51] Using data from previous issue: {"categories": ["#open_source", "#interpretability"], "emoji": "üëÇ", "ru": {"title": "–†–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–µ –∞–≤—Ç–æ–∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–∏ —Ä–∞—Å–∫—Ä—ã–≤–∞—é—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∑–≤—É–∫–∞", "desc": "–í —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É—é—Ç—Å—è —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–µ –∞–≤—Ç–æ–∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–∏ (SAE) –¥–ª—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –≤ –∞—É–¥–∏–æ–º–æ–¥–µ–ª—è—Ö Whisper –∏ HuBERT. –ê–≤—Ç–æ—Ä—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, 
[09.02.2026 10:51] Using data from previous issue: {"categories": ["#optimization", "#architecture", "#training"], "emoji": "‚ö°", "ru": {"title": "–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≥–∞—Ä–º–æ–Ω–∏—è: –º–∞—Ç—Ä–∏—á–Ω—ã–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä—ã –≤—Å—Ç—Ä–µ—á–∞—é—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã–µ —Ç–µ–Ω–∑–æ—Ä—ã", "desc": "Canzona –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –∫–æ—Ç–æ—Ä—ã–π —Ä–µ—à–∞–µ—Ç –∫–æ–Ω—Ñ–ª–∏–∫—Ç –º–µ–∂–¥—É –º–∞—Ç—Ä–∏—á–Ω—ã–º–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞–º–∏ 
[09.02.2026 10:51] Using data from previous issue: {"categories": ["#small_models", "#optimization", "#rl", "#training", "#reasoning", "#long_context"], "emoji": "üß†", "ru": {"title": "–£–º–Ω–æ–µ —Ä–µ–∑—é–º–∏—Ä–æ–≤–∞–Ω–∏–µ –º—ã—Å–ª–µ–π —á–µ—Ä–µ–∑ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ InftyThink+, –∫–æ—Ç–æ—Ä–∞—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü
[09.02.2026 10:51] Using data from previous issue: {"categories": ["#survey", "#multimodal", "#benchmark", "#cv", "#dataset"], "emoji": "üìã", "ru": {"title": "–û—Ü–µ–Ω–∫–∞ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –∑–∞–¥–∞—á–∞—Ö –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∫–æ–º–ø—å—é—Ç–µ—Ä–∞", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –±–µ–Ω—á–º–∞—Ä–∫ PlanViz –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –æ–±—ä–µ–¥–∏–Ω—ë–Ω–Ω—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ
[09.02.2026 10:51] Using data from previous issue: {"categories": ["#inference", "#architecture", "#training", "#optimization"], "emoji": "‚ö°", "ru": {"title": "–ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è –±–∏–Ω–∞—Ä–∏–∑–∞—Ü–∏—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ LLM", "desc": "RaBiT ‚Äî —ç—Ç–æ –Ω–æ–≤–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä–∞—è —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É —Å–æ–≤–º–µ—Å—Ç–Ω–æ–π –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ
[09.02.2026 10:51] Using data from previous issue: {"categories": ["#inference", "#training", "#architecture"], "emoji": "‚ö°", "ru": {"title": "–ê—Ç–æ–º–∞—Ä–Ω—ã–µ —ç–∫—Å–ø–µ—Ä—Ç—ã: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ —Å–∫–æ—Ä–æ—Å—Ç–∏", "desc": "OmniMoE –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–∏—Å—Ç–µ–º—É, –∫–æ—Ç–æ—Ä–∞—è –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –≤ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ Mixture-of-Experts –±–ª–∞–≥–æ–¥–∞—Ä
[09.02.2026 10:51] Using data from previous issue: {"categories": ["#training", "#rl", "#alignment", "#security", "#open_source"], "emoji": "üéØ", "ru": {"title": "–ú–Ω–æ–≥–æ—Ö–æ–¥–æ–≤—ã–µ –∞—Ç–∞–∫–∏ –Ω–∞ LLM —á–µ—Ä–µ–∑ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –±–µ–∑ –≤–Ω–µ—à–Ω–∏—Ö –¥–∞–Ω–Ω—ã—Ö", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ SEMA –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–Ω–æ–≥–æ—Ö–æ–¥–æ–≤—ã—Ö –∞—Ç–∞–∫ –Ω–∞ –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏
[09.02.2026 10:51] Using data from previous issue: {"categories": ["#benchmark", "#plp", "#training", "#agents"], "emoji": "üß¨", "ru": {"title": "–ì—Ä—É–ø–ø–æ–≤–∞—è —ç–≤–æ–ª—é—Ü–∏—è –∞–≥–µ–Ω—Ç–æ–≤ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Å–∞–º–æ—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–∏—è", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –Ω–æ–≤–∞—è –ø–∞—Ä–∞–¥–∏–≥–º–∞ Group-Evolving Agents (GEA) –¥–ª—è –æ—Ç–∫—Ä—ã—Ç–æ–≥–æ —Å–∞–º–æ—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤, –∫–æ—Ç–æ—Ä–∞—è —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ
[09.02.2026 10:51] Using data from previous issue: {"categories": ["#agents", "#training", "#rl", "#alignment", "#optimization", "#reasoning"], "emoji": "üéØ", "ru": {"title": "–ì–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –±–µ–∑ –∫—Ä–∏—Ç–∏–∫–∞ —á–µ—Ä–µ–∑ –æ–±—Ä–∞—Ç–Ω—É—é –∏–Ω–¥—É–∫—Ü–∏—é", "desc": "SeeUPO ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –±–µ–∑ –∫—Ä–∏—Ç–∏–∫–∞, –∫–æ—Ç–æ—Ä—ã–π –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç 
[09.02.2026 10:51] Using data from previous issue: {"categories": ["#optimization"], "emoji": "‚è≥", "ru": {"title": "–ü–µ—Å–æ—á–Ω—ã–µ —á–∞—Å—ã –≤–º–µ—Å—Ç–æ —Ç—Ä–∞–¥–∏—Ü–∏–∏: –ø–µ—Ä–µ–æ—Å–º—ã—Å–ª–µ–Ω–∏–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –∑–∞–º–µ–Ω–∏—Ç—å —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–µ –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–µ —Å–µ—Ç–∏ (FFN) –≤ –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞—Ö –Ω–∞ –±–æ–ª–µ–µ –≥–ª—É–±–æ–∫–∏–µ —Å–µ—Ç–∏ –≤ —Ñ–æ—Ä–º–µ –ø–µ—Å–æ—á–Ω—ã—Ö —á–∞—Å–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –ª—É—á—à–µ –∏—Å–ø–æ–ª
[09.02.2026 10:51] Using data from previous issue: {"categories": ["#benchmark", "#agents", "#dataset"], "emoji": "üóÇÔ∏è", "ru": {"title": "–¢–∞–±–ª–∏—Ü–∞ –∫–∞–∫ –Ω–∞–≤–∏–≥–∞—Ç–æ—Ä: —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –¥–ª—è –¥–æ–ª–≥–∏—Ö –≥–æ—Ä–∏–∑–æ–Ω—Ç–æ–≤", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ Table-as-Search (TaS) –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á –ø–æ–∏—Å–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å –ø–æ–∏—Å–∫–∞ –≤ –∑–∞–¥
[09.02.2026 10:51] Using data from previous issue: {"categories": ["#audio", "#multimodal", "#benchmark", "#video", "#training", "#robotics", "#dataset"], "emoji": "üëÅÔ∏è‚Äçüó®Ô∏è", "ru": {"title": "–ì–∞—Ä–º–æ–Ω–∏—á–Ω–æ–µ —Å–ª–∏—è–Ω–∏–µ –∑–≤—É–∫–∞ –∏ –≤–∏–¥–µ–Ω–∏—è –≤ —ç–≥–æ—Ü–µ–Ω—Ç—Ä–∏—á–µ—Å–∫–∏—Ö —Å—Ü–µ–Ω–∞—Ö", "desc": "–†–∞–±–æ—Ç–∞ –∏—Å—Å–ª–µ–¥—É–µ—Ç –ø—Ä–æ–±–ª–µ–º—É —Å–æ–≤–º–µ—Å—Ç–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∞—É–¥–∏–æ –∏ –≤–∏–∑—É–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ —ç–≥–æ—Ü–µ–Ω—Ç—Ä–∏
[09.02.2026 10:51] Using data from previous issue: {"categories": ["#graphs", "#open_source", "#benchmark", "#diffusion", "#dataset", "#science", "#transfer_learning", "#architecture", "#multimodal"], "emoji": "üè¢", "ru": {"title": "–ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–ª–∏–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∏—Å–∫–æ–≤ –∏ –º–∞—Ä—à—Ä—É—Ç–æ–≤ —ç–≤–∞–∫—É–∞—Ü–∏–∏ –≤ –≥–æ—Ä–æ–¥–∞—Ö —Å –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–º —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–º", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø
[09.02.2026 10:51] Using data from previous issue: {"categories": ["#open_source", "#reasoning", "#dataset", "#training", "#multimodal", "#rag", "#cv", "#benchmark", "#video"], "emoji": "üîç", "ru": {"title": "–í—ã—Ö–æ–¥ –∑–∞ –ø—Ä–µ–¥–µ–ª—ã –∑–∞–º–æ—Ä–æ–∂–µ–Ω–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π: —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è —Å –ø–æ–∏—Å–∫–æ–º –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ–º", "desc": "Seg-ReSearch –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏, –∫–æ—Ç
[09.02.2026 10:51] Using data from previous issue: {"categories": ["#optimization", "#inference", "#benchmark", "#training", "#reasoning"], "emoji": "‚öñÔ∏è", "ru": {"title": "–ó–∞—â–∏—Ç–∞ –æ–±–æ–∏—Ö –∫–æ–Ω—Ü–æ–≤: –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è —á–µ—Ä–µ–∑ —Å–∏–≥–Ω–∞–ª—ã fine-tuning", "desc": "QuantLRM ‚Äî —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ 
[09.02.2026 10:51] Using data from previous issue: {"categories": ["#optimization"], "emoji": "‚ö°", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∂–∏–º–æ–≤ –≤ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–º —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–µ –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä", "desc": "–í —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –≤—ã—è–≤–∏–ª–∏ –ø—Ä–æ–±–ª–µ–º—É –ø—Ä–µ–∂–¥–µ–≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –∫–æ–ª–ª–∞–ø—Å–∞ –º–æ–¥ –≤ –º–æ–¥–µ–ª—è—Ö —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–æ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã—Ö –Ω–∞ –æ–ø—Ç–∏
[09.02.2026 10:51] Using data from previous issue: {"categories": ["#open_source", "#transfer_learning", "#interpretability"], "emoji": "üîÑ", "ru": {"title": "–ü–ª–∞—Å—Ç–∏—á–Ω–æ—Å—Ç—å –≤–∞–∂–Ω–µ–µ –≥–ª–∞–¥–∫–æ—Å—Ç–∏: –∫–ª—é—á –∫ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–º—É –¥–æ–æ–±—É—á–µ–Ω–∏—é —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤", "desc": "–í —ç—Ç–æ–π —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –ø–ª–∞—Å—Ç–∏—á–Ω–æ—Å—Ç—å –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ Vision Transformer ‚Äî –∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å –≤—ã—Ö–æ–¥—ã
[09.02.2026 10:51] Using data from previous issue: {"categories": ["#transfer_learning", "#small_models", "#training"], "emoji": "üßπ", "ru": {"title": "–û—á–∏—Å—Ç–∫–∞ –∑–Ω–∞–Ω–∏–π –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –∏–∑ –º–Ω–æ–≥–∏—Ö —É—á–∏—Ç–µ–ª–µ–π", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –º–µ—Ç–æ–¥ –æ—á–∏—Å—Ç–∫–∏ –∑–Ω–∞–Ω–∏–π –¥–ª—è —Ç—Ä–∞–Ω—Å—Ñ–µ—Ä–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –º–µ–∂–¥—É –±–æ–ª—å—à–∏–º–∏ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ –∏ –±–æ–ª–µ–µ –∫–æ–º–ø–∞–∫—Ç–Ω—ã–º–∏ –º–æ–¥–µ
[09.02.2026 10:51] Renaming data file.
[09.02.2026 10:51] Renaming previous data. hf_papers.json to ./d/2026-02-09.json
[09.02.2026 10:51] Saving new data file.
[09.02.2026 10:51] Generating page.
[09.02.2026 10:51] Renaming previous page.
[09.02.2026 10:51] Renaming previous data. index.html to ./d/2026-02-09.html
[09.02.2026 10:51] Writing result.
[09.02.2026 10:51] Renaming log file.
[09.02.2026 10:51] Renaming previous data. log.txt to ./logs/2026-02-09_last_log.txt
