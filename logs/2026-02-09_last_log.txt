[09.02.2026 18:48] Read previous papers.
[09.02.2026 18:48] Generating top page (month).
[09.02.2026 18:48] Writing top page (month).
[09.02.2026 19:59] Read previous papers.
[09.02.2026 19:59] Get feed.
[09.02.2026 19:59] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06570
[09.02.2026 19:59] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06717
[09.02.2026 19:59] Get page data from previous paper. URL: https://huggingface.co/papers/2602.05843
[09.02.2026 19:59] Get page data from previous paper. URL: https://huggingface.co/papers/2602.05027
[09.02.2026 19:59] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03392
[09.02.2026 19:59] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01734
[09.02.2026 19:59] Get page data from previous paper. URL: https://huggingface.co/papers/2601.18415
[09.02.2026 19:59] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06949
[09.02.2026 19:59] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06130
[09.02.2026 19:59] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06291
[09.02.2026 19:59] Get page data from previous paper. URL: https://huggingface.co/papers/2602.05940
[09.02.2026 19:59] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06391
[09.02.2026 19:59] Get page data from previous paper. URL: https://huggingface.co/papers/2602.05281
[09.02.2026 19:59] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06075
[09.02.2026 19:59] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06079
[09.02.2026 19:59] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06139
[09.02.2026 19:59] Get page data from previous paper. URL: https://huggingface.co/papers/2602.05367
[09.02.2026 19:59] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06960
[09.02.2026 19:59] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06669
[09.02.2026 19:59] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06663
[09.02.2026 19:59] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02581
[09.02.2026 19:59] Extract page data from URL. URL: https://huggingface.co/papers/2602.06869
[09.02.2026 19:59] Get page data from previous paper. URL: https://huggingface.co/papers/2602.05711
[09.02.2026 19:59] Get page data from previous paper. URL: https://huggingface.co/papers/2602.04837
[09.02.2026 19:59] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06854
[09.02.2026 19:59] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06471
[09.02.2026 19:59] Get page data from previous paper. URL: https://huggingface.co/papers/2602.05847
[09.02.2026 19:59] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03075
[09.02.2026 19:59] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06883
[09.02.2026 19:59] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06554
[09.02.2026 19:59] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03548
[09.02.2026 19:59] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06724
[09.02.2026 19:59] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06129
[09.02.2026 19:59] Get page data from previous paper. URL: https://huggingface.co/papers/2602.04454
[09.02.2026 19:59] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01064
[09.02.2026 19:59] Get page data from previous paper. URL: https://huggingface.co/papers/2601.23039
[09.02.2026 19:59] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06964
[09.02.2026 19:59] Extract page data from URL. URL: https://huggingface.co/papers/2602.04811
[09.02.2026 19:59] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03998
[09.02.2026 19:59] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[09.02.2026 19:59] No deleted papers detected.
[09.02.2026 19:59] Downloading and parsing papers (pdf, html). Total: 39.
[09.02.2026 19:59] Downloading and parsing paper https://huggingface.co/papers/2602.06570.
[09.02.2026 19:59] Extra JSON file exists (./assets/json/2602.06570.json), skip PDF parsing.
[09.02.2026 19:59] Paper image links file exists (./assets/img_data/2602.06570.json), skip HTML parsing.
[09.02.2026 19:59] Success.
[09.02.2026 19:59] Downloading and parsing paper https://huggingface.co/papers/2602.06717.
[09.02.2026 19:59] Extra JSON file exists (./assets/json/2602.06717.json), skip PDF parsing.
[09.02.2026 19:59] Paper image links file exists (./assets/img_data/2602.06717.json), skip HTML parsing.
[09.02.2026 19:59] Success.
[09.02.2026 19:59] Downloading and parsing paper https://huggingface.co/papers/2602.05843.
[09.02.2026 19:59] Extra JSON file exists (./assets/json/2602.05843.json), skip PDF parsing.
[09.02.2026 19:59] Paper image links file exists (./assets/img_data/2602.05843.json), skip HTML parsing.
[09.02.2026 19:59] Success.
[09.02.2026 19:59] Downloading and parsing paper https://huggingface.co/papers/2602.05027.
[09.02.2026 19:59] Extra JSON file exists (./assets/json/2602.05027.json), skip PDF parsing.
[09.02.2026 19:59] Paper image links file exists (./assets/img_data/2602.05027.json), skip HTML parsing.
[09.02.2026 19:59] Success.
[09.02.2026 19:59] Downloading and parsing paper https://huggingface.co/papers/2602.03392.
[09.02.2026 19:59] Extra JSON file exists (./assets/json/2602.03392.json), skip PDF parsing.
[09.02.2026 19:59] Paper image links file exists (./assets/img_data/2602.03392.json), skip HTML parsing.
[09.02.2026 19:59] Success.
[09.02.2026 19:59] Downloading and parsing paper https://huggingface.co/papers/2602.01734.
[09.02.2026 19:59] Extra JSON file exists (./assets/json/2602.01734.json), skip PDF parsing.
[09.02.2026 19:59] Paper image links file exists (./assets/img_data/2602.01734.json), skip HTML parsing.
[09.02.2026 19:59] Success.
[09.02.2026 19:59] Downloading and parsing paper https://huggingface.co/papers/2601.18415.
[09.02.2026 19:59] Extra JSON file exists (./assets/json/2601.18415.json), skip PDF parsing.
[09.02.2026 19:59] Paper image links file exists (./assets/img_data/2601.18415.json), skip HTML parsing.
[09.02.2026 19:59] Success.
[09.02.2026 19:59] Downloading and parsing paper https://huggingface.co/papers/2602.06949.
[09.02.2026 19:59] Extra JSON file exists (./assets/json/2602.06949.json), skip PDF parsing.
[09.02.2026 19:59] Paper image links file exists (./assets/img_data/2602.06949.json), skip HTML parsing.
[09.02.2026 19:59] Success.
[09.02.2026 19:59] Downloading and parsing paper https://huggingface.co/papers/2602.06130.
[09.02.2026 19:59] Extra JSON file exists (./assets/json/2602.06130.json), skip PDF parsing.
[09.02.2026 19:59] Paper image links file exists (./assets/img_data/2602.06130.json), skip HTML parsing.
[09.02.2026 19:59] Success.
[09.02.2026 19:59] Downloading and parsing paper https://huggingface.co/papers/2602.06291.
[09.02.2026 19:59] Extra JSON file exists (./assets/json/2602.06291.json), skip PDF parsing.
[09.02.2026 19:59] Paper image links file exists (./assets/img_data/2602.06291.json), skip HTML parsing.
[09.02.2026 19:59] Success.
[09.02.2026 19:59] Downloading and parsing paper https://huggingface.co/papers/2602.05940.
[09.02.2026 19:59] Extra JSON file exists (./assets/json/2602.05940.json), skip PDF parsing.
[09.02.2026 19:59] Paper image links file exists (./assets/img_data/2602.05940.json), skip HTML parsing.
[09.02.2026 19:59] Success.
[09.02.2026 19:59] Downloading and parsing paper https://huggingface.co/papers/2602.06391.
[09.02.2026 19:59] Extra JSON file exists (./assets/json/2602.06391.json), skip PDF parsing.
[09.02.2026 19:59] Paper image links file exists (./assets/img_data/2602.06391.json), skip HTML parsing.
[09.02.2026 19:59] Success.
[09.02.2026 19:59] Downloading and parsing paper https://huggingface.co/papers/2602.05281.
[09.02.2026 19:59] Extra JSON file exists (./assets/json/2602.05281.json), skip PDF parsing.
[09.02.2026 19:59] Paper image links file exists (./assets/img_data/2602.05281.json), skip HTML parsing.
[09.02.2026 19:59] Success.
[09.02.2026 19:59] Downloading and parsing paper https://huggingface.co/papers/2602.06075.
[09.02.2026 19:59] Extra JSON file exists (./assets/json/2602.06075.json), skip PDF parsing.
[09.02.2026 19:59] Paper image links file exists (./assets/img_data/2602.06075.json), skip HTML parsing.
[09.02.2026 19:59] Success.
[09.02.2026 19:59] Downloading and parsing paper https://huggingface.co/papers/2602.06079.
[09.02.2026 19:59] Extra JSON file exists (./assets/json/2602.06079.json), skip PDF parsing.
[09.02.2026 19:59] Paper image links file exists (./assets/img_data/2602.06079.json), skip HTML parsing.
[09.02.2026 19:59] Success.
[09.02.2026 19:59] Downloading and parsing paper https://huggingface.co/papers/2602.06139.
[09.02.2026 19:59] Extra JSON file exists (./assets/json/2602.06139.json), skip PDF parsing.
[09.02.2026 19:59] Paper image links file exists (./assets/img_data/2602.06139.json), skip HTML parsing.
[09.02.2026 19:59] Success.
[09.02.2026 19:59] Downloading and parsing paper https://huggingface.co/papers/2602.05367.
[09.02.2026 19:59] Extra JSON file exists (./assets/json/2602.05367.json), skip PDF parsing.
[09.02.2026 19:59] Paper image links file exists (./assets/img_data/2602.05367.json), skip HTML parsing.
[09.02.2026 19:59] Success.
[09.02.2026 19:59] Downloading and parsing paper https://huggingface.co/papers/2602.06960.
[09.02.2026 19:59] Extra JSON file exists (./assets/json/2602.06960.json), skip PDF parsing.
[09.02.2026 19:59] Paper image links file exists (./assets/img_data/2602.06960.json), skip HTML parsing.
[09.02.2026 19:59] Success.
[09.02.2026 19:59] Downloading and parsing paper https://huggingface.co/papers/2602.06669.
[09.02.2026 19:59] Extra JSON file exists (./assets/json/2602.06669.json), skip PDF parsing.
[09.02.2026 19:59] Paper image links file exists (./assets/img_data/2602.06669.json), skip HTML parsing.
[09.02.2026 19:59] Success.
[09.02.2026 19:59] Downloading and parsing paper https://huggingface.co/papers/2602.06663.
[09.02.2026 19:59] Extra JSON file exists (./assets/json/2602.06663.json), skip PDF parsing.
[09.02.2026 19:59] Paper image links file exists (./assets/img_data/2602.06663.json), skip HTML parsing.
[09.02.2026 19:59] Success.
[09.02.2026 19:59] Downloading and parsing paper https://huggingface.co/papers/2602.02581.
[09.02.2026 19:59] Extra JSON file exists (./assets/json/2602.02581.json), skip PDF parsing.
[09.02.2026 19:59] Paper image links file exists (./assets/img_data/2602.02581.json), skip HTML parsing.
[09.02.2026 19:59] Success.
[09.02.2026 19:59] Downloading and parsing paper https://huggingface.co/papers/2602.06869.
[09.02.2026 19:59] Downloading paper 2602.06869 from https://arxiv.org/pdf/2602.06869v1...
[09.02.2026 19:59] Extracting affiliations from text.
[09.02.2026 19:59] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 6 ] . [ 1 9 6 8 6 0 . 2 0 6 2 : r Uncovering Cross-Objective Interference in Multi-Objective Alignment Yining Lu 1 Meng Jiang "
[09.02.2026 19:59] Response: ```python
[]
```
[09.02.2026 19:59] Extracting affiliations from text.
[09.02.2026 19:59] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 6 ] . [ 1 9 6 8 6 0 . 2 0 6 2 : r Uncovering Cross-Objective Interference in Multi-Objective Alignment Yining Lu 1 Meng JiangWe study persistent failure mode in multiobjective alignment for large language models (LLMs): training improves performance on only subset of objectives while causing others to degrade. We formalize this phenomenon as crossobjective interference and conduct the first systematic study across classic scalarization algorithms, showing that interference is pervasive and exhibits strong model dependence. To explain this phenomenon, we derive local covariance law showing that an objective improves at first order when its reward exhibits positive covariance with the scalarized score. We extend this analysis to clipped surrogate objectives used in modern alignment, demonstrating that the covariance law remains valid under mild conditions despite clipping. Building on this analysis, we propose Covariance Targeted Weight Adaptation (CTWA), plug-and-play method that maintains positive covariance between objective rewards and the training signal to effectively mitigate cross-objective interference. Finally, we complement these local improvement conditions with global convergence analysis under the Polyak Łojasiewicz condition, establishing when nonconvex scalarized optimization achieves global convergence and how cross-objective interference depends on specific model geometric properties. 1. Introduction Existing approaches to multi-objective LLM alignment predominantly build on reinforcement fine-tuning (RFT) methods (Ouyang et al., 2022; Bai et al., 2022; Lambert et al., 2025; Shen et al., 2025). These methods commonly reduce the multi-objective problem to optimizing single scalar objective through scalarization, applying either static weights (Kimi et al., 2025) or dynamic weights (Lu et al., 2025) at 1Department of Computer Science and Engineering, University of Notre Dame, USA. Correspondence to: Yining Lu <ylu33@nd.edu>, Meng Jiang <mjiang2@nd.edu>. Preprint. February 9, 2026. 1 the reward- (Guo et al., 2024) or gradient-level (Li et al., 2025). Despite its simplicity and popularity, we observe persistent and underexplored failure mode: scalarized training frequently fails to improve all objectives simultaneously. Instead, the model continues making progress on subset of easy objectives while others degrade, pattern we formalize as cross-objective interference. To investigate whether this phenomenon is an artifact of naive implementations or fundamental limitation, we turn to the rich literature of Multi-Task Learning (MTL) and Multi-Objective Optimization (MOO). We evaluated broad set of well-established algorithms with known convergence properties, spanning both rewardand gradient-level scalarization algorithms. Reward-level scalarization includes linear weighting (Barrett & Narayanan, 2008), Lagrangian primal-dual formulation (Mahdavi et al., 2013), Tchebycheff scalarization (Bowman, 1976), PAMA (He & Maghsudi, 2026), and more recent dynamic weighting (Lu et al., 2025). Gradient-level approaches aggregate per-objective gradients to unified update direction via MGDA (Desideri, 2009) or GradNorm (Chen et al., 2018). To our knowledge, this is the first systematic evaluation of classic scalarization algorithms for multi-objective LLM alignment.1 Our results reveal that all evaluated methods suffer from the cross-objective interference issue when applied to certain models (e.g., Figure 1a and Figure 1b). Critically, this occurs even when objectives are not fundamentally conflicting under traditional gradient-based definitions from MTL and MOO (Evgeniou & Pontil, 2004; Liu et al., 2021; SHI et al., 2023; Kim et al., 2025).2 This finding suggests the failure mode is model-dependent and runs deeper than existing MOO theories on linear scalarization (Lu et al., 2023), convexity (Wei & Niethammer, 2021), gradient conflict (Sener & Koltun, 2018), and generalization tradeoffs (Chen et al., 2023), as these theories are developed for simplified settings rather than LLM alignment. To address this gap, we develop theoretical framework analyzing multi-objective alignment through first-order improvement conditions. Beginning with the classic policy gradient algorithm (Sutton et al., 1999; Hu et al., 2025), we derive reward-level local covariance law that precisely 1We provide pseudocode of each algorithm in Appendix A. 2Analysis is presented in Figure 4 in Appendix B.1. Uncovering Cross-Objective Interference in Multi-Objective Alignment (a) Qwen2.5-1.5B-Base (b) Qwen2.5-1.5B-IFT (c) Qwen3-1.7B-Base Figure 1. Multi-objective alignment under different scalarization algorithms. We report moving-averaged test performance along training for three objectives: accuracy, conciseness, and clarity (left to right). We aim to train models with strong problem-solving ability (higher accuracy), computational efficiency (fewer response tokens), and clear reasoning processes (higher clarity). Results are shown for three models trained on the Math500 dataset with different scalarization algorithms adapted from MTL and MOO. Our method, CTWA, effectively mitigates cross-objective interference compared to others. Competing methods either quickly sacrifice accuracy to achieve superficially high conciseness and clarity (e.g., GradNorm in 1a, Linear and Dynamic weighting in 1b), or trying to maintain high accuracy while overlooking the improvment of others (e.g., Lagrangian in 1a and PAMA in 1b). In contrast, CTWA achieves strong, balanced performance across all three objectives. For instance, in 1c, CTWA maintains the highest accuracy without any degradation while achieving competitive conciseness and clarity. Even when CTWAs accuracy is slightly lower than Lagrangians (e.g., at training step 500 in 1a and 1b), it still surpasses all other methods and excels on both conciseness and clarity. characterizes when an objective improves under scalarized alignment: when its true reward exhibits positive covariance with the scalarized score. This explains why cross-objective interference happens: objectives that are easy to optimize can dominate the training, inducing negative covariance for harder objectives and causing them to degrade even as the overall scalarized return increases. We then extend this analysis to clipped surrogate objectives used in modern RFT, such as GRPO (Shao et al., 2024), demonstrating that under mild conditions, the first-order covariance law remains valid despite clip"
[09.02.2026 19:59] Mistral response. {"id": "c1c93073761a47fb9291f8ba51054b2e", "created": 1770667188, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1532, "total_tokens": 1552, "completion_tokens": 20, "num_cached_tokens": 1531}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\"Department of Computer Science and Engineering, University of Notre Dame, USA\"]\n```"}}]}
[09.02.2026 19:59] Response: ```python
["Department of Computer Science and Engineering, University of Notre Dame, USA"]
```
[09.02.2026 19:59] Deleting PDF ./assets/pdf/2602.06869.pdf.
[09.02.2026 19:59] Success.
[09.02.2026 19:59] Downloading and parsing paper https://huggingface.co/papers/2602.05711.
[09.02.2026 19:59] Extra JSON file exists (./assets/json/2602.05711.json), skip PDF parsing.
[09.02.2026 19:59] Paper image links file exists (./assets/img_data/2602.05711.json), skip HTML parsing.
[09.02.2026 19:59] Success.
[09.02.2026 19:59] Downloading and parsing paper https://huggingface.co/papers/2602.04837.
[09.02.2026 19:59] Extra JSON file exists (./assets/json/2602.04837.json), skip PDF parsing.
[09.02.2026 19:59] Paper image links file exists (./assets/img_data/2602.04837.json), skip HTML parsing.
[09.02.2026 19:59] Success.
[09.02.2026 19:59] Downloading and parsing paper https://huggingface.co/papers/2602.06854.
[09.02.2026 19:59] Extra JSON file exists (./assets/json/2602.06854.json), skip PDF parsing.
[09.02.2026 19:59] Paper image links file exists (./assets/img_data/2602.06854.json), skip HTML parsing.
[09.02.2026 19:59] Success.
[09.02.2026 19:59] Downloading and parsing paper https://huggingface.co/papers/2602.06471.
[09.02.2026 19:59] Extra JSON file exists (./assets/json/2602.06471.json), skip PDF parsing.
[09.02.2026 19:59] Paper image links file exists (./assets/img_data/2602.06471.json), skip HTML parsing.
[09.02.2026 19:59] Success.
[09.02.2026 19:59] Downloading and parsing paper https://huggingface.co/papers/2602.05847.
[09.02.2026 19:59] Extra JSON file exists (./assets/json/2602.05847.json), skip PDF parsing.
[09.02.2026 19:59] Paper image links file exists (./assets/img_data/2602.05847.json), skip HTML parsing.
[09.02.2026 19:59] Success.
[09.02.2026 19:59] Downloading and parsing paper https://huggingface.co/papers/2602.03075.
[09.02.2026 19:59] Extra JSON file exists (./assets/json/2602.03075.json), skip PDF parsing.
[09.02.2026 19:59] Paper image links file exists (./assets/img_data/2602.03075.json), skip HTML parsing.
[09.02.2026 19:59] Success.
[09.02.2026 19:59] Downloading and parsing paper https://huggingface.co/papers/2602.06883.
[09.02.2026 19:59] Extra JSON file exists (./assets/json/2602.06883.json), skip PDF parsing.
[09.02.2026 19:59] Paper image links file exists (./assets/img_data/2602.06883.json), skip HTML parsing.
[09.02.2026 19:59] Success.
[09.02.2026 19:59] Downloading and parsing paper https://huggingface.co/papers/2602.06554.
[09.02.2026 19:59] Extra JSON file exists (./assets/json/2602.06554.json), skip PDF parsing.
[09.02.2026 19:59] Paper image links file exists (./assets/img_data/2602.06554.json), skip HTML parsing.
[09.02.2026 19:59] Success.
[09.02.2026 19:59] Downloading and parsing paper https://huggingface.co/papers/2602.03548.
[09.02.2026 19:59] Extra JSON file exists (./assets/json/2602.03548.json), skip PDF parsing.
[09.02.2026 19:59] Paper image links file exists (./assets/img_data/2602.03548.json), skip HTML parsing.
[09.02.2026 19:59] Success.
[09.02.2026 19:59] Downloading and parsing paper https://huggingface.co/papers/2602.06724.
[09.02.2026 19:59] Extra JSON file exists (./assets/json/2602.06724.json), skip PDF parsing.
[09.02.2026 19:59] Paper image links file exists (./assets/img_data/2602.06724.json), skip HTML parsing.
[09.02.2026 19:59] Success.
[09.02.2026 19:59] Downloading and parsing paper https://huggingface.co/papers/2602.06129.
[09.02.2026 19:59] Extra JSON file exists (./assets/json/2602.06129.json), skip PDF parsing.
[09.02.2026 19:59] Paper image links file exists (./assets/img_data/2602.06129.json), skip HTML parsing.
[09.02.2026 19:59] Success.
[09.02.2026 19:59] Downloading and parsing paper https://huggingface.co/papers/2602.04454.
[09.02.2026 19:59] Extra JSON file exists (./assets/json/2602.04454.json), skip PDF parsing.
[09.02.2026 19:59] Paper image links file exists (./assets/img_data/2602.04454.json), skip HTML parsing.
[09.02.2026 19:59] Success.
[09.02.2026 19:59] Downloading and parsing paper https://huggingface.co/papers/2602.01064.
[09.02.2026 19:59] Extra JSON file exists (./assets/json/2602.01064.json), skip PDF parsing.
[09.02.2026 19:59] Paper image links file exists (./assets/img_data/2602.01064.json), skip HTML parsing.
[09.02.2026 19:59] Success.
[09.02.2026 19:59] Downloading and parsing paper https://huggingface.co/papers/2601.23039.
[09.02.2026 19:59] Extra JSON file exists (./assets/json/2601.23039.json), skip PDF parsing.
[09.02.2026 19:59] Paper image links file exists (./assets/img_data/2601.23039.json), skip HTML parsing.
[09.02.2026 19:59] Success.
[09.02.2026 19:59] Downloading and parsing paper https://huggingface.co/papers/2602.06964.
[09.02.2026 19:59] Extra JSON file exists (./assets/json/2602.06964.json), skip PDF parsing.
[09.02.2026 19:59] Paper image links file exists (./assets/img_data/2602.06964.json), skip HTML parsing.
[09.02.2026 19:59] Success.
[09.02.2026 19:59] Downloading and parsing paper https://huggingface.co/papers/2602.04811.
[09.02.2026 19:59] Downloading paper 2602.04811 from https://arxiv.org/pdf/2602.04811v1...
[09.02.2026 19:59] Extracting affiliations from text.
[09.02.2026 19:59] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"SE-BENCH: Benchmarking Self-Evolution with Knowledge Internalization Jiarui Yuan * 1 Tailin Jin * 1 Weize Chen * 1 Zeyuan Liu 1 Zhiyuan Liu 1 Maosong Sun "
[09.02.2026 19:59] Response: ```python
[]
```
[09.02.2026 19:59] Extracting affiliations from text.
[09.02.2026 19:59] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"SE-BENCH: Benchmarking Self-Evolution with Knowledge Internalization Jiarui Yuan * 1 Tailin Jin * 1 Weize Chen * 1 Zeyuan Liu 1 Zhiyuan Liu 1 Maosong SunTrue self-evolution requires agents to act as lifelong learners that internalize novel experiences to solve future problems. However, rigorously measuring this foundational capability is hinthe entanglement of dered by two obstacles: prior knowledge, where new knowledge may appear in pre-training data, and the entanglement of reasoning complexity, where failures may stem from problem difficulty rather than an inability to recall learned knowledge. We introduce SE-BENCH, diagnostic environment that obfuscates the NumPy library and its API doc into pseudo-novel package with randomized identifiers. Agents are trained to internalize this package and evaluated on simple coding tasks without access to documentation, yielding clean setting where tasks are trivial with the new API doc but impossible for base models without it. Our investigation reveals three insights: (1) the Open-Book Paradox, where training with reference documentation inhibits retention, requiring Closed-Book Training to force knowledge compression into weights; (2) the RL Gap, where standard RL fails to internalize new knowledge completely due to PPO clipping and negative gradients; and (3) the viability of Self-Play for internalization, proving models can learn from self-generated, noisy tasks when coupled with SFT, but not RL. Overall, SEBENCH establishes rigorous diagnostic platform for self-evolution with knowledge internalization. Our code and dataset can be found at https: //github.com/thunlp/SE-Bench. 6 2 0 2 4 ] . [ 1 1 1 8 4 0 . 2 0 6 2 : r 1. Introduction Self-evolution, the capacity for an autonomous agent to recursively improve its own capabilities, is often viewed as prerequisite for Artificial General Intelligence (AGI) (Go- *Equal contribution 1Tsinghua University. Correspondence to: Weize Chen <chenwz21@mails.tsinghua.edu.cn>, Zhiyuan Liu <liuzy@tsinghua.edu.cn>. 1 ertzel & Pennachin, 2007; Legg & Hutter, 2006). An ideal self-evolving agent acts as lifelong learner, continuously assimilating information from its environment, optimizing its solutions, and expanding its skill set without human intervention. However, current approaches often limit the scope of this evolution to transient or localized adaptations, such as inference-time response refinement (Novikov et al., 2025; Wang et al., 2025b) or iterative self-code modification (Zhang et al., 2025a; Wang et al., 2025a). While valuable, these mechanisms differ fundamentally from the expansive definition we explore here: the self-evolution requires agents to actively learn from experience by internalizing novel skills or knowledge, akin to human expert accumulating domain knowledge over time (Wang et al., 2024; Zhang et al., 2025b; Ouyang et al., 2025). Despite rapid progress in large language model (LLM) reasoning capabilities, we lack rigorous measurement for this foundational internalization ability. Existing benchmarks have made strides in evaluating specific sub-skills related to self-evolution, such as long-horizon information retrieval (Wei et al., 2025a; Li et al., 2025), iterative response refinement (Lee et al., 2025), and complex task execution (Team, 2025c; Jimenez et al., 2024) However, current evaluations fail to cleanly isolate an agents ability to process and restore experience due to two fundamental obstacles. First, the entanglement of prior knowledge: when model solves task involving novel knowledge, it is indistinguishable whether the agent learned from the relevant experience or merely recalled pre-training data. Second, the entanglement of reasoning complexity: if an agent fails complex task, it is ambiguous whether it failed to internalize the necessary knowledge or failed to reason over it. This mirrors student who memorizes textbook but fails frontier math problem due to logical difficulty rather than memory gaps. To address these limitations, we argue that the community needs Needle in Haystack test (Kamradt, 2023) for self-evolution: an environment where tasks are algorithmically trivial if knowledge is internalized, and impossible if it is not. To this end, we introduce SE-BENCH. SE-BENCH relies on knowledge obfuscation mechanism to create this clean environment. We employ knowledge obfuscation mechanism, mapping the core functions of the NumPy (Harris et al., 2020) library to randomized, nonsense identifiers SE-BENCH: Benchmarking Self-Evolution with Knowledge Internalization (e.g., numpy.mean zwc.kocito) and rewriting the documentation to describe new package. At training time, agents have access to the documentation, but at test time, agents are tasked with solving simple problems using this obfuscated package, with the strict constraint that any use of the original NumPy library is deemed to fail. This design grants SE-BENCH three diagnostic properties: (1) Impossible without information: Without documentation, the probability of guessing the correct API is mathematically zero, eliminating prior knowledge confounds. (2) Trivial with information: Because the underlying logic maps 1to-1 to standard NumPy, tasks are trivial for any agent that internalizes the mapping. Any ideal self-evolving method should theoretically achieve near-100% success rate, thus cleanly isolating the internalization capability. (3) Compositional generalization: While the training set consists of tasks solvable with single function calls, the test set requires composing multiple internalized functions, assessing generalization beyond simple memorization. Beyond serving as rigorous metric, SE-BENCH functions as clean testbed that enables us to dissect the fundamental mechanisms of self-evolution. We investigate whether standard parameter-optimization paradigms, specifically Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL), can genuinely support the internalization capability. Our experiments uncover three critical insights: (1) The Open-Book Paradox: We find that the presence of reference material during parameter update inhibits long-term retention. True internalization requires Closed-Book Training: removing the documentation during parameter updates forces the model to compress external logic into its weights, significantly outperforming standard SFT. (2) The RL Gap: While SFT effectively internalizes new knowledge, standard RL fails even under the Closed-Book training setting. We "
[09.02.2026 19:59] Mistral response. {"id": "823c05deb8364efb8691c6399044c551", "created": 1770667194, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1469, "total_tokens": 1481, "completion_tokens": 12, "num_cached_tokens": 1468}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\"Tsinghua University\"]\n```"}}]}
[09.02.2026 19:59] Response: ```python
["Tsinghua University"]
```
[09.02.2026 19:59] Deleting PDF ./assets/pdf/2602.04811.pdf.
[09.02.2026 19:59] Success.
[09.02.2026 19:59] Downloading and parsing paper https://huggingface.co/papers/2602.03998.
[09.02.2026 19:59] Extra JSON file exists (./assets/json/2602.03998.json), skip PDF parsing.
[09.02.2026 19:59] Paper image links file exists (./assets/img_data/2602.03998.json), skip HTML parsing.
[09.02.2026 19:59] Success.
[09.02.2026 19:59] Enriching papers with extra data.
[09.02.2026 19:59] ********************************************************************************
[09.02.2026 19:59] Abstract 0. Baichuan-M3 is a medical-enhanced large language model designed for clinical decision support with capabilities in proactive information gathering, long-horizon reasoning, and hallucination suppression.  					AI-generated summary 				 We introduce Baichuan-M3, a medical-enhanced large language model...
[09.02.2026 19:59] ********************************************************************************
[09.02.2026 19:59] Abstract 1. RLVR methods using group sampling suffer from bias toward likely trajectories and missed rare-correct ones; a difficulty-aware advantage scaling technique improves performance on benchmarks without increasing computational cost.  					AI-generated summary 				 Reinforcement Learning with Verifiable ...
[09.02.2026 19:59] ********************************************************************************
[09.02.2026 19:59] Abstract 2. OdysseyArena presents a new framework for evaluating large language models on long-horizon, inductive agent tasks that emphasize autonomous discovery of environmental transition laws.  					AI-generated summary 				 The rapid advancement of Large Language Models (LLMs) has catalyzed the development ...
[09.02.2026 19:59] ********************************************************************************
[09.02.2026 19:59] Abstract 3. Sparse Autoencoders trained on Whisper and HuBERT models demonstrate stable feature extraction and effective disentanglement of acoustic and semantic information, showing practical applications in audio processing and correlation with human neural activity.  					AI-generated summary 				 Sparse Aut...
[09.02.2026 19:59] ********************************************************************************
[09.02.2026 19:59] Abstract 4. The paper establishes a theoretical framework for analyzing entropy dynamics in reinforcement fine-tuning of large language models, deriving expressions for entropy change and proposing entropy control methods based on discriminant analysis.  					AI-generated summary 				 Entropy serves as a critic...
[09.02.2026 19:59] ********************************************************************************
[09.02.2026 19:59] Abstract 5. Training instability in large language models is linked to weight matrix stable rank decline and Jacobian alignment, which MSign addresses through matrix sign operations to prevent gradient explosions.  					AI-generated summary 				 Training instability remains a critical challenge in large languag...
[09.02.2026 19:59] ********************************************************************************
[09.02.2026 19:59] Abstract 6. A three-component speech-to-text system combines Wav2Vec2, AST, and Whisper models with curriculum learning and uncertainty modeling to improve transcription accuracy and reduce hallucinations in Russian-language speech.  					AI-generated summary 				 This work presents a speech-to-text system "Pis...
[09.02.2026 19:59] ********************************************************************************
[09.02.2026 19:59] Abstract 7. DreamDojo is a foundation world model trained on 44k hours of egocentric human videos that enables efficient simulation of dexterous robotic tasks through continuous latent actions and real-time distillation.  					AI-generated summary 				 Being able to simulate the outcomes of actions in varied en...
[09.02.2026 19:59] ********************************************************************************
[09.02.2026 19:59] Abstract 8. SWIRL is a self-improvement framework that learns world models from state-only sequences by alternating between forward and inverse dynamics modeling with variational information maximization and ELBO maximization, achieving improved performance on various reasoning and planning benchmarks.  					AI...
[09.02.2026 19:59] ********************************************************************************
[09.02.2026 19:59] Abstract 9. Consequence-Based Utility evaluates mathematical solutions by testing their effectiveness as exemplars for related problems, outperforming reward models and LLM judges in ranking quality and correct-wrong separation.  					AI-generated summary 				 Recent progress in reasoning models suggests that g...
[09.02.2026 19:59] ********************************************************************************
[09.02.2026 19:59] Abstract 10. TRIT framework improves multilingual reasoning by jointly training translation and reasoning components, enhancing question understanding and response generation across languages.  					AI-generated summary 				 Long reasoning models often struggle in multilingual settings: they tend to reason in En...
[09.02.2026 19:59] ********************************************************************************
[09.02.2026 19:59] Abstract 11. GUI agents for automated digital tasks rely on vision-language models with enhanced grounding capabilities, achieved through refined data engineering, improved training strategies, and reinforcement learning with verifiable rewards.  					AI-generated summary 				 The rapid advancement of vision-lan...
[09.02.2026 19:59] ********************************************************************************
[09.02.2026 19:59] Abstract 12. A novel reinforcement learning approach called ARM addresses entropy collapse in LLM reasoning by equilibrating confidence levels across correct responses through dynamic reward shaping.  					AI-generated summary 				 Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as an indispens...
[09.02.2026 19:59] ********************************************************************************
[09.02.2026 19:59] Abstract 13. A comprehensive memory-focused benchmark for mobile GUI agents reveals significant memory capability gaps and provides systematic evaluation methods and design insights.  					AI-generated summary 				 Current mobile GUI agent benchmarks systematically fail to assess memory capabilities, with only 5...
[09.02.2026 19:59] ********************************************************************************
[09.02.2026 19:59] Abstract 14. Canzona presents a unified asynchronous framework that addresses the conflict between matrix-based optimizers and distributed tensor fragmentation in LLM training, improving efficiency and reducing latency.  					AI-generated summary 				 The scaling of Large Language Models (LLMs) drives interest i...
[09.02.2026 19:59] ********************************************************************************
[09.02.2026 19:59] Abstract 15. Multi-modal large language models struggle to jointly understand audio and visual signals in egocentric videos, but a new scalable data engine and dataset significantly improve their performance through targeted fine-tuning.  					AI-generated summary 				 Understanding egocentric videos plays a vit...
[09.02.2026 19:59] ********************************************************************************
[09.02.2026 19:59] Abstract 16. Residual binarization framework RaBiT addresses feature co-adaptation in quantized LLMs through hierarchical path derivation and robust initialization, achieving superior accuracy-efficiency trade-offs.  					AI-generated summary 				 Efficient deployment of large language models (LLMs) requires ext...
[09.02.2026 19:59] ********************************************************************************
[09.02.2026 19:59] Abstract 17. InftyThink+ uses reinforcement learning to optimize iterative reasoning processes, improving accuracy and efficiency in large language models.  					AI-generated summary 				 Large reasoning models achieve strong performance by scaling inference-time chain-of-thought, but this paradigm suffers from ...
[09.02.2026 19:59] ********************************************************************************
[09.02.2026 19:59] Abstract 18. Compar:IA is an open-source platform that collects large-scale human preference data for multilingual language model training and evaluation, featuring a blind pairwise comparison interface and releasing three datasets under open licenses.  					AI-generated summary 				 Large Language Models (LLMs)...
[09.02.2026 19:59] ********************************************************************************
[09.02.2026 19:59] Abstract 19. PlanViz benchmark evaluates unified multimodal models' capabilities in computer-use planning tasks through route planning, work diagramming, and web&UI displaying sub-tasks with a task-adaptive scoring system.  					AI-generated summary 				 Unified multimodal models (UMMs) have shown impressive cap...
[09.02.2026 19:59] ********************************************************************************
[09.02.2026 19:59] Abstract 20. QuantLRM uses weight update magnitude signals from fine-tuning to improve quantization of Large Reasoning Models, achieving better performance than traditional methods through channel importance estimation.  					AI-generated summary 				 Weight-only quantization is important for compressing Large L...
[09.02.2026 19:59] ********************************************************************************
[09.02.2026 19:59] Abstract 21. Multi-objective alignment in LLMs suffers from cross-objective interference where improving performance on some objectives degrades others, with a covariance-based analysis and a proposed method to maintain positive correlations between rewards and training signals.  					AI-generated summary 				 W...
[09.02.2026 19:59] ********************************************************************************
[09.02.2026 19:59] Abstract 22. OmniMoE presents a system-algorithm co-designed framework that achieves fine-grained expert specialization in Mixture-of-Experts architectures through vector-level atomic experts and optimized routing and scheduling mechanisms.  					AI-generated summary 				 Mixture-of-Experts (MoE) architectures a...
[09.02.2026 19:59] ********************************************************************************
[09.02.2026 19:59] Abstract 23. Group-Evolving Agents enable open-ended self-improvement by treating groups of agents as evolutionary units, allowing efficient experience sharing and reuse to enhance coding performance and robustness.  					AI-generated summary 				 Open-ended self-improving agents can autonomously modify their ow...
[09.02.2026 19:59] ********************************************************************************
[09.02.2026 19:59] Abstract 24. A novel framework called SEMA is introduced that effectively trains multi-turn attackers for large language models without relying on existing strategies or external data, achieving state-of-the-art attack success rates while being compact, reproducible, and transferable across different models and ...
[09.02.2026 19:59] ********************************************************************************
[09.02.2026 19:59] Abstract 25. Replacing conventional feed-forward networks with hourglass-shaped MLPs in Transformers improves model efficiency and performance by enabling better parameter utilization and competitive scaling.  					AI-generated summary 				 Dense Transformer language models have largely adhered to one consistent...
[09.02.2026 19:59] ********************************************************************************
[09.02.2026 19:59] Abstract 26. OmniVideo-R1 enhances audio-visual understanding through reinforced frameworks that integrate self-supervised and contrastive learning for multimodal reasoning.  					AI-generated summary 				 While humans perceive the world through diverse modalities that operate synergistically to support a holist...
[09.02.2026 19:59] ********************************************************************************
[09.02.2026 19:59] Abstract 27. ReMiT introduces a bidirectional training approach where reinforcement learning-guided mid-training token reweighting improves large language model pre-training and post-training performance through an iterative feedback loop.  					AI-generated summary 				 Standard training pipelines for large lan...
[09.02.2026 19:59] ********************************************************************************
[09.02.2026 19:59] Abstract 28. Vision transformer components exhibit varying plasticity levels that correlate with finetuning performance, challenging the assumption that smoothness is always beneficial.  					AI-generated summary 				 The smoothness of the transformer architecture has been extensively studied in the context of g...
[09.02.2026 19:59] ********************************************************************************
[09.02.2026 19:59] Abstract 29. SeeUPO is a critic-free reinforcement learning method that ensures convergence guarantees in multi-turn agent interactions by modeling sequential decision-making as multi-agent bandit problems and using backward induction for policy updates.  					AI-generated summary 				 Reinforcement learning (RL...
[09.02.2026 19:59] ********************************************************************************
[09.02.2026 19:59] Abstract 30. SEAD framework enables service dialogue agents to learn effective strategies through self-evolving user modeling components, achieving superior task completion and dialogue efficiency compared to existing foundation and commercial models.  					AI-generated summary 				 Large Language Models have de...
[09.02.2026 19:59] ********************************************************************************
[09.02.2026 19:59] Abstract 31. Table-as-Search framework reformulates information seeking tasks as table completion problems, improving long-horizon search robustness through structured state management.  					AI-generated summary 				 Current Information Seeking (InfoSeeking) agents struggle to maintain focus and coherence durin...
[09.02.2026 19:59] ********************************************************************************
[09.02.2026 19:59] Abstract 32. A diffusion-transformer framework integrates spatio-temporal urban data to predict building-level climate risks while incorporating transportation network structures for emergency response applications.  					AI-generated summary 				 Climate hazards increasingly disrupt urban transportation and eme...
[09.02.2026 19:59] ********************************************************************************
[09.02.2026 19:59] Abstract 33. Seg-ReSearch introduces a novel segmentation approach that combines interleaved reasoning with external search to overcome limitations of frozen MLLM knowledge, using hierarchical reward design for training and demonstrating superior performance on video object segmentation benchmarks.  					AI-gene...
[09.02.2026 19:59] ********************************************************************************
[09.02.2026 19:59] Abstract 34. Knowledge purification techniques consolidate rationales from multiple teacher LLMs to reduce conflicts and improve efficiency in distillation processes.  					AI-generated summary 				 Knowledge distillation has emerged as a pivotal technique for transferring knowledge from stronger large language ...
[09.02.2026 19:59] ********************************************************************************
[09.02.2026 19:59] Abstract 35. Researchers identify and address premature mode collapse in optimal transport-based structural prediction models through an adaptive stability control algorithm that prevents gradient explosions during large-scale training.  					AI-generated summary 				 Differentiable matching layers and residual ...
[09.02.2026 19:59] ********************************************************************************
[09.02.2026 19:59] Abstract 36. Training diffusion models on neural network activations creates meta-models that learn internal state distributions and improve intervention fidelity without restrictive structural assumptions.  					AI-generated summary 				 Existing approaches for analyzing neural network activations, such as PCA ...
[09.02.2026 19:59] ********************************************************************************
[09.02.2026 19:59] Abstract 37. SE-Bench presents a diagnostic environment that obscures NumPy's API to evaluate agents' ability to internally store and utilize novel knowledge without external documentation, revealing challenges in knowledge retention and internalization through different training approaches.  					AI-generated s...
[09.02.2026 19:59] ********************************************************************************
[09.02.2026 19:59] Abstract 38. AtlasPatch is an efficient and scalable whole-slide image preprocessing framework that uses fine-tuned Segment-Anything model for accurate tissue detection and high-throughput patch extraction with reduced computational overhead.  					AI-generated summary 				 Whole-slide image (WSI) preprocessing,...
[09.02.2026 19:59] Read previous papers.
[09.02.2026 19:59] Generating reviews via LLM API.
[09.02.2026 19:59] Using data from previous issue: {"categories": ["#open_source", "#science", "#hallucinations", "#reasoning"], "emoji": "⚕️", "ru": {"title": "От пассивных ответов к активной врачебной поддержке", "desc": "Baichuan-M3 — это медицинская LLM, которая переводит парадигму взаимодействия с пациентами от пассивного ответа на вопросы к ак
[09.02.2026 19:59] Using data from previous issue: {"categories": [], "emoji": "🎯", "ru": {"title": "Справедливое обучение: как найти редкие правильные решения", "desc": "В статье рассматривается проблема смещения в методах обучения с подкреплением и проверяемыми вознаграждениями (RLVR), которые используют групповую выборку для оценки преимуществ. А
[09.02.2026 19:59] Using data from previous issue: {"categories": ["#agents", "#benchmark", "#reasoning", "#long_context", "#open_source", "#dataset"], "emoji": "🧭", "ru": {"title": "Оценка автономного открытия законов природы в долгосрочных взаимодействиях агентов", "desc": "В работе представлен OdysseyArena — новый бенчмарк для оценки больших язык
[09.02.2026 19:59] Using data from previous issue: {"categories": ["#open_source", "#interpretability"], "emoji": "👂", "ru": {"title": "Разреженные автокодировщики раскрывают структуру звука", "desc": "В статье исследуются разреженные автокодировщики (SAE) для интерпретации нейронных представлений в аудиомоделях Whisper и HuBERT. Авторы показывают, 
[09.02.2026 19:59] Using data from previous issue: {"categories": ["#rlhf", "#math", "#training", "#rl"], "emoji": "⚖️", "ru": {"title": "Теория энтропии в усиленном обучении языковых моделей", "desc": "В статье разработан теоретический фреймворк для анализа динамики энтропии при усиленном обучении больших языковых моделей. Авторы выводят выражения 
[09.02.2026 19:59] Using data from previous issue: {"categories": ["#training", "#architecture", "#optimization"], "emoji": "⚡", "ru": {"title": "MSign: стабилизация обучения больших моделей через восстановление ранга матриц", "desc": "В статье исследуется проблема нестабильности обучения больших языковых моделей, которая проявляется в виде взрывног
[09.02.2026 19:59] Using data from previous issue: {"categories": ["#multilingual", "#low_resource", "#architecture", "#open_source", "#training", "#audio", "#hallucinations"], "emoji": "🎤", "ru": {"title": "Ансамбль моделей для надёжного распознавания русской речи без галлюцинаций", "desc": "В статье представлена система распознавания речи «Pisets»
[09.02.2026 19:59] Using data from previous issue: {"categories": ["#inference", "#open_source", "#synthetic", "#transfer_learning", "#video", "#training", "#robotics", "#dataset"], "emoji": "🤖", "ru": {"title": "Универсальная модель мира для ловкого робоуправления из видео человека", "desc": "DreamDojo — это фундаментальная модель мира, обученная н
[09.02.2026 19:59] Using data from previous issue: {"categories": ["#reasoning", "#rl", "#benchmark", "#multimodal", "#optimization", "#training"], "emoji": "🔄", "ru": {"title": "Самообучение моделей мира через чередование прямой и обратной динамики", "desc": "SWIRL — это фреймворк самообучения, который изучает модели мира из последовательностей тол
[09.02.2026 19:59] Using data from previous issue: {"categories": ["#benchmark", "#reasoning", "#science", "#math", "#dataset"], "emoji": "🧮", "ru": {"title": "Оценка математических решений через их практическую полезность на соседних задачах", "desc": "В статье предлагается метод Consequence-Based Utility для оценки качества математических решений 
[09.02.2026 19:59] Using data from previous issue: {"categories": ["#machine_translation", "#transfer_learning", "#multilingual", "#training", "#reasoning"], "emoji": "🌍", "ru": {"title": "Объединение перевода и рассуждений для улучшения многоязычного понимания", "desc": "В статье представлен фреймворк TRIT, который улучшает многоязычное рассуждение
[09.02.2026 19:59] Using data from previous issue: {"categories": ["#multimodal", "#rl", "#benchmark", "#agents", "#training", "#cv", "#data"], "emoji": "🖱️", "ru": {"title": "Точное определение элементов GUI через инженерию данных и обучение с подкреплением", "desc": "В статье представлена модель POINTS-GUI-G-8B для автоматизации задач с графически
[09.02.2026 19:59] Using data from previous issue: {"categories": ["#training", "#reasoning", "#optimization", "#rl"], "emoji": "⚖️", "ru": {"title": "Уравновешивание уверенности для разнообразного рассуждения в больших языковых моделях", "desc": "В статье предложен новый метод ARM (Advantage Re-weighting Mechanism) для решения проблемы коллапса энт
[09.02.2026 19:59] Using data from previous issue: {"categories": [], "emoji": "🧠", "ru": {"title": "Память как критический недостаток мобильных GUI-агентов: комплексная оценка и решения", "desc": "В работе представлена MemGUI-Bench — комплексный бенчмарк для оценки способности мобильных GUI-агентов к запоминанию информации и обучению на основе пред
[09.02.2026 19:59] Using data from previous issue: {"categories": ["#optimization", "#architecture", "#training"], "emoji": "⚡", "ru": {"title": "Асинхронная гармония: матричные оптимизаторы встречают распределённые тензоры", "desc": "Canzona представляет унифицированный асинхронный фреймворк, который решает конфликт между матричными оптимизаторами 
[09.02.2026 19:59] Using data from previous issue: {"categories": ["#audio", "#multimodal", "#benchmark", "#video", "#training", "#robotics", "#dataset"], "emoji": "👁️‍🗨️", "ru": {"title": "Гармоничное слияние звука и видения в эгоцентрических сценах", "desc": "Работа исследует проблему совместного понимания аудио и визуальной информации в эгоцентри
[09.02.2026 19:59] Using data from previous issue: {"categories": ["#inference", "#architecture", "#training", "#optimization"], "emoji": "⚡", "ru": {"title": "Иерархическая бинаризация для эффективного инференса LLM", "desc": "RaBiT — это новая структура квантизации для больших языковых моделей, которая решает проблему совместной адаптации признако
[09.02.2026 19:59] Using data from previous issue: {"categories": ["#small_models", "#optimization", "#rl", "#training", "#reasoning", "#long_context"], "emoji": "🧠", "ru": {"title": "Умное резюмирование мыслей через обучение с подкреплением", "desc": "В статье предложена система InftyThink+, которая использует обучение с подкреплением для оптимизац
[09.02.2026 19:59] Using data from previous issue: {"categories": ["#multilingual", "#data", "#low_resource", "#rlhf", "#alignment", "#dataset", "#open_source", "#benchmark"], "emoji": "🌍", "ru": {"title": "Открытые данные о предпочтениях человека для многоязычных моделей", "desc": "Compar:IA — это открытая платформа для сбора данных о предпочтениях
[09.02.2026 19:59] Using data from previous issue: {"categories": ["#survey", "#multimodal", "#benchmark", "#cv", "#dataset"], "emoji": "📋", "ru": {"title": "Оценка мультимодальных моделей в задачах планирования с использованием компьютера", "desc": "Исследование представляет бенчмарк PlanViz для оценки способностей объединённых мультимодальных моде
[09.02.2026 19:59] Using data from previous issue: {"categories": ["#optimization", "#inference", "#benchmark", "#training", "#reasoning"], "emoji": "⚖️", "ru": {"title": "Защита обоих концов: квантизация через сигналы fine-tuning", "desc": "QuantLRM — это новый метод квантизации больших языковых моделей рассуждения, который использует информацию о 
[09.02.2026 19:59] Querying the API.
[09.02.2026 19:59] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Multi-objective alignment in LLMs suffers from cross-objective interference where improving performance on some objectives degrades others, with a covariance-based analysis and a proposed method to maintain positive correlations between rewards and training signals.  					AI-generated summary 				 We study a persistent failure mode in multi-objective alignment for large language models (LLMs): training improves performance on only a subset of objectives while causing others to degrade. We formalize this phenomenon as cross-objective interference and conduct the first systematic study across classic scalarization algorithms, showing that interference is pervasive and exhibits strong model dependence.   To explain this phenomenon, we derive a local covariance law showing that an objective improves at first order when its reward exhibits positive covariance with the scalarized score. We extend this analysis to clipped surrogate objectives used in modern alignment, demonstrating that the covariance law remains valid under mild conditions despite clipping. Building on this analysis, we propose Covariance Targeted Weight Adaptation (CTWA), a plug-and-play method that maintains positive covariance between objective rewards and the training signal to effectively mitigate cross-objective interference. Finally, we complement these local improvement conditions with a global convergence analysis under the Polyak--Łojasiewicz condition, establishing when non-convex scalarized optimization achieves global convergence and how cross-objective interference depends on specific model geometric properties.
[09.02.2026 19:59] Response: ```json
{
  "desc": "В работе изучается проблема кросс-объективной интерференции при многоцелевом выравнивании больших языковых моделей, когда улучшение по одним целям приводит к деградации по другим. Авторы формализуют это явление и выводят локальный закон ковариации, показывающий что целевая функция улучшается, когда её награда имеет положительную ковариацию с оптимизируемым сигналом. Предложен метод Covariance Targeted Weight Adaptation (CTWA), который поддерживает положительную корреляцию между наградами и тренировочным сигналом для эффективного снижения интерференции. Исследование дополнено глобальным анализом сходимости под условием Полиака-Лойасиевича, который устанавливает условия достижения глобальной оптимальности в невыпуклой оптимизации.",
  "emoji": "⚖️",
  "title": "Баланс целей: преодоление конфликтов при многоцелевом обучении языковых моделей"
}
```
[09.02.2026 19:59] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Multi-objective alignment in LLMs suffers from cross-objective interference where improving performance on some objectives degrades others, with a covariance-based analysis and a proposed method to maintain positive correlations between rewards and training signals.  					AI-generated summary 				 We study a persistent failure mode in multi-objective alignment for large language models (LLMs): training improves performance on only a subset of objectives while causing others to degrade. We formalize this phenomenon as cross-objective interference and conduct the first systematic study across classic scalarization algorithms, showing that interference is pervasive and exhibits strong model dependence.   To explain this phenomenon, we derive a local covariance law showing that an objective improves at first order when its reward exhibits positive covariance with the scalarized score. We extend this analysis to clipped surrogate objectives used in modern alignment, demonstrating that the covariance law remains valid under mild conditions despite clipping. Building on this analysis, we propose Covariance Targeted Weight Adaptation (CTWA), a plug-and-play method that maintains positive covariance between objective rewards and the training signal to effectively mitigate cross-objective interference. Finally, we complement these local improvement conditions with a global convergence analysis under the Polyak--Łojasiewicz condition, establishing when non-convex scalarized optimization achieves global convergence and how cross-objective interference depends on specific model geometric properties."

[09.02.2026 19:59] Response: ```python
["RLHF", "TRAINING", "ARCHITECTURE"]
```
[09.02.2026 19:59] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Multi-objective alignment in LLMs suffers from cross-objective interference where improving performance on some objectives degrades others, with a covariance-based analysis and a proposed method to maintain positive correlations between rewards and training signals.  					AI-generated summary 				 We study a persistent failure mode in multi-objective alignment for large language models (LLMs): training improves performance on only a subset of objectives while causing others to degrade. We formalize this phenomenon as cross-objective interference and conduct the first systematic study across classic scalarization algorithms, showing that interference is pervasive and exhibits strong model dependence.   To explain this phenomenon, we derive a local covariance law showing that an objective improves at first order when its reward exhibits positive covariance with the scalarized score. We extend this analysis to clipped surrogate objectives used in modern alignment, demonstrating that the covariance law remains valid under mild conditions despite clipping. Building on this analysis, we propose Covariance Targeted Weight Adaptation (CTWA), a plug-and-play method that maintains positive covariance between objective rewards and the training signal to effectively mitigate cross-objective interference. Finally, we complement these local improvement conditions with a global convergence analysis under the Polyak--Łojasiewicz condition, establishing when non-convex scalarized optimization achieves global convergence and how cross-objective interference depends on specific model geometric properties."

[09.02.2026 20:00] Response: ```python
["ALIGNMENT", "OPTIMIZATION"]
```
[09.02.2026 20:00] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper addresses a challenge in training large language models (LLMs) where improving one objective can negatively impact others, a phenomenon known as cross-objective interference. The authors conduct a systematic analysis of this issue across various scalarization algorithms, revealing that this interference is common and varies with different models. They introduce a local covariance law that explains how objectives can improve when their rewards are positively correlated with the overall score. To combat this interference, they propose a new method called Covariance Targeted Weight Adaptation (CTWA), which helps maintain positive correlations between objectives during training, thus enhancing overall performance.","title":"Mitigating Cross-Objective Interference in LLMs"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper addresses a challenge in training large language models (LLMs) where improving one objective can negatively impact others, a phenomenon known as cross-objective interference. The authors conduct a systematic analysis of this issue across various scalarization algorithms, revealing that this interference is common and varies with different models. They introduce a local covariance law that explains how objectives can improve when their rewards are positively correlated with the overall score. To combat this interference, they propose a new method called Covariance Targeted Weight Adaptation (CTWA), which helps maintain positive correlations between objectives during training, thus enhancing overall performance.', title='Mitigating Cross-Objective Interference in LLMs'))
[09.02.2026 20:00] Response: ParsedChatCompletionMessage[Article](content='{"desc":"在大型语言模型（LLMs）的多目标对齐中，存在一种持续的失败模式，即在训练过程中，某些目标的性能提升会导致其他目标的性能下降。这种现象被称为跨目标干扰，我们通过系统研究经典的标量化算法，发现这种干扰普遍存在且与模型密切相关。我们提出了一种局部协方差法则，表明当目标的奖励与标量化得分呈正协方差时，目标的性能会得到提升。基于此分析，我们提出了一种新的方法——协方差目标权重适应（CTWA），旨在保持目标奖励与训练信号之间的正协方差，从而有效减轻跨目标干扰。","title":"解决多目标对齐中的跨目标干扰"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='在大型语言模型（LLMs）的多目标对齐中，存在一种持续的失败模式，即在训练过程中，某些目标的性能提升会导致其他目标的性能下降。这种现象被称为跨目标干扰，我们通过系统研究经典的标量化算法，发现这种干扰普遍存在且与模型密切相关。我们提出了一种局部协方差法则，表明当目标的奖励与标量化得分呈正协方差时，目标的性能会得到提升。基于此分析，我们提出了一种新的方法——协方差目标权重适应（CTWA），旨在保持目标奖励与训练信号之间的正协方差，从而有效减轻跨目标干扰。', title='解决多目标对齐中的跨目标干扰'))
[09.02.2026 20:00] Using data from previous issue: {"categories": ["#inference", "#training", "#architecture"], "emoji": "⚡", "ru": {"title": "Атомарные эксперты: максимальная специализация при сохранении скорости", "desc": "OmniMoE представляет систему, которая достигает максимальной специализации экспертов в архитектуре Mixture-of-Experts благодар
[09.02.2026 20:00] Using data from previous issue: {"categories": ["#benchmark", "#plp", "#training", "#agents"], "emoji": "🧬", "ru": {"title": "Групповая эволюция агентов для эффективного самосовершенствования", "desc": "В работе предложена новая парадигма Group-Evolving Agents (GEA) для открытого самосовершенствования агентов, которая рассматривае
[09.02.2026 20:00] Using data from previous issue: {"categories": ["#training", "#rl", "#alignment", "#security", "#open_source"], "emoji": "🎯", "ru": {"title": "Многоходовые атаки на LLM через обучение с подкреплением без внешних данных", "desc": "Представлена фреймворк SEMA для обучения многоходовых атак на большие языковые модели без использовани
[09.02.2026 20:00] Using data from previous issue: {"categories": ["#optimization"], "emoji": "⏳", "ru": {"title": "Песочные часы вместо традиции: переосмысление архитектуры Трансформеров", "desc": "Исследование предлагает заменить традиционные полносвязные сети (FFN) в Трансформерах на более глубокие сети в форме песочных часов, которые лучше испол
[09.02.2026 20:00] Using data from previous issue: {"categories": ["#training", "#video", "#rl", "#multimodal", "#audio"], "emoji": "🎬", "ru": {"title": "Мультимодальное мышление через объединение звука и видео", "desc": "OmniVideo-R1 — это усовершенствованная система для анализа видео, которая одновременно обрабатывает звуковые и визуальные данные.
[09.02.2026 20:00] Using data from previous issue: {"categories": ["#optimization", "#training", "#rl", "#reasoning"], "emoji": "🔄", "ru": {"title": "Обучение в два направления: как постобучение улучшает фундамент модели", "desc": "ReMiT представляет двунаправленный подход к обучению больших языковых моделей, где подкрепление используется для переwe
[09.02.2026 20:00] Using data from previous issue: {"categories": ["#open_source", "#transfer_learning", "#interpretability"], "emoji": "🔄", "ru": {"title": "Пластичность важнее гладкости: ключ к эффективному дообучению трансформеров", "desc": "В этой работе исследуется пластичность компонентов Vision Transformer — их способность адаптировать выходы
[09.02.2026 20:00] Using data from previous issue: {"categories": ["#agents", "#training", "#rl", "#alignment", "#optimization", "#reasoning"], "emoji": "🎯", "ru": {"title": "Гарантированная сходимость обучения с подкреплением без критика через обратную индукцию", "desc": "SeeUPO — это метод обучения с подкреплением без критика, который гарантирует 
[09.02.2026 20:00] Using data from previous issue: {"categories": ["#multimodal", "#training", "#agents"], "emoji": "🤖", "ru": {"title": "Самоэволюционирующий агент для диалогов в системах обслуживания", "desc": "SEAD — это фреймворк для обучения диалоговых агентов в задачах сервисного обслуживания без больших объёмов размеченных данных. Система исп
[09.02.2026 20:00] Using data from previous issue: {"categories": ["#benchmark", "#agents", "#dataset"], "emoji": "🗂️", "ru": {"title": "Таблица как навигатор: структурированный поиск для долгих горизонтов", "desc": "В статье предлагается новый подход Table-as-Search (TaS) для решения задач поиска информации, который преобразует процесс поиска в зад
[09.02.2026 20:00] Using data from previous issue: {"categories": ["#graphs", "#open_source", "#benchmark", "#diffusion", "#dataset", "#science", "#transfer_learning", "#architecture", "#multimodal"], "emoji": "🏢", "ru": {"title": "Прогнозирование климатических рисков и маршрутов эвакуации в городах с диффузионным трансформером", "desc": "В работе п
[09.02.2026 20:00] Using data from previous issue: {"categories": ["#open_source", "#reasoning", "#dataset", "#training", "#multimodal", "#rag", "#cv", "#benchmark", "#video"], "emoji": "🔍", "ru": {"title": "Выход за пределы замороженных знаний: сегментация с поиском и рассуждением", "desc": "Seg-ReSearch представляет новый подход к сегментации, кот
[09.02.2026 20:00] Using data from previous issue: {"categories": ["#transfer_learning", "#small_models", "#training"], "emoji": "🧹", "ru": {"title": "Очистка знаний для эффективной дистилляции из многих учителей", "desc": "В работе предлагается метод очистки знаний для трансферного обучения между большими языковыми моделями и более компактными моде
[09.02.2026 20:00] Using data from previous issue: {"categories": ["#optimization"], "emoji": "⚡", "ru": {"title": "Адаптивная стабилизация режимов в оптимальном транспорте для масштабируемого предсказания структур", "desc": "В работе исследователи выявили проблему преждевременного коллапса мод в моделях структурного предсказания, основанных на опти
[09.02.2026 20:00] Using data from previous issue: {"categories": ["#training", "#interpretability", "#diffusion", "#architecture"], "emoji": "🧠", "ru": {"title": "Генеративные мета-модели как универсальный путь к интерпретируемости", "desc": "В работе предлагается использовать диффузионные модели для обучения на активациях нейронных сетей, создавая
[09.02.2026 20:00] Querying the API.
[09.02.2026 20:00] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

SE-Bench presents a diagnostic environment that obscures NumPy's API to evaluate agents' ability to internally store and utilize novel knowledge without external documentation, revealing challenges in knowledge retention and internalization through different training approaches.  					AI-generated summary 				 True self-evolution requires agents to act as lifelong learners that internalize novel experiences to solve future problems. However, rigorously measuring this foundational capability is hindered by two obstacles: the entanglement of prior knowledge, where ``new'' knowledge may appear in pre-training data, and the entanglement of reasoning complexity, where failures may stem from problem difficulty rather than an inability to recall learned knowledge. We introduce SE-Bench, a diagnostic environment that obfuscates the NumPy library and its API doc into a pseudo-novel package with randomized identifiers. Agents are trained to internalize this package and evaluated on simple coding tasks without access to documentation, yielding a clean setting where tasks are trivial with the new API doc but impossible for base models without it. Our investigation reveals three insights: (1) the Open-Book Paradox, where training with reference documentation inhibits retention, requiring "Closed-Book Training" to force knowledge compression into weights; (2) the RL Gap, where standard RL fails to internalize new knowledge completely due to PPO clipping and negative gradients; and (3) the viability of Self-Play for internalization, proving models can learn from self-generated, noisy tasks when coupled with SFT, but not RL. Overall, SE-Bench establishes a rigorous diagnostic platform for self-evolution with knowledge internalization. Our code and dataset can be found at https://github.com/thunlp/SE-Bench.
[09.02.2026 20:00] Response: ```json
{
  "desc": "Статья представляет SE-Bench — диагностическую среду для оценки способности агентов усваивать и сохранять новые знания без доступа к документации. Авторы выявили три ключевых проблемы: парадокс открытой книги, когда обучение с документацией подавляет запоминание, разрыв между обучением с подкреплением и интернализацией знаний из-за техник вроде PPO clipping, а также потенциал самоигры для эффективного усвоения. Исследование показывает, что модели могут интернализировать новые знания в веса нейронной сети через обучение на самогенерируемых задачах в сочетании с fine-tuning, но не через стандартное обучение с подкреплением.",
  "emoji": "🧠",
  "title": "Как заставить нейросети учиться на всю жизнь"
}
```
[09.02.2026 20:00] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"SE-Bench presents a diagnostic environment that obscures NumPy's API to evaluate agents' ability to internally store and utilize novel knowledge without external documentation, revealing challenges in knowledge retention and internalization through different training approaches.  					AI-generated summary 				 True self-evolution requires agents to act as lifelong learners that internalize novel experiences to solve future problems. However, rigorously measuring this foundational capability is hindered by two obstacles: the entanglement of prior knowledge, where ``new'' knowledge may appear in pre-training data, and the entanglement of reasoning complexity, where failures may stem from problem difficulty rather than an inability to recall learned knowledge. We introduce SE-Bench, a diagnostic environment that obfuscates the NumPy library and its API doc into a pseudo-novel package with randomized identifiers. Agents are trained to internalize this package and evaluated on simple coding tasks without access to documentation, yielding a clean setting where tasks are trivial with the new API doc but impossible for base models without it. Our investigation reveals three insights: (1) the Open-Book Paradox, where training with reference documentation inhibits retention, requiring "Closed-Book Training" to force knowledge compression into weights; (2) the RL Gap, where standard RL fails to internalize new knowledge completely due to PPO clipping and negative gradients; and (3) the viability of Self-Play for internalization, proving models can learn from self-generated, noisy tasks when coupled with SFT, but not RL. Overall, SE-Bench establishes a rigorous diagnostic platform for self-evolution with knowledge internalization. Our code and dataset can be found at https://github.com/thunlp/SE-Bench."

[09.02.2026 20:00] Response: ```python
["DATASET", "BENCHMARK", "AGENTS", "PLP", "RL", "RLHF", "TRAINING"]
```
[09.02.2026 20:00] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"SE-Bench presents a diagnostic environment that obscures NumPy's API to evaluate agents' ability to internally store and utilize novel knowledge without external documentation, revealing challenges in knowledge retention and internalization through different training approaches.  					AI-generated summary 				 True self-evolution requires agents to act as lifelong learners that internalize novel experiences to solve future problems. However, rigorously measuring this foundational capability is hindered by two obstacles: the entanglement of prior knowledge, where ``new'' knowledge may appear in pre-training data, and the entanglement of reasoning complexity, where failures may stem from problem difficulty rather than an inability to recall learned knowledge. We introduce SE-Bench, a diagnostic environment that obfuscates the NumPy library and its API doc into a pseudo-novel package with randomized identifiers. Agents are trained to internalize this package and evaluated on simple coding tasks without access to documentation, yielding a clean setting where tasks are trivial with the new API doc but impossible for base models without it. Our investigation reveals three insights: (1) the Open-Book Paradox, where training with reference documentation inhibits retention, requiring "Closed-Book Training" to force knowledge compression into weights; (2) the RL Gap, where standard RL fails to internalize new knowledge completely due to PPO clipping and negative gradients; and (3) the viability of Self-Play for internalization, proving models can learn from self-generated, noisy tasks when coupled with SFT, but not RL. Overall, SE-Bench establishes a rigorous diagnostic platform for self-evolution with knowledge internalization. Our code and dataset can be found at https://github.com/thunlp/SE-Bench."

[09.02.2026 20:00] Response: ```python
['REASONING', 'OPTIMIZATION', 'OPEN_SOURCE']
```
[09.02.2026 20:00] Response: ParsedChatCompletionMessage[Article](content='{"desc":"SE-Bench is a diagnostic tool designed to evaluate how well agents can store and use new knowledge without relying on external documentation. It addresses challenges in knowledge retention by creating a pseudo-novel environment that obscures the familiar NumPy API, forcing agents to learn and adapt. The study uncovers key insights, such as the Open-Book Paradox, which shows that using reference materials can hinder knowledge retention, and the RL Gap, where traditional reinforcement learning struggles to fully internalize new information. Additionally, it highlights the effectiveness of Self-Play combined with supervised fine-tuning for knowledge internalization, establishing a robust framework for assessing lifelong learning in AI.","title":"Unlocking Lifelong Learning: The SE-Bench Approach"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='SE-Bench is a diagnostic tool designed to evaluate how well agents can store and use new knowledge without relying on external documentation. It addresses challenges in knowledge retention by creating a pseudo-novel environment that obscures the familiar NumPy API, forcing agents to learn and adapt. The study uncovers key insights, such as the Open-Book Paradox, which shows that using reference materials can hinder knowledge retention, and the RL Gap, where traditional reinforcement learning struggles to fully internalize new information. Additionally, it highlights the effectiveness of Self-Play combined with supervised fine-tuning for knowledge internalization, establishing a robust framework for assessing lifelong learning in AI.', title='Unlocking Lifelong Learning: The SE-Bench Approach'))
[09.02.2026 20:00] Response: ParsedChatCompletionMessage[Article](content='{"desc":"SE-Bench是一个诊断环境，旨在评估智能体在没有外部文档的情况下，内部存储和利用新知识的能力。该研究揭示了知识保留和内化的挑战，特别是在不同的训练方法下。通过模糊NumPy的API，智能体被训练以内部化一个伪新包，并在没有文档的情况下完成简单的编码任务。研究发现，参考文档的训练会抑制知识的保留，而自我学习和自我生成的任务结合微调可以有效促进知识的内化。","title":"自我进化：知识内化的新挑战"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='SE-Bench是一个诊断环境，旨在评估智能体在没有外部文档的情况下，内部存储和利用新知识的能力。该研究揭示了知识保留和内化的挑战，特别是在不同的训练方法下。通过模糊NumPy的API，智能体被训练以内部化一个伪新包，并在没有文档的情况下完成简单的编码任务。研究发现，参考文档的训练会抑制知识的保留，而自我学习和自我生成的任务结合微调可以有效促进知识的内化。', title='自我进化：知识内化的新挑战'))
[09.02.2026 20:00] Using data from previous issue: {"categories": ["#healthcare", "#cv", "#dataset", "#open_source", "#data", "#inference", "#science"], "emoji": "🔬", "ru": {"title": "Быстрая подготовка медицинских снимков через адаптированный Segment-Anything", "desc": "AtlasPatch — это эффективная и масштабируемая система предварительной обработки
[09.02.2026 20:00] Renaming data file.
[09.02.2026 20:00] Renaming previous data. hf_papers.json to ./d/2026-02-09.json
[09.02.2026 20:00] Saving new data file.
[09.02.2026 20:00] Generating page.
[09.02.2026 20:00] Renaming previous page.
[09.02.2026 20:00] Renaming previous data. index.html to ./d/2026-02-09.html
[09.02.2026 20:00] Writing result.
[09.02.2026 20:00] Renaming log file.
[09.02.2026 20:00] Renaming previous data. log.txt to ./logs/2026-02-09_last_log.txt
