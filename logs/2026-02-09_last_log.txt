[09.02.2026 07:54] Read previous papers.
[09.02.2026 07:54] Generating top page (month).
[09.02.2026 07:54] Writing top page (month).
[09.02.2026 08:42] Read previous papers.
[09.02.2026 08:42] Get feed.
[09.02.2026 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06570
[09.02.2026 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2602.05843
[09.02.2026 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03392
[09.02.2026 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01734
[09.02.2026 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06717
[09.02.2026 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06291
[09.02.2026 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2602.05940
[09.02.2026 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06391
[09.02.2026 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2602.05281
[09.02.2026 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06075
[09.02.2026 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06949
[09.02.2026 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06960
[09.02.2026 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06663
[09.02.2026 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2602.05367
[09.02.2026 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2602.05711
[09.02.2026 08:42] Extract page data from URL. URL: https://huggingface.co/papers/2602.05027
[09.02.2026 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06854
[09.02.2026 08:42] Extract page data from URL. URL: https://huggingface.co/papers/2602.04837
[09.02.2026 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06554
[09.02.2026 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06471
[09.02.2026 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06139
[09.02.2026 08:42] Extract page data from URL. URL: https://huggingface.co/papers/2602.06129
[09.02.2026 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2602.04454
[09.02.2026 08:42] Extract page data from URL. URL: https://huggingface.co/papers/2602.06079
[09.02.2026 08:42] Extract page data from URL. URL: https://huggingface.co/papers/2602.02581
[09.02.2026 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2601.23039
[09.02.2026 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06883
[09.02.2026 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01064
[09.02.2026 08:42] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[09.02.2026 08:42] No deleted papers detected.
[09.02.2026 08:42] Downloading and parsing papers (pdf, html). Total: 28.
[09.02.2026 08:42] Downloading and parsing paper https://huggingface.co/papers/2602.06570.
[09.02.2026 08:42] Extra JSON file exists (./assets/json/2602.06570.json), skip PDF parsing.
[09.02.2026 08:42] Paper image links file exists (./assets/img_data/2602.06570.json), skip HTML parsing.
[09.02.2026 08:42] Success.
[09.02.2026 08:42] Downloading and parsing paper https://huggingface.co/papers/2602.05843.
[09.02.2026 08:42] Extra JSON file exists (./assets/json/2602.05843.json), skip PDF parsing.
[09.02.2026 08:42] Paper image links file exists (./assets/img_data/2602.05843.json), skip HTML parsing.
[09.02.2026 08:42] Success.
[09.02.2026 08:42] Downloading and parsing paper https://huggingface.co/papers/2602.03392.
[09.02.2026 08:42] Extra JSON file exists (./assets/json/2602.03392.json), skip PDF parsing.
[09.02.2026 08:42] Paper image links file exists (./assets/img_data/2602.03392.json), skip HTML parsing.
[09.02.2026 08:42] Success.
[09.02.2026 08:42] Downloading and parsing paper https://huggingface.co/papers/2602.01734.
[09.02.2026 08:42] Extra JSON file exists (./assets/json/2602.01734.json), skip PDF parsing.
[09.02.2026 08:42] Paper image links file exists (./assets/img_data/2602.01734.json), skip HTML parsing.
[09.02.2026 08:42] Success.
[09.02.2026 08:42] Downloading and parsing paper https://huggingface.co/papers/2602.06717.
[09.02.2026 08:42] Downloading paper 2602.06717 from https://arxiv.org/pdf/2602.06717v1...
[09.02.2026 08:42] Extracting affiliations from text.
[09.02.2026 08:42] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 6 ] . [ 1 7 1 7 6 0 . 2 0 6 2 : r F-GRPO: Dont Let Your Policy Learn the Obvious and Forget the Rare Daniil Plyusov * 1 2 Alexey Gorbatovski * 1 Boris Shaposhnikov 1 Viacheslav Sinii 1 Alexey Malakhov 1 Daniil Gavrilov "
[09.02.2026 08:42] Response: ```python
[]
```
[09.02.2026 08:42] Extracting affiliations from text.
[09.02.2026 08:42] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 6 ] . [ 1 7 1 7 6 0 . 2 0 6 2 : r F-GRPO: Dont Let Your Policy Learn the Obvious and Forget the Rare Daniil Plyusov * 1 2 Alexey Gorbatovski * 1 Boris Shaposhnikov 1 Viacheslav Sinii 1 Alexey Malakhov 1 Daniil GavrilovReinforcement Learning with Verifiable Rewards (RLVR) is commonly based on group sampling to estimate advantages and stabilize policy updates. In practice, large group sizes are not feasible due to computational limits, which biases learning toward trajectories that are already likely. Smaller groups often miss rare-correct trajectories while still containing mixed rewards, concentrating probability on common solutions. We derive the probability that updates miss rare-correct modes as function of group size, showing nonmonotonic behavior, and characterize how updates redistribute mass within the correct set, revealing that unsampled-correct mass can shrink even as total correct mass grows. Motivated by this analysis, we propose difficulty-aware advantage scaling coefficient, inspired by Focal loss, that down-weights updates on high-success prompts. The lightweight modification can be directly integrated into any group-relative RLVR algorithm such as GRPO, DAPO, and CISPO. On Qwen2.5-7B across in-domain and out-of-domain benchmarks, our method improves pass@256 from 64.1 70.3 (GRPO), 69.3 72.5 (DAPO), and 73.2 76.8 (CISPO), while preserving or improving pass@1, without increasing group size or computational cost. 1. Introduction Reinforcement Learning with Verifiable Rewards (RLVR) has become standard paradigm for post-training large language models (LLMs), enabling strong gains on reasoningintensive tasks without reliance on human preference data (Zhang et al., 2025). By leveraging automatically checkable reward signals, RLVR has driven state-of-the-art performance in mathematical reasoning (Li et al., 2024), code *Equal contribution 1T-Tech 2Saint Petersburg Electrotechnical University LETI. Correspondence to: Alexey Gorbatovski <a.gorbatovskiy@t-tech.dev>. Preprint. February 9, 2026. generation (Jimenez et al., 2023), and general problem solving (Chollet et al., 2025), and is now widely adopted in large-scale post-training (Guo et al., 2025; Yang et al., 2025; Team et al., 2025; Shao et al., 2024). Despite these successes, growing body of work suggests that RLVR does not primarily introduce new knowledge, but instead sharpens the output distribution toward solutions already accessible to the base model (Yue et al., 2025; Ni et al., 2025; Wu et al., 2025a; Dang et al., 2025). Empirical evidence based on pass@k (Chen et al., 2021) indicates that RLVR-trained models may underperform their base counterparts at sufficiently large sampling budgets, consistent with narrowing of solution diversity (Matsutani et al., 2025). At the same time, other studies argue that prolonged or carefully scaled RL can expand the effective reasoning boundary (Liu et al., 2025b; Yuan et al., 2025), leaving the role of RLVR an open question. Most modern RLVR systems rely on group-relative methods such as GRPO (Shao et al., 2024) and its variants (Yu et al., 2025; Chen et al., 2025a; Liu et al., 2025c), which compute advantages from multiple rollouts per prompt. The group size thus becomes critical design choice, yet existing work provides conflicting guidance: Wu et al. (2025b) show that two rollouts suffice and connect GRPO to DPO (Rafailov et al., 2023), while Hu et al. (2025) advocate scaling rollouts to broaden exploration. Since group size directly controls which trajectories receive learning signal, understanding its interaction with sharpening is essential. This raises fundamental question: how does group size affect the optimization dynamics of group-relative RLVR with binary rewards, and can we mitigate sharpening without scaling computational cost? In this paper, we analyze the sampling dynamics of grouprelative RLVR and propose F-GRPO, lightweight modification that addresses sharpening at practical group sizes. Our contributions are as follows: We derive closed-form tail-miss probability characterizing when active RLVR updates miss rare-correct modes, revealing non-monotonic dependence on group size that reconciles conflicting prior findings: small groups preserve diversity through inactivity, large groups through coverage, while intermediate groups, most common in F-GRPO: Dont Let Your Policy Learn the Obvious and Forget the Rare Figure 1. (a) Probability that training update is active (mixed rewards in batch) yet misses rare-correct solutions, as function of group size . This probability peaks at intermediate : small groups rarely produce learning signal, large groups cover rare modes, but moderate groups combine active updates with poor coverage. (b,c) Empirical consequences on AIME 2025 (math) and IFEval (OOD): GRPO at =8 improves pass@1 over =2 but degrades pass@256, consistent with the sharpening regime. F-GRPO at =8 recovers pass@256 while maintaining pass@1, using 4 less compute than =32. practice, maximize sharpening risk. the success probability Building on the categorical framework of Hu et al. (2025), we analyze how probability mass redistributes within the correct set, showing that unsampled-correct mass can decrease even when total correct mass increases. We propose F-GRPO, difficulty-aware advantage scaling applicable to any group-relative objective including GRPO, DAPO, and CISPO, and demonstrate consistent pass@256 improvements on both reasoning math and OOD benchmarks while preserving or improving pass@1 across three model families, without additional computational cost. Figure 1 illustrates the core finding: tail-miss probability peaks at intermediate group sizes, and F-GRPO at =8 matches or exceeds GRPO at =32, achieving higher pass@256 (52.6 vs. 49.5 on AIME 2025; 75.7 vs. 71.4 on IFEval) and improved OOD pass@1 (34.0 vs. 31.0), while using 4 fewer rollouts. 2. Preliminaries 2.1. Reinforcement Learning with Verifiable Rewards We consider reinforcement learning with verifiable rewards (RLVR) for language model reasoning. Given prompt x, the policy πθ generates complete responses (trajectories). We sample group of i.i.d. rollouts {oi}N i=1 πθ( x) and assign binary outcome rewards Ri = Rw + (Rc Rw) I[oi is correct] (1) µpos(x) := Pr [o C(x)], oπθ(x) (2) For analysis, we consider designated subset Crare(x) C(x) of correct rollouts, with mass under the current policy τ (x) := Pr oπθ(x) [o Crare(x)]. (3) By construction 0 τ (x) µpos(x). We call Crare(x) ra"
[09.02.2026 08:42] Mistral response. {"id": "e60b3cdb58004474b8b35e28a0c75da3", "created": 1770626538, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1728, "total_tokens": 1750, "completion_tokens": 22, "num_cached_tokens": 1727}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\"T-Tech\", \"Saint Petersburg Electrotechnical University LETI\"]\n```"}}]}
[09.02.2026 08:42] Response: ```python
["T-Tech", "Saint Petersburg Electrotechnical University LETI"]
```
[09.02.2026 08:42] Deleting PDF ./assets/pdf/2602.06717.pdf.
[09.02.2026 08:42] Success.
[09.02.2026 08:42] Downloading and parsing paper https://huggingface.co/papers/2602.06291.
[09.02.2026 08:42] Extra JSON file exists (./assets/json/2602.06291.json), skip PDF parsing.
[09.02.2026 08:42] Paper image links file exists (./assets/img_data/2602.06291.json), skip HTML parsing.
[09.02.2026 08:42] Success.
[09.02.2026 08:42] Downloading and parsing paper https://huggingface.co/papers/2602.05940.
[09.02.2026 08:42] Extra JSON file exists (./assets/json/2602.05940.json), skip PDF parsing.
[09.02.2026 08:42] Paper image links file exists (./assets/img_data/2602.05940.json), skip HTML parsing.
[09.02.2026 08:42] Success.
[09.02.2026 08:42] Downloading and parsing paper https://huggingface.co/papers/2602.06391.
[09.02.2026 08:42] Extra JSON file exists (./assets/json/2602.06391.json), skip PDF parsing.
[09.02.2026 08:42] Paper image links file exists (./assets/img_data/2602.06391.json), skip HTML parsing.
[09.02.2026 08:42] Success.
[09.02.2026 08:42] Downloading and parsing paper https://huggingface.co/papers/2602.05281.
[09.02.2026 08:42] Extra JSON file exists (./assets/json/2602.05281.json), skip PDF parsing.
[09.02.2026 08:42] Paper image links file exists (./assets/img_data/2602.05281.json), skip HTML parsing.
[09.02.2026 08:42] Success.
[09.02.2026 08:42] Downloading and parsing paper https://huggingface.co/papers/2602.06075.
[09.02.2026 08:42] Extra JSON file exists (./assets/json/2602.06075.json), skip PDF parsing.
[09.02.2026 08:42] Paper image links file exists (./assets/img_data/2602.06075.json), skip HTML parsing.
[09.02.2026 08:42] Success.
[09.02.2026 08:42] Downloading and parsing paper https://huggingface.co/papers/2602.06949.
[09.02.2026 08:42] Extra JSON file exists (./assets/json/2602.06949.json), skip PDF parsing.
[09.02.2026 08:42] Paper image links file exists (./assets/img_data/2602.06949.json), skip HTML parsing.
[09.02.2026 08:42] Success.
[09.02.2026 08:42] Downloading and parsing paper https://huggingface.co/papers/2602.06960.
[09.02.2026 08:42] Extra JSON file exists (./assets/json/2602.06960.json), skip PDF parsing.
[09.02.2026 08:42] Paper image links file exists (./assets/img_data/2602.06960.json), skip HTML parsing.
[09.02.2026 08:42] Success.
[09.02.2026 08:42] Downloading and parsing paper https://huggingface.co/papers/2602.06663.
[09.02.2026 08:42] Extra JSON file exists (./assets/json/2602.06663.json), skip PDF parsing.
[09.02.2026 08:42] Paper image links file exists (./assets/img_data/2602.06663.json), skip HTML parsing.
[09.02.2026 08:42] Success.
[09.02.2026 08:42] Downloading and parsing paper https://huggingface.co/papers/2602.05367.
[09.02.2026 08:42] Extra JSON file exists (./assets/json/2602.05367.json), skip PDF parsing.
[09.02.2026 08:42] Paper image links file exists (./assets/img_data/2602.05367.json), skip HTML parsing.
[09.02.2026 08:42] Success.
[09.02.2026 08:42] Downloading and parsing paper https://huggingface.co/papers/2602.05711.
[09.02.2026 08:42] Extra JSON file exists (./assets/json/2602.05711.json), skip PDF parsing.
[09.02.2026 08:42] Paper image links file exists (./assets/img_data/2602.05711.json), skip HTML parsing.
[09.02.2026 08:42] Success.
[09.02.2026 08:42] Downloading and parsing paper https://huggingface.co/papers/2602.05027.
[09.02.2026 08:42] Downloading paper 2602.05027 from https://arxiv.org/pdf/2602.05027v2...
[09.02.2026 08:42] Extracting affiliations from text.
[09.02.2026 08:42] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 6 ] . [ 2 7 2 0 5 0 . 2 0 6 2 : r AudioSAE: Towards Understanding of Audio-Processing Models with Sparse AutoEncoders Georgii Aparin*, Tasnima Sadekova*, Alexey Rukhovich, Assel Yermekova, Laida Kushnareva, Vadim Popov, Kristian Kuznetsov, Irina Piontkovskaya Huawei Noahs Ark Lab aparingm@gmail.com, sadekova.t.r@gmail.com "
[09.02.2026 08:42] Response: ```python
["Huawei Noahs Ark Lab"]
```
[09.02.2026 08:42] Deleting PDF ./assets/pdf/2602.05027.pdf.
[09.02.2026 08:42] Success.
[09.02.2026 08:42] Downloading and parsing paper https://huggingface.co/papers/2602.06854.
[09.02.2026 08:42] Extra JSON file exists (./assets/json/2602.06854.json), skip PDF parsing.
[09.02.2026 08:42] Paper image links file exists (./assets/img_data/2602.06854.json), skip HTML parsing.
[09.02.2026 08:42] Success.
[09.02.2026 08:42] Downloading and parsing paper https://huggingface.co/papers/2602.04837.
[09.02.2026 08:42] Downloading paper 2602.04837 from https://arxiv.org/pdf/2602.04837v1...
[09.02.2026 08:42] Extracting affiliations from text.
[09.02.2026 08:42] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 4 ] . [ 1 7 3 8 4 0 . 2 0 6 2 : r Group-Evolving Agents: Open-Ended Self-Improvement via Experience Sharing Zhaotian Weng1 Antonis Antoniades1 Deepak Nathani1 Zhen Zhang1 Xiao Pu 1 Xin Eric Wang 1 University of California, Santa Barbara {zhaotian,ericxwang}@ucsb.edu Open-ended self-improving agents can autonomously modify their own structural designs to advance their capabilities and overcome the limits of pre-defined architectures, thus reducing reliance on human intervention. We introduce Group-Evolving Agents (GEA), new paradigm for open-ended self-improvements, which treats group of agents as the fundamental evolutionary unit, enabling explicit experience sharing and reuse within the group throughout evolution. Unlike existing open-ended self-evolving paradigms that adopt tree-structured evolution, GEA overcomes the limitation of inefficient utilization of exploratory diversity caused by isolated evolutionary branches. We evaluate GEA on challenging coding benchmarks, where it significantly outperforms state-of-the-art self-evolving methods (71.0% vs. 56.7% on SWE-bench Verified, 88.3% vs. 68.3% on Polyglot) and matches or exceeds top human-designed agent frameworks (71.8% and 52.0% on two benchmarks, respectively). Analysis reveals that GEA more effectively converts early-stage exploratory diversity into sustained, long-term progress, achieving stronger performance under the same number of evolved agents. Furthermore, GEA exhibits consistent transferability across different coding models and greater robustness, fixing framework-level bugs in 1.4 iterations on average, versus 5 for self-evolving methods. Figure 1: Overview of Group-Evolving Agents (GEA) vs. tree-structured self-evolution for open-endedness. GEA treats group of agents, rather than an individual agent, as the fundamental unit of evolution. At each iteration, parent group jointly gives rise to an offspring group through explicit intra-group Experience sharing and reuse. 1. Introduction Open"
[09.02.2026 08:42] Response: ```python
["University of California, Santa Barbara"]
```
[09.02.2026 08:42] Deleting PDF ./assets/pdf/2602.04837.pdf.
[09.02.2026 08:42] Success.
[09.02.2026 08:42] Downloading and parsing paper https://huggingface.co/papers/2602.06554.
[09.02.2026 08:42] Extra JSON file exists (./assets/json/2602.06554.json), skip PDF parsing.
[09.02.2026 08:42] Paper image links file exists (./assets/img_data/2602.06554.json), skip HTML parsing.
[09.02.2026 08:42] Success.
[09.02.2026 08:42] Downloading and parsing paper https://huggingface.co/papers/2602.06471.
[09.02.2026 08:42] Extra JSON file exists (./assets/json/2602.06471.json), skip PDF parsing.
[09.02.2026 08:42] Paper image links file exists (./assets/img_data/2602.06471.json), skip HTML parsing.
[09.02.2026 08:42] Success.
[09.02.2026 08:42] Downloading and parsing paper https://huggingface.co/papers/2602.06139.
[09.02.2026 08:42] Extra JSON file exists (./assets/json/2602.06139.json), skip PDF parsing.
[09.02.2026 08:42] Paper image links file exists (./assets/img_data/2602.06139.json), skip HTML parsing.
[09.02.2026 08:42] Success.
[09.02.2026 08:42] Downloading and parsing paper https://huggingface.co/papers/2602.06129.
[09.02.2026 08:42] Downloading paper 2602.06129 from https://arxiv.org/pdf/2602.06129v1...
[09.02.2026 08:42] Extracting affiliations from text.
[09.02.2026 08:42] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"IEEE TRANSACTIONS ON INTELLIGENT VEHICLES, FEBRUARY 2026 1 Urban Spatio-Temporal Foundation Models for Climate-Resilient Housing: Scaling Diffusion Transformers for Disaster Risk Prediction Olaf Yunus Laitinen Imanov, Derya Umut Kulali, and Taner Yilmaz 6 2 0 2 ] . [ 1 9 2 1 6 0 . 2 0 6 2 : r AbstractClimate hazards increasingly disrupt urban transportation and emergency-response operations by damaging housing stock, degrading infrastructure, and reducing network accessibility. This paper presents Skjold-DiT, diffusiontransformer framework that integrates heterogeneous spatiotemporal urban data to forecast building-level climate-risk indicators while explicitly incorporating transportation-network structure and accessibility signals relevant to intelligent vehicles (e.g., emergency reachability and evacuation-route constraints). Concretely, Skjold-DiT enables hazard-conditioned routing constraints by producing calibrated, uncertainty-aware accessibility layers (reachability, travel-time inflation, and route redundancy) that can be consumed by intelligent-vehicle routing and emergency dispatch systems. Skjold-DiT combines: (1) Fjell-Prompt, prompt-based conditioning interface designed to support crosscity transfer; (2) Norrland-Fusion, cross-modal attention mechanism unifying hazard maps/imagery, building attributes, demographics, and transportation infrastructure into shared latent representation; and (3) Valkyrie-Forecast, counterfactual simulator for generating probabilistic risk trajectories under intervention prompts. We introduce the Baltic-Caspian Urban Resilience (BCUR) dataset with 847,392 building-level observations across six cities, including multi-hazard annotations (e.g., flood and heat indicators) and transportation accessibility features. Experiments evaluate prediction quality, cross-city generalization, calibration, and downstream transportation-relevant outcomes, including reachability and hazard-conditioned travel times under counterfactual interv"
[09.02.2026 08:42] Response: ```python
[]
```
[09.02.2026 08:42] Extracting affiliations from text.
[09.02.2026 08:42] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"IEEE TRANSACTIONS ON INTELLIGENT VEHICLES, FEBRUARY 2026 1 Urban Spatio-Temporal Foundation Models for Climate-Resilient Housing: Scaling Diffusion Transformers for Disaster Risk Prediction Olaf Yunus Laitinen Imanov, Derya Umut Kulali, and Taner Yilmaz 6 2 0 2 ] . [ 1 9 2 1 6 0 . 2 0 6 2 : r AbstractClimate hazards increasingly disrupt urban transportation and emergency-response operations by damaging housing stock, degrading infrastructure, and reducing network accessibility. This paper presents Skjold-DiT, diffusiontransformer framework that integrates heterogeneous spatiotemporal urban data to forecast building-level climate-risk indicators while explicitly incorporating transportation-network structure and accessibility signals relevant to intelligent vehicles (e.g., emergency reachability and evacuation-route constraints). Concretely, Skjold-DiT enables hazard-conditioned routing constraints by producing calibrated, uncertainty-aware accessibility layers (reachability, travel-time inflation, and route redundancy) that can be consumed by intelligent-vehicle routing and emergency dispatch systems. Skjold-DiT combines: (1) Fjell-Prompt, prompt-based conditioning interface designed to support crosscity transfer; (2) Norrland-Fusion, cross-modal attention mechanism unifying hazard maps/imagery, building attributes, demographics, and transportation infrastructure into shared latent representation; and (3) Valkyrie-Forecast, counterfactual simulator for generating probabilistic risk trajectories under intervention prompts. We introduce the Baltic-Caspian Urban Resilience (BCUR) dataset with 847,392 building-level observations across six cities, including multi-hazard annotations (e.g., flood and heat indicators) and transportation accessibility features. Experiments evaluate prediction quality, cross-city generalization, calibration, and downstream transportation-relevant outcomes, including reachability and hazard-conditioned travel times under counterfactual interventions. Index TermsDiffusion Transformers, Intelligent Transportation Systems, Climate-Resilient Housing, Urban Foundation Models, Multi-Modal Learning, Counterfactual Generation, Smart Cities I. INTRODUCTION HE convergence of rapid urbanization, climate change, and evolving transportation needs presents critical challenges to global urban resilience. In the context of World Urban Forum 13 (WUF13), where resilient housing and inclusive mobility are emphasized, there is growing need for quantitative, city-scale Manuscript received February 5, 2026. (Corresponding author: Olaf Yunus Laitinen Imanov.) This work was supported by the Technical University of Denmarks Climate Adaptation Initiative and the Baltic-Caspian Urban Research Consortium. O. Y. L. Laitinen Imanov is with the Department of Applied Mathematics and Computer Science (DTU Compute), Technical University of Denmark, Kongens Lyngby, Denmark (e-mail: oyli@dtu.dk; ORCID: 0009-0006-51840810). D. U. Kulali is with the Department of Engineering, Eskisehir Technical University, Eskisehir, Türkiye (e-mail: d_u_k@ogr.eskisehir.edu.tr; ORCID: 0009-0004-8844-6601). T. Yilmaz is with the Department of Computer Engineering, Afyon Kocatepe taner.yilmaz@usr.aku.edu.tr; University, Afyonkarahisar, Türkiye (e-mail: ORCID: 0009-0004-5197-5227). decision support that links climate risk, housing vulnerability, and transportation accessibility [1]. Urban population growth increases exposure of housing and transportation infrastructure to climate hazards. Climate disasters compound these challenges: flood-related damages exceed $157 billion annually [2], heat-related mortality has risen 68% since 2000 [3], and sea-level rise threatens 410 million coastal residents by 2100 [4]. These hazards critically impact transportation accessibility, emergency vehicle routing, and evacuation planning in urban environments. Copenhagens catastrophic 2011 cloudburst caused $1.9 billion in damages within two hours, demonstrating vulnerability even in sustainability-leading cities [5]. The event paralyzed transportation networks, preventing emergency vehicle access to affected neighborhoods. Baku faces 18-25 million USD in annual flood losses with inadequate early warning systems [6], severely limiting intelligent vehicle navigation during flood events. These crises reveal fundamental gap: existing urban planning lacks predictive tools integrating climate science, housing vulnerability, transportation infrastructure, and policy intervention scenarios at building-level granularity. A. Intelligent Transportation Systems and Climate Resilience Modern intelligent vehicle systems rely on comprehensive understanding of urban infrastructure health and accessibility. Climate-induced housing damage directly impacts: Emergency Response: Flood and heat events impair ambulance, fire, and police vehicle routing when road networks become impassable or buildings require evacuation support. Autonomous Vehicle Navigation: Self-driving vehicles require accurate predictions of infrastructure degradation, road closures, and hazardous conditions to ensure passenger safety. Traffic Management: Housing vulnerability patterns influence evacuation route planning, temporary shelter logistics, and post-disaster traffic flow redistribution. Urban Planning Integration: Transportation infrastructure investments must account for climate resilience to ensure long-term accessibility and prevent stranded assets. Despite growing research in intelligent transportation systems [7], few frameworks integrate housing vulnerability prediction with transportation network analysis, creating critical gaps in disaster preparedness and autonomous vehicle safety protocols. IEEE TRANSACTIONS ON INTELLIGENT VEHICLES, FEBRUARY 2026 B. Foundation Models for Urban Intelligence Recent advances in large-scale representation learning have motivated foundation model approaches in several domains, yet urban systems remain underexplored. UrbanDiT [8] pioneered diffusion transformers for spatio-temporal urban forecasting, demonstrating cross-city generalization for mobility prediction. However, UrbanDiT focuses on traffic flow and does not integrate multi-hazard climate risks, building-level vulnerability, transportation accessibility signals, or counterfactual intervention simulation required for resilience-aware intelligent-vehicle applications. Parallel work in climate science employs physics-based flood modeling requiring 10,000+ CPU hours per city"
[09.02.2026 08:42] Mistral response. {"id": "a338ef57678c49cf9654b4332f4add7d", "created": 1770626563, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1369, "total_tokens": 1443, "completion_tokens": 74, "num_cached_tokens": 1368}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\n    \"Department of Applied Mathematics and Computer Science (DTU Compute), Technical University of Denmark, Kongens Lyngby, Denmark\",\n    \"Department of Engineering, Eskisehir Technical University, Eskisehir, T\u00fcrkiye\",\n    \"Department of Computer Engineering, Afyon Kocatepe University, Afyonkarahisar, T\u00fcrkiye\"\n]\n```"}}]}
[09.02.2026 08:42] Response: ```python
[
    "Department of Applied Mathematics and Computer Science (DTU Compute), Technical University of Denmark, Kongens Lyngby, Denmark",
    "Department of Engineering, Eskisehir Technical University, Eskisehir, Türkiye",
    "Department of Computer Engineering, Afyon Kocatepe University, Afyonkarahisar, Türkiye"
]
```
[09.02.2026 08:42] Deleting PDF ./assets/pdf/2602.06129.pdf.
[09.02.2026 08:42] Success.
[09.02.2026 08:42] Downloading and parsing paper https://huggingface.co/papers/2602.04454.
[09.02.2026 08:42] Downloading paper 2602.04454 from https://arxiv.org/pdf/2602.04454v1...
[09.02.2026 08:42] Extracting affiliations from text.
[09.02.2026 08:42] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Seg-ReSearch: Segmentation with Interleaved Reasoning and External Search Tianming Liang 1 Qirui Du 1 Jian-Fang Hu 1 (cid:66) Haichao Jiang 1 Zicheng Lin 1 Wei-Shi Zheng 1 6 2 0 2 ] . [ 1 4 5 4 4 0 . 2 0 6 2 : r Figure 1. Through multi-turn interleaved reasoning and web search, our Seg-ReSearch is able to localize and segment any languagereferred target in videos, even those involving new concepts or up-to-date information that lies beyond the internal knowledge of MLLMs. "
[09.02.2026 08:42] Response: ```python
[]
```
[09.02.2026 08:42] Extracting affiliations from text.
[09.02.2026 08:42] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Seg-ReSearch: Segmentation with Interleaved Reasoning and External Search Tianming Liang 1 Qirui Du 1 Jian-Fang Hu 1 (cid:66) Haichao Jiang 1 Zicheng Lin 1 Wei-Shi Zheng 1 6 2 0 2 ] . [ 1 4 5 4 4 0 . 2 0 6 2 : r Figure 1. Through multi-turn interleaved reasoning and web search, our Seg-ReSearch is able to localize and segment any languagereferred target in videos, even those involving new concepts or up-to-date information that lies beyond the internal knowledge of MLLMs.Segmentation based on language has been popular topic in computer vision. While recent advances in multimodal large language models (MLLMs) have endowed segmentation systems with reasoning capabilities, these efforts remain confined by the frozen internal knowledge of MLLMs, which limits their potential for real-world scenarios that involve up-to-date information or domain-specific concepts. In this work, we propose Seg-ReSearch, novel segmentation paradigm that overcomes the knowledge bottleneck of existing approaches. By en1ISEE Lab, Sun Yat-sen University. Correspondence to: Tianming Liang <liangtm@mail2.sysu.edu.cn>, Jian-Fang Hu <hujf5@mail.sysu.edu.cn>. Preprint. February 5, 2026. 1 abling interleaved reasoning and external search, Seg-ReSearch empowers segmentation systems to handle dynamic, open-world queries that extend beyond the frozen knowledge of MLLMs. To effectively train this capability, we introduce hierarchical reward design that harmonizes initial guidance with progressive incentives, mitigating the dilemma between sparse outcome signals and rigid step-wise supervision. For evaluation, we construct OK-VOS, challenging benchmark that explicitly requires outside knowledge for video object segmentation. Experiments on OK-VOS and two existing reasoning segmentation benchmarks demonstrate that our Seg-ReSearch improves state-of-the-art approaches by substantial margin. Code and data will be released at https://github.com/ iSEE-Laboratory/Seg-ReSearch. Seg-ReSearch: Segmentation with Interleaved Reasoning and External Search 1. Introduction Localizing and segmenting the objects of interest in images or videos has been long-standing topic in AI community. Language, as the most natural interface, is commonly used as the reference for humans to indicate the target objects. With the recent advent of multi-modal large language models (MLLMs), this language reference is evolving from explicit descriptions to implicit reasoning instructions. However, existing research is still limited to understanding objects within the given visual context. In reality, we live in dynamic world, where user queries often involve up-to-date information or domain-specific concepts that exceed the frozen knowledge of MLLMs. For example, user may directly request to segment the new Tesla instead of the white vehicle on the left. While recent works, from LISA (Lai et al., 2024) to VideoSeg-R1 (Xu et al., 2026), have endowed segmentation models with reasoning capabilities, their potential is bounded by the static internal knowledge of MLLMs, as they lack the ability to continuously acquire necessary information from external sources. Although recent advances in text-centric domains have adopted reinforcement learning (RL) to empower LLMs with external search abilities, adapting such methods to multi-modal tasks remains challenging, primarily due to the dilemma in reward designs. Existing approaches either rely solely on ultimate outcome rewards (Li et al., 2025c; Zheng et al., 2025b; Hong et al., 2025) or impose step-wise process supervision (Zheng et al., 2025a; Liu et al., 2025d; Deng et al., 2025). However, neither approach effectively balances external search and visual reasoning. The former suffers from sparse signals, often leading the agent to bypass search steps and seek visual shortcuts. Conversely, the latter tends to over-prioritize the imitation of the search trajectory and thus neglect visual reasoning. In this work, we propose Seg-ReSearch, novel agentic Segmentation framework capable of interleaved Reasoning and external Search. As illustrated in Figure 1, SegReSearch performs task decomposition, reasoning, and interacting with search engines iteratively until it finds the target objects. This paradigm breaks the knowledge bottleneck of MLLMs, enabling segmentation systems to handle broader range of user queries. To effectively incentivize such capabilities, we introduce hierarchical reward mechanism that strikes balance between sparse outcome rewards and rigid step-wise imitation. Specifically, we decouple the process supervision into initial guidance and progressive incentives. For initial guidance, we utilize expert actions to supervise the first step, thereby providing reasonable starting point for the subsequent exploration. For intermediate steps, rather than enforcing trajectory imitation, we use format-based rewards to encourage diverse yet valid search steps, coupled with tapering bonus to prevent infinite search loops. This mechanism effectively bridges the gap between external search and visual reasoning, driving the evolution of the search behavior from initial exploration to efficient and accurate exploitation. To assess the effectiveness of Seg-ReSearch, we establish OK-VOS, novel language-instructed Video Object Segmentation benchmark that specifically requires Outside Knowledge. This benchmark is carefully annotated by human experts to ensure that each query contains up-to-date information or new concepts that lie beyond the internal knowledge of existing MLLMs. These queries may involve single or multi-hop searches, demanding complex reasoning across local visual contexts and external information. Our benchmark reveals that existing state-of-the-art (SOTA) reasoning segmentation models struggle significantly in such open-world scenarios. In contrast, our Seg-ReSearch demonstrates superior performance, even outperforming the baselines equipped with the same search tools by over 10 points. Furthermore, Seg-ReSearch also establishes new SOTA results on conventional reasoning segmentation benchmarks, including ReasonSeg (image) and ReasonVOS (video). Our main contributions are summarized as follows: Framework. We propose Seg-Research, novel trainable agentic segmentation framework capable of interleaved reasoning and external search. Reward Design. We introduce hierarchical reward mechanism to effectively address the dilemma between sparse outcome signals and rigid process supervision. Benchmark. We e"
[09.02.2026 08:42] Mistral response. {"id": "8ba735a53a1e43618de3eb435ef418dc", "created": 1770626570, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1358, "total_tokens": 1375, "completion_tokens": 17, "num_cached_tokens": 1357}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\"ISEE Lab, Sun Yat-sen University\"]\n```"}}]}
[09.02.2026 08:42] Response: ```python
["ISEE Lab, Sun Yat-sen University"]
```
[09.02.2026 08:42] Deleting PDF ./assets/pdf/2602.04454.pdf.
[09.02.2026 08:42] Success.
[09.02.2026 08:42] Downloading and parsing paper https://huggingface.co/papers/2602.06079.
[09.02.2026 08:42] Downloading paper 2602.06079 from https://arxiv.org/pdf/2602.06079v1...
[09.02.2026 08:42] Extracting affiliations from text.
[09.02.2026 08:42] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Canzona: Unified, Asynchronous, and Load-Balanced Framework for Distributed Matrix-based Optimizers Liangyu Wang * 1 Siqi Zhang * 2 Junjie Wang 3 Yiming Dong 2 Bo Zheng 2 Zihan Qiu 2 Shengkun Tang 4 Di Wang 1 Rui Men 2 Dayiheng Liu 2 Abstract The scaling of Large Language Models (LLMs) drives interest in matrix-based optimizers (e.g., Shampoo, Muon, SOAP) for their convergence efficiency; yet their requirement for holistic updates conflicts with the tensor fragmentation in distributed frameworks like Megatron. Existing solutions are suboptimal: synchronous approaches suffer from computational redundancy, while layer-wise partitioning fails to reconcile this conflict without violating the geometric constraints of efficient communication primitives. To bridge this gap, we propose Canzona, Unified, Asynchronous, and Load-Balanced framework that decouples logical optimizer assignment from physical parameter distribution. For Data Parallelism, we introduce an α-Balanced Static Partitioning strategy that respects atomicity while neutralizing the load imbalance. For Tensor Parallelism, we design an Asynchronous Compute pipeline utilizing Micro-Group Scheduling to batch fragmented updates and hide reconstruction overhead. Extensive evaluations on the Qwen3 model family (up to 32B parameters) on 256 GPUs demonstrate that our approach preserves the efficiency of established parallel architectures, achieving 1.57 speedup in end-to-end iteration time and reducing optimizer step latency by 5.8 compared to the baseline. 6 2 0 2 4 ] . [ 1 9 7 0 6 0 . 2 0 6 2 : r 1. Introduction The relentless scaling of Large Language Models (LLMs) (Qwen et al., 2025; Yang et al., 2025; Grattafiori et al., 2024; Guo et al., 2025; Kaplan et al., 2020; Hoffmann et al., 2022) has driven continuous search for more effi1KAUST 2Alibaba Group 3Peking University 4MBZUAI. Correspondence to: Dayiheng Liu <liudayiheng.ldyh@alibabainc.com>, Rui Men <menrui.mr@alibaba-inc.com>. Preprint. February 9, 2026. Work"
[09.02.2026 08:42] Response: ```python
['KAUST', 'Alibaba Group', 'Peking University', 'MBZUAI']
```
[09.02.2026 08:42] Deleting PDF ./assets/pdf/2602.06079.pdf.
[09.02.2026 08:42] Success.
[09.02.2026 08:42] Downloading and parsing paper https://huggingface.co/papers/2602.02581.
[09.02.2026 08:42] Downloading paper 2602.02581 from https://arxiv.org/pdf/2602.02581v1...
[09.02.2026 08:42] Extracting affiliations from text.
[09.02.2026 08:42] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"QuantLRM: Quantization of Large Reasoning Models via Fine-Tuning Signals Nan Zhang 1 Eugene Kwek 1 Yusen Zhang 1 Muyu Pan 1 Suhang Wang 1 Prasenjit Mitra 2 Rui Zhang "
[09.02.2026 08:43] Response: ```python
[
    "Pennsylvania State University",
    "IBM Research"
]
```
[09.02.2026 08:43] Deleting PDF ./assets/pdf/2602.02581.pdf.
[09.02.2026 08:43] Success.
[09.02.2026 08:43] Downloading and parsing paper https://huggingface.co/papers/2601.23039.
[09.02.2026 08:43] Extra JSON file exists (./assets/json/2601.23039.json), skip PDF parsing.
[09.02.2026 08:43] Paper image links file exists (./assets/img_data/2601.23039.json), skip HTML parsing.
[09.02.2026 08:43] Success.
[09.02.2026 08:43] Downloading and parsing paper https://huggingface.co/papers/2602.06883.
[09.02.2026 08:43] Extra JSON file exists (./assets/json/2602.06883.json), skip PDF parsing.
[09.02.2026 08:43] Paper image links file exists (./assets/img_data/2602.06883.json), skip HTML parsing.
[09.02.2026 08:43] Success.
[09.02.2026 08:43] Downloading and parsing paper https://huggingface.co/papers/2602.01064.
[09.02.2026 08:43] Extra JSON file exists (./assets/json/2602.01064.json), skip PDF parsing.
[09.02.2026 08:43] Paper image links file exists (./assets/img_data/2602.01064.json), skip HTML parsing.
[09.02.2026 08:43] Success.
[09.02.2026 08:43] Enriching papers with extra data.
[09.02.2026 08:43] ********************************************************************************
[09.02.2026 08:43] Abstract 0. Baichuan-M3 is a medical-enhanced large language model designed for clinical decision support with capabilities in proactive information gathering, long-horizon reasoning, and hallucination suppression.  					AI-generated summary 				 We introduce Baichuan-M3, a medical-enhanced large language model...
[09.02.2026 08:43] ********************************************************************************
[09.02.2026 08:43] Abstract 1. OdysseyArena presents a new framework for evaluating large language models on long-horizon, inductive agent tasks that emphasize autonomous discovery of environmental transition laws.  					AI-generated summary 				 The rapid advancement of Large Language Models (LLMs) has catalyzed the development ...
[09.02.2026 08:43] ********************************************************************************
[09.02.2026 08:43] Abstract 2. The paper establishes a theoretical framework for analyzing entropy dynamics in reinforcement fine-tuning of large language models, deriving expressions for entropy change and proposing entropy control methods based on discriminant analysis.  					AI-generated summary 				 Entropy serves as a critic...
[09.02.2026 08:43] ********************************************************************************
[09.02.2026 08:43] Abstract 3. Training instability in large language models is linked to weight matrix stable rank decline and Jacobian alignment, which MSign addresses through matrix sign operations to prevent gradient explosions.  					AI-generated summary 				 Training instability remains a critical challenge in large languag...
[09.02.2026 08:43] ********************************************************************************
[09.02.2026 08:43] Abstract 4. RLVR methods using group sampling suffer from bias toward likely trajectories and missed rare-correct ones; a difficulty-aware advantage scaling technique improves performance on benchmarks without increasing computational cost.  					AI-generated summary 				 Reinforcement Learning with Verifiable ...
[09.02.2026 08:43] ********************************************************************************
[09.02.2026 08:43] Abstract 5. Consequence-Based Utility evaluates mathematical solutions by testing their effectiveness as exemplars for related problems, outperforming reward models and LLM judges in ranking quality and correct-wrong separation.  					AI-generated summary 				 Recent progress in reasoning models suggests that g...
[09.02.2026 08:43] ********************************************************************************
[09.02.2026 08:43] Abstract 6. TRIT framework improves multilingual reasoning by jointly training translation and reasoning components, enhancing question understanding and response generation across languages.  					AI-generated summary 				 Long reasoning models often struggle in multilingual settings: they tend to reason in En...
[09.02.2026 08:43] ********************************************************************************
[09.02.2026 08:43] Abstract 7. GUI agents for automated digital tasks rely on vision-language models with enhanced grounding capabilities, achieved through refined data engineering, improved training strategies, and reinforcement learning with verifiable rewards.  					AI-generated summary 				 The rapid advancement of vision-lan...
[09.02.2026 08:43] ********************************************************************************
[09.02.2026 08:43] Abstract 8. A novel reinforcement learning approach called ARM addresses entropy collapse in LLM reasoning by equilibrating confidence levels across correct responses through dynamic reward shaping.  					AI-generated summary 				 Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as an indispens...
[09.02.2026 08:43] ********************************************************************************
[09.02.2026 08:43] Abstract 9. A comprehensive memory-focused benchmark for mobile GUI agents reveals significant memory capability gaps and provides systematic evaluation methods and design insights.  					AI-generated summary 				 Current mobile GUI agent benchmarks systematically fail to assess memory capabilities, with only 5...
[09.02.2026 08:43] ********************************************************************************
[09.02.2026 08:43] Abstract 10. DreamDojo is a foundation world model trained on 44k hours of egocentric human videos that enables efficient simulation of dexterous robotic tasks through continuous latent actions and real-time distillation.  					AI-generated summary 				 Being able to simulate the outcomes of actions in varied en...
[09.02.2026 08:43] ********************************************************************************
[09.02.2026 08:43] Abstract 11. InftyThink+ uses reinforcement learning to optimize iterative reasoning processes, improving accuracy and efficiency in large language models.  					AI-generated summary 				 Large reasoning models achieve strong performance by scaling inference-time chain-of-thought, but this paradigm suffers from ...
[09.02.2026 08:43] ********************************************************************************
[09.02.2026 08:43] Abstract 12. PlanViz benchmark evaluates unified multimodal models' capabilities in computer-use planning tasks through route planning, work diagramming, and web&UI displaying sub-tasks with a task-adaptive scoring system.  					AI-generated summary 				 Unified multimodal models (UMMs) have shown impressive cap...
[09.02.2026 08:43] ********************************************************************************
[09.02.2026 08:43] Abstract 13. Residual binarization framework RaBiT addresses feature co-adaptation in quantized LLMs through hierarchical path derivation and robust initialization, achieving superior accuracy-efficiency trade-offs.  					AI-generated summary 				 Efficient deployment of large language models (LLMs) requires ext...
[09.02.2026 08:43] ********************************************************************************
[09.02.2026 08:43] Abstract 14. OmniMoE presents a system-algorithm co-designed framework that achieves fine-grained expert specialization in Mixture-of-Experts architectures through vector-level atomic experts and optimized routing and scheduling mechanisms.  					AI-generated summary 				 Mixture-of-Experts (MoE) architectures a...
[09.02.2026 08:43] ********************************************************************************
[09.02.2026 08:43] Abstract 15. Sparse Autoencoders trained on Whisper and HuBERT models demonstrate stable feature extraction and effective disentanglement of acoustic and semantic information, showing practical applications in audio processing and correlation with human neural activity.  					AI-generated summary 				 Sparse Aut...
[09.02.2026 08:43] ********************************************************************************
[09.02.2026 08:43] Abstract 16. A novel framework called SEMA is introduced that effectively trains multi-turn attackers for large language models without relying on existing strategies or external data, achieving state-of-the-art attack success rates while being compact, reproducible, and transferable across different models and ...
[09.02.2026 08:43] ********************************************************************************
[09.02.2026 08:43] Abstract 17. Group-Evolving Agents enable open-ended self-improvement by treating groups of agents as evolutionary units, allowing efficient experience sharing and reuse to enhance coding performance and robustness.  					AI-generated summary 				 Open-ended self-improving agents can autonomously modify their ow...
[09.02.2026 08:43] ********************************************************************************
[09.02.2026 08:43] Abstract 18. SeeUPO is a critic-free reinforcement learning method that ensures convergence guarantees in multi-turn agent interactions by modeling sequential decision-making as multi-agent bandit problems and using backward induction for policy updates.  					AI-generated summary 				 Reinforcement learning (RL...
[09.02.2026 08:43] ********************************************************************************
[09.02.2026 08:43] Abstract 19. Replacing conventional feed-forward networks with hourglass-shaped MLPs in Transformers improves model efficiency and performance by enabling better parameter utilization and competitive scaling.  					AI-generated summary 				 Dense Transformer language models have largely adhered to one consistent...
[09.02.2026 08:43] ********************************************************************************
[09.02.2026 08:43] Abstract 20. Multi-modal large language models struggle to jointly understand audio and visual signals in egocentric videos, but a new scalable data engine and dataset significantly improve their performance through targeted fine-tuning.  					AI-generated summary 				 Understanding egocentric videos plays a vit...
[09.02.2026 08:43] ********************************************************************************
[09.02.2026 08:43] Abstract 21. A diffusion-transformer framework integrates spatio-temporal urban data to predict building-level climate risks while incorporating transportation network structures for emergency response applications.  					AI-generated summary 				 Climate hazards increasingly disrupt urban transportation and eme...
[09.02.2026 08:43] ********************************************************************************
[09.02.2026 08:43] Abstract 22. Seg-ReSearch introduces a novel segmentation approach that combines interleaved reasoning with external search to overcome limitations of frozen MLLM knowledge, using hierarchical reward design for training and demonstrating superior performance on video object segmentation benchmarks.  					AI-gene...
[09.02.2026 08:43] ********************************************************************************
[09.02.2026 08:43] Abstract 23. Canzona presents a unified asynchronous framework that addresses the conflict between matrix-based optimizers and distributed tensor fragmentation in LLM training, improving efficiency and reducing latency.  					AI-generated summary 				 The scaling of Large Language Models (LLMs) drives interest i...
[09.02.2026 08:43] ********************************************************************************
[09.02.2026 08:43] Abstract 24. QuantLRM uses weight update magnitude signals from fine-tuning to improve quantization of Large Reasoning Models, achieving better performance than traditional methods through channel importance estimation.  					AI-generated summary 				 Weight-only quantization is important for compressing Large L...
[09.02.2026 08:43] ********************************************************************************
[09.02.2026 08:43] Abstract 25. Researchers identify and address premature mode collapse in optimal transport-based structural prediction models through an adaptive stability control algorithm that prevents gradient explosions during large-scale training.  					AI-generated summary 				 Differentiable matching layers and residual ...
[09.02.2026 08:43] ********************************************************************************
[09.02.2026 08:43] Abstract 26. Vision transformer components exhibit varying plasticity levels that correlate with finetuning performance, challenging the assumption that smoothness is always beneficial.  					AI-generated summary 				 The smoothness of the transformer architecture has been extensively studied in the context of g...
[09.02.2026 08:43] ********************************************************************************
[09.02.2026 08:43] Abstract 27. Knowledge purification techniques consolidate rationales from multiple teacher LLMs to reduce conflicts and improve efficiency in distillation processes.  					AI-generated summary 				 Knowledge distillation has emerged as a pivotal technique for transferring knowledge from stronger large language ...
[09.02.2026 08:43] Read previous papers.
[09.02.2026 08:43] Generating reviews via LLM API.
[09.02.2026 08:43] Using data from previous issue: {"categories": ["#open_source", "#science", "#hallucinations", "#reasoning"], "emoji": "⚕️", "ru": {"title": "От пассивных ответов к активной врачебной поддержке", "desc": "Baichuan-M3 — это медицинская LLM, которая переводит парадигму взаимодействия с пациентами от пассивного ответа на вопросы к ак
[09.02.2026 08:43] Using data from previous issue: {"categories": ["#agents", "#benchmark", "#reasoning", "#long_context", "#open_source", "#dataset"], "emoji": "🧭", "ru": {"title": "Оценка автономного открытия законов природы в долгосрочных взаимодействиях агентов", "desc": "В работе представлен OdysseyArena — новый бенчмарк для оценки больших язык
[09.02.2026 08:43] Using data from previous issue: {"categories": ["#rlhf", "#math", "#training", "#rl"], "emoji": "⚖️", "ru": {"title": "Теория энтропии в усиленном обучении языковых моделей", "desc": "В статье разработан теоретический фреймворк для анализа динамики энтропии при усиленном обучении больших языковых моделей. Авторы выводят выражения 
[09.02.2026 08:43] Using data from previous issue: {"categories": ["#training", "#architecture", "#optimization"], "emoji": "⚡", "ru": {"title": "MSign: стабилизация обучения больших моделей через восстановление ранга матриц", "desc": "В статье исследуется проблема нестабильности обучения больших языковых моделей, которая проявляется в виде взрывног
[09.02.2026 08:43] Using data from previous issue: {"categories": [], "emoji": "🎯", "ru": {"title": "Справедливое обучение: как найти редкие правильные решения", "desc": "В статье рассматривается проблема смещения в методах обучения с подкреплением и проверяемыми вознаграждениями (RLVR), которые используют групповую выборку для оценки преимуществ. А
[09.02.2026 08:43] Using data from previous issue: {"categories": ["#benchmark", "#reasoning", "#science", "#math", "#dataset"], "emoji": "🧮", "ru": {"title": "Оценка математических решений через их практическую полезность на соседних задачах", "desc": "В статье предлагается метод Consequence-Based Utility для оценки качества математических решений 
[09.02.2026 08:43] Using data from previous issue: {"categories": ["#machine_translation", "#transfer_learning", "#multilingual", "#training", "#reasoning"], "emoji": "🌍", "ru": {"title": "Объединение перевода и рассуждений для улучшения многоязычного понимания", "desc": "В статье представлен фреймворк TRIT, который улучшает многоязычное рассуждение
[09.02.2026 08:43] Using data from previous issue: {"categories": ["#multimodal", "#rl", "#benchmark", "#agents", "#training", "#cv", "#data"], "emoji": "🖱️", "ru": {"title": "Точное определение элементов GUI через инженерию данных и обучение с подкреплением", "desc": "В статье представлена модель POINTS-GUI-G-8B для автоматизации задач с графически
[09.02.2026 08:43] Using data from previous issue: {"categories": ["#training", "#reasoning", "#optimization", "#rl"], "emoji": "⚖️", "ru": {"title": "Уравновешивание уверенности для разнообразного рассуждения в больших языковых моделях", "desc": "В статье предложен новый метод ARM (Advantage Re-weighting Mechanism) для решения проблемы коллапса энт
[09.02.2026 08:43] Using data from previous issue: {"categories": [], "emoji": "🧠", "ru": {"title": "Память как критический недостаток мобильных GUI-агентов: комплексная оценка и решения", "desc": "В работе представлена MemGUI-Bench — комплексный бенчмарк для оценки способности мобильных GUI-агентов к запоминанию информации и обучению на основе пред
[09.02.2026 08:43] Using data from previous issue: {"categories": ["#inference", "#open_source", "#synthetic", "#transfer_learning", "#video", "#training", "#robotics", "#dataset"], "emoji": "🤖", "ru": {"title": "Универсальная модель мира для ловкого робоуправления из видео человека", "desc": "DreamDojo — это фундаментальная модель мира, обученная н
[09.02.2026 08:43] Using data from previous issue: {"categories": ["#small_models", "#optimization", "#rl", "#training", "#reasoning", "#long_context"], "emoji": "🧠", "ru": {"title": "Умное резюмирование мыслей через обучение с подкреплением", "desc": "В статье предложена система InftyThink+, которая использует обучение с подкреплением для оптимизац
[09.02.2026 08:43] Using data from previous issue: {"categories": ["#survey", "#multimodal", "#benchmark", "#cv", "#dataset"], "emoji": "📋", "ru": {"title": "Оценка мультимодальных моделей в задачах планирования с использованием компьютера", "desc": "Исследование представляет бенчмарк PlanViz для оценки способностей объединённых мультимодальных моде
[09.02.2026 08:43] Using data from previous issue: {"categories": ["#inference", "#architecture", "#training", "#optimization"], "emoji": "⚡", "ru": {"title": "Иерархическая бинаризация для эффективного инференса LLM", "desc": "RaBiT — это новая структура квантизации для больших языковых моделей, которая решает проблему совместной адаптации признако
[09.02.2026 08:43] Using data from previous issue: {"categories": ["#inference", "#training", "#architecture"], "emoji": "⚡", "ru": {"title": "Атомарные эксперты: максимальная специализация при сохранении скорости", "desc": "OmniMoE представляет систему, которая достигает максимальной специализации экспертов в архитектуре Mixture-of-Experts благодар
[09.02.2026 08:43] Querying the API.
[09.02.2026 08:43] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Sparse Autoencoders trained on Whisper and HuBERT models demonstrate stable feature extraction and effective disentanglement of acoustic and semantic information, showing practical applications in audio processing and correlation with human neural activity.  					AI-generated summary 				 Sparse Autoencoders (SAEs) are powerful tools for interpreting neural representations, yet their use in audio remains underexplored. We train SAEs across all encoder layers of Whisper and HuBERT, provide an extensive evaluation of their stability, interpretability, and show their practical utility. Over 50% of the features remain consistent across random seeds, and reconstruction quality is preserved. SAE features capture general acoustic and semantic information as well as specific events, including environmental noises and paralinguistic sounds (e.g. laughter, whispering) and disentangle them effectively, requiring removal of only 19-27% of features to erase a concept. Feature steering reduces Whisper's false speech detections by 70% with negligible WER increase, demonstrating real-world applicability. Finally, we find SAE features correlated with human EEG activity during speech perception, indicating alignment with human neural processing. The code and checkpoints are available at https://github.com/audiosae/audiosae_demo.
[09.02.2026 08:43] Response: ```json
{
  "desc": "В статье исследуются разреженные автокодировщики (SAE) для интерпретации нейронных представлений в аудиомоделях Whisper и HuBERT. Авторы показывают, что более 50% изученных признаков остаются стабильными при переобучении, и SAE эффективно разделяют акустическую и семантическую информацию. Демонстрируется практическое применение метода для улучшения качества распознавания речи путём управления признаками, что снижает ложные срабатывания на 70%. Обнаружена корреляция между признаками SAE и активностью человеческого мозга во время восприятия речи, что подтверждает соответствие модели биологическим процессам.",
  "emoji": "👂",
  "title": "Разреженные автокодировщики раскрывают структуру звука"
}
```
[09.02.2026 08:43] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Sparse Autoencoders trained on Whisper and HuBERT models demonstrate stable feature extraction and effective disentanglement of acoustic and semantic information, showing practical applications in audio processing and correlation with human neural activity.  					AI-generated summary 				 Sparse Autoencoders (SAEs) are powerful tools for interpreting neural representations, yet their use in audio remains underexplored. We train SAEs across all encoder layers of Whisper and HuBERT, provide an extensive evaluation of their stability, interpretability, and show their practical utility. Over 50% of the features remain consistent across random seeds, and reconstruction quality is preserved. SAE features capture general acoustic and semantic information as well as specific events, including environmental noises and paralinguistic sounds (e.g. laughter, whispering) and disentangle them effectively, requiring removal of only 19-27% of features to erase a concept. Feature steering reduces Whisper's false speech detections by 70% with negligible WER increase, demonstrating real-world applicability. Finally, we find SAE features correlated with human EEG activity during speech perception, indicating alignment with human neural processing. The code and checkpoints are available at https://github.com/audiosae/audiosae_demo."

[09.02.2026 08:43] Response: ```python
["AUDIO", "INTERPRETABILITY", "TRAINING"]
```

Wait, let me reconsider. "INTERPRETABILITY" is not in the provided topic list. Let me classify based only on the topics provided:

```python
["AUDIO"]
```

The paper focuses on Sparse Autoencoders applied to audio models (Whisper and HuBERT), with emphasis on audio processing, feature extraction, and audio-related applications. While it involves training methods and model interpretation, the primary focus is on audio processing and generation, making AUDIO the most directly relevant topic from the provided list.
[09.02.2026 08:43] Error. Failed to parse JSON from LLM. ["AUDIO", "INTERPRETABILITY", "TRAINING"]


Wait, let me reconsider. "INTERPRETABILITY" is not in the provided topic list. Let me classify based only on the topics provided:


["AUDIO"]


The paper focuses on Sparse Autoencoders applied to audio models (Whisper and HuBERT), with emphasis on audio processing, feature extraction, and audio-related applications. While it involves training methods and model interpretation, the primary focus is on audio processing and generation, making AUDIO the most directly relevant topic from the provided list.
[09.02.2026 08:43] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Sparse Autoencoders trained on Whisper and HuBERT models demonstrate stable feature extraction and effective disentanglement of acoustic and semantic information, showing practical applications in audio processing and correlation with human neural activity.  					AI-generated summary 				 Sparse Autoencoders (SAEs) are powerful tools for interpreting neural representations, yet their use in audio remains underexplored. We train SAEs across all encoder layers of Whisper and HuBERT, provide an extensive evaluation of their stability, interpretability, and show their practical utility. Over 50% of the features remain consistent across random seeds, and reconstruction quality is preserved. SAE features capture general acoustic and semantic information as well as specific events, including environmental noises and paralinguistic sounds (e.g. laughter, whispering) and disentangle them effectively, requiring removal of only 19-27% of features to erase a concept. Feature steering reduces Whisper's false speech detections by 70% with negligible WER increase, demonstrating real-world applicability. Finally, we find SAE features correlated with human EEG activity during speech perception, indicating alignment with human neural processing. The code and checkpoints are available at https://github.com/audiosae/audiosae_demo."

[09.02.2026 08:43] Response: ```python
["INTERPRETABILITY", "OPEN_SOURCE"]
```
[09.02.2026 08:43] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper explores the use of Sparse Autoencoders (SAEs) for extracting features from audio data, specifically using the Whisper and HuBERT models. The authors demonstrate that SAEs can effectively disentangle acoustic and semantic information, maintaining stability and interpretability across different training conditions. They show that a significant portion of features remains consistent, allowing for the identification of specific audio events while minimizing the loss of important information. Additionally, the study reveals a strong correlation between SAE features and human neural activity, suggesting that these models align well with how humans process speech.","title":"Unlocking Audio Insights with Sparse Autoencoders"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper explores the use of Sparse Autoencoders (SAEs) for extracting features from audio data, specifically using the Whisper and HuBERT models. The authors demonstrate that SAEs can effectively disentangle acoustic and semantic information, maintaining stability and interpretability across different training conditions. They show that a significant portion of features remains consistent, allowing for the identification of specific audio events while minimizing the loss of important information. Additionally, the study reveals a strong correlation between SAE features and human neural activity, suggesting that these models align well with how humans process speech.', title='Unlocking Audio Insights with Sparse Autoencoders'))
[09.02.2026 08:43] Response: ParsedChatCompletionMessage[Article](content='{"desc":"稀疏自编码器（SAE）是一种强大的工具，用于解释神经表示，但在音频领域的应用仍然较少。我们在Whisper和HuBERT的所有编码器层上训练SAE，并对其稳定性和可解释性进行了广泛评估。结果显示，超过50%的特征在随机种子之间保持一致，重建质量得以保持。SAE特征有效地分离了声学和语义信息，并与人类在语音感知过程中的脑电活动相关联，展示了其在音频处理中的实际应用。","title":"稀疏自编码器：音频处理的新工具"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='稀疏自编码器（SAE）是一种强大的工具，用于解释神经表示，但在音频领域的应用仍然较少。我们在Whisper和HuBERT的所有编码器层上训练SAE，并对其稳定性和可解释性进行了广泛评估。结果显示，超过50%的特征在随机种子之间保持一致，重建质量得以保持。SAE特征有效地分离了声学和语义信息，并与人类在语音感知过程中的脑电活动相关联，展示了其在音频处理中的实际应用。', title='稀疏自编码器：音频处理的新工具'))
[09.02.2026 08:43] Using data from previous issue: {"categories": ["#training", "#rl", "#alignment", "#security", "#open_source"], "emoji": "🎯", "ru": {"title": "Многоходовые атаки на LLM через обучение с подкреплением без внешних данных", "desc": "Представлена фреймворк SEMA для обучения многоходовых атак на большие языковые модели без использовани
[09.02.2026 08:43] Querying the API.
[09.02.2026 08:43] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Group-Evolving Agents enable open-ended self-improvement by treating groups of agents as evolutionary units, allowing efficient experience sharing and reuse to enhance coding performance and robustness.  					AI-generated summary 				 Open-ended self-improving agents can autonomously modify their own structural designs to advance their capabilities and overcome the limits of pre-defined architectures, thus reducing reliance on human intervention. We introduce Group-Evolving Agents (GEA), a new paradigm for open-ended self-improvements, which treats a group of agents as the fundamental evolutionary unit, enabling explicit experience sharing and reuse within the group throughout evolution. Unlike existing open-ended self-evolving paradigms that adopt tree-structured evolution, GEA overcomes the limitation of inefficient utilization of exploratory diversity caused by isolated evolutionary branches. We evaluate GEA on challenging coding benchmarks, where it significantly outperforms state-of-the-art self-evolving methods (71.0% vs. 56.7% on SWE-bench Verified, 88.3% vs. 68.3% on Polyglot) and matches or exceeds top human-designed agent frameworks (71.8% and 52.0% on two benchmarks, respectively). Analysis reveals that GEA more effectively converts early-stage exploratory diversity into sustained, long-term progress, achieving stronger performance under the same number of evolved agents. Furthermore, GEA exhibits consistent transferability across different coding models and greater robustness, fixing framework-level bugs in 1.4 iterations on average, versus 5 for self-evolving methods.
[09.02.2026 08:43] Response: ```json
{
  "desc": "В работе предложена новая парадигма Group-Evolving Agents (GEA) для открытого самосовершенствования агентов, которая рассматривает группу агентов как единую эволюционную единицу. В отличие от древовидной эволюции, GEA обеспечивает эффективный обмен опытом внутри группы, что позволяет лучше использовать исследовательское разнообразие и избежать изоляции эволюционных ветвей. Метод продемонстрировал значительное превосходство над существующими методами самоэволюции на бенчмарках кодирования (71.0% на SWE-bench Verified против 56.7% у предыдущих методов). GEA также показывает хорошую трансферабельность между моделями и высокую устойчивость к ошибкам, эффективнее конвертируя раннюю исследовательскую активность в долгосрочный прогресс.",
  "emoji": "🧬",
  "title": "Групповая эволюция агентов для эффективного самосовершенствования"
}
```
[09.02.2026 08:43] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Group-Evolving Agents enable open-ended self-improvement by treating groups of agents as evolutionary units, allowing efficient experience sharing and reuse to enhance coding performance and robustness.  					AI-generated summary 				 Open-ended self-improving agents can autonomously modify their own structural designs to advance their capabilities and overcome the limits of pre-defined architectures, thus reducing reliance on human intervention. We introduce Group-Evolving Agents (GEA), a new paradigm for open-ended self-improvements, which treats a group of agents as the fundamental evolutionary unit, enabling explicit experience sharing and reuse within the group throughout evolution. Unlike existing open-ended self-evolving paradigms that adopt tree-structured evolution, GEA overcomes the limitation of inefficient utilization of exploratory diversity caused by isolated evolutionary branches. We evaluate GEA on challenging coding benchmarks, where it significantly outperforms state-of-the-art self-evolving methods (71.0% vs. 56.7% on SWE-bench Verified, 88.3% vs. 68.3% on Polyglot) and matches or exceeds top human-designed agent frameworks (71.8% and 52.0% on two benchmarks, respectively). Analysis reveals that GEA more effectively converts early-stage exploratory diversity into sustained, long-term progress, achieving stronger performance under the same number of evolved agents. Furthermore, GEA exhibits consistent transferability across different coding models and greater robustness, fixing framework-level bugs in 1.4 iterations on average, versus 5 for self-evolving methods."

[09.02.2026 08:43] Response: ```python
["AGENTS", "PLP", "BENCHMARK", "TRAINING"]
```
[09.02.2026 08:43] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Group-Evolving Agents enable open-ended self-improvement by treating groups of agents as evolutionary units, allowing efficient experience sharing and reuse to enhance coding performance and robustness.  					AI-generated summary 				 Open-ended self-improving agents can autonomously modify their own structural designs to advance their capabilities and overcome the limits of pre-defined architectures, thus reducing reliance on human intervention. We introduce Group-Evolving Agents (GEA), a new paradigm for open-ended self-improvements, which treats a group of agents as the fundamental evolutionary unit, enabling explicit experience sharing and reuse within the group throughout evolution. Unlike existing open-ended self-evolving paradigms that adopt tree-structured evolution, GEA overcomes the limitation of inefficient utilization of exploratory diversity caused by isolated evolutionary branches. We evaluate GEA on challenging coding benchmarks, where it significantly outperforms state-of-the-art self-evolving methods (71.0% vs. 56.7% on SWE-bench Verified, 88.3% vs. 68.3% on Polyglot) and matches or exceeds top human-designed agent frameworks (71.8% and 52.0% on two benchmarks, respectively). Analysis reveals that GEA more effectively converts early-stage exploratory diversity into sustained, long-term progress, achieving stronger performance under the same number of evolved agents. Furthermore, GEA exhibits consistent transferability across different coding models and greater robustness, fixing framework-level bugs in 1.4 iterations on average, versus 5 for self-evolving methods."

[09.02.2026 08:43] Response: ```python
["OPTIMIZATION", "TRANSFER_LEARNING"]
```

**Justification:**

1. **OPTIMIZATION**: The paper focuses on improving training and evolution methods for agents through group-based evolutionary strategies, which is fundamentally an optimization approach to enhance agent performance and capabilities.

2. **TRANSFER_LEARNING**: The paper explicitly mentions "consistent transferability across different coding models," demonstrating knowledge transfer between different models, which is a core aspect of transfer learning.
[09.02.2026 08:43] Error. Failed to parse JSON from LLM. ["OPTIMIZATION", "TRANSFER_LEARNING"]


**Justification:**

1. **OPTIMIZATION**: The paper focuses on improving training and evolution methods for agents through group-based evolutionary strategies, which is fundamentally an optimization approach to enhance agent performance and capabilities.

2. **TRANSFER_LEARNING**: The paper explicitly mentions "consistent transferability across different coding models," demonstrating knowledge transfer between different models, which is a core aspect of transfer learning.
[09.02.2026 08:43] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Group-Evolving Agents (GEA) represent a novel approach in machine learning that allows groups of agents to evolve together, enhancing their ability to share and reuse experiences. This method enables agents to autonomously adapt their structures, leading to improved performance without heavy reliance on human input. GEA outperforms traditional self-evolving methods by effectively utilizing exploratory diversity, resulting in better coding capabilities and robustness. The framework demonstrates significant advancements in coding benchmarks, showcasing its ability to achieve long-term progress and adaptability across various models.","title":"Empowering Agents Through Group Evolution"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Group-Evolving Agents (GEA) represent a novel approach in machine learning that allows groups of agents to evolve together, enhancing their ability to share and reuse experiences. This method enables agents to autonomously adapt their structures, leading to improved performance without heavy reliance on human input. GEA outperforms traditional self-evolving methods by effectively utilizing exploratory diversity, resulting in better coding capabilities and robustness. The framework demonstrates significant advancements in coding benchmarks, showcasing its ability to achieve long-term progress and adaptability across various models.', title='Empowering Agents Through Group Evolution'))
[09.02.2026 08:43] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本文介绍了一种新的开放式自我改进代理模型，称为群体进化代理（GEA）。GEA将代理组视为基本的进化单元，允许在进化过程中实现经验的显式共享和重用，从而提高编码性能和鲁棒性。与现有的树状进化方法不同，GEA克服了由于孤立进化分支导致的探索多样性利用效率低下的问题。实验结果表明，GEA在多个编码基准测试中显著超越了最先进的自我进化方法，并在不同编码模型中表现出一致的可迁移性和更强的鲁棒性。","title":"群体进化代理：开启自我改进的新纪元"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本文介绍了一种新的开放式自我改进代理模型，称为群体进化代理（GEA）。GEA将代理组视为基本的进化单元，允许在进化过程中实现经验的显式共享和重用，从而提高编码性能和鲁棒性。与现有的树状进化方法不同，GEA克服了由于孤立进化分支导致的探索多样性利用效率低下的问题。实验结果表明，GEA在多个编码基准测试中显著超越了最先进的自我进化方法，并在不同编码模型中表现出一致的可迁移性和更强的鲁棒性。', title='群体进化代理：开启自我改进的新纪元'))
[09.02.2026 08:43] Using data from previous issue: {"categories": ["#agents", "#training", "#rl", "#alignment", "#optimization", "#reasoning"], "emoji": "🎯", "ru": {"title": "Гарантированная сходимость обучения с подкреплением без критика через обратную индукцию", "desc": "SeeUPO — это метод обучения с подкреплением без критика, который гарантирует 
[09.02.2026 08:43] Using data from previous issue: {"categories": ["#optimization"], "emoji": "⏳", "ru": {"title": "Песочные часы вместо традиции: переосмысление архитектуры Трансформеров", "desc": "Исследование предлагает заменить традиционные полносвязные сети (FFN) в Трансформерах на более глубокие сети в форме песочных часов, которые лучше испол
[09.02.2026 08:43] Using data from previous issue: {"categories": ["#audio", "#multimodal", "#benchmark", "#video", "#training", "#robotics", "#dataset"], "emoji": "👁️‍🗨️", "ru": {"title": "Гармоничное слияние звука и видения в эгоцентрических сценах", "desc": "Работа исследует проблему совместного понимания аудио и визуальной информации в эгоцентри
[09.02.2026 08:43] Querying the API.
[09.02.2026 08:43] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A diffusion-transformer framework integrates spatio-temporal urban data to predict building-level climate risks while incorporating transportation network structures for emergency response applications.  					AI-generated summary 				 Climate hazards increasingly disrupt urban transportation and emergency-response operations by damaging housing stock, degrading infrastructure, and reducing network accessibility. This paper presents Skjold-DiT, a diffusion-transformer framework that integrates heterogeneous spatio-temporal urban data to forecast building-level climate-risk indicators while explicitly incorporating transportation-network structure and accessibility signals relevant to intelligent vehicles (e.g., emergency reachability and evacuation-route constraints). Concretely, Skjold-DiT enables hazard-conditioned routing constraints by producing calibrated, uncertainty-aware accessibility layers (reachability, travel-time inflation, and route redundancy) that can be consumed by intelligent-vehicle routing and emergency dispatch systems. Skjold-DiT combines: (1) Fjell-Prompt, a prompt-based conditioning interface designed to support cross-city transfer; (2) Norrland-Fusion, a cross-modal attention mechanism unifying hazard maps/imagery, building attributes, demographics, and transportation infrastructure into a shared latent representation; and (3) Valkyrie-Forecast, a counterfactual simulator for generating probabilistic risk trajectories under intervention prompts. We introduce the Baltic-Caspian Urban Resilience (BCUR) dataset with 847,392 building-level observations across six cities, including multi-hazard annotations (e.g., flood and heat indicators) and transportation accessibility features. Experiments evaluate prediction quality, cross-city generalization, calibration, and downstream transportation-relevant outcomes, including reachability and hazard-conditioned travel times under counterfactual interventions.
[09.02.2026 08:43] Response: ```json
{
  "desc": "В работе представлена Skjold-DiT — фреймворк на основе диффузионной модели и трансформера для предсказания климатических рисков на уровне зданий в городах. Модель интегрирует разнородные пространственно-временные данные, включая информацию о транспортной сети, и генерирует калибровки карты доступности с оценками неопределённости для систем экстренного реагирования. Ключевыми компонентами являются механизм условного промптирования для трансфера между городами, кросс-модальный механизм внимания для объединения опасностей и инфраструктуры в общее скрытое представление, и контрфактический симулятор для генерации вероятностных траекторий рисков. Работа включает новый датасет BCUR с информацией о 847 тысячах зданий в шести городах с аннотациями множественных опасностей и характеристиками транспортной доступности.",
  "emoji": "🏢",
  "title": "Прогнозирование климатических рисков и маршрутов эвакуации в городах с диффузионным трансформером"
}
```
[09.02.2026 08:43] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A diffusion-transformer framework integrates spatio-temporal urban data to predict building-level climate risks while incorporating transportation network structures for emergency response applications.  					AI-generated summary 				 Climate hazards increasingly disrupt urban transportation and emergency-response operations by damaging housing stock, degrading infrastructure, and reducing network accessibility. This paper presents Skjold-DiT, a diffusion-transformer framework that integrates heterogeneous spatio-temporal urban data to forecast building-level climate-risk indicators while explicitly incorporating transportation-network structure and accessibility signals relevant to intelligent vehicles (e.g., emergency reachability and evacuation-route constraints). Concretely, Skjold-DiT enables hazard-conditioned routing constraints by producing calibrated, uncertainty-aware accessibility layers (reachability, travel-time inflation, and route redundancy) that can be consumed by intelligent-vehicle routing and emergency dispatch systems. Skjold-DiT combines: (1) Fjell-Prompt, a prompt-based conditioning interface designed to support cross-city transfer; (2) Norrland-Fusion, a cross-modal attention mechanism unifying hazard maps/imagery, building attributes, demographics, and transportation infrastructure into a shared latent representation; and (3) Valkyrie-Forecast, a counterfactual simulator for generating probabilistic risk trajectories under intervention prompts. We introduce the Baltic-Caspian Urban Resilience (BCUR) dataset with 847,392 building-level observations across six cities, including multi-hazard annotations (e.g., flood and heat indicators) and transportation accessibility features. Experiments evaluate prediction quality, cross-city generalization, calibration, and downstream transportation-relevant outcomes, including reachability and hazard-conditioned travel times under counterfactual interventions."

[09.02.2026 08:43] Response: ```python
['DATASET', 'MULTIMODAL', 'ARCHITECTURE', 'BENCHMARK']
```
[09.02.2026 08:43] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A diffusion-transformer framework integrates spatio-temporal urban data to predict building-level climate risks while incorporating transportation network structures for emergency response applications.  					AI-generated summary 				 Climate hazards increasingly disrupt urban transportation and emergency-response operations by damaging housing stock, degrading infrastructure, and reducing network accessibility. This paper presents Skjold-DiT, a diffusion-transformer framework that integrates heterogeneous spatio-temporal urban data to forecast building-level climate-risk indicators while explicitly incorporating transportation-network structure and accessibility signals relevant to intelligent vehicles (e.g., emergency reachability and evacuation-route constraints). Concretely, Skjold-DiT enables hazard-conditioned routing constraints by producing calibrated, uncertainty-aware accessibility layers (reachability, travel-time inflation, and route redundancy) that can be consumed by intelligent-vehicle routing and emergency dispatch systems. Skjold-DiT combines: (1) Fjell-Prompt, a prompt-based conditioning interface designed to support cross-city transfer; (2) Norrland-Fusion, a cross-modal attention mechanism unifying hazard maps/imagery, building attributes, demographics, and transportation infrastructure into a shared latent representation; and (3) Valkyrie-Forecast, a counterfactual simulator for generating probabilistic risk trajectories under intervention prompts. We introduce the Baltic-Caspian Urban Resilience (BCUR) dataset with 847,392 building-level observations across six cities, including multi-hazard annotations (e.g., flood and heat indicators) and transportation accessibility features. Experiments evaluate prediction quality, cross-city generalization, calibration, and downstream transportation-relevant outcomes, including reachability and hazard-conditioned travel times under counterfactual interventions."

[09.02.2026 08:43] Response: ```python
["DIFFUSION", "GRAPHS", "TRANSFER_LEARNING", "SCIENCE", "OPEN_SOURCE"]
```
[09.02.2026 08:43] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces Skjold-DiT, a diffusion-transformer framework designed to predict climate risks at the building level by utilizing diverse urban data. It integrates transportation network structures to enhance emergency response capabilities, focusing on factors like accessibility and evacuation routes. The framework employs advanced techniques such as prompt-based conditioning and cross-modal attention to create a unified representation of various urban features. Additionally, it includes a counterfactual simulator to assess risk trajectories, supported by a comprehensive dataset of building observations across multiple cities.","title":"Predicting Climate Risks with Smart Urban Data Integration"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper introduces Skjold-DiT, a diffusion-transformer framework designed to predict climate risks at the building level by utilizing diverse urban data. It integrates transportation network structures to enhance emergency response capabilities, focusing on factors like accessibility and evacuation routes. The framework employs advanced techniques such as prompt-based conditioning and cross-modal attention to create a unified representation of various urban features. Additionally, it includes a counterfactual simulator to assess risk trajectories, supported by a comprehensive dataset of building observations across multiple cities.', title='Predicting Climate Risks with Smart Urban Data Integration'))
[09.02.2026 08:43] Response: ParsedChatCompletionMessage[Article](content='{"desc":"这篇论文提出了一种名为Skjold-DiT的扩散-变换器框架，旨在整合城市的时空数据，以预测建筑级别的气候风险。该框架特别考虑了交通网络结构，以便在紧急响应应用中提高效率。Skjold-DiT能够生成经过校准的可达性层，帮助智能车辆进行更好的路径规划和应急调度。研究还引入了Baltic-Caspian城市韧性数据集，包含847,392个建筑观察数据，支持多种气候风险的评估。","title":"智能应急响应的气候风险预测新框架"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='这篇论文提出了一种名为Skjold-DiT的扩散-变换器框架，旨在整合城市的时空数据，以预测建筑级别的气候风险。该框架特别考虑了交通网络结构，以便在紧急响应应用中提高效率。Skjold-DiT能够生成经过校准的可达性层，帮助智能车辆进行更好的路径规划和应急调度。研究还引入了Baltic-Caspian城市韧性数据集，包含847,392个建筑观察数据，支持多种气候风险的评估。', title='智能应急响应的气候风险预测新框架'))
[09.02.2026 08:43] Using data from previous issue: {"categories": ["#open_source", "#reasoning", "#dataset", "#training", "#multimodal", "#rag", "#cv", "#benchmark", "#video"], "emoji": "🔍", "ru": {"title": "Выход за пределы замороженных знаний: сегментация с поиском и рассуждением", "desc": "Seg-ReSearch представляет новый подход к сегментации, кот
[09.02.2026 08:43] Querying the API.
[09.02.2026 08:43] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Canzona presents a unified asynchronous framework that addresses the conflict between matrix-based optimizers and distributed tensor fragmentation in LLM training, improving efficiency and reducing latency.  					AI-generated summary 				 The scaling of Large Language Models (LLMs) drives interest in matrix-based optimizers (e.g., Shampoo, Muon, SOAP) for their convergence efficiency; yet their requirement for holistic updates conflicts with the tensor fragmentation in distributed frameworks like Megatron. Existing solutions are suboptimal: synchronous approaches suffer from computational redundancy, while layer-wise partitioning fails to reconcile this conflict without violating the geometric constraints of efficient communication primitives. To bridge this gap, we propose Canzona, a Unified, Asynchronous, and Load-Balanced framework that decouples logical optimizer assignment from physical parameter distribution. For Data Parallelism, we introduce an alpha-Balanced Static Partitioning strategy that respects atomicity while neutralizing the load imbalance. For Tensor Parallelism, we design an Asynchronous Compute pipeline utilizing Micro-Group Scheduling to batch fragmented updates and hide reconstruction overhead. Extensive evaluations on the Qwen3 model family (up to 32B parameters) on 256 GPUs demonstrate that our approach preserves the efficiency of established parallel architectures, achieving a 1.57x speedup in end-to-end iteration time and reducing optimizer step latency by 5.8x compared to the baseline.
[09.02.2026 08:43] Response: ```json
{
  "desc": "Canzona представляет унифицированный асинхронный фреймворк, который решает конфликт между матричными оптимизаторами и распределённой фрагментацией тензоров при обучении больших языковых моделей. Матричные оптимизаторы, такие как Shampoo и Muon, обеспечивают хорошую сходимость, но требуют полного доступа к параметрам, что противоречит распределённым фреймворкам типа Megatron, которые фрагментируют тензоры. Предложенный подход использует асинхронное вычисление с балансировкой нагрузки и микро-групповым планированием, отделяя логическое назначение оптимизатора от физического распределения параметров. На практике достигнуто ускорение в 1.57 раза для итерации обучения и сокращение задержки шага оптимизатора в 5.8 раз на модели Qwen3 размером до 32B параметров.",
  "emoji": "⚡",
  "title": "Асинхронная гармония: матричные оптимизаторы встречают распределённые тензоры"
}
```
[09.02.2026 08:43] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Canzona presents a unified asynchronous framework that addresses the conflict between matrix-based optimizers and distributed tensor fragmentation in LLM training, improving efficiency and reducing latency.  					AI-generated summary 				 The scaling of Large Language Models (LLMs) drives interest in matrix-based optimizers (e.g., Shampoo, Muon, SOAP) for their convergence efficiency; yet their requirement for holistic updates conflicts with the tensor fragmentation in distributed frameworks like Megatron. Existing solutions are suboptimal: synchronous approaches suffer from computational redundancy, while layer-wise partitioning fails to reconcile this conflict without violating the geometric constraints of efficient communication primitives. To bridge this gap, we propose Canzona, a Unified, Asynchronous, and Load-Balanced framework that decouples logical optimizer assignment from physical parameter distribution. For Data Parallelism, we introduce an alpha-Balanced Static Partitioning strategy that respects atomicity while neutralizing the load imbalance. For Tensor Parallelism, we design an Asynchronous Compute pipeline utilizing Micro-Group Scheduling to batch fragmented updates and hide reconstruction overhead. Extensive evaluations on the Qwen3 model family (up to 32B parameters) on 256 GPUs demonstrate that our approach preserves the efficiency of established parallel architectures, achieving a 1.57x speedup in end-to-end iteration time and reducing optimizer step latency by 5.8x compared to the baseline."

[09.02.2026 08:43] Response: ```python
["TRAINING", "ARCHITECTURE"]
```
[09.02.2026 08:43] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Canzona presents a unified asynchronous framework that addresses the conflict between matrix-based optimizers and distributed tensor fragmentation in LLM training, improving efficiency and reducing latency.  					AI-generated summary 				 The scaling of Large Language Models (LLMs) drives interest in matrix-based optimizers (e.g., Shampoo, Muon, SOAP) for their convergence efficiency; yet their requirement for holistic updates conflicts with the tensor fragmentation in distributed frameworks like Megatron. Existing solutions are suboptimal: synchronous approaches suffer from computational redundancy, while layer-wise partitioning fails to reconcile this conflict without violating the geometric constraints of efficient communication primitives. To bridge this gap, we propose Canzona, a Unified, Asynchronous, and Load-Balanced framework that decouples logical optimizer assignment from physical parameter distribution. For Data Parallelism, we introduce an alpha-Balanced Static Partitioning strategy that respects atomicity while neutralizing the load imbalance. For Tensor Parallelism, we design an Asynchronous Compute pipeline utilizing Micro-Group Scheduling to batch fragmented updates and hide reconstruction overhead. Extensive evaluations on the Qwen3 model family (up to 32B parameters) on 256 GPUs demonstrate that our approach preserves the efficiency of established parallel architectures, achieving a 1.57x speedup in end-to-end iteration time and reducing optimizer step latency by 5.8x compared to the baseline."

[09.02.2026 08:43] Response: ```python
["OPTIMIZATION"]
```
[09.02.2026 08:43] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Canzona introduces a new framework that improves the training of Large Language Models (LLMs) by resolving issues between matrix-based optimizers and distributed tensor fragmentation. It offers a unified, asynchronous approach that allows for efficient updates without the need for holistic parameter updates, which are often problematic in distributed systems. The framework includes a novel alpha-Balanced Static Partitioning strategy for Data Parallelism and an Asynchronous Compute pipeline for Tensor Parallelism, which together enhance load balancing and reduce latency. Evaluations show that Canzona significantly speeds up training times and optimizes performance on large models across multiple GPUs.","title":"Canzona: Bridging the Gap in LLM Training Efficiency"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Canzona introduces a new framework that improves the training of Large Language Models (LLMs) by resolving issues between matrix-based optimizers and distributed tensor fragmentation. It offers a unified, asynchronous approach that allows for efficient updates without the need for holistic parameter updates, which are often problematic in distributed systems. The framework includes a novel alpha-Balanced Static Partitioning strategy for Data Parallelism and an Asynchronous Compute pipeline for Tensor Parallelism, which together enhance load balancing and reduce latency. Evaluations show that Canzona significantly speeds up training times and optimizes performance on large models across multiple GPUs.', title='Canzona: Bridging the Gap in LLM Training Efficiency'))
[09.02.2026 08:43] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Canzona提出了一个统一的异步框架，解决了矩阵优化器与分布式张量碎片化之间的冲突，从而提高了效率并减少了延迟。大规模语言模型（LLM）的扩展推动了对矩阵优化器的关注，但它们对整体更新的需求与分布式框架中的张量碎片化相矛盾。现有的解决方案效率不高，导致计算冗余或无法有效沟通。Canzona通过解耦逻辑优化器分配与物理参数分布，提出了一种新的数据并行和张量并行策略，显著提升了训练效率。","title":"Canzona：提升大规模语言模型训练效率的异步框架"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Canzona提出了一个统一的异步框架，解决了矩阵优化器与分布式张量碎片化之间的冲突，从而提高了效率并减少了延迟。大规模语言模型（LLM）的扩展推动了对矩阵优化器的关注，但它们对整体更新的需求与分布式框架中的张量碎片化相矛盾。现有的解决方案效率不高，导致计算冗余或无法有效沟通。Canzona通过解耦逻辑优化器分配与物理参数分布，提出了一种新的数据并行和张量并行策略，显著提升了训练效率。', title='Canzona：提升大规模语言模型训练效率的异步框架'))
[09.02.2026 08:43] Querying the API.
[09.02.2026 08:43] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

QuantLRM uses weight update magnitude signals from fine-tuning to improve quantization of Large Reasoning Models, achieving better performance than traditional methods through channel importance estimation.  					AI-generated summary 				 Weight-only quantization is important for compressing Large Language Models (LLMs). Inspired by the spirit of classical magnitude pruning, we study whether the magnitude of weight updates during reasoning-incentivized fine-tuning can provide valuable signals for quantizing Large Reasoning Models (LRMs). We hypothesize that the smallest and largest weight updates during fine-tuning are more important than those of intermediate magnitude, a phenomenon we term "protecting both ends". Upon hypothesis validation, we introduce QuantLRM, which stands for weight quantization of LRMs via fine-tuning signals. We fit simple restricted quadratic functions on weight updates to protect both ends. By multiplying the average quadratic values with the count of zero weight updates of channels, we compute channel importance that is more effective than using activation or second-order information. We run QuantLRM to quantize various fine-tuned models (including supervised, direct preference optimization, and reinforcement learning fine-tuning) over four reasoning benchmarks (AIME-120, FOLIO, temporal sequences, and GPQA-Diamond) and empirically find that QuantLRM delivers a consistent improvement for LRMs quantization, with an average improvement of 6.55% on a reinforcement learning fine-tuned model. Also supporting non-fine-tuned LRMs, QuantLRM gathers effective signals via pseudo-fine-tuning, which greatly enhances its applicability.
[09.02.2026 08:43] Response: ```json
{
  "desc": "QuantLRM — это новый метод квантизации больших языковых моделей рассуждения, который использует информацию о величине обновлений весов во время fine-tuning для оценки важности каналов. Авторы предположили, что наиболее и наименее значительные изменения весов более важны для сохранения производительности модели, чем промежуточные обновления. Метод подбирает квадратичные функции на основе обновлений весов и вычисляет важность канала через количество нулевых обновлений, что оказывается более эффективно, чем традиционные подходы с использованием активаций или информации второго порядка. QuantLRM показывает стабильное улучшение на 6.55% при квантизации моделей с reinforcement learning fine-tuning и поддерживает работу с немодифицированными моделями через pseudo-fine-tuning.",
  "emoji": "⚖️",
  "title": "Защита обоих концов: квантизация через сигналы fine-tuning"
}
```
[09.02.2026 08:43] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"QuantLRM uses weight update magnitude signals from fine-tuning to improve quantization of Large Reasoning Models, achieving better performance than traditional methods through channel importance estimation.  					AI-generated summary 				 Weight-only quantization is important for compressing Large Language Models (LLMs). Inspired by the spirit of classical magnitude pruning, we study whether the magnitude of weight updates during reasoning-incentivized fine-tuning can provide valuable signals for quantizing Large Reasoning Models (LRMs). We hypothesize that the smallest and largest weight updates during fine-tuning are more important than those of intermediate magnitude, a phenomenon we term "protecting both ends". Upon hypothesis validation, we introduce QuantLRM, which stands for weight quantization of LRMs via fine-tuning signals. We fit simple restricted quadratic functions on weight updates to protect both ends. By multiplying the average quadratic values with the count of zero weight updates of channels, we compute channel importance that is more effective than using activation or second-order information. We run QuantLRM to quantize various fine-tuned models (including supervised, direct preference optimization, and reinforcement learning fine-tuning) over four reasoning benchmarks (AIME-120, FOLIO, temporal sequences, and GPQA-Diamond) and empirically find that QuantLRM delivers a consistent improvement for LRMs quantization, with an average improvement of 6.55% on a reinforcement learning fine-tuned model. Also supporting non-fine-tuned LRMs, QuantLRM gathers effective signals via pseudo-fine-tuning, which greatly enhances its applicability."

[09.02.2026 08:43] Response: ```python
["INFERENCE", "TRAINING", "BENCHMARK"]
```
[09.02.2026 08:43] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"QuantLRM uses weight update magnitude signals from fine-tuning to improve quantization of Large Reasoning Models, achieving better performance than traditional methods through channel importance estimation.  					AI-generated summary 				 Weight-only quantization is important for compressing Large Language Models (LLMs). Inspired by the spirit of classical magnitude pruning, we study whether the magnitude of weight updates during reasoning-incentivized fine-tuning can provide valuable signals for quantizing Large Reasoning Models (LRMs). We hypothesize that the smallest and largest weight updates during fine-tuning are more important than those of intermediate magnitude, a phenomenon we term "protecting both ends". Upon hypothesis validation, we introduce QuantLRM, which stands for weight quantization of LRMs via fine-tuning signals. We fit simple restricted quadratic functions on weight updates to protect both ends. By multiplying the average quadratic values with the count of zero weight updates of channels, we compute channel importance that is more effective than using activation or second-order information. We run QuantLRM to quantize various fine-tuned models (including supervised, direct preference optimization, and reinforcement learning fine-tuning) over four reasoning benchmarks (AIME-120, FOLIO, temporal sequences, and GPQA-Diamond) and empirically find that QuantLRM delivers a consistent improvement for LRMs quantization, with an average improvement of 6.55% on a reinforcement learning fine-tuned model. Also supporting non-fine-tuned LRMs, QuantLRM gathers effective signals via pseudo-fine-tuning, which greatly enhances its applicability."

[09.02.2026 08:43] Response: ```python
['OPTIMIZATION', 'REASONING']
```
[09.02.2026 08:43] Response: ParsedChatCompletionMessage[Article](content='{"desc":"QuantLRM is a novel approach that enhances the quantization of Large Reasoning Models (LRMs) by utilizing weight update signals from fine-tuning. It operates on the principle that the magnitude of weight updates during fine-tuning can indicate the importance of different channels, particularly focusing on the extremes of these updates. By applying a quadratic function to these weight updates, QuantLRM effectively estimates channel importance, leading to better performance than traditional quantization methods. The method has been validated across various reasoning benchmarks, showing an average performance improvement of 6.55% for reinforcement learning fine-tuned models.","title":"Enhancing LRM Quantization with Fine-Tuning Signals"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='QuantLRM is a novel approach that enhances the quantization of Large Reasoning Models (LRMs) by utilizing weight update signals from fine-tuning. It operates on the principle that the magnitude of weight updates during fine-tuning can indicate the importance of different channels, particularly focusing on the extremes of these updates. By applying a quadratic function to these weight updates, QuantLRM effectively estimates channel importance, leading to better performance than traditional quantization methods. The method has been validated across various reasoning benchmarks, showing an average performance improvement of 6.55% for reinforcement learning fine-tuned models.', title='Enhancing LRM Quantization with Fine-Tuning Signals'))
[09.02.2026 08:43] Response: ParsedChatCompletionMessage[Article](content='{"desc":"QuantLRM是一种通过微调信号来改进大型推理模型量化的方法。它利用权重更新的幅度信号，估计通道的重要性，从而实现比传统方法更好的性能。我们提出了“保护两端”的假设，认为微调过程中最小和最大权重更新比中间值更重要。通过这种方法，QuantLRM在多个推理基准上实现了量化的一致性提升，平均提高了6.55%。","title":"量化大型推理模型的新方法"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='QuantLRM是一种通过微调信号来改进大型推理模型量化的方法。它利用权重更新的幅度信号，估计通道的重要性，从而实现比传统方法更好的性能。我们提出了“保护两端”的假设，认为微调过程中最小和最大权重更新比中间值更重要。通过这种方法，QuantLRM在多个推理基准上实现了量化的一致性提升，平均提高了6.55%。', title='量化大型推理模型的新方法'))
[09.02.2026 08:43] Using data from previous issue: {"categories": ["#optimization"], "emoji": "⚡", "ru": {"title": "Адаптивная стабилизация режимов в оптимальном транспорте для масштабируемого предсказания структур", "desc": "В работе исследователи выявили проблему преждевременного коллапса мод в моделях структурного предсказания, основанных на опти
[09.02.2026 08:43] Using data from previous issue: {"categories": ["#open_source", "#transfer_learning", "#interpretability"], "emoji": "🔄", "ru": {"title": "Пластичность важнее гладкости: ключ к эффективному дообучению трансформеров", "desc": "В этой работе исследуется пластичность компонентов Vision Transformer — их способность адаптировать выходы
[09.02.2026 08:43] Using data from previous issue: {"categories": ["#transfer_learning", "#small_models", "#training"], "emoji": "🧹", "ru": {"title": "Очистка знаний для эффективной дистилляции из многих учителей", "desc": "В работе предлагается метод очистки знаний для трансферного обучения между большими языковыми моделями и более компактными моде
[09.02.2026 08:43] Renaming data file.
[09.02.2026 08:43] Renaming previous data. hf_papers.json to ./d/2026-02-09.json
[09.02.2026 08:43] Saving new data file.
[09.02.2026 08:43] Generating page.
[09.02.2026 08:43] Renaming previous page.
[09.02.2026 08:43] Renaming previous data. index.html to ./d/2026-02-09.html
[09.02.2026 08:43] Writing result.
[09.02.2026 08:43] Renaming log file.
[09.02.2026 08:43] Renaming previous data. log.txt to ./logs/2026-02-09_last_log.txt
