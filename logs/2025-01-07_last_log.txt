[07.01.2025 17:09] Read previous papers.
[07.01.2025 17:09] Generating top page (month).
[07.01.2025 17:09] Writing top page (month).
[07.01.2025 18:13] Read previous papers.
[07.01.2025 18:13] Get feed.
[07.01.2025 18:13] Get page data from previous paper. URL: https://huggingface.co/papers/2501.02976
[07.01.2025 18:13] Get page data from previous paper. URL: https://huggingface.co/papers/2501.03226
[07.01.2025 18:13] Get page data from previous paper. URL: https://huggingface.co/papers/2501.03218
[07.01.2025 18:13] Get page data from previous paper. URL: https://huggingface.co/papers/2501.02157
[07.01.2025 18:13] Get page data from previous paper. URL: https://huggingface.co/papers/2501.02497
[07.01.2025 18:13] Get page data from previous paper. URL: https://huggingface.co/papers/2501.02690
[07.01.2025 18:13] Get page data from previous paper. URL: https://huggingface.co/papers/2501.02045
[07.01.2025 18:13] Get page data from previous paper. URL: https://huggingface.co/papers/2501.03059
[07.01.2025 18:13] Get page data from previous paper. URL: https://huggingface.co/papers/2501.03006
[07.01.2025 18:13] Get page data from previous paper. URL: https://huggingface.co/papers/2501.01790
[07.01.2025 18:13] Get page data from previous paper. URL: https://huggingface.co/papers/2501.02576
[07.01.2025 18:13] Get page data from previous paper. URL: https://huggingface.co/papers/2501.01830
[07.01.2025 18:13] Get page data from previous paper. URL: https://huggingface.co/papers/2501.02506
[07.01.2025 18:13] Get page data from previous paper. URL: https://huggingface.co/papers/2501.02423
[07.01.2025 18:13] Get page data from previous paper. URL: https://huggingface.co/papers/2501.02832
[07.01.2025 18:13] Get page data from previous paper. URL: https://huggingface.co/papers/2501.00912
[07.01.2025 18:13] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[07.01.2025 18:13] No deleted papers detected.
[07.01.2025 18:13] Downloading and parsing papers (pdf, html). Total: 16.
[07.01.2025 18:13] Downloading and parsing paper https://huggingface.co/papers/2501.02976.
[07.01.2025 18:13] Extra JSON file exists (./assets/json/2501.02976.json), skip PDF parsing.
[07.01.2025 18:13] Paper image links file exists (./assets/img_data/2501.02976.json), skip HTML parsing.
[07.01.2025 18:13] Success.
[07.01.2025 18:13] Downloading and parsing paper https://huggingface.co/papers/2501.03226.
[07.01.2025 18:13] Extra JSON file exists (./assets/json/2501.03226.json), skip PDF parsing.
[07.01.2025 18:13] Paper image links file exists (./assets/img_data/2501.03226.json), skip HTML parsing.
[07.01.2025 18:13] Success.
[07.01.2025 18:13] Downloading and parsing paper https://huggingface.co/papers/2501.03218.
[07.01.2025 18:13] Extra JSON file exists (./assets/json/2501.03218.json), skip PDF parsing.
[07.01.2025 18:13] Paper image links file exists (./assets/img_data/2501.03218.json), skip HTML parsing.
[07.01.2025 18:13] Success.
[07.01.2025 18:13] Downloading and parsing paper https://huggingface.co/papers/2501.02157.
[07.01.2025 18:13] Extra JSON file exists (./assets/json/2501.02157.json), skip PDF parsing.
[07.01.2025 18:13] Paper image links file exists (./assets/img_data/2501.02157.json), skip HTML parsing.
[07.01.2025 18:13] Success.
[07.01.2025 18:13] Downloading and parsing paper https://huggingface.co/papers/2501.02497.
[07.01.2025 18:13] Extra JSON file exists (./assets/json/2501.02497.json), skip PDF parsing.
[07.01.2025 18:13] Paper image links file exists (./assets/img_data/2501.02497.json), skip HTML parsing.
[07.01.2025 18:13] Success.
[07.01.2025 18:13] Downloading and parsing paper https://huggingface.co/papers/2501.02690.
[07.01.2025 18:13] Extra JSON file exists (./assets/json/2501.02690.json), skip PDF parsing.
[07.01.2025 18:13] Paper image links file exists (./assets/img_data/2501.02690.json), skip HTML parsing.
[07.01.2025 18:13] Success.
[07.01.2025 18:13] Downloading and parsing paper https://huggingface.co/papers/2501.02045.
[07.01.2025 18:13] Extra JSON file exists (./assets/json/2501.02045.json), skip PDF parsing.
[07.01.2025 18:13] Paper image links file exists (./assets/img_data/2501.02045.json), skip HTML parsing.
[07.01.2025 18:13] Success.
[07.01.2025 18:13] Downloading and parsing paper https://huggingface.co/papers/2501.03059.
[07.01.2025 18:13] Extra JSON file exists (./assets/json/2501.03059.json), skip PDF parsing.
[07.01.2025 18:13] Paper image links file exists (./assets/img_data/2501.03059.json), skip HTML parsing.
[07.01.2025 18:13] Success.
[07.01.2025 18:13] Downloading and parsing paper https://huggingface.co/papers/2501.03006.
[07.01.2025 18:13] Extra JSON file exists (./assets/json/2501.03006.json), skip PDF parsing.
[07.01.2025 18:13] Paper image links file exists (./assets/img_data/2501.03006.json), skip HTML parsing.
[07.01.2025 18:13] Success.
[07.01.2025 18:13] Downloading and parsing paper https://huggingface.co/papers/2501.01790.
[07.01.2025 18:13] Extra JSON file exists (./assets/json/2501.01790.json), skip PDF parsing.
[07.01.2025 18:13] Paper image links file exists (./assets/img_data/2501.01790.json), skip HTML parsing.
[07.01.2025 18:13] Success.
[07.01.2025 18:13] Downloading and parsing paper https://huggingface.co/papers/2501.02576.
[07.01.2025 18:13] Extra JSON file exists (./assets/json/2501.02576.json), skip PDF parsing.
[07.01.2025 18:13] Paper image links file exists (./assets/img_data/2501.02576.json), skip HTML parsing.
[07.01.2025 18:13] Success.
[07.01.2025 18:13] Downloading and parsing paper https://huggingface.co/papers/2501.01830.
[07.01.2025 18:13] Extra JSON file exists (./assets/json/2501.01830.json), skip PDF parsing.
[07.01.2025 18:13] Paper image links file exists (./assets/img_data/2501.01830.json), skip HTML parsing.
[07.01.2025 18:13] Success.
[07.01.2025 18:13] Downloading and parsing paper https://huggingface.co/papers/2501.02506.
[07.01.2025 18:13] Extra JSON file exists (./assets/json/2501.02506.json), skip PDF parsing.
[07.01.2025 18:13] Paper image links file exists (./assets/img_data/2501.02506.json), skip HTML parsing.
[07.01.2025 18:13] Success.
[07.01.2025 18:13] Downloading and parsing paper https://huggingface.co/papers/2501.02423.
[07.01.2025 18:13] Extra JSON file exists (./assets/json/2501.02423.json), skip PDF parsing.
[07.01.2025 18:13] Paper image links file exists (./assets/img_data/2501.02423.json), skip HTML parsing.
[07.01.2025 18:13] Success.
[07.01.2025 18:13] Downloading and parsing paper https://huggingface.co/papers/2501.02832.
[07.01.2025 18:13] Extra JSON file exists (./assets/json/2501.02832.json), skip PDF parsing.
[07.01.2025 18:13] Paper image links file exists (./assets/img_data/2501.02832.json), skip HTML parsing.
[07.01.2025 18:13] Success.
[07.01.2025 18:13] Downloading and parsing paper https://huggingface.co/papers/2501.00912.
[07.01.2025 18:13] Extra JSON file exists (./assets/json/2501.00912.json), skip PDF parsing.
[07.01.2025 18:13] Paper image links file exists (./assets/img_data/2501.00912.json), skip HTML parsing.
[07.01.2025 18:13] Success.
[07.01.2025 18:13] Enriching papers with extra data.
[07.01.2025 18:13] ********************************************************************************
[07.01.2025 18:13] Abstract 0. Image diffusion models have been adapted for real-world video super-resolution to tackle over-smoothing issues in GAN-based methods. However, these models struggle to maintain temporal consistency, as they are trained on static images, limiting their ability to capture temporal dynamics effectively....
[07.01.2025 18:13] ********************************************************************************
[07.01.2025 18:13] Abstract 1. Cutting-edge large language models (LLMs) demonstrate promising performance in solving complex math problems with a divide-and-conquer pipeline and the assistance of in-context learning (ICL) examples. However, their potential for improvement is limited by two critical problems within their ICL exam...
[07.01.2025 18:13] ********************************************************************************
[07.01.2025 18:13] Abstract 2. Active Real-time interaction with video LLMs introduces a new paradigm for human-computer interaction, where the model not only understands user intent but also responds while continuously processing streaming video on the fly. Unlike offline video LLMs, which analyze the entire video before answeri...
[07.01.2025 18:13] ********************************************************************************
[07.01.2025 18:13] Abstract 3. As large language models (LLMs) evolve, their ability to deliver personalized and context-aware responses offers transformative potential for improving user experiences. Existing personalization approaches, however, often rely solely on user history to augment the prompt, limiting their effectivenes...
[07.01.2025 18:13] ********************************************************************************
[07.01.2025 18:13] Abstract 4. The remarkable performance of the o1 model in complex reasoning demonstrates that test-time computing scaling can further unlock the model's potential, enabling powerful System-2 thinking. However, there is still a lack of comprehensive surveys for test-time computing scaling. We trace the concept o...
[07.01.2025 18:13] ********************************************************************************
[07.01.2025 18:13] Abstract 5. 4D video control is essential in video generation as it enables the use of sophisticated lens techniques, such as multi-camera shooting and dolly zoom, which are currently unsupported by existing methods. Training a video Diffusion Transformer (DiT) directly to control 4D content requires expensive ...
[07.01.2025 18:13] ********************************************************************************
[07.01.2025 18:13] Abstract 6. We pretrain METAGENE-1, a 7-billion-parameter autoregressive transformer model, which we refer to as a metagenomic foundation model, on a novel corpus of diverse metagenomic DNA and RNA sequences comprising over 1.5 trillion base pairs. This dataset is sourced from a large collection of human wastew...
[07.01.2025 18:13] ********************************************************************************
[07.01.2025 18:13] Abstract 7. We consider the task of Image-to-Video (I2V) generation, which involves transforming static images into realistic video sequences based on a textual description. While recent advancements produce photorealistic outputs, they frequently struggle to create videos with accurate and consistent object mo...
[07.01.2025 18:13] ********************************************************************************
[07.01.2025 18:13] Abstract 8. Text-to-video generative models have made significant strides, enabling diverse applications in entertainment, advertising, and education. However, generating RGBA video, which includes alpha channels for transparency, remains a challenge due to limited datasets and the difficulty of adapting existi...
[07.01.2025 18:13] ********************************************************************************
[07.01.2025 18:13] Abstract 9. This paper presents a powerful framework to customize video creations by incorporating multiple specific identity (ID) photos, with video diffusion Transformers, referred to as Ingredients. Generally, our method consists of three primary modules: (i) a facial extractor that captures versatile and pr...
[07.01.2025 18:13] ********************************************************************************
[07.01.2025 18:13] Abstract 10. Monocular depth estimation within the diffusion-denoising paradigm demonstrates impressive generalization ability but suffers from low inference speed. Recent methods adopt a single-step deterministic paradigm to improve inference efficiency while maintaining comparable performance. However, they ov...
[07.01.2025 18:13] ********************************************************************************
[07.01.2025 18:13] Abstract 11. Automated red-teaming has become a crucial approach for uncovering vulnerabilities in large language models (LLMs). However, most existing methods focus on isolated safety flaws, limiting their ability to adapt to dynamic defenses and uncover complex vulnerabilities efficiently. To address this chal...
[07.01.2025 18:13] ********************************************************************************
[07.01.2025 18:13] Abstract 12. Effective evaluation of multi-hop tool use is critical for analyzing the understanding, reasoning, and function-calling capabilities of large language models (LLMs). However, progress has been hindered by a lack of reliable evaluation datasets. To address this, we present ToolHop, a dataset comprisi...
[07.01.2025 18:13] ********************************************************************************
[07.01.2025 18:13] Abstract 13. Low-precision training is considered an effective strategy for reducing both training and downstream inference costs. Previous scaling laws for precision mainly focus on integer quantization, which pay less attention to the constituents in floating-point quantization and thus cannot well fit the LLM...
[07.01.2025 18:13] ********************************************************************************
[07.01.2025 18:13] Abstract 14. We propose Samba ASR, the first state-of-the-art Automatic Speech Recognition (ASR) model leveraging the novel Mamba architecture as both encoder and decoder, built on the foundation of state-space models (SSMs). Unlike transformer-based ASR models, which rely on self-attention mechanisms to capture...
[07.01.2025 18:13] ********************************************************************************
[07.01.2025 18:13] Abstract 15. Designing structured visuals such as presentation slides is essential for communicative needs, necessitating both content creation and visual planning skills. In this work, we tackle the challenge of automated slide generation, where models produce slide presentations from natural language (NL) inst...
[07.01.2025 18:13] Read previous papers.
[07.01.2025 18:13] Generating reviews via LLM API.
[07.01.2025 18:13] Using data from previous issue: {"categories": ["#cv", "#optimization", "#diffusion", "#multimodal", "#video"], "emoji": "üé•", "ru": {"title": "–ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ —Å—É–ø–µ—Ä—Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é T2V –º–æ–¥–µ–ª–µ–π", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–∞—è –º–µ—Ç–æ–¥–∏–∫–∞ STAR –¥–ª—è —Å—É–ø–µ—Ä—Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –≤–∏–¥–µ–æ –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö —É—Å–ª–æ–≤–∏—è—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–æ–¥–µ–ª–µ–π text-to-video. –ü—Ä
[07.01.2025 18:13] Using data from previous issue: {"categories": ["#training", "#optimization", "#math", "#reasoning"], "emoji": "üßÆ", "ru": {"title": "BoostStep: –ü–æ–≤—ã—à–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –ò–ò –≤ —Ä–µ—à–µ–Ω–∏–∏ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ BoostStep –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–µ—à–µ–Ω–∏—è —Å–ª–æ–∂–Ω—ã—Ö –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á –±–æ–ª—å—à–∏–º–∏ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è
[07.01.2025 18:13] Using data from previous issue: {"categories": ["#long_context", "#video", "#optimization", "#architecture", "#interpretability"], "emoji": "üé•", "ru": {"title": "Dispider: –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ —Å –≤–∏–¥–µ–æ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–∏—Å—Ç–µ–º—É Dispider –¥–ª—è –∞–∫—Ç–∏–≤–Ω–æ–≥–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å –≤–∏–¥–µ–æ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ
[07.01.2025 18:13] Using data from previous issue: {"categories": ["#rag", "#optimization", "#graphs", "#multimodal", "#benchmark", "#games"], "emoji": "üï∏Ô∏è", "ru": {"title": "–ì—Ä–∞—Ñ—ã –∑–Ω–∞–Ω–∏–π –Ω–∞ —Å–ª—É–∂–±–µ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º PGraphR
[07.01.2025 18:13] Using data from previous issue: {"categories": ["#reasoning", "#math", "#survey", "#training"], "emoji": "üß†", "ru": {"title": "–ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π: –ø—É—Ç—å –∫ –º—ã—à–ª–µ–Ω–∏—é System-2", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –≤–æ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã
[07.01.2025 18:13] Using data from previous issue: {"categories": ["#video", "#games", "#diffusion", "#3d"], "emoji": "üé•", "ru": {"title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ: 4D-–∫–æ–Ω—Ç—Ä–æ–ª—å —Å –ø–æ–º–æ—â—å—é –≥–∞—É—Å—Å–æ–≤—ã—Ö –ø–æ–ª–µ–π", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ —Å 4D-–∫–æ–Ω—Ç—Ä–æ–ª–µ–º, –∏—Å–ø–æ–ª—å–∑—É—è –ø—Å–µ–≤–¥–æ-4D –≥–∞—É—Å—Å–æ–≤—ã –ø–æ–ª—è –∏ –º–æ–¥–µ–ª—å Diffusion T
[07.01.2025 18:13] Using data from previous issue: {"categories": ["#benchmark", "#data", "#training", "#architecture", "#science", "#dataset", "#healthcare"], "emoji": "üß¨", "ru": {"title": "METAGENE-1: –ú–µ—Ç–∞–≥–µ–Ω–æ–º–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –∑–¥–æ—Ä–æ–≤—å—è –Ω–∞—Å–µ–ª–µ–Ω–∏—è", "desc": "METAGENE-1 - —ç—Ç–æ –∞–≤—Ç–æ—Ä–µ–≥—Ä–∞—Å—Å–∏–≤–Ω–∞—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–Ω–∞—è –º–æ–¥–µ–ª—å —Å 7 –º–∏–ª–ª–∏–∞—Ä–¥–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
[07.01.2025 18:13] Using data from previous issue: {"categories": ["#video", "#multimodal", "#benchmark"], "emoji": "üé¨", "ru": {"title": "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö –≤–∏–¥–µ–æ –∏–∑ —Å—Ç–∞—Ç–∏—á–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é –º–∞—Å–æ–∫ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π –¥–≤–∏–∂–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π (I2V) –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –æ–ø–∏—Å–∞–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã –ø—Ä
[07.01.2025 18:13] Using data from previous issue: {"categories": ["#optimization", "#architecture", "#training", "#diffusion", "#video"], "emoji": "üé¨", "ru": {"title": "TransPixar: –ü—Ä–æ—Ä—ã–≤ –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ RGBA-–≤–∏–¥–µ–æ –¥–ª—è –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —ç—Ñ—Ñ–µ–∫—Ç–æ–≤", "desc": "TransPixar - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ RGBA-–≤–∏–¥–µ–æ, —Ä–∞—Å—à–∏—Ä—è—é—â–∏–π –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ–º–æ–¥–µ–ª–µ–π. –û
[07.01.2025 18:13] Using data from previous issue: {"categories": ["#open_source", "#training", "#architecture", "#video", "#dataset", "#diffusion", "#multimodal"], "emoji": "üé¨", "ru": {"title": "–ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –≤–∏–¥–µ–æ –∏–∑ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π: –Ω–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å –∫–æ–Ω—Ç—Ä–æ–ª—è –≤ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Ingredients
[07.01.2025 18:13] Using data from previous issue: {"categories": ["#optimization", "#training", "#diffusion", "#cv", "#dataset"], "emoji": "üîç", "ru": {"title": "DepthMaster: –û–¥–Ω–æ–ø—Ä–æ—Ö–æ–¥–Ω–∞—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Ç–æ—á–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –≥–ª—É–±–∏–Ω—ã —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞–ª–∏–∑–∞—Ü–∏–µ–π", "desc": "DepthMaster - —ç—Ç–æ –æ–¥–Ω–æ–ø—Ä–æ—Ö–æ–¥–Ω–∞—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –º–æ–Ω–æ–∫—É–ª—è—Ä–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –≥
[07.01.2025 18:13] Using data from previous issue: {"categories": ["#security", "#rl", "#rlhf"], "emoji": "üõ°Ô∏è", "ru": {"title": "Auto-RT: –£–º–Ω–∞—è –∑–∞—â–∏—Ç–∞ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç Auto-RT - —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ —É—è–∑–≤–∏–º–æ—Å—Ç–µ–π –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM). –°–∏—Å—Ç–µ–º–∞ –∏—Å–ø–æ
[07.01.2025 18:13] Using data from previous issue: {"categories": ["#benchmark", "#reasoning", "#dataset", "#optimization"], "emoji": "üõ†Ô∏è", "ru": {"title": "ToolHop: –Ω–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–Ω–æ–≥–æ—ç—Ç–∞–ø–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –≤ LLM", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö ToolHop –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–Ω–æ–≥–æ—ç—Ç–∞–ø–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –±–æ
[07.01.2025 18:13] Using data from previous issue: {"categories": ["#training", "#optimization", "#inference"], "emoji": "üßÆ", "ru": {"title": "–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –≤ –æ–±—É—á–µ–Ω–∏–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –≤–ª–∏—è–Ω–∏–µ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è —Å –ø–ª–∞–≤–∞—é—â–µ–π –∑–∞–ø—è—Ç–æ–π –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –ê–≤—Ç–æ—Ä—ã –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç —Ä–æ–ª—å —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª
[07.01.2025 18:13] Using data from previous issue: {"categories": ["#audio", "#architecture", "#benchmark", "#low_resource", "#open_source"], "emoji": "üéôÔ∏è", "ru": {"title": "Samba ASR: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–∏ —Ä–µ—á–∏ —Å –ø–æ–º–æ—â—å—é –º–æ–¥–µ–ª–µ–π –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ —Å–æ—Å—Ç–æ—è–Ω–∏–π", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –º–æ–¥–µ–ª—å Samba ASR - –ø–µ—Ä–≤–∞—è —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞—Å–ø–æ–∑–Ω–∞
[07.01.2025 18:13] Using data from previous issue: {"categories": ["#dataset", "#story_generation", "#training", "#benchmark", "#multimodal"], "emoji": "üéØ", "ru": {"title": "–ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è —Å–æ–∑–¥–∞–Ω–∏—è –ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏–π: –æ—Ç —Ç–µ–∫—Å—Ç–∞ –∫ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –≤–∏–∑—É–∞–ª—å–Ω—ã–º –º–∞—Ç–µ—Ä–∏–∞–ª–∞–º", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ SlidesBench –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏
[07.01.2025 18:13] Loading Chinese text from previous data.
[07.01.2025 18:13] Renaming data file.
[07.01.2025 18:13] Renaming previous data. hf_papers.json to ./d/2025-01-07.json
[07.01.2025 18:13] Saving new data file.
[07.01.2025 18:13] Generating page.
[07.01.2025 18:13] Renaming previous page.
[07.01.2025 18:13] Renaming previous data. index.html to ./d/2025-01-07.html
[07.01.2025 18:13] [Experimental] Generating Chinese page for reading.
[07.01.2025 18:13] Chinese vocab [{'word': 'ËÆ®ËÆ∫', 'pinyin': 't«éo l√πn', 'trans': 'discuss'}, {'word': 'Â§ßËØ≠Ë®ÄÊ®°Âûã', 'pinyin': 'd√† y«î y√°n m√≥ x√≠ng', 'trans': 'large language model'}, {'word': 'Â§çÊùÇ', 'pinyin': 'f√π z√°', 'trans': 'complex'}, {'word': 'Êï∞Â≠¶', 'pinyin': 'sh√π xu√©', 'trans': 'mathematics'}, {'word': 'Ë°®Áé∞', 'pinyin': 'bi«éo xi√†n', 'trans': 'performance'}, {'word': 'ÂàÜËÄåÊ≤ª‰πã', 'pinyin': 'fƒìn √©r zh√¨ zhƒ´', 'trans': 'divide and conquer'}, {'word': '‰∏ä‰∏ãÊñáÂ≠¶‰π†', 'pinyin': 'sh√†ng xi√† w√©n xu√© x√≠', 'trans': 'in-context learning'}, {'word': 'Á§∫‰æã', 'pinyin': 'sh√¨ l√¨', 'trans': 'example'}, {'word': 'ÂÖ≥ÈîÆ', 'pinyin': 'gu«én ji√†n', 'trans': 'key'}, {'word': 'Á≤íÂ∫¶', 'pinyin': 'l√¨ d√π', 'trans': 'granularity'}, {'word': 'Ë¥üÈù¢ÊïàÂ∫î', 'pinyin': 'f√π mi√†n xi√†o y√¨ng', 'trans': 'negative effect'}, {'word': 'Âô™Â£∞', 'pinyin': 'z√†o shƒìng', 'trans': 'noise'}, {'word': 'Êé®ÁêÜ', 'pinyin': 'tuƒ´ l«ê', 'trans': 'reasoning'}, {'word': 'Ë¥®Èáè', 'pinyin': 'zh√¨ li√†ng', 'trans': 'quality'}, {'word': 'ÊèêÂá∫', 'pinyin': 't√≠ ch≈´', 'trans': 'propose'}, {'word': 'BoostStep', 'pinyin': 'BoostStep', 'trans': 'BoostStep'}, {'word': 'ÂØπÈΩê', 'pinyin': 'du√¨ q√≠', 'trans': 'align'}, {'word': 'Á≠ñÁï•', 'pinyin': 'c√® l√º√®', 'trans': 'strategy'}, {'word': 'Áõ∏ÂÖ≥', 'pinyin': 'xiƒÅng guƒÅn', 'trans': 'relevant'}, {'word': 'ÊèêÈ´ò', 'pinyin': 't√≠ gƒÅo', 'trans': 'improve'}, {'word': 'ËíôÁâπÂç°ÁΩóÊ†ëÊêúÁ¥¢ÊñπÊ≥ï', 'pinyin': 'm√©ng t√® k«é lu√≥ sh√π s≈çu su«í fƒÅng f«é', 'trans': 'Monte Carlo Tree Search method'}, {'word': 'ÁªìÂêà', 'pinyin': 'ji√© h√©', 'trans': 'combine'}, {'word': '‰ΩøÁî®', 'pinyin': 'sh«ê y√≤ng', 'trans': 'use'}, {'word': 'ÊèêÂçá', 'pinyin': 't√≠ shƒìng', 'trans': 'enhance'}, {'word': 'ÂÆûÈ™å', 'pinyin': 'sh√≠ y√†n', 'trans': 'experiment'}, {'word': 'ÁªìÊûú', 'pinyin': 'ji√© gu«í', 'trans': 'result'}, {'word': 'ÊòæËëó', 'pinyin': 'xi«én zh√π', 'trans': 'significant'}, {'word': 'GPT-4o', 'pinyin': 'GPT-4o', 'trans': 'GPT-4o'}, {'word': 'Qwen2.5-Math-72B', 'pinyin': 'Qwen2.5-Math-72B', 'trans': 'Qwen2.5-Math-72B'}]
[07.01.2025 18:13] Renaming previous Chinese page.
[07.01.2025 18:13] Renaming previous data. zh.html to ./d/2025-01-06_zh_reading_task.html
[07.01.2025 18:13] Writing Chinese reading task.
[07.01.2025 18:13] Writing result.
[07.01.2025 18:13] Renaming log file.
[07.01.2025 18:13] Renaming previous data. log.txt to ./logs/2025-01-07_last_log.txt
