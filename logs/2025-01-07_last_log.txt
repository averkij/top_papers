[07.01.2025 08:14] Read previous papers.
[07.01.2025 08:14] Generating top page (month).
[07.01.2025 08:14] Writing top page (month).
[07.01.2025 09:11] Read previous papers.
[07.01.2025 09:11] Get feed.
[07.01.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2501.03226
[07.01.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2501.03218
[07.01.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2501.02157
[07.01.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2501.02045
[07.01.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2501.03006
[07.01.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2501.02497
[07.01.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2501.02976
[07.01.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2501.01790
[07.01.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2501.02832
[07.01.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2501.01830
[07.01.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2501.03059
[07.01.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2501.02690
[07.01.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2501.02506
[07.01.2025 09:11] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[07.01.2025 09:11] No deleted papers detected.
[07.01.2025 09:11] Downloading and parsing papers (pdf, html). Total: 13.
[07.01.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2501.03226.
[07.01.2025 09:11] Extra JSON file exists (./assets/json/2501.03226.json), skip PDF parsing.
[07.01.2025 09:11] Paper image links file exists (./assets/img_data/2501.03226.json), skip HTML parsing.
[07.01.2025 09:11] Success.
[07.01.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2501.03218.
[07.01.2025 09:11] Extra JSON file exists (./assets/json/2501.03218.json), skip PDF parsing.
[07.01.2025 09:11] Paper image links file exists (./assets/img_data/2501.03218.json), skip HTML parsing.
[07.01.2025 09:11] Success.
[07.01.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2501.02157.
[07.01.2025 09:11] Extra JSON file exists (./assets/json/2501.02157.json), skip PDF parsing.
[07.01.2025 09:11] Paper image links file exists (./assets/img_data/2501.02157.json), skip HTML parsing.
[07.01.2025 09:11] Success.
[07.01.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2501.02045.
[07.01.2025 09:11] Extra JSON file exists (./assets/json/2501.02045.json), skip PDF parsing.
[07.01.2025 09:11] Paper image links file exists (./assets/img_data/2501.02045.json), skip HTML parsing.
[07.01.2025 09:11] Success.
[07.01.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2501.03006.
[07.01.2025 09:11] Extra JSON file exists (./assets/json/2501.03006.json), skip PDF parsing.
[07.01.2025 09:11] Paper image links file exists (./assets/img_data/2501.03006.json), skip HTML parsing.
[07.01.2025 09:11] Success.
[07.01.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2501.02497.
[07.01.2025 09:11] Extra JSON file exists (./assets/json/2501.02497.json), skip PDF parsing.
[07.01.2025 09:11] Paper image links file exists (./assets/img_data/2501.02497.json), skip HTML parsing.
[07.01.2025 09:11] Success.
[07.01.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2501.02976.
[07.01.2025 09:11] Extra JSON file exists (./assets/json/2501.02976.json), skip PDF parsing.
[07.01.2025 09:11] Paper image links file exists (./assets/img_data/2501.02976.json), skip HTML parsing.
[07.01.2025 09:11] Success.
[07.01.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2501.01790.
[07.01.2025 09:11] Extra JSON file exists (./assets/json/2501.01790.json), skip PDF parsing.
[07.01.2025 09:11] Paper image links file exists (./assets/img_data/2501.01790.json), skip HTML parsing.
[07.01.2025 09:11] Success.
[07.01.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2501.02832.
[07.01.2025 09:11] Extra JSON file exists (./assets/json/2501.02832.json), skip PDF parsing.
[07.01.2025 09:11] Paper image links file exists (./assets/img_data/2501.02832.json), skip HTML parsing.
[07.01.2025 09:11] Success.
[07.01.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2501.01830.
[07.01.2025 09:11] Extra JSON file exists (./assets/json/2501.01830.json), skip PDF parsing.
[07.01.2025 09:11] Paper image links file exists (./assets/img_data/2501.01830.json), skip HTML parsing.
[07.01.2025 09:11] Success.
[07.01.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2501.03059.
[07.01.2025 09:11] Extra JSON file exists (./assets/json/2501.03059.json), skip PDF parsing.
[07.01.2025 09:11] Paper image links file exists (./assets/img_data/2501.03059.json), skip HTML parsing.
[07.01.2025 09:11] Success.
[07.01.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2501.02690.
[07.01.2025 09:11] Extra JSON file exists (./assets/json/2501.02690.json), skip PDF parsing.
[07.01.2025 09:11] Paper image links file exists (./assets/img_data/2501.02690.json), skip HTML parsing.
[07.01.2025 09:11] Success.
[07.01.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2501.02506.
[07.01.2025 09:11] Extra JSON file exists (./assets/json/2501.02506.json), skip PDF parsing.
[07.01.2025 09:11] Paper image links file exists (./assets/img_data/2501.02506.json), skip HTML parsing.
[07.01.2025 09:11] Success.
[07.01.2025 09:11] Enriching papers with extra data.
[07.01.2025 09:11] ********************************************************************************
[07.01.2025 09:11] Abstract 0. Cutting-edge large language models (LLMs) demonstrate promising performance in solving complex math problems with a divide-and-conquer pipeline and the assistance of in-context learning (ICL) examples. However, their potential for improvement is limited by two critical problems within their ICL exam...
[07.01.2025 09:11] ********************************************************************************
[07.01.2025 09:11] Abstract 1. Active Real-time interaction with video LLMs introduces a new paradigm for human-computer interaction, where the model not only understands user intent but also responds while continuously processing streaming video on the fly. Unlike offline video LLMs, which analyze the entire video before answeri...
[07.01.2025 09:11] ********************************************************************************
[07.01.2025 09:11] Abstract 2. As large language models (LLMs) evolve, their ability to deliver personalized and context-aware responses offers transformative potential for improving user experiences. Existing personalization approaches, however, often rely solely on user history to augment the prompt, limiting their effectivenes...
[07.01.2025 09:11] ********************************************************************************
[07.01.2025 09:11] Abstract 3. We pretrain METAGENE-1, a 7-billion-parameter autoregressive transformer model, which we refer to as a metagenomic foundation model, on a novel corpus of diverse metagenomic DNA and RNA sequences comprising over 1.5 trillion base pairs. This dataset is sourced from a large collection of human wastew...
[07.01.2025 09:11] ********************************************************************************
[07.01.2025 09:11] Abstract 4. Text-to-video generative models have made significant strides, enabling diverse applications in entertainment, advertising, and education. However, generating RGBA video, which includes alpha channels for transparency, remains a challenge due to limited datasets and the difficulty of adapting existi...
[07.01.2025 09:11] ********************************************************************************
[07.01.2025 09:11] Abstract 5. The remarkable performance of the o1 model in complex reasoning demonstrates that test-time computing scaling can further unlock the model's potential, enabling powerful System-2 thinking. However, there is still a lack of comprehensive surveys for test-time computing scaling. We trace the concept o...
[07.01.2025 09:11] ********************************************************************************
[07.01.2025 09:11] Abstract 6. Image diffusion models have been adapted for real-world video super-resolution to tackle over-smoothing issues in GAN-based methods. However, these models struggle to maintain temporal consistency, as they are trained on static images, limiting their ability to capture temporal dynamics effectively....
[07.01.2025 09:11] ********************************************************************************
[07.01.2025 09:11] Abstract 7. This paper presents a powerful framework to customize video creations by incorporating multiple specific identity (ID) photos, with video diffusion Transformers, referred to as Ingredients. Generally, our method consists of three primary modules: (i) a facial extractor that captures versatile and pr...
[07.01.2025 09:11] ********************************************************************************
[07.01.2025 09:11] Abstract 8. We propose Samba ASR, the first state-of-the-art Automatic Speech Recognition (ASR) model leveraging the novel Mamba architecture as both encoder and decoder, built on the foundation of state-space models (SSMs). Unlike transformer-based ASR models, which rely on self-attention mechanisms to capture...
[07.01.2025 09:11] ********************************************************************************
[07.01.2025 09:11] Abstract 9. Automated red-teaming has become a crucial approach for uncovering vulnerabilities in large language models (LLMs). However, most existing methods focus on isolated safety flaws, limiting their ability to adapt to dynamic defenses and uncover complex vulnerabilities efficiently. To address this chal...
[07.01.2025 09:11] ********************************************************************************
[07.01.2025 09:11] Abstract 10. We consider the task of Image-to-Video (I2V) generation, which involves transforming static images into realistic video sequences based on a textual description. While recent advancements produce photorealistic outputs, they frequently struggle to create videos with accurate and consistent object mo...
[07.01.2025 09:11] ********************************************************************************
[07.01.2025 09:11] Abstract 11. 4D video control is essential in video generation as it enables the use of sophisticated lens techniques, such as multi-camera shooting and dolly zoom, which are currently unsupported by existing methods. Training a video Diffusion Transformer (DiT) directly to control 4D content requires expensive ...
[07.01.2025 09:11] ********************************************************************************
[07.01.2025 09:11] Abstract 12. Effective evaluation of multi-hop tool use is critical for analyzing the understanding, reasoning, and function-calling capabilities of large language models (LLMs). However, progress has been hindered by a lack of reliable evaluation datasets. To address this, we present ToolHop, a dataset comprisi...
[07.01.2025 09:11] Read previous papers.
[07.01.2025 09:11] Generating reviews via LLM API.
[07.01.2025 09:11] Using data from previous issue: {"categories": ["#training", "#optimization", "#math", "#reasoning"], "emoji": "ğŸ§®", "ru": {"title": "BoostStep: ĞŸĞ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ˜Ğ˜ Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ BoostStep Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ñ
[07.01.2025 09:11] Using data from previous issue: {"categories": ["#long_context", "#video", "#optimization", "#architecture", "#interpretability"], "emoji": "ğŸ¥", "ru": {"title": "Dispider: Ğ˜Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Dispider Ğ´Ğ»Ñ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼Ğµ
[07.01.2025 09:11] Using data from previous issue: {"categories": ["#rag", "#optimization", "#graphs", "#multimodal", "#benchmark", "#games"], "emoji": "ğŸ•¸ï¸", "ru": {"title": "Ğ“Ñ€Ğ°Ñ„Ñ‹ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ½Ğ° ÑĞ»ÑƒĞ¶Ğ±Ğµ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ PGraphR
[07.01.2025 09:11] Using data from previous issue: {"categories": ["#benchmark", "#data", "#training", "#architecture", "#science", "#dataset", "#healthcare"], "emoji": "ğŸ§¬", "ru": {"title": "METAGENE-1: ĞœĞµÑ‚Ğ°Ğ³ĞµĞ½Ğ¾Ğ¼Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³Ğ° Ğ·Ğ´Ğ¾Ñ€Ğ¾Ğ²ÑŒÑ Ğ½Ğ°ÑĞµĞ»ĞµĞ½Ğ¸Ñ", "desc": "METAGENE-1 - ÑÑ‚Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€Ğ°ÑÑĞ¸Ğ²Ğ½Ğ°Ñ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ 7 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²
[07.01.2025 09:11] Using data from previous issue: {"categories": ["#optimization", "#architecture", "#training", "#diffusion", "#video"], "emoji": "ğŸ¬", "ru": {"title": "TransPixar: ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ RGBA-Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ„Ñ„ĞµĞºÑ‚Ğ¾Ğ²", "desc": "TransPixar - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ RGBA-Ğ²Ğ¸Ğ´ĞµĞ¾, Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑÑÑ‰Ğ¸Ğ¹ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ
[07.01.2025 09:11] Using data from previous issue: {"categories": ["#reasoning", "#math", "#survey", "#training"], "emoji": "ğŸ§ ", "ru": {"title": "ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹: Ğ¿ÑƒÑ‚ÑŒ Ğº Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ System-2", "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹
[07.01.2025 09:11] Using data from previous issue: {"categories": ["#cv", "#optimization", "#diffusion", "#multimodal", "#video"], "emoji": "ğŸ¥", "ru": {"title": "ĞšĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ ÑÑƒĞ¿ĞµÑ€Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ T2V Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹", "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¸ĞºĞ° STAR Ğ´Ğ»Ñ ÑÑƒĞ¿ĞµÑ€Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ text-to-video. ĞŸÑ€
[07.01.2025 09:11] Using data from previous issue: {"categories": ["#open_source", "#training", "#architecture", "#video", "#dataset", "#diffusion", "#multimodal"], "emoji": "ğŸ¬", "ru": {"title": "ĞŸĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· Ñ„Ğ¾Ñ‚Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ğ¹: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ingredients
[07.01.2025 09:11] Using data from previous issue: {"categories": ["#audio", "#architecture", "#benchmark", "#low_resource", "#open_source"], "emoji": "ğŸ™ï¸", "ru": {"title": "Samba ASR: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ñ€ĞµÑ‡Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹", "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Samba ASR - Ğ¿ĞµÑ€Ğ²Ğ°Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°
[07.01.2025 09:11] Using data from previous issue: {"categories": ["#security", "#rl", "#rlhf"], "emoji": "ğŸ›¡ï¸", "ru": {"title": "Auto-RT: Ğ£Ğ¼Ğ½Ğ°Ñ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹", "desc": "ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Auto-RT - Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM). Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾
[07.01.2025 09:11] Using data from previous issue: {"categories": ["#video", "#multimodal", "#benchmark"], "emoji": "ğŸ¬", "ru": {"title": "Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ°ÑĞ¾Ğº Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ (I2V) Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€
[07.01.2025 09:11] Using data from previous issue: {"categories": ["#video", "#games", "#diffusion", "#3d"], "emoji": "ğŸ¥", "ru": {"title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾: 4D-ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ñ‹Ñ… Ğ¿Ğ¾Ğ»ĞµĞ¹", "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ 4D-ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¿ÑĞµĞ²Ğ´Ğ¾-4D Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ñ‹ Ğ¿Ğ¾Ğ»Ñ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Diffusion T
[07.01.2025 09:11] Using data from previous issue: {"categories": ["#benchmark", "#reasoning", "#dataset", "#optimization"], "emoji": "ğŸ› ï¸", "ru": {"title": "ToolHop: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² LLM", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ToolHop Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ±Ğ¾
[07.01.2025 09:11] Trying to get texts in Chinese.
[07.01.2025 09:11] Mistral request. Model: mistral-large-latest. Prompt: Write simple and brief explanation (4-5 sentences) of an article in Chinese. Use short sentences. Text:

Cutting-edge large language models (LLMs) demonstrate promising performance in solving complex math problems with a divide-and-conquer pipeline and the assistance of in-context learning (ICL) examples. However, their potential for improvement is limited by two critical problems within their ICL examples: granularity-mismatch and the ensuing negative-effect noise problem. Specifically, the LLMs are capable of the dividing process yet mostly failed by inaccurate reasoning within a few conquer steps, while the ICL examples retrieved in question-grained sometimes lack relevant steps for a specific challenging reasoning step. Further, this disconnect may hinder the correct reasoning due to its irrelevance. To this end, we focus on improving the reasoning quality within each step and present BoostStep. BoostStep aligns the granularity between the retrieving and reasoning on step grained, and provides highly related ICL examples for each reasoning step with a novel `first-try' strategy. BoostStep provides more relevant examples than the coarse question-grained strategy, enhancing the model reasoning quality within each step steadily. BoostStep is a general and robust reasoning-enhancing method that not only improves standalone reasoning performance but also integrates seamlessly with Monte Carlo Tree Search methods (MCTS) to refine both candidate generation and decision-making. Quantitatively, it improves GPT-4o and Qwen2.5-Math-72B by 3.6\% and 2.0\% respectively on various mathematical benchmarks, and 7.5\% gain combined with MCTS.
[07.01.2025 09:11] Mistral response. {"id": "3717d3e89ae2408aad46e61fffeb0c21", "object": "chat.completion", "created": 1736241086, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "\u8fd9\u7bc7\u6587\u7ae0\u8ba8\u8bba\u4e86\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u89e3\u51b3\u590d\u6742\u6570\u5b66\u95ee\u9898\u65f6\u7684\u8868\u73b0\u3002\u6a21\u578b\u4f7f\u7528\u5206\u800c\u6cbb\u4e4b\u7684\u65b9\u6cd5\u548c\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u793a\u4f8b\u3002\u7136\u800c\uff0cICL\u793a\u4f8b\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u95ee\u9898\uff1a\u7c92\u5ea6\u4e0d\u5339\u914d\u548c\u8d1f\u9762\u6548\u5e94\u566a\u58f0\u95ee\u9898\u3002\u8fd9\u4e9b\u95ee\u9898\u4f1a\u5f71\u54cd\u6a21\u578b\u7684\u63a8\u7406\u8d28\u91cf\u3002\u4f5c\u8005\u63d0\u51fa\u4e86BoostStep\u65b9\u6cd5\uff0c\u901a\u8fc7\u6b65\u9aa4\u7c92\u5ea6\u7684\u5bf9\u9f50\u548c\u201c\u9996\u6b21\u5c1d\u8bd5\u201d\u7b56\u7565\u63d0\u4f9b\u76f8\u5173\u7684ICL\u793a\u4f8b\uff0c\u63d0\u9ad8\u63a8\u7406\u8d28\u91cf\u3002BoostStep\u8fd8\u53ef\u4ee5\u4e0e\u8499\u7279\u5361\u7f57\u6811\u641c\u7d22\u65b9\u6cd5\uff08MCTS\uff09\u7ed3\u5408\u4f7f\u7528\uff0c\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cBoostStep\u663e\u8457\u63d0\u9ad8\u4e86GPT-4o\u548cQwen2.5-Math-72B\u7684\u8868\u73b0\u3002"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 371, "total_tokens": 599, "completion_tokens": 228}}
[07.01.2025 09:11] Response: è¿™ç¯‡æ–‡ç« è®¨è®ºäº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è§£å†³å¤æ‚æ•°å­¦é—®é¢˜æ—¶çš„è¡¨ç°ã€‚æ¨¡å‹ä½¿ç”¨åˆ†è€Œæ²»ä¹‹çš„æ–¹æ³•å’Œä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰ç¤ºä¾‹ã€‚ç„¶è€Œï¼ŒICLç¤ºä¾‹å­˜åœ¨ä¸¤ä¸ªå…³é”®é—®é¢˜ï¼šç²’åº¦ä¸åŒ¹é…å’Œè´Ÿé¢æ•ˆåº”å™ªå£°é—®é¢˜ã€‚è¿™äº›é—®é¢˜ä¼šå½±å“æ¨¡å‹çš„æ¨ç†è´¨é‡ã€‚ä½œè€…æå‡ºäº†BoostStepæ–¹æ³•ï¼Œé€šè¿‡æ­¥éª¤ç²’åº¦çš„å¯¹é½å’Œâ€œé¦–æ¬¡å°è¯•â€ç­–ç•¥æä¾›ç›¸å…³çš„ICLç¤ºä¾‹ï¼Œæé«˜æ¨ç†è´¨é‡ã€‚BoostStepè¿˜å¯ä»¥ä¸è’™ç‰¹å¡ç½—æ ‘æœç´¢æ–¹æ³•ï¼ˆMCTSï¼‰ç»“åˆä½¿ç”¨ï¼Œæå‡æ¨¡å‹æ€§èƒ½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒBoostStepæ˜¾è‘—æé«˜äº†GPT-4oå’ŒQwen2.5-Math-72Bçš„è¡¨ç°ã€‚
[07.01.2025 09:11] Mistral request. Model: mistral-large-latest. Prompt: Write pinyin transcription for text. Text:

è¿™ç¯‡æ–‡ç« è®¨è®ºäº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è§£å†³å¤æ‚æ•°å­¦é—®é¢˜æ—¶çš„è¡¨ç°ã€‚æ¨¡å‹ä½¿ç”¨åˆ†è€Œæ²»ä¹‹çš„æ–¹æ³•å’Œä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰ç¤ºä¾‹ã€‚ç„¶è€Œï¼ŒICLç¤ºä¾‹å­˜åœ¨ä¸¤ä¸ªå…³é”®é—®é¢˜ï¼šç²’åº¦ä¸åŒ¹é…å’Œè´Ÿé¢æ•ˆåº”å™ªå£°é—®é¢˜ã€‚è¿™äº›é—®é¢˜ä¼šå½±å“æ¨¡å‹çš„æ¨ç†è´¨é‡ã€‚ä½œè€…æå‡ºäº†BoostStepæ–¹æ³•ï¼Œé€šè¿‡æ­¥éª¤ç²’åº¦çš„å¯¹é½å’Œâ€œé¦–æ¬¡å°è¯•â€ç­–ç•¥æä¾›ç›¸å…³çš„ICLç¤ºä¾‹ï¼Œæé«˜æ¨ç†è´¨é‡ã€‚BoostStepè¿˜å¯ä»¥ä¸è’™ç‰¹å¡ç½—æ ‘æœç´¢æ–¹æ³•ï¼ˆMCTSï¼‰ç»“åˆä½¿ç”¨ï¼Œæå‡æ¨¡å‹æ€§èƒ½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒBoostStepæ˜¾è‘—æé«˜äº†GPT-4oå’ŒQwen2.5-Math-72Bçš„è¡¨ç°ã€‚
[07.01.2025 09:11] Mistral response. {"id": "a573b9ab7eb349d69817894343cf6cd4", "object": "chat.completion", "created": 1736241091, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "\u8fd9\u7bc7\u6587\u7ae0\u8ba8\u8bba\u4e86\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u89e3\u51b3\u590d\u6742\u6570\u5b66\u95ee\u9898\u65f6\u7684\u8868\u73b0\u3002\u6a21\u578b\u4f7f\u7528\u5206\u800c\u6cbb\u4e4b\u7684\u65b9\u6cd5\u548c\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u793a\u4f8b\u3002\u7136\u800c\uff0cICL\u793a\u4f8b\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u95ee\u9898\uff1a\u7c92\u5ea6\u4e0d\u5339\u914d\u548c\u8d1f\u9762\u6548\u5e94\u566a\u58f0\u95ee\u9898\u3002\u8fd9\u4e9b\u95ee\u9898\u4f1a\u5f71\u54cd\u6a21\u578b\u7684\u63a8\u7406\u8d28\u91cf\u3002\u4f5c\u8005\u63d0\u51fa\u4e86BoostStep\u65b9\u6cd5\uff0c\u901a\u8fc7\u6b65\u9aa4\u7c92\u5ea6\u7684\u5bf9\u9f50\u548c\u201c\u9996\u6b21\u5c1d\u8bd5\u201d\u7b56\u7565\u63d0\u4f9b\u76f8\u5173\u7684ICL\u793a\u4f8b\uff0c\u63d0\u9ad8\u63a8\u7406\u8d28\u91cf\u3002BoostStep\u8fd8\u53ef\u4ee5\u4e0e\u8499\u7279\u5361\u7f57\u6811\u641c\u7d22\u65b9\u6cd5\uff08MCTS\uff09\u7ed3\u5408\u4f7f\u7528\uff0c\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cBoostStep\u663e\u8457\u63d0\u9ad8\u4e86GPT-4o\u548cQwen2.5-Math-72B\u7684\u8868\u73b0\u3002\n\nZh\u00e8 pi\u0101n w\u00e9nzh\u0101ng t\u01ceol\u00f9n le d\u00e0 y\u01d4y\u00e1n m\u00f3x\u00edng (LLMs) z\u00e0i ji\u011bju\u00e9 f\u00f9z\u00e1 x\u00f9exu\u00e9 w\u00e8nt\u00ed sh\u00ed de bi\u01ceoxi\u00e0n. M\u00f3x\u00edng sh\u01d0y\u00f2ng f\u0113n \u00e9r zh\u00eczh\u012b de f\u0101ngf\u01ce h\u00e9 sh\u00e0ngxi\u00e0w\u00e9n xu\u00e9x\u00ed (ICL) sh\u00ecl\u00ec. R\u00e1n'\u00e9r, ICL sh\u00ecl\u00ec c\u00fanz\u00e0i li\u01ceng g\u00e8 gu\u01cenji\u00e0n w\u00e8nt\u00ed: l\u00ecd\u00f9 b\u00f9 p\u01d0p\u011bi h\u00e9 f\u00f9mi\u00e0n xi\u00e0oy\u00ecng z\u00e0osh\u0113ng w\u00e8nt\u00ed. Zh\u00e8xi\u0113 w\u00e8nt\u00ed hu\u00ec y\u01d0ngxi\u01ceng m\u00f3x\u00edng de tu\u012bl\u01d0 zh\u00ecli\u00e0ng. Zu\u00f2zh\u011b t\u00edch\u016b le BoostStep f\u0101ngf\u01ce, t\u014dnggu\u00f2 b\u00f9zh\u00f2u l\u00ecd\u00f9 de du\u00ecq\u00ed h\u00e9 \u201csh\u01d2uc\u00ec ch\u00e1ngsh\u00ec\u201d c\u00e8l\u00fc\u00e8 t\u00edg\u014dng xi\u0101nggu\u0101n de ICL sh\u00ecl\u00ec, t\u00edg\u0101o tu\u012bl\u01d0 zh\u00ecli\u00e0ng. BoostStep h\u00e1i k\u011by\u01d0 y\u01d4 m\u00e9ngt\u00e8k\u01celu\u00f3 sh\u00f9 s\u014dusu\u01d2 f\u0101ngf\u01ce (MCTS) ji\u00e9h\u00e9 sh\u01d0y\u00f2ng, t\u00edsh\u0113ng m\u00f3x\u00edng x\u00ecngn\u00e9ng. Sh\u00edy\u00e0n ji\u00e9gu\u01d2 xi\u01censh\u00ec, BoostStep xi\u01cenzh\u00f9 t\u00edg\u0101o le GPT-4o h\u00e9 Qwen2.5-Math-72B de bi\u01ceoxi\u00e0n."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 243, "total_tokens": 894, "completion_tokens": 651}}
[07.01.2025 09:11] Response: è¿™ç¯‡æ–‡ç« è®¨è®ºäº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è§£å†³å¤æ‚æ•°å­¦é—®é¢˜æ—¶çš„è¡¨ç°ã€‚æ¨¡å‹ä½¿ç”¨åˆ†è€Œæ²»ä¹‹çš„æ–¹æ³•å’Œä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰ç¤ºä¾‹ã€‚ç„¶è€Œï¼ŒICLç¤ºä¾‹å­˜åœ¨ä¸¤ä¸ªå…³é”®é—®é¢˜ï¼šç²’åº¦ä¸åŒ¹é…å’Œè´Ÿé¢æ•ˆåº”å™ªå£°é—®é¢˜ã€‚è¿™äº›é—®é¢˜ä¼šå½±å“æ¨¡å‹çš„æ¨ç†è´¨é‡ã€‚ä½œè€…æå‡ºäº†BoostStepæ–¹æ³•ï¼Œé€šè¿‡æ­¥éª¤ç²’åº¦çš„å¯¹é½å’Œâ€œé¦–æ¬¡å°è¯•â€ç­–ç•¥æä¾›ç›¸å…³çš„ICLç¤ºä¾‹ï¼Œæé«˜æ¨ç†è´¨é‡ã€‚BoostStepè¿˜å¯ä»¥ä¸è’™ç‰¹å¡ç½—æ ‘æœç´¢æ–¹æ³•ï¼ˆMCTSï¼‰ç»“åˆä½¿ç”¨ï¼Œæå‡æ¨¡å‹æ€§èƒ½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒBoostStepæ˜¾è‘—æé«˜äº†GPT-4oå’ŒQwen2.5-Math-72Bçš„è¡¨ç°ã€‚

ZhÃ¨ piÄn wÃ©nzhÄng tÇolÃ¹n le dÃ  yÇ”yÃ¡n mÃ³xÃ­ng (LLMs) zÃ i jiÄ›juÃ© fÃ¹zÃ¡ xÃ¹exuÃ© wÃ¨ntÃ­ shÃ­ de biÇoxiÃ n. MÃ³xÃ­ng shÇyÃ²ng fÄ“n Ã©r zhÃ¬zhÄ« de fÄngfÇ hÃ© shÃ ngxiÃ wÃ©n xuÃ©xÃ­ (ICL) shÃ¬lÃ¬. RÃ¡n'Ã©r, ICL shÃ¬lÃ¬ cÃºnzÃ i liÇng gÃ¨ guÇnjiÃ n wÃ¨ntÃ­: lÃ¬dÃ¹ bÃ¹ pÇpÄ›i hÃ© fÃ¹miÃ n xiÃ oyÃ¬ng zÃ oshÄ“ng wÃ¨ntÃ­. ZhÃ¨xiÄ“ wÃ¨ntÃ­ huÃ¬ yÇngxiÇng mÃ³xÃ­ng de tuÄ«lÇ zhÃ¬liÃ ng. ZuÃ²zhÄ› tÃ­chÅ« le BoostStep fÄngfÇ, tÅngguÃ² bÃ¹zhÃ²u lÃ¬dÃ¹ de duÃ¬qÃ­ hÃ© â€œshÇ’ucÃ¬ chÃ¡ngshÃ¬â€ cÃ¨lÃ¼Ã¨ tÃ­gÅng xiÄngguÄn de ICL shÃ¬lÃ¬, tÃ­gÄo tuÄ«lÇ zhÃ¬liÃ ng. BoostStep hÃ¡i kÄ›yÇ yÇ” mÃ©ngtÃ¨kÇluÃ³ shÃ¹ sÅusuÇ’ fÄngfÇ (MCTS) jiÃ©hÃ© shÇyÃ²ng, tÃ­shÄ“ng mÃ³xÃ­ng xÃ¬ngnÃ©ng. ShÃ­yÃ n jiÃ©guÇ’ xiÇnshÃ¬, BoostStep xiÇnzhÃ¹ tÃ­gÄo le GPT-4o hÃ© Qwen2.5-Math-72B de biÇoxiÃ n.
[07.01.2025 09:11] Mistral request. Model: mistral-large-latest. Prompt: Write vocab of difficult words for this text as an array of objects with fields 'word', 'pinyin', 'trans'. Return as python list without formatting. Return list and nothing else. Text:

è¿™ç¯‡æ–‡ç« è®¨è®ºäº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è§£å†³å¤æ‚æ•°å­¦é—®é¢˜æ—¶çš„è¡¨ç°ã€‚æ¨¡å‹ä½¿ç”¨åˆ†è€Œæ²»ä¹‹çš„æ–¹æ³•å’Œä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰ç¤ºä¾‹ã€‚ç„¶è€Œï¼ŒICLç¤ºä¾‹å­˜åœ¨ä¸¤ä¸ªå…³é”®é—®é¢˜ï¼šç²’åº¦ä¸åŒ¹é…å’Œè´Ÿé¢æ•ˆåº”å™ªå£°é—®é¢˜ã€‚è¿™äº›é—®é¢˜ä¼šå½±å“æ¨¡å‹çš„æ¨ç†è´¨é‡ã€‚ä½œè€…æå‡ºäº†BoostStepæ–¹æ³•ï¼Œé€šè¿‡æ­¥éª¤ç²’åº¦çš„å¯¹é½å’Œâ€œé¦–æ¬¡å°è¯•â€ç­–ç•¥æä¾›ç›¸å…³çš„ICLç¤ºä¾‹ï¼Œæé«˜æ¨ç†è´¨é‡ã€‚BoostStepè¿˜å¯ä»¥ä¸è’™ç‰¹å¡ç½—æ ‘æœç´¢æ–¹æ³•ï¼ˆMCTSï¼‰ç»“åˆä½¿ç”¨ï¼Œæå‡æ¨¡å‹æ€§èƒ½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒBoostStepæ˜¾è‘—æé«˜äº†GPT-4oå’ŒQwen2.5-Math-72Bçš„è¡¨ç°ã€‚
[07.01.2025 09:12] Mistral response. {"id": "ffb98a4e399f48ed905c0d9f1230a179", "object": "chat.completion", "created": 1736241104, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "[{'word': '\u8ba8\u8bba', 'pinyin': 't\u01ceo l\u00f9n', 'trans': 'discuss'}, {'word': '\u5927\u8bed\u8a00\u6a21\u578b', 'pinyin': 'd\u00e0 y\u01d4 y\u00e1n m\u00f3 x\u00edng', 'trans': 'large language model'}, {'word': '\u590d\u6742', 'pinyin': 'f\u00f9 z\u00e1', 'trans': 'complex'}, {'word': '\u6570\u5b66', 'pinyin': 'sh\u00f9 xu\u00e9', 'trans': 'mathematics'}, {'word': '\u8868\u73b0', 'pinyin': 'bi\u01ceo xi\u00e0n', 'trans': 'performance'}, {'word': '\u5206\u800c\u6cbb\u4e4b', 'pinyin': 'f\u0113n \u00e9r zh\u00ec zh\u012b', 'trans': 'divide and conquer'}, {'word': '\u4e0a\u4e0b\u6587\u5b66\u4e60', 'pinyin': 'sh\u00e0ng xi\u00e0 w\u00e9n xu\u00e9 x\u00ed', 'trans': 'in-context learning'}, {'word': '\u793a\u4f8b', 'pinyin': 'sh\u00ec l\u00ec', 'trans': 'example'}, {'word': '\u5173\u952e', 'pinyin': 'gu\u01cen ji\u00e0n', 'trans': 'key'}, {'word': '\u7c92\u5ea6', 'pinyin': 'l\u00ec d\u00f9', 'trans': 'granularity'}, {'word': '\u8d1f\u9762\u6548\u5e94', 'pinyin': 'f\u00f9 mi\u00e0n xi\u00e0o y\u00ecng', 'trans': 'negative effect'}, {'word': '\u566a\u58f0', 'pinyin': 'z\u00e0o sh\u0113ng', 'trans': 'noise'}, {'word': '\u63a8\u7406', 'pinyin': 'tu\u012b l\u01d0', 'trans': 'reasoning'}, {'word': '\u8d28\u91cf', 'pinyin': 'zh\u00ec li\u00e0ng', 'trans': 'quality'}, {'word': '\u63d0\u51fa', 'pinyin': 't\u00ed ch\u016b', 'trans': 'propose'}, {'word': 'BoostStep', 'pinyin': 'BoostStep', 'trans': 'BoostStep'}, {'word': '\u5bf9\u9f50', 'pinyin': 'du\u00ec q\u00ed', 'trans': 'align'}, {'word': '\u7b56\u7565', 'pinyin': 'c\u00e8 l\u00fc\u00e8', 'trans': 'strategy'}, {'word': '\u76f8\u5173', 'pinyin': 'xi\u0101ng gu\u0101n', 'trans': 'relevant'}, {'word': '\u63d0\u9ad8', 'pinyin': 't\u00ed g\u0101o', 'trans': 'improve'}, {'word': '\u8499\u7279\u5361\u7f57\u6811\u641c\u7d22\u65b9\u6cd5', 'pinyin': 'm\u00e9ng t\u00e8 k\u01ce lu\u00f3 sh\u00f9 s\u014du su\u01d2 f\u0101ng f\u01ce', 'trans': 'Monte Carlo Tree Search method'}, {'word': '\u7ed3\u5408', 'pinyin': 'ji\u00e9 h\u00e9', 'trans': 'combine'}, {'word': '\u4f7f\u7528', 'pinyin': 'sh\u01d0 y\u00f2ng', 'trans': 'use'}, {'word': '\u63d0\u5347', 'pinyin': 't\u00ed sh\u0113ng', 'trans': 'enhance'}, {'word': '\u5b9e\u9a8c', 'pinyin': 'sh\u00ed y\u00e0n', 'trans': 'experiment'}, {'word': '\u7ed3\u679c', 'pinyin': 'ji\u00e9 gu\u01d2', 'trans': 'result'}, {'word': '\u663e\u8457', 'pinyin': 'xi\u01cen zh\u00f9', 'trans': 'significant'}, {'word': 'GPT-4o', 'pinyin': 'GPT-4o', 'trans': 'GPT-4o'}, {'word': 'Qwen2.5-Math-72B', 'pinyin': 'Qwen2.5-Math-72B', 'trans': 'Qwen2.5-Math-72B'}]"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 275, "total_tokens": 1132, "completion_tokens": 857}}
[07.01.2025 09:12] Response: [{'word': 'è®¨è®º', 'pinyin': 'tÇo lÃ¹n', 'trans': 'discuss'}, {'word': 'å¤§è¯­è¨€æ¨¡å‹', 'pinyin': 'dÃ  yÇ” yÃ¡n mÃ³ xÃ­ng', 'trans': 'large language model'}, {'word': 'å¤æ‚', 'pinyin': 'fÃ¹ zÃ¡', 'trans': 'complex'}, {'word': 'æ•°å­¦', 'pinyin': 'shÃ¹ xuÃ©', 'trans': 'mathematics'}, {'word': 'è¡¨ç°', 'pinyin': 'biÇo xiÃ n', 'trans': 'performance'}, {'word': 'åˆ†è€Œæ²»ä¹‹', 'pinyin': 'fÄ“n Ã©r zhÃ¬ zhÄ«', 'trans': 'divide and conquer'}, {'word': 'ä¸Šä¸‹æ–‡å­¦ä¹ ', 'pinyin': 'shÃ ng xiÃ  wÃ©n xuÃ© xÃ­', 'trans': 'in-context learning'}, {'word': 'ç¤ºä¾‹', 'pinyin': 'shÃ¬ lÃ¬', 'trans': 'example'}, {'word': 'å…³é”®', 'pinyin': 'guÇn jiÃ n', 'trans': 'key'}, {'word': 'ç²’åº¦', 'pinyin': 'lÃ¬ dÃ¹', 'trans': 'granularity'}, {'word': 'è´Ÿé¢æ•ˆåº”', 'pinyin': 'fÃ¹ miÃ n xiÃ o yÃ¬ng', 'trans': 'negative effect'}, {'word': 'å™ªå£°', 'pinyin': 'zÃ o shÄ“ng', 'trans': 'noise'}, {'word': 'æ¨ç†', 'pinyin': 'tuÄ« lÇ', 'trans': 'reasoning'}, {'word': 'è´¨é‡', 'pinyin': 'zhÃ¬ liÃ ng', 'trans': 'quality'}, {'word': 'æå‡º', 'pinyin': 'tÃ­ chÅ«', 'trans': 'propose'}, {'word': 'BoostStep', 'pinyin': 'BoostStep', 'trans': 'BoostStep'}, {'word': 'å¯¹é½', 'pinyin': 'duÃ¬ qÃ­', 'trans': 'align'}, {'word': 'ç­–ç•¥', 'pinyin': 'cÃ¨ lÃ¼Ã¨', 'trans': 'strategy'}, {'word': 'ç›¸å…³', 'pinyin': 'xiÄng guÄn', 'trans': 'relevant'}, {'word': 'æé«˜', 'pinyin': 'tÃ­ gÄo', 'trans': 'improve'}, {'word': 'è’™ç‰¹å¡ç½—æ ‘æœç´¢æ–¹æ³•', 'pinyin': 'mÃ©ng tÃ¨ kÇ luÃ³ shÃ¹ sÅu suÇ’ fÄng fÇ', 'trans': 'Monte Carlo Tree Search method'}, {'word': 'ç»“åˆ', 'pinyin': 'jiÃ© hÃ©', 'trans': 'combine'}, {'word': 'ä½¿ç”¨', 'pinyin': 'shÇ yÃ²ng', 'trans': 'use'}, {'word': 'æå‡', 'pinyin': 'tÃ­ shÄ“ng', 'trans': 'enhance'}, {'word': 'å®éªŒ', 'pinyin': 'shÃ­ yÃ n', 'trans': 'experiment'}, {'word': 'ç»“æœ', 'pinyin': 'jiÃ© guÇ’', 'trans': 'result'}, {'word': 'æ˜¾è‘—', 'pinyin': 'xiÇn zhÃ¹', 'trans': 'significant'}, {'word': 'GPT-4o', 'pinyin': 'GPT-4o', 'trans': 'GPT-4o'}, {'word': 'Qwen2.5-Math-72B', 'pinyin': 'Qwen2.5-Math-72B', 'trans': 'Qwen2.5-Math-72B'}]
[07.01.2025 09:12] Mistral request. Model: mistral-large-latest. Prompt: Translate this text in English. Text:

è¿™ç¯‡æ–‡ç« è®¨è®ºäº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è§£å†³å¤æ‚æ•°å­¦é—®é¢˜æ—¶çš„è¡¨ç°ã€‚æ¨¡å‹ä½¿ç”¨åˆ†è€Œæ²»ä¹‹çš„æ–¹æ³•å’Œä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰ç¤ºä¾‹ã€‚ç„¶è€Œï¼ŒICLç¤ºä¾‹å­˜åœ¨ä¸¤ä¸ªå…³é”®é—®é¢˜ï¼šç²’åº¦ä¸åŒ¹é…å’Œè´Ÿé¢æ•ˆåº”å™ªå£°é—®é¢˜ã€‚è¿™äº›é—®é¢˜ä¼šå½±å“æ¨¡å‹çš„æ¨ç†è´¨é‡ã€‚ä½œè€…æå‡ºäº†BoostStepæ–¹æ³•ï¼Œé€šè¿‡æ­¥éª¤ç²’åº¦çš„å¯¹é½å’Œâ€œé¦–æ¬¡å°è¯•â€ç­–ç•¥æä¾›ç›¸å…³çš„ICLç¤ºä¾‹ï¼Œæé«˜æ¨ç†è´¨é‡ã€‚BoostStepè¿˜å¯ä»¥ä¸è’™ç‰¹å¡ç½—æ ‘æœç´¢æ–¹æ³•ï¼ˆMCTSï¼‰ç»“åˆä½¿ç”¨ï¼Œæå‡æ¨¡å‹æ€§èƒ½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒBoostStepæ˜¾è‘—æé«˜äº†GPT-4oå’ŒQwen2.5-Math-72Bçš„è¡¨ç°ã€‚
[07.01.2025 09:12] Mistral response. {"id": "36ab705ed1304566b6393f2bfecc936b", "object": "chat.completion", "created": 1736241122, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "This article discusses the performance of large language models (LLMs) in solving complex mathematical problems. The models use a divide-and-conquer approach and in-context learning (ICL) examples. However, ICL examples face two key issues: granularity mismatch and negative impact noise problems. These issues affect the quality of the model's reasoning. The authors propose the BoostStep method, which aligns step granularity and provides relevant ICL examples through a \"first attempt\" strategy, thereby improving reasoning quality. BoostStep can also be combined with the Monte Carlo Tree Search (MCTS) method to enhance model performance. Experimental results show that BoostStep significantly improves the performance of GPT-4o and Qwen2.5-Math-72B."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 241, "total_tokens": 407, "completion_tokens": 166}}
[07.01.2025 09:12] Response: This article discusses the performance of large language models (LLMs) in solving complex mathematical problems. The models use a divide-and-conquer approach and in-context learning (ICL) examples. However, ICL examples face two key issues: granularity mismatch and negative impact noise problems. These issues affect the quality of the model's reasoning. The authors propose the BoostStep method, which aligns step granularity and provides relevant ICL examples through a "first attempt" strategy, thereby improving reasoning quality. BoostStep can also be combined with the Monte Carlo Tree Search (MCTS) method to enhance model performance. Experimental results show that BoostStep significantly improves the performance of GPT-4o and Qwen2.5-Math-72B.
[07.01.2025 09:12] Renaming data file.
[07.01.2025 09:12] Renaming previous data. hf_papers.json to ./d/2025-01-07.json
[07.01.2025 09:12] Saving new data file.
[07.01.2025 09:12] Generating page.
[07.01.2025 09:12] Renaming previous page.
[07.01.2025 09:12] Renaming previous data. index.html to ./d/2025-01-07.html
[07.01.2025 09:12] [Experimental] Generating Chinese page for reading.
[07.01.2025 09:12] Chinese vocab [{'word': 'è®¨è®º', 'pinyin': 'tÇo lÃ¹n', 'trans': 'discuss'}, {'word': 'å¤§è¯­è¨€æ¨¡å‹', 'pinyin': 'dÃ  yÇ” yÃ¡n mÃ³ xÃ­ng', 'trans': 'large language model'}, {'word': 'å¤æ‚', 'pinyin': 'fÃ¹ zÃ¡', 'trans': 'complex'}, {'word': 'æ•°å­¦', 'pinyin': 'shÃ¹ xuÃ©', 'trans': 'mathematics'}, {'word': 'è¡¨ç°', 'pinyin': 'biÇo xiÃ n', 'trans': 'performance'}, {'word': 'åˆ†è€Œæ²»ä¹‹', 'pinyin': 'fÄ“n Ã©r zhÃ¬ zhÄ«', 'trans': 'divide and conquer'}, {'word': 'ä¸Šä¸‹æ–‡å­¦ä¹ ', 'pinyin': 'shÃ ng xiÃ  wÃ©n xuÃ© xÃ­', 'trans': 'in-context learning'}, {'word': 'ç¤ºä¾‹', 'pinyin': 'shÃ¬ lÃ¬', 'trans': 'example'}, {'word': 'å…³é”®', 'pinyin': 'guÇn jiÃ n', 'trans': 'key'}, {'word': 'ç²’åº¦', 'pinyin': 'lÃ¬ dÃ¹', 'trans': 'granularity'}, {'word': 'è´Ÿé¢æ•ˆåº”', 'pinyin': 'fÃ¹ miÃ n xiÃ o yÃ¬ng', 'trans': 'negative effect'}, {'word': 'å™ªå£°', 'pinyin': 'zÃ o shÄ“ng', 'trans': 'noise'}, {'word': 'æ¨ç†', 'pinyin': 'tuÄ« lÇ', 'trans': 'reasoning'}, {'word': 'è´¨é‡', 'pinyin': 'zhÃ¬ liÃ ng', 'trans': 'quality'}, {'word': 'æå‡º', 'pinyin': 'tÃ­ chÅ«', 'trans': 'propose'}, {'word': 'BoostStep', 'pinyin': 'BoostStep', 'trans': 'BoostStep'}, {'word': 'å¯¹é½', 'pinyin': 'duÃ¬ qÃ­', 'trans': 'align'}, {'word': 'ç­–ç•¥', 'pinyin': 'cÃ¨ lÃ¼Ã¨', 'trans': 'strategy'}, {'word': 'ç›¸å…³', 'pinyin': 'xiÄng guÄn', 'trans': 'relevant'}, {'word': 'æé«˜', 'pinyin': 'tÃ­ gÄo', 'trans': 'improve'}, {'word': 'è’™ç‰¹å¡ç½—æ ‘æœç´¢æ–¹æ³•', 'pinyin': 'mÃ©ng tÃ¨ kÇ luÃ³ shÃ¹ sÅu suÇ’ fÄng fÇ', 'trans': 'Monte Carlo Tree Search method'}, {'word': 'ç»“åˆ', 'pinyin': 'jiÃ© hÃ©', 'trans': 'combine'}, {'word': 'ä½¿ç”¨', 'pinyin': 'shÇ yÃ²ng', 'trans': 'use'}, {'word': 'æå‡', 'pinyin': 'tÃ­ shÄ“ng', 'trans': 'enhance'}, {'word': 'å®éªŒ', 'pinyin': 'shÃ­ yÃ n', 'trans': 'experiment'}, {'word': 'ç»“æœ', 'pinyin': 'jiÃ© guÇ’', 'trans': 'result'}, {'word': 'æ˜¾è‘—', 'pinyin': 'xiÇn zhÃ¹', 'trans': 'significant'}, {'word': 'GPT-4o', 'pinyin': 'GPT-4o', 'trans': 'GPT-4o'}, {'word': 'Qwen2.5-Math-72B', 'pinyin': 'Qwen2.5-Math-72B', 'trans': 'Qwen2.5-Math-72B'}]
[07.01.2025 09:12] Renaming previous Chinese page.
[07.01.2025 09:12] Renaming previous data. zh.html to ./d/2025-01-06_zh_reading_task.html
[07.01.2025 09:12] Writing Chinese reading task.
[07.01.2025 09:12] Writing result.
[07.01.2025 09:12] Renaming log file.
[07.01.2025 09:12] Renaming previous data. log.txt to ./logs/2025-01-07_last_log.txt
