[07.01.2025 12:18] Read previous papers.
[07.01.2025 12:18] Generating top page (month).
[07.01.2025 12:18] Writing top page (month).
[07.01.2025 13:18] Read previous papers.
[07.01.2025 13:18] Get feed.
[07.01.2025 13:18] Get page data from previous paper. URL: https://huggingface.co/papers/2501.02976
[07.01.2025 13:18] Get page data from previous paper. URL: https://huggingface.co/papers/2501.03226
[07.01.2025 13:18] Get page data from previous paper. URL: https://huggingface.co/papers/2501.03218
[07.01.2025 13:18] Get page data from previous paper. URL: https://huggingface.co/papers/2501.02157
[07.01.2025 13:18] Get page data from previous paper. URL: https://huggingface.co/papers/2501.02497
[07.01.2025 13:18] Get page data from previous paper. URL: https://huggingface.co/papers/2501.02045
[07.01.2025 13:18] Get page data from previous paper. URL: https://huggingface.co/papers/2501.03006
[07.01.2025 13:18] Get page data from previous paper. URL: https://huggingface.co/papers/2501.02690
[07.01.2025 13:18] Get page data from previous paper. URL: https://huggingface.co/papers/2501.01790
[07.01.2025 13:18] Get page data from previous paper. URL: https://huggingface.co/papers/2501.02576
[07.01.2025 13:18] Get page data from previous paper. URL: https://huggingface.co/papers/2501.03059
[07.01.2025 13:18] Get page data from previous paper. URL: https://huggingface.co/papers/2501.01830
[07.01.2025 13:18] Get page data from previous paper. URL: https://huggingface.co/papers/2501.02506
[07.01.2025 13:18] Get page data from previous paper. URL: https://huggingface.co/papers/2501.02832
[07.01.2025 13:18] Extract page data from URL. URL: https://huggingface.co/papers/2501.02423
[07.01.2025 13:18] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[07.01.2025 13:18] No deleted papers detected.
[07.01.2025 13:18] Downloading and parsing papers (pdf, html). Total: 15.
[07.01.2025 13:18] Downloading and parsing paper https://huggingface.co/papers/2501.02976.
[07.01.2025 13:18] Extra JSON file exists (./assets/json/2501.02976.json), skip PDF parsing.
[07.01.2025 13:18] Paper image links file exists (./assets/img_data/2501.02976.json), skip HTML parsing.
[07.01.2025 13:18] Success.
[07.01.2025 13:18] Downloading and parsing paper https://huggingface.co/papers/2501.03226.
[07.01.2025 13:18] Extra JSON file exists (./assets/json/2501.03226.json), skip PDF parsing.
[07.01.2025 13:18] Paper image links file exists (./assets/img_data/2501.03226.json), skip HTML parsing.
[07.01.2025 13:18] Success.
[07.01.2025 13:18] Downloading and parsing paper https://huggingface.co/papers/2501.03218.
[07.01.2025 13:18] Extra JSON file exists (./assets/json/2501.03218.json), skip PDF parsing.
[07.01.2025 13:18] Paper image links file exists (./assets/img_data/2501.03218.json), skip HTML parsing.
[07.01.2025 13:18] Success.
[07.01.2025 13:18] Downloading and parsing paper https://huggingface.co/papers/2501.02157.
[07.01.2025 13:18] Extra JSON file exists (./assets/json/2501.02157.json), skip PDF parsing.
[07.01.2025 13:18] Paper image links file exists (./assets/img_data/2501.02157.json), skip HTML parsing.
[07.01.2025 13:18] Success.
[07.01.2025 13:18] Downloading and parsing paper https://huggingface.co/papers/2501.02497.
[07.01.2025 13:18] Extra JSON file exists (./assets/json/2501.02497.json), skip PDF parsing.
[07.01.2025 13:18] Paper image links file exists (./assets/img_data/2501.02497.json), skip HTML parsing.
[07.01.2025 13:18] Success.
[07.01.2025 13:18] Downloading and parsing paper https://huggingface.co/papers/2501.02045.
[07.01.2025 13:18] Extra JSON file exists (./assets/json/2501.02045.json), skip PDF parsing.
[07.01.2025 13:18] Paper image links file exists (./assets/img_data/2501.02045.json), skip HTML parsing.
[07.01.2025 13:18] Success.
[07.01.2025 13:18] Downloading and parsing paper https://huggingface.co/papers/2501.03006.
[07.01.2025 13:18] Extra JSON file exists (./assets/json/2501.03006.json), skip PDF parsing.
[07.01.2025 13:18] Paper image links file exists (./assets/img_data/2501.03006.json), skip HTML parsing.
[07.01.2025 13:18] Success.
[07.01.2025 13:18] Downloading and parsing paper https://huggingface.co/papers/2501.02690.
[07.01.2025 13:18] Extra JSON file exists (./assets/json/2501.02690.json), skip PDF parsing.
[07.01.2025 13:18] Paper image links file exists (./assets/img_data/2501.02690.json), skip HTML parsing.
[07.01.2025 13:18] Success.
[07.01.2025 13:18] Downloading and parsing paper https://huggingface.co/papers/2501.01790.
[07.01.2025 13:18] Extra JSON file exists (./assets/json/2501.01790.json), skip PDF parsing.
[07.01.2025 13:18] Paper image links file exists (./assets/img_data/2501.01790.json), skip HTML parsing.
[07.01.2025 13:18] Success.
[07.01.2025 13:18] Downloading and parsing paper https://huggingface.co/papers/2501.02576.
[07.01.2025 13:18] Extra JSON file exists (./assets/json/2501.02576.json), skip PDF parsing.
[07.01.2025 13:18] Paper image links file exists (./assets/img_data/2501.02576.json), skip HTML parsing.
[07.01.2025 13:18] Success.
[07.01.2025 13:18] Downloading and parsing paper https://huggingface.co/papers/2501.03059.
[07.01.2025 13:18] Extra JSON file exists (./assets/json/2501.03059.json), skip PDF parsing.
[07.01.2025 13:18] Paper image links file exists (./assets/img_data/2501.03059.json), skip HTML parsing.
[07.01.2025 13:18] Success.
[07.01.2025 13:18] Downloading and parsing paper https://huggingface.co/papers/2501.01830.
[07.01.2025 13:18] Extra JSON file exists (./assets/json/2501.01830.json), skip PDF parsing.
[07.01.2025 13:18] Paper image links file exists (./assets/img_data/2501.01830.json), skip HTML parsing.
[07.01.2025 13:18] Success.
[07.01.2025 13:18] Downloading and parsing paper https://huggingface.co/papers/2501.02506.
[07.01.2025 13:18] Extra JSON file exists (./assets/json/2501.02506.json), skip PDF parsing.
[07.01.2025 13:18] Paper image links file exists (./assets/img_data/2501.02506.json), skip HTML parsing.
[07.01.2025 13:18] Success.
[07.01.2025 13:18] Downloading and parsing paper https://huggingface.co/papers/2501.02832.
[07.01.2025 13:18] Extra JSON file exists (./assets/json/2501.02832.json), skip PDF parsing.
[07.01.2025 13:18] Paper image links file exists (./assets/img_data/2501.02832.json), skip HTML parsing.
[07.01.2025 13:18] Success.
[07.01.2025 13:18] Downloading and parsing paper https://huggingface.co/papers/2501.02423.
[07.01.2025 13:18] Downloading paper 2501.02423 from http://arxiv.org/pdf/2501.02423v1...
[07.01.2025 13:18] Extracting affiliations from text.
[07.01.2025 13:18] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Scaling Laws for FloatingPoint Quantization Training Xingwu Sun * 1 2 Shuaipeng Li * 1 Ruobing Xie 1 Weidong Han 1 Kan Wu 1 Zhen Yang 1 Yixing Li 1 3 An Wang 1 4 Shuai Li 1 Jinbao Xue 1 Yu Cheng 3 Yangyu Tao 1 Zhanhui Kang 1 Chengzhong Xu 2 Di Wang 1 Jie Jiang 1 5 2 0 2 5 ] . [ 1 3 2 4 2 0 . 1 0 5 2 : r a "
[07.01.2025 13:18] Response: ```python
[]
```
[07.01.2025 13:18] Extracting affiliations from text.
[07.01.2025 13:18] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Scaling Laws for FloatingPoint Quantization Training Xingwu Sun * 1 2 Shuaipeng Li * 1 Ruobing Xie 1 Weidong Han 1 Kan Wu 1 Zhen Yang 1 Yixing Li 1 3 An Wang 1 4 Shuai Li 1 Jinbao Xue 1 Yu Cheng 3 Yangyu Tao 1 Zhanhui Kang 1 Chengzhong Xu 2 Di Wang 1 Jie Jiang 1 5 2 0 2 5 ] . [ 1 3 2 4 2 0 . 1 0 5 2 : r aLow-precision training is considered an effective strategy for reducing both training and downstream inference costs. Previous scaling laws for precision mainly focus on integer quantization, which pay less attention to the constituents in floating-point quantization and thus cannot well fit the LLM losses in this scenario. In contrast, while floating-point quantization training is more commonly implemented in production, the research on it has been relatively superficial. In this paper, we thoroughly explore the effects of floating-point quantization targets, exponent bits, mantissa bits, and the calculation granularity of the scaling factor in floating-point quantization training performance of LLM models. While presenting an accurate floating-point quantization unified scaling law, we also provide valuable suggestions for the community: (1) Exponent bits contribute slightly more to the model performance than mantissa bits. We provide the optimal exponent-mantissa bit ratio for different bit numbers, which is available for future reference by hardware manufacturers; (2) We discover the formation of the critical data size in low-precision LLM training. Too much training data exceeding the critical data size will inversely bring in degradation of LLM performance; (3) The optimal floating-point quantization precision is directly proportional to the computational power, but within wide computational power range, we estimate that the best cost-performance precision lies between 4-8 bits. 1. Introduction The various scaling laws of large language models (LLMs) could help developers effectively select superior parame- *Equal contribution Corresponding author 1Tencent Hunyuan 2University of Macau 3The Chinese University of Hong Kong 4Tokyo Institute of Technology. ter settings before experiments and accurately predict the model performance under different configurations, which are regarded as excellent guidance in LLM training. The widely acknowledged scaling law efforts such as Kaplan et al. (2020), Hoffmann et al. (2022), and Li et al. (2024) mainly concentrate on the central factors, i.e., model size and trained token size, which significantly impact the performance of LLMs. With the rapid growth of both model and data sizes, there has been increasing attention to the efficiency and cost of LLM training. Training and serving with lower precision becomes popular solution. Currently, lots of representative LLMs are trained in BF16 and even lower precision (Dubey et al., 2024; Sun et al., 2024; Liu et al., 2024; Yang et al., 2024; Ma et al., 2024; Wang et al., 2023; Peng et al., 2023), aiming to balance effectiveness and efficiency. Compared to integer quantization, floatingpoint (FP) quantization can better maintain LLMs accuracy at extremely lower bit rates and thus is often equipped in low-precision LLMs. Therefore, exploring the scaling laws of LLM performance under different low precision settings with floating-point quantization becomes essential to shed light on future low-precision LLM training. Recently, there is pioneer work that conducts in-depth analyses and explorations on the LLMs scaling laws for precision in both training and inference (Kumar et al., 2024), quantitatively measuring the degradation rules of post-train quantization and quantized training. This scaling law provides an appropriate conclusion explaining the potential damage of excessively increasing training data to low-precision LLMs performance. However, Kumar et al. (2024) directly adopts the bit width as the precision in its low-precision scaling laws, which may lose finer-grained modeling of the relationship between various parameter settings related to the FP quantization and the final loss of LLMs. In practice, the key factors of FP quantization such as the exponent, mantissa, and the block size of scaling factors may have different impacts on the final loss. more comprehensive, precise, and practical scaling law for float quantized training related to the data size (D), model size (N), exponent (E), mantissa (M), and block size of scaling factors (B) is urgently desired. Our work concentrates on establishing, verifying, and analyzing the scaling law for float quantized training in LLMs. Scaling Laws for FloatingPoint Quantization Training At the beginning, we first predict the model performance via the precision-related scaling law from previous work under different data/model sizes and precision settings. Surprisingly, we discover that the predictive performance was not perfectly satisfactory under different float quantized training settings. Subsequently, we carefully design comprehensive set of explorations with experiments of different precision settings (training 366 models), exploring the basic scaling law formation, as well as the potential impact of the quantization targets, exponent and mantissa, and block sizes on the loss. Finally, we aggregate these factors to get our final scaling law for float quantized training with valuable insights to guide the LLM training under low precision. Our scaling law is formulated as follows: L(N, D, E, M, B) = + DŒ≤ + œµ Œ± + DŒ≤ Œ± log2 Œ≥(E + 0.5)Œ¥(M + 0.5)ŒΩ . (1) The first two factors and indicate the data size and model size respectively, which show the main impacts on training loss given by the key factors of data and model size similar with the Chinchilla scaling law (Hoffmann et al., 2022). œµ represents the bias. The last factor could be regarded as the additional negative impact deriving from low precision training, where DŒ≤ Œ± implies certain form of knowledge intensity in LLM, and log2 B, (E + 0.5)Œ¥, and (M + 0.5)ŒΩ jointly reflect the low precision information loss of float quantized training. We have conducted extensive fitting experiments with various possible scaling law formulations to ensure the accuracy and simplicity of our scaling laws. Note that the exponential hyper-parameters Œ± and Œ≤ of model and data sizes are exactly the same as those in the first two factors. The product of the above knowledge intensity and low precision information loss forms the last factor. Figure 1 and Figure 8 illustrates the fitting results of our scaling law compa"
[07.01.2025 13:18] Mistral response. {"id": "23b6dff285d2422bbf26b1f11f998382", "object": "chat.completion", "created": 1736255933, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "```python\n['Tencent Hunyuan', 'University of Macau', 'The Chinese University of Hong Kong', 'Tokyo Institute of Technology']\n```"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1582, "total_tokens": 1618, "completion_tokens": 36}}
[07.01.2025 13:18] Response: ```python
['Tencent Hunyuan', 'University of Macau', 'The Chinese University of Hong Kong', 'Tokyo Institute of Technology']
```
[07.01.2025 13:18] Deleting PDF ./assets/pdf/2501.02423.pdf.
[07.01.2025 13:18] Success.
[07.01.2025 13:18] Enriching papers with extra data.
[07.01.2025 13:18] ********************************************************************************
[07.01.2025 13:18] Abstract 0. Image diffusion models have been adapted for real-world video super-resolution to tackle over-smoothing issues in GAN-based methods. However, these models struggle to maintain temporal consistency, as they are trained on static images, limiting their ability to capture temporal dynamics effectively....
[07.01.2025 13:18] ********************************************************************************
[07.01.2025 13:18] Abstract 1. Cutting-edge large language models (LLMs) demonstrate promising performance in solving complex math problems with a divide-and-conquer pipeline and the assistance of in-context learning (ICL) examples. However, their potential for improvement is limited by two critical problems within their ICL exam...
[07.01.2025 13:18] ********************************************************************************
[07.01.2025 13:18] Abstract 2. Active Real-time interaction with video LLMs introduces a new paradigm for human-computer interaction, where the model not only understands user intent but also responds while continuously processing streaming video on the fly. Unlike offline video LLMs, which analyze the entire video before answeri...
[07.01.2025 13:18] ********************************************************************************
[07.01.2025 13:18] Abstract 3. As large language models (LLMs) evolve, their ability to deliver personalized and context-aware responses offers transformative potential for improving user experiences. Existing personalization approaches, however, often rely solely on user history to augment the prompt, limiting their effectivenes...
[07.01.2025 13:18] ********************************************************************************
[07.01.2025 13:18] Abstract 4. The remarkable performance of the o1 model in complex reasoning demonstrates that test-time computing scaling can further unlock the model's potential, enabling powerful System-2 thinking. However, there is still a lack of comprehensive surveys for test-time computing scaling. We trace the concept o...
[07.01.2025 13:18] ********************************************************************************
[07.01.2025 13:18] Abstract 5. We pretrain METAGENE-1, a 7-billion-parameter autoregressive transformer model, which we refer to as a metagenomic foundation model, on a novel corpus of diverse metagenomic DNA and RNA sequences comprising over 1.5 trillion base pairs. This dataset is sourced from a large collection of human wastew...
[07.01.2025 13:18] ********************************************************************************
[07.01.2025 13:18] Abstract 6. Text-to-video generative models have made significant strides, enabling diverse applications in entertainment, advertising, and education. However, generating RGBA video, which includes alpha channels for transparency, remains a challenge due to limited datasets and the difficulty of adapting existi...
[07.01.2025 13:18] ********************************************************************************
[07.01.2025 13:18] Abstract 7. 4D video control is essential in video generation as it enables the use of sophisticated lens techniques, such as multi-camera shooting and dolly zoom, which are currently unsupported by existing methods. Training a video Diffusion Transformer (DiT) directly to control 4D content requires expensive ...
[07.01.2025 13:18] ********************************************************************************
[07.01.2025 13:18] Abstract 8. This paper presents a powerful framework to customize video creations by incorporating multiple specific identity (ID) photos, with video diffusion Transformers, referred to as Ingredients. Generally, our method consists of three primary modules: (i) a facial extractor that captures versatile and pr...
[07.01.2025 13:18] ********************************************************************************
[07.01.2025 13:18] Abstract 9. Monocular depth estimation within the diffusion-denoising paradigm demonstrates impressive generalization ability but suffers from low inference speed. Recent methods adopt a single-step deterministic paradigm to improve inference efficiency while maintaining comparable performance. However, they ov...
[07.01.2025 13:18] ********************************************************************************
[07.01.2025 13:18] Abstract 10. We consider the task of Image-to-Video (I2V) generation, which involves transforming static images into realistic video sequences based on a textual description. While recent advancements produce photorealistic outputs, they frequently struggle to create videos with accurate and consistent object mo...
[07.01.2025 13:18] ********************************************************************************
[07.01.2025 13:18] Abstract 11. Automated red-teaming has become a crucial approach for uncovering vulnerabilities in large language models (LLMs). However, most existing methods focus on isolated safety flaws, limiting their ability to adapt to dynamic defenses and uncover complex vulnerabilities efficiently. To address this chal...
[07.01.2025 13:18] ********************************************************************************
[07.01.2025 13:18] Abstract 12. Effective evaluation of multi-hop tool use is critical for analyzing the understanding, reasoning, and function-calling capabilities of large language models (LLMs). However, progress has been hindered by a lack of reliable evaluation datasets. To address this, we present ToolHop, a dataset comprisi...
[07.01.2025 13:18] ********************************************************************************
[07.01.2025 13:18] Abstract 13. We propose Samba ASR, the first state-of-the-art Automatic Speech Recognition (ASR) model leveraging the novel Mamba architecture as both encoder and decoder, built on the foundation of state-space models (SSMs). Unlike transformer-based ASR models, which rely on self-attention mechanisms to capture...
[07.01.2025 13:18] ********************************************************************************
[07.01.2025 13:18] Abstract 14. Low-precision training is considered an effective strategy for reducing both training and downstream inference costs. Previous scaling laws for precision mainly focus on integer quantization, which pay less attention to the constituents in floating-point quantization and thus cannot well fit the LLM...
[07.01.2025 13:18] Read previous papers.
[07.01.2025 13:18] Generating reviews via LLM API.
[07.01.2025 13:18] Using data from previous issue: {"categories": ["#cv", "#optimization", "#diffusion", "#multimodal", "#video"], "emoji": "üé•", "ru": {"title": "–ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ —Å—É–ø–µ—Ä—Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é T2V –º–æ–¥–µ–ª–µ–π", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–∞—è –º–µ—Ç–æ–¥–∏–∫–∞ STAR –¥–ª—è —Å—É–ø–µ—Ä—Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –≤–∏–¥–µ–æ –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö —É—Å–ª–æ–≤–∏—è—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–æ–¥–µ–ª–µ–π text-to-video. –ü—Ä
[07.01.2025 13:18] Using data from previous issue: {"categories": ["#training", "#optimization", "#math", "#reasoning"], "emoji": "üßÆ", "ru": {"title": "BoostStep: –ü–æ–≤—ã—à–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –ò–ò –≤ —Ä–µ—à–µ–Ω–∏–∏ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ BoostStep –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–µ—à–µ–Ω–∏—è —Å–ª–æ–∂–Ω—ã—Ö –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á –±–æ–ª—å—à–∏–º–∏ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è
[07.01.2025 13:18] Using data from previous issue: {"categories": ["#long_context", "#video", "#optimization", "#architecture", "#interpretability"], "emoji": "üé•", "ru": {"title": "Dispider: –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ —Å –≤–∏–¥–µ–æ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–∏—Å—Ç–µ–º—É Dispider –¥–ª—è –∞–∫—Ç–∏–≤–Ω–æ–≥–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å –≤–∏–¥–µ–æ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ
[07.01.2025 13:18] Using data from previous issue: {"categories": ["#rag", "#optimization", "#graphs", "#multimodal", "#benchmark", "#games"], "emoji": "üï∏Ô∏è", "ru": {"title": "–ì—Ä–∞—Ñ—ã –∑–Ω–∞–Ω–∏–π –Ω–∞ —Å–ª—É–∂–±–µ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º PGraphR
[07.01.2025 13:18] Using data from previous issue: {"categories": ["#reasoning", "#math", "#survey", "#training"], "emoji": "üß†", "ru": {"title": "–ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π: –ø—É—Ç—å –∫ –º—ã—à–ª–µ–Ω–∏—é System-2", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –≤–æ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã
[07.01.2025 13:18] Using data from previous issue: {"categories": ["#benchmark", "#data", "#training", "#architecture", "#science", "#dataset", "#healthcare"], "emoji": "üß¨", "ru": {"title": "METAGENE-1: –ú–µ—Ç–∞–≥–µ–Ω–æ–º–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –∑–¥–æ—Ä–æ–≤—å—è –Ω–∞—Å–µ–ª–µ–Ω–∏—è", "desc": "METAGENE-1 - —ç—Ç–æ –∞–≤—Ç–æ—Ä–µ–≥—Ä–∞—Å—Å–∏–≤–Ω–∞—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–Ω–∞—è –º–æ–¥–µ–ª—å —Å 7 –º–∏–ª–ª–∏–∞—Ä–¥–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
[07.01.2025 13:18] Using data from previous issue: {"categories": ["#optimization", "#architecture", "#training", "#diffusion", "#video"], "emoji": "üé¨", "ru": {"title": "TransPixar: –ü—Ä–æ—Ä—ã–≤ –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ RGBA-–≤–∏–¥–µ–æ –¥–ª—è –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —ç—Ñ—Ñ–µ–∫—Ç–æ–≤", "desc": "TransPixar - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ RGBA-–≤–∏–¥–µ–æ, —Ä–∞—Å—à–∏—Ä—è—é—â–∏–π –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ–º–æ–¥–µ–ª–µ–π. –û
[07.01.2025 13:18] Using data from previous issue: {"categories": ["#video", "#games", "#diffusion", "#3d"], "emoji": "üé•", "ru": {"title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ: 4D-–∫–æ–Ω—Ç—Ä–æ–ª—å —Å –ø–æ–º–æ—â—å—é –≥–∞—É—Å—Å–æ–≤—ã—Ö –ø–æ–ª–µ–π", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ —Å 4D-–∫–æ–Ω—Ç—Ä–æ–ª–µ–º, –∏—Å–ø–æ–ª—å–∑—É—è –ø—Å–µ–≤–¥–æ-4D –≥–∞—É—Å—Å–æ–≤—ã –ø–æ–ª—è –∏ –º–æ–¥–µ–ª—å Diffusion T
[07.01.2025 13:18] Using data from previous issue: {"categories": ["#open_source", "#training", "#architecture", "#video", "#dataset", "#diffusion", "#multimodal"], "emoji": "üé¨", "ru": {"title": "–ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –≤–∏–¥–µ–æ –∏–∑ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π: –Ω–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å –∫–æ–Ω—Ç—Ä–æ–ª—è –≤ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Ingredients
[07.01.2025 13:18] Using data from previous issue: {"categories": ["#optimization", "#training", "#diffusion", "#cv", "#dataset"], "emoji": "üîç", "ru": {"title": "DepthMaster: –û–¥–Ω–æ–ø—Ä–æ—Ö–æ–¥–Ω–∞—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Ç–æ—á–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –≥–ª—É–±–∏–Ω—ã —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞–ª–∏–∑–∞—Ü–∏–µ–π", "desc": "DepthMaster - —ç—Ç–æ –æ–¥–Ω–æ–ø—Ä–æ—Ö–æ–¥–Ω–∞—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –º–æ–Ω–æ–∫—É–ª—è—Ä–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –≥
[07.01.2025 13:18] Using data from previous issue: {"categories": ["#video", "#multimodal", "#benchmark"], "emoji": "üé¨", "ru": {"title": "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö –≤–∏–¥–µ–æ –∏–∑ —Å—Ç–∞—Ç–∏—á–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é –º–∞—Å–æ–∫ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π –¥–≤–∏–∂–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π (I2V) –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –æ–ø–∏—Å–∞–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã –ø—Ä
[07.01.2025 13:18] Using data from previous issue: {"categories": ["#security", "#rl", "#rlhf"], "emoji": "üõ°Ô∏è", "ru": {"title": "Auto-RT: –£–º–Ω–∞—è –∑–∞—â–∏—Ç–∞ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç Auto-RT - —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ —É—è–∑–≤–∏–º–æ—Å—Ç–µ–π –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM). –°–∏—Å—Ç–µ–º–∞ –∏—Å–ø–æ
[07.01.2025 13:18] Using data from previous issue: {"categories": ["#benchmark", "#reasoning", "#dataset", "#optimization"], "emoji": "üõ†Ô∏è", "ru": {"title": "ToolHop: –Ω–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–Ω–æ–≥–æ—ç—Ç–∞–ø–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –≤ LLM", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö ToolHop –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–Ω–æ–≥–æ—ç—Ç–∞–ø–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –±–æ
[07.01.2025 13:18] Using data from previous issue: {"categories": ["#audio", "#architecture", "#benchmark", "#low_resource", "#open_source"], "emoji": "üéôÔ∏è", "ru": {"title": "Samba ASR: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–∏ —Ä–µ—á–∏ —Å –ø–æ–º–æ—â—å—é –º–æ–¥–µ–ª–µ–π –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ —Å–æ—Å—Ç–æ—è–Ω–∏–π", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –º–æ–¥–µ–ª—å Samba ASR - –ø–µ—Ä–≤–∞—è —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞—Å–ø–æ–∑–Ω–∞
[07.01.2025 13:18] Querying the API.
[07.01.2025 13:18] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Low-precision training is considered an effective strategy for reducing both training and downstream inference costs. Previous scaling laws for precision mainly focus on integer quantization, which pay less attention to the constituents in floating-point quantization and thus cannot well fit the LLM losses in this scenario. In contrast, while floating-point quantization training is more commonly implemented in production, the research on it has been relatively superficial. In this paper, we thoroughly explore the effects of floating-point quantization targets, exponent bits, mantissa bits, and the calculation granularity of the scaling factor in floating-point quantization training performance of LLM models. While presenting an accurate floating-point quantization unified scaling law, we also provide valuable suggestions for the community: (1) Exponent bits contribute slightly more to the model performance than mantissa bits. We provide the optimal exponent-mantissa bit ratio for different bit numbers, which is available for future reference by hardware manufacturers; (2) We discover the formation of the critical data size in low-precision LLM training. Too much training data exceeding the critical data size will inversely bring in degradation of LLM performance; (3) The optimal floating-point quantization precision is directly proportional to the computational power, but within a wide computational power range, we estimate that the best cost-performance precision lies between 4-8 bits.
[07.01.2025 13:18] Response: {
  "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –≤–ª–∏—è–Ω–∏–µ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è —Å –ø–ª–∞–≤–∞—é—â–µ–π –∑–∞–ø—è—Ç–æ–π –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –ê–≤—Ç–æ—Ä—ã –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç —Ä–æ–ª—å —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö –∏ –º–∞–Ω—Ç–∏—Å—Å–Ω—ã—Ö –±–∏—Ç–æ–≤, –∞ —Ç–∞–∫–∂–µ —Ä–∞–∑–º–µ—Ä–∞ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö –≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π. –û–Ω–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∑–∞–∫–æ–Ω –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è —Å –ø–ª–∞–≤–∞—é—â–µ–π –∑–∞–ø—è—Ç–æ–π –∏ –¥–∞—é—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–º—É —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—é –±–∏—Ç–æ–≤ –∏ —Ä–∞–∑–º–µ—Ä—É –¥–∞–Ω–Ω—ã—Ö. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –æ–ø—Ç–∏–º–∞–ª—å–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ 4-8 –±–∏—Ç –¥–ª—è —à–∏—Ä–æ–∫–æ–≥–æ —Å–ø–µ–∫—Ç—Ä–∞ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –º–æ—â–Ω–æ—Å—Ç–µ–π.",
  "emoji": "üßÆ",
  "title": "–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –≤ –æ–±—É—á–µ–Ω–∏–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π"
}
[07.01.2025 13:18] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Low-precision training is considered an effective strategy for reducing both training and downstream inference costs. Previous scaling laws for precision mainly focus on integer quantization, which pay less attention to the constituents in floating-point quantization and thus cannot well fit the LLM losses in this scenario. In contrast, while floating-point quantization training is more commonly implemented in production, the research on it has been relatively superficial. In this paper, we thoroughly explore the effects of floating-point quantization targets, exponent bits, mantissa bits, and the calculation granularity of the scaling factor in floating-point quantization training performance of LLM models. While presenting an accurate floating-point quantization unified scaling law, we also provide valuable suggestions for the community: (1) Exponent bits contribute slightly more to the model performance than mantissa bits. We provide the optimal exponent-mantissa bit ratio for different bit numbers, which is available for future reference by hardware manufacturers; (2) We discover the formation of the critical data size in low-precision LLM training. Too much training data exceeding the critical data size will inversely bring in degradation of LLM performance; (3) The optimal floating-point quantization precision is directly proportional to the computational power, but within a wide computational power range, we estimate that the best cost-performance precision lies between 4-8 bits."

[07.01.2025 13:18] Response: ```python
['INFERENCE', 'TRAINING']
```
[07.01.2025 13:18] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Low-precision training is considered an effective strategy for reducing both training and downstream inference costs. Previous scaling laws for precision mainly focus on integer quantization, which pay less attention to the constituents in floating-point quantization and thus cannot well fit the LLM losses in this scenario. In contrast, while floating-point quantization training is more commonly implemented in production, the research on it has been relatively superficial. In this paper, we thoroughly explore the effects of floating-point quantization targets, exponent bits, mantissa bits, and the calculation granularity of the scaling factor in floating-point quantization training performance of LLM models. While presenting an accurate floating-point quantization unified scaling law, we also provide valuable suggestions for the community: (1) Exponent bits contribute slightly more to the model performance than mantissa bits. We provide the optimal exponent-mantissa bit ratio for different bit numbers, which is available for future reference by hardware manufacturers; (2) We discover the formation of the critical data size in low-precision LLM training. Too much training data exceeding the critical data size will inversely bring in degradation of LLM performance; (3) The optimal floating-point quantization precision is directly proportional to the computational power, but within a wide computational power range, we estimate that the best cost-performance precision lies between 4-8 bits."

[07.01.2025 13:18] Response: ```python
["OPTIMIZATION"]
```
[07.01.2025 13:19] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper investigates the impact of floating-point quantization on the training performance of large language models (LLMs). It highlights that previous research primarily focused on integer quantization, neglecting the nuances of floating-point quantization. The authors establish a unified scaling law for floating-point quantization and provide insights on the optimal ratio of exponent to mantissa bits, emphasizing that exponent bits have a greater influence on model performance. Additionally, they identify a critical data size threshold, beyond which performance may degrade, and suggest that the best precision for cost-performance lies between 4-8 bits, depending on computational power.","title":"Optimizing Floating-Point Quantization for Better LLM Performance"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[], parsed=Article(desc='This paper investigates the impact of floating-point quantization on the training performance of large language models (LLMs). It highlights that previous research primarily focused on integer quantization, neglecting the nuances of floating-point quantization. The authors establish a unified scaling law for floating-point quantization and provide insights on the optimal ratio of exponent to mantissa bits, emphasizing that exponent bits have a greater influence on model performance. Additionally, they identify a critical data size threshold, beyond which performance may degrade, and suggest that the best precision for cost-performance lies between 4-8 bits, depending on computational power.', title='Optimizing Floating-Point Quantization for Better LLM Performance'))
[07.01.2025 13:19] Response: ParsedChatCompletionMessage[Article](content='{"desc":"‰ΩéÁ≤æÂ∫¶ËÆ≠ÁªÉË¢´ËÆ§‰∏∫ÊòØÈôç‰ΩéËÆ≠ÁªÉÂíåÊé®ÁêÜÊàêÊú¨ÁöÑÊúâÊïàÁ≠ñÁï•„ÄÇ‰ª•ÂæÄÁöÑÁ†îÁ©∂‰∏ªË¶ÅÈõÜ‰∏≠Âú®Êï¥Êï∞ÈáèÂåñ‰∏äÔºåËÄåÂØπÊµÆÁÇπÈáèÂåñÁöÑÁ†îÁ©∂Áõ∏ÂØπËæÉÂ∞ëÔºåÂØºËá¥Êó†Ê≥ïÂæàÂ•ΩÂú∞ÈÄÇÂ∫îÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÊçüÂ§±ÊÉÖÂÜµ„ÄÇÊú¨ÊñáÊ∑±ÂÖ•Êé¢ËÆ®‰∫ÜÊµÆÁÇπÈáèÂåñËÆ≠ÁªÉ‰∏≠ÁõÆÊ†á„ÄÅÊåáÊï∞‰Ωç„ÄÅÂ∞æÊï∞‰ΩçÂíåÁº©ÊîæÂõ†Â≠êÁöÑËÆ°ÁÆóÁ≤íÂ∫¶ÂØπÂ§ßËØ≠Ë®ÄÊ®°ÂûãÊÄßËÉΩÁöÑÂΩ±ÂìçÔºåÂπ∂ÊèêÂá∫‰∫ÜÁªü‰∏ÄÁöÑÊµÆÁÇπÈáèÂåñÁº©ÊîæÊ≥ïÂàô„ÄÇÁ†îÁ©∂ÁªìÊûúË°®ÊòéÔºåÊåáÊï∞‰ΩçÂØπÊ®°ÂûãÊÄßËÉΩÁöÑË¥°ÁåÆÁï•È´ò‰∫éÂ∞æÊï∞‰ΩçÔºåÂπ∂ÂèëÁé∞‰∫Ü‰ΩéÁ≤æÂ∫¶ËÆ≠ÁªÉ‰∏≠ÁöÑÂÖ≥ÈîÆÊï∞ÊçÆÂ§ßÂ∞è„ÄÇ","title":"‰ΩéÁ≤æÂ∫¶ËÆ≠ÁªÉÔºö‰ºòÂåñÊµÆÁÇπÈáèÂåñÁöÑÂÖ≥ÈîÆ"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[], parsed=Article(desc='‰ΩéÁ≤æÂ∫¶ËÆ≠ÁªÉË¢´ËÆ§‰∏∫ÊòØÈôç‰ΩéËÆ≠ÁªÉÂíåÊé®ÁêÜÊàêÊú¨ÁöÑÊúâÊïàÁ≠ñÁï•„ÄÇ‰ª•ÂæÄÁöÑÁ†îÁ©∂‰∏ªË¶ÅÈõÜ‰∏≠Âú®Êï¥Êï∞ÈáèÂåñ‰∏äÔºåËÄåÂØπÊµÆÁÇπÈáèÂåñÁöÑÁ†îÁ©∂Áõ∏ÂØπËæÉÂ∞ëÔºåÂØºËá¥Êó†Ê≥ïÂæàÂ•ΩÂú∞ÈÄÇÂ∫îÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÊçüÂ§±ÊÉÖÂÜµ„ÄÇÊú¨ÊñáÊ∑±ÂÖ•Êé¢ËÆ®‰∫ÜÊµÆÁÇπÈáèÂåñËÆ≠ÁªÉ‰∏≠ÁõÆÊ†á„ÄÅÊåáÊï∞‰Ωç„ÄÅÂ∞æÊï∞‰ΩçÂíåÁº©ÊîæÂõ†Â≠êÁöÑËÆ°ÁÆóÁ≤íÂ∫¶ÂØπÂ§ßËØ≠Ë®ÄÊ®°ÂûãÊÄßËÉΩÁöÑÂΩ±ÂìçÔºåÂπ∂ÊèêÂá∫‰∫ÜÁªü‰∏ÄÁöÑÊµÆÁÇπÈáèÂåñÁº©ÊîæÊ≥ïÂàô„ÄÇÁ†îÁ©∂ÁªìÊûúË°®ÊòéÔºåÊåáÊï∞‰ΩçÂØπÊ®°ÂûãÊÄßËÉΩÁöÑË¥°ÁåÆÁï•È´ò‰∫éÂ∞æÊï∞‰ΩçÔºåÂπ∂ÂèëÁé∞‰∫Ü‰ΩéÁ≤æÂ∫¶ËÆ≠ÁªÉ‰∏≠ÁöÑÂÖ≥ÈîÆÊï∞ÊçÆÂ§ßÂ∞è„ÄÇ', title='‰ΩéÁ≤æÂ∫¶ËÆ≠ÁªÉÔºö‰ºòÂåñÊµÆÁÇπÈáèÂåñÁöÑÂÖ≥ÈîÆ'))
[07.01.2025 13:19] Loading Chinese text from previous data.
[07.01.2025 13:19] Renaming data file.
[07.01.2025 13:19] Renaming previous data. hf_papers.json to ./d/2025-01-07.json
[07.01.2025 13:19] Saving new data file.
[07.01.2025 13:19] Generating page.
[07.01.2025 13:19] Renaming previous page.
[07.01.2025 13:19] Renaming previous data. index.html to ./d/2025-01-07.html
[07.01.2025 13:19] [Experimental] Generating Chinese page for reading.
[07.01.2025 13:19] Chinese vocab [{'word': 'ËÆ®ËÆ∫', 'pinyin': 't«éo l√πn', 'trans': 'discuss'}, {'word': 'Â§ßËØ≠Ë®ÄÊ®°Âûã', 'pinyin': 'd√† y«î y√°n m√≥ x√≠ng', 'trans': 'large language model'}, {'word': 'Â§çÊùÇ', 'pinyin': 'f√π z√°', 'trans': 'complex'}, {'word': 'Êï∞Â≠¶', 'pinyin': 'sh√π xu√©', 'trans': 'mathematics'}, {'word': 'Ë°®Áé∞', 'pinyin': 'bi«éo xi√†n', 'trans': 'performance'}, {'word': 'ÂàÜËÄåÊ≤ª‰πã', 'pinyin': 'fƒìn √©r zh√¨ zhƒ´', 'trans': 'divide and conquer'}, {'word': '‰∏ä‰∏ãÊñáÂ≠¶‰π†', 'pinyin': 'sh√†ng xi√† w√©n xu√© x√≠', 'trans': 'in-context learning'}, {'word': 'Á§∫‰æã', 'pinyin': 'sh√¨ l√¨', 'trans': 'example'}, {'word': 'ÂÖ≥ÈîÆ', 'pinyin': 'gu«én ji√†n', 'trans': 'key'}, {'word': 'Á≤íÂ∫¶', 'pinyin': 'l√¨ d√π', 'trans': 'granularity'}, {'word': 'Ë¥üÈù¢ÊïàÂ∫î', 'pinyin': 'f√π mi√†n xi√†o y√¨ng', 'trans': 'negative effect'}, {'word': 'Âô™Â£∞', 'pinyin': 'z√†o shƒìng', 'trans': 'noise'}, {'word': 'Êé®ÁêÜ', 'pinyin': 'tuƒ´ l«ê', 'trans': 'reasoning'}, {'word': 'Ë¥®Èáè', 'pinyin': 'zh√¨ li√†ng', 'trans': 'quality'}, {'word': 'ÊèêÂá∫', 'pinyin': 't√≠ ch≈´', 'trans': 'propose'}, {'word': 'BoostStep', 'pinyin': 'BoostStep', 'trans': 'BoostStep'}, {'word': 'ÂØπÈΩê', 'pinyin': 'du√¨ q√≠', 'trans': 'align'}, {'word': 'Á≠ñÁï•', 'pinyin': 'c√® l√º√®', 'trans': 'strategy'}, {'word': 'Áõ∏ÂÖ≥', 'pinyin': 'xiƒÅng guƒÅn', 'trans': 'relevant'}, {'word': 'ÊèêÈ´ò', 'pinyin': 't√≠ gƒÅo', 'trans': 'improve'}, {'word': 'ËíôÁâπÂç°ÁΩóÊ†ëÊêúÁ¥¢ÊñπÊ≥ï', 'pinyin': 'm√©ng t√® k«é lu√≥ sh√π s≈çu su«í fƒÅng f«é', 'trans': 'Monte Carlo Tree Search method'}, {'word': 'ÁªìÂêà', 'pinyin': 'ji√© h√©', 'trans': 'combine'}, {'word': '‰ΩøÁî®', 'pinyin': 'sh«ê y√≤ng', 'trans': 'use'}, {'word': 'ÊèêÂçá', 'pinyin': 't√≠ shƒìng', 'trans': 'enhance'}, {'word': 'ÂÆûÈ™å', 'pinyin': 'sh√≠ y√†n', 'trans': 'experiment'}, {'word': 'ÁªìÊûú', 'pinyin': 'ji√© gu«í', 'trans': 'result'}, {'word': 'ÊòæËëó', 'pinyin': 'xi«én zh√π', 'trans': 'significant'}, {'word': 'GPT-4o', 'pinyin': 'GPT-4o', 'trans': 'GPT-4o'}, {'word': 'Qwen2.5-Math-72B', 'pinyin': 'Qwen2.5-Math-72B', 'trans': 'Qwen2.5-Math-72B'}]
[07.01.2025 13:19] Renaming previous Chinese page.
[07.01.2025 13:19] Renaming previous data. zh.html to ./d/2025-01-06_zh_reading_task.html
[07.01.2025 13:19] Writing Chinese reading task.
[07.01.2025 13:19] Writing result.
[07.01.2025 13:19] Renaming log file.
[07.01.2025 13:19] Renaming previous data. log.txt to ./logs/2025-01-07_last_log.txt
