[23.06.2025 04:27] Read previous papers.
[23.06.2025 04:27] Generating top page (month).
[23.06.2025 04:27] Writing top page (month).
[23.06.2025 05:15] Read previous papers.
[23.06.2025 05:15] Get feed.
[23.06.2025 05:15] Get page data from previous paper. URL: https://huggingface.co/papers/2506.16406
[23.06.2025 05:15] Get page data from previous paper. URL: https://huggingface.co/papers/2506.09049
[23.06.2025 05:15] Get page data from previous paper. URL: https://huggingface.co/papers/2506.17206
[23.06.2025 05:15] Extract page data from URL. URL: https://huggingface.co/papers/2506.16035
[23.06.2025 05:15] Get page data from previous paper. URL: https://huggingface.co/papers/2506.17201
[23.06.2025 05:15] Get page data from previous paper. URL: https://huggingface.co/papers/2506.16504
[23.06.2025 05:15] Get page data from previous paper. URL: https://huggingface.co/papers/2506.15925
[23.06.2025 05:15] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[23.06.2025 05:15] No deleted papers detected.
[23.06.2025 05:15] Downloading and parsing papers (pdf, html). Total: 7.
[23.06.2025 05:15] Downloading and parsing paper https://huggingface.co/papers/2506.16406.
[23.06.2025 05:15] Extra JSON file exists (./assets/json/2506.16406.json), skip PDF parsing.
[23.06.2025 05:15] Paper image links file exists (./assets/img_data/2506.16406.json), skip HTML parsing.
[23.06.2025 05:15] Success.
[23.06.2025 05:15] Downloading and parsing paper https://huggingface.co/papers/2506.09049.
[23.06.2025 05:15] Extra JSON file exists (./assets/json/2506.09049.json), skip PDF parsing.
[23.06.2025 05:15] Paper image links file exists (./assets/img_data/2506.09049.json), skip HTML parsing.
[23.06.2025 05:15] Success.
[23.06.2025 05:15] Downloading and parsing paper https://huggingface.co/papers/2506.17206.
[23.06.2025 05:15] Extra JSON file exists (./assets/json/2506.17206.json), skip PDF parsing.
[23.06.2025 05:15] Paper image links file exists (./assets/img_data/2506.17206.json), skip HTML parsing.
[23.06.2025 05:15] Success.
[23.06.2025 05:15] Downloading and parsing paper https://huggingface.co/papers/2506.16035.
[23.06.2025 05:15] Downloading paper 2506.16035 from http://arxiv.org/pdf/2506.16035v1...
[23.06.2025 05:15] Extracting affiliations from text.
[23.06.2025 05:15] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 9 1 ] . [ 1 5 3 0 6 1 . 6 0 5 2 : r Vision-Guided Chunking Is All You Need: Enhancing RAG with Multimodal Document Understanding Vishesh Tripathi, Tanmay Odapally, Indraneel Das, Uday Allu, and Biddwan Ahmed AI Research Team, Yellow.ai Abstract Retrieval-Augmented Generation (RAG) systems have revolutionized information retrieval and question answering, but traditional text-based chunking methods struggle with complex document structures, multi-page tables, embedded figures, and contextual dependencies across page boundaries. We present novel multimodal document chunking approach that leverages Large Multimodal Models (LMMs) to process PDF documents in batches while maintaining semantic coherence and structural integrity. Our method processes documents in configurable page batches with cross-batch context preservation, enabling accurate handling of tables spanning multiple pages, embedded visual elements, and procedural content. We evaluate our approach on our internal benchmark dataset of diverse PDF documents, demonstrating improvements in chunk quality and downstream RAG performance. Our vision-guided approach achieves better quantitative performance on our internal benchmark compared to traditional vanilla RAG systems, with qualitative analysis showing better preservation of document structure and semantic coherence. Retrieval-Augmented Generation (RAG) [Lewis et al., 2020] has emerged as an important paradigm for enhancing large language models with external knowledge sources. The effectiveness of RAG systems fundamentally depends on the quality of document chunking - the process of segmenting documents into coherent, retrievable units. Traditional approaches rely on simple text extraction followed by rule-based or sliding-window chunking [Carbonell & Goldstein, 1998], which often fails to preserve semantic coherence and structural relationships in complex documents. Modern documents, particularly technical manuals, research papers, and business reports"
[23.06.2025 05:15] Response: ```python
["AI Research Team, Yellow.ai"]
```
[23.06.2025 05:15] Deleting PDF ./assets/pdf/2506.16035.pdf.
[23.06.2025 05:15] Success.
[23.06.2025 05:15] Downloading and parsing paper https://huggingface.co/papers/2506.17201.
[23.06.2025 05:15] Extra JSON file exists (./assets/json/2506.17201.json), skip PDF parsing.
[23.06.2025 05:15] Paper image links file exists (./assets/img_data/2506.17201.json), skip HTML parsing.
[23.06.2025 05:15] Success.
[23.06.2025 05:15] Downloading and parsing paper https://huggingface.co/papers/2506.16504.
[23.06.2025 05:15] Extra JSON file exists (./assets/json/2506.16504.json), skip PDF parsing.
[23.06.2025 05:15] Paper image links file exists (./assets/img_data/2506.16504.json), skip HTML parsing.
[23.06.2025 05:15] Success.
[23.06.2025 05:15] Downloading and parsing paper https://huggingface.co/papers/2506.15925.
[23.06.2025 05:15] Extra JSON file exists (./assets/json/2506.15925.json), skip PDF parsing.
[23.06.2025 05:15] Paper image links file exists (./assets/img_data/2506.15925.json), skip HTML parsing.
[23.06.2025 05:15] Success.
[23.06.2025 05:15] Enriching papers with extra data.
[23.06.2025 05:15] ********************************************************************************
[23.06.2025 05:15] Abstract 0. Drag-and-Drop LLMs generate task-specific parameters through prompt-conditioned parameter generation, achieving significant efficiency gains and cross-domain generalization without per-task training.  					AI-generated summary 				 Modern Parameter-Efficient Fine-Tuning (PEFT) methods such as low-ra...
[23.06.2025 05:15] ********************************************************************************
[23.06.2025 05:15] Abstract 1. VIKI-Bench and VIKI-R provide a benchmark and framework for evaluating and improving visual-driven cooperation among diverse embodied agents using vision-language models and reinforcement learning.  					AI-generated summary 				 Coordinating multiple embodied agents in dynamic environments remains ...
[23.06.2025 05:15] ********************************************************************************
[23.06.2025 05:15] Abstract 2. Multi-plane synchronization extends 2D foundation models to 3D panorama generation, introducing DreamCube to achieve diverse appearances and accurate geometry.  					AI-generated summary 				 3D panorama synthesis is a promising yet challenging task that demands high-quality and diverse visual appea...
[23.06.2025 05:15] ********************************************************************************
[23.06.2025 05:15] Abstract 3. A novel multimodal document chunking approach using Large Multimodal Models (LMMs) enhances RAG performance by accurately processing complex PDF documents, including multi-page tables and embedded visuals.  					AI-generated summary 				 Retrieval-Augmented Generation (RAG) systems have revolutioniz...
[23.06.2025 05:15] ********************************************************************************
[23.06.2025 05:15] Abstract 4. Hunyuan-GameCraft is a novel framework for high-dynamic interactive video generation in game environments that addresses limitations in dynamics, generality, and efficiency through unified input representation, hybrid history-conditioned training, and model distillation.  					AI-generated summary 	...
[23.06.2025 05:15] ********************************************************************************
[23.06.2025 05:15] Abstract 5. Hunyuan3D 2.5, a suite of 3D diffusion models, advances shape and texture generation with a new LATTICE model and physical-based rendering in a multi-view architecture.  					AI-generated summary 				 In this report, we present Hunyuan3D 2.5, a robust suite of 3D diffusion models aimed at generating...
[23.06.2025 05:15] ********************************************************************************
[23.06.2025 05:15] Abstract 6. Reranking and preference tuning improve the quality of perspective summaries generated by LLMs, as measured by language model-based metrics that outperform traditional ones.  					AI-generated summary 				 Generating unbiased summaries in real-world settings such as political perspective summarizati...
[23.06.2025 05:15] Read previous papers.
[23.06.2025 05:15] Generating reviews via LLM API.
[23.06.2025 05:15] Using data from previous issue: {"categories": ["#transfer_learning", "#multimodal", "#training", "#optimization"], "emoji": "üé≠", "ru": {"title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–µ –Ø–ú: –±—ã—Å—Ç—Ä–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ Drag-and-Drop LLMs (DnD) –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º
[23.06.2025 05:15] Using data from previous issue: {"categories": ["#reasoning", "#agents", "#games", "#rl", "#benchmark", "#cv"], "emoji": "ü§ñ", "ru": {"title": "–í–∏–∑—É–∞–ª—å–Ω–∞—è –∫–æ–æ–ø–µ—Ä–∞—Ü–∏—è –∞–≥–µ–Ω—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è", "desc": "VIKI-Bench –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–æ–æ–ø–µ—Ä–∞—Ü–∏–∏ –º–µ–∂–¥—É –≤–æ–ø–ª–æ—â–µ–Ω–Ω—ã–º–∏ –∞–≥–µ–Ω—Ç–∞–º–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º
[23.06.2025 05:15] Using data from previous issue: {"categories": ["#optimization", "#diffusion", "#3d", "#cv"], "emoji": "üåê", "ru": {"title": "–ú–Ω–æ–≥–æ–ø–ª–æ—Å–∫–æ—Å—Ç–Ω–∞—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è: –º–æ—Å—Ç –º–µ–∂–¥—É 2D –∏ 3D –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–∞–Ω–æ—Ä–∞–º", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç—Ä–µ—Ö–º–µ—Ä–Ω—ã—Ö –ø–∞–Ω–æ—Ä–∞–º —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–Ω–æ–≥–æ–ø–ª–æ—Å–∫–æ—Å—Ç–Ω–æ–π —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏ –¥–≤—É–º–µ—Ä–Ω—ã—Ö
[23.06.2025 05:15] Querying the API.
[23.06.2025 05:15] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A novel multimodal document chunking approach using Large Multimodal Models (LMMs) enhances RAG performance by accurately processing complex PDF documents, including multi-page tables and embedded visuals.  					AI-generated summary 				 Retrieval-Augmented Generation (RAG) systems have revolutionized information retrieval and question answering, but traditional text-based chunking methods struggle with complex document structures, multi-page tables, embedded figures, and contextual dependencies across page boundaries. We present a novel multimodal document chunking approach that leverages Large Multimodal Models (LMMs) to process PDF documents in batches while maintaining semantic coherence and structural integrity. Our method processes documents in configurable page batches with cross-batch context preservation, enabling accurate handling of tables spanning multiple pages, embedded visual elements, and procedural content. We evaluate our approach on a curated dataset of PDF documents with manually crafted queries, demonstrating improvements in chunk quality and downstream RAG performance. Our vision-guided approach achieves better accuracy compared to traditional vanilla RAG systems, with qualitative analysis showing superior preservation of document structure and semantic coherence.
[23.06.2025 05:15] Response: {
  "desc": "–ü—Ä–µ–¥–ª–æ–∂–µ–Ω –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ä–∞–∑–±–∏–µ–Ω–∏—é —Å–ª–æ–∂–Ω—ã—Ö PDF-–¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–∞ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LMM). –ú–µ—Ç–æ–¥ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã –ø–∞—Ä—Ç–∏—è–º–∏ —Å—Ç—Ä–∞–Ω–∏—Ü, —Å–æ—Ö—Ä–∞–Ω—è—è –∫–æ–Ω—Ç–µ–∫—Å—Ç –º–µ–∂–¥—É –ø–∞—Ä—Ç–∏—è–º–∏, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –º–Ω–æ–≥–æ—Å—Ç—Ä–∞–Ω–∏—á–Ω—ã–µ —Ç–∞–±–ª–∏—Ü—ã –∏ –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ –≤–∏–∑—É–∞–ª—å–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏ —É–ª—É—á—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –∏ –ø–æ–≤—ã—à–µ–Ω–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ —Å–∏—Å—Ç–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ–º (RAG). –ù–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã RAG –≤ —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞.",
  "emoji": "üìÑ",
  "title": "–£–º–Ω–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è RAG-—Å–∏—Å—Ç–µ–º"
}
[23.06.2025 05:15] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A novel multimodal document chunking approach using Large Multimodal Models (LMMs) enhances RAG performance by accurately processing complex PDF documents, including multi-page tables and embedded visuals.  					AI-generated summary 				 Retrieval-Augmented Generation (RAG) systems have revolutionized information retrieval and question answering, but traditional text-based chunking methods struggle with complex document structures, multi-page tables, embedded figures, and contextual dependencies across page boundaries. We present a novel multimodal document chunking approach that leverages Large Multimodal Models (LMMs) to process PDF documents in batches while maintaining semantic coherence and structural integrity. Our method processes documents in configurable page batches with cross-batch context preservation, enabling accurate handling of tables spanning multiple pages, embedded visual elements, and procedural content. We evaluate our approach on a curated dataset of PDF documents with manually crafted queries, demonstrating improvements in chunk quality and downstream RAG performance. Our vision-guided approach achieves better accuracy compared to traditional vanilla RAG systems, with qualitative analysis showing superior preservation of document structure and semantic coherence."

[23.06.2025 05:15] Response: ```python
['RAG', 'MULTIMODAL', 'DATASET']
```
[23.06.2025 05:15] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A novel multimodal document chunking approach using Large Multimodal Models (LMMs) enhances RAG performance by accurately processing complex PDF documents, including multi-page tables and embedded visuals.  					AI-generated summary 				 Retrieval-Augmented Generation (RAG) systems have revolutionized information retrieval and question answering, but traditional text-based chunking methods struggle with complex document structures, multi-page tables, embedded figures, and contextual dependencies across page boundaries. We present a novel multimodal document chunking approach that leverages Large Multimodal Models (LMMs) to process PDF documents in batches while maintaining semantic coherence and structural integrity. Our method processes documents in configurable page batches with cross-batch context preservation, enabling accurate handling of tables spanning multiple pages, embedded visual elements, and procedural content. We evaluate our approach on a curated dataset of PDF documents with manually crafted queries, demonstrating improvements in chunk quality and downstream RAG performance. Our vision-guided approach achieves better accuracy compared to traditional vanilla RAG systems, with qualitative analysis showing superior preservation of document structure and semantic coherence."

[23.06.2025 05:15] Response: ```python
["OPTIMIZATION", "LONG_CONTEXT"]
```
[23.06.2025 05:15] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces a new method for breaking down complex PDF documents using Large Multimodal Models (LMMs). Traditional methods struggle with intricate layouts, such as multi-page tables and embedded visuals, which can lead to poor information retrieval. The proposed approach processes documents in batches while keeping the context intact across pages, ensuring that the meaning and structure are preserved. Evaluations show that this method significantly enhances the quality of document chunks and improves the performance of Retrieval-Augmented Generation (RAG) systems.","title":"Enhancing RAG with Multimodal Document Chunking"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper introduces a new method for breaking down complex PDF documents using Large Multimodal Models (LMMs). Traditional methods struggle with intricate layouts, such as multi-page tables and embedded visuals, which can lead to poor information retrieval. The proposed approach processes documents in batches while keeping the context intact across pages, ensuring that the meaning and structure are preserved. Evaluations show that this method significantly enhances the quality of document chunks and improves the performance of Retrieval-Augmented Generation (RAG) systems.', title='Enhancing RAG with Multimodal Document Chunking'))
[23.06.2025 05:15] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÂ§öÊ®°ÊÄÅÊñáÊ°£ÂàÜÂùóÊñπÊ≥ïÔºåÂà©Áî®Â§ßÂûãÂ§öÊ®°ÊÄÅÊ®°ÂûãÔºàLMMsÔºâÊù•Â§ÑÁêÜÂ§çÊùÇÁöÑPDFÊñáÊ°£„ÄÇËØ•ÊñπÊ≥ïËÉΩÂ§üÊúâÊïàÂ§ÑÁêÜÂ§öÈ°µË°®Ê†ºÂíåÂµåÂÖ•ËßÜËßâÂÖÉÁ¥†ÔºåÂêåÊó∂‰øùÊåÅËØ≠‰πâ‰∏ÄËá¥ÊÄßÂíåÁªìÊûÑÂÆåÊï¥ÊÄß„ÄÇÈÄöËøáÈÖçÁΩÆÈ°µÈù¢ÊâπÊ¨°ËøõË°åÂ§ÑÁêÜÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïËÉΩÂ§üË∑®ÊâπÊ¨°‰øùÁïô‰∏ä‰∏ãÊñáÔºå‰ªéËÄåÊèêÈ´ò‰∫ÜÂàÜÂùóË¥®ÈáèÂíåÂêéÁª≠ÁöÑÊ£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàêÔºàRAGÔºâÊÄßËÉΩ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºå‰∏é‰º†ÁªüÁöÑRAGÁ≥ªÁªüÁõ∏ÊØîÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÂú®ÂáÜÁ°ÆÊÄßÂíåÊñáÊ°£ÁªìÊûÑ‰øùÁïôÊñπÈù¢Ë°®Áé∞Êõ¥‰ºò„ÄÇ","title":"Â§öÊ®°ÊÄÅÊñáÊ°£ÂàÜÂùóÔºåÊèêÂçá‰ø°ÊÅØÊ£ÄÁ¥¢Êñ∞È´òÂ∫¶"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÂ§öÊ®°ÊÄÅÊñáÊ°£ÂàÜÂùóÊñπÊ≥ïÔºåÂà©Áî®Â§ßÂûãÂ§öÊ®°ÊÄÅÊ®°ÂûãÔºàLMMsÔºâÊù•Â§ÑÁêÜÂ§çÊùÇÁöÑPDFÊñáÊ°£„ÄÇËØ•ÊñπÊ≥ïËÉΩÂ§üÊúâÊïàÂ§ÑÁêÜÂ§öÈ°µË°®Ê†ºÂíåÂµåÂÖ•ËßÜËßâÂÖÉÁ¥†ÔºåÂêåÊó∂‰øùÊåÅËØ≠‰πâ‰∏ÄËá¥ÊÄßÂíåÁªìÊûÑÂÆåÊï¥ÊÄß„ÄÇÈÄöËøáÈÖçÁΩÆÈ°µÈù¢ÊâπÊ¨°ËøõË°åÂ§ÑÁêÜÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïËÉΩÂ§üË∑®ÊâπÊ¨°‰øùÁïô‰∏ä‰∏ãÊñáÔºå‰ªéËÄåÊèêÈ´ò‰∫ÜÂàÜÂùóË¥®ÈáèÂíåÂêéÁª≠ÁöÑÊ£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàêÔºàRAGÔºâÊÄßËÉΩ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºå‰∏é‰º†ÁªüÁöÑRAGÁ≥ªÁªüÁõ∏ÊØîÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÂú®ÂáÜÁ°ÆÊÄßÂíåÊñáÊ°£ÁªìÊûÑ‰øùÁïôÊñπÈù¢Ë°®Áé∞Êõ¥‰ºò„ÄÇ', title='Â§öÊ®°ÊÄÅÊñáÊ°£ÂàÜÂùóÔºåÊèêÂçá‰ø°ÊÅØÊ£ÄÁ¥¢Êñ∞È´òÂ∫¶'))
[23.06.2025 05:15] Using data from previous issue: {"categories": ["#inference", "#video", "#diffusion", "#synthetic", "#dataset", "#games", "#training"], "emoji": "üéÆ", "ru": {"title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–≥–æ –∏–≥—Ä–æ–≤–æ–≥–æ –≤–∏–¥–µ–æ", "desc": "Hunyuan-GameCraft - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–≥–æ –≤–∏–¥–µ–æ –≤ –∏–≥—Ä–æ–≤—ã—Ö —Å—Ä–µ–¥–∞—Ö —Å –≤—ã—Å–æ–∫–æ–π –¥–∏
[23.06.2025 05:15] Using data from previous issue: {"categories": ["#diffusion", "#3d"], "emoji": "üßä", "ru": {"title": "–ù–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å 3D-–≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: —Ç–æ—á–Ω—ã–µ —Ñ–æ—Ä–º—ã –∏ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ —Ç–µ–∫—Å—Ç—É—Ä—ã", "desc": "Hunyuan3D 2.5 –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –Ω–∞–±–æ—Ä –º–æ–¥–µ–ª–µ–π –¥–∏—Ñ—Ñ—É–∑–∏–∏ –¥–ª—è 3D-–≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, —É–ª—É—á—à–∞—é—â–∏–π —Å–æ–∑–¥–∞–Ω–∏–µ —Ñ–æ—Ä–º –∏ —Ç–µ–∫—Å—Ç—É—Ä. –ö–ª—é—á–µ–≤–æ–µ –Ω–æ–≤–æ–≤–≤–µ–¥–µ–Ω–∏–µ - –º–æ–¥–µ–ª—å LATTICE –¥–ª—è –≥–µ
[23.06.2025 05:15] Using data from previous issue: {"categories": ["#synthetic", "#multimodal", "#dataset", "#interpretability", "#alignment", "#training", "#benchmark"], "emoji": "üîç", "ru": {"title": "–£–ª—É—á—à–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ–±–∑–æ—Ä–æ–≤ —Å —Ä–∞–∑–Ω—ã—Ö —Ç–æ—á–µ–∫ –∑—Ä–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ LLM", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ —É–ª—É—á—à–µ–Ω–∏—é –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü
[23.06.2025 05:15] Renaming data file.
[23.06.2025 05:15] Renaming previous data. hf_papers.json to ./d/2025-06-23.json
[23.06.2025 05:15] Saving new data file.
[23.06.2025 05:15] Generating page.
[23.06.2025 05:15] Renaming previous page.
[23.06.2025 05:15] Renaming previous data. index.html to ./d/2025-06-23.html
[23.06.2025 05:15] Writing result.
[23.06.2025 05:15] Renaming log file.
[23.06.2025 05:15] Renaming previous data. log.txt to ./logs/2025-06-23_last_log.txt
