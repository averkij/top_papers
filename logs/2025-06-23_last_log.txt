[23.06.2025 02:56] Read previous papers.
[23.06.2025 02:56] Generating top page (month).
[23.06.2025 02:56] Writing top page (month).
[23.06.2025 03:51] Read previous papers.
[23.06.2025 03:51] Get feed.
[23.06.2025 03:51] Extract page data from URL. URL: https://huggingface.co/papers/2506.09049
[23.06.2025 03:51] Extract page data from URL. URL: https://huggingface.co/papers/2506.17206
[23.06.2025 03:51] Get page data from previous paper. URL: https://huggingface.co/papers/2506.16504
[23.06.2025 03:51] Extract page data from URL. URL: https://huggingface.co/papers/2506.15925
[23.06.2025 03:51] Extract page data from URL. URL: https://huggingface.co/papers/2506.17201
[23.06.2025 03:51] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[23.06.2025 03:51] No deleted papers detected.
[23.06.2025 03:51] Downloading and parsing papers (pdf, html). Total: 5.
[23.06.2025 03:51] Downloading and parsing paper https://huggingface.co/papers/2506.09049.
[23.06.2025 03:51] Downloading paper 2506.09049 from http://arxiv.org/pdf/2506.09049v1...
[23.06.2025 03:52] Extracting affiliations from text.
[23.06.2025 03:52] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 0 1 ] . [ 1 9 4 0 9 0 . 6 0 5 2 : r VIKI-R: Coordinating Embodied Multi-Agent Cooperation via Reinforcement Learning Li Kang1,2, Xiufeng Song1,2, Heng Zhou2,3, Yiran Qin2,5, Jie Yang5, Xiaohong Liu1, Philip Torr4, Lei Bai2, Zhenfei Yin4 1Shanghai Jiao Tong University 2Shanghai Artificial Intelligence Laboratory 3University of Science and Technology of China 4University of Oxford 5The Chinese University of Hong Kong, Shenzhen {faceong02, sparklexfantasy, hengzzzhou}@gmail.com Equal contribution Corresponding author https://faceong.github.io/VIKI-R/ Figure 1: Embodied multi-agent cooperation involves two key aspects: (1) cross-embodiment collaboration, where different embodiments are required for different tasks (e.g., washing requires humanoid, while only wheeled robots can fetch from high cabinets); and (2) efficient coordination, where agents work in parallel (e.g., multiple arms passing apples while humanoid washes them) to improve overall efficiency. To support such fine-grained teamwork, we propose VIKI-Bench , which structures the process into three levels of visual reasoning: Level 1 agent activation, Level 2 task planning, and Level 3 trajectory perception, aiming to realize an embodied multi-agent system. "
[23.06.2025 03:52] Response: ```python
[
    "Shanghai Jiao Tong University",
    "Shanghai Artificial Intelligence Laboratory",
    "University of Science and Technology of China",
    "University of Oxford",
    "The Chinese University of Hong Kong, Shenzhen"
]
```
[23.06.2025 03:52] Deleting PDF ./assets/pdf/2506.09049.pdf.
[23.06.2025 03:52] Success.
[23.06.2025 03:52] Downloading and parsing paper https://huggingface.co/papers/2506.17206.
[23.06.2025 03:52] Downloading paper 2506.17206 from http://arxiv.org/pdf/2506.17206v1...
[23.06.2025 03:52] Extracting affiliations from text.
[23.06.2025 03:52] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"DreamCube: 3D Panorama Generation via Multi-plane Synchronization Yukun Huang1, Yanning Zhou2, Jianan Wang3, Kaiyi Huang1, Xihui Liu1 1The University of Hong Kong 2Tencent 3Astribot https://yukun-huang.github.io/DreamCube/ 5 2 0 J 0 2 ] . [ 1 6 0 2 7 1 . 6 0 5 2 : r Figure 1. In this work, we introduce Multi-plane Synchronization to generalize 2D diffusion models to multi-plane omnidirectional representations (i.e., cubemaps), and DreamCube for RGB-D cubemap generation. The proposed approaches can be applied to different tasks including RGB-D panorama generation, panorama depth estimation, and 3D scene generation. "
[23.06.2025 03:52] Response: ```python
["The University of Hong Kong", "Tencent", "Astribot"]
```
[23.06.2025 03:52] Deleting PDF ./assets/pdf/2506.17206.pdf.
[23.06.2025 03:52] Success.
[23.06.2025 03:52] Downloading and parsing paper https://huggingface.co/papers/2506.16504.
[23.06.2025 03:52] Extra JSON file exists (./assets/json/2506.16504.json), skip PDF parsing.
[23.06.2025 03:52] Paper image links file exists (./assets/img_data/2506.16504.json), skip HTML parsing.
[23.06.2025 03:52] Success.
[23.06.2025 03:52] Downloading and parsing paper https://huggingface.co/papers/2506.15925.
[23.06.2025 03:52] Downloading paper 2506.15925 from http://arxiv.org/pdf/2506.15925v1...
[23.06.2025 03:52] Extracting affiliations from text.
[23.06.2025 03:52] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Reranking-based Generation for Unbiased Perspective Summarization Narutatsu Ri and Nicholas Deas and Kathleen McKeown Department of Computer Science, Columbia University {wl2787, nid2107, km}@columbia.edu 5 2 0 2 9 1 ] . [ 1 5 2 9 5 1 . 6 0 5 2 : r Abstract Generating unbiased summaries in real-world settings such as political perspective summarization remains crucial application of Large Language Models (LLMs). Yet, existing evaluation frameworks rely on traditional metrics for measuring key attributes such as coverage and faithfulness without verifying their applicability, and efforts to develop improved summarizers are still nascent. We address these gaps by (1) identifying reliable metrics for measuring perspective summary quality, and (2) investigating the efficacy of LLM-based methods beyond zero-shot inference. Namely, we build test set for benchmarking metric reliability using human annotations and show that traditional metrics underperform compared to language modelbased metrics, which prove to be strong evaluators. Using these metrics, we show that reranking-based methods yield strong results, and preference tuning with synthetically generated and reranking-labeled data further boosts performance. Our findings aim to contribute to the reliable evaluation and development of perspective summarization methods. (Zhang et al., 2024c; Feng et al., 2024), or preserve the source perspectives (Lei et al., 2024; Liu et al., 2024b). Within this domain, perspective summarization (Deas and McKeown, 2025) serves as representative evaluation setting, where models are tasked to generate precise, perspective-specific summaries from multi-document inputs containing diverse political views. However, two gaps remain unaddressed in this setting: (1) existing evaluation metrics are primarily derived from news summarization domains and have not been validated for measuring perspective summary quality, and (2) the effectiveness of LLM-based methods beyond zeroshot inference in ge"
[23.06.2025 03:52] Response: ```python
["Department of Computer Science, Columbia University"]
```
[23.06.2025 03:52] Deleting PDF ./assets/pdf/2506.15925.pdf.
[23.06.2025 03:52] Success.
[23.06.2025 03:52] Downloading and parsing paper https://huggingface.co/papers/2506.17201.
[23.06.2025 03:52] Downloading paper 2506.17201 from http://arxiv.org/pdf/2506.17201v1...
[23.06.2025 03:52] Extracting affiliations from text.
[23.06.2025 03:52] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 0 2 ] . [ 1 1 0 2 7 1 . 6 0 5 2 : r Hunyuan-GameCraft: High-dynamic Interactive Game Video Generation with Hybrid History Condition Jiaqi Li1,2* Junshu Tang1* Zhiyong Xu1 Longhuang Wu1 Yuan Zhou1 Shuai Shao Tianbao Yu1 Zhiguo Cao2 Qinglin Lu1 1 Tencent Hunyuan 2 Huazhong University of Science and Technology https://hunyuan-gamecraft.github.io/ Figure 1. Hunyuan-GameCraft can create high-dynamic interactive game video content from single image and corresponding prompt. We simulate series of action signals. The left and right frames depict key moments from game video sequences generated in response to different inputs. Hunyuan-GameCraft can accurately produce content aligned with each interaction, supports long-term video generation with temporal and 3D consistency, and effectively preserves historical scene information throughout the sequence. In this case, W, A, S, represent transition movement and , , , denote changes in view angles. "
[23.06.2025 03:52] Response: ```python
["Tencent Hunyuan", "Huazhong University of Science and Technology"]
```
[23.06.2025 03:52] Deleting PDF ./assets/pdf/2506.17201.pdf.
[23.06.2025 03:52] Success.
[23.06.2025 03:52] Enriching papers with extra data.
[23.06.2025 03:52] ********************************************************************************
[23.06.2025 03:52] Abstract 0. VIKI-Bench and VIKI-R provide a benchmark and framework for evaluating and improving visual-driven cooperation among diverse embodied agents using vision-language models and reinforcement learning.  					AI-generated summary 				 Coordinating multiple embodied agents in dynamic environments remains ...
[23.06.2025 03:52] ********************************************************************************
[23.06.2025 03:52] Abstract 1. Multi-plane synchronization extends 2D foundation models to 3D panorama generation, introducing DreamCube to achieve diverse appearances and accurate geometry.  					AI-generated summary 				 3D panorama synthesis is a promising yet challenging task that demands high-quality and diverse visual appea...
[23.06.2025 03:52] ********************************************************************************
[23.06.2025 03:52] Abstract 2. Hunyuan3D 2.5, a suite of 3D diffusion models, advances shape and texture generation with a new LATTICE model and physical-based rendering in a multi-view architecture.  					AI-generated summary 				 In this report, we present Hunyuan3D 2.5, a robust suite of 3D diffusion models aimed at generating...
[23.06.2025 03:52] ********************************************************************************
[23.06.2025 03:52] Abstract 3. Reranking and preference tuning improve the quality of perspective summaries generated by LLMs, as measured by language model-based metrics that outperform traditional ones.  					AI-generated summary 				 Generating unbiased summaries in real-world settings such as political perspective summarizati...
[23.06.2025 03:52] ********************************************************************************
[23.06.2025 03:52] Abstract 4. Hunyuan-GameCraft is a novel framework for high-dynamic interactive video generation in game environments that addresses limitations in dynamics, generality, and efficiency through unified input representation, hybrid history-conditioned training, and model distillation.  					AI-generated summary 	...
[23.06.2025 03:52] Read previous papers.
[23.06.2025 03:52] Generating reviews via LLM API.
[23.06.2025 03:52] Querying the API.
[23.06.2025 03:52] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

VIKI-Bench and VIKI-R provide a benchmark and framework for evaluating and improving visual-driven cooperation among diverse embodied agents using vision-language models and reinforcement learning.  					AI-generated summary 				 Coordinating multiple embodied agents in dynamic environments remains a core challenge in artificial intelligence, requiring both perception-driven reasoning and scalable cooperation strategies. While recent works have leveraged large language models (LLMs) for multi-agent planning, a few have begun to explore vision-language models (VLMs) for visual reasoning. However, these VLM-based approaches remain limited in their support for diverse embodiment types. In this work, we introduce VIKI-Bench, the first hierarchical benchmark tailored for embodied multi-agent cooperation, featuring three structured levels: agent activation, task planning, and trajectory perception. VIKI-Bench includes diverse robot embodiments, multi-view visual observations, and structured supervision signals to evaluate reasoning grounded in visual inputs. To demonstrate the utility of VIKI-Bench, we propose VIKI-R, a two-stage framework that fine-tunes a pretrained vision-language model (VLM) using Chain-of-Thought annotated demonstrations, followed by reinforcement learning under multi-level reward signals. Our extensive experiments show that VIKI-R significantly outperforms baselines method across all task levels. Furthermore, we show that reinforcement learning enables the emergence of compositional cooperation patterns among heterogeneous agents. Together, VIKI-Bench and VIKI-R offer a unified testbed and method for advancing multi-agent, visual-driven cooperation in embodied AI systems.
[23.06.2025 03:52] Response: {
  "desc": "VIKI-Bench представляет собой иерархический бенчмарк для оценки кооперации между воплощенными агентами с использованием визуально-языковых моделей. VIKI-R - это двухэтапный фреймворк, который дообучает предобученную визуально-языковую модель с помощью аннотированных демонстраций и обучения с подкреплением. Эксперименты показывают, что VIKI-R значительно превосходит базовые методы на всех уровнях задач. Обучение с подкреплением позволяет появиться композиционным паттернам кооперации между разнородными агентами.",
  "emoji": "🤖",
  "title": "Визуальная кооперация агентов с помощью глубокого обучения"
}
[23.06.2025 03:52] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"VIKI-Bench and VIKI-R provide a benchmark and framework for evaluating and improving visual-driven cooperation among diverse embodied agents using vision-language models and reinforcement learning.  					AI-generated summary 				 Coordinating multiple embodied agents in dynamic environments remains a core challenge in artificial intelligence, requiring both perception-driven reasoning and scalable cooperation strategies. While recent works have leveraged large language models (LLMs) for multi-agent planning, a few have begun to explore vision-language models (VLMs) for visual reasoning. However, these VLM-based approaches remain limited in their support for diverse embodiment types. In this work, we introduce VIKI-Bench, the first hierarchical benchmark tailored for embodied multi-agent cooperation, featuring three structured levels: agent activation, task planning, and trajectory perception. VIKI-Bench includes diverse robot embodiments, multi-view visual observations, and structured supervision signals to evaluate reasoning grounded in visual inputs. To demonstrate the utility of VIKI-Bench, we propose VIKI-R, a two-stage framework that fine-tunes a pretrained vision-language model (VLM) using Chain-of-Thought annotated demonstrations, followed by reinforcement learning under multi-level reward signals. Our extensive experiments show that VIKI-R significantly outperforms baselines method across all task levels. Furthermore, we show that reinforcement learning enables the emergence of compositional cooperation patterns among heterogeneous agents. Together, VIKI-Bench and VIKI-R offer a unified testbed and method for advancing multi-agent, visual-driven cooperation in embodied AI systems."

[23.06.2025 03:52] Response: ```python
['BENCHMARK', 'AGENTS', 'CV', 'RL']
```
[23.06.2025 03:52] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"VIKI-Bench and VIKI-R provide a benchmark and framework for evaluating and improving visual-driven cooperation among diverse embodied agents using vision-language models and reinforcement learning.  					AI-generated summary 				 Coordinating multiple embodied agents in dynamic environments remains a core challenge in artificial intelligence, requiring both perception-driven reasoning and scalable cooperation strategies. While recent works have leveraged large language models (LLMs) for multi-agent planning, a few have begun to explore vision-language models (VLMs) for visual reasoning. However, these VLM-based approaches remain limited in their support for diverse embodiment types. In this work, we introduce VIKI-Bench, the first hierarchical benchmark tailored for embodied multi-agent cooperation, featuring three structured levels: agent activation, task planning, and trajectory perception. VIKI-Bench includes diverse robot embodiments, multi-view visual observations, and structured supervision signals to evaluate reasoning grounded in visual inputs. To demonstrate the utility of VIKI-Bench, we propose VIKI-R, a two-stage framework that fine-tunes a pretrained vision-language model (VLM) using Chain-of-Thought annotated demonstrations, followed by reinforcement learning under multi-level reward signals. Our extensive experiments show that VIKI-R significantly outperforms baselines method across all task levels. Furthermore, we show that reinforcement learning enables the emergence of compositional cooperation patterns among heterogeneous agents. Together, VIKI-Bench and VIKI-R offer a unified testbed and method for advancing multi-agent, visual-driven cooperation in embodied AI systems."

[23.06.2025 03:52] Response: ```python
['GAMES', 'REASONING']
```
[23.06.2025 03:52] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces VIKI-Bench and VIKI-R, a benchmark and framework designed to enhance cooperation among various embodied agents using vision-language models and reinforcement learning. VIKI-Bench is structured into three levels: agent activation, task planning, and trajectory perception, allowing for comprehensive evaluation of visual reasoning in multi-agent scenarios. VIKI-R fine-tunes a pretrained vision-language model with Chain-of-Thought demonstrations and applies reinforcement learning to optimize performance across different tasks. The results demonstrate that VIKI-R significantly improves cooperation patterns among diverse agents, showcasing the potential of visual-driven strategies in embodied AI.","title":"Enhancing Multi-Agent Cooperation with VIKI-Bench and VIKI-R"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper introduces VIKI-Bench and VIKI-R, a benchmark and framework designed to enhance cooperation among various embodied agents using vision-language models and reinforcement learning. VIKI-Bench is structured into three levels: agent activation, task planning, and trajectory perception, allowing for comprehensive evaluation of visual reasoning in multi-agent scenarios. VIKI-R fine-tunes a pretrained vision-language model with Chain-of-Thought demonstrations and applies reinforcement learning to optimize performance across different tasks. The results demonstrate that VIKI-R significantly improves cooperation patterns among diverse agents, showcasing the potential of visual-driven strategies in embodied AI.', title='Enhancing Multi-Agent Cooperation with VIKI-Bench and VIKI-R'))
[23.06.2025 03:52] Response: ParsedChatCompletionMessage[Article](content='{"desc":"VIKI-Bench和VIKI-R是用于评估和改善多样化具身智能体之间视觉驱动合作的基准和框架。该研究提出了一个分层基准，包含代理激活、任务规划和轨迹感知三个结构化层次。VIKI-R是一个两阶段框架，通过链式思维注释示例微调预训练的视觉语言模型，并在多层次奖励信号下进行强化学习。实验结果表明，VIKI-R在所有任务层次上显著优于基线方法，并且强化学习促进了异构智能体之间的组合合作模式。","title":"VIKI-Bench与VIKI-R：推动具身智能体的视觉驱动合作"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='VIKI-Bench和VIKI-R是用于评估和改善多样化具身智能体之间视觉驱动合作的基准和框架。该研究提出了一个分层基准，包含代理激活、任务规划和轨迹感知三个结构化层次。VIKI-R是一个两阶段框架，通过链式思维注释示例微调预训练的视觉语言模型，并在多层次奖励信号下进行强化学习。实验结果表明，VIKI-R在所有任务层次上显著优于基线方法，并且强化学习促进了异构智能体之间的组合合作模式。', title='VIKI-Bench与VIKI-R：推动具身智能体的视觉驱动合作'))
[23.06.2025 03:52] Querying the API.
[23.06.2025 03:52] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Multi-plane synchronization extends 2D foundation models to 3D panorama generation, introducing DreamCube to achieve diverse appearances and accurate geometry.  					AI-generated summary 				 3D panorama synthesis is a promising yet challenging task that demands high-quality and diverse visual appearance and geometry of the generated omnidirectional content. Existing methods leverage rich image priors from pre-trained 2D foundation models to circumvent the scarcity of 3D panoramic data, but the incompatibility between 3D panoramas and 2D single views limits their effectiveness. In this work, we demonstrate that by applying multi-plane synchronization to the operators from 2D foundation models, their capabilities can be seamlessly extended to the omnidirectional domain. Based on this design, we further introduce DreamCube, a multi-plane RGB-D diffusion model for 3D panorama generation, which maximizes the reuse of 2D foundation model priors to achieve diverse appearances and accurate geometry while maintaining multi-view consistency. Extensive experiments demonstrate the effectiveness of our approach in panoramic image generation, panoramic depth estimation, and 3D scene generation.
[23.06.2025 03:52] Response: {
  "desc": "Эта статья представляет новый подход к генерации трехмерных панорам с использованием многоплоскостной синхронизации двумерных фундаментальных моделей. Авторы предлагают модель DreamCube, которая максимально использует преимущества двумерных моделей для создания разнообразных и геометрически точных трехмерных панорам. Метод решает проблему несовместимости между трехмерными панорамами и двумерными изображениями, расширяя возможности существующих алгоритмов. Эксперименты показывают эффективность подхода в генерации панорамных изображений, оценке глубины и создании трехмерных сцен.",
  "emoji": "🌐",
  "title": "Многоплоскостная синхронизация: мост между 2D и 3D в генерации панорам"
}
[23.06.2025 03:52] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Multi-plane synchronization extends 2D foundation models to 3D panorama generation, introducing DreamCube to achieve diverse appearances and accurate geometry.  					AI-generated summary 				 3D panorama synthesis is a promising yet challenging task that demands high-quality and diverse visual appearance and geometry of the generated omnidirectional content. Existing methods leverage rich image priors from pre-trained 2D foundation models to circumvent the scarcity of 3D panoramic data, but the incompatibility between 3D panoramas and 2D single views limits their effectiveness. In this work, we demonstrate that by applying multi-plane synchronization to the operators from 2D foundation models, their capabilities can be seamlessly extended to the omnidirectional domain. Based on this design, we further introduce DreamCube, a multi-plane RGB-D diffusion model for 3D panorama generation, which maximizes the reuse of 2D foundation model priors to achieve diverse appearances and accurate geometry while maintaining multi-view consistency. Extensive experiments demonstrate the effectiveness of our approach in panoramic image generation, panoramic depth estimation, and 3D scene generation."

[23.06.2025 03:52] Response: ```python
['3D', 'CV']
```
[23.06.2025 03:52] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Multi-plane synchronization extends 2D foundation models to 3D panorama generation, introducing DreamCube to achieve diverse appearances and accurate geometry.  					AI-generated summary 				 3D panorama synthesis is a promising yet challenging task that demands high-quality and diverse visual appearance and geometry of the generated omnidirectional content. Existing methods leverage rich image priors from pre-trained 2D foundation models to circumvent the scarcity of 3D panoramic data, but the incompatibility between 3D panoramas and 2D single views limits their effectiveness. In this work, we demonstrate that by applying multi-plane synchronization to the operators from 2D foundation models, their capabilities can be seamlessly extended to the omnidirectional domain. Based on this design, we further introduce DreamCube, a multi-plane RGB-D diffusion model for 3D panorama generation, which maximizes the reuse of 2D foundation model priors to achieve diverse appearances and accurate geometry while maintaining multi-view consistency. Extensive experiments demonstrate the effectiveness of our approach in panoramic image generation, panoramic depth estimation, and 3D scene generation."

[23.06.2025 03:52] Response: ```python
["DIFFUSION", "OPTIMIZATION"]
```
[23.06.2025 03:52] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents a method called DreamCube that enhances 2D foundation models for generating 3D panoramas. It addresses the challenge of creating high-quality and diverse omnidirectional content by using multi-plane synchronization. This technique allows the model to effectively utilize existing 2D image data to improve the geometry and appearance of 3D images. The results show that DreamCube can generate consistent panoramic images and accurately estimate depth, making it a significant advancement in 3D scene generation.","title":"DreamCube: Bridging 2D and 3D for Stunning Panoramas"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper presents a method called DreamCube that enhances 2D foundation models for generating 3D panoramas. It addresses the challenge of creating high-quality and diverse omnidirectional content by using multi-plane synchronization. This technique allows the model to effectively utilize existing 2D image data to improve the geometry and appearance of 3D images. The results show that DreamCube can generate consistent panoramic images and accurately estimate depth, making it a significant advancement in 3D scene generation.', title='DreamCube: Bridging 2D and 3D for Stunning Panoramas'))
[23.06.2025 03:52] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本论文提出了一种名为DreamCube的多平面同步方法，旨在将二维基础模型扩展到三维全景生成。该方法通过利用预训练的二维模型的丰富图像先验，克服了三维全景数据稀缺的问题。我们的方法能够实现多样化的视觉效果和准确的几何形状，同时保持多视图的一致性。实验结果表明，我们的技术在全景图像生成、深度估计和三维场景生成方面表现出色。","title":"多平面同步，开启三维全景新视界"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本论文提出了一种名为DreamCube的多平面同步方法，旨在将二维基础模型扩展到三维全景生成。该方法通过利用预训练的二维模型的丰富图像先验，克服了三维全景数据稀缺的问题。我们的方法能够实现多样化的视觉效果和准确的几何形状，同时保持多视图的一致性。实验结果表明，我们的技术在全景图像生成、深度估计和三维场景生成方面表现出色。', title='多平面同步，开启三维全景新视界'))
[23.06.2025 03:52] Using data from previous issue: {"categories": ["#diffusion", "#3d"], "emoji": "🧊", "ru": {"title": "Новый уровень 3D-генерации: точные формы и реалистичные текстуры", "desc": "Hunyuan3D 2.5 представляет собой набор моделей диффузии для 3D-генерации, улучшающий создание форм и текстур. Ключевое нововведение - модель LATTICE для ге
[23.06.2025 03:52] Querying the API.
[23.06.2025 03:52] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Reranking and preference tuning improve the quality of perspective summaries generated by LLMs, as measured by language model-based metrics that outperform traditional ones.  					AI-generated summary 				 Generating unbiased summaries in real-world settings such as political perspective summarization remains a crucial application of Large Language Models (LLMs). Yet, existing evaluation frameworks rely on traditional metrics for measuring key attributes such as coverage and faithfulness without verifying their applicability, and efforts to develop improved summarizers are still nascent. We address these gaps by (1) identifying reliable metrics for measuring perspective summary quality, and (2) investigating the efficacy of LLM-based methods beyond zero-shot inference. Namely, we build a test set for benchmarking metric reliability using human annotations and show that traditional metrics underperform compared to language model-based metrics, which prove to be strong evaluators. Using these metrics, we show that reranking-based methods yield strong results, and preference tuning with synthetically generated and reranking-labeled data further boosts performance. Our findings aim to contribute to the reliable evaluation and development of perspective summarization methods.
[23.06.2025 03:52] Response: {
  "desc": "Исследование посвящено улучшению качества генерации кратких обзоров с учетом различных точек зрения с помощью больших языковых моделей (БЯМ). Авторы разработали надежные метрики для оценки качества таких обзоров, основанные на языковых моделях. Они показали, что методы ранжирования и настройки предпочтений с использованием синтетических данных значительно повышают эффективность генерации. Результаты исследования вносят вклад в развитие методов надежной оценки и создания систем суммаризации с учетом различных перспектив.",
  "emoji": "🔍",
  "title": "Улучшение генерации обзоров с разных точек зрения с помощью ранжирования и настройки БЯМ"
}
[23.06.2025 03:52] Renaming some terms.
[23.06.2025 03:52] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Reranking and preference tuning improve the quality of perspective summaries generated by LLMs, as measured by language model-based metrics that outperform traditional ones.  					AI-generated summary 				 Generating unbiased summaries in real-world settings such as political perspective summarization remains a crucial application of Large Language Models (LLMs). Yet, existing evaluation frameworks rely on traditional metrics for measuring key attributes such as coverage and faithfulness without verifying their applicability, and efforts to develop improved summarizers are still nascent. We address these gaps by (1) identifying reliable metrics for measuring perspective summary quality, and (2) investigating the efficacy of LLM-based methods beyond zero-shot inference. Namely, we build a test set for benchmarking metric reliability using human annotations and show that traditional metrics underperform compared to language model-based metrics, which prove to be strong evaluators. Using these metrics, we show that reranking-based methods yield strong results, and preference tuning with synthetically generated and reranking-labeled data further boosts performance. Our findings aim to contribute to the reliable evaluation and development of perspective summarization methods."

[23.06.2025 03:52] Response: ```python
["DATASET", "BENCHMARK", "MULTIMODAL", "TRAINING"]
```
[23.06.2025 03:52] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Reranking and preference tuning improve the quality of perspective summaries generated by LLMs, as measured by language model-based metrics that outperform traditional ones.  					AI-generated summary 				 Generating unbiased summaries in real-world settings such as political perspective summarization remains a crucial application of Large Language Models (LLMs). Yet, existing evaluation frameworks rely on traditional metrics for measuring key attributes such as coverage and faithfulness without verifying their applicability, and efforts to develop improved summarizers are still nascent. We address these gaps by (1) identifying reliable metrics for measuring perspective summary quality, and (2) investigating the efficacy of LLM-based methods beyond zero-shot inference. Namely, we build a test set for benchmarking metric reliability using human annotations and show that traditional metrics underperform compared to language model-based metrics, which prove to be strong evaluators. Using these metrics, we show that reranking-based methods yield strong results, and preference tuning with synthetically generated and reranking-labeled data further boosts performance. Our findings aim to contribute to the reliable evaluation and development of perspective summarization methods."

[23.06.2025 03:52] Response: ```python
['ALIGNMENT', 'INTERPRETABILITY', 'SYNTHETIC']
```
[23.06.2025 03:52] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper discusses how to improve the quality of summaries generated by Large Language Models (LLMs) for political perspectives. It highlights the limitations of traditional evaluation metrics, which do not effectively measure important aspects like coverage and faithfulness. The authors propose new, reliable metrics based on language models and demonstrate that these outperform traditional methods. They also show that using reranking and preference tuning techniques can significantly enhance the performance of LLM-generated summaries.","title":"Enhancing Perspective Summaries with Reranking and Preference Tuning"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper discusses how to improve the quality of summaries generated by Large Language Models (LLMs) for political perspectives. It highlights the limitations of traditional evaluation metrics, which do not effectively measure important aspects like coverage and faithfulness. The authors propose new, reliable metrics based on language models and demonstrate that these outperform traditional methods. They also show that using reranking and preference tuning techniques can significantly enhance the performance of LLM-generated summaries.', title='Enhancing Perspective Summaries with Reranking and Preference Tuning'))
[23.06.2025 03:52] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本文探讨了如何提高大型语言模型（LLMs）生成的观点摘要的质量。我们发现，传统的评估指标在测量摘要的覆盖率和忠实度时效果不佳，而基于语言模型的指标表现更为出色。通过建立一个基准测试集并进行人类标注，我们验证了这些新指标的可靠性。最终，我们提出了重排序和偏好调优的方法，显著提升了摘要生成的效果。","title":"提升观点摘要质量的关键在于重排序与偏好调优"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本文探讨了如何提高大型语言模型（LLMs）生成的观点摘要的质量。我们发现，传统的评估指标在测量摘要的覆盖率和忠实度时效果不佳，而基于语言模型的指标表现更为出色。通过建立一个基准测试集并进行人类标注，我们验证了这些新指标的可靠性。最终，我们提出了重排序和偏好调优的方法，显著提升了摘要生成的效果。', title='提升观点摘要质量的关键在于重排序与偏好调优'))
[23.06.2025 03:52] Querying the API.
[23.06.2025 03:52] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Hunyuan-GameCraft is a novel framework for high-dynamic interactive video generation in game environments that addresses limitations in dynamics, generality, and efficiency through unified input representation, hybrid history-conditioned training, and model distillation.  					AI-generated summary 				 Recent advances in diffusion-based and controllable video generation have enabled high-quality and temporally coherent video synthesis, laying the groundwork for immersive interactive gaming experiences. However, current methods face limitations in dynamics, generality, long-term consistency, and efficiency, which limit the ability to create various gameplay videos. To address these gaps, we introduce Hunyuan-GameCraft, a novel framework for high-dynamic interactive video generation in game environments. To achieve fine-grained action control, we unify standard keyboard and mouse inputs into a shared camera representation space, facilitating smooth interpolation between various camera and movement operations. Then we propose a hybrid history-conditioned training strategy that extends video sequences autoregressively while preserving game scene information. Additionally, to enhance inference efficiency and playability, we achieve model distillation to reduce computational overhead while maintaining consistency across long temporal sequences, making it suitable for real-time deployment in complex interactive environments. The model is trained on a large-scale dataset comprising over one million gameplay recordings across over 100 AAA games, ensuring broad coverage and diversity, then fine-tuned on a carefully annotated synthetic dataset to enhance precision and control. The curated game scene data significantly improves the visual fidelity, realism and action controllability. Extensive experiments demonstrate that Hunyuan-GameCraft significantly outperforms existing models, advancing the realism and playability of interactive game video generation.
[23.06.2025 03:53] Response: {
  "desc": "Hunyuan-GameCraft - это новая система для генерации интерактивного видео в игровых средах с высокой динамикой. Она решает проблемы ограничений в динамике, универсальности и эффективности с помощью унифицированного представления ввода, гибридного обучения с учетом истории и дистилляции модели. Система обучается на масштабном наборе данных из более чем миллиона записей геймплея из более 100 ААА-игр, а затем дообучается на тщательно размеченном синтетическом наборе данных для повышения точности и контроля. Эксперименты показывают, что Hunyuan-GameCraft значительно превосходит существующие модели, повышая реалистичность и играбельность интерактивной генерации игрового видео.",
  "emoji": "🎮",
  "title": "Революция в генерации интерактивного игрового видео"
}
[23.06.2025 03:53] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Hunyuan-GameCraft is a novel framework for high-dynamic interactive video generation in game environments that addresses limitations in dynamics, generality, and efficiency through unified input representation, hybrid history-conditioned training, and model distillation.  					AI-generated summary 				 Recent advances in diffusion-based and controllable video generation have enabled high-quality and temporally coherent video synthesis, laying the groundwork for immersive interactive gaming experiences. However, current methods face limitations in dynamics, generality, long-term consistency, and efficiency, which limit the ability to create various gameplay videos. To address these gaps, we introduce Hunyuan-GameCraft, a novel framework for high-dynamic interactive video generation in game environments. To achieve fine-grained action control, we unify standard keyboard and mouse inputs into a shared camera representation space, facilitating smooth interpolation between various camera and movement operations. Then we propose a hybrid history-conditioned training strategy that extends video sequences autoregressively while preserving game scene information. Additionally, to enhance inference efficiency and playability, we achieve model distillation to reduce computational overhead while maintaining consistency across long temporal sequences, making it suitable for real-time deployment in complex interactive environments. The model is trained on a large-scale dataset comprising over one million gameplay recordings across over 100 AAA games, ensuring broad coverage and diversity, then fine-tuned on a carefully annotated synthetic dataset to enhance precision and control. The curated game scene data significantly improves the visual fidelity, realism and action controllability. Extensive experiments demonstrate that Hunyuan-GameCraft significantly outperforms existing models, advancing the realism and playability of interactive game video generation."

[23.06.2025 03:53] Response: ```python
['VIDEO', 'DATASET', 'TRAINING', 'INFERENCE']
```
[23.06.2025 03:53] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Hunyuan-GameCraft is a novel framework for high-dynamic interactive video generation in game environments that addresses limitations in dynamics, generality, and efficiency through unified input representation, hybrid history-conditioned training, and model distillation.  					AI-generated summary 				 Recent advances in diffusion-based and controllable video generation have enabled high-quality and temporally coherent video synthesis, laying the groundwork for immersive interactive gaming experiences. However, current methods face limitations in dynamics, generality, long-term consistency, and efficiency, which limit the ability to create various gameplay videos. To address these gaps, we introduce Hunyuan-GameCraft, a novel framework for high-dynamic interactive video generation in game environments. To achieve fine-grained action control, we unify standard keyboard and mouse inputs into a shared camera representation space, facilitating smooth interpolation between various camera and movement operations. Then we propose a hybrid history-conditioned training strategy that extends video sequences autoregressively while preserving game scene information. Additionally, to enhance inference efficiency and playability, we achieve model distillation to reduce computational overhead while maintaining consistency across long temporal sequences, making it suitable for real-time deployment in complex interactive environments. The model is trained on a large-scale dataset comprising over one million gameplay recordings across over 100 AAA games, ensuring broad coverage and diversity, then fine-tuned on a carefully annotated synthetic dataset to enhance precision and control. The curated game scene data significantly improves the visual fidelity, realism and action controllability. Extensive experiments demonstrate that Hunyuan-GameCraft significantly outperforms existing models, advancing the realism and playability of interactive game video generation."

[23.06.2025 03:53] Response: ```python
['GAMES', 'DIFFUSION', 'SYNTHETIC']
```
[23.06.2025 03:53] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Hunyuan-GameCraft is a new framework designed for generating interactive videos in gaming environments, overcoming issues related to dynamics, generality, and efficiency. It uses a unified input representation that combines keyboard and mouse controls, allowing for smooth transitions in camera and movement actions. The framework employs a hybrid history-conditioned training method to extend video sequences while keeping important game scene details intact. Additionally, model distillation is utilized to improve computational efficiency, making it suitable for real-time applications in complex gaming scenarios.","title":"Revolutionizing Interactive Video Generation in Gaming"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Hunyuan-GameCraft is a new framework designed for generating interactive videos in gaming environments, overcoming issues related to dynamics, generality, and efficiency. It uses a unified input representation that combines keyboard and mouse controls, allowing for smooth transitions in camera and movement actions. The framework employs a hybrid history-conditioned training method to extend video sequences while keeping important game scene details intact. Additionally, model distillation is utilized to improve computational efficiency, making it suitable for real-time applications in complex gaming scenarios.', title='Revolutionizing Interactive Video Generation in Gaming'))
[23.06.2025 03:53] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Hunyuan-GameCraft是一个新颖的框架，旨在解决游戏环境中高动态互动视频生成的局限性。它通过统一输入表示、混合历史条件训练和模型蒸馏来提高动态性、通用性和效率。该框架能够实现精细的动作控制，并在复杂的互动环境中保持长时间的一致性。经过大规模数据集的训练，Hunyuan-GameCraft在生成互动游戏视频的真实感和可玩性方面显著优于现有模型。","title":"Hunyuan-GameCraft：提升互动游戏视频生成的真实感与可玩性"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Hunyuan-GameCraft是一个新颖的框架，旨在解决游戏环境中高动态互动视频生成的局限性。它通过统一输入表示、混合历史条件训练和模型蒸馏来提高动态性、通用性和效率。该框架能够实现精细的动作控制，并在复杂的互动环境中保持长时间的一致性。经过大规模数据集的训练，Hunyuan-GameCraft在生成互动游戏视频的真实感和可玩性方面显著优于现有模型。', title='Hunyuan-GameCraft：提升互动游戏视频生成的真实感与可玩性'))
[23.06.2025 03:53] Renaming data file.
[23.06.2025 03:53] Renaming previous data. hf_papers.json to ./d/2025-06-23.json
[23.06.2025 03:53] Saving new data file.
[23.06.2025 03:53] Generating page.
[23.06.2025 03:53] Renaming previous page.
[23.06.2025 03:53] Renaming previous data. index.html to ./d/2025-06-23.html
[23.06.2025 03:53] Writing result.
[23.06.2025 03:53] Renaming log file.
[23.06.2025 03:53] Renaming previous data. log.txt to ./logs/2025-06-23_last_log.txt
