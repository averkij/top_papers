[23.06.2025 15:12] Read previous papers.
[23.06.2025 15:12] Generating top page (month).
[23.06.2025 15:12] Writing top page (month).
[23.06.2025 16:16] Read previous papers.
[23.06.2025 16:16] Get feed.
[23.06.2025 16:16] Get page data from previous paper. URL: https://huggingface.co/papers/2506.16406
[23.06.2025 16:16] Get page data from previous paper. URL: https://huggingface.co/papers/2506.16035
[23.06.2025 16:16] Get page data from previous paper. URL: https://huggingface.co/papers/2506.16054
[23.06.2025 16:16] Get page data from previous paper. URL: https://huggingface.co/papers/2506.09049
[23.06.2025 16:16] Get page data from previous paper. URL: https://huggingface.co/papers/2506.17201
[23.06.2025 16:16] Get page data from previous paper. URL: https://huggingface.co/papers/2506.16310
[23.06.2025 16:16] Get page data from previous paper. URL: https://huggingface.co/papers/2506.17206
[23.06.2025 16:16] Get page data from previous paper. URL: https://huggingface.co/papers/2506.16504
[23.06.2025 16:16] Get page data from previous paper. URL: https://huggingface.co/papers/2506.15745
[23.06.2025 16:16] Get page data from previous paper. URL: https://huggingface.co/papers/2506.15442
[23.06.2025 16:16] Get page data from previous paper. URL: https://huggingface.co/papers/2506.17202
[23.06.2025 16:16] Get page data from previous paper. URL: https://huggingface.co/papers/2506.15925
[23.06.2025 16:16] Get page data from previous paper. URL: https://huggingface.co/papers/2506.17213
[23.06.2025 16:16] Extract page data from URL. URL: https://huggingface.co/papers/2506.09930
[23.06.2025 16:16] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[23.06.2025 16:16] No deleted papers detected.
[23.06.2025 16:16] Downloading and parsing papers (pdf, html). Total: 14.
[23.06.2025 16:16] Downloading and parsing paper https://huggingface.co/papers/2506.16406.
[23.06.2025 16:16] Extra JSON file exists (./assets/json/2506.16406.json), skip PDF parsing.
[23.06.2025 16:16] Paper image links file exists (./assets/img_data/2506.16406.json), skip HTML parsing.
[23.06.2025 16:16] Success.
[23.06.2025 16:16] Downloading and parsing paper https://huggingface.co/papers/2506.16035.
[23.06.2025 16:16] Extra JSON file exists (./assets/json/2506.16035.json), skip PDF parsing.
[23.06.2025 16:16] Paper image links file exists (./assets/img_data/2506.16035.json), skip HTML parsing.
[23.06.2025 16:16] Success.
[23.06.2025 16:16] Downloading and parsing paper https://huggingface.co/papers/2506.16054.
[23.06.2025 16:16] Extra JSON file exists (./assets/json/2506.16054.json), skip PDF parsing.
[23.06.2025 16:16] Paper image links file exists (./assets/img_data/2506.16054.json), skip HTML parsing.
[23.06.2025 16:16] Success.
[23.06.2025 16:16] Downloading and parsing paper https://huggingface.co/papers/2506.09049.
[23.06.2025 16:16] Extra JSON file exists (./assets/json/2506.09049.json), skip PDF parsing.
[23.06.2025 16:16] Paper image links file exists (./assets/img_data/2506.09049.json), skip HTML parsing.
[23.06.2025 16:16] Success.
[23.06.2025 16:16] Downloading and parsing paper https://huggingface.co/papers/2506.17201.
[23.06.2025 16:16] Extra JSON file exists (./assets/json/2506.17201.json), skip PDF parsing.
[23.06.2025 16:16] Paper image links file exists (./assets/img_data/2506.17201.json), skip HTML parsing.
[23.06.2025 16:16] Success.
[23.06.2025 16:16] Downloading and parsing paper https://huggingface.co/papers/2506.16310.
[23.06.2025 16:16] Extra JSON file exists (./assets/json/2506.16310.json), skip PDF parsing.
[23.06.2025 16:16] Paper image links file exists (./assets/img_data/2506.16310.json), skip HTML parsing.
[23.06.2025 16:16] Success.
[23.06.2025 16:16] Downloading and parsing paper https://huggingface.co/papers/2506.17206.
[23.06.2025 16:16] Extra JSON file exists (./assets/json/2506.17206.json), skip PDF parsing.
[23.06.2025 16:16] Paper image links file exists (./assets/img_data/2506.17206.json), skip HTML parsing.
[23.06.2025 16:16] Success.
[23.06.2025 16:16] Downloading and parsing paper https://huggingface.co/papers/2506.16504.
[23.06.2025 16:16] Extra JSON file exists (./assets/json/2506.16504.json), skip PDF parsing.
[23.06.2025 16:16] Paper image links file exists (./assets/img_data/2506.16504.json), skip HTML parsing.
[23.06.2025 16:16] Success.
[23.06.2025 16:16] Downloading and parsing paper https://huggingface.co/papers/2506.15745.
[23.06.2025 16:16] Extra JSON file exists (./assets/json/2506.15745.json), skip PDF parsing.
[23.06.2025 16:16] Paper image links file exists (./assets/img_data/2506.15745.json), skip HTML parsing.
[23.06.2025 16:16] Success.
[23.06.2025 16:16] Downloading and parsing paper https://huggingface.co/papers/2506.15442.
[23.06.2025 16:16] Extra JSON file exists (./assets/json/2506.15442.json), skip PDF parsing.
[23.06.2025 16:16] Paper image links file exists (./assets/img_data/2506.15442.json), skip HTML parsing.
[23.06.2025 16:16] Success.
[23.06.2025 16:16] Downloading and parsing paper https://huggingface.co/papers/2506.17202.
[23.06.2025 16:16] Extra JSON file exists (./assets/json/2506.17202.json), skip PDF parsing.
[23.06.2025 16:16] Paper image links file exists (./assets/img_data/2506.17202.json), skip HTML parsing.
[23.06.2025 16:16] Success.
[23.06.2025 16:16] Downloading and parsing paper https://huggingface.co/papers/2506.15925.
[23.06.2025 16:16] Extra JSON file exists (./assets/json/2506.15925.json), skip PDF parsing.
[23.06.2025 16:16] Paper image links file exists (./assets/img_data/2506.15925.json), skip HTML parsing.
[23.06.2025 16:16] Success.
[23.06.2025 16:16] Downloading and parsing paper https://huggingface.co/papers/2506.17213.
[23.06.2025 16:16] Extra JSON file exists (./assets/json/2506.17213.json), skip PDF parsing.
[23.06.2025 16:16] Paper image links file exists (./assets/img_data/2506.17213.json), skip HTML parsing.
[23.06.2025 16:16] Success.
[23.06.2025 16:16] Downloading and parsing paper https://huggingface.co/papers/2506.09930.
[23.06.2025 16:16] Downloading paper 2506.09930 from http://arxiv.org/pdf/2506.09930v1...
[23.06.2025 16:16] Extracting affiliations from text.
[23.06.2025 16:16] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 1 1 ] . [ 1 0 3 9 9 0 . 6 0 5 2 : r From Intention to Execution: Probing the Generalization Boundaries of Vision-Language-Action Models Irving Fang Juexiao Zhang Shengbang Tong Chen Feng "
[23.06.2025 16:16] Response: []
[23.06.2025 16:16] Extracting affiliations from text.
[23.06.2025 16:16] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 1 1 ] . [ 1 0 3 9 9 0 . 6 0 5 2 : r From Intention to Execution: Probing the Generalization Boundaries of Vision-Language-Action Models Irving Fang Juexiao Zhang Shengbang Tong Chen FengOne promise that Vision-Language-Action (VLA) models hold over traditional imitation learning for robotics is to leverage the broad generalization capabilities of large Vision-Language Models (VLMs) to produce versatile, generalist robot policies. However, current evaluations of VLAs remain insufficient. Traditional imitation learning benchmarks are unsuitable due to the lack of language instructions. Emerging benchmarks for VLAs that incorporate language often come with limited evaluation tasks and do not intend to investigate how much VLM pretraining truly contributes to the generalization capabilities of the downstream robotic policy. Meanwhile, much research relies on real-world robot setups designed in isolation by different institutions, which creates barrier for reproducibility and accessibility. To address this gap, we introduce unified probing suite of 50 simulation-based tasks across 10 subcategories spanning language instruction, vision, and objects. We systematically evaluate several state-of-the-art VLA architectures on this suite to understand their generalization capability. Our results show that while VLM backbones endow VLAs with robust perceptual understanding and high-level planning, which we refer to as good intentions, this does not reliably translate into precise motor execution: when faced with out-of-distribution observations, policies often exhibit coherent intentions, but falter in action execution. Moreover, fine-tuning on action data can erode the original VLMs generalist reasoning abilities. We release our task suite and evaluation code to serve as standardized benchmark for future VLAs and to drive research on closing the perception-to-action gap. More information, including the source code, can be found at https://ai4ce.github.io/INT-ACT/Building on the significant progress of large Vision-Language Models (VLMs) [7, 13, 23, 30] in computer vision and multi-modal tasks, Vision-Language-Action (VLA) [2, 16, 19] models are emerging to transfer these generalist skills to embodied agents and robotics. The ultimate goal is to create robots that can interpret natural language instructions, perceive and plan under complex and diverse environments, and execute versatile behaviors, all under fewor zero-shot scenarios. This goal calls for strong generalization capability beyond the training and fine-tuning of robotics data. Early results are encouraging: VLAs can natively follow high-level language instructions in cluttered scenes and perform highly dexterous manipulation on challenging objects [2, 16, 19]. Moreover, since VLAs are typically pre-trained on large-scale cross-embodiment dataset [9], they can be fine-tuned on new tasks with novel configurations for quick adaptation, partially alleviating the data scarcity problem plaguing robotics research. Equal contribution Project Lead Preprint. Under review. Despite this progress, there is no consensus on how to systematically measure the generalization capability of VLAs, which is supposedly strong point of leveraging the modern VLMs over previous perceptual systems. Notably, the current evaluation procedure for VLA generally relies on: 1. Recent simulation benchmarks such as CALVIN [27], LIBERO [22] and SimplerEnv [20] are equipped with natural language instructions, enabling the evaluation of vision-language-action (VLA) models instruction-following capabilities, which differentiate them from the more traditional benchmarks such as RLBench [17] or RoboMimic [26] that focus primarily on low-level control and imitation, with limited or no support for diverse language-conditioned tasks. However, the variety of tasks is relatively limited, and VLAs generalization capability is not targeted. 2. Real-world robot setup as seen in [2, 16, 19]. While this type of evaluation offers endless flexibility and opportunities to test the generalization of VLA policy, it has considerably bigger barriers than any vision language benchmark due to the time and monetary commitment required. As result, it remains unclear to many how well the state-of-the-art VLA models can generalize beyond the robotics datasets they train on by leveraging the remarkable generalization capability of their built-in VLM. To fill this void, we propose unified probing suite comprising 50 simulationbased tasks organized into 10 categories that vary systematically along three categories: Object diversity: out-of-distribution (OOD) object, appearance, and affordances unseen during embodiment-finetuning. Language complexity: from templated commands (e.g., Put on B) to compositional, knowledge and reasoning-intensive instructions. Vision-language thinking: parsing various distractors that are challenging at the perception or planning level. In this work, we make two primary contributions: We introduce and open-source INT-ACT, comprehensive VLA generalization probing suite comprising 50 tasks across 3 major categories and 10 subcategories, substantially extending the scope of existing VLA benchmarks. Through extensive benchmarking, we uncover two key failure modes in current state-of-the-art VLA models: persistent and pronounced Intention-Action Gap, where strong semantic understanding under distribution shift fails to translate into reliable execution. Fragile multimodal generalization, particularly under language variations and compounding visual-language distribution shifts.In robotics, Robotic Foundation Models aim to equip embodied agents with similar generalist abilities by training on diverse robotics datasets like Open-X Embodiment [9]. Early works, such as RT-1 [3], Octo [29], and RDT [25], typically use separate pretrained encoders (e.g., T5 [37] for language, SigLIP [44] for vision) to encode multimodal inputs, and subsequently train imitation learning policies (e.g., diffusion policies [8]) to map encoded features to robot actions. Following the success of VLMs [1, 6, 7, 18, 30, 39], Vision-Language-Action (VLA) models aim to integrate the powerful pretrained vision-language models (VLMs) directly, instead of leveraging only pretrained encoders. For instance, RT-2 [4] fine-tunes PaLM-E [11] and PaLI-X [6] on combined vision-language and robotics data via autoregressive token prediction, treating discretized robot actions as tokens. OpenVLA [19] finetu"
[23.06.2025 16:16] Mistral response. {"id": "2864cdcedd9743c7bd36cdbeb006598b", "object": "chat.completion", "created": 1750695380, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[]\n```"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1582, "total_tokens": 1590, "completion_tokens": 8}}
[23.06.2025 16:16] Response: ```python
[]
```
[23.06.2025 16:16] Deleting PDF ./assets/pdf/2506.09930.pdf.
[23.06.2025 16:16] Success.
[23.06.2025 16:16] Enriching papers with extra data.
[23.06.2025 16:16] ********************************************************************************
[23.06.2025 16:16] Abstract 0. Drag-and-Drop LLMs generate task-specific parameters through prompt-conditioned parameter generation, achieving significant efficiency gains and cross-domain generalization without per-task training.  					AI-generated summary 				 Modern Parameter-Efficient Fine-Tuning (PEFT) methods such as low-ra...
[23.06.2025 16:16] ********************************************************************************
[23.06.2025 16:16] Abstract 1. A novel multimodal document chunking approach using Large Multimodal Models (LMMs) enhances RAG performance by accurately processing complex PDF documents, including multi-page tables and embedded visuals.  					AI-generated summary 				 Retrieval-Augmented Generation (RAG) systems have revolutioniz...
[23.06.2025 16:16] ********************************************************************************
[23.06.2025 16:16] Abstract 2. PAROAttention reorganizes visual attention patterns to enable efficient sparsification and quantization, reducing memory and computational costs with minimal impact on performance.  					AI-generated summary 				 In visual generation, the quadratic complexity of attention mechanisms results in high ...
[23.06.2025 16:16] ********************************************************************************
[23.06.2025 16:16] Abstract 3. VIKI-Bench and VIKI-R provide a benchmark and framework for evaluating and improving visual-driven cooperation among diverse embodied agents using vision-language models and reinforcement learning.  					AI-generated summary 				 Coordinating multiple embodied agents in dynamic environments remains ...
[23.06.2025 16:16] ********************************************************************************
[23.06.2025 16:16] Abstract 4. Hunyuan-GameCraft is a novel framework for high-dynamic interactive video generation in game environments that addresses limitations in dynamics, generality, and efficiency through unified input representation, hybrid history-conditioned training, and model distillation.  					AI-generated summary 	...
[23.06.2025 16:16] ********************************************************************************
[23.06.2025 16:16] Abstract 5. A new TTS architecture improves accent accuracy and emotion recognition for Hindi and Indian English by integrating phoneme alignment, culture-sensitive emotion embeddings, and dynamic accent code switching.  					AI-generated summary 				 State-of-the-art text-to-speech (TTS) systems realize high n...
[23.06.2025 16:16] ********************************************************************************
[23.06.2025 16:16] Abstract 6. Multi-plane synchronization extends 2D foundation models to 3D panorama generation, introducing DreamCube to achieve diverse appearances and accurate geometry.  					AI-generated summary 				 3D panorama synthesis is a promising yet challenging task that demands high-quality and diverse visual appea...
[23.06.2025 16:16] ********************************************************************************
[23.06.2025 16:16] Abstract 7. Hunyuan3D 2.5, a suite of 3D diffusion models, advances shape and texture generation with a new LATTICE model and physical-based rendering in a multi-view architecture.  					AI-generated summary 				 In this report, we present Hunyuan3D 2.5, a robust suite of 3D diffusion models aimed at generating...
[23.06.2025 16:16] ********************************************************************************
[23.06.2025 16:16] Abstract 8. InfiniPot-V is a training-free, query-agnostic framework that compresses the key-value cache during video encoding to maintain a fixed memory cap for streaming video understanding, enhancing real-time performance and accuracy.  					AI-generated summary 				 Modern multimodal large language models (...
[23.06.2025 16:16] ********************************************************************************
[23.06.2025 16:16] Abstract 9. The tutorial provides a comprehensive guide on using Hunyuan3D 2.1 for generating high-resolution, textured 3D models, covering data preparation, model architecture, training, evaluation, and deployment.  					AI-generated summary 				 3D AI-generated content (AIGC) is a passionate field that has si...
[23.06.2025 16:16] ********************************************************************************
[23.06.2025 16:16] Abstract 10. A Y-shaped architecture, UniFork, balances shared learning and task specialization for unified image understanding and generation, outperforming conventional fully shared Transformer models.  					AI-generated summary 				 Unified image understanding and generation has emerged as a promising paradig...
[23.06.2025 16:16] ********************************************************************************
[23.06.2025 16:16] Abstract 11. Reranking and preference tuning improve the quality of perspective summaries generated by LLMs, as measured by language model-based metrics that outperform traditional ones.  					AI-generated summary 				 Generating unbiased summaries in real-world settings such as political perspective summarizati...
[23.06.2025 16:16] ********************************************************************************
[23.06.2025 16:16] Abstract 12. InfGen, a unified next-token prediction model, enables stable long-term traffic simulation by interleaving closed-loop motion simulation and scene generation.  					AI-generated summary 				 An ideal traffic simulator replicates the realistic long-term point-to-point trip that a self-driving system ...
[23.06.2025 16:16] ********************************************************************************
[23.06.2025 16:16] Abstract 13. A unified benchmark suite evaluates Vision-Language-Action models' generalization and motor execution capabilities, highlighting the disparity between perceptual understanding and precise action execution.  					AI-generated summary 				 One promise that Vision-Language-Action (VLA) models hold over...
[23.06.2025 16:16] Read previous papers.
[23.06.2025 16:16] Generating reviews via LLM API.
[23.06.2025 16:16] Using data from previous issue: {"categories": ["#transfer_learning", "#multimodal", "#training", "#optimization"], "emoji": "üé≠", "ru": {"title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–µ –Ø–ú: –±—ã—Å—Ç—Ä–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ Drag-and-Drop LLMs (DnD) –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º
[23.06.2025 16:16] Using data from previous issue: {"categories": ["#dataset", "#long_context", "#optimization", "#rag", "#multimodal"], "emoji": "üìÑ", "ru": {"title": "–£–º–Ω–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è RAG-—Å–∏—Å—Ç–µ–º", "desc": "–ü—Ä–µ–¥–ª–æ–∂–µ–Ω –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ä–∞–∑–±–∏–µ–Ω–∏—é —Å–ª–æ–∂–Ω—ã—Ö PDF-–¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–∞ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π
[23.06.2025 16:16] Using data from previous issue: {"categories": ["#optimization", "#cv", "#inference", "#architecture", "#video"], "emoji": "üî¨", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ: –º–µ–Ω—å—à–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π, —Ç–∞ –∂–µ —Ç–æ—á–Ω–æ—Å—Ç—å", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ PAROAttention –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –º–µ—Ö–∞–Ω–∏–∑–º–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è –≤ –∑–∞–¥–∞—á–∞—Ö –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –≤–∏–¥–µ
[23.06.2025 16:16] Using data from previous issue: {"categories": ["#reasoning", "#agents", "#games", "#rl", "#benchmark", "#cv"], "emoji": "ü§ñ", "ru": {"title": "–í–∏–∑—É–∞–ª—å–Ω–∞—è –∫–æ–æ–ø–µ—Ä–∞—Ü–∏—è –∞–≥–µ–Ω—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è", "desc": "VIKI-Bench –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–æ–æ–ø–µ—Ä–∞—Ü–∏–∏ –º–µ–∂–¥—É –≤–æ–ø–ª–æ—â–µ–Ω–Ω—ã–º–∏ –∞–≥–µ–Ω—Ç–∞–º–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º
[23.06.2025 16:16] Using data from previous issue: {"categories": ["#inference", "#video", "#diffusion", "#synthetic", "#dataset", "#games", "#training"], "emoji": "üéÆ", "ru": {"title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–≥–æ –∏–≥—Ä–æ–≤–æ–≥–æ –≤–∏–¥–µ–æ", "desc": "Hunyuan-GameCraft - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–≥–æ –≤–∏–¥–µ–æ –≤ –∏–≥—Ä–æ–≤—ã—Ö —Å—Ä–µ–¥–∞—Ö —Å –≤—ã—Å–æ–∫–æ–π –¥–∏
[23.06.2025 16:16] Using data from previous issue: {"categories": ["#low_resource", "#machine_translation", "#multilingual", "#audio"], "emoji": "üó£Ô∏è", "ru": {"title": "–£–ª—É—á—à–µ–Ω–∏–µ —Å–∏–Ω—Ç–µ–∑–∞ —Ä–µ—á–∏ –¥–ª—è –∏–Ω–¥–∏–π—Å–∫–∏—Ö —è–∑—ã–∫–æ–≤ —Å —É—á–µ—Ç–æ–º –∞–∫—Ü–µ–Ω—Ç–∞ –∏ —ç–º–æ—Ü–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É —Å–∏–Ω—Ç–µ–∑–∞ —Ä–µ—á–∏ (TTS) –¥–ª—è —Ö–∏–Ω–¥–∏ –∏ –∏–Ω–¥–∏–π—Å–∫–æ–≥–æ –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ. –°–∏—Å—Ç–µ–º–∞ –∏
[23.06.2025 16:16] Using data from previous issue: {"categories": ["#optimization", "#diffusion", "#3d", "#cv"], "emoji": "üåê", "ru": {"title": "–ú–Ω–æ–≥–æ–ø–ª–æ—Å–∫–æ—Å—Ç–Ω–∞—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è: –º–æ—Å—Ç –º–µ–∂–¥—É 2D –∏ 3D –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–∞–Ω–æ—Ä–∞–º", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç—Ä–µ—Ö–º–µ—Ä–Ω—ã—Ö –ø–∞–Ω–æ—Ä–∞–º —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–Ω–æ–≥–æ–ø–ª–æ—Å–∫–æ—Å—Ç–Ω–æ–π —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏ –¥–≤—É–º–µ—Ä–Ω—ã—Ö
[23.06.2025 16:16] Using data from previous issue: {"categories": ["#diffusion", "#3d"], "emoji": "üßä", "ru": {"title": "–ù–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å 3D-–≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: —Ç–æ—á–Ω—ã–µ —Ñ–æ—Ä–º—ã –∏ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ —Ç–µ–∫—Å—Ç—É—Ä—ã", "desc": "Hunyuan3D 2.5 –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –Ω–∞–±–æ—Ä –º–æ–¥–µ–ª–µ–π –¥–∏—Ñ—Ñ—É–∑–∏–∏ –¥–ª—è 3D-–≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, —É–ª—É—á—à–∞—é—â–∏–π —Å–æ–∑–¥–∞–Ω–∏–µ —Ñ–æ—Ä–º –∏ —Ç–µ–∫—Å—Ç—É—Ä. –ö–ª—é—á–µ–≤–æ–µ –Ω–æ–≤–æ–≤–≤–µ–¥–µ–Ω–∏–µ - –º–æ–¥–µ–ª—å LATTICE –¥–ª—è –≥–µ
[23.06.2025 16:16] Using data from previous issue: {"categories": ["#training", "#multimodal", "#optimization", "#long_context", "#video"], "emoji": "üé•", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Å–∂–∞—Ç–∏–µ –∫—ç—à–∞ –¥–ª—è –ø–æ—Ç–æ–∫–æ–≤–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –≤–∏–¥–µ–æ –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ —Ç–æ—á–Ω–æ—Å—Ç–∏", "desc": "InfiniPot-V - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Å–∂–∞—Ç–∏—è –∫—ç—à–∞ –∫–ª—é—á-–∑–Ω–∞—á–µ–Ω–∏–µ –ø—Ä–∏ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–∏ –≤–∏–¥–µ–æ, –Ω–µ —Ç—Ä–µ–±—É—é—â–∏–π –æ–±
[23.06.2025 16:16] Using data from previous issue: {"categories": ["#training", "#3d", "#architecture", "#synthetic", "#games", "#data"], "emoji": "üé®", "ru": {"title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ 3D-–º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–∏: –¥–æ—Å—Ç—É–ø–Ω–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ Hunyuan3D 2.1", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –ø–æ–¥—Ä–æ–±–Ω–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é Hunyuan3D 2.1 –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤—ã—Å–æ–∫–æ–∫–∞—á
[23.06.2025 16:16] Using data from previous issue: {"categories": ["#optimization", "#architecture", "#multimodal"], "emoji": "üç¥", "ru": {"title": "UniFork: –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –±–∞–ª–∞–Ω—Å –º–µ–∂–¥—É –æ–±—â–∏–º –æ–±—É—á–µ–Ω–∏–µ–º –∏ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–µ–π –∑–∞–¥–∞—á", "desc": "UniFork - —ç—Ç–æ –Ω–æ–≤–∞—è Y-–æ–±—Ä–∞–∑–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –û–Ω–∞ —Å–æ—á–µ—Ç–∞–µ—Ç –æ–±—â–∏–µ —Å–ª–æ–∏
[23.06.2025 16:16] Using data from previous issue: {"categories": ["#synthetic", "#multimodal", "#dataset", "#interpretability", "#alignment", "#training", "#benchmark"], "emoji": "üîç", "ru": {"title": "–£–ª—É—á—à–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ–±–∑–æ—Ä–æ–≤ —Å —Ä–∞–∑–Ω—ã—Ö —Ç–æ—á–µ–∫ –∑—Ä–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ LLM", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ —É–ª—É—á—à–µ–Ω–∏—é –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü
[23.06.2025 16:16] Using data from previous issue: {"categories": ["#training", "#optimization", "#agents", "#open_source", "#video"], "emoji": "üöó", "ru": {"title": "InfGen: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–º –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–∏ –¥–æ—Ä–æ–∂–Ω–æ–≥–æ –¥–≤–∏–∂–µ–Ω–∏—è", "desc": "InfGen - —ç—Ç–æ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–æ–∫–µ–Ω–∞, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø—Ä–æ–≤–æ–¥–∏—Ç—å —Å—Ç–∞–±–∏–ª—å–Ω–æ–µ –¥
[23.06.2025 16:16] Querying the API.
[23.06.2025 16:16] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A unified benchmark suite evaluates Vision-Language-Action models' generalization and motor execution capabilities, highlighting the disparity between perceptual understanding and precise action execution.  					AI-generated summary 				 One promise that Vision-Language-Action (VLA) models hold over traditional imitation learning for robotics is to leverage the broad generalization capabilities of large Vision-Language Models (VLMs) to produce versatile, "generalist" robot policies. However, current evaluations of VLAs remain insufficient. Traditional imitation learning benchmarks are unsuitable due to the lack of language instructions. Emerging benchmarks for VLAs that incorporate language often come with limited evaluation tasks and do not intend to investigate how much VLM pretraining truly contributes to the generalization capabilities of the downstream robotic policy. Meanwhile, much research relies on real-world robot setups designed in isolation by different institutions, which creates a barrier for reproducibility and accessibility. To address this gap, we introduce a unified probing suite of 50 simulation-based tasks across 10 subcategories spanning language instruction, vision, and objects. We systematically evaluate several state-of-the-art VLA architectures on this suite to understand their generalization capability. Our results show that while VLM backbones endow VLAs with robust perceptual understanding and high level planning, which we refer to as good intentions, this does not reliably translate into precise motor execution: when faced with out-of-distribution observations, policies often exhibit coherent intentions, but falter in action execution. Moreover, finetuning on action data can erode the original VLM's generalist reasoning abilities. We release our task suite and evaluation code to serve as a standardized benchmark for future VLAs and to drive research on closing the perception-to-action gap. More information, including the source code, can be found at https://ai4ce.github.io/INT-ACT/
[23.06.2025 16:16] Response: {
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –Ω–∞–±–æ—Ä —Ç–µ—Å—Ç–æ–≤ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π Vision-Language-Action (VLA) –≤ —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–µ. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ —Ö–æ—Ä–æ—à–µ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –∏ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—ã—Å–æ–∫–æ–≥–æ —É—Ä–æ–≤–Ω—è, –º–æ–¥–µ–ª–∏ VLA —á–∞—Å—Ç–æ –Ω–µ —Å–ø—Ä–∞–≤–ª—è—é—Ç—Å—è —Å —Ç–æ—á–Ω—ã–º –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ–º –¥–µ–π—Å—Ç–≤–∏–π –ø—Ä–∏ –≤—Å—Ç—Ä–µ—á–µ —Å –Ω–µ—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–º–∏ —Å–∏—Ç—É–∞—Ü–∏—è–º–∏. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –Ω–∞–±–æ—Ä –∏–∑ 50 –∑–∞–¥–∞—á –≤ —Å–∏–º—É–ª—è—Ü–∏–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –æ–±–æ–±—â–∞—é—â–µ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ VLA –º–æ–¥–µ–ª–µ–π. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —É–∫–∞–∑—ã–≤–∞—é—Ç –Ω–∞ —Ä–∞–∑—Ä—ã–≤ –º–µ–∂–¥—É –ø–µ—Ä—Ü–µ–ø—Ç–∏–≤–Ω—ã–º –ø–æ–Ω–∏–º–∞–Ω–∏–µ–º –∏ —Ç–æ—á–Ω—ã–º –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ–º –¥–µ–π—Å—Ç–≤–∏–π –≤ —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–µ.",
  "emoji": "ü§ñ",
  "title": "–†–∞–∑—Ä—ã–≤ –º–µ–∂–¥—É –Ω–∞–º–µ—Ä–µ–Ω–∏–µ–º –∏ –¥–µ–π—Å—Ç–≤–∏–µ–º: –æ—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–µ–π VLA –≤ —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–µ"
}
[23.06.2025 16:16] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A unified benchmark suite evaluates Vision-Language-Action models' generalization and motor execution capabilities, highlighting the disparity between perceptual understanding and precise action execution.  					AI-generated summary 				 One promise that Vision-Language-Action (VLA) models hold over traditional imitation learning for robotics is to leverage the broad generalization capabilities of large Vision-Language Models (VLMs) to produce versatile, "generalist" robot policies. However, current evaluations of VLAs remain insufficient. Traditional imitation learning benchmarks are unsuitable due to the lack of language instructions. Emerging benchmarks for VLAs that incorporate language often come with limited evaluation tasks and do not intend to investigate how much VLM pretraining truly contributes to the generalization capabilities of the downstream robotic policy. Meanwhile, much research relies on real-world robot setups designed in isolation by different institutions, which creates a barrier for reproducibility and accessibility. To address this gap, we introduce a unified probing suite of 50 simulation-based tasks across 10 subcategories spanning language instruction, vision, and objects. We systematically evaluate several state-of-the-art VLA architectures on this suite to understand their generalization capability. Our results show that while VLM backbones endow VLAs with robust perceptual understanding and high level planning, which we refer to as good intentions, this does not reliably translate into precise motor execution: when faced with out-of-distribution observations, policies often exhibit coherent intentions, but falter in action execution. Moreover, finetuning on action data can erode the original VLM's generalist reasoning abilities. We release our task suite and evaluation code to serve as a standardized benchmark for future VLAs and to drive research on closing the perception-to-action gap. More information, including the source code, can be found at https://ai4ce.github.io/INT-ACT/"

[23.06.2025 16:16] Response: ```python
['BENCHMARK', 'AGENTS', 'CV', 'ROBOTICS']
```
[23.06.2025 16:16] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A unified benchmark suite evaluates Vision-Language-Action models' generalization and motor execution capabilities, highlighting the disparity between perceptual understanding and precise action execution.  					AI-generated summary 				 One promise that Vision-Language-Action (VLA) models hold over traditional imitation learning for robotics is to leverage the broad generalization capabilities of large Vision-Language Models (VLMs) to produce versatile, "generalist" robot policies. However, current evaluations of VLAs remain insufficient. Traditional imitation learning benchmarks are unsuitable due to the lack of language instructions. Emerging benchmarks for VLAs that incorporate language often come with limited evaluation tasks and do not intend to investigate how much VLM pretraining truly contributes to the generalization capabilities of the downstream robotic policy. Meanwhile, much research relies on real-world robot setups designed in isolation by different institutions, which creates a barrier for reproducibility and accessibility. To address this gap, we introduce a unified probing suite of 50 simulation-based tasks across 10 subcategories spanning language instruction, vision, and objects. We systematically evaluate several state-of-the-art VLA architectures on this suite to understand their generalization capability. Our results show that while VLM backbones endow VLAs with robust perceptual understanding and high level planning, which we refer to as good intentions, this does not reliably translate into precise motor execution: when faced with out-of-distribution observations, policies often exhibit coherent intentions, but falter in action execution. Moreover, finetuning on action data can erode the original VLM's generalist reasoning abilities. We release our task suite and evaluation code to serve as a standardized benchmark for future VLAs and to drive research on closing the perception-to-action gap. More information, including the source code, can be found at https://ai4ce.github.io/INT-ACT/"

[23.06.2025 16:16] Response: ```python
["AGI", "GAMES", "OPTIMIZATION", "SURVEY"]
```
[23.06.2025 16:16] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents a unified benchmark suite designed to evaluate Vision-Language-Action (VLA) models, focusing on their ability to generalize and execute motor actions. It highlights the gap between a model\'s perceptual understanding, derived from large Vision-Language Models (VLMs), and its actual performance in executing precise actions. The study reveals that while VLA models can plan effectively, they struggle with action execution, especially when faced with unfamiliar scenarios. The authors provide a comprehensive set of simulation-based tasks to facilitate standardized evaluations and encourage further research in bridging the perception-to-action divide.","title":"Bridging the Gap: Evaluating Vision-Language-Action Models for Better Robot Execution"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="This paper presents a unified benchmark suite designed to evaluate Vision-Language-Action (VLA) models, focusing on their ability to generalize and execute motor actions. It highlights the gap between a model's perceptual understanding, derived from large Vision-Language Models (VLMs), and its actual performance in executing precise actions. The study reveals that while VLA models can plan effectively, they struggle with action execution, especially when faced with unfamiliar scenarios. The authors provide a comprehensive set of simulation-based tasks to facilitate standardized evaluations and encourage further research in bridging the perception-to-action divide.", title='Bridging the Gap: Evaluating Vision-Language-Action Models for Better Robot Execution'))
[23.06.2025 16:16] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏Ä‰∏™Áªü‰∏ÄÁöÑÂü∫ÂáÜÂ•ó‰ª∂ÔºåÁî®‰∫éËØÑ‰º∞ËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÔºàVLAÔºâÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõÂíåËøêÂä®ÊâßË°åËÉΩÂäõ„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÂ∞ΩÁÆ°Â§ßÂûãËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÔºàVLMÔºâËÉΩÂ§üÊèê‰æõËâØÂ•ΩÁöÑÊÑüÁü•ÁêÜËß£ÂíåÈ´òÂ±ÇÊ¨°ËßÑÂàíÔºå‰ΩÜÂú®Èù¢ÂØπ‰∏çÂêåÂàÜÂ∏ÉÁöÑËßÇÂØüÊó∂ÔºåÊ®°ÂûãÁöÑÂä®‰ΩúÊâßË°åËÉΩÂäõÂ∏∏Â∏∏‰∏çË∂≥„ÄÇÊàë‰ª¨ËÆæËÆ°‰∫Ü50‰∏™Âü∫‰∫é‰ªøÁúüÁöÑ‰ªªÂä°ÔºåÊ∂µÁõñËØ≠Ë®ÄÊåá‰ª§„ÄÅËßÜËßâÂíåÁâ©‰ΩìÁ≠âÂ§ö‰∏™Â≠êÁ±ªÂà´Ôºå‰ª•Á≥ªÁªüÊÄßÂú∞ËØÑ‰º∞ÂΩìÂâçÊúÄÂÖàËøõÁöÑVLAÊû∂ÊûÑ„ÄÇÊúÄÁªàÔºåÊàë‰ª¨Â∏åÊúõÈÄöËøáËøô‰∏™Âü∫ÂáÜÂ•ó‰ª∂Êé®Âä®Êú™Êù•VLAÁöÑÁ†îÁ©∂ÔºåÁº©Â∞èÊÑüÁü•‰∏éÂä®‰Ωú‰πãÈó¥ÁöÑÂ∑ÆË∑ù„ÄÇ","title":"Áº©Â∞èÊÑüÁü•‰∏éÂä®‰Ωú‰πãÈó¥ÁöÑÂ∑ÆË∑ù"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏Ä‰∏™Áªü‰∏ÄÁöÑÂü∫ÂáÜÂ•ó‰ª∂ÔºåÁî®‰∫éËØÑ‰º∞ËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÔºàVLAÔºâÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõÂíåËøêÂä®ÊâßË°åËÉΩÂäõ„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÂ∞ΩÁÆ°Â§ßÂûãËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÔºàVLMÔºâËÉΩÂ§üÊèê‰æõËâØÂ•ΩÁöÑÊÑüÁü•ÁêÜËß£ÂíåÈ´òÂ±ÇÊ¨°ËßÑÂàíÔºå‰ΩÜÂú®Èù¢ÂØπ‰∏çÂêåÂàÜÂ∏ÉÁöÑËßÇÂØüÊó∂ÔºåÊ®°ÂûãÁöÑÂä®‰ΩúÊâßË°åËÉΩÂäõÂ∏∏Â∏∏‰∏çË∂≥„ÄÇÊàë‰ª¨ËÆæËÆ°‰∫Ü50‰∏™Âü∫‰∫é‰ªøÁúüÁöÑ‰ªªÂä°ÔºåÊ∂µÁõñËØ≠Ë®ÄÊåá‰ª§„ÄÅËßÜËßâÂíåÁâ©‰ΩìÁ≠âÂ§ö‰∏™Â≠êÁ±ªÂà´Ôºå‰ª•Á≥ªÁªüÊÄßÂú∞ËØÑ‰º∞ÂΩìÂâçÊúÄÂÖàËøõÁöÑVLAÊû∂ÊûÑ„ÄÇÊúÄÁªàÔºåÊàë‰ª¨Â∏åÊúõÈÄöËøáËøô‰∏™Âü∫ÂáÜÂ•ó‰ª∂Êé®Âä®Êú™Êù•VLAÁöÑÁ†îÁ©∂ÔºåÁº©Â∞èÊÑüÁü•‰∏éÂä®‰Ωú‰πãÈó¥ÁöÑÂ∑ÆË∑ù„ÄÇ', title='Áº©Â∞èÊÑüÁü•‰∏éÂä®‰Ωú‰πãÈó¥ÁöÑÂ∑ÆË∑ù'))
[23.06.2025 16:16] Renaming data file.
[23.06.2025 16:16] Renaming previous data. hf_papers.json to ./d/2025-06-23.json
[23.06.2025 16:16] Saving new data file.
[23.06.2025 16:16] Generating page.
[23.06.2025 16:16] Renaming previous page.
[23.06.2025 16:16] Renaming previous data. index.html to ./d/2025-06-23.html
[23.06.2025 16:16] Writing result.
[23.06.2025 16:16] Renaming log file.
[23.06.2025 16:16] Renaming previous data. log.txt to ./logs/2025-06-23_last_log.txt
