[25.07.2025 02:58] Read previous papers.
[25.07.2025 02:58] Generating top page (month).
[25.07.2025 02:58] Writing top page (month).
[25.07.2025 03:57] Read previous papers.
[25.07.2025 03:57] Get feed.
[25.07.2025 03:57] Extract page data from URL. URL: https://huggingface.co/papers/2507.15758
[25.07.2025 03:57] Get page data from previous paper. URL: https://huggingface.co/papers/2507.18537
[25.07.2025 03:57] Extract page data from URL. URL: https://huggingface.co/papers/2507.15844
[25.07.2025 03:57] Get page data from previous paper. URL: https://huggingface.co/papers/2507.16535
[25.07.2025 03:57] Extract page data from URL. URL: https://huggingface.co/papers/2507.18634
[25.07.2025 03:57] Extract page data from URL. URL: https://huggingface.co/papers/2507.18192
[25.07.2025 03:57] Get page data from previous paper. URL: https://huggingface.co/papers/2507.14988
[25.07.2025 03:57] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[25.07.2025 03:57] No deleted papers detected.
[25.07.2025 03:57] Downloading and parsing papers (pdf, html). Total: 7.
[25.07.2025 03:57] Downloading and parsing paper https://huggingface.co/papers/2507.15758.
[25.07.2025 03:57] Downloading paper 2507.15758 from http://arxiv.org/pdf/2507.15758v1...
[25.07.2025 03:57] Extracting affiliations from text.
[25.07.2025 03:57] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 1 2 ] . [ 1 8 5 7 5 1 . 7 0 5 2 : r a LAPO: INTERNALIZING REASONING EFFICIENCY VIA LENGTH-ADAPTIVE POLICY OPTIMIZATION Xingyu Wu1, Yuchen Yan1, Shangke Lyu1, Linjuan Wu1, Yiwen Qiu1, Yongliang Shen1, Weiming Lu1, Jian Shao1, Jun Xiao1, Yueting Zhuang1 1Zhejiang University {wuxingyu, syl}@zju.edu.cn GitHub: https://github.com/zju-real/lapo (cid:128) Project: https://zju-real.github.io/lapo "
[25.07.2025 03:57] Response: ```python
["Zhejiang University"]
```
[25.07.2025 03:57] Deleting PDF ./assets/pdf/2507.15758.pdf.
[25.07.2025 03:57] Success.
[25.07.2025 03:57] Downloading and parsing paper https://huggingface.co/papers/2507.18537.
[25.07.2025 03:57] Extra JSON file exists (./assets/json/2507.18537.json), skip PDF parsing.
[25.07.2025 03:57] Paper image links file exists (./assets/img_data/2507.18537.json), skip HTML parsing.
[25.07.2025 03:57] Success.
[25.07.2025 03:57] Downloading and parsing paper https://huggingface.co/papers/2507.15844.
[25.07.2025 03:57] Downloading paper 2507.15844 from http://arxiv.org/pdf/2507.15844v2...
[25.07.2025 03:57] Extracting affiliations from text.
[25.07.2025 03:57] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 2 2 ] . [ 2 4 4 8 5 1 . 7 0 5 2 : r a Shangke Lyu1,, Linjuan Wu1,, Yuchen Yan1, Xingyu Wu1, Hao Li2 Yongliang Shen1, Peisheng Jiang2, Weiming Lu1, Jun Xiao1, Yueting Zhuang1 1Zhejiang University 2SF Technology {lyusk, wulinjuan525, syl, luwm}@zju.edu.cn GitHub: https://github.com/zju-real/hbpo (cid:128) Project: https://zju-real.github.io/hbpo "
[25.07.2025 03:57] Response: ```python
["Zhejiang University", "SF Technology"]
```
[25.07.2025 03:57] Deleting PDF ./assets/pdf/2507.15844.pdf.
[25.07.2025 03:57] Success.
[25.07.2025 03:57] Downloading and parsing paper https://huggingface.co/papers/2507.16535.
[25.07.2025 03:57] Extra JSON file exists (./assets/json/2507.16535.json), skip PDF parsing.
[25.07.2025 03:57] Paper image links file exists (./assets/img_data/2507.16535.json), skip HTML parsing.
[25.07.2025 03:57] Success.
[25.07.2025 03:57] Downloading and parsing paper https://huggingface.co/papers/2507.18634.
[25.07.2025 03:57] Downloading paper 2507.18634 from http://arxiv.org/pdf/2507.18634v1...
[25.07.2025 03:57] Extracting affiliations from text.
[25.07.2025 03:57] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 4 2 ] . [ 1 4 3 6 8 1 . 7 0 5 2 : r Captain Cinema: Towards Short Movie Generation Junfei Xiao1,2,, Ceyuan Yang2,, Lvmin Zhang3, Shengqu Cai2,3, Yang Zhao2, Yuwei Guo2,4, Gordon Wetzstein3, Maneesh Agrawala3, Alan Yuille1, Lu Jiang2, 1Johns Hopkins University, 2ByteDance Seed, 3Stanford University, 4CUHK Project Lead, Corresponding author "
[25.07.2025 03:57] Response: ```python
["Johns Hopkins University", "ByteDance Seed", "Stanford University", "CUHK Project Lead"]
```
[25.07.2025 03:57] Deleting PDF ./assets/pdf/2507.18634.pdf.
[25.07.2025 03:57] Success.
[25.07.2025 03:57] Downloading and parsing paper https://huggingface.co/papers/2507.18192.
[25.07.2025 03:57] Downloading paper 2507.18192 from http://arxiv.org/pdf/2507.18192v1...
[25.07.2025 03:58] Extracting affiliations from text.
[25.07.2025 03:58] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 4 2 ] . [ 1 2 9 1 8 1 . 7 0 5 2 : r TeEFusion: Blending Text Embeddings to Distill Classifier-Free Guidance Minghao Fu*1,2,3 Guo-Hua Wang3 Xiaohao Chen3 Qing-Guo Chen3 Zhao Xu3 Weihua Luo3 Kaifu Zhang3 1 School of Artificial Intelligence, Nanjing University 2 National Key Laboratory for Novel Software Technology, Nanjing University 3 Alibaba International Digital Commerce Group fumh@lamda.nju.edu.cn, {wangguohua, xiaohao.cxh, qingguo.cqg, changgong.xz, weihua.luowh, kaifu.zkf}@alibaba-inc.com "
[25.07.2025 03:58] Response: ```python
[
    "School of Artificial Intelligence, Nanjing University",
    "National Key Laboratory for Novel Software Technology, Nanjing University",
    "Alibaba International Digital Commerce Group"
]
```
[25.07.2025 03:58] Deleting PDF ./assets/pdf/2507.18192.pdf.
[25.07.2025 03:58] Success.
[25.07.2025 03:58] Downloading and parsing paper https://huggingface.co/papers/2507.14988.
[25.07.2025 03:58] Extra JSON file exists (./assets/json/2507.14988.json), skip PDF parsing.
[25.07.2025 03:58] Paper image links file exists (./assets/img_data/2507.14988.json), skip HTML parsing.
[25.07.2025 03:58] Success.
[25.07.2025 03:58] Enriching papers with extra data.
[25.07.2025 03:58] ********************************************************************************
[25.07.2025 03:58] Abstract 0. Large reasoning models have achieved remarkable performance through extended chain-of-thought sequences, yet this computational freedom leads to excessive token generation even for simple problems. We present Length-Adaptive Policy Optimization (LAPO), a novel framework that transforms reasoning len...
[25.07.2025 03:58] ********************************************************************************
[25.07.2025 03:58] Abstract 1. TTS-VAR, a test-time scaling framework for visual auto-regressive models, improves generation quality by dynamically adjusting batch sizes and using clustering and resampling techniques.  					AI-generated summary 				 Scaling visual generation models is essential for real-world content creation, ye...
[25.07.2025 03:58] ********************************************************************************
[25.07.2025 03:58] Abstract 2. Large reasoning models achieve remarkable performance through extensive chain-of-thought generation, yet exhibit significant computational inefficiency by applying uniform reasoning strategies regardless of problem complexity. We present Hierarchical Budget Policy Optimization (HBPO), a reinforcemen...
[25.07.2025 03:58] ********************************************************************************
[25.07.2025 03:58] Abstract 3. Despite the remarkable developments achieved by recent 3D generation works, scaling these methods to geographic extents, such as modeling thousands of square kilometers of Earth's surface, remains an open challenge. We address this through a dual innovation in data infrastructure and model architect...
[25.07.2025 03:58] ********************************************************************************
[25.07.2025 03:58] Abstract 4. Captain Cinema generates high-quality short movies from textual descriptions using top-down keyframe planning and bottom-up video synthesis with interleaved training of Multimodal Diffusion Transformers.  					AI-generated summary 				 We present Captain Cinema, a generation framework for short movi...
[25.07.2025 03:58] ********************************************************************************
[25.07.2025 03:58] Abstract 5. TeEFusion enhances text-to-image synthesis by efficiently incorporating classifier-free guidance into text embeddings, reducing inference costs without sacrificing image quality.  					AI-generated summary 				 Recent advances in text-to-image synthesis largely benefit from sophisticated sampling st...
[25.07.2025 03:58] ********************************************************************************
[25.07.2025 03:58] Abstract 6. DMOSpeech 2 optimizes duration prediction and introduces teacher-guided sampling to enhance speech synthesis performance and diversity.  					AI-generated summary 				 Diffusion-based text-to-speech (TTS) systems have made remarkable progress in zero-shot speech synthesis, yet optimizing all compone...
[25.07.2025 03:58] Read previous papers.
[25.07.2025 03:58] Generating reviews via LLM API.
[25.07.2025 03:58] Querying the API.
[25.07.2025 03:58] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Large reasoning models have achieved remarkable performance through extended chain-of-thought sequences, yet this computational freedom leads to excessive token generation even for simple problems. We present Length-Adaptive Policy Optimization (LAPO), a novel framework that transforms reasoning length control from an external constraint into an intrinsic model capability. Unlike existing approaches that impose rigid limits or rely on post-hoc interventions, LAPO enables models to internalize an understanding of appropriate reasoning depth through a two-stage reinforcement learning process. In the first stage, models learn natural reasoning patterns by discovering the statistical distribution of successful solution lengths. The second stage leverages these patterns as meta-cognitive guidance, embedding them directly within the model's reasoning context to ensure inference-time flexibility. Experiments on mathematical reasoning benchmarks demonstrate that LAPO reduces token usage by up to 40.9\% while improving accuracy by 2.3\%. Our analysis reveals that models trained with LAPO develop emergent abilities to allocate computational resources based on problem complexity, achieving efficient reasoning without sacrificing quality.
[25.07.2025 03:58] Response: {
  "desc": "Статья представляет новый подход к оптимизации длины рассуждений в больших языковых моделях под названием LAPO (Length-Adaptive Policy Optimization). Этот метод использует двухэтапный процесс обучения с подкреплением, чтобы модели могли самостоятельно определять оптимальную глубину рассуждений. LAPO позволяет сократить использование токенов на 40.9%, одновременно повышая точность на 2.3% на задачах математических рассуждений. Исследование показывает, что модели, обученные с помощью LAPO, развивают способность эффективно распределять вычислительные ресурсы в зависимости от сложности задачи.",
  "emoji": "🧠",
  "title": "Эффективные рассуждения: меньше слов, больше смысла"
}
[25.07.2025 03:58] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Large reasoning models have achieved remarkable performance through extended chain-of-thought sequences, yet this computational freedom leads to excessive token generation even for simple problems. We present Length-Adaptive Policy Optimization (LAPO), a novel framework that transforms reasoning length control from an external constraint into an intrinsic model capability. Unlike existing approaches that impose rigid limits or rely on post-hoc interventions, LAPO enables models to internalize an understanding of appropriate reasoning depth through a two-stage reinforcement learning process. In the first stage, models learn natural reasoning patterns by discovering the statistical distribution of successful solution lengths. The second stage leverages these patterns as meta-cognitive guidance, embedding them directly within the model's reasoning context to ensure inference-time flexibility. Experiments on mathematical reasoning benchmarks demonstrate that LAPO reduces token usage by up to 40.9\% while improving accuracy by 2.3\%. Our analysis reveals that models trained with LAPO develop emergent abilities to allocate computational resources based on problem complexity, achieving efficient reasoning without sacrificing quality."

[25.07.2025 03:58] Response: ```python
["RL", "TRAINING", "BENCHMARK", "MATH"]
```
[25.07.2025 03:58] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Large reasoning models have achieved remarkable performance through extended chain-of-thought sequences, yet this computational freedom leads to excessive token generation even for simple problems. We present Length-Adaptive Policy Optimization (LAPO), a novel framework that transforms reasoning length control from an external constraint into an intrinsic model capability. Unlike existing approaches that impose rigid limits or rely on post-hoc interventions, LAPO enables models to internalize an understanding of appropriate reasoning depth through a two-stage reinforcement learning process. In the first stage, models learn natural reasoning patterns by discovering the statistical distribution of successful solution lengths. The second stage leverages these patterns as meta-cognitive guidance, embedding them directly within the model's reasoning context to ensure inference-time flexibility. Experiments on mathematical reasoning benchmarks demonstrate that LAPO reduces token usage by up to 40.9\% while improving accuracy by 2.3\%. Our analysis reveals that models trained with LAPO develop emergent abilities to allocate computational resources based on problem complexity, achieving efficient reasoning without sacrificing quality."

[25.07.2025 03:58] Response: ```python
['REASONING', 'OPTIMIZATION']
```
[25.07.2025 03:58] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces Length-Adaptive Policy Optimization (LAPO), a new method for improving reasoning models in machine learning. LAPO allows models to learn how much reasoning is needed for different problems, rather than setting strict limits on reasoning length. It uses a two-stage reinforcement learning process where models first learn successful reasoning patterns and then apply this knowledge during inference. The results show that LAPO can significantly reduce the number of tokens used while also increasing the accuracy of the models.","title":"Smart Reasoning: Less is More with LAPO"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper introduces Length-Adaptive Policy Optimization (LAPO), a new method for improving reasoning models in machine learning. LAPO allows models to learn how much reasoning is needed for different problems, rather than setting strict limits on reasoning length. It uses a two-stage reinforcement learning process where models first learn successful reasoning patterns and then apply this knowledge during inference. The results show that LAPO can significantly reduce the number of tokens used while also increasing the accuracy of the models.', title='Smart Reasoning: Less is More with LAPO'))
[25.07.2025 03:58] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本文提出了一种新的框架，称为长度自适应策略优化（LAPO），旨在改善推理模型的效率。LAPO通过强化学习的两阶段过程，使模型能够内在理解适当的推理深度，从而减少不必要的计算。第一阶段，模型学习成功解决方案长度的统计分布；第二阶段，模型将这些模式作为元认知指导，嵌入推理上下文中。实验结果表明，LAPO在数学推理基准测试中减少了多达40.9%的标记使用，同时提高了2.3%的准确率。","title":"智能推理，灵活控制！"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本文提出了一种新的框架，称为长度自适应策略优化（LAPO），旨在改善推理模型的效率。LAPO通过强化学习的两阶段过程，使模型能够内在理解适当的推理深度，从而减少不必要的计算。第一阶段，模型学习成功解决方案长度的统计分布；第二阶段，模型将这些模式作为元认知指导，嵌入推理上下文中。实验结果表明，LAPO在数学推理基准测试中减少了多达40.9%的标记使用，同时提高了2.3%的准确率。', title='智能推理，灵活控制！'))
[25.07.2025 03:58] Using data from previous issue: {"categories": ["#cv", "#training", "#optimization"], "emoji": "🖼️", "ru": {"title": "Умное масштабирование для лучшей генерации изображений", "desc": "TTS-VAR - это новая система масштабирования визуальных авторегрессионных моделей во время тестирования. Она улучшает качество генерации изображений,
[25.07.2025 03:58] Querying the API.
[25.07.2025 03:58] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Large reasoning models achieve remarkable performance through extensive chain-of-thought generation, yet exhibit significant computational inefficiency by applying uniform reasoning strategies regardless of problem complexity. We present Hierarchical Budget Policy Optimization (HBPO), a reinforcement learning framework that enables models to learn problem-specific reasoning depths without sacrificing capability. HBPO addresses the fundamental challenge of exploration space collapse in efficiency-oriented training, where penalties on long output length systematically bias models away from necessary long reasoning paths. Through hierarchical budget exploration, our approach partitions rollout samples into multiple subgroups with distinct token budgets, aiming to enable efficient resource allocation while preventing degradation of capability. We introduce differentiated reward mechanisms that create budget-aware incentives aligned with the complexity of the problem, allowing models to discover natural correspondences between task requirements and computational effort. Extensive experiments demonstrate that HBPO reduces average token usage by up to 60.6% while improving accuracy by 3.14% across four reasoning benchmarks. Unlike existing methods that impose external constraints or rely on discrete mode selection, HBPO exhibits emergent adaptive behavior where models automatically adjust reasoning depth based on problem complexity. Our results suggest that reasoning efficiency and capability are not inherently conflicting, and can be simultaneously optimized through appropriately structured hierarchical training that preserves exploration diversity.
[25.07.2025 03:58] Response: {
  "desc": "Статья представляет метод Hierarchical Budget Policy Optimization (HBPO) для оптимизации глубины рассуждений в больших языковых моделях. HBPO использует иерархическое исследование бюджета токенов и дифференцированные механизмы вознаграждения, чтобы модели могли адаптировать глубину рассуждений к сложности задачи. Эксперименты показывают, что HBPO снижает использование токенов на 60.6% при одновременном повышении точности на 3.14% на четырех эталонных тестах. Метод позволяет оптимизировать как эффективность, так и способности моделей машинного обучения.",
  "emoji": "🧠",
  "title": "Адаптивные рассуждения: эффективность без потери возможностей"
}
[25.07.2025 03:58] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Large reasoning models achieve remarkable performance through extensive chain-of-thought generation, yet exhibit significant computational inefficiency by applying uniform reasoning strategies regardless of problem complexity. We present Hierarchical Budget Policy Optimization (HBPO), a reinforcement learning framework that enables models to learn problem-specific reasoning depths without sacrificing capability. HBPO addresses the fundamental challenge of exploration space collapse in efficiency-oriented training, where penalties on long output length systematically bias models away from necessary long reasoning paths. Through hierarchical budget exploration, our approach partitions rollout samples into multiple subgroups with distinct token budgets, aiming to enable efficient resource allocation while preventing degradation of capability. We introduce differentiated reward mechanisms that create budget-aware incentives aligned with the complexity of the problem, allowing models to discover natural correspondences between task requirements and computational effort. Extensive experiments demonstrate that HBPO reduces average token usage by up to 60.6% while improving accuracy by 3.14% across four reasoning benchmarks. Unlike existing methods that impose external constraints or rely on discrete mode selection, HBPO exhibits emergent adaptive behavior where models automatically adjust reasoning depth based on problem complexity. Our results suggest that reasoning efficiency and capability are not inherently conflicting, and can be simultaneously optimized through appropriately structured hierarchical training that preserves exploration diversity."

[25.07.2025 03:58] Response: ```python
["RL", "TRAINING", "BENCHMARK"]
```
[25.07.2025 03:58] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Large reasoning models achieve remarkable performance through extensive chain-of-thought generation, yet exhibit significant computational inefficiency by applying uniform reasoning strategies regardless of problem complexity. We present Hierarchical Budget Policy Optimization (HBPO), a reinforcement learning framework that enables models to learn problem-specific reasoning depths without sacrificing capability. HBPO addresses the fundamental challenge of exploration space collapse in efficiency-oriented training, where penalties on long output length systematically bias models away from necessary long reasoning paths. Through hierarchical budget exploration, our approach partitions rollout samples into multiple subgroups with distinct token budgets, aiming to enable efficient resource allocation while preventing degradation of capability. We introduce differentiated reward mechanisms that create budget-aware incentives aligned with the complexity of the problem, allowing models to discover natural correspondences between task requirements and computational effort. Extensive experiments demonstrate that HBPO reduces average token usage by up to 60.6% while improving accuracy by 3.14% across four reasoning benchmarks. Unlike existing methods that impose external constraints or rely on discrete mode selection, HBPO exhibits emergent adaptive behavior where models automatically adjust reasoning depth based on problem complexity. Our results suggest that reasoning efficiency and capability are not inherently conflicting, and can be simultaneously optimized through appropriately structured hierarchical training that preserves exploration diversity."

[25.07.2025 03:58] Response: ```python
['REASONING', 'OPTIMIZATION']
```
[25.07.2025 03:58] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces Hierarchical Budget Policy Optimization (HBPO), a new reinforcement learning framework designed to improve the efficiency of reasoning models. HBPO allows models to adapt their reasoning depth based on the complexity of the problem, rather than using a one-size-fits-all approach. By implementing hierarchical budget exploration and differentiated reward mechanisms, the framework encourages models to allocate resources effectively while maintaining their performance. The results show that HBPO can significantly reduce token usage and enhance accuracy, demonstrating that efficiency and capability can be optimized together.","title":"Optimizing Reasoning Efficiency with Adaptive Depth"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper introduces Hierarchical Budget Policy Optimization (HBPO), a new reinforcement learning framework designed to improve the efficiency of reasoning models. HBPO allows models to adapt their reasoning depth based on the complexity of the problem, rather than using a one-size-fits-all approach. By implementing hierarchical budget exploration and differentiated reward mechanisms, the framework encourages models to allocate resources effectively while maintaining their performance. The results show that HBPO can significantly reduce token usage and enhance accuracy, demonstrating that efficiency and capability can be optimized together.', title='Optimizing Reasoning Efficiency with Adaptive Depth'))
[25.07.2025 03:58] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本文提出了一种名为层次预算策略优化（HBPO）的强化学习框架，旨在提高推理模型的计算效率。HBPO通过分层预算探索，将样本分成多个具有不同令牌预算的子组，从而实现高效的资源分配。该方法引入了差异化奖励机制，使模型能够根据问题复杂性调整推理深度，避免了长输出长度的惩罚对模型的负面影响。实验结果表明，HBPO在四个推理基准上平均减少了60.6%的令牌使用，同时提高了3.14%的准确率，证明了推理效率与能力可以同时优化。","title":"高效推理，能力与效率并存"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本文提出了一种名为层次预算策略优化（HBPO）的强化学习框架，旨在提高推理模型的计算效率。HBPO通过分层预算探索，将样本分成多个具有不同令牌预算的子组，从而实现高效的资源分配。该方法引入了差异化奖励机制，使模型能够根据问题复杂性调整推理深度，避免了长输出长度的惩罚对模型的负面影响。实验结果表明，HBPO在四个推理基准上平均减少了60.6%的令牌使用，同时提高了3.14%的准确率，证明了推理效率与能力可以同时优化。', title='高效推理，能力与效率并存'))
[25.07.2025 03:58] Using data from previous issue: {"categories": ["#diffusion", "#architecture", "#synthetic", "#3d", "#dataset"], "emoji": "🌎", "ru": {"title": "EarthCrafter: революция в широкомасштабном 3D-моделировании Земли", "desc": "Статья представляет Aerial-Earth3D - крупнейший набор данных 3D аэросъемки, охватывающий 50 тысяч сцен по всей 
[25.07.2025 03:58] Querying the API.
[25.07.2025 03:58] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Captain Cinema generates high-quality short movies from textual descriptions using top-down keyframe planning and bottom-up video synthesis with interleaved training of Multimodal Diffusion Transformers.  					AI-generated summary 				 We present Captain Cinema, a generation framework for short movie generation. Given a detailed textual description of a movie storyline, our approach firstly generates a sequence of keyframes that outline the entire narrative, which ensures long-range coherence in both the storyline and visual appearance (e.g., scenes and characters). We refer to this step as top-down keyframe planning. These keyframes then serve as conditioning signals for a video synthesis model, which supports long context learning, to produce the spatio-temporal dynamics between them. This step is referred to as bottom-up video synthesis. To support stable and efficient generation of multi-scene long narrative cinematic works, we introduce an interleaved training strategy for Multimodal Diffusion Transformers (MM-DiT), specifically adapted for long-context video data. Our model is trained on a specially curated cinematic dataset consisting of interleaved data pairs. Our experiments demonstrate that Captain Cinema performs favorably in the automated creation of visually coherent and narrative consistent short movies in high quality and efficiency. Project page: https://thecinema.ai
[25.07.2025 03:58] Response: {
  "desc": "Captain Cinema - это система генерации коротких фильмов на основе текстовых описаний. Она использует двухэтапный подход: сначала планирование ключевых кадров сверху вниз, а затем синтез видео снизу вверх. Для обучения применяются мультимодальные диффузионные трансформеры с чередующейся стратегией обучения. Система демонстрирует хорошие результаты в создании визуально согласованных и нарративно последовательных коротких фильмов высокого качества.",
  "emoji": "🎬",
  "title": "От текста к кино: ИИ-режиссер Captain Cinema"
}
[25.07.2025 03:58] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Captain Cinema generates high-quality short movies from textual descriptions using top-down keyframe planning and bottom-up video synthesis with interleaved training of Multimodal Diffusion Transformers.  					AI-generated summary 				 We present Captain Cinema, a generation framework for short movie generation. Given a detailed textual description of a movie storyline, our approach firstly generates a sequence of keyframes that outline the entire narrative, which ensures long-range coherence in both the storyline and visual appearance (e.g., scenes and characters). We refer to this step as top-down keyframe planning. These keyframes then serve as conditioning signals for a video synthesis model, which supports long context learning, to produce the spatio-temporal dynamics between them. This step is referred to as bottom-up video synthesis. To support stable and efficient generation of multi-scene long narrative cinematic works, we introduce an interleaved training strategy for Multimodal Diffusion Transformers (MM-DiT), specifically adapted for long-context video data. Our model is trained on a specially curated cinematic dataset consisting of interleaved data pairs. Our experiments demonstrate that Captain Cinema performs favorably in the automated creation of visually coherent and narrative consistent short movies in high quality and efficiency. Project page: https://thecinema.ai"

[25.07.2025 03:58] Response: ```python
['DATASET', 'MULTIMODAL', 'VIDEO']
```
[25.07.2025 03:58] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Captain Cinema generates high-quality short movies from textual descriptions using top-down keyframe planning and bottom-up video synthesis with interleaved training of Multimodal Diffusion Transformers.  					AI-generated summary 				 We present Captain Cinema, a generation framework for short movie generation. Given a detailed textual description of a movie storyline, our approach firstly generates a sequence of keyframes that outline the entire narrative, which ensures long-range coherence in both the storyline and visual appearance (e.g., scenes and characters). We refer to this step as top-down keyframe planning. These keyframes then serve as conditioning signals for a video synthesis model, which supports long context learning, to produce the spatio-temporal dynamics between them. This step is referred to as bottom-up video synthesis. To support stable and efficient generation of multi-scene long narrative cinematic works, we introduce an interleaved training strategy for Multimodal Diffusion Transformers (MM-DiT), specifically adapted for long-context video data. Our model is trained on a specially curated cinematic dataset consisting of interleaved data pairs. Our experiments demonstrate that Captain Cinema performs favorably in the automated creation of visually coherent and narrative consistent short movies in high quality and efficiency. Project page: https://thecinema.ai"

[25.07.2025 03:58] Response: ```python
['DIFFUSION', 'LONG_CONTEXT', 'STORY_GENERATION']
```
[25.07.2025 03:58] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Captain Cinema is a framework designed to create short movies from text descriptions. It uses a two-step process: first, it generates keyframes that outline the movie\'s storyline, ensuring that the visuals and narrative are coherent. Next, it synthesizes video by connecting these keyframes, allowing for smooth transitions and dynamic scenes. The model employs Multimodal Diffusion Transformers with a unique training strategy to enhance the quality and efficiency of the movie generation process.","title":"Transforming Text to Cinematic Reality with Captain Cinema"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="Captain Cinema is a framework designed to create short movies from text descriptions. It uses a two-step process: first, it generates keyframes that outline the movie's storyline, ensuring that the visuals and narrative are coherent. Next, it synthesizes video by connecting these keyframes, allowing for smooth transitions and dynamic scenes. The model employs Multimodal Diffusion Transformers with a unique training strategy to enhance the quality and efficiency of the movie generation process.", title='Transforming Text to Cinematic Reality with Captain Cinema'))
[25.07.2025 03:58] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Captain Cinema 是一个短片生成框架，可以根据文本描述生成高质量的短电影。首先，它通过自上而下的关键帧规划生成一系列关键帧，以确保故事情节和视觉外观的一致性。接着，这些关键帧作为条件信号，供视频合成模型使用，从而生成场景之间的时空动态。我们还引入了一种交错训练策略，专门为长上下文视频数据设计的多模态扩散变换器，以支持多场景长叙事电影的稳定高效生成。","title":"用文本描述生成高质量短片的创新框架"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Captain Cinema 是一个短片生成框架，可以根据文本描述生成高质量的短电影。首先，它通过自上而下的关键帧规划生成一系列关键帧，以确保故事情节和视觉外观的一致性。接着，这些关键帧作为条件信号，供视频合成模型使用，从而生成场景之间的时空动态。我们还引入了一种交错训练策略，专门为长上下文视频数据设计的多模态扩散变换器，以支持多场景长叙事电影的稳定高效生成。', title='用文本描述生成高质量短片的创新框架'))
[25.07.2025 03:58] Querying the API.
[25.07.2025 03:58] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

TeEFusion enhances text-to-image synthesis by efficiently incorporating classifier-free guidance into text embeddings, reducing inference costs without sacrificing image quality.  					AI-generated summary 				 Recent advances in text-to-image synthesis largely benefit from sophisticated sampling strategies and classifier-free guidance (CFG) to ensure high-quality generation. However, CFG's reliance on two forward passes, especially when combined with intricate sampling algorithms, results in prohibitively high inference costs. To address this, we introduce TeEFusion (Text Embeddings Fusion), a novel and efficient distillation method that directly incorporates the guidance magnitude into the text embeddings and distills the teacher model's complex sampling strategy. By simply fusing conditional and unconditional text embeddings using linear operations, TeEFusion reconstructs the desired guidance without adding extra parameters, simultaneously enabling the student model to learn from the teacher's output produced via its sophisticated sampling approach. Extensive experiments on state-of-the-art models such as SD3 demonstrate that our method allows the student to closely mimic the teacher's performance with a far simpler and more efficient sampling strategy. Consequently, the student model achieves inference speeds up to 6times faster than the teacher model, while maintaining image quality at levels comparable to those obtained through the teacher's complex sampling approach. The code is publicly available at https://github.com/AIDC-AI/TeEFusion{github.com/AIDC-AI/TeEFusion}.
[25.07.2025 03:58] Response: {
  "desc": "TeEFusion - это новый метод дистилляции для синтеза изображений по тексту. Он объединяет условные и безусловные текстовые эмбеддинги, позволяя эффективно применять classifier-free guidance без дополнительных параметров. Метод позволяет ученической модели имитировать сложную стратегию сэмплирования учительской модели, значительно ускоряя инференс. Эксперименты на современных моделях вроде SD3 показывают ускорение до 6 раз при сохранении качества изображений.",
  "emoji": "🖼️",
  "title": "TeEFusion: быстрый синтез изображений без потери качества"
}
[25.07.2025 03:58] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"TeEFusion enhances text-to-image synthesis by efficiently incorporating classifier-free guidance into text embeddings, reducing inference costs without sacrificing image quality.  					AI-generated summary 				 Recent advances in text-to-image synthesis largely benefit from sophisticated sampling strategies and classifier-free guidance (CFG) to ensure high-quality generation. However, CFG's reliance on two forward passes, especially when combined with intricate sampling algorithms, results in prohibitively high inference costs. To address this, we introduce TeEFusion (Text Embeddings Fusion), a novel and efficient distillation method that directly incorporates the guidance magnitude into the text embeddings and distills the teacher model's complex sampling strategy. By simply fusing conditional and unconditional text embeddings using linear operations, TeEFusion reconstructs the desired guidance without adding extra parameters, simultaneously enabling the student model to learn from the teacher's output produced via its sophisticated sampling approach. Extensive experiments on state-of-the-art models such as SD3 demonstrate that our method allows the student to closely mimic the teacher's performance with a far simpler and more efficient sampling strategy. Consequently, the student model achieves inference speeds up to 6times faster than the teacher model, while maintaining image quality at levels comparable to those obtained through the teacher's complex sampling approach. The code is publicly available at https://github.com/AIDC-AI/TeEFusion{github.com/AIDC-AI/TeEFusion}."

[25.07.2025 03:58] Response: ```python
['CV', 'INFERENCE', 'TRAINING']
```
[25.07.2025 03:58] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"TeEFusion enhances text-to-image synthesis by efficiently incorporating classifier-free guidance into text embeddings, reducing inference costs without sacrificing image quality.  					AI-generated summary 				 Recent advances in text-to-image synthesis largely benefit from sophisticated sampling strategies and classifier-free guidance (CFG) to ensure high-quality generation. However, CFG's reliance on two forward passes, especially when combined with intricate sampling algorithms, results in prohibitively high inference costs. To address this, we introduce TeEFusion (Text Embeddings Fusion), a novel and efficient distillation method that directly incorporates the guidance magnitude into the text embeddings and distills the teacher model's complex sampling strategy. By simply fusing conditional and unconditional text embeddings using linear operations, TeEFusion reconstructs the desired guidance without adding extra parameters, simultaneously enabling the student model to learn from the teacher's output produced via its sophisticated sampling approach. Extensive experiments on state-of-the-art models such as SD3 demonstrate that our method allows the student to closely mimic the teacher's performance with a far simpler and more efficient sampling strategy. Consequently, the student model achieves inference speeds up to 6times faster than the teacher model, while maintaining image quality at levels comparable to those obtained through the teacher's complex sampling approach. The code is publicly available at https://github.com/AIDC-AI/TeEFusion{github.com/AIDC-AI/TeEFusion}."

[25.07.2025 03:58] Response: ```python
["OPTIMIZATION", "OPEN_SOURCE"]
```
[25.07.2025 03:58] Response: ParsedChatCompletionMessage[Article](content='{"desc":"TeEFusion is a new method that improves text-to-image synthesis by integrating classifier-free guidance directly into text embeddings. This approach reduces the need for multiple forward passes, which are costly in terms of computation, while still producing high-quality images. By fusing conditional and unconditional text embeddings through simple linear operations, TeEFusion allows a student model to learn from a more complex teacher model without increasing the number of parameters. As a result, the student model can generate images up to six times faster than the teacher model, while maintaining similar image quality.","title":"TeEFusion: Fast and Efficient Text-to-Image Synthesis"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='TeEFusion is a new method that improves text-to-image synthesis by integrating classifier-free guidance directly into text embeddings. This approach reduces the need for multiple forward passes, which are costly in terms of computation, while still producing high-quality images. By fusing conditional and unconditional text embeddings through simple linear operations, TeEFusion allows a student model to learn from a more complex teacher model without increasing the number of parameters. As a result, the student model can generate images up to six times faster than the teacher model, while maintaining similar image quality.', title='TeEFusion: Fast and Efficient Text-to-Image Synthesis'))
[25.07.2025 03:58] Response: ParsedChatCompletionMessage[Article](content='{"desc":"TeEFusion是一种新颖的文本嵌入融合方法，旨在提高文本到图像合成的效率。它通过将无分类器引导直接融入文本嵌入中，减少了推理成本，同时保持了图像质量。该方法通过线性操作简单地融合条件和无条件文本嵌入，避免了额外参数的增加。实验表明，TeEFusion使得学生模型在推理速度上比教师模型快6倍，同时图像质量与教师模型相当。","title":"TeEFusion：高效的文本到图像合成方法"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='TeEFusion是一种新颖的文本嵌入融合方法，旨在提高文本到图像合成的效率。它通过将无分类器引导直接融入文本嵌入中，减少了推理成本，同时保持了图像质量。该方法通过线性操作简单地融合条件和无条件文本嵌入，避免了额外参数的增加。实验表明，TeEFusion使得学生模型在推理速度上比教师模型快6倍，同时图像质量与教师模型相当。', title='TeEFusion：高效的文本到图像合成方法'))
[25.07.2025 03:58] Using data from previous issue: {"categories": ["#diffusion", "#training", "#rl", "#audio", "#optimization"], "emoji": "🎙️", "ru": {"title": "Оптимизация синтеза речи на всех уровнях", "desc": "DMOSpeech 2 - это улучшенная система синтеза речи, оптимизирующая предсказание длительности звуков с помощью обучения с подкреплением. Сис
[25.07.2025 03:58] Renaming data file.
[25.07.2025 03:58] Renaming previous data. hf_papers.json to ./d/2025-07-25.json
[25.07.2025 03:58] Saving new data file.
[25.07.2025 03:58] Generating page.
[25.07.2025 03:58] Renaming previous page.
[25.07.2025 03:58] Renaming previous data. index.html to ./d/2025-07-25.html
[25.07.2025 03:58] Writing result.
[25.07.2025 03:58] Renaming log file.
[25.07.2025 03:58] Renaming previous data. log.txt to ./logs/2025-07-25_last_log.txt
