[25.07.2025 00:59] Read previous papers.
[25.07.2025 00:59] Generating top page (month).
[25.07.2025 00:59] Writing top page (month).
[25.07.2025 02:57] Read previous papers.
[25.07.2025 02:57] Get feed.
[25.07.2025 02:57] Extract page data from URL. URL: https://huggingface.co/papers/2507.18537
[25.07.2025 02:57] Extract page data from URL. URL: https://huggingface.co/papers/2507.16535
[25.07.2025 02:57] Extract page data from URL. URL: https://huggingface.co/papers/2507.14988
[25.07.2025 02:57] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[25.07.2025 02:57] Downloading and parsing papers (pdf, html). Total: 3.
[25.07.2025 02:57] Downloading and parsing paper https://huggingface.co/papers/2507.18537.
[25.07.2025 02:57] Downloading paper 2507.18537 from http://arxiv.org/pdf/2507.18537v1...
[25.07.2025 02:57] Extracting affiliations from text.
[25.07.2025 02:57] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 4 2 ] . [ 1 7 3 5 8 1 . 7 0 5 2 : r TTS-VAR: Test-Time Scaling Framework for Visual Auto-Regressive Generation Zhekai Chen1 Ruihang Chu2 Yukang Chen3 Shiwei Zhang2 Yujie Wei2 Yingya Zhang2 Xihui Liu1 1 HKU MMLab 2 Tongyi Lab, Alibaba Group 3 CUHK zkchen66@outlook.com "
[25.07.2025 02:57] Response: ```python
["HKU MMLab", "Tongyi Lab, Alibaba Group", "CUHK"]
```
[25.07.2025 02:57] Deleting PDF ./assets/pdf/2507.18537.pdf.
[25.07.2025 02:57] Success.
[25.07.2025 02:57] Downloading and parsing paper https://huggingface.co/papers/2507.16535.
[25.07.2025 02:57] Downloading paper 2507.16535 from http://arxiv.org/pdf/2507.16535v2...
[25.07.2025 02:57] Extracting affiliations from text.
[25.07.2025 02:57] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"EarthCrafter: Scalable 3D Earth Generation via Dual-Sparse Latent Diffusion Shang Liu1,2*, Chenjie Cao1,2,3*, Chaohui Yu1,2, Wen Qian1,2, Jing Wang1,2, Fan Wang1 1DAMO Academy, Alibaba Group, 2Hupan Lab, 3Fudan University {liushang.ls,caochenjie.ccj,huakun.ych, qianwen.qian, yunfei.wj, fan.w}@alibaba-inc.com Project page (code, model, and data): https://whiteinblue.github.io/earthcrafter/ 5 2 0 2 3 2 ] . [ 2 5 3 5 6 1 . 7 0 5 2 : r Figure 1: EarthCrafter enjoys impressive generations conditioned on various guidance, including (a) 1-view aerial semantic and (b) 1-view RGBD. (c) EarthCrafter is powerful enough to handle unconditional generation, sampling reasonable geographic-scale 3D assets from the prior distribution. (d) EarthCrafter is enabled to produce diverse outcomes. "
[25.07.2025 02:57] Response: ```python
["DAMO Academy, Alibaba Group", "Hupan Lab", "Fudan University"]
```
[25.07.2025 02:57] Deleting PDF ./assets/pdf/2507.16535.pdf.
[25.07.2025 02:57] Success.
[25.07.2025 02:57] Downloading and parsing paper https://huggingface.co/papers/2507.14988.
[25.07.2025 02:57] Downloading paper 2507.14988 from http://arxiv.org/pdf/2507.14988v1...
[25.07.2025 02:57] Extracting affiliations from text.
[25.07.2025 02:57] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 0 2 ] . e [ 1 8 8 9 4 1 . 7 0 5 2 : r DMOSpeech 2: Reinforcement Learning for Duration Prediction in Metric-Optimized Speech Synthesis Yingahao Aaron Li1, Xilin Jiang1, Fei Tao2, Cheng Niu2, Kaifeng Xu2, Juntong Song2, Nima Mesgarani1 1Columbia University, 2NewsBreak "
[25.07.2025 02:57] Response: ["Columbia University", "NewsBreak"]
[25.07.2025 02:57] Deleting PDF ./assets/pdf/2507.14988.pdf.
[25.07.2025 02:57] Success.
[25.07.2025 02:57] Enriching papers with extra data.
[25.07.2025 02:57] ********************************************************************************
[25.07.2025 02:57] Abstract 0. TTS-VAR, a test-time scaling framework for visual auto-regressive models, improves generation quality by dynamically adjusting batch sizes and using clustering and resampling techniques.  					AI-generated summary 				 Scaling visual generation models is essential for real-world content creation, ye...
[25.07.2025 02:57] ********************************************************************************
[25.07.2025 02:57] Abstract 1. Despite the remarkable developments achieved by recent 3D generation works, scaling these methods to geographic extents, such as modeling thousands of square kilometers of Earth's surface, remains an open challenge. We address this through a dual innovation in data infrastructure and model architect...
[25.07.2025 02:57] ********************************************************************************
[25.07.2025 02:57] Abstract 2. DMOSpeech 2 optimizes duration prediction and introduces teacher-guided sampling to enhance speech synthesis performance and diversity.  					AI-generated summary 				 Diffusion-based text-to-speech (TTS) systems have made remarkable progress in zero-shot speech synthesis, yet optimizing all compone...
[25.07.2025 02:57] Read previous papers.
[25.07.2025 02:57] Generating reviews via LLM API.
[25.07.2025 02:57] Querying the API.
[25.07.2025 02:57] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

TTS-VAR, a test-time scaling framework for visual auto-regressive models, improves generation quality by dynamically adjusting batch sizes and using clustering and resampling techniques.  					AI-generated summary 				 Scaling visual generation models is essential for real-world content creation, yet requires substantial training and computational expenses. Alternatively, test-time scaling has garnered growing attention due to resource efficiency and promising performance. In this work, we present TTS-VAR, the first general test-time scaling framework for visual auto-regressive (VAR) models, modeling the generation process as a path searching problem. To dynamically balance computational efficiency with exploration capacity, we first introduce an adaptive descending batch size schedule throughout the causal generation process. Besides, inspired by VAR's hierarchical coarse-to-fine multi-scale generation, our framework integrates two key components: (i) At coarse scales, we observe that generated tokens are hard for evaluation, possibly leading to erroneous acceptance of inferior samples or rejection of superior samples. Noticing that the coarse scales contain sufficient structural information, we propose clustering-based diversity search. It preserves structural variety through semantic feature clustering, enabling later selection on samples with higher potential. (ii) In fine scales, resampling-based potential selection prioritizes promising candidates using potential scores, which are defined as reward functions incorporating multi-scale generation history. Experiments on the powerful VAR model Infinity show a notable 8.7% GenEval score improvement (from 0.69 to 0.75). Key insights reveal that early-stage structural features effectively influence final quality, and resampling efficacy varies across generation scales. Code is available at https://github.com/ali-vilab/TTS-VAR.
[25.07.2025 02:57] Response: {
  "desc": "TTS-VAR - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ½Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ€ĞµĞ³ÑƒĞ»Ğ¸Ñ€ÑƒÑ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ñ‹ Ğ±Ğ°Ñ‚Ñ‡ĞµĞ¹ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµÑÑĞ¼Ğ¿Ğ»Ğ¸Ğ½Ğ³Ğ°. TTS-VAR Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ°Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¿ÑƒÑ‚Ğ¸, Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€ÑƒÑ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ GenEval Ğ½Ğ° 8.7% Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Infinity.",
  "emoji": "ğŸ–¼ï¸",
  "title": "Ğ£Ğ¼Ğ½Ğ¾Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹"
}
[25.07.2025 02:57] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"TTS-VAR, a test-time scaling framework for visual auto-regressive models, improves generation quality by dynamically adjusting batch sizes and using clustering and resampling techniques.  					AI-generated summary 				 Scaling visual generation models is essential for real-world content creation, yet requires substantial training and computational expenses. Alternatively, test-time scaling has garnered growing attention due to resource efficiency and promising performance. In this work, we present TTS-VAR, the first general test-time scaling framework for visual auto-regressive (VAR) models, modeling the generation process as a path searching problem. To dynamically balance computational efficiency with exploration capacity, we first introduce an adaptive descending batch size schedule throughout the causal generation process. Besides, inspired by VAR's hierarchical coarse-to-fine multi-scale generation, our framework integrates two key components: (i) At coarse scales, we observe that generated tokens are hard for evaluation, possibly leading to erroneous acceptance of inferior samples or rejection of superior samples. Noticing that the coarse scales contain sufficient structural information, we propose clustering-based diversity search. It preserves structural variety through semantic feature clustering, enabling later selection on samples with higher potential. (ii) In fine scales, resampling-based potential selection prioritizes promising candidates using potential scores, which are defined as reward functions incorporating multi-scale generation history. Experiments on the powerful VAR model Infinity show a notable 8.7% GenEval score improvement (from 0.69 to 0.75). Key insights reveal that early-stage structural features effectively influence final quality, and resampling efficacy varies across generation scales. Code is available at https://github.com/ali-vilab/TTS-VAR."

[25.07.2025 02:57] Response: ```python
['CV', 'TRAINING']
```
[25.07.2025 02:57] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"TTS-VAR, a test-time scaling framework for visual auto-regressive models, improves generation quality by dynamically adjusting batch sizes and using clustering and resampling techniques.  					AI-generated summary 				 Scaling visual generation models is essential for real-world content creation, yet requires substantial training and computational expenses. Alternatively, test-time scaling has garnered growing attention due to resource efficiency and promising performance. In this work, we present TTS-VAR, the first general test-time scaling framework for visual auto-regressive (VAR) models, modeling the generation process as a path searching problem. To dynamically balance computational efficiency with exploration capacity, we first introduce an adaptive descending batch size schedule throughout the causal generation process. Besides, inspired by VAR's hierarchical coarse-to-fine multi-scale generation, our framework integrates two key components: (i) At coarse scales, we observe that generated tokens are hard for evaluation, possibly leading to erroneous acceptance of inferior samples or rejection of superior samples. Noticing that the coarse scales contain sufficient structural information, we propose clustering-based diversity search. It preserves structural variety through semantic feature clustering, enabling later selection on samples with higher potential. (ii) In fine scales, resampling-based potential selection prioritizes promising candidates using potential scores, which are defined as reward functions incorporating multi-scale generation history. Experiments on the powerful VAR model Infinity show a notable 8.7% GenEval score improvement (from 0.69 to 0.75). Key insights reveal that early-stage structural features effectively influence final quality, and resampling efficacy varies across generation scales. Code is available at https://github.com/ali-vilab/TTS-VAR."

[25.07.2025 02:57] Response: ```python
["OPTIMIZATION"]
```
[25.07.2025 02:57] Response: ParsedChatCompletionMessage[Article](content='{"desc":"TTS-VAR is a novel framework designed to enhance the quality of visual auto-regressive (VAR) models during the generation process. It introduces a dynamic batch size adjustment strategy that balances computational efficiency with the ability to explore diverse outputs. The framework employs clustering techniques at coarse scales to maintain structural diversity and resampling methods at fine scales to prioritize high-potential candidates based on their generation history. Experiments demonstrate a significant improvement in generation quality, highlighting the importance of early-stage features in the overall output.","title":"Dynamic Scaling for Enhanced Visual Generation Quality"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='TTS-VAR is a novel framework designed to enhance the quality of visual auto-regressive (VAR) models during the generation process. It introduces a dynamic batch size adjustment strategy that balances computational efficiency with the ability to explore diverse outputs. The framework employs clustering techniques at coarse scales to maintain structural diversity and resampling methods at fine scales to prioritize high-potential candidates based on their generation history. Experiments demonstrate a significant improvement in generation quality, highlighting the importance of early-stage features in the overall output.', title='Dynamic Scaling for Enhanced Visual Generation Quality'))
[25.07.2025 02:57] Response: ParsedChatCompletionMessage[Article](content='{"desc":"TTS-VARæ˜¯ä¸€ä¸ªç”¨äºè§†è§‰è‡ªå›å½’æ¨¡å‹çš„æµ‹è¯•æ—¶é—´ç¼©æ”¾æ¡†æ¶ï¼Œé€šè¿‡åŠ¨æ€è°ƒæ•´æ‰¹é‡å¤§å°å’Œä½¿ç”¨èšç±»ä¸é‡é‡‡æ ·æŠ€æœ¯æ¥æé«˜ç”Ÿæˆè´¨é‡ã€‚è¯¥æ¡†æ¶å°†ç”Ÿæˆè¿‡ç¨‹å»ºæ¨¡ä¸ºè·¯å¾„æœç´¢é—®é¢˜ï¼Œæ—¨åœ¨å¹³è¡¡è®¡ç®—æ•ˆç‡ä¸æ¢ç´¢èƒ½åŠ›ã€‚å®ƒåœ¨ç²—å°ºåº¦ä¸Šé‡‡ç”¨åŸºäºèšç±»çš„å¤šæ ·æ€§æœç´¢ï¼Œä»¥ä¿ç•™ç»“æ„å¤šæ ·æ€§ï¼Œå¹¶åœ¨ç»†å°ºåº¦ä¸Šé€šè¿‡é‡é‡‡æ ·ä¼˜å…ˆé€‰æ‹©æ½œåœ¨çš„ä¼˜è´¨æ ·æœ¬ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTTS-VARåœ¨å¼ºå¤§çš„VARæ¨¡å‹Infinityä¸Šå®ç°äº†8.7%çš„GenEvalåˆ†æ•°æå‡ï¼Œæ˜¾ç¤ºå‡ºæ—©æœŸç»“æ„ç‰¹å¾å¯¹æœ€ç»ˆè´¨é‡çš„æœ‰æ•ˆå½±å“ã€‚","title":"åŠ¨æ€è°ƒæ•´ï¼Œæå‡ç”Ÿæˆè´¨é‡çš„åˆ›æ–°æ¡†æ¶"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='TTS-VARæ˜¯ä¸€ä¸ªç”¨äºè§†è§‰è‡ªå›å½’æ¨¡å‹çš„æµ‹è¯•æ—¶é—´ç¼©æ”¾æ¡†æ¶ï¼Œé€šè¿‡åŠ¨æ€è°ƒæ•´æ‰¹é‡å¤§å°å’Œä½¿ç”¨èšç±»ä¸é‡é‡‡æ ·æŠ€æœ¯æ¥æé«˜ç”Ÿæˆè´¨é‡ã€‚è¯¥æ¡†æ¶å°†ç”Ÿæˆè¿‡ç¨‹å»ºæ¨¡ä¸ºè·¯å¾„æœç´¢é—®é¢˜ï¼Œæ—¨åœ¨å¹³è¡¡è®¡ç®—æ•ˆç‡ä¸æ¢ç´¢èƒ½åŠ›ã€‚å®ƒåœ¨ç²—å°ºåº¦ä¸Šé‡‡ç”¨åŸºäºèšç±»çš„å¤šæ ·æ€§æœç´¢ï¼Œä»¥ä¿ç•™ç»“æ„å¤šæ ·æ€§ï¼Œå¹¶åœ¨ç»†å°ºåº¦ä¸Šé€šè¿‡é‡é‡‡æ ·ä¼˜å…ˆé€‰æ‹©æ½œåœ¨çš„ä¼˜è´¨æ ·æœ¬ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTTS-VARåœ¨å¼ºå¤§çš„VARæ¨¡å‹Infinityä¸Šå®ç°äº†8.7%çš„GenEvalåˆ†æ•°æå‡ï¼Œæ˜¾ç¤ºå‡ºæ—©æœŸç»“æ„ç‰¹å¾å¯¹æœ€ç»ˆè´¨é‡çš„æœ‰æ•ˆå½±å“ã€‚', title='åŠ¨æ€è°ƒæ•´ï¼Œæå‡ç”Ÿæˆè´¨é‡çš„åˆ›æ–°æ¡†æ¶'))
[25.07.2025 02:57] Querying the API.
[25.07.2025 02:57] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Despite the remarkable developments achieved by recent 3D generation works, scaling these methods to geographic extents, such as modeling thousands of square kilometers of Earth's surface, remains an open challenge. We address this through a dual innovation in data infrastructure and model architecture. First, we introduce Aerial-Earth3D, the largest 3D aerial dataset to date, consisting of 50k curated scenes (each measuring 600m x 600m) captured across the U.S. mainland, comprising 45M multi-view Google Earth frames. Each scene provides pose-annotated multi-view images, depth maps, normals, semantic segmentation, and camera poses, with explicit quality control to ensure terrain diversity. Building on this foundation, we propose EarthCrafter, a tailored framework for large-scale 3D Earth generation via sparse-decoupled latent diffusion. Our architecture separates structural and textural generation: 1) Dual sparse 3D-VAEs compress high-resolution geometric voxels and textural 2D Gaussian Splats (2DGS) into compact latent spaces, largely alleviating the costly computation suffering from vast geographic scales while preserving critical information. 2) We propose condition-aware flow matching models trained on mixed inputs (semantics, images, or neither) to flexibly model latent geometry and texture features independently. Extensive experiments demonstrate that EarthCrafter performs substantially better in extremely large-scale generation. The framework further supports versatile applications, from semantic-guided urban layout generation to unconditional terrain synthesis, while maintaining geographic plausibility through our rich data priors from Aerial-Earth3D. Our project page is available at https://whiteinblue.github.io/earthcrafter/
[25.07.2025 02:57] Response: {
  "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Aerial-Earth3D - ĞºÑ€ÑƒĞ¿Ğ½ĞµĞ¹ÑˆĞ¸Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… 3D Ğ°ÑÑ€Ğ¾ÑÑŠĞµĞ¼ĞºĞ¸, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ 50 Ñ‚Ñ‹ÑÑÑ‡ ÑÑ†ĞµĞ½ Ğ¿Ğ¾ Ğ²ÑĞµĞ¹ Ñ‚ĞµÑ€Ñ€Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ¡Ğ¨Ğ. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº EarthCrafter Ğ´Ğ»Ñ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ·ĞµĞ¼Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸. ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° EarthCrafter Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ¸ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€Ñ‹, Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ğ¸ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€ Ğ² ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… 3D-ÑÑ†ĞµĞ½ Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ³ĞµĞ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ´Ğ¾ÑÑ‚Ğ¾Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ.",

  "emoji": "ğŸŒ",

  "title": "EarthCrafter: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ¼ 3D-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ—ĞµĞ¼Ğ»Ğ¸"
}
[25.07.2025 02:57] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Despite the remarkable developments achieved by recent 3D generation works, scaling these methods to geographic extents, such as modeling thousands of square kilometers of Earth's surface, remains an open challenge. We address this through a dual innovation in data infrastructure and model architecture. First, we introduce Aerial-Earth3D, the largest 3D aerial dataset to date, consisting of 50k curated scenes (each measuring 600m x 600m) captured across the U.S. mainland, comprising 45M multi-view Google Earth frames. Each scene provides pose-annotated multi-view images, depth maps, normals, semantic segmentation, and camera poses, with explicit quality control to ensure terrain diversity. Building on this foundation, we propose EarthCrafter, a tailored framework for large-scale 3D Earth generation via sparse-decoupled latent diffusion. Our architecture separates structural and textural generation: 1) Dual sparse 3D-VAEs compress high-resolution geometric voxels and textural 2D Gaussian Splats (2DGS) into compact latent spaces, largely alleviating the costly computation suffering from vast geographic scales while preserving critical information. 2) We propose condition-aware flow matching models trained on mixed inputs (semantics, images, or neither) to flexibly model latent geometry and texture features independently. Extensive experiments demonstrate that EarthCrafter performs substantially better in extremely large-scale generation. The framework further supports versatile applications, from semantic-guided urban layout generation to unconditional terrain synthesis, while maintaining geographic plausibility through our rich data priors from Aerial-Earth3D. Our project page is available at https://whiteinblue.github.io/earthcrafter/"

[25.07.2025 02:57] Response: ```python
['DATASET', '3D', 'ARCHITECTURE']
```
[25.07.2025 02:57] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Despite the remarkable developments achieved by recent 3D generation works, scaling these methods to geographic extents, such as modeling thousands of square kilometers of Earth's surface, remains an open challenge. We address this through a dual innovation in data infrastructure and model architecture. First, we introduce Aerial-Earth3D, the largest 3D aerial dataset to date, consisting of 50k curated scenes (each measuring 600m x 600m) captured across the U.S. mainland, comprising 45M multi-view Google Earth frames. Each scene provides pose-annotated multi-view images, depth maps, normals, semantic segmentation, and camera poses, with explicit quality control to ensure terrain diversity. Building on this foundation, we propose EarthCrafter, a tailored framework for large-scale 3D Earth generation via sparse-decoupled latent diffusion. Our architecture separates structural and textural generation: 1) Dual sparse 3D-VAEs compress high-resolution geometric voxels and textural 2D Gaussian Splats (2DGS) into compact latent spaces, largely alleviating the costly computation suffering from vast geographic scales while preserving critical information. 2) We propose condition-aware flow matching models trained on mixed inputs (semantics, images, or neither) to flexibly model latent geometry and texture features independently. Extensive experiments demonstrate that EarthCrafter performs substantially better in extremely large-scale generation. The framework further supports versatile applications, from semantic-guided urban layout generation to unconditional terrain synthesis, while maintaining geographic plausibility through our rich data priors from Aerial-Earth3D. Our project page is available at https://whiteinblue.github.io/earthcrafter/"

[25.07.2025 02:57] Response: ```python
["DIFFUSION", "SYNTHETIC"]
```
[25.07.2025 02:57] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents a solution to the challenge of generating large-scale 3D models of Earth\'s surface by introducing a new dataset and a novel model architecture. The Aerial-Earth3D dataset is the largest of its kind, containing 50,000 scenes with detailed annotations that support diverse terrain representation. The EarthCrafter framework utilizes a dual approach with sparse-decoupled latent diffusion, allowing for efficient generation of 3D structures and textures while managing computational costs. The results show significant improvements in generating realistic and plausible large-scale 3D environments, with applications in urban planning and terrain synthesis.","title":"Revolutionizing Large-Scale 3D Earth Generation"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="This paper presents a solution to the challenge of generating large-scale 3D models of Earth's surface by introducing a new dataset and a novel model architecture. The Aerial-Earth3D dataset is the largest of its kind, containing 50,000 scenes with detailed annotations that support diverse terrain representation. The EarthCrafter framework utilizes a dual approach with sparse-decoupled latent diffusion, allowing for efficient generation of 3D structures and textures while managing computational costs. The results show significant improvements in generating realistic and plausible large-scale 3D environments, with applications in urban planning and terrain synthesis.", title='Revolutionizing Large-Scale 3D Earth Generation'))
[25.07.2025 02:57] Response: ParsedChatCompletionMessage[Article](content='{"desc":"å°½ç®¡æœ€è¿‘çš„3Dç”ŸæˆæŠ€æœ¯å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å°†è¿™äº›æ–¹æ³•æ‰©å±•åˆ°å¤§è§„æ¨¡åœ°ç†èŒƒå›´ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æˆ‘ä»¬é€šè¿‡æ•°æ®åŸºç¡€è®¾æ–½å’Œæ¨¡å‹æ¶æ„çš„åŒé‡åˆ›æ–°æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚æˆ‘ä»¬ä»‹ç»äº†Aerial-Earth3Dï¼Œè¿™æ˜¯è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„3Dèˆªç©ºæ•°æ®é›†ï¼ŒåŒ…å«50,000ä¸ªåœºæ™¯ï¼Œæ”¯æŒå¤§è§„æ¨¡çš„3Dåœ°çƒç”Ÿæˆã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†EarthCrafteræ¡†æ¶ï¼Œé€šè¿‡ç¨€ç–è§£è€¦çš„æ½œåœ¨æ‰©æ•£æŠ€æœ¯ï¼Œå®ç°äº†é«˜æ•ˆçš„åœ°å½¢ç”Ÿæˆã€‚","title":"å¤§è§„æ¨¡3Dåœ°çƒç”Ÿæˆçš„æ–°çªç ´"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='å°½ç®¡æœ€è¿‘çš„3Dç”ŸæˆæŠ€æœ¯å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å°†è¿™äº›æ–¹æ³•æ‰©å±•åˆ°å¤§è§„æ¨¡åœ°ç†èŒƒå›´ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æˆ‘ä»¬é€šè¿‡æ•°æ®åŸºç¡€è®¾æ–½å’Œæ¨¡å‹æ¶æ„çš„åŒé‡åˆ›æ–°æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚æˆ‘ä»¬ä»‹ç»äº†Aerial-Earth3Dï¼Œè¿™æ˜¯è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„3Dèˆªç©ºæ•°æ®é›†ï¼ŒåŒ…å«50,000ä¸ªåœºæ™¯ï¼Œæ”¯æŒå¤§è§„æ¨¡çš„3Dåœ°çƒç”Ÿæˆã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†EarthCrafteræ¡†æ¶ï¼Œé€šè¿‡ç¨€ç–è§£è€¦çš„æ½œåœ¨æ‰©æ•£æŠ€æœ¯ï¼Œå®ç°äº†é«˜æ•ˆçš„åœ°å½¢ç”Ÿæˆã€‚', title='å¤§è§„æ¨¡3Dåœ°çƒç”Ÿæˆçš„æ–°çªç ´'))
[25.07.2025 02:57] Querying the API.
[25.07.2025 02:57] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

DMOSpeech 2 optimizes duration prediction and introduces teacher-guided sampling to enhance speech synthesis performance and diversity.  					AI-generated summary 				 Diffusion-based text-to-speech (TTS) systems have made remarkable progress in zero-shot speech synthesis, yet optimizing all components for perceptual metrics remains challenging. Prior work with DMOSpeech demonstrated direct metric optimization for speech generation components, but duration prediction remained unoptimized. This paper presents DMOSpeech 2, which extends metric optimization to the duration predictor through a reinforcement learning approach. The proposed system implements a novel duration policy framework using group relative preference optimization (GRPO) with speaker similarity and word error rate as reward signals. By optimizing this previously unoptimized component, DMOSpeech 2 creates a more complete metric-optimized synthesis pipeline. Additionally, this paper introduces teacher-guided sampling, a hybrid approach leveraging a teacher model for initial denoising steps before transitioning to the student model, significantly improving output diversity while maintaining efficiency. Comprehensive evaluations demonstrate superior performance across all metrics compared to previous systems, while reducing sampling steps by half without quality degradation. These advances represent a significant step toward speech synthesis systems with metric optimization across multiple components. The audio samples, code and pre-trained models are available at https://dmospeech2.github.io/.
[25.07.2025 02:58] Response: {
  "desc": "DMOSpeech 2 - ÑÑ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ñ€ĞµÑ‡Ğ¸, Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‰Ğ°Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ²ÑƒĞºĞ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞµ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¾Ğ²Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ (GRPO) Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑÑ…Ğ¾Ğ¶ĞµÑÑ‚Ğ¸ Ğ³Ğ¾Ğ»Ğ¾ÑĞ° Ğ´Ğ¸ĞºÑ‚Ğ¾Ñ€Ğ° Ğ¸ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ»Ğ¾Ğ² Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ. DMOSpeech 2 Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğ¼ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸. ĞšĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ Ğ²ÑĞµĞ¼ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼Ğ¸.",
  "emoji": "ğŸ™ï¸",
  "title": "ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ñ€ĞµÑ‡Ğ¸ Ğ½Ğ° Ğ²ÑĞµÑ… ÑƒÑ€Ğ¾Ğ²Ğ½ÑÑ…"
}
[25.07.2025 02:58] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"DMOSpeech 2 optimizes duration prediction and introduces teacher-guided sampling to enhance speech synthesis performance and diversity.  					AI-generated summary 				 Diffusion-based text-to-speech (TTS) systems have made remarkable progress in zero-shot speech synthesis, yet optimizing all components for perceptual metrics remains challenging. Prior work with DMOSpeech demonstrated direct metric optimization for speech generation components, but duration prediction remained unoptimized. This paper presents DMOSpeech 2, which extends metric optimization to the duration predictor through a reinforcement learning approach. The proposed system implements a novel duration policy framework using group relative preference optimization (GRPO) with speaker similarity and word error rate as reward signals. By optimizing this previously unoptimized component, DMOSpeech 2 creates a more complete metric-optimized synthesis pipeline. Additionally, this paper introduces teacher-guided sampling, a hybrid approach leveraging a teacher model for initial denoising steps before transitioning to the student model, significantly improving output diversity while maintaining efficiency. Comprehensive evaluations demonstrate superior performance across all metrics compared to previous systems, while reducing sampling steps by half without quality degradation. These advances represent a significant step toward speech synthesis systems with metric optimization across multiple components. The audio samples, code and pre-trained models are available at https://dmospeech2.github.io/."

[25.07.2025 02:58] Response: ```python
['AUDIO', 'RL', 'TRAINING']
```
[25.07.2025 02:58] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"DMOSpeech 2 optimizes duration prediction and introduces teacher-guided sampling to enhance speech synthesis performance and diversity.  					AI-generated summary 				 Diffusion-based text-to-speech (TTS) systems have made remarkable progress in zero-shot speech synthesis, yet optimizing all components for perceptual metrics remains challenging. Prior work with DMOSpeech demonstrated direct metric optimization for speech generation components, but duration prediction remained unoptimized. This paper presents DMOSpeech 2, which extends metric optimization to the duration predictor through a reinforcement learning approach. The proposed system implements a novel duration policy framework using group relative preference optimization (GRPO) with speaker similarity and word error rate as reward signals. By optimizing this previously unoptimized component, DMOSpeech 2 creates a more complete metric-optimized synthesis pipeline. Additionally, this paper introduces teacher-guided sampling, a hybrid approach leveraging a teacher model for initial denoising steps before transitioning to the student model, significantly improving output diversity while maintaining efficiency. Comprehensive evaluations demonstrate superior performance across all metrics compared to previous systems, while reducing sampling steps by half without quality degradation. These advances represent a significant step toward speech synthesis systems with metric optimization across multiple components. The audio samples, code and pre-trained models are available at https://dmospeech2.github.io/."

[25.07.2025 02:58] Response: ```python
["OPTIMIZATION", "DIFFUSION"]
```
[25.07.2025 02:58] Response: ParsedChatCompletionMessage[Article](content='{"desc":"DMOSpeech 2 enhances speech synthesis by optimizing duration prediction, which was previously unaddressed. It employs a reinforcement learning strategy with group relative preference optimization (GRPO) to improve the duration predictor using metrics like speaker similarity and word error rate. Additionally, the paper introduces teacher-guided sampling, which combines a teacher model for initial processing with a student model for efficiency, leading to greater output diversity. Overall, DMOSpeech 2 achieves superior performance in speech synthesis while reducing the number of sampling steps needed, marking a significant advancement in metric-optimized TTS systems.","title":"Optimizing Duration for Diverse Speech Synthesis"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='DMOSpeech 2 enhances speech synthesis by optimizing duration prediction, which was previously unaddressed. It employs a reinforcement learning strategy with group relative preference optimization (GRPO) to improve the duration predictor using metrics like speaker similarity and word error rate. Additionally, the paper introduces teacher-guided sampling, which combines a teacher model for initial processing with a student model for efficiency, leading to greater output diversity. Overall, DMOSpeech 2 achieves superior performance in speech synthesis while reducing the number of sampling steps needed, marking a significant advancement in metric-optimized TTS systems.', title='Optimizing Duration for Diverse Speech Synthesis'))
[25.07.2025 02:58] Response: ParsedChatCompletionMessage[Article](content='{"desc":"DMOSpeech 2 æ˜¯ä¸€ç§ä¼˜åŒ–è¯­éŸ³åˆæˆä¸­æŒç»­æ—¶é—´é¢„æµ‹çš„æ–°æ–¹æ³•ï¼Œé‡‡ç”¨äº†å¼ºåŒ–å­¦ä¹ çš„ç­–ç•¥æ¥æå‡åˆæˆæ•ˆæœã€‚è¯¥ç³»ç»Ÿå¼•å…¥äº†åŸºäºç»„ç›¸å¯¹åå¥½çš„ä¼˜åŒ–æ¡†æ¶ï¼Œåˆ©ç”¨è¯´è¯è€…ç›¸ä¼¼æ€§å’Œè¯é”™è¯¯ç‡ä½œä¸ºå¥–åŠ±ä¿¡å·ï¼Œä»è€Œä¼˜åŒ–äº†ä¹‹å‰æœªä¼˜åŒ–çš„æŒç»­æ—¶é—´é¢„æµ‹ç»„ä»¶ã€‚é™¤æ­¤ä¹‹å¤–ï¼ŒDMOSpeech 2 è¿˜é‡‡ç”¨äº†æ•™å¸ˆå¼•å¯¼é‡‡æ ·çš„æ–¹æ³•ï¼Œé€šè¿‡æ•™å¸ˆæ¨¡å‹è¿›è¡Œåˆæ­¥å»å™ªï¼Œå†è½¬å‘å­¦ç”Ÿæ¨¡å‹ï¼Œä»è€Œæ˜¾è‘—æé«˜äº†è¾“å‡ºçš„å¤šæ ·æ€§ã€‚ç»¼åˆè¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œè¯¥ç³»ç»Ÿåœ¨å„é¡¹æŒ‡æ ‡ä¸Šå‡ä¼˜äºä¹‹å‰çš„ç³»ç»Ÿï¼ŒåŒæ—¶å°†é‡‡æ ·æ­¥éª¤å‡å°‘äº†ä¸€åŠï¼Œä¸”æ²¡æœ‰é™ä½è´¨é‡ã€‚","title":"ä¼˜åŒ–è¯­éŸ³åˆæˆï¼Œæå‡å¤šæ ·æ€§ä¸æ•ˆç‡"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='DMOSpeech 2 æ˜¯ä¸€ç§ä¼˜åŒ–è¯­éŸ³åˆæˆä¸­æŒç»­æ—¶é—´é¢„æµ‹çš„æ–°æ–¹æ³•ï¼Œé‡‡ç”¨äº†å¼ºåŒ–å­¦ä¹ çš„ç­–ç•¥æ¥æå‡åˆæˆæ•ˆæœã€‚è¯¥ç³»ç»Ÿå¼•å…¥äº†åŸºäºç»„ç›¸å¯¹åå¥½çš„ä¼˜åŒ–æ¡†æ¶ï¼Œåˆ©ç”¨è¯´è¯è€…ç›¸ä¼¼æ€§å’Œè¯é”™è¯¯ç‡ä½œä¸ºå¥–åŠ±ä¿¡å·ï¼Œä»è€Œä¼˜åŒ–äº†ä¹‹å‰æœªä¼˜åŒ–çš„æŒç»­æ—¶é—´é¢„æµ‹ç»„ä»¶ã€‚é™¤æ­¤ä¹‹å¤–ï¼ŒDMOSpeech 2 è¿˜é‡‡ç”¨äº†æ•™å¸ˆå¼•å¯¼é‡‡æ ·çš„æ–¹æ³•ï¼Œé€šè¿‡æ•™å¸ˆæ¨¡å‹è¿›è¡Œåˆæ­¥å»å™ªï¼Œå†è½¬å‘å­¦ç”Ÿæ¨¡å‹ï¼Œä»è€Œæ˜¾è‘—æé«˜äº†è¾“å‡ºçš„å¤šæ ·æ€§ã€‚ç»¼åˆè¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œè¯¥ç³»ç»Ÿåœ¨å„é¡¹æŒ‡æ ‡ä¸Šå‡ä¼˜äºä¹‹å‰çš„ç³»ç»Ÿï¼ŒåŒæ—¶å°†é‡‡æ ·æ­¥éª¤å‡å°‘äº†ä¸€åŠï¼Œä¸”æ²¡æœ‰é™ä½è´¨é‡ã€‚', title='ä¼˜åŒ–è¯­éŸ³åˆæˆï¼Œæå‡å¤šæ ·æ€§ä¸æ•ˆç‡'))
[25.07.2025 02:58] Renaming data file.
[25.07.2025 02:58] Renaming previous data. hf_papers.json to ./d/2025-07-25.json
[25.07.2025 02:58] Saving new data file.
[25.07.2025 02:58] Generating page.
[25.07.2025 02:58] Renaming previous page.
[25.07.2025 02:58] Renaming previous data. index.html to ./d/2025-07-25.html
[25.07.2025 02:58] Writing result.
[25.07.2025 02:58] Renaming log file.
[25.07.2025 02:58] Renaming previous data. log.txt to ./logs/2025-07-25_last_log.txt
