[25.07.2025 21:11] Read previous papers.
[25.07.2025 21:11] Generating top page (month).
[25.07.2025 21:11] Writing top page (month).
[25.07.2025 22:12] Read previous papers.
[25.07.2025 22:12] Get feed.
[25.07.2025 22:12] Get page data from previous paper. URL: https://huggingface.co/papers/2507.13546
[25.07.2025 22:12] Get page data from previous paper. URL: https://huggingface.co/papers/2507.18071
[25.07.2025 22:12] Get page data from previous paper. URL: https://huggingface.co/papers/2507.14958
[25.07.2025 22:12] Get page data from previous paper. URL: https://huggingface.co/papers/2507.15758
[25.07.2025 22:12] Get page data from previous paper. URL: https://huggingface.co/papers/2507.18634
[25.07.2025 22:12] Get page data from previous paper. URL: https://huggingface.co/papers/2507.18537
[25.07.2025 22:12] Get page data from previous paper. URL: https://huggingface.co/papers/2507.15844
[25.07.2025 22:12] Get page data from previous paper. URL: https://huggingface.co/papers/2507.16535
[25.07.2025 22:12] Get page data from previous paper. URL: https://huggingface.co/papers/2507.18464
[25.07.2025 22:12] Get page data from previous paper. URL: https://huggingface.co/papers/2507.14988
[25.07.2025 22:12] Get page data from previous paper. URL: https://huggingface.co/papers/2507.18103
[25.07.2025 22:12] Get page data from previous paper. URL: https://huggingface.co/papers/2507.18013
[25.07.2025 22:12] Get page data from previous paper. URL: https://huggingface.co/papers/2507.18546
[25.07.2025 22:12] Get page data from previous paper. URL: https://huggingface.co/papers/2507.15595
[25.07.2025 22:12] Get page data from previous paper. URL: https://huggingface.co/papers/2507.18192
[25.07.2025 22:12] Get page data from previous paper. URL: https://huggingface.co/papers/2507.16038
[25.07.2025 22:12] Get page data from previous paper. URL: https://huggingface.co/papers/2507.18405
[25.07.2025 22:12] Get page data from previous paper. URL: https://huggingface.co/papers/2507.16802
[25.07.2025 22:12] Get page data from previous paper. URL: https://huggingface.co/papers/2507.18565
[25.07.2025 22:12] Get page data from previous paper. URL: https://huggingface.co/papers/2507.17402
[25.07.2025 22:12] Extract page data from URL. URL: https://huggingface.co/papers/2507.15807
[25.07.2025 22:12] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[25.07.2025 22:12] No deleted papers detected.
[25.07.2025 22:12] Downloading and parsing papers (pdf, html). Total: 21.
[25.07.2025 22:12] Downloading and parsing paper https://huggingface.co/papers/2507.13546.
[25.07.2025 22:12] Extra JSON file exists (./assets/json/2507.13546.json), skip PDF parsing.
[25.07.2025 22:12] Paper image links file exists (./assets/img_data/2507.13546.json), skip HTML parsing.
[25.07.2025 22:12] Success.
[25.07.2025 22:12] Downloading and parsing paper https://huggingface.co/papers/2507.18071.
[25.07.2025 22:12] Extra JSON file exists (./assets/json/2507.18071.json), skip PDF parsing.
[25.07.2025 22:12] Paper image links file exists (./assets/img_data/2507.18071.json), skip HTML parsing.
[25.07.2025 22:12] Success.
[25.07.2025 22:12] Downloading and parsing paper https://huggingface.co/papers/2507.14958.
[25.07.2025 22:12] Extra JSON file exists (./assets/json/2507.14958.json), skip PDF parsing.
[25.07.2025 22:12] Paper image links file exists (./assets/img_data/2507.14958.json), skip HTML parsing.
[25.07.2025 22:12] Success.
[25.07.2025 22:12] Downloading and parsing paper https://huggingface.co/papers/2507.15758.
[25.07.2025 22:12] Extra JSON file exists (./assets/json/2507.15758.json), skip PDF parsing.
[25.07.2025 22:12] Paper image links file exists (./assets/img_data/2507.15758.json), skip HTML parsing.
[25.07.2025 22:12] Success.
[25.07.2025 22:12] Downloading and parsing paper https://huggingface.co/papers/2507.18634.
[25.07.2025 22:12] Extra JSON file exists (./assets/json/2507.18634.json), skip PDF parsing.
[25.07.2025 22:12] Paper image links file exists (./assets/img_data/2507.18634.json), skip HTML parsing.
[25.07.2025 22:12] Success.
[25.07.2025 22:12] Downloading and parsing paper https://huggingface.co/papers/2507.18537.
[25.07.2025 22:12] Extra JSON file exists (./assets/json/2507.18537.json), skip PDF parsing.
[25.07.2025 22:12] Paper image links file exists (./assets/img_data/2507.18537.json), skip HTML parsing.
[25.07.2025 22:12] Success.
[25.07.2025 22:12] Downloading and parsing paper https://huggingface.co/papers/2507.15844.
[25.07.2025 22:12] Extra JSON file exists (./assets/json/2507.15844.json), skip PDF parsing.
[25.07.2025 22:12] Paper image links file exists (./assets/img_data/2507.15844.json), skip HTML parsing.
[25.07.2025 22:12] Success.
[25.07.2025 22:12] Downloading and parsing paper https://huggingface.co/papers/2507.16535.
[25.07.2025 22:12] Extra JSON file exists (./assets/json/2507.16535.json), skip PDF parsing.
[25.07.2025 22:12] Paper image links file exists (./assets/img_data/2507.16535.json), skip HTML parsing.
[25.07.2025 22:12] Success.
[25.07.2025 22:12] Downloading and parsing paper https://huggingface.co/papers/2507.18464.
[25.07.2025 22:12] Extra JSON file exists (./assets/json/2507.18464.json), skip PDF parsing.
[25.07.2025 22:12] Paper image links file exists (./assets/img_data/2507.18464.json), skip HTML parsing.
[25.07.2025 22:12] Success.
[25.07.2025 22:12] Downloading and parsing paper https://huggingface.co/papers/2507.14988.
[25.07.2025 22:12] Extra JSON file exists (./assets/json/2507.14988.json), skip PDF parsing.
[25.07.2025 22:12] Paper image links file exists (./assets/img_data/2507.14988.json), skip HTML parsing.
[25.07.2025 22:12] Success.
[25.07.2025 22:12] Downloading and parsing paper https://huggingface.co/papers/2507.18103.
[25.07.2025 22:12] Extra JSON file exists (./assets/json/2507.18103.json), skip PDF parsing.
[25.07.2025 22:12] Paper image links file exists (./assets/img_data/2507.18103.json), skip HTML parsing.
[25.07.2025 22:12] Success.
[25.07.2025 22:12] Downloading and parsing paper https://huggingface.co/papers/2507.18013.
[25.07.2025 22:12] Extra JSON file exists (./assets/json/2507.18013.json), skip PDF parsing.
[25.07.2025 22:12] Paper image links file exists (./assets/img_data/2507.18013.json), skip HTML parsing.
[25.07.2025 22:12] Success.
[25.07.2025 22:12] Downloading and parsing paper https://huggingface.co/papers/2507.18546.
[25.07.2025 22:12] Extra JSON file exists (./assets/json/2507.18546.json), skip PDF parsing.
[25.07.2025 22:12] Paper image links file exists (./assets/img_data/2507.18546.json), skip HTML parsing.
[25.07.2025 22:12] Success.
[25.07.2025 22:12] Downloading and parsing paper https://huggingface.co/papers/2507.15595.
[25.07.2025 22:12] Extra JSON file exists (./assets/json/2507.15595.json), skip PDF parsing.
[25.07.2025 22:12] Paper image links file exists (./assets/img_data/2507.15595.json), skip HTML parsing.
[25.07.2025 22:12] Success.
[25.07.2025 22:12] Downloading and parsing paper https://huggingface.co/papers/2507.18192.
[25.07.2025 22:12] Extra JSON file exists (./assets/json/2507.18192.json), skip PDF parsing.
[25.07.2025 22:12] Paper image links file exists (./assets/img_data/2507.18192.json), skip HTML parsing.
[25.07.2025 22:12] Success.
[25.07.2025 22:12] Downloading and parsing paper https://huggingface.co/papers/2507.16038.
[25.07.2025 22:12] Extra JSON file exists (./assets/json/2507.16038.json), skip PDF parsing.
[25.07.2025 22:12] Paper image links file exists (./assets/img_data/2507.16038.json), skip HTML parsing.
[25.07.2025 22:12] Success.
[25.07.2025 22:12] Downloading and parsing paper https://huggingface.co/papers/2507.18405.
[25.07.2025 22:12] Extra JSON file exists (./assets/json/2507.18405.json), skip PDF parsing.
[25.07.2025 22:12] Paper image links file exists (./assets/img_data/2507.18405.json), skip HTML parsing.
[25.07.2025 22:12] Success.
[25.07.2025 22:12] Downloading and parsing paper https://huggingface.co/papers/2507.16802.
[25.07.2025 22:12] Extra JSON file exists (./assets/json/2507.16802.json), skip PDF parsing.
[25.07.2025 22:12] Paper image links file exists (./assets/img_data/2507.16802.json), skip HTML parsing.
[25.07.2025 22:12] Success.
[25.07.2025 22:12] Downloading and parsing paper https://huggingface.co/papers/2507.18565.
[25.07.2025 22:12] Extra JSON file exists (./assets/json/2507.18565.json), skip PDF parsing.
[25.07.2025 22:12] Paper image links file exists (./assets/img_data/2507.18565.json), skip HTML parsing.
[25.07.2025 22:12] Success.
[25.07.2025 22:12] Downloading and parsing paper https://huggingface.co/papers/2507.17402.
[25.07.2025 22:12] Extra JSON file exists (./assets/json/2507.17402.json), skip PDF parsing.
[25.07.2025 22:12] Paper image links file exists (./assets/img_data/2507.17402.json), skip HTML parsing.
[25.07.2025 22:12] Success.
[25.07.2025 22:12] Downloading and parsing paper https://huggingface.co/papers/2507.15807.
[25.07.2025 22:12] Downloading paper 2507.15807 from http://arxiv.org/pdf/2507.15807v1...
[25.07.2025 22:12] Extracting affiliations from text.
[25.07.2025 22:12] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 1 2 ] . [ 1 7 0 8 5 1 . 7 0 5 2 : r Published as conference paper at COLM 2025 True Multimodal In-Context Learning Needs Attention to the Visual Context Shuo Chen1,3,5,6, Jianzhe Liu2, Zhen Han1, Yan Xia4, Daniel Cremers2,5, Philip Torr7, Volker Tresp1,5, Jindong Gu7 1LMU Munich, 2Technical University of Munich, 3Siemens AG 4University of Science and Technology of China, 5Munich Center for Machine Learning (MCML), 6Konrad Zuse School of Excellence in Reliable AI (relAI), 7University of Oxford "
[25.07.2025 22:12] Response: ```python
[
    "LMU Munich",
    "Technical University of Munich",
    "Siemens AG",
    "University of Science and Technology of China",
    "Munich Center for Machine Learning (MCML)",
    "Konrad Zuse School of Excellence in Reliable AI (relAI)",
    "University of Oxford"
]
```
[25.07.2025 22:12] Deleting PDF ./assets/pdf/2507.15807.pdf.
[25.07.2025 22:12] Success.
[25.07.2025 22:12] Enriching papers with extra data.
[25.07.2025 22:12] ********************************************************************************
[25.07.2025 22:12] Abstract 0. NABLA, a dynamic block-level attention mechanism, improves video diffusion transformers by enhancing computational efficiency without sacrificing generative quality.  					AI-generated summary 				 Recent progress in transformer-based architectures has demonstrated remarkable success in video genera...
[25.07.2025 22:12] ********************************************************************************
[25.07.2025 22:12] Abstract 1. This paper introduces Group Sequence Policy Optimization (GSPO), our stable, efficient, and performant reinforcement learning algorithm for training large language models. Unlike previous algorithms that adopt token-level importance ratios, GSPO defines the importance ratio based on sequence likelih...
[25.07.2025 22:12] ********************************************************************************
[25.07.2025 22:12] Abstract 2. Momentum Uncertainty-guided Reasoning (MUR) dynamically optimizes reasoning budgets in Large Language Models during inference, reducing computation and enhancing accuracy.  					AI-generated summary 				 Large Language Models (LLMs) have achieved impressive performance on reasoning-intensive tasks, ...
[25.07.2025 22:12] ********************************************************************************
[25.07.2025 22:12] Abstract 3. Large reasoning models have achieved remarkable performance through extended chain-of-thought sequences, yet this computational freedom leads to excessive token generation even for simple problems. We present Length-Adaptive Policy Optimization (LAPO), a novel framework that transforms reasoning len...
[25.07.2025 22:12] ********************************************************************************
[25.07.2025 22:12] Abstract 4. Captain Cinema generates high-quality short movies from textual descriptions using top-down keyframe planning and bottom-up video synthesis with interleaved training of Multimodal Diffusion Transformers.  					AI-generated summary 				 We present Captain Cinema, a generation framework for short movi...
[25.07.2025 22:12] ********************************************************************************
[25.07.2025 22:12] Abstract 5. TTS-VAR, a test-time scaling framework for visual auto-regressive models, improves generation quality by dynamically adjusting batch sizes and using clustering and resampling techniques.  					AI-generated summary 				 Scaling visual generation models is essential for real-world content creation, ye...
[25.07.2025 22:12] ********************************************************************************
[25.07.2025 22:12] Abstract 6. Large reasoning models achieve remarkable performance through extensive chain-of-thought generation, yet exhibit significant computational inefficiency by applying uniform reasoning strategies regardless of problem complexity. We present Hierarchical Budget Policy Optimization (HBPO), a reinforcemen...
[25.07.2025 22:12] ********************************************************************************
[25.07.2025 22:12] Abstract 7. Despite the remarkable developments achieved by recent 3D generation works, scaling these methods to geographic extents, such as modeling thousands of square kilometers of Earth's surface, remains an open challenge. We address this through a dual innovation in data infrastructure and model architect...
[25.07.2025 22:12] ********************************************************************************
[25.07.2025 22:12] Abstract 8. DriftMoE, an online Mixture-of-Experts architecture with a compact neural router, achieves competitive results in adapting to concept drift in data streams through a symbiotic learning loop.  					AI-generated summary 				 Learning from non-stationary data streams subject to concept drift requires m...
[25.07.2025 22:12] ********************************************************************************
[25.07.2025 22:12] Abstract 9. DMOSpeech 2 optimizes duration prediction and introduces teacher-guided sampling to enhance speech synthesis performance and diversity.  					AI-generated summary 				 Diffusion-based text-to-speech (TTS) systems have made remarkable progress in zero-shot speech synthesis, yet optimizing all compone...
[25.07.2025 22:12] ********************************************************************************
[25.07.2025 22:12] Abstract 10. New 2024 GloVe models improve upon 2014 versions by incorporating updated datasets and demonstrating enhanced performance on culturally and temporally relevant Named Entity Recognition tasks.  					AI-generated summary 				 This report documents, describes, and evaluates new 2024 English GloVe (Glob...
[25.07.2025 22:12] ********************************************************************************
[25.07.2025 22:12] Abstract 11. The TeleChat2, TeleChat2.5, and T1 models enhance language capabilities through advanced training strategies, including Supervised Fine-Tuning, Direct Preference Optimization, and reinforcement learning, achieving superior performance in reasoning and speed compared to previous models.  					AI-gene...
[25.07.2025 22:12] ********************************************************************************
[25.07.2025 22:12] Abstract 12. GLiNER2 is a unified framework that supports multiple NLP tasks using a single efficient transformer model, improving deployment accessibility over large language models.  					AI-generated summary 				 Information extraction (IE) is fundamental to numerous NLP applications, yet existing solutions o...
[25.07.2025 22:12] ********************************************************************************
[25.07.2025 22:12] Abstract 13. SegDT, a diffusion transformer-based segmentation model, achieves state-of-the-art results in skin lesion segmentation with fast inference speeds, making it suitable for real-world medical applications.  					AI-generated summary 				 Medical image segmentation is crucial for many healthcare tasks, ...
[25.07.2025 22:12] ********************************************************************************
[25.07.2025 22:12] Abstract 14. TeEFusion enhances text-to-image synthesis by efficiently incorporating classifier-free guidance into text embeddings, reducing inference costs without sacrificing image quality.  					AI-generated summary 				 Recent advances in text-to-image synthesis largely benefit from sophisticated sampling st...
[25.07.2025 22:12] ********************************************************************************
[25.07.2025 22:12] Abstract 15. A visual world model called SpelkeNet outperforms existing methods in identifying Spelke objects in images, improving performance in tasks like physical object manipulation.  					AI-generated summary 				 Segments in computer vision are often defined by semantic considerations and are highly depend...
[25.07.2025 22:12] ********************************************************************************
[25.07.2025 22:12] Abstract 16. Iwin Transformer, a hierarchical vision transformer without position embeddings, combines interleaved window attention and depthwise separable convolution for efficient global information exchange, achieving competitive performance in image classification, semantic segmentation, and video action rec...
[25.07.2025 22:12] ********************************************************************************
[25.07.2025 22:12] Abstract 17. The Agentar-Fin-R1 series of financial large language models enhances reasoning, reliability, and domain specialization through a trustworthiness assurance framework and achieves state-of-the-art performance on financial and general reasoning tasks.  					AI-generated summary 				 Large Language Mod...
[25.07.2025 22:12] ********************************************************************************
[25.07.2025 22:12] Abstract 18. A custom CNN architecture simultaneously classifies age and gender from facial images, improving performance by learning shared representations and achieving high accuracy and low mean absolute error.  					AI-generated summary 				 This paper presents a novel deep learning-based approach for simult...
[25.07.2025 22:12] ********************************************************************************
[25.07.2025 22:12] Abstract 19. HLFormer uses a hyperbolic modeling framework with Lorentz and Euclidean attention blocks to improve video-text retrieval by addressing hierarchical and partial relevance issues.  					AI-generated summary 				 Partially Relevant Video Retrieval (PRVR) addresses the critical challenge of matching un...
[25.07.2025 22:12] ********************************************************************************
[25.07.2025 22:12] Abstract 20. Multimodal Large Language Models (MLLMs), built on powerful language backbones, have enabled Multimodal In-Context Learning (MICL)-adapting to new tasks from a few multimodal demonstrations consisting of images, questions, and answers. Despite showing noticeable improvement on standard vision-langua...
[25.07.2025 22:12] Read previous papers.
[25.07.2025 22:12] Generating reviews via LLM API.
[25.07.2025 22:12] Using data from previous issue: {"categories": ["#open_source", "#training", "#video", "#optimization", "#diffusion", "#architecture"], "emoji": "üé¨", "ru": {"title": "NABLA: –£–º–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ", "desc": "NABLA - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è –≤–∏–¥–µ–æ-–¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤, –∫–æ—Ç–æ—Ä—ã–π –∞–¥–∞–ø—Ç–∏–≤–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ 
[25.07.2025 22:12] Using data from previous issue: {"categories": ["#rl", "#training", "#optimization", "#games"], "emoji": "üöÄ", "ru": {"title": "GSPO: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ —É—Ä–æ–≤–Ω–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π - Group Seq
[25.07.2025 22:12] Using data from previous issue: {"categories": ["#inference", "#reasoning", "#benchmark", "#architecture", "#training", "#optimization"], "emoji": "üß†", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ LLM: –º–µ–Ω—å—à–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π, –≤—ã—à–µ —Ç–æ—á–Ω–æ—Å—Ç—å", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ Momentum Uncertainty-guided Reasoning (MUR) –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Ä–∞
[25.07.2025 22:12] Using data from previous issue: {"categories": ["#optimization", "#training", "#reasoning", "#math", "#rl", "#benchmark"], "emoji": "üß†", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è: –º–µ–Ω—å—à–µ —Å–ª–æ–≤, –±–æ–ª—å—à–µ —Å–º—ã—Å–ª–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª–∏–Ω—ã —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º LAPO (Len
[25.07.2025 22:12] Using data from previous issue: {"categories": ["#diffusion", "#video", "#long_context", "#story_generation", "#multimodal", "#dataset"], "emoji": "üé¨", "ru": {"title": "–û—Ç —Ç–µ–∫—Å—Ç–∞ –∫ –∫–∏–Ω–æ: –ò–ò-—Ä–µ–∂–∏—Å—Å–µ—Ä Captain Cinema", "desc": "Captain Cinema - —ç—Ç–æ —Å–∏—Å—Ç–µ–º–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ñ–∏–ª—å–º–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –æ–ø–∏—Å–∞–Ω–∏–π. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–≤—É—Ö—ç—Ç
[25.07.2025 22:12] Using data from previous issue: {"categories": ["#cv", "#training", "#optimization"], "emoji": "üñºÔ∏è", "ru": {"title": "–£–º–Ω–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –ª—É—á—à–µ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "TTS-VAR - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤–æ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è. –û–Ω–∞ —É–ª—É—á—à–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π,
[25.07.2025 22:12] Using data from previous issue: {"categories": ["#optimization", "#training", "#reasoning", "#rl", "#benchmark"], "emoji": "üß†", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ Hierarchical Budget Policy Optimization (HBPO) –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≥–ª—É–±–∏–Ω—ã —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –±–æ–ª
[25.07.2025 22:12] Using data from previous issue: {"categories": ["#diffusion", "#architecture", "#synthetic", "#3d", "#dataset"], "emoji": "üåé", "ru": {"title": "EarthCrafter: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ —à–∏—Ä–æ–∫–æ–º–∞—Å—à—Ç–∞–±–Ω–æ–º 3D-–º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–∏ –ó–µ–º–ª–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Aerial-Earth3D - –∫—Ä—É–ø–Ω–µ–π—à–∏–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö 3D –∞—ç—Ä–æ—Å—ä–µ–º–∫–∏, –æ—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏–π 50 —Ç—ã—Å—è—á —Å—Ü–µ–Ω –ø–æ –≤—Å–µ–π 
[25.07.2025 22:12] Using data from previous issue: {"categories": ["#open_source", "#architecture", "#data", "#optimization", "#benchmark"], "emoji": "üåä", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –æ–Ω–ª–∞–π–Ω-–æ–±—É—á–µ–Ω–∏–µ —Å DriftMoE: —É–∫—Ä–æ—â–µ–Ω–∏–µ –ø–æ—Ç–æ–∫–∞ –¥–∞–Ω–Ω—ã—Ö", "desc": "DriftMoE - —ç—Ç–æ –Ω–æ–≤–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –æ–Ω–ª–∞–π–Ω-–æ–±—É—á–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–º–µ—Å–∏ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –∫ –∫–æ–Ω—Ü–µ–ø—Ç—É–∞–ª—å–Ω–æ–º
[25.07.2025 22:12] Using data from previous issue: {"categories": ["#diffusion", "#training", "#rl", "#audio", "#optimization"], "emoji": "üéôÔ∏è", "ru": {"title": "–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Å–∏–Ω—Ç–µ–∑–∞ —Ä–µ—á–∏ –Ω–∞ –≤—Å–µ—Ö —É—Ä–æ–≤–Ω—è—Ö", "desc": "DMOSpeech 2 - —ç—Ç–æ —É–ª—É—á—à–µ–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ —Å–∏–Ω—Ç–µ–∑–∞ —Ä–µ—á–∏, –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É—é—â–∞—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∑–≤—É–∫–æ–≤ —Å –ø–æ–º–æ—â—å—é –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º. –°–∏—Å
[25.07.2025 22:12] Using data from previous issue: {"categories": ["#data", "#open_source", "#dataset", "#transfer_learning", "#benchmark"], "emoji": "üîÑ", "ru": {"title": "–û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ GloVe: —à–∞–≥ –≤–ø–µ—Ä–µ–¥ –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞", "desc": "–ù–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ GloVe 2024 –≥–æ–¥–∞ —É–ª—É—á—à–∞—é—Ç –≤–µ—Ä—Å–∏–∏ 2014 –≥–æ–¥–∞, –≤–∫–ª—é—á–∞—è –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö. –û–Ω–∏ –¥–µ–º–æ–Ω
[25.07.2025 22:12] Using data from previous issue: {"categories": ["#reasoning", "#rlhf", "#open_source", "#architecture", "#training", "#optimization", "#rl"], "emoji": "üöÄ", "ru": {"title": "TeleChat: –Ω–æ–≤–æ–µ –ø–æ–∫–æ–ª–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—è–º–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è", "desc": "–ù–æ–≤–∞—è —Å–µ—Ä–∏—è –º–æ–¥–µ–ª–µ–π TeleChat (TeleChat2, TeleChat2.5 –∏ T1) –¥–µ–º–æ–Ω—Å
[25.07.2025 22:12] Using data from previous issue: {"categories": ["#multimodal", "#dataset", "#open_source", "#data", "#architecture", "#training"], "emoji": "ü§ñ", "ru": {"title": "–ï–¥–∏–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –º–Ω–æ–≥–æ–∑–∞–¥–∞—á–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞", "desc": "GLiNER2 - —ç—Ç–æ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∑–∞–¥–∞—á –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ —Å –ø–æ–º–æ—â—å—é –æ–¥–Ω
[25.07.2025 22:12] Using data from previous issue: {"categories": ["#inference", "#cv", "#diffusion", "#healthcare", "#open_source", "#benchmark"], "emoji": "üî¨", "ru": {"title": "SegDT: –ü—Ä–æ—Ä—ã–≤ –≤ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –ø–æ—Ä–∞–∂–µ–Ω–∏–π –∫–æ–∂–∏ —Å –ø–æ–º–æ—â—å—é –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç SegDT - –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–æ–≥–æ —Ç—Ä–∞–Ω
[25.07.2025 22:12] Using data from previous issue: {"categories": ["#optimization", "#training", "#open_source", "#inference", "#cv"], "emoji": "üñºÔ∏è", "ru": {"title": "TeEFusion: –±—ã—Å—Ç—Ä—ã–π —Å–∏–Ω—Ç–µ–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞", "desc": "TeEFusion - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ç–µ–∫—Å—Ç—É. –û–Ω –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç —É—Å–ª–æ–≤–Ω—ã–µ –∏ –±–µ–∑—É—Å–ª–æ–≤–Ω—ã–µ —Ç–µ–∫
[25.07.2025 22:12] Using data from previous issue: {"categories": ["#multimodal", "#games", "#cv", "#dataset", "#3d", "#optimization", "#benchmark"], "emoji": "üß†", "ru": {"title": "SpelkeNet: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –≤–æ—Å–ø—Ä–∏—è—Ç–∏–∏ –æ–±—ä–µ–∫—Ç–æ–≤ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–æ–º", "desc": "SpelkeNet - —ç—Ç–æ –≤–∏–∑—É–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –º–∏—Ä–∞, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã –≤ –∏–¥–µ–Ω—Ç–∏—Ñ–∏
[25.07.2025 22:12] Using data from previous issue: {"categories": ["#open_source", "#video", "#optimization", "#architecture", "#cv"], "emoji": "üî¨", "ru": {"title": "–ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä –±–µ–∑ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω Iwin Transformer - –Ω–æ–≤–∞—è –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ 
[25.07.2025 22:12] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#multimodal", "#training", "#optimization", "#science", "#agents", "#reasoning"], "emoji": "üíπ", "ru": {"title": "–ù–∞–¥–µ–∂–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –∑–∞–¥–∞—á", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ —Å–µ—Ä–∏—é —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π Agentar-Fin-R1, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω
[25.07.2025 22:12] Using data from previous issue: {"categories": ["#cv", "#dataset", "#architecture", "#ethics", "#optimization", "#training"], "emoji": "üë§", "ru": {"title": "–ï–¥–∏–Ω–∞—è CNN –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤–æ–∑—Ä–∞—Å—Ç–∞ –∏ –ø–æ–ª–∞ –ø–æ –ª–∏—Ü—É", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –≤–æ–∑—Ä–∞—Å—Ç–∞ –∏ –ø–æ–ª–∞ –ø–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º –ª–∏—Ü —Å –∏—Å–ø–æ
[25.07.2025 22:12] Using data from previous issue: {"categories": ["#multimodal", "#architecture", "#optimization", "#video", "#games"], "emoji": "üîç", "ru": {"title": "–ì–∏–ø–µ—Ä–±–æ–ª–∏—á–µ—Å–∫–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–∏—Å–∫–∞ –≤–∏–¥–µ–æ –ø–æ —Ç–µ–∫—Å—Ç—É", "desc": "HLFormer - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∑–∞–¥–∞—á–µ —á–∞—Å—Ç–∏—á–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –≤–∏–¥–µ–æ (PRVR), –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –≥–∏–ø–µ—Ä–±–æ–ª–∏—á–µ—Å
[25.07.2025 22:12] Querying the API.
[25.07.2025 22:12] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Multimodal Large Language Models (MLLMs), built on powerful language backbones, have enabled Multimodal In-Context Learning (MICL)-adapting to new tasks from a few multimodal demonstrations consisting of images, questions, and answers. Despite showing noticeable improvement on standard vision-language datasets, current MLLMs struggle to leverage visual information in the demonstrations. Specifically, they tend to neglect visual cues and over-rely on textual patterns, leading to mere text imitation rather than genuine multimodal adaptation. This behavior makes MICL still unimodal and largely restricts its practical utility. More importantly, this limitation is often concealed by the improved performance on tasks that do not require understanding the visual context. As a result, how to effectively enhance MICL ability and reliably evaluate the MICL performance remains underexplored. To address these issues, we first introduce Dynamic Attention Reallocation (DARA), an efficient fine-tuning strategy that encourages models to attend to the visual context by rebalancing attention across visual and textual tokens. In addition, we present TrueMICL, an MICL-dedicated dataset with both support and test sets that explicitly requires the integration of multimodal information-particularly visual content-for correct task completion. Extensive experiments demonstrate the effectiveness of our holistic solution, showcasing substantial improvements in the true multimodal in-context learning capabilities. Code and datasets are available at https://chenxshuo.github.io/true-micl-colm .
[25.07.2025 22:12] Response: {
  "desc": "–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –ø—Ä–æ–±–ª–µ–º–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ (MICL) –¥–ª—è –∫—Ä—É–ø–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–æ–¥–µ–ª–∏ —á–∞—Å—Ç–æ –∏–≥–Ω–æ—Ä–∏—Ä—É—é—Ç –≤–∏–∑—É–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –ø–æ–ª–∞–≥–∞—è—Å—å –≤ –æ—Å–Ω–æ–≤–Ω–æ–º –Ω–∞ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —à–∞–±–ª–æ–Ω—ã. –î–ª—è —Ä–µ—à–µ–Ω–∏—è —ç—Ç–æ–π –ø—Ä–æ–±–ª–µ–º—ã –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –ø–µ—Ä–µ—Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤–Ω–∏–º–∞–Ω–∏—è (DARA), –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π —É—á–∏—Ç—ã–≤–∞—Ç—å –≤–∏–∑—É–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç. –¢–∞–∫–∂–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö TrueMICL, —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∏—Å—Ç–∏–Ω–Ω—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –º–æ–¥–µ–ª–µ–π.",
  "emoji": "üëÅÔ∏è",
  "title": "–£–ª—É—á—à–µ–Ω–∏–µ –∏—Å—Ç–∏–Ω–Ω–æ–≥–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –¥–ª—è –∫—Ä—É–ø–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π"
}
[25.07.2025 22:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Multimodal Large Language Models (MLLMs), built on powerful language backbones, have enabled Multimodal In-Context Learning (MICL)-adapting to new tasks from a few multimodal demonstrations consisting of images, questions, and answers. Despite showing noticeable improvement on standard vision-language datasets, current MLLMs struggle to leverage visual information in the demonstrations. Specifically, they tend to neglect visual cues and over-rely on textual patterns, leading to mere text imitation rather than genuine multimodal adaptation. This behavior makes MICL still unimodal and largely restricts its practical utility. More importantly, this limitation is often concealed by the improved performance on tasks that do not require understanding the visual context. As a result, how to effectively enhance MICL ability and reliably evaluate the MICL performance remains underexplored. To address these issues, we first introduce Dynamic Attention Reallocation (DARA), an efficient fine-tuning strategy that encourages models to attend to the visual context by rebalancing attention across visual and textual tokens. In addition, we present TrueMICL, an MICL-dedicated dataset with both support and test sets that explicitly requires the integration of multimodal information-particularly visual content-for correct task completion. Extensive experiments demonstrate the effectiveness of our holistic solution, showcasing substantial improvements in the true multimodal in-context learning capabilities. Code and datasets are available at https://chenxshuo.github.io/true-micl-colm ."

[25.07.2025 22:12] Response: ```python
['MULTIMODAL', 'DATASET', 'TRAINING']
```
[25.07.2025 22:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Multimodal Large Language Models (MLLMs), built on powerful language backbones, have enabled Multimodal In-Context Learning (MICL)-adapting to new tasks from a few multimodal demonstrations consisting of images, questions, and answers. Despite showing noticeable improvement on standard vision-language datasets, current MLLMs struggle to leverage visual information in the demonstrations. Specifically, they tend to neglect visual cues and over-rely on textual patterns, leading to mere text imitation rather than genuine multimodal adaptation. This behavior makes MICL still unimodal and largely restricts its practical utility. More importantly, this limitation is often concealed by the improved performance on tasks that do not require understanding the visual context. As a result, how to effectively enhance MICL ability and reliably evaluate the MICL performance remains underexplored. To address these issues, we first introduce Dynamic Attention Reallocation (DARA), an efficient fine-tuning strategy that encourages models to attend to the visual context by rebalancing attention across visual and textual tokens. In addition, we present TrueMICL, an MICL-dedicated dataset with both support and test sets that explicitly requires the integration of multimodal information-particularly visual content-for correct task completion. Extensive experiments demonstrate the effectiveness of our holistic solution, showcasing substantial improvements in the true multimodal in-context learning capabilities. Code and datasets are available at https://chenxshuo.github.io/true-micl-colm ."

[25.07.2025 22:12] Response: ```python
['GAMES', 'OPTIMIZATION']
```
[25.07.2025 22:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper discusses the limitations of Multimodal Large Language Models (MLLMs) in effectively utilizing visual information during Multimodal In-Context Learning (MICL). The authors identify that current models often focus too much on text and fail to integrate visual cues, resulting in suboptimal performance in tasks that require understanding both modalities. To improve this, they propose a new method called Dynamic Attention Reallocation (DARA) that helps models better balance their attention between visual and textual inputs. Additionally, they introduce a new dataset, TrueMICL, designed to test and enhance the integration of multimodal information, leading to significant advancements in true multimodal learning capabilities.","title":"Enhancing Multimodal Learning with Dynamic Attention"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper discusses the limitations of Multimodal Large Language Models (MLLMs) in effectively utilizing visual information during Multimodal In-Context Learning (MICL). The authors identify that current models often focus too much on text and fail to integrate visual cues, resulting in suboptimal performance in tasks that require understanding both modalities. To improve this, they propose a new method called Dynamic Attention Reallocation (DARA) that helps models better balance their attention between visual and textual inputs. Additionally, they introduce a new dataset, TrueMICL, designed to test and enhance the integration of multimodal information, leading to significant advancements in true multimodal learning capabilities.', title='Enhancing Multimodal Learning with Dynamic Attention'))
[25.07.2025 22:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Â§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâÂà©Áî®Âº∫Â§ßÁöÑËØ≠Ë®ÄÂü∫Á°ÄÊû∂ÊûÑÔºåÊîØÊåÅÂ§öÊ®°ÊÄÅ‰∏ä‰∏ãÊñáÂ≠¶‰π†ÔºàMICLÔºâÔºåÂèØ‰ª•ÈÄöËøáÂ∞ëÈáèÁöÑÂõæÂÉè„ÄÅÈóÆÈ¢òÂíåÁ≠îÊ°àÁ§∫‰æãÈÄÇÂ∫îÊñ∞‰ªªÂä°„ÄÇÂ∞ΩÁÆ°Âú®Ê†áÂáÜÁöÑËßÜËßâ-ËØ≠Ë®ÄÊï∞ÊçÆÈõÜ‰∏äË°®Áé∞Âá∫ÊòæËëóÁöÑÊîπËøõÔºå‰ΩÜÂΩìÂâçÁöÑMLLMsÂú®Âà©Áî®Á§∫‰æã‰∏≠ÁöÑËßÜËßâ‰ø°ÊÅØÊñπÈù¢Â≠òÂú®Âõ∞ÈöæÔºåÂæÄÂæÄÂøΩËßÜËßÜËßâÁ∫øÁ¥¢ÔºåËøáÂ∫¶‰æùËµñÊñáÊú¨Ê®°Âºè„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊú¨ÊñáÊèêÂá∫‰∫ÜÂä®ÊÄÅÊ≥®ÊÑèÂäõÈáçÊñ∞ÂàÜÈÖçÔºàDARAÔºâÁ≠ñÁï•ÔºåÊó®Âú®ÈÄöËøáÈáçÊñ∞Âπ≥Ë°°ËßÜËßâÂíåÊñáÊú¨Ê†áËÆ∞ÁöÑÊ≥®ÊÑèÂäõÔºåÈºìÂä±Ê®°ÂûãÂÖ≥Ê≥®ËßÜËßâ‰∏ä‰∏ãÊñá„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ËøòÊé®Âá∫‰∫ÜTrueMICLÊï∞ÊçÆÈõÜÔºå‰∏ìÈó®Áî®‰∫éËØÑ‰º∞Â§öÊ®°ÊÄÅ‰ø°ÊÅØÁöÑÊï¥ÂêàËÉΩÂäõÔºåÂÆûÈ™åÁªìÊûúÊòæÁ§∫Êàë‰ª¨ÁöÑÊñπÊ≥ïÂú®ÁúüÊ≠£ÁöÑÂ§öÊ®°ÊÄÅ‰∏ä‰∏ãÊñáÂ≠¶‰π†ËÉΩÂäõ‰∏äÊúâÊòæËëóÊèêÂçá„ÄÇ","title":"ÊèêÂçáÂ§öÊ®°ÊÄÅÂ≠¶‰π†ËÉΩÂäõÁöÑÂàõÊñ∞ÊñπÊ≥ï"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Â§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâÂà©Áî®Âº∫Â§ßÁöÑËØ≠Ë®ÄÂü∫Á°ÄÊû∂ÊûÑÔºåÊîØÊåÅÂ§öÊ®°ÊÄÅ‰∏ä‰∏ãÊñáÂ≠¶‰π†ÔºàMICLÔºâÔºåÂèØ‰ª•ÈÄöËøáÂ∞ëÈáèÁöÑÂõæÂÉè„ÄÅÈóÆÈ¢òÂíåÁ≠îÊ°àÁ§∫‰æãÈÄÇÂ∫îÊñ∞‰ªªÂä°„ÄÇÂ∞ΩÁÆ°Âú®Ê†áÂáÜÁöÑËßÜËßâ-ËØ≠Ë®ÄÊï∞ÊçÆÈõÜ‰∏äË°®Áé∞Âá∫ÊòæËëóÁöÑÊîπËøõÔºå‰ΩÜÂΩìÂâçÁöÑMLLMsÂú®Âà©Áî®Á§∫‰æã‰∏≠ÁöÑËßÜËßâ‰ø°ÊÅØÊñπÈù¢Â≠òÂú®Âõ∞ÈöæÔºåÂæÄÂæÄÂøΩËßÜËßÜËßâÁ∫øÁ¥¢ÔºåËøáÂ∫¶‰æùËµñÊñáÊú¨Ê®°Âºè„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊú¨ÊñáÊèêÂá∫‰∫ÜÂä®ÊÄÅÊ≥®ÊÑèÂäõÈáçÊñ∞ÂàÜÈÖçÔºàDARAÔºâÁ≠ñÁï•ÔºåÊó®Âú®ÈÄöËøáÈáçÊñ∞Âπ≥Ë°°ËßÜËßâÂíåÊñáÊú¨Ê†áËÆ∞ÁöÑÊ≥®ÊÑèÂäõÔºåÈºìÂä±Ê®°ÂûãÂÖ≥Ê≥®ËßÜËßâ‰∏ä‰∏ãÊñá„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ËøòÊé®Âá∫‰∫ÜTrueMICLÊï∞ÊçÆÈõÜÔºå‰∏ìÈó®Áî®‰∫éËØÑ‰º∞Â§öÊ®°ÊÄÅ‰ø°ÊÅØÁöÑÊï¥ÂêàËÉΩÂäõÔºåÂÆûÈ™åÁªìÊûúÊòæÁ§∫Êàë‰ª¨ÁöÑÊñπÊ≥ïÂú®ÁúüÊ≠£ÁöÑÂ§öÊ®°ÊÄÅ‰∏ä‰∏ãÊñáÂ≠¶‰π†ËÉΩÂäõ‰∏äÊúâÊòæËëóÊèêÂçá„ÄÇ', title='ÊèêÂçáÂ§öÊ®°ÊÄÅÂ≠¶‰π†ËÉΩÂäõÁöÑÂàõÊñ∞ÊñπÊ≥ï'))
[25.07.2025 22:12] Renaming data file.
[25.07.2025 22:12] Renaming previous data. hf_papers.json to ./d/2025-07-25.json
[25.07.2025 22:12] Saving new data file.
[25.07.2025 22:12] Generating page.
[25.07.2025 22:12] Renaming previous page.
[25.07.2025 22:12] Renaming previous data. index.html to ./d/2025-07-25.html
[25.07.2025 22:12] Writing result.
[25.07.2025 22:12] Renaming log file.
[25.07.2025 22:12] Renaming previous data. log.txt to ./logs/2025-07-25_last_log.txt
