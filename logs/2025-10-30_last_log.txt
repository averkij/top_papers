[30.10.2025 07:13] Read previous papers.
[30.10.2025 07:13] Generating top page (month).
[30.10.2025 07:13] Writing top page (month).
[30.10.2025 08:16] Read previous papers.
[30.10.2025 08:16] Get feed.
[30.10.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2510.23473
[30.10.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2510.23538
[30.10.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2510.25741
[30.10.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2510.24592
[30.10.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2510.25065
[30.10.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2510.25726
[30.10.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2510.25590
[30.10.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2510.25772
[30.10.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18455
[30.10.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2510.25682
[30.10.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2510.19195
[30.10.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2510.24824
[30.10.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2510.24654
[30.10.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2510.25092
[30.10.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2510.24821
[30.10.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2510.24803
[30.10.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2510.25760
[30.10.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2510.22543
[30.10.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2510.25758
[30.10.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2510.25409
[30.10.2025 08:16] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[30.10.2025 08:16] No deleted papers detected.
[30.10.2025 08:16] Downloading and parsing papers (pdf, html). Total: 20.
[30.10.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2510.23473.
[30.10.2025 08:16] Extra JSON file exists (./assets/json/2510.23473.json), skip PDF parsing.
[30.10.2025 08:16] Paper image links file exists (./assets/img_data/2510.23473.json), skip HTML parsing.
[30.10.2025 08:16] Success.
[30.10.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2510.23538.
[30.10.2025 08:16] Extra JSON file exists (./assets/json/2510.23538.json), skip PDF parsing.
[30.10.2025 08:16] Paper image links file exists (./assets/img_data/2510.23538.json), skip HTML parsing.
[30.10.2025 08:16] Success.
[30.10.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2510.25741.
[30.10.2025 08:16] Extra JSON file exists (./assets/json/2510.25741.json), skip PDF parsing.
[30.10.2025 08:16] Paper image links file exists (./assets/img_data/2510.25741.json), skip HTML parsing.
[30.10.2025 08:16] Success.
[30.10.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2510.24592.
[30.10.2025 08:16] Extra JSON file exists (./assets/json/2510.24592.json), skip PDF parsing.
[30.10.2025 08:16] Paper image links file exists (./assets/img_data/2510.24592.json), skip HTML parsing.
[30.10.2025 08:16] Success.
[30.10.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2510.25065.
[30.10.2025 08:16] Extra JSON file exists (./assets/json/2510.25065.json), skip PDF parsing.
[30.10.2025 08:16] Paper image links file exists (./assets/img_data/2510.25065.json), skip HTML parsing.
[30.10.2025 08:16] Success.
[30.10.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2510.25726.
[30.10.2025 08:16] Extra JSON file exists (./assets/json/2510.25726.json), skip PDF parsing.
[30.10.2025 08:16] Paper image links file exists (./assets/img_data/2510.25726.json), skip HTML parsing.
[30.10.2025 08:16] Success.
[30.10.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2510.25590.
[30.10.2025 08:16] Extra JSON file exists (./assets/json/2510.25590.json), skip PDF parsing.
[30.10.2025 08:16] Paper image links file exists (./assets/img_data/2510.25590.json), skip HTML parsing.
[30.10.2025 08:16] Success.
[30.10.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2510.25772.
[30.10.2025 08:16] Extra JSON file exists (./assets/json/2510.25772.json), skip PDF parsing.
[30.10.2025 08:16] Paper image links file exists (./assets/img_data/2510.25772.json), skip HTML parsing.
[30.10.2025 08:16] Success.
[30.10.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2510.18455.
[30.10.2025 08:16] Downloading paper 2510.18455 from http://arxiv.org/pdf/2510.18455v1...
[30.10.2025 08:16] Failed to download and parse paper https://huggingface.co/papers/2510.18455: 'LTChar' object is not iterable
[30.10.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2510.25682.
[30.10.2025 08:16] Extra JSON file exists (./assets/json/2510.25682.json), skip PDF parsing.
[30.10.2025 08:16] Paper image links file exists (./assets/img_data/2510.25682.json), skip HTML parsing.
[30.10.2025 08:16] Success.
[30.10.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2510.19195.
[30.10.2025 08:16] Extra JSON file exists (./assets/json/2510.19195.json), skip PDF parsing.
[30.10.2025 08:16] Paper image links file exists (./assets/img_data/2510.19195.json), skip HTML parsing.
[30.10.2025 08:16] Success.
[30.10.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2510.24824.
[30.10.2025 08:16] Extra JSON file exists (./assets/json/2510.24824.json), skip PDF parsing.
[30.10.2025 08:16] Paper image links file exists (./assets/img_data/2510.24824.json), skip HTML parsing.
[30.10.2025 08:16] Success.
[30.10.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2510.24654.
[30.10.2025 08:16] Extra JSON file exists (./assets/json/2510.24654.json), skip PDF parsing.
[30.10.2025 08:16] Paper image links file exists (./assets/img_data/2510.24654.json), skip HTML parsing.
[30.10.2025 08:16] Success.
[30.10.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2510.25092.
[30.10.2025 08:16] Extra JSON file exists (./assets/json/2510.25092.json), skip PDF parsing.
[30.10.2025 08:16] Paper image links file exists (./assets/img_data/2510.25092.json), skip HTML parsing.
[30.10.2025 08:16] Success.
[30.10.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2510.24821.
[30.10.2025 08:16] Extra JSON file exists (./assets/json/2510.24821.json), skip PDF parsing.
[30.10.2025 08:16] Paper image links file exists (./assets/img_data/2510.24821.json), skip HTML parsing.
[30.10.2025 08:16] Success.
[30.10.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2510.24803.
[30.10.2025 08:16] Extra JSON file exists (./assets/json/2510.24803.json), skip PDF parsing.
[30.10.2025 08:16] Paper image links file exists (./assets/img_data/2510.24803.json), skip HTML parsing.
[30.10.2025 08:16] Success.
[30.10.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2510.25760.
[30.10.2025 08:16] Extra JSON file exists (./assets/json/2510.25760.json), skip PDF parsing.
[30.10.2025 08:16] Paper image links file exists (./assets/img_data/2510.25760.json), skip HTML parsing.
[30.10.2025 08:16] Success.
[30.10.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2510.22543.
[30.10.2025 08:16] Extra JSON file exists (./assets/json/2510.22543.json), skip PDF parsing.
[30.10.2025 08:16] Paper image links file exists (./assets/img_data/2510.22543.json), skip HTML parsing.
[30.10.2025 08:16] Success.
[30.10.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2510.25758.
[30.10.2025 08:16] Extra JSON file exists (./assets/json/2510.25758.json), skip PDF parsing.
[30.10.2025 08:16] Paper image links file exists (./assets/img_data/2510.25758.json), skip HTML parsing.
[30.10.2025 08:16] Success.
[30.10.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2510.25409.
[30.10.2025 08:16] Extra JSON file exists (./assets/json/2510.25409.json), skip PDF parsing.
[30.10.2025 08:16] Paper image links file exists (./assets/img_data/2510.25409.json), skip HTML parsing.
[30.10.2025 08:16] Success.
[30.10.2025 08:16] Enriching papers with extra data.
[30.10.2025 08:16] ********************************************************************************
[30.10.2025 08:16] Abstract 0. Video-Thinker, a multimodal large language model, autonomously reasons with videos using intrinsic grounding and captioning capabilities, achieving state-of-the-art performance on various video reasoning benchmarks.  					AI-generated summary 				 Recent advances in image reasoning methods, particul...
[30.10.2025 08:16] ********************************************************************************
[30.10.2025 08:16] Abstract 1. A unified multimodal code corpus and model, JanusCoder, generate code from text and visual inputs, outperforming commercial models in various coding tasks.  					AI-generated summary 				 The scope of neural code intelligence is rapidly expanding beyond text-based source code to encompass the rich v...
[30.10.2025 08:16] ********************************************************************************
[30.10.2025 08:16] Abstract 2. Modern LLMs are trained to "think" primarily via explicit text generation, such as chain-of-thought (CoT), which defers reasoning to post-training and under-leverages pre-training data. We present and open-source Ouro, named after the recursive Ouroboros, a family of pre-trained Looped Language Mode...
[30.10.2025 08:16] ********************************************************************************
[30.10.2025 08:16] Abstract 3. ReForm, a reflective autoformalization method, improves the semantic accuracy of formal statements generated from natural language mathematics through iterative refinement and semantic consistency evaluation.  					AI-generated summary 				 Autoformalization, which translates natural language mathem...
[30.10.2025 08:16] ********************************************************************************
[30.10.2025 08:16] Abstract 4. Reinforcement learning (RL)-based post-training has been crucial for enabling multi-step reasoning in large reasoning models (LRMs), yet current reward schemes are typically outcome-centric. We propose PM4GRPO, a reasoning-aware Group Relative Policy Optimization (GRPO) that augments standard answer...
[30.10.2025 08:16] ********************************************************************************
[30.10.2025 08:16] Abstract 5. Real-world language agents must handle complex, multi-step workflows across diverse Apps. For instance, an agent may manage emails by coordinating with calendars and file systems, or monitor a production database to detect anomalies and generate reports following an operating manual. However, existi...
[30.10.2025 08:16] ********************************************************************************
[30.10.2025 08:16] Abstract 6. Recently, instruction-based image editing (IIE) has received widespread attention. In practice, IIE often modifies only specific regions of an image, while the remaining areas largely remain unchanged. Although these two types of regions differ significantly in generation difficulty and computationa...
[30.10.2025 08:16] ********************************************************************************
[30.10.2025 08:16] Abstract 7. Visual effects (VFX) are crucial to the expressive power of digital media, yet their creation remains a major challenge for generative AI. Prevailing methods often rely on the one-LoRA-per-effect paradigm, which is resource-intensive and fundamentally incapable of generalizing to unseen effects, thu...
[30.10.2025 08:16] ********************************************************************************
[30.10.2025 08:16] Abstract 8. ChronoPlay is a framework for generating dynamic RAG benchmarks in gaming, addressing the challenges of game content updates and player focus shifts with a dual-dynamic update mechanism and dual-source synthesis engine.  					AI-generated summary 				 Retrieval Augmented Generation (RAG) systems are...
[30.10.2025 08:16] ********************************************************************************
[30.10.2025 08:16] Abstract 9. Unified vision-language models (UVLMs) must perform both understanding and generation within a single architecture, but these tasks rely on heterogeneous data and supervision, making it difficult to balance them during reinforcement learning (RL). We propose PairUni, a unified framework that reorgan...
[30.10.2025 08:16] ********************************************************************************
[30.10.2025 08:16] Abstract 10. Dream4Drive is a synthetic data generation framework that enhances downstream perception tasks in autonomous driving by decomposing videos into 3D-aware guidance maps and rendering 3D assets, leading to improved performance across various training epochs.  					AI-generated summary 				 Recent advan...
[30.10.2025 08:16] ********************************************************************************
[30.10.2025 08:16] Abstract 11. Large Language Models (LLMs) are powerful but often too slow and costly for real-world use during inference. Looped transformers save on parameters by reusing the same weights for multiple computational steps, or "loops." However, this approach has a major flaw: the loops run one after another, caus...
[30.10.2025 08:16] ********************************************************************************
[30.10.2025 08:16] Abstract 12. In this paper, we present a framework for training large language models (LLMs) as diagnostic agents with reinforcement learning, enabling them to manage multi-turn diagnostic processes, adaptively select examinations, and commit to final diagnoses. Unlike instruction-tuned models trained on static ...
[30.10.2025 08:16] ********************************************************************************
[30.10.2025 08:16] Abstract 13. Recent advances in text-only large language models (LLMs), such as DeepSeek-R1, demonstrate remarkable reasoning ability. However, these models remain fragile or entirely incapable when extended to multi-modal tasks. Existing approaches largely rely on single-form captions, which lack diversity and ...
[30.10.2025 08:16] ********************************************************************************
[30.10.2025 08:16] Abstract 14. We propose Ming-Flash-Omni, an upgraded version of Ming-Omni, built upon a sparser Mixture-of-Experts (MoE) variant of Ling-Flash-2.0 with 100 billion total parameters, of which only 6.1 billion are active per token. This architecture enables highly efficient scaling (dramatically improving computat...
[30.10.2025 08:16] ********************************************************************************
[30.10.2025 08:16] Abstract 15. Practical deployment of Multi-Agent Systems (MAS) demands strong test-time performance, motivating methods that guide inference-time search and selectively spend compute to improve quality. We present the Multi-Agent System Process Reward Model (MASPRM). It assigns per-action, per-agent values to pa...
[30.10.2025 08:16] ********************************************************************************
[30.10.2025 08:16] Abstract 16. Humans possess spatial reasoning abilities that enable them to understand spaces through multimodal observations, such as vision and sound. Large multimodal reasoning models extend these abilities by learning to perceive and reason, showing promising performance across diverse spatial tasks. However...
[30.10.2025 08:16] ********************************************************************************
[30.10.2025 08:16] Abstract 17. Flawed-Aware Policy Optimization (FAPO) improves reinforcement learning with verifiable rewards by penalizing flawed-positive rollouts, enhancing reasoning capability and training stability without increasing computational cost.  					AI-generated summary 				 Reinforcement learning with verifiable ...
[30.10.2025 08:16] ********************************************************************************
[30.10.2025 08:16] Abstract 18. Large language models (LLMs) in psychological counseling have attracted increasing attention. However, existing approaches often lack emotional understanding, adaptive strategies, and the use of therapeutic methods across multiple sessions with long-term memory, leaving them far from real clinical p...
[30.10.2025 08:16] ********************************************************************************
[30.10.2025 08:16] Abstract 19. The rapid advancement of large language models(LLMs) has intensified the need for domain and culture specific evaluation. Existing benchmarks are largely Anglocentric and domain-agnostic, limiting their applicability to India-centric contexts. To address this gap, we introduce BhashaBench V1, the fi...
[30.10.2025 08:16] Read previous papers.
[30.10.2025 08:16] Generating reviews via LLM API.
[30.10.2025 08:16] Using data from previous issue: {"categories": ["#multimodal", "#training", "#games", "#dataset", "#reasoning", "#video"], "emoji": "üé¨", "ru": {"title": "–ù–∞—É—á–∏—Ç—å LLM –¥—É–º–∞—Ç—å –Ω–∞–¥ –≤–∏–¥–µ–æ —á–µ—Ä–µ–∑ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ", "desc": "Video-Thinker ‚Äî —ç—Ç–æ multimodal LLM, –∫–æ—Ç–æ—Ä—ã–π —É–º–µ–µ—Ç —Ä–∞—Å—Å—É–∂–¥–∞—Ç—å –Ω–∞–¥ –≤–∏–¥–µ–æ, –∏—Å–ø–æ–ª—å–∑—É—è –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ grou
[30.10.2025 08:16] Using data from previous issue: {"categories": ["#architecture", "#training", "#optimization", "#dataset", "#multimodal", "#data", "#games", "#open_source"], "emoji": "üé®", "ru": {"title": "JanusCoder: –µ–¥–∏–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞ –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ JanusCoder ‚Äî —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—É—é –º—É–ª—å—Ç–∏–º–æ
[30.10.2025 08:16] Using data from previous issue: {"categories": ["#architecture", "#training", "#open_source", "#benchmark", "#dataset", "#reasoning"], "emoji": "üîÅ", "ru": {"title": "–†–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤–Ω—É—Ç—Ä–∏ –º–æ–¥–µ–ª–∏: –æ–±—É—á–µ–Ω–∏–µ LLM –¥—É–º–∞—Ç—å –≤ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ Ouro - —Å–µ–º–µ–π—Å—Ç–≤–æ Looped Language Models (LoopLM),
[30.10.2025 08:16] Using data from previous issue: {"categories": ["#training", "#benchmark", "#math", "#dataset", "#reasoning", "#optimization", "#data"], "emoji": "üîÑ", "ru": {"title": "–†–µ—Ñ–ª–µ–∫—Å–∏–≤–Ω–∞—è –∞–≤—Ç–æ—Ñ–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –º–∞—Ç–µ–º–∞—Ç–∏–∫–∏ —Å —Å–∞–º–æ–ø—Ä–æ–≤–µ—Ä–∫–æ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç ReForm ‚Äî –º–µ—Ç–æ–¥ –∞–≤—Ç–æ—Ñ–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏, –∫–æ—Ç–æ—Ä—ã–π –ø–µ—Ä–µ–≤–æ–¥–∏—Ç –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–¥–∞—á–∏ –Ω–∞ 
[30.10.2025 08:16] Using data from previous issue: {"categories": ["#rlhf", "#optimization", "#rl", "#benchmark", "#reasoning"], "emoji": "üîç", "ru": {"title": "–ù–∞–≥—Ä–∞–¥–∞ –∑–∞ –ø—Ä–æ—Ü–µ—Å—Å –º—ã—à–ª–µ–Ω–∏—è: –æ–±—É—á–µ–Ω–∏–µ LLM —Ä–∞—Å—Å—É–∂–¥–∞—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω–æ, –∞ –Ω–µ —Ç–æ–ª—å–∫–æ –¥–∞–≤–∞—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç PM4GRPO - —É–ª—É—á—à–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –±–æ–ª
[30.10.2025 08:16] Using data from previous issue: {"categories": ["#agents", "#optimization", "#benchmark", "#agi"], "emoji": "üèÖ", "ru": {"title": "–î–µ—Å—è—Ç–∏–±–æ—Ä—å–µ –¥–ª—è AI-–∞–≥–µ–Ω—Ç–æ–≤: –Ω–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –ø—Ä–æ–≤–µ—Ä–∫–∏ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ Toolathlon ‚Äî –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —è–∑—ã–∫–æ–≤—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤, —Å–ø–æ—Å–æ–±–Ω—ã—Ö –≤—ã–ø–æ–ª–Ω—è—Ç—å —Å–ª–æ–∂–Ω—ã–µ –º–Ω–æ–≥–æ—à–∞–≥
[30.10.2025 08:16] Using data from previous issue: {"categories": ["#cv", "#multimodal", "#optimization"], "emoji": "üéØ", "ru": {"title": "–£–º–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Ä–µ–≥–∏–æ–Ω–æ–≤ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç RegionE ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ç–µ–∫—Å—Ç–æ–≤—ã–º –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –ö–ª
[30.10.2025 08:16] Using data from previous issue: {"categories": ["#training", "#open_source", "#games", "#dataset", "#video"], "emoji": "‚ú®", "ru": {"title": "–ï–¥–∏–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —ç—Ñ—Ñ–µ–∫—Ç–æ–≤ –∏–∑ –ø—Ä–∏–º–µ—Ä–∞", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ VFXMaster ‚Äî –ø–µ—Ä–≤—É—é —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ —Å –≤–∏–∑—É–∞–ª—å–Ω—ã–º–∏ —ç—Ñ—Ñ–µ–∫—Ç–∞–º–∏ –Ω–∞ –æ—Å–Ω–æ–≤
[30.10.2025 08:16] Using data from previous issue: {"categories": ["#rag", "#benchmark", "#optimization", "#games"], "emoji": "üéÆ", "ru": {"title": "–î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è RAG-—Å–∏—Å—Ç–µ–º –≤ –º–∏—Ä–µ –∏–≥—Ä", "desc": "ChronoPlay ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö –±–µ–Ω—á–º–∞—Ä–∫–æ–≤ RAG-—Å–∏—Å—Ç–µ–º –≤ –∏–≥—Ä–æ–≤–æ–π –∏–Ω–¥—É—Å—Ç—Ä–∏–∏. –ö–ª—é—á–µ–≤–∞—è –ø—Ä–æ–±–ª–µ–º–∞ –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ 
[30.10.2025 08:16] Using data from previous issue: {"categories": ["#training", "#rl", "#dataset", "#rlhf", "#optimization", "#multimodal"], "emoji": "üîó", "ru": {"title": "–ü–∞—Ä–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è –±–∞–ª–∞–Ω—Å–∞ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤ vision-language –º–æ–¥–µ–ª—è—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç PairUni ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö vision-language –º–æ–¥–µ–ª–µ–π
[30.10.2025 08:16] Using data from previous issue: {"categories": ["#multimodal", "#training", "#3d", "#dataset", "#agents", "#synthetic"], "emoji": "üöó", "ru": {"title": "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å–∏—Å—Ç–µ–º –∞–≤—Ç–æ–ø–∏–ª–æ—Ç–∞ —á–µ—Ä–µ–∑ 3D-—Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥", "desc": "Dream4Drive - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç –∑–∞–¥–∞—á–∏ 
[30.10.2025 08:16] Using data from previous issue: {"categories": ["#optimization", "#architecture", "#inference"], "emoji": "üîÑ", "ru": {"title": "–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ –ø–µ—Ç–ª–∏ –¥–ª—è –±—ã—Å—Ç—Ä—ã—Ö LLM –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞", "desc": "Large Language Models —á–∞—Å—Ç–æ —Å–ª–∏—à–∫–æ–º –º–µ–¥–ª–µ–Ω–Ω—ã–µ –∏ –¥–æ—Ä–æ–≥–∏–µ –¥–ª—è –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è. Looped —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã —ç–∫–æ–Ω–æ–º—è—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã, –ø–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑—É
[30.10.2025 08:16] Using data from previous issue: {"categories": ["#reasoning", "#agents", "#rl", "#science", "#healthcare", "#benchmark"], "emoji": "ü©∫", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ AI-–¥–∏–∞–≥–Ω–æ—Å—Ç–∞ —á–µ—Ä–µ–∑ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ —Å –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–π –∫–ª–∏–Ω–∏–∫–æ–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ DiagAgent ‚Äî LLM, –æ–±—É—á–µ–Ω–Ω—É—é —Å—Ç–∞–≤–∏—Ç—å –¥–∏–∞–≥–Ω–æ–∑—ã —á–µ—Ä–µ–∑ reinforcement learning –≤ –≤
[30.10.2025 08:16] Using data from previous issue: {"categories": ["#agents", "#reasoning", "#small_models", "#multimodal", "#games", "#inference"], "emoji": "üëÅÔ∏è", "ru": {"title": "–†–∞–∑–¥–µ–ª—è–π –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ: –∞–≥–µ–Ω—Ç–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–º—É –ø–æ–Ω–∏–º–∞–Ω–∏—é", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Seeing Eye - –º–æ–¥—É–ª—å–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤
[30.10.2025 08:16] Using data from previous issue: {"categories": ["#cv", "#multimodal", "#architecture", "#audio", "#agi"], "emoji": "üé≠", "ru": {"title": "–ï–¥–∏–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å —Å —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–π MoE-–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π –¥–ª—è —Ä–µ—á–∏, —Ç–µ–∫—Å—Ç–∞ –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ Ming-Flash-Omni ‚Äî —É–ª—É—á—à–µ–Ω–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–∞ –±–∞–∑–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä
[30.10.2025 08:16] Using data from previous issue: {"categories": ["#math", "#agents", "#transfer_learning", "#reasoning", "#optimization", "#inference"], "emoji": "ü§ù", "ru": {"title": "–£–º–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω—ã–º–∏ —Å–∏—Å—Ç–µ–º–∞–º–∏ —á–µ—Ä–µ–∑ –æ—Ü–µ–Ω–∫—É –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –∫–∞–∂–¥–æ–≥–æ –∞–≥–µ–Ω—Ç–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç MASPRM ‚Äî –º–æ–¥–µ–ª—å –Ω–∞–≥—Ä–∞–¥ –¥–ª—è –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º, –∫–æ—Ç
[30.10.2025 08:16] Using data from previous issue: {"categories": ["#survey", "#agents", "#benchmark", "#reasoning", "#multimodal", "#audio", "#3d"], "emoji": "üß≠", "ru": {"title": "–ü—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ –¥–ª—è AI: –æ–±–∑–æ—Ä –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –æ–±–∑–æ—Ä —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –±–æ–ª—å—à–∏—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∫ –ø—Ä–æ—Å—Ç—Ä
[30.10.2025 08:16] Using data from previous issue: {"categories": ["#optimization", "#reasoning", "#rl", "#rlhf", "#training"], "emoji": "üéØ", "ru": {"title": "–£–º–Ω–æ–µ –Ω–∞–∫–∞–∑–∞–Ω–∏–µ –∑–∞ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã —Å –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –ª–æ–≥–∏–∫–æ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ FAPO –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö. –ü—Ä–æ–±–ª–µ–º–∞ –≤ —Ç–æ–º, —á—Ç–æ LLM
[30.10.2025 08:16] Using data from previous issue: {"categories": ["#architecture", "#open_source", "#long_context", "#alignment", "#agents", "#healthcare"], "emoji": "üß†", "ru": {"title": "TheraMind: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –≤ –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–º –∫–æ–Ω—Å—É–ª—å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º LLM", "desc": "–í —Å—Ç–∞—Ç—å–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ LLM –≤ –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–º –∫–æ–Ω—Å—É–ª—å—Ç–∏—Ä
[30.10.2025 08:16] Using data from previous issue: {"categories": ["#multilingual", "#low_resource", "#open_source", "#benchmark", "#dataset", "#science"], "emoji": "üáÆüá≥", "ru": {"title": "BhashaBench V1: –û—Ü–µ–Ω–∫–∞ LLM –≤ –∏–Ω–¥–∏–π—Å–∫–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ", "desc": "–í —Å—Ç–∞—Ç—å–µ –æ–±—Å—É–∂–¥–∞–µ—Ç—Å—è —Å–æ–∑–¥–∞–Ω–∏–µ BhashaBench V1, –ø–µ—Ä–≤–æ–≥–æ –¥–æ–º–µ–Ω–Ω–æ-—Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–æ–≥–æ, –º–Ω–æ–≥–æ—Ü–µ–ª–µ–≤–æ–≥–æ, –¥–≤—É—è–∑—ã—á–Ω–æ–≥
[30.10.2025 08:16] Renaming data file.
[30.10.2025 08:16] Renaming previous data. hf_papers.json to ./d/2025-10-30.json
[30.10.2025 08:16] Saving new data file.
[30.10.2025 08:16] Generating page.
[30.10.2025 08:16] Renaming previous page.
[30.10.2025 08:16] Renaming previous data. index.html to ./d/2025-10-30.html
[30.10.2025 08:16] Writing result.
[30.10.2025 08:16] Renaming log file.
[30.10.2025 08:16] Renaming previous data. log.txt to ./logs/2025-10-30_last_log.txt
