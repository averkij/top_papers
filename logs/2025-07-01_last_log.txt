[01.07.2025 06:19] Read previous papers.
[01.07.2025 06:19] Generating top page (month).
[01.07.2025 06:19] Writing top page (month).
[01.07.2025 07:13] Read previous papers.
[01.07.2025 07:13] Get feed.
[01.07.2025 07:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.23044
[01.07.2025 07:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.23858
[01.07.2025 07:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.24123
[01.07.2025 07:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.24119
[01.07.2025 07:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.23542
[01.07.2025 07:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.17930
[01.07.2025 07:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.17417
[01.07.2025 07:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.16500
[01.07.2025 07:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.23394
[01.07.2025 07:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.23219
[01.07.2025 07:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.22694
[01.07.2025 07:14] Extract page data from URL. URL: https://huggingface.co/papers/2506.23135
[01.07.2025 07:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.21448
[01.07.2025 07:14] Extract page data from URL. URL: https://huggingface.co/papers/2506.22992
[01.07.2025 07:14] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[01.07.2025 07:14] No deleted papers detected.
[01.07.2025 07:14] Downloading and parsing papers (pdf, html). Total: 14.
[01.07.2025 07:14] Downloading and parsing paper https://huggingface.co/papers/2506.23044.
[01.07.2025 07:14] Extra JSON file exists (./assets/json/2506.23044.json), skip PDF parsing.
[01.07.2025 07:14] Paper image links file exists (./assets/img_data/2506.23044.json), skip HTML parsing.
[01.07.2025 07:14] Success.
[01.07.2025 07:14] Downloading and parsing paper https://huggingface.co/papers/2506.23858.
[01.07.2025 07:14] Extra JSON file exists (./assets/json/2506.23858.json), skip PDF parsing.
[01.07.2025 07:14] Paper image links file exists (./assets/img_data/2506.23858.json), skip HTML parsing.
[01.07.2025 07:14] Success.
[01.07.2025 07:14] Downloading and parsing paper https://huggingface.co/papers/2506.24123.
[01.07.2025 07:14] Extra JSON file exists (./assets/json/2506.24123.json), skip PDF parsing.
[01.07.2025 07:14] Paper image links file exists (./assets/img_data/2506.24123.json), skip HTML parsing.
[01.07.2025 07:14] Success.
[01.07.2025 07:14] Downloading and parsing paper https://huggingface.co/papers/2506.24119.
[01.07.2025 07:14] Extra JSON file exists (./assets/json/2506.24119.json), skip PDF parsing.
[01.07.2025 07:14] Paper image links file exists (./assets/img_data/2506.24119.json), skip HTML parsing.
[01.07.2025 07:14] Success.
[01.07.2025 07:14] Downloading and parsing paper https://huggingface.co/papers/2506.23542.
[01.07.2025 07:14] Extra JSON file exists (./assets/json/2506.23542.json), skip PDF parsing.
[01.07.2025 07:14] Paper image links file exists (./assets/img_data/2506.23542.json), skip HTML parsing.
[01.07.2025 07:14] Success.
[01.07.2025 07:14] Downloading and parsing paper https://huggingface.co/papers/2506.17930.
[01.07.2025 07:14] Extra JSON file exists (./assets/json/2506.17930.json), skip PDF parsing.
[01.07.2025 07:14] Paper image links file exists (./assets/img_data/2506.17930.json), skip HTML parsing.
[01.07.2025 07:14] Success.
[01.07.2025 07:14] Downloading and parsing paper https://huggingface.co/papers/2506.17417.
[01.07.2025 07:14] Extra JSON file exists (./assets/json/2506.17417.json), skip PDF parsing.
[01.07.2025 07:14] Paper image links file exists (./assets/img_data/2506.17417.json), skip HTML parsing.
[01.07.2025 07:14] Success.
[01.07.2025 07:14] Downloading and parsing paper https://huggingface.co/papers/2506.16500.
[01.07.2025 07:14] Extra JSON file exists (./assets/json/2506.16500.json), skip PDF parsing.
[01.07.2025 07:14] Paper image links file exists (./assets/img_data/2506.16500.json), skip HTML parsing.
[01.07.2025 07:14] Success.
[01.07.2025 07:14] Downloading and parsing paper https://huggingface.co/papers/2506.23394.
[01.07.2025 07:14] Extra JSON file exists (./assets/json/2506.23394.json), skip PDF parsing.
[01.07.2025 07:14] Paper image links file exists (./assets/img_data/2506.23394.json), skip HTML parsing.
[01.07.2025 07:14] Success.
[01.07.2025 07:14] Downloading and parsing paper https://huggingface.co/papers/2506.23219.
[01.07.2025 07:14] Extra JSON file exists (./assets/json/2506.23219.json), skip PDF parsing.
[01.07.2025 07:14] Paper image links file exists (./assets/img_data/2506.23219.json), skip HTML parsing.
[01.07.2025 07:14] Success.
[01.07.2025 07:14] Downloading and parsing paper https://huggingface.co/papers/2506.22694.
[01.07.2025 07:14] Extra JSON file exists (./assets/json/2506.22694.json), skip PDF parsing.
[01.07.2025 07:14] Paper image links file exists (./assets/img_data/2506.22694.json), skip HTML parsing.
[01.07.2025 07:14] Success.
[01.07.2025 07:14] Downloading and parsing paper https://huggingface.co/papers/2506.23135.
[01.07.2025 07:14] Downloading paper 2506.23135 from http://arxiv.org/pdf/2506.23135v1...
[01.07.2025 07:14] Extracting affiliations from text.
[01.07.2025 07:14] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 9 2 ] . [ 1 5 3 1 3 2 . 6 0 5 2 : r RoboScape: Physics-informed Embodied World Model Yu Shang1 Xin Zhang2 Yinzhou Tang1 Lei Jin1 Chen Gao1 Wei Wu2 Yong Li1 1Tsinghua University 2Manifold AI "
[01.07.2025 07:14] Response: ```python
["Tsinghua University", "Manifold AI"]
```
[01.07.2025 07:14] Deleting PDF ./assets/pdf/2506.23135.pdf.
[01.07.2025 07:14] Success.
[01.07.2025 07:14] Downloading and parsing paper https://huggingface.co/papers/2506.21448.
[01.07.2025 07:14] Extra JSON file exists (./assets/json/2506.21448.json), skip PDF parsing.
[01.07.2025 07:14] Paper image links file exists (./assets/img_data/2506.21448.json), skip HTML parsing.
[01.07.2025 07:14] Success.
[01.07.2025 07:14] Downloading and parsing paper https://huggingface.co/papers/2506.22992.
[01.07.2025 07:14] Downloading paper 2506.22992 from http://arxiv.org/pdf/2506.22992v1...
[01.07.2025 07:14] Extracting affiliations from text.
[01.07.2025 07:14] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 8 2 ] . [ 1 2 9 9 2 2 . 6 0 5 2 : r MARBLE: Hard Benchmark for Multimodal Spatial Reasoning and Planning Yulun Jiang1,2 Yekun Chai2 Maria Brbić1 Michael Moor2 1 EPFL 2 ETH Zurich https://marble-benchmark.github.io July 1, 2025 Abstract The ability to process information from multiple modalities and to reason through it step-by-step remains critical challenge in advancing artificial intelligence. However, existing reasoning benchmarks focus on text-only reasoning, or employ multimodal questions that can be answered by directly retrieving information from non-text modality. Thus, complex reasoning remains poorly understood in multimodal domains. Here, we present MARBLE, challenging multimodal reasoning benchmark that is designed to scrutinize multimodal language models (MLLMs) in their ability to carefully reason step-by-step through complex multimodal problems and environments. MARBLE is composed of two highly challenging tasks, M-Portal and M-Cube, that require the crafting and understanding of multistep plans under spatial, visual, and physical constraints. We find that current MLLMs perform poorly on MARBLEall the 12 advanced models obtain near-random performance on M-Portal and 0% accuracy on M-Cube. Only in simplified subtasks some models outperform the random baseline, indicating that complex reasoning is still challenge for existing MLLMs. Moreover, we show that perception remains bottleneck, where MLLMs occasionally fail to extract information from the visual inputs. By shedding light on the limitations of MLLMs, we hope that MARBLE will spur the development of the next generation of models with the ability to reason and plan across many, multimodal reasoning steps. Human reasoning is inherently multimodal and sequentialintegrating modalities such as language or vision as context to draw conclusions through structured, step-by-step thought. While LLMs have made significant strides in step-by-step reasoning [27, 11, 8, 18], the multimodal reasoning abi"
[01.07.2025 07:14] Response: ```python
["EPFL", "ETH Zurich"]
```
[01.07.2025 07:14] Deleting PDF ./assets/pdf/2506.22992.pdf.
[01.07.2025 07:14] Success.
[01.07.2025 07:14] Enriching papers with extra data.
[01.07.2025 07:14] ********************************************************************************
[01.07.2025 07:14] Abstract 0. Ovis-U1, a 3-billion-parameter model, combines multimodal understanding, text-to-image generation, and image editing, achieving state-of-the-art performance in various benchmarks.  					AI-generated summary 				 In this report, we introduce Ovis-U1, a 3-billion-parameter unified model that integrate...
[01.07.2025 07:14] ********************************************************************************
[01.07.2025 07:14] Abstract 1. VMoBA, a novel sparse attention mechanism for Video Diffusion Models (VDMs), accelerates training and inference by addressing the quadratic complexity of full attention mechanisms while maintaining or improving video generation quality.  					AI-generated summary 				 The quadratic complexity of ful...
[01.07.2025 07:14] ********************************************************************************
[01.07.2025 07:14] Abstract 2. Calligrapher uses a diffusion-based framework with self-distillation and localized style injection to generate high-quality, stylistically consistent digital typography.  					AI-generated summary 				 We introduce Calligrapher, a novel diffusion-based framework that innovatively integrates advanced...
[01.07.2025 07:14] ********************************************************************************
[01.07.2025 07:14] Abstract 3. Self-play in zero-sum games using SPIRAL enhances reasoning capabilities in language models through self-improvement and transfer learning.  					AI-generated summary 				 Recent advances in reinforcement learning have shown that language models can develop sophisticated reasoning through training o...
[01.07.2025 07:14] ********************************************************************************
[01.07.2025 07:14] Abstract 4. A novel ToF depth denoising network uses motion-invariant graph fusion and adaptive filters to improve temporal stability and spatial sharpness, achieving state-of-the-art performance.  					AI-generated summary 				 Depth images captured by Time-of-Flight (ToF) sensors are prone to noise, requiring...
[01.07.2025 07:14] ********************************************************************************
[01.07.2025 07:14] Abstract 5. A novel prompt design paradigm, PromptQuine, shows that pruning random demonstrations into "gibberish" can improve large language model performance across various tasks, surpassing state-of-the-art methods.  					AI-generated summary 				 We propose a novel prompt design paradigm that challenges con...
[01.07.2025 07:14] ********************************************************************************
[01.07.2025 07:14] Abstract 6. Inference-time techniques like decoding-time scaling and self-refinement enhance reasoning in vision-language models, with generation-based methods providing greater improvement than verification-based methods, despite RL-trained models not showing self-correction benefits.  					AI-generated summar...
[01.07.2025 07:14] ********************************************************************************
[01.07.2025 07:14] Abstract 7. SparseLoRA reduces computational cost and speeds up fine-tuning of LLMs by dynamically selecting a sparse subset of weights for loss and gradient computation.  					AI-generated summary 				 Fine-tuning LLMs is both computationally and memory-intensive. While parameter-efficient fine-tuning methods,...
[01.07.2025 07:14] ********************************************************************************
[01.07.2025 07:14] Abstract 8. A methodology is presented to adapt language models for robust tool use across languages, specifically improving function-calling accuracy in Bulgarian.  					AI-generated summary 				 External tool integration through function-calling is essential for practical language model applications, yet most...
[01.07.2025 07:14] ********************************************************************************
[01.07.2025 07:14] Abstract 9. UrbanLLaVA, a multi-modal large language model, effectively processes urban datasets for various tasks, outperforming existing models in both single-modal and complex cross-modal scenarios.  					AI-generated summary 				 Urban research involves a wide range of scenarios and tasks that require the u...
[01.07.2025 07:14] ********************************************************************************
[01.07.2025 07:14] Abstract 10. A technique called VocabTrim improves drafter-based speculative decoding by reducing the vocabulary of the drafter language model, thus decreasing drafting latency in memory-bound environments.  					AI-generated summary 				 In this paper, we introduce a simple training-free technique to improve th...
[01.07.2025 07:14] ********************************************************************************
[01.07.2025 07:14] Abstract 11. RoboScape is a unified physics-informed world model that enhances visual fidelity and physical plausibility in robotic video generation by integrating temporal depth prediction and keypoint dynamics learning.  					AI-generated summary 				 World models have become indispensable tools for embodied i...
[01.07.2025 07:14] ********************************************************************************
[01.07.2025 07:14] Abstract 12. ThinkSound, a novel framework, uses Chain-of-Thought reasoning with a multimodal large language model to generate high-quality audio from videos, achieving state-of-the-art results in various benchmarks.  					AI-generated summary 				 While end-to-end video-to-audio generation has greatly improved,...
[01.07.2025 07:14] ********************************************************************************
[01.07.2025 07:14] Abstract 13. MARBLE is a challenging multimodal reasoning benchmark that highlights the limitations of existing multimodal language models in step-by-step reasoning and plan crafting under spatial and visual constraints.  					AI-generated summary 				 The ability to process information from multiple modalities ...
[01.07.2025 07:14] Read previous papers.
[01.07.2025 07:14] Generating reviews via LLM API.
[01.07.2025 07:14] Using data from previous issue: {"categories": ["#cv", "#diffusion", "#architecture", "#multimodal", "#benchmark"], "emoji": "🦾", "ru": {"title": "Ovis-U1: Новый уровень мультимодального ИИ", "desc": "Ovis-U1 - это мультимодальная языковая модель с 3 миллиардами параметров, объединяющая понимание разных типов данных, генерацию изо
[01.07.2025 07:14] Using data from previous issue: {"categories": ["#optimization", "#architecture", "#video", "#diffusion", "#training"], "emoji": "🎬", "ru": {"title": "VMoBA: Эффективное внимание для генерации видео", "desc": "VMoBA - это новый механизм разреженного внимания для видео-диффузионных моделей (VDM). Он решает проблему квадратичной сло
[01.07.2025 07:14] Using data from previous issue: {"categories": ["#architecture", "#benchmark", "#diffusion", "#dataset"], "emoji": "✒️", "ru": {"title": "Искусственный каллиграф: новый уровень цифровой типографики", "desc": "Статья представляет Calligrapher - новую систему для генерации цифровой типографики на основе диффузионных моделей. Ключевы
[01.07.2025 07:14] Using data from previous issue: {"categories": ["#rl", "#reasoning", "#games", "#transfer_learning", "#rlhf", "#training"], "emoji": "🧠", "ru": {"title": "Самосовершенствование ИИ через игры с нулевой суммой", "desc": "Статья представляет SPIRAL - фреймворк для самообучения языковых моделей через игру в игры с нулевой суммой проти
[01.07.2025 07:14] Using data from previous issue: {"categories": ["#cv", "#optimization", "#dataset", "#graphs", "#synthetic", "#open_source"], "emoji": "🔍", "ru": {"title": "Революционное шумоподавление ToF: стабильность и четкость через графовое слияние", "desc": "Предложена новая нейронная сеть для шумоподавления в датчиках глубины Time-of-Fligh
[01.07.2025 07:14] Using data from previous issue: {"categories": ["#optimization", "#alignment", "#multimodal", "#training"], "emoji": "✂️", "ru": {"title": "Бессмыслица в промптах улучшает работу языковых моделей", "desc": "В статье представлена новая парадигма проектирования промптов под названием PromptQuine. Исследование показывает, что обрезан
[01.07.2025 07:14] Using data from previous issue: {"categories": ["#rl", "#reasoning", "#inference", "#multimodal"], "emoji": "🧠", "ru": {"title": "Улучшение рассуждений в визуально-языковых моделях: победа генерации над верификацией", "desc": "Исследование показывает, что методы вывода, такие как масштабирование во время декодирования и самоуточне
[01.07.2025 07:14] Using data from previous issue: {"categories": ["#training", "#optimization"], "emoji": "🚀", "ru": {"title": "SparseLoRA: Ускорение тонкой настройки LLM без потери качества", "desc": "SparseLoRA - это новый метод ускорения тонкой настройки больших языковых моделей (LLM). Он использует контекстную разреженность для динамического вы
[01.07.2025 07:14] Using data from previous issue: {"categories": ["#multilingual", "#dataset", "#low_resource", "#benchmark", "#open_source", "#training"], "emoji": "🔧", "ru": {"title": "Многоязычное расширение возможностей ИИ: точные вызовы функций на любом языке", "desc": "Представлена методология адаптации языковых моделей для надежного использо
[01.07.2025 07:14] Using data from previous issue: {"categories": ["#dataset", "#science", "#training", "#multimodal", "#benchmark", "#open_source"], "emoji": "🏙️", "ru": {"title": "UrbanLLaVA: Революция в анализе городских данных с помощью мультимодального ИИ", "desc": "UrbanLLaVA - это мультимодальная большая языковая модель, разработанная для обр
[01.07.2025 07:14] Using data from previous issue: {"categories": ["#inference", "#training", "#optimization", "#small_models"], "emoji": "✂️", "ru": {"title": "Ускорение генерации текста путем обрезки словаря", "desc": "Эта статья представляет технику VocabTrim, которая улучшает спекулятивное декодирование на основе драфтера путем уменьшения словар
[01.07.2025 07:14] Querying the API.
[01.07.2025 07:14] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

RoboScape is a unified physics-informed world model that enhances visual fidelity and physical plausibility in robotic video generation by integrating temporal depth prediction and keypoint dynamics learning.  					AI-generated summary 				 World models have become indispensable tools for embodied intelligence, serving as powerful simulators capable of generating realistic robotic videos while addressing critical data scarcity challenges. However, current embodied world models exhibit limited physical awareness, particularly in modeling 3D geometry and motion dynamics, resulting in unrealistic video generation for contact-rich robotic scenarios. In this paper, we present RoboScape, a unified physics-informed world model that jointly learns RGB video generation and physics knowledge within an integrated framework. We introduce two key physics-informed joint training tasks: temporal depth prediction that enhances 3D geometric consistency in video rendering, and keypoint dynamics learning that implicitly encodes physical properties (e.g., object shape and material characteristics) while improving complex motion modeling. Extensive experiments demonstrate that RoboScape generates videos with superior visual fidelity and physical plausibility across diverse robotic scenarios. We further validate its practical utility through downstream applications including robotic policy training with generated data and policy evaluation. Our work provides new insights for building efficient physics-informed world models to advance embodied intelligence research. The code is available at: https://github.com/tsinghua-fib-lab/RoboScape.
[01.07.2025 07:14] Response: {
  "desc": "RoboScape - это унифицированная физически-информированная мировая модель для генерации видео роботов. Она объединяет предсказание временной глубины и обучение динамике ключевых точек для улучшения визуальной достоверности и физической правдоподобности. Модель совместно обучается генерации RGB-видео и физическим знаниям в интегрированной структуре. Эксперименты показывают превосходное качество генерируемых видео и практическую пользу для обучения и оценки политик роботов.",
  "emoji": "🤖",
  "title": "RoboScape: физически достоверная генерация видео для продвижения воплощенного ИИ"
}
[01.07.2025 07:14] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"RoboScape is a unified physics-informed world model that enhances visual fidelity and physical plausibility in robotic video generation by integrating temporal depth prediction and keypoint dynamics learning.  					AI-generated summary 				 World models have become indispensable tools for embodied intelligence, serving as powerful simulators capable of generating realistic robotic videos while addressing critical data scarcity challenges. However, current embodied world models exhibit limited physical awareness, particularly in modeling 3D geometry and motion dynamics, resulting in unrealistic video generation for contact-rich robotic scenarios. In this paper, we present RoboScape, a unified physics-informed world model that jointly learns RGB video generation and physics knowledge within an integrated framework. We introduce two key physics-informed joint training tasks: temporal depth prediction that enhances 3D geometric consistency in video rendering, and keypoint dynamics learning that implicitly encodes physical properties (e.g., object shape and material characteristics) while improving complex motion modeling. Extensive experiments demonstrate that RoboScape generates videos with superior visual fidelity and physical plausibility across diverse robotic scenarios. We further validate its practical utility through downstream applications including robotic policy training with generated data and policy evaluation. Our work provides new insights for building efficient physics-informed world models to advance embodied intelligence research. The code is available at: https://github.com/tsinghua-fib-lab/RoboScape."

[01.07.2025 07:14] Response: ```python
['3D', 'VIDEO', 'ROBOTICS']
```
[01.07.2025 07:14] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"RoboScape is a unified physics-informed world model that enhances visual fidelity and physical plausibility in robotic video generation by integrating temporal depth prediction and keypoint dynamics learning.  					AI-generated summary 				 World models have become indispensable tools for embodied intelligence, serving as powerful simulators capable of generating realistic robotic videos while addressing critical data scarcity challenges. However, current embodied world models exhibit limited physical awareness, particularly in modeling 3D geometry and motion dynamics, resulting in unrealistic video generation for contact-rich robotic scenarios. In this paper, we present RoboScape, a unified physics-informed world model that jointly learns RGB video generation and physics knowledge within an integrated framework. We introduce two key physics-informed joint training tasks: temporal depth prediction that enhances 3D geometric consistency in video rendering, and keypoint dynamics learning that implicitly encodes physical properties (e.g., object shape and material characteristics) while improving complex motion modeling. Extensive experiments demonstrate that RoboScape generates videos with superior visual fidelity and physical plausibility across diverse robotic scenarios. We further validate its practical utility through downstream applications including robotic policy training with generated data and policy evaluation. Our work provides new insights for building efficient physics-informed world models to advance embodied intelligence research. The code is available at: https://github.com/tsinghua-fib-lab/RoboScape."

[01.07.2025 07:14] Response: ```python
["GAMES", "OPTIMIZATION", "SCIENCE", "OPEN_SOURCE"]
```
[01.07.2025 07:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"RoboScape is a new model that improves how robots generate videos by making them look more realistic and physically accurate. It combines two important tasks: predicting depth over time to ensure 3D shapes are consistent, and learning how key points move to capture the physical properties of objects. This model helps robots create videos that are not only visually appealing but also reflect real-world physics, especially in complex interactions. The research shows that RoboScape can be used effectively for training robotic policies and evaluating their performance using the generated videos.","title":"RoboScape: Realistic Robotic Video Generation with Physics Awareness"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='RoboScape is a new model that improves how robots generate videos by making them look more realistic and physically accurate. It combines two important tasks: predicting depth over time to ensure 3D shapes are consistent, and learning how key points move to capture the physical properties of objects. This model helps robots create videos that are not only visually appealing but also reflect real-world physics, especially in complex interactions. The research shows that RoboScape can be used effectively for training robotic policies and evaluating their performance using the generated videos.', title='RoboScape: Realistic Robotic Video Generation with Physics Awareness'))
[01.07.2025 07:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"RoboScape 是一个统一的物理信息世界模型，通过整合时间深度预测和关键点动态学习，提升了机器人视频生成的视觉真实感和物理合理性。当前的世界模型在建模三维几何和运动动态方面存在局限，导致生成的视频在接触丰富的机器人场景中不够真实。本文提出的 RoboScape 通过联合学习 RGB 视频生成和物理知识，采用了两个关键的物理信息联合训练任务，增强了视频渲染中的三维几何一致性。实验结果表明，RoboScape 在多种机器人场景中生成的视频具有更高的视觉真实感和物理合理性，并在下游应用中验证了其实用性。","title":"RoboScape：提升机器人视频生成的物理真实感"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='RoboScape 是一个统一的物理信息世界模型，通过整合时间深度预测和关键点动态学习，提升了机器人视频生成的视觉真实感和物理合理性。当前的世界模型在建模三维几何和运动动态方面存在局限，导致生成的视频在接触丰富的机器人场景中不够真实。本文提出的 RoboScape 通过联合学习 RGB 视频生成和物理知识，采用了两个关键的物理信息联合训练任务，增强了视频渲染中的三维几何一致性。实验结果表明，RoboScape 在多种机器人场景中生成的视频具有更高的视觉真实感和物理合理性，并在下游应用中验证了其实用性。', title='RoboScape：提升机器人视频生成的物理真实感'))
[01.07.2025 07:14] Using data from previous issue: {"categories": ["#dataset", "#games", "#multimodal", "#benchmark", "#reasoning", "#audio"], "emoji": "🎬", "ru": {"title": "Думай как звукорежиссер: ИИ создает аудио для видео с помощью рассуждений", "desc": "ThinkSound - это новая система для генерации высококачественного аудио из видео с использова
[01.07.2025 07:14] Querying the API.
[01.07.2025 07:14] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

MARBLE is a challenging multimodal reasoning benchmark that highlights the limitations of existing multimodal language models in step-by-step reasoning and plan crafting under spatial and visual constraints.  					AI-generated summary 				 The ability to process information from multiple modalities and to reason through it step-by-step remains a critical challenge in advancing artificial intelligence. However, existing reasoning benchmarks focus on text-only reasoning, or employ multimodal questions that can be answered by directly retrieving information from a non-text modality. Thus, complex reasoning remains poorly understood in multimodal domains. Here, we present MARBLE, a challenging multimodal reasoning benchmark that is designed to scrutinize multimodal language models (MLLMs) in their ability to carefully reason step-by-step through complex multimodal problems and environments. MARBLE is composed of two highly challenging tasks, M-Portal and M-Cube, that require the crafting and understanding of multistep plans under spatial, visual, and physical constraints. We find that current MLLMs perform poorly on MARBLE -- all the 12 advanced models obtain near-random performance on M-Portal and 0% accuracy on M-Cube. Only in simplified subtasks some models outperform the random baseline, indicating that complex reasoning is still a challenge for existing MLLMs. Moreover, we show that perception remains a bottleneck, where MLLMs occasionally fail to extract information from the visual inputs. By shedding a light on the limitations of MLLMs, we hope that MARBLE will spur the development of the next generation of models with the ability to reason and plan across many, multimodal reasoning steps.
[01.07.2025 07:14] Response: {
  "desc": "MARBLE - это сложный мультимодальный бенчмарк для оценки рассуждений, который выявляет ограничения существующих мультимодальных языковых моделей в пошаговом рассуждении и создании планов с учетом пространственных и визуальных ограничений. Бенчмарк состоит из двух задач: M-Portal и M-Cube, требующих составления и понимания многоэтапных планов. Текущие мультимодальные языковые модели показывают низкую производительность на MARBLE, демонстрируя случайные результаты на M-Portal и 0% точности на M-Cube. MARBLE призван стимулировать разработку нового поколения моделей, способных рассуждать и планировать в мультимодальных задачах.",
  "emoji": "🧠",
  "title": "MARBLE: Вызов мультимодальным языковым моделям в сложных рассуждениях"
}
[01.07.2025 07:14] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"MARBLE is a challenging multimodal reasoning benchmark that highlights the limitations of existing multimodal language models in step-by-step reasoning and plan crafting under spatial and visual constraints.  					AI-generated summary 				 The ability to process information from multiple modalities and to reason through it step-by-step remains a critical challenge in advancing artificial intelligence. However, existing reasoning benchmarks focus on text-only reasoning, or employ multimodal questions that can be answered by directly retrieving information from a non-text modality. Thus, complex reasoning remains poorly understood in multimodal domains. Here, we present MARBLE, a challenging multimodal reasoning benchmark that is designed to scrutinize multimodal language models (MLLMs) in their ability to carefully reason step-by-step through complex multimodal problems and environments. MARBLE is composed of two highly challenging tasks, M-Portal and M-Cube, that require the crafting and understanding of multistep plans under spatial, visual, and physical constraints. We find that current MLLMs perform poorly on MARBLE -- all the 12 advanced models obtain near-random performance on M-Portal and 0% accuracy on M-Cube. Only in simplified subtasks some models outperform the random baseline, indicating that complex reasoning is still a challenge for existing MLLMs. Moreover, we show that perception remains a bottleneck, where MLLMs occasionally fail to extract information from the visual inputs. By shedding a light on the limitations of MLLMs, we hope that MARBLE will spur the development of the next generation of models with the ability to reason and plan across many, multimodal reasoning steps."

[01.07.2025 07:14] Response: ```python
['BENCHMARK', 'MULTIMODAL']
```
[01.07.2025 07:14] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"MARBLE is a challenging multimodal reasoning benchmark that highlights the limitations of existing multimodal language models in step-by-step reasoning and plan crafting under spatial and visual constraints.  					AI-generated summary 				 The ability to process information from multiple modalities and to reason through it step-by-step remains a critical challenge in advancing artificial intelligence. However, existing reasoning benchmarks focus on text-only reasoning, or employ multimodal questions that can be answered by directly retrieving information from a non-text modality. Thus, complex reasoning remains poorly understood in multimodal domains. Here, we present MARBLE, a challenging multimodal reasoning benchmark that is designed to scrutinize multimodal language models (MLLMs) in their ability to carefully reason step-by-step through complex multimodal problems and environments. MARBLE is composed of two highly challenging tasks, M-Portal and M-Cube, that require the crafting and understanding of multistep plans under spatial, visual, and physical constraints. We find that current MLLMs perform poorly on MARBLE -- all the 12 advanced models obtain near-random performance on M-Portal and 0% accuracy on M-Cube. Only in simplified subtasks some models outperform the random baseline, indicating that complex reasoning is still a challenge for existing MLLMs. Moreover, we show that perception remains a bottleneck, where MLLMs occasionally fail to extract information from the visual inputs. By shedding a light on the limitations of MLLMs, we hope that MARBLE will spur the development of the next generation of models with the ability to reason and plan across many, multimodal reasoning steps."

[01.07.2025 07:14] Response: ```python
["REASONING"]
```
[01.07.2025 07:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"MARBLE is a new benchmark designed to test multimodal language models (MLLMs) on their ability to perform complex reasoning tasks that involve both visual and spatial elements. It includes two main tasks, M-Portal and M-Cube, which require models to create and understand multistep plans while adhering to various constraints. Current MLLMs struggle significantly with these tasks, showing near-random performance on M-Portal and failing completely on M-Cube, highlighting their limitations in complex reasoning. By identifying these challenges, MARBLE aims to encourage the development of more advanced models capable of effective multimodal reasoning and planning.","title":"MARBLE: Unveiling the Limits of Multimodal Reasoning in AI"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='MARBLE is a new benchmark designed to test multimodal language models (MLLMs) on their ability to perform complex reasoning tasks that involve both visual and spatial elements. It includes two main tasks, M-Portal and M-Cube, which require models to create and understand multistep plans while adhering to various constraints. Current MLLMs struggle significantly with these tasks, showing near-random performance on M-Portal and failing completely on M-Cube, highlighting their limitations in complex reasoning. By identifying these challenges, MARBLE aims to encourage the development of more advanced models capable of effective multimodal reasoning and planning.', title='MARBLE: Unveiling the Limits of Multimodal Reasoning in AI'))
[01.07.2025 07:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"MARBLE是一个具有挑战性的多模态推理基准，旨在揭示现有多模态语言模型在逐步推理和计划制定方面的局限性。该基准包含两个高度挑战性的任务，M-Portal和M-Cube，要求在空间、视觉和物理约束下进行多步骤计划的制定和理解。研究发现，当前的多模态语言模型在MARBLE上的表现不佳，所有12个先进模型在M-Portal上的表现接近随机，而在M-Cube上的准确率为0%。通过揭示多模态语言模型的局限性，MARBLE希望推动下一代模型的发展，使其能够在多模态推理步骤中进行有效的推理和计划。","title":"MARBLE：揭示多模态推理的挑战"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='MARBLE是一个具有挑战性的多模态推理基准，旨在揭示现有多模态语言模型在逐步推理和计划制定方面的局限性。该基准包含两个高度挑战性的任务，M-Portal和M-Cube，要求在空间、视觉和物理约束下进行多步骤计划的制定和理解。研究发现，当前的多模态语言模型在MARBLE上的表现不佳，所有12个先进模型在M-Portal上的表现接近随机，而在M-Cube上的准确率为0%。通过揭示多模态语言模型的局限性，MARBLE希望推动下一代模型的发展，使其能够在多模态推理步骤中进行有效的推理和计划。', title='MARBLE：揭示多模态推理的挑战'))
[01.07.2025 07:14] Renaming data file.
[01.07.2025 07:14] Renaming previous data. hf_papers.json to ./d/2025-07-01.json
[01.07.2025 07:14] Saving new data file.
[01.07.2025 07:14] Generating page.
[01.07.2025 07:14] Renaming previous page.
[01.07.2025 07:14] Renaming previous data. index.html to ./d/2025-07-01.html
[01.07.2025 07:14] Writing result.
[01.07.2025 07:14] Renaming log file.
[01.07.2025 07:14] Renaming previous data. log.txt to ./logs/2025-07-01_last_log.txt
