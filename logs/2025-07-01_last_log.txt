[01.07.2025 11:11] Read previous papers.
[01.07.2025 11:11] Generating top page (month).
[01.07.2025 11:11] Writing top page (month).
[01.07.2025 12:22] Read previous papers.
[01.07.2025 12:22] Get feed.
[01.07.2025 12:22] Get page data from previous paper. URL: https://huggingface.co/papers/2506.23044
[01.07.2025 12:22] Get page data from previous paper. URL: https://huggingface.co/papers/2506.23858
[01.07.2025 12:22] Get page data from previous paper. URL: https://huggingface.co/papers/2506.24123
[01.07.2025 12:22] Get page data from previous paper. URL: https://huggingface.co/papers/2506.22832
[01.07.2025 12:22] Get page data from previous paper. URL: https://huggingface.co/papers/2506.24119
[01.07.2025 12:22] Get page data from previous paper. URL: https://huggingface.co/papers/2506.17930
[01.07.2025 12:22] Get page data from previous paper. URL: https://huggingface.co/papers/2506.23542
[01.07.2025 12:22] Get page data from previous paper. URL: https://huggingface.co/papers/2506.17417
[01.07.2025 12:22] Get page data from previous paper. URL: https://huggingface.co/papers/2506.23151
[01.07.2025 12:22] Get page data from previous paper. URL: https://huggingface.co/papers/2506.16500
[01.07.2025 12:22] Get page data from previous paper. URL: https://huggingface.co/papers/2506.22992
[01.07.2025 12:22] Get page data from previous paper. URL: https://huggingface.co/papers/2506.23394
[01.07.2025 12:22] Get page data from previous paper. URL: https://huggingface.co/papers/2506.23219
[01.07.2025 12:22] Get page data from previous paper. URL: https://huggingface.co/papers/2506.22694
[01.07.2025 12:22] Get page data from previous paper. URL: https://huggingface.co/papers/2506.23135
[01.07.2025 12:22] Get page data from previous paper. URL: https://huggingface.co/papers/2506.22753
[01.07.2025 12:22] Get page data from previous paper. URL: https://huggingface.co/papers/2506.21448
[01.07.2025 12:22] Get page data from previous paper. URL: https://huggingface.co/papers/2506.17080
[01.07.2025 12:22] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[01.07.2025 12:22] No deleted papers detected.
[01.07.2025 12:22] Downloading and parsing papers (pdf, html). Total: 18.
[01.07.2025 12:22] Downloading and parsing paper https://huggingface.co/papers/2506.23044.
[01.07.2025 12:22] Extra JSON file exists (./assets/json/2506.23044.json), skip PDF parsing.
[01.07.2025 12:22] Paper image links file exists (./assets/img_data/2506.23044.json), skip HTML parsing.
[01.07.2025 12:22] Success.
[01.07.2025 12:22] Downloading and parsing paper https://huggingface.co/papers/2506.23858.
[01.07.2025 12:22] Extra JSON file exists (./assets/json/2506.23858.json), skip PDF parsing.
[01.07.2025 12:22] Paper image links file exists (./assets/img_data/2506.23858.json), skip HTML parsing.
[01.07.2025 12:22] Success.
[01.07.2025 12:22] Downloading and parsing paper https://huggingface.co/papers/2506.24123.
[01.07.2025 12:22] Extra JSON file exists (./assets/json/2506.24123.json), skip PDF parsing.
[01.07.2025 12:22] Paper image links file exists (./assets/img_data/2506.24123.json), skip HTML parsing.
[01.07.2025 12:22] Success.
[01.07.2025 12:22] Downloading and parsing paper https://huggingface.co/papers/2506.22832.
[01.07.2025 12:22] Extra JSON file exists (./assets/json/2506.22832.json), skip PDF parsing.
[01.07.2025 12:22] Paper image links file exists (./assets/img_data/2506.22832.json), skip HTML parsing.
[01.07.2025 12:22] Success.
[01.07.2025 12:22] Downloading and parsing paper https://huggingface.co/papers/2506.24119.
[01.07.2025 12:22] Extra JSON file exists (./assets/json/2506.24119.json), skip PDF parsing.
[01.07.2025 12:22] Paper image links file exists (./assets/img_data/2506.24119.json), skip HTML parsing.
[01.07.2025 12:22] Success.
[01.07.2025 12:22] Downloading and parsing paper https://huggingface.co/papers/2506.17930.
[01.07.2025 12:22] Extra JSON file exists (./assets/json/2506.17930.json), skip PDF parsing.
[01.07.2025 12:22] Paper image links file exists (./assets/img_data/2506.17930.json), skip HTML parsing.
[01.07.2025 12:22] Success.
[01.07.2025 12:22] Downloading and parsing paper https://huggingface.co/papers/2506.23542.
[01.07.2025 12:22] Extra JSON file exists (./assets/json/2506.23542.json), skip PDF parsing.
[01.07.2025 12:22] Paper image links file exists (./assets/img_data/2506.23542.json), skip HTML parsing.
[01.07.2025 12:22] Success.
[01.07.2025 12:22] Downloading and parsing paper https://huggingface.co/papers/2506.17417.
[01.07.2025 12:22] Extra JSON file exists (./assets/json/2506.17417.json), skip PDF parsing.
[01.07.2025 12:22] Paper image links file exists (./assets/img_data/2506.17417.json), skip HTML parsing.
[01.07.2025 12:22] Success.
[01.07.2025 12:22] Downloading and parsing paper https://huggingface.co/papers/2506.23151.
[01.07.2025 12:22] Extra JSON file exists (./assets/json/2506.23151.json), skip PDF parsing.
[01.07.2025 12:22] Paper image links file exists (./assets/img_data/2506.23151.json), skip HTML parsing.
[01.07.2025 12:22] Success.
[01.07.2025 12:22] Downloading and parsing paper https://huggingface.co/papers/2506.16500.
[01.07.2025 12:22] Extra JSON file exists (./assets/json/2506.16500.json), skip PDF parsing.
[01.07.2025 12:22] Paper image links file exists (./assets/img_data/2506.16500.json), skip HTML parsing.
[01.07.2025 12:22] Success.
[01.07.2025 12:22] Downloading and parsing paper https://huggingface.co/papers/2506.22992.
[01.07.2025 12:22] Extra JSON file exists (./assets/json/2506.22992.json), skip PDF parsing.
[01.07.2025 12:22] Paper image links file exists (./assets/img_data/2506.22992.json), skip HTML parsing.
[01.07.2025 12:22] Success.
[01.07.2025 12:22] Downloading and parsing paper https://huggingface.co/papers/2506.23394.
[01.07.2025 12:22] Extra JSON file exists (./assets/json/2506.23394.json), skip PDF parsing.
[01.07.2025 12:22] Paper image links file exists (./assets/img_data/2506.23394.json), skip HTML parsing.
[01.07.2025 12:22] Success.
[01.07.2025 12:22] Downloading and parsing paper https://huggingface.co/papers/2506.23219.
[01.07.2025 12:22] Extra JSON file exists (./assets/json/2506.23219.json), skip PDF parsing.
[01.07.2025 12:22] Paper image links file exists (./assets/img_data/2506.23219.json), skip HTML parsing.
[01.07.2025 12:22] Success.
[01.07.2025 12:22] Downloading and parsing paper https://huggingface.co/papers/2506.22694.
[01.07.2025 12:22] Extra JSON file exists (./assets/json/2506.22694.json), skip PDF parsing.
[01.07.2025 12:22] Paper image links file exists (./assets/img_data/2506.22694.json), skip HTML parsing.
[01.07.2025 12:22] Success.
[01.07.2025 12:22] Downloading and parsing paper https://huggingface.co/papers/2506.23135.
[01.07.2025 12:22] Extra JSON file exists (./assets/json/2506.23135.json), skip PDF parsing.
[01.07.2025 12:22] Paper image links file exists (./assets/img_data/2506.23135.json), skip HTML parsing.
[01.07.2025 12:22] Success.
[01.07.2025 12:22] Downloading and parsing paper https://huggingface.co/papers/2506.22753.
[01.07.2025 12:22] Downloading paper 2506.22753 from http://arxiv.org/pdf/2506.22753v1...
[01.07.2025 12:22] Extracting affiliations from text.
[01.07.2025 12:22] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Degradation-Modeled Multipath Diffusion for Tunable Metalens Photography 5 2 0 2 8 2 ] . [ 1 3 5 7 2 2 . 6 0 5 2 : r Figure 1. metasurface-based ultra-compact camera system empowered by large-model-driven image restoration algorithm. Compared to state-of-the-art image restoration methods, our approach achieves superior reconstruction quality and enhanced visual perception. "
[01.07.2025 12:22] Response: []
[01.07.2025 12:22] Extracting affiliations from text.
[01.07.2025 12:22] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Degradation-Modeled Multipath Diffusion for Tunable Metalens Photography5 2 0 2 8 2 ] . [ 1 3 5 7 2 2 . 6 0 5 2 : r Figure 1. metasurface-based ultra-compact camera system empowered by large-model-driven image restoration algorithm. Compared to state-of-the-art image restoration methods, our approach achieves superior reconstruction quality and enhanced visual perception.1. Introduction Metalenses offer significant potential for ultra-compact computational imaging but face challenges from complex optical degradation and computational restoration difficulties. Existing methods typically rely on precise optical calibration or massive paired datasets, which are non-trivial for real-world imaging systems. Furthermore, lack of control over the inference process often results in undesirable hallucinated artifacts. We introduce DegradationModeled Multipath Diffusion for tunable metalens photography, leveraging powerful natural image priors from pretrained models instead of large datasets. Our framework uses positive, neutral, and negative-prompt paths to balance high-frequency detail generation, structural fidelity, and suppression of metalens-specific degradation, alongside pseudo data augmentation. tunable decoder enables controlled trade-offs between fidelity and perceptual quality. Additionally, spatially varying degradation-aware attention (SVDA) module adaptively models complex optical and sensor-induced degradation. Finally, we design and build millimeter-scale MetaCamera for real-world validation. Extensive results show that our approach outperforms stateof-the-art methods, achieving high-fidelity and sharp image reconstruction. More materials: https://dmdiff.github.io/. The demand for compact, high-performance imaging systems has grown rapidly with advancements in biomedical imaging, implantable vision systems, point-of-care diagnostics, and augmented/virtual reality (AR/VR) [29, 31, 43, 60]. However, conventional compound lens systems rely on multi-element optics to correct aberrations, making it challenging to achieve both miniaturization and high imaging quality. Metalenses, class of engineered optical interfaces composed of subwavelength-scale nanostructures, enable precise optical wavefront control by manipulating phase, amplitude, and polarization within an ultrathin form factor. Combined with computational restoration methods, their unique capabilities offer new opportunities for developing compact, high-performance imaging systems. [16, 23, 29, 31, 43, 60]. While metalenses present promising solution for ultracompact imaging systems, their practical implementation is constrained by optical limitations and computational complexities. Specifically, images captured by metalenses suffer from spatially varying degradations due to aberrations. Conventional model-based restoration techniques, such as Wiener filtering [55] and iterative deconvolution methods [8, 25, 58], typically assume uniform distortions and struggle to correct the highly localized aberrations caused by the metalens. Although dividing images into small patches can mitigate this issue, it leads to reduced robustness and stability in the algorithms. Deep learning-based approaches [1, 33, 51, 61] have demonstrated more significant potential for restoration, offering powerful capabilities. However, these approaches typically require large datasets of pixelaligned image pairs for training, which are often difficult to obtain for computational imaging systems. As result, recent research has shifted toward employing large-scale models pretrained on extensive datasets as priors for image super-resolution and restoration tasks, leading to significant improvements in robustness, generalization, and overall performance. Among the various techniques, generative diffusion models (DMs) have demonstrated exceptional performance [2, 18, 27, 40, 56]. These models, pretrained on vast collections of image and text data, possess powerful natural image priors that allow them to synthesize vivid and authentic details more effectively than conventional GANbased methods [15]. However, despite their impressive capabilities, DMs are not devoid of limitations. Their restoration process is inherently prone to generating hallucinated details that do not exist in the original image, due to the lack of fine-grained control over the restoration process during inference. In this manuscript, we propose degradation-modeled multipath diffusion model for tunable metalens photography, leveraging powerful natural image priors from pretrained models instead of large datasets. We first introduce the spatially varying degradation-aware attention (SVDA) module, which quantifies degradation by analyzing both the optical point spread function (PSF) of the metalens and the quality of image patches. This allows it to effectively account for degradation from both the metalens and image sensors. The degradation characterization results are further employed by the attention network to guide the LoRA finetuning process, enabling the diffusion model to adaptively handle region-specific degradations. We then propose the multipath diffusion model comprising three distinct pathways: positive-prompt path for generating high-frequency details, neutral-prompt path for restoring structural fidelity, and negative-prompt path for mitigating metalensspecific degradation. The negative-prompt path also facilitates the generation of pseudo input-output paired samples, which expand the training dataset and enhance generalization. To enable smooth control over the reconstruction results, we introduce tunable decoder that adaptively merges the latent codes from the positive and neutral paths, allowing users to adjust the balance between reconstruction fidelity and perceptual quality. Finally, as demonstrated in Fig. 1, we design and build micro metalens-based camera (MetaCamera) to validate our algorithm. MetaCamera achieves size of approximately 1 mm3. The experimental results demonstrate that our algorithm successfully reconstructs images with both high fidelity and sharpness, outperforming state-of-the-art approaches. Collectively, the contributions of this manuscript are as follows: Multipath Diffusion Model. We propose multipath diffusion model incorporating positive, neutral, and negative-prompt paths to balance high-frequency details and structural fidelity while mitigating metalens-specific degradation. Additionally, we introduce an instantly tunable decoder that e"
[01.07.2025 12:22] Mistral response. {"id": "4d43e45c362f498bace47b9fcd64561b", "object": "chat.completion", "created": 1751372566, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "[]"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1458, "total_tokens": 1460, "completion_tokens": 2}}
[01.07.2025 12:22] Response: []
[01.07.2025 12:22] Deleting PDF ./assets/pdf/2506.22753.pdf.
[01.07.2025 12:22] Success.
[01.07.2025 12:22] Downloading and parsing paper https://huggingface.co/papers/2506.21448.
[01.07.2025 12:22] Extra JSON file exists (./assets/json/2506.21448.json), skip PDF parsing.
[01.07.2025 12:22] Paper image links file exists (./assets/img_data/2506.21448.json), skip HTML parsing.
[01.07.2025 12:22] Success.
[01.07.2025 12:22] Downloading and parsing paper https://huggingface.co/papers/2506.17080.
[01.07.2025 12:22] Extra JSON file exists (./assets/json/2506.17080.json), skip PDF parsing.
[01.07.2025 12:22] Paper image links file exists (./assets/img_data/2506.17080.json), skip HTML parsing.
[01.07.2025 12:22] Success.
[01.07.2025 12:22] Enriching papers with extra data.
[01.07.2025 12:22] ********************************************************************************
[01.07.2025 12:22] Abstract 0. Ovis-U1, a 3-billion-parameter model, combines multimodal understanding, text-to-image generation, and image editing, achieving state-of-the-art performance in various benchmarks.  					AI-generated summary 				 In this report, we introduce Ovis-U1, a 3-billion-parameter unified model that integrate...
[01.07.2025 12:22] ********************************************************************************
[01.07.2025 12:22] Abstract 1. VMoBA, a novel sparse attention mechanism for Video Diffusion Models (VDMs), accelerates training and inference by addressing the quadratic complexity of full attention mechanisms while maintaining or improving video generation quality.  					AI-generated summary 				 The quadratic complexity of ful...
[01.07.2025 12:22] ********************************************************************************
[01.07.2025 12:22] Abstract 2. Calligrapher uses a diffusion-based framework with self-distillation and localized style injection to generate high-quality, stylistically consistent digital typography.  					AI-generated summary 				 We introduce Calligrapher, a novel diffusion-based framework that innovatively integrates advanced...
[01.07.2025 12:22] ********************************************************************************
[01.07.2025 12:22] Abstract 3. A listener-augmented Group Relative Policy Optimization framework improves reward models by re-evaluating reasoning processes, leading to enhanced accuracy and out-of-distribution performance in aligning vision-language models with human preferences.  					AI-generated summary 				 Training robust a...
[01.07.2025 12:22] ********************************************************************************
[01.07.2025 12:22] Abstract 4. Self-play in zero-sum games using SPIRAL enhances reasoning capabilities in language models through self-improvement and transfer learning.  					AI-generated summary 				 Recent advances in reinforcement learning have shown that language models can develop sophisticated reasoning through training o...
[01.07.2025 12:22] ********************************************************************************
[01.07.2025 12:22] Abstract 5. A novel prompt design paradigm, PromptQuine, shows that pruning random demonstrations into "gibberish" can improve large language model performance across various tasks, surpassing state-of-the-art methods.  					AI-generated summary 				 We propose a novel prompt design paradigm that challenges con...
[01.07.2025 12:22] ********************************************************************************
[01.07.2025 12:22] Abstract 6. A novel ToF depth denoising network uses motion-invariant graph fusion and adaptive filters to improve temporal stability and spatial sharpness, achieving state-of-the-art performance.  					AI-generated summary 				 Depth images captured by Time-of-Flight (ToF) sensors are prone to noise, requiring...
[01.07.2025 12:22] ********************************************************************************
[01.07.2025 12:22] Abstract 7. Inference-time techniques like decoding-time scaling and self-refinement enhance reasoning in vision-language models, with generation-based methods providing greater improvement than verification-based methods, despite RL-trained models not showing self-correction benefits.  					AI-generated summar...
[01.07.2025 12:22] ********************************************************************************
[01.07.2025 12:22] Abstract 8. MEMFOF is a memory-efficient multi-frame optical flow method that achieves state-of-the-art performance on high-resolution inputs with reduced GPU memory usage.  					AI-generated summary 				 Recent advances in optical flow estimation have prioritized accuracy at the cost of growing GPU memory cons...
[01.07.2025 12:22] ********************************************************************************
[01.07.2025 12:22] Abstract 9. SparseLoRA reduces computational cost and speeds up fine-tuning of LLMs by dynamically selecting a sparse subset of weights for loss and gradient computation.  					AI-generated summary 				 Fine-tuning LLMs is both computationally and memory-intensive. While parameter-efficient fine-tuning methods,...
[01.07.2025 12:22] ********************************************************************************
[01.07.2025 12:22] Abstract 10. MARBLE is a challenging multimodal reasoning benchmark that highlights the limitations of existing multimodal language models in step-by-step reasoning and plan crafting under spatial and visual constraints.  					AI-generated summary 				 The ability to process information from multiple modalities ...
[01.07.2025 12:22] ********************************************************************************
[01.07.2025 12:22] Abstract 11. A methodology is presented to adapt language models for robust tool use across languages, specifically improving function-calling accuracy in Bulgarian.  					AI-generated summary 				 External tool integration through function-calling is essential for practical language model applications, yet most...
[01.07.2025 12:22] ********************************************************************************
[01.07.2025 12:22] Abstract 12. UrbanLLaVA, a multi-modal large language model, effectively processes urban datasets for various tasks, outperforming existing models in both single-modal and complex cross-modal scenarios.  					AI-generated summary 				 Urban research involves a wide range of scenarios and tasks that require the u...
[01.07.2025 12:22] ********************************************************************************
[01.07.2025 12:22] Abstract 13. A technique called VocabTrim improves drafter-based speculative decoding by reducing the vocabulary of the drafter language model, thus decreasing drafting latency in memory-bound environments.  					AI-generated summary 				 In this paper, we introduce a simple training-free technique to improve th...
[01.07.2025 12:22] ********************************************************************************
[01.07.2025 12:22] Abstract 14. RoboScape is a unified physics-informed world model that enhances visual fidelity and physical plausibility in robotic video generation by integrating temporal depth prediction and keypoint dynamics learning.  					AI-generated summary 				 World models have become indispensable tools for embodied i...
[01.07.2025 12:22] ********************************************************************************
[01.07.2025 12:22] Abstract 15. The proposed Degradation-Modeled Multipath Diffusion framework improves metalens image quality by using natural image priors and specific modules to balance detail, fidelity, and perceptual quality while addressing optical degradation.  					AI-generated summary 				 Metalenses offer significant pot...
[01.07.2025 12:22] ********************************************************************************
[01.07.2025 12:22] Abstract 16. ThinkSound, a novel framework, uses Chain-of-Thought reasoning with a multimodal large language model to generate high-quality audio from videos, achieving state-of-the-art results in various benchmarks.  					AI-generated summary 				 While end-to-end video-to-audio generation has greatly improved,...
[01.07.2025 12:22] ********************************************************************************
[01.07.2025 12:22] Abstract 17. Tower+, a suite of fine-tuned language models, achieves strong performance in both translation and multilingual general-purpose text tasks through a novel training recipe that includes continued pretraining, supervised fine-tuning, preference optimization, and reinforcement learning.  					AI-genera...
[01.07.2025 12:22] Read previous papers.
[01.07.2025 12:22] Generating reviews via LLM API.
[01.07.2025 12:22] Using data from previous issue: {"categories": ["#cv", "#diffusion", "#architecture", "#multimodal", "#benchmark"], "emoji": "ü¶æ", "ru": {"title": "Ovis-U1: –ù–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –ò–ò", "desc": "Ovis-U1 - —ç—Ç–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å —Å 3 –º–∏–ª–ª–∏–∞—Ä–¥–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∞—è –ø–æ–Ω–∏–º–∞–Ω–∏–µ —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö, –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –∏–∑–æ
[01.07.2025 12:22] Using data from previous issue: {"categories": ["#optimization", "#architecture", "#video", "#diffusion", "#training"], "emoji": "üé¨", "ru": {"title": "VMoBA: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ", "desc": "VMoBA - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ö–∞–Ω–∏–∑–º —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è –≤–∏–¥–µ–æ-–¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π (VDM). –û–Ω —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–æ–π —Å–ª–æ
[01.07.2025 12:22] Using data from previous issue: {"categories": ["#architecture", "#benchmark", "#diffusion", "#dataset"], "emoji": "‚úíÔ∏è", "ru": {"title": "–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∫–∞–ª–ª–∏–≥—Ä–∞—Ñ: –Ω–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å —Ü–∏—Ñ—Ä–æ–≤–æ–π —Ç–∏–ø–æ–≥—Ä–∞—Ñ–∏–∫–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Calligrapher - –Ω–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ü–∏—Ñ—Ä–æ–≤–æ–π —Ç–∏–ø–æ–≥—Ä–∞—Ñ–∏–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –ö–ª—é—á–µ–≤—ã
[01.07.2025 12:22] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#alignment", "#rl", "#reasoning", "#rlhf"], "emoji": "üëÇ", "ru": {"title": "–°–ª—É—à–∞–π –∏ —É—á–∏—Å—å: –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é –º–æ–¥–µ–ª–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –¥–ª—è –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∏–∑–æ–±—Ä
[01.07.2025 12:22] Using data from previous issue: {"categories": ["#rl", "#reasoning", "#games", "#transfer_learning", "#rlhf", "#training"], "emoji": "üß†", "ru": {"title": "–°–∞–º–æ—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –ò–ò —á–µ—Ä–µ–∑ –∏–≥—Ä—ã —Å –Ω—É–ª–µ–≤–æ–π —Å—É–º–º–æ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç SPIRAL - —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –∏–≥—Ä—É –≤ –∏–≥—Ä—ã —Å –Ω—É–ª–µ–≤–æ–π —Å—É–º–º–æ–π –ø—Ä–æ—Ç–∏
[01.07.2025 12:22] Using data from previous issue: {"categories": ["#optimization", "#alignment", "#multimodal", "#training"], "emoji": "‚úÇÔ∏è", "ru": {"title": "–ë–µ—Å—Å–º—ã—Å–ª–∏—Ü–∞ –≤ –ø—Ä–æ–º–ø—Ç–∞—Ö —É–ª—É—á—à–∞–µ—Ç —Ä–∞–±–æ—Ç—É —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–∞—è –ø–∞—Ä–∞–¥–∏–≥–º–∞ –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ–º–ø—Ç–æ–≤ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º PromptQuine. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –æ–±—Ä–µ–∑–∞–Ω
[01.07.2025 12:22] Using data from previous issue: {"categories": ["#cv", "#optimization", "#dataset", "#graphs", "#synthetic", "#open_source"], "emoji": "üîç", "ru": {"title": "–†–µ–≤–æ–ª—é—Ü–∏–æ–Ω–Ω–æ–µ —à—É–º–æ–ø–æ–¥–∞–≤–ª–µ–Ω–∏–µ ToF: —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –∏ —á–µ—Ç–∫–æ—Å—Ç—å —á–µ—Ä–µ–∑ –≥—Ä–∞—Ñ–æ–≤–æ–µ —Å–ª–∏—è–Ω–∏–µ", "desc": "–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –Ω–æ–≤–∞—è –Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å –¥–ª—è —à—É–º–æ–ø–æ–¥–∞–≤–ª–µ–Ω–∏—è –≤ –¥–∞—Ç—á–∏–∫–∞—Ö –≥–ª—É–±–∏–Ω—ã Time-of-Fligh
[01.07.2025 12:22] Using data from previous issue: {"categories": ["#rl", "#reasoning", "#inference", "#multimodal"], "emoji": "üß†", "ru": {"title": "–£–ª—É—á—à–µ–Ω–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö: –ø–æ–±–µ–¥–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –Ω–∞–¥ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–µ–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –º–µ—Ç–æ–¥—ã –≤—ã–≤–æ–¥–∞, —Ç–∞–∫–∏–µ –∫–∞–∫ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –≤–æ –≤—Ä–µ–º—è –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –∏ —Å–∞–º–æ—É—Ç–æ—á–Ω–µ
[01.07.2025 12:22] Using data from previous issue: {"categories": ["#benchmark", "#optimization", "#architecture", "#cv", "#inference"], "emoji": "üîç", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –æ–ø—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ—Ç–æ–∫–∞ —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø–∞–º—è—Ç–∏", "desc": "MEMFOF - —ç—Ç–æ –º–µ—Ç–æ–¥ –æ–ø—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ—Ç–æ–∫–∞, –∫–æ—Ç–æ—Ä—ã–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø–∞–º—è—Ç—å GPU –¥–ª—è –≤—ã—Å–æ–∫–æ—Ä–∞–∑—Ä–µ—à–∞—é
[01.07.2025 12:22] Using data from previous issue: {"categories": ["#training", "#optimization"], "emoji": "üöÄ", "ru": {"title": "SparseLoRA: –£—Å–∫–æ—Ä–µ–Ω–∏–µ —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ LLM –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞", "desc": "SparseLoRA - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —É—Å–∫–æ—Ä–µ–Ω–∏—è —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—É—é —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç—å –¥–ª—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –≤—ã
[01.07.2025 12:22] Using data from previous issue: {"categories": ["#benchmark", "#multimodal", "#reasoning"], "emoji": "üß†", "ru": {"title": "MARBLE: –í—ã–∑–æ–≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–º —è–∑—ã–∫–æ–≤—ã–º –º–æ–¥–µ–ª—è–º –≤ —Å–ª–æ–∂–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è—Ö", "desc": "MARBLE - —ç—Ç–æ —Å–ª–æ–∂–Ω—ã–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä—ã–π –≤—ã—è–≤–ª—è–µ—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö 
[01.07.2025 12:22] Using data from previous issue: {"categories": ["#multilingual", "#dataset", "#low_resource", "#benchmark", "#open_source", "#training"], "emoji": "üîß", "ru": {"title": "–ú–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –ò–ò: —Ç–æ—á–Ω—ã–µ –≤—ã–∑–æ–≤—ã —Ñ—É–Ω–∫—Ü–∏–π –Ω–∞ –ª—é–±–æ–º —è–∑—ã–∫–µ", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ
[01.07.2025 12:22] Using data from previous issue: {"categories": ["#dataset", "#science", "#training", "#multimodal", "#benchmark", "#open_source"], "emoji": "üèôÔ∏è", "ru": {"title": "UrbanLLaVA: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –∞–Ω–∞–ª–∏–∑–µ –≥–æ—Ä–æ–¥—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö —Å –ø–æ–º–æ—â—å—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –ò–ò", "desc": "UrbanLLaVA - —ç—Ç–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –±–æ–ª—å—à–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –¥–ª—è –æ–±—Ä
[01.07.2025 12:22] Using data from previous issue: {"categories": ["#inference", "#training", "#optimization", "#small_models"], "emoji": "‚úÇÔ∏è", "ru": {"title": "–£—Å–∫–æ—Ä–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ –ø—É—Ç–µ–º –æ–±—Ä–µ–∑–∫–∏ —Å–ª–æ–≤–∞—Ä—è", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Ç–µ—Ö–Ω–∏–∫—É VocabTrim, –∫–æ—Ç–æ—Ä–∞—è —É–ª—É—á—à–∞–µ—Ç —Å–ø–µ–∫—É–ª—è—Ç–∏–≤–Ω–æ–µ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥—Ä–∞—Ñ—Ç–µ—Ä–∞ –ø—É—Ç–µ–º —É–º–µ–Ω—å—à–µ–Ω–∏—è —Å–ª–æ–≤–∞—Ä
[01.07.2025 12:22] Using data from previous issue: {"categories": ["#optimization", "#science", "#open_source", "#robotics", "#video", "#games", "#3d"], "emoji": "ü§ñ", "ru": {"title": "RoboScape: —Ñ–∏–∑–∏—á–µ—Å–∫–∏ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∏–¥–µ–æ –¥–ª—è –ø—Ä–æ–¥–≤–∏–∂–µ–Ω–∏—è –≤–æ–ø–ª–æ—â–µ–Ω–Ω–æ–≥–æ –ò–ò", "desc": "RoboScape - —ç—Ç–æ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ñ–∏–∑–∏—á–µ—Å–∫–∏-–∏–Ω—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–∏—Ä–æ–≤–∞—è –º–æ–¥–µ–ª—å –¥–ª
[01.07.2025 12:22] Using data from previous issue: {"categories": ["#hallucinations", "#cv", "#data", "#inference", "#diffusion"], "emoji": "üîç", "ru": {"title": "–£–ª—É—á—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –º–µ—Ç–∞–ª–∏–Ω–∑ –±–µ–∑ –º–∞—Å—à—Ç–∞–±–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤", "desc": "–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å Degradation-Modeled Multipath Diffusion –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –ø–æ–ª—É—á–∞–µ–º—ã—Ö 
[01.07.2025 12:22] Using data from previous issue: {"categories": ["#dataset", "#games", "#multimodal", "#benchmark", "#reasoning", "#audio"], "emoji": "üé¨", "ru": {"title": "–î—É–º–∞–π –∫–∞–∫ –∑–≤—É–∫–æ—Ä–µ–∂–∏—Å—Å–µ—Ä: –ò–ò —Å–æ–∑–¥–∞–µ—Ç –∞—É–¥–∏–æ –¥–ª—è –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π", "desc": "ThinkSound - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∞—É–¥–∏–æ –∏–∑ –≤–∏–¥–µ–æ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞
[01.07.2025 12:22] Using data from previous issue: {"categories": ["#multilingual", "#benchmark", "#optimization", "#dataset", "#training", "#translation"], "emoji": "üóº", "ru": {"title": "Tower+: –Ø–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∏–µ –ø–µ—Ä–µ–≤–æ–¥ –∏ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–µ –∑–∞–¥–∞—á–∏", "desc": "Tower+ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –Ω–∞–±–æ—Ä —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –¥–æ—Å—Ç–∏–≥–∞—é—â–∏—Ö –≤—ã—Å–æ–∫–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω
[01.07.2025 12:22] Renaming data file.
[01.07.2025 12:22] Renaming previous data. hf_papers.json to ./d/2025-07-01.json
[01.07.2025 12:22] Saving new data file.
[01.07.2025 12:22] Generating page.
[01.07.2025 12:22] Renaming previous page.
[01.07.2025 12:22] Renaming previous data. index.html to ./d/2025-07-01.html
[01.07.2025 12:22] Writing result.
[01.07.2025 12:22] Renaming log file.
[01.07.2025 12:22] Renaming previous data. log.txt to ./logs/2025-07-01_last_log.txt
