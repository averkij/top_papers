[01.07.2025 14:12] Read previous papers.
[01.07.2025 14:12] Generating top page (month).
[01.07.2025 14:12] Writing top page (month).
[01.07.2025 15:12] Read previous papers.
[01.07.2025 15:12] Get feed.
[01.07.2025 15:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.23044
[01.07.2025 15:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.23858
[01.07.2025 15:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.24123
[01.07.2025 15:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.24119
[01.07.2025 15:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.22832
[01.07.2025 15:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.17930
[01.07.2025 15:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.23542
[01.07.2025 15:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.17417
[01.07.2025 15:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.23151
[01.07.2025 15:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.16500
[01.07.2025 15:12] Extract page data from URL. URL: https://huggingface.co/papers/2506.22598
[01.07.2025 15:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.23219
[01.07.2025 15:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.22992
[01.07.2025 15:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.23394
[01.07.2025 15:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.22694
[01.07.2025 15:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.23135
[01.07.2025 15:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.22753
[01.07.2025 15:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.21448
[01.07.2025 15:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.17080
[01.07.2025 15:12] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[01.07.2025 15:12] No deleted papers detected.
[01.07.2025 15:12] Downloading and parsing papers (pdf, html). Total: 19.
[01.07.2025 15:12] Downloading and parsing paper https://huggingface.co/papers/2506.23044.
[01.07.2025 15:12] Extra JSON file exists (./assets/json/2506.23044.json), skip PDF parsing.
[01.07.2025 15:12] Paper image links file exists (./assets/img_data/2506.23044.json), skip HTML parsing.
[01.07.2025 15:12] Success.
[01.07.2025 15:12] Downloading and parsing paper https://huggingface.co/papers/2506.23858.
[01.07.2025 15:12] Extra JSON file exists (./assets/json/2506.23858.json), skip PDF parsing.
[01.07.2025 15:12] Paper image links file exists (./assets/img_data/2506.23858.json), skip HTML parsing.
[01.07.2025 15:12] Success.
[01.07.2025 15:12] Downloading and parsing paper https://huggingface.co/papers/2506.24123.
[01.07.2025 15:12] Extra JSON file exists (./assets/json/2506.24123.json), skip PDF parsing.
[01.07.2025 15:12] Paper image links file exists (./assets/img_data/2506.24123.json), skip HTML parsing.
[01.07.2025 15:12] Success.
[01.07.2025 15:12] Downloading and parsing paper https://huggingface.co/papers/2506.24119.
[01.07.2025 15:12] Extra JSON file exists (./assets/json/2506.24119.json), skip PDF parsing.
[01.07.2025 15:12] Paper image links file exists (./assets/img_data/2506.24119.json), skip HTML parsing.
[01.07.2025 15:12] Success.
[01.07.2025 15:12] Downloading and parsing paper https://huggingface.co/papers/2506.22832.
[01.07.2025 15:12] Extra JSON file exists (./assets/json/2506.22832.json), skip PDF parsing.
[01.07.2025 15:12] Paper image links file exists (./assets/img_data/2506.22832.json), skip HTML parsing.
[01.07.2025 15:12] Success.
[01.07.2025 15:12] Downloading and parsing paper https://huggingface.co/papers/2506.17930.
[01.07.2025 15:12] Extra JSON file exists (./assets/json/2506.17930.json), skip PDF parsing.
[01.07.2025 15:12] Paper image links file exists (./assets/img_data/2506.17930.json), skip HTML parsing.
[01.07.2025 15:12] Success.
[01.07.2025 15:12] Downloading and parsing paper https://huggingface.co/papers/2506.23542.
[01.07.2025 15:12] Extra JSON file exists (./assets/json/2506.23542.json), skip PDF parsing.
[01.07.2025 15:12] Paper image links file exists (./assets/img_data/2506.23542.json), skip HTML parsing.
[01.07.2025 15:12] Success.
[01.07.2025 15:12] Downloading and parsing paper https://huggingface.co/papers/2506.17417.
[01.07.2025 15:12] Extra JSON file exists (./assets/json/2506.17417.json), skip PDF parsing.
[01.07.2025 15:12] Paper image links file exists (./assets/img_data/2506.17417.json), skip HTML parsing.
[01.07.2025 15:12] Success.
[01.07.2025 15:12] Downloading and parsing paper https://huggingface.co/papers/2506.23151.
[01.07.2025 15:12] Extra JSON file exists (./assets/json/2506.23151.json), skip PDF parsing.
[01.07.2025 15:12] Paper image links file exists (./assets/img_data/2506.23151.json), skip HTML parsing.
[01.07.2025 15:12] Success.
[01.07.2025 15:12] Downloading and parsing paper https://huggingface.co/papers/2506.16500.
[01.07.2025 15:12] Extra JSON file exists (./assets/json/2506.16500.json), skip PDF parsing.
[01.07.2025 15:12] Paper image links file exists (./assets/img_data/2506.16500.json), skip HTML parsing.
[01.07.2025 15:12] Success.
[01.07.2025 15:12] Downloading and parsing paper https://huggingface.co/papers/2506.22598.
[01.07.2025 15:12] Downloading paper 2506.22598 from http://arxiv.org/pdf/2506.22598v1...
[01.07.2025 15:13] Extracting affiliations from text.
[01.07.2025 15:13] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 7 2 ] . [ 1 8 9 5 2 2 . 6 0 5 2 : r REXBENCH: Can coding agents autonomously implement AI research extensions? Nicholas Edwards1 Yukyung Lee2 Yujun (Audrey) Mao2 Yulu Qin2 Sebastian Schuster1,3 Najoung Kim2 1University College London 2Boston University 3University of Vienna nicholas.edwards@ucl.ac.uk sebastian.schuster@univie.ac.at {ylee5, amao, yuluqin, najoung}@bu.edu "
[01.07.2025 15:13] Response: ```python
["University College London", "Boston University", "University of Vienna"]
```
[01.07.2025 15:13] Deleting PDF ./assets/pdf/2506.22598.pdf.
[01.07.2025 15:13] Success.
[01.07.2025 15:13] Downloading and parsing paper https://huggingface.co/papers/2506.23219.
[01.07.2025 15:13] Extra JSON file exists (./assets/json/2506.23219.json), skip PDF parsing.
[01.07.2025 15:13] Paper image links file exists (./assets/img_data/2506.23219.json), skip HTML parsing.
[01.07.2025 15:13] Success.
[01.07.2025 15:13] Downloading and parsing paper https://huggingface.co/papers/2506.22992.
[01.07.2025 15:13] Extra JSON file exists (./assets/json/2506.22992.json), skip PDF parsing.
[01.07.2025 15:13] Paper image links file exists (./assets/img_data/2506.22992.json), skip HTML parsing.
[01.07.2025 15:13] Success.
[01.07.2025 15:13] Downloading and parsing paper https://huggingface.co/papers/2506.23394.
[01.07.2025 15:13] Extra JSON file exists (./assets/json/2506.23394.json), skip PDF parsing.
[01.07.2025 15:13] Paper image links file exists (./assets/img_data/2506.23394.json), skip HTML parsing.
[01.07.2025 15:13] Success.
[01.07.2025 15:13] Downloading and parsing paper https://huggingface.co/papers/2506.22694.
[01.07.2025 15:13] Extra JSON file exists (./assets/json/2506.22694.json), skip PDF parsing.
[01.07.2025 15:13] Paper image links file exists (./assets/img_data/2506.22694.json), skip HTML parsing.
[01.07.2025 15:13] Success.
[01.07.2025 15:13] Downloading and parsing paper https://huggingface.co/papers/2506.23135.
[01.07.2025 15:13] Extra JSON file exists (./assets/json/2506.23135.json), skip PDF parsing.
[01.07.2025 15:13] Paper image links file exists (./assets/img_data/2506.23135.json), skip HTML parsing.
[01.07.2025 15:13] Success.
[01.07.2025 15:13] Downloading and parsing paper https://huggingface.co/papers/2506.22753.
[01.07.2025 15:13] Extra JSON file exists (./assets/json/2506.22753.json), skip PDF parsing.
[01.07.2025 15:13] Paper image links file exists (./assets/img_data/2506.22753.json), skip HTML parsing.
[01.07.2025 15:13] Success.
[01.07.2025 15:13] Downloading and parsing paper https://huggingface.co/papers/2506.21448.
[01.07.2025 15:13] Extra JSON file exists (./assets/json/2506.21448.json), skip PDF parsing.
[01.07.2025 15:13] Paper image links file exists (./assets/img_data/2506.21448.json), skip HTML parsing.
[01.07.2025 15:13] Success.
[01.07.2025 15:13] Downloading and parsing paper https://huggingface.co/papers/2506.17080.
[01.07.2025 15:13] Extra JSON file exists (./assets/json/2506.17080.json), skip PDF parsing.
[01.07.2025 15:13] Paper image links file exists (./assets/img_data/2506.17080.json), skip HTML parsing.
[01.07.2025 15:13] Success.
[01.07.2025 15:13] Enriching papers with extra data.
[01.07.2025 15:13] ********************************************************************************
[01.07.2025 15:13] Abstract 0. Ovis-U1, a 3-billion-parameter model, combines multimodal understanding, text-to-image generation, and image editing, achieving state-of-the-art performance in various benchmarks.  					AI-generated summary 				 In this report, we introduce Ovis-U1, a 3-billion-parameter unified model that integrate...
[01.07.2025 15:13] ********************************************************************************
[01.07.2025 15:13] Abstract 1. VMoBA, a novel sparse attention mechanism for Video Diffusion Models (VDMs), accelerates training and inference by addressing the quadratic complexity of full attention mechanisms while maintaining or improving video generation quality.  					AI-generated summary 				 The quadratic complexity of ful...
[01.07.2025 15:13] ********************************************************************************
[01.07.2025 15:13] Abstract 2. Calligrapher uses a diffusion-based framework with self-distillation and localized style injection to generate high-quality, stylistically consistent digital typography.  					AI-generated summary 				 We introduce Calligrapher, a novel diffusion-based framework that innovatively integrates advanced...
[01.07.2025 15:13] ********************************************************************************
[01.07.2025 15:13] Abstract 3. Self-play in zero-sum games using SPIRAL enhances reasoning capabilities in language models through self-improvement and transfer learning.  					AI-generated summary 				 Recent advances in reinforcement learning have shown that language models can develop sophisticated reasoning through training o...
[01.07.2025 15:13] ********************************************************************************
[01.07.2025 15:13] Abstract 4. A listener-augmented Group Relative Policy Optimization framework improves reward models by re-evaluating reasoning processes, leading to enhanced accuracy and out-of-distribution performance in aligning vision-language models with human preferences.  					AI-generated summary 				 Training robust a...
[01.07.2025 15:13] ********************************************************************************
[01.07.2025 15:13] Abstract 5. A novel prompt design paradigm, PromptQuine, shows that pruning random demonstrations into "gibberish" can improve large language model performance across various tasks, surpassing state-of-the-art methods.  					AI-generated summary 				 We propose a novel prompt design paradigm that challenges con...
[01.07.2025 15:13] ********************************************************************************
[01.07.2025 15:13] Abstract 6. A novel ToF depth denoising network uses motion-invariant graph fusion and adaptive filters to improve temporal stability and spatial sharpness, achieving state-of-the-art performance.  					AI-generated summary 				 Depth images captured by Time-of-Flight (ToF) sensors are prone to noise, requiring...
[01.07.2025 15:13] ********************************************************************************
[01.07.2025 15:13] Abstract 7. Inference-time techniques like decoding-time scaling and self-refinement enhance reasoning in vision-language models, with generation-based methods providing greater improvement than verification-based methods, despite RL-trained models not showing self-correction benefits.  					AI-generated summar...
[01.07.2025 15:13] ********************************************************************************
[01.07.2025 15:13] Abstract 8. MEMFOF is a memory-efficient multi-frame optical flow method that achieves state-of-the-art performance on high-resolution inputs with reduced GPU memory usage.  					AI-generated summary 				 Recent advances in optical flow estimation have prioritized accuracy at the cost of growing GPU memory cons...
[01.07.2025 15:13] ********************************************************************************
[01.07.2025 15:13] Abstract 9. SparseLoRA reduces computational cost and speeds up fine-tuning of LLMs by dynamically selecting a sparse subset of weights for loss and gradient computation.  					AI-generated summary 				 Fine-tuning LLMs is both computationally and memory-intensive. While parameter-efficient fine-tuning methods,...
[01.07.2025 15:13] ********************************************************************************
[01.07.2025 15:13] Abstract 10. RExBench evaluates the ability of LLM agents to autonomously implement research extensions, failing to achieve significant success without human hints.  					AI-generated summary 				 Agents based on Large Language Models (LLMs) have shown promise for performing sophisticated software engineering ta...
[01.07.2025 15:13] ********************************************************************************
[01.07.2025 15:13] Abstract 11. UrbanLLaVA, a multi-modal large language model, effectively processes urban datasets for various tasks, outperforming existing models in both single-modal and complex cross-modal scenarios.  					AI-generated summary 				 Urban research involves a wide range of scenarios and tasks that require the u...
[01.07.2025 15:13] ********************************************************************************
[01.07.2025 15:13] Abstract 12. MARBLE is a challenging multimodal reasoning benchmark that highlights the limitations of existing multimodal language models in step-by-step reasoning and plan crafting under spatial and visual constraints.  					AI-generated summary 				 The ability to process information from multiple modalities ...
[01.07.2025 15:13] ********************************************************************************
[01.07.2025 15:13] Abstract 13. A methodology is presented to adapt language models for robust tool use across languages, specifically improving function-calling accuracy in Bulgarian.  					AI-generated summary 				 External tool integration through function-calling is essential for practical language model applications, yet most...
[01.07.2025 15:13] ********************************************************************************
[01.07.2025 15:13] Abstract 14. A technique called VocabTrim improves drafter-based speculative decoding by reducing the vocabulary of the drafter language model, thus decreasing drafting latency in memory-bound environments.  					AI-generated summary 				 In this paper, we introduce a simple training-free technique to improve th...
[01.07.2025 15:13] ********************************************************************************
[01.07.2025 15:13] Abstract 15. RoboScape is a unified physics-informed world model that enhances visual fidelity and physical plausibility in robotic video generation by integrating temporal depth prediction and keypoint dynamics learning.  					AI-generated summary 				 World models have become indispensable tools for embodied i...
[01.07.2025 15:13] ********************************************************************************
[01.07.2025 15:13] Abstract 16. The proposed Degradation-Modeled Multipath Diffusion framework improves metalens image quality by using natural image priors and specific modules to balance detail, fidelity, and perceptual quality while addressing optical degradation.  					AI-generated summary 				 Metalenses offer significant pot...
[01.07.2025 15:13] ********************************************************************************
[01.07.2025 15:13] Abstract 17. ThinkSound, a novel framework, uses Chain-of-Thought reasoning with a multimodal large language model to generate high-quality audio from videos, achieving state-of-the-art results in various benchmarks.  					AI-generated summary 				 While end-to-end video-to-audio generation has greatly improved,...
[01.07.2025 15:13] ********************************************************************************
[01.07.2025 15:13] Abstract 18. Tower+, a suite of fine-tuned language models, achieves strong performance in both translation and multilingual general-purpose text tasks through a novel training recipe that includes continued pretraining, supervised fine-tuning, preference optimization, and reinforcement learning.  					AI-genera...
[01.07.2025 15:13] Read previous papers.
[01.07.2025 15:13] Generating reviews via LLM API.
[01.07.2025 15:13] Using data from previous issue: {"categories": ["#cv", "#diffusion", "#architecture", "#multimodal", "#benchmark"], "emoji": "ü¶æ", "ru": {"title": "Ovis-U1: –ù–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –ò–ò", "desc": "Ovis-U1 - —ç—Ç–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å —Å 3 –º–∏–ª–ª–∏–∞—Ä–¥–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∞—è –ø–æ–Ω–∏–º–∞–Ω–∏–µ —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö, –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –∏–∑–æ
[01.07.2025 15:13] Using data from previous issue: {"categories": ["#optimization", "#architecture", "#video", "#diffusion", "#training"], "emoji": "üé¨", "ru": {"title": "VMoBA: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ", "desc": "VMoBA - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ö–∞–Ω–∏–∑–º —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è –≤–∏–¥–µ–æ-–¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π (VDM). –û–Ω —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–æ–π —Å–ª–æ
[01.07.2025 15:13] Using data from previous issue: {"categories": ["#architecture", "#benchmark", "#diffusion", "#dataset"], "emoji": "‚úíÔ∏è", "ru": {"title": "–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∫–∞–ª–ª–∏–≥—Ä–∞—Ñ: –Ω–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å —Ü–∏—Ñ—Ä–æ–≤–æ–π —Ç–∏–ø–æ–≥—Ä–∞—Ñ–∏–∫–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Calligrapher - –Ω–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ü–∏—Ñ—Ä–æ–≤–æ–π —Ç–∏–ø–æ–≥—Ä–∞—Ñ–∏–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –ö–ª—é—á–µ–≤—ã
[01.07.2025 15:13] Using data from previous issue: {"categories": ["#rl", "#reasoning", "#games", "#transfer_learning", "#rlhf", "#training"], "emoji": "üß†", "ru": {"title": "–°–∞–º–æ—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –ò–ò —á–µ—Ä–µ–∑ –∏–≥—Ä—ã —Å –Ω—É–ª–µ–≤–æ–π —Å—É–º–º–æ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç SPIRAL - —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –∏–≥—Ä—É –≤ –∏–≥—Ä—ã —Å –Ω—É–ª–µ–≤–æ–π —Å—É–º–º–æ–π –ø—Ä–æ—Ç–∏
[01.07.2025 15:13] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#alignment", "#rl", "#reasoning", "#rlhf"], "emoji": "üëÇ", "ru": {"title": "–°–ª—É—à–∞–π –∏ —É—á–∏—Å—å: –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é –º–æ–¥–µ–ª–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –¥–ª—è –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∏–∑–æ–±—Ä
[01.07.2025 15:13] Using data from previous issue: {"categories": ["#optimization", "#alignment", "#multimodal", "#training"], "emoji": "‚úÇÔ∏è", "ru": {"title": "–ë–µ—Å—Å–º—ã—Å–ª–∏—Ü–∞ –≤ –ø—Ä–æ–º–ø—Ç–∞—Ö —É–ª—É—á—à–∞–µ—Ç —Ä–∞–±–æ—Ç—É —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–∞—è –ø–∞—Ä–∞–¥–∏–≥–º–∞ –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ–º–ø—Ç–æ–≤ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º PromptQuine. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –æ–±—Ä–µ–∑–∞–Ω
[01.07.2025 15:13] Using data from previous issue: {"categories": ["#cv", "#optimization", "#dataset", "#graphs", "#synthetic", "#open_source"], "emoji": "üîç", "ru": {"title": "–†–µ–≤–æ–ª—é—Ü–∏–æ–Ω–Ω–æ–µ —à—É–º–æ–ø–æ–¥–∞–≤–ª–µ–Ω–∏–µ ToF: —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –∏ —á–µ—Ç–∫–æ—Å—Ç—å —á–µ—Ä–µ–∑ –≥—Ä–∞—Ñ–æ–≤–æ–µ —Å–ª–∏—è–Ω–∏–µ", "desc": "–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –Ω–æ–≤–∞—è –Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å –¥–ª—è —à—É–º–æ–ø–æ–¥–∞–≤–ª–µ–Ω–∏—è –≤ –¥–∞—Ç—á–∏–∫–∞—Ö –≥–ª—É–±–∏–Ω—ã Time-of-Fligh
[01.07.2025 15:13] Using data from previous issue: {"categories": ["#rl", "#reasoning", "#inference", "#multimodal"], "emoji": "üß†", "ru": {"title": "–£–ª—É—á—à–µ–Ω–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö: –ø–æ–±–µ–¥–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –Ω–∞–¥ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–µ–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –º–µ—Ç–æ–¥—ã –≤—ã–≤–æ–¥–∞, —Ç–∞–∫–∏–µ –∫–∞–∫ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –≤–æ –≤—Ä–µ–º—è –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –∏ —Å–∞–º–æ—É—Ç–æ—á–Ω–µ
[01.07.2025 15:13] Using data from previous issue: {"categories": ["#benchmark", "#optimization", "#architecture", "#cv", "#inference"], "emoji": "üîç", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –æ–ø—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ—Ç–æ–∫–∞ —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø–∞–º—è—Ç–∏", "desc": "MEMFOF - —ç—Ç–æ –º–µ—Ç–æ–¥ –æ–ø—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ—Ç–æ–∫–∞, –∫–æ—Ç–æ—Ä—ã–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø–∞–º—è—Ç—å GPU –¥–ª—è –≤—ã—Å–æ–∫–æ—Ä–∞–∑—Ä–µ—à–∞—é
[01.07.2025 15:13] Using data from previous issue: {"categories": ["#training", "#optimization"], "emoji": "üöÄ", "ru": {"title": "SparseLoRA: –£—Å–∫–æ—Ä–µ–Ω–∏–µ —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ LLM –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞", "desc": "SparseLoRA - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —É—Å–∫–æ—Ä–µ–Ω–∏—è —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—É—é —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç—å –¥–ª—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –≤—ã
[01.07.2025 15:13] Querying the API.
[01.07.2025 15:13] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

RExBench evaluates the ability of LLM agents to autonomously implement research extensions, failing to achieve significant success without human hints.  					AI-generated summary 				 Agents based on Large Language Models (LLMs) have shown promise for performing sophisticated software engineering tasks autonomously. In addition, there has been progress towards developing agents that can perform parts of the research pipeline in machine learning and the natural sciences. We argue that research extension and its implementation is a critical capability for such systems, and introduce RExBench to support the evaluation of this capability. RExBench is a benchmark consisting of 12 realistic research experiment implementation tasks that aim to investigate research hypotheses that have not previously been implemented. Each task is set up as an extension to an existing research paper and codebase, accompanied by domain expert-written instructions. RExBench is robust to data contamination, and supports an automatic evaluation infrastructure that executes agent outputs to determine whether the success criteria are met. We use this benchmark to evaluate nine LLM agents implemented using three different frameworks: aider, Claude Code, and OpenHands. We find that all agents evaluated fail to autonomously implement the majority of the extensions. Although the success rate improves with additional human-written hints, the best performance under this setting remains below 40%. This indicates that current agents are still short of being able to handle realistic research extension tasks without substantial human guidance.
[01.07.2025 15:13] Response: {
  "desc": "RExBench - —ç—Ç–æ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –∞–≤—Ç–æ–Ω–æ–º–Ω–æ —Ä–µ–∞–ª–∏–∑–æ–≤—ã–≤–∞—Ç—å —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π. –û–Ω –≤–∫–ª—é—á–∞–µ—Ç 12 —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á –ø–æ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã—Ö –Ω–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –Ω–∞—É—á–Ω—ã—Ö —Ä–∞–±–æ—Ç–∞—Ö –∏ –∫–æ–¥–æ–≤—ã—Ö –±–∞–∑–∞—Ö. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –∞–≥–µ–Ω—Ç—ã –Ω–µ —Å–ø–æ—Å–æ–±–Ω—ã —É—Å–ø–µ—à–Ω–æ –≤—ã–ø–æ–ª–Ω—è—Ç—å –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ –∑–∞–¥–∞—á –±–µ–∑ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–π –ø–æ–º–æ—â–∏ —á–µ–ª–æ–≤–µ–∫–∞. –î–∞–∂–µ —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –ø–æ–¥—Å–∫–∞–∑–∫–∞–º–∏ –ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –Ω–µ –ø—Ä–µ–≤—ã—à–∞–µ—Ç 40% —É—Å–ø–µ—à–Ω—ã—Ö —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–π.",
  "emoji": "üß™",
  "title": "LLM-–∞–≥–µ–Ω—Ç—ã –ø–æ–∫–∞ –Ω–µ –≥–æ—Ç–æ–≤—ã –∫ —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω—ã–º –Ω–∞—É—á–Ω—ã–º –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è–º"
}
[01.07.2025 15:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"RExBench evaluates the ability of LLM agents to autonomously implement research extensions, failing to achieve significant success without human hints.  					AI-generated summary 				 Agents based on Large Language Models (LLMs) have shown promise for performing sophisticated software engineering tasks autonomously. In addition, there has been progress towards developing agents that can perform parts of the research pipeline in machine learning and the natural sciences. We argue that research extension and its implementation is a critical capability for such systems, and introduce RExBench to support the evaluation of this capability. RExBench is a benchmark consisting of 12 realistic research experiment implementation tasks that aim to investigate research hypotheses that have not previously been implemented. Each task is set up as an extension to an existing research paper and codebase, accompanied by domain expert-written instructions. RExBench is robust to data contamination, and supports an automatic evaluation infrastructure that executes agent outputs to determine whether the success criteria are met. We use this benchmark to evaluate nine LLM agents implemented using three different frameworks: aider, Claude Code, and OpenHands. We find that all agents evaluated fail to autonomously implement the majority of the extensions. Although the success rate improves with additional human-written hints, the best performance under this setting remains below 40%. This indicates that current agents are still short of being able to handle realistic research extension tasks without substantial human guidance."

[01.07.2025 15:13] Response: ```python
['BENCHMARK', 'AGENTS']
```
[01.07.2025 15:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"RExBench evaluates the ability of LLM agents to autonomously implement research extensions, failing to achieve significant success without human hints.  					AI-generated summary 				 Agents based on Large Language Models (LLMs) have shown promise for performing sophisticated software engineering tasks autonomously. In addition, there has been progress towards developing agents that can perform parts of the research pipeline in machine learning and the natural sciences. We argue that research extension and its implementation is a critical capability for such systems, and introduce RExBench to support the evaluation of this capability. RExBench is a benchmark consisting of 12 realistic research experiment implementation tasks that aim to investigate research hypotheses that have not previously been implemented. Each task is set up as an extension to an existing research paper and codebase, accompanied by domain expert-written instructions. RExBench is robust to data contamination, and supports an automatic evaluation infrastructure that executes agent outputs to determine whether the success criteria are met. We use this benchmark to evaluate nine LLM agents implemented using three different frameworks: aider, Claude Code, and OpenHands. We find that all agents evaluated fail to autonomously implement the majority of the extensions. Although the success rate improves with additional human-written hints, the best performance under this setting remains below 40%. This indicates that current agents are still short of being able to handle realistic research extension tasks without substantial human guidance."

[01.07.2025 15:13] Response: ```python
["SCIENCE"]
```
[01.07.2025 15:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"RExBench is a benchmark designed to assess the capability of Large Language Model (LLM) agents in autonomously implementing research extensions. The benchmark includes 12 tasks that require agents to extend existing research papers and codebases, with expert-written instructions provided for guidance. Despite the potential of LLMs in software engineering, the study found that these agents struggled to successfully complete the tasks without significant human assistance. The results highlight the limitations of current LLM agents in handling complex research tasks independently, achieving less than 40% success even with hints.","title":"Evaluating LLM Agents: The Challenge of Autonomous Research Extensions"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='RExBench is a benchmark designed to assess the capability of Large Language Model (LLM) agents in autonomously implementing research extensions. The benchmark includes 12 tasks that require agents to extend existing research papers and codebases, with expert-written instructions provided for guidance. Despite the potential of LLMs in software engineering, the study found that these agents struggled to successfully complete the tasks without significant human assistance. The results highlight the limitations of current LLM agents in handling complex research tasks independently, achieving less than 40% success even with hints.', title='Evaluating LLM Agents: The Challenge of Autonomous Research Extensions'))
[01.07.2025 15:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"RExBenchÊòØ‰∏Ä‰∏™ËØÑ‰º∞Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâ‰ª£ÁêÜÂú®Ëá™‰∏ªÂÆûÁé∞Á†îÁ©∂Êâ©Â±ïËÉΩÂäõÁöÑÂü∫ÂáÜÂ∑•ÂÖ∑„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÁé∞ÊúâÁöÑLLM‰ª£ÁêÜÂú®Ê≤°Êúâ‰∫∫Á±ªÊèêÁ§∫ÁöÑÊÉÖÂÜµ‰∏ãÔºåÊó†Ê≥ïÊàêÂäüÂÆåÊàêÂ§ßÂ§öÊï∞Á†îÁ©∂Êâ©Â±ï‰ªªÂä°„ÄÇÂ∞ΩÁÆ°Âú®Êèê‰æõÈ¢ùÂ§ñÁöÑ‰∫∫Á±ªÊèêÁ§∫ÂêéÔºåÊàêÂäüÁéáÊúâÊâÄÊèêÈ´òÔºå‰ΩÜ‰ªçÁÑ∂‰Ωé‰∫é40%„ÄÇËøôË°®ÊòéÂΩìÂâçÁöÑ‰ª£ÁêÜÂú®Â§ÑÁêÜÁé∞ÂÆûÁ†îÁ©∂Êâ©Â±ï‰ªªÂä°Êó∂Ôºå‰ªçÁÑ∂ÈúÄË¶ÅÂ§ßÈáèÁöÑ‰∫∫Á±ªÊåáÂØº„ÄÇ","title":"ËØÑ‰º∞LLM‰ª£ÁêÜÁöÑÁ†îÁ©∂Êâ©Â±ïËÉΩÂäõ"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='RExBenchÊòØ‰∏Ä‰∏™ËØÑ‰º∞Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâ‰ª£ÁêÜÂú®Ëá™‰∏ªÂÆûÁé∞Á†îÁ©∂Êâ©Â±ïËÉΩÂäõÁöÑÂü∫ÂáÜÂ∑•ÂÖ∑„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÁé∞ÊúâÁöÑLLM‰ª£ÁêÜÂú®Ê≤°Êúâ‰∫∫Á±ªÊèêÁ§∫ÁöÑÊÉÖÂÜµ‰∏ãÔºåÊó†Ê≥ïÊàêÂäüÂÆåÊàêÂ§ßÂ§öÊï∞Á†îÁ©∂Êâ©Â±ï‰ªªÂä°„ÄÇÂ∞ΩÁÆ°Âú®Êèê‰æõÈ¢ùÂ§ñÁöÑ‰∫∫Á±ªÊèêÁ§∫ÂêéÔºåÊàêÂäüÁéáÊúâÊâÄÊèêÈ´òÔºå‰ΩÜ‰ªçÁÑ∂‰Ωé‰∫é40%„ÄÇËøôË°®ÊòéÂΩìÂâçÁöÑ‰ª£ÁêÜÂú®Â§ÑÁêÜÁé∞ÂÆûÁ†îÁ©∂Êâ©Â±ï‰ªªÂä°Êó∂Ôºå‰ªçÁÑ∂ÈúÄË¶ÅÂ§ßÈáèÁöÑ‰∫∫Á±ªÊåáÂØº„ÄÇ', title='ËØÑ‰º∞LLM‰ª£ÁêÜÁöÑÁ†îÁ©∂Êâ©Â±ïËÉΩÂäõ'))
[01.07.2025 15:13] Using data from previous issue: {"categories": ["#dataset", "#science", "#training", "#multimodal", "#benchmark", "#open_source"], "emoji": "üèôÔ∏è", "ru": {"title": "UrbanLLaVA: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –∞–Ω–∞–ª–∏–∑–µ –≥–æ—Ä–æ–¥—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö —Å –ø–æ–º–æ—â—å—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –ò–ò", "desc": "UrbanLLaVA - —ç—Ç–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –±–æ–ª—å—à–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –¥–ª—è –æ–±—Ä
[01.07.2025 15:13] Using data from previous issue: {"categories": ["#benchmark", "#multimodal", "#reasoning"], "emoji": "üß†", "ru": {"title": "MARBLE: –í—ã–∑–æ–≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–º —è–∑—ã–∫–æ–≤—ã–º –º–æ–¥–µ–ª—è–º –≤ —Å–ª–æ–∂–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è—Ö", "desc": "MARBLE - —ç—Ç–æ —Å–ª–æ–∂–Ω—ã–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä—ã–π –≤—ã—è–≤–ª—è–µ—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö 
[01.07.2025 15:13] Using data from previous issue: {"categories": ["#multilingual", "#dataset", "#low_resource", "#benchmark", "#open_source", "#training"], "emoji": "üîß", "ru": {"title": "–ú–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –ò–ò: —Ç–æ—á–Ω—ã–µ –≤—ã–∑–æ–≤—ã —Ñ—É–Ω–∫—Ü–∏–π –Ω–∞ –ª—é–±–æ–º —è–∑—ã–∫–µ", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ
[01.07.2025 15:13] Using data from previous issue: {"categories": ["#inference", "#training", "#optimization", "#small_models"], "emoji": "‚úÇÔ∏è", "ru": {"title": "–£—Å–∫–æ—Ä–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ –ø—É—Ç–µ–º –æ–±—Ä–µ–∑–∫–∏ —Å–ª–æ–≤–∞—Ä—è", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Ç–µ—Ö–Ω–∏–∫—É VocabTrim, –∫–æ—Ç–æ—Ä–∞—è —É–ª—É—á—à–∞–µ—Ç —Å–ø–µ–∫—É–ª—è—Ç–∏–≤–Ω–æ–µ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥—Ä–∞—Ñ—Ç–µ—Ä–∞ –ø—É—Ç–µ–º —É–º–µ–Ω—å—à–µ–Ω–∏—è —Å–ª–æ–≤–∞—Ä
[01.07.2025 15:13] Using data from previous issue: {"categories": ["#optimization", "#science", "#open_source", "#robotics", "#video", "#games", "#3d"], "emoji": "ü§ñ", "ru": {"title": "RoboScape: —Ñ–∏–∑–∏—á–µ—Å–∫–∏ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∏–¥–µ–æ –¥–ª—è –ø—Ä–æ–¥–≤–∏–∂–µ–Ω–∏—è –≤–æ–ø–ª–æ—â–µ–Ω–Ω–æ–≥–æ –ò–ò", "desc": "RoboScape - —ç—Ç–æ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ñ–∏–∑–∏—á–µ—Å–∫–∏-–∏–Ω—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–∏—Ä–æ–≤–∞—è –º–æ–¥–µ–ª—å –¥–ª
[01.07.2025 15:13] Using data from previous issue: {"categories": ["#hallucinations", "#cv", "#data", "#inference", "#diffusion"], "emoji": "üîç", "ru": {"title": "–£–ª—É—á—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –º–µ—Ç–∞–ª–∏–Ω–∑ –±–µ–∑ –º–∞—Å—à—Ç–∞–±–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤", "desc": "–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å Degradation-Modeled Multipath Diffusion –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –ø–æ–ª—É—á–∞–µ–º—ã—Ö 
[01.07.2025 15:13] Using data from previous issue: {"categories": ["#dataset", "#games", "#multimodal", "#benchmark", "#reasoning", "#audio"], "emoji": "üé¨", "ru": {"title": "–î—É–º–∞–π –∫–∞–∫ –∑–≤—É–∫–æ—Ä–µ–∂–∏—Å—Å–µ—Ä: –ò–ò —Å–æ–∑–¥–∞–µ—Ç –∞—É–¥–∏–æ –¥–ª—è –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π", "desc": "ThinkSound - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∞—É–¥–∏–æ –∏–∑ –≤–∏–¥–µ–æ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞
[01.07.2025 15:13] Using data from previous issue: {"categories": ["#multilingual", "#benchmark", "#optimization", "#dataset", "#training", "#machine_translation"], "emoji": "üóº", "ru": {"title": "Tower+: –Ø–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∏–µ –ø–µ—Ä–µ–≤–æ–¥ –∏ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–µ –∑–∞–¥–∞—á–∏", "desc": "Tower+ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –Ω–∞–±–æ—Ä —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –¥–æ—Å—Ç–∏–≥–∞—é—â–∏—Ö –≤—ã—Å–æ–∫–æ–π –ø—Ä–æ–∏–∑–≤
[01.07.2025 15:13] Renaming data file.
[01.07.2025 15:13] Renaming previous data. hf_papers.json to ./d/2025-07-01.json
[01.07.2025 15:13] Saving new data file.
[01.07.2025 15:13] Generating page.
[01.07.2025 15:13] Renaming previous page.
[01.07.2025 15:13] Renaming previous data. index.html to ./d/2025-07-01.html
[01.07.2025 15:13] Writing result.
[01.07.2025 15:13] Renaming log file.
[01.07.2025 15:13] Renaming previous data. log.txt to ./logs/2025-07-01_last_log.txt
