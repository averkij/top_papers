[02.01.2026 14:23] Read previous papers.
[02.01.2026 14:23] Generating top page (month).
[02.01.2026 14:23] Writing top page (month).
[02.01.2026 15:24] Read previous papers.
[02.01.2026 15:24] Get feed.
[02.01.2026 15:24] Get page data from previous paper. URL: https://huggingface.co/papers/2512.23959
[02.01.2026 15:24] Get page data from previous paper. URL: https://huggingface.co/papers/2512.24617
[02.01.2026 15:24] Get page data from previous paper. URL: https://huggingface.co/papers/2512.24165
[02.01.2026 15:24] Get page data from previous paper. URL: https://huggingface.co/papers/2512.22630
[02.01.2026 15:24] Extract page data from URL. URL: https://huggingface.co/papers/2512.24766
[02.01.2026 15:24] Extract page data from URL. URL: https://huggingface.co/papers/2512.24724
[02.01.2026 15:24] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[02.01.2026 15:24] No deleted papers detected.
[02.01.2026 15:24] Downloading and parsing papers (pdf, html). Total: 6.
[02.01.2026 15:24] Downloading and parsing paper https://huggingface.co/papers/2512.23959.
[02.01.2026 15:24] Extra JSON file exists (./assets/json/2512.23959.json), skip PDF parsing.
[02.01.2026 15:24] Paper image links file exists (./assets/img_data/2512.23959.json), skip HTML parsing.
[02.01.2026 15:24] Success.
[02.01.2026 15:24] Downloading and parsing paper https://huggingface.co/papers/2512.24617.
[02.01.2026 15:24] Extra JSON file exists (./assets/json/2512.24617.json), skip PDF parsing.
[02.01.2026 15:24] Paper image links file exists (./assets/img_data/2512.24617.json), skip HTML parsing.
[02.01.2026 15:24] Success.
[02.01.2026 15:24] Downloading and parsing paper https://huggingface.co/papers/2512.24165.
[02.01.2026 15:24] Extra JSON file exists (./assets/json/2512.24165.json), skip PDF parsing.
[02.01.2026 15:24] Paper image links file exists (./assets/img_data/2512.24165.json), skip HTML parsing.
[02.01.2026 15:24] Success.
[02.01.2026 15:24] Downloading and parsing paper https://huggingface.co/papers/2512.22630.
[02.01.2026 15:24] Extra JSON file exists (./assets/json/2512.22630.json), skip PDF parsing.
[02.01.2026 15:24] Paper image links file exists (./assets/img_data/2512.22630.json), skip HTML parsing.
[02.01.2026 15:24] Success.
[02.01.2026 15:24] Downloading and parsing paper https://huggingface.co/papers/2512.24766.
[02.01.2026 15:24] Downloading paper 2512.24766 from https://arxiv.org/pdf/2512.24766v1...
[02.01.2026 15:24] Extracting affiliations from text.
[02.01.2026 15:24] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Dream2Flow: Bridging Video Generation and Open-World Manipulation with 3D Object Flow Karthik Dharmarajan, Wenlong Huang, Jiajun Wu, Li Fei-Fei*, Ruohan Zhang* Stanford University 5 2 0 2 1 3 ] . [ 1 6 6 7 4 2 . 2 1 5 2 : r Fig. 1: Dream2Flow leverages off-the-shelf video generation models to produce videos of the task being performed in the same scene of the robot. Dream2Flow then extracts 3D object flow from the motion in the video, allowing for downstream planning and execution with robot across wide variety of tasks. Abstract Generative video modeling has emerged as compelling tool to zero-shot reason about plausible physical interactions for open-world manipulation. Yet, it remains challenge to translate such human-led motions into the low-level actions demanded by robotic systems. We observe that given an initial image and task instruction, these models excel at synthesizing sensible object motions. Thus, we introduce Dream2Flow, framework that bridges video generation and robotic control through 3D object flow as an intermediate representation. Our method reconstructs 3D object motions from generated videos and formulates manipulation as object trajectory tracking. By separating the state changes from the actuators that realize those changes, Dream2Flow overcomes the embodiment gap and enables zero-shot guidance from pre-trained video models to manipulate objects of diverse categoriesincluding rigid, articulated, deformable, and granular. Through trajectory optimization or reinforcement learning, Dream2Flow converts reconstructed 3D object flow into executable low-level commands without task-specific demonstrations. Simulation and real-world experiments highlight 3D object flow as general and scalable interface for adapting video generation models to open-world robotic manipulation. Videos and visualizations are available at https://dream2flow.github.io/. I. INTRODUCTION Robotic manipulation in the open world could greatly benefit from visual world models tha"
[02.01.2026 15:24] Response: ```python
["Stanford University"]
```
[02.01.2026 15:24] Deleting PDF ./assets/pdf/2512.24766.pdf.
[02.01.2026 15:24] Success.
[02.01.2026 15:24] Downloading and parsing paper https://huggingface.co/papers/2512.24724.
[02.01.2026 15:24] Downloading paper 2512.24724 from https://arxiv.org/pdf/2512.24724v1...
[02.01.2026 15:24] Extracting affiliations from text.
[02.01.2026 15:24] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"FlowBlending: Stage-Aware Multi-Model Sampling for Fast and High-Fidelity Video Generation Jibin Song, Mingi Kwon, Jaeseok Jeong, Youngjung Uh* Yonsei University {sjbpsh1, kwonmingi, jete jeong, yj.uh}@yonsei.ac.kr 5 2 0 2 1 3 ] . [ 1 4 2 7 4 2 . 2 1 5 2 : r Figure 1. Overview of FlowBlending. The videos in each column are generated from the same initial noise and text prompt, but with different model allocation strategies. FlowBlending assigns large model to process early and late denoising stagesestablishing global structure and refining details, respectively and assigns small model to process intermediate denoising stages, where velocity divergence between the two models is minimal. This approach preserves visual fidelity of the large model while reducing computation. "
[02.01.2026 15:24] Response: ```python
["Yonsei University"]
```
[02.01.2026 15:24] Deleting PDF ./assets/pdf/2512.24724.pdf.
[02.01.2026 15:24] Success.
[02.01.2026 15:24] Enriching papers with extra data.
[02.01.2026 15:24] ********************************************************************************
[02.01.2026 15:24] Abstract 0. Multi-step retrieval-augmented generation (RAG) has become a widely adopted strategy for enhancing large language models (LLMs) on tasks that demand global comprehension and intensive reasoning. Many RAG systems incorporate a working memory module to consolidate retrieved information. However, exist...
[02.01.2026 15:24] ********************************************************************************
[02.01.2026 15:24] Abstract 1. Large Language Models (LLMs) apply uniform computation to all tokens, despite language exhibiting highly non-uniform information density. This token-uniform regime wastes capacity on locally predictable spans while under-allocating computation to semantically critical transitions. We propose Dynamic...
[02.01.2026 15:24] ********************************************************************************
[02.01.2026 15:24] Abstract 2. While recent Multimodal Large Language Models (MLLMs) have attained significant strides in multimodal reasoning, their reasoning processes remain predominantly text-centric, leading to suboptimal performance in complex long-horizon, vision-centric tasks. In this paper, we establish a novel Generativ...
[02.01.2026 15:24] ********************************************************************************
[02.01.2026 15:24] Abstract 3. Diffusion models offer appealing properties for language generation, such as parallel decoding and iterative refinement, but the discrete and highly structured nature of text challenges the direct application of diffusion principles. In this paper, we revisit diffusion language modeling from the vie...
[02.01.2026 15:24] ********************************************************************************
[02.01.2026 15:24] Abstract 4. Generative video modeling has emerged as a compelling tool to zero-shot reason about plausible physical interactions for open-world manipulation. Yet, it remains a challenge to translate such human-led motions into the low-level actions demanded by robotic systems. We observe that given an initial i...
[02.01.2026 15:24] ********************************************************************************
[02.01.2026 15:24] Abstract 5. In this work, we show that the impact of model capacity varies across timesteps: it is crucial for the early and late stages but largely negligible during the intermediate stage. Accordingly, we propose FlowBlending, a stage-aware multi-model sampling strategy that employs a large model and a small ...
[02.01.2026 15:24] Read previous papers.
[02.01.2026 15:24] Generating reviews via LLM API.
[02.01.2026 15:24] Using data from previous issue: {"categories": ["#graphs", "#training", "#rag", "#reasoning", "#long_context"], "emoji": "ğŸ•¸ï¸", "ru": {"title": "ĞÑ‚ Ñ…Ñ€Ğ°Ğ½Ğ¸Ğ»Ğ¸Ñ‰Ğ° Ğº ÑĞµÑ‚ĞµĞ²Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸: Ğ³Ğ¸Ğ¿ĞµÑ€Ğ³Ñ€Ğ°Ñ„Ñ‹ Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ", "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ HGMem Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ retrieval-augmented generati
[02.01.2026 15:24] Using data from previous issue: {"categories": ["#training", "#architecture"], "emoji": "ğŸ§ ", "ru": {"title": "Ğ¡Ğ¼Ğ°Ñ€Ñ‚-ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ: ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ‚Ñ‹ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹", "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ‚Ğ¾Ğ² (DLCM), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ²ÑĞµÑ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ñ Ğ¾Ğ´Ğ¸Ğ½Ğ°ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½ĞµÑ€Ğ°
[02.01.2026 15:24] Using data from previous issue: {"categories": ["#cv", "#reasoning", "#diffusion", "#architecture", "#multimodal"], "emoji": "ğŸ¨", "ru": {"title": "Ğ”Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ: Ğ¾Ñ‚ Ñ‚ĞµĞºÑÑ‚Ğ° Ğº Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ", "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»Ñ
[02.01.2026 15:24] Using data from previous issue: {"categories": ["#training", "#architecture", "#diffusion"], "emoji": "ğŸ”„", "ru": {"title": "Ğ”Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ñ Ğ´Ğ»Ñ Ñ‚ĞµĞºÑÑ‚Ğ°: Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ñ Ğ¿Ñ€Ğ¸Ñ€Ğ¾Ğ´Ğ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ°", "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚ÑÑ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾Ğ±Ğ»Ğ°Ğ´Ğ°ÑÑ‚ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ğ¼Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğ¼Ğ¸ ĞºĞ°Ğº Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸
[02.01.2026 15:24] Querying the API.
[02.01.2026 15:24] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Generative video modeling has emerged as a compelling tool to zero-shot reason about plausible physical interactions for open-world manipulation. Yet, it remains a challenge to translate such human-led motions into the low-level actions demanded by robotic systems. We observe that given an initial image and task instruction, these models excel at synthesizing sensible object motions. Thus, we introduce Dream2Flow, a framework that bridges video generation and robotic control through 3D object flow as an intermediate representation. Our method reconstructs 3D object motions from generated videos and formulates manipulation as object trajectory tracking. By separating the state changes from the actuators that realize those changes, Dream2Flow overcomes the embodiment gap and enables zero-shot guidance from pre-trained video models to manipulate objects of diverse categories-including rigid, articulated, deformable, and granular. Through trajectory optimization or reinforcement learning, Dream2Flow converts reconstructed 3D object flow into executable low-level commands without task-specific demonstrations. Simulation and real-world experiments highlight 3D object flow as a general and scalable interface for adapting video generation models to open-world robotic manipulation. Videos and visualizations are available at https://dream2flow.github.io/.
[02.01.2026 15:24] Response: ```json
{
  "desc": "Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Dream2Flow â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ¼-Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ¾Ğ¼ Ğ±ĞµĞ· Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ° Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² ĞºĞ°Ğº Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ ÑĞ²ÑĞ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ½Ğ¸Ğ·ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğ¼Ğ¸ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ°Ğ¼Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ¼. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ 3D-Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸Ğ· ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ñ ĞºĞ°Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ ÑÑ‚Ğ¸Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹. Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¾Ñ€Ñ„Ğ¾Ğ»Ğ¾Ğ³Ğ¸ĞµĞ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ² â€” Ğ¶Ñ‘ÑÑ‚ĞºĞ¸Ğ¼Ğ¸, ÑĞ¾Ñ‡Ğ»ĞµĞ½Ñ‘Ğ½Ğ½Ñ‹Ğ¼Ğ¸, Ğ´ĞµÑ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ¸ ÑÑ‹Ğ¿ÑƒÑ‡Ğ¸Ğ¼Ğ¸ â€” Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ¸Ğ»Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼.",
  "emoji": "ğŸ¤–",
  "title": "ĞÑ‚ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğº Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞµ Ñ‡ĞµÑ€ĞµĞ· 3D-Ğ¿Ğ¾Ñ‚Ğ¾Ğº Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ²"
}
```
[02.01.2026 15:24] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Generative video modeling has emerged as a compelling tool to zero-shot reason about plausible physical interactions for open-world manipulation. Yet, it remains a challenge to translate such human-led motions into the low-level actions demanded by robotic systems. We observe that given an initial image and task instruction, these models excel at synthesizing sensible object motions. Thus, we introduce Dream2Flow, a framework that bridges video generation and robotic control through 3D object flow as an intermediate representation. Our method reconstructs 3D object motions from generated videos and formulates manipulation as object trajectory tracking. By separating the state changes from the actuators that realize those changes, Dream2Flow overcomes the embodiment gap and enables zero-shot guidance from pre-trained video models to manipulate objects of diverse categories-including rigid, articulated, deformable, and granular. Through trajectory optimization or reinforcement learning, Dream2Flow converts reconstructed 3D object flow into executable low-level commands without task-specific demonstrations. Simulation and real-world experiments highlight 3D object flow as a general and scalable interface for adapting video generation models to open-world robotic manipulation. Videos and visualizations are available at https://dream2flow.github.io/."

[02.01.2026 15:24] Response: ```python
['VIDEO', 'ROBOTICS', '3D', 'RL']
```
[02.01.2026 15:24] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Generative video modeling has emerged as a compelling tool to zero-shot reason about plausible physical interactions for open-world manipulation. Yet, it remains a challenge to translate such human-led motions into the low-level actions demanded by robotic systems. We observe that given an initial image and task instruction, these models excel at synthesizing sensible object motions. Thus, we introduce Dream2Flow, a framework that bridges video generation and robotic control through 3D object flow as an intermediate representation. Our method reconstructs 3D object motions from generated videos and formulates manipulation as object trajectory tracking. By separating the state changes from the actuators that realize those changes, Dream2Flow overcomes the embodiment gap and enables zero-shot guidance from pre-trained video models to manipulate objects of diverse categories-including rigid, articulated, deformable, and granular. Through trajectory optimization or reinforcement learning, Dream2Flow converts reconstructed 3D object flow into executable low-level commands without task-specific demonstrations. Simulation and real-world experiments highlight 3D object flow as a general and scalable interface for adapting video generation models to open-world robotic manipulation. Videos and visualizations are available at https://dream2flow.github.io/."

[02.01.2026 15:24] Response: ```python
['REASONING', 'OPEN_SOURCE']
```
[02.01.2026 15:24] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents Dream2Flow, a novel framework that connects generative video modeling with robotic control. It focuses on translating high-level human motions into low-level robotic actions by using 3D object flow as a key representation. The framework allows robots to manipulate various object types by tracking their trajectories, effectively bridging the gap between video generation and physical manipulation. Dream2Flow enables zero-shot learning, meaning it can adapt to new tasks without needing specific training for each one, showcasing its versatility in real-world applications.","title":"Bridging Video Generation and Robotic Control with Dream2Flow"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper presents Dream2Flow, a novel framework that connects generative video modeling with robotic control. It focuses on translating high-level human motions into low-level robotic actions by using 3D object flow as a key representation. The framework allows robots to manipulate various object types by tracking their trajectories, effectively bridging the gap between video generation and physical manipulation. Dream2Flow enables zero-shot learning, meaning it can adapt to new tasks without needing specific training for each one, showcasing its versatility in real-world applications.', title='Bridging Video Generation and Robotic Control with Dream2Flow'))
[02.01.2026 15:24] Response: ParsedChatCompletionMessage[Article](content='{"desc":"ç”Ÿæˆè§†é¢‘å»ºæ¨¡å·²æˆä¸ºä¸€ç§æœ‰æ•ˆå·¥å…·ï¼Œå¯ä»¥åœ¨å¼€æ”¾ä¸–ç•Œä¸­è¿›è¡Œåˆç†çš„ç‰©ç†äº¤äº’æ¨ç†ã€‚ç„¶è€Œï¼Œå°†äººç±»çš„åŠ¨ä½œè½¬åŒ–ä¸ºæœºå™¨äººç³»ç»Ÿæ‰€éœ€çš„ä½çº§åŠ¨ä½œä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†Dream2Flowæ¡†æ¶ï¼Œé€šè¿‡3Dç‰©ä½“æµä½œä¸ºä¸­é—´è¡¨ç¤ºï¼Œè¿æ¥è§†é¢‘ç”Ÿæˆå’Œæœºå™¨äººæ§åˆ¶ã€‚è¯¥æ–¹æ³•ä»ç”Ÿæˆçš„è§†é¢‘ä¸­é‡å»º3Dç‰©ä½“è¿åŠ¨ï¼Œå¹¶å°†æ“ä½œå½¢å¼åŒ–ä¸ºç‰©ä½“è½¨è¿¹è·Ÿè¸ªï¼Œä»è€Œå®ç°äº†é›¶æ ·æœ¬æŒ‡å¯¼ã€‚","title":"Dream2Flowï¼šè¿æ¥è§†é¢‘ç”Ÿæˆä¸æœºå™¨äººæ§åˆ¶çš„æ¡¥æ¢"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='ç”Ÿæˆè§†é¢‘å»ºæ¨¡å·²æˆä¸ºä¸€ç§æœ‰æ•ˆå·¥å…·ï¼Œå¯ä»¥åœ¨å¼€æ”¾ä¸–ç•Œä¸­è¿›è¡Œåˆç†çš„ç‰©ç†äº¤äº’æ¨ç†ã€‚ç„¶è€Œï¼Œå°†äººç±»çš„åŠ¨ä½œè½¬åŒ–ä¸ºæœºå™¨äººç³»ç»Ÿæ‰€éœ€çš„ä½çº§åŠ¨ä½œä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†Dream2Flowæ¡†æ¶ï¼Œé€šè¿‡3Dç‰©ä½“æµä½œä¸ºä¸­é—´è¡¨ç¤ºï¼Œè¿æ¥è§†é¢‘ç”Ÿæˆå’Œæœºå™¨äººæ§åˆ¶ã€‚è¯¥æ–¹æ³•ä»ç”Ÿæˆçš„è§†é¢‘ä¸­é‡å»º3Dç‰©ä½“è¿åŠ¨ï¼Œå¹¶å°†æ“ä½œå½¢å¼åŒ–ä¸ºç‰©ä½“è½¨è¿¹è·Ÿè¸ªï¼Œä»è€Œå®ç°äº†é›¶æ ·æœ¬æŒ‡å¯¼ã€‚', title='Dream2Flowï¼šè¿æ¥è§†é¢‘ç”Ÿæˆä¸æœºå™¨äººæ§åˆ¶çš„æ¡¥æ¢'))
[02.01.2026 15:24] Querying the API.
[02.01.2026 15:24] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

In this work, we show that the impact of model capacity varies across timesteps: it is crucial for the early and late stages but largely negligible during the intermediate stage. Accordingly, we propose FlowBlending, a stage-aware multi-model sampling strategy that employs a large model and a small model at capacity-sensitive stages and intermediate stages, respectively. We further introduce simple criteria to choose stage boundaries and provide a velocity-divergence analysis as an effective proxy for identifying capacity-sensitive regions. Across LTX-Video (2B/13B) and WAN 2.1 (1.3B/14B), FlowBlending achieves up to 1.65x faster inference with 57.35% fewer FLOPs, while maintaining the visual fidelity, temporal coherence, and semantic alignment of the large models. FlowBlending is also compatible with existing sampling-acceleration techniques, enabling up to 2x additional speedup. Project page is available at: https://jibin86.github.io/flowblending_project_page.
[02.01.2026 15:25] Response: ```json
{
  "desc": "Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ¼Ğ¾Ñ‰Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑÑ‚Ğ°Ğ¿Ñ‹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ğ° Ğ½Ğ° Ñ€Ğ°Ğ½Ğ½Ğ¸Ñ… Ğ¸ Ğ¿Ğ¾Ğ·Ğ´Ğ½Ğ¸Ñ… ÑÑ‚Ğ°Ğ´Ğ¸ÑÑ…, Ğ° Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ğ¿Ğ°Ñ… Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. ĞĞ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ FlowBlending â€” ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞµĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ¸ Ğ¼Ğ°Ğ»ĞµĞ½ÑŒĞºĞ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ÑÑ‚Ğ°Ğ¿Ğ° Ğº ĞµĞ¼ĞºĞ¾ÑÑ‚Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ´Ğ¾ 1.65x Ğ¸ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ½Ğ° 57% Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾.",
  "emoji": "âš¡",
  "title": "ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ¼ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸"
}
```
[02.01.2026 15:25] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"In this work, we show that the impact of model capacity varies across timesteps: it is crucial for the early and late stages but largely negligible during the intermediate stage. Accordingly, we propose FlowBlending, a stage-aware multi-model sampling strategy that employs a large model and a small model at capacity-sensitive stages and intermediate stages, respectively. We further introduce simple criteria to choose stage boundaries and provide a velocity-divergence analysis as an effective proxy for identifying capacity-sensitive regions. Across LTX-Video (2B/13B) and WAN 2.1 (1.3B/14B), FlowBlending achieves up to 1.65x faster inference with 57.35% fewer FLOPs, while maintaining the visual fidelity, temporal coherence, and semantic alignment of the large models. FlowBlending is also compatible with existing sampling-acceleration techniques, enabling up to 2x additional speedup. Project page is available at: https://jibin86.github.io/flowblending_project_page."

[02.01.2026 15:25] Response: ```python
["VIDEO", "INFERENCE", "SMALL_MODELS"]
```
[02.01.2026 15:25] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"In this work, we show that the impact of model capacity varies across timesteps: it is crucial for the early and late stages but largely negligible during the intermediate stage. Accordingly, we propose FlowBlending, a stage-aware multi-model sampling strategy that employs a large model and a small model at capacity-sensitive stages and intermediate stages, respectively. We further introduce simple criteria to choose stage boundaries and provide a velocity-divergence analysis as an effective proxy for identifying capacity-sensitive regions. Across LTX-Video (2B/13B) and WAN 2.1 (1.3B/14B), FlowBlending achieves up to 1.65x faster inference with 57.35% fewer FLOPs, while maintaining the visual fidelity, temporal coherence, and semantic alignment of the large models. FlowBlending is also compatible with existing sampling-acceleration techniques, enabling up to 2x additional speedup. Project page is available at: https://jibin86.github.io/flowblending_project_page."

[02.01.2026 15:25] Response: ```python
['OPTIMIZATION']
```

The paper focuses on optimizing inference efficiency through a stage-aware multi-model sampling strategy that reduces computational cost (FLOPs) and inference time while maintaining quality. This is directly related to training and inference optimization methods.
[02.01.2026 15:25] Error. Failed to parse JSON from LLM. ["OPTIMIZATION"]


The paper focuses on optimizing inference efficiency through a stage-aware multi-model sampling strategy that reduces computational cost (FLOPs) and inference time while maintaining quality. This is directly related to training and inference optimization methods.
[02.01.2026 15:25] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper investigates how the capacity of machine learning models affects their performance at different stages of processing. It finds that model capacity is important at the beginning and end of the process, but not as much in the middle. To address this, the authors introduce FlowBlending, a method that uses a large model for critical stages and a smaller model for less critical ones. This approach significantly speeds up inference and reduces computational load while preserving the quality of the output.","title":"Optimizing Model Capacity for Efficient Inference with FlowBlending"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper investigates how the capacity of machine learning models affects their performance at different stages of processing. It finds that model capacity is important at the beginning and end of the process, but not as much in the middle. To address this, the authors introduce FlowBlending, a method that uses a large model for critical stages and a smaller model for less critical ones. This approach significantly speeds up inference and reduces computational load while preserving the quality of the output.', title='Optimizing Model Capacity for Efficient Inference with FlowBlending'))
[02.01.2026 15:25] Response: ParsedChatCompletionMessage[Article](content='{"desc":"æœ¬ç ”ç©¶è¡¨æ˜ï¼Œæ¨¡å‹å®¹é‡åœ¨ä¸åŒæ—¶é—´æ­¥çš„å½±å“æ˜¯ä¸åŒçš„ï¼šåœ¨æ—©æœŸå’Œæ™šæœŸé˜¶æ®µè‡³å…³é‡è¦ï¼Œè€Œåœ¨ä¸­é—´é˜¶æ®µåˆ™å‡ ä¹å¯ä»¥å¿½ç•¥ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºFlowBlendingçš„é˜¶æ®µæ„ŸçŸ¥å¤šæ¨¡å‹é‡‡æ ·ç­–ç•¥ï¼Œåœ¨å¯¹å®¹é‡æ•æ„Ÿçš„é˜¶æ®µä½¿ç”¨å¤§æ¨¡å‹ï¼Œè€Œåœ¨ä¸­é—´é˜¶æ®µä½¿ç”¨å°æ¨¡å‹ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ç®€å•çš„æ ‡å‡†æ¥é€‰æ‹©é˜¶æ®µè¾¹ç•Œï¼Œå¹¶æä¾›äº†é€Ÿåº¦-å‘æ•£åˆ†æä½œä¸ºè¯†åˆ«å®¹é‡æ•æ„ŸåŒºåŸŸçš„æœ‰æ•ˆä»£ç†ã€‚é€šè¿‡åœ¨LTX-Videoå’ŒWAN 2.1ä¸Šçš„å®éªŒï¼ŒFlowBlendingå®ç°äº†é«˜è¾¾1.65å€çš„æ¨ç†é€Ÿåº¦æå‡ï¼ŒåŒæ—¶å‡å°‘äº†57.35%çš„FLOPsï¼Œä¿æŒäº†å¤§æ¨¡å‹çš„è§†è§‰ä¿çœŸåº¦ã€æ—¶é—´ä¸€è‡´æ€§å’Œè¯­ä¹‰å¯¹é½ã€‚","title":"é˜¶æ®µæ„ŸçŸ¥çš„æ¨¡å‹é‡‡æ ·ç­–ç•¥ï¼šFlowBlending"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='æœ¬ç ”ç©¶è¡¨æ˜ï¼Œæ¨¡å‹å®¹é‡åœ¨ä¸åŒæ—¶é—´æ­¥çš„å½±å“æ˜¯ä¸åŒçš„ï¼šåœ¨æ—©æœŸå’Œæ™šæœŸé˜¶æ®µè‡³å…³é‡è¦ï¼Œè€Œåœ¨ä¸­é—´é˜¶æ®µåˆ™å‡ ä¹å¯ä»¥å¿½ç•¥ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºFlowBlendingçš„é˜¶æ®µæ„ŸçŸ¥å¤šæ¨¡å‹é‡‡æ ·ç­–ç•¥ï¼Œåœ¨å¯¹å®¹é‡æ•æ„Ÿçš„é˜¶æ®µä½¿ç”¨å¤§æ¨¡å‹ï¼Œè€Œåœ¨ä¸­é—´é˜¶æ®µä½¿ç”¨å°æ¨¡å‹ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ç®€å•çš„æ ‡å‡†æ¥é€‰æ‹©é˜¶æ®µè¾¹ç•Œï¼Œå¹¶æä¾›äº†é€Ÿåº¦-å‘æ•£åˆ†æä½œä¸ºè¯†åˆ«å®¹é‡æ•æ„ŸåŒºåŸŸçš„æœ‰æ•ˆä»£ç†ã€‚é€šè¿‡åœ¨LTX-Videoå’ŒWAN 2.1ä¸Šçš„å®éªŒï¼ŒFlowBlendingå®ç°äº†é«˜è¾¾1.65å€çš„æ¨ç†é€Ÿåº¦æå‡ï¼ŒåŒæ—¶å‡å°‘äº†57.35%çš„FLOPsï¼Œä¿æŒäº†å¤§æ¨¡å‹çš„è§†è§‰ä¿çœŸåº¦ã€æ—¶é—´ä¸€è‡´æ€§å’Œè¯­ä¹‰å¯¹é½ã€‚', title='é˜¶æ®µæ„ŸçŸ¥çš„æ¨¡å‹é‡‡æ ·ç­–ç•¥ï¼šFlowBlending'))
[02.01.2026 15:25] Renaming data file.
[02.01.2026 15:25] Renaming previous data. hf_papers.json to ./d/2026-01-02.json
[02.01.2026 15:25] Saving new data file.
[02.01.2026 15:25] Generating page.
[02.01.2026 15:25] Renaming previous page.
[02.01.2026 15:25] Renaming previous data. index.html to ./d/2026-01-02.html
[02.01.2026 15:25] Writing result.
[02.01.2026 15:25] Renaming log file.
[02.01.2026 15:25] Renaming previous data. log.txt to ./logs/2026-01-02_last_log.txt
