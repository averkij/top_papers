[10.11.2024 10:12] Read previous papers.
[10.11.2024 10:12] Generating top page (month).
[10.11.2024 10:12] Writing top page (month).
[10.11.2024 12:20] Read previous papers.
[10.11.2024 12:20] Get feed.
[10.11.2024 12:20] Get page data from previous paper. URL: https://huggingface.co/papers/2411.04905
[10.11.2024 12:20] Get page data from previous paper. URL: https://huggingface.co/papers/2411.05003
[10.11.2024 12:20] Get page data from previous paper. URL: https://huggingface.co/papers/2411.04965
[10.11.2024 12:20] Get page data from previous paper. URL: https://huggingface.co/papers/2411.04996
[10.11.2024 12:20] Get page data from previous paper. URL: https://huggingface.co/papers/2411.04928
[10.11.2024 12:20] Get page data from previous paper. URL: https://huggingface.co/papers/2411.04709
[10.11.2024 12:20] Get page data from previous paper. URL: https://huggingface.co/papers/2411.04952
[10.11.2024 12:20] Get page data from previous paper. URL: https://huggingface.co/papers/2411.04496
[10.11.2024 12:20] Get page data from previous paper. URL: https://huggingface.co/papers/2411.05000
[10.11.2024 12:20] Get page data from previous paper. URL: https://huggingface.co/papers/2411.04923
[10.11.2024 12:20] Get page data from previous paper. URL: https://huggingface.co/papers/2411.05001
[10.11.2024 12:20] Get page data from previous paper. URL: https://huggingface.co/papers/2411.05007
[10.11.2024 12:20] Get page data from previous paper. URL: https://huggingface.co/papers/2411.04999
[10.11.2024 12:20] Get page data from previous paper. URL: https://huggingface.co/papers/2411.04335
[10.11.2024 12:20] Get page data from previous paper. URL: https://huggingface.co/papers/2411.05005
[10.11.2024 12:20] Get page data from previous paper. URL: https://huggingface.co/papers/2411.04752
[10.11.2024 12:20] Get page data from previous paper. URL: https://huggingface.co/papers/2411.04989
[10.11.2024 12:20] Get page data from previous paper. URL: https://huggingface.co/papers/2411.04075
[10.11.2024 12:20] ********************************************************************************
[10.11.2024 12:20] Abstract 0. Large language models (LLMs) for code have become indispensable in various domains, including code generation, reasoning tasks and agent systems.While open-access code LLMs are increasingly approaching the performance levels of proprietary models, high-quality code LLMs suitable for rigorous scienti...
[10.11.2024 12:20] ********************************************************************************
[10.11.2024 12:20] Abstract 1. Recently, breakthroughs in video modeling have allowed for controllable camera trajectories in generated videos. However, these methods cannot be directly applied to user-provided videos that are not generated by a video model. In this paper, we present ReCapture, a method for generating new videos ...
[10.11.2024 12:20] ********************************************************************************
[10.11.2024 12:20] Abstract 2. Recent research on the 1-bit Large Language Models (LLMs), such as BitNet b1.58, presents a promising direction for reducing the inference cost of LLMs while maintaining their performance. In this work, we introduce BitNet a4.8, enabling 4-bit activations for 1-bit LLMs. BitNet a4.8 employs a hybrid...
[10.11.2024 12:20] ********************************************************************************
[10.11.2024 12:20] Abstract 3. The development of large language models (LLMs) has expanded to multi-modal systems capable of processing text, images, and speech within a unified framework. Training these models demands significantly larger datasets and computational resources compared to text-only LLMs. To address the scaling ch...
[10.11.2024 12:20] ********************************************************************************
[10.11.2024 12:20] Abstract 4. In this paper, we introduce DimensionX, a framework designed to generate photorealistic 3D and 4D scenes from just a single image with video diffusion. Our approach begins with the insight that both the spatial structure of a 3D scene and the temporal evolution of a 4D scene can be effectively repre...
[10.11.2024 12:20] ********************************************************************************
[10.11.2024 12:20] Abstract 5. Video generation models are revolutionizing content creation, with image-to-video models drawing increasing attention due to their enhanced controllability, visual consistency, and practical applications. However, despite their popularity, these models rely on user-provided text and image prompts, a...
[10.11.2024 12:20] ********************************************************************************
[10.11.2024 12:20] Abstract 6. Document visual question answering (DocVQA) pipelines that answer questions from documents have broad applications. Existing methods focus on handling single-page documents with multi-modal language models (MLMs), or rely on text-based retrieval-augmented generation (RAG) that uses text extraction t...
[10.11.2024 12:20] ********************************************************************************
[10.11.2024 12:20] Abstract 7. To increase social bonding with interlocutors, humans naturally acquire the ability to respond appropriately in a given situation by considering which conversational skill is most suitable for the response - a process we call skill-of-mind. For large language model (LLM)-based conversational agents,...
[10.11.2024 12:20] ********************************************************************************
[10.11.2024 12:20] Abstract 8. As the context limits of Large Language Models (LLMs) increase, the range of possible applications and downstream functions broadens. In many real-world tasks, decisions depend on details scattered across collections of often disparate documents containing mostly irrelevant information. Long-context...
[10.11.2024 12:20] ********************************************************************************
[10.11.2024 12:20] Abstract 9. Fine-grained alignment between videos and text is challenging due to complex spatial and temporal dynamics in videos. Existing video-based Large Multimodal Models (LMMs) handle basic conversations but struggle with precise pixel-level grounding in videos. To address this, we introduce VideoGLaMM, a ...
[10.11.2024 12:20] ********************************************************************************
[10.11.2024 12:20] Abstract 10. With the introduction of transformer-based models for vision and language tasks, such as LLaVA and Chameleon, there has been renewed interest in the discrete tokenized representation of images. These models often treat image patches as discrete tokens, analogous to words in natural language, learnin...
[10.11.2024 12:20] ********************************************************************************
[10.11.2024 12:20] Abstract 11. Diffusion models have been proven highly effective at generating high-quality images. However, as these models grow larger, they require significantly more memory and suffer from higher latency, posing substantial challenges for deployment. In this work, we aim to accelerate diffusion models by quan...
[10.11.2024 12:20] ********************************************************************************
[10.11.2024 12:20] Abstract 12. Significant progress has been made in open-vocabulary mobile manipulation, where the goal is for a robot to perform tasks in any environment given a natural language description. However, most current systems assume a static environment, which limits the system's applicability in real-world scenario...
[10.11.2024 12:20] ********************************************************************************
[10.11.2024 12:20] Abstract 13. We present GazeGen, a user interaction system that generates visual content (images and videos) for locations indicated by the user's eye gaze. GazeGen allows intuitive manipulation of visual content by targeting regions of interest with gaze. Using advanced techniques in object detection and genera...
[10.11.2024 12:20] ********************************************************************************
[10.11.2024 12:20] Abstract 14. Beyond high-fidelity image synthesis, diffusion models have recently exhibited promising results in dense visual perception tasks. However, most existing work treats diffusion models as a standalone component for perception tasks, employing them either solely for off-the-shelf data augmentation or a...
[10.11.2024 12:20] ********************************************************************************
[10.11.2024 12:20] Abstract 15. Code-mixing, the integration of lexical and grammatical elements from multiple languages within a single sentence, is a widespread linguistic phenomenon, particularly prevalent in multilingual societies. In India, social media users frequently engage in code-mixed conversations using the Roman scrip...
[10.11.2024 12:20] ********************************************************************************
[10.11.2024 12:20] Abstract 16. Methods for image-to-video generation have achieved impressive, photo-realistic quality. However, adjusting specific elements in generated videos, such as object motion or camera movement, is often a tedious process of trial and error, e.g., involving re-generating videos with different random seeds...
[10.11.2024 12:20] ********************************************************************************
[10.11.2024 12:20] Abstract 17. Existing benchmarks for evaluating foundation models mainly focus on single-document, text-only tasks. However, they often fail to fully capture the complexity of research workflows, which typically involve interpreting non-textual data and gathering information across multiple documents. To address...
[10.11.2024 12:20] Read previous papers.
[10.11.2024 12:20] Generating reviews via LLM API.
[10.11.2024 12:20] Using data from previous issue: {"categories": ["#dataset", "#data", "#training", "#synthetic", "#agents", "#plp"], "emoji": "üßë‚Äçüíª", "ru": {"title": "OpenCoder: –æ—Ç–∫—Ä—ã—Ç–∞—è –∫–Ω–∏–≥–∞ —Ä–µ—Ü–µ–ø—Ç–æ–≤ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ç–æ–ø–æ–≤—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∫–æ–¥–∞", "desc": "OpenCoder - —ç—Ç–æ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –∫–æ–¥–æ–º, —Å–æ–ø–æ—Å—Ç–∞–≤–∏–º–∞—è –ø–æ –ø—Ä–æ–∏–∑–≤–æ
[10.11.2024 12:20] Using data from previous issue: {"categories": ["#video", "#diffusion", "#hallucinations"], "emoji": "üé•", "ru": {"title": "–ü–µ—Ä–µ—Å–Ω–∏–º–∞–µ–º —Ä–µ–∞–ª—å–Ω–æ—Å—Ç—å: –Ω–æ–≤—ã–µ —Ä–∞–∫—É—Ä—Å—ã –¥–ª—è –ª—é–±–æ–≥–æ –≤–∏–¥–µ–æ", "desc": "ReCapture - —ç—Ç–æ –º–µ—Ç–æ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –Ω–æ–≤—ã—Ö –≤–∏–¥–µ–æ —Å –∏–∑–º–µ–Ω–µ–Ω–Ω—ã–º–∏ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—è–º–∏ –∫–∞–º–µ—Ä—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–¥–Ω–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –≤–∏–¥–µ–æ. –û–Ω –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø–µ—Ä–µ–≥–µ–Ω–µ—Ä–∏
[10.11.2024 12:20] Using data from previous issue: {"categories": ["#inference", "#optimization", "#architecture"], "emoji": "üß†", "ru": {"title": "BitNet a4.8: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ –º–∏—Ä–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç BitNet a4.8 - —É—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é 1-–±–∏—Ç–Ω–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏. –≠—Ç–∞ –º–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç 4-–±–∏—Ç–Ω—ã–µ –∞–∫—Ç
[10.11.2024 12:20] Using data from previous issue: {"categories": ["#architecture", "#multimodal", "#training", "#optimization"], "emoji": "üß†", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã: –º–µ–Ω—å—à–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π, —Ç–∞ –∂–µ –º–æ—â–Ω–æ—Å—Ç—å", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Mixture-of-Transformers (MoT) - –Ω–æ–≤—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö 
[10.11.2024 12:20] Using data from previous issue: {"categories": ["#3d", "#video", "#synthetic"], "emoji": "üé≠", "ru": {"title": "–û—Ç 2D –∫ 4D: DimensionX —Ä–∞–∑–¥–≤–∏–≥–∞–µ—Ç –≥—Ä–∞–Ω–∏—Ü—ã –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "DimensionX - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ñ–æ—Ç–æ—Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö 3D –∏ 4D —Å—Ü–µ–Ω –∏–∑ –æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é –≤–∏–¥–µ–æ-–¥–∏—Ñ—Ñ—É–∑–∏–∏. –ö–ª—é—á–µ–≤—ã–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–º —è–≤–ª—è–µ—Ç—Å
[10.11.2024 12:20] Using data from previous issue: {"categories": ["#dataset", "#video"], "emoji": "üé¨", "ru": {"title": "TIP-I2V: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –∏–∑—É—á–µ–Ω–∏–∏ –ø—Ä–æ–º–ø—Ç–æ–≤ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ TIP-I2V - –ø–µ—Ä–≤—ã–π –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π –±–æ–ª–µ–µ 1,70 –º–∏–ª–ª–∏–æ–Ω–∞ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∏ 
[10.11.2024 12:20] Using data from previous issue: {"categories": ["#benchmark", "#rag", "#dataset", "#multimodal", "#games", "#long_context"], "emoji": "üìÑ", "ru": {"title": "M3DocRAG: –ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º –Ω–æ–≤–æ–≥–æ –ø–æ–∫–æ–ª–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç M3DocRAG - –Ω–æ–≤—É—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –ø–æ –¥–æ–∫
[10.11.2024 12:20] Using data from previous issue: {"categories": ["#dataset", "#agents", "#multimodal"], "emoji": "üó£Ô∏è", "ru": {"title": "–Ø–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ —É—á–∞—Ç—Å—è –∏—Å–∫—É—Å—Å—Ç–≤—É –æ–±—â–µ–Ω–∏—è", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ –Ω–æ–≤—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Multifaceted Skill-of-Mind, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –Ω–∞–≤—ã–∫–æ–≤ –≤–µ–¥–µ–Ω–∏—è –¥–∏–∞–ª–æ–≥–∞ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã—Ö —Å—Ü–µ
[10.11.2024 12:20] Using data from previous issue: {"categories": ["#benchmark", "#multimodal", "#dataset", "#long_context"], "emoji": "üßµ", "ru": {"title": "–Ø–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –≤ –ª–∞–±–∏—Ä–∏–Ω—Ç–µ –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: –Ω–∏—Ç–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏ –≥—Ä–∞–Ω–∏—Ü—ã –ø–æ–Ω–∏–º–∞–Ω–∏—è", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ –∞–Ω–∞–ª–∏–∑—É —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤
[10.11.2024 12:20] Using data from previous issue: {"categories": ["#dataset", "#multimodal", "#architecture", "#games", "#alignment", "#cv"], "emoji": "üé•", "ru": {"title": "–¢–æ—á–Ω–∞—è –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏—è –≤ –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –ò–ò", "desc": "VideoGLaMM - —ç—Ç–æ –Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω–æ–≥–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –¥–ª—è —Ç–æ—á–Ω–æ–π –ø–∏–∫—Å–µ–ª—å–Ω–æ
[10.11.2024 12:20] Using data from previous issue: {"categories": ["#multimodal", "#cv", "#interpretability", "#alignment"], "emoji": "üñºÔ∏è", "ru": {"title": "–í–∏–∑—É–∞–ª—å–Ω—ã–µ —è–∑—ã–∫–∏: –º–µ–∂–¥—É –¶–∏–ø—Ñ–æ–º –∏ —Ö–∞–æ—Å–æ–º", "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ —Å–≤–æ–π—Å—Ç–≤–∞ –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã—Ö –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –≤ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã –æ–±–Ω–∞—Ä—É–∂–∏
[10.11.2024 12:20] Using data from previous issue: {"categories": ["#inference", "#optimization", "#diffusion"], "emoji": "üöÄ", "ru": {"title": "–£—Å–∫–æ—Ä–µ–Ω–∏–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –ø–æ–º–æ—â—å—é 4-–±–∏—Ç–Ω–æ–≥–æ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º SVDQuant –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–∏–∑–∫–æ—Ä–∞–Ω–≥–æ–≤—É—é
[10.11.2024 12:20] Using data from previous issue: {"categories": ["#robotics", "#3d", "#multimodal", "#agents"], "emoji": "ü§ñ", "ru": {"title": "–î–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è –ø–∞–º—è—Ç—å –¥–ª—è —Ä–æ–±–æ—Ç–æ–≤ –≤ –∏–∑–º–µ–Ω—è—é—â–µ–º—Å—è –º–∏—Ä–µ", "desc": "DynaMem - –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –º–æ–±–∏–ª—å–Ω–æ–π –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–∏ —Å –æ—Ç–∫—Ä—ã—Ç—ã–º —Å–ª–æ–≤–∞—Ä–µ–º –≤ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö —Å—Ä–µ–¥–∞—Ö. –°–∏—Å—Ç–µ–º–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–∏–Ω–∞–º–∏—á–µ—Å–∫—É—é –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ-—Å–µ–º–∞–Ω
[10.11.2024 12:20] Using data from previous issue: {"categories": ["#agents", "#cv", "#video", "#edge_computing", "#training"], "emoji": "üëÅÔ∏è", "ru": {"title": "–í–∑–≥–ª—è–¥–æ–º —É–ø—Ä–∞–≤–ª—è–π: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç —Å–∏—Å—Ç–µ–º—É GazeGen, –∫–æ—Ç–æ—Ä–∞—è –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –≤–∏–∑—É–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –≤–∑–≥–ª—è–¥–∞ –ø–æ–ª—å–∑–æ
[10.11.2024 12:20] Using data from previous issue: {"categories": ["#training", "#diffusion", "#data", "#multimodal", "#optimization", "#cv"], "emoji": "üîÑ", "ru": {"title": "Diff-2-in-1: –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è –≤ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º 
[10.11.2024 12:20] Using data from previous issue: {"categories": ["#multilingual", "#dataset", "#data"], "emoji": "üó£Ô∏è", "ru": {"title": "–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö —Ä–∞–∑–≥–æ–≤–æ—Ä–æ–≤: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Å–º–µ—à–∞–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –ø—Ä–æ–±–ª–µ–º–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ —Å–º–µ—à–∞–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö —Ä–∞–∑–≥–æ–≤–æ—Ä–æ–≤ –≤ —Å–æ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–µ—Ç
[10.11.2024 12:20] Using data from previous issue: {"categories": ["#video", "#diffusion", "#dataset"], "emoji": "üé¨", "ru": {"title": "–£–ø—Ä–∞–≤–ª—è–µ–º–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∏–¥–µ–æ –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è", "desc": "SG-I2V - —ç—Ç–æ –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É–ø—Ä–∞–≤–ª—è–µ–º–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—É—é –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –±–µ–∑ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –¥–æ–ø–æ–ª
[10.11.2024 12:20] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#reasoning", "#science"], "emoji": "üß†", "ru": {"title": "M3SciQA: –ù–æ–≤—ã–π —Ä—É–±–µ–∂ –≤ –æ—Ü–µ–Ω–∫–µ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –Ω–∞—É—á–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞", "desc": "M3SciQA - —ç—Ç–æ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –æ–±–ª–∞—Å—Ç–∏ –Ω–∞—É—á–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ –∏ –æ—Ç–≤–µ—Ç–æ–≤. –û–Ω –≤–∫–ª—é—á–∞
[10.11.2024 12:20] Loading Chinese text from previous data.
[10.11.2024 12:20] Renaming data file.
[10.11.2024 12:20] Renaming previous data. hf_papers.json to ./d/2024-11-08.json
[10.11.2024 12:20] Saving new data file.
[10.11.2024 12:20] Generating page.
[10.11.2024 12:20] Renaming previous page.
[10.11.2024 12:20] Renaming previous data. index.html to ./d/2024-11-08.html
[10.11.2024 12:20] [Experimental] Generating Chinese page for reading.
[10.11.2024 12:20] Chinese vocab [{'word': 'ËÆ®ËÆ∫', 'pinyin': 't«éo l√πn', 'trans': 'discuss'}, {'word': 'Â§ßÂûã', 'pinyin': 'd√† x√≠ng', 'trans': 'large-scale'}, {'word': 'ËØ≠Ë®ÄÊ®°Âûã', 'pinyin': 'y«î y√°n m√≥ x√≠ng', 'trans': 'language model'}, {'word': '‰ª£Á†ÅÁîüÊàê', 'pinyin': 'd√†i m«é shƒìng ch√©ng', 'trans': 'code generation'}, {'word': 'Êé®ÁêÜ‰ªªÂä°', 'pinyin': 'tuƒ´ l«ê r√®n w√π', 'trans': 'reasoning tasks'}, {'word': '‰ª£ÁêÜÁ≥ªÁªü', 'pinyin': 'd√†i l«ê x√¨ t«íng', 'trans': 'proxy system'}, {'word': 'ÈáçË¶ÅÊÄß', 'pinyin': 'zh√≤ng y√†o x√¨ng', 'trans': 'importance'}, {'word': 'ÂºÄÊîæËÆøÈóÆ', 'pinyin': 'kƒÅi f√†ng f«éng w√®n', 'trans': 'open access'}, {'word': 'ÊÄßËÉΩ', 'pinyin': 'x√¨ng n√©ng', 'trans': 'performance'}, {'word': 'Êé•Ëøë', 'pinyin': 'jiƒì j√¨n', 'trans': 'close to'}, {'word': '‰∏ìÊúâÊ®°Âûã', 'pinyin': 'zhuƒÅn y«íu m√≥ x√≠ng', 'trans': 'proprietary model'}, {'word': 'ÈÄÇÂêà', 'pinyin': 'sh√¨ h√©', 'trans': 'suitable'}, {'word': '‰∏•Ê†º', 'pinyin': 'y√°n g√©', 'trans': 'strict'}, {'word': 'ÁßëÂ≠¶Á†îÁ©∂', 'pinyin': 'kƒì xu√© y√°n ji≈´', 'trans': 'scientific research'}, {'word': 'È´òË¥®Èáè', 'pinyin': 'gƒÅo zh√¨ li√†ng', 'trans': 'high quality'}, {'word': 'ÊúâÈôê', 'pinyin': 'y«íu xi√†n', 'trans': 'limited'}, {'word': 'Â°´Ë°•', 'pinyin': 'ti√°n b«î', 'trans': 'fill'}, {'word': 'Á©∫ÁôΩ', 'pinyin': 'k√≤ng b√°i', 'trans': 'gap'}, {'word': '‰ΩúËÄÖ', 'pinyin': 'zu√≤ zhƒõ', 'trans': 'author'}, {'word': '‰ªãÁªç', 'pinyin': 'ji√® sh√†o', 'trans': 'introduce'}, {'word': 'OpenCoder', 'pinyin': 'OpenCoder', 'trans': 'OpenCoder'}, {'word': 'È°∂Â∞ñ', 'pinyin': 'd«êng jiƒÅn', 'trans': 'top-notch'}, {'word': 'Â™≤Áæé', 'pinyin': 'p√¨ mƒõi', 'trans': 'rival'}, {'word': 'È¢ÜÂÖàÊ®°Âûã', 'pinyin': 'l«êng xiƒÅn m√≥ x√≠ng', 'trans': 'leading model'}, {'word': 'ËØ¶ÁªÜ', 'pinyin': 'xi√°ng x√¨', 'trans': 'detailed'}, {'word': 'ËÆ≠ÁªÉÊï∞ÊçÆ', 'pinyin': 'x√πn li√†n sh√π j√π', 'trans': 'training data'}, {'word': 'ÂçèËÆÆ', 'pinyin': 'xi√© y√¨', 'trans': 'protocol'}, {'word': 'ÂºÄÊîæÊÄß', 'pinyin': 'kƒÅi f√†ng x√¨ng', 'trans': 'openness'}, {'word': 'Âä†ÈÄü', 'pinyin': 'jiƒÅ s√π', 'trans': 'accelerate'}, {'word': 'ÂèØÈáçÂ§ç', 'pinyin': 'kƒõ ch√≥ng f√π', 'trans': 'reproducible'}, {'word': 'ËøõÂ±ï', 'pinyin': 'j√¨n zh«én', 'trans': 'progress'}]
[10.11.2024 12:20] Renaming previous Chinese page.
[10.11.2024 12:20] Renaming previous data. zh.html to ./d/2024-11-09_zh_reading_task.html
[10.11.2024 12:20] Writing Chinese reading task.
[10.11.2024 12:20] Writing result.
[10.11.2024 12:20] Renaming log file.
[10.11.2024 12:20] Renaming previous data. log.txt to ./logs/2024-11-10_last_log.txt
