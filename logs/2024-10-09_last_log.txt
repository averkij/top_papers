[10.10.2024 08:15] Get feed.
[10.10.2024 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2410.07113
[10.10.2024 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2410.05363
[10.10.2024 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2410.07167
[10.10.2024 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2410.07171
[10.10.2024 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2410.05993
[10.10.2024 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2410.05954
[10.10.2024 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2410.07073
[10.10.2024 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2410.07177
[10.10.2024 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2410.06244
[10.10.2024 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2410.06961
[10.10.2024 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2410.05643
[10.10.2024 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2410.07064
[10.10.2024 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2410.06166
[10.10.2024 08:15] Extract page data from URL. URL: https://huggingface.co/papers/2410.05355
[10.10.2024 08:15] Extract page data from URL. URL: https://huggingface.co/papers/2410.07002
[10.10.2024 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2410.06373
[10.10.2024 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2410.06172
[10.10.2024 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2410.05591
[10.10.2024 08:15] Extract page data from URL. URL: https://huggingface.co/papers/2410.07071
[10.10.2024 08:15] Extract page data from URL. URL: https://huggingface.co/papers/2410.07160
[10.10.2024 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2410.07170
[10.10.2024 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2410.05677
[10.10.2024 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2410.05664
[10.10.2024 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2410.06241
[10.10.2024 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2410.06949
[10.10.2024 08:15] Extract page data from URL. URL: https://huggingface.co/papers/2410.06845
[10.10.2024 08:15] Extract page data from URL. URL: https://huggingface.co/papers/2410.05791
[10.10.2024 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2410.05295
[10.10.2024 08:15] Extract page data from URL. URL: https://huggingface.co/papers/2410.07062
[10.10.2024 08:15] Extract page data from URL. URL: https://huggingface.co/papers/2410.06885
[10.10.2024 08:15] Extract page data from URL. URL: https://huggingface.co/papers/2410.07095
[10.10.2024 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2410.05651
[10.10.2024 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2410.04223
[10.10.2024 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2410.06524
[10.10.2024 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2410.06462
[10.10.2024 08:15] ********************************************************************************
[10.10.2024 08:15] Abstract 0. Recent advancements in multimodal large language models (MLLMs) have demonstrated significant progress; however, these models exhibit a notable limitation, which we refer to as "face blindness". Specifically, they can engage in general conversations but fail to conduct personalized dialogues targeti...
[10.10.2024 08:15] ********************************************************************************
[10.10.2024 08:15] Abstract 1. Text-to-video (T2V) models like Sora have made significant strides in visualizing complex prompts, which is increasingly viewed as a promising path towards constructing the universal world simulator. Cognitive psychologists believe that the foundation for achieving this goal is the ability to unders...
[10.10.2024 08:15] ********************************************************************************
[10.10.2024 08:15] Abstract 2. We present the Modality Integration Rate (MIR), an effective, robust, and generalized metric to indicate the multi-modal pre-training quality of Large Vision Language Models (LVLMs). Large-scale pre-training plays a critical role in building capable LVLMs, while evaluating its training quality witho...
[10.10.2024 08:15] ********************************************************************************
[10.10.2024 08:15] Abstract 3. Advanced diffusion models like RPG, Stable Diffusion 3 and FLUX have made notable strides in compositional text-to-image generation. However, these methods typically exhibit distinct strengths for compositional generation, with some excelling in handling attribute binding and others in spatial relat...
[10.10.2024 08:15] ********************************************************************************
[10.10.2024 08:15] Abstract 4. Information comes in diverse modalities. Multimodal native AI models are essential to integrate real-world information and deliver comprehensive understanding. While proprietary multimodal native models exist, their lack of openness imposes obstacles for adoptions, let alone adaptations. To fill thi...
[10.10.2024 08:15] ********************************************************************************
[10.10.2024 08:15] Abstract 5. Video generation requires modeling a vast spatiotemporal space, which demands significant computational resources and data usage. To reduce the complexity, the prevailing approaches employ a cascaded architecture to avoid direct training with full resolution. Despite reducing computational demands, ...
[10.10.2024 08:15] ********************************************************************************
[10.10.2024 08:15] Abstract 6. We introduce Pixtral-12B, a 12--billion-parameter multimodal language model. Pixtral-12B is trained to understand both natural images and documents, achieving leading performance on various multimodal benchmarks, surpassing a number of larger models. Unlike many open-source models, Pixtral is also a...
[10.10.2024 08:15] ********************************************************************************
[10.10.2024 08:15] Abstract 7. This research aims to comprehensively explore building a multimodal foundation model for egocentric video understanding. To achieve this goal, we work on three fronts. First, as there is a lack of QA data for egocentric video understanding, we develop a data engine that efficiently generates 7M high...
[10.10.2024 08:15] ********************************************************************************
[10.10.2024 08:15] Abstract 8. Story visualization, the task of generating coherent images based on a narrative, has seen significant advancements with the emergence of text-to-image models, particularly diffusion models. However, maintaining semantic consistency, generating high-quality fine-grained interactions, and ensuring co...
[10.10.2024 08:15] ********************************************************************************
[10.10.2024 08:15] Abstract 9. Through alignment with human preferences, Large Language Models (LLMs) have advanced significantly in generating honest, harmless, and helpful responses. However, collecting high-quality preference data is a resource-intensive and creativity-demanding process, especially for the continual improvemen...
[10.10.2024 08:15] ********************************************************************************
[10.10.2024 08:15] Abstract 10. Video Temporal Grounding (VTG) is a crucial capability for video understanding models and plays a vital role in downstream tasks such as video browsing and editing. To effectively handle various tasks simultaneously and enable zero-shot prediction, there is a growing trend in employing video LLMs fo...
[10.10.2024 08:15] ********************************************************************************
[10.10.2024 08:15] Abstract 11. This work investigates the selection of high-quality pre-training data from massive corpora to enhance LMs' capabilities for downstream usage. We formulate data selection as a generalized Optimal Control problem, which can be solved theoretically by Pontryagin's Maximum Principle (PMP), yielding a s...
[10.10.2024 08:15] ********************************************************************************
[10.10.2024 08:15] Abstract 12. Video Large Language Models (Video LLMs) have shown promising capabilities in video comprehension, yet they struggle with tracking temporal changes and reasoning about temporal relationships. While previous research attributed this limitation to the ineffective temporal encoding of visual inputs, ou...
[10.10.2024 08:15] ********************************************************************************
[10.10.2024 08:15] Abstract 13. In this technical report, we present Falcon Mamba 7B, a new base large language model based on the novel Mamba architecture. Falcon Mamba 7B is trained on 5.8 trillion tokens with carefully selected data mixtures. As a pure Mamba-based model, Falcon Mamba 7B surpasses leading open-weight models base...
[10.10.2024 08:15] ********************************************************************************
[10.10.2024 08:15] Abstract 14. Large language models have been successfully applied to programming assistance tasks, such as code completion, code insertion, and instructional code editing. However, these applications remain insufficiently automated and struggle to effectively integrate various types of information during the pro...
[10.10.2024 08:15] ********************************************************************************
[10.10.2024 08:15] Abstract 15. This paper delves into the interplay between vision backbones and optimizers, unvealing an inter-dependent phenomenon termed \textbf{backbone-optimizer coupling bias} (BOCB). We observe that canonical CNNs, such as VGG and ResNet, exhibit a marked co-dependency with SGD families, while recent archit...
[10.10.2024 08:15] ********************************************************************************
[10.10.2024 08:15] Abstract 16. Multimodal Large Language Models (MLLMs) are rapidly evolving, demonstrating impressive capabilities as multimodal assistants that interact with both humans and their environments. However, this increased sophistication introduces significant safety concerns. In this paper, we present the first eval...
[10.10.2024 08:15] ********************************************************************************
[10.10.2024 08:15] Abstract 17. Despite significant advancements in customizing text-to-image and video generation models, generating images and videos that effectively integrate multiple personalized concepts remains a challenging task. To address this, we present TweedieMix, a novel method for composing customized diffusion mode...
[10.10.2024 08:15] ********************************************************************************
[10.10.2024 08:15] Abstract 18. In-context learning (ICL) is the ability of a model to learn a new task by observing a few exemplars in its context. While prevalent in NLP, this capability has recently also been observed in Reinforcement Learning (RL) settings. Prior in-context RL methods, however, require entire episodes in the a...
[10.10.2024 08:15] ********************************************************************************
[10.10.2024 08:15] Abstract 19. We propose TextToon, a method to generate a drivable toonified avatar. Given a short monocular video sequence and a written instruction about the avatar style, our model can generate a high-fidelity toonified avatar that can be driven in real-time by another video with arbitrary identities. Existing...
[10.10.2024 08:15] ********************************************************************************
[10.10.2024 08:15] Abstract 20. Foundation models (FMs) are pre-trained on large-scale datasets and then fine-tuned on a downstream task for a specific application. The most successful and most commonly used fine-tuning method is to update the pre-trained weights via a low-rank adaptation (LoRA). LoRA introduces new weight matrice...
[10.10.2024 08:15] ********************************************************************************
[10.10.2024 08:15] Abstract 21. In this paper, we focus on enhancing a diffusion-based text-to-video (T2V) model during the post-training phase by distilling a highly capable consistency model from a pretrained T2V model. Our proposed method, T2V-Turbo-v2, introduces a significant advancement by integrating various supervision sig...
[10.10.2024 08:15] ********************************************************************************
[10.10.2024 08:15] Abstract 22. As text-to-image diffusion models become advanced enough for commercial applications, there is also increasing concern about their potential for malicious and harmful use. Model unlearning has been proposed to mitigate the concerns by removing undesired and potentially harmful information from the p...
[10.10.2024 08:15] ********************************************************************************
[10.10.2024 08:15] Abstract 23. The text-to-video (T2V) generation models, offering convenient visual creation, have recently garnered increasing attention. Despite their substantial potential, the generated videos may present artifacts, including structural implausibility, temporal inconsistency, and a lack of motion, often resul...
[10.10.2024 08:15] ********************************************************************************
[10.10.2024 08:15] Abstract 24. In real world software development, improper or missing exception handling can severely impact the robustness and reliability of code. Exception handling mechanisms require developers to detect, capture, and manage exceptions according to high standards, but many developers struggle with these tasks...
[10.10.2024 08:15] ********************************************************************************
[10.10.2024 08:15] Abstract 25. Mental health disorders are one of the most serious diseases in the world. Most people with such a disease lack access to adequate care, which highlights the importance of training models for the diagnosis and treatment of mental health disorders. However, in the mental health domain, privacy concer...
[10.10.2024 08:15] ********************************************************************************
[10.10.2024 08:15] Abstract 26. Piano playing requires agile, precise, and coordinated hand control that stretches the limits of dexterity. Hand motion models with the sophistication to accurately recreate piano playing have a wide range of applications in character animation, embodied AI, biomechanics, and VR/AR. In this paper, w...
[10.10.2024 08:15] ********************************************************************************
[10.10.2024 08:15] Abstract 27. In this paper, we propose AutoDAN-Turbo, a black-box jailbreak method that can automatically discover as many jailbreak strategies as possible from scratch, without any human intervention or predefined scopes (e.g., specified candidate strategies), and use them for red-teaming. As a result, AutoDAN-...
[10.10.2024 08:15] ********************************************************************************
[10.10.2024 08:15] Abstract 28. This paper introduces TinyEmo, a family of small multi-modal language models for emotional reasoning and classification. Our approach features: (1) a synthetic emotional instruct dataset for both pre-training and fine-tuning stages, (2) a Metric Projector that delegates classification from the langu...
[10.10.2024 08:15] ********************************************************************************
[10.10.2024 08:15] Abstract 29. This paper introduces F5-TTS, a fully non-autoregressive text-to-speech system based on flow matching with Diffusion Transformer (DiT). Without requiring complex designs such as duration model, text encoder, and phoneme alignment, the text input is simply padded with filler tokens to the same length...
[10.10.2024 08:15] ********************************************************************************
[10.10.2024 08:15] Abstract 30. We introduce MLE-bench, a benchmark for measuring how well AI agents perform at machine learning engineering. To this end, we curate 75 ML engineering-related competitions from Kaggle, creating a diverse set of challenging tasks that test real-world ML engineering skills such as training models, pre...
[10.10.2024 08:15] ********************************************************************************
[10.10.2024 08:15] Abstract 31. Recent progress in large-scale text-to-video (T2V) and image-to-video (I2V) diffusion models has greatly enhanced video generation, especially in terms of keyframe interpolation. However, current image-to-video diffusion models, while powerful in generating videos from a single conditioning frame, n...
[10.10.2024 08:15] ********************************************************************************
[10.10.2024 08:15] Abstract 32. While large language models (LLMs) have integrated images, adapting them to graphs remains challenging, limiting their applications in materials and drug design. This difficulty stems from the need for coherent autoregressive generation across texts and graphs. To address this, we introduce Llamole,...
[10.10.2024 08:15] ********************************************************************************
[10.10.2024 08:15] Abstract 33. Recent advancements of large language models (LLMs) have led to claims of AI surpassing humans in natural language processing (NLP) tasks such as textual understanding and reasoning. This work investigates these assertions by introducing CAIMIRA, a novel framework rooted in item response theory (IRT...
[10.10.2024 08:15] ********************************************************************************
[10.10.2024 08:15] Abstract 34. The research builds and evaluates the adversarial potential to introduce copied code or hallucinated AI recommendations for malicious code in popular code repositories. While foundational large language models (LLMs) from OpenAI, Google, and Anthropic guard against both harmful behaviors and toxic s...
[10.10.2024 08:15] Read previous papers.
[10.10.2024 08:15] Generating reviews via LLM API.
[10.10.2024 08:15] Using data from previous issue: {"desc": "Статья представляет новый подход к обучению мультимодальных языковых моделей (MLLM) для персонализированного общения. Авторы предлагают метод PVIT (Personalized Visual Instruction Tuning), который позволяет моделям идентифицировать конкретных людей на изображениях и вести персонализированн
[10.10.2024 08:15] Using data from previous issue: {"desc": "Статья представляет PhyGenBench - комплексный бенчмарк для оценки корректности физического здравого смысла в моделях текст-в-видео (T2V). Бенчмарк включает 160 промптов, охватывающих 27 физических законов в четырех фундаментальных областях. Авторы также предлагают PhyGenEval - новую структ
[10.10.2024 08:15] Using data from previous issue: {"desc": "В статье представлен новый метрический показатель Modality Integration Rate (MIR) для оценки качества предварительного обучения мультимодальных моделей компьютерного зрения и обработки естественного языка (Large Vision Language Models, LVLMs). MIR позволяет эффективно оценивать качество об
[10.10.2024 08:15] Using data from previous issue: {"desc": "IterComp - это новый фреймворк для улучшения композиционной генерации изображений по тексту. Он агрегирует предпочтения нескольких моделей диффузии и использует итеративное обучение с обратной связью. IterComp оценивает модели по трем ключевым метрикам композиции и создает набор данных для
[10.10.2024 08:15] Using data from previous issue: {"desc": "Статья представляет Aria - открытую мультимодальную модель искусственного интеллекта. Модель использует архитектуру смеси экспертов и имеет 3,9 млрд и 3,5 млрд активируемых параметров для визуальных и текстовых токенов соответственно. Aria превосходит Pixtral-12B и Llama3.2-11B, конкурируя
[10.10.2024 08:15] Using data from previous issue: {"desc": "Статья представляет новый алгоритм пирамидального сопоставления потоков для генерации видео. Этот метод переосмысливает траекторию шумоподавления как серию пирамидальных этапов, где только финальный этап работает в полном разрешении. Подход позволяет оптимизировать весь процесс end-to-end 
[10.10.2024 08:15] Using data from previous issue: {"desc": "Pixtral-12B - это мультимодальная языковая модель с 12 миллиардами параметров, способная понимать как изображения, так и текст. Она превосходит более крупные модели по различным мультимодальным показателям, при этом не уступая в задачах обработки естественного языка. Pixtral использует нов
[10.10.2024 08:15] Using data from previous issue: {"desc": "Исследование направлено на создание мультимодальной модели для понимания эгоцентрического видео. Авторы разработали генератор данных, создающий 7 миллионов высококачественных пар вопрос-ответ для эгоцентрических видео различной длительности. Они также представили новый бенчмарк для оценки 
[10.10.2024 08:15] Using data from previous issue: {"desc": "В статье представлен фреймворк Story-Adapter для улучшения генерации изображений на основе длинных историй. Он использует итеративный подход с глобальным модулем кросс-внимания для сохранения семантической согласованности. Story-Adapter не требует дополнительного обучения и эффективен с вы
[10.10.2024 08:15] Using data from previous issue: {"desc": "В статье представлен метод SynPO для улучшения языковых моделей без использования ручной разметки. SynPO использует итеративный механизм, где генератор создает разнообразные промпты, а улучшатель ответов постепенно совершенствует ответы модели. После четырех итераций SynPO, модели Llama3-8
[10.10.2024 08:15] Using data from previous issue: {"desc": "Статья представляет новый подход к задаче временной локализации в видео (Video Temporal Grounding). Авторы вводят концепцию каузального моделирования событий, представляя видео как последовательность событий. Они предлагают модель TRACE - мультизадачную видео-LLM, которая обрабатывает визу
[10.10.2024 08:15] Using data from previous issue: {"desc": "Эта работа исследует выбор высококачественных данных для предобучения из массивных корпусов с целью улучшения возможностей языковых моделей. Авторы формулируют выбор данных как обобщенную задачу оптимального управления, решаемую с помощью принципа максимума Понтрягина. На основе теоретичес
[10.10.2024 08:15] Using data from previous issue: {"desc": "Исследователи обнаружили, что видео-модели большого языка (Video LLMs) испытывают трудности с пониманием временных отношений не из-за неэффективного кодирования визуальных входных данных, а из-за ограничений базовой языковой модели в понимании временных концепций. Для решения этой проблемы
[10.10.2024 08:15] Querying the API.
[10.10.2024 08:16] Got response. {
  "desc": "Falcon Mamba 7B - это новая базовая языковая модель, основанная на архитектуре Mamba. Она обучена на 5,8 триллионах токенов и превосходит ведущие модели на основе трансформеров, такие как Mistral 7B и Llama3.1 8B. Falcon Mamba 7B демонстрирует лучшую производительность среди моделей Mamba своего масштаба и значительно быстрее работает при инференсе. Модель показывает, что чистая архитектура Mamba может достигать результатов, сравнимых или превосходящих гибридные модели Mamba-Transformer.",

  "tags": ["#FalconMamba", "#LanguageModelArchitecture", "#InferenceEfficiency"],

  "emoji": "🦅",

  "title": "Falcon Mamba 7B: Прорыв в эффективности языковых моделей"
}
[10.10.2024 08:16] Querying the API.
[10.10.2024 08:16] Got response. {
  "desc": "Статья представляет новую систему для помощи в программировании, использующую большие языковые модели. Авторы разработали фреймворк, интегрирующий историю кодирования, текущий код и инструкции пользователя. Они также создали новый бенчмарк APEval для оценки моделей и pipeline для генерации обучающих данных. Результаты показывают, что их модель CursorCore превосходит аналоги сопоставимого размера.",
  "tags": ["#программная_помощь", "#генерация_кода", "#языковые_модели"],
  "emoji": "💻",
  "title": "Интеллектуальный помощник программиста: новый подход к автоматизации кодирования"
}
[10.10.2024 08:16] Using data from previous issue: {"desc": "Статья исследует взаимосвязь между архитектурами нейронных сетей для компьютерного зрения и оптимизаторами, выявляя феномен 'смещения связи между архитектурой и оптимизатором' (BOCB). Авторы обнаружили, что классические CNN лучше работают с семейством SGD-оптимизаторов, а современные архит
[10.10.2024 08:16] Using data from previous issue: {"desc": "Статья представляет первый анализ проблемы многомодальной ситуационной безопасности в контексте мультимодальных больших языковых моделей (MLLM). Авторы разработали набор данных MSSBench для оценки способности MLLM учитывать безопасность в различных ситуациях. Исследование показало, что сущ
[10.10.2024 08:16] Using data from previous issue: {"desc": "TweedieMix - это новый метод для объединения персонализированных диффузионных моделей на этапе вывода. Он разделяет процесс сэмплирования на два этапа: сначала применяется техника сэмплирования с учетом нескольких объектов, а затем используется формула Твиди для смешивания внешнего вида пе
[10.10.2024 08:16] Querying the API.
[10.10.2024 08:16] Got response. {
  "desc": "Статья представляет новый метод обучения с подкреплением в контексте - Retrieval-Augmented Decision Transformer (RA-DT). RA-DT использует внешнюю память для хранения прошлого опыта и извлекает только релевантные текущей ситуации под-траектории. Метод не требует обучения компонента извлечения и может быть независимым от домена. RA-DT превосходит базовые модели на сеточных мирах, используя лишь часть длины их контекста.",
  "tags": ["#InContextRL", "#RetrievalAugmentation", "#DecisionTransformer"],
  "emoji": "🧠",
  "title": "RA-DT: Эффективное обучение с подкреплением в контексте с помощью извлечения релевантного опыта"
}
[10.10.2024 08:16] Querying the API.
[10.10.2024 08:16] Got response. {
  "desc": "TextToon - это метод создания управляемого мультяшного аватара на основе короткого видео и текстового описания стиля. Он использует условное встраивание Tri-plane для реалистичного и стилизованного представления лица в гауссовом поле деформации. Модель улучшает возможности стилизации 3D Gaussian Splatting с помощью адаптивной нейронной сети пиксельного преобразования и контрастного обучения с учетом патчей. TextToon работает в режиме реального времени и превосходит существующие методы по качеству и анимации.",
  "tags": ["#3DGaussianSplatting", "#AvatarGeneration", "#RealTimeRendering"],
  "emoji": "🎭",
  "title": "TextToon: Создание управляемых мультяшных аватаров по видео и тексту"
}
[10.10.2024 08:16] Using data from previous issue: {"desc": "Статья представляет новый метод тонкой настройки фундаментальных моделей под названием EVA (Explained Variance Adaptation). Этот метод улучшает традиционный подход LoRA, инициализируя новые веса на основе данных с помощью сингулярного разложения. EVA перераспределяет ранги между матрицами 
[10.10.2024 08:16] Using data from previous issue: {"desc": "В статье представлен метод T2V-Turbo-v2 для улучшения модели преобразования текста в видео на основе диффузии. Авторы предлагают дистиллировать модель согласованности из предобученной T2V модели, используя различные сигналы обучения. Метод включает интеграцию высококачественных данных, обр
[10.10.2024 08:16] Using data from previous issue: {"desc": "Статья посвящена исследованию методов разобучения моделей генерации изображений для удаления нежелательной информации. Авторы проанализировали существующие подходы к разобучению по пяти ключевым аспектам в различных сценариях. Исследование выявило, что все методы имеют побочные эффекты или
[10.10.2024 08:16] Using data from previous issue: {"desc": "Статья представляет метод BroadWay для улучшения качества генерации видео из текста без дополнительного обучения или параметров. Метод основан на наблюдении связи между различиями во временных картах внимания и временной несогласованностью в видео. BroadWay включает два компонента: Tempora
[10.10.2024 08:16] Using data from previous issue: {"desc": "Статья посвящена проблеме обработки исключений в разработке программного обеспечения и предлагает решение с использованием больших языковых моделей (LLM). Авторы выявили три ключевые проблемы: нечувствительное обнаружение хрупкого кода, неточный захват типов исключений и искаженные решения
[10.10.2024 08:16] Querying the API.
[10.10.2024 08:16] Got response. {
  "desc": "Статья представляет MentalArena - фреймворк для обучения языковых моделей в области психического здоровья путем генерации персонализированных данных. Авторы разработали Symptom Encoder для моделирования поведения пациентов и Symptom Decoder для управления диалогом между пациентом и терапевтом. Модели, обученные с помощью MentalArena на GPT-3.5 и Llama-3-8b, превзошли другие современные модели на различных тестах по биомедицине и психическому здоровью. Исследование направлено на улучшение персонализированной диагностики и лечения психических расстройств.",
  "tags": ["#MentalHealthAI", "#SelfPlayLM", "#PersonalizedDiagnostics"],
  "emoji": "🧠",
  "title": "MentalArena: ИИ-терапевт нового поколения для персонализированной диагностики"
}
[10.10.2024 08:16] Querying the API.
[10.10.2024 08:16] Got response. {
  "desc": "Эта статья представляет новый набор данных, содержащий 10 часов 3D-движений рук и аудио от 15 пианистов высокого уровня, играющих классическую музыку. Авторы разработали систему безмаркерного захвата движений с использованием мультиракурсной съемки и современных моделей оценки позы. На основе собранных данных создан конвейер для синтеза реалистичных движений рук при игре на пианино, используя имитационное обучение и обучение с подкреплением. Модель генерирует естественные и точные движения, которые обобщаются на музыку вне обучающего набора.",
  "tags": ["#3DHandMotionCapture", "#PianoPerformanceModeling", "#DiffusionModelForMotion"],
  "emoji": "🎹",
  "title": "Виртуозное воссоздание игры на пианино с помощью ИИ"
}
[10.10.2024 08:16] Using data from previous issue: {"desc": "AutoDAN-Turbo - это метод автоматического обнаружения уязвимостей в системах искусственного интеллекта без вмешательства человека. Он значительно превосходит базовые методы, достигая на 74.3% более высокого среднего показателя успешности атак на публичных бенчмарках. На модели GPT-4-1106-t
[10.10.2024 08:16] Querying the API.
[10.10.2024 08:16] Got response. {
  "desc": "TinyEmo - это семейство небольших мультимодальных языковых моделей для эмоционального анализа и классификации. Модель использует синтетический набор данных для обучения и настройки, а также включает Metric Projector для эффективной классификации. TinyEmo способна выполнять классификацию эмоций и эмоциональные рассуждения, используя значительно меньше параметров, чем сопоставимые модели. Кроме того, модель предлагает подход к пониманию и улучшению систем ИИ через интерпретируемость и косвенное обнаружение предвзятости.",
  "tags": ["#EmotionalAI", "#EfficientMLM", "#InterpretableML"],
  "emoji": "🧠",
  "title": "TinyEmo: Эффективный анализ эмоций с меньшими ресурсами"
}
[10.10.2024 08:16] Querying the API.
[10.10.2024 08:16] Got response. {
  "desc": "F5-TTS - это неавторегрессивная система синтеза речи, основанная на сопоставлении потоков с использованием Diffusion Transformer. В отличие от сложных моделей, F5-TTS просто дополняет текстовый ввод до длины речи и выполняет шумоподавление для генерации. Система использует ConvNeXt для улучшения представления текста и стратегию Sway Sampling для повышения производительности. F5-TTS демонстрирует высокую естественность речи, возможность переключения кодов и контроль скорости.",

  "tags": ["#text-to-speech", "#flow-matching", "#non-autoregressive"],

  "emoji": "🗣️",

  "title": "F5-TTS: Революция в синтезе речи без авторегрессии"
}
[10.10.2024 08:16] Querying the API.
[10.10.2024 08:16] Got response. {
  "desc": "MLE-bench - это новый бенчмарк для оценки способностей ИИ-агентов в области инженерии машинного обучения. Он включает 75 соревнований с Kaggle, охватывающих различные аспекты ML-инженерии. Авторы установили базовые показатели человеческой производительности и протестировали несколько языковых моделей, используя открытые фреймворки для агентов. Лучший результат показала модель OpenAI o1-preview с AIDE, достигнув уровня бронзовой медали Kaggle в 16.9% соревнований.",
  "tags": ["#MLE-bench", "#ML-инженерия", "#ИИ-агенты"],
  "emoji": "🤖",
  "title": "MLE-bench: измеряем инженерные навыки ИИ в машинном обучении"
}
[10.10.2024 08:16] Using data from previous issue: {"desc": "Статья представляет новую стратегию двунаправленной выборки для улучшения интерполяции ключевых кадров в моделях диффузии изображение-видео. Авторы предлагают последовательную выборку вдоль прямого и обратного путей, обусловленную начальным и конечным кадрами, что обеспечивает более соглас
[10.10.2024 08:16] Using data from previous issue: {"desc": "Llamole - это первая мультимодальная языковая модель, способная генерировать текст и графы молекул. Она объединяет базовую языковую модель с Graph Diffusion Transformer и графовыми нейронными сетями для многоусловной генерации молекул и вывода реакций. Llamole также интегрирует A* поиск с 
[10.10.2024 08:16] Using data from previous issue: {"desc": "CAIMIRA - новая система оценки способностей к решению задач у людей и ИИ в области обработки естественного языка. Исследование, основанное на анализе более 300 000 ответов от ~70 систем ИИ и 155 людей, выявило различия в профессиональных навыках в разных областях знаний. Люди превосходят И
[10.10.2024 08:16] Using data from previous issue: {"desc": "Данное исследование посвящено изучению потенциальных уязвимостей больших языковых моделей (LLM) в контексте рекомендаций кода. Авторы демонстрируют, что при резком изменении контекста, например, при решении задачи программирования, LLM могут снизить свою защиту и предложить потенциально вр
[10.10.2024 08:16] Renaming data file.
[10.10.2024 08:16] Renaming previous data. hf_papers.json to 2024-10-09_hf_papers.json
[10.10.2024 08:16] Saving new data file.
[10.10.2024 08:16] Generating page.
[10.10.2024 08:16] Renaming previous page.
[10.10.2024 08:16] Renaming previous data. index.html to 2024-10-09_hf_papers.html
[10.10.2024 08:16] Writing result.
[10.10.2024 08:16] Renaming log file.
[10.10.2024 08:16] Renaming previous data. log.txt to 2024-10-09_last_log.txt
