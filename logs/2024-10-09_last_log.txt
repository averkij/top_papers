[10.10.2024 14:10] Get feed.
[10.10.2024 14:10] Get page data from previous paper. URL: https://huggingface.co/papers/2410.05254
[10.10.2024 14:10] Get page data from previous paper. URL: https://huggingface.co/papers/2410.07113
[10.10.2024 14:10] Get page data from previous paper. URL: https://huggingface.co/papers/2410.07171
[10.10.2024 14:10] Get page data from previous paper. URL: https://huggingface.co/papers/2410.05363
[10.10.2024 14:10] Get page data from previous paper. URL: https://huggingface.co/papers/2410.07167
[10.10.2024 14:10] Get page data from previous paper. URL: https://huggingface.co/papers/2410.05993
[10.10.2024 14:10] Get page data from previous paper. URL: https://huggingface.co/papers/2410.07073
[10.10.2024 14:10] Get page data from previous paper. URL: https://huggingface.co/papers/2410.06373
[10.10.2024 14:10] Get page data from previous paper. URL: https://huggingface.co/papers/2410.05954
[10.10.2024 14:10] Get page data from previous paper. URL: https://huggingface.co/papers/2410.07177
[10.10.2024 14:10] Get page data from previous paper. URL: https://huggingface.co/papers/2410.06244
[10.10.2024 14:10] Get page data from previous paper. URL: https://huggingface.co/papers/2410.06961
[10.10.2024 14:10] Get page data from previous paper. URL: https://huggingface.co/papers/2410.05355
[10.10.2024 14:10] Get page data from previous paper. URL: https://huggingface.co/papers/2410.07170
[10.10.2024 14:10] Get page data from previous paper. URL: https://huggingface.co/papers/2410.05591
[10.10.2024 14:10] Get page data from previous paper. URL: https://huggingface.co/papers/2410.06166
[10.10.2024 14:10] Get page data from previous paper. URL: https://huggingface.co/papers/2410.07002
[10.10.2024 14:10] Get page data from previous paper. URL: https://huggingface.co/papers/2410.05295
[10.10.2024 14:10] Get page data from previous paper. URL: https://huggingface.co/papers/2410.05643
[10.10.2024 14:10] Get page data from previous paper. URL: https://huggingface.co/papers/2410.07064
[10.10.2024 14:10] Get page data from previous paper. URL: https://huggingface.co/papers/2410.02465
[10.10.2024 14:10] Get page data from previous paper. URL: https://huggingface.co/papers/2410.05651
[10.10.2024 14:10] Get page data from previous paper. URL: https://huggingface.co/papers/2410.02503
[10.10.2024 14:10] Get page data from previous paper. URL: https://huggingface.co/papers/2410.06084
[10.10.2024 14:10] Get page data from previous paper. URL: https://huggingface.co/papers/2410.06555
[10.10.2024 14:10] Get page data from previous paper. URL: https://huggingface.co/papers/2410.05677
[10.10.2024 14:10] Get page data from previous paper. URL: https://huggingface.co/papers/2410.04223
[10.10.2024 14:10] Get page data from previous paper. URL: https://huggingface.co/papers/2410.02428
[10.10.2024 14:10] Get page data from previous paper. URL: https://huggingface.co/papers/2410.07071
[10.10.2024 14:10] Get page data from previous paper. URL: https://huggingface.co/papers/2410.06885
[10.10.2024 14:10] Get page data from previous paper. URL: https://huggingface.co/papers/2410.05791
[10.10.2024 14:10] Get page data from previous paper. URL: https://huggingface.co/papers/2410.06172
[10.10.2024 14:10] Get page data from previous paper. URL: https://huggingface.co/papers/2410.05664
[10.10.2024 14:10] Get page data from previous paper. URL: https://huggingface.co/papers/2410.06241
[10.10.2024 14:10] Get page data from previous paper. URL: https://huggingface.co/papers/2410.06462
[10.10.2024 14:10] Get page data from previous paper. URL: https://huggingface.co/papers/2410.06949
[10.10.2024 14:10] Get page data from previous paper. URL: https://huggingface.co/papers/2410.06845
[10.10.2024 14:10] Get page data from previous paper. URL: https://huggingface.co/papers/2410.06985
[10.10.2024 14:10] Get page data from previous paper. URL: https://huggingface.co/papers/2410.07062
[10.10.2024 14:10] Get page data from previous paper. URL: https://huggingface.co/papers/2410.07160
[10.10.2024 14:10] Get page data from previous paper. URL: https://huggingface.co/papers/2410.06524
[10.10.2024 14:10] Extract page data from URL. URL: https://huggingface.co/papers/2410.06458
[10.10.2024 14:10] Get page data from previous paper. URL: https://huggingface.co/papers/2410.06468
[10.10.2024 14:10] Get page data from previous paper. URL: https://huggingface.co/papers/2410.07095
[10.10.2024 14:10] ********************************************************************************
[10.10.2024 14:10] Abstract 0. Large Language Models (LLMs) show significant potential in economic and strategic interactions, where communication via natural language is often prevalent. This raises key questions: Do LLMs behave rationally? Can they mimic human behavior? Do they tend to reach an efficient and fair outcome? What ...
[10.10.2024 14:10] ********************************************************************************
[10.10.2024 14:10] Abstract 1. Recent advancements in multimodal large language models (MLLMs) have demonstrated significant progress; however, these models exhibit a notable limitation, which we refer to as "face blindness". Specifically, they can engage in general conversations but fail to conduct personalized dialogues targeti...
[10.10.2024 14:10] ********************************************************************************
[10.10.2024 14:10] Abstract 2. Advanced diffusion models like RPG, Stable Diffusion 3 and FLUX have made notable strides in compositional text-to-image generation. However, these methods typically exhibit distinct strengths for compositional generation, with some excelling in handling attribute binding and others in spatial relat...
[10.10.2024 14:10] ********************************************************************************
[10.10.2024 14:10] Abstract 3. Text-to-video (T2V) models like Sora have made significant strides in visualizing complex prompts, which is increasingly viewed as a promising path towards constructing the universal world simulator. Cognitive psychologists believe that the foundation for achieving this goal is the ability to unders...
[10.10.2024 14:10] ********************************************************************************
[10.10.2024 14:10] Abstract 4. We present the Modality Integration Rate (MIR), an effective, robust, and generalized metric to indicate the multi-modal pre-training quality of Large Vision Language Models (LVLMs). Large-scale pre-training plays a critical role in building capable LVLMs, while evaluating its training quality witho...
[10.10.2024 14:10] ********************************************************************************
[10.10.2024 14:10] Abstract 5. Information comes in diverse modalities. Multimodal native AI models are essential to integrate real-world information and deliver comprehensive understanding. While proprietary multimodal native models exist, their lack of openness imposes obstacles for adoptions, let alone adaptations. To fill thi...
[10.10.2024 14:10] ********************************************************************************
[10.10.2024 14:10] Abstract 6. We introduce Pixtral-12B, a 12--billion-parameter multimodal language model. Pixtral-12B is trained to understand both natural images and documents, achieving leading performance on various multimodal benchmarks, surpassing a number of larger models. Unlike many open-source models, Pixtral is also a...
[10.10.2024 14:10] ********************************************************************************
[10.10.2024 14:10] Abstract 7. This paper delves into the interplay between vision backbones and optimizers, unvealing an inter-dependent phenomenon termed \textbf{backbone-optimizer coupling bias} (BOCB). We observe that canonical CNNs, such as VGG and ResNet, exhibit a marked co-dependency with SGD families, while recent archit...
[10.10.2024 14:10] ********************************************************************************
[10.10.2024 14:10] Abstract 8. Video generation requires modeling a vast spatiotemporal space, which demands significant computational resources and data usage. To reduce the complexity, the prevailing approaches employ a cascaded architecture to avoid direct training with full resolution. Despite reducing computational demands, ...
[10.10.2024 14:10] ********************************************************************************
[10.10.2024 14:10] Abstract 9. This research aims to comprehensively explore building a multimodal foundation model for egocentric video understanding. To achieve this goal, we work on three fronts. First, as there is a lack of QA data for egocentric video understanding, we develop a data engine that efficiently generates 7M high...
[10.10.2024 14:10] ********************************************************************************
[10.10.2024 14:10] Abstract 10. Story visualization, the task of generating coherent images based on a narrative, has seen significant advancements with the emergence of text-to-image models, particularly diffusion models. However, maintaining semantic consistency, generating high-quality fine-grained interactions, and ensuring co...
[10.10.2024 14:10] ********************************************************************************
[10.10.2024 14:10] Abstract 11. Through alignment with human preferences, Large Language Models (LLMs) have advanced significantly in generating honest, harmless, and helpful responses. However, collecting high-quality preference data is a resource-intensive and creativity-demanding process, especially for the continual improvemen...
[10.10.2024 14:10] ********************************************************************************
[10.10.2024 14:10] Abstract 12. In this technical report, we present Falcon Mamba 7B, a new base large language model based on the novel Mamba architecture. Falcon Mamba 7B is trained on 5.8 trillion tokens with carefully selected data mixtures. As a pure Mamba-based model, Falcon Mamba 7B surpasses leading open-weight models base...
[10.10.2024 14:10] ********************************************************************************
[10.10.2024 14:10] Abstract 13. Foundation models (FMs) are pre-trained on large-scale datasets and then fine-tuned on a downstream task for a specific application. The most successful and most commonly used fine-tuning method is to update the pre-trained weights via a low-rank adaptation (LoRA). LoRA introduces new weight matrice...
[10.10.2024 14:10] ********************************************************************************
[10.10.2024 14:10] Abstract 14. Despite significant advancements in customizing text-to-image and video generation models, generating images and videos that effectively integrate multiple personalized concepts remains a challenging task. To address this, we present TweedieMix, a novel method for composing customized diffusion mode...
[10.10.2024 14:10] ********************************************************************************
[10.10.2024 14:10] Abstract 15. Video Large Language Models (Video LLMs) have shown promising capabilities in video comprehension, yet they struggle with tracking temporal changes and reasoning about temporal relationships. While previous research attributed this limitation to the ineffective temporal encoding of visual inputs, ou...
[10.10.2024 14:10] ********************************************************************************
[10.10.2024 14:10] Abstract 16. Large language models have been successfully applied to programming assistance tasks, such as code completion, code insertion, and instructional code editing. However, these applications remain insufficiently automated and struggle to effectively integrate various types of information during the pro...
[10.10.2024 14:10] ********************************************************************************
[10.10.2024 14:10] Abstract 17. In this paper, we propose AutoDAN-Turbo, a black-box jailbreak method that can automatically discover as many jailbreak strategies as possible from scratch, without any human intervention or predefined scopes (e.g., specified candidate strategies), and use them for red-teaming. As a result, AutoDAN-...
[10.10.2024 14:10] ********************************************************************************
[10.10.2024 14:10] Abstract 18. Video Temporal Grounding (VTG) is a crucial capability for video understanding models and plays a vital role in downstream tasks such as video browsing and editing. To effectively handle various tasks simultaneously and enable zero-shot prediction, there is a growing trend in employing video LLMs fo...
[10.10.2024 14:10] ********************************************************************************
[10.10.2024 14:10] Abstract 19. This work investigates the selection of high-quality pre-training data from massive corpora to enhance LMs' capabilities for downstream usage. We formulate data selection as a generalized Optimal Control problem, which can be solved theoretically by Pontryagin's Maximum Principle (PMP), yielding a s...
[10.10.2024 14:10] ********************************************************************************
[10.10.2024 14:10] Abstract 20. Instruction tuning-supervised fine-tuning using instruction-response pairs-is a foundational step in transitioning pre-trained Large Language Models (LLMs) into helpful and safe chat assistants. Our hypothesis is that establishing an adequate output space can enable such a transition given the capab...
[10.10.2024 14:10] ********************************************************************************
[10.10.2024 14:10] Abstract 21. Recent progress in large-scale text-to-video (T2V) and image-to-video (I2V) diffusion models has greatly enhanced video generation, especially in terms of keyframe interpolation. However, current image-to-video diffusion models, while powerful in generating videos from a single conditioning frame, n...
[10.10.2024 14:10] ********************************************************************************
[10.10.2024 14:10] Abstract 22. Recently introduced dialogue systems have demonstrated high usability. However, they still fall short of reflecting real-world conversation scenarios. Current dialogue systems exhibit an inability to replicate the dynamic, continuous, long-term interactions involving multiple partners. This shortfal...
[10.10.2024 14:10] ********************************************************************************
[10.10.2024 14:10] Abstract 23. Generative models are transforming creative domains such as music generation, with inference-time strategies like Classifier-Free Guidance (CFG) playing a crucial role. However, CFG doubles inference cost while limiting originality and diversity across generated contents. In this paper, we introduce...
[10.10.2024 14:10] ********************************************************************************
[10.10.2024 14:10] Abstract 24. As multimodal large language models (MLLMs) continue to demonstrate increasingly competitive performance across a broad spectrum of tasks, more intricate and comprehensive benchmarks have been developed to assess these cutting-edge models. These benchmarks introduce new challenges to core capabiliti...
[10.10.2024 14:10] ********************************************************************************
[10.10.2024 14:10] Abstract 25. In this paper, we focus on enhancing a diffusion-based text-to-video (T2V) model during the post-training phase by distilling a highly capable consistency model from a pretrained T2V model. Our proposed method, T2V-Turbo-v2, introduces a significant advancement by integrating various supervision sig...
[10.10.2024 14:10] ********************************************************************************
[10.10.2024 14:10] Abstract 26. While large language models (LLMs) have integrated images, adapting them to graphs remains challenging, limiting their applications in materials and drug design. This difficulty stems from the need for coherent autoregressive generation across texts and graphs. To address this, we introduce Llamole,...
[10.10.2024 14:10] ********************************************************************************
[10.10.2024 14:10] Abstract 27. Generating a long story of several thousand words with narrative coherence using Large Language Models (LLMs) has been a challenging task. Previous research has addressed this challenge by proposing different frameworks that create a story plan and generate a long story based on that plan. However, ...
[10.10.2024 14:10] ********************************************************************************
[10.10.2024 14:10] Abstract 28. In-context learning (ICL) is the ability of a model to learn a new task by observing a few exemplars in its context. While prevalent in NLP, this capability has recently also been observed in Reinforcement Learning (RL) settings. Prior in-context RL methods, however, require entire episodes in the a...
[10.10.2024 14:10] ********************************************************************************
[10.10.2024 14:10] Abstract 29. This paper introduces F5-TTS, a fully non-autoregressive text-to-speech system based on flow matching with Diffusion Transformer (DiT). Without requiring complex designs such as duration model, text encoder, and phoneme alignment, the text input is simply padded with filler tokens to the same length...
[10.10.2024 14:10] ********************************************************************************
[10.10.2024 14:10] Abstract 30. Piano playing requires agile, precise, and coordinated hand control that stretches the limits of dexterity. Hand motion models with the sophistication to accurately recreate piano playing have a wide range of applications in character animation, embodied AI, biomechanics, and VR/AR. In this paper, w...
[10.10.2024 14:10] ********************************************************************************
[10.10.2024 14:10] Abstract 31. Multimodal Large Language Models (MLLMs) are rapidly evolving, demonstrating impressive capabilities as multimodal assistants that interact with both humans and their environments. However, this increased sophistication introduces significant safety concerns. In this paper, we present the first eval...
[10.10.2024 14:10] ********************************************************************************
[10.10.2024 14:10] Abstract 32. As text-to-image diffusion models become advanced enough for commercial applications, there is also increasing concern about their potential for malicious and harmful use. Model unlearning has been proposed to mitigate the concerns by removing undesired and potentially harmful information from the p...
[10.10.2024 14:10] ********************************************************************************
[10.10.2024 14:10] Abstract 33. The text-to-video (T2V) generation models, offering convenient visual creation, have recently garnered increasing attention. Despite their substantial potential, the generated videos may present artifacts, including structural implausibility, temporal inconsistency, and a lack of motion, often resul...
[10.10.2024 14:10] ********************************************************************************
[10.10.2024 14:10] Abstract 34. The research builds and evaluates the adversarial potential to introduce copied code or hallucinated AI recommendations for malicious code in popular code repositories. While foundational large language models (LLMs) from OpenAI, Google, and Anthropic guard against both harmful behaviors and toxic s...
[10.10.2024 14:10] ********************************************************************************
[10.10.2024 14:10] Abstract 35. In real world software development, improper or missing exception handling can severely impact the robustness and reliability of code. Exception handling mechanisms require developers to detect, capture, and manage exceptions according to high standards, but many developers struggle with these tasks...
[10.10.2024 14:10] ********************************************************************************
[10.10.2024 14:10] Abstract 36. Mental health disorders are one of the most serious diseases in the world. Most people with such a disease lack access to adequate care, which highlights the importance of training models for the diagnosis and treatment of mental health disorders. However, in the mental health domain, privacy concer...
[10.10.2024 14:10] ********************************************************************************
[10.10.2024 14:10] Abstract 37. Multi-view consistency remains a challenge for image diffusion models. Even within the Text-to-Texture problem, where perfect geometric correspondences are known a priori, many methods fail to yield aligned predictions across views, necessitating non-trivial fusion methods to incorporate the results...
[10.10.2024 14:10] ********************************************************************************
[10.10.2024 14:10] Abstract 38. This paper introduces TinyEmo, a family of small multi-modal language models for emotional reasoning and classification. Our approach features: (1) a synthetic emotional instruct dataset for both pre-training and fine-tuning stages, (2) a Metric Projector that delegates classification from the langu...
[10.10.2024 14:10] ********************************************************************************
[10.10.2024 14:10] Abstract 39. We propose TextToon, a method to generate a drivable toonified avatar. Given a short monocular video sequence and a written instruction about the avatar style, our model can generate a high-fidelity toonified avatar that can be driven in real-time by another video with arbitrary identities. Existing...
[10.10.2024 14:10] ********************************************************************************
[10.10.2024 14:10] Abstract 40. Recent advancements of large language models (LLMs) have led to claims of AI surpassing humans in natural language processing (NLP) tasks such as textual understanding and reasoning. This work investigates these assertions by introducing CAIMIRA, a novel framework rooted in item response theory (IRT...
[10.10.2024 14:10] ********************************************************************************
[10.10.2024 14:10] Abstract 41. Instruction following is a key capability for LLMs. However, recent studies have shown that LLMs often struggle with instructions containing multiple constraints (e.g. a request to create a social media post "in a funny tone" with "no hashtag"). Despite this, most evaluations focus solely on synthet...
[10.10.2024 14:10] ********************************************************************************
[10.10.2024 14:10] Abstract 42. Not yet. We present SPACE, a benchmark that systematically evaluates spatial cognition in frontier models. Our benchmark builds on decades of research in cognitive science. It evaluates large-scale mapping abilities that are brought to bear when an organism traverses physical environments, smaller-s...
[10.10.2024 14:10] ********************************************************************************
[10.10.2024 14:10] Abstract 43. We introduce MLE-bench, a benchmark for measuring how well AI agents perform at machine learning engineering. To this end, we curate 75 ML engineering-related competitions from Kaggle, creating a diverse set of challenging tasks that test real-world ML engineering skills such as training models, pre...
[10.10.2024 14:10] Read previous papers.
[10.10.2024 14:10] Generating reviews via LLM API.
[10.10.2024 14:10] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –ø–æ–≤–µ–¥–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≤ —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏—Ö –∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–∏—Ö –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è—Ö. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Å–∏–º—É–ª—è—Ü–∏–∏ –∏ –∞–Ω–∞–ª–∏–∑–∞ –∏–≥—Ä –º–µ–∂–¥—É –¥–≤—É–º—è —É—á–∞—Å—Ç–Ω–∏–∫–∞–º–∏, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã—Ö –Ω–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–º —è–∑—ã–∫–µ. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –≤–∫–ª—é—á–∞–µ—Ç —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ
[10.10.2024 14:10] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (MLLM) –¥–ª—è –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –æ–±—â–µ–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥ PVIT (Personalized Visual Instruction Tuning), –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª—è–º –∏–¥–µ–Ω—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –ª—é–¥–µ–π –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö –∏ –≤–µ—Å—Ç–∏ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω
[10.10.2024 14:10] Using data from previous issue: {"desc": "IterComp - —ç—Ç–æ –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–æ–º–ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ç–µ–∫—Å—Ç—É. –û–Ω –∞–≥—Ä–µ–≥–∏—Ä—É–µ—Ç –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π –¥–∏—Ñ—Ñ—É–∑–∏–∏ –∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑—å—é. IterComp –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –º–æ–¥–µ–ª–∏ –ø–æ —Ç—Ä–µ–º –∫–ª—é—á–µ–≤—ã–º –º–µ—Ç—Ä–∏–∫–∞–º –∫–æ–º–ø–æ–∑–∏—Ü–∏–∏ –∏ —Å–æ–∑–¥–∞–µ—Ç –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è
[10.10.2024 14:10] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç PhyGenBench - –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏ —Ñ–∏–∑–∏—á–µ—Å–∫–æ–≥–æ –∑–¥—Ä–∞–≤–æ–≥–æ —Å–º—ã—Å–ª–∞ –≤ –º–æ–¥–µ–ª—è—Ö —Ç–µ–∫—Å—Ç-–≤-–≤–∏–¥–µ–æ (T2V). –ë–µ–Ω—á–º–∞—Ä–∫ –≤–∫–ª—é—á–∞–µ—Ç 160 –ø—Ä–æ–º–ø—Ç–æ–≤, –æ—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏—Ö 27 —Ñ–∏–∑–∏—á–µ—Å–∫–∏—Ö –∑–∞–∫–æ–Ω–æ–≤ –≤ —á–µ—Ç—ã—Ä–µ—Ö —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –æ–±–ª–∞—Å—Ç—è—Ö. –ê–≤—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç PhyGenEval - –Ω–æ–≤—É—é —Å—Ç—Ä—É–∫—Ç
[10.10.2024 14:10] Using data from previous issue: {"desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–π –ø–æ–∫–∞–∑–∞—Ç–µ–ª—å Modality Integration Rate (MIR) –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ (Large Vision Language Models, LVLMs). MIR –ø–æ–∑–≤–æ–ª—è–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –æ—Ü–µ–Ω–∏–≤–∞—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –æ–±
[10.10.2024 14:10] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Aria - –æ—Ç–∫—Ä—ã—Ç—É—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞. –ú–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É —Å–º–µ—Å–∏ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –∏ –∏–º–µ–µ—Ç 3,9 –º–ª—Ä–¥ –∏ 3,5 –º–ª—Ä–¥ –∞–∫—Ç–∏–≤–∏—Ä—É–µ–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –∏ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ. Aria –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç Pixtral-12B –∏ Llama3.2-11B, –∫–æ–Ω–∫—É—Ä–∏—Ä—É—è
[10.10.2024 14:10] Using data from previous issue: {"desc": "Pixtral-12B - —ç—Ç–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å —Å 12 –º–∏–ª–ª–∏–∞—Ä–¥–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, —Å–ø–æ—Å–æ–±–Ω–∞—è –ø–æ–Ω–∏–º–∞—Ç—å –∫–∞–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, —Ç–∞–∫ –∏ —Ç–µ–∫—Å—Ç. –û–Ω–∞ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –±–æ–ª–µ–µ –∫—Ä—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ –ø–æ —Ä–∞–∑–ª–∏—á–Ω—ã–º –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–º –ø–æ–∫–∞–∑–∞—Ç–µ–ª—è–º, –ø—Ä–∏ —ç—Ç–æ–º –Ω–µ —É—Å—Ç—É–ø–∞—è –≤ –∑–∞–¥–∞—á–∞—Ö –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞. Pixtral –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–æ–≤
[10.10.2024 14:10] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –≤–∑–∞–∏–º–æ—Å–≤—è–∑—å –º–µ–∂–¥—É –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞–º–∏ –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π –¥–ª—è –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞–º–∏, –≤—ã—è–≤–ª—è—è —Ñ–µ–Ω–æ–º–µ–Ω '—Å–º–µ—â–µ–Ω–∏—è —Å–≤—è–∑–∏ –º–µ–∂–¥—É –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–æ–º' (BOCB). –ê–≤—Ç–æ—Ä—ã –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–µ CNN –ª—É—á—à–µ —Ä–∞–±–æ—Ç–∞—é—Ç —Å —Å–µ–º–µ–π—Å—Ç–≤–æ–º SGD-–æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–æ–≤, –∞ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –∞—Ä—Ö–∏—Ç
[10.10.2024 14:10] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –ø–∏—Ä–∞–º–∏–¥–∞–ª—å–Ω–æ–≥–æ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è –ø–æ—Ç–æ–∫–æ–≤ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –ø–µ—Ä–µ–æ—Å–º—ã—Å–ª–∏–≤–∞–µ—Ç —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—é —à—É–º–æ–ø–æ–¥–∞–≤–ª–µ–Ω–∏—è –∫–∞–∫ —Å–µ—Ä–∏—é –ø–∏—Ä–∞–º–∏–¥–∞–ª—å–Ω—ã—Ö —ç—Ç–∞–ø–æ–≤, –≥–¥–µ —Ç–æ–ª—å–∫–æ —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —ç—Ç–∞–ø —Ä–∞–±–æ—Ç–∞–µ—Ç –≤ –ø–æ–ª–Ω–æ–º —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–∏. –ü–æ–¥—Ö–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≤–µ—Å—å –ø—Ä–æ—Ü–µ—Å—Å end-to-end 
[10.10.2024 14:10] Using data from previous issue: {"desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–æ –Ω–∞ —Å–æ–∑–¥–∞–Ω–∏–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è —ç–≥–æ—Ü–µ–Ω—Ç—Ä–∏—á–µ—Å–∫–æ–≥–æ –≤–∏–¥–µ–æ. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –¥–∞–Ω–Ω—ã—Ö, —Å–æ–∑–¥–∞—é—â–∏–π 7 –º–∏–ª–ª–∏–æ–Ω–æ–≤ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –ø–∞—Ä –≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç –¥–ª—è —ç–≥–æ—Ü–µ–Ω—Ç—Ä–∏—á–µ—Å–∫–∏—Ö –≤–∏–¥–µ–æ —Ä–∞–∑–ª–∏—á–Ω–æ–π –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏. –û–Ω–∏ —Ç–∞–∫–∂–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ 
[10.10.2024 14:10] Using data from previous issue: {"desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ Story-Adapter –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–ª–∏–Ω–Ω—ã—Ö –∏—Å—Ç–æ—Ä–∏–π. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω—ã–π –ø–æ–¥—Ö–æ–¥ —Å –≥–ª–æ–±–∞–ª—å–Ω—ã–º –º–æ–¥—É–ª–µ–º –∫—Ä–æ—Å—Å-–≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏. Story-Adapter –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–µ–Ω —Å –≤—ã
[10.10.2024 14:10] Using data from previous issue: {"desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –º–µ—Ç–æ–¥ SynPO –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ä—É—á–Ω–æ–π —Ä–∞–∑–º–µ—Ç–∫–∏. SynPO –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω—ã–π –º–µ—Ö–∞–Ω–∏–∑–º, –≥–¥–µ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Å–æ–∑–¥–∞–µ—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã, –∞ —É–ª—É—á—à–∞—Ç–µ–ª—å –æ—Ç–≤–µ—Ç–æ–≤ –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ —Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤—É–µ—Ç –æ—Ç–≤–µ—Ç—ã –º–æ–¥–µ–ª–∏. –ü–æ—Å–ª–µ —á–µ—Ç—ã—Ä–µ—Ö –∏—Ç–µ—Ä–∞—Ü–∏–π SynPO, –º–æ–¥–µ–ª–∏ Llama3-8
[10.10.2024 14:10] Using data from previous issue: {"desc": "Falcon Mamba 7B - —ç—Ç–æ –Ω–æ–≤–∞—è –±–∞–∑–æ–≤–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å, –æ—Å–Ω–æ–≤–∞–Ω–Ω–∞—è –Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ Mamba. –û–Ω–∞ –æ–±—É—á–µ–Ω–∞ –Ω–∞ 5,8 —Ç—Ä–∏–ª–ª–∏–æ–Ω–∞—Ö —Ç–æ–∫–µ–Ω–æ–≤ –∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –≤–µ–¥—É—â–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤, —Ç–∞–∫–∏–µ –∫–∞–∫ Mistral 7B –∏ Llama3.1 8B. Falcon Mamba 7B –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ª—É—á—à—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å—Ä–µ–¥–∏ –º–æ–¥–µ–ª–µ–π Mamba 
[10.10.2024 14:10] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º EVA (Explained Variance Adaptation). –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ —É–ª—É—á—à–∞–µ—Ç —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ LoRA, –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É—è –Ω–æ–≤—ã–µ –≤–µ—Å–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∞–Ω–Ω—ã—Ö —Å –ø–æ–º–æ—â—å—é —Å–∏–Ω–≥—É–ª—è—Ä–Ω–æ–≥–æ —Ä–∞–∑–ª–æ–∂–µ–Ω–∏—è. EVA –ø–µ—Ä–µ—Ä–∞—Å–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ä–∞–Ω–≥–∏ –º–µ–∂–¥—É –º–∞—Ç—Ä–∏—Ü–∞–º–∏ 
[10.10.2024 14:10] Using data from previous issue: {"desc": "TweedieMix - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ —ç—Ç–∞–ø–µ –≤—ã–≤–æ–¥–∞. –û–Ω —Ä–∞–∑–¥–µ–ª—è–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞ –¥–≤–∞ —ç—Ç–∞–ø–∞: —Å–Ω–∞—á–∞–ª–∞ –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è —Ç–µ—Ö–Ω–∏–∫–∞ —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏—è —Å —É—á–µ—Ç–æ–º –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –æ–±—ä–µ–∫—Ç–æ–≤, –∞ –∑–∞—Ç–µ–º –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ñ–æ—Ä–º—É–ª–∞ –¢–≤–∏–¥–∏ –¥–ª—è —Å–º–µ—à–∏–≤–∞–Ω–∏—è –≤–Ω–µ—à–Ω–µ–≥–æ –≤–∏–¥–∞ –ø–µ
[10.10.2024 14:10] Using data from previous issue: {"desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ –≤–∏–¥–µ–æ-–º–æ–¥–µ–ª–∏ –±–æ–ª—å—à–æ–≥–æ —è–∑—ã–∫–∞ (Video LLMs) –∏—Å–ø—ã—Ç—ã–≤–∞—é—Ç —Ç—Ä—É–¥–Ω–æ—Å—Ç–∏ —Å –ø–æ–Ω–∏–º–∞–Ω–∏–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ—Ç–Ω–æ—à–µ–Ω–∏–π –Ω–µ –∏–∑-–∑–∞ –Ω–µ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –∞ –∏–∑-–∑–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –±–∞–∑–æ–≤–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∫–æ–Ω—Ü–µ–ø—Ü–∏–π. –î–ª—è —Ä–µ—à–µ–Ω–∏—è —ç—Ç–æ–π –ø—Ä–æ–±–ª–µ–º—ã
[10.10.2024 14:10] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è –ø–æ–º–æ—â–∏ –≤ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–∏, –∏—Å–ø–æ–ª—å–∑—É—é—â—É—é –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É—é—â–∏–π –∏—Å—Ç–æ—Ä–∏—é –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è, —Ç–µ–∫—É—â–∏–π –∫–æ–¥ –∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è. –û–Ω–∏ —Ç–∞–∫–∂–µ —Å–æ–∑–¥–∞–ª–∏ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ APEval –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π –∏ pipeline –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
[10.10.2024 14:10] Using data from previous issue: {"desc": "AutoDAN-Turbo - —ç—Ç–æ –º–µ—Ç–æ–¥ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è —É—è–∑–≤–∏–º–æ—Å—Ç–µ–π –≤ —Å–∏—Å—Ç–µ–º–∞—Ö –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ –±–µ–∑ –≤–º–µ—à–∞—Ç–µ–ª—å—Å—Ç–≤–∞ —á–µ–ª–æ–≤–µ–∫–∞. –û–Ω –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –±–∞–∑–æ–≤—ã–µ –º–µ—Ç–æ–¥—ã, –¥–æ—Å—Ç–∏–≥–∞—è –Ω–∞ 74.3% –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–æ–≥–æ —Å—Ä–µ–¥–Ω–µ–≥–æ –ø–æ–∫–∞–∑–∞—Ç–µ–ª—è —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏ –∞—Ç–∞–∫ –Ω–∞ –ø—É–±–ª–∏—á–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö. –ù–∞ –º–æ–¥–µ–ª–∏ GPT-4-1106-t
[10.10.2024 14:10] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∑–∞–¥–∞—á–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏ –≤ –≤–∏–¥–µ–æ (Video Temporal Grounding). –ê–≤—Ç–æ—Ä—ã –≤–≤–æ–¥—è—Ç –∫–æ–Ω—Ü–µ–ø—Ü–∏—é –∫–∞—É–∑–∞–ª—å–Ω–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è —Å–æ–±—ã—Ç–∏–π, –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—è –≤–∏–¥–µ–æ –∫–∞–∫ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–æ–±—ã—Ç–∏–π. –û–Ω–∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–æ–¥–µ–ª—å TRACE - –º—É–ª—å—Ç–∏–∑–∞–¥–∞—á–Ω—É—é –≤–∏–¥–µ–æ-LLM, –∫–æ—Ç–æ—Ä–∞—è –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤–∏–∑—É
[10.10.2024 14:10] Using data from previous issue: {"desc": "–≠—Ç–∞ —Ä–∞–±–æ—Ç–∞ –∏—Å—Å–ª–µ–¥—É–µ—Ç –≤—ã–±–æ—Ä –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è –∏–∑ –º–∞—Å—Å–∏–≤–Ω—ã—Ö –∫–æ—Ä–ø—É—Å–æ–≤ —Å —Ü–µ–ª—å—é —É–ª—É—á—à–µ–Ω–∏—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã —Ñ–æ—Ä–º—É–ª–∏—Ä—É—é—Ç –≤—ã–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∫–∞–∫ –æ–±–æ–±—â–µ–Ω–Ω—É—é –∑–∞–¥–∞—á—É –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è, —Ä–µ—à–∞–µ–º—É—é —Å –ø–æ–º–æ—â—å—é –ø—Ä–∏–Ω—Ü–∏–ø–∞ –º–∞–∫—Å–∏–º—É–º–∞ –ü–æ–Ω—Ç—Ä—è–≥–∏–Ω–∞. –ù–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å
[10.10.2024 14:10] Using data from previous issue: {"desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥ Response Tuning (RT) –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π. RT —Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –Ω–∞ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –æ—Ç–≤–µ—Ç–æ–≤, –ø–æ–∑–≤–æ–ª—è—è –º–æ–¥–µ–ª—è–º —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ —Ä–µ–∞–≥–∏—Ä–æ–≤–∞—Ç—å –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –º–æ–¥–µ–ª–∏ RT –º–æ–≥—É—Ç –±—ã—Ç—å —Ç–∞–∫–∏–º–∏ –∂–µ –ø–æ–ª–µ–∑–Ω—ã–º
[10.10.2024 14:10] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –¥–≤—É–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏–∏ –∫–ª—é—á–µ–≤—ã—Ö –∫–∞–¥—Ä–æ–≤ –≤ –º–æ–¥–µ–ª—è—Ö –¥–∏—Ñ—Ñ—É–∑–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ-–≤–∏–¥–µ–æ. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—É—é –≤—ã–±–æ—Ä–∫—É –≤–¥–æ–ª—å –ø—Ä—è–º–æ–≥–æ –∏ –æ–±—Ä–∞—Ç–Ω–æ–≥–æ –ø—É—Ç–µ–π, –æ–±—É—Å–ª–æ–≤–ª–µ–Ω–Ω—É—é –Ω–∞—á–∞–ª—å–Ω—ã–º –∏ –∫–æ–Ω–µ—á–Ω—ã–º –∫–∞–¥—Ä–∞–º–∏, —á—Ç–æ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –±–æ–ª–µ–µ —Å–æ–≥–ª–∞—Å
[10.10.2024 14:10] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É –¥–∏–∞–ª–æ–≥–æ–≤ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Mixed-Session Conversation, –∫–æ—Ç–æ—Ä–∞—è —Å–ø–æ—Å–æ–±–Ω–∞ –≤–µ—Å—Ç–∏ —Ä–∞–∑–≥–æ–≤–æ—Ä—ã —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –ø–∞—Ä—Ç–Ω–µ—Ä–∞–º–∏ –≤ –º–Ω–æ–≥–æ—Å–µ—Å—Å–∏–æ–Ω–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö MiSC, —Å–æ—Å—Ç–æ—è—â–∏–π –∏–∑ 6 –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö —Å–µ—Å—Å–∏–π —Å —É—á–∞—Å—Ç–∏–µ–º —á–µ—Ç—ã—Ä–µ—Ö —Å–æ–±–µ—Å–µ–¥–Ω–∏–∫–æ–≤. –¢–∞–∫–∂–µ –ø—Ä–µ–¥—Å
[10.10.2024 14:10] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –º—É–∑—ã–∫–∏, –Ω–∞–∑—ã–≤–∞–µ–º—ã–π diversity-rewarded CFG distillation. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—é Classifier-Free Guidance (CFG) —Å –æ–±—É—á–µ–Ω–∏–µ–º —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –ø–æ–æ—â—Ä–µ–Ω–∏—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è. –ê–≤—Ç–æ—Ä—ã –ø—Ä–∏–º–µ–Ω—è—é—Ç –º–µ—Ç–æ–¥ –∫ –º–æ–¥–µ–ª–∏ MusicLM –¥–ª—è –≥–µ
[10.10.2024 14:10] Using data from previous issue: {"desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ ING-VP –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –≤–æ–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ –º–Ω–æ–≥–æ—à–∞–≥–æ–≤–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ë–µ–Ω—á–º–∞—Ä–∫ —Å–æ–¥–µ—Ä–∂–∏—Ç 6 –∏–≥—Ä —Å 300 —É—Ä–æ–≤–Ω—è–º–∏ –∏ 6 –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è–º–∏ –∫–∞–∂–¥—ã–π, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø—Ä–æ–≤–µ—Å—Ç–∏ –±–æ–ª–µ–µ 60 000 —Ä–∞—É–Ω–¥–æ–≤ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è. ING-VP –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –º–æ–¥–µ–ª–∏ –≤ —Ä
[10.10.2024 14:10] Using data from previous issue: {"desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –º–µ—Ç–æ–¥ T2V-Turbo-v2 –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –≤ –≤–∏–¥–µ–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∏—Ñ—Ñ—É–∑–∏–∏. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –¥–∏—Å—Ç–∏–ª–ª–∏—Ä–æ–≤–∞—Ç—å –º–æ–¥–µ–ª—å —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –∏–∑ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π T2V –º–æ–¥–µ–ª–∏, –∏—Å–ø–æ–ª—å–∑—É—è —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Å–∏–≥–Ω–∞–ª—ã –æ–±—É—á–µ–Ω–∏—è. –ú–µ—Ç–æ–¥ –≤–∫–ª—é—á–∞–µ—Ç –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –æ–±—Ä
[10.10.2024 14:10] Using data from previous issue: {"desc": "Llamole - —ç—Ç–æ –ø–µ—Ä–≤–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å, —Å–ø–æ—Å–æ–±–Ω–∞—è –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç –∏ –≥—Ä–∞—Ñ—ã –º–æ–ª–µ–∫—É–ª. –û–Ω–∞ –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –±–∞–∑–æ–≤—É—é —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å —Å Graph Diffusion Transformer –∏ –≥—Ä–∞—Ñ–æ–≤—ã–º–∏ –Ω–µ–π—Ä–æ–Ω–Ω—ã–º–∏ —Å–µ—Ç—è–º–∏ –¥–ª—è –º–Ω–æ–≥–æ—É—Å–ª–æ–≤–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –º–æ–ª–µ–∫—É–ª –∏ –≤—ã–≤–æ–¥–∞ —Ä–µ–∞–∫—Ü–∏–π. Llamole —Ç–∞–∫–∂–µ –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç A* –ø–æ–∏—Å–∫ —Å 
[10.10.2024 14:10] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –∏—Å—Ç–æ—Ä–∏–π —Å –ø–æ–º–æ—â—å—é –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –Ω–∞–∑—ã–≤–∞–µ–º—ã–π CritiCS. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∫–æ–ª–ª–µ–∫—Ç–∏–≤–Ω—ã–π –º–µ—Ö–∞–Ω–∏–∑–º –∫—Ä–∏—Ç–∏–∫–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∏ –≤—ã—Ä–∞–∑–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã—Ö –∏—Å—Ç–æ—Ä–∏–π. CritiCS —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –¥–≤—É—Ö —ç—Ç–∞–ø–æ–≤: —É—Ç–æ—á–Ω–µ–Ω–∏—è –ø–ª–∞–Ω–∞ (CrPla
[10.10.2024 14:10] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ - Retrieval-Augmented Decision Transformer (RA-DT). RA-DT –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≤–Ω–µ—à–Ω—é—é –ø–∞–º—è—Ç—å –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –ø—Ä–æ—à–ª–æ–≥–æ –æ–ø—ã—Ç–∞ –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–æ–ª—å–∫–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ç–µ–∫—É—â–µ–π —Å–∏—Ç—É–∞—Ü–∏–∏ –ø–æ–¥-—Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏. –ú–µ—Ç–æ–¥ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –æ–±—É—á–µ–Ω–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞ –∏–∑–≤–ª–µ—á–µ–Ω
[10.10.2024 14:10] Using data from previous issue: {"desc": "F5-TTS - —ç—Ç–æ –Ω–µ–∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ —Å–∏–Ω—Ç–µ–∑–∞ —Ä–µ—á–∏, –æ—Å–Ω–æ–≤–∞–Ω–Ω–∞—è –Ω–∞ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–∏ –ø–æ—Ç–æ–∫–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Diffusion Transformer. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Å–ª–æ–∂–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, F5-TTS –ø—Ä–æ—Å—Ç–æ –¥–æ–ø–æ–ª–Ω—è–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–π –≤–≤–æ–¥ –¥–æ –¥–ª–∏–Ω—ã —Ä–µ—á–∏ –∏ –≤—ã–ø–æ–ª–Ω—è–µ—Ç —à—É–º–æ–ø–æ–¥–∞–≤–ª–µ–Ω–∏–µ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏. –°–∏—Å—Ç–µ–º–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç ConvNeXt –¥–ª—è —É–ª—É
[10.10.2024 14:10] Using data from previous issue: {"desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π 10 —á–∞—Å–æ–≤ 3D-–¥–≤–∏–∂–µ–Ω–∏–π —Ä—É–∫ –∏ –∞—É–¥–∏–æ –æ—Ç 15 –ø–∏–∞–Ω–∏—Å—Ç–æ–≤ –≤—ã—Å–æ–∫–æ–≥–æ —É—Ä–æ–≤–Ω—è, –∏–≥—Ä–∞—é—â–∏—Ö –∫–ª–∞—Å—Å–∏—á–µ—Å–∫—É—é –º—É–∑—ã–∫—É. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ —Å–∏—Å—Ç–µ–º—É –±–µ–∑–º–∞—Ä–∫–µ—Ä–Ω–æ–≥–æ –∑–∞—Ö–≤–∞—Ç–∞ –¥–≤–∏–∂–µ–Ω–∏–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º—É–ª—å—Ç–∏—Ä–∞–∫—É—Ä—Å–Ω–æ–π —Å—ä–µ–º–∫–∏ –∏ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –æ—Ü–µ–Ω–∫–∏ –ø–æ–∑—ã. –ù–∞ –æ—Å–Ω
[10.10.2024 14:10] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –ø–µ—Ä–≤—ã–π –∞–Ω–∞–ª–∏–∑ –ø—Ä–æ–±–ª–µ–º—ã –º–Ω–æ–≥–æ–º–æ–¥–∞–ª—å–Ω–æ–π —Å–∏—Ç—É–∞—Ü–∏–æ–Ω–Ω–æ–π –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (MLLM). –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö MSSBench –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ MLLM —É—á–∏—Ç—ã–≤–∞—Ç—å –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å–∏—Ç—É–∞—Ü–∏—è—Ö. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑–∞–ª–æ, —á—Ç–æ —Å—É—â
[10.10.2024 14:10] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—é –º–µ—Ç–æ–¥–æ–≤ —Ä–∞–∑–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è –Ω–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏. –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–ª–∏ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø–æ–¥—Ö–æ–¥—ã –∫ —Ä–∞–∑–æ–±—É—á–µ–Ω–∏—é –ø–æ –ø—è—Ç–∏ –∫–ª—é—á–µ–≤—ã–º –∞—Å–ø–µ–∫—Ç–∞–º –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –≤—ã—è–≤–∏–ª–æ, —á—Ç–æ –≤—Å–µ –º–µ—Ç–æ–¥—ã –∏–º–µ—é—Ç –ø–æ–±–æ—á–Ω—ã–µ —ç—Ñ—Ñ–µ–∫—Ç—ã –∏–ª–∏
[10.10.2024 14:10] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ BroadWay –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –∏–∑ —Ç–µ–∫—Å—Ç–∞ –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏–ª–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤. –ú–µ—Ç–æ–¥ –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ –Ω–∞–±–ª—é–¥–µ–Ω–∏–∏ —Å–≤—è–∑–∏ –º–µ–∂–¥—É —Ä–∞–∑–ª–∏—á–∏—è–º–∏ –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∫–∞—Ä—Ç–∞—Ö –≤–Ω–∏–º–∞–Ω–∏—è –∏ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –Ω–µ—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å—é –≤ –≤–∏–¥–µ–æ. BroadWay –≤–∫–ª—é—á–∞–µ—Ç –¥–≤–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞: Tempora
[10.10.2024 14:10] Using data from previous issue: {"desc": "–î–∞–Ω–Ω–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ –∏–∑—É—á–µ–Ω–∏—é –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö —É—è–∑–≤–∏–º–æ—Å—Ç–µ–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –∫–æ–¥–∞. –ê–≤—Ç–æ—Ä—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç, —á—Ç–æ –ø—Ä–∏ —Ä–µ–∑–∫–æ–º –∏–∑–º–µ–Ω–µ–Ω–∏–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, –Ω–∞–ø—Ä–∏–º–µ—Ä, –ø—Ä–∏ —Ä–µ—à–µ–Ω–∏–∏ –∑–∞–¥–∞—á–∏ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è, LLM –º–æ–≥—É—Ç —Å–Ω–∏–∑–∏—Ç—å —Å–≤–æ—é –∑–∞—â–∏—Ç—É –∏ –ø—Ä–µ–¥–ª–æ–∂–∏—Ç—å –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ –≤—Ä
[10.10.2024 14:10] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –ø—Ä–æ–±–ª–µ–º–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏—Å–∫–ª—é—á–µ–Ω–∏–π –≤ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–≥–æ –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –∏ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç —Ä–µ—à–µ–Ω–∏–µ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –ê–≤—Ç–æ—Ä—ã –≤—ã—è–≤–∏–ª–∏ —Ç—Ä–∏ –∫–ª—é—á–µ–≤—ã–µ –ø—Ä–æ–±–ª–µ–º—ã: –Ω–µ—á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —Ö—Ä—É–ø–∫–æ–≥–æ –∫–æ–¥–∞, –Ω–µ—Ç–æ—á–Ω—ã–π –∑–∞—Ö–≤–∞—Ç —Ç–∏–ø–æ–≤ –∏—Å–∫–ª—é—á–µ–Ω–∏–π –∏ –∏—Å–∫–∞–∂–µ–Ω–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è
[10.10.2024 14:10] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç MentalArena - —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –æ–±–ª–∞—Å—Ç–∏ –ø—Å–∏—Ö–∏—á–µ—Å–∫–æ–≥–æ –∑–¥–æ—Ä–æ–≤—å—è –ø—É—Ç–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ Symptom Encoder –¥–ª—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ–≤–µ–¥–µ–Ω–∏—è –ø–∞—Ü–∏–µ–Ω—Ç–æ–≤ –∏ Symptom Decoder –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–∏–∞–ª–æ–≥–æ–º –º–µ–∂–¥—É –ø–∞—Ü–∏–µ–Ω—Ç–æ–º –∏ —Ç–µ—Ä–∞–ø–µ–≤
[10.10.2024 14:10] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –ø—Ä–æ–±–ª–µ–º–µ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –≤ –º–æ–¥–µ–ª—è—Ö –¥–∏—Ñ—Ñ—É–∑–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –ê–≤—Ç–æ—Ä—ã –∏—Å—Å–ª–µ–¥—É—é—Ç —ç—Ç–æ—Ç –≤–æ–ø—Ä–æ—Å –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ —Ä–∞–±–æ—á–µ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞ Collaborative Control –¥–ª—è –∑–∞–¥–∞—á–∏ Text-to-Texture —Å PBR (Physically Based Rendering). –ú–æ–¥–µ–ª—å Collaborative Control –Ω–∞–ø—Ä—è–º—É—é –º–æ–¥–µ–ª–∏—Ä—É–µ—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª
[10.10.2024 14:10] Using data from previous issue: {"desc": "TinyEmo - —ç—Ç–æ —Å–µ–º–µ–π—Å—Ç–≤–æ –Ω–µ–±–æ–ª—å—à–∏—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏. –ú–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏, –∞ —Ç–∞–∫–∂–µ –≤–∫–ª—é—á–∞–µ—Ç Metric Projector –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏. TinyEmo —Å–ø–æ—Å–æ–±–Ω–∞ –≤—ã–ø–æ–ª–Ω—è—Ç—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é —ç–º
[10.10.2024 14:10] Using data from previous issue: {"desc": "TextToon - —ç—Ç–æ –º–µ—Ç–æ–¥ —Å–æ–∑–¥–∞–Ω–∏—è —É–ø—Ä–∞–≤–ª—è–µ–º–æ–≥–æ –º—É–ª—å—Ç—è—à–Ω–æ–≥–æ –∞–≤–∞—Ç–∞—Ä–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ—Ä–æ—Ç–∫–æ–≥–æ –≤–∏–¥–µ–æ –∏ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –æ–ø–∏—Å–∞–Ω–∏—è —Å—Ç–∏–ª—è. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —É—Å–ª–æ–≤–Ω–æ–µ –≤—Å—Ç—Ä–∞–∏–≤–∞–Ω–∏–µ Tri-plane –¥–ª—è —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ–≥–æ –∏ —Å—Ç–∏–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–≥–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –ª–∏—Ü–∞ –≤ –≥–∞—É—Å—Å–æ–≤–æ–º –ø–æ–ª–µ –¥–µ—Ñ–æ—Ä–º–∞—Ü–∏–∏. –ú–æ–¥–µ–ª—å —É–ª—É—á—à–∞–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —Å—Ç–∏–ª–∏–∑–∞—Ü–∏–∏ 3D Ga
[10.10.2024 14:10] Using data from previous issue: {"desc": "CAIMIRA - –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –∫ —Ä–µ—à–µ–Ω–∏—é –∑–∞–¥–∞—á —É –ª—é–¥–µ–π –∏ –ò–ò –≤ –æ–±–ª–∞—Å—Ç–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ, –æ—Å–Ω–æ–≤–∞–Ω–Ω–æ–µ –Ω–∞ –∞–Ω–∞–ª–∏–∑–µ –±–æ–ª–µ–µ 300 000 –æ—Ç–≤–µ—Ç–æ–≤ –æ—Ç ~70 —Å–∏—Å—Ç–µ–º –ò–ò –∏ 155 –ª—é–¥–µ–π, –≤—ã—è–≤–∏–ª–æ —Ä–∞–∑–ª–∏—á–∏—è –≤ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã—Ö –Ω–∞–≤—ã–∫–∞—Ö –≤ —Ä–∞–∑–Ω—ã—Ö –æ–±–ª–∞—Å—Ç—è—Ö –∑–Ω–∞–Ω–∏–π. –õ—é–¥–∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç –ò
[10.10.2024 14:10] Querying the API.
[10.10.2024 14:10] Got response. {
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç RealInstruct - –ø–µ—Ä–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å–ª–µ–¥–æ–≤–∞—Ç—å —Ä–µ–∞–ª—å–Ω—ã–º –º–Ω–æ–≥–æ–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –¥–∞–∂–µ GPT-4 –Ω–µ —Å–æ–±–ª—é–¥–∞–µ—Ç —Ö–æ—Ç—è –±—ã –æ–¥–Ω–æ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –≤ –±–æ–ª–µ–µ —á–µ–º 21% –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç pipeline DeCRIM –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π —Å–ª–µ–¥–æ–≤–∞—Ç—å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è–º. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç, —á—Ç–æ DeCRIM –ø–æ–≤—ã—à–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å Mistral –Ω–∞ 7.3% –≤ RealInstruct –∏ 8.0% –≤ IFEval.",
  "tags": ["#–º–Ω–æ–≥–æ–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–µ_–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏", "#DeCRIM", "#RealInstruct"],
  "categories": ["#nlp", "#benchmark", "#dataset", "#code", "#rag"],
  "emoji": "üéØ",
  "title": "–ü—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –≤ —Å–ª–µ–¥–æ–≤–∞–Ω–∏–∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º –¥–ª—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π"
}
[10.10.2024 14:10] Using data from previous issue: {"desc": "SPACE - —ç—Ç–æ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è —É –ø–µ—Ä–µ–¥–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞. –û–Ω –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ –º–Ω–æ–≥–æ–ª–µ—Ç–Ω–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è—Ö –≤ –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω–æ–π –Ω–∞—É–∫–µ –∏ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫ –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω–æ–º—É –∫–∞—Ä—Ç–æ–≥—Ä–∞—Ñ–∏—Ä–æ–≤–∞–Ω–∏—é, –º–µ–ª–∫–æ–º–∞—Å—à—Ç–∞–±–Ω–æ–º—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é –æ —Ñ–æ—Ä–º–∞—Ö –∏ —Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω–∏–∏ –æ–±
[10.10.2024 14:10] Using data from previous issue: {"desc": "MLE-bench - —ç—Ç–æ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤ –≤ –æ–±–ª–∞—Å—Ç–∏ –∏–Ω–∂–µ–Ω–µ—Ä–∏–∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –û–Ω –≤–∫–ª—é—á–∞–µ—Ç 75 —Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏–π —Å Kaggle, –æ—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏—Ö —Ä–∞–∑–ª–∏—á–Ω—ã–µ –∞—Å–ø–µ–∫—Ç—ã ML-–∏–Ω–∂–µ–Ω–µ—Ä–∏–∏. –ê–≤—Ç–æ—Ä—ã —É—Å—Ç–∞–Ω–æ–≤–∏–ª–∏ –±–∞–∑–æ–≤—ã–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ —è–∑—ã–∫–æ
[10.10.2024 14:10] Renaming data file.
[10.10.2024 14:10] Renaming previous data. hf_papers.json to 2024-10-09_hf_papers.json
[10.10.2024 14:10] Saving new data file.
[10.10.2024 14:10] Generating page.
[10.10.2024 14:10] Renaming previous page.
[10.10.2024 14:10] Renaming previous data. index.html to 2024-10-09_hf_papers.html
[10.10.2024 14:10] Writing result.
[10.10.2024 14:10] Renaming log file.
[10.10.2024 14:10] Renaming previous data. log.txt to 2024-10-09_last_log.txt
