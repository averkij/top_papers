[31.10.2024 20:13] [Experimental] Generating an image for paper CORAL: Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.
[31.10.2024 20:13] [Experimental] Image for paper CORAL: Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation already exists.
[31.10.2024 20:13] [Experimental] Generating an image for paper A Large Recurrent Action Model: xLSTM enables Fast Inference for Robotics Tasks.
[31.10.2024 20:13] [Experimental] Image for paper A Large Recurrent Action Model: xLSTM enables Fast Inference for Robotics Tasks already exists.
[31.10.2024 20:13] [Experimental] Generating an image for paper Decoding Reading Goals from Eye Movements.
[31.10.2024 20:13] [Experimental] Image for paper Decoding Reading Goals from Eye Movements already exists.
[31.10.2024 20:13] [Experimental] Generating an image for paper ReferEverything: Towards Segmenting Everything We Can Speak of in Videos.
[31.10.2024 20:13] OpenAI request. Model: gpt-4o-mini. Prompt: Write a text with image prompt in style of surrealism and modern art based on the following paper. Use key themes and elements from it. Add instruction to write a text that reads as brief paper title as a label on some object on an image. Style: linear art on white background. Return only prompt and nothing else. Title: 'ReferEverything: Towards Segmenting Everything We Can Speak of in Videos' Text: 'We present REM, a framework for segmenting a wide range of concepts in video that can be described through natural language. Our method capitalizes on visual-language representations learned by video diffusion models on Internet-scale datasets. A key insight of our approach is preserving as much of the generative model's original representation as possible, while fine-tuning it on narrow-domain Referral Object Segmentation datasets. As a result, our framework can accurately segment and track rare and unseen objects, despite being trained on object masks from a limited set of categories. Additionally, it can generalize to non-object dynamic concepts, such as waves crashing in the ocean, as demonstrated in our newly introduced benchmark for Referral Video Process Segmentation (Ref-VPS). Our experiments show that REM performs on par with state-of-the-art approaches on in-domain datasets, like Ref-DAVIS, while outperforming them by up to twelve points in terms of region similarity on out-of-domain data, leveraging the power of Internet-scale pre-training.'
[31.10.2024 20:13] Response: **Prompt:** Create an image in the style of surrealism and modern art featuring a linear representation of a vast ocean scene where dynamic waves are segmented into distinct, colorful objects resembling various everyday items such as clocks, books, and plants, illustrating the concept of segmenting everything we can speak of in videos. In the background, abstract shapes suggest the presence of a video frame, blending into the ocean. Above the scene, include a label in a minimalist font that reads: "ReferEverything: Towards Segmenting Everything We Can Speak of in Videos." The image should have a clean white background to highlight the linear art and vivid colors of the segmented objects.
[31.10.2024 20:13] Generating image by prompt: **Prompt:** Create an image in the style of surrealism and modern art featuring a linear representation of a vast ocean scene where dynamic waves are segmented into distinct, colorful objects resembling various everyday items such as clocks, books, and plants, illustrating the concept of segmenting everything we can speak of in videos. In the background, abstract shapes suggest the presence of a video frame, blending into the ocean. Above the scene, include a label in a minimalist font that reads: "ReferEverything: Towards Segmenting Everything We Can Speak of in Videos." The image should have a clean white background to highlight the linear art and vivid colors of the segmented objects..
[31.10.2024 20:13] Saving generated image from https://fal.media/files/rabbit/7-jvxKligxH5GWau1QY0E.png to cf2371629ffd5ab5.jpg.
[31.10.2024 22:11] Read previous papers.
[31.10.2024 22:11] Get feed.
[31.10.2024 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2410.23090
[31.10.2024 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2410.23287
[31.10.2024 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2410.22391
[31.10.2024 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2410.20779
[31.10.2024 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2410.23168
[31.10.2024 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2410.22884
[31.10.2024 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2410.20050
[31.10.2024 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2410.23277
[31.10.2024 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2410.23123
[31.10.2024 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2410.22587
[31.10.2024 22:11] ********************************************************************************
[31.10.2024 22:11] Abstract 0. Retrieval-Augmented Generation (RAG) has become a powerful paradigm for enhancing large language models (LLMs) through external knowledge retrieval. Despite its widespread attention, existing academic research predominantly focuses on single-turn RAG, leaving a significant gap in addressing the comp...
[31.10.2024 22:11] ********************************************************************************
[31.10.2024 22:11] Abstract 1. We present REM, a framework for segmenting a wide range of concepts in video that can be described through natural language. Our method capitalizes on visual-language representations learned by video diffusion models on Internet-scale datasets. A key insight of our approach is preserving as much of ...
[31.10.2024 22:11] ********************************************************************************
[31.10.2024 22:11] Abstract 2. In recent years, there has been a trend in the field of Reinforcement Learning (RL) towards large action models trained offline on large-scale datasets via sequence modeling. Existing models are primarily based on the Transformer architecture, which result in powerful agents. However, due to slow in...
[31.10.2024 22:11] ********************************************************************************
[31.10.2024 22:11] Abstract 3. Readers can have different goals with respect to the text they are reading. Can these goals be decoded from the pattern of their eye movements over the text? In this work, we examine for the first time whether it is possible to decode two types of reading goals that are common in daily life: informa...
[31.10.2024 22:11] ********************************************************************************
[31.10.2024 22:11] Abstract 4. Transformers have become the predominant architecture in foundation models due to their excellent performance across various domains. However, the substantial cost of scaling these models remains a significant concern. This problem arises primarily from their dependence on a fixed number of paramete...
[31.10.2024 22:11] ********************************************************************************
[31.10.2024 22:11] Abstract 5. Mixture-of-Experts (MoE) models improve the efficiency and scalability of dense language models by routing each token to a small number of experts in each layer. In this paper, we show how an adversary that can arrange for their queries to appear in the same batch of examples as a victim's queries c...
[31.10.2024 22:11] ********************************************************************************
[31.10.2024 22:11] Abstract 6. Medical information retrieval (MIR) is essential for retrieving relevant medical knowledge from diverse sources, including electronic health records, scientific literature, and medical databases. However, achieving effective zero-shot dense retrieval in the medical domain poses substantial challenge...
[31.10.2024 22:11] ********************************************************************************
[31.10.2024 22:11] Abstract 7. Human beings are endowed with a complementary learning system, which bridges the slow learning of general world dynamics with fast storage of episodic memory from a new experience. Previous video generation models, however, primarily focus on slow learning by pre-training on vast amounts of data, ov...
[31.10.2024 22:11] ********************************************************************************
[31.10.2024 22:11] Abstract 8. Large language models (LLMs) achieve good performance on challenging reasoning benchmarks, yet could also make basic reasoning mistakes. This contrasting behavior is puzzling when it comes to understanding the mechanisms behind LLMs' reasoning capabilities. One hypothesis is that the increasingly hi...
[31.10.2024 22:11] ********************************************************************************
[31.10.2024 22:11] Abstract 9. Open-source large language models are becoming increasingly available and popular among researchers and practitioners. While significant progress has been made on open-weight models, open training data is a practice yet to be adopted by the leading open-weight models creators. At the same time, ther...
[31.10.2024 22:11] Read previous papers.
[31.10.2024 22:11] Generating reviews via LLM API.
[31.10.2024 22:11] Using data from previous issue: {"categories": ["#rag", "#benchmark"], "emoji": "üó£Ô∏è", "ru": {"title": "CORAL: –ù–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–Ω–æ–≥–æ—Ö–æ–¥–æ–≤—ã—Ö –¥–∏–∞–ª–æ–≥–æ–≤—ã—Ö —Å–∏—Å—Ç–µ–º —Å RAG", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ CORAL –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–∏—Å—Ç–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π (RAG) –≤ –º–Ω–æ–≥–æ—Ö–æ–¥–æ–≤—ã—Ö –¥–∏–∞–ª–æ–≥–∞—Ö. CORAL –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ
[31.10.2024 22:11] Using data from previous issue: {"categories": ["#video", "#benchmark", "#multimodal", "#cv"], "emoji": "üé•", "ru": {"title": "REM: —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞", "desc": "REM - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∫–æ–Ω—Ü–µ–ø—Ü–∏–π –≤ –≤–∏–¥–µ–æ, –æ–ø–∏—Å—ã–≤–∞–µ–º—ã—Ö —Å –ø–æ–º–æ—â—å—é –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≤–∏–∑—É–∞–ª
[31.10.2024 22:11] Using data from previous issue: {"categories": ["#rl", "#agents", "#architecture"], "emoji": "ü§ñ", "ru": {"title": "LRAM: –ë—ã—Å—Ç—Ä–µ–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤, —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å LRAM (Large Recurrent Action Model) –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –æ—Å–Ω–æ–≤–∞–Ω–Ω–∞—è –Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ xLSTM. LRAM –ø—Ä–µ–¥–ª–∞–≥
[31.10.2024 22:11] Using data from previous issue: {"categories": ["#cv", "#interpretability", "#dataset", "#benchmark"], "emoji": "üëÅÔ∏è", "ru": {"title": "–†–∞–∑–≥–∞–¥–∫–∞ —Ü–µ–ª–µ–π —á—Ç–µ–Ω–∏—è –ø–æ –¥–≤–∏–∂–µ–Ω–∏—è–º –≥–ª–∞–∑", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—é —Ü–µ–ª–µ–π —á—Ç–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–≤–∏–∂–µ–Ω–∏–π –≥–ª–∞–∑. –ê–≤—Ç–æ—Ä—ã –ø—Ä–∏–º–µ–Ω—è—é—Ç —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫—Ä—É–ø
[31.10.2024 22:11] Using data from previous issue: {"categories": ["#architecture", "#optimization"], "emoji": "üîÑ", "ru": {"title": "–ì–∏–±–∫–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π –±–µ–∑ –ø–æ–ª–Ω–æ–≥–æ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è", "desc": "TokenFormer - —ç—Ç–æ –Ω–æ–≤–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª–∏, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–∑–≤–æ–ª—è–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞—Ç—å –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ –±–µ–∑ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –ø–æ–ª–Ω–æ–≥–æ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è. –í –æ—Ç–ª
[31.10.2024 22:11] Using data from previous issue: {"categories": ["#security", "#architecture"], "emoji": "üïµÔ∏è", "ru": {"title": "–£—è–∑–≤–∏–º–æ—Å—Ç—å –≤ MoE –º–æ–¥–µ–ª—è—Ö: –∫–∞–∫ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ –º–æ–≥—É—Ç —Ä–∞—Å–∫—Ä—ã—Ç—å –≤–∞—à –ø—Ä–æ–º–ø—Ç", "desc": "–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç —É—è–∑–≤–∏–º–æ—Å—Ç—å –≤ –º–æ–¥–µ–ª—è—Ö Mixture-of-Experts (MoE), –∏—Å–ø–æ–ª—å–∑—É—é—â–∏—Ö –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—é Expert-Choice-Routing. –ê–≤—Ç–æ—Ä—ã –¥–µ–º–æ–Ω—Å—Ç
[31.10.2024 22:11] Using data from previous issue: {"categories": ["#dataset", "#data", "#benchmark", "#medicine"], "emoji": "ü©∫", "ru": {"title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–º –ø–æ–∏—Å–∫–µ: SL-HyDE –∏ CMIRB –æ—Ç–∫—Ä—ã–≤–∞—é—Ç –Ω–æ–≤—ã–µ –≥–æ—Ä–∏–∑–æ–Ω—Ç—ã", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–º—É –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–º—É –ø–æ–∏—Å–∫—É –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º SL-HyDE. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –±–æ–ª
[31.10.2024 22:11] Using data from previous issue: {"categories": ["#video", "#dataset", "#training", "#benchmark"], "emoji": "üé¨", "ru": {"title": "–î–≤—É—Ö—Å–∫–æ—Ä–æ—Å—Ç–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç SlowFast-VGen - –Ω–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É –¥–≤—É—Ö—Å–∫–æ—Ä–æ—Å—Ç–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–µ–π—Å—Ç–≤–∏–π. –ú–æ–¥
[31.10.2024 22:11] Using data from previous issue: {"categories": ["#reasoning", "#interpretability"], "emoji": "üß†", "ru": {"title": "–ó–∞–ø–æ–º–∏–Ω–∞–Ω–∏–µ vs –†–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ: –°–ª–æ–∂–Ω–æ–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–í —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –≤–∑–∞–∏–º–æ—Å–≤—è–∑—å –º–µ–∂–¥—É —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å—é –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –∫ –∑–∞–ø–æ–º–∏–Ω–∞–Ω–∏—é –∏ –∏—Ö –Ω–∞–≤—ã–∫–∞–º–∏ –ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –º—ã—à–ª–µ
[31.10.2024 22:11] Using data from previous issue: {"categories": ["#dataset", "#data", "#interpretability"], "emoji": "üõ°Ô∏è", "ru": {"title": "–ë–µ–∑–æ–ø–∞—Å–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –¥–∞–Ω–Ω—ã—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ —Ç–æ–∫—Å–∏—á–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –≤ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã —Å–æ–∑–¥–∞–ª–∏ –¥–∞—Ç–∞—Å–µ—Ç ToxicCo
[31.10.2024 22:11] Loading Chinese text from previous data.
[31.10.2024 22:11] Renaming data file.
[31.10.2024 22:11] Renaming previous data. hf_papers.json to ./d/2024-10-31.json
[31.10.2024 22:11] Saving new data file.
[31.10.2024 22:11] Generating page.
[31.10.2024 22:11] Renaming previous page.
[31.10.2024 22:11] Renaming previous data. index.html to ./d/2024-10-31.html
[31.10.2024 22:11] [Experimental] Generating Chinese page for reading.
[31.10.2024 22:11] Chinese vocab [{'word': 'Ê£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàê', 'pinyin': 'ji«énsu«í zƒìngqi√°ng shƒìngch√©ng', 'trans': 'Retrieval-Augmented Generation'}, {'word': 'ËåÉÂºè', 'pinyin': 'f√†nsh√¨', 'trans': 'paradigm'}, {'word': 'ÂπøÊ≥õ', 'pinyin': 'gu«éngf√†n', 'trans': 'extensive'}, {'word': 'ÂçïËΩÆ', 'pinyin': 'dƒÅnl√∫n', 'trans': 'single-turn'}, {'word': 'ÂØπËØù', 'pinyin': 'du√¨hu√†', 'trans': 'dialogue'}, {'word': 'Â§çÊùÇ', 'pinyin': 'f√πz√°', 'trans': 'complex'}, {'word': 'Â§öËΩÆ', 'pinyin': 'du≈çl√∫n', 'trans': 'multi-turn'}, {'word': 'Â°´Ë°•', 'pinyin': 'ti√°nb«î', 'trans': 'fill'}, {'word': 'Á©∫ÁôΩ', 'pinyin': 'k√≤ngb√°i', 'trans': 'gap'}, {'word': 'Âü∫ÂáÜ', 'pinyin': 'jƒ´zh«în', 'trans': 'benchmark'}, {'word': 'Êï∞ÊçÆÈõÜ', 'pinyin': 'sh√πj√πj√≠', 'trans': 'dataset'}, {'word': 'ËØÑ‰º∞', 'pinyin': 'p√≠ngg≈´', 'trans': 'evaluate'}, {'word': '‰ø°ÊÅØÂØªÊ±Ç', 'pinyin': 'x√¨nxƒ´ x√∫nqi√∫', 'trans': 'information-seeking'}, {'word': 'ÂºÄÊîæÂüü', 'pinyin': 'kƒÅif√†ng y√π', 'trans': 'open-domain'}, {'word': 'Áü•ËØÜÂØÜÈõÜÂ∫¶', 'pinyin': 'zhƒ´shi m√¨jƒ´d√π', 'trans': 'knowledge intensity'}, {'word': 'Ëá™Áî±ÂΩ¢Âºè', 'pinyin': 'z√¨y√≥u x√≠ngsh√¨', 'trans': 'free-form'}, {'word': 'ÂõûÂ§ç', 'pinyin': 'hu√≠f√π', 'trans': 'response'}, {'word': 'ËØùÈ¢òËΩ¨Êç¢', 'pinyin': 'hu√†t√≠ zhu«énhu√†n', 'trans': 'topic switching'}, {'word': 'ÊÆµËêΩÊ£ÄÁ¥¢', 'pinyin': 'du√†nlu√≤ ji«énsu«í', 'trans': 'paragraph retrieval'}, {'word': 'ÂºïÁî®Ê†áËÆ∞', 'pinyin': 'y«êny√≤ng biƒÅoj√¨', 'trans': 'citation marking'}, {'word': 'Ê°ÜÊû∂', 'pinyin': 'ku√†ngji√†', 'trans': 'framework'}, {'word': 'ÁªºÂêà', 'pinyin': 'z≈çngh√©', 'trans': 'comprehensive'}, {'word': 'ÊΩúÂäõ', 'pinyin': 'qi√°nl√¨', 'trans': 'potential'}]
[31.10.2024 22:11] Renaming previous Chinese page.
[31.10.2024 22:11] Renaming previous data. zh.html to ./d/2024-10-30_zh_reading_task.html
[31.10.2024 22:11] Writing result.
[31.10.2024 22:11] Writing Chinese reading task.
[31.10.2024 22:11] Renaming log file.
[31.10.2024 22:11] Renaming previous data. log.txt to ./logs/2024-10-31_last_log.txt
