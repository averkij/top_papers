[23.05.2025 03:42] Read previous papers.
[23.05.2025 03:42] Generating top page (month).
[23.05.2025 03:42] Writing top page (month).
[23.05.2025 04:15] Read previous papers.
[23.05.2025 04:15] Get feed.
[23.05.2025 04:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.16938
[23.05.2025 04:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.16707
[23.05.2025 04:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.14810
[23.05.2025 04:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.16175
[23.05.2025 04:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.16410
[23.05.2025 04:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15966
[23.05.2025 04:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.16933
[23.05.2025 04:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.16990
[23.05.2025 04:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.16916
[23.05.2025 04:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15270
[23.05.2025 04:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.16839
[23.05.2025 04:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.14625
[23.05.2025 04:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.17012
[23.05.2025 04:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.16181
[23.05.2025 04:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.17018
[23.05.2025 04:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15952
[23.05.2025 04:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.16864
[23.05.2025 04:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.16854
[23.05.2025 04:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.16151
[23.05.2025 04:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15879
[23.05.2025 04:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.16400
[23.05.2025 04:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.11711
[23.05.2025 04:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15963
[23.05.2025 04:15] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[23.05.2025 04:15] No deleted papers detected.
[23.05.2025 04:15] Downloading and parsing papers (pdf, html). Total: 23.
[23.05.2025 04:15] Downloading and parsing paper https://huggingface.co/papers/2505.16938.
[23.05.2025 04:15] Extra JSON file exists (./assets/json/2505.16938.json), skip PDF parsing.
[23.05.2025 04:15] Paper image links file exists (./assets/img_data/2505.16938.json), skip HTML parsing.
[23.05.2025 04:15] Success.
[23.05.2025 04:15] Downloading and parsing paper https://huggingface.co/papers/2505.16707.
[23.05.2025 04:15] Extra JSON file exists (./assets/json/2505.16707.json), skip PDF parsing.
[23.05.2025 04:15] Paper image links file exists (./assets/img_data/2505.16707.json), skip HTML parsing.
[23.05.2025 04:15] Success.
[23.05.2025 04:15] Downloading and parsing paper https://huggingface.co/papers/2505.14810.
[23.05.2025 04:15] Extra JSON file exists (./assets/json/2505.14810.json), skip PDF parsing.
[23.05.2025 04:15] Paper image links file exists (./assets/img_data/2505.14810.json), skip HTML parsing.
[23.05.2025 04:15] Success.
[23.05.2025 04:15] Downloading and parsing paper https://huggingface.co/papers/2505.16175.
[23.05.2025 04:15] Extra JSON file exists (./assets/json/2505.16175.json), skip PDF parsing.
[23.05.2025 04:15] Paper image links file exists (./assets/img_data/2505.16175.json), skip HTML parsing.
[23.05.2025 04:15] Success.
[23.05.2025 04:15] Downloading and parsing paper https://huggingface.co/papers/2505.16410.
[23.05.2025 04:15] Extra JSON file exists (./assets/json/2505.16410.json), skip PDF parsing.
[23.05.2025 04:15] Paper image links file exists (./assets/img_data/2505.16410.json), skip HTML parsing.
[23.05.2025 04:15] Success.
[23.05.2025 04:15] Downloading and parsing paper https://huggingface.co/papers/2505.15966.
[23.05.2025 04:15] Extra JSON file exists (./assets/json/2505.15966.json), skip PDF parsing.
[23.05.2025 04:15] Paper image links file exists (./assets/img_data/2505.15966.json), skip HTML parsing.
[23.05.2025 04:15] Success.
[23.05.2025 04:15] Downloading and parsing paper https://huggingface.co/papers/2505.16933.
[23.05.2025 04:15] Extra JSON file exists (./assets/json/2505.16933.json), skip PDF parsing.
[23.05.2025 04:15] Paper image links file exists (./assets/img_data/2505.16933.json), skip HTML parsing.
[23.05.2025 04:15] Success.
[23.05.2025 04:15] Downloading and parsing paper https://huggingface.co/papers/2505.16990.
[23.05.2025 04:15] Extra JSON file exists (./assets/json/2505.16990.json), skip PDF parsing.
[23.05.2025 04:15] Paper image links file exists (./assets/img_data/2505.16990.json), skip HTML parsing.
[23.05.2025 04:15] Success.
[23.05.2025 04:15] Downloading and parsing paper https://huggingface.co/papers/2505.16916.
[23.05.2025 04:15] Extra JSON file exists (./assets/json/2505.16916.json), skip PDF parsing.
[23.05.2025 04:15] Paper image links file exists (./assets/img_data/2505.16916.json), skip HTML parsing.
[23.05.2025 04:15] Success.
[23.05.2025 04:15] Downloading and parsing paper https://huggingface.co/papers/2505.15270.
[23.05.2025 04:15] Extra JSON file exists (./assets/json/2505.15270.json), skip PDF parsing.
[23.05.2025 04:15] Paper image links file exists (./assets/img_data/2505.15270.json), skip HTML parsing.
[23.05.2025 04:15] Success.
[23.05.2025 04:15] Downloading and parsing paper https://huggingface.co/papers/2505.16839.
[23.05.2025 04:15] Extra JSON file exists (./assets/json/2505.16839.json), skip PDF parsing.
[23.05.2025 04:15] Paper image links file exists (./assets/img_data/2505.16839.json), skip HTML parsing.
[23.05.2025 04:15] Success.
[23.05.2025 04:15] Downloading and parsing paper https://huggingface.co/papers/2505.14625.
[23.05.2025 04:15] Extra JSON file exists (./assets/json/2505.14625.json), skip PDF parsing.
[23.05.2025 04:15] Paper image links file exists (./assets/img_data/2505.14625.json), skip HTML parsing.
[23.05.2025 04:15] Success.
[23.05.2025 04:15] Downloading and parsing paper https://huggingface.co/papers/2505.17012.
[23.05.2025 04:15] Extra JSON file exists (./assets/json/2505.17012.json), skip PDF parsing.
[23.05.2025 04:15] Paper image links file exists (./assets/img_data/2505.17012.json), skip HTML parsing.
[23.05.2025 04:15] Success.
[23.05.2025 04:15] Downloading and parsing paper https://huggingface.co/papers/2505.16181.
[23.05.2025 04:15] Extra JSON file exists (./assets/json/2505.16181.json), skip PDF parsing.
[23.05.2025 04:15] Paper image links file exists (./assets/img_data/2505.16181.json), skip HTML parsing.
[23.05.2025 04:15] Success.
[23.05.2025 04:15] Downloading and parsing paper https://huggingface.co/papers/2505.17018.
[23.05.2025 04:15] Extra JSON file exists (./assets/json/2505.17018.json), skip PDF parsing.
[23.05.2025 04:15] Paper image links file exists (./assets/img_data/2505.17018.json), skip HTML parsing.
[23.05.2025 04:15] Success.
[23.05.2025 04:15] Downloading and parsing paper https://huggingface.co/papers/2505.15952.
[23.05.2025 04:15] Extra JSON file exists (./assets/json/2505.15952.json), skip PDF parsing.
[23.05.2025 04:15] Paper image links file exists (./assets/img_data/2505.15952.json), skip HTML parsing.
[23.05.2025 04:15] Success.
[23.05.2025 04:15] Downloading and parsing paper https://huggingface.co/papers/2505.16864.
[23.05.2025 04:15] Extra JSON file exists (./assets/json/2505.16864.json), skip PDF parsing.
[23.05.2025 04:15] Paper image links file exists (./assets/img_data/2505.16864.json), skip HTML parsing.
[23.05.2025 04:15] Success.
[23.05.2025 04:15] Downloading and parsing paper https://huggingface.co/papers/2505.16854.
[23.05.2025 04:15] Extra JSON file exists (./assets/json/2505.16854.json), skip PDF parsing.
[23.05.2025 04:15] Paper image links file exists (./assets/img_data/2505.16854.json), skip HTML parsing.
[23.05.2025 04:15] Success.
[23.05.2025 04:15] Downloading and parsing paper https://huggingface.co/papers/2505.16151.
[23.05.2025 04:15] Extra JSON file exists (./assets/json/2505.16151.json), skip PDF parsing.
[23.05.2025 04:15] Paper image links file exists (./assets/img_data/2505.16151.json), skip HTML parsing.
[23.05.2025 04:15] Success.
[23.05.2025 04:15] Downloading and parsing paper https://huggingface.co/papers/2505.15879.
[23.05.2025 04:15] Extra JSON file exists (./assets/json/2505.15879.json), skip PDF parsing.
[23.05.2025 04:15] Paper image links file exists (./assets/img_data/2505.15879.json), skip HTML parsing.
[23.05.2025 04:15] Success.
[23.05.2025 04:15] Downloading and parsing paper https://huggingface.co/papers/2505.16400.
[23.05.2025 04:15] Extra JSON file exists (./assets/json/2505.16400.json), skip PDF parsing.
[23.05.2025 04:15] Paper image links file exists (./assets/img_data/2505.16400.json), skip HTML parsing.
[23.05.2025 04:15] Success.
[23.05.2025 04:15] Downloading and parsing paper https://huggingface.co/papers/2505.11711.
[23.05.2025 04:15] Extra JSON file exists (./assets/json/2505.11711.json), skip PDF parsing.
[23.05.2025 04:15] Paper image links file exists (./assets/img_data/2505.11711.json), skip HTML parsing.
[23.05.2025 04:15] Success.
[23.05.2025 04:15] Downloading and parsing paper https://huggingface.co/papers/2505.15963.
[23.05.2025 04:15] Extra JSON file exists (./assets/json/2505.15963.json), skip PDF parsing.
[23.05.2025 04:15] Paper image links file exists (./assets/img_data/2505.15963.json), skip HTML parsing.
[23.05.2025 04:15] Success.
[23.05.2025 04:15] Enriching papers with extra data.
[23.05.2025 04:15] ********************************************************************************
[23.05.2025 04:15] Abstract 0. Artificial Intelligence (AI) is accelerating the transformation of scientific research paradigms, not only enhancing research efficiency but also driving innovation. We introduce NovelSeek, a unified closed-loop multi-agent framework to conduct Autonomous Scientific Research (ASR) across various sci...
[23.05.2025 04:15] ********************************************************************************
[23.05.2025 04:15] Abstract 1. Recent advances in multi-modal generative models have enabled significant progress in instruction-based image editing. However, while these models produce visually plausible outputs, their capacity for knowledge-based reasoning editing tasks remains under-explored. In this paper, we introduce KRIS-B...
[23.05.2025 04:15] ********************************************************************************
[23.05.2025 04:15] Abstract 2. An empirical analysis of MathIF identifies a tension between enhancing reasoning capacity and maintaining instruction adherence in large language models.  					AI-generated summary 				 Instruction-following is essential for aligning large language models (LLMs) with user intent. While recent reason...
[23.05.2025 04:15] ********************************************************************************
[23.05.2025 04:15] Abstract 3. QuickVideo accelerates long-video understanding by combining a parallelized video decoder, memory-efficient prefilling, and overlapping video decoding with inference, enabling real-time performance.  					AI-generated summary 				 Long-video understanding has emerged as a crucial capability in real-...
[23.05.2025 04:15] ********************************************************************************
[23.05.2025 04:15] Abstract 4. Tool-Star, an RL-based framework, enables LLMs to autonomously use multiple tools for stepwise reasoning, leveraging data synthesis and hierarchical reward design.  					AI-generated summary 				 Recently, large language models (LLMs) have shown remarkable reasoning capabilities via large-scale rein...
[23.05.2025 04:15] ********************************************************************************
[23.05.2025 04:15] Abstract 5. Introducing pixel-space reasoning in Vision-Language Models (VLMs) through visual operations like zoom-in and select-frame enhances their performance on visual tasks.  					AI-generated summary 				 Chain-of-thought reasoning has significantly improved the performance of Large Language Models (LLMs)...
[23.05.2025 04:15] ********************************************************************************
[23.05.2025 04:15] Abstract 6. A diffusion-based Multimodal Large Language Model (LLaDA-V) with integrated visual instruction tuning performs competitively on multimodal tasks and outperforms existing models in multimodal understanding.  					AI-generated summary 				 In this work, we introduce LLaDA-V, a purely diffusion-based M...
[23.05.2025 04:15] ********************************************************************************
[23.05.2025 04:15] Abstract 7. Dimple, a Discrete Diffusion Multimodal Large Language Model, achieves performance comparable to autoregressive models through a hybrid training approach and enhances inference efficiency with confident decoding and structure priors.  					AI-generated summary 				 In this work, we propose Dimple, t...
[23.05.2025 04:15] ********************************************************************************
[23.05.2025 04:15] Abstract 8. Multimodal Large Language Models (MLLMs) are increasingly deployed in fine-tuning-as-a-service (FTaaS) settings, where user-submitted datasets adapt general-purpose models to downstream tasks. This flexibility, however, introduces serious security risks, as malicious fine-tuning can implant backdoor...
[23.05.2025 04:15] ********************************************************************************
[23.05.2025 04:15] Abstract 9. Maximal Update Parametrization (ŒºP) is extended to diffusion Transformers, demonstrating efficient hyperparameter transferability and reduced tuning costs across various models and tasks.  					AI-generated summary 				 Diffusion Transformers have emerged as the foundation for vision generative mode...
[23.05.2025 04:15] ********************************************************************************
[23.05.2025 04:15] Abstract 10. LaViDa, a family of vision-language models built on discrete diffusion models, offers competitive performance on multimodal benchmarks with advantages in speed, controllability, and bidirectional reasoning.  					AI-generated summary 				 Modern Vision-Language Models (VLMs) can solve a wide range o...
[23.05.2025 04:15] ********************************************************************************
[23.05.2025 04:15] Abstract 11. Reinforcement Learning (RL) has become a powerful tool for enhancing the reasoning abilities of large language models (LLMs) by optimizing their policies with reward signals. Yet, RL's success relies on the reliability of rewards, which are provided by verifiers. In this paper, we expose and analyze...
[23.05.2025 04:15] ********************************************************************************
[23.05.2025 04:15] Abstract 12. SpatialScore benchmarks multimodal large language models for 3D spatial understanding, revealing challenges and showcasing the effectiveness of SpatialAgent with specialized tools.  					AI-generated summary 				 Multimodal large language models (MLLMs) have achieved impressive success in question-a...
[23.05.2025 04:15] ********************************************************************************
[23.05.2025 04:15] Abstract 13. Analysis of 83k image editing requests reveals that AI editors, including GPT-4o, struggle with low-creativity tasks and precise editing, while performing better on open-ended tasks, and human and VLM judges differ in their preferences for AI versus human edits.  					AI-generated summary 				 Gener...
[23.05.2025 04:15] ********************************************************************************
[23.05.2025 04:15] Abstract 14. An enhanced multimodal language model incorporates thinking process rewards to improve reasoning and generalization, achieving superior performance on benchmarks compared to larger models.  					AI-generated summary 				 Recent advances have shown success in eliciting strong reasoning abilities in m...
[23.05.2025 04:15] ********************************************************************************
[23.05.2025 04:15] Abstract 15. A benchmark called VideoGameQA-Bench is introduced to assess Vision-Language Models in video game quality assurance tasks.  					AI-generated summary 				 With video games now generating the highest revenues in the entertainment industry, optimizing game development workflows has become essential fo...
[23.05.2025 04:15] ********************************************************************************
[23.05.2025 04:15] Abstract 16. Jenga, a novel inference pipeline for video Diffusion Transformer models, combines dynamic attention carving and progressive resolution generation to significantly speed up video generation while maintaining high quality.  					AI-generated summary 				 Despite the remarkable generation quality of v...
[23.05.2025 04:15] ********************************************************************************
[23.05.2025 04:15] Abstract 17. TON, a two-stage training strategy combining supervised fine-tuning with thought dropout and Group Relative Policy Optimization, reduces unnecessary reasoning steps in vision-language models without sacrificing performance.  					AI-generated summary 				 Reinforcement Learning (RL) has proven to be...
[23.05.2025 04:15] ********************************************************************************
[23.05.2025 04:15] Abstract 18. The FRANK Model enhances multimodal LLMs with reasoning and reflection abilities without retraining, using a hierarchical weight merging approach that merges visual-pretrained and reasoning-specialized models.  					AI-generated summary 				 Recent advances in Reasoning LLMs (e.g., DeepSeek-R1 and O...
[23.05.2025 04:15] ********************************************************************************
[23.05.2025 04:15] Abstract 19. Recent studies have demonstrated the efficacy of using Reinforcement Learning (RL) in building reasoning models that articulate chains of thoughts prior to producing final answers. However, despite ongoing advances that aim at enabling reasoning for vision-language tasks, existing open-source visual...
[23.05.2025 04:15] ********************************************************************************
[23.05.2025 04:15] Abstract 20. Large-scale reinforcement learning enhances reasoning capabilities in small and mid-sized models more effectively than distillation, achieving superior results in both math and code benchmarks.  					AI-generated summary 				 Despite recent progress in large-scale reinforcement learning (RL) for rea...
[23.05.2025 04:15] ********************************************************************************
[23.05.2025 04:15] Abstract 21. Reinforcement learning improves large language models with minimal parameter updates, affecting only a small subnetwork without explicit sparsity techniques.  					AI-generated summary 				 Reinforcement learning (RL) yields substantial improvements in large language models (LLMs) downstream task pe...
[23.05.2025 04:15] ********************************************************************************
[23.05.2025 04:15] Abstract 22. Large vision-language models (LVLMs) remain vulnerable to hallucination, often generating content misaligned with visual inputs. While recent approaches advance multi-modal Direct Preference Optimization (DPO) to mitigate hallucination, they typically rely on predefined or randomly edited negative s...
[23.05.2025 04:15] Read previous papers.
[23.05.2025 04:15] Generating reviews via LLM API.
[23.05.2025 04:15] Using data from previous issue: {"categories": ["#healthcare", "#multimodal", "#agents", "#science"], "emoji": "üß¨", "ru": {"title": "NovelSeek: –ò–ò-—É—Å–∫–æ—Ä–∏—Ç–µ–ª—å –Ω–∞—É—á–Ω—ã—Ö –æ—Ç–∫—Ä—ã—Ç–∏–π", "desc": "NovelSeek - —ç—Ç–æ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –∞–≤—Ç–æ–Ω–æ–º–Ω—ã—Ö –Ω–∞—É—á–Ω—ã—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –æ–±–ª–∞—Å—Ç—è—Ö. –û–Ω–∞ –æ–±–ª–∞–¥–∞–µ—Ç –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å—é, –ø
[23.05.2025 04:15] Using data from previous issue: {"categories": ["#reasoning", "#benchmark", "#multimodal"], "emoji": "üß†", "ru": {"title": "KRIS-Bench: –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–µ–π —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç KRIS-Bench - –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∏–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã—Ö —Å
[23.05.2025 04:15] Using data from previous issue: {"categories": ["#math", "#reasoning", "#training", "#benchmark", "#optimization", "#alignment"], "emoji": "ü§ñ", "ru": {"title": "–ë–∞–ª–∞–Ω—Å –º–µ–∂–¥—É —Ä–∞–∑—É–º–æ–º –∏ –ø–æ—Å–ª—É—à–∞–Ω–∏–µ–º –≤ –ò–ò", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ MathIF –≤—ã—è–≤–ª—è–µ—Ç –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–µ –º–µ–∂–¥—É —É–ª—É—á—à–µ–Ω–∏–µ–º —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è
[23.05.2025 04:15] Using data from previous issue: {"categories": ["#inference", "#optimization", "#video", "#long_context"], "emoji": "üöÄ", "ru": {"title": "–£—Å–∫–æ—Ä–µ–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∞ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é QuickVideo", "desc": "QuickVideo - —ç—Ç–æ —Å–∏—Å—Ç–µ–º–∞, —É—Å–∫–æ—Ä—è—é—â–∞—è –ø–æ–Ω–∏–º–∞–Ω–∏–µ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ –¥–ª—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π —Ä–µ–∞–ª—å–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –¥–µ–∫–æ–¥–µ—Ä
[23.05.2025 04:15] Using data from previous issue: {"categories": ["#reasoning", "#training", "#dataset", "#benchmark", "#rl", "#optimization"], "emoji": "üõ†Ô∏è", "ru": {"title": "Tool-Star: –ê–≤—Ç–æ–Ω–æ–º–Ω–æ–µ –º—É–ª—å—Ç–∏–∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "Tool-Star - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è
[23.05.2025 04:15] Using data from previous issue: {"categories": ["#multimodal", "#reasoning", "#cv", "#rl", "#optimization", "#benchmark", "#open_source", "#training"], "emoji": "üîç", "ru": {"title": "–†–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –ø–∏–∫—Å–µ–ª—è—Ö: –Ω–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è –ò–ò", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é —Ä–∞–±–æ—Ç—ã –º–æ–¥–µ–ª–µ–π –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ
[23.05.2025 04:15] Using data from previous issue: {"categories": ["#multimodal", "#diffusion", "#architecture"], "emoji": "üß†", "ru": {"title": "–î–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –ò–ò-–º–æ–¥–µ–ª—å –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –∞–Ω–∞–ª–æ–≥–∏ –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ —Ç–µ–∫—Å—Ç–∞ –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –º–æ–¥–µ–ª—å LLaDA-V - –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –±–æ–ª—å—à–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∏—Ñ—Ñ—É–∑–∏–∏, –∏–Ω—Ç–µ
[23.05.2025 04:15] Using data from previous issue: {"categories": ["#multimodal", "#inference", "#diffusion", "#optimization", "#open_source", "#architecture", "#training"], "emoji": "üß†", "ru": {"title": "Dimple: –î–∏—Å–∫—Ä–µ—Ç–Ω–∞—è –¥–∏—Ñ—Ñ—É–∑–∏—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö –∏ –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "Dimple - —ç—Ç–æ –ø–µ—Ä–≤–∞—è –¥–∏—Å–∫—Ä–µ—Ç–Ω–∞—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω
[23.05.2025 04:15] Using data from previous issue: {"categories": ["#multimodal", "#data", "#training", "#dataset", "#security"], "emoji": "üõ°Ô∏è", "ru": {"title": "–ó–∞—â–∏—Ç–∞ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –ò–ò –æ—Ç —Å–∫—Ä—ã—Ç—ã—Ö —É–≥—Ä–æ–∑: –¥–æ–≤–µ—Ä—è–π —Å–≤–æ–∏–º –≥–ª–∞–∑–∞–º", "desc": "–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –ø—Ä–æ–±–ª–µ–º–µ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (MLLM) –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –¥–æ–æ–±—É—á–µ–Ω–∏—è –Ω–∞ 
[23.05.2025 04:15] Using data from previous issue: {"categories": ["#architecture", "#training", "#transfer_learning", "#diffusion", "#optimization"], "emoji": "üî¨", "ru": {"title": "ŒºP: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –º–µ—Ç–æ–¥–∞ Maximal Update Parametrization (ŒºP) –¥–ª—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä
[23.05.2025 04:15] Using data from previous issue: {"categories": ["#multimodal", "#architecture", "#training", "#games", "#cv", "#diffusion"], "emoji": "üß†", "ru": {"title": "LaViDa: –ë—ã—Å—Ç—Ä—ã–µ –∏ –≥–∏–±–∫–∏–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∏—Å–∫—Ä–µ—Ç–Ω–æ–π –¥–∏—Ñ—Ñ—É–∑–∏–∏", "desc": "LaViDa - —ç—Ç–æ —Å–µ–º–µ–π—Å—Ç–≤–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã—Ö –Ω–∞ –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã—Ö –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º
[23.05.2025 04:15] Using data from previous issue: {"categories": ["#rlhf", "#reasoning", "#training", "#dataset", "#rl", "#optimization"], "emoji": "üîç", "ru": {"title": "–ë–æ—Ä—å–±–∞ —Å –ª–æ–∂–Ω–æ–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–º–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è RL-–æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —ç—Ç–æ–π —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ –ª–æ–∂–Ω–æ–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞—Ö, –∏
[23.05.2025 04:15] Using data from previous issue: {"categories": ["#multimodal", "#reasoning", "#survey", "#3d", "#benchmark", "#agents"], "emoji": "üß†", "ru": {"title": "SpatialScore: –Ω–æ–≤—ã–π —Ä—É–±–µ–∂ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–º –ø–æ–Ω–∏–º–∞–Ω–∏–∏ –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –ò–ò", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç SpatialScore - –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞
[23.05.2025 04:15] Using data from previous issue: {"categories": ["#multimodal", "#games", "#dataset", "#cv", "#optimization", "#interpretability"], "emoji": "üñºÔ∏è", "ru": {"title": "–ò–ò –≤ —Ñ–æ—Ç–æ—Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏: –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç—å vs —Ç–æ—á–Ω–æ—Å—Ç—å", "desc": "–ê–Ω–∞–ª–∏–∑ 83 —Ç—ã—Å—è—á –∑–∞–ø—Ä–æ—Å–æ–≤ –Ω–∞ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –ò–ò-—Ä–µ–¥–∞–∫—Ç–æ—Ä—ã, –≤–∫–ª—é—á–∞—è GPT-4o, –∏—Å–ø—ã—Ç—ã–≤
[23.05.2025 04:15] Using data from previous issue: {"categories": ["#multimodal", "#reasoning", "#rlhf", "#rl", "#optimization", "#benchmark", "#open_source", "#training"], "emoji": "üß†", "ru": {"title": "–£–ª—É—á—à–µ–Ω–∏–µ –º—ã—à–ª–µ–Ω–∏—è –ò–ò —á–µ—Ä–µ–∑ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ –∑–∞ –ø—Ä–æ—Ü–µ—Å—Å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ —É–ª—É—á—à–µ–Ω–Ω—É—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—É—é —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å So
[23.05.2025 04:15] Using data from previous issue: {"categories": ["#games", "#cv", "#video", "#optimization", "#benchmark"], "emoji": "üéÆ", "ru": {"title": "VideoGameQA-Bench: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –∫–æ–Ω—Ç—Ä–æ–ª—è –∫–∞—á–µ—Å—Ç–≤–∞ –≤–∏–¥–µ–æ–∏–≥—Ä", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ VideoGameQA-Bench –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –∑–∞–¥–∞—á–∞—Ö –∫–æ–Ω—Ç—Ä–æ–ª—è –∫–∞—á–µ—Å—Ç–≤–∞ 
[23.05.2025 04:15] Using data from previous issue: {"categories": ["#video", "#inference", "#diffusion", "#optimization"], "emoji": "üß©", "ru": {"title": "–£—Å–∫–æ—Ä–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞", "desc": "Jenga - —ç—Ç–æ –Ω–æ–≤—ã–π –∫–æ–Ω–≤–µ–π–µ—Ä –≤—ã–≤–æ–¥–∞ –¥–ª—è –º–æ–¥–µ–ª–µ–π –≤–∏–¥–µ–æ-–¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤, –∫–æ—Ç–æ—Ä—ã–π —Å–æ—á–µ—Ç–∞–µ—Ç –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ –≤—ã—Ä–µ–∑–∞–Ω–∏–µ –≤–Ω–∏–º–∞–Ω–∏—è –∏ –ø—Ä–æ–≥—Ä–µ—Å—Å–∏
[23.05.2025 04:15] Using data from previous issue: {"categories": ["#rl", "#reasoning", "#optimization", "#training"], "emoji": "üß†", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –ò–ò —Ä–∞—Å—Å—É–∂–¥–∞—Ç—å –∫–∞–∫ —á–µ–ª–æ–≤–µ–∫", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –æ–±—É—á–µ–Ω–∏—è TON –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å
[23.05.2025 04:15] Using data from previous issue: {"categories": ["#architecture", "#reasoning", "#benchmark", "#multimodal"], "emoji": "üß†", "ru": {"title": "FRANK: –ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è", "desc": "–ú–æ–¥–µ–ª—å FRANK —É–ª—É—á—à–∞–µ—Ç –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (MLLM), –¥–æ–±–∞–≤–ª—è—è –∏–º —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é –∏ —Ä–µ—Ñ–ª–µ–∫—Å–∏–∏ –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è.
[23.05.2025 04:15] Using data from previous issue: {"categories": ["#multimodal", "#reasoning", "#rl", "#open_source", "#training"], "emoji": "üß†", "ru": {"title": "GRIT: –í–∏–∑—É–∞–ª—å–Ω–æ–µ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (MLLM) –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º GRI
[23.05.2025 04:15] Using data from previous issue: {"categories": ["#reasoning", "#dataset", "#rl", "#optimization", "#training"], "emoji": "üß†", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º —Ä–∞—Å–∫—Ä—ã–≤–∞–µ—Ç —Å–∫—Ä—ã—Ç—ã–π –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –º–∞–ª—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º (RL) –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫
[23.05.2025 04:15] Using data from previous issue: {"categories": ["#rlhf", "#training", "#rl", "#optimization", "#alignment"], "emoji": "üß†", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π: –º–µ–Ω—å—à–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –±–æ–ª—å—à–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç", "desc": "–≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –º–µ—Ç–æ–¥—ã –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º (RL) –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞—é—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ
[23.05.2025 04:15] Using data from previous issue: {"categories": ["#multimodal", "#training", "#hallucinations", "#benchmark", "#diffusion", "#rag", "#alignment"], "emoji": "üîÆ", "ru": {"title": "OViP: –û–±—É—á–µ–Ω–∏–µ –±–µ–∑ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π –¥–ª—è –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –Ω–∞–∑—ã–≤–∞–µ–º—ã–π O
[23.05.2025 04:15] Loading Chinese text from previous data.
[23.05.2025 04:15] Renaming data file.
[23.05.2025 04:15] Renaming previous data. hf_papers.json to ./d/2025-05-23.json
[23.05.2025 04:15] Saving new data file.
[23.05.2025 04:15] Generating page.
[23.05.2025 04:15] Renaming previous page.
[23.05.2025 04:15] Renaming previous data. index.html to ./d/2025-05-23.html
[23.05.2025 04:15] [Experimental] Generating Chinese page for reading.
[23.05.2025 04:15] Chinese vocab [{'word': 'ËÆ®ËÆ∫', 'pinyin': 't«éo l√πn', 'trans': 'discuss'}, {'word': 'ÁΩëÈ°µÂØºËà™', 'pinyin': 'w«éng y√® d«éo h√°ng', 'trans': 'web navigation'}, {'word': 'Ëá™Âä®Âåñ', 'pinyin': 'z√¨ d√≤ng hu√†', 'trans': 'automation'}, {'word': '‰ªªÂä°', 'pinyin': 'r√®n wu', 'trans': 'task'}, {'word': 'Á†îÁ©∂', 'pinyin': 'y√°n ji≈´', 'trans': 'research'}, {'word': 'Â§öÊ®°ÊÄÅ', 'pinyin': 'du≈ç m√≥ t√†i', 'trans': 'multimodal'}, {'word': 'Â§ßËØ≠Ë®ÄÊ®°Âûã', 'pinyin': 'd√† y«î y√°n m√≥ x√≠ng', 'trans': 'large language model'}, {'word': 'Â•ñÂä±Ê®°Âûã', 'pinyin': 'ji«éng l√¨ m√≥ x√≠ng', 'trans': 'reward model'}, {'word': 'ÈôêÂà∂', 'pinyin': 'xi√†n zh√¨', 'trans': 'limit'}, {'word': 'ÊèêÂá∫', 'pinyin': 't√≠ ch≈´', 'trans': 'propose'}, {'word': 'ËøáÁ®ãÂ•ñÂä±Ê®°Âûã', 'pinyin': 'gu√≤ ch√©ng ji«éng l√¨ m√≥ x√≠ng', 'trans': 'process reward model'}, {'word': 'ÈÄêÊ≠•', 'pinyin': 'zhu√≥ b√π', 'trans': 'step-by-step'}, {'word': 'ËØÑ‰º∞', 'pinyin': 'p√≠ng g≈´', 'trans': 'evaluate'}, {'word': 'Ë∑ØÂæÑ', 'pinyin': 'l√π j√¨ng', 'trans': 'path'}, {'word': 'ÂàõÂª∫', 'pinyin': 'chu√†ng ji√†n', 'trans': 'create'}, {'word': 'Êï∞ÊçÆÈõÜ', 'pinyin': 'sh√π j√π j√≠', 'trans': 'dataset'}, {'word': 'ËØÑ‰º∞Âü∫ÂáÜ', 'pinyin': 'p√≠ng g≈´ jƒ´ zh«în', 'trans': 'evaluation benchmark'}, {'word': 'ÂÆûÈ™å', 'pinyin': 'sh√≠ y√†n', 'trans': 'experiment'}, {'word': 'ËØÅÊòé', 'pinyin': 'zh√®ng m√≠ng', 'trans': 'prove'}, {'word': 'ÊúâÊïàÊÄß', 'pinyin': 'y«íu xi√†o x√¨ng', 'trans': 'effectiveness'}, {'word': 'ÊàêÊú¨ÊïàÁõä', 'pinyin': 'ch√©ng bƒõn xi√†o y√¨', 'trans': 'cost-effectiveness'}, {'word': 'ÂÖ¨ÂºÄÂèØÁî®', 'pinyin': 'g≈çng kƒÅi kƒõ y√≤ng', 'trans': 'publicly available'}]
[23.05.2025 04:15] Renaming previous Chinese page.
[23.05.2025 04:15] Renaming previous data. zh.html to ./d/2025-05-22_zh_reading_task.html
[23.05.2025 04:15] Writing Chinese reading task.
[23.05.2025 04:15] Writing result.
[23.05.2025 04:15] Renaming log file.
[23.05.2025 04:15] Renaming previous data. log.txt to ./logs/2025-05-23_last_log.txt
