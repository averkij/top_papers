[10.04.2025 07:12] Read previous papers.
[10.04.2025 07:12] Generating top page (month).
[10.04.2025 07:12] Writing top page (month).
[10.04.2025 08:14] Read previous papers.
[10.04.2025 08:14] Get feed.
[10.04.2025 08:14] Get page data from previous paper. URL: https://huggingface.co/papers/2504.05741
[10.04.2025 08:14] Get page data from previous paper. URL: https://huggingface.co/papers/2504.07083
[10.04.2025 08:14] Get page data from previous paper. URL: https://huggingface.co/papers/2504.07096
[10.04.2025 08:14] Get page data from previous paper. URL: https://huggingface.co/papers/2504.06514
[10.04.2025 08:14] Get page data from previous paper. URL: https://huggingface.co/papers/2504.04842
[10.04.2025 08:14] Get page data from previous paper. URL: https://huggingface.co/papers/2504.07086
[10.04.2025 08:14] Get page data from previous paper. URL: https://huggingface.co/papers/2504.07089
[10.04.2025 08:14] Get page data from previous paper. URL: https://huggingface.co/papers/2504.04010
[10.04.2025 08:14] Extract page data from URL. URL: https://huggingface.co/papers/2504.05287
[10.04.2025 08:14] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[10.04.2025 08:14] No deleted papers detected.
[10.04.2025 08:14] Downloading and parsing papers (pdf, html). Total: 9.
[10.04.2025 08:14] Downloading and parsing paper https://huggingface.co/papers/2504.05741.
[10.04.2025 08:14] Extra JSON file exists (./assets/json/2504.05741.json), skip PDF parsing.
[10.04.2025 08:14] Paper image links file exists (./assets/img_data/2504.05741.json), skip HTML parsing.
[10.04.2025 08:14] Success.
[10.04.2025 08:14] Downloading and parsing paper https://huggingface.co/papers/2504.07083.
[10.04.2025 08:14] Extra JSON file exists (./assets/json/2504.07083.json), skip PDF parsing.
[10.04.2025 08:14] Paper image links file exists (./assets/img_data/2504.07083.json), skip HTML parsing.
[10.04.2025 08:14] Success.
[10.04.2025 08:14] Downloading and parsing paper https://huggingface.co/papers/2504.07096.
[10.04.2025 08:14] Extra JSON file exists (./assets/json/2504.07096.json), skip PDF parsing.
[10.04.2025 08:14] Paper image links file exists (./assets/img_data/2504.07096.json), skip HTML parsing.
[10.04.2025 08:14] Success.
[10.04.2025 08:14] Downloading and parsing paper https://huggingface.co/papers/2504.06514.
[10.04.2025 08:14] Extra JSON file exists (./assets/json/2504.06514.json), skip PDF parsing.
[10.04.2025 08:14] Paper image links file exists (./assets/img_data/2504.06514.json), skip HTML parsing.
[10.04.2025 08:14] Success.
[10.04.2025 08:14] Downloading and parsing paper https://huggingface.co/papers/2504.04842.
[10.04.2025 08:14] Extra JSON file exists (./assets/json/2504.04842.json), skip PDF parsing.
[10.04.2025 08:14] Paper image links file exists (./assets/img_data/2504.04842.json), skip HTML parsing.
[10.04.2025 08:14] Success.
[10.04.2025 08:14] Downloading and parsing paper https://huggingface.co/papers/2504.07086.
[10.04.2025 08:14] Extra JSON file exists (./assets/json/2504.07086.json), skip PDF parsing.
[10.04.2025 08:14] Paper image links file exists (./assets/img_data/2504.07086.json), skip HTML parsing.
[10.04.2025 08:14] Success.
[10.04.2025 08:14] Downloading and parsing paper https://huggingface.co/papers/2504.07089.
[10.04.2025 08:14] Extra JSON file exists (./assets/json/2504.07089.json), skip PDF parsing.
[10.04.2025 08:14] Paper image links file exists (./assets/img_data/2504.07089.json), skip HTML parsing.
[10.04.2025 08:14] Success.
[10.04.2025 08:14] Downloading and parsing paper https://huggingface.co/papers/2504.04010.
[10.04.2025 08:14] Extra JSON file exists (./assets/json/2504.04010.json), skip PDF parsing.
[10.04.2025 08:14] Paper image links file exists (./assets/img_data/2504.04010.json), skip HTML parsing.
[10.04.2025 08:14] Success.
[10.04.2025 08:14] Downloading and parsing paper https://huggingface.co/papers/2504.05287.
[10.04.2025 08:14] Downloading paper 2504.05287 from http://arxiv.org/pdf/2504.05287v1...
[10.04.2025 08:15] Extracting affiliations from text.
[10.04.2025 08:15] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Robust Dexterous Grasping of General Objects from Single-view Perception Hui Zhang1, Zijian Wu2, Linyi Huang2, Sammy Christen1 and Jie Song2,3 2HKUST (Guangzhou), China 3HKUST, Hong Kong (China) 1ETH Zurich, Switzerland Email: huizhang@ethz.ch Equal Contribution 5 2 0 2 7 ] . [ 1 7 8 2 5 0 . 4 0 5 2 : r Fig. 1: Our method achieves robust dexterous grasping from single-view object point clouds. It performs adaptive motions to disturbances such as object movement and external forces (a), and can grasp various objects with random poses, diverse shapes, sizes, materials, and masses, including shiny, heavy, deformable, thin, and transparent objects (b). AbstractRobust grasping of various objects from single-view perception is fundamental for dexterous robots. Previous works often rely on fully observable objects, expert demonstrations, or static grasping poses, which restrict their generalization ability and adaptability to external disturbances. In this paper, we present reinforcement-learning-based framework that enables zero-shot dynamic dexterous grasping of wide range of unseen objects from single-view perception, while performing adaptive motions to external disturbances. We utilize hand-centric object representation for shape feature extraction that emphasizes interaction-relevant local shapes, enhancing robustness to shape variance and uncertainty. To enable effective hand adaptation to disturbances with limited observations, we propose mixed curriculum learning strategy, which first utilizes imitation learning to distill policy trained with privileged real-time visual-tactile feedback, and gradually transfers to reinforcement learning to learn adaptive motions under disturbances caused by observation noises and dynamic randomization. Our experiments demonstrate strong generalization in grasping unseen objects with random poses, achieving success rates of 97.0% across 247,786 simulated objects and 94.6% across 512 real objects. We also demonstrate the robustness "
[10.04.2025 08:15] Response: ```python
[
    "HKUST (Guangzhou), China",
    "HKUST, Hong Kong (China)",
    "ETH Zurich, Switzerland"
]
```
[10.04.2025 08:15] Deleting PDF ./assets/pdf/2504.05287.pdf.
[10.04.2025 08:15] Success.
[10.04.2025 08:15] Enriching papers with extra data.
[10.04.2025 08:15] ********************************************************************************
[10.04.2025 08:15] Abstract 0. Diffusion transformers have demonstrated remarkable generation quality, albeit requiring longer training iterations and numerous inference steps. In each denoising step, diffusion transformers encode the noisy inputs to extract the lower-frequency semantic component and then decode the higher freque...
[10.04.2025 08:15] ********************************************************************************
[10.04.2025 08:15] Abstract 1. Camera trajectory design plays a crucial role in video production, serving as a fundamental tool for conveying directorial intent and enhancing visual storytelling. In cinematography, Directors of Photography meticulously craft camera movements to achieve expressive and intentional framing. However,...
[10.04.2025 08:15] ********************************************************************************
[10.04.2025 08:15] Abstract 2. We present OLMoTrace, the first system that traces the outputs of language models back to their full, multi-trillion-token training data in real time. OLMoTrace finds and shows verbatim matches between segments of language model output and documents in the training text corpora. Powered by an extend...
[10.04.2025 08:15] ********************************************************************************
[10.04.2025 08:15] Abstract 3. We find that the response length of reasoning LLMs, whether trained by reinforcement learning or supervised learning, drastically increases for ill-posed questions with missing premises (MiP), ending up with redundant and ineffective thinking. This newly introduced scenario exacerbates the general o...
[10.04.2025 08:15] ********************************************************************************
[10.04.2025 08:15] Abstract 4. Creating a realistic animatable avatar from a single static portrait remains challenging. Existing approaches often struggle to capture subtle facial expressions, the associated global body movements, and the dynamic background. To address these limitations, we propose a novel framework that leverag...
[10.04.2025 08:15] ********************************************************************************
[10.04.2025 08:15] Abstract 5. Reasoning has emerged as the next major frontier for language models (LMs), with rapid advances from both academic and industrial labs. However, this progress often outpaces methodological rigor, with many evaluations relying on benchmarking practices that lack transparency, robustness, or statistic...
[10.04.2025 08:15] ********************************************************************************
[10.04.2025 08:15] Abstract 6. We propose OmniCaptioner, a versatile visual captioning framework for generating fine-grained textual descriptions across a wide variety of visual domains. Unlike prior methods limited to specific image types (e.g., natural images or geometric visuals), our framework provides a unified solution for ...
[10.04.2025 08:15] ********************************************************************************
[10.04.2025 08:15] Abstract 7. Generating naturalistic and nuanced listener motions for extended interactions remains an open problem. Existing methods often rely on low-dimensional motion codes for facial behavior generation followed by photorealistic rendering, limiting both visual fidelity and expressive richness. To address t...
[10.04.2025 08:15] ********************************************************************************
[10.04.2025 08:15] Abstract 8. Robust grasping of various objects from single-view perception is fundamental for dexterous robots. Previous works often rely on fully observable objects, expert demonstrations, or static grasping poses, which restrict their generalization ability and adaptability to external disturbances. In this p...
[10.04.2025 08:15] Read previous papers.
[10.04.2025 08:15] Generating reviews via LLM API.
[10.04.2025 08:15] Using data from previous issue: {"categories": ["#training", "#optimization", "#architecture", "#cv", "#diffusion"], "emoji": "ğŸ§ ", "ru": {"title": "DDT: Ğ Ğ°Ğ·Ğ´ĞµĞ»ÑĞ¹ Ğ¸ Ğ²Ğ»Ğ°ÑÑ‚Ğ²ÑƒĞ¹ Ğ² Ğ¼Ğ¸Ñ€Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ²", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ², Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ Decoupled Diffusion Transfor
[10.04.2025 08:15] Using data from previous issue: {"categories": ["#multimodal", "#dataset", "#cv"], "emoji": "ğŸ¥", "ru": {"title": "Ğ˜Ğ˜-Ğ¾Ğ¿ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ¹ ĞºĞ¸Ğ½ĞµĞ¼Ğ°Ñ‚Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ğ¸", "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ² ĞºĞ¸Ğ½Ğ¾Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğµ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´
[10.04.2025 08:15] Using data from previous issue: {"categories": ["#data", "#hallucinations", "#inference", "#open_source", "#interpretability", "#dataset"], "emoji": "ğŸ”", "ru": {"title": "Ğ—Ğ°Ğ³Ğ»ÑĞ½ÑƒÑ‚ÑŒ Ğ² Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸", "desc": "OLMoTrace - ÑÑ‚Ğ¾ Ğ¿ĞµÑ€Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ² Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ²ÑĞ·ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»Ğµ
[10.04.2025 08:15] Using data from previous issue: {"categories": ["#training", "#rl", "#hallucinations", "#reasoning"], "emoji": "ğŸ§ ", "ru": {"title": "ĞÑÑ‚Ğ¾Ñ€Ğ¾Ğ¶Ğ½Ğ¾: ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞºĞ»Ğ¾Ğ½Ğ½Ñ‹ Ğº Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼", "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ñ‡Ñ€ĞµĞ·Ğ¼ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ (overthinking) Ñƒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğµ Ğ½Ğ° Ğ½ĞµĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ¿
[10.04.2025 08:15] Using data from previous issue: {"categories": ["#multimodal", "#video", "#diffusion"], "emoji": "ğŸ—£ï¸", "ru": {"title": "ĞĞ¶Ğ¸Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ñ€Ñ‚Ñ€ĞµÑ‚Ğ¾Ğ²: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ¼Ğ° Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ", "desc": "Ğ”Ğ°Ğ½Ğ½Ğ°Ñ ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ°Ğ½Ğ¸Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ²Ğ°Ñ‚Ğ°Ñ€Ğ¾Ğ² Ğ¸Ğ· Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ€Ñ‚Ñ€ĞµÑ‚Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸
[10.04.2025 08:15] Using data from previous issue: {"categories": ["#open_source", "#survey", "#rl", "#reasoning", "#benchmark", "#training"], "emoji": "ğŸ§®", "ru": {"title": "Ğ¡Ñ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ°Ğ¼ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€
[10.04.2025 08:15] Using data from previous issue: {"categories": ["#optimization", "#multimodal", "#long_context", "#cv", "#training", "#reasoning"], "emoji": "ğŸ”", "ru": {"title": "Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜", "desc": "OmniCaptioner - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ Ñ€Ğ°Ğ·Ğ»Ğ¸
[10.04.2025 08:15] Using data from previous issue: {"categories": ["#benchmark", "#diffusion", "#video", "#multimodal"], "emoji": "ğŸ­", "ru": {"title": "DiTaiListener: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ€ĞµĞ°ĞºÑ†Ğ¸Ğ¹ ÑĞ»ÑƒÑˆĞ°Ñ‚ĞµĞ»Ñ", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ DiTaiListener - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ ÑĞ»ÑƒÑˆĞ°Ñ‚ĞµĞ»Ñ Ğ² Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°Ñ… Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒ
[10.04.2025 08:15] Querying the API.
[10.04.2025 08:15] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Robust grasping of various objects from single-view perception is fundamental for dexterous robots. Previous works often rely on fully observable objects, expert demonstrations, or static grasping poses, which restrict their generalization ability and adaptability to external disturbances. In this paper, we present a reinforcement-learning-based framework that enables zero-shot dynamic dexterous grasping of a wide range of unseen objects from single-view perception, while performing adaptive motions to external disturbances. We utilize a hand-centric object representation for shape feature extraction that emphasizes interaction-relevant local shapes, enhancing robustness to shape variance and uncertainty. To enable effective hand adaptation to disturbances with limited observations, we propose a mixed curriculum learning strategy, which first utilizes imitation learning to distill a policy trained with privileged real-time visual-tactile feedback, and gradually transfers to reinforcement learning to learn adaptive motions under disturbances caused by observation noises and dynamic randomization. Our experiments demonstrate strong generalization in grasping unseen objects with random poses, achieving success rates of 97.0% across 247,786 simulated objects and 94.6% across 512 real objects. We also demonstrate the robustness of our method to various disturbances, including unobserved object movement and external forces, through both quantitative and qualitative evaluations. Project Page: https://zdchan.github.io/Robust_DexGrasp/
[10.04.2025 08:15] Response: {
  "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ñ€ÑƒĞºĞ¾Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ¾ÑĞ¾Ğ±Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°, Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ½Ğ° Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ñ Ñ€ÑƒĞºĞ¾Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğº Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸ÑĞ¼ Ñ„Ğ¾Ñ€Ğ¼Ñ‹. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰Ğ°Ñ Ğ¸Ğ¼Ğ¸Ñ‚Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğº Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğ¼ Ğ¿Ğ¾Ğ¼ĞµÑ…Ğ°Ğ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ¿Ñ€Ğ¸ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğµ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸.",
  "emoji": "ğŸ¦¾",
  "title": "ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ¼ Ğ¿Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ñƒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ"
}
[10.04.2025 08:15] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Robust grasping of various objects from single-view perception is fundamental for dexterous robots. Previous works often rely on fully observable objects, expert demonstrations, or static grasping poses, which restrict their generalization ability and adaptability to external disturbances. In this paper, we present a reinforcement-learning-based framework that enables zero-shot dynamic dexterous grasping of a wide range of unseen objects from single-view perception, while performing adaptive motions to external disturbances. We utilize a hand-centric object representation for shape feature extraction that emphasizes interaction-relevant local shapes, enhancing robustness to shape variance and uncertainty. To enable effective hand adaptation to disturbances with limited observations, we propose a mixed curriculum learning strategy, which first utilizes imitation learning to distill a policy trained with privileged real-time visual-tactile feedback, and gradually transfers to reinforcement learning to learn adaptive motions under disturbances caused by observation noises and dynamic randomization. Our experiments demonstrate strong generalization in grasping unseen objects with random poses, achieving success rates of 97.0% across 247,786 simulated objects and 94.6% across 512 real objects. We also demonstrate the robustness of our method to various disturbances, including unobserved object movement and external forces, through both quantitative and qualitative evaluations. Project Page: https://zdchan.github.io/Robust_DexGrasp/"

[10.04.2025 08:15] Response: ```python
['RL', 'ROBOTICS', 'TRAINING']
```
[10.04.2025 08:15] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Robust grasping of various objects from single-view perception is fundamental for dexterous robots. Previous works often rely on fully observable objects, expert demonstrations, or static grasping poses, which restrict their generalization ability and adaptability to external disturbances. In this paper, we present a reinforcement-learning-based framework that enables zero-shot dynamic dexterous grasping of a wide range of unseen objects from single-view perception, while performing adaptive motions to external disturbances. We utilize a hand-centric object representation for shape feature extraction that emphasizes interaction-relevant local shapes, enhancing robustness to shape variance and uncertainty. To enable effective hand adaptation to disturbances with limited observations, we propose a mixed curriculum learning strategy, which first utilizes imitation learning to distill a policy trained with privileged real-time visual-tactile feedback, and gradually transfers to reinforcement learning to learn adaptive motions under disturbances caused by observation noises and dynamic randomization. Our experiments demonstrate strong generalization in grasping unseen objects with random poses, achieving success rates of 97.0% across 247,786 simulated objects and 94.6% across 512 real objects. We also demonstrate the robustness of our method to various disturbances, including unobserved object movement and external forces, through both quantitative and qualitative evaluations. Project Page: https://zdchan.github.io/Robust_DexGrasp/"

[10.04.2025 08:15] Response: ```python
["GAMES", "OPTIMIZATION"]
```
[10.04.2025 08:15] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces a reinforcement learning framework for robots to grasp various unseen objects using only a single view. Unlike previous methods that depend on fully visible objects or expert demonstrations, this approach allows for dynamic and adaptive grasping in response to disturbances. The authors utilize a hand-centric object representation to focus on important shape features, improving the robot\'s ability to handle shape variations. Their mixed curriculum learning strategy combines imitation learning and reinforcement learning, resulting in high success rates for grasping both simulated and real objects under challenging conditions.","title":"Dynamic Grasping: Robots That Adapt and Overcome!"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="This paper introduces a reinforcement learning framework for robots to grasp various unseen objects using only a single view. Unlike previous methods that depend on fully visible objects or expert demonstrations, this approach allows for dynamic and adaptive grasping in response to disturbances. The authors utilize a hand-centric object representation to focus on important shape features, improving the robot's ability to handle shape variations. Their mixed curriculum learning strategy combines imitation learning and reinforcement learning, resulting in high success rates for grasping both simulated and real objects under challenging conditions.", title='Dynamic Grasping: Robots That Adapt and Overcome!'))
[10.04.2025 08:15] Response: ParsedChatCompletionMessage[Article](content='{"desc":"æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°ä»å•è§†è§’æ„ŸçŸ¥ä¸­å¯¹å„ç§æœªçŸ¥ç‰©ä½“çš„é›¶-shotåŠ¨æ€çµå·§æŠ“å–ã€‚ä¸ä»¥å¾€ä¾èµ–å®Œå…¨å¯è§‚å¯Ÿç‰©ä½“æˆ–ä¸“å®¶æ¼”ç¤ºçš„æ–¹æ³•ä¸åŒï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿé€‚åº”å¤–éƒ¨å¹²æ‰°å¹¶è¿›è¡Œè‡ªé€‚åº”åŠ¨ä½œã€‚æˆ‘ä»¬é‡‡ç”¨ä»¥æ‰‹ä¸ºä¸­å¿ƒçš„ç‰©ä½“è¡¨ç¤ºæ³•ï¼Œæå–ä¸äº¤äº’ç›¸å…³çš„å±€éƒ¨å½¢çŠ¶ç‰¹å¾ï¼Œä»è€Œå¢å¼ºå¯¹å½¢çŠ¶å˜åŒ–å’Œä¸ç¡®å®šæ€§çš„é²æ£’æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æŠ“å–æœªçŸ¥ç‰©ä½“æ—¶å…·æœ‰å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼ŒæˆåŠŸç‡é«˜è¾¾97.0%ã€‚","title":"å®ç°çµå·§æŠ“å–çš„é›¶-shotå­¦ä¹ æ–°æ–¹æ³•"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°ä»å•è§†è§’æ„ŸçŸ¥ä¸­å¯¹å„ç§æœªçŸ¥ç‰©ä½“çš„é›¶-shotåŠ¨æ€çµå·§æŠ“å–ã€‚ä¸ä»¥å¾€ä¾èµ–å®Œå…¨å¯è§‚å¯Ÿç‰©ä½“æˆ–ä¸“å®¶æ¼”ç¤ºçš„æ–¹æ³•ä¸åŒï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿé€‚åº”å¤–éƒ¨å¹²æ‰°å¹¶è¿›è¡Œè‡ªé€‚åº”åŠ¨ä½œã€‚æˆ‘ä»¬é‡‡ç”¨ä»¥æ‰‹ä¸ºä¸­å¿ƒçš„ç‰©ä½“è¡¨ç¤ºæ³•ï¼Œæå–ä¸äº¤äº’ç›¸å…³çš„å±€éƒ¨å½¢çŠ¶ç‰¹å¾ï¼Œä»è€Œå¢å¼ºå¯¹å½¢çŠ¶å˜åŒ–å’Œä¸ç¡®å®šæ€§çš„é²æ£’æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æŠ“å–æœªçŸ¥ç‰©ä½“æ—¶å…·æœ‰å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼ŒæˆåŠŸç‡é«˜è¾¾97.0%ã€‚', title='å®ç°çµå·§æŠ“å–çš„é›¶-shotå­¦ä¹ æ–°æ–¹æ³•'))
[10.04.2025 08:15] Loading Chinese text from previous data.
[10.04.2025 08:15] Renaming data file.
[10.04.2025 08:15] Renaming previous data. hf_papers.json to ./d/2025-04-10.json
[10.04.2025 08:15] Saving new data file.
[10.04.2025 08:15] Generating page.
[10.04.2025 08:15] Renaming previous page.
[10.04.2025 08:15] Renaming previous data. index.html to ./d/2025-04-10.html
[10.04.2025 08:15] [Experimental] Generating Chinese page for reading.
[10.04.2025 08:15] Chinese vocab [{'word': 'å¤šæ¨¡æ€', 'pinyin': 'duÅ mÃ³ tÃ i', 'trans': 'multimodal'}, {'word': 'æ¨ç†', 'pinyin': 'tuÄ« lÇ', 'trans': 'inference'}, {'word': 'æ¨¡å‹', 'pinyin': 'mÃ³ xÃ­ng', 'trans': 'model'}, {'word': 'è½¬ç§»', 'pinyin': 'zhuÇn yÃ­', 'trans': 'transfer'}, {'word': 'æ–¹æ³•', 'pinyin': 'fÄng fÇ', 'trans': 'method'}, {'word': 'æ‰©å±•', 'pinyin': 'kuÃ² zhÇn', 'trans': 'extend'}, {'word': 'è§†è§‰', 'pinyin': 'shÃ¬ juÃ©', 'trans': 'visual'}, {'word': 'æŠ•å½±å™¨', 'pinyin': 'tÃ³u yÇng qÃ¬', 'trans': 'projector'}, {'word': 'å®ç°', 'pinyin': 'shÃ­ xiÃ n', 'trans': 'achieve'}, {'word': 'é‡æ–°', 'pinyin': 'chÃ³ng xÄ«n', 'trans': 're-'}, {'word': 'è®­ç»ƒ', 'pinyin': 'xÃ¹n liÃ n', 'trans': 'train'}, {'word': 'ç¼–ç å™¨', 'pinyin': 'biÄn mÇ qÃ¬', 'trans': 'encoder'}, {'word': 'é€‚åº”', 'pinyin': 'shÃ¬ yÃ¬ng', 'trans': 'adapt'}, {'word': 'æ··åˆ', 'pinyin': 'hÃ¹n hÃ©', 'trans': 'hybrid'}, {'word': 'ä¼˜åŒ–', 'pinyin': 'yÅu huÃ ', 'trans': 'optimization'}, {'word': 'ç­–ç•¥', 'pinyin': 'cÃ¨ lÃ¼Ã¨', 'trans': 'strategy'}, {'word': 'è‡ªé€‚åº”', 'pinyin': 'zÃ¬ shÃ¬ yÃ¬ng', 'trans': 'adaptive'}, {'word': 'é•¿åº¦', 'pinyin': 'chÃ¡ng dÃ¹', 'trans': 'length'}, {'word': 'æ€ç»´é“¾', 'pinyin': 'sÄ« wÃ©i liÃ¡n', 'trans': 'chain of thought'}, {'word': 'æç‚¼', 'pinyin': 'tÃ­ liÃ n', 'trans': 'extract'}, {'word': 'æ•ˆç‡', 'pinyin': 'xiÃ o lÇœ', 'trans': 'efficiency'}, {'word': 'å®éªŒ', 'pinyin': 'shÃ­ yÃ n', 'trans': 'experiment'}, {'word': 'ç»“æœ', 'pinyin': 'jiÃ© guÇ’', 'trans': 'result'}, {'word': 'æ˜¾ç¤º', 'pinyin': 'xiÇn shÃ¬', 'trans': 'show'}, {'word': 'åŸºå‡†', 'pinyin': 'jÄ« zhÇ”n', 'trans': 'benchmark'}, {'word': 'æµ‹è¯•', 'pinyin': 'cÃ¨ shÃ¬', 'trans': 'test'}, {'word': 'è¡¨ç°', 'pinyin': 'biÇo xiÃ n', 'trans': 'performance'}, {'word': 'å‡ºè‰²', 'pinyin': 'chÅ« sÃ¨', 'trans': 'outstanding'}, {'word': 'æƒé‡', 'pinyin': 'quÃ¡n zhÃ²ng', 'trans': 'weights'}, {'word': 'å…¬å¼€', 'pinyin': 'gÅng kÄi', 'trans': 'public'}]
[10.04.2025 08:15] Renaming previous Chinese page.
[10.04.2025 08:15] Renaming previous data. zh.html to ./d/2025-04-09_zh_reading_task.html
[10.04.2025 08:15] Writing Chinese reading task.
[10.04.2025 08:15] Writing result.
[10.04.2025 08:15] Renaming log file.
[10.04.2025 08:15] Renaming previous data. log.txt to ./logs/2025-04-10_last_log.txt
