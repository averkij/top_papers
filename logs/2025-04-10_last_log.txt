[10.04.2025 07:12] Read previous papers.
[10.04.2025 07:12] Generating top page (month).
[10.04.2025 07:12] Writing top page (month).
[10.04.2025 08:14] Read previous papers.
[10.04.2025 08:14] Get feed.
[10.04.2025 08:14] Get page data from previous paper. URL: https://huggingface.co/papers/2504.05741
[10.04.2025 08:14] Get page data from previous paper. URL: https://huggingface.co/papers/2504.07083
[10.04.2025 08:14] Get page data from previous paper. URL: https://huggingface.co/papers/2504.07096
[10.04.2025 08:14] Get page data from previous paper. URL: https://huggingface.co/papers/2504.06514
[10.04.2025 08:14] Get page data from previous paper. URL: https://huggingface.co/papers/2504.04842
[10.04.2025 08:14] Get page data from previous paper. URL: https://huggingface.co/papers/2504.07086
[10.04.2025 08:14] Get page data from previous paper. URL: https://huggingface.co/papers/2504.07089
[10.04.2025 08:14] Get page data from previous paper. URL: https://huggingface.co/papers/2504.04010
[10.04.2025 08:14] Extract page data from URL. URL: https://huggingface.co/papers/2504.05287
[10.04.2025 08:14] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[10.04.2025 08:14] No deleted papers detected.
[10.04.2025 08:14] Downloading and parsing papers (pdf, html). Total: 9.
[10.04.2025 08:14] Downloading and parsing paper https://huggingface.co/papers/2504.05741.
[10.04.2025 08:14] Extra JSON file exists (./assets/json/2504.05741.json), skip PDF parsing.
[10.04.2025 08:14] Paper image links file exists (./assets/img_data/2504.05741.json), skip HTML parsing.
[10.04.2025 08:14] Success.
[10.04.2025 08:14] Downloading and parsing paper https://huggingface.co/papers/2504.07083.
[10.04.2025 08:14] Extra JSON file exists (./assets/json/2504.07083.json), skip PDF parsing.
[10.04.2025 08:14] Paper image links file exists (./assets/img_data/2504.07083.json), skip HTML parsing.
[10.04.2025 08:14] Success.
[10.04.2025 08:14] Downloading and parsing paper https://huggingface.co/papers/2504.07096.
[10.04.2025 08:14] Extra JSON file exists (./assets/json/2504.07096.json), skip PDF parsing.
[10.04.2025 08:14] Paper image links file exists (./assets/img_data/2504.07096.json), skip HTML parsing.
[10.04.2025 08:14] Success.
[10.04.2025 08:14] Downloading and parsing paper https://huggingface.co/papers/2504.06514.
[10.04.2025 08:14] Extra JSON file exists (./assets/json/2504.06514.json), skip PDF parsing.
[10.04.2025 08:14] Paper image links file exists (./assets/img_data/2504.06514.json), skip HTML parsing.
[10.04.2025 08:14] Success.
[10.04.2025 08:14] Downloading and parsing paper https://huggingface.co/papers/2504.04842.
[10.04.2025 08:14] Extra JSON file exists (./assets/json/2504.04842.json), skip PDF parsing.
[10.04.2025 08:14] Paper image links file exists (./assets/img_data/2504.04842.json), skip HTML parsing.
[10.04.2025 08:14] Success.
[10.04.2025 08:14] Downloading and parsing paper https://huggingface.co/papers/2504.07086.
[10.04.2025 08:14] Extra JSON file exists (./assets/json/2504.07086.json), skip PDF parsing.
[10.04.2025 08:14] Paper image links file exists (./assets/img_data/2504.07086.json), skip HTML parsing.
[10.04.2025 08:14] Success.
[10.04.2025 08:14] Downloading and parsing paper https://huggingface.co/papers/2504.07089.
[10.04.2025 08:14] Extra JSON file exists (./assets/json/2504.07089.json), skip PDF parsing.
[10.04.2025 08:14] Paper image links file exists (./assets/img_data/2504.07089.json), skip HTML parsing.
[10.04.2025 08:14] Success.
[10.04.2025 08:14] Downloading and parsing paper https://huggingface.co/papers/2504.04010.
[10.04.2025 08:14] Extra JSON file exists (./assets/json/2504.04010.json), skip PDF parsing.
[10.04.2025 08:14] Paper image links file exists (./assets/img_data/2504.04010.json), skip HTML parsing.
[10.04.2025 08:14] Success.
[10.04.2025 08:14] Downloading and parsing paper https://huggingface.co/papers/2504.05287.
[10.04.2025 08:14] Downloading paper 2504.05287 from http://arxiv.org/pdf/2504.05287v1...
[10.04.2025 08:15] Extracting affiliations from text.
[10.04.2025 08:15] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Robust Dexterous Grasping of General Objects from Single-view Perception Hui Zhang1, Zijian Wu2, Linyi Huang2, Sammy Christen1 and Jie Song2,3 2HKUST (Guangzhou), China 3HKUST, Hong Kong (China) 1ETH Zurich, Switzerland Email: huizhang@ethz.ch Equal Contribution 5 2 0 2 7 ] . [ 1 7 8 2 5 0 . 4 0 5 2 : r Fig. 1: Our method achieves robust dexterous grasping from single-view object point clouds. It performs adaptive motions to disturbances such as object movement and external forces (a), and can grasp various objects with random poses, diverse shapes, sizes, materials, and masses, including shiny, heavy, deformable, thin, and transparent objects (b). AbstractRobust grasping of various objects from single-view perception is fundamental for dexterous robots. Previous works often rely on fully observable objects, expert demonstrations, or static grasping poses, which restrict their generalization ability and adaptability to external disturbances. In this paper, we present reinforcement-learning-based framework that enables zero-shot dynamic dexterous grasping of wide range of unseen objects from single-view perception, while performing adaptive motions to external disturbances. We utilize hand-centric object representation for shape feature extraction that emphasizes interaction-relevant local shapes, enhancing robustness to shape variance and uncertainty. To enable effective hand adaptation to disturbances with limited observations, we propose mixed curriculum learning strategy, which first utilizes imitation learning to distill policy trained with privileged real-time visual-tactile feedback, and gradually transfers to reinforcement learning to learn adaptive motions under disturbances caused by observation noises and dynamic randomization. Our experiments demonstrate strong generalization in grasping unseen objects with random poses, achieving success rates of 97.0% across 247,786 simulated objects and 94.6% across 512 real objects. We also demonstrate the robustness "
[10.04.2025 08:15] Response: ```python
[
    "HKUST (Guangzhou), China",
    "HKUST, Hong Kong (China)",
    "ETH Zurich, Switzerland"
]
```
[10.04.2025 08:15] Deleting PDF ./assets/pdf/2504.05287.pdf.
[10.04.2025 08:15] Success.
[10.04.2025 08:15] Enriching papers with extra data.
[10.04.2025 08:15] ********************************************************************************
[10.04.2025 08:15] Abstract 0. Diffusion transformers have demonstrated remarkable generation quality, albeit requiring longer training iterations and numerous inference steps. In each denoising step, diffusion transformers encode the noisy inputs to extract the lower-frequency semantic component and then decode the higher freque...
[10.04.2025 08:15] ********************************************************************************
[10.04.2025 08:15] Abstract 1. Camera trajectory design plays a crucial role in video production, serving as a fundamental tool for conveying directorial intent and enhancing visual storytelling. In cinematography, Directors of Photography meticulously craft camera movements to achieve expressive and intentional framing. However,...
[10.04.2025 08:15] ********************************************************************************
[10.04.2025 08:15] Abstract 2. We present OLMoTrace, the first system that traces the outputs of language models back to their full, multi-trillion-token training data in real time. OLMoTrace finds and shows verbatim matches between segments of language model output and documents in the training text corpora. Powered by an extend...
[10.04.2025 08:15] ********************************************************************************
[10.04.2025 08:15] Abstract 3. We find that the response length of reasoning LLMs, whether trained by reinforcement learning or supervised learning, drastically increases for ill-posed questions with missing premises (MiP), ending up with redundant and ineffective thinking. This newly introduced scenario exacerbates the general o...
[10.04.2025 08:15] ********************************************************************************
[10.04.2025 08:15] Abstract 4. Creating a realistic animatable avatar from a single static portrait remains challenging. Existing approaches often struggle to capture subtle facial expressions, the associated global body movements, and the dynamic background. To address these limitations, we propose a novel framework that leverag...
[10.04.2025 08:15] ********************************************************************************
[10.04.2025 08:15] Abstract 5. Reasoning has emerged as the next major frontier for language models (LMs), with rapid advances from both academic and industrial labs. However, this progress often outpaces methodological rigor, with many evaluations relying on benchmarking practices that lack transparency, robustness, or statistic...
[10.04.2025 08:15] ********************************************************************************
[10.04.2025 08:15] Abstract 6. We propose OmniCaptioner, a versatile visual captioning framework for generating fine-grained textual descriptions across a wide variety of visual domains. Unlike prior methods limited to specific image types (e.g., natural images or geometric visuals), our framework provides a unified solution for ...
[10.04.2025 08:15] ********************************************************************************
[10.04.2025 08:15] Abstract 7. Generating naturalistic and nuanced listener motions for extended interactions remains an open problem. Existing methods often rely on low-dimensional motion codes for facial behavior generation followed by photorealistic rendering, limiting both visual fidelity and expressive richness. To address t...
[10.04.2025 08:15] ********************************************************************************
[10.04.2025 08:15] Abstract 8. Robust grasping of various objects from single-view perception is fundamental for dexterous robots. Previous works often rely on fully observable objects, expert demonstrations, or static grasping poses, which restrict their generalization ability and adaptability to external disturbances. In this p...
[10.04.2025 08:15] Read previous papers.
[10.04.2025 08:15] Generating reviews via LLM API.
[10.04.2025 08:15] Using data from previous issue: {"categories": ["#training", "#optimization", "#architecture", "#cv", "#diffusion"], "emoji": "🧠", "ru": {"title": "DDT: Разделяй и властвуй в мире диффузионных трансформеров", "desc": "Статья представляет новый подход к архитектуре диффузионных трансформеров, называемый Decoupled Diffusion Transfor
[10.04.2025 08:15] Using data from previous issue: {"categories": ["#multimodal", "#dataset", "#cv"], "emoji": "🎥", "ru": {"title": "ИИ-оператор: новый стандарт компьютерной кинематографии", "desc": "Эта статья представляет новый подход к генерации траекторий движения камеры в кинопроизводстве с использованием методов машинного обучения. Авторы созд
[10.04.2025 08:15] Using data from previous issue: {"categories": ["#data", "#hallucinations", "#inference", "#open_source", "#interpretability", "#dataset"], "emoji": "🔍", "ru": {"title": "Заглянуть в память языковой модели", "desc": "OLMoTrace - это первая система, которая в режиме реального времени отслеживает связь между выводами языковых моделе
[10.04.2025 08:15] Using data from previous issue: {"categories": ["#training", "#rl", "#hallucinations", "#reasoning"], "emoji": "🧠", "ru": {"title": "Осторожно: языковые модели склонны к избыточным рассуждениям", "desc": "В статье исследуется проблема чрезмерного мышления (overthinking) у языковых моделей при ответе на некорректно поставленные воп
[10.04.2025 08:15] Using data from previous issue: {"categories": ["#multimodal", "#video", "#diffusion"], "emoji": "🗣️", "ru": {"title": "Оживление статичных портретов: новый уровень реализма и контроля", "desc": "Данная статья представляет новый подход к созданию анимированных аватаров из одного статичного портрета с помощью предобученной видео-ди
[10.04.2025 08:15] Using data from previous issue: {"categories": ["#open_source", "#survey", "#rl", "#reasoning", "#benchmark", "#training"], "emoji": "🧮", "ru": {"title": "Стандартизация оценки математических рассуждений языковых моделей", "desc": "Статья посвящена проблемам оценки способностей языковых моделей к математическим рассуждениям. Автор
[10.04.2025 08:15] Using data from previous issue: {"categories": ["#optimization", "#multimodal", "#long_context", "#cv", "#training", "#reasoning"], "emoji": "🔍", "ru": {"title": "Универсальное описание изображений для улучшения мультимодального ИИ", "desc": "OmniCaptioner - это универсальная система для создания подробных текстовых описаний разли
[10.04.2025 08:15] Using data from previous issue: {"categories": ["#benchmark", "#diffusion", "#video", "#multimodal"], "emoji": "🎭", "ru": {"title": "DiTaiListener: революция в генерации реалистичных реакций слушателя", "desc": "Статья представляет DiTaiListener - новый метод генерации естественных движений слушателя в длительных диалогах с исполь
[10.04.2025 08:15] Querying the API.
[10.04.2025 08:15] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Robust grasping of various objects from single-view perception is fundamental for dexterous robots. Previous works often rely on fully observable objects, expert demonstrations, or static grasping poses, which restrict their generalization ability and adaptability to external disturbances. In this paper, we present a reinforcement-learning-based framework that enables zero-shot dynamic dexterous grasping of a wide range of unseen objects from single-view perception, while performing adaptive motions to external disturbances. We utilize a hand-centric object representation for shape feature extraction that emphasizes interaction-relevant local shapes, enhancing robustness to shape variance and uncertainty. To enable effective hand adaptation to disturbances with limited observations, we propose a mixed curriculum learning strategy, which first utilizes imitation learning to distill a policy trained with privileged real-time visual-tactile feedback, and gradually transfers to reinforcement learning to learn adaptive motions under disturbances caused by observation noises and dynamic randomization. Our experiments demonstrate strong generalization in grasping unseen objects with random poses, achieving success rates of 97.0% across 247,786 simulated objects and 94.6% across 512 real objects. We also demonstrate the robustness of our method to various disturbances, including unobserved object movement and external forces, through both quantitative and qualitative evaluations. Project Page: https://zdchan.github.io/Robust_DexGrasp/
[10.04.2025 08:15] Response: {
  "desc": "Статья представляет метод обучения с подкреплением для захвата разнообразных объектов роботизированной рукой на основе одного изображения. Авторы используют особое представление объекта, ориентированное на взаимодействие с рукой, что повышает устойчивость к вариациям формы. Предложена стратегия смешанного обучения, сочетающая имитационное обучение и обучение с подкреплением для адаптации к внешним помехам. Эксперименты показывают высокую обобщающую способность метода при захвате новых объектов в симуляции и реальности.",
  "emoji": "🦾",
  "title": "Адаптивный захват объектов роботом по одному изображению"
}
[10.04.2025 08:15] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Robust grasping of various objects from single-view perception is fundamental for dexterous robots. Previous works often rely on fully observable objects, expert demonstrations, or static grasping poses, which restrict their generalization ability and adaptability to external disturbances. In this paper, we present a reinforcement-learning-based framework that enables zero-shot dynamic dexterous grasping of a wide range of unseen objects from single-view perception, while performing adaptive motions to external disturbances. We utilize a hand-centric object representation for shape feature extraction that emphasizes interaction-relevant local shapes, enhancing robustness to shape variance and uncertainty. To enable effective hand adaptation to disturbances with limited observations, we propose a mixed curriculum learning strategy, which first utilizes imitation learning to distill a policy trained with privileged real-time visual-tactile feedback, and gradually transfers to reinforcement learning to learn adaptive motions under disturbances caused by observation noises and dynamic randomization. Our experiments demonstrate strong generalization in grasping unseen objects with random poses, achieving success rates of 97.0% across 247,786 simulated objects and 94.6% across 512 real objects. We also demonstrate the robustness of our method to various disturbances, including unobserved object movement and external forces, through both quantitative and qualitative evaluations. Project Page: https://zdchan.github.io/Robust_DexGrasp/"

[10.04.2025 08:15] Response: ```python
['RL', 'ROBOTICS', 'TRAINING']
```
[10.04.2025 08:15] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Robust grasping of various objects from single-view perception is fundamental for dexterous robots. Previous works often rely on fully observable objects, expert demonstrations, or static grasping poses, which restrict their generalization ability and adaptability to external disturbances. In this paper, we present a reinforcement-learning-based framework that enables zero-shot dynamic dexterous grasping of a wide range of unseen objects from single-view perception, while performing adaptive motions to external disturbances. We utilize a hand-centric object representation for shape feature extraction that emphasizes interaction-relevant local shapes, enhancing robustness to shape variance and uncertainty. To enable effective hand adaptation to disturbances with limited observations, we propose a mixed curriculum learning strategy, which first utilizes imitation learning to distill a policy trained with privileged real-time visual-tactile feedback, and gradually transfers to reinforcement learning to learn adaptive motions under disturbances caused by observation noises and dynamic randomization. Our experiments demonstrate strong generalization in grasping unseen objects with random poses, achieving success rates of 97.0% across 247,786 simulated objects and 94.6% across 512 real objects. We also demonstrate the robustness of our method to various disturbances, including unobserved object movement and external forces, through both quantitative and qualitative evaluations. Project Page: https://zdchan.github.io/Robust_DexGrasp/"

[10.04.2025 08:15] Response: ```python
["GAMES", "OPTIMIZATION"]
```
[10.04.2025 08:15] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces a reinforcement learning framework for robots to grasp various unseen objects using only a single view. Unlike previous methods that depend on fully visible objects or expert demonstrations, this approach allows for dynamic and adaptive grasping in response to disturbances. The authors utilize a hand-centric object representation to focus on important shape features, improving the robot\'s ability to handle shape variations. Their mixed curriculum learning strategy combines imitation learning and reinforcement learning, resulting in high success rates for grasping both simulated and real objects under challenging conditions.","title":"Dynamic Grasping: Robots That Adapt and Overcome!"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="This paper introduces a reinforcement learning framework for robots to grasp various unseen objects using only a single view. Unlike previous methods that depend on fully visible objects or expert demonstrations, this approach allows for dynamic and adaptive grasping in response to disturbances. The authors utilize a hand-centric object representation to focus on important shape features, improving the robot's ability to handle shape variations. Their mixed curriculum learning strategy combines imitation learning and reinforcement learning, resulting in high success rates for grasping both simulated and real objects under challenging conditions.", title='Dynamic Grasping: Robots That Adapt and Overcome!'))
[10.04.2025 08:15] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本论文提出了一种基于强化学习的框架，旨在实现从单视角感知中对各种未知物体的零-shot动态灵巧抓取。与以往依赖完全可观察物体或专家演示的方法不同，该方法能够适应外部干扰并进行自适应动作。我们采用以手为中心的物体表示法，提取与交互相关的局部形状特征，从而增强对形状变化和不确定性的鲁棒性。实验结果表明，该方法在抓取未知物体时具有强大的泛化能力，成功率高达97.0%。","title":"实现灵巧抓取的零-shot学习新方法"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本论文提出了一种基于强化学习的框架，旨在实现从单视角感知中对各种未知物体的零-shot动态灵巧抓取。与以往依赖完全可观察物体或专家演示的方法不同，该方法能够适应外部干扰并进行自适应动作。我们采用以手为中心的物体表示法，提取与交互相关的局部形状特征，从而增强对形状变化和不确定性的鲁棒性。实验结果表明，该方法在抓取未知物体时具有强大的泛化能力，成功率高达97.0%。', title='实现灵巧抓取的零-shot学习新方法'))
[10.04.2025 08:15] Loading Chinese text from previous data.
[10.04.2025 08:15] Renaming data file.
[10.04.2025 08:15] Renaming previous data. hf_papers.json to ./d/2025-04-10.json
[10.04.2025 08:15] Saving new data file.
[10.04.2025 08:15] Generating page.
[10.04.2025 08:15] Renaming previous page.
[10.04.2025 08:15] Renaming previous data. index.html to ./d/2025-04-10.html
[10.04.2025 08:15] [Experimental] Generating Chinese page for reading.
[10.04.2025 08:15] Chinese vocab [{'word': '多模态', 'pinyin': 'duō mó tài', 'trans': 'multimodal'}, {'word': '推理', 'pinyin': 'tuī lǐ', 'trans': 'inference'}, {'word': '模型', 'pinyin': 'mó xíng', 'trans': 'model'}, {'word': '转移', 'pinyin': 'zhuǎn yí', 'trans': 'transfer'}, {'word': '方法', 'pinyin': 'fāng fǎ', 'trans': 'method'}, {'word': '扩展', 'pinyin': 'kuò zhǎn', 'trans': 'extend'}, {'word': '视觉', 'pinyin': 'shì jué', 'trans': 'visual'}, {'word': '投影器', 'pinyin': 'tóu yǐng qì', 'trans': 'projector'}, {'word': '实现', 'pinyin': 'shí xiàn', 'trans': 'achieve'}, {'word': '重新', 'pinyin': 'chóng xīn', 'trans': 're-'}, {'word': '训练', 'pinyin': 'xùn liàn', 'trans': 'train'}, {'word': '编码器', 'pinyin': 'biān mǎ qì', 'trans': 'encoder'}, {'word': '适应', 'pinyin': 'shì yìng', 'trans': 'adapt'}, {'word': '混合', 'pinyin': 'hùn hé', 'trans': 'hybrid'}, {'word': '优化', 'pinyin': 'yōu huà', 'trans': 'optimization'}, {'word': '策略', 'pinyin': 'cè lüè', 'trans': 'strategy'}, {'word': '自适应', 'pinyin': 'zì shì yìng', 'trans': 'adaptive'}, {'word': '长度', 'pinyin': 'cháng dù', 'trans': 'length'}, {'word': '思维链', 'pinyin': 'sī wéi lián', 'trans': 'chain of thought'}, {'word': '提炼', 'pinyin': 'tí liàn', 'trans': 'extract'}, {'word': '效率', 'pinyin': 'xiào lǜ', 'trans': 'efficiency'}, {'word': '实验', 'pinyin': 'shí yàn', 'trans': 'experiment'}, {'word': '结果', 'pinyin': 'jié guǒ', 'trans': 'result'}, {'word': '显示', 'pinyin': 'xiǎn shì', 'trans': 'show'}, {'word': '基准', 'pinyin': 'jī zhǔn', 'trans': 'benchmark'}, {'word': '测试', 'pinyin': 'cè shì', 'trans': 'test'}, {'word': '表现', 'pinyin': 'biǎo xiàn', 'trans': 'performance'}, {'word': '出色', 'pinyin': 'chū sè', 'trans': 'outstanding'}, {'word': '权重', 'pinyin': 'quán zhòng', 'trans': 'weights'}, {'word': '公开', 'pinyin': 'gōng kāi', 'trans': 'public'}]
[10.04.2025 08:15] Renaming previous Chinese page.
[10.04.2025 08:15] Renaming previous data. zh.html to ./d/2025-04-09_zh_reading_task.html
[10.04.2025 08:15] Writing Chinese reading task.
[10.04.2025 08:15] Writing result.
[10.04.2025 08:15] Renaming log file.
[10.04.2025 08:15] Renaming previous data. log.txt to ./logs/2025-04-10_last_log.txt
