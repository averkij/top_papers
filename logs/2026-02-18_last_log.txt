[18.02.2026 07:46] Read previous papers.
[18.02.2026 07:46] Generating top page (month).
[18.02.2026 07:46] Writing top page (month).
[18.02.2026 08:37] Read previous papers.
[18.02.2026 08:37] Get feed.
[18.02.2026 08:37] Get page data from previous paper. URL: https://huggingface.co/papers/2602.15763
[18.02.2026 08:37] Get page data from previous paper. URL: https://huggingface.co/papers/2602.14299
[18.02.2026 08:37] Get page data from previous paper. URL: https://huggingface.co/papers/2602.15112
[18.02.2026 08:37] Extract page data from URL. URL: https://huggingface.co/papers/2602.15200
[18.02.2026 08:37] Get page data from previous paper. URL: https://huggingface.co/papers/2602.14486
[18.02.2026 08:37] Get page data from previous paper. URL: https://huggingface.co/papers/2602.09653
[18.02.2026 08:37] Get page data from previous paper. URL: https://huggingface.co/papers/2602.15772
[18.02.2026 08:37] Get page data from previous paper. URL: https://huggingface.co/papers/2602.15620
[18.02.2026 08:37] Get page data from previous paper. URL: https://huggingface.co/papers/2602.15322
[18.02.2026 08:37] Get page data from previous paper. URL: https://huggingface.co/papers/2602.15278
[18.02.2026 08:37] Get page data from previous paper. URL: https://huggingface.co/papers/2602.15327
[18.02.2026 08:37] Get page data from previous paper. URL: https://huggingface.co/papers/2602.12978
[18.02.2026 08:37] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07854
[18.02.2026 08:37] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[18.02.2026 08:37] No deleted papers detected.
[18.02.2026 08:37] Downloading and parsing papers (pdf, html). Total: 13.
[18.02.2026 08:37] Downloading and parsing paper https://huggingface.co/papers/2602.15763.
[18.02.2026 08:37] Extra JSON file exists (./assets/json/2602.15763.json), skip PDF parsing.
[18.02.2026 08:37] Paper image links file exists (./assets/img_data/2602.15763.json), skip HTML parsing.
[18.02.2026 08:37] Success.
[18.02.2026 08:37] Downloading and parsing paper https://huggingface.co/papers/2602.14299.
[18.02.2026 08:37] Extra JSON file exists (./assets/json/2602.14299.json), skip PDF parsing.
[18.02.2026 08:37] Paper image links file exists (./assets/img_data/2602.14299.json), skip HTML parsing.
[18.02.2026 08:37] Success.
[18.02.2026 08:37] Downloading and parsing paper https://huggingface.co/papers/2602.15112.
[18.02.2026 08:37] Extra JSON file exists (./assets/json/2602.15112.json), skip PDF parsing.
[18.02.2026 08:37] Paper image links file exists (./assets/img_data/2602.15112.json), skip HTML parsing.
[18.02.2026 08:37] Success.
[18.02.2026 08:37] Downloading and parsing paper https://huggingface.co/papers/2602.15200.
[18.02.2026 08:37] Downloading paper 2602.15200 from https://arxiv.org/pdf/2602.15200v1...
[18.02.2026 08:37] Extracting affiliations from text.
[18.02.2026 08:37] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 6 1 ] . [ 1 0 0 2 5 1 . 2 0 6 2 : r COMPOT: Calibration-Optimized Matrix Procrustes Orthogonalization for Denis Makhov 1 Dmitriy Shopkhoev 1 Magauiya Zhussip 1 Ammar Ali 1 2 Baher Mohammad 1 2 Stamatios Lefkimmiatis "
[18.02.2026 08:37] Response: ```python
[]
```
[18.02.2026 08:37] Extracting affiliations from text.
[18.02.2026 08:37] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 6 1 ] . [ 1 0 0 2 5 1 . 2 0 6 2 : r COMPOT: Calibration-Optimized Matrix Procrustes Orthogonalization forDenis Makhov 1 Dmitriy Shopkhoev 1 Magauiya Zhussip 1 Ammar Ali 1 2 Baher Mohammad 1 2 Stamatios LefkimmiatisPost-training compression of Transformer models commonly relies on truncated singular value decomposition (SVD). However, enforcing single shared subspace can degrade accuracy even at moderate compression. Sparse dictionary learning provides more flexible union-of-subspaces representation, but existing approaches often suffer from iterative dictionary and coefficient updates. We propose COMPOT (CalibrationOptimized Matrix Procrustes Orthogonalization for Transformers), training-free compression framework that uses small calibration dataset to estimate sparse weight factorization. COMPOT employs orthogonal dictionaries that enable closed-form Procrustes updates for the dictionary and analytical single-step sparse coding for the coefficients, eliminating iterative optimization. To handle heterogeneous layer sensitivity under global compression budget, COMPOT further introduces one-shot dynamic allocation strategy that adaptively redistributes layer-wise compression rates. Extensive experiments across diverse architectures and tasks show that COMPOT consistently delivers superior qualitycompression trade-off over strong low-rank and sparse baselines, while remaining fully compatible with posttraining quantization for extreme compression. Code is available here. 1. Introduction Transformer-based foundation models underpin state-of-theart systems across modalities, including language, vision, vision-language, and audio (Vaswani et al., 2017; Dosovitskiy, 2020; Radford et al., 2021; Gong et al., 2021). As they scale to billions of parameters, deployment is increasingly limited by memory footprint, bandwidth, and compute 1Fundamental Research Center MWS AI 2ITMO. Correspondence to: Denis Makhov <d.makhov@mts.ai>, Stamatios Lefkimmiatis <s.lefkimmiatis@mts.ai>. Preprint. February 18, 2026. (Brown et al., 2020; Chowdhery et al., 2023). Yet Transformers are highly redundant: attention heads and other components can often be removed or simplified with limited degradation, indicating that parameters contribute unevenly to downstream behavior (Michel et al., 2019; Voita et al., 2019). This has motivated extensive post-training compression work: quantization, pruning, distillation, and factorization - to reduce inference cost and memory footprint without expensive retraining (Tang et al., 2024; Liu et al., 2025). Low-rank matrix factorization is widely used post-training paradigm, replacing dense projection matrices by products of smaller matrices. Activation-aware and task-aware lowrank methods show that calibration/activation information is crucial for preserving accuracy (Hsu et al., 2022; Yuan et al., 2023). Recent SVD-based methods further tailor truncation using calibration and principled losses: SVDLLM uses whitening-based calibration to relate singular values to truncation loss and applies closed-form updates to mitigate degradation (Wang et al., 2025b). Despite these advances, SVD methods still enforce single shared subspace per weight matrix, which can be restrictive when different columns lie in different local subspaces. Sparse dictionary learning offers complementary alternative, modeling weight matrix as dense dictionary times column-sparse coefficient matrix, enabling union-ofsubspaces representation. CoSpaDi shows that calibrationguided sparse dictionary learning can outperform dataaware low-rank baselines for training-free LLM compression (Shopkhoev et al., 2025b). Relatedly, MASA proposes matrix-based dictionary learning for parameter sharing in Transformer attention, representing layer-specific projections as combinations of shared matrix atoms (Zhussip et al., 2025). However, existing dictionary learning pipelines typically rely on iterative and expensive dictionary and sparse coding updates (e.g., K-SVD/OMP (Aharon et al., 2006)), limiting practicality at billion-parameter scale. We introduce COMPOT (Calibration-Optimized Matrix Procrustes Orthogonalization for Transformers), trainingfree compression framework that improves calibrationguided dictionary learning in both accuracy and compression time. Our key design choice is to enforce orthogonality and restrict the dictionary to be complete or undercomplete, COMPOT: Calibration-Optimized Matrix Procrustes Orthogonalization for Transformers Compression Figure 1. Comparison of low-rank decomposition, dictionary learning with sparse coding, and COMPOT. Low-rank uses rigid shared orthogonal basis B; dictionary learning enables flexible union-of-subspaces; COMPOT lies in between by using unionof-orthogonal-subspaces (denoted with O), which enables fast closed-form dictionary updates and lightweight coefficient update, improving compression performance. leveraging evidence that orthogonal dictionary learning can be effective and substantially faster than overcomplete alternatives (Bao et al., 2013). Under orthogonality, the dictionary update becomes the classical orthogonal Procrustes problem with closed-form SVD solution (Schonemann, 1966), replacing atom-wise K-SVD updates. Meanwhile, the sparse coding update also reduces to closed-form hardthresholding operation, avoiding iterative pursuits and yielding an efficient alternating procedure (Elad, 2010). Transformer layers and projection types exhibit heterogeneous redundancy. Prior work addresses this via nonuniform rank allocation (e.g., SVD-LLM V2) (Wang et al., 2025a) or differentiable truncation optimization (DobiSVD) (Qinsi et al., 2025). We instead propose one-shot global allocation strategy: we normalize each weight matrix, pool singular values across matrices into global spectrum, and truncate globally to satisfy model-wide budget, while enforcing constraints that prevent negative or low per-matrix compression and cap over-compression of sensitive layers. This yields simple, deterministic allocation procedure without iterative search over layer-wise ratios. COMPOT integrates naturally with post-training quantization. Among PTQ methods such as SmoothQuant (Xiao et al., 2023), GPTQ (Frantar et al., 2023), and AWQ (Lin et al., 2024), we show that 4-bit GPTQ applied on top of COMPOT yields superior results compared to quantization alone under equal memory budgets. Contributions. (i) We introduce an orthogonal dictionarybased sparse factorization for Tran"
[18.02.2026 08:37] Mistral response. {"id": "c340203d667b44e68b6a772cd1abb6a7", "created": 1771403832, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1467, "total_tokens": 1485, "completion_tokens": 18, "prompt_tokens_details": {"cached_tokens": 0}}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\"Fundamental Research Center MWS AI\", \"ITMO\"]\n```"}}]}
[18.02.2026 08:37] Response: ```python
["Fundamental Research Center MWS AI", "ITMO"]
```
[18.02.2026 08:37] Deleting PDF ./assets/pdf/2602.15200.pdf.
[18.02.2026 08:37] Success.
[18.02.2026 08:37] Downloading and parsing paper https://huggingface.co/papers/2602.14486.
[18.02.2026 08:37] Extra JSON file exists (./assets/json/2602.14486.json), skip PDF parsing.
[18.02.2026 08:37] Paper image links file exists (./assets/img_data/2602.14486.json), skip HTML parsing.
[18.02.2026 08:37] Success.
[18.02.2026 08:37] Downloading and parsing paper https://huggingface.co/papers/2602.09653.
[18.02.2026 08:37] Extra JSON file exists (./assets/json/2602.09653.json), skip PDF parsing.
[18.02.2026 08:37] Paper image links file exists (./assets/img_data/2602.09653.json), skip HTML parsing.
[18.02.2026 08:37] Success.
[18.02.2026 08:37] Downloading and parsing paper https://huggingface.co/papers/2602.15772.
[18.02.2026 08:37] Extra JSON file exists (./assets/json/2602.15772.json), skip PDF parsing.
[18.02.2026 08:37] Paper image links file exists (./assets/img_data/2602.15772.json), skip HTML parsing.
[18.02.2026 08:37] Success.
[18.02.2026 08:37] Downloading and parsing paper https://huggingface.co/papers/2602.15620.
[18.02.2026 08:37] Extra JSON file exists (./assets/json/2602.15620.json), skip PDF parsing.
[18.02.2026 08:37] Paper image links file exists (./assets/img_data/2602.15620.json), skip HTML parsing.
[18.02.2026 08:37] Success.
[18.02.2026 08:37] Downloading and parsing paper https://huggingface.co/papers/2602.15322.
[18.02.2026 08:37] Extra JSON file exists (./assets/json/2602.15322.json), skip PDF parsing.
[18.02.2026 08:37] Paper image links file exists (./assets/img_data/2602.15322.json), skip HTML parsing.
[18.02.2026 08:37] Success.
[18.02.2026 08:37] Downloading and parsing paper https://huggingface.co/papers/2602.15278.
[18.02.2026 08:37] Extra JSON file exists (./assets/json/2602.15278.json), skip PDF parsing.
[18.02.2026 08:37] Paper image links file exists (./assets/img_data/2602.15278.json), skip HTML parsing.
[18.02.2026 08:37] Success.
[18.02.2026 08:37] Downloading and parsing paper https://huggingface.co/papers/2602.15327.
[18.02.2026 08:37] Extra JSON file exists (./assets/json/2602.15327.json), skip PDF parsing.
[18.02.2026 08:37] Paper image links file exists (./assets/img_data/2602.15327.json), skip HTML parsing.
[18.02.2026 08:37] Success.
[18.02.2026 08:37] Downloading and parsing paper https://huggingface.co/papers/2602.12978.
[18.02.2026 08:37] Extra JSON file exists (./assets/json/2602.12978.json), skip PDF parsing.
[18.02.2026 08:37] Paper image links file exists (./assets/img_data/2602.12978.json), skip HTML parsing.
[18.02.2026 08:37] Success.
[18.02.2026 08:37] Downloading and parsing paper https://huggingface.co/papers/2602.07854.
[18.02.2026 08:37] Extra JSON file exists (./assets/json/2602.07854.json), skip PDF parsing.
[18.02.2026 08:37] Paper image links file exists (./assets/img_data/2602.07854.json), skip HTML parsing.
[18.02.2026 08:37] Success.
[18.02.2026 08:37] Enriching papers with extra data.
[18.02.2026 08:37] ********************************************************************************
[18.02.2026 08:37] Abstract 0. GLM-5 advances foundation models with DSA for cost reduction, asynchronous reinforcement learning for improved alignment, and enhanced coding capabilities for real-world software engineering.  					AI-generated summary 				 We present GLM-5, a next-generation foundation model designed to transition ...
[18.02.2026 08:37] ********************************************************************************
[18.02.2026 08:37] Abstract 1. Large language model agents in networked environments exhibit dynamic stability without true social convergence, maintaining individual diversity while lacking collective influence structures.  					AI-generated summary 				 As large language model agents increasingly populate networked environments...
[18.02.2026 08:37] ********************************************************************************
[18.02.2026 08:37] Abstract 2. ResearchGym presents a benchmark environment for evaluating AI agents on end-to-end research tasks, revealing significant capability-reliability gaps in current autonomous agents despite occasional state-of-the-art performance.  					AI-generated summary 				 We introduce ResearchGym, a benchmark an...
[18.02.2026 08:37] ********************************************************************************
[18.02.2026 08:37] Abstract 3. COMPOT is a training-free compression framework for Transformer models that uses sparse dictionary learning with orthogonal dictionaries and closed-form updates to achieve better quality-compression trade-offs than traditional low-rank methods.  					AI-generated summary 				 Post-training compressi...
[18.02.2026 08:37] ********************************************************************************
[18.02.2026 08:37] Abstract 4. Network representations converge toward shared local neighborhood relationships rather than global statistical models, as revealed by calibrated similarity metrics.  					AI-generated summary 				 The Platonic Representation Hypothesis suggests that representations from neural networks are convergin...
[18.02.2026 08:37] ********************************************************************************
[18.02.2026 08:37] Abstract 5. A two-stage framework addresses alignment of large language models with clinician preferences through physician-verified examples and distilled clinical principles for improved medical reasoning.  					AI-generated summary 				 Although large language models (LLMs) demonstrate expert-level medical k...
[18.02.2026 08:37] ********************************************************************************
[18.02.2026 08:37] Abstract 6. The Reason-Reflect-Refine framework addresses the trade-off between generation and understanding in multimodal models by reformulating single-step generation into a multi-step process that explicitly incorporates understanding during generation.  					AI-generated summary 				 Current research in mu...
[18.02.2026 08:37] ********************************************************************************
[18.02.2026 08:37] Abstract 7. Research identifies spurious tokens as the cause of training instability in reinforcement learning fine-tuning of large language models and proposes a solution that selectively masks problematic gradient updates to improve reasoning performance.  					AI-generated summary 				 Reinforcement Learning...
[18.02.2026 08:37] ********************************************************************************
[18.02.2026 08:37] Abstract 8. Random parameter update masking achieves superior optimization for large language models by inducing curvature-dependent regularization, with a momentum-aligned variant delivering significant performance improvements over state-of-the-art adaptive optimizers.  					AI-generated summary 				 Training...
[18.02.2026 08:37] ********************************************************************************
[18.02.2026 08:37] Abstract 9. Visual-language models' decision-making preferences are studied through controlled image choice tasks with systematic input perturbations, revealing visual vulnerabilities and safety concerns.  					AI-generated summary 				 The web is littered with images, once created for human consumption and now...
[18.02.2026 08:37] ********************************************************************************
[18.02.2026 08:37] Abstract 10. Large-scale observational analysis estimates capability boundaries and performance predictions for foundation models using quantile regression and evaluates temporal stability across tasks.  					AI-generated summary 				 For deploying foundation models, practitioners increasingly need prescriptive ...
[18.02.2026 08:37] ********************************************************************************
[18.02.2026 08:37] Abstract 11. Legato improves action-chunked Vision Language Action models by using training-time continuation methods that ensure smooth trajectories and reduce multimodal switching during real-time execution.  					AI-generated summary 				 Action chunking enables Vision Language Action (VLA) models to run in r...
[18.02.2026 08:37] ********************************************************************************
[18.02.2026 08:37] Abstract 12. ViewRope, a geometry-aware encoding method, enhances long-term consistency in predictive world models by injecting camera-ray directions into video transformer attention layers, addressing spatial persistence issues through relative ray geometry parameterization.  					AI-generated summary 				 Pred...
[18.02.2026 08:37] Read previous papers.
[18.02.2026 08:37] Generating reviews via LLM API.
[18.02.2026 08:37] Using data from previous issue: {"categories": ["#inference", "#agents", "#training", "#alignment", "#optimization", "#architecture", "#plp", "#open_source", "#reasoning", "#rl"], "emoji": "ü§ñ", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è —á–µ—Ä–µ–∑ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ", "desc": "GLM-5 –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –º–æ–¥–µ–ª—å
[18.02.2026 08:37] Using data from previous issue: {"categories": [], "emoji": "ü§ñ", "ru": {"title": "–ê–≥–µ–Ω—Ç—ã –±–µ–∑ —Å–æ—Ü–∏—É–º–∞: –ø–æ—á–µ–º—É –ò–ò-–æ–±—â–µ—Å—Ç–≤–∞ –Ω–µ —Å—Ö–æ–¥—è—Ç—Å—è –∫ –∫–æ–Ω—Å–µ–Ω—Å—É—Å—É", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∏–∑—É—á–∞–µ—Ç –¥–∏–Ω–∞–º–∏–∫—É –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ —Å–µ—Ç–µ–≤—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è—Ö, –∞–Ω–∞–ª–∏–∑–∏—Ä—É—è, –ø–æ–¥–≤–µ—Ä–≥–∞—é—Ç—Å—è –ª–∏ –æ–Ω–∏ —Å–æ—Ü–∏–∞–ª—å–Ω–æ–π –∫–æ–Ω–≤–µ—Ä–≥–µ–Ω—Ü–∏–∏ –ø–æ–¥–æ–±–Ω–æ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º —Å–∏—Å—Ç–µ–º
[18.02.2026 08:37] Using data from previous issue: {"categories": ["#benchmark", "#science", "#agents", "#long_context", "#dataset"], "emoji": "üß™", "ru": {"title": "–û—Ü–µ–Ω–∫–∞ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –∞–≤—Ç–æ–Ω–æ–º–Ω—ã—Ö AI –∞–≥–µ–Ω—Ç–æ–≤ –≤ –Ω–∞—É—á–Ω—ã—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è—Ö", "desc": "ResearchGym –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –±–µ–Ω—á–º–∞—Ä–∫ –∏ –æ–∫—Ä—É–∂–µ–Ω–∏–µ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ AI –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ —Å–∫–≤–æ–∑–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö –Ω–∞—É—á–Ω—ã—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞
[18.02.2026 08:37] Querying the API.
[18.02.2026 08:37] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

COMPOT is a training-free compression framework for Transformer models that uses sparse dictionary learning with orthogonal dictionaries and closed-form updates to achieve better quality-compression trade-offs than traditional low-rank methods.  					AI-generated summary 				 Post-training compression of Transformer models commonly relies on truncated singular value decomposition (SVD). However, enforcing a single shared subspace can degrade accuracy even at moderate compression. Sparse dictionary learning provides a more flexible union-of-subspaces representation, but existing approaches often suffer from iterative dictionary and coefficient updates. We propose COMPOT (Calibration-Optimized Matrix Procrustes Orthogonalization for Transformers), a training-free compression framework that uses a small calibration dataset to estimate a sparse weight factorization. COMPOT employs orthogonal dictionaries that enable closed-form Procrustes updates for the dictionary and analytical single-step sparse coding for the coefficients, eliminating iterative optimization. To handle heterogeneous layer sensitivity under a global compression budget, COMPOT further introduces a one-shot dynamic allocation strategy that adaptively redistributes layer-wise compression rates. Extensive experiments across diverse architectures and tasks show that COMPOT consistently delivers a superior quality-compression trade-off over strong low-rank and sparse baselines, while remaining fully compatible with post-training quantization for extreme compression. Code is available https://github.com/mts-ai/COMPOT{here}.
[18.02.2026 08:37] Response: ```json
{
  "desc": "COMPOT ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Å–∂–∞—Ç–∏—è –º–æ–¥–µ–ª–µ–π Transformer –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–µ —Å–ª–æ–≤–∞—Ä–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –æ—Ä—Ç–æ–≥–æ–Ω–∞–ª—å–Ω—ã–º–∏ —Å–ª–æ–≤–∞—Ä—è–º–∏ –≤–º–µ—Å—Ç–æ —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ —Å–∏–Ω–≥—É–ª—è—Ä–Ω–æ–≥–æ —Ä–∞–∑–ª–æ–∂–µ–Ω–∏—è. –ú–µ—Ç–æ–¥ –æ–ø–∏—Ä–∞–µ—Ç—Å—è –Ω–∞ –Ω–µ–±–æ–ª—å—à–æ–π –∫–∞–ª–∏–±—Ä–æ–≤–æ—á–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∏ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø–æ–ª—É—á–∏—Ç—å –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Å–ª–æ–≤–∞—Ä—è —á–µ—Ä–µ–∑ –ø—Ä–æ—Ü–µ–¥—É—Ä—É –ü—Ä–æ–∫—Ä—É—Å—Ç–∞ –∏ –æ–¥–Ω–æ—à–∞–≥–æ–≤–æ–µ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–≤ –±–µ–∑ –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏. –î–ª—è —É—á–µ—Ç–∞ —Ä–∞–∑–ª–∏—á–Ω–æ–π —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å–ª–æ–µ–≤ —Å–µ—Ç–∏, —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–∏–Ω–∞–º–∏—á–µ—Å–∫—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –±—é–¥–∂–µ—Ç–∞ —Å–∂–∞—Ç–∏—è –ø–æ —Å–ª–æ—è–º. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ COMPOT –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –ª—É—á—à–µ–≥–æ –±–∞–ª–∞–Ω—Å–∞ –º–µ–∂–¥—É –∫–∞—á–µ—Å—Ç–≤–æ–º –∏ —Å—Ç–µ–ø–µ–Ω—å—é —Å–∂–∞—Ç–∏—è –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –º–µ—Ç–æ–¥–∞–º–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∏–∑–∫–æ—Ä–∞–Ω–≥–æ–≤—ã—Ö —Ä–∞–∑–ª–æ–∂–µ–Ω–∏–π –∏ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç–∏, –æ—Å—Ç–∞–≤–∞—è—Å—å —Å–æ–≤–º–µ—Å—Ç–∏–º—ã–º —Å –º–µ—Ç–æ–¥–∞–º–∏ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏.",
  "emoji": "üóúÔ∏è",
  "title": "–û—Ä—Ç–æ–≥–æ–Ω–∞–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞—Ä–∏ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Å–∂–∞—Ç–∏—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è"
}
```
[18.02.2026 08:37] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"COMPOT is a training-free compression framework for Transformer models that uses sparse dictionary learning with orthogonal dictionaries and closed-form updates to achieve better quality-compression trade-offs than traditional low-rank methods.  					AI-generated summary 				 Post-training compression of Transformer models commonly relies on truncated singular value decomposition (SVD). However, enforcing a single shared subspace can degrade accuracy even at moderate compression. Sparse dictionary learning provides a more flexible union-of-subspaces representation, but existing approaches often suffer from iterative dictionary and coefficient updates. We propose COMPOT (Calibration-Optimized Matrix Procrustes Orthogonalization for Transformers), a training-free compression framework that uses a small calibration dataset to estimate a sparse weight factorization. COMPOT employs orthogonal dictionaries that enable closed-form Procrustes updates for the dictionary and analytical single-step sparse coding for the coefficients, eliminating iterative optimization. To handle heterogeneous layer sensitivity under a global compression budget, COMPOT further introduces a one-shot dynamic allocation strategy that adaptively redistributes layer-wise compression rates. Extensive experiments across diverse architectures and tasks show that COMPOT consistently delivers a superior quality-compression trade-off over strong low-rank and sparse baselines, while remaining fully compatible with post-training quantization for extreme compression. Code is available https://github.com/mts-ai/COMPOT{here}."

[18.02.2026 08:37] Response: ```python
["INFERENCE", "ARCHITECTURE", "TRAINING"]
```

**Justification:**

- **INFERENCE**: The paper focuses on post-training compression of Transformer models, which is a key inference optimization technique for model deployment.

- **ARCHITECTURE**: The paper proposes COMPOT, a novel compression framework with specific technical components (orthogonal dictionaries, Procrustes updates, sparse coding) that represent methodological innovations in how models are structured and optimized.

- **TRAINING**: While described as "training-free," the method involves calibration and optimization procedures (dictionary learning, coefficient updates, dynamic allocation strategy) that are part of the model preparation/training pipeline.
[18.02.2026 08:37] Error. Failed to parse JSON from LLM. ["INFERENCE", "ARCHITECTURE", "TRAINING"]


**Justification:**

- **INFERENCE**: The paper focuses on post-training compression of Transformer models, which is a key inference optimization technique for model deployment.

- **ARCHITECTURE**: The paper proposes COMPOT, a novel compression framework with specific technical components (orthogonal dictionaries, Procrustes updates, sparse coding) that represent methodological innovations in how models are structured and optimized.

- **TRAINING**: While described as "training-free," the method involves calibration and optimization procedures (dictionary learning, coefficient updates, dynamic allocation strategy) that are part of the model preparation/training pipeline.
[18.02.2026 08:37] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"COMPOT is a training-free compression framework for Transformer models that uses sparse dictionary learning with orthogonal dictionaries and closed-form updates to achieve better quality-compression trade-offs than traditional low-rank methods.  					AI-generated summary 				 Post-training compression of Transformer models commonly relies on truncated singular value decomposition (SVD). However, enforcing a single shared subspace can degrade accuracy even at moderate compression. Sparse dictionary learning provides a more flexible union-of-subspaces representation, but existing approaches often suffer from iterative dictionary and coefficient updates. We propose COMPOT (Calibration-Optimized Matrix Procrustes Orthogonalization for Transformers), a training-free compression framework that uses a small calibration dataset to estimate a sparse weight factorization. COMPOT employs orthogonal dictionaries that enable closed-form Procrustes updates for the dictionary and analytical single-step sparse coding for the coefficients, eliminating iterative optimization. To handle heterogeneous layer sensitivity under a global compression budget, COMPOT further introduces a one-shot dynamic allocation strategy that adaptively redistributes layer-wise compression rates. Extensive experiments across diverse architectures and tasks show that COMPOT consistently delivers a superior quality-compression trade-off over strong low-rank and sparse baselines, while remaining fully compatible with post-training quantization for extreme compression. Code is available https://github.com/mts-ai/COMPOT{here}."

[18.02.2026 08:37] Response: ```python
['OPTIMIZATION', 'OPEN_SOURCE']
```

**Justification:**

- **OPTIMIZATION**: The paper presents COMPOT, a framework focused on training optimization and compression efficiency. It discusses optimization methods including closed-form Procrustes updates, sparse coding, and dynamic allocation strategies to improve the quality-compression trade-off during model compression.

- **OPEN_SOURCE**: The paper explicitly states "Code is available https://github.com/mts-ai/COMPOT" indicating the authors are releasing their code/framework publicly.
[18.02.2026 08:37] Error. Failed to parse JSON from LLM. ["OPTIMIZATION", "OPEN_SOURCE"]


**Justification:**

- **OPTIMIZATION**: The paper presents COMPOT, a framework focused on training optimization and compression efficiency. It discusses optimization methods including closed-form Procrustes updates, sparse coding, and dynamic allocation strategies to improve the quality-compression trade-off during model compression.

- **OPEN_SOURCE**: The paper explicitly states "Code is available https://github.com/mts-ai/COMPOT" indicating the authors are releasing their code/framework publicly.
[18.02.2026 08:37] Response: ParsedChatCompletionMessage[~ResponseFormatT](content='{"desc":"COMPOT is a novel framework designed for compressing Transformer models without the need for additional training. It utilizes sparse dictionary learning with orthogonal dictionaries, allowing for efficient updates that improve the balance between model quality and compression. Unlike traditional methods that rely on singular value decomposition, COMPOT avoids iterative optimization by using closed-form updates for both the dictionary and coefficients. Additionally, it introduces a dynamic allocation strategy to adaptively manage compression across different layers, ensuring optimal performance across various tasks and architectures.","title":"Effortless Compression for Transformers with COMPOT"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='COMPOT is a novel framework designed for compressing Transformer models without the need for additional training. It utilizes sparse dictionary learning with orthogonal dictionaries, allowing for efficient updates that improve the balance between model quality and compression. Unlike traditional methods that rely on singular value decomposition, COMPOT avoids iterative optimization by using closed-form updates for both the dictionary and coefficients. Additionally, it introduces a dynamic allocation strategy to adaptively manage compression across different layers, ensuring optimal performance across various tasks and architectures.', title='Effortless Compression for Transformers with COMPOT'))
[18.02.2026 08:37] Response: ParsedChatCompletionMessage[~ResponseFormatT](content='{"desc":"COMPOTÊòØ‰∏ÄÁßçÊó†ËÆ≠ÁªÉÁöÑÂéãÁº©Ê°ÜÊû∂Ôºå‰∏ì‰∏∫TransformerÊ®°ÂûãËÆæËÆ°„ÄÇÂÆÉÂà©Áî®Á®ÄÁñèÂ≠óÂÖ∏Â≠¶‰π†ÂíåÊ≠£‰∫§Â≠óÂÖ∏ÔºåÈÄöËøáÈó≠ÂºèÊõ¥Êñ∞ÂÆûÁé∞Êõ¥Â•ΩÁöÑË¥®Èáè‰∏éÂéãÁº©‰πãÈó¥ÁöÑÂπ≥Ë°°„ÄÇ‰∏é‰º†ÁªüÁöÑ‰ΩéÁß©ÊñπÊ≥ïÁõ∏ÊØîÔºåCOMPOTËÉΩÂ§üÂú®‰∏çÊçüÂ§±ÂáÜÁ°ÆÂ∫¶ÁöÑÊÉÖÂÜµ‰∏ãÔºåÁÅµÊ¥ªÂú∞Â§ÑÁêÜ‰∏çÂêåÂ±ÇÁöÑÊïèÊÑüÊÄß„ÄÇÈÄöËøáÂä®ÊÄÅÂàÜÈÖçÂéãÁº©ÁéáÔºåCOMPOTÂú®Â§öÁßçÊû∂ÊûÑÂíå‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÊèê‰æõ‰∫Ü‰ºò‰∫éÁé∞ÊúâÂü∫Á∫øÁöÑÊñπÊ≥ï„ÄÇ","title":"COMPOTÔºöÊó†ËÆ≠ÁªÉÁöÑTransformerÂéãÁº©Êñ∞ÊñπÊ≥ï"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='COMPOTÊòØ‰∏ÄÁßçÊó†ËÆ≠ÁªÉÁöÑÂéãÁº©Ê°ÜÊû∂Ôºå‰∏ì‰∏∫TransformerÊ®°ÂûãËÆæËÆ°„ÄÇÂÆÉÂà©Áî®Á®ÄÁñèÂ≠óÂÖ∏Â≠¶‰π†ÂíåÊ≠£‰∫§Â≠óÂÖ∏ÔºåÈÄöËøáÈó≠ÂºèÊõ¥Êñ∞ÂÆûÁé∞Êõ¥Â•ΩÁöÑË¥®Èáè‰∏éÂéãÁº©‰πãÈó¥ÁöÑÂπ≥Ë°°„ÄÇ‰∏é‰º†ÁªüÁöÑ‰ΩéÁß©ÊñπÊ≥ïÁõ∏ÊØîÔºåCOMPOTËÉΩÂ§üÂú®‰∏çÊçüÂ§±ÂáÜÁ°ÆÂ∫¶ÁöÑÊÉÖÂÜµ‰∏ãÔºåÁÅµÊ¥ªÂú∞Â§ÑÁêÜ‰∏çÂêåÂ±ÇÁöÑÊïèÊÑüÊÄß„ÄÇÈÄöËøáÂä®ÊÄÅÂàÜÈÖçÂéãÁº©ÁéáÔºåCOMPOTÂú®Â§öÁßçÊû∂ÊûÑÂíå‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÊèê‰æõ‰∫Ü‰ºò‰∫éÁé∞ÊúâÂü∫Á∫øÁöÑÊñπÊ≥ï„ÄÇ', title='COMPOTÔºöÊó†ËÆ≠ÁªÉÁöÑTransformerÂéãÁº©Êñ∞ÊñπÊ≥ï'))
[18.02.2026 08:37] Using data from previous issue: {"categories": ["#interpretability"], "emoji": "üèõÔ∏è", "ru": {"title": "–û—Ç –ü–ª–∞—Ç–æ–Ω–∞ –∫ –ê—Ä–∏—Å—Ç–æ—Ç–µ–ª—é: –ª–æ–∫–∞–ª—å–Ω–æ–µ —Å–æ—Å–µ–¥—Å—Ç–≤–æ –≤–º–µ—Å—Ç–æ –≥–ª–æ–±–∞–ª—å–Ω–æ–π –∫–æ–Ω–≤–µ—Ä–≥–µ–Ω—Ü–∏–∏", "desc": "–í —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –≥–∏–ø–æ—Ç–µ–∑–∞ –æ —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π –∫ –µ–¥–∏–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–æ–π –º–æ–¥–µ–ª–∏ —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏. –ê–≤—Ç–æ—Ä—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —Å—É—â–µ
[18.02.2026 08:37] Using data from previous issue: {"categories": ["#dataset", "#training", "#healthcare", "#rlhf", "#alignment", "#reasoning", "#science", "#small_models"], "emoji": "üè•", "ru": {"title": "–ö–ª–∏–Ω–∏—á–µ—Å–∫–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ LLM —á–µ—Ä–µ–∑ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–µ –≤—Ä–∞—á–∞–º–∏ –ø—Ä–∏–º–µ—Ä—ã –∏ –¥–∏—Å—Ç–∏–ª–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–∏–Ω—Ü–∏–ø—ã", "desc": "–†–∞–±–æ—Ç–∞ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è 
[18.02.2026 08:37] Using data from previous issue: {"categories": ["#open_source", "#training", "#architecture", "#reasoning", "#multimodal"], "emoji": "üîÑ", "ru": {"title": "–ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è —á–µ—Ä–µ–∑ –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–µ —É—Ç–æ—á–Ω–µ–Ω–∏–µ", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ Reason-Reflect-Refine (R3), –∫–æ—Ç–æ—Ä–∞—è —Ä–µ—à–∞–µ—Ç –∫–æ–Ω—Ñ–ª–∏–∫—Ç –º–µ–∂–¥—É —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—è–º–∏
[18.02.2026 08:37] Using data from previous issue: {"categories": ["#training", "#optimization", "#reasoning", "#rl"], "emoji": "üéØ", "ru": {"title": "–ù–∞—Ö–æ–¥–∏ –∏ –∏—Å–∫–ª—é—á–∞–π: –∫–∞–∫ –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–¥–∫–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤ —Å—Ç–∞–±–∏–ª–∏–∑–∏—Ä—É–µ—Ç –æ–±—É—á–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –≤—ã—è–≤–∏–ª–æ, —á—Ç–æ –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è –ø—Ä–∏ fine-tuning –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –∏—Å–ø–æ–ª—å
[18.02.2026 08:37] Using data from previous issue: {"categories": ["#training", "#optimization", "#architecture"], "emoji": "üéØ", "ru": {"title": "–°–ª—É—á–∞–π–Ω–æ–µ –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π ‚Äî –ø—Ä–æ—Å—Ç–æ–π –ø—É—Ç—å –∫ –ª—É—á—à–µ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ LLM", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ —Å–ª—É—á–∞–π–Ω–æ–µ –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π –ø–∞—Ä–∞–º–µ—Ç—Ä–æ
[18.02.2026 08:37] Using data from previous issue: {"categories": ["#benchmark", "#agents", "#multimodal", "#cv"], "emoji": "üîç", "ru": {"title": "–í—ã—è–≤–ª–µ–Ω–∏–µ —Å–∫—Ä—ã—Ç—ã—Ö –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –≤ –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∏–∑—É—á–∞–µ—Ç –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º—ã–µ –∑–∞–¥–∞—á–∏ –≤—ã–±–æ—Ä–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫
[18.02.2026 08:37] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#training", "#optimization", "#science"], "emoji": "üìà", "ru": {"title": "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –≥—Ä–∞–Ω–∏—Ü –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–π –±—é–¥–∂–µ—Ç", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ –æ—Ü–µ–Ω–∫–∏ –≥—Ä–∞–Ω–∏—á–Ω—ã—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∫–≤–∞–Ω—Ç–∏–ª—å
[18.02.2026 08:37] Using data from previous issue: {"categories": ["#robotics", "#cv", "#training", "#multimodal"], "emoji": "ü§ñ", "ru": {"title": "–ì–ª–∞–¥–∫–∏–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ —á–µ—Ä–µ–∑ –æ–±—É—á–µ–Ω–∏–µ —Å –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ–º", "desc": "Legato ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π Vision Language Action, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—é—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–µ–π—Å—Ç–≤–∏–π –¥–ª—è —Ä–æ–±–æ—Ç–æ–≤. –û—Å–Ω–æ–≤–Ω–∞
[18.02.2026 08:37] Using data from previous issue: {"categories": ["#video", "#benchmark", "#architecture", "#3d"], "emoji": "üé¨", "ru": {"title": "–ì–µ–æ–º–µ—Ç—Ä–∏—è –∫–∞–º–µ—Ä—ã –¥–ª—è —É—Å—Ç–æ–π—á–∏–≤–æ–≥–æ –≤–∏–¥–µ–Ω–∏—è –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –∏ –≤—Ä–µ–º–µ–Ω–∏", "desc": "ViewRope ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥ –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏ –æ—Å–≤–µ–¥–æ–º–ª—ë–Ω–Ω–æ–≥–æ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—É—é —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –≤ –ø—Ä–µ–¥–∏–∫—Ç–∏–≤–Ω—ã—Ö –º–æ–¥
[18.02.2026 08:37] Renaming data file.
[18.02.2026 08:37] Renaming previous data. hf_papers.json to ./d/2026-02-18.json
[18.02.2026 08:37] Saving new data file.
[18.02.2026 08:37] Generating page.
[18.02.2026 08:37] Renaming previous page.
[18.02.2026 08:37] Renaming previous data. index.html to ./d/2026-02-18.html
[18.02.2026 08:37] Writing result.
[18.02.2026 08:37] Renaming log file.
[18.02.2026 08:37] Renaming previous data. log.txt to ./logs/2026-02-18_last_log.txt
