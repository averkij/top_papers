[05.09.2025 07:11] Read previous papers.
[05.09.2025 07:11] Generating top page (month).
[05.09.2025 07:11] Writing top page (month).
[05.09.2025 08:15] Read previous papers.
[05.09.2025 08:15] Get feed.
[05.09.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2509.04338
[05.09.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2509.04292
[05.09.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2509.04419
[05.09.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2509.01396
[05.09.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2509.04394
[05.09.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2509.04011
[05.09.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2509.04406
[05.09.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2509.04434
[05.09.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2508.18733
[05.09.2025 08:15] Extract page data from URL. URL: https://huggingface.co/papers/2509.04442
[05.09.2025 08:15] Extract page data from URL. URL: https://huggingface.co/papers/2509.03867
[05.09.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2509.03888
[05.09.2025 08:15] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[05.09.2025 08:15] No deleted papers detected.
[05.09.2025 08:15] Downloading and parsing papers (pdf, html). Total: 12.
[05.09.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2509.04338.
[05.09.2025 08:15] Extra JSON file exists (./assets/json/2509.04338.json), skip PDF parsing.
[05.09.2025 08:15] Paper image links file exists (./assets/img_data/2509.04338.json), skip HTML parsing.
[05.09.2025 08:15] Success.
[05.09.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2509.04292.
[05.09.2025 08:15] Extra JSON file exists (./assets/json/2509.04292.json), skip PDF parsing.
[05.09.2025 08:15] Paper image links file exists (./assets/img_data/2509.04292.json), skip HTML parsing.
[05.09.2025 08:15] Success.
[05.09.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2509.04419.
[05.09.2025 08:15] Extra JSON file exists (./assets/json/2509.04419.json), skip PDF parsing.
[05.09.2025 08:15] Paper image links file exists (./assets/img_data/2509.04419.json), skip HTML parsing.
[05.09.2025 08:15] Success.
[05.09.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2509.01396.
[05.09.2025 08:15] Extra JSON file exists (./assets/json/2509.01396.json), skip PDF parsing.
[05.09.2025 08:15] Paper image links file exists (./assets/img_data/2509.01396.json), skip HTML parsing.
[05.09.2025 08:15] Success.
[05.09.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2509.04394.
[05.09.2025 08:15] Extra JSON file exists (./assets/json/2509.04394.json), skip PDF parsing.
[05.09.2025 08:15] Paper image links file exists (./assets/img_data/2509.04394.json), skip HTML parsing.
[05.09.2025 08:15] Success.
[05.09.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2509.04011.
[05.09.2025 08:15] Extra JSON file exists (./assets/json/2509.04011.json), skip PDF parsing.
[05.09.2025 08:15] Paper image links file exists (./assets/img_data/2509.04011.json), skip HTML parsing.
[05.09.2025 08:15] Success.
[05.09.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2509.04406.
[05.09.2025 08:15] Extra JSON file exists (./assets/json/2509.04406.json), skip PDF parsing.
[05.09.2025 08:15] Paper image links file exists (./assets/img_data/2509.04406.json), skip HTML parsing.
[05.09.2025 08:15] Success.
[05.09.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2509.04434.
[05.09.2025 08:15] Extra JSON file exists (./assets/json/2509.04434.json), skip PDF parsing.
[05.09.2025 08:15] Paper image links file exists (./assets/img_data/2509.04434.json), skip HTML parsing.
[05.09.2025 08:15] Success.
[05.09.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2508.18733.
[05.09.2025 08:15] Extra JSON file exists (./assets/json/2508.18733.json), skip PDF parsing.
[05.09.2025 08:15] Paper image links file exists (./assets/img_data/2508.18733.json), skip HTML parsing.
[05.09.2025 08:15] Success.
[05.09.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2509.04442.
[05.09.2025 08:15] Downloading paper 2509.04442 from http://arxiv.org/pdf/2509.04442v1...
[05.09.2025 08:15] Extracting affiliations from text.
[05.09.2025 08:15] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 4 ] . [ 1 2 4 4 4 0 . 9 0 5 2 : r Delta Activations: Representation for Finetuned Large Language Models Zhiqiu Xu1 Amish Sethi1 Mayur Naik1 Ser-Nam Lim2 1University of Pennsylvania 2University of Central Florida "
[05.09.2025 08:15] Response: ```python
["University of Pennsylvania", "University of Central Florida"]
```
[05.09.2025 08:15] Deleting PDF ./assets/pdf/2509.04442.pdf.
[05.09.2025 08:15] Success.
[05.09.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2509.03867.
[05.09.2025 08:15] Downloading paper 2509.03867 from http://arxiv.org/pdf/2509.03867v1...
[05.09.2025 08:15] Extracting affiliations from text.
[05.09.2025 08:15] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Drivel-ology: Challenging LLMs with Interpreting Nonsense with Depth Yang Wang1, Chenghao Xiao2, Chia-Yi Hsiao2, Zi Yan Chang3, Chi-Li Chen3, Tyler Loakman3, Chenghua Lin1, 1The University of Manchester, 2Durham University, 3The University of Sheffield 5 2 0 2 4 ] . [ 1 7 6 8 3 0 . 9 0 5 2 : r a "
[05.09.2025 08:15] Response: ```python
["The University of Manchester", "Durham University", "The University of Sheffield"]
```
[05.09.2025 08:15] Deleting PDF ./assets/pdf/2509.03867.pdf.
[05.09.2025 08:15] Success.
[05.09.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2509.03888.
[05.09.2025 08:15] Extra JSON file exists (./assets/json/2509.03888.json), skip PDF parsing.
[05.09.2025 08:15] Paper image links file exists (./assets/img_data/2509.03888.json), skip HTML parsing.
[05.09.2025 08:15] Success.
[05.09.2025 08:15] Enriching papers with extra data.
[05.09.2025 08:15] ********************************************************************************
[05.09.2025 08:15] Abstract 0. FE2E, a framework using a Diffusion Transformer for dense geometry prediction, outperforms generative models in zero-shot monocular depth and normal estimation with improved performance and efficiency.  					AI-generated summary 				 Leveraging visual priors from pre-trained text-to-image (T2I) gene...
[05.09.2025 08:15] ********************************************************************************
[05.09.2025 08:15] Abstract 1. Inverse IFEval evaluates Large Language Models' ability to override training biases and follow unconventional instructions, highlighting the need for adaptability in diverse contexts.  					AI-generated summary 				 Large Language Models (LLMs) achieve strong performance on diverse tasks but often e...
[05.09.2025 08:15] ********************************************************************************
[05.09.2025 08:15] Abstract 2. A unified policy gradient estimator and Hybrid Post-Training algorithm effectively combine online and offline data for post-training language models, improving performance across various benchmarks.  					AI-generated summary 				 Two major sources of training data exist for post-training modern lan...
[05.09.2025 08:15] ********************************************************************************
[05.09.2025 08:15] Abstract 3. DeepResearch Arena, a benchmark using academic seminar transcripts, provides high-quality research tasks to evaluate deep research agents across multiple disciplines.  					AI-generated summary 				 Deep research agents have attracted growing attention for their potential to orchestrate multi-stage ...
[05.09.2025 08:15] ********************************************************************************
[05.09.2025 08:15] Abstract 4. A novel generative paradigm, Transition Models (TiM), addresses the trade-off between computational cost and output quality in generative modeling by using a continuous-time dynamics equation.  					AI-generated summary 				 A fundamental dilemma in generative modeling persists: iterative diffusion ...
[05.09.2025 08:15] ********************************************************************************
[05.09.2025 08:15] Abstract 5. NER Retriever uses internal representations from large language models to perform zero-shot named entity retrieval by embedding entity mentions and type descriptions into a shared semantic space, outperforming lexical and dense sentence-level retrieval methods.  					AI-generated summary 				 We pre...
[05.09.2025 08:15] ********************************************************************************
[05.09.2025 08:15] Abstract 6. A novel framework, MDT-dist, accelerates 3D flow generation by distilling pretrained models to learn Marginal-Data Transport through Velocity Matching and Velocity Distillation, reducing sampling steps and improving speed and fidelity.  					AI-generated summary 				 Flow-based 3D generation models ...
[05.09.2025 08:15] ********************************************************************************
[05.09.2025 08:15] Abstract 7. Durian uses dual reference networks and a diffusion model to generate high-fidelity portrait animations with attribute transfer from a reference image to a target portrait in a zero-shot manner.  					AI-generated summary 				 We present Durian, the first method for generating portrait animation vid...
[05.09.2025 08:15] ********************************************************************************
[05.09.2025 08:15] Abstract 8. Drawing2CAD is a framework that converts 2D vector drawings into parametric CAD models using a sequence-to-sequence learning approach with a dual-decoder transformer architecture and a soft target distribution loss function.  					AI-generated summary 				 Computer-Aided Design (CAD) generative mode...
[05.09.2025 08:15] ********************************************************************************
[05.09.2025 08:15] Abstract 9. Delta Activations represent fine-tuned models as vector embeddings based on internal activation shifts, enabling effective clustering and model reuse.  					AI-generated summary 				 The success of powerful open source Large Language Models (LLMs) has enabled the community to create a vast collectio...
[05.09.2025 08:15] ********************************************************************************
[05.09.2025 08:15] Abstract 10. LLMs struggle with understanding the nuanced, context-dependent meanings of Drivelological text, which appears nonsensical but contains deeper semantic layers.  					AI-generated summary 				 We introduce Drivelology, a unique linguistic phenomenon characterised as "nonsense with depth", utterances ...
[05.09.2025 08:15] ********************************************************************************
[05.09.2025 08:15] Abstract 11. Probing-based approaches for detecting harmful instructions in LLMs are found to rely on superficial patterns rather than semantic understanding, indicating a need for redesigning models and evaluation methods.  					AI-generated summary 				 Large Language Models (LLMs) can comply with harmful inst...
[05.09.2025 08:15] Read previous papers.
[05.09.2025 08:15] Generating reviews via LLM API.
[05.09.2025 08:15] Using data from previous issue: {"categories": ["#inference", "#cv", "#diffusion", "#optimization", "#training"], "emoji": "ğŸ”¬", "ru": {"title": "FE2E: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ¸ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Diffusion Transformer", "desc": "FE2E - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Diffusion Transformer Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸. Ğ
[05.09.2025 08:15] Using data from previous issue: {"categories": ["#multilingual", "#hallucinations", "#dataset", "#benchmark", "#alignment"], "emoji": "ğŸ§ ", "ru": {"title": "ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¸Ğ½ĞµÑ€Ñ†Ğ¸Ğ¸ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Inverse IFEval Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¿Ñ€
[05.09.2025 08:15] Using data from previous issue: {"categories": ["#benchmark", "#reasoning", "#optimization", "#training", "#rl"], "emoji": "ğŸ§ ", "ru": {"title": "ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½ Ğ¸ Ğ¾Ñ„Ğ»Ğ°Ğ¹Ğ½ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹", "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¾Ñ†ĞµĞ½Ñ‰Ğ¸Ğº Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ° Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ¸ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ÑÑ‚-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²
[05.09.2025 08:15] Using data from previous issue: {"categories": ["#leakage", "#science", "#agents", "#benchmark", "#survey", "#dataset"], "emoji": "ğŸ§ ", "ru": {"title": "ĞĞºĞ°Ğ´ĞµĞ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞµĞ¼Ğ¸Ğ½Ğ°Ñ€Ñ‹ ĞºĞ°Ğº Ğ¾ÑĞ½Ğ¾Ğ²Ğ° Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ˜Ğ˜-Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹", "desc": "DeepResearch Arena - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ñ‚Ñ€Ğ°Ğ½Ñ
[05.09.2025 08:15] Using data from previous issue: {"categories": ["#small_models", "#optimization", "#generative_modeling", "#diffusion", "#training"], "emoji": "ğŸ”„", "ru": {"title": "TiM: Ğ³Ğ¸Ğ±ĞºĞ¾Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½ĞµĞ¼", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ - Transition Models (TiM). TiM
[05.09.2025 08:15] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#transfer_learning", "#multimodal", "#open_source"], "emoji": "ğŸ”", "ru": {"title": "Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±ĞµĞ· ÑÑ…ĞµĞ¼ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹", "desc": "NER Retriever - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ¼ĞµĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±ĞµĞ· Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚
[05.09.2025 08:15] Using data from previous issue: {"categories": ["#3d", "#inference", "#diffusion", "#optimization", "#training"], "emoji": "ğŸš€", "ru": {"title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ 3D-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²", "desc": "MDT-dist - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ². ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ
[05.09.2025 08:15] Using data from previous issue: {"categories": ["#cv", "#diffusion", "#multimodal", "#video"], "emoji": "ğŸ", "ru": {"title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ€Ñ‚Ñ€ĞµÑ‚Ğ¾Ğ²: Ğ¿ĞµÑ€ĞµĞ½Ğ¾Ñ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ¾Ğ² Ğ±ĞµĞ· Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Durian Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ°Ğ½Ğ¸Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ñ€Ñ‚Ñ€ĞµÑ‚Ğ¾Ğ² Ñ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ¾Ğ¼ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ¾Ğ² Ñ Ñ€ĞµÑ„ĞµÑ€ĞµĞ½ÑĞ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶
[05.09.2025 08:15] Using data from previous issue: {"categories": ["#optimization", "#training", "#architecture", "#dataset", "#open_source"], "emoji": "ğŸ“", "ru": {"title": "ĞÑ‚ Ñ‡ĞµÑ€Ñ‚ĞµĞ¶Ğ° Ğº CAD: Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ 3D-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸Ğ· 2D-Ñ‡ĞµÑ€Ñ‚ĞµĞ¶ĞµĞ¹", "desc": "Drawing2CAD - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ 2D Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ğµ Ñ‡ĞµÑ€Ñ‚ĞµĞ¶Ğ¸ Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ CAD-Ğ¼Ğ¾Ğ´ĞµĞ»
[05.09.2025 08:15] Querying the API.
[05.09.2025 08:15] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Delta Activations represent fine-tuned models as vector embeddings based on internal activation shifts, enabling effective clustering and model reuse.  					AI-generated summary 				 The success of powerful open source Large Language Models (LLMs) has enabled the community to create a vast collection of post-trained models adapted to specific tasks and domains. However, navigating and understanding these models remains challenging due to inconsistent metadata and unstructured repositories. We introduce Delta Activations, a method to represent finetuned models as vector embeddings by measuring shifts in their internal activations relative to a base model. This representation allows for effective clustering by domain and task, revealing structure in the model landscape. Delta Activations also demonstrate desirable properties: it is robust across finetuning settings and exhibits an additive property when finetuning datasets are mixed. In addition, we show that Delta Activations can embed tasks via few-shot finetuning, and further explore its use for model selection and merging. We hope Delta Activations can facilitate the practice of reusing publicly available models. Code is available at https://github.com/OscarXZQ/delta_activations.
[05.09.2025 08:15] Response: {
  "desc": "ĞœĞµÑ‚Ğ¾Ğ´ Delta Activations Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ²Ğ¸Ğ´Ğµ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ², Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸ÑÑ… Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ñ… Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¹ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¸Ğ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ğ¼ Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼, Ğ²Ñ‹ÑĞ²Ğ»ÑÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ² Ğ»Ğ°Ğ½Ğ´ÑˆĞ°Ñ„Ñ‚Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Delta Activations Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğº Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ°Ğ¼ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ğ»Ğ°Ğ´Ğ°ĞµÑ‚ Ğ°Ğ´Ğ´Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¿Ñ€Ğ¸ ÑĞ¼ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ´Ğ»Ñ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ‡ĞµÑ€ĞµĞ· few-shot Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ‚ÑŒÑÑ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ¸ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.",

  "emoji": "ğŸ§ ",

  "title": "Delta Activations: ĞĞ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ñ Ğ² Ğ¼Ğ¸Ñ€Ğµ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹"
}
[05.09.2025 08:15] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Delta Activations represent fine-tuned models as vector embeddings based on internal activation shifts, enabling effective clustering and model reuse.  					AI-generated summary 				 The success of powerful open source Large Language Models (LLMs) has enabled the community to create a vast collection of post-trained models adapted to specific tasks and domains. However, navigating and understanding these models remains challenging due to inconsistent metadata and unstructured repositories. We introduce Delta Activations, a method to represent finetuned models as vector embeddings by measuring shifts in their internal activations relative to a base model. This representation allows for effective clustering by domain and task, revealing structure in the model landscape. Delta Activations also demonstrate desirable properties: it is robust across finetuning settings and exhibits an additive property when finetuning datasets are mixed. In addition, we show that Delta Activations can embed tasks via few-shot finetuning, and further explore its use for model selection and merging. We hope Delta Activations can facilitate the practice of reusing publicly available models. Code is available at https://github.com/OscarXZQ/delta_activations."

[05.09.2025 08:15] Response: ```python
['DATASET', 'DATA', 'TRAINING', 'ARCHITECTURE']
```
[05.09.2025 08:15] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Delta Activations represent fine-tuned models as vector embeddings based on internal activation shifts, enabling effective clustering and model reuse.  					AI-generated summary 				 The success of powerful open source Large Language Models (LLMs) has enabled the community to create a vast collection of post-trained models adapted to specific tasks and domains. However, navigating and understanding these models remains challenging due to inconsistent metadata and unstructured repositories. We introduce Delta Activations, a method to represent finetuned models as vector embeddings by measuring shifts in their internal activations relative to a base model. This representation allows for effective clustering by domain and task, revealing structure in the model landscape. Delta Activations also demonstrate desirable properties: it is robust across finetuning settings and exhibits an additive property when finetuning datasets are mixed. In addition, we show that Delta Activations can embed tasks via few-shot finetuning, and further explore its use for model selection and merging. We hope Delta Activations can facilitate the practice of reusing publicly available models. Code is available at https://github.com/OscarXZQ/delta_activations."

[05.09.2025 08:15] Response: ```python
['OPEN_SOURCE', 'TRANSFER_LEARNING']
```
[05.09.2025 08:15] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Delta Activations is a novel approach that represents fine-tuned models as vector embeddings by analyzing shifts in their internal activations compared to a base model. This method enhances the organization of models by enabling effective clustering based on specific tasks and domains, making it easier to navigate the vast array of post-trained models. Additionally, Delta Activations shows robustness across different finetuning scenarios and maintains an additive property when combining datasets. The technique also supports few-shot finetuning for task embedding, aiding in model selection and merging, ultimately promoting the reuse of publicly available models.","title":"Streamlining Model Reuse with Delta Activations"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Delta Activations is a novel approach that represents fine-tuned models as vector embeddings by analyzing shifts in their internal activations compared to a base model. This method enhances the organization of models by enabling effective clustering based on specific tasks and domains, making it easier to navigate the vast array of post-trained models. Additionally, Delta Activations shows robustness across different finetuning scenarios and maintains an additive property when combining datasets. The technique also supports few-shot finetuning for task embedding, aiding in model selection and merging, ultimately promoting the reuse of publicly available models.', title='Streamlining Model Reuse with Delta Activations'))
[05.09.2025 08:15] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Delta Activationsæ˜¯ä¸€ç§é€šè¿‡æµ‹é‡æ¨¡å‹å†…éƒ¨æ¿€æ´»çš„å˜åŒ–ï¼Œå°†å¾®è°ƒæ¨¡å‹è¡¨ç¤ºä¸ºå‘é‡åµŒå…¥çš„æ–¹æ³•ã€‚è¿™ç§è¡¨ç¤ºæ–¹å¼ä½¿å¾—æ ¹æ®é¢†åŸŸå’Œä»»åŠ¡è¿›è¡Œæœ‰æ•ˆçš„èšç±»æˆä¸ºå¯èƒ½ï¼Œä»è€Œæ­ç¤ºæ¨¡å‹çš„ç»“æ„ã€‚Delta Activationsåœ¨å¾®è°ƒè®¾ç½®ä¸­è¡¨ç°å‡ºè‰¯å¥½çš„é²æ£’æ€§ï¼Œå¹¶ä¸”åœ¨æ··åˆå¾®è°ƒæ•°æ®é›†æ—¶å…·æœ‰å¯åŠ æ€§ã€‚æˆ‘ä»¬å¸Œæœ›Delta Activationsèƒ½å¤Ÿä¿ƒè¿›å…¬å…±å¯ç”¨æ¨¡å‹çš„é‡ç”¨å®è·µã€‚","title":"Delta Activationsï¼šæ¨¡å‹é‡ç”¨çš„æ–°æ–¹æ³•"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Delta Activationsæ˜¯ä¸€ç§é€šè¿‡æµ‹é‡æ¨¡å‹å†…éƒ¨æ¿€æ´»çš„å˜åŒ–ï¼Œå°†å¾®è°ƒæ¨¡å‹è¡¨ç¤ºä¸ºå‘é‡åµŒå…¥çš„æ–¹æ³•ã€‚è¿™ç§è¡¨ç¤ºæ–¹å¼ä½¿å¾—æ ¹æ®é¢†åŸŸå’Œä»»åŠ¡è¿›è¡Œæœ‰æ•ˆçš„èšç±»æˆä¸ºå¯èƒ½ï¼Œä»è€Œæ­ç¤ºæ¨¡å‹çš„ç»“æ„ã€‚Delta Activationsåœ¨å¾®è°ƒè®¾ç½®ä¸­è¡¨ç°å‡ºè‰¯å¥½çš„é²æ£’æ€§ï¼Œå¹¶ä¸”åœ¨æ··åˆå¾®è°ƒæ•°æ®é›†æ—¶å…·æœ‰å¯åŠ æ€§ã€‚æˆ‘ä»¬å¸Œæœ›Delta Activationsèƒ½å¤Ÿä¿ƒè¿›å…¬å…±å¯ç”¨æ¨¡å‹çš„é‡ç”¨å®è·µã€‚', title='Delta Activationsï¼šæ¨¡å‹é‡ç”¨çš„æ–°æ–¹æ³•'))
[05.09.2025 08:15] Querying the API.
[05.09.2025 08:15] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

LLMs struggle with understanding the nuanced, context-dependent meanings of Drivelological text, which appears nonsensical but contains deeper semantic layers.  					AI-generated summary 				 We introduce Drivelology, a unique linguistic phenomenon characterised as "nonsense with depth", utterances that are syntactically coherent yet pragmatically paradoxical, emotionally loaded, or rhetorically subversive. While such expressions may resemble surface-level nonsense, they encode implicit meaning requiring contextual inference, moral reasoning, or emotional interpretation. We find that current large language models (LLMs), despite excelling at many natural language processing (NLP) tasks, consistently fail to grasp the layered semantics of Drivelological text. To investigate this, we construct a small but diverse benchmark dataset of over 1,200 meticulously curated examples, with select instances in English, Mandarin, Spanish, French, Japanese, and Korean. Annotation was especially challenging: each of the examples required careful expert review to verify that it truly reflected Drivelological characteristics. The process involved multiple rounds of discussion and adjudication to address disagreements, highlighting the subtle and subjective nature of the Drivelology. We evaluate a range of LLMs on classification, generation, and reasoning tasks. Our results reveal clear limitations of LLMs: models often confuse Drivelology with shallow nonsense, produce incoherent justifications, or miss the implied rhetorical function altogether. These findings highlight a deeper representational gap in LLMs' pragmatic understanding and challenge the assumption that statistical fluency implies cognitive comprehension. We release our dataset and code to facilitate further research in modelling linguistic depth beyond surface-level coherence.
[05.09.2025 08:15] Response: {
  "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ 'Ğ´Ñ€Ğ¸Ğ²ĞµĞ»Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸' - Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ„ĞµĞ½Ğ¾Ğ¼ĞµĞ½Ğ°, Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸Ğ·ÑƒÑÑ‰ĞµĞ³Ğ¾ÑÑ ĞºĞ°Ğº 'Ğ±ĞµÑÑĞ¼Ñ‹ÑĞ»Ğ¸Ñ†Ğ° Ñ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğ¾Ğ¹'. ĞĞ½Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 1200 Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ°Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ»Ğ¾Ğ¹Ğ½ÑƒÑ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºÑƒ Ğ´Ñ€Ğ¸Ğ²ĞµĞ»Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ LLM Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾-Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ñ‹Ñ… Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¸Ğ¼Ğ¿Ğ»Ğ¸Ñ†Ğ¸Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¼Ñ‹ÑĞ»Ğ° Ñ‚Ğ°ĞºĞ¸Ñ… Ğ²Ñ‹ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğ¹. Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ°Ğ³Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ LLM Ğ¸ ÑÑ‚Ğ°Ğ²Ğ¸Ñ‚ Ğ¿Ğ¾Ğ´ ÑĞ¾Ğ¼Ğ½ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ±ĞµĞ³Ğ»Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ´Ñ€Ğ°Ğ·ÑƒĞ¼ĞµĞ²Ğ°ĞµÑ‚ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ.",
  "emoji": "ğŸ§ ",
  "title": "Ğ‘ĞµÑÑĞ¼Ñ‹ÑĞ»Ğ¸Ñ†Ğ° Ñ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğ¾Ğ¹: Ğ²Ñ‹Ğ·Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°"
}
[05.09.2025 08:15] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"LLMs struggle with understanding the nuanced, context-dependent meanings of Drivelological text, which appears nonsensical but contains deeper semantic layers.  					AI-generated summary 				 We introduce Drivelology, a unique linguistic phenomenon characterised as "nonsense with depth", utterances that are syntactically coherent yet pragmatically paradoxical, emotionally loaded, or rhetorically subversive. While such expressions may resemble surface-level nonsense, they encode implicit meaning requiring contextual inference, moral reasoning, or emotional interpretation. We find that current large language models (LLMs), despite excelling at many natural language processing (NLP) tasks, consistently fail to grasp the layered semantics of Drivelological text. To investigate this, we construct a small but diverse benchmark dataset of over 1,200 meticulously curated examples, with select instances in English, Mandarin, Spanish, French, Japanese, and Korean. Annotation was especially challenging: each of the examples required careful expert review to verify that it truly reflected Drivelological characteristics. The process involved multiple rounds of discussion and adjudication to address disagreements, highlighting the subtle and subjective nature of the Drivelology. We evaluate a range of LLMs on classification, generation, and reasoning tasks. Our results reveal clear limitations of LLMs: models often confuse Drivelology with shallow nonsense, produce incoherent justifications, or miss the implied rhetorical function altogether. These findings highlight a deeper representational gap in LLMs' pragmatic understanding and challenge the assumption that statistical fluency implies cognitive comprehension. We release our dataset and code to facilitate further research in modelling linguistic depth beyond surface-level coherence."

[05.09.2025 08:15] Response: ```python
['DATASET', 'BENCHMARK', 'MULTILINGUAL']
```
[05.09.2025 08:15] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"LLMs struggle with understanding the nuanced, context-dependent meanings of Drivelological text, which appears nonsensical but contains deeper semantic layers.  					AI-generated summary 				 We introduce Drivelology, a unique linguistic phenomenon characterised as "nonsense with depth", utterances that are syntactically coherent yet pragmatically paradoxical, emotionally loaded, or rhetorically subversive. While such expressions may resemble surface-level nonsense, they encode implicit meaning requiring contextual inference, moral reasoning, or emotional interpretation. We find that current large language models (LLMs), despite excelling at many natural language processing (NLP) tasks, consistently fail to grasp the layered semantics of Drivelological text. To investigate this, we construct a small but diverse benchmark dataset of over 1,200 meticulously curated examples, with select instances in English, Mandarin, Spanish, French, Japanese, and Korean. Annotation was especially challenging: each of the examples required careful expert review to verify that it truly reflected Drivelological characteristics. The process involved multiple rounds of discussion and adjudication to address disagreements, highlighting the subtle and subjective nature of the Drivelology. We evaluate a range of LLMs on classification, generation, and reasoning tasks. Our results reveal clear limitations of LLMs: models often confuse Drivelology with shallow nonsense, produce incoherent justifications, or miss the implied rhetorical function altogether. These findings highlight a deeper representational gap in LLMs' pragmatic understanding and challenge the assumption that statistical fluency implies cognitive comprehension. We release our dataset and code to facilitate further research in modelling linguistic depth beyond surface-level coherence."

[05.09.2025 08:15] Response: ```python
["REASONING", "ALIGNMENT", "HALLUCINATIONS"]
```
[05.09.2025 08:15] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces Drivelology, a linguistic concept that describes seemingly nonsensical text that actually contains deeper meanings. The authors demonstrate that large language models (LLMs) struggle to interpret these nuanced expressions, which require contextual understanding and moral reasoning. They created a diverse dataset of over 1,200 examples of Drivelological text, carefully annotated to reflect its unique characteristics. The study reveals significant limitations in LLMs\' ability to grasp the layered semantics of such text, suggesting that fluency in language does not equate to true comprehension.","title":"Unlocking the Depths of Nonsense: Understanding Drivelology"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="This paper introduces Drivelology, a linguistic concept that describes seemingly nonsensical text that actually contains deeper meanings. The authors demonstrate that large language models (LLMs) struggle to interpret these nuanced expressions, which require contextual understanding and moral reasoning. They created a diverse dataset of over 1,200 examples of Drivelological text, carefully annotated to reflect its unique characteristics. The study reveals significant limitations in LLMs' ability to grasp the layered semantics of such text, suggesting that fluency in language does not equate to true comprehension.", title='Unlocking the Depths of Nonsense: Understanding Drivelology'))
[05.09.2025 08:15] Response: ParsedChatCompletionMessage[Article](content='{"desc":"æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºâ€œæ— æ„ä¹‰å­¦â€çš„ç‹¬ç‰¹è¯­è¨€ç°è±¡ï¼Œç‰¹å¾æ˜¯è¡¨é¢ä¸Šçœ‹ä¼¼æ— æ„ä¹‰çš„è¡¨è¾¾å®é™…ä¸Šè•´å«æ·±å±‚è¯­ä¹‰ã€‚è¿™äº›è¡¨è¾¾åœ¨è¯­æ³•ä¸Šæ˜¯è¿è´¯çš„ï¼Œä½†åœ¨è¯­ç”¨ä¸Šå´å­˜åœ¨çŸ›ç›¾ï¼Œæƒ…æ„Ÿä¸Šå……æ»¡è´Ÿè½½ï¼Œæˆ–åœ¨ä¿®è¾ä¸Šå…·æœ‰é¢ è¦†æ€§ã€‚å°½ç®¡å½“å‰çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è®¸å¤šè‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬åœ¨ç†è§£æ— æ„ä¹‰å­¦æ–‡æœ¬çš„å±‚æ¬¡è¯­ä¹‰æ–¹é¢å­˜åœ¨æ˜æ˜¾å±€é™ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåŒ…å«1200å¤šä¸ªç»è¿‡ç²¾å¿ƒç­–åˆ’çš„ç¤ºä¾‹çš„å°å‹å¤šæ ·åŒ–åŸºå‡†æ•°æ®é›†ï¼Œä»¥è¯„ä¼°è¿™äº›æ¨¡å‹åœ¨åˆ†ç±»ã€ç”Ÿæˆå’Œæ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚","title":"æ­ç¤ºæ— æ„ä¹‰å­¦çš„æ·±å±‚è¯­ä¹‰æŒ‘æˆ˜"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºâ€œæ— æ„ä¹‰å­¦â€çš„ç‹¬ç‰¹è¯­è¨€ç°è±¡ï¼Œç‰¹å¾æ˜¯è¡¨é¢ä¸Šçœ‹ä¼¼æ— æ„ä¹‰çš„è¡¨è¾¾å®é™…ä¸Šè•´å«æ·±å±‚è¯­ä¹‰ã€‚è¿™äº›è¡¨è¾¾åœ¨è¯­æ³•ä¸Šæ˜¯è¿è´¯çš„ï¼Œä½†åœ¨è¯­ç”¨ä¸Šå´å­˜åœ¨çŸ›ç›¾ï¼Œæƒ…æ„Ÿä¸Šå……æ»¡è´Ÿè½½ï¼Œæˆ–åœ¨ä¿®è¾ä¸Šå…·æœ‰é¢ è¦†æ€§ã€‚å°½ç®¡å½“å‰çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è®¸å¤šè‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬åœ¨ç†è§£æ— æ„ä¹‰å­¦æ–‡æœ¬çš„å±‚æ¬¡è¯­ä¹‰æ–¹é¢å­˜åœ¨æ˜æ˜¾å±€é™ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåŒ…å«1200å¤šä¸ªç»è¿‡ç²¾å¿ƒç­–åˆ’çš„ç¤ºä¾‹çš„å°å‹å¤šæ ·åŒ–åŸºå‡†æ•°æ®é›†ï¼Œä»¥è¯„ä¼°è¿™äº›æ¨¡å‹åœ¨åˆ†ç±»ã€ç”Ÿæˆå’Œæ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚', title='æ­ç¤ºæ— æ„ä¹‰å­¦çš„æ·±å±‚è¯­ä¹‰æŒ‘æˆ˜'))
[05.09.2025 08:15] Using data from previous issue: {"categories": ["#security", "#training", "#alignment", "#benchmark", "#data", "#open_source"], "emoji": "ğŸ•µï¸", "ru": {"title": "Ğ—Ğ¾Ğ½Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ LLM: Ğ·Ğ° Ñ„Ğ°ÑĞ°Ğ´Ğ¾Ğ¼ ĞºĞ°Ğ¶ÑƒÑ‰ĞµĞ¹ÑÑ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸", "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ·Ğ¾Ğ½Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğµ
[05.09.2025 08:15] Renaming data file.
[05.09.2025 08:15] Renaming previous data. hf_papers.json to ./d/2025-09-05.json
[05.09.2025 08:15] Saving new data file.
[05.09.2025 08:15] Generating page.
[05.09.2025 08:15] Renaming previous page.
[05.09.2025 08:15] Renaming previous data. index.html to ./d/2025-09-05.html
[05.09.2025 08:15] Writing result.
[05.09.2025 08:15] Renaming log file.
[05.09.2025 08:15] Renaming previous data. log.txt to ./logs/2025-09-05_last_log.txt
