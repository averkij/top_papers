[05.09.2025 07:11] Read previous papers.
[05.09.2025 07:11] Generating top page (month).
[05.09.2025 07:11] Writing top page (month).
[05.09.2025 08:15] Read previous papers.
[05.09.2025 08:15] Get feed.
[05.09.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2509.04338
[05.09.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2509.04292
[05.09.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2509.04419
[05.09.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2509.01396
[05.09.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2509.04394
[05.09.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2509.04011
[05.09.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2509.04406
[05.09.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2509.04434
[05.09.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2508.18733
[05.09.2025 08:15] Extract page data from URL. URL: https://huggingface.co/papers/2509.04442
[05.09.2025 08:15] Extract page data from URL. URL: https://huggingface.co/papers/2509.03867
[05.09.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2509.03888
[05.09.2025 08:15] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[05.09.2025 08:15] No deleted papers detected.
[05.09.2025 08:15] Downloading and parsing papers (pdf, html). Total: 12.
[05.09.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2509.04338.
[05.09.2025 08:15] Extra JSON file exists (./assets/json/2509.04338.json), skip PDF parsing.
[05.09.2025 08:15] Paper image links file exists (./assets/img_data/2509.04338.json), skip HTML parsing.
[05.09.2025 08:15] Success.
[05.09.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2509.04292.
[05.09.2025 08:15] Extra JSON file exists (./assets/json/2509.04292.json), skip PDF parsing.
[05.09.2025 08:15] Paper image links file exists (./assets/img_data/2509.04292.json), skip HTML parsing.
[05.09.2025 08:15] Success.
[05.09.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2509.04419.
[05.09.2025 08:15] Extra JSON file exists (./assets/json/2509.04419.json), skip PDF parsing.
[05.09.2025 08:15] Paper image links file exists (./assets/img_data/2509.04419.json), skip HTML parsing.
[05.09.2025 08:15] Success.
[05.09.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2509.01396.
[05.09.2025 08:15] Extra JSON file exists (./assets/json/2509.01396.json), skip PDF parsing.
[05.09.2025 08:15] Paper image links file exists (./assets/img_data/2509.01396.json), skip HTML parsing.
[05.09.2025 08:15] Success.
[05.09.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2509.04394.
[05.09.2025 08:15] Extra JSON file exists (./assets/json/2509.04394.json), skip PDF parsing.
[05.09.2025 08:15] Paper image links file exists (./assets/img_data/2509.04394.json), skip HTML parsing.
[05.09.2025 08:15] Success.
[05.09.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2509.04011.
[05.09.2025 08:15] Extra JSON file exists (./assets/json/2509.04011.json), skip PDF parsing.
[05.09.2025 08:15] Paper image links file exists (./assets/img_data/2509.04011.json), skip HTML parsing.
[05.09.2025 08:15] Success.
[05.09.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2509.04406.
[05.09.2025 08:15] Extra JSON file exists (./assets/json/2509.04406.json), skip PDF parsing.
[05.09.2025 08:15] Paper image links file exists (./assets/img_data/2509.04406.json), skip HTML parsing.
[05.09.2025 08:15] Success.
[05.09.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2509.04434.
[05.09.2025 08:15] Extra JSON file exists (./assets/json/2509.04434.json), skip PDF parsing.
[05.09.2025 08:15] Paper image links file exists (./assets/img_data/2509.04434.json), skip HTML parsing.
[05.09.2025 08:15] Success.
[05.09.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2508.18733.
[05.09.2025 08:15] Extra JSON file exists (./assets/json/2508.18733.json), skip PDF parsing.
[05.09.2025 08:15] Paper image links file exists (./assets/img_data/2508.18733.json), skip HTML parsing.
[05.09.2025 08:15] Success.
[05.09.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2509.04442.
[05.09.2025 08:15] Downloading paper 2509.04442 from http://arxiv.org/pdf/2509.04442v1...
[05.09.2025 08:15] Extracting affiliations from text.
[05.09.2025 08:15] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 4 ] . [ 1 2 4 4 4 0 . 9 0 5 2 : r Delta Activations: Representation for Finetuned Large Language Models Zhiqiu Xu1 Amish Sethi1 Mayur Naik1 Ser-Nam Lim2 1University of Pennsylvania 2University of Central Florida "
[05.09.2025 08:15] Response: ```python
["University of Pennsylvania", "University of Central Florida"]
```
[05.09.2025 08:15] Deleting PDF ./assets/pdf/2509.04442.pdf.
[05.09.2025 08:15] Success.
[05.09.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2509.03867.
[05.09.2025 08:15] Downloading paper 2509.03867 from http://arxiv.org/pdf/2509.03867v1...
[05.09.2025 08:15] Extracting affiliations from text.
[05.09.2025 08:15] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Drivel-ology: Challenging LLMs with Interpreting Nonsense with Depth Yang Wang1, Chenghao Xiao2, Chia-Yi Hsiao2, Zi Yan Chang3, Chi-Li Chen3, Tyler Loakman3, Chenghua Lin1, 1The University of Manchester, 2Durham University, 3The University of Sheffield 5 2 0 2 4 ] . [ 1 7 6 8 3 0 . 9 0 5 2 : r a "
[05.09.2025 08:15] Response: ```python
["The University of Manchester", "Durham University", "The University of Sheffield"]
```
[05.09.2025 08:15] Deleting PDF ./assets/pdf/2509.03867.pdf.
[05.09.2025 08:15] Success.
[05.09.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2509.03888.
[05.09.2025 08:15] Extra JSON file exists (./assets/json/2509.03888.json), skip PDF parsing.
[05.09.2025 08:15] Paper image links file exists (./assets/img_data/2509.03888.json), skip HTML parsing.
[05.09.2025 08:15] Success.
[05.09.2025 08:15] Enriching papers with extra data.
[05.09.2025 08:15] ********************************************************************************
[05.09.2025 08:15] Abstract 0. FE2E, a framework using a Diffusion Transformer for dense geometry prediction, outperforms generative models in zero-shot monocular depth and normal estimation with improved performance and efficiency.  					AI-generated summary 				 Leveraging visual priors from pre-trained text-to-image (T2I) gene...
[05.09.2025 08:15] ********************************************************************************
[05.09.2025 08:15] Abstract 1. Inverse IFEval evaluates Large Language Models' ability to override training biases and follow unconventional instructions, highlighting the need for adaptability in diverse contexts.  					AI-generated summary 				 Large Language Models (LLMs) achieve strong performance on diverse tasks but often e...
[05.09.2025 08:15] ********************************************************************************
[05.09.2025 08:15] Abstract 2. A unified policy gradient estimator and Hybrid Post-Training algorithm effectively combine online and offline data for post-training language models, improving performance across various benchmarks.  					AI-generated summary 				 Two major sources of training data exist for post-training modern lan...
[05.09.2025 08:15] ********************************************************************************
[05.09.2025 08:15] Abstract 3. DeepResearch Arena, a benchmark using academic seminar transcripts, provides high-quality research tasks to evaluate deep research agents across multiple disciplines.  					AI-generated summary 				 Deep research agents have attracted growing attention for their potential to orchestrate multi-stage ...
[05.09.2025 08:15] ********************************************************************************
[05.09.2025 08:15] Abstract 4. A novel generative paradigm, Transition Models (TiM), addresses the trade-off between computational cost and output quality in generative modeling by using a continuous-time dynamics equation.  					AI-generated summary 				 A fundamental dilemma in generative modeling persists: iterative diffusion ...
[05.09.2025 08:15] ********************************************************************************
[05.09.2025 08:15] Abstract 5. NER Retriever uses internal representations from large language models to perform zero-shot named entity retrieval by embedding entity mentions and type descriptions into a shared semantic space, outperforming lexical and dense sentence-level retrieval methods.  					AI-generated summary 				 We pre...
[05.09.2025 08:15] ********************************************************************************
[05.09.2025 08:15] Abstract 6. A novel framework, MDT-dist, accelerates 3D flow generation by distilling pretrained models to learn Marginal-Data Transport through Velocity Matching and Velocity Distillation, reducing sampling steps and improving speed and fidelity.  					AI-generated summary 				 Flow-based 3D generation models ...
[05.09.2025 08:15] ********************************************************************************
[05.09.2025 08:15] Abstract 7. Durian uses dual reference networks and a diffusion model to generate high-fidelity portrait animations with attribute transfer from a reference image to a target portrait in a zero-shot manner.  					AI-generated summary 				 We present Durian, the first method for generating portrait animation vid...
[05.09.2025 08:15] ********************************************************************************
[05.09.2025 08:15] Abstract 8. Drawing2CAD is a framework that converts 2D vector drawings into parametric CAD models using a sequence-to-sequence learning approach with a dual-decoder transformer architecture and a soft target distribution loss function.  					AI-generated summary 				 Computer-Aided Design (CAD) generative mode...
[05.09.2025 08:15] ********************************************************************************
[05.09.2025 08:15] Abstract 9. Delta Activations represent fine-tuned models as vector embeddings based on internal activation shifts, enabling effective clustering and model reuse.  					AI-generated summary 				 The success of powerful open source Large Language Models (LLMs) has enabled the community to create a vast collectio...
[05.09.2025 08:15] ********************************************************************************
[05.09.2025 08:15] Abstract 10. LLMs struggle with understanding the nuanced, context-dependent meanings of Drivelological text, which appears nonsensical but contains deeper semantic layers.  					AI-generated summary 				 We introduce Drivelology, a unique linguistic phenomenon characterised as "nonsense with depth", utterances ...
[05.09.2025 08:15] ********************************************************************************
[05.09.2025 08:15] Abstract 11. Probing-based approaches for detecting harmful instructions in LLMs are found to rely on superficial patterns rather than semantic understanding, indicating a need for redesigning models and evaluation methods.  					AI-generated summary 				 Large Language Models (LLMs) can comply with harmful inst...
[05.09.2025 08:15] Read previous papers.
[05.09.2025 08:15] Generating reviews via LLM API.
[05.09.2025 08:15] Using data from previous issue: {"categories": ["#inference", "#cv", "#diffusion", "#optimization", "#training"], "emoji": "🔬", "ru": {"title": "FE2E: Революция в оценке глубины и нормалей с помощью Diffusion Transformer", "desc": "FE2E - это новый фреймворк, использующий Diffusion Transformer для предсказания плотной геометрии. О
[05.09.2025 08:15] Using data from previous issue: {"categories": ["#multilingual", "#hallucinations", "#dataset", "#benchmark", "#alignment"], "emoji": "🧠", "ru": {"title": "Преодоление когнитивной инерции в больших языковых моделях", "desc": "Статья представляет новый бенчмарк Inverse IFEval для оценки способности больших языковых моделей (LLM) пр
[05.09.2025 08:15] Using data from previous issue: {"categories": ["#benchmark", "#reasoning", "#optimization", "#training", "#rl"], "emoji": "🧠", "ru": {"title": "Объединение онлайн и офлайн данных для улучшения языковых моделей", "desc": "В статье представлен унифицированный оценщик градиента политики и алгоритм гибридного пост-обучения для языков
[05.09.2025 08:15] Using data from previous issue: {"categories": ["#leakage", "#science", "#agents", "#benchmark", "#survey", "#dataset"], "emoji": "🧠", "ru": {"title": "Академические семинары как основа для оценки ИИ-исследователей", "desc": "DeepResearch Arena - это новый бенчмарк для оценки глубоких исследовательских агентов, основанный на транс
[05.09.2025 08:15] Using data from previous issue: {"categories": ["#small_models", "#optimization", "#generative_modeling", "#diffusion", "#training"], "emoji": "🔄", "ru": {"title": "TiM: гибкое генеративное моделирование с непрерывным временем", "desc": "Статья представляет новую парадигму генеративного моделирования - Transition Models (TiM). TiM
[05.09.2025 08:15] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#transfer_learning", "#multimodal", "#open_source"], "emoji": "🔍", "ru": {"title": "Извлечение сущностей без схем с помощью внутренних представлений языковых моделей", "desc": "NER Retriever - это фреймворк для извлечения именованных сущностей без предварит
[05.09.2025 08:15] Using data from previous issue: {"categories": ["#3d", "#inference", "#diffusion", "#optimization", "#training"], "emoji": "🚀", "ru": {"title": "Революционное ускорение 3D-генерации с помощью дистилляции потоков", "desc": "MDT-dist - это новая система для ускорения генерации 3D-моделей на основе потоков. Она использует дистилляцию
[05.09.2025 08:15] Using data from previous issue: {"categories": ["#cv", "#diffusion", "#multimodal", "#video"], "emoji": "🍍", "ru": {"title": "Революция в анимации портретов: перенос атрибутов без предварительного обучения", "desc": "Статья представляет метод Durian для генерации анимированных портретов с переносом атрибутов с референсного изображ
[05.09.2025 08:15] Using data from previous issue: {"categories": ["#optimization", "#training", "#architecture", "#dataset", "#open_source"], "emoji": "📐", "ru": {"title": "От чертежа к CAD: автоматическая генерация 3D-моделей из 2D-чертежей", "desc": "Drawing2CAD - это фреймворк, который преобразует 2D векторные чертежи в параметрические CAD-модел
[05.09.2025 08:15] Querying the API.
[05.09.2025 08:15] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Delta Activations represent fine-tuned models as vector embeddings based on internal activation shifts, enabling effective clustering and model reuse.  					AI-generated summary 				 The success of powerful open source Large Language Models (LLMs) has enabled the community to create a vast collection of post-trained models adapted to specific tasks and domains. However, navigating and understanding these models remains challenging due to inconsistent metadata and unstructured repositories. We introduce Delta Activations, a method to represent finetuned models as vector embeddings by measuring shifts in their internal activations relative to a base model. This representation allows for effective clustering by domain and task, revealing structure in the model landscape. Delta Activations also demonstrate desirable properties: it is robust across finetuning settings and exhibits an additive property when finetuning datasets are mixed. In addition, we show that Delta Activations can embed tasks via few-shot finetuning, and further explore its use for model selection and merging. We hope Delta Activations can facilitate the practice of reusing publicly available models. Code is available at https://github.com/OscarXZQ/delta_activations.
[05.09.2025 08:15] Response: {
  "desc": "Метод Delta Activations представляет дообученные модели в виде векторных эмбеддингов, основанных на изменениях внутренних активаций относительно базовой модели. Это позволяет эффективно кластеризовать модели по доменам и задачам, выявляя структуру в ландшафте моделей. Delta Activations демонстрирует устойчивость к различным настройкам дообучения и обладает аддитивным свойством при смешивании наборов данных для дообучения. Метод также может использоваться для эмбеддинга задач через few-shot дообучение и применяться для выбора и объединения моделей.",

  "emoji": "🧠",

  "title": "Delta Activations: Навигация в мире дообученных языковых моделей"
}
[05.09.2025 08:15] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Delta Activations represent fine-tuned models as vector embeddings based on internal activation shifts, enabling effective clustering and model reuse.  					AI-generated summary 				 The success of powerful open source Large Language Models (LLMs) has enabled the community to create a vast collection of post-trained models adapted to specific tasks and domains. However, navigating and understanding these models remains challenging due to inconsistent metadata and unstructured repositories. We introduce Delta Activations, a method to represent finetuned models as vector embeddings by measuring shifts in their internal activations relative to a base model. This representation allows for effective clustering by domain and task, revealing structure in the model landscape. Delta Activations also demonstrate desirable properties: it is robust across finetuning settings and exhibits an additive property when finetuning datasets are mixed. In addition, we show that Delta Activations can embed tasks via few-shot finetuning, and further explore its use for model selection and merging. We hope Delta Activations can facilitate the practice of reusing publicly available models. Code is available at https://github.com/OscarXZQ/delta_activations."

[05.09.2025 08:15] Response: ```python
['DATASET', 'DATA', 'TRAINING', 'ARCHITECTURE']
```
[05.09.2025 08:15] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Delta Activations represent fine-tuned models as vector embeddings based on internal activation shifts, enabling effective clustering and model reuse.  					AI-generated summary 				 The success of powerful open source Large Language Models (LLMs) has enabled the community to create a vast collection of post-trained models adapted to specific tasks and domains. However, navigating and understanding these models remains challenging due to inconsistent metadata and unstructured repositories. We introduce Delta Activations, a method to represent finetuned models as vector embeddings by measuring shifts in their internal activations relative to a base model. This representation allows for effective clustering by domain and task, revealing structure in the model landscape. Delta Activations also demonstrate desirable properties: it is robust across finetuning settings and exhibits an additive property when finetuning datasets are mixed. In addition, we show that Delta Activations can embed tasks via few-shot finetuning, and further explore its use for model selection and merging. We hope Delta Activations can facilitate the practice of reusing publicly available models. Code is available at https://github.com/OscarXZQ/delta_activations."

[05.09.2025 08:15] Response: ```python
['OPEN_SOURCE', 'TRANSFER_LEARNING']
```
[05.09.2025 08:15] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Delta Activations is a novel approach that represents fine-tuned models as vector embeddings by analyzing shifts in their internal activations compared to a base model. This method enhances the organization of models by enabling effective clustering based on specific tasks and domains, making it easier to navigate the vast array of post-trained models. Additionally, Delta Activations shows robustness across different finetuning scenarios and maintains an additive property when combining datasets. The technique also supports few-shot finetuning for task embedding, aiding in model selection and merging, ultimately promoting the reuse of publicly available models.","title":"Streamlining Model Reuse with Delta Activations"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Delta Activations is a novel approach that represents fine-tuned models as vector embeddings by analyzing shifts in their internal activations compared to a base model. This method enhances the organization of models by enabling effective clustering based on specific tasks and domains, making it easier to navigate the vast array of post-trained models. Additionally, Delta Activations shows robustness across different finetuning scenarios and maintains an additive property when combining datasets. The technique also supports few-shot finetuning for task embedding, aiding in model selection and merging, ultimately promoting the reuse of publicly available models.', title='Streamlining Model Reuse with Delta Activations'))
[05.09.2025 08:15] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Delta Activations是一种通过测量模型内部激活的变化，将微调模型表示为向量嵌入的方法。这种表示方式使得根据领域和任务进行有效的聚类成为可能，从而揭示模型的结构。Delta Activations在微调设置中表现出良好的鲁棒性，并且在混合微调数据集时具有可加性。我们希望Delta Activations能够促进公共可用模型的重用实践。","title":"Delta Activations：模型重用的新方法"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Delta Activations是一种通过测量模型内部激活的变化，将微调模型表示为向量嵌入的方法。这种表示方式使得根据领域和任务进行有效的聚类成为可能，从而揭示模型的结构。Delta Activations在微调设置中表现出良好的鲁棒性，并且在混合微调数据集时具有可加性。我们希望Delta Activations能够促进公共可用模型的重用实践。', title='Delta Activations：模型重用的新方法'))
[05.09.2025 08:15] Querying the API.
[05.09.2025 08:15] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

LLMs struggle with understanding the nuanced, context-dependent meanings of Drivelological text, which appears nonsensical but contains deeper semantic layers.  					AI-generated summary 				 We introduce Drivelology, a unique linguistic phenomenon characterised as "nonsense with depth", utterances that are syntactically coherent yet pragmatically paradoxical, emotionally loaded, or rhetorically subversive. While such expressions may resemble surface-level nonsense, they encode implicit meaning requiring contextual inference, moral reasoning, or emotional interpretation. We find that current large language models (LLMs), despite excelling at many natural language processing (NLP) tasks, consistently fail to grasp the layered semantics of Drivelological text. To investigate this, we construct a small but diverse benchmark dataset of over 1,200 meticulously curated examples, with select instances in English, Mandarin, Spanish, French, Japanese, and Korean. Annotation was especially challenging: each of the examples required careful expert review to verify that it truly reflected Drivelological characteristics. The process involved multiple rounds of discussion and adjudication to address disagreements, highlighting the subtle and subjective nature of the Drivelology. We evaluate a range of LLMs on classification, generation, and reasoning tasks. Our results reveal clear limitations of LLMs: models often confuse Drivelology with shallow nonsense, produce incoherent justifications, or miss the implied rhetorical function altogether. These findings highlight a deeper representational gap in LLMs' pragmatic understanding and challenge the assumption that statistical fluency implies cognitive comprehension. We release our dataset and code to facilitate further research in modelling linguistic depth beyond surface-level coherence.
[05.09.2025 08:15] Response: {
  "desc": "Исследователи представили концепцию 'дривелологии' - лингвистического феномена, характеризующегося как 'бессмыслица с глубиной'. Они создали набор данных из более чем 1200 тщательно отобранных примеров на нескольких языках для оценки способности больших языковых моделей (LLM) понимать многослойную семантику дривелологических текстов. Результаты показали, что современные LLM испытывают значительные трудности с пониманием контекстно-зависимых значений и имплицитного смысла таких высказываний. Это исследование выявляет ограничения в прагматическом понимании LLM и ставит под сомнение предположение, что статистическая беглость подразумевает когнитивное понимание.",
  "emoji": "🧠",
  "title": "Бессмыслица с глубиной: вызов для искусственного интеллекта"
}
[05.09.2025 08:15] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"LLMs struggle with understanding the nuanced, context-dependent meanings of Drivelological text, which appears nonsensical but contains deeper semantic layers.  					AI-generated summary 				 We introduce Drivelology, a unique linguistic phenomenon characterised as "nonsense with depth", utterances that are syntactically coherent yet pragmatically paradoxical, emotionally loaded, or rhetorically subversive. While such expressions may resemble surface-level nonsense, they encode implicit meaning requiring contextual inference, moral reasoning, or emotional interpretation. We find that current large language models (LLMs), despite excelling at many natural language processing (NLP) tasks, consistently fail to grasp the layered semantics of Drivelological text. To investigate this, we construct a small but diverse benchmark dataset of over 1,200 meticulously curated examples, with select instances in English, Mandarin, Spanish, French, Japanese, and Korean. Annotation was especially challenging: each of the examples required careful expert review to verify that it truly reflected Drivelological characteristics. The process involved multiple rounds of discussion and adjudication to address disagreements, highlighting the subtle and subjective nature of the Drivelology. We evaluate a range of LLMs on classification, generation, and reasoning tasks. Our results reveal clear limitations of LLMs: models often confuse Drivelology with shallow nonsense, produce incoherent justifications, or miss the implied rhetorical function altogether. These findings highlight a deeper representational gap in LLMs' pragmatic understanding and challenge the assumption that statistical fluency implies cognitive comprehension. We release our dataset and code to facilitate further research in modelling linguistic depth beyond surface-level coherence."

[05.09.2025 08:15] Response: ```python
['DATASET', 'BENCHMARK', 'MULTILINGUAL']
```
[05.09.2025 08:15] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"LLMs struggle with understanding the nuanced, context-dependent meanings of Drivelological text, which appears nonsensical but contains deeper semantic layers.  					AI-generated summary 				 We introduce Drivelology, a unique linguistic phenomenon characterised as "nonsense with depth", utterances that are syntactically coherent yet pragmatically paradoxical, emotionally loaded, or rhetorically subversive. While such expressions may resemble surface-level nonsense, they encode implicit meaning requiring contextual inference, moral reasoning, or emotional interpretation. We find that current large language models (LLMs), despite excelling at many natural language processing (NLP) tasks, consistently fail to grasp the layered semantics of Drivelological text. To investigate this, we construct a small but diverse benchmark dataset of over 1,200 meticulously curated examples, with select instances in English, Mandarin, Spanish, French, Japanese, and Korean. Annotation was especially challenging: each of the examples required careful expert review to verify that it truly reflected Drivelological characteristics. The process involved multiple rounds of discussion and adjudication to address disagreements, highlighting the subtle and subjective nature of the Drivelology. We evaluate a range of LLMs on classification, generation, and reasoning tasks. Our results reveal clear limitations of LLMs: models often confuse Drivelology with shallow nonsense, produce incoherent justifications, or miss the implied rhetorical function altogether. These findings highlight a deeper representational gap in LLMs' pragmatic understanding and challenge the assumption that statistical fluency implies cognitive comprehension. We release our dataset and code to facilitate further research in modelling linguistic depth beyond surface-level coherence."

[05.09.2025 08:15] Response: ```python
["REASONING", "ALIGNMENT", "HALLUCINATIONS"]
```
[05.09.2025 08:15] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces Drivelology, a linguistic concept that describes seemingly nonsensical text that actually contains deeper meanings. The authors demonstrate that large language models (LLMs) struggle to interpret these nuanced expressions, which require contextual understanding and moral reasoning. They created a diverse dataset of over 1,200 examples of Drivelological text, carefully annotated to reflect its unique characteristics. The study reveals significant limitations in LLMs\' ability to grasp the layered semantics of such text, suggesting that fluency in language does not equate to true comprehension.","title":"Unlocking the Depths of Nonsense: Understanding Drivelology"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="This paper introduces Drivelology, a linguistic concept that describes seemingly nonsensical text that actually contains deeper meanings. The authors demonstrate that large language models (LLMs) struggle to interpret these nuanced expressions, which require contextual understanding and moral reasoning. They created a diverse dataset of over 1,200 examples of Drivelological text, carefully annotated to reflect its unique characteristics. The study reveals significant limitations in LLMs' ability to grasp the layered semantics of such text, suggesting that fluency in language does not equate to true comprehension.", title='Unlocking the Depths of Nonsense: Understanding Drivelology'))
[05.09.2025 08:15] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本文介绍了一种名为“无意义学”的独特语言现象，特征是表面上看似无意义的表达实际上蕴含深层语义。这些表达在语法上是连贯的，但在语用上却存在矛盾，情感上充满负载，或在修辞上具有颠覆性。尽管当前的大型语言模型在许多自然语言处理任务中表现出色，但它们在理解无意义学文本的层次语义方面存在明显局限。我们构建了一个包含1200多个经过精心策划的示例的小型多样化基准数据集，以评估这些模型在分类、生成和推理任务中的表现。","title":"揭示无意义学的深层语义挑战"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本文介绍了一种名为“无意义学”的独特语言现象，特征是表面上看似无意义的表达实际上蕴含深层语义。这些表达在语法上是连贯的，但在语用上却存在矛盾，情感上充满负载，或在修辞上具有颠覆性。尽管当前的大型语言模型在许多自然语言处理任务中表现出色，但它们在理解无意义学文本的层次语义方面存在明显局限。我们构建了一个包含1200多个经过精心策划的示例的小型多样化基准数据集，以评估这些模型在分类、生成和推理任务中的表现。', title='揭示无意义学的深层语义挑战'))
[05.09.2025 08:15] Using data from previous issue: {"categories": ["#security", "#training", "#alignment", "#benchmark", "#data", "#open_source"], "emoji": "🕵️", "ru": {"title": "Зондирование LLM: за фасадом кажущейся безопасности", "desc": "Исследование показало, что методы зондирования для обнаружения вредоносных инструкций в больших языковых моде
[05.09.2025 08:15] Renaming data file.
[05.09.2025 08:15] Renaming previous data. hf_papers.json to ./d/2025-09-05.json
[05.09.2025 08:15] Saving new data file.
[05.09.2025 08:15] Generating page.
[05.09.2025 08:15] Renaming previous page.
[05.09.2025 08:15] Renaming previous data. index.html to ./d/2025-09-05.html
[05.09.2025 08:15] Writing result.
[05.09.2025 08:15] Renaming log file.
[05.09.2025 08:15] Renaming previous data. log.txt to ./logs/2025-09-05_last_log.txt
