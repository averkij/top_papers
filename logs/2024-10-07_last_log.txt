[08.10.2024 08:18] Get feed.
[08.10.2024 08:18] Get page data from previous paper. URL: https://huggingface.co/papers/2410.05258
[08.10.2024 08:18] Get page data from previous paper. URL: https://huggingface.co/papers/2410.04364
[08.10.2024 08:18] Get page data from previous paper. URL: https://huggingface.co/papers/2410.02675
[08.10.2024 08:18] Extract page data from URL. URL: https://huggingface.co/papers/2410.02707
[08.10.2024 08:18] Get page data from previous paper. URL: https://huggingface.co/papers/2410.05167
[08.10.2024 08:18] Get page data from previous paper. URL: https://huggingface.co/papers/2410.04932
[08.10.2024 08:18] Extract page data from URL. URL: https://huggingface.co/papers/2410.04698
[08.10.2024 08:18] Get page data from previous paper. URL: https://huggingface.co/papers/2410.04734
[08.10.2024 08:18] Get page data from previous paper. URL: https://huggingface.co/papers/2410.03959
[08.10.2024 08:18] Get page data from previous paper. URL: https://huggingface.co/papers/2410.03825
[08.10.2024 08:18] Get page data from previous paper. URL: https://huggingface.co/papers/2410.04534
[08.10.2024 08:18] Get page data from previous paper. URL: https://huggingface.co/papers/2410.05262
[08.10.2024 08:18] Extract page data from URL. URL: https://huggingface.co/papers/2410.03187
[08.10.2024 08:18] ********************************************************************************
[08.10.2024 08:18] Abstract 0. Transformer tends to overallocate attention to irrelevant context. In this work, we introduce Diff Transformer, which amplifies attention to the relevant context while canceling noise. Specifically, the differential attention mechanism calculates attention scores as the difference between two separa...
[08.10.2024 08:18] ********************************************************************************
[08.10.2024 08:18] Abstract 1. Text-to-image (T2I) diffusion models have revolutionized visual content creation, but extending these capabilities to text-to-video (T2V) generation remains a challenge, particularly in preserving temporal consistency. Existing methods that aim to improve consistency often cause trade-offs such as r...
[08.10.2024 08:18] ********************************************************************************
[08.10.2024 08:18] Abstract 2. Despite the remarkable success achieved by neural networks, particularly those represented by MLP and Transformer, we reveal that they exhibit potential flaws in the modeling and reasoning of periodicity, i.e., they tend to memorize the periodic data rather than genuinely understanding the underlyin...
[08.10.2024 08:18] ********************************************************************************
[08.10.2024 08:18] Abstract 3. Large language models (LLMs) often produce errors, including factual inaccuracies, biases, and reasoning failures, collectively referred to as "hallucinations". Recent studies have demonstrated that LLMs' internal states encode information regarding the truthfulness of their outputs, and that this i...
[08.10.2024 08:18] ********************************************************************************
[08.10.2024 08:18] Abstract 4. Despite advances in diffusion-based text-to-music (TTM) methods, efficient, high-quality generation remains a challenge. We introduce Presto!, an approach to inference acceleration for score-based diffusion transformers via reducing both sampling steps and cost per step. To reduce steps, we develop ...
[08.10.2024 08:18] ********************************************************************************
[08.10.2024 08:18] Abstract 5. We present OmniBooth, an image generation framework that enables spatial control with instance-level multi-modal customization. For all instances, the multimodal instruction can be described through text prompts or image references. Given a set of user-defined masks and associated text or image guid...
[08.10.2024 08:18] ********************************************************************************
[08.10.2024 08:18] Abstract 6. Recent large language models (LLMs) have demonstrated versatile capabilities in long-context scenarios. Although some recent benchmarks have been developed to evaluate the long-context capabilities of LLMs, there is a lack of benchmarks evaluating the mathematical reasoning abilities of LLMs over lo...
[08.10.2024 08:18] ********************************************************************************
[08.10.2024 08:18] Abstract 7. Although reward models have been successful in improving multimodal large language models, the reward models themselves remain brutal and contain minimal information. Notably, existing reward models only mimic human annotations by assigning only one binary feedback to any text, no matter how long th...
[08.10.2024 08:18] ********************************************************************************
[08.10.2024 08:18] Abstract 8. We introduce a task and dataset for referring expression generation and comprehension in multi-agent embodied environments. In this task, two agents in a shared scene must take into account one another's visual perspective, which may be different from their own, to both produce and understand refere...
[08.10.2024 08:18] ********************************************************************************
[08.10.2024 08:18] Abstract 9. Estimating geometry from dynamic scenes, where objects move and deform over time, remains a core challenge in computer vision. Current approaches often rely on multi-stage pipelines or global optimizations that decompose the problem into subtasks, like depth and flow, leading to complex systems pron...
[08.10.2024 08:18] ********************************************************************************
[08.10.2024 08:18] Abstract 10. We introduce UniMuMo, a unified multimodal model capable of taking arbitrary text, music, and motion data as input conditions to generate outputs across all three modalities. To address the lack of time-synchronized data, we align unpaired music and motion data based on rhythmic patterns to leverage...
[08.10.2024 08:18] ********************************************************************************
[08.10.2024 08:18] Abstract 11. As the application of Large Language Models (LLMs) expands, the demand for reliable evaluations increases. Existing LLM evaluation benchmarks primarily rely on static datasets, making it challenging to assess model performance in dynamic interactions with users. Moreover, these benchmarks often depe...
[08.10.2024 08:18] ********************************************************************************
[08.10.2024 08:18] Abstract 12. Synthesizing human motions in 3D environments, particularly those with complex activities such as locomotion, hand-reaching, and human-object interaction, presents substantial demands for user-defined waypoints and stage transitions. These requirements pose challenges for current models, leading to ...
[08.10.2024 08:18] Read previous papers.
[08.10.2024 08:18] Generating reviews via LLM API.
[08.10.2024 08:18] Using data from previous issue: {"desc": "Статья представляет новую архитектуру Diff Transformer, которая улучшает внимание к релевантному контексту и подавляет шум. Механизм дифференциального внимания вычисляет оценки внимания как разницу между двумя отдельными картами внимания softmax. Эксперименты показывают, что Diff Transform
[08.10.2024 08:18] Using data from previous issue: {"desc": "VideoGuide - это новая система, которая улучшает временную согласованность предобученных моделей генерации видео по тексту без дополнительного обучения. Она использует предобученную модель диффузии видео в качестве гида на ранних этапах вывода, интерполируя ее образцы в процесс шумоподавле
[08.10.2024 08:18] Using data from previous issue: {"desc": "Статья представляет новую архитектуру нейронной сети под названием FAN, основанную на анализе Фурье. FAN эффективно моделирует и обрабатывает периодические явления, преодолевая ограничения традиционных нейронных сетей в этой области. Авторы демонстрируют превосходство FAN над многослойным 
[08.10.2024 08:18] Querying the API.
[08.10.2024 08:18] Got response. {
  "desc": "Исследование показывает, что внутренние представления больших языковых моделей (LLM) содержат больше информации о достоверности генерируемых данных, чем считалось ранее. Авторы обнаружили, что эта информация сконцентрирована в определенных токенах, что позволяет значительно улучшить обнаружение ошибок. Однако детекторы ошибок не обобщаются между датасетами, что указывает на отсутствие универсального кодирования достоверности. Исследование также выявило, что LLM могут внутренне кодировать правильный ответ, но при этом генерировать неверный.",
  "tags": ["#hallucinations", "#errorDetection", "#internalRepresentations"],
  "emoji": "🔍",
  "title": "Раскрытие тайн внутренних представлений LLM для улучшения обнаружения ошибок"
}
[08.10.2024 08:18] Using data from previous issue: {"desc": "Статья представляет Presto! - новый подход к ускорению генерации музыки на основе текста с использованием диффузионных трансформеров. Авторы разработали метод дистилляции на основе согласования распределений (DMD) для уменьшения количества шагов сэмплирования. Они также улучшили метод посл
[08.10.2024 08:18] Using data from previous issue: {"desc": "OmniBooth - это фреймворк для генерации изображений с пространственным контролем и мультимодальной кастомизацией на уровне отдельных объектов. Система позволяет задавать расположение и атрибуты объектов с помощью текстовых запросов или изображений-образцов. Ключевым элементом является пред
[08.10.2024 08:18] Querying the API.
[08.10.2024 08:18] Got response. {
  "desc": "Статья представляет новый бенчмарк MathHay для оценки способностей больших языковых моделей (LLM) к математическим рассуждениям в контексте длинных текстов. В отличие от существующих бенчмарков, MathHay требует от моделей не только поиска информации, но и сложных математических вычислений. Эксперименты с восемью ведущими LLM показали, что даже лучшая модель Gemini-1.5-Pro-002 достигает точности всего 51.26% на текстах длиной 128 тысяч токенов. Результаты указывают на значительный потенциал для улучшения способностей LLM в области длинных математических рассуждений.",

  "tags": ["#LongContextMathReasoning", "#MathHayBenchmark", "#LLMEvaluation"],

  "emoji": "🧮",

  "title": "MathHay: Новый вызов для математических способностей языковых моделей"
}
[08.10.2024 08:18] Using data from previous issue: {"desc": "В статье представлена модель вознаграждения на уровне токенов (TLDR) для мультимодальных языковых моделей. TLDR обеспечивает детальную аннотацию каждого текстового токена, что позволяет лучше учитывать как текстовую, так и визуальную информацию. Метод использует синтетические 'сложные отри
[08.10.2024 08:18] Using data from previous issue: {"desc": "Статья представляет новую задачу и набор данных для генерации и понимания референциальных выражений в многоагентных средах. Два агента в общей сцене должны учитывать визуальную перспективу друг друга для создания и понимания ссылок на объекты и их пространственные отношения. Авторы собрали
[08.10.2024 08:18] Using data from previous issue: {"desc": "MonST3R - это новый подход к оценке геометрии динамических сцен в компьютерном зрении. Вместо сложных многоступенчатых систем, он напрямую оценивает геометрию для каждого кадра, адаптируя представление DUST3R для динамических сцен. Несмотря на проблему нехватки данных для обучения, авторы 
[08.10.2024 08:18] Using data from previous issue: {"desc": "UniMuMo - это унифицированная мультимодальная модель, способная генерировать текст, музыку и движения на основе входных данных в этих модальностях. Модель использует выравнивание неспаренных музыкальных и двигательных данных на основе ритмических паттернов. Архитектура UniMuMo основана на 
[08.10.2024 08:18] Using data from previous issue: {"desc": "TurtleBench - новый метод оценки больших языковых моделей (LLM), основанный на реальных догадках пользователей в игре Turtle Soup Puzzle. Этот подход позволяет динамически генерировать наборы данных для оценки, снижая риск обмана со стороны моделей и более точно отражая потребности пользов
[08.10.2024 08:18] Querying the API.
[08.10.2024 08:18] Got response. {
  "desc": "Статья представляет новую систему для синтеза сложных движений человека в 3D-средах на основе текстовых инструкций и целевого положения. Авторы используют авторегрессионную диффузионную модель для генерации последовательных сегментов движения и автономный планировщик для предсказания переходов между этапами действий. Для интеграции движений с окружением предложено специальное представление сцены, учитывающее локальное восприятие. Также создан обширный датасет из 16 часов захваченных движений в 120 indoor-сценах для обучения модели.",
  "tags": ["#синтез_движений", "#диффузионные_модели", "#3D_анимация"],
  "emoji": "🤖",
  "title": "ИИ оживляет персонажей в 3D по текстовым командам"
}
[08.10.2024 08:18] Renaming data file.
[08.10.2024 08:18] Renaming previous data. hf_papers.json to 2024-10-07_hf_papers.json
[08.10.2024 08:18] Saving new data file.
[08.10.2024 08:18] Generating page.
[08.10.2024 08:18] Renaming previous page.
[08.10.2024 08:18] Renaming previous data. index.html to 2024-10-07_hf_papers.html
[08.10.2024 08:18] Writing result.
[08.10.2024 08:18] Renaming log file.
[08.10.2024 08:18] Renaming previous data. log.txt to 2024-10-07_last_log.txt
