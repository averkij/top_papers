[29.01.2026 14:05] Read previous papers.
[29.01.2026 14:05] Generating top page (month).
[29.01.2026 14:05] Writing top page (month).
[29.01.2026 15:37] Read previous papers.
[29.01.2026 15:37] Get feed.
[29.01.2026 15:37] Get page data from previous paper. URL: https://huggingface.co/papers/2601.20614
[29.01.2026 15:37] Get page data from previous paper. URL: https://huggingface.co/papers/2601.20540
[29.01.2026 15:37] Get page data from previous paper. URL: https://huggingface.co/papers/2601.19325
[29.01.2026 15:37] Get page data from previous paper. URL: https://huggingface.co/papers/2601.20552
[29.01.2026 15:37] Get page data from previous paper. URL: https://huggingface.co/papers/2601.20209
[29.01.2026 15:37] Get page data from previous paper. URL: https://huggingface.co/papers/2601.20834
[29.01.2026 15:37] Get page data from previous paper. URL: https://huggingface.co/papers/2601.20789
[29.01.2026 15:37] Get page data from previous paper. URL: https://huggingface.co/papers/2601.20380
[29.01.2026 15:37] Get page data from previous paper. URL: https://huggingface.co/papers/2601.20802
[29.01.2026 15:37] Get page data from previous paper. URL: https://huggingface.co/papers/2601.19949
[29.01.2026 15:37] Extract page data from URL. URL: https://huggingface.co/papers/2601.20829
[29.01.2026 15:37] Get page data from previous paper. URL: https://huggingface.co/papers/2601.20618
[29.01.2026 15:37] Get page data from previous paper. URL: https://huggingface.co/papers/2601.19194
[29.01.2026 15:37] Get page data from previous paper. URL: https://huggingface.co/papers/2601.17950
[29.01.2026 15:37] Get page data from previous paper. URL: https://huggingface.co/papers/2601.20622
[29.01.2026 15:37] Get page data from previous paper. URL: https://huggingface.co/papers/2601.20262
[29.01.2026 15:37] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[29.01.2026 15:37] No deleted papers detected.
[29.01.2026 15:37] Downloading and parsing papers (pdf, html). Total: 16.
[29.01.2026 15:37] Downloading and parsing paper https://huggingface.co/papers/2601.20614.
[29.01.2026 15:37] Extra JSON file exists (./assets/json/2601.20614.json), skip PDF parsing.
[29.01.2026 15:37] Paper image links file exists (./assets/img_data/2601.20614.json), skip HTML parsing.
[29.01.2026 15:37] Success.
[29.01.2026 15:37] Downloading and parsing paper https://huggingface.co/papers/2601.20540.
[29.01.2026 15:37] Extra JSON file exists (./assets/json/2601.20540.json), skip PDF parsing.
[29.01.2026 15:37] Paper image links file exists (./assets/img_data/2601.20540.json), skip HTML parsing.
[29.01.2026 15:37] Success.
[29.01.2026 15:37] Downloading and parsing paper https://huggingface.co/papers/2601.19325.
[29.01.2026 15:37] Extra JSON file exists (./assets/json/2601.19325.json), skip PDF parsing.
[29.01.2026 15:37] Paper image links file exists (./assets/img_data/2601.19325.json), skip HTML parsing.
[29.01.2026 15:37] Success.
[29.01.2026 15:37] Downloading and parsing paper https://huggingface.co/papers/2601.20552.
[29.01.2026 15:37] Extra JSON file exists (./assets/json/2601.20552.json), skip PDF parsing.
[29.01.2026 15:37] Paper image links file exists (./assets/img_data/2601.20552.json), skip HTML parsing.
[29.01.2026 15:37] Success.
[29.01.2026 15:37] Downloading and parsing paper https://huggingface.co/papers/2601.20209.
[29.01.2026 15:37] Extra JSON file exists (./assets/json/2601.20209.json), skip PDF parsing.
[29.01.2026 15:37] Paper image links file exists (./assets/img_data/2601.20209.json), skip HTML parsing.
[29.01.2026 15:37] Success.
[29.01.2026 15:37] Downloading and parsing paper https://huggingface.co/papers/2601.20834.
[29.01.2026 15:37] Extra JSON file exists (./assets/json/2601.20834.json), skip PDF parsing.
[29.01.2026 15:37] Paper image links file exists (./assets/img_data/2601.20834.json), skip HTML parsing.
[29.01.2026 15:37] Success.
[29.01.2026 15:37] Downloading and parsing paper https://huggingface.co/papers/2601.20789.
[29.01.2026 15:37] Extra JSON file exists (./assets/json/2601.20789.json), skip PDF parsing.
[29.01.2026 15:37] Paper image links file exists (./assets/img_data/2601.20789.json), skip HTML parsing.
[29.01.2026 15:37] Success.
[29.01.2026 15:37] Downloading and parsing paper https://huggingface.co/papers/2601.20380.
[29.01.2026 15:37] Extra JSON file exists (./assets/json/2601.20380.json), skip PDF parsing.
[29.01.2026 15:37] Paper image links file exists (./assets/img_data/2601.20380.json), skip HTML parsing.
[29.01.2026 15:37] Success.
[29.01.2026 15:37] Downloading and parsing paper https://huggingface.co/papers/2601.20802.
[29.01.2026 15:37] Extra JSON file exists (./assets/json/2601.20802.json), skip PDF parsing.
[29.01.2026 15:37] Paper image links file exists (./assets/img_data/2601.20802.json), skip HTML parsing.
[29.01.2026 15:37] Success.
[29.01.2026 15:37] Downloading and parsing paper https://huggingface.co/papers/2601.19949.
[29.01.2026 15:37] Extra JSON file exists (./assets/json/2601.19949.json), skip PDF parsing.
[29.01.2026 15:37] Paper image links file exists (./assets/img_data/2601.19949.json), skip HTML parsing.
[29.01.2026 15:37] Success.
[29.01.2026 15:37] Downloading and parsing paper https://huggingface.co/papers/2601.20829.
[29.01.2026 15:37] Downloading paper 2601.20829 from https://arxiv.org/pdf/2601.20829v1...
[29.01.2026 15:37] Extracting affiliations from text.
[29.01.2026 15:37] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Training Reasoning Models on Saturated Problems via Failure-Prefix Conditioning Minwu Kim 1 Safal Shrestha 1 Keith Ross "
[29.01.2026 15:37] Response: ```python
[]
```
[29.01.2026 15:37] Extracting affiliations from text.
[29.01.2026 15:37] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Training Reasoning Models on Saturated Problems via Failure-Prefix Conditioning Minwu Kim 1 Safal Shrestha 1 Keith RossReinforcement Learning with Verifiable Rewards (RLVR) has substantially improved the reasoning abilities of large language models (LLMs), yet training often stalls as problems become saturated. We identify the core challenge as the poor accessibility of informative failures: learning signals exist but are rarely encountered during standard rollouts. To address this, we propose failure-prefix conditioning, simple and effective method for learning from saturated problems. Rather than starting from the original question, our approach reallocates exploration by conditioning training on prefixes derived from rare incorrect reasoning trajectories, thereby exposing the model to failureprone states. We observe that failure-prefix conditioning yields performance gains matching those of training on medium-difficulty problems, while preserving token efficiency. Furthermore, we analyze the models robustness, finding that our method reduces performance degradation under misleading failure prefixes, albeit with mild trade-off in adherence to correct early reasoning. Finally, we demonstrate that an iterative approach, which refreshes failure prefixes during training, unlocks additional gains after performance plateaus. Overall, our results suggest that failure-prefix conditioning offers an effective pathway to extend RLVR training on saturated problems.1. 6 2 0 2 8 2 ] . [ 1 9 2 8 0 2 . 1 0 6 2 : r 1. Introduction Reinforcement Learning with Verifiable Rewards (RLVR) has substantially improved the reasoning capabilities of large language models (LLMs) (Lambert et al., 2025; Guo 1New York University Abu Dhabi. Correspondence to: Minwu Kim <mwk300@nyu.edu>. Preprint. January 29, 2026. 1Code available at https://github.com/minwukim/ training-on-saturated-problems 1 Figure 1. Illustration of standard GRPO training and failure-prefix conditioning on saturated problems. While standard GRPO predominantly generates correct rollouts, failure-prefix conditioning exposes the model to failure-prone reasoning states, making informative failures more accessible. et al., 2025; OpenAI, 2024). However, as models improve, an increasing number of training problems become saturated, meaning the model solves them correctly in nearly all rollouts. On these problems, training often stalls because rewards become nearly deterministic, leaving little learning signals. Importantly, saturation does not imply that the learning signal is exhausted. Even on problems with near-perfect rollout accuracy, incorrect reasoning trajectories still exist, albeit extremely rarely under standard sampling. As established in prior work, such informative failures play crucial role in training (Setlur et al., 2025; Zhu et al., 2025b). However, most of the sampling budget is spent generating redundant correct solutions, with failures becoming exceedingly sparse (Wang et al., 2025). This suggests that the core challenge in learning from satTraining Reasoning Models on Saturated Problems via Failure-Prefix Conditioning urated problems is not the absence of informative failures, but their poor accessibility. If failures could be encountered more frequently, RLVR could continue to make progress even on saturated problems. To this end, we propose failure-prefix conditioning, simple and effective method for learning from saturated problems in RLVR. As shown in Figure 1, instead of sampling from the original question, this method explicitly targets failureprone reasoning states. We identify rare incorrect rollouts from saturated questions, slice them into prefixes, and select the specific prefix lengths that maximize the learning signal (i.e., target accuracy œÑ = 0.5) (Li et al., 2025). Training on the resulting failure-prefix-conditioned dataset reallocates exploration toward failure-prone regions of the response space, enabling effective learning from saturated problems. We validate the effectiveness of this approach through an in-depth experimental study. Using DeepSeek-R1-DistillQwen-1.5B (Guo et al., 2025) as our base model, we identify 1,000 saturated math questions where the model achieves rollout accuracy 31/32 ( 97%). We apply failure-prefix conditioning to these questions and train our failure-prefix model using RLVR on the resulting prefix-conditioned dataset. We compare our approach against two baselines: (i) the saturate model, trained via standard RLVR on the same 1,000 saturated questions without prefix conditioning, and (ii) the medium model, trained with standard RLVR on 1,000 medium-difficulty questions with rollout accuracy close to 50%, the regime commonly regarded as maximizes the learning signal (Zhan et al., 2025; Li et al., 2025). We evaluate these models on various math reasoning benchmarks spanning wide range of difficulty. The saturate model shows negligible improvement relative to the base model, confirming that standard RLVR training stalls on saturated questions. In contrast, training on the failure-prefixconditioned dataset yields consistent and substantial gains over the base model across all benchmarks. This performance is on par with that of the medium model, demonstrating that failure-prefix conditioning can effectively recover learning signal from saturated problems. Notably, these improvements are achieved without inflating response length, indicating that our methodology preserves token efficiency. We also demonstrate that our approach is robust to the target accuracy hyperparameter œÑ ; while our default œÑ = 0.5 yields the best performance, ablation studies confirm that the method achieves comparable peak performance across range of values. Next, we analyze why failure-prefix conditioning is effective. While standard RLVR primarily incentivizes better decision-making from the initial state, failure-prefix conditioning explicitly trains the model to recover from incorrect intermediate reasoning states. Consequently, the method promotes robustness to misleading partial trajectories early in the reasoning process. Empirically, we find that the failure-prefix model exhibits greater robustness to failure prefixes, showing smaller drop in rollout accuracy compared to baseline methods. Additionally, we also observe mild trade-off, where improved robustness to failure prefixes can occasionally reduce adherence to correct intermediate reasoning. Finally, we explore whether learning from saturated pro"
[29.01.2026 15:37] Mistral response. {"id": "7095fca2f5bb4870a3cbdff4867d835e", "created": 1769701064, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1371, "total_tokens": 1383, "completion_tokens": 12, "num_cached_tokens": 1370}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\"New York University Abu Dhabi\"]\n```"}}]}
[29.01.2026 15:37] Response: ```python
["New York University Abu Dhabi"]
```
[29.01.2026 15:37] Deleting PDF ./assets/pdf/2601.20829.pdf.
[29.01.2026 15:37] Success.
[29.01.2026 15:37] Downloading and parsing paper https://huggingface.co/papers/2601.20618.
[29.01.2026 15:37] Extra JSON file exists (./assets/json/2601.20618.json), skip PDF parsing.
[29.01.2026 15:37] Paper image links file exists (./assets/img_data/2601.20618.json), skip HTML parsing.
[29.01.2026 15:37] Success.
[29.01.2026 15:37] Downloading and parsing paper https://huggingface.co/papers/2601.19194.
[29.01.2026 15:37] Extra JSON file exists (./assets/json/2601.19194.json), skip PDF parsing.
[29.01.2026 15:37] Paper image links file exists (./assets/img_data/2601.19194.json), skip HTML parsing.
[29.01.2026 15:37] Success.
[29.01.2026 15:37] Downloading and parsing paper https://huggingface.co/papers/2601.17950.
[29.01.2026 15:37] Extra JSON file exists (./assets/json/2601.17950.json), skip PDF parsing.
[29.01.2026 15:37] Paper image links file exists (./assets/img_data/2601.17950.json), skip HTML parsing.
[29.01.2026 15:37] Success.
[29.01.2026 15:37] Downloading and parsing paper https://huggingface.co/papers/2601.20622.
[29.01.2026 15:37] Extra JSON file exists (./assets/json/2601.20622.json), skip PDF parsing.
[29.01.2026 15:37] Paper image links file exists (./assets/img_data/2601.20622.json), skip HTML parsing.
[29.01.2026 15:37] Success.
[29.01.2026 15:37] Downloading and parsing paper https://huggingface.co/papers/2601.20262.
[29.01.2026 15:37] Extra JSON file exists (./assets/json/2601.20262.json), skip PDF parsing.
[29.01.2026 15:37] Paper image links file exists (./assets/img_data/2601.20262.json), skip HTML parsing.
[29.01.2026 15:37] Success.
[29.01.2026 15:37] Enriching papers with extra data.
[29.01.2026 15:37] ********************************************************************************
[29.01.2026 15:37] Abstract 0. MathForge enhances mathematical reasoning in large models through a dual framework combining difficulty-aware policy optimization and multi-aspect question reformulation to address limitations in existing reinforcement learning methods.  					AI-generated summary 				 Reinforcement Learning with Ver...
[29.01.2026 15:37] ********************************************************************************
[29.01.2026 15:37] Abstract 1. LingBot-World is an open-source world simulator with high-fidelity dynamics, long-term memory capabilities, and real-time interactivity for diverse environments.  					AI-generated summary 				 We present LingBot-World, an open-sourced world simulator stemming from video generation. Positioned as a ...
[29.01.2026 15:37] ********************************************************************************
[29.01.2026 15:37] Abstract 2. Innovator-VL demonstrates that principled training design and transparent methodology can achieve strong scientific intelligence with reduced data requirements while maintaining general vision performance.  					AI-generated summary 				 We present Innovator-VL, a scientific multimodal large languag...
[29.01.2026 15:37] ********************************************************************************
[29.01.2026 15:37] Abstract 3. DeepSeek-OCR 2 introduces DeepEncoder V2 that dynamically reorders visual tokens based on semantic content, enabling more human-like causal reasoning in 2D image understanding through cascaded 1D causal structures.  					AI-generated summary 				 We present DeepSeek-OCR 2 to investigate the feasibil...
[29.01.2026 15:37] ********************************************************************************
[29.01.2026 15:37] Abstract 4. Spark is a reinforcement learning framework that strategically allocates computational resources by branching at critical decision states, improving sample efficiency and generalization for long-horizon tasks.  					AI-generated summary 				 Reinforcement learning has empowered large language models...
[29.01.2026 15:37] ********************************************************************************
[29.01.2026 15:37] Abstract 5. Linear representation directions in language models dynamically shift during conversations, affecting how factual information is encoded while preserving generic content, with implications for interpretability and context-adaptive model behavior.  					AI-generated summary 				 Language model repres...
[29.01.2026 15:37] ********************************************************************************
[29.01.2026 15:37] Abstract 6. Soft-Verified Efficient Repository Agents (SERA) enables cost-effective training of coding agents through supervised fine-tuning, achieving state-of-the-art performance while enabling specialization to private codebases at a fraction of the cost of previous methods.  					AI-generated summary 				 O...
[29.01.2026 15:37] ********************************************************************************
[29.01.2026 15:37] Abstract 7. OmegaUse is a general-purpose GUI agent model that achieves state-of-the-art performance on mobile and desktop platforms through a combination of high-quality data construction, decoupled training methods, and a Mixture-of-Experts architecture.  					AI-generated summary 				 Graphical User Interfac...
[29.01.2026 15:37] ********************************************************************************
[29.01.2026 15:37] Abstract 8. Self-Distillation Policy Optimization (SDPO) enhances reinforcement learning with verifiable rewards by utilizing rich textual feedback to improve sample efficiency and accuracy in language model training.  					AI-generated summary 				 Large language models are increasingly post-trained with reinf...
[29.01.2026 15:37] ********************************************************************************
[29.01.2026 15:37] Abstract 9. A large-scale reverberant speech corpus with detailed acoustic annotations is introduced to facilitate standardized comparison and reproduction of speech processing research.  					AI-generated summary 				 Despite decades of research on reverberant speech, comparing methods remains difficult becaus...
[29.01.2026 15:37] ********************************************************************************
[29.01.2026 15:37] Abstract 10. Failure-prefix conditioning enables more effective reinforcement learning from saturated problems by focusing exploration on informative failure trajectories, maintaining token efficiency while improving robustness.  					AI-generated summary 				 Reinforcement Learning with Verifiable Rewards (RLVR...
[29.01.2026 15:37] ********************************************************************************
[29.01.2026 15:37] Abstract 11. A multimodal sarcasm detection approach uses generative models to create stable semantic anchors and measures cross-modal discrepancies for improved accuracy and robustness.  					AI-generated summary 				 Multimodal sarcasm detection (MSD) aims to identify sarcasm within image-text pairs by modelin...
[29.01.2026 15:37] ********************************************************************************
[29.01.2026 15:37] Abstract 12. SE-DiCoW improves speaker-attributed ASR performance by using diarization output to identify enrollment segments for fixed conditioning in cross-attention layers, achieving significant reductions in transcription error rates.  					AI-generated summary 				 Speaker-attributed automatic speech recogn...
[29.01.2026 15:37] ********************************************************************************
[29.01.2026 15:37] Abstract 13. Iterative upsampling methods can match cross-attention-based approaches while achieving lower inference costs through the proposed UPLiFT architecture with Local Attender operator for dense feature generation.  					AI-generated summary 				 The space of task-agnostic feature upsampling has emerged ...
[29.01.2026 15:37] ********************************************************************************
[29.01.2026 15:37] Abstract 14. Free-form sketching enables intuitive dynamic intent communication for automated content creation, bridging human intention and digital output in animation workflows.  					AI-generated summary 				 Sketching provides an intuitive way to convey dynamic intent in animation authoring (i.e., how elemen...
[29.01.2026 15:37] ********************************************************************************
[29.01.2026 15:37] Abstract 15. A knowledge distillation framework called Shallow-pi is presented that reduces transformer depth in vision-language-action models, achieving faster inference with minimal performance loss in real-world robotic applications.  					AI-generated summary 				 The growing demand for real-time robotic dep...
[29.01.2026 15:37] Read previous papers.
[29.01.2026 15:37] Generating reviews via LLM API.
[29.01.2026 15:37] Using data from previous issue: {"categories": ["#rl", "#optimization", "#open_source", "#training", "#math", "#reasoning", "#data"], "emoji": "üßÆ", "ru": {"title": "–¶–µ–ª–µ–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö –¥–ª—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è LLM", "desc": "MathForge –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –¥–≤–æ–π–Ω–æ–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥
[29.01.2026 15:37] Using data from previous issue: {"categories": ["#video", "#open_source", "#games", "#robotics", "#agents"], "emoji": "üåç", "ru": {"title": "–û—Ç–∫—Ä—ã—Ç—ã–π world model –¥–ª—è –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –≤–∏—Ä—Ç—É–∞–ª—å–Ω—ã–º–∏ –º–∏—Ä–∞–º–∏ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏", "desc": "LingBot-World ‚Äî —ç—Ç–æ –æ—Ç–∫—Ä—ã—Ç—ã–π —Å–∏–º—É–ª—è—Ç–æ—Ä –æ–∫—Ä—É–∂–µ–Ω–∏—è, –ø–æ—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–æ–¥–µ–ª–µ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤
[29.01.2026 15:37] Using data from previous issue: {"categories": ["#benchmark", "#open_source", "#training", "#multimodal", "#science", "#synthetic", "#reasoning", "#data"], "emoji": "üî¨", "ru": {"title": "–ù–∞—É—á–Ω—ã–π –ò–ò –±–µ–∑ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è: –∫–∞—á–µ—Å—Ç–≤–æ —á–µ—Ä–µ–∑ –ø—Ä–∏–Ω—Ü–∏–ø—ã, –∞ –Ω–µ –æ–±—ä—ë–º", "desc": "Innovator-VL ‚Äî —ç—Ç–æ –Ω–∞—É—á–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å, —Ä–∞–∑—Ä–∞–±
[29.01.2026 15:37] Using data from previous issue: {"categories": ["#open_source", "#multimodal", "#architecture", "#reasoning", "#cv"], "emoji": "üëÅÔ∏è", "ru": {"title": "–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ –ø–µ—Ä–µ—É–ø–æ—Ä—è–¥–æ—á–µ–Ω–∏–µ: –æ—Ç —Ä–∞—Å—Ç—Ä–æ–≤–æ–π —Ä–∞–∑–≤—ë—Ä—Ç–∫–∏ –∫ –ø—Ä–∏—á–∏–Ω–Ω–æ–º—É –∑—Ä–µ–Ω–∏—é", "desc": "DeepSeek-OCR 2 –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π —ç–Ω–∫–æ–¥–µ—Ä DeepEncoder V2, –∫–æ—Ç–æ—Ä—ã–π –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –ø–µ—Ä–µ—É–ø–æ—Ä—è–¥–æ—á–∏–≤–∞–µ—Ç 
[29.01.2026 15:37] Using data from previous issue: {"categories": ["#rl", "#training", "#robotics", "#agents"], "emoji": "üå≥", "ru": {"title": "–£–º–Ω–æ–µ –≤–µ—Ç–≤–ª–µ–Ω–∏–µ –≤ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –º–æ–º–µ–Ω—Ç–∞—Ö –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è", "desc": "Spark ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –∫–æ—Ç–æ—Ä—ã–π —É–º–Ω–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—è–µ—Ç –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã –º–µ–∂–¥—É –∫–ª—é—á–µ–≤—ã–º–∏ —Å–æ—Å—Ç–æ—è–Ω–∏—è–º–∏
[29.01.2026 15:37] Using data from previous issue: {"categories": ["#architecture", "#interpretability"], "emoji": "üß†", "ru": {"title": "–î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–µ —Å–º–µ—â–µ–Ω–∏—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π: –∫–∞–∫ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –ø–µ—Ä–µ–∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É—é—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –¥–∏–∞–ª–æ–≥–∞", "desc": "–í —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è, –∫–∞–∫ –ª–∏–Ω–µ–π–Ω—ã–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –≤ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –º–µ–Ω
[29.01.2026 15:37] Using data from previous issue: {"categories": ["#optimization", "#open_source", "#training", "#dataset", "#plp", "#synthetic", "#agents"], "emoji": "ü§ñ", "ru": {"title": "–î–µ—à—ë–≤–∞—è —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–¥–æ–≤—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –ø—Ä–∏–≤–∞—Ç–Ω—ã–µ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏", "desc": "–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç SERA ‚Äî –º–µ—Ç–æ–¥ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∫–æ–¥–æ–≤—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ supervised fi
[29.01.2026 15:37] Using data from previous issue: {"categories": ["#benchmark", "#optimization", "#training", "#dataset", "#rlhf", "#architecture", "#synthetic", "#agents"], "emoji": "ü§ñ", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∞–≥–µ–Ω—Ç –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –ª—é–±—ã—Ö –∑–∞–¥–∞—á –≤ –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–æ–º –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–µ", "desc": "OmegaUse ‚Äî —ç—Ç–æ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∞–≥–µ–Ω—Ç –¥–ª—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å –≥—Ä–∞
[29.01.2026 15:37] Using data from previous issue: {"categories": ["#plp", "#optimization", "#rlhf", "#rl", "#reasoning", "#training", "#science"], "emoji": "üß†", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ —á–µ—Ä–µ–∑ —Å–∞–º–æ–∫—Ä–∏—Ç–∏–∫—É: –∫–∞–∫ LLM —É—á–∏—Ç—Å—è –Ω–∞ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ—à–∏–±–∫–∞—Ö", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –º–µ—Ç–æ–¥ Self-Distillation Policy Optimization (SDPO), –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç –æ–±—É—á–µ
[29.01.2026 15:37] Using data from previous issue: {"categories": ["#benchmark", "#dataset", "#audio", "#open_source"], "emoji": "üé§", "ru": {"title": "–°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏ –∫ —Ä–µ–≤–µ—Ä–±–µ—Ä–∞—Ü–∏–∏ –≤ –∑–∞–¥–∞—á–∞—Ö —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ—á–∏", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ RIR-Mega-Speech ‚Äî –±–æ–ª—å—à–∞—è —Ç–µ—Å—Ç–æ–≤–∞—è –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö —Å –ø—Ä–∏–º–µ—Ä–Ω–æ 117,5 —á–∞—Å
[29.01.2026 15:37] Querying the API.
[29.01.2026 15:37] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Failure-prefix conditioning enables more effective reinforcement learning from saturated problems by focusing exploration on informative failure trajectories, maintaining token efficiency while improving robustness.  					AI-generated summary 				 Reinforcement Learning with Verifiable Rewards (RLVR) has substantially improved the reasoning abilities of large language models (LLMs), yet training often stalls as problems become saturated. We identify the core challenge as the poor accessibility of informative failures: learning signals exist but are rarely encountered during standard rollouts. To address this, we propose failure-prefix conditioning, a simple and effective method for learning from saturated problems. Rather than starting from the original question, our approach reallocates exploration by conditioning training on prefixes derived from rare incorrect reasoning trajectories, thereby exposing the model to failure-prone states. We observe that failure-prefix conditioning yields performance gains matching those of training on medium-difficulty problems, while preserving token efficiency. Furthermore, we analyze the model's robustness, finding that our method reduces performance degradation under misleading failure prefixes, albeit with a mild trade-off in adherence to correct early reasoning. Finally, we demonstrate that an iterative approach, which refreshes failure prefixes during training, unlocks additional gains after performance plateaus. Overall, our results suggest that failure-prefix conditioning offers an effective pathway to extend RLVR training on saturated problems.
[29.01.2026 15:37] Response: ```json
{
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –º–µ—Ç–æ–¥ failure-prefix conditioning –¥–ª—è –ø—Ä–µ–æ–¥–æ–ª–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º—ã –Ω–∞—Å—ã—â–µ–Ω–∏—è –≤ –æ–±—É—á–µ–Ω–∏–∏ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –°—É—Ç—å –ø–æ–¥—Ö–æ–¥–∞ –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ –ø–µ—Ä–µ–æ—Ä–∏–µ–Ω—Ç–∞—Ü–∏–∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞ –ø—É—Ç—ë–º –∫–æ–Ω–¥–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞ –ø—Ä–µ—Ñ–∏–∫—Å–∞—Ö –∏–∑ —Ä–µ–¥–∫–∏—Ö –æ—à–∏–±–æ—á–Ω—ã—Ö —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ —Å—Ç–æ–ª–∫–∏–≤–∞—Ç—å—Å—è —Å —Å–æ—Å—Ç–æ—è–Ω–∏—è–º–∏, —Å–∫–ª–æ–Ω–Ω—ã–º–∏ –∫ –æ—à–∏–±–∫–∞–º. –ú–µ—Ç–æ–¥ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –ø—Ä–∏—Ä–æ—Å—Ç–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—é –Ω–∞ –∑–∞–¥–∞—á–∞—Ö —Å—Ä–µ–¥–Ω–µ–π —Å–ª–æ–∂–Ω–æ—Å—Ç–∏, –ø—Ä–∏ —ç—Ç–æ–º —Å–æ—Ö—Ä–∞–Ω—è—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤. –ê–≤—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç, —á—Ç–æ –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–µ—Ñ–∏–∫—Å–æ–≤ –æ—à–∏–±–æ–∫ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å —É–ª—É—á—à–µ–Ω–∏–µ –ø–æ—Å–ª–µ –≤—ã—Ö–æ–¥–∞ –Ω–∞ –ø–ª–∞—Ç–æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.",
  "emoji": "üéØ",
  "title": "–û–±—É—á–µ–Ω–∏–µ —á–µ—Ä–µ–∑ –æ—à–∏–±–∫–∏: –ø—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ –Ω–∞—Å—ã—â–µ–Ω–∏—è –≤ –æ–±—É—á–µ–Ω–∏–∏ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º"
}
```
[29.01.2026 15:37] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Failure-prefix conditioning enables more effective reinforcement learning from saturated problems by focusing exploration on informative failure trajectories, maintaining token efficiency while improving robustness.  					AI-generated summary 				 Reinforcement Learning with Verifiable Rewards (RLVR) has substantially improved the reasoning abilities of large language models (LLMs), yet training often stalls as problems become saturated. We identify the core challenge as the poor accessibility of informative failures: learning signals exist but are rarely encountered during standard rollouts. To address this, we propose failure-prefix conditioning, a simple and effective method for learning from saturated problems. Rather than starting from the original question, our approach reallocates exploration by conditioning training on prefixes derived from rare incorrect reasoning trajectories, thereby exposing the model to failure-prone states. We observe that failure-prefix conditioning yields performance gains matching those of training on medium-difficulty problems, while preserving token efficiency. Furthermore, we analyze the model's robustness, finding that our method reduces performance degradation under misleading failure prefixes, albeit with a mild trade-off in adherence to correct early reasoning. Finally, we demonstrate that an iterative approach, which refreshes failure prefixes during training, unlocks additional gains after performance plateaus. Overall, our results suggest that failure-prefix conditioning offers an effective pathway to extend RLVR training on saturated problems."

[29.01.2026 15:37] Response: ```python
["RL", "TRAINING"]
```
[29.01.2026 15:37] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Failure-prefix conditioning enables more effective reinforcement learning from saturated problems by focusing exploration on informative failure trajectories, maintaining token efficiency while improving robustness.  					AI-generated summary 				 Reinforcement Learning with Verifiable Rewards (RLVR) has substantially improved the reasoning abilities of large language models (LLMs), yet training often stalls as problems become saturated. We identify the core challenge as the poor accessibility of informative failures: learning signals exist but are rarely encountered during standard rollouts. To address this, we propose failure-prefix conditioning, a simple and effective method for learning from saturated problems. Rather than starting from the original question, our approach reallocates exploration by conditioning training on prefixes derived from rare incorrect reasoning trajectories, thereby exposing the model to failure-prone states. We observe that failure-prefix conditioning yields performance gains matching those of training on medium-difficulty problems, while preserving token efficiency. Furthermore, we analyze the model's robustness, finding that our method reduces performance degradation under misleading failure prefixes, albeit with a mild trade-off in adherence to correct early reasoning. Finally, we demonstrate that an iterative approach, which refreshes failure prefixes during training, unlocks additional gains after performance plateaus. Overall, our results suggest that failure-prefix conditioning offers an effective pathway to extend RLVR training on saturated problems."

[29.01.2026 15:37] Response: ```python
["REASONING", "OPTIMIZATION"]
```
[29.01.2026 15:37] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces failure-prefix conditioning as a method to enhance reinforcement learning in scenarios where problems are saturated. The approach focuses on exploring informative failure trajectories, which are often overlooked during standard training. By conditioning the model on prefixes from incorrect reasoning paths, it allows the model to learn from its mistakes more effectively. The results show that this method improves performance while maintaining token efficiency and robustness against misleading failure signals.","title":"Unlocking Learning from Mistakes in Reinforcement Learning"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper introduces failure-prefix conditioning as a method to enhance reinforcement learning in scenarios where problems are saturated. The approach focuses on exploring informative failure trajectories, which are often overlooked during standard training. By conditioning the model on prefixes from incorrect reasoning paths, it allows the model to learn from its mistakes more effectively. The results show that this method improves performance while maintaining token efficiency and robustness against misleading failure signals.', title='Unlocking Learning from Mistakes in Reinforcement Learning'))
[29.01.2026 15:37] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫Â§±Ë¥•ÂâçÁºÄÊù°‰ª∂ÂåñÁöÑÊñπÊ≥ïÔºå‰ª•ÊèêÈ´òÂº∫ÂåñÂ≠¶‰π†Âú®È•±ÂíåÈóÆÈ¢ò‰∏äÁöÑÊúâÊïàÊÄß„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÂÖ≥Ê≥®‰ø°ÊÅØ‰∏∞ÂØåÁöÑÂ§±Ë¥•ËΩ®ËøπÔºåÈáçÊñ∞ÂàÜÈÖçÊé¢Á¥¢Ôºå‰ªéËÄå‰ΩøÊ®°ÂûãËÉΩÂ§üÊé•Ëß¶Âà∞ÂÆπÊòìÂ§±Ë¥•ÁöÑÁä∂ÊÄÅ„ÄÇÂÆûÈ™åË°®ÊòéÔºåÂ§±Ë¥•ÂâçÁºÄÊù°‰ª∂ÂåñÂú®‰øùÊåÅ‰ª§ÁâåÊïàÁéáÁöÑÂêåÊó∂ÔºåËÉΩÂ§üËé∑Âæó‰∏é‰∏≠Á≠âÈöæÂ∫¶ÈóÆÈ¢òÁõ∏ÂΩìÁöÑÊÄßËÉΩÊèêÂçá„ÄÇÊ≠§Â§ñÔºåËø≠‰ª£Êõ¥Êñ∞Â§±Ë¥•ÂâçÁºÄÁöÑËÆ≠ÁªÉÊñπÊ≥ïËøõ‰∏ÄÊ≠•Ëß£ÈîÅ‰∫ÜÊÄßËÉΩÁöÑÊèêÂçá„ÄÇ","title":"Â§±Ë¥•ÂâçÁºÄÊù°‰ª∂ÂåñÔºöÂº∫ÂåñÂ≠¶‰π†ÁöÑÊñ∞Ë∑ØÂæÑ"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫Â§±Ë¥•ÂâçÁºÄÊù°‰ª∂ÂåñÁöÑÊñπÊ≥ïÔºå‰ª•ÊèêÈ´òÂº∫ÂåñÂ≠¶‰π†Âú®È•±ÂíåÈóÆÈ¢ò‰∏äÁöÑÊúâÊïàÊÄß„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÂÖ≥Ê≥®‰ø°ÊÅØ‰∏∞ÂØåÁöÑÂ§±Ë¥•ËΩ®ËøπÔºåÈáçÊñ∞ÂàÜÈÖçÊé¢Á¥¢Ôºå‰ªéËÄå‰ΩøÊ®°ÂûãËÉΩÂ§üÊé•Ëß¶Âà∞ÂÆπÊòìÂ§±Ë¥•ÁöÑÁä∂ÊÄÅ„ÄÇÂÆûÈ™åË°®ÊòéÔºåÂ§±Ë¥•ÂâçÁºÄÊù°‰ª∂ÂåñÂú®‰øùÊåÅ‰ª§ÁâåÊïàÁéáÁöÑÂêåÊó∂ÔºåËÉΩÂ§üËé∑Âæó‰∏é‰∏≠Á≠âÈöæÂ∫¶ÈóÆÈ¢òÁõ∏ÂΩìÁöÑÊÄßËÉΩÊèêÂçá„ÄÇÊ≠§Â§ñÔºåËø≠‰ª£Êõ¥Êñ∞Â§±Ë¥•ÂâçÁºÄÁöÑËÆ≠ÁªÉÊñπÊ≥ïËøõ‰∏ÄÊ≠•Ëß£ÈîÅ‰∫ÜÊÄßËÉΩÁöÑÊèêÂçá„ÄÇ', title='Â§±Ë¥•ÂâçÁºÄÊù°‰ª∂ÂåñÔºöÂº∫ÂåñÂ≠¶‰π†ÁöÑÊñ∞Ë∑ØÂæÑ'))
[29.01.2026 15:37] Using data from previous issue: {"categories": ["#benchmark", "#multimodal"], "emoji": "üòè", "ru": {"title": "–Ø–∫–æ—Ä—è —Å–º—ã—Å–ª–∞ –ø—Ä–æ—Ç–∏–≤ —Å–∞—Ä–∫–∞–∑–º–∞: –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–º—É –ø–æ–Ω–∏–º–∞–Ω–∏—é", "desc": "–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–º—É –≤—ã—è–≤–ª–µ–Ω–∏—é —Å–∞—Ä–∫–∞–∑–º–∞ –≤ –ø–∞—Ä–∞—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ-—Ç–µ–∫—Å—Ç –ø—É—Ç—ë–º –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–π –º–µ–∂–¥—É
[29.01.2026 15:37] Using data from previous issue: {"categories": ["#benchmark", "#audio", "#multilingual"], "emoji": "üé§", "ru": {"title": "–°–∞–º–æ–≤—ã–±—Ä–∞–Ω–Ω–∞—è –¥–∏–∞—Ä–∏–∑–∞—Ü–∏—è –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ—á–∏ —Å–ø–∏–∫–µ—Ä–æ–≤", "desc": "SE-DiCoW —É–ª—É—á—à–∞–µ—Ç —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏ —Å –∞—Ç—Ä–∏–±—É—Ü–∏–µ–π —Å–ø–∏–∫–µ—Ä–∞ –≤ –º–Ω–æ–≥–æ–≥–æ–≤–æ—Ä—è—â–∏—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è—Ö, –∏—Å–ø–æ–ª—å–∑—É—è –≤—ã—Ö–æ–¥ –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏ –¥–ª—è –≤—ã–±–æ—Ä–∞ —Å–µ–≥–º–µ–Ω—Ç–∞ —Ä
[29.01.2026 15:37] Using data from previous issue: {"categories": [], "emoji": "‚¨ÜÔ∏è", "ru": {"title": "–ë—ã—Å—Ç—Ä–æ–µ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —á–µ—Ä–µ–∑ –ª–æ–∫–∞–ª—å–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç UPLiFT ‚Äî –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —É–≤–µ–ª–∏—á–µ–Ω–∏—è —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω—ã—Ö –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã –≤–≤–æ–¥—è—Ç –æ–ø–µ—Ä–∞—Ç–æ—Ä Local Attender
[29.01.2026 15:37] Using data from previous issue: {"categories": ["#video", "#3d", "#multimodal", "#cv", "#story_generation"], "emoji": "‚úèÔ∏è", "ru": {"title": "–°–≤–æ–±–æ–¥–Ω—ã–π —Ä–∏—Å—É–Ω–æ–∫ –∫–∞–∫ —è–∑—ã–∫ –æ–±—â–µ–Ω–∏—è –º–µ–∂–¥—É —á–µ–ª–æ–≤–µ–∫–æ–º –∏ AI –≤ —Å–æ–∑–¥–∞–Ω–∏–∏ –∞–Ω–∏–º–∞—Ü–∏–∏", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–æ–∑–¥–∞–Ω–∏—é –∞–Ω–∏–º–∞—Ü–∏–∏, –≥–¥–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç —Å–≤–æ–±–æ–¥–Ω—É—é —Ñ–æ—Ä–º—É —Ä–∏—Å–æ–≤
[29.01.2026 15:37] Using data from previous issue: {"categories": ["#robotics", "#training", "#multimodal", "#inference"], "emoji": "‚ö°", "ru": {"title": "–ë—ã—Å—Ç—Ä—ã–µ —Ä–æ–±–æ—Ç—ã: –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –¥–ª—è —Ä–µ–∞–ª—å–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ Shallow-pi –¥–ª—è –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –∑–Ω–∞–Ω–∏–π, –∫–æ—Ç–æ—Ä—ã–π –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–Ω–∏–∂–∞–µ—Ç –≥–ª—É–±–∏–Ω—É —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ –≤ –º–æ–¥–µ–ª—è—Ö
[29.01.2026 15:37] Renaming data file.
[29.01.2026 15:37] Renaming previous data. hf_papers.json to ./d/2026-01-29.json
[29.01.2026 15:37] Saving new data file.
[29.01.2026 15:37] Generating page.
[29.01.2026 15:37] Renaming previous page.
[29.01.2026 15:37] Renaming previous data. index.html to ./d/2026-01-29.html
[29.01.2026 15:37] Writing result.
[29.01.2026 15:37] Renaming log file.
[29.01.2026 15:37] Renaming previous data. log.txt to ./logs/2026-01-29_last_log.txt
