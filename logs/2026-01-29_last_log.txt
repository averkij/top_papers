[29.01.2026 16:45] Read previous papers.
[29.01.2026 16:45] Generating top page (month).
[29.01.2026 16:45] Writing top page (month).
[29.01.2026 17:39] Read previous papers.
[29.01.2026 17:39] Get feed.
[29.01.2026 17:39] Get page data from previous paper. URL: https://huggingface.co/papers/2601.20614
[29.01.2026 17:39] Get page data from previous paper. URL: https://huggingface.co/papers/2601.20540
[29.01.2026 17:39] Get page data from previous paper. URL: https://huggingface.co/papers/2601.19325
[29.01.2026 17:39] Get page data from previous paper. URL: https://huggingface.co/papers/2601.20552
[29.01.2026 17:39] Get page data from previous paper. URL: https://huggingface.co/papers/2601.20209
[29.01.2026 17:39] Get page data from previous paper. URL: https://huggingface.co/papers/2601.20834
[29.01.2026 17:39] Get page data from previous paper. URL: https://huggingface.co/papers/2601.20802
[29.01.2026 17:39] Get page data from previous paper. URL: https://huggingface.co/papers/2601.20789
[29.01.2026 17:39] Get page data from previous paper. URL: https://huggingface.co/papers/2601.20380
[29.01.2026 17:39] Get page data from previous paper. URL: https://huggingface.co/papers/2601.19949
[29.01.2026 17:39] Get page data from previous paper. URL: https://huggingface.co/papers/2601.17950
[29.01.2026 17:39] Get page data from previous paper. URL: https://huggingface.co/papers/2601.20829
[29.01.2026 17:39] Get page data from previous paper. URL: https://huggingface.co/papers/2601.20618
[29.01.2026 17:39] Get page data from previous paper. URL: https://huggingface.co/papers/2601.19194
[29.01.2026 17:39] Get page data from previous paper. URL: https://huggingface.co/papers/2601.20622
[29.01.2026 17:39] Get page data from previous paper. URL: https://huggingface.co/papers/2601.20262
[29.01.2026 17:39] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[29.01.2026 17:39] No deleted papers detected.
[29.01.2026 17:39] Downloading and parsing papers (pdf, html). Total: 16.
[29.01.2026 17:39] Downloading and parsing paper https://huggingface.co/papers/2601.20614.
[29.01.2026 17:39] Extra JSON file exists (./assets/json/2601.20614.json), skip PDF parsing.
[29.01.2026 17:39] Paper image links file exists (./assets/img_data/2601.20614.json), skip HTML parsing.
[29.01.2026 17:39] Success.
[29.01.2026 17:39] Downloading and parsing paper https://huggingface.co/papers/2601.20540.
[29.01.2026 17:39] Extra JSON file exists (./assets/json/2601.20540.json), skip PDF parsing.
[29.01.2026 17:39] Paper image links file exists (./assets/img_data/2601.20540.json), skip HTML parsing.
[29.01.2026 17:39] Success.
[29.01.2026 17:39] Downloading and parsing paper https://huggingface.co/papers/2601.19325.
[29.01.2026 17:39] Extra JSON file exists (./assets/json/2601.19325.json), skip PDF parsing.
[29.01.2026 17:39] Paper image links file exists (./assets/img_data/2601.19325.json), skip HTML parsing.
[29.01.2026 17:39] Success.
[29.01.2026 17:39] Downloading and parsing paper https://huggingface.co/papers/2601.20552.
[29.01.2026 17:39] Extra JSON file exists (./assets/json/2601.20552.json), skip PDF parsing.
[29.01.2026 17:39] Paper image links file exists (./assets/img_data/2601.20552.json), skip HTML parsing.
[29.01.2026 17:39] Success.
[29.01.2026 17:39] Downloading and parsing paper https://huggingface.co/papers/2601.20209.
[29.01.2026 17:39] Extra JSON file exists (./assets/json/2601.20209.json), skip PDF parsing.
[29.01.2026 17:39] Paper image links file exists (./assets/img_data/2601.20209.json), skip HTML parsing.
[29.01.2026 17:39] Success.
[29.01.2026 17:39] Downloading and parsing paper https://huggingface.co/papers/2601.20834.
[29.01.2026 17:39] Extra JSON file exists (./assets/json/2601.20834.json), skip PDF parsing.
[29.01.2026 17:39] Paper image links file exists (./assets/img_data/2601.20834.json), skip HTML parsing.
[29.01.2026 17:39] Success.
[29.01.2026 17:39] Downloading and parsing paper https://huggingface.co/papers/2601.20802.
[29.01.2026 17:39] Extra JSON file exists (./assets/json/2601.20802.json), skip PDF parsing.
[29.01.2026 17:39] Paper image links file exists (./assets/img_data/2601.20802.json), skip HTML parsing.
[29.01.2026 17:39] Success.
[29.01.2026 17:39] Downloading and parsing paper https://huggingface.co/papers/2601.20789.
[29.01.2026 17:39] Extra JSON file exists (./assets/json/2601.20789.json), skip PDF parsing.
[29.01.2026 17:39] Paper image links file exists (./assets/img_data/2601.20789.json), skip HTML parsing.
[29.01.2026 17:39] Success.
[29.01.2026 17:39] Downloading and parsing paper https://huggingface.co/papers/2601.20380.
[29.01.2026 17:39] Extra JSON file exists (./assets/json/2601.20380.json), skip PDF parsing.
[29.01.2026 17:39] Paper image links file exists (./assets/img_data/2601.20380.json), skip HTML parsing.
[29.01.2026 17:39] Success.
[29.01.2026 17:39] Downloading and parsing paper https://huggingface.co/papers/2601.19949.
[29.01.2026 17:39] Extra JSON file exists (./assets/json/2601.19949.json), skip PDF parsing.
[29.01.2026 17:39] Paper image links file exists (./assets/img_data/2601.19949.json), skip HTML parsing.
[29.01.2026 17:39] Success.
[29.01.2026 17:39] Downloading and parsing paper https://huggingface.co/papers/2601.17950.
[29.01.2026 17:39] Extra JSON file exists (./assets/json/2601.17950.json), skip PDF parsing.
[29.01.2026 17:39] Paper image links file exists (./assets/img_data/2601.17950.json), skip HTML parsing.
[29.01.2026 17:39] Success.
[29.01.2026 17:39] Downloading and parsing paper https://huggingface.co/papers/2601.20829.
[29.01.2026 17:39] Extra JSON file exists (./assets/json/2601.20829.json), skip PDF parsing.
[29.01.2026 17:39] Paper image links file exists (./assets/img_data/2601.20829.json), skip HTML parsing.
[29.01.2026 17:39] Success.
[29.01.2026 17:39] Downloading and parsing paper https://huggingface.co/papers/2601.20618.
[29.01.2026 17:39] Extra JSON file exists (./assets/json/2601.20618.json), skip PDF parsing.
[29.01.2026 17:39] Paper image links file exists (./assets/img_data/2601.20618.json), skip HTML parsing.
[29.01.2026 17:39] Success.
[29.01.2026 17:39] Downloading and parsing paper https://huggingface.co/papers/2601.19194.
[29.01.2026 17:39] Extra JSON file exists (./assets/json/2601.19194.json), skip PDF parsing.
[29.01.2026 17:39] Paper image links file exists (./assets/img_data/2601.19194.json), skip HTML parsing.
[29.01.2026 17:39] Success.
[29.01.2026 17:39] Downloading and parsing paper https://huggingface.co/papers/2601.20622.
[29.01.2026 17:39] Extra JSON file exists (./assets/json/2601.20622.json), skip PDF parsing.
[29.01.2026 17:39] Paper image links file exists (./assets/img_data/2601.20622.json), skip HTML parsing.
[29.01.2026 17:39] Success.
[29.01.2026 17:39] Downloading and parsing paper https://huggingface.co/papers/2601.20262.
[29.01.2026 17:39] Extra JSON file exists (./assets/json/2601.20262.json), skip PDF parsing.
[29.01.2026 17:39] Paper image links file exists (./assets/img_data/2601.20262.json), skip HTML parsing.
[29.01.2026 17:39] Success.
[29.01.2026 17:39] Enriching papers with extra data.
[29.01.2026 17:39] ********************************************************************************
[29.01.2026 17:39] Abstract 0. MathForge enhances mathematical reasoning in large models through a dual framework combining difficulty-aware policy optimization and multi-aspect question reformulation to address limitations in existing reinforcement learning methods.  					AI-generated summary 				 Reinforcement Learning with Ver...
[29.01.2026 17:39] ********************************************************************************
[29.01.2026 17:39] Abstract 1. LingBot-World is an open-source world simulator with high-fidelity dynamics, long-term memory capabilities, and real-time interactivity for diverse environments.  					AI-generated summary 				 We present LingBot-World, an open-sourced world simulator stemming from video generation. Positioned as a ...
[29.01.2026 17:39] ********************************************************************************
[29.01.2026 17:39] Abstract 2. Innovator-VL demonstrates that principled training design and transparent methodology can achieve strong scientific intelligence with reduced data requirements while maintaining general vision performance.  					AI-generated summary 				 We present Innovator-VL, a scientific multimodal large languag...
[29.01.2026 17:39] ********************************************************************************
[29.01.2026 17:39] Abstract 3. DeepSeek-OCR 2 introduces DeepEncoder V2 that dynamically reorders visual tokens based on semantic content, enabling more human-like causal reasoning in 2D image understanding through cascaded 1D causal structures.  					AI-generated summary 				 We present DeepSeek-OCR 2 to investigate the feasibil...
[29.01.2026 17:39] ********************************************************************************
[29.01.2026 17:39] Abstract 4. Spark is a reinforcement learning framework that strategically allocates computational resources by branching at critical decision states, improving sample efficiency and generalization for long-horizon tasks.  					AI-generated summary 				 Reinforcement learning has empowered large language models...
[29.01.2026 17:39] ********************************************************************************
[29.01.2026 17:39] Abstract 5. Linear representation directions in language models dynamically shift during conversations, affecting how factual information is encoded while preserving generic content, with implications for interpretability and context-adaptive model behavior.  					AI-generated summary 				 Language model repres...
[29.01.2026 17:39] ********************************************************************************
[29.01.2026 17:39] Abstract 6. Self-Distillation Policy Optimization (SDPO) enhances reinforcement learning with verifiable rewards by utilizing rich textual feedback to improve sample efficiency and accuracy in language model training.  					AI-generated summary 				 Large language models are increasingly post-trained with reinf...
[29.01.2026 17:39] ********************************************************************************
[29.01.2026 17:39] Abstract 7. Soft-Verified Efficient Repository Agents (SERA) enables cost-effective training of coding agents through supervised fine-tuning, achieving state-of-the-art performance while enabling specialization to private codebases at a fraction of the cost of previous methods.  					AI-generated summary 				 O...
[29.01.2026 17:39] ********************************************************************************
[29.01.2026 17:39] Abstract 8. OmegaUse is a general-purpose GUI agent model that achieves state-of-the-art performance on mobile and desktop platforms through a combination of high-quality data construction, decoupled training methods, and a Mixture-of-Experts architecture.  					AI-generated summary 				 Graphical User Interfac...
[29.01.2026 17:39] ********************************************************************************
[29.01.2026 17:39] Abstract 9. A large-scale reverberant speech corpus with detailed acoustic annotations is introduced to facilitate standardized comparison and reproduction of speech processing research.  					AI-generated summary 				 Despite decades of research on reverberant speech, comparing methods remains difficult becaus...
[29.01.2026 17:39] ********************************************************************************
[29.01.2026 17:39] Abstract 10. Iterative upsampling methods can match cross-attention-based approaches while achieving lower inference costs through the proposed UPLiFT architecture with Local Attender operator for dense feature generation.  					AI-generated summary 				 The space of task-agnostic feature upsampling has emerged ...
[29.01.2026 17:39] ********************************************************************************
[29.01.2026 17:39] Abstract 11. Failure-prefix conditioning enables more effective reinforcement learning from saturated problems by focusing exploration on informative failure trajectories, maintaining token efficiency while improving robustness.  					AI-generated summary 				 Reinforcement Learning with Verifiable Rewards (RLVR...
[29.01.2026 17:39] ********************************************************************************
[29.01.2026 17:39] Abstract 12. A multimodal sarcasm detection approach uses generative models to create stable semantic anchors and measures cross-modal discrepancies for improved accuracy and robustness.  					AI-generated summary 				 Multimodal sarcasm detection (MSD) aims to identify sarcasm within image-text pairs by modelin...
[29.01.2026 17:39] ********************************************************************************
[29.01.2026 17:39] Abstract 13. SE-DiCoW improves speaker-attributed ASR performance by using diarization output to identify enrollment segments for fixed conditioning in cross-attention layers, achieving significant reductions in transcription error rates.  					AI-generated summary 				 Speaker-attributed automatic speech recogn...
[29.01.2026 17:39] ********************************************************************************
[29.01.2026 17:39] Abstract 14. Free-form sketching enables intuitive dynamic intent communication for automated content creation, bridging human intention and digital output in animation workflows.  					AI-generated summary 				 Sketching provides an intuitive way to convey dynamic intent in animation authoring (i.e., how elemen...
[29.01.2026 17:39] ********************************************************************************
[29.01.2026 17:39] Abstract 15. A knowledge distillation framework called Shallow-pi is presented that reduces transformer depth in vision-language-action models, achieving faster inference with minimal performance loss in real-world robotic applications.  					AI-generated summary 				 The growing demand for real-time robotic dep...
[29.01.2026 17:39] Read previous papers.
[29.01.2026 17:39] Generating reviews via LLM API.
[29.01.2026 17:39] Using data from previous issue: {"categories": ["#rl", "#optimization", "#open_source", "#training", "#math", "#reasoning", "#data"], "emoji": "üßÆ", "ru": {"title": "–¶–µ–ª–µ–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö –¥–ª—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è LLM", "desc": "MathForge –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –¥–≤–æ–π–Ω–æ–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥
[29.01.2026 17:39] Using data from previous issue: {"categories": ["#video", "#open_source", "#games", "#robotics", "#agents"], "emoji": "üåç", "ru": {"title": "–û—Ç–∫—Ä—ã—Ç—ã–π world model –¥–ª—è –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –≤–∏—Ä—Ç—É–∞–ª—å–Ω—ã–º–∏ –º–∏—Ä–∞–º–∏ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏", "desc": "LingBot-World ‚Äî —ç—Ç–æ –æ—Ç–∫—Ä—ã—Ç—ã–π —Å–∏–º—É–ª—è—Ç–æ—Ä –æ–∫—Ä—É–∂–µ–Ω–∏—è, –ø–æ—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–æ–¥–µ–ª–µ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤
[29.01.2026 17:39] Using data from previous issue: {"categories": ["#benchmark", "#open_source", "#training", "#multimodal", "#science", "#synthetic", "#reasoning", "#data"], "emoji": "üî¨", "ru": {"title": "–ù–∞—É—á–Ω—ã–π –ò–ò –±–µ–∑ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è: –∫–∞—á–µ—Å—Ç–≤–æ —á–µ—Ä–µ–∑ –ø—Ä–∏–Ω—Ü–∏–ø—ã, –∞ –Ω–µ –æ–±—ä—ë–º", "desc": "Innovator-VL ‚Äî —ç—Ç–æ –Ω–∞—É—á–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å, —Ä–∞–∑—Ä–∞–±
[29.01.2026 17:39] Using data from previous issue: {"categories": ["#open_source", "#multimodal", "#architecture", "#reasoning", "#cv"], "emoji": "üëÅÔ∏è", "ru": {"title": "–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ –ø–µ—Ä–µ—É–ø–æ—Ä—è–¥–æ—á–µ–Ω–∏–µ: –æ—Ç —Ä–∞—Å—Ç—Ä–æ–≤–æ–π —Ä–∞–∑–≤—ë—Ä—Ç–∫–∏ –∫ –ø—Ä–∏—á–∏–Ω–Ω–æ–º—É –∑—Ä–µ–Ω–∏—é", "desc": "DeepSeek-OCR 2 –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π —ç–Ω–∫–æ–¥–µ—Ä DeepEncoder V2, –∫–æ—Ç–æ—Ä—ã–π –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –ø–µ—Ä–µ—É–ø–æ—Ä—è–¥–æ—á–∏–≤–∞–µ—Ç 
[29.01.2026 17:39] Using data from previous issue: {"categories": ["#rl", "#training", "#robotics", "#agents"], "emoji": "üå≥", "ru": {"title": "–£–º–Ω–æ–µ –≤–µ—Ç–≤–ª–µ–Ω–∏–µ –≤ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –º–æ–º–µ–Ω—Ç–∞—Ö –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è", "desc": "Spark ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –∫–æ—Ç–æ—Ä—ã–π —É–º–Ω–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—è–µ—Ç –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã –º–µ–∂–¥—É –∫–ª—é—á–µ–≤—ã–º–∏ —Å–æ—Å—Ç–æ—è–Ω–∏—è–º–∏
[29.01.2026 17:39] Using data from previous issue: {"categories": ["#architecture", "#interpretability"], "emoji": "üß†", "ru": {"title": "–î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–µ —Å–º–µ—â–µ–Ω–∏—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π: –∫–∞–∫ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –ø–µ—Ä–µ–∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É—é—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –¥–∏–∞–ª–æ–≥–∞", "desc": "–í —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è, –∫–∞–∫ –ª–∏–Ω–µ–π–Ω—ã–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –≤ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –º–µ–Ω
[29.01.2026 17:39] Using data from previous issue: {"categories": ["#plp", "#optimization", "#rlhf", "#rl", "#reasoning", "#training", "#science"], "emoji": "üß†", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ —á–µ—Ä–µ–∑ —Å–∞–º–æ–∫—Ä–∏—Ç–∏–∫—É: –∫–∞–∫ LLM —É—á–∏—Ç—Å—è –Ω–∞ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ—à–∏–±–∫–∞—Ö", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –º–µ—Ç–æ–¥ Self-Distillation Policy Optimization (SDPO), –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç –æ–±—É—á–µ
[29.01.2026 17:39] Using data from previous issue: {"categories": ["#optimization", "#open_source", "#training", "#dataset", "#plp", "#synthetic", "#agents"], "emoji": "ü§ñ", "ru": {"title": "–î–µ—à—ë–≤–∞—è —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–¥–æ–≤—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –ø—Ä–∏–≤–∞—Ç–Ω—ã–µ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏", "desc": "–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç SERA ‚Äî –º–µ—Ç–æ–¥ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∫–æ–¥–æ–≤—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ supervised fi
[29.01.2026 17:39] Using data from previous issue: {"categories": ["#benchmark", "#optimization", "#training", "#dataset", "#rlhf", "#architecture", "#synthetic", "#agents"], "emoji": "ü§ñ", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∞–≥–µ–Ω—Ç –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –ª—é–±—ã—Ö –∑–∞–¥–∞—á –≤ –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–æ–º –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–µ", "desc": "OmegaUse ‚Äî —ç—Ç–æ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∞–≥–µ–Ω—Ç –¥–ª—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å –≥—Ä–∞
[29.01.2026 17:39] Using data from previous issue: {"categories": ["#benchmark", "#dataset", "#audio", "#open_source"], "emoji": "üé§", "ru": {"title": "–°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏ –∫ —Ä–µ–≤–µ—Ä–±–µ—Ä–∞—Ü–∏–∏ –≤ –∑–∞–¥–∞—á–∞—Ö —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ—á–∏", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ RIR-Mega-Speech ‚Äî –±–æ–ª—å—à–∞—è —Ç–µ—Å—Ç–æ–≤–∞—è –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö —Å –ø—Ä–∏–º–µ—Ä–Ω–æ 117,5 —á–∞—Å
[29.01.2026 17:39] Using data from previous issue: {"categories": [], "emoji": "‚¨ÜÔ∏è", "ru": {"title": "–ë—ã—Å—Ç—Ä–æ–µ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —á–µ—Ä–µ–∑ –ª–æ–∫–∞–ª—å–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç UPLiFT ‚Äî –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —É–≤–µ–ª–∏—á–µ–Ω–∏—è —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω—ã—Ö –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã –≤–≤–æ–¥—è—Ç –æ–ø–µ—Ä–∞—Ç–æ—Ä Local Attender
[29.01.2026 17:39] Using data from previous issue: {"categories": ["#reasoning", "#rl", "#optimization", "#training"], "emoji": "üéØ", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ —á–µ—Ä–µ–∑ –æ—à–∏–±–∫–∏: –ø—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ –Ω–∞—Å—ã—â–µ–Ω–∏—è –≤ –æ–±—É—á–µ–Ω–∏–∏ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –º–µ—Ç–æ–¥ failure-prefix conditioning –¥–ª—è –ø—Ä–µ–æ–¥–æ–ª–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º—ã –Ω–∞—Å—ã—â–µ–Ω–∏—è –≤ –æ–±—É—á–µ–Ω–∏–∏ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –±–æ–ª—å
[29.01.2026 17:39] Using data from previous issue: {"categories": ["#benchmark", "#multimodal"], "emoji": "üòè", "ru": {"title": "–Ø–∫–æ—Ä—è —Å–º—ã—Å–ª–∞ –ø—Ä–æ—Ç–∏–≤ —Å–∞—Ä–∫–∞–∑–º–∞: –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–º—É –ø–æ–Ω–∏–º–∞–Ω–∏—é", "desc": "–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–º—É –≤—ã—è–≤–ª–µ–Ω–∏—é —Å–∞—Ä–∫–∞–∑–º–∞ –≤ –ø–∞—Ä–∞—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ-—Ç–µ–∫—Å—Ç –ø—É—Ç—ë–º –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–π –º–µ–∂–¥—É
[29.01.2026 17:39] Using data from previous issue: {"categories": ["#benchmark", "#audio", "#multilingual"], "emoji": "üé§", "ru": {"title": "–°–∞–º–æ–≤—ã–±—Ä–∞–Ω–Ω–∞—è –¥–∏–∞—Ä–∏–∑–∞—Ü–∏—è –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ—á–∏ —Å–ø–∏–∫–µ—Ä–æ–≤", "desc": "SE-DiCoW —É–ª—É—á—à–∞–µ—Ç —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏ —Å –∞—Ç—Ä–∏–±—É—Ü–∏–µ–π —Å–ø–∏–∫–µ—Ä–∞ –≤ –º–Ω–æ–≥–æ–≥–æ–≤–æ—Ä—è—â–∏—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è—Ö, –∏—Å–ø–æ–ª—å–∑—É—è –≤—ã—Ö–æ–¥ –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏ –¥–ª—è –≤—ã–±–æ—Ä–∞ —Å–µ–≥–º–µ–Ω—Ç–∞ —Ä
[29.01.2026 17:39] Using data from previous issue: {"categories": ["#video", "#3d", "#multimodal", "#cv", "#story_generation"], "emoji": "‚úèÔ∏è", "ru": {"title": "–°–≤–æ–±–æ–¥–Ω—ã–π —Ä–∏—Å—É–Ω–æ–∫ –∫–∞–∫ —è–∑—ã–∫ –æ–±—â–µ–Ω–∏—è –º–µ–∂–¥—É —á–µ–ª–æ–≤–µ–∫–æ–º –∏ AI –≤ —Å–æ–∑–¥–∞–Ω–∏–∏ –∞–Ω–∏–º–∞—Ü–∏–∏", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–æ–∑–¥–∞–Ω–∏—é –∞–Ω–∏–º–∞—Ü–∏–∏, –≥–¥–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç —Å–≤–æ–±–æ–¥–Ω—É—é —Ñ–æ—Ä–º—É —Ä–∏—Å–æ–≤
[29.01.2026 17:39] Using data from previous issue: {"categories": ["#robotics", "#training", "#multimodal", "#inference"], "emoji": "‚ö°", "ru": {"title": "–ë—ã—Å—Ç—Ä—ã–µ —Ä–æ–±–æ—Ç—ã: –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –¥–ª—è —Ä–µ–∞–ª—å–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ Shallow-pi –¥–ª—è –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –∑–Ω–∞–Ω–∏–π, –∫–æ—Ç–æ—Ä—ã–π –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–Ω–∏–∂–∞–µ—Ç –≥–ª—É–±–∏–Ω—É —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ –≤ –º–æ–¥–µ–ª—è—Ö
[29.01.2026 17:39] Renaming data file.
[29.01.2026 17:39] Renaming previous data. hf_papers.json to ./d/2026-01-29.json
[29.01.2026 17:39] Saving new data file.
[29.01.2026 17:39] Generating page.
[29.01.2026 17:39] Renaming previous page.
[29.01.2026 17:39] Renaming previous data. index.html to ./d/2026-01-29.html
[29.01.2026 17:39] Writing result.
[29.01.2026 17:39] Renaming log file.
[29.01.2026 17:39] Renaming previous data. log.txt to ./logs/2026-01-29_last_log.txt
