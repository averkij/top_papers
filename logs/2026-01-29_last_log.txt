[29.01.2026 11:33] Read previous papers.
[29.01.2026 11:33] Generating top page (month).
[29.01.2026 11:33] Writing top page (month).
[29.01.2026 13:00] Read previous papers.
[29.01.2026 13:00] Get feed.
[29.01.2026 13:00] Get page data from previous paper. URL: https://huggingface.co/papers/2601.20614
[29.01.2026 13:00] Get page data from previous paper. URL: https://huggingface.co/papers/2601.20540
[29.01.2026 13:00] Get page data from previous paper. URL: https://huggingface.co/papers/2601.19325
[29.01.2026 13:00] Get page data from previous paper. URL: https://huggingface.co/papers/2601.20552
[29.01.2026 13:00] Get page data from previous paper. URL: https://huggingface.co/papers/2601.20209
[29.01.2026 13:00] Get page data from previous paper. URL: https://huggingface.co/papers/2601.20834
[29.01.2026 13:00] Get page data from previous paper. URL: https://huggingface.co/papers/2601.20789
[29.01.2026 13:00] Get page data from previous paper. URL: https://huggingface.co/papers/2601.20380
[29.01.2026 13:00] Get page data from previous paper. URL: https://huggingface.co/papers/2601.19949
[29.01.2026 13:00] Extract page data from URL. URL: https://huggingface.co/papers/2601.20802
[29.01.2026 13:00] Get page data from previous paper. URL: https://huggingface.co/papers/2601.20618
[29.01.2026 13:00] Get page data from previous paper. URL: https://huggingface.co/papers/2601.19194
[29.01.2026 13:00] Get page data from previous paper. URL: https://huggingface.co/papers/2601.20622
[29.01.2026 13:00] Extract page data from URL. URL: https://huggingface.co/papers/2601.20262
[29.01.2026 13:00] Get page data from previous paper. URL: https://huggingface.co/papers/2601.17950
[29.01.2026 13:00] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[29.01.2026 13:00] No deleted papers detected.
[29.01.2026 13:00] Downloading and parsing papers (pdf, html). Total: 15.
[29.01.2026 13:00] Downloading and parsing paper https://huggingface.co/papers/2601.20614.
[29.01.2026 13:00] Extra JSON file exists (./assets/json/2601.20614.json), skip PDF parsing.
[29.01.2026 13:00] Paper image links file exists (./assets/img_data/2601.20614.json), skip HTML parsing.
[29.01.2026 13:00] Success.
[29.01.2026 13:00] Downloading and parsing paper https://huggingface.co/papers/2601.20540.
[29.01.2026 13:00] Extra JSON file exists (./assets/json/2601.20540.json), skip PDF parsing.
[29.01.2026 13:00] Paper image links file exists (./assets/img_data/2601.20540.json), skip HTML parsing.
[29.01.2026 13:00] Success.
[29.01.2026 13:00] Downloading and parsing paper https://huggingface.co/papers/2601.19325.
[29.01.2026 13:00] Extra JSON file exists (./assets/json/2601.19325.json), skip PDF parsing.
[29.01.2026 13:00] Paper image links file exists (./assets/img_data/2601.19325.json), skip HTML parsing.
[29.01.2026 13:00] Success.
[29.01.2026 13:00] Downloading and parsing paper https://huggingface.co/papers/2601.20552.
[29.01.2026 13:00] Extra JSON file exists (./assets/json/2601.20552.json), skip PDF parsing.
[29.01.2026 13:00] Paper image links file exists (./assets/img_data/2601.20552.json), skip HTML parsing.
[29.01.2026 13:00] Success.
[29.01.2026 13:00] Downloading and parsing paper https://huggingface.co/papers/2601.20209.
[29.01.2026 13:00] Extra JSON file exists (./assets/json/2601.20209.json), skip PDF parsing.
[29.01.2026 13:00] Paper image links file exists (./assets/img_data/2601.20209.json), skip HTML parsing.
[29.01.2026 13:00] Success.
[29.01.2026 13:00] Downloading and parsing paper https://huggingface.co/papers/2601.20834.
[29.01.2026 13:00] Extra JSON file exists (./assets/json/2601.20834.json), skip PDF parsing.
[29.01.2026 13:00] Paper image links file exists (./assets/img_data/2601.20834.json), skip HTML parsing.
[29.01.2026 13:00] Success.
[29.01.2026 13:00] Downloading and parsing paper https://huggingface.co/papers/2601.20789.
[29.01.2026 13:00] Extra JSON file exists (./assets/json/2601.20789.json), skip PDF parsing.
[29.01.2026 13:00] Paper image links file exists (./assets/img_data/2601.20789.json), skip HTML parsing.
[29.01.2026 13:00] Success.
[29.01.2026 13:00] Downloading and parsing paper https://huggingface.co/papers/2601.20380.
[29.01.2026 13:00] Extra JSON file exists (./assets/json/2601.20380.json), skip PDF parsing.
[29.01.2026 13:00] Paper image links file exists (./assets/img_data/2601.20380.json), skip HTML parsing.
[29.01.2026 13:00] Success.
[29.01.2026 13:00] Downloading and parsing paper https://huggingface.co/papers/2601.19949.
[29.01.2026 13:00] Extra JSON file exists (./assets/json/2601.19949.json), skip PDF parsing.
[29.01.2026 13:00] Paper image links file exists (./assets/img_data/2601.19949.json), skip HTML parsing.
[29.01.2026 13:00] Success.
[29.01.2026 13:00] Downloading and parsing paper https://huggingface.co/papers/2601.20802.
[29.01.2026 13:00] Downloading paper 2601.20802 from https://arxiv.org/pdf/2601.20802v1...
[29.01.2026 13:01] Extracting affiliations from text.
[29.01.2026 13:01] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Reinforcement Learning via Self-Distillation Reinforcement Learning via Self-Distillation Jonas HÃ¼botter1 Frederike LÃ¼beck Marco Bagatella1, 2 Daniel Marta1 Thomas Kleine Buening1 Carlos Guestrin4 Andreas Krause1 1ETH Zurich 2Max Planck Institute for Intelligent Systems 3MIT 4Stanford , 1, 2 Lejs Behric Ido Hakimi1 , 1 , 1 Anton Baumann Idan Shenfeld 6 2 0 2 8 2 ] . [ 1 2 0 8 0 2 . 1 0 6 2 : r https://github.com/lasgroup/SDPO "
[29.01.2026 13:01] Response: ```python
[
    "ETH Zurich",
    "Max Planck Institute for Intelligent Systems",
    "MIT",
    "Stanford"
]
```
[29.01.2026 13:01] Deleting PDF ./assets/pdf/2601.20802.pdf.
[29.01.2026 13:01] Success.
[29.01.2026 13:01] Downloading and parsing paper https://huggingface.co/papers/2601.20618.
[29.01.2026 13:01] Extra JSON file exists (./assets/json/2601.20618.json), skip PDF parsing.
[29.01.2026 13:01] Paper image links file exists (./assets/img_data/2601.20618.json), skip HTML parsing.
[29.01.2026 13:01] Success.
[29.01.2026 13:01] Downloading and parsing paper https://huggingface.co/papers/2601.19194.
[29.01.2026 13:01] Extra JSON file exists (./assets/json/2601.19194.json), skip PDF parsing.
[29.01.2026 13:01] Paper image links file exists (./assets/img_data/2601.19194.json), skip HTML parsing.
[29.01.2026 13:01] Success.
[29.01.2026 13:01] Downloading and parsing paper https://huggingface.co/papers/2601.20622.
[29.01.2026 13:01] Extra JSON file exists (./assets/json/2601.20622.json), skip PDF parsing.
[29.01.2026 13:01] Paper image links file exists (./assets/img_data/2601.20622.json), skip HTML parsing.
[29.01.2026 13:01] Success.
[29.01.2026 13:01] Downloading and parsing paper https://huggingface.co/papers/2601.20262.
[29.01.2026 13:01] Downloading paper 2601.20262 from https://arxiv.org/pdf/2601.20262v1...
[29.01.2026 13:01] Extracting affiliations from text.
[29.01.2026 13:01] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Shallow-Ï€: Knowledge Distillation for Flow-based VLAs Boseong Jeon, Yunho Choi, Taehan Kim Samsung Research South Korea junbs95@gmail.com, yunho10.choi@samsung.com, taehan11.kim@samsung.com 6 2 0 2 8 2 ] . [ 1 2 6 2 0 2 . 1 0 6 2 : r a "
[29.01.2026 13:01] Response: ```python
["Samsung Research"]
```
[29.01.2026 13:01] Deleting PDF ./assets/pdf/2601.20262.pdf.
[29.01.2026 13:01] Success.
[29.01.2026 13:01] Downloading and parsing paper https://huggingface.co/papers/2601.17950.
[29.01.2026 13:01] Extra JSON file exists (./assets/json/2601.17950.json), skip PDF parsing.
[29.01.2026 13:01] Paper image links file exists (./assets/img_data/2601.17950.json), skip HTML parsing.
[29.01.2026 13:01] Success.
[29.01.2026 13:01] Enriching papers with extra data.
[29.01.2026 13:01] ********************************************************************************
[29.01.2026 13:01] Abstract 0. MathForge enhances mathematical reasoning in large models through a dual framework combining difficulty-aware policy optimization and multi-aspect question reformulation to address limitations in existing reinforcement learning methods.  					AI-generated summary 				 Reinforcement Learning with Ver...
[29.01.2026 13:01] ********************************************************************************
[29.01.2026 13:01] Abstract 1. LingBot-World is an open-source world simulator with high-fidelity dynamics, long-term memory capabilities, and real-time interactivity for diverse environments.  					AI-generated summary 				 We present LingBot-World, an open-sourced world simulator stemming from video generation. Positioned as a ...
[29.01.2026 13:01] ********************************************************************************
[29.01.2026 13:01] Abstract 2. Innovator-VL demonstrates that principled training design and transparent methodology can achieve strong scientific intelligence with reduced data requirements while maintaining general vision performance.  					AI-generated summary 				 We present Innovator-VL, a scientific multimodal large languag...
[29.01.2026 13:01] ********************************************************************************
[29.01.2026 13:01] Abstract 3. DeepSeek-OCR 2 introduces DeepEncoder V2 that dynamically reorders visual tokens based on semantic content, enabling more human-like causal reasoning in 2D image understanding through cascaded 1D causal structures.  					AI-generated summary 				 We present DeepSeek-OCR 2 to investigate the feasibil...
[29.01.2026 13:01] ********************************************************************************
[29.01.2026 13:01] Abstract 4. Spark is a reinforcement learning framework that strategically allocates computational resources by branching at critical decision states, improving sample efficiency and generalization for long-horizon tasks.  					AI-generated summary 				 Reinforcement learning has empowered large language models...
[29.01.2026 13:01] ********************************************************************************
[29.01.2026 13:01] Abstract 5. Linear representation directions in language models dynamically shift during conversations, affecting how factual information is encoded while preserving generic content, with implications for interpretability and context-adaptive model behavior.  					AI-generated summary 				 Language model repres...
[29.01.2026 13:01] ********************************************************************************
[29.01.2026 13:01] Abstract 6. Soft-Verified Efficient Repository Agents (SERA) enables cost-effective training of coding agents through supervised fine-tuning, achieving state-of-the-art performance while enabling specialization to private codebases at a fraction of the cost of previous methods.  					AI-generated summary 				 O...
[29.01.2026 13:01] ********************************************************************************
[29.01.2026 13:01] Abstract 7. OmegaUse is a general-purpose GUI agent model that achieves state-of-the-art performance on mobile and desktop platforms through a combination of high-quality data construction, decoupled training methods, and a Mixture-of-Experts architecture.  					AI-generated summary 				 Graphical User Interfac...
[29.01.2026 13:01] ********************************************************************************
[29.01.2026 13:01] Abstract 8. A large-scale reverberant speech corpus with detailed acoustic annotations is introduced to facilitate standardized comparison and reproduction of speech processing research.  					AI-generated summary 				 Despite decades of research on reverberant speech, comparing methods remains difficult becaus...
[29.01.2026 13:01] ********************************************************************************
[29.01.2026 13:01] Abstract 9. Self-Distillation Policy Optimization (SDPO) enhances reinforcement learning with verifiable rewards by utilizing rich textual feedback to improve sample efficiency and accuracy in language model training.  					AI-generated summary 				 Large language models are increasingly post-trained with reinf...
[29.01.2026 13:01] ********************************************************************************
[29.01.2026 13:01] Abstract 10. A multimodal sarcasm detection approach uses generative models to create stable semantic anchors and measures cross-modal discrepancies for improved accuracy and robustness.  					AI-generated summary 				 Multimodal sarcasm detection (MSD) aims to identify sarcasm within image-text pairs by modelin...
[29.01.2026 13:01] ********************************************************************************
[29.01.2026 13:01] Abstract 11. SE-DiCoW improves speaker-attributed ASR performance by using diarization output to identify enrollment segments for fixed conditioning in cross-attention layers, achieving significant reductions in transcription error rates.  					AI-generated summary 				 Speaker-attributed automatic speech recogn...
[29.01.2026 13:01] ********************************************************************************
[29.01.2026 13:01] Abstract 12. Free-form sketching enables intuitive dynamic intent communication for automated content creation, bridging human intention and digital output in animation workflows.  					AI-generated summary 				 Sketching provides an intuitive way to convey dynamic intent in animation authoring (i.e., how elemen...
[29.01.2026 13:01] ********************************************************************************
[29.01.2026 13:01] Abstract 13. A knowledge distillation framework called Shallow-pi is presented that reduces transformer depth in vision-language-action models, achieving faster inference with minimal performance loss in real-world robotic applications.  					AI-generated summary 				 The growing demand for real-time robotic dep...
[29.01.2026 13:01] ********************************************************************************
[29.01.2026 13:01] Abstract 14. Iterative upsampling methods can match cross-attention-based approaches while achieving lower inference costs through the proposed UPLiFT architecture with Local Attender operator for dense feature generation.  					AI-generated summary 				 The space of task-agnostic feature upsampling has emerged ...
[29.01.2026 13:01] Read previous papers.
[29.01.2026 13:01] Generating reviews via LLM API.
[29.01.2026 13:01] Using data from previous issue: {"categories": ["#rl", "#optimization", "#open_source", "#training", "#math", "#reasoning", "#data"], "emoji": "ğŸ§®", "ru": {"title": "Ğ¦ĞµĞ»ĞµĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ´Ğ»Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ LLM", "desc": "MathForge Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ¾Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´
[29.01.2026 13:01] Using data from previous issue: {"categories": ["#video", "#open_source", "#games", "#robotics", "#agents"], "emoji": "ğŸŒ", "ru": {"title": "ĞÑ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ world model Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¸Ñ€Ğ°Ğ¼Ğ¸ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸", "desc": "LingBot-World â€” ÑÑ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²
[29.01.2026 13:01] Using data from previous issue: {"categories": ["#benchmark", "#open_source", "#training", "#multimodal", "#science", "#synthetic", "#reasoning", "#data"], "emoji": "ğŸ”¬", "ru": {"title": "ĞĞ°ÑƒÑ‡Ğ½Ñ‹Ğ¹ Ğ˜Ğ˜ Ğ±ĞµĞ· Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ: ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ñ‹, Ğ° Ğ½Ğµ Ğ¾Ğ±ÑŠÑ‘Ğ¼", "desc": "Innovator-VL â€” ÑÑ‚Ğ¾ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±
[29.01.2026 13:01] Using data from previous issue: {"categories": ["#open_source", "#multimodal", "#architecture", "#reasoning", "#cv"], "emoji": "ğŸ‘ï¸", "ru": {"title": "Ğ¡ĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿ĞµÑ€ĞµÑƒĞ¿Ğ¾Ñ€ÑĞ´Ğ¾Ñ‡ĞµĞ½Ğ¸Ğµ: Ğ¾Ñ‚ Ñ€Ğ°ÑÑ‚Ñ€Ğ¾Ğ²Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ²Ñ‘Ñ€Ñ‚ĞºĞ¸ Ğº Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾Ğ¼Ñƒ Ğ·Ñ€ĞµĞ½Ğ¸Ñ", "desc": "DeepSeek-OCR 2 Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ DeepEncoder V2, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿ĞµÑ€ĞµÑƒĞ¿Ğ¾Ñ€ÑĞ´Ğ¾Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ 
[29.01.2026 13:01] Using data from previous issue: {"categories": ["#rl", "#training", "#robotics", "#agents"], "emoji": "ğŸŒ³", "ru": {"title": "Ğ£Ğ¼Ğ½Ğ¾Ğµ Ğ²ĞµÑ‚Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ² ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚Ğ°Ñ… Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ", "desc": "Spark â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ¼Ğ½Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµÑÑƒÑ€ÑÑ‹ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼Ğ¸ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸ÑĞ¼Ğ¸
[29.01.2026 13:01] Using data from previous issue: {"categories": ["#architecture", "#interpretability"], "emoji": "ğŸ§ ", "ru": {"title": "Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹: ĞºĞ°Ğº ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿ĞµÑ€ĞµĞ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒÑÑ‚ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°", "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ, ĞºĞ°Ğº Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ² Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑÑ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¼ĞµĞ½
[29.01.2026 13:01] Using data from previous issue: {"categories": ["#optimization", "#open_source", "#training", "#dataset", "#plp", "#synthetic", "#agents"], "emoji": "ğŸ¤–", "ru": {"title": "Ğ”ĞµÑˆÑ‘Ğ²Ğ°Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ĞºĞ¾Ğ´Ğ¾Ğ²Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¿Ñ€Ğ¸Ğ²Ğ°Ñ‚Ğ½Ñ‹Ğµ Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ¸", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ SERA â€” Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ´Ğ¾Ğ²Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· supervised fi
[29.01.2026 13:01] Using data from previous issue: {"categories": ["#benchmark", "#optimization", "#training", "#dataset", "#rlhf", "#architecture", "#synthetic", "#agents"], "emoji": "ğŸ¤–", "ru": {"title": "Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ»ÑĞ±Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞµ", "desc": "OmegaUse â€” ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ğ³Ñ€Ğ°
[29.01.2026 13:01] Using data from previous issue: {"categories": ["#benchmark", "#dataset", "#audio", "#open_source"], "emoji": "ğŸ¤", "ru": {"title": "Ğ¡Ñ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ±Ğ°Ğ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğº Ñ€ĞµĞ²ĞµÑ€Ğ±ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑ‡Ğ¸", "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° RIR-Mega-Speech â€” Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ğ°Ñ Ğ±Ğ°Ğ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ½Ğ¾ 117,5 Ñ‡Ğ°Ñ
[29.01.2026 13:01] Querying the API.
[29.01.2026 13:01] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Self-Distillation Policy Optimization (SDPO) enhances reinforcement learning with verifiable rewards by utilizing rich textual feedback to improve sample efficiency and accuracy in language model training.  					AI-generated summary 				 Large language models are increasingly post-trained with reinforcement learning in verifiable domains such as code and math. Yet, current methods for reinforcement learning with verifiable rewards (RLVR) learn only from a scalar outcome reward per attempt, creating a severe credit-assignment bottleneck. Many verifiable environments actually provide rich textual feedback, such as runtime errors or judge evaluations, that explain why an attempt failed. We formalize this setting as reinforcement learning with rich feedback and introduce Self-Distillation Policy Optimization (SDPO), which converts tokenized feedback into a dense learning signal without any external teacher or explicit reward model. SDPO treats the current model conditioned on feedback as a self-teacher and distills its feedback-informed next-token predictions back into the policy. In this way, SDPO leverages the model's ability to retrospectively identify its own mistakes in-context. Across scientific reasoning, tool use, and competitive programming on LiveCodeBench v6, SDPO improves sample efficiency and final accuracy over strong RLVR baselines. Notably, SDPO also outperforms baselines in standard RLVR environments that only return scalar feedback by using successful rollouts as implicit feedback for failed attempts. Finally, applying SDPO to individual questions at test time accelerates discovery on difficult binary-reward tasks, achieving the same discovery probability as best-of-k sampling or multi-turn conversations with 3x fewer attempts.
[29.01.2026 13:01] Response: ```json
{
  "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Self-Distillation Policy Optimization (SDPO), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²ÑƒÑ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ÑĞºĞ°Ğ»ÑÑ€Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ñ‹ÑÑ‚ÑƒĞ¿Ğ°ĞµÑ‚ Ğ² Ñ€Ğ¾Ğ»Ğ¸ ÑĞ°Ğ¼Ğ¾ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ, Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ ÑĞ²Ğ¾Ğ¸ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ (Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº ĞºĞ¾Ğ¼Ğ¿Ğ¸Ğ»ÑÑ†Ğ¸Ğ¸) Ğ¸ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»Ğ¸Ñ€ÑƒÑ ÑÑ‚Ğ¾Ñ‚ Ğ¾Ğ¿Ñ‹Ñ‚ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾ Ğ² Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºÑƒ. SDPO Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ RL Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°Ğ¼Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ¶Ğµ Ğ² ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ…, Ğ³Ğ´Ğµ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ÑĞºĞ°Ğ»ÑÑ€Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹, Ğ¸ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ Ğ¿Ğ¾Ğ¸ÑĞº Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ.",
  "emoji": "ğŸ§ ",
  "title": "ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· ÑĞ°Ğ¼Ğ¾ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºÑƒ: ĞºĞ°Ğº LLM ÑƒÑ‡Ğ¸Ñ‚ÑÑ Ğ½Ğ° ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾ÑˆĞ¸Ğ±ĞºĞ°Ñ…"
}
```
[29.01.2026 13:01] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Self-Distillation Policy Optimization (SDPO) enhances reinforcement learning with verifiable rewards by utilizing rich textual feedback to improve sample efficiency and accuracy in language model training.  					AI-generated summary 				 Large language models are increasingly post-trained with reinforcement learning in verifiable domains such as code and math. Yet, current methods for reinforcement learning with verifiable rewards (RLVR) learn only from a scalar outcome reward per attempt, creating a severe credit-assignment bottleneck. Many verifiable environments actually provide rich textual feedback, such as runtime errors or judge evaluations, that explain why an attempt failed. We formalize this setting as reinforcement learning with rich feedback and introduce Self-Distillation Policy Optimization (SDPO), which converts tokenized feedback into a dense learning signal without any external teacher or explicit reward model. SDPO treats the current model conditioned on feedback as a self-teacher and distills its feedback-informed next-token predictions back into the policy. In this way, SDPO leverages the model's ability to retrospectively identify its own mistakes in-context. Across scientific reasoning, tool use, and competitive programming on LiveCodeBench v6, SDPO improves sample efficiency and final accuracy over strong RLVR baselines. Notably, SDPO also outperforms baselines in standard RLVR environments that only return scalar feedback by using successful rollouts as implicit feedback for failed attempts. Finally, applying SDPO to individual questions at test time accelerates discovery on difficult binary-reward tasks, achieving the same discovery probability as best-of-k sampling or multi-turn conversations with 3x fewer attempts."

[29.01.2026 13:01] Response: ```python
["RL", "RLHF", "TRAINING", "PLP"]
```
[29.01.2026 13:01] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Self-Distillation Policy Optimization (SDPO) enhances reinforcement learning with verifiable rewards by utilizing rich textual feedback to improve sample efficiency and accuracy in language model training.  					AI-generated summary 				 Large language models are increasingly post-trained with reinforcement learning in verifiable domains such as code and math. Yet, current methods for reinforcement learning with verifiable rewards (RLVR) learn only from a scalar outcome reward per attempt, creating a severe credit-assignment bottleneck. Many verifiable environments actually provide rich textual feedback, such as runtime errors or judge evaluations, that explain why an attempt failed. We formalize this setting as reinforcement learning with rich feedback and introduce Self-Distillation Policy Optimization (SDPO), which converts tokenized feedback into a dense learning signal without any external teacher or explicit reward model. SDPO treats the current model conditioned on feedback as a self-teacher and distills its feedback-informed next-token predictions back into the policy. In this way, SDPO leverages the model's ability to retrospectively identify its own mistakes in-context. Across scientific reasoning, tool use, and competitive programming on LiveCodeBench v6, SDPO improves sample efficiency and final accuracy over strong RLVR baselines. Notably, SDPO also outperforms baselines in standard RLVR environments that only return scalar feedback by using successful rollouts as implicit feedback for failed attempts. Finally, applying SDPO to individual questions at test time accelerates discovery on difficult binary-reward tasks, achieving the same discovery probability as best-of-k sampling or multi-turn conversations with 3x fewer attempts."

[29.01.2026 13:01] Response: ```python
["REASONING", "OPTIMIZATION", "SCIENCE"]
```
[29.01.2026 13:01] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Self-Distillation Policy Optimization (SDPO) is a novel approach in reinforcement learning that utilizes rich textual feedback to enhance learning efficiency and accuracy. By converting detailed feedback into a dense learning signal, SDPO allows models to learn from their own mistakes without needing an external teacher. This method addresses the credit-assignment problem by treating the model as a self-teacher, distilling its predictions based on contextual feedback. As a result, SDPO demonstrates improved performance in various tasks, outperforming traditional methods that rely solely on scalar rewards.","title":"Empowering Reinforcement Learning with Rich Feedback"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Self-Distillation Policy Optimization (SDPO) is a novel approach in reinforcement learning that utilizes rich textual feedback to enhance learning efficiency and accuracy. By converting detailed feedback into a dense learning signal, SDPO allows models to learn from their own mistakes without needing an external teacher. This method addresses the credit-assignment problem by treating the model as a self-teacher, distilling its predictions based on contextual feedback. As a result, SDPO demonstrates improved performance in various tasks, outperforming traditional methods that rely solely on scalar rewards.', title='Empowering Reinforcement Learning with Rich Feedback'))
[29.01.2026 13:01] Response: ParsedChatCompletionMessage[Article](content='{"desc":"è‡ªè’¸é¦ç­–ç•¥ä¼˜åŒ–ï¼ˆSDPOï¼‰é€šè¿‡åˆ©ç”¨ä¸°å¯Œçš„æ–‡æœ¬åé¦ˆæ¥å¢å¼ºå¼ºåŒ–å­¦ä¹ ä¸­çš„å¯éªŒè¯å¥–åŠ±ï¼Œä»è€Œæé«˜æ ·æœ¬æ•ˆç‡å’Œè¯­è¨€æ¨¡å‹è®­ç»ƒçš„å‡†ç¡®æ€§ã€‚ç°æœ‰çš„å¯éªŒè¯å¥–åŠ±å¼ºåŒ–å­¦ä¹ æ–¹æ³•ä»…ä¾èµ–äºæ¯æ¬¡å°è¯•çš„æ ‡é‡ç»“æœå¥–åŠ±ï¼Œå¯¼è‡´ä¿¡ç”¨åˆ†é…ç“¶é¢ˆã€‚SDPOå°†æ ‡è®°åŒ–çš„åé¦ˆè½¬åŒ–ä¸ºå¯†é›†çš„å­¦ä¹ ä¿¡å·ï¼Œåˆ©ç”¨å½“å‰æ¨¡å‹ä½œä¸ºè‡ªæˆ‘æ•™å¸ˆï¼Œæç‚¼åé¦ˆä¿¡æ¯ä»¥æ”¹è¿›ç­–ç•¥ã€‚é€šè¿‡åœ¨ç§‘å­¦æ¨ç†ã€å·¥å…·ä½¿ç”¨å’Œç«äº‰ç¼–ç¨‹ç­‰ä»»åŠ¡ä¸­åº”ç”¨SDPOï¼Œæ˜¾è‘—æé«˜äº†æ ·æœ¬æ•ˆç‡å’Œæœ€ç»ˆå‡†ç¡®æ€§ã€‚","title":"è‡ªè’¸é¦ç­–ç•¥ä¼˜åŒ–ï¼šæå‡å¼ºåŒ–å­¦ä¹ çš„æ ·æœ¬æ•ˆç‡ä¸å‡†ç¡®æ€§"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='è‡ªè’¸é¦ç­–ç•¥ä¼˜åŒ–ï¼ˆSDPOï¼‰é€šè¿‡åˆ©ç”¨ä¸°å¯Œçš„æ–‡æœ¬åé¦ˆæ¥å¢å¼ºå¼ºåŒ–å­¦ä¹ ä¸­çš„å¯éªŒè¯å¥–åŠ±ï¼Œä»è€Œæé«˜æ ·æœ¬æ•ˆç‡å’Œè¯­è¨€æ¨¡å‹è®­ç»ƒçš„å‡†ç¡®æ€§ã€‚ç°æœ‰çš„å¯éªŒè¯å¥–åŠ±å¼ºåŒ–å­¦ä¹ æ–¹æ³•ä»…ä¾èµ–äºæ¯æ¬¡å°è¯•çš„æ ‡é‡ç»“æœå¥–åŠ±ï¼Œå¯¼è‡´ä¿¡ç”¨åˆ†é…ç“¶é¢ˆã€‚SDPOå°†æ ‡è®°åŒ–çš„åé¦ˆè½¬åŒ–ä¸ºå¯†é›†çš„å­¦ä¹ ä¿¡å·ï¼Œåˆ©ç”¨å½“å‰æ¨¡å‹ä½œä¸ºè‡ªæˆ‘æ•™å¸ˆï¼Œæç‚¼åé¦ˆä¿¡æ¯ä»¥æ”¹è¿›ç­–ç•¥ã€‚é€šè¿‡åœ¨ç§‘å­¦æ¨ç†ã€å·¥å…·ä½¿ç”¨å’Œç«äº‰ç¼–ç¨‹ç­‰ä»»åŠ¡ä¸­åº”ç”¨SDPOï¼Œæ˜¾è‘—æé«˜äº†æ ·æœ¬æ•ˆç‡å’Œæœ€ç»ˆå‡†ç¡®æ€§ã€‚', title='è‡ªè’¸é¦ç­–ç•¥ä¼˜åŒ–ï¼šæå‡å¼ºåŒ–å­¦ä¹ çš„æ ·æœ¬æ•ˆç‡ä¸å‡†ç¡®æ€§'))
[29.01.2026 13:01] Using data from previous issue: {"categories": ["#benchmark", "#multimodal"], "emoji": "ğŸ˜", "ru": {"title": "Ğ¯ĞºĞ¾Ñ€Ñ ÑĞ¼Ñ‹ÑĞ»Ğ° Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ² ÑĞ°Ñ€ĞºĞ°Ğ·Ğ¼Ğ°: Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞ°Ñ€ĞºĞ°Ğ·Ğ¼Ğ° Ğ² Ğ¿Ğ°Ñ€Ğ°Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ-Ñ‚ĞµĞºÑÑ‚ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ
[29.01.2026 13:01] Using data from previous issue: {"categories": ["#benchmark", "#audio", "#multilingual"], "emoji": "ğŸ¤", "ru": {"title": "Ğ¡Ğ°Ğ¼Ğ¾Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ½Ğ°Ñ Ğ´Ğ¸Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑ‡Ğ¸ ÑĞ¿Ğ¸ĞºĞµÑ€Ğ¾Ğ²", "desc": "SE-DiCoW ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€ĞµÑ‡Ğ¸ Ñ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ†Ğ¸ĞµĞ¹ ÑĞ¿Ğ¸ĞºĞµÑ€Ğ° Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰Ğ¸Ñ… Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸ÑÑ…, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²Ñ‹Ñ…Ğ¾Ğ´ Ğ´Ğ¸Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ° Ñ€
[29.01.2026 13:01] Using data from previous issue: {"categories": ["#video", "#3d", "#multimodal", "#cv", "#story_generation"], "emoji": "âœï¸", "ru": {"title": "Ğ¡Ğ²Ğ¾Ğ±Ğ¾Ğ´Ğ½Ñ‹Ğ¹ Ñ€Ğ¸ÑÑƒĞ½Ğ¾Ğº ĞºĞ°Ğº ÑĞ·Ñ‹Ğº Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¼ Ğ¸ AI Ğ² ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸", "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸, Ğ³Ğ´Ğµ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ ÑĞ²Ğ¾Ğ±Ğ¾Ğ´Ğ½ÑƒÑ Ñ„Ğ¾Ñ€Ğ¼Ñƒ Ñ€Ğ¸ÑĞ¾Ğ²
[29.01.2026 13:01] Querying the API.
[29.01.2026 13:01] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A knowledge distillation framework called Shallow-pi is presented that reduces transformer depth in vision-language-action models, achieving faster inference with minimal performance loss in real-world robotic applications.  					AI-generated summary 				 The growing demand for real-time robotic deployment necessitates fast and on-device inference for vision-language-action (VLA) models. Within the VLA literature, efficiency has been extensively studied at the token level, such as visual token pruning. In contrast, systematic transformer layer reduction has received limited attention and, to the best of our knowledge, has not been explored for flow-based VLA models under knowledge distillation. In this work, we propose Shallow-pi, a principled knowledge distillation framework that aggressively reduces the transformer depth of both the VLM backbone and the flow-based action head, compressing the model from 18 to 6 layers. Shallow-pi achieves over two times faster inference with less than one percent absolute drop in success rate on standard manipulation benchmarks, establishing state-of-the-art performance among reduced VLA models. Crucially, we validate our approach through industrial-scale real-world experiments on Jetson Orin and Jetson Thor across multiple robot platforms, including humanoid systems, in complex and dynamic manipulation scenarios.
[29.01.2026 13:01] Response: ```json
{
  "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Shallow-pi Ğ´Ğ»Ñ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñƒ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ° Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… vision-language-action (VLA) Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸. ĞœĞµÑ‚Ğ¾Ğ´ ÑĞ¶Ğ¸Ğ¼Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ 18 Ğ´Ğ¾ 6 ÑĞ»Ğ¾Ñ‘Ğ² Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ Ğ´Ğ²ÑƒĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ¾Ñ‚ĞµÑ€ĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ (Ğ¼ĞµĞ½ĞµĞµ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚Ğ°). ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ³ÑƒĞ¼Ğ°Ğ½Ğ¾Ğ¸Ğ´Ğ½Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹, Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ°. Shallow-pi ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ state-of-the-art Ğ´Ğ»Ñ ÑĞ¶Ğ°Ñ‚Ñ‹Ñ… VLA Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ²Ñ‘Ñ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑƒÑĞºĞ¾Ñ€Ğ¸Ñ‚ĞµĞ»ÑÑ… Jetson.",
  "emoji": "âš¡",
  "title": "Ğ‘Ñ‹ÑÑ‚Ñ€Ñ‹Ğµ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ñ‹: Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ğ´Ğ»Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸"
}
```
[29.01.2026 13:01] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A knowledge distillation framework called Shallow-pi is presented that reduces transformer depth in vision-language-action models, achieving faster inference with minimal performance loss in real-world robotic applications.  					AI-generated summary 				 The growing demand for real-time robotic deployment necessitates fast and on-device inference for vision-language-action (VLA) models. Within the VLA literature, efficiency has been extensively studied at the token level, such as visual token pruning. In contrast, systematic transformer layer reduction has received limited attention and, to the best of our knowledge, has not been explored for flow-based VLA models under knowledge distillation. In this work, we propose Shallow-pi, a principled knowledge distillation framework that aggressively reduces the transformer depth of both the VLM backbone and the flow-based action head, compressing the model from 18 to 6 layers. Shallow-pi achieves over two times faster inference with less than one percent absolute drop in success rate on standard manipulation benchmarks, establishing state-of-the-art performance among reduced VLA models. Crucially, we validate our approach through industrial-scale real-world experiments on Jetson Orin and Jetson Thor across multiple robot platforms, including humanoid systems, in complex and dynamic manipulation scenarios."

[29.01.2026 13:01] Response: ```python
["ROBOTICS", "INFERENCE", "TRAINING", "MULTIMODAL"]
```
[29.01.2026 13:01] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A knowledge distillation framework called Shallow-pi is presented that reduces transformer depth in vision-language-action models, achieving faster inference with minimal performance loss in real-world robotic applications.  					AI-generated summary 				 The growing demand for real-time robotic deployment necessitates fast and on-device inference for vision-language-action (VLA) models. Within the VLA literature, efficiency has been extensively studied at the token level, such as visual token pruning. In contrast, systematic transformer layer reduction has received limited attention and, to the best of our knowledge, has not been explored for flow-based VLA models under knowledge distillation. In this work, we propose Shallow-pi, a principled knowledge distillation framework that aggressively reduces the transformer depth of both the VLM backbone and the flow-based action head, compressing the model from 18 to 6 layers. Shallow-pi achieves over two times faster inference with less than one percent absolute drop in success rate on standard manipulation benchmarks, establishing state-of-the-art performance among reduced VLA models. Crucially, we validate our approach through industrial-scale real-world experiments on Jetson Orin and Jetson Thor across multiple robot platforms, including humanoid systems, in complex and dynamic manipulation scenarios."

[29.01.2026 13:01] Response: ```python
["OPTIMIZATION", "TRANSFER_LEARNING"]
```

**Justification:**

- **OPTIMIZATION**: The paper presents Shallow-pi, a knowledge distillation framework focused on reducing model depth and achieving faster inference with minimal performance loss. Knowledge distillation and model compression are core optimization techniques for improving computational efficiency.

- **TRANSFER_LEARNING**: Knowledge distillation inherently involves transferring knowledge from a larger teacher model to a smaller student model, which is a form of knowledge transfer between models.
[29.01.2026 13:01] Error. Failed to parse JSON from LLM. ["OPTIMIZATION", "TRANSFER_LEARNING"]


**Justification:**

- **OPTIMIZATION**: The paper presents Shallow-pi, a knowledge distillation framework focused on reducing model depth and achieving faster inference with minimal performance loss. Knowledge distillation and model compression are core optimization techniques for improving computational efficiency.

- **TRANSFER_LEARNING**: Knowledge distillation inherently involves transferring knowledge from a larger teacher model to a smaller student model, which is a form of knowledge transfer between models.
[29.01.2026 13:01] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The paper introduces Shallow-pi, a knowledge distillation framework designed to optimize vision-language-action (VLA) models by significantly reducing the depth of transformers. By compressing the model from 18 layers to just 6, Shallow-pi enables faster inference times while maintaining high performance, with less than a 1% drop in success rates on manipulation tasks. This approach is particularly beneficial for real-time robotic applications, where efficiency is crucial. The effectiveness of Shallow-pi is demonstrated through extensive real-world testing on various robotic platforms, showcasing its potential for practical deployment.","title":"Accelerating VLA Models with Shallow-pi: Less Depth, More Speed!"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='The paper introduces Shallow-pi, a knowledge distillation framework designed to optimize vision-language-action (VLA) models by significantly reducing the depth of transformers. By compressing the model from 18 layers to just 6, Shallow-pi enables faster inference times while maintaining high performance, with less than a 1% drop in success rates on manipulation tasks. This approach is particularly beneficial for real-time robotic applications, where efficiency is crucial. The effectiveness of Shallow-pi is demonstrated through extensive real-world testing on various robotic platforms, showcasing its potential for practical deployment.', title='Accelerating VLA Models with Shallow-pi: Less Depth, More Speed!'))
[29.01.2026 13:01] Response: ParsedChatCompletionMessage[Article](content='{"desc":"æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºShallow-piçš„çŸ¥è¯†è’¸é¦æ¡†æ¶ï¼Œæ—¨åœ¨å‡å°‘è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ä¸­çš„å˜æ¢å™¨æ·±åº¦ï¼Œä»è€Œå®ç°æ›´å¿«çš„æ¨ç†é€Ÿåº¦ï¼ŒåŒæ—¶åœ¨å®é™…æœºå™¨äººåº”ç”¨ä¸­ä¿æŒè¾ƒå°çš„æ€§èƒ½æŸå¤±ã€‚Shallow-pié€šè¿‡ç³»ç»Ÿæ€§åœ°å‡å°‘å˜æ¢å™¨å±‚æ•°ï¼Œå°†æ¨¡å‹ä»18å±‚å‹ç¼©åˆ°6å±‚ï¼Œæ˜¾è‘—æé«˜äº†æ¨ç†æ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ ‡å‡†æ“ä½œåŸºå‡†ä¸ŠæˆåŠŸç‡ä»…ä¸‹é™ä¸åˆ°1%ï¼Œæ¨ç†é€Ÿåº¦æé«˜äº†ä¸¤å€ï¼Œå±•ç¤ºäº†åœ¨å‡å°‘å˜æ¢å™¨æ·±åº¦çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ä¸­çš„é¢†å…ˆæ€§èƒ½ã€‚æˆ‘ä»¬è¿˜é€šè¿‡åœ¨Jetson Orinå’ŒJetson Thorç­‰å¤šä¸ªæœºå™¨äººå¹³å°ä¸Šçš„å·¥ä¸šè§„æ¨¡çœŸå®ä¸–ç•Œå®éªŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚","title":"çŸ¥è¯†è’¸é¦ï¼Œæ·±åº¦å‹ç¼©ï¼Œå¿«é€Ÿæ¨ç†"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºShallow-piçš„çŸ¥è¯†è’¸é¦æ¡†æ¶ï¼Œæ—¨åœ¨å‡å°‘è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ä¸­çš„å˜æ¢å™¨æ·±åº¦ï¼Œä»è€Œå®ç°æ›´å¿«çš„æ¨ç†é€Ÿåº¦ï¼ŒåŒæ—¶åœ¨å®é™…æœºå™¨äººåº”ç”¨ä¸­ä¿æŒè¾ƒå°çš„æ€§èƒ½æŸå¤±ã€‚Shallow-pié€šè¿‡ç³»ç»Ÿæ€§åœ°å‡å°‘å˜æ¢å™¨å±‚æ•°ï¼Œå°†æ¨¡å‹ä»18å±‚å‹ç¼©åˆ°6å±‚ï¼Œæ˜¾è‘—æé«˜äº†æ¨ç†æ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ ‡å‡†æ“ä½œåŸºå‡†ä¸ŠæˆåŠŸç‡ä»…ä¸‹é™ä¸åˆ°1%ï¼Œæ¨ç†é€Ÿåº¦æé«˜äº†ä¸¤å€ï¼Œå±•ç¤ºäº†åœ¨å‡å°‘å˜æ¢å™¨æ·±åº¦çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ä¸­çš„é¢†å…ˆæ€§èƒ½ã€‚æˆ‘ä»¬è¿˜é€šè¿‡åœ¨Jetson Orinå’ŒJetson Thorç­‰å¤šä¸ªæœºå™¨äººå¹³å°ä¸Šçš„å·¥ä¸šè§„æ¨¡çœŸå®ä¸–ç•Œå®éªŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚', title='çŸ¥è¯†è’¸é¦ï¼Œæ·±åº¦å‹ç¼©ï¼Œå¿«é€Ÿæ¨ç†'))
[29.01.2026 13:01] Using data from previous issue: {"categories": [], "emoji": "â¬†ï¸", "ru": {"title": "Ğ‘Ñ‹ÑÑ‚Ñ€Ğ¾Ğµ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ UPLiFT â€” Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¸Ğ· Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€ Local Attender
[29.01.2026 13:01] Renaming data file.
[29.01.2026 13:01] Renaming previous data. hf_papers.json to ./d/2026-01-29.json
[29.01.2026 13:01] Saving new data file.
[29.01.2026 13:01] Generating page.
[29.01.2026 13:01] Renaming previous page.
[29.01.2026 13:01] Renaming previous data. index.html to ./d/2026-01-29.html
[29.01.2026 13:01] Writing result.
[29.01.2026 13:01] Renaming log file.
[29.01.2026 13:01] Renaming previous data. log.txt to ./logs/2026-01-29_last_log.txt
