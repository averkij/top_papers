[18.10.2025 06:33] Read previous papers.
[18.10.2025 06:33] Generating top page (month).
[18.10.2025 06:33] Writing top page (month).
[18.10.2025 12:44] Read previous papers.
[18.10.2025 12:44] Get feed.
[18.10.2025 12:44] Get page data from previous paper. URL: https://huggingface.co/papers/2510.04849
[18.10.2025 12:44] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14545
[18.10.2025 12:44] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14975
[18.10.2025 12:44] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14359
[18.10.2025 12:44] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14979
[18.10.2025 12:44] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14847
[18.10.2025 12:44] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14943
[18.10.2025 12:44] Get page data from previous paper. URL: https://huggingface.co/papers/2510.13998
[18.10.2025 12:44] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14967
[18.10.2025 12:44] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14528
[18.10.2025 12:44] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14972
[18.10.2025 12:44] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14973
[18.10.2025 12:44] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14958
[18.10.2025 12:44] Get page data from previous paper. URL: https://huggingface.co/papers/2510.10518
[18.10.2025 12:44] Get page data from previous paper. URL: https://huggingface.co/papers/2510.09033
[18.10.2025 12:44] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14902
[18.10.2025 12:44] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14763
[18.10.2025 12:44] Get page data from previous paper. URL: https://huggingface.co/papers/2510.13217
[18.10.2025 12:44] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14880
[18.10.2025 12:44] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14616
[18.10.2025 12:44] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14276
[18.10.2025 12:44] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14240
[18.10.2025 12:44] Get page data from previous paper. URL: https://huggingface.co/papers/2510.13928
[18.10.2025 12:44] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14300
[18.10.2025 12:44] Get page data from previous paper. URL: https://huggingface.co/papers/2510.13054
[18.10.2025 12:44] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14980
[18.10.2025 12:44] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14807
[18.10.2025 12:44] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14211
[18.10.2025 12:44] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14978
[18.10.2025 12:44] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14974
[18.10.2025 12:44] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14969
[18.10.2025 12:44] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14949
[18.10.2025 12:44] Get page data from previous paper. URL: https://huggingface.co/papers/2510.13454
[18.10.2025 12:44] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14961
[18.10.2025 12:44] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14955
[18.10.2025 12:44] Get page data from previous paper. URL: https://huggingface.co/papers/2510.13996
[18.10.2025 12:44] Get page data from previous paper. URL: https://huggingface.co/papers/2510.10472
[18.10.2025 12:44] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14913
[18.10.2025 12:44] Get page data from previous paper. URL: https://huggingface.co/papers/2510.13697
[18.10.2025 12:44] Get page data from previous paper. URL: https://huggingface.co/papers/2510.12764
[18.10.2025 12:44] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14976
[18.10.2025 12:44] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14919
[18.10.2025 12:44] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14252
[18.10.2025 12:44] Get page data from previous paper. URL: https://huggingface.co/papers/2510.13913
[18.10.2025 12:44] Get page data from previous paper. URL: https://huggingface.co/papers/2510.06694
[18.10.2025 12:44] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14942
[18.10.2025 12:44] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14351
[18.10.2025 12:44] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14095
[18.10.2025 12:44] Get page data from previous paper. URL: https://huggingface.co/papers/2510.13910
[18.10.2025 12:44] Get page data from previous paper. URL: https://huggingface.co/papers/2510.10390
[18.10.2025 12:44] Get page data from previous paper. URL: https://huggingface.co/papers/2510.13161
[18.10.2025 12:44] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[18.10.2025 12:44] No deleted papers detected.
[18.10.2025 12:44] Downloading and parsing papers (pdf, html). Total: 51.
[18.10.2025 12:44] Downloading and parsing paper https://huggingface.co/papers/2510.04849.
[18.10.2025 12:44] Extra JSON file exists (./assets/json/2510.04849.json), skip PDF parsing.
[18.10.2025 12:44] Paper image links file exists (./assets/img_data/2510.04849.json), skip HTML parsing.
[18.10.2025 12:44] Success.
[18.10.2025 12:44] Downloading and parsing paper https://huggingface.co/papers/2510.14545.
[18.10.2025 12:44] Extra JSON file exists (./assets/json/2510.14545.json), skip PDF parsing.
[18.10.2025 12:44] Paper image links file exists (./assets/img_data/2510.14545.json), skip HTML parsing.
[18.10.2025 12:44] Success.
[18.10.2025 12:44] Downloading and parsing paper https://huggingface.co/papers/2510.14975.
[18.10.2025 12:44] Extra JSON file exists (./assets/json/2510.14975.json), skip PDF parsing.
[18.10.2025 12:44] Paper image links file exists (./assets/img_data/2510.14975.json), skip HTML parsing.
[18.10.2025 12:44] Success.
[18.10.2025 12:44] Downloading and parsing paper https://huggingface.co/papers/2510.14359.
[18.10.2025 12:44] Extra JSON file exists (./assets/json/2510.14359.json), skip PDF parsing.
[18.10.2025 12:44] Paper image links file exists (./assets/img_data/2510.14359.json), skip HTML parsing.
[18.10.2025 12:44] Success.
[18.10.2025 12:44] Downloading and parsing paper https://huggingface.co/papers/2510.14979.
[18.10.2025 12:44] Extra JSON file exists (./assets/json/2510.14979.json), skip PDF parsing.
[18.10.2025 12:44] Paper image links file exists (./assets/img_data/2510.14979.json), skip HTML parsing.
[18.10.2025 12:44] Success.
[18.10.2025 12:44] Downloading and parsing paper https://huggingface.co/papers/2510.14847.
[18.10.2025 12:44] Extra JSON file exists (./assets/json/2510.14847.json), skip PDF parsing.
[18.10.2025 12:44] Paper image links file exists (./assets/img_data/2510.14847.json), skip HTML parsing.
[18.10.2025 12:44] Success.
[18.10.2025 12:44] Downloading and parsing paper https://huggingface.co/papers/2510.14943.
[18.10.2025 12:44] Extra JSON file exists (./assets/json/2510.14943.json), skip PDF parsing.
[18.10.2025 12:44] Paper image links file exists (./assets/img_data/2510.14943.json), skip HTML parsing.
[18.10.2025 12:44] Success.
[18.10.2025 12:44] Downloading and parsing paper https://huggingface.co/papers/2510.13998.
[18.10.2025 12:44] Extra JSON file exists (./assets/json/2510.13998.json), skip PDF parsing.
[18.10.2025 12:44] Paper image links file exists (./assets/img_data/2510.13998.json), skip HTML parsing.
[18.10.2025 12:44] Success.
[18.10.2025 12:44] Downloading and parsing paper https://huggingface.co/papers/2510.14967.
[18.10.2025 12:44] Extra JSON file exists (./assets/json/2510.14967.json), skip PDF parsing.
[18.10.2025 12:44] Paper image links file exists (./assets/img_data/2510.14967.json), skip HTML parsing.
[18.10.2025 12:44] Success.
[18.10.2025 12:44] Downloading and parsing paper https://huggingface.co/papers/2510.14528.
[18.10.2025 12:44] Extra JSON file exists (./assets/json/2510.14528.json), skip PDF parsing.
[18.10.2025 12:44] Paper image links file exists (./assets/img_data/2510.14528.json), skip HTML parsing.
[18.10.2025 12:44] Success.
[18.10.2025 12:44] Downloading and parsing paper https://huggingface.co/papers/2510.14972.
[18.10.2025 12:44] Extra JSON file exists (./assets/json/2510.14972.json), skip PDF parsing.
[18.10.2025 12:44] Paper image links file exists (./assets/img_data/2510.14972.json), skip HTML parsing.
[18.10.2025 12:44] Success.
[18.10.2025 12:44] Downloading and parsing paper https://huggingface.co/papers/2510.14973.
[18.10.2025 12:44] Extra JSON file exists (./assets/json/2510.14973.json), skip PDF parsing.
[18.10.2025 12:44] Paper image links file exists (./assets/img_data/2510.14973.json), skip HTML parsing.
[18.10.2025 12:44] Success.
[18.10.2025 12:44] Downloading and parsing paper https://huggingface.co/papers/2510.14958.
[18.10.2025 12:44] Extra JSON file exists (./assets/json/2510.14958.json), skip PDF parsing.
[18.10.2025 12:44] Paper image links file exists (./assets/img_data/2510.14958.json), skip HTML parsing.
[18.10.2025 12:44] Success.
[18.10.2025 12:44] Downloading and parsing paper https://huggingface.co/papers/2510.10518.
[18.10.2025 12:44] Extra JSON file exists (./assets/json/2510.10518.json), skip PDF parsing.
[18.10.2025 12:44] Paper image links file exists (./assets/img_data/2510.10518.json), skip HTML parsing.
[18.10.2025 12:44] Success.
[18.10.2025 12:44] Downloading and parsing paper https://huggingface.co/papers/2510.09033.
[18.10.2025 12:44] Extra JSON file exists (./assets/json/2510.09033.json), skip PDF parsing.
[18.10.2025 12:44] Paper image links file exists (./assets/img_data/2510.09033.json), skip HTML parsing.
[18.10.2025 12:44] Success.
[18.10.2025 12:44] Downloading and parsing paper https://huggingface.co/papers/2510.14902.
[18.10.2025 12:44] Extra JSON file exists (./assets/json/2510.14902.json), skip PDF parsing.
[18.10.2025 12:44] Paper image links file exists (./assets/img_data/2510.14902.json), skip HTML parsing.
[18.10.2025 12:44] Success.
[18.10.2025 12:44] Downloading and parsing paper https://huggingface.co/papers/2510.14763.
[18.10.2025 12:44] Extra JSON file exists (./assets/json/2510.14763.json), skip PDF parsing.
[18.10.2025 12:44] Paper image links file exists (./assets/img_data/2510.14763.json), skip HTML parsing.
[18.10.2025 12:44] Success.
[18.10.2025 12:44] Downloading and parsing paper https://huggingface.co/papers/2510.13217.
[18.10.2025 12:44] Extra JSON file exists (./assets/json/2510.13217.json), skip PDF parsing.
[18.10.2025 12:44] Paper image links file exists (./assets/img_data/2510.13217.json), skip HTML parsing.
[18.10.2025 12:44] Success.
[18.10.2025 12:44] Downloading and parsing paper https://huggingface.co/papers/2510.14880.
[18.10.2025 12:44] Extra JSON file exists (./assets/json/2510.14880.json), skip PDF parsing.
[18.10.2025 12:44] Paper image links file exists (./assets/img_data/2510.14880.json), skip HTML parsing.
[18.10.2025 12:44] Success.
[18.10.2025 12:44] Downloading and parsing paper https://huggingface.co/papers/2510.14616.
[18.10.2025 12:44] Extra JSON file exists (./assets/json/2510.14616.json), skip PDF parsing.
[18.10.2025 12:44] Paper image links file exists (./assets/img_data/2510.14616.json), skip HTML parsing.
[18.10.2025 12:44] Success.
[18.10.2025 12:44] Downloading and parsing paper https://huggingface.co/papers/2510.14276.
[18.10.2025 12:44] Extra JSON file exists (./assets/json/2510.14276.json), skip PDF parsing.
[18.10.2025 12:44] Paper image links file exists (./assets/img_data/2510.14276.json), skip HTML parsing.
[18.10.2025 12:44] Success.
[18.10.2025 12:44] Downloading and parsing paper https://huggingface.co/papers/2510.14240.
[18.10.2025 12:44] Extra JSON file exists (./assets/json/2510.14240.json), skip PDF parsing.
[18.10.2025 12:44] Paper image links file exists (./assets/img_data/2510.14240.json), skip HTML parsing.
[18.10.2025 12:44] Success.
[18.10.2025 12:44] Downloading and parsing paper https://huggingface.co/papers/2510.13928.
[18.10.2025 12:44] Extra JSON file exists (./assets/json/2510.13928.json), skip PDF parsing.
[18.10.2025 12:44] Paper image links file exists (./assets/img_data/2510.13928.json), skip HTML parsing.
[18.10.2025 12:44] Success.
[18.10.2025 12:44] Downloading and parsing paper https://huggingface.co/papers/2510.14300.
[18.10.2025 12:44] Extra JSON file exists (./assets/json/2510.14300.json), skip PDF parsing.
[18.10.2025 12:44] Paper image links file exists (./assets/img_data/2510.14300.json), skip HTML parsing.
[18.10.2025 12:44] Success.
[18.10.2025 12:44] Downloading and parsing paper https://huggingface.co/papers/2510.13054.
[18.10.2025 12:44] Extra JSON file exists (./assets/json/2510.13054.json), skip PDF parsing.
[18.10.2025 12:44] Paper image links file exists (./assets/img_data/2510.13054.json), skip HTML parsing.
[18.10.2025 12:44] Success.
[18.10.2025 12:44] Downloading and parsing paper https://huggingface.co/papers/2510.14980.
[18.10.2025 12:44] Extra JSON file exists (./assets/json/2510.14980.json), skip PDF parsing.
[18.10.2025 12:44] Paper image links file exists (./assets/img_data/2510.14980.json), skip HTML parsing.
[18.10.2025 12:44] Success.
[18.10.2025 12:44] Downloading and parsing paper https://huggingface.co/papers/2510.14807.
[18.10.2025 12:44] Extra JSON file exists (./assets/json/2510.14807.json), skip PDF parsing.
[18.10.2025 12:44] Paper image links file exists (./assets/img_data/2510.14807.json), skip HTML parsing.
[18.10.2025 12:44] Success.
[18.10.2025 12:44] Downloading and parsing paper https://huggingface.co/papers/2510.14211.
[18.10.2025 12:44] Extra JSON file exists (./assets/json/2510.14211.json), skip PDF parsing.
[18.10.2025 12:44] Paper image links file exists (./assets/img_data/2510.14211.json), skip HTML parsing.
[18.10.2025 12:44] Success.
[18.10.2025 12:44] Downloading and parsing paper https://huggingface.co/papers/2510.14978.
[18.10.2025 12:44] Extra JSON file exists (./assets/json/2510.14978.json), skip PDF parsing.
[18.10.2025 12:44] Paper image links file exists (./assets/img_data/2510.14978.json), skip HTML parsing.
[18.10.2025 12:44] Success.
[18.10.2025 12:44] Downloading and parsing paper https://huggingface.co/papers/2510.14974.
[18.10.2025 12:44] Extra JSON file exists (./assets/json/2510.14974.json), skip PDF parsing.
[18.10.2025 12:44] Paper image links file exists (./assets/img_data/2510.14974.json), skip HTML parsing.
[18.10.2025 12:44] Success.
[18.10.2025 12:44] Downloading and parsing paper https://huggingface.co/papers/2510.14969.
[18.10.2025 12:44] Extra JSON file exists (./assets/json/2510.14969.json), skip PDF parsing.
[18.10.2025 12:44] Paper image links file exists (./assets/img_data/2510.14969.json), skip HTML parsing.
[18.10.2025 12:44] Success.
[18.10.2025 12:44] Downloading and parsing paper https://huggingface.co/papers/2510.14949.
[18.10.2025 12:44] Extra JSON file exists (./assets/json/2510.14949.json), skip PDF parsing.
[18.10.2025 12:44] Paper image links file exists (./assets/img_data/2510.14949.json), skip HTML parsing.
[18.10.2025 12:44] Success.
[18.10.2025 12:44] Downloading and parsing paper https://huggingface.co/papers/2510.13454.
[18.10.2025 12:44] Extra JSON file exists (./assets/json/2510.13454.json), skip PDF parsing.
[18.10.2025 12:44] Paper image links file exists (./assets/img_data/2510.13454.json), skip HTML parsing.
[18.10.2025 12:44] Success.
[18.10.2025 12:44] Downloading and parsing paper https://huggingface.co/papers/2510.14961.
[18.10.2025 12:44] Extra JSON file exists (./assets/json/2510.14961.json), skip PDF parsing.
[18.10.2025 12:44] Paper image links file exists (./assets/img_data/2510.14961.json), skip HTML parsing.
[18.10.2025 12:44] Success.
[18.10.2025 12:44] Downloading and parsing paper https://huggingface.co/papers/2510.14955.
[18.10.2025 12:44] Extra JSON file exists (./assets/json/2510.14955.json), skip PDF parsing.
[18.10.2025 12:44] Paper image links file exists (./assets/img_data/2510.14955.json), skip HTML parsing.
[18.10.2025 12:44] Success.
[18.10.2025 12:44] Downloading and parsing paper https://huggingface.co/papers/2510.13996.
[18.10.2025 12:44] Extra JSON file exists (./assets/json/2510.13996.json), skip PDF parsing.
[18.10.2025 12:44] Paper image links file exists (./assets/img_data/2510.13996.json), skip HTML parsing.
[18.10.2025 12:44] Success.
[18.10.2025 12:44] Downloading and parsing paper https://huggingface.co/papers/2510.10472.
[18.10.2025 12:44] Extra JSON file exists (./assets/json/2510.10472.json), skip PDF parsing.
[18.10.2025 12:44] Paper image links file exists (./assets/img_data/2510.10472.json), skip HTML parsing.
[18.10.2025 12:44] Success.
[18.10.2025 12:44] Downloading and parsing paper https://huggingface.co/papers/2510.14913.
[18.10.2025 12:44] Extra JSON file exists (./assets/json/2510.14913.json), skip PDF parsing.
[18.10.2025 12:44] Paper image links file exists (./assets/img_data/2510.14913.json), skip HTML parsing.
[18.10.2025 12:44] Success.
[18.10.2025 12:44] Downloading and parsing paper https://huggingface.co/papers/2510.13697.
[18.10.2025 12:44] Extra JSON file exists (./assets/json/2510.13697.json), skip PDF parsing.
[18.10.2025 12:44] Paper image links file exists (./assets/img_data/2510.13697.json), skip HTML parsing.
[18.10.2025 12:44] Success.
[18.10.2025 12:44] Downloading and parsing paper https://huggingface.co/papers/2510.12764.
[18.10.2025 12:44] Extra JSON file exists (./assets/json/2510.12764.json), skip PDF parsing.
[18.10.2025 12:44] Paper image links file exists (./assets/img_data/2510.12764.json), skip HTML parsing.
[18.10.2025 12:44] Success.
[18.10.2025 12:44] Downloading and parsing paper https://huggingface.co/papers/2510.14976.
[18.10.2025 12:44] Extra JSON file exists (./assets/json/2510.14976.json), skip PDF parsing.
[18.10.2025 12:44] Paper image links file exists (./assets/img_data/2510.14976.json), skip HTML parsing.
[18.10.2025 12:44] Success.
[18.10.2025 12:44] Downloading and parsing paper https://huggingface.co/papers/2510.14919.
[18.10.2025 12:44] Extra JSON file exists (./assets/json/2510.14919.json), skip PDF parsing.
[18.10.2025 12:44] Paper image links file exists (./assets/img_data/2510.14919.json), skip HTML parsing.
[18.10.2025 12:44] Success.
[18.10.2025 12:44] Downloading and parsing paper https://huggingface.co/papers/2510.14252.
[18.10.2025 12:44] Extra JSON file exists (./assets/json/2510.14252.json), skip PDF parsing.
[18.10.2025 12:44] Paper image links file exists (./assets/img_data/2510.14252.json), skip HTML parsing.
[18.10.2025 12:44] Success.
[18.10.2025 12:44] Downloading and parsing paper https://huggingface.co/papers/2510.13913.
[18.10.2025 12:44] Extra JSON file exists (./assets/json/2510.13913.json), skip PDF parsing.
[18.10.2025 12:44] Paper image links file exists (./assets/img_data/2510.13913.json), skip HTML parsing.
[18.10.2025 12:44] Success.
[18.10.2025 12:44] Downloading and parsing paper https://huggingface.co/papers/2510.06694.
[18.10.2025 12:44] Extra JSON file exists (./assets/json/2510.06694.json), skip PDF parsing.
[18.10.2025 12:44] Paper image links file exists (./assets/img_data/2510.06694.json), skip HTML parsing.
[18.10.2025 12:44] Success.
[18.10.2025 12:44] Downloading and parsing paper https://huggingface.co/papers/2510.14942.
[18.10.2025 12:44] Extra JSON file exists (./assets/json/2510.14942.json), skip PDF parsing.
[18.10.2025 12:44] Paper image links file exists (./assets/img_data/2510.14942.json), skip HTML parsing.
[18.10.2025 12:44] Success.
[18.10.2025 12:44] Downloading and parsing paper https://huggingface.co/papers/2510.14351.
[18.10.2025 12:44] Extra JSON file exists (./assets/json/2510.14351.json), skip PDF parsing.
[18.10.2025 12:44] Paper image links file exists (./assets/img_data/2510.14351.json), skip HTML parsing.
[18.10.2025 12:44] Success.
[18.10.2025 12:44] Downloading and parsing paper https://huggingface.co/papers/2510.14095.
[18.10.2025 12:44] Extra JSON file exists (./assets/json/2510.14095.json), skip PDF parsing.
[18.10.2025 12:44] Paper image links file exists (./assets/img_data/2510.14095.json), skip HTML parsing.
[18.10.2025 12:44] Success.
[18.10.2025 12:44] Downloading and parsing paper https://huggingface.co/papers/2510.13910.
[18.10.2025 12:44] Extra JSON file exists (./assets/json/2510.13910.json), skip PDF parsing.
[18.10.2025 12:44] Paper image links file exists (./assets/img_data/2510.13910.json), skip HTML parsing.
[18.10.2025 12:44] Success.
[18.10.2025 12:44] Downloading and parsing paper https://huggingface.co/papers/2510.10390.
[18.10.2025 12:44] Extra JSON file exists (./assets/json/2510.10390.json), skip PDF parsing.
[18.10.2025 12:44] Paper image links file exists (./assets/img_data/2510.10390.json), skip HTML parsing.
[18.10.2025 12:44] Success.
[18.10.2025 12:44] Downloading and parsing paper https://huggingface.co/papers/2510.13161.
[18.10.2025 12:44] Extra JSON file exists (./assets/json/2510.13161.json), skip PDF parsing.
[18.10.2025 12:44] Paper image links file exists (./assets/img_data/2510.13161.json), skip HTML parsing.
[18.10.2025 12:44] Success.
[18.10.2025 12:44] Enriching papers with extra data.
[18.10.2025 12:44] ********************************************************************************
[18.10.2025 12:44] Abstract 0. PsiloQA, a multilingual dataset with span-level hallucinations, enhances hallucination detection in large language models across 14 languages using an automated pipeline and encoder-based models.  					AI-generated summary 				 Hallucination detection remains a fundamental challenge for the safe and...
[18.10.2025 12:44] ********************************************************************************
[18.10.2025 12:44] Abstract 1. AEPO, an agentic RL algorithm, addresses entropy-related challenges in web agent training, enhancing performance and stability across various datasets.  					AI-generated summary 				 Recently, Agentic Reinforcement Learning (Agentic RL) has made significant progress in incentivizing the multi-turn,...
[18.10.2025 12:44] ********************************************************************************
[18.10.2025 12:44] Abstract 2. A diffusion-based model addresses copy-paste artifacts in text-to-image generation by using a large-scale paired dataset and a contrastive identity loss to balance identity fidelity and variation.  					AI-generated summary 				 Identity-consistent generation has become an important focus in text-to...
[18.10.2025 12:44] ********************************************************************************
[18.10.2025 12:44] Abstract 3. Alpha-Service, a unified framework for proactive AI assistance, uses a multi-agent system on AI glasses to detect service opportunities and provide timely, personalized assistance.  					AI-generated summary 				 In an era where AI is evolving from a passive tool into an active and adaptive companio...
[18.10.2025 12:44] ********************************************************************************
[18.10.2025 12:44] Abstract 4. NEO, a novel family of native Vision-Language Models, addresses fundamental constraints and integrates vision and language within a unified framework, achieving competitive performance with limited data.  					AI-generated summary 				 The edifice of native Vision-Language Models (VLMs) has emerged ...
[18.10.2025 12:44] ********************************************************************************
[18.10.2025 12:44] Abstract 5. ImagerySearch, a prompt-guided adaptive test-time search strategy, enhances video generation in imaginative scenarios by dynamically adjusting search spaces and reward functions, outperforming existing methods on a new benchmark, LDT-Bench.  					AI-generated summary 				 Video generation models hav...
[18.10.2025 12:44] ********************************************************************************
[18.10.2025 12:44] Abstract 6. LaSeR, a reinforcement learning algorithm, enhances Large Language Models by aligning last-token self-rewarding scores with verifier-based reasoning rewards, improving reasoning performance and inference-time scaling.  					AI-generated summary 				 Reinforcement Learning with Verifiable Rewards (RL...
[18.10.2025 12:44] ********************************************************************************
[18.10.2025 12:44] Abstract 7. BitNet Distillation fine-tunes large language models to 1.58-bit precision using SubLN, multi-head attention distillation, and continual pre-training, achieving comparable performance with significant memory and inference speed improvements.  					AI-generated summary 				 In this paper, we present ...
[18.10.2025 12:44] ********************************************************************************
[18.10.2025 12:44] Abstract 8. Information Gain-based Policy Optimization (IGPO) enhances multi-turn reasoning in large language models by providing dense intrinsic rewards derived from the model's belief updates, improving accuracy and sample efficiency.  					AI-generated summary 				 Large language model (LLM)-based agents are...
[18.10.2025 12:44] ********************************************************************************
[18.10.2025 12:44] Abstract 9. PaddleOCR-VL, a vision-language model combining NaViT-style visual encoder and ERNIE-4.5 language model, achieves state-of-the-art performance in document parsing with minimal resource consumption.  					AI-generated summary 				 In this report, we propose PaddleOCR-VL, a SOTA and resource-efficient...
[18.10.2025 12:44] ********************************************************************************
[18.10.2025 12:44] Abstract 10. Misaligned tokenization in large language models for code leads to inconsistent model behavior, necessitating grammar-aware tokenization.  					AI-generated summary 				 Large language models (LLMs) for code rely on subword tokenizers, such as byte-pair encoding (BPE), learned from mixed natural lan...
[18.10.2025 12:44] ********************************************************************************
[18.10.2025 12:44] Abstract 11. Elastic-Cache optimizes key-value cache management in diffusion large language models to reduce decoding latency without sacrificing prediction accuracy.  					AI-generated summary 				 This work studies how to adaptively recompute key-value (KV) caches for diffusion large language models (DLMs) to ...
[18.10.2025 12:44] ********************************************************************************
[18.10.2025 12:44] Abstract 12. MathCanvas enhances Large Multimodal Models with Visual Chain-of-Thought capabilities for mathematics through pre-training on diagram generation and fine-tuning on visual-textual reasoning, achieving significant improvements on math benchmarks.  					AI-generated summary 				 While Large Language Mo...
[18.10.2025 12:44] ********************************************************************************
[18.10.2025 12:44] Abstract 13. VideoReward Thinker enhances multimodal reward models with visual reasoning operations and a configurable memory window, improving accuracy on video preference benchmarks.  					AI-generated summary 				 Recent advancements in multimodal reward models (RMs) have substantially improved post-training ...
[18.10.2025 12:44] ********************************************************************************
[18.10.2025 12:44] Abstract 14. LLMs process factual queries and hallucinations similarly when associated with subject knowledge, leading to indistinguishable internal representations, but produce distinct representations for hallucinations without subject knowledge.  					AI-generated summary 				 Recent work suggests that large ...
[18.10.2025 12:44] ********************************************************************************
[18.10.2025 12:44] Abstract 15. A novel agentic framework, VLA^2, enhances vision-language-action models by integrating external modules like web retrieval and object detection, improving generalization to unseen objects and descriptions.  					AI-generated summary 				 Current vision-language-action (VLA) models, pre-trained on l...
[18.10.2025 12:44] ********************************************************************************
[18.10.2025 12:44] Abstract 16. COIG-Writer, a Chinese creative writing dataset, reveals that process supervision and general-purpose data are crucial for creative writing, with cultural-bound capabilities and lexical diversity impacting performance.  					AI-generated summary 				 Large language models exhibit systematic deficien...
[18.10.2025 12:44] ********************************************************************************
[18.10.2025 12:44] Abstract 17. LATTICE, a hierarchical retrieval framework, enables efficient and accurate reasoning over large document collections using a semantic tree structure and a traversal algorithm that calibrates relevance scores.  					AI-generated summary 				 Modern IR systems are increasingly tasked with answering c...
[18.10.2025 12:44] ********************************************************************************
[18.10.2025 12:44] Abstract 18. mxbai-edge-colbert-v0 models, with 17M and 32M parameters, demonstrate superior retrieval performance on short-text and long-context benchmarks compared to ColBERTv2.  					AI-generated summary 				 In this work, we introduce mxbai-edge-colbert-v0 models, at two different parameter counts: 17M and 3...
[18.10.2025 12:44] ********************************************************************************
[18.10.2025 12:44] Abstract 19. Generative reward models with explicit reasoning chains outperform sequence-based reward models and zero-shot language models in preference learning for creative writing, indicating the need for intermediate reasoning in capturing subjective quality.  					AI-generated summary 				 Current preferenc...
[18.10.2025 12:44] ********************************************************************************
[18.10.2025 12:44] Abstract 20. Qwen3Guard introduces multilingual safety guardrail models with fine-grained tri-class judgments and real-time token-level safety monitoring for large language models.  					AI-generated summary 				 As large language models (LLMs) become more capable and widely used, ensuring the safety of their ou...
[18.10.2025 12:44] ********************************************************************************
[18.10.2025 12:44] Abstract 21. LiveResearchBench and DeepEval provide a comprehensive framework for evaluating deep research systems across various domains, focusing on real-time web search, synthesis, and citation-grounded long-form reports.  					AI-generated summary 				 Deep research -- producing comprehensive, citation-groun...
[18.10.2025 12:44] ********************************************************************************
[18.10.2025 12:44] Abstract 22. Continual exposure to low-quality web text leads to cognitive decline in large language models, affecting reasoning, context understanding, safety, and personality traits, with partial recovery possible through instruction tuning and clean data pre-training.  					AI-generated summary 				 We propos...
[18.10.2025 12:44] ********************************************************************************
[18.10.2025 12:44] Abstract 23. AdaMoE, a Mixture-of-Experts architecture, enhances VLA models by leveraging pretrained weights and improving computational efficiency, achieving superior performance in robotic manipulation tasks.  					AI-generated summary 				 Vision-Language-Action (VLA) models are experiencing rapid development...
[18.10.2025 12:44] ********************************************************************************
[18.10.2025 12:44] Abstract 24. ...
[18.10.2025 12:44] ********************************************************************************
[18.10.2025 12:44] Abstract 25. State-of-the-art LLMs are benchmarked in a machine design testbed, BesiegeField, highlighting the need for reinforcement learning to improve spatial reasoning, strategic assembly, and instruction-following capabilities.  					AI-generated summary 				 The design of complex machines stands as both a ...
[18.10.2025 12:44] ********************************************************************************
[18.10.2025 12:44] Abstract 26. Simple Pass@K Optimization (SimKO) addresses over-concentration in reinforcement learning with verifiable rewards (RLVR) by asymmetrically adjusting token probabilities, enhancing exploration and pass@K performance.  					AI-generated summary 				 Reinforcement learning with verifiable rewards (RLVR...
[18.10.2025 12:44] ********************************************************************************
[18.10.2025 12:44] Abstract 27. LiteStage, a latency-aware layer skipping framework, enhances multi-stage reasoning by optimizing layer budgets and suppressing redundant output tokens, achieving significant speedup with minimal accuracy loss.  					AI-generated summary 				 Multi-stage reasoning has emerged as an effective strateg...
[18.10.2025 12:44] ********************************************************************************
[18.10.2025 12:44] Abstract 28. A new training paradigm for image editing models uses unrolled diffusion models and vision-language feedback to achieve performance comparable to supervised models without paired data.  					AI-generated summary 				 Recent image editing models have achieved impressive results while following natura...
[18.10.2025 12:44] ********************************************************************************
[18.10.2025 12:44] Abstract 29. Policy-based flow models enable efficient and high-quality image generation by distilling teacher models into student models with dynamic flow velocities, improving diversity and quality.  					AI-generated summary 				 Few-step diffusion or flow-based generative models typically distill a velocity-...
[18.10.2025 12:44] ********************************************************************************
[18.10.2025 12:44] Abstract 30. ...
[18.10.2025 12:44] ********************************************************************************
[18.10.2025 12:44] Abstract 31. A new benchmark and encoder-based mitigation strategy improve multimodal generative models' performance on dialectal textual input without degrading performance on Standard American English.  					AI-generated summary 				 Contact languages like English exhibit rich regional variations in the form o...
[18.10.2025 12:44] ********************************************************************************
[18.10.2025 12:44] Abstract 32. VIST3A combines latent text-to-video models and 3D reconstruction systems to generate high-quality 3D scenes from text, improving upon prior methods.  					AI-generated summary 				 The rapid progress of large, pretrained models for both visual content generation and 3D reconstruction opens up new p...
[18.10.2025 12:44] ********************************************************************************
[18.10.2025 12:44] Abstract 33. A new diffusion forcing sampler accelerates token generation in recurrent-depth language models, offering a 5x speedup without tuning.  					AI-generated summary 				 Language models with recurrent depth, also referred to as universal or looped when considering transformers, are defined by the capac...
[18.10.2025 12:44] ********************************************************************************
[18.10.2025 12:44] Abstract 34. RealDPO, a novel preference learning paradigm using real-world data, enhances motion realism in video generative models through Direct Preference Optimization and iterative self-correction.  					AI-generated summary 				 Video generative models have recently achieved notable advancements in synthes...
[18.10.2025 12:44] ********************************************************************************
[18.10.2025 12:44] Abstract 35. The German Commons provides a large-scale, openly licensed dataset for training German language models, addressing the scarcity of such data.  					AI-generated summary 				 Large language model development relies on large-scale training corpora, yet most contain data of unclear licensing status, li...
[18.10.2025 12:44] ********************************************************************************
[18.10.2025 12:44] Abstract 36. FML-bench evaluates automatic machine learning research agents on diverse fundamental problems using a unified framework with multiple metrics, highlighting the importance of broad exploration strategies.  					AI-generated summary 				 Large language models (LLMs) have sparked growing interest in a...
[18.10.2025 12:44] ********************************************************************************
[18.10.2025 12:44] Abstract 37. A hybrid approach combining discriminative verification with self-consistency outperforms generative verification in test-time scaling for large language models, achieving higher accuracy within a fixed compute budget.  					AI-generated summary 				 Test-time scaling is a powerful strategy for boos...
[18.10.2025 12:44] ********************************************************************************
[18.10.2025 12:44] Abstract 38. Extending the context window and adapting to a new rotary positional embedding scaling parameter improve repository-level code completion in OpenCoder, achieving performance comparable to larger models with less data.  					AI-generated summary 				 Repository-level pretraining is commonly used to e...
[18.10.2025 12:44] ********************************************************************************
[18.10.2025 12:44] Abstract 39. AnyUp is a feature-agnostic upsampling method that generalizes across different vision features and resolutions without requiring re-training.  					AI-generated summary 				 We introduce AnyUp, a method for feature upsampling that can be applied to any vision feature at any resolution, without enco...
[18.10.2025 12:44] ********************************************************************************
[18.10.2025 12:44] Abstract 40. Ponimator uses conditional diffusion models to generate and synthesize interactive poses from motion capture data, enabling versatile interaction animation tasks.  					AI-generated summary 				 Close-proximity human-human interactive poses convey rich contextual information about interaction dynami...
[18.10.2025 12:44] ********************************************************************************
[18.10.2025 12:44] Abstract 41. A framework models downstream performance of large language models as a function of training compute and context, offering insights into efficient design for long-context tasks.  					AI-generated summary 				 Scaling laws have transformed our understanding of large language models by linking upstre...
[18.10.2025 12:44] ********************************************************************************
[18.10.2025 12:44] Abstract 42. The MoM framework enhances RAG by transforming text processing from passive chunking to proactive understanding, enabling LLMs to generate structured document memories and SLMs to develop human-like reading abilities.  					AI-generated summary 				 The traditional RAG paradigm, which typically enga...
[18.10.2025 12:44] ********************************************************************************
[18.10.2025 12:44] Abstract 43. A two-pronged data synthesis pipeline generates complex question-answer pairs, enabling the training of more effective web-based research agents with higher diversity in tool use.  					AI-generated summary 				 Web-based 'deep research' agents aim to solve complex question - answering tasks through...
[18.10.2025 12:44] ********************************************************************************
[18.10.2025 12:44] Abstract 44. SCas4D, a cascaded optimization framework using 3D Gaussian Splatting, efficiently models dynamic scenes by leveraging hierarchical deformation patterns, enabling fast convergence and high-quality results in various tasks.  					AI-generated summary 				 Persistent dynamic scene modeling for trackin...
[18.10.2025 12:44] ********************************************************************************
[18.10.2025 12:44] Abstract 45. GroundedPRM uses Monte Carlo Tree Search and external validation to improve multi-step reasoning in LLMs with fewer, higher-quality annotations.  					AI-generated summary 				 Process Reward Models (PRMs) aim to improve multi-step reasoning in Large Language Models (LLMs) by supervising intermediat...
[18.10.2025 12:44] ********************************************************************************
[18.10.2025 12:44] Abstract 46. Beyond One World benchmark evaluates LLMs' ability to consistently portray version-specific superheroes across different canons through factual recall and ethical reasoning tasks.  					AI-generated summary 				 Large language models (LLMs) are increasingly used as role-playing agents, yet their cap...
[18.10.2025 12:44] ********************************************************************************
[18.10.2025 12:44] Abstract 47. Transformer networks are enhanced with four architectural mechanisms to improve out-of-distribution generalization and algorithmic reasoning.  					AI-generated summary 				 Systematic, compositional generalization beyond the training distribution remains a core challenge in machine learning -- and ...
[18.10.2025 12:44] ********************************************************************************
[18.10.2025 12:44] Abstract 48. RAGCap-Bench evaluates intermediate tasks in agentic RAG workflows, highlighting the importance of enhancing these capabilities for better end-to-end performance.  					AI-generated summary 				 Retrieval-Augmented Generation (RAG) mitigates key limitations of Large Language Models (LLMs)-such as fa...
[18.10.2025 12:44] ********************************************************************************
[18.10.2025 12:44] Abstract 49. RefusalBench evaluates the selective refusal capability of language models in RAG systems using programmatically generated test cases, revealing systematic failure patterns and offering a path for improvement.  					AI-generated summary 				 The ability of language models in RAG systems to selective...
[18.10.2025 12:44] ********************************************************************************
[18.10.2025 12:44] Abstract 50. Mirror Speculative Decoding accelerates large language model inference by parallelizing speculative execution across heterogeneous accelerators and using multi-token speculative streaming to reduce draft latency without compromising acceptance rates.  					AI-generated summary 				 Speculative decod...
[18.10.2025 12:44] Read previous papers.
[18.10.2025 12:44] Generating reviews via LLM API.
[18.10.2025 12:44] Using data from previous issue: {"categories": ["#hallucinations", "#low_resource", "#multilingual", "#dataset"], "emoji": "üçÑ", "ru": {"title": "–ü–æ–π–º–∞—Ç—å –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏—é: –¥–µ—Ç–µ–∫—Ü–∏—è –Ω–∞ —É—Ä–æ–≤–Ω–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –≤ 14 —è–∑—ã–∫–∞—Ö", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω PsiloQA ‚Äî –∫—Ä—É–ø–Ω—ã–π –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π –≤ LLM –Ω–∞ —É—Ä–æ–≤–Ω–µ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö 
[18.10.2025 12:44] Using data from previous issue: {"categories": ["#agents", "#rl", "#training", "#optimization"], "emoji": "‚öñÔ∏è", "ru": {"title": "–ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ —ç–Ω—Ç—Ä–æ–ø–∏–∏ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –≤–µ–±-–∞–≥–µ–Ω—Ç–æ–≤", "desc": "AEPO ‚Äî —ç—Ç–æ –∞–ª–≥–æ—Ä–∏—Ç–º –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –∞–≥–µ–Ω—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–π —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å —ç–Ω—Ç—Ä–æ–ø–∏–µ–π –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –≤–µ–±-–∞–≥–µ–Ω—Ç–æ–≤. –ê–ª
[18.10.2025 12:44] Using data from previous issue: {"categories": ["#dataset", "#cv", "#training", "#diffusion", "#benchmark"], "emoji": "üé≠", "ru": {"title": "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ª–∏—Ü –±–µ–∑ –∫–æ–ø–∏–ø–∞—Å—Ç–∞: –±–∞–ª–∞–Ω—Å –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç–∏ –∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Ä–µ—à–∞—é—Ç –ø—Ä–æ–±–ª–µ–º—É ¬´–∫–æ–ø–∏–ø–∞—Å—Ç–∞¬ª –≤ text-to-image –º–æ–¥–µ–ª—è—Ö, –∫–æ–≥–¥–∞ AI –ø—Ä–æ—Å—Ç–æ –∫–æ–ø–∏—Ä—É–µ—Ç —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω–æ–µ –ª–∏—Ü–æ –≤–º–µ—Å—Ç–æ —Å
[18.10.2025 12:44] Using data from previous issue: {"categories": ["#agents", "#agi", "#multimodal", "#optimization", "#games", "#interpretability"], "emoji": "ü§ñ", "ru": {"title": "–ü—Ä–æ–∞–∫—Ç–∏–≤–Ω—ã–π AI: –ø–æ–º–æ—â–Ω–∏–∫, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–µ–¥—É–≥–∞–¥—ã–≤–∞–µ—Ç –≤–∞—à–∏ –Ω—É–∂–¥—ã", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ Alpha-Service, –∫–æ—Ç–æ—Ä–∞—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç AI-–æ—á–∫–∏ –¥–ª—è –ø—Ä–æ–∞–∫—Ç–∏–≤–Ω–æ–π –ø–æ–º–æ—â–∏ –ø–æ–ª—å–∑
[18.10.2025 12:44] Using data from previous issue: {"categories": ["#agi", "#multimodal", "#alignment", "#architecture", "#open_source"], "emoji": "üîó", "ru": {"title": "NEO: –Ω–∞—Ç–∏–≤–Ω—ã–µ Vision-Language –º–æ–¥–µ–ª–∏ —Å –µ–¥–∏–Ω—ã–º –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç NEO ‚Äî –Ω–æ–≤–æ–µ —Å–µ–º–µ–π—Å—Ç–≤–æ –Ω–∞—Ç–∏–≤–Ω—ã—Ö Vision-Language Models (VLM), –∫–æ—Ç–æ—Ä—ã–µ –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É—é—Ç –≤–∏–∑—É–∞–ª—å–Ω—É
[18.10.2025 12:44] Using data from previous issue: {"categories": ["#video", "#benchmark", "#optimization", "#long_context"], "emoji": "üé®", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ –¥–ª—è –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ–π –≤–∏–¥–µ–æ–≥–µ–Ω–µ—Ä–∞—Ü–∏–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç ImagerySearch - –º–µ—Ç–æ–¥ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –≤–æ –≤—Ä–µ–º—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –≤ —Ç–≤–æ—Ä—á–µ—Å–∫–∏—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö. –û—Å
[18.10.2025 12:44] Using data from previous issue: {"categories": ["#rl", "#reasoning", "#rlhf", "#training", "#optimization"], "emoji": "üéØ", "ru": {"title": "–°–∞–º–æ–æ—Ü–µ–Ω–∫–∞ —á–µ—Ä–µ–∑ –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ç–æ–∫–µ–Ω –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è reasoning", "desc": "LaSeR ‚Äî —ç—Ç–æ –∞–ª–≥–æ—Ä–∏—Ç–º reinforcement learning, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç reasoning —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ LLM –ø—É—Ç—ë–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–µ—à–µ–Ω–∏–π 
[18.10.2025 12:44] Using data from previous issue: {"categories": ["#training", "#inference", "#architecture", "#optimization", "#small_models"], "emoji": "üîΩ", "ru": {"title": "–°–∂–∞—Ç–∏–µ LLM –¥–æ —Ç–µ—Ä–Ω–∞—Ä–Ω—ã—Ö –≤–µ—Å–æ–≤ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∫–∞—á–µ—Å—Ç–≤–∞", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω BitNet Distillation (BitDistill) ‚Äî –ª–µ–≥–∫–æ–≤–µ—Å–Ω—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –ø–æ–ª–Ω–æ—Ç–æ—á–Ω—ã—Ö LLM –≤ –º–æ
[18.10.2025 12:44] Using data from previous issue: {"categories": ["#agents", "#rl", "#reasoning", "#rlhf", "#training", "#optimization"], "emoji": "üéØ", "ru": {"title": "–ü–ª–æ—Ç–Ω—ã–µ –Ω–∞–≥—Ä–∞–¥—ã —á–µ—Ä–µ–∑ –ø—Ä–∏—Ä–æ—Å—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è –º–Ω–æ–≥–æ—à–∞–≥–æ–≤–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ IGPO –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è LLM-–∞–≥–µ–Ω—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é reinforcement lear
[18.10.2025 12:44] Using data from previous issue: {"categories": ["#cv", "#multimodal", "#training", "#science", "#low_resource", "#benchmark"], "emoji": "üìÑ", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏", "desc": "PaddleOCR-VL ‚Äî —ç—Ç–æ –∫–æ–º–ø–∞–∫—Ç–Ω–∞—è vision-language –º–æ–¥–µ–ª—å –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∞—è –≤–∏–∑—É–∞–ª—å–Ω—ã–π
[18.10.2025 12:44] Using data from previous issue: {"categories": ["#alignment", "#interpretability", "#data", "#plp", "#dataset", "#architecture"], "emoji": "üî§", "ru": {"title": "–ü—Ä–æ–±–ª–µ–º–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –∫–æ–¥–∞: –∫–æ–≥–¥–∞ –ø—Ä–æ–±–µ–ª—ã –º–µ–Ω—è—é—Ç –ø–æ–≤–µ–¥–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏ —Å–µ—Ä—å—ë–∑–Ω—É—é –ø—Ä–æ–±–ª–µ–º—É –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö –¥–ª—è –∫–æ–¥–∞: —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ —Ç–æ–∫–µ–Ω–∏–∑
[18.10.2025 12:44] Using data from previous issue: {"categories": ["#training", "#inference", "#architecture", "#optimization", "#diffusion"], "emoji": "‚ö°", "ru": {"title": "–£–º–Ω–æ–µ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –º–µ—Ç–æ–¥ Elastic-Cache –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è key-value –∫—ç—à–µ–º –≤ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö LLM. –ê–≤—Ç
[18.10.2025 12:44] Using data from previous issue: {"categories": ["#dataset", "#reasoning", "#multimodal", "#games", "#math", "#benchmark"], "emoji": "üìê", "ru": {"title": "–í–∏–∑—É–∞–ª—å–Ω–∞—è —Ü–µ–ø–æ—á–∫–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –¥–ª—è –º–∞—Ç–µ–º–∞—Ç–∏–∫–∏", "desc": "MathCanvas ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Ä–µ—à–µ–Ω–∏—é –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤–∏–∑
[18.10.2025 12:44] Using data from previous issue: {"categories": ["#rl", "#reasoning", "#multimodal", "#open_source", "#long_context", "#benchmark"], "emoji": "üé¨", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ AI —Ä–∞–∑–º—ã—à–ª—è—Ç—å –≤–∏–∑—É–∞–ª—å–Ω–æ –ø—Ä–∏ –æ—Ü–µ–Ω–∫–µ –≤–∏–¥–µ–æ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç VideoReward Thinker ‚Äî –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–º –º–æ–¥–µ–ª—è–º –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑
[18.10.2025 12:44] Using data from previous issue: {"categories": ["#data", "#interpretability", "#multimodal", "#hallucinations"], "emoji": "üé≠", "ru": {"title": "LLM –Ω–µ –∑–Ω–∞—é—Ç, —á—Ç–æ –æ–Ω–∏ –Ω–µ –∑–Ω–∞—é—Ç: –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏ –Ω–µ–æ—Ç–ª–∏—á–∏–º—ã –æ—Ç —Ñ–∞–∫—Ç–æ–≤", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ LLM –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–ø—Ä–æ—Å—ã –∏ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏ —Å—Ö–æ–∂–∏–º –æ–±—Ä–∞–∑–æ–º, –∫–æ–≥–¥–∞ –æ–Ω–∏ —Å–≤—è–∑
[18.10.2025 12:44] Using data from previous issue: {"categories": ["#agi", "#benchmark", "#optimization", "#cv", "#agents"], "emoji": "ü§ñ", "ru": {"title": "–ê–≥–µ–Ω—Ç–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±–æ–±—â–µ–Ω–∏—é VLA –º–æ–¥–µ–ª–µ–π –Ω–∞ –Ω–µ–≤–∏–¥–µ–Ω–Ω—ã–µ –æ–±—ä–µ–∫—Ç—ã", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç VLA^2 ‚Äî –Ω–æ–≤—ã–π –∞–≥–µ–Ω—Ç–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è vision-language-action –º–æ–¥–µ–ª–µ–π –≤ —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–µ. –û—Å–Ω–æ–≤–Ω–∞
[18.10.2025 12:44] Using data from previous issue: {"categories": ["#story_generation", "#multilingual", "#low_resource", "#dataset"], "emoji": "‚úçÔ∏è", "ru": {"title": "–ö—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç—å AI —Ç—Ä–µ–±—É–µ—Ç –±–∞–ª–∞–Ω—Å–∞ –ª–æ–≥–∏–∫–∏ –∏ —è–∑—ã–∫–∞", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Å–æ–∑–¥–∞–ª–∏ –¥–∞—Ç–∞—Å–µ—Ç COIG-Writer –¥–ª—è –æ–±—É—á–µ–Ω–∏—è LLM –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ–º—É –ø–∏—Å—å–º—É –Ω–∞ –∫–∏—Ç–∞–π—Å–∫–æ–º —è–∑—ã–∫–µ, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π 1665 –ø—Ä–∏–º–µ—Ä–æ–≤ —Å –ø
[18.10.2025 12:44] Using data from previous issue: {"categories": ["#rag", "#reasoning", "#optimization", "#benchmark"], "emoji": "üå≥", "ru": {"title": "–ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ –¥–µ—Ä–µ–≤–æ —Å –ª–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç—å—é", "desc": "LATTICE ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è information retrieval, –∫–æ—Ç–æ—Ä—ã–π –æ—Ä–≥–∞–Ω–∏–∑—É–µ—Ç –±–æ–ª—å—à–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –≤–∏–¥–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –¥–µ—Ä–µ–≤
[18.10.2025 12:44] Using data from previous issue: {"categories": ["#long_context", "#training", "#dataset", "#optimization", "#benchmark", "#small_models"], "emoji": "üîç", "ru": {"title": "–ú–æ—â–Ω—ã–π –ø–æ–∏—Å–∫ –≤ –∫–∞—Ä–º–∞–Ω–µ: –∫–æ–º–ø–∞–∫—Ç–Ω—ã–µ –º–æ–¥–µ–ª–∏ –ø–æ–±–µ–∂–¥–∞—é—Ç –≥–∏–≥–∞–Ω—Ç–æ–≤", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ mxbai-edge-colbert-v0 ‚Äî –∫–æ–º–ø–∞–∫—Ç–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –ø–æ–∏—Å–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ 
[18.10.2025 12:44] Using data from previous issue: {"categories": ["#rlhf", "#benchmark", "#reasoning", "#low_resource", "#dataset", "#story_generation"], "emoji": "‚úçÔ∏è", "ru": {"title": "–†–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤–∞–∂–Ω–µ–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ç–≤–æ—Ä—á–µ—Å–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Å–æ–∑–¥–∞–ª–∏ WritingPreferenceBench ‚Äî –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ 1800 –ø—Ä–∏–º–µ—Ä–æ–≤ —Ç–≤–æ—Ä—á–µ—Å–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤
[18.10.2025 12:44] Using data from previous issue: {"categories": ["#data", "#open_source", "#alignment", "#training", "#ethics", "#multilingual", "#low_resource", "#benchmark"], "emoji": "üõ°Ô∏è", "ru": {"title": "–ú–Ω–æ–≥–æ—è–∑—ã—á–Ω–∞—è –∑–∞—â–∏—Ç–∞ LLM —Å —Ç—Ä—ë—Ö—É—Ä–æ–≤–Ω–µ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–µ–π –∏ –ø—Ä–æ–≤–µ—Ä–∫–æ–π –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏", "desc": "Qwen3Guard ‚Äî —ç—Ç–æ —Å–µ–º–µ–π—Å—Ç–≤–æ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö –º–æ–¥
[18.10.2025 12:44] Using data from previous issue: {"categories": ["#agents", "#survey", "#benchmark", "#science"], "emoji": "üîç", "ru": {"title": "–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ AI-–∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–µ–π –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç LiveResearchBench ‚Äî –±–µ–Ω—á–º–∞—Ä–∫ –∏–∑ 100 —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã—Ö –∑–∞–¥–∞—á –¥–ª—è –æ—Ü–µ–Ω–∫–∏ AI-—Å–∏—Å—Ç–µ–º –≥–ª—É–±–æ–∫–æ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è, —Ç—Ä–µ–±—É—é—â–∏—Ö –ø–æ–∏—Å–∫–∞ –∏ —Å–∏–Ω—Ç–µ
[18.10.2025 12:44] Using data from previous issue: {"categories": ["#ethics", "#dataset", "#long_context", "#reasoning", "#data", "#training"], "emoji": "üß†", "ru": {"title": "–î–µ–≥—Ä–∞–¥–∞—Ü–∏—è –º–æ–∑–≥–∞ LLM: –∫–∞–∫ –Ω–µ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Ä–∞–∑—Ä—É—à–∞—é—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç, —á—Ç–æ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –Ω–∏–∑–∫–æ–∫
[18.10.2025 12:44] Using data from previous issue: {"categories": ["#optimization", "#robotics", "#agi", "#benchmark", "#architecture"], "emoji": "ü§ñ", "ru": {"title": "–°–æ–≤–º–µ—Å—Ç–Ω–∞—è —Ä–∞–±–æ—Ç–∞ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ä–æ–±–æ—Ç–∞–º–∏", "desc": "–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ AdaMoE –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø–æ–¥—Ö–æ–¥ Mixture-of-Experts –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è Vision-Language-Action –º–æ–¥–µ–ª–µ–π,
[18.10.2025 12:44] Using data from previous issue: {"categories": [], "emoji": "üéØ", "ru": {"title": "–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–æ–º–ø—Ç–æ–≤ —á–µ—Ä–µ–∑ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ–º–ø—Ç–æ–≤ –¥–ª—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∏—Å–ø–æ–ª—å–∑—É—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫ –Ω–µ–ø–æ—Å—Ä–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Ç–æ–∫–µ–Ω–æ–≤
[18.10.2025 12:44] Using data from previous issue: {"categories": ["#agents", "#reasoning", "#benchmark", "#rl", "#optimization", "#open_source", "#dataset", "#games"], "emoji": "üîß", "ru": {"title": "–£—á–∏–º LLM –∫–æ–Ω—Å—Ç—Ä—É–∏—Ä–æ–≤–∞—Ç—å –º–∞—à–∏–Ω—ã –∫–∞–∫ –∏–Ω–∂–µ–Ω–µ—Ä—ã", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Å–æ–∑–¥–∞–ª–∏ —Ç–µ—Å—Ç–æ–≤—É—é —Å—Ä–µ–¥—É BesiegeField –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–≥—Ä—ã Besiege –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç
[18.10.2025 12:44] Using data from previous issue: {"categories": ["#rlhf", "#training", "#rl", "#optimization", "#reasoning"], "emoji": "üîç", "ru": {"title": "–£–ª—É—á—à–µ–Ω–∏–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –≤ RLVR —Å –ø–æ–º–æ—â—å—é SimKO", "desc": "–í —Å—Ç–∞—Ç—å–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ —á—Ä–µ–∑–º–µ—Ä–Ω–æ–π –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü–∏–∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –≤ –º–µ—Ç–æ–¥–∞—Ö –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º —Å –ø—Ä–æ–≤–µ—Ä—è–µ–º—ã–º–∏ –Ω–∞–≥—Ä–∞–¥–∞–º–∏ (RLV
[18.10.2025 12:44] Using data from previous issue: {"categories": ["#optimization", "#small_models", "#training", "#reasoning", "#inference"], "emoji": "‚ö°", "ru": {"title": "–£—Å–∫–æ—Ä–µ–Ω–∏–µ –º–Ω–æ–≥–æ—ç—Ç–∞–ø–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è —á–µ—Ä–µ–∑ —É–º–Ω—ã–π –ø—Ä–æ–ø—É—Å–∫ —Å–ª–æ—ë–≤", "desc": "LiteStage ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –º–Ω–æ–≥–æ—ç—Ç–∞–ø–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –º–∞–ª—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö –ø—É—Ç—ë–º –ø—Ä–æ–ø—É—Å–∫
[18.10.2025 12:44] Using data from previous issue: {"categories": ["#cv", "#rlhf", "#training", "#optimization", "#synthetic", "#diffusion", "#benchmark"], "emoji": "‚úÇÔ∏è", "ru": {"title": "–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –±–µ–∑ –ø–∞—Ä–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ –æ–±—Ä–∞—Ç–Ω—É—é —Å–≤—è–∑—å –æ—Ç VLM", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –Ω–æ–≤—É—é –ø–∞—Ä–∞–¥–∏–≥–º—É –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, 
[18.10.2025 12:44] Using data from previous issue: {"categories": ["#training", "#optimization", "#cv", "#diffusion"], "emoji": "üåä", "ru": {"title": "–î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç pi-Flow ‚Äî –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ flow-based –º–æ–¥–µ–ª–µ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –í–º–µ—Å—Ç–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –ø—Ä—è–º–æ–≥–æ –ø—É—Ç–∏ 
[18.10.2025 12:44] Using data from previous issue: {"categories": [], "emoji": "ü§ù", "ru": {"title": "–ö–æ–≥–¥–∞ AI –º–æ–¥–µ–ª—å –Ω–µ —É–≤–µ—Ä–µ–Ω–∞ ‚Äî –ª—É—á—à–µ —Å–ø—Ä–æ—Å–∏—Ç—å —á–µ–ª–æ–≤–µ–∫–∞", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç LLM –æ–ø—Ä–µ–¥–µ–ª—è—Ç—å, –∫–æ–≥–¥–∞ –æ–Ω–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —É–≤–µ—Ä–µ–Ω—ã –≤ –æ—Ç–≤–µ—Ç–µ –∏ –Ω—É–∂–¥–∞—é—Ç—Å—è –≤ –ø–æ–º–æ—â–∏ —á–µ–ª–æ–≤–µ–∫–∞. –°–∏—Å—Ç–µ–º–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∫–∞–ª–∏–±—Ä–æ–≤–∫
[18.10.2025 12:44] Using data from previous issue: {"categories": ["#multimodal", "#low_resource", "#benchmark", "#synthetic"], "emoji": "üó£Ô∏è", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –ø–æ–Ω–∏–º–∞—Ç—å –¥–∏–∞–ª–µ–∫—Ç—ã –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ —è–∑—ã–∫–∞", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Å–æ–∑–¥–∞–ª–∏ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ —à–µ—Å—Ç–∏ –¥–∏–∞–ª–µ–∫—Ç–∞—Ö –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥
[18.10.2025 12:44] Using data from previous issue: {"categories": ["#3d", "#multimodal", "#alignment", "#training", "#optimization"], "emoji": "üé¨", "ru": {"title": "–û—Ç —Ç–µ–∫—Å—Ç–∞ –∫ 3D —á–µ—Ä–µ–∑ –≤–∏–¥–µ–æ: —Å—à–∏–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Å—Ü–µ–Ω", "desc": "VIST3A ‚Äî —ç—Ç–æ –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ 3D-—Å—Ü–µ–Ω –∏–∑ —Ç–µ–∫—Å—Ç–∞, –∫–æ—Ç–æ—Ä—ã–π –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç latent text-to-video –º–æ–¥–µ–ª–∏ —Å —Å
[18.10.2025 12:44] Using data from previous issue: {"categories": ["#architecture", "#inference", "#training", "#reasoning", "#optimization", "#diffusion"], "emoji": "üîÑ", "ru": {"title": "–£—Å–∫–æ—Ä–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤ —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö —á–µ—Ä–µ–∑ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–π —Å—ç–º–ø–ª–∏–Ω–≥", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏ —Å–≤—è–∑—å –º–µ–∂–¥—É —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ —Å —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω–æ–π –≥–ª—É–±–∏–Ω–æ–π (
[18.10.2025 12:44] Using data from previous issue: {"categories": ["#video", "#dataset", "#alignment", "#optimization", "#training"], "emoji": "üé¨", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –≤–∏–¥–µ–æ –¥–µ–ª–∞–µ—Ç AI-–¥–≤–∏–∂–µ–Ω–∏—è –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏", "desc": "RealDPO ‚Äî –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –≤–∏–¥–µ–æ-–º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ä–µ–∞–ª—å–Ω—ã–µ –≤–∏–¥–µ–æ –∫–∞–∫ —ç—Ç–∞–ª–æ–Ω–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã 
[18.10.2025 12:44] Using data from previous issue: {"categories": ["#open_source", "#data", "#low_resource", "#dataset"], "emoji": "üá©üá™", "ru": {"title": "–ù–µ–º–µ—Ü–∫–∏–µ –æ–±—â–∏–Ω—ã: –æ—Ç–∫—Ä—ã—Ç—ã–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –Ω–µ–º–µ—Ü–∫–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ German Commons ‚Äî –∫—Ä—É–ø–Ω–µ–π—à—É—é –∫–æ–ª–ª–µ–∫—Ü–∏—é —Ç–µ–∫—Å—Ç–æ–≤ –Ω–∞ –Ω–µ–º–µ—Ü–∫–æ–º —è–∑—ã–∫–µ —Å –æ—Ç–∫—Ä—ã—Ç—ã–º–∏ –ª–∏—Ü–µ–Ω–∑–∏—è–º–∏ –¥–ª—è –æ–±—É—á–µ
[18.10.2025 12:44] Using data from previous issue: {"categories": ["#agents", "#survey", "#optimization", "#science", "#benchmark"], "emoji": "üî¨", "ru": {"title": "–®–∏—Ä–æ—Ç–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –≤–∞–∂–Ω–µ–µ –≥–ª—É–±–∏–Ω—ã –¥–ª—è AI-–∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–µ–π", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω FML-bench ‚Äî –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∞–≥–µ–Ω—Ç–æ–≤ –¥–ª—è –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç —Å–∞–º–æ—Å—Ç–æ—è
[18.10.2025 12:44] Using data from previous issue: {"categories": ["#optimization", "#inference", "#reasoning", "#training"], "emoji": "‚öñÔ∏è", "ru": {"title": "–î–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ç–∏–≤–Ω–∞—è –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è –ø–æ–±–µ–∂–¥–∞–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—É—é –ø—Ä–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–º –±—é–¥–∂–µ—Ç–µ", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø—Ä–∏ —Ä
[18.10.2025 12:44] Using data from previous issue: {"categories": ["#transfer_learning", "#long_context", "#dataset", "#benchmark", "#training", "#data", "#architecture"], "emoji": "üìÅ", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ –∫–æ–¥–∞ –Ω–∞ —É—Ä–æ–≤–Ω–µ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —É–ª—É—á—à–∏–ª–∏ –º–æ–¥–µ–ª—å OpenCoder –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –∫–æ–¥–∞ –Ω–∞ —É
[18.10.2025 12:44] Using data from previous issue: {"categories": ["#architecture", "#optimization", "#cv", "#inference"], "emoji": "üîç", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∞–ø—Å–µ–º–ø–ª–∏–Ω–≥ –¥–ª—è –ª—é–±—ã—Ö –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Ñ–∏—á–µ–π –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è", "desc": "AnyUp ‚Äî —ç—Ç–æ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –º–µ—Ç–æ–¥ –∞–ø—Å–µ–º–ø–ª–∏–Ω–≥–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –ª—é–±—ã–º–∏ —Ç–∏–ø–∞–º–∏ —Ñ–∏—á–µ
[18.10.2025 12:44] Using data from previous issue: {"categories": ["#diffusion", "#dataset", "#transfer_learning", "#multimodal", "#cv"], "emoji": "ü§ù", "ru": {"title": "–ê–Ω–∏–º–∞—Ü–∏—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π —á–µ—Ä–µ–∑ –ø–æ–∑—ã –±–ª–∏–∑–∫–æ–≥–æ –∫–æ–Ω—Ç–∞–∫—Ç–∞", "desc": "Ponimator ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∞–Ω–∏–º–∞—Ü–∏–π –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –¥–≤—É—Ö –ª—é–¥–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ–∑ –±–ª–∏–∑–∫–æ–≥–æ –∫–æ–Ω—Ç–∞–∫—Ç–∞ –∏–∑ motion c
[18.10.2025 12:44] Using data from previous issue: {"categories": ["#dataset", "#long_context", "#reasoning", "#machine_translation", "#optimization", "#training"], "emoji": "üìè", "ru": {"title": "–ó–∞–∫–æ–Ω—ã –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤ LLM", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –Ω–æ–≤—É—é framework –¥–ª—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤
[18.10.2025 12:44] Using data from previous issue: {"categories": ["#rag", "#reasoning", "#multimodal", "#training", "#interpretability"], "emoji": "üß†", "ru": {"title": "–û—Ç –ø–∞—Å—Å–∏–≤–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ –∫ –∞–∫—Ç–∏–≤–Ω–æ–π –ø–∞–º—è—Ç–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ MoM, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç RAG-—Å–∏—Å—Ç–µ–º—ã, –ø—Ä–µ–≤—Ä–∞—â–∞—è –ø–∞—Å—Å–∏–≤–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏ –≤ –∞–∫—Ç–∏–≤–Ω–æ
[18.10.2025 12:44] Using data from previous issue: {"categories": ["#dataset", "#agents", "#long_context", "#synthetic", "#optimization", "#data", "#benchmark"], "emoji": "üîç", "ru": {"title": "–°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –≤–µ–±-–∞–≥–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ–µ —É—Å–ª–æ–∂–Ω–µ–Ω–∏–µ –∑–∞–¥–∞—á", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–π –º–µ—Ç–æ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏
[18.10.2025 12:44] Using data from previous issue: {"categories": ["#optimization", "#3d"], "emoji": "üéØ", "ru": {"title": "–ë—ã—Å—Ç—Ä–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö —Å—Ü–µ–Ω —á–µ—Ä–µ–∑ –∫–∞—Å–∫–∞–¥–Ω—É—é –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é", "desc": "SCas4D ‚Äî —ç—Ç–æ –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö 3D-—Å—Ü–µ–Ω —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º 3D Gaussian Splatting. –ö–ª—é—á–µ–≤–∞—è –∏–¥–µ—è –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —Ç–æ–º, —á—Ç–æ –¥–µ—Ñ–æ
[18.10.2025 12:44] Using data from previous issue: {"categories": ["#training", "#interpretability", "#rl", "#reasoning"], "emoji": "üå≥", "ru": {"title": "–£–ª—É—á—à–µ–Ω–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ LLM —Å –ø–æ–º–æ—â—å—é GroundedPRM", "desc": "GroundedPRM –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–µ—Ç–æ–¥ –ú–æ–Ω—Ç–µ-–ö–∞—Ä–ª–æ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º–Ω–æ–≥–æ—à–∞–≥–æ–≤–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ LLM, —Å–Ω–∏–∂–∞—è –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –≤ –±–æ–ª—å—à–æ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ –∞–Ω–Ω–æ—Ç–∞
[18.10.2025 12:44] Using data from previous issue: {"categories": ["#agents", "#reasoning", "#multimodal", "#alignment", "#ethics", "#benchmark"], "emoji": "ü¶∏", "ru": {"title": "–°—É–ø–µ—Ä–≥–µ—Ä–æ–∏ –∏–∑ —Ä–∞–∑–Ω—ã—Ö –≤—Å–µ–ª–µ–Ω–Ω—ã—Ö: –ø—Ä–æ–≤–µ—Ä–∫–∞ LLM –Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ä–æ–ª–µ–≤–æ–π –∏–≥—Ä—ã", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Å–æ–∑–¥–∞–ª–∏ –±–µ–Ω—á–º–∞—Ä–∫ Beyond One World –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ LLM –ø–æ—Å–ª–µ–¥–æ
[18.10.2025 12:44] Using data from previous issue: {"categories": ["#reasoning", "#optimization", "#interpretability", "#architecture", "#training"], "emoji": "üîß", "ru": {"title": "–ß–µ—Ç—ã—Ä–µ –º–µ—Ö–∞–Ω–∏–∑–º–∞ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∞–ª–≥–æ—Ä–∏—Ç–º–∏—á–µ—Å–∫–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ —á–µ—Ç—ã—Ä–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã—Ö –º–µ—Ö–∞–Ω–∏–∑–º–∞ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ Transf
[18.10.2025 12:44] Using data from previous issue: {"categories": ["#optimization", "#agents", "#benchmark", "#rag", "#reasoning", "#hallucinations"], "emoji": "üîç", "ru": {"title": "–û—Ü–µ–Ω–∫–∞ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —à–∞–≥–æ–≤ –≤ –∞–≥–µ–Ω—Ç–Ω—ã—Ö RAG-—Å–∏—Å—Ç–µ–º–∞—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç RAGCap-Bench ‚Äî –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö –∑–∞–¥–∞—á –≤ –∞–≥–µ–Ω—Ç–Ω—ã—Ö RAG-—Å
[18.10.2025 12:44] Using data from previous issue: {"categories": ["#security", "#hallucinations", "#alignment", "#benchmark", "#rag"], "emoji": "üö´", "ru": {"title": "–ö–æ–≥–¥–∞ AI –¥–æ–ª–∂–µ–Ω —Å–∫–∞–∑–∞—Ç—å ¬´–Ω–µ –∑–Ω–∞—é¬ª: —Ç–µ—Å—Ç–∏—Ä—É–µ–º —É–º–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –æ—Ç–∫–∞–∑—ã–≤–∞—Ç—å—Å—è –æ—Ç–≤–µ—á–∞—Ç—å", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏ –∫—Ä–∏—Ç–∏—á–µ—Å–∫—É—é –ø—Ä–æ–±–ª–µ–º—É –≤ RAG-—Å–∏—Å—Ç–µ–º–∞—Ö: —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –ø–ª–æ—Ö–æ —Å–ø—Ä–∞–≤–ª—è
[18.10.2025 12:44] Using data from previous issue: {"categories": ["#optimization", "#architecture", "#inference"], "emoji": "ü™û", "ru": {"title": "–î–≤—É—Å—Ç–æ—Ä–æ–Ω–Ω–µ–µ –∑–µ—Ä–∫–∞–ª–æ —É—Å–∫–æ—Ä—è–µ—Ç LLM —á–µ—Ä–µ–∑ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ —Å–ø–µ–∫—É–ª—è—Ç–∏–≤–Ω–æ–µ –∏—Å–ø–æ–ª–Ω–µ–Ω–∏–µ", "desc": "Mirror Speculative Decoding - —ç—Ç–æ –Ω–æ–≤—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–µ–æ–¥–æ–ª–µ–≤–∞–µ—Ç –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π –∫–æ–º–ø—Ä–æ–º–∏—Å—Å –º–µ–∂–¥—É –∑–∞–¥–µ
[18.10.2025 12:44] Renaming data file.
[18.10.2025 12:44] Renaming previous data. hf_papers.json to ./d/2025-10-17.json
[18.10.2025 12:44] Saving new data file.
[18.10.2025 12:44] Generating page.
[18.10.2025 12:44] Renaming previous page.
[18.10.2025 12:44] Renaming previous data. index.html to ./d/2025-10-17.html
[18.10.2025 12:44] Writing result.
[18.10.2025 12:44] Renaming log file.
[18.10.2025 12:44] Renaming previous data. log.txt to ./logs/2025-10-18_last_log.txt
