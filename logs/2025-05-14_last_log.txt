[14.05.2025 08:16] Read previous papers.
[14.05.2025 08:16] Generating top page (month).
[14.05.2025 08:16] Writing top page (month).
[14.05.2025 09:12] Read previous papers.
[14.05.2025 09:12] Get feed.
[14.05.2025 09:12] Extract page data from URL. URL: https://huggingface.co/papers/2505.07916
[14.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.07591
[14.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.07215
[14.05.2025 09:12] Extract page data from URL. URL: https://huggingface.co/papers/2505.08712
[14.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.08665
[14.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.07416
[14.05.2025 09:12] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[14.05.2025 09:12] No deleted papers detected.
[14.05.2025 09:12] Downloading and parsing papers (pdf, html). Total: 6.
[14.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.07916.
[14.05.2025 09:12] Downloading paper 2505.07916 from http://arxiv.org/pdf/2505.07916v1...
[14.05.2025 09:12] Extracting affiliations from text.
[14.05.2025 09:12] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 2 1 ] . e [ 1 6 1 9 7 0 . 5 0 5 2 : r MiniMax1 We introduce MiniMax-Speech, an autoregressive Transformer-based Text-to-Speech (TTS) model that generates high-quality speech. key innovation is our learnable speaker encoder, which extracts timbre features from reference audio without requiring its transcription. This enables MiniMax-Speech to produce highly expressive speech with timbre consistent with the reference in zero-shot manner, while also supporting one-shot voice cloning with exceptionally high similarity to the reference voice. In addition, the overall quality of the synthesized audio is enhanced through the proposed Flow-VAE. Our model supports 32 languages and demonstrates excellent performance across multiple objective and subjective evaluations metrics. Notably, it achieves state-of-the-art (SOTA) results on objective voice cloning metrics (Word Error Rate and Speaker Similarity) and has secured the top position on the public TTS Arena leaderboard. Another key strength of MiniMax-Speech, granted by the robust and disentangled representations from the speaker encoder, is its extensibility without modifying the base model, enabling various applications such as: arbitrary voice emotion control via LoRA; text to voice (T2V) by synthesizing timbre features directly from text description; and professional voice cloning (PVC) by fine-tuning timbre features with additional data. We encourage readers to visit https://minimax-ai.github.io/tts_tech_report for more examples. 1. Introduction The advent of codec-based models has catalyzed significant advancements in TTS. These models, when trained on large-scale datasets, demonstrate the remarkable capability to generate high-quality speech from only few seconds of reference audio. This proficiency has fostered their widespread adoption across diverse applications, including conversational AI, audio content creation for blogs, interactive voice assistants, and immersive e-book narration. Two primary modeling"
[14.05.2025 09:12] Response: []
[14.05.2025 09:12] Extracting affiliations from text.
[14.05.2025 09:12] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 2 1 ] . e [ 1 6 1 9 7 0 . 5 0 5 2 : r MiniMax1 We introduce MiniMax-Speech, an autoregressive Transformer-based Text-to-Speech (TTS) model that generates high-quality speech. key innovation is our learnable speaker encoder, which extracts timbre features from reference audio without requiring its transcription. This enables MiniMax-Speech to produce highly expressive speech with timbre consistent with the reference in zero-shot manner, while also supporting one-shot voice cloning with exceptionally high similarity to the reference voice. In addition, the overall quality of the synthesized audio is enhanced through the proposed Flow-VAE. Our model supports 32 languages and demonstrates excellent performance across multiple objective and subjective evaluations metrics. Notably, it achieves state-of-the-art (SOTA) results on objective voice cloning metrics (Word Error Rate and Speaker Similarity) and has secured the top position on the public TTS Arena leaderboard. Another key strength of MiniMax-Speech, granted by the robust and disentangled representations from the speaker encoder, is its extensibility without modifying the base model, enabling various applications such as: arbitrary voice emotion control via LoRA; text to voice (T2V) by synthesizing timbre features directly from text description; and professional voice cloning (PVC) by fine-tuning timbre features with additional data. We encourage readers to visit https://minimax-ai.github.io/tts_tech_report for more examples. 1. Introduction The advent of codec-based models has catalyzed significant advancements in TTS. These models, when trained on large-scale datasets, demonstrate the remarkable capability to generate high-quality speech from only few seconds of reference audio. This proficiency has fostered their widespread adoption across diverse applications, including conversational AI, audio content creation for blogs, interactive voice assistants, and immersive e-book narration. Two primary modeling methodologies are prevalent in large TTS models: autoregressive (AR) language models and non-autoregressive (NAR) diffusion models. NAR diffusion models have gained attention for their rapid inference capabilities. However, such NAR models often employ duration modeling techniques. One approach is explicit phoneme-duration alignment (Gao et al., 2023a; Ju et al., 2024; Le et al., 2023; Mehta et al., 2024), which can constrain naturalness and diversity. An alternative, global duration modeling (Anastassiou et al., 2024; Chen et al., 2024; Eskimez et al., 2024; Jiang et al., 2025; Lee et al., 2024; Wang et al., 2024; Yang et al., 2024), involves the model learning implicit alignments; this, however, can increase modeling complexity and reduce robustness in challenging cases. Conversely, AR models are renowned for their potent modeling capacity, an inherent strength that allows them to generate speech exhibiting superior prosody, intonation, and 1Please send correspondence to model@minimax.io. 2025 MiniMax. All rights reserved MiniMax-Speech: Intrinsic Zero-Shot Text-to-Speech with Learnable Speaker Encoder overall naturalness (Borsos et al., 2023; Casanova et al., 2024; Guo et al., 2024; Wang et al., 2023a, 2025). Most previous AR TTS models (Du et al., 2024b; Wang et al., 2023a) have required both speech and transcription as prompts in voice cloning, methodology categorized as one-shot learning. However, semantic or linguistic mismatches between prompt and target speech, compounded by decoding length limitations, often result in suboptimal generation quality. MiniMax-Speech distinguishes itself by integrating speaker encoder into its AR model. This key feature, based on concepts explored in works like (Betker, 2023), enables zero-shot voice cloning using only speech prompt. This approach eliminates the dependency on reference transcription, thus inherently supporting cross-lingual and multilingual synthesis and avoiding issues from text-speech mismatches. By conditioning solely on vocal characteristics, it allows for wider, more flexible decoding space for prosody and style during generation, leading to richer and more varied outputs. Notably, even in one-shot scenarios where reference transcription is available, this integrated speaker encoder contributes to superior speaker similarity in the synthesized speech. The strategy employed by MiniMax-Speech thus effectively mitigates these common issues and offers greater flexibility for various extensions, such as T2V and PVC. Another technical aspect warranting discussion is the inherent learnability of the speaker encoder we adopted (Betker, 2023; Casanova et al., 2024). Some models often utilize modules pre-trained on Speaker Verification (SV) tasks as their fixed speaker encoders (Du et al., 2024a). However, the training data and optimization objectives inherent to SV tasks can differ from the specific requirements of the TTS task itself. In contrast, our learnable approach involves jointly training the speaker encoder with the AR model. This end-to-end optimization strategy allows the speaker encoder to be better tailored to the demands of the TTS task. Subsequent experimental results compellingly demonstrate that employing learnable speaker encoder yields superior performance in terms of both speaker similarity and intelligibility of the synthesized speech. Flow matching models (Lipman et al., 2023; Tong et al., 2024), when utilized as decoders, are capable of producing high-fidelity speech outputs (Du et al., 2024a,b). prevalent paradigm involves flow matching models first predicting mel-spectrogram, which is subsequently converted to an audio waveform by vocoder. However, the mel-spectrogram can act as an information bottleneck, inherently limiting the ultimate achievable speech quality. In contrast, VAE, benefiting from end-to-end training, demonstrates stronger representation learning capability. Employing VAE-derived representations as the modeling objective for flow matching systems subsequently improves speech quality (Hung et al., 2024; Kim et al., 2021; Tan et al., 2024). Building upon this VAE foundation, MiniMax-Speech innovatively introduces Flow-VAE. This hybrid approach integrates VAE and flow models to enhance the information representation power of the VAE encoder, thereby further improving both audio quality and speaker similarity. We summarize our contributions as follows: 1. We present MiniMax-Speech, TTS model that supports 32 languages and gene"
[14.05.2025 09:12] Mistral response. {"id": "f8f689c76ae94dd0813fd1d8539f5c1d", "object": "chat.completion", "created": 1747213977, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "[]"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1644, "total_tokens": 1646, "completion_tokens": 2}}
[14.05.2025 09:12] Response: []
[14.05.2025 09:12] Deleting PDF ./assets/pdf/2505.07916.pdf.
[14.05.2025 09:12] Success.
[14.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.07591.
[14.05.2025 09:12] Extra JSON file exists (./assets/json/2505.07591.json), skip PDF parsing.
[14.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.07591.json), skip HTML parsing.
[14.05.2025 09:12] Success.
[14.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.07215.
[14.05.2025 09:12] Extra JSON file exists (./assets/json/2505.07215.json), skip PDF parsing.
[14.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.07215.json), skip HTML parsing.
[14.05.2025 09:12] Success.
[14.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.08712.
[14.05.2025 09:12] Downloading paper 2505.08712 from http://arxiv.org/pdf/2505.08712v1...
[14.05.2025 09:13] Extracting affiliations from text.
[14.05.2025 09:13] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 3 1 ] . [ 1 2 1 7 8 0 . 5 0 5 2 : r NavDP: Learning Sim-to-Real Navigation Diffusion Policy with Privileged Information Guidance Wenzhe Cai1, Jiaqi Peng1,2, Yuqiang Yang1, Yujian Zhang3, Meng Wei1,4, Hanqing Wang1, Yilun Chen1, Tai Wang1,, Jiangmiao Pang1, 1Shanghai AI Lab 2Tsinghua University 3Zhejiang University 4The University of Hong Kong Corresponding Authors Abstract: Learning navigation in dynamic open-world environments is an important yet challenging skill for robots. Most previous methods rely on precise localization and mapping or learn from expensive real-world demonstrations. In this paper, we propose the Navigation Diffusion Policy (NavDP), an end-to-end framework trained solely in simulation and can zero-shot transfer to different embodiments in diverse real-world environments. The key ingredient of NavDPs network is the combination of diffusion-based trajectory generation and critic function for trajectory selection, which are conditioned on only local observation tokens encoded from shared policy transformer. Given the privileged information of the global environment in simulation, we scale up the demonstrations of good quality to train the diffusion policy and formulate the critic value function targets with contrastive negative samples. Our demonstration generation approach achieves about 2,500 trajectories/GPU per day, 20 more efficient than real-world data collection, and results in large-scale navigation dataset with 363.2km trajectories across 1244 scenes. Trained with this simulation dataset, NavDP achieves state-of-the-art performance and consistently outstanding generalization capability on quadruped, wheeled, and humanoid robots in diverse indoor and outdoor environments. In addition, we present preliminary attempt at using Gaussian Splatting to make in-domain real-to-sim fine-tuning to further bridge the sim-toreal gap. Experiments show that adding such real-to-sim data can improve the success rate by 30% without hurting its gener"
[14.05.2025 09:13] Response: ```python
["Shanghai AI Lab", "Tsinghua University", "Zhejiang University", "The University of Hong Kong"]
```
[14.05.2025 09:13] Deleting PDF ./assets/pdf/2505.08712.pdf.
[14.05.2025 09:13] Success.
[14.05.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2505.08665.
[14.05.2025 09:13] Extra JSON file exists (./assets/json/2505.08665.json), skip PDF parsing.
[14.05.2025 09:13] Paper image links file exists (./assets/img_data/2505.08665.json), skip HTML parsing.
[14.05.2025 09:13] Success.
[14.05.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2505.07416.
[14.05.2025 09:13] Extra JSON file exists (./assets/json/2505.07416.json), skip PDF parsing.
[14.05.2025 09:13] Paper image links file exists (./assets/img_data/2505.07416.json), skip HTML parsing.
[14.05.2025 09:13] Success.
[14.05.2025 09:13] Enriching papers with extra data.
[14.05.2025 09:13] ********************************************************************************
[14.05.2025 09:13] Abstract 0. We introduce MiniMax-Speech, an autoregressive Transformer-based Text-to-Speech (TTS) model that generates high-quality speech. A key innovation is our learnable speaker encoder, which extracts timbre features from a reference audio without requiring its transcription. This enables MiniMax-Speech to...
[14.05.2025 09:13] ********************************************************************************
[14.05.2025 09:13] Abstract 1. Instruction following evaluates large language models (LLMs) on their ability to generate outputs that adhere to user-defined constraints. However, existing benchmarks often rely on templated constraint prompts, which lack the diversity of real-world usage and limit fine-grained performance assessme...
[14.05.2025 09:13] ********************************************************************************
[14.05.2025 09:13] Abstract 2. We present gg-bench, a collection of game environments designed to evaluate general reasoning capabilities in language models. Unlike most static benchmarks, gg-bench is a data generating process where new evaluation instances can be generated at will. In particular, gg-bench is synthetically genera...
[14.05.2025 09:13] ********************************************************************************
[14.05.2025 09:13] Abstract 3. Learning navigation in dynamic open-world environments is an important yet challenging skill for robots. Most previous methods rely on precise localization and mapping or learn from expensive real-world demonstrations. In this paper, we propose the Navigation Diffusion Policy (NavDP), an end-to-end ...
[14.05.2025 09:13] ********************************************************************************
[14.05.2025 09:13] Abstract 4. Assessing human skill levels in complex activities is a challenging problem with applications in sports, rehabilitation, and training. In this work, we present SkillFormer, a parameter-efficient architecture for unified multi-view proficiency estimation from egocentric and exocentric videos. Buildin...
[14.05.2025 09:13] ********************************************************************************
[14.05.2025 09:13] Abstract 5. Multimodal Review Helpfulness Prediction (MRHP) is an essential task in recommender systems, particularly in E-commerce platforms. Determining the helpfulness of user-generated reviews enhances user experience and improves consumer decision-making. However, existing datasets focus predominantly on E...
[14.05.2025 09:13] Read previous papers.
[14.05.2025 09:13] Generating reviews via LLM API.
[14.05.2025 09:13] Querying the API.
[14.05.2025 09:13] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

We introduce MiniMax-Speech, an autoregressive Transformer-based Text-to-Speech (TTS) model that generates high-quality speech. A key innovation is our learnable speaker encoder, which extracts timbre features from a reference audio without requiring its transcription. This enables MiniMax-Speech to produce highly expressive speech with timbre consistent with the reference in a zero-shot manner, while also supporting one-shot voice cloning with exceptionally high similarity to the reference voice. In addition, the overall quality of the synthesized audio is enhanced through the proposed Flow-VAE. Our model supports 32 languages and demonstrates excellent performance across multiple objective and subjective evaluations metrics. Notably, it achieves state-of-the-art (SOTA) results on objective voice cloning metrics (Word Error Rate and Speaker Similarity) and has secured the top position on the public TTS Arena leaderboard. Another key strength of MiniMax-Speech, granted by the robust and disentangled representations from the speaker encoder, is its extensibility without modifying the base model, enabling various applications such as: arbitrary voice emotion control via LoRA; text to voice (T2V) by synthesizing timbre features directly from text description; and professional voice cloning (PVC) by fine-tuning timbre features with additional data. We encourage readers to visit https://minimax-ai.github.io/tts_tech_report for more examples.
[14.05.2025 09:13] Response: {
  "desc": "MiniMax-Speech - это автоматическая модель синтеза речи на основе Трансформера. Ключевой инновацией является обучаемый кодировщик говорящего, который извлекает характеристики тембра из эталонного аудио без необходимости его транскрипции. Модель поддерживает 32 языка и демонстрирует отличные результаты по многим метрикам, достигая лучших показателей в клонировании голоса. MiniMax-Speech также обладает расширяемостью, позволяющей реализовать различные приложения, такие как управление эмоциями голоса и синтез тембра из текстового описания.",
  "emoji": "🗣️",
  "title": "Революция в синтезе речи: MiniMax-Speech - универсальный голосовой клон"
}
[14.05.2025 09:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"We introduce MiniMax-Speech, an autoregressive Transformer-based Text-to-Speech (TTS) model that generates high-quality speech. A key innovation is our learnable speaker encoder, which extracts timbre features from a reference audio without requiring its transcription. This enables MiniMax-Speech to produce highly expressive speech with timbre consistent with the reference in a zero-shot manner, while also supporting one-shot voice cloning with exceptionally high similarity to the reference voice. In addition, the overall quality of the synthesized audio is enhanced through the proposed Flow-VAE. Our model supports 32 languages and demonstrates excellent performance across multiple objective and subjective evaluations metrics. Notably, it achieves state-of-the-art (SOTA) results on objective voice cloning metrics (Word Error Rate and Speaker Similarity) and has secured the top position on the public TTS Arena leaderboard. Another key strength of MiniMax-Speech, granted by the robust and disentangled representations from the speaker encoder, is its extensibility without modifying the base model, enabling various applications such as: arbitrary voice emotion control via LoRA; text to voice (T2V) by synthesizing timbre features directly from text description; and professional voice cloning (PVC) by fine-tuning timbre features with additional data. We encourage readers to visit https://minimax-ai.github.io/tts_tech_report for more examples."

[14.05.2025 09:13] Response: ```python
['AUDIO', 'MULTILINGUAL']
```
[14.05.2025 09:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"We introduce MiniMax-Speech, an autoregressive Transformer-based Text-to-Speech (TTS) model that generates high-quality speech. A key innovation is our learnable speaker encoder, which extracts timbre features from a reference audio without requiring its transcription. This enables MiniMax-Speech to produce highly expressive speech with timbre consistent with the reference in a zero-shot manner, while also supporting one-shot voice cloning with exceptionally high similarity to the reference voice. In addition, the overall quality of the synthesized audio is enhanced through the proposed Flow-VAE. Our model supports 32 languages and demonstrates excellent performance across multiple objective and subjective evaluations metrics. Notably, it achieves state-of-the-art (SOTA) results on objective voice cloning metrics (Word Error Rate and Speaker Similarity) and has secured the top position on the public TTS Arena leaderboard. Another key strength of MiniMax-Speech, granted by the robust and disentangled representations from the speaker encoder, is its extensibility without modifying the base model, enabling various applications such as: arbitrary voice emotion control via LoRA; text to voice (T2V) by synthesizing timbre features directly from text description; and professional voice cloning (PVC) by fine-tuning timbre features with additional data. We encourage readers to visit https://minimax-ai.github.io/tts_tech_report for more examples."

[14.05.2025 09:13] Response: ```python
["GAMES", "OPTIMIZATION"]
```
[14.05.2025 09:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"MiniMax-Speech is a cutting-edge Text-to-Speech (TTS) model that utilizes an autoregressive Transformer architecture to generate high-quality speech. A significant feature of this model is its learnable speaker encoder, which captures timbre characteristics from audio samples without needing their text transcriptions. This allows the model to create expressive speech that matches the timbre of the reference audio in a zero-shot manner, and it can also perform one-shot voice cloning with remarkable accuracy. Additionally, MiniMax-Speech enhances audio quality through a Flow-VAE and supports multiple languages, achieving state-of-the-art results in voice cloning metrics and demonstrating versatility for various applications.","title":"Revolutionizing Speech Synthesis with MiniMax-Speech"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='MiniMax-Speech is a cutting-edge Text-to-Speech (TTS) model that utilizes an autoregressive Transformer architecture to generate high-quality speech. A significant feature of this model is its learnable speaker encoder, which captures timbre characteristics from audio samples without needing their text transcriptions. This allows the model to create expressive speech that matches the timbre of the reference audio in a zero-shot manner, and it can also perform one-shot voice cloning with remarkable accuracy. Additionally, MiniMax-Speech enhances audio quality through a Flow-VAE and supports multiple languages, achieving state-of-the-art results in voice cloning metrics and demonstrating versatility for various applications.', title='Revolutionizing Speech Synthesis with MiniMax-Speech'))
[14.05.2025 09:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"MiniMax-Speech是一种基于自回归Transformer的文本到语音（TTS）模型，能够生成高质量的语音。其创新之处在于可学习的说话人编码器，可以从参考音频中提取音色特征，而无需其转录。该模型支持零样本生成具有参考音色的表达性语音，并且在单样本语音克隆中表现出极高的相似度。此外，MiniMax-Speech在多个客观和主观评估指标上表现优异，支持32种语言，并在语音克隆的客观指标上达到了最先进的水平。","title":"MiniMax-Speech：高质量语音生成的新突破"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='MiniMax-Speech是一种基于自回归Transformer的文本到语音（TTS）模型，能够生成高质量的语音。其创新之处在于可学习的说话人编码器，可以从参考音频中提取音色特征，而无需其转录。该模型支持零样本生成具有参考音色的表达性语音，并且在单样本语音克隆中表现出极高的相似度。此外，MiniMax-Speech在多个客观和主观评估指标上表现优异，支持32种语言，并在语音克隆的客观指标上达到了最先进的水平。', title='MiniMax-Speech：高质量语音生成的新突破'))
[14.05.2025 09:13] Using data from previous issue: {"categories": ["#training", "#rl", "#alignment", "#optimization", "#benchmark"], "emoji": "🤖", "ru": {"title": "Многомерная оценка следования инструкциям для больших языковых моделей", "desc": "Статья представляет новый подход к оценке способности больших языковых моделей (LLM) следовать инструкция
[14.05.2025 09:13] Using data from previous issue: {"categories": ["#benchmark", "#dataset", "#reasoning", "#rl", "#games", "#synthetic"], "emoji": "🎮", "ru": {"title": "gg-bench: Новый подход к оценке способностей языковых моделей через игровые среды", "desc": "В статье представлен gg-bench - набор игровых сред для оценки способностей языковых моде
[14.05.2025 09:13] Querying the API.
[14.05.2025 09:13] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Learning navigation in dynamic open-world environments is an important yet challenging skill for robots. Most previous methods rely on precise localization and mapping or learn from expensive real-world demonstrations. In this paper, we propose the Navigation Diffusion Policy (NavDP), an end-to-end framework trained solely in simulation and can zero-shot transfer to different embodiments in diverse real-world environments. The key ingredient of NavDP's network is the combination of diffusion-based trajectory generation and a critic function for trajectory selection, which are conditioned on only local observation tokens encoded from a shared policy transformer. Given the privileged information of the global environment in simulation, we scale up the demonstrations of good quality to train the diffusion policy and formulate the critic value function targets with contrastive negative samples. Our demonstration generation approach achieves about 2,500 trajectories/GPU per day, 20times more efficient than real-world data collection, and results in a large-scale navigation dataset with 363.2km trajectories across 1244 scenes. Trained with this simulation dataset, NavDP achieves state-of-the-art performance and consistently outstanding generalization capability on quadruped, wheeled, and humanoid robots in diverse indoor and outdoor environments. In addition, we present a preliminary attempt at using Gaussian Splatting to make in-domain real-to-sim fine-tuning to further bridge the sim-to-real gap. Experiments show that adding such real-to-sim data can improve the success rate by 30\% without hurting its generalization capability.
[14.05.2025 09:13] Response: {
  "desc": "Статья представляет NavDP - систему навигации роботов в динамичной открытой среде, обученную исключительно на симуляциях. NavDP использует комбинацию генерации траектории на основе диффузии и функции критика для выбора траектории, что позволяет ей успешно работать с разными типами роботов в реальном мире. Система обучается на большом наборе данных, сгенерированном в симуляции, что значительно эффективнее сбора реальных данных. Эксперименты показывают высокую производительность и способность к обобщению для различных типов роботов в разнообразных средах.",
  "emoji": "🤖",
  "title": "NavDP: Универсальная навигация роботов из симуляции в реальность"
}
[14.05.2025 09:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Learning navigation in dynamic open-world environments is an important yet challenging skill for robots. Most previous methods rely on precise localization and mapping or learn from expensive real-world demonstrations. In this paper, we propose the Navigation Diffusion Policy (NavDP), an end-to-end framework trained solely in simulation and can zero-shot transfer to different embodiments in diverse real-world environments. The key ingredient of NavDP's network is the combination of diffusion-based trajectory generation and a critic function for trajectory selection, which are conditioned on only local observation tokens encoded from a shared policy transformer. Given the privileged information of the global environment in simulation, we scale up the demonstrations of good quality to train the diffusion policy and formulate the critic value function targets with contrastive negative samples. Our demonstration generation approach achieves about 2,500 trajectories/GPU per day, 20times more efficient than real-world data collection, and results in a large-scale navigation dataset with 363.2km trajectories across 1244 scenes. Trained with this simulation dataset, NavDP achieves state-of-the-art performance and consistently outstanding generalization capability on quadruped, wheeled, and humanoid robots in diverse indoor and outdoor environments. In addition, we present a preliminary attempt at using Gaussian Splatting to make in-domain real-to-sim fine-tuning to further bridge the sim-to-real gap. Experiments show that adding such real-to-sim data can improve the success rate by 30\% without hurting its generalization capability."

[14.05.2025 09:13] Response: ```python
['AGENTS', 'ROBOTICS', 'DATASET']
```
[14.05.2025 09:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Learning navigation in dynamic open-world environments is an important yet challenging skill for robots. Most previous methods rely on precise localization and mapping or learn from expensive real-world demonstrations. In this paper, we propose the Navigation Diffusion Policy (NavDP), an end-to-end framework trained solely in simulation and can zero-shot transfer to different embodiments in diverse real-world environments. The key ingredient of NavDP's network is the combination of diffusion-based trajectory generation and a critic function for trajectory selection, which are conditioned on only local observation tokens encoded from a shared policy transformer. Given the privileged information of the global environment in simulation, we scale up the demonstrations of good quality to train the diffusion policy and formulate the critic value function targets with contrastive negative samples. Our demonstration generation approach achieves about 2,500 trajectories/GPU per day, 20times more efficient than real-world data collection, and results in a large-scale navigation dataset with 363.2km trajectories across 1244 scenes. Trained with this simulation dataset, NavDP achieves state-of-the-art performance and consistently outstanding generalization capability on quadruped, wheeled, and humanoid robots in diverse indoor and outdoor environments. In addition, we present a preliminary attempt at using Gaussian Splatting to make in-domain real-to-sim fine-tuning to further bridge the sim-to-real gap. Experiments show that adding such real-to-sim data can improve the success rate by 30\% without hurting its generalization capability."

[14.05.2025 09:13] Response: ```python
['DIFFUSION', 'TRANSFER_LEARNING']
```
[14.05.2025 09:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces the Navigation Diffusion Policy (NavDP), a novel framework for robot navigation in dynamic environments. Unlike traditional methods that depend on accurate mapping or costly real-world training, NavDP is trained entirely in simulation and can adapt to various robot types in real-world settings. The framework utilizes diffusion-based trajectory generation combined with a critic function to select optimal paths based on local observations. By generating a large dataset of simulated navigation trajectories efficiently, NavDP demonstrates superior performance and generalization across different robotic embodiments and environments.","title":"Efficient Robot Navigation with Simulation-Driven Learning"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper introduces the Navigation Diffusion Policy (NavDP), a novel framework for robot navigation in dynamic environments. Unlike traditional methods that depend on accurate mapping or costly real-world training, NavDP is trained entirely in simulation and can adapt to various robot types in real-world settings. The framework utilizes diffusion-based trajectory generation combined with a critic function to select optimal paths based on local observations. By generating a large dataset of simulated navigation trajectories efficiently, NavDP demonstrates superior performance and generalization across different robotic embodiments and environments.', title='Efficient Robot Navigation with Simulation-Driven Learning'))
[14.05.2025 09:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本论文提出了一种名为导航扩散策略（NavDP）的新框架，旨在帮助机器人在动态开放世界环境中进行导航。该方法完全在模拟环境中训练，能够零-shot迁移到不同的实际应用中。NavDP结合了基于扩散的轨迹生成和用于轨迹选择的评价函数，利用共享策略变换器编码的局部观察信息进行条件化。通过在模拟中获取全球环境的特权信息，我们生成了高质量的演示数据，并在多种室内外环境中实现了最先进的导航性能和出色的泛化能力。","title":"机器人导航的新突破：导航扩散策略"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本论文提出了一种名为导航扩散策略（NavDP）的新框架，旨在帮助机器人在动态开放世界环境中进行导航。该方法完全在模拟环境中训练，能够零-shot迁移到不同的实际应用中。NavDP结合了基于扩散的轨迹生成和用于轨迹选择的评价函数，利用共享策略变换器编码的局部观察信息进行条件化。通过在模拟中获取全球环境的特权信息，我们生成了高质量的演示数据，并在多种室内外环境中实现了最先进的导航性能和出色的泛化能力。', title='机器人导航的新突破：导航扩散策略'))
[14.05.2025 09:13] Using data from previous issue: {"categories": ["#architecture", "#dataset", "#training"], "emoji": "🎥", "ru": {"title": "SkillFormer: эффективная оценка мастерства по мультиракурсному видео", "desc": "SkillFormer - это эффективная архитектура для оценки уровня мастерства в сложных действиях по видео с нескольких ракурсов. Она исп
[14.05.2025 09:13] Using data from previous issue: {"categories": ["#optimization", "#dataset", "#data", "#multilingual", "#low_resource", "#open_source"], "emoji": "🇻🇳", "ru": {"title": "ИИ ускоряет создание датасета для анализа отзывов на вьетнамском языке", "desc": "Статья представляет новый набор данных ViMRHP для прогнозирования полезности отзы
[14.05.2025 09:13] Trying to get texts in Chinese.
[14.05.2025 09:13] Mistral request. Model: mistral-large-latest. Prompt: Write simple and brief explanation (4-5 sentences) of an article in Chinese. Use short sentences. Text:

We introduce MiniMax-Speech, an autoregressive Transformer-based Text-to-Speech (TTS) model that generates high-quality speech. A key innovation is our learnable speaker encoder, which extracts timbre features from a reference audio without requiring its transcription. This enables MiniMax-Speech to produce highly expressive speech with timbre consistent with the reference in a zero-shot manner, while also supporting one-shot voice cloning with exceptionally high similarity to the reference voice. In addition, the overall quality of the synthesized audio is enhanced through the proposed Flow-VAE. Our model supports 32 languages and demonstrates excellent performance across multiple objective and subjective evaluations metrics. Notably, it achieves state-of-the-art (SOTA) results on objective voice cloning metrics (Word Error Rate and Speaker Similarity) and has secured the top position on the public TTS Arena leaderboard. Another key strength of MiniMax-Speech, granted by the robust and disentangled representations from the speaker encoder, is its extensibility without modifying the base model, enabling various applications such as: arbitrary voice emotion control via LoRA; text to voice (T2V) by synthesizing timbre features directly from text description; and professional voice cloning (PVC) by fine-tuning timbre features with additional data. We encourage readers to visit https://minimax-ai.github.io/tts_tech_report for more examples.
[14.05.2025 09:13] Mistral response. {"id": "bfa91fa6e0cc4541987984ab1c5eead1", "object": "chat.completion", "created": 1747214011, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "\u6211\u4eec\u4ecb\u7ecd\u4e86MiniMax-Speech\uff0c\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u6587\u672c\u8f6c\u8bed\u97f3\u6a21\u578b\u3002\u5b83\u80fd\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u8bed\u97f3\u3002\u6a21\u578b\u7684\u5173\u952e\u521b\u65b0\u662f\u53ef\u5b66\u4e60\u7684\u6f14\u8bb2\u8005\u7f16\u7801\u5668\uff0c\u4ece\u53c2\u8003\u97f3\u9891\u4e2d\u63d0\u53d6\u97f3\u8272\u7279\u5f81\uff0c\u65e0\u9700\u8f6c\u5f55\u3002\u8fd9\u4f7f\u5f97MiniMax-Speech\u80fd\u5728\u96f6\u6837\u672c\u60c5\u51b5\u4e0b\u751f\u6210\u8868\u73b0\u529b\u5f3a\u3001\u97f3\u8272\u4e00\u81f4\u7684\u8bed\u97f3\uff0c\u5e76\u652f\u6301\u4e00\u6837\u672c\u58f0\u97f3\u514b\u9686\u3002\u901a\u8fc7Flow-VAE\uff0c\u5408\u6210\u97f3\u9891\u7684\u6574\u4f53\u8d28\u91cf\u5f97\u5230\u63d0\u5347\u3002\u6a21\u578b\u652f\u630132\u79cd\u8bed\u8a00\uff0c\u5728\u591a\u79cd\u8bc4\u4f30\u6307\u6807\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 349, "total_tokens": 525, "completion_tokens": 176}}
[14.05.2025 09:13] Response: 我们介绍了MiniMax-Speech，一种基于Transformer的文本转语音模型。它能生成高质量的语音。模型的关键创新是可学习的演讲者编码器，从参考音频中提取音色特征，无需转录。这使得MiniMax-Speech能在零样本情况下生成表现力强、音色一致的语音，并支持一样本声音克隆。通过Flow-VAE，合成音频的整体质量得到提升。模型支持32种语言，在多种评估指标上表现出色，达到了最先进的结果。
[14.05.2025 09:13] Mistral request. Model: mistral-large-latest. Prompt: Write pinyin transcription for text. Text:

我们介绍了MiniMax-Speech，一种基于Transformer的文本转语音模型。它能生成高质量的语音。模型的关键创新是可学习的演讲者编码器，从参考音频中提取音色特征，无需转录。这使得MiniMax-Speech能在零样本情况下生成表现力强、音色一致的语音，并支持一样本声音克隆。通过Flow-VAE，合成音频的整体质量得到提升。模型支持32种语言，在多种评估指标上表现出色，达到了最先进的结果。
[14.05.2025 09:13] Mistral response. {"id": "851955a7599c4511a950c200b4eaae86", "object": "chat.completion", "created": 1747214014, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "\u6211\u4eec\u4ecb\u7ecd\u4e86MiniMax-Speech\uff0c\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u6587\u672c\u8f6c\u8bed\u97f3\u6a21\u578b\u3002\u5b83\u80fd\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u8bed\u97f3\u3002\u6a21\u578b\u7684\u5173\u952e\u521b\u65b0\u662f\u53ef\u5b66\u4e60\u7684\u6f14\u8bb2\u8005\u7f16\u7801\u5668\uff0c\u4ece\u53c2\u8003\u97f3\u9891\u4e2d\u63d0\u53d6\u97f3\u8272\u7279\u5f81\uff0c\u65e0\u9700\u8f6c\u5f55\u3002\u8fd9\u4f7f\u5f97MiniMax-Speech\u80fd\u5728\u96f6\u6837\u672c\u60c5\u51b5\u4e0b\u751f\u6210\u8868\u73b0\u529b\u5f3a\u3001\u97f3\u8272\u4e00\u81f4\u7684\u8bed\u97f3\uff0c\u5e76\u652f\u6301\u4e00\u6837\u672c\u58f0\u97f3\u514b\u9686\u3002\u901a\u8fc7Flow-VAE\uff0c\u5408\u6210\u97f3\u9891\u7684\u6574\u4f53\u8d28\u91cf\u5f97\u5230\u63d0\u5347\u3002\u6a21\u578b\u652f\u630132\u79cd\u8bed\u8a00\uff0c\u5728\u591a\u79cd\u8bc4\u4f30\u6307\u6807\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002\n\nW\u01d2men ji\u00e8sh\u00e0o le MiniMax-Speech, y\u012b zh\u01d2ng j\u012by\u00fa Transformer de w\u00e9nb\u011bn zhu\u01cen y\u01d4y\u012bn m\u00f3x\u00edng. T\u0101 n\u00e9ng sh\u0113ngch\u00e9ng g\u0101o zh\u00ecli\u00e0ng de y\u01d4y\u012bn. M\u00f3x\u00edng de gu\u01cenji\u00e0n chu\u00e0ngx\u012bn sh\u00ec k\u011b xu\u00e9x\u00ed de y\u01cenji\u01cengzh\u011b bi\u0101nm\u01ceq\u00ec, c\u00f3ng c\u0101nk\u01ceo y\u012bnp\u00edn zh\u014dng t\u00edqu\u0101n y\u012bns\u00e8 t\u00e8zh\u0113ng, w\u00fax\u016b zhu\u01cenl\u00f9. Zh\u00e8 sh\u01d0 de MiniMax-Speech n\u00e9ng z\u00e0i l\u00edng y\u00e0ngb\u011bn q\u00edngku\u00e0ng xi\u00e0 sh\u0113ngch\u00e9ng bi\u01ceoxi\u00e0nl\u00ec qi\u00e1ng, y\u012bns\u00e8 y\u012bzh\u00ec de y\u01d4y\u012bn, b\u00ecng zh\u012bch\u00ed y\u012b y\u00e0ngb\u011bn sh\u0113ngy\u012bn k\u00e8l\u00f3ng. T\u014dnggu\u00f2 Flow-VAE, h\u00e9ch\u00e9ng y\u012bnp\u00edn de zh\u011bngt\u01d0 zh\u00ecli\u00e0ng d\u00e9d\u00e0o t\u00edsh\u0113ng. M\u00f3x\u00edng zh\u012bch\u00ed 32 zh\u01d2ng y\u01d4y\u00e1n, z\u00e0i du\u014d zh\u01d2ng p\u00edngg\u016b zh\u01d0bi\u0101o sh\u00e0ng bi\u01ceoxi\u00e0n ch\u016bs\u00e8, d\u00e1d\u00e0o le zu\u00ec xi\u0101nj\u00ecn de ji\u00e9gu\u01d2."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 190, "total_tokens": 736, "completion_tokens": 546}}
[14.05.2025 09:13] Response: 我们介绍了MiniMax-Speech，一种基于Transformer的文本转语音模型。它能生成高质量的语音。模型的关键创新是可学习的演讲者编码器，从参考音频中提取音色特征，无需转录。这使得MiniMax-Speech能在零样本情况下生成表现力强、音色一致的语音，并支持一样本声音克隆。通过Flow-VAE，合成音频的整体质量得到提升。模型支持32种语言，在多种评估指标上表现出色，达到了最先进的结果。

Wǒmen jièshào le MiniMax-Speech, yī zhǒng jīyú Transformer de wénběn zhuǎn yǔyīn móxíng. Tā néng shēngchéng gāo zhìliàng de yǔyīn. Móxíng de guǎnjiàn chuàngxīn shì kě xuéxí de yǎnjiǎngzhě biānmǎqì, cóng cānkǎo yīnpín zhōng tíquān yīnsè tèzhēng, wúxū zhuǎnlù. Zhè shǐ de MiniMax-Speech néng zài líng yàngběn qíngkuàng xià shēngchéng biǎoxiànlì qiáng, yīnsè yīzhì de yǔyīn, bìng zhīchí yī yàngběn shēngyīn kèlóng. Tōngguò Flow-VAE, héchéng yīnpín de zhěngtǐ zhìliàng dédào tíshēng. Móxíng zhīchí 32 zhǒng yǔyán, zài duō zhǒng pínggū zhǐbiāo shàng biǎoxiàn chūsè, dádào le zuì xiānjìn de jiéguǒ.
[14.05.2025 09:13] Mistral request. Model: mistral-large-latest. Prompt: Write vocab of difficult words for this text as an array of objects with fields 'word', 'pinyin', 'trans'. Return as python list without formatting. Return list and nothing else. Text:

我们介绍了MiniMax-Speech，一种基于Transformer的文本转语音模型。它能生成高质量的语音。模型的关键创新是可学习的演讲者编码器，从参考音频中提取音色特征，无需转录。这使得MiniMax-Speech能在零样本情况下生成表现力强、音色一致的语音，并支持一样本声音克隆。通过Flow-VAE，合成音频的整体质量得到提升。模型支持32种语言，在多种评估指标上表现出色，达到了最先进的结果。
[14.05.2025 09:14] Mistral response. {"id": "588ef4423a6f4718977f2effabb9baf6", "object": "chat.completion", "created": 1747214030, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "[\n    {\"word\": \"\u4ecb\u7ecd\", \"pinyin\": \"ji\u00e8 sh\u00e0o\", \"trans\": \"introduce\"},\n    {\"word\": \"MiniMax-Speech\", \"pinyin\": \"MiniMax-Speech\", \"trans\": \"MiniMax-Speech\"},\n    {\"word\": \"\u57fa\u4e8e\", \"pinyin\": \"j\u012b y\u00fa\", \"trans\": \"based on\"},\n    {\"word\": \"Transformer\", \"pinyin\": \"Transformer\", \"trans\": \"Transformer\"},\n    {\"word\": \"\u6587\u672c\u8f6c\u8bed\u97f3\", \"pinyin\": \"w\u00e9n b\u011bn zhu\u01cen y\u01d4 y\u012bn\", \"trans\": \"text-to-speech\"},\n    {\"word\": \"\u6a21\u578b\", \"pinyin\": \"m\u00f3 x\u00edng\", \"trans\": \"model\"},\n    {\"word\": \"\u751f\u6210\", \"pinyin\": \"sh\u0113ng ch\u00e9ng\", \"trans\": \"generate\"},\n    {\"word\": \"\u9ad8\u8d28\u91cf\", \"pinyin\": \"g\u0101o zh\u00ec li\u00e0ng\", \"trans\": \"high quality\"},\n    {\"word\": \"\u8bed\u97f3\", \"pinyin\": \"y\u01d4 y\u012bn\", \"trans\": \"speech\"},\n    {\"word\": \"\u5173\u952e\", \"pinyin\": \"gu\u01cen ji\u00e0n\", \"trans\": \"key\"},\n    {\"word\": \"\u521b\u65b0\", \"pinyin\": \"chu\u00e0ng x\u012bn\", \"trans\": \"innovation\"},\n    {\"word\": \"\u53ef\u5b66\u4e60\", \"pinyin\": \"k\u011b xu\u00e9 x\u00ed\", \"trans\": \"learnable\"},\n    {\"word\": \"\u6f14\u8bb2\u8005\", \"pinyin\": \"y\u01cen ji\u01ceng zh\u011b\", \"trans\": \"speaker\"},\n    {\"word\": \"\u7f16\u7801\u5668\", \"pinyin\": \"bi\u0101n m\u01ce q\u00ec\", \"trans\": \"encoder\"},\n    {\"word\": \"\u4ece\", \"pinyin\": \"c\u00f3ng\", \"trans\": \"from\"},\n    {\"word\": \"\u53c2\u8003\", \"pinyin\": \"c\u0101n k\u01ceo\", \"trans\": \"reference\"},\n    {\"word\": \"\u97f3\u9891\", \"pinyin\": \"y\u012bn p\u00edn\", \"trans\": \"audio\"},\n    {\"word\": \"\u63d0\u53d6\", \"pinyin\": \"t\u00ed qu\", \"trans\": \"extract\"},\n    {\"word\": \"\u97f3\u8272\", \"pinyin\": \"y\u012bn s\u00e8\", \"trans\": \"timbre\"},\n    {\"word\": \"\u7279\u5f81\", \"pinyin\": \"t\u00e8 zh\u0113ng\", \"trans\": \"features\"},\n    {\"word\": \"\u65e0\u9700\", \"pinyin\": \"w\u00fa x\u016b\", \"trans\": \"no need\"},\n    {\"word\": \"\u8f6c\u5f55\", \"pinyin\": \"zhu\u01cen l\u00f9\", \"trans\": \"transcription\"},\n    {\"word\": \"\u96f6\u6837\u672c\", \"pinyin\": \"l\u00edng y\u00e0ng b\u011bn\", \"trans\": \"zero-shot\"},\n    {\"word\": \"\u60c5\u51b5\", \"pinyin\": \"q\u00edng ku\u00e0ng\", \"trans\": \"situation\"},\n    {\"word\": \"\u8868\u73b0\u529b\", \"pinyin\": \"bi\u01ceo xi\u00e0n l\u00ec\", \"trans\": \"expressiveness\"},\n    {\"word\": \"\u5f3a\", \"pinyin\": \"qi\u00e1ng\", \"trans\": \"strong\"},\n    {\"word\": \"\u4e00\u81f4\", \"pinyin\": \"y\u012b zh\u00ec\", \"trans\": \"consistent\"},\n    {\"word\": \"\u652f\u6301\", \"pinyin\": \"zh\u012b ch\u00ed\", \"trans\": \"support\"},\n    {\"word\": \"\u4e00\u6837\u672c\", \"pinyin\": \"y\u012b y\u00e0ng b\u011bn\", \"trans\": \"one-shot\"},\n    {\"word\": \"\u58f0\u97f3\", \"pinyin\": \"sh\u0113ng y\u012bn\", \"trans\": \"voice\"},\n    {\"word\": \"\u514b\u9686\", \"pinyin\": \"k\u00e8 l\u00f3ng\", \"trans\": \"clone\"},\n    {\"word\": \"\u901a\u8fc7\", \"pinyin\": \"t\u014dng gu\u00f2\", \"trans\": \"through\"},\n    {\"word\": \"Flow-VAE\", \"pinyin\": \"Flow-VAE\", \"trans\": \"Flow-VAE\"},\n    {\"word\": \"\u5408\u6210\", \"pinyin\": \"h\u00e9 ch\u00e9ng\", \"trans\": \"synthesis\"},\n    {\"word\": \"\u6574\u4f53\", \"pinyin\": \"zh\u011bng t\u01d0\", \"trans\": \"overall\"},\n    {\"word\": \"\u8d28\u91cf\", \"pinyin\": \"zh\u00ec li\u00e0ng\", \"trans\": \"quality\"},\n    {\"word\": \"\u5f97\u5230\", \"pinyin\": \"d\u00e9 d\u00e0o\", \"trans\": \"obtain\"},\n    {\"word\": \"\u63d0\u5347\", \"pinyin\": \"t\u00ed sh\u0113ng\", \"trans\": \"improvement\"},\n    {\"word\": \"\u591a\u79cd\", \"pinyin\": \"du\u014d zh\u01d2ng\", \"trans\": \"various\"},\n    {\"word\": \"\u8bc4\u4f30\", \"pinyin\": \"p\u00edng g\u016b\", \"trans\": \"evaluation\"},\n    {\"word\": \"\u6307\u6807\", \"pinyin\": \"zh\u01d0 bi\u0101o\", \"trans\": \"metrics\"},\n    {\"word\": \"\u8868\u73b0\", \"pinyin\": \"bi\u01ceo xi\u00e0n\", \"trans\": \"performance\"},\n    {\"word\": \"\u51fa\u8272\", \"pinyin\": \"ch\u016b s\u00e8\", \"trans\": \"outstanding\"},\n    {\"word\": \"\u8fbe\u5230\", \"pinyin\": \"d\u00e1 d\u00e0o\", \"trans\": \"achieve\"},\n    {\"word\": \"\u6700\u5148\u8fdb\", \"pinyin\": \"zu\u00ec xi\u0101n j\u00ecn\", \"trans\": \"state-of-the-art\"}\n]"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 222, "total_tokens": 1546, "completion_tokens": 1324}}
[14.05.2025 09:14] Response: [
    {"word": "介绍", "pinyin": "jiè shào", "trans": "introduce"},
    {"word": "MiniMax-Speech", "pinyin": "MiniMax-Speech", "trans": "MiniMax-Speech"},
    {"word": "基于", "pinyin": "jī yú", "trans": "based on"},
    {"word": "Transformer", "pinyin": "Transformer", "trans": "Transformer"},
    {"word": "文本转语音", "pinyin": "wén běn zhuǎn yǔ yīn", "trans": "text-to-speech"},
    {"word": "模型", "pinyin": "mó xíng", "trans": "model"},
    {"word": "生成", "pinyin": "shēng chéng", "trans": "generate"},
    {"word": "高质量", "pinyin": "gāo zhì liàng", "trans": "high quality"},
    {"word": "语音", "pinyin": "yǔ yīn", "trans": "speech"},
    {"word": "关键", "pinyin": "guǎn jiàn", "trans": "key"},
    {"word": "创新", "pinyin": "chuàng xīn", "trans": "innovation"},
    {"word": "可学习", "pinyin": "kě xué xí", "trans": "learnable"},
    {"word": "演讲者", "pinyin": "yǎn jiǎng zhě", "trans": "speaker"},
    {"word": "编码器", "pinyin": "biān mǎ qì", "trans": "encoder"},
    {"word": "从", "pinyin": "cóng", "trans": "from"},
    {"word": "参考", "pinyin": "cān kǎo", "trans": "reference"},
    {"word": "音频", "pinyin": "yīn pín", "trans": "audio"},
    {"word": "提取", "pinyin": "tí qu", "trans": "extract"},
    {"word": "音色", "pinyin": "yīn sè", "trans": "timbre"},
    {"word": "特征", "pinyin": "tè zhēng", "trans": "features"},
    {"word": "无需", "pinyin": "wú xū", "trans": "no need"},
    {"word": "转录", "pinyin": "zhuǎn lù", "trans": "transcription"},
    {"word": "零样本", "pinyin": "líng yàng běn", "trans": "zero-shot"},
    {"word": "情况", "pinyin": "qíng kuàng", "trans": "situation"},
    {"word": "表现力", "pinyin": "biǎo xiàn lì", "trans": "expressiveness"},
    {"word": "强", "pinyin": "qiáng", "trans": "strong"},
    {"word": "一致", "pinyin": "yī zhì", "trans": "consistent"},
    {"word": "支持", "pinyin": "zhī chí", "trans": "support"},
    {"word": "一样本", "pinyin": "yī yàng běn", "trans": "one-shot"},
    {"word": "声音", "pinyin": "shēng yīn", "trans": "voice"},
    {"word": "克隆", "pinyin": "kè lóng", "trans": "clone"},
    {"word": "通过", "pinyin": "tōng guò", "trans": "through"},
    {"word": "Flow-VAE", "pinyin": "Flow-VAE", "trans": "Flow-VAE"},
    {"word": "合成", "pinyin": "hé chéng", "trans": "synthesis"},
    {"word": "整体", "pinyin": "zhěng tǐ", "trans": "overall"},
    {"word": "质量", "pinyin": "zhì liàng", "trans": "quality"},
    {"word": "得到", "pinyin": "dé dào", "trans": "obtain"},
    {"word": "提升", "pinyin": "tí shēng", "trans": "improvement"},
    {"word": "多种", "pinyin": "duō zhǒng", "trans": "various"},
    {"word": "评估", "pinyin": "píng gū", "trans": "evaluation"},
    {"word": "指标", "pinyin": "zhǐ biāo", "trans": "metrics"},
    {"word": "表现", "pinyin": "biǎo xiàn", "trans": "performance"},
    {"word": "出色", "pinyin": "chū sè", "trans": "outstanding"},
    {"word": "达到", "pinyin": "dá dào", "trans": "achieve"},
    {"word": "最先进", "pinyin": "zuì xiān jìn", "trans": "state-of-the-art"}
]
[14.05.2025 09:14] Mistral request. Model: mistral-large-latest. Prompt: Translate this text in English. Text:

我们介绍了MiniMax-Speech，一种基于Transformer的文本转语音模型。它能生成高质量的语音。模型的关键创新是可学习的演讲者编码器，从参考音频中提取音色特征，无需转录。这使得MiniMax-Speech能在零样本情况下生成表现力强、音色一致的语音，并支持一样本声音克隆。通过Flow-VAE，合成音频的整体质量得到提升。模型支持32种语言，在多种评估指标上表现出色，达到了最先进的结果。
[14.05.2025 09:14] Mistral response. {"id": "fcad18b60b4241149b8f1fb413af4987", "object": "chat.completion", "created": 1747214041, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "We introduce MiniMax-Speech, a Transformer-based text-to-speech model capable of generating high-quality speech. The key innovation of the model is a learnable speaker encoder that extracts voice characteristics from reference audio without the need for transcription. This enables MiniMax-Speech to generate expressive and voice-consistent speech in zero-shot scenarios and supports one-shot voice cloning. Through Flow-VAE, the overall quality of the synthesized audio is enhanced. The model supports 32 languages and performs excellently across various evaluation metrics, achieving state-of-the-art results."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 188, "total_tokens": 320, "completion_tokens": 132}}
[14.05.2025 09:14] Response: We introduce MiniMax-Speech, a Transformer-based text-to-speech model capable of generating high-quality speech. The key innovation of the model is a learnable speaker encoder that extracts voice characteristics from reference audio without the need for transcription. This enables MiniMax-Speech to generate expressive and voice-consistent speech in zero-shot scenarios and supports one-shot voice cloning. Through Flow-VAE, the overall quality of the synthesized audio is enhanced. The model supports 32 languages and performs excellently across various evaluation metrics, achieving state-of-the-art results.
[14.05.2025 09:14] Renaming data file.
[14.05.2025 09:14] Renaming previous data. hf_papers.json to ./d/2025-05-14.json
[14.05.2025 09:14] Saving new data file.
[14.05.2025 09:14] Generating page.
[14.05.2025 09:14] Renaming previous page.
[14.05.2025 09:14] Renaming previous data. index.html to ./d/2025-05-14.html
[14.05.2025 09:14] [Experimental] Generating Chinese page for reading.
[14.05.2025 09:14] Chinese vocab [{'word': '介绍', 'pinyin': 'jiè shào', 'trans': 'introduce'}, {'word': 'MiniMax-Speech', 'pinyin': 'MiniMax-Speech', 'trans': 'MiniMax-Speech'}, {'word': '基于', 'pinyin': 'jī yú', 'trans': 'based on'}, {'word': 'Transformer', 'pinyin': 'Transformer', 'trans': 'Transformer'}, {'word': '文本转语音', 'pinyin': 'wén běn zhuǎn yǔ yīn', 'trans': 'text-to-speech'}, {'word': '模型', 'pinyin': 'mó xíng', 'trans': 'model'}, {'word': '生成', 'pinyin': 'shēng chéng', 'trans': 'generate'}, {'word': '高质量', 'pinyin': 'gāo zhì liàng', 'trans': 'high quality'}, {'word': '语音', 'pinyin': 'yǔ yīn', 'trans': 'speech'}, {'word': '关键', 'pinyin': 'guǎn jiàn', 'trans': 'key'}, {'word': '创新', 'pinyin': 'chuàng xīn', 'trans': 'innovation'}, {'word': '可学习', 'pinyin': 'kě xué xí', 'trans': 'learnable'}, {'word': '演讲者', 'pinyin': 'yǎn jiǎng zhě', 'trans': 'speaker'}, {'word': '编码器', 'pinyin': 'biān mǎ qì', 'trans': 'encoder'}, {'word': '从', 'pinyin': 'cóng', 'trans': 'from'}, {'word': '参考', 'pinyin': 'cān kǎo', 'trans': 'reference'}, {'word': '音频', 'pinyin': 'yīn pín', 'trans': 'audio'}, {'word': '提取', 'pinyin': 'tí qu', 'trans': 'extract'}, {'word': '音色', 'pinyin': 'yīn sè', 'trans': 'timbre'}, {'word': '特征', 'pinyin': 'tè zhēng', 'trans': 'features'}, {'word': '无需', 'pinyin': 'wú xū', 'trans': 'no need'}, {'word': '转录', 'pinyin': 'zhuǎn lù', 'trans': 'transcription'}, {'word': '零样本', 'pinyin': 'líng yàng běn', 'trans': 'zero-shot'}, {'word': '情况', 'pinyin': 'qíng kuàng', 'trans': 'situation'}, {'word': '表现力', 'pinyin': 'biǎo xiàn lì', 'trans': 'expressiveness'}, {'word': '强', 'pinyin': 'qiáng', 'trans': 'strong'}, {'word': '一致', 'pinyin': 'yī zhì', 'trans': 'consistent'}, {'word': '支持', 'pinyin': 'zhī chí', 'trans': 'support'}, {'word': '一样本', 'pinyin': 'yī yàng běn', 'trans': 'one-shot'}, {'word': '声音', 'pinyin': 'shēng yīn', 'trans': 'voice'}, {'word': '克隆', 'pinyin': 'kè lóng', 'trans': 'clone'}, {'word': '通过', 'pinyin': 'tōng guò', 'trans': 'through'}, {'word': 'Flow-VAE', 'pinyin': 'Flow-VAE', 'trans': 'Flow-VAE'}, {'word': '合成', 'pinyin': 'hé chéng', 'trans': 'synthesis'}, {'word': '整体', 'pinyin': 'zhěng tǐ', 'trans': 'overall'}, {'word': '质量', 'pinyin': 'zhì liàng', 'trans': 'quality'}, {'word': '得到', 'pinyin': 'dé dào', 'trans': 'obtain'}, {'word': '提升', 'pinyin': 'tí shēng', 'trans': 'improvement'}, {'word': '多种', 'pinyin': 'duō zhǒng', 'trans': 'various'}, {'word': '评估', 'pinyin': 'píng gū', 'trans': 'evaluation'}, {'word': '指标', 'pinyin': 'zhǐ biāo', 'trans': 'metrics'}, {'word': '表现', 'pinyin': 'biǎo xiàn', 'trans': 'performance'}, {'word': '出色', 'pinyin': 'chū sè', 'trans': 'outstanding'}, {'word': '达到', 'pinyin': 'dá dào', 'trans': 'achieve'}, {'word': '最先进', 'pinyin': 'zuì xiān jìn', 'trans': 'state-of-the-art'}]
[14.05.2025 09:14] Renaming previous Chinese page.
[14.05.2025 09:14] Renaming previous data. zh.html to ./d/2025-05-13_zh_reading_task.html
[14.05.2025 09:14] Writing Chinese reading task.
[14.05.2025 09:14] Writing result.
[14.05.2025 09:14] Renaming log file.
[14.05.2025 09:14] Renaming previous data. log.txt to ./logs/2025-05-14_last_log.txt
