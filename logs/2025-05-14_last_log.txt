[14.05.2025 13:26] Read previous papers.
[14.05.2025 13:26] Generating top page (month).
[14.05.2025 13:26] Writing top page (month).
[14.05.2025 14:12] Read previous papers.
[14.05.2025 14:12] Get feed.
[14.05.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.07916
[14.05.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.07591
[14.05.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.07215
[14.05.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.08665
[14.05.2025 14:12] Extract page data from URL. URL: https://huggingface.co/papers/2505.08751
[14.05.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.08712
[14.05.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.08445
[14.05.2025 14:12] Extract page data from URL. URL: https://huggingface.co/papers/2505.08311
[14.05.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.07416
[14.05.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.21475
[14.05.2025 14:12] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[14.05.2025 14:12] No deleted papers detected.
[14.05.2025 14:12] Downloading and parsing papers (pdf, html). Total: 10.
[14.05.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2505.07916.
[14.05.2025 14:12] Extra JSON file exists (./assets/json/2505.07916.json), skip PDF parsing.
[14.05.2025 14:12] Paper image links file exists (./assets/img_data/2505.07916.json), skip HTML parsing.
[14.05.2025 14:12] Success.
[14.05.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2505.07591.
[14.05.2025 14:12] Extra JSON file exists (./assets/json/2505.07591.json), skip PDF parsing.
[14.05.2025 14:12] Paper image links file exists (./assets/img_data/2505.07591.json), skip HTML parsing.
[14.05.2025 14:12] Success.
[14.05.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2505.07215.
[14.05.2025 14:12] Extra JSON file exists (./assets/json/2505.07215.json), skip PDF parsing.
[14.05.2025 14:12] Paper image links file exists (./assets/img_data/2505.07215.json), skip HTML parsing.
[14.05.2025 14:12] Success.
[14.05.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2505.08665.
[14.05.2025 14:12] Extra JSON file exists (./assets/json/2505.08665.json), skip PDF parsing.
[14.05.2025 14:12] Paper image links file exists (./assets/img_data/2505.08665.json), skip HTML parsing.
[14.05.2025 14:12] Success.
[14.05.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2505.08751.
[14.05.2025 14:12] Downloading paper 2505.08751 from http://arxiv.org/pdf/2505.08751v1...
[14.05.2025 14:12] Extracting affiliations from text.
[14.05.2025 14:12] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 3 1 ] . [ 1 1 5 7 8 0 . 5 0 5 2 : r Aya Vision: Advancing the Frontier of 1, Yiyang Nan 1, John Dang1, Arash Ahmadian1,2, Shivalika Singh1, Madeline Smith1, Bharat Venkitesh2, Vlad Shmyhlo2, Viraat Aryabumi2, Walter Beller-Morales2, Jeremy Pekmez2, Jason Ozuzu2, Pierre Richemond2, Acyr Locatelli2, Nick Frosst2, Phil Blunsom2, Aidan Gomez2, Ivan Zhang2, Marzieh Fadaee1, Manoj Govindassamy2, Sudip Roy2, Matthias Gallé1, Beyza Ermis1, Ahmet Üstün1, and Sara Hooker1 Corresponding authors: {saurabh, olivernan, matthias, beyza, ahmet, sarahooker}@cohere.com 1Cohere Labs, 2Cohere "
[14.05.2025 14:12] Response: ```python
["Cohere Labs", "Cohere"]
```
[14.05.2025 14:12] Deleting PDF ./assets/pdf/2505.08751.pdf.
[14.05.2025 14:12] Success.
[14.05.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2505.08712.
[14.05.2025 14:12] Extra JSON file exists (./assets/json/2505.08712.json), skip PDF parsing.
[14.05.2025 14:12] Paper image links file exists (./assets/img_data/2505.08712.json), skip HTML parsing.
[14.05.2025 14:12] Success.
[14.05.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2505.08445.
[14.05.2025 14:12] Extra JSON file exists (./assets/json/2505.08445.json), skip PDF parsing.
[14.05.2025 14:12] Paper image links file exists (./assets/img_data/2505.08445.json), skip HTML parsing.
[14.05.2025 14:12] Success.
[14.05.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2505.08311.
[14.05.2025 14:12] Downloading paper 2505.08311 from http://arxiv.org/pdf/2505.08311v1...
[14.05.2025 14:12] Extracting affiliations from text.
[14.05.2025 14:12] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"AM-Thinking-v1: Advancing the Frontier of Reasoning at 32B Scale Yunjie Ji, Xiaoyu Tian, Sitong Zhao, Haotian Wang, Shuaiting Chen, Yiping Peng, Han Zhao, Xiangang Li a-m-team "
[14.05.2025 14:12] Response: []
[14.05.2025 14:12] Extracting affiliations from text.
[14.05.2025 14:12] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"AM-Thinking-v1: Advancing the Frontier of Reasoning at 32B Scale Yunjie Ji, Xiaoyu Tian, Sitong Zhao, Haotian Wang, Shuaiting Chen, Yiping Peng, Han Zhao, Xiangang Li a-m-teamWe present AM-Thinking-v1, 32B dense language model that advances the frontier of reasoning, embodying the collaborative spirit of open-source innovation. Outperforming DeepSeek-R1 and rivaling leading Mixture-of-Experts (MoE) models like Qwen3-235B-A22B and Seed1.5-Thinking, AM-Thinking-v1 achieves impressive scores of 85.3 on AIME 2024, 74.4 on AIME 2025, and 70.3 on LiveCodeBench, showcasing state-of-the-art mathematical and coding capabilities among open-source models of similar scale. Built entirely from the open-source Qwen2.5-32B base model and publicly available queries, AM-Thinking-v1 leverages meticulously crafted post-training pipeline combining supervised fine-tuning and reinforcement learning to deliver exceptional reasoning capabilities. This work demonstrates that the opensource community can achieve high performance at the 32B scale, practical sweet spot for deployment and fine-tuning. By striking balance between top-tier performance and real-world usability, we hope AM-Thinking-v1 inspires further collaborative efforts to harness mid-scale models, pushing reasoning boundaries while keeping accessibility at the core of innovation. We have open-sourced our model on Hugging Face2. 5 2 0 2 3 1 ] . [ 1 1 1 3 8 0 . 5 0 5 2 : r Figure 1: Comparison of Model Performance on Reasoning Benchmarks 1The a-m-team is an internal team at Beike (Ke.com), dedicated to exploring AGI technology. 2https://huggingface.co/a-m-team/AM-Thinking-vOver the past six months, large language models (LLMs) have demonstrated remarkable improvements in reasoning, particularly in domains such as mathematical problem solving and code generationtasks that require sophisticated logical inference. These advancements are expanding the practical applicability of LLMs across broader range of real-world scenarios. The release of DeepSeek-R1[1] has shown that open-source communities are increasingly capable of building models that rival proprietary systems such as OpenAIs o1[2], Googles Gemini 2.5[3], and Anthropics Claude 3.7[4]. More recently, the emergence of Qwen3-235B-A22B[5] has further advanced the reasoning frontier of open-source models. However, many recent breakthroughs rely on extremely large-scale Mixture-of-Experts (MoE) architectures, which impose significant infrastructure burdens and make model deployment and fine-tuning considerably more complex. In contrast, dense models of moderate size (e.g., 32B) offer better efficiency and deployability, yet often lag behind their MoE counterparts in reasoning performance. This contrast raises critical research question: Can we unlock the reasoning potential of 32B-scale dense modelswithout relying on private data or massive MoE architecturesthrough carefully designed post-training pipeline? To explore this question, we introduce and open-source AM-Thinking-v1, reasoning-optimized language model built upon the publicly available Qwen2.5-32B[6, 7] base model. Our model achieves state-of-the-art performance among dense models of comparable size and even outperforms much larger MoE models in several reasoning benchmarks. Specifically, AM-Thinking-v1 achieves impressive scores of 85.3 and 74.4 on AIME2024[8] and AIME2025[9], two challenging math competition-style benchmarks, and 70.3 on LiveCodeBench[10], widely used benchmark for evaluating code generation. It surpasses DeepSeek-R1 (671B MoE) and approaches or matches the performance of other top-tier MoE models such as Qwen3-235B-A22B and Seed1.5-Thinking[11], despite having only fraction of their parameters. Our success stems from meticulously designed post-training framework that leverages publicly available training queries. We applied strict preprocessing to various open-source queries and instructions, including deduplication, removal of low-quality or multi-modal queries (e.g., those involving images), and thorough decontamination with respect to our evaluation benchmarks. In particular, for mathematical querieswhere we observed high prevalence of noisy itemswe constructed comprehensive data processing pipeline that spans query filtering and ground-truth verification. The post-training pipeline comprises two main stages: Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL). Starting from Qwen2.5-32B base model, we apply SFT using cold-start dataset that encourages "think-then-answer" pattern and builds initial reasoning capability. During RL, we incorporate difficulty-aware query selection and two-stage training procedure to ensure both training stability and progressive improvement in performance. In summary, AM-Thinking-v1 demonstrates that even without large-scale MoE architectures, dense models at the 32B scale can achieve reasoning capabilities comparable to the best available models. We hope this work serves as practical reference for the community, highlighting how careful post-training design can bridge the performance gap while retaining the deployability advantages of moderate-scale models. This offers promising direction for future research at the intersection of scalability, accessibility, and reasoning performance.All queries used in our training come from publicly available datasets. We begin by deduplicating the queries and filtering out low-quality ones. For mathematical queries with ground-truth answers, we further verify the correctness of the provided ground truth. Additionally, we filter model-generated responses based on quality and assign difficulty levels to each query based on the pass rate observed across multiple response attempts. 2 2.1 Data Collection Our training data is collected from multiple publicly available open source datasets, spanning tasks such as mathematical reasoning, code generation, scientific reasoning, instruction follow, and general chat. Mathematical Reasoning During the collection of mathematical data, we ensure that each data point include verifiable ground truth. We incorporate datasets such as OpenR1-Math-220k[12], Big-Math-RL-Verified[13], data_ablation_full59K[14], NuminaMath[15], MetaMathQA[16], 2023_amc_data[17], DeepMath-103K[18], and AIME_1983_2024[19]. Code Generation We ensure that all collected code data include verifiable test cases. this category include PRIME[20], DeepCoder[21], KodCode[22], Datasets selected for liveincode_generation[10], opencoder[25"
[14.05.2025 14:13] Mistral response. {"id": "2da99d80f70a441587b562565428fe58", "object": "chat.completion", "created": 1747231969, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\"Beike (Ke.com)\"]\n```"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1699, "total_tokens": 1715, "completion_tokens": 16}}
[14.05.2025 14:13] Response: ```python
["Beike (Ke.com)"]
```
[14.05.2025 14:13] Deleting PDF ./assets/pdf/2505.08311.pdf.
[14.05.2025 14:13] Success.
[14.05.2025 14:13] Downloading and parsing paper https://huggingface.co/papers/2505.07416.
[14.05.2025 14:13] Extra JSON file exists (./assets/json/2505.07416.json), skip PDF parsing.
[14.05.2025 14:13] Paper image links file exists (./assets/img_data/2505.07416.json), skip HTML parsing.
[14.05.2025 14:13] Success.
[14.05.2025 14:13] Downloading and parsing paper https://huggingface.co/papers/2504.21475.
[14.05.2025 14:13] Extra JSON file exists (./assets/json/2504.21475.json), skip PDF parsing.
[14.05.2025 14:13] Paper image links file exists (./assets/img_data/2504.21475.json), skip HTML parsing.
[14.05.2025 14:13] Success.
[14.05.2025 14:13] Enriching papers with extra data.
[14.05.2025 14:13] ********************************************************************************
[14.05.2025 14:13] Abstract 0. We introduce MiniMax-Speech, an autoregressive Transformer-based Text-to-Speech (TTS) model that generates high-quality speech. A key innovation is our learnable speaker encoder, which extracts timbre features from a reference audio without requiring its transcription. This enables MiniMax-Speech to...
[14.05.2025 14:13] ********************************************************************************
[14.05.2025 14:13] Abstract 1. Instruction following evaluates large language models (LLMs) on their ability to generate outputs that adhere to user-defined constraints. However, existing benchmarks often rely on templated constraint prompts, which lack the diversity of real-world usage and limit fine-grained performance assessme...
[14.05.2025 14:13] ********************************************************************************
[14.05.2025 14:13] Abstract 2. We present gg-bench, a collection of game environments designed to evaluate general reasoning capabilities in language models. Unlike most static benchmarks, gg-bench is a data generating process where new evaluation instances can be generated at will. In particular, gg-bench is synthetically genera...
[14.05.2025 14:13] ********************************************************************************
[14.05.2025 14:13] Abstract 3. Assessing human skill levels in complex activities is a challenging problem with applications in sports, rehabilitation, and training. In this work, we present SkillFormer, a parameter-efficient architecture for unified multi-view proficiency estimation from egocentric and exocentric videos. Buildin...
[14.05.2025 14:13] ********************************************************************************
[14.05.2025 14:13] Abstract 4. Building multimodal language models is fundamentally challenging: it requires aligning vision and language modalities, curating high-quality instruction data, and avoiding the degradation of existing text-only capabilities once vision is introduced. These difficulties are further magnified in the mu...
[14.05.2025 14:13] ********************************************************************************
[14.05.2025 14:13] Abstract 5. Learning navigation in dynamic open-world environments is an important yet challenging skill for robots. Most previous methods rely on precise localization and mapping or learn from expensive real-world demonstrations. In this paper, we propose the Navigation Diffusion Policy (NavDP), an end-to-end ...
[14.05.2025 14:13] ********************************************************************************
[14.05.2025 14:13] Abstract 6. Large language models achieve high task performance yet often hallucinate or rely on outdated knowledge. Retrieval-augmented generation (RAG) addresses these gaps by coupling generation with external search. We analyse how hyperparameters influence speed and quality in RAG systems, covering Chroma a...
[14.05.2025 14:13] ********************************************************************************
[14.05.2025 14:13] Abstract 7. We present AM-Thinking-v1, a 32B dense language model that advances the frontier of reasoning, embodying the collaborative spirit of open-source innovation. Outperforming DeepSeek-R1 and rivaling leading Mixture-of-Experts (MoE) models like Qwen3-235B-A22B and Seed1.5-Thinking, AM-Thinking-v1 achiev...
[14.05.2025 14:13] ********************************************************************************
[14.05.2025 14:13] Abstract 8. Multimodal Review Helpfulness Prediction (MRHP) is an essential task in recommender systems, particularly in E-commerce platforms. Determining the helpfulness of user-generated reviews enhances user experience and improves consumer decision-making. However, existing datasets focus predominantly on E...
[14.05.2025 14:13] ********************************************************************************
[14.05.2025 14:13] Abstract 9. This study addresses the critical gap in Arabic natural language processing by developing an effective Arabic Reverse Dictionary (RD) system that enables users to find words based on their descriptions or meanings. We present a novel transformer-based approach with a semi-encoder neural network arch...
[14.05.2025 14:13] Read previous papers.
[14.05.2025 14:13] Generating reviews via LLM API.
[14.05.2025 14:13] Using data from previous issue: {"categories": ["#optimization", "#multilingual", "#games", "#audio"], "emoji": "🗣️", "ru": {"title": "Революция в синтезе речи: MiniMax-Speech - универсальный голосовой клон", "desc": "MiniMax-Speech - это автоматическая модель синтеза речи на основе Трансформера. Ключевой инновацией является обуча
[14.05.2025 14:13] Using data from previous issue: {"categories": ["#training", "#rl", "#alignment", "#optimization", "#benchmark"], "emoji": "🤖", "ru": {"title": "Многомерная оценка следования инструкциям для больших языковых моделей", "desc": "Статья представляет новый подход к оценке способности больших языковых моделей (LLM) следовать инструкция
[14.05.2025 14:13] Using data from previous issue: {"categories": ["#benchmark", "#dataset", "#reasoning", "#rl", "#games", "#synthetic"], "emoji": "🎮", "ru": {"title": "gg-bench: Новый подход к оценке способностей языковых моделей через игровые среды", "desc": "В статье представлен gg-bench - набор игровых сред для оценки способностей языковых моде
[14.05.2025 14:13] Using data from previous issue: {"categories": ["#architecture", "#dataset", "#training"], "emoji": "🎥", "ru": {"title": "SkillFormer: эффективная оценка мастерства по мультиракурсному видео", "desc": "SkillFormer - это эффективная архитектура для оценки уровня мастерства в сложных действиях по видео с нескольких ракурсов. Она исп
[14.05.2025 14:13] Querying the API.
[14.05.2025 14:13] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Building multimodal language models is fundamentally challenging: it requires aligning vision and language modalities, curating high-quality instruction data, and avoiding the degradation of existing text-only capabilities once vision is introduced. These difficulties are further magnified in the multilingual setting, where the need for multimodal data in different languages exacerbates existing data scarcity, machine translation often distorts meaning, and catastrophic forgetting is more pronounced. To address the aforementioned challenges, we introduce novel techniques spanning both data and modeling. First, we develop a synthetic annotation framework that curates high-quality, diverse multilingual multimodal instruction data, enabling Aya Vision models to produce natural, human-preferred responses to multimodal inputs across many languages. Complementing this, we propose a cross-modal model merging technique that mitigates catastrophic forgetting, effectively preserving text-only capabilities while simultaneously enhancing multimodal generative performance. Aya-Vision-8B achieves best-in-class performance compared to strong multimodal models such as Qwen-2.5-VL-7B, Pixtral-12B, and even much larger Llama-3.2-90B-Vision. We further scale this approach with Aya-Vision-32B, which outperforms models more than twice its size, such as Molmo-72B and LLaMA-3.2-90B-Vision. Our work advances multilingual progress on the multi-modal frontier, and provides insights into techniques that effectively bend the need for compute while delivering extremely high performance.
[14.05.2025 14:13] Response: {
  "desc": "Статья представляет новые методы для создания многоязычных мультимодальных языковых моделей. Авторы разработали систему синтетической аннотации для создания качественных многоязычных мультимодальных данных для обучения. Они также предложили технику объединения кросс-модальных моделей для предотвращения катастрофического забывания. Модели Aya-Vision, созданные с использованием этих методов, показывают лучшие результаты по сравнению с более крупными конкурентами в многоязычных мультимодальных задачах.",
  "emoji": "🌐",
  "title": "Прорыв в создании эффективных многоязычных мультимодальных ИИ-моделей"
}
[14.05.2025 14:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Building multimodal language models is fundamentally challenging: it requires aligning vision and language modalities, curating high-quality instruction data, and avoiding the degradation of existing text-only capabilities once vision is introduced. These difficulties are further magnified in the multilingual setting, where the need for multimodal data in different languages exacerbates existing data scarcity, machine translation often distorts meaning, and catastrophic forgetting is more pronounced. To address the aforementioned challenges, we introduce novel techniques spanning both data and modeling. First, we develop a synthetic annotation framework that curates high-quality, diverse multilingual multimodal instruction data, enabling Aya Vision models to produce natural, human-preferred responses to multimodal inputs across many languages. Complementing this, we propose a cross-modal model merging technique that mitigates catastrophic forgetting, effectively preserving text-only capabilities while simultaneously enhancing multimodal generative performance. Aya-Vision-8B achieves best-in-class performance compared to strong multimodal models such as Qwen-2.5-VL-7B, Pixtral-12B, and even much larger Llama-3.2-90B-Vision. We further scale this approach with Aya-Vision-32B, which outperforms models more than twice its size, such as Molmo-72B and LLaMA-3.2-90B-Vision. Our work advances multilingual progress on the multi-modal frontier, and provides insights into techniques that effectively bend the need for compute while delivering extremely high performance."

[14.05.2025 14:13] Response: ```python
['MULTIMODAL', 'DATA', 'MULTILINGUAL', 'TRAINING']
```
[14.05.2025 14:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Building multimodal language models is fundamentally challenging: it requires aligning vision and language modalities, curating high-quality instruction data, and avoiding the degradation of existing text-only capabilities once vision is introduced. These difficulties are further magnified in the multilingual setting, where the need for multimodal data in different languages exacerbates existing data scarcity, machine translation often distorts meaning, and catastrophic forgetting is more pronounced. To address the aforementioned challenges, we introduce novel techniques spanning both data and modeling. First, we develop a synthetic annotation framework that curates high-quality, diverse multilingual multimodal instruction data, enabling Aya Vision models to produce natural, human-preferred responses to multimodal inputs across many languages. Complementing this, we propose a cross-modal model merging technique that mitigates catastrophic forgetting, effectively preserving text-only capabilities while simultaneously enhancing multimodal generative performance. Aya-Vision-8B achieves best-in-class performance compared to strong multimodal models such as Qwen-2.5-VL-7B, Pixtral-12B, and even much larger Llama-3.2-90B-Vision. We further scale this approach with Aya-Vision-32B, which outperforms models more than twice its size, such as Molmo-72B and LLaMA-3.2-90B-Vision. Our work advances multilingual progress on the multi-modal frontier, and provides insights into techniques that effectively bend the need for compute while delivering extremely high performance."

[14.05.2025 14:13] Response: ```python
['MULTIMODAL', 'SYNTHETIC', 'LOW_RESOURCE']
```
[14.05.2025 14:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper addresses the challenges of building multimodal language models that integrate both vision and language, particularly in a multilingual context. It introduces a synthetic annotation framework to create high-quality, diverse multilingual multimodal instruction data, which helps the Aya Vision models generate human-like responses. Additionally, the authors propose a cross-modal model merging technique to prevent catastrophic forgetting, ensuring that text-only capabilities are maintained while improving multimodal performance. The results show that Aya-Vision models outperform existing multimodal models, demonstrating significant advancements in multilingual multimodal processing.","title":"Advancing Multimodal Language Models for Multilingual Mastery"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper addresses the challenges of building multimodal language models that integrate both vision and language, particularly in a multilingual context. It introduces a synthetic annotation framework to create high-quality, diverse multilingual multimodal instruction data, which helps the Aya Vision models generate human-like responses. Additionally, the authors propose a cross-modal model merging technique to prevent catastrophic forgetting, ensuring that text-only capabilities are maintained while improving multimodal performance. The results show that Aya-Vision models outperform existing multimodal models, demonstrating significant advancements in multilingual multimodal processing.', title='Advancing Multimodal Language Models for Multilingual Mastery'))
[14.05.2025 14:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"构建多模态语言模型面临许多挑战，包括视觉和语言模态的对齐、高质量指令数据的整理，以及在引入视觉后避免文本能力的退化。在多语言环境中，这些困难更加突出，因为不同语言的多模态数据需求加剧了数据稀缺，机器翻译常常扭曲意义，并且灾难性遗忘现象更加明显。为了解决这些问题，我们提出了新的技术，包括一个合成注释框架，用于整理高质量、多样化的多语言多模态指令数据，使Aya Vision模型能够在多种语言中对多模态输入生成自然、符合人类偏好的响应。此外，我们还提出了一种跨模态模型合并技术，有效减轻灾难性遗忘，同时增强多模态生成性能。","title":"多模态语言模型的突破性进展"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='构建多模态语言模型面临许多挑战，包括视觉和语言模态的对齐、高质量指令数据的整理，以及在引入视觉后避免文本能力的退化。在多语言环境中，这些困难更加突出，因为不同语言的多模态数据需求加剧了数据稀缺，机器翻译常常扭曲意义，并且灾难性遗忘现象更加明显。为了解决这些问题，我们提出了新的技术，包括一个合成注释框架，用于整理高质量、多样化的多语言多模态指令数据，使Aya Vision模型能够在多种语言中对多模态输入生成自然、符合人类偏好的响应。此外，我们还提出了一种跨模态模型合并技术，有效减轻灾难性遗忘，同时增强多模态生成性能。', title='多模态语言模型的突破性进展'))
[14.05.2025 14:13] Using data from previous issue: {"categories": ["#transfer_learning", "#agents", "#diffusion", "#robotics", "#dataset"], "emoji": "🤖", "ru": {"title": "NavDP: Универсальная навигация роботов из симуляции в реальность", "desc": "Статья представляет NavDP - систему навигации роботов в динамичной открытой среде, обученную исключитель
[14.05.2025 14:13] Using data from previous issue: {"categories": ["#optimization", "#data", "#benchmark", "#healthcare", "#hallucinations", "#rag"], "emoji": "🔍", "ru": {"title": "Оптимизация RAG: баланс между скоростью и точностью", "desc": "Статья анализирует влияние гиперпараметров на скорость и качество в системах генерации с дополнением извлеч
[14.05.2025 14:13] Querying the API.
[14.05.2025 14:13] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

We present AM-Thinking-v1, a 32B dense language model that advances the frontier of reasoning, embodying the collaborative spirit of open-source innovation. Outperforming DeepSeek-R1 and rivaling leading Mixture-of-Experts (MoE) models like Qwen3-235B-A22B and Seed1.5-Thinking, AM-Thinking-v1 achieves impressive scores of 85.3 on AIME 2024, 74.4 on AIME 2025, and 70.3 on LiveCodeBench, showcasing state-of-the-art mathematical and coding capabilities among open-source models of similar scale.   Built entirely from the open-source Qwen2.5-32B base model and publicly available queries, AM-Thinking-v1 leverages a meticulously crafted post-training pipeline - combining supervised fine-tuning and reinforcement learning - to deliver exceptional reasoning capabilities. This work demonstrates that the open-source community can achieve high performance at the 32B scale, a practical sweet spot for deployment and fine-tuning. By striking a balance between top-tier performance and real-world usability, we hope AM-Thinking-v1 inspires further collaborative efforts to harness mid-scale models, pushing reasoning boundaries while keeping accessibility at the core of innovation. We have open-sourced our model on https://huggingface.co/a-m-team/AM-Thinking-v1{Hugging Face}.
[14.05.2025 14:13] Response: {
  "desc": "AM-Thinking-v1 - это языковая модель с 32 миллиардами параметров, которая демонстрирует передовые способности к рассуждению. Модель превосходит DeepSeek-R1 и конкурирует с ведущими MoE-моделями, показывая впечатляющие результаты в математических и кодинговых тестах. AM-Thinking-v1 была создана на основе открытой модели Qwen2.5-32B с использованием тщательно разработанного процесса дообучения, включающего supervised fine-tuning и reinforcement learning. Авторы подчеркивают, что модель с 32 миллиардами параметров представляет собой практичный баланс между производительностью и удобством использования.",
  "emoji": "🧠",
  "title": "Открытая 32B-модель устанавливает новую планку в рассуждениях"
}
[14.05.2025 14:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"We present AM-Thinking-v1, a 32B dense language model that advances the frontier of reasoning, embodying the collaborative spirit of open-source innovation. Outperforming DeepSeek-R1 and rivaling leading Mixture-of-Experts (MoE) models like Qwen3-235B-A22B and Seed1.5-Thinking, AM-Thinking-v1 achieves impressive scores of 85.3 on AIME 2024, 74.4 on AIME 2025, and 70.3 on LiveCodeBench, showcasing state-of-the-art mathematical and coding capabilities among open-source models of similar scale.   Built entirely from the open-source Qwen2.5-32B base model and publicly available queries, AM-Thinking-v1 leverages a meticulously crafted post-training pipeline - combining supervised fine-tuning and reinforcement learning - to deliver exceptional reasoning capabilities. This work demonstrates that the open-source community can achieve high performance at the 32B scale, a practical sweet spot for deployment and fine-tuning. By striking a balance between top-tier performance and real-world usability, we hope AM-Thinking-v1 inspires further collaborative efforts to harness mid-scale models, pushing reasoning boundaries while keeping accessibility at the core of innovation. We have open-sourced our model on https://huggingface.co/a-m-team/AM-Thinking-v1{Hugging Face}."

[14.05.2025 14:13] Response: ```python
['DATASET', 'TRAINING', 'RL']
```
[14.05.2025 14:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"We present AM-Thinking-v1, a 32B dense language model that advances the frontier of reasoning, embodying the collaborative spirit of open-source innovation. Outperforming DeepSeek-R1 and rivaling leading Mixture-of-Experts (MoE) models like Qwen3-235B-A22B and Seed1.5-Thinking, AM-Thinking-v1 achieves impressive scores of 85.3 on AIME 2024, 74.4 on AIME 2025, and 70.3 on LiveCodeBench, showcasing state-of-the-art mathematical and coding capabilities among open-source models of similar scale.   Built entirely from the open-source Qwen2.5-32B base model and publicly available queries, AM-Thinking-v1 leverages a meticulously crafted post-training pipeline - combining supervised fine-tuning and reinforcement learning - to deliver exceptional reasoning capabilities. This work demonstrates that the open-source community can achieve high performance at the 32B scale, a practical sweet spot for deployment and fine-tuning. By striking a balance between top-tier performance and real-world usability, we hope AM-Thinking-v1 inspires further collaborative efforts to harness mid-scale models, pushing reasoning boundaries while keeping accessibility at the core of innovation. We have open-sourced our model on https://huggingface.co/a-m-team/AM-Thinking-v1{Hugging Face}."

[14.05.2025 14:13] Response: ```python
['REASONING', 'OPEN_SOURCE']
```
[14.05.2025 14:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"AM-Thinking-v1 is a 32 billion parameter dense language model that enhances reasoning capabilities, showcasing the power of open-source collaboration. It surpasses previous models like DeepSeek-R1 and competes with advanced Mixture-of-Experts models, achieving high scores on various benchmarks. The model is built on the Qwen2.5-32B base and utilizes a combination of supervised fine-tuning and reinforcement learning in its post-training process. This work highlights the potential of mid-scale models for practical applications, aiming to inspire further innovation in the open-source community.","title":"Unlocking Reasoning with Open-Source Innovation"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='AM-Thinking-v1 is a 32 billion parameter dense language model that enhances reasoning capabilities, showcasing the power of open-source collaboration. It surpasses previous models like DeepSeek-R1 and competes with advanced Mixture-of-Experts models, achieving high scores on various benchmarks. The model is built on the Qwen2.5-32B base and utilizes a combination of supervised fine-tuning and reinforcement learning in its post-training process. This work highlights the potential of mid-scale models for practical applications, aiming to inspire further innovation in the open-source community.', title='Unlocking Reasoning with Open-Source Innovation'))
[14.05.2025 14:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"我们介绍了AM-Thinking-v1，这是一个32B的密集语言模型，推动了推理的前沿，体现了开源创新的合作精神。它在AIME 2024、AIME 2025和LiveCodeBench等基准测试中表现出色，超越了DeepSeek-R1，并与领先的专家混合模型如Qwen3-235B-A22B和Seed1.5-Thinking相媲美。AM-Thinking-v1完全基于开源的Qwen2.5-32B基础模型，结合监督微调和强化学习，展现了卓越的推理能力。我们的工作证明了开源社区可以在32B规模上实现高性能，平衡顶级性能与实际可用性，激励更多合作努力。","title":"开源创新，推理新高度"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='我们介绍了AM-Thinking-v1，这是一个32B的密集语言模型，推动了推理的前沿，体现了开源创新的合作精神。它在AIME 2024、AIME 2025和LiveCodeBench等基准测试中表现出色，超越了DeepSeek-R1，并与领先的专家混合模型如Qwen3-235B-A22B和Seed1.5-Thinking相媲美。AM-Thinking-v1完全基于开源的Qwen2.5-32B基础模型，结合监督微调和强化学习，展现了卓越的推理能力。我们的工作证明了开源社区可以在32B规模上实现高性能，平衡顶级性能与实际可用性，激励更多合作努力。', title='开源创新，推理新高度'))
[14.05.2025 14:13] Using data from previous issue: {"categories": ["#optimization", "#dataset", "#data", "#multilingual", "#low_resource", "#open_source"], "emoji": "🇻🇳", "ru": {"title": "ИИ ускоряет создание датасета для анализа отзывов на вьетнамском языке", "desc": "Статья представляет новый набор данных ViMRHP для прогнозирования полезности отзы
[14.05.2025 14:13] Using data from previous issue: {"categories": ["#training", "#data", "#translation", "#dataset", "#architecture", "#multilingual", "#low_resource"], "emoji": "📚", "ru": {"title": "Революция в арабской лингвистике: трансформеры на службе обратного словаря", "desc": "Исследование посвящено разработке эффективной системы обратного с
[14.05.2025 14:13] Loading Chinese text from previous data.
[14.05.2025 14:13] Renaming data file.
[14.05.2025 14:13] Renaming previous data. hf_papers.json to ./d/2025-05-14.json
[14.05.2025 14:13] Saving new data file.
[14.05.2025 14:13] Generating page.
[14.05.2025 14:13] Renaming previous page.
[14.05.2025 14:13] Renaming previous data. index.html to ./d/2025-05-14.html
[14.05.2025 14:13] [Experimental] Generating Chinese page for reading.
[14.05.2025 14:13] Chinese vocab [{'word': '介绍', 'pinyin': 'jiè shào', 'trans': 'introduce'}, {'word': 'MiniMax-Speech', 'pinyin': 'MiniMax-Speech', 'trans': 'MiniMax-Speech'}, {'word': '基于', 'pinyin': 'jī yú', 'trans': 'based on'}, {'word': 'Transformer', 'pinyin': 'Transformer', 'trans': 'Transformer'}, {'word': '文本转语音', 'pinyin': 'wén běn zhuǎn yǔ yīn', 'trans': 'text-to-speech'}, {'word': '模型', 'pinyin': 'mó xíng', 'trans': 'model'}, {'word': '生成', 'pinyin': 'shēng chéng', 'trans': 'generate'}, {'word': '高质量', 'pinyin': 'gāo zhì liàng', 'trans': 'high quality'}, {'word': '语音', 'pinyin': 'yǔ yīn', 'trans': 'speech'}, {'word': '关键', 'pinyin': 'guǎn jiàn', 'trans': 'key'}, {'word': '创新', 'pinyin': 'chuàng xīn', 'trans': 'innovation'}, {'word': '可学习', 'pinyin': 'kě xué xí', 'trans': 'learnable'}, {'word': '演讲者', 'pinyin': 'yǎn jiǎng zhě', 'trans': 'speaker'}, {'word': '编码器', 'pinyin': 'biān mǎ qì', 'trans': 'encoder'}, {'word': '从', 'pinyin': 'cóng', 'trans': 'from'}, {'word': '参考', 'pinyin': 'cān kǎo', 'trans': 'reference'}, {'word': '音频', 'pinyin': 'yīn pín', 'trans': 'audio'}, {'word': '提取', 'pinyin': 'tí qu', 'trans': 'extract'}, {'word': '音色', 'pinyin': 'yīn sè', 'trans': 'timbre'}, {'word': '特征', 'pinyin': 'tè zhēng', 'trans': 'features'}, {'word': '无需', 'pinyin': 'wú xū', 'trans': 'no need'}, {'word': '转录', 'pinyin': 'zhuǎn lù', 'trans': 'transcription'}, {'word': '零样本', 'pinyin': 'líng yàng běn', 'trans': 'zero-shot'}, {'word': '情况', 'pinyin': 'qíng kuàng', 'trans': 'situation'}, {'word': '表现力', 'pinyin': 'biǎo xiàn lì', 'trans': 'expressiveness'}, {'word': '强', 'pinyin': 'qiáng', 'trans': 'strong'}, {'word': '一致', 'pinyin': 'yī zhì', 'trans': 'consistent'}, {'word': '支持', 'pinyin': 'zhī chí', 'trans': 'support'}, {'word': '一样本', 'pinyin': 'yī yàng běn', 'trans': 'one-shot'}, {'word': '声音', 'pinyin': 'shēng yīn', 'trans': 'voice'}, {'word': '克隆', 'pinyin': 'kè lóng', 'trans': 'clone'}, {'word': '通过', 'pinyin': 'tōng guò', 'trans': 'through'}, {'word': 'Flow-VAE', 'pinyin': 'Flow-VAE', 'trans': 'Flow-VAE'}, {'word': '合成', 'pinyin': 'hé chéng', 'trans': 'synthesis'}, {'word': '整体', 'pinyin': 'zhěng tǐ', 'trans': 'overall'}, {'word': '质量', 'pinyin': 'zhì liàng', 'trans': 'quality'}, {'word': '得到', 'pinyin': 'dé dào', 'trans': 'obtain'}, {'word': '提升', 'pinyin': 'tí shēng', 'trans': 'improvement'}, {'word': '多种', 'pinyin': 'duō zhǒng', 'trans': 'various'}, {'word': '评估', 'pinyin': 'píng gū', 'trans': 'evaluation'}, {'word': '指标', 'pinyin': 'zhǐ biāo', 'trans': 'metrics'}, {'word': '表现', 'pinyin': 'biǎo xiàn', 'trans': 'performance'}, {'word': '出色', 'pinyin': 'chū sè', 'trans': 'outstanding'}, {'word': '达到', 'pinyin': 'dá dào', 'trans': 'achieve'}, {'word': '最先进', 'pinyin': 'zuì xiān jìn', 'trans': 'state-of-the-art'}]
[14.05.2025 14:13] Renaming previous Chinese page.
[14.05.2025 14:13] Renaming previous data. zh.html to ./d/2025-05-13_zh_reading_task.html
[14.05.2025 14:13] Writing Chinese reading task.
[14.05.2025 14:13] Writing result.
[14.05.2025 14:13] Renaming log file.
[14.05.2025 14:13] Renaming previous data. log.txt to ./logs/2025-05-14_last_log.txt
