[14.05.2025 13:26] Read previous papers.
[14.05.2025 13:26] Generating top page (month).
[14.05.2025 13:26] Writing top page (month).
[14.05.2025 14:12] Read previous papers.
[14.05.2025 14:12] Get feed.
[14.05.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.07916
[14.05.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.07591
[14.05.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.07215
[14.05.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.08665
[14.05.2025 14:12] Extract page data from URL. URL: https://huggingface.co/papers/2505.08751
[14.05.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.08712
[14.05.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.08445
[14.05.2025 14:12] Extract page data from URL. URL: https://huggingface.co/papers/2505.08311
[14.05.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.07416
[14.05.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.21475
[14.05.2025 14:12] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[14.05.2025 14:12] No deleted papers detected.
[14.05.2025 14:12] Downloading and parsing papers (pdf, html). Total: 10.
[14.05.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2505.07916.
[14.05.2025 14:12] Extra JSON file exists (./assets/json/2505.07916.json), skip PDF parsing.
[14.05.2025 14:12] Paper image links file exists (./assets/img_data/2505.07916.json), skip HTML parsing.
[14.05.2025 14:12] Success.
[14.05.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2505.07591.
[14.05.2025 14:12] Extra JSON file exists (./assets/json/2505.07591.json), skip PDF parsing.
[14.05.2025 14:12] Paper image links file exists (./assets/img_data/2505.07591.json), skip HTML parsing.
[14.05.2025 14:12] Success.
[14.05.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2505.07215.
[14.05.2025 14:12] Extra JSON file exists (./assets/json/2505.07215.json), skip PDF parsing.
[14.05.2025 14:12] Paper image links file exists (./assets/img_data/2505.07215.json), skip HTML parsing.
[14.05.2025 14:12] Success.
[14.05.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2505.08665.
[14.05.2025 14:12] Extra JSON file exists (./assets/json/2505.08665.json), skip PDF parsing.
[14.05.2025 14:12] Paper image links file exists (./assets/img_data/2505.08665.json), skip HTML parsing.
[14.05.2025 14:12] Success.
[14.05.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2505.08751.
[14.05.2025 14:12] Downloading paper 2505.08751 from http://arxiv.org/pdf/2505.08751v1...
[14.05.2025 14:12] Extracting affiliations from text.
[14.05.2025 14:12] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 3 1 ] . [ 1 1 5 7 8 0 . 5 0 5 2 : r Aya Vision: Advancing the Frontier of 1, Yiyang Nan 1, John Dang1, Arash Ahmadian1,2, Shivalika Singh1, Madeline Smith1, Bharat Venkitesh2, Vlad Shmyhlo2, Viraat Aryabumi2, Walter Beller-Morales2, Jeremy Pekmez2, Jason Ozuzu2, Pierre Richemond2, Acyr Locatelli2, Nick Frosst2, Phil Blunsom2, Aidan Gomez2, Ivan Zhang2, Marzieh Fadaee1, Manoj Govindassamy2, Sudip Roy2, Matthias GallÃ©1, Beyza Ermis1, Ahmet ÃœstÃ¼n1, and Sara Hooker1 Corresponding authors: {saurabh, olivernan, matthias, beyza, ahmet, sarahooker}@cohere.com 1Cohere Labs, 2Cohere "
[14.05.2025 14:12] Response: ```python
["Cohere Labs", "Cohere"]
```
[14.05.2025 14:12] Deleting PDF ./assets/pdf/2505.08751.pdf.
[14.05.2025 14:12] Success.
[14.05.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2505.08712.
[14.05.2025 14:12] Extra JSON file exists (./assets/json/2505.08712.json), skip PDF parsing.
[14.05.2025 14:12] Paper image links file exists (./assets/img_data/2505.08712.json), skip HTML parsing.
[14.05.2025 14:12] Success.
[14.05.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2505.08445.
[14.05.2025 14:12] Extra JSON file exists (./assets/json/2505.08445.json), skip PDF parsing.
[14.05.2025 14:12] Paper image links file exists (./assets/img_data/2505.08445.json), skip HTML parsing.
[14.05.2025 14:12] Success.
[14.05.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2505.08311.
[14.05.2025 14:12] Downloading paper 2505.08311 from http://arxiv.org/pdf/2505.08311v1...
[14.05.2025 14:12] Extracting affiliations from text.
[14.05.2025 14:12] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"AM-Thinking-v1: Advancing the Frontier of Reasoning at 32B Scale Yunjie Ji, Xiaoyu Tian, Sitong Zhao, Haotian Wang, Shuaiting Chen, Yiping Peng, Han Zhao, Xiangang Li a-m-team "
[14.05.2025 14:12] Response: []
[14.05.2025 14:12] Extracting affiliations from text.
[14.05.2025 14:12] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"AM-Thinking-v1: Advancing the Frontier of Reasoning at 32B Scale Yunjie Ji, Xiaoyu Tian, Sitong Zhao, Haotian Wang, Shuaiting Chen, Yiping Peng, Han Zhao, Xiangang Li a-m-teamWe present AM-Thinking-v1, 32B dense language model that advances the frontier of reasoning, embodying the collaborative spirit of open-source innovation. Outperforming DeepSeek-R1 and rivaling leading Mixture-of-Experts (MoE) models like Qwen3-235B-A22B and Seed1.5-Thinking, AM-Thinking-v1 achieves impressive scores of 85.3 on AIME 2024, 74.4 on AIME 2025, and 70.3 on LiveCodeBench, showcasing state-of-the-art mathematical and coding capabilities among open-source models of similar scale. Built entirely from the open-source Qwen2.5-32B base model and publicly available queries, AM-Thinking-v1 leverages meticulously crafted post-training pipeline combining supervised fine-tuning and reinforcement learning to deliver exceptional reasoning capabilities. This work demonstrates that the opensource community can achieve high performance at the 32B scale, practical sweet spot for deployment and fine-tuning. By striking balance between top-tier performance and real-world usability, we hope AM-Thinking-v1 inspires further collaborative efforts to harness mid-scale models, pushing reasoning boundaries while keeping accessibility at the core of innovation. We have open-sourced our model on Hugging Face2. 5 2 0 2 3 1 ] . [ 1 1 1 3 8 0 . 5 0 5 2 : r Figure 1: Comparison of Model Performance on Reasoning Benchmarks 1The a-m-team is an internal team at Beike (Ke.com), dedicated to exploring AGI technology. 2https://huggingface.co/a-m-team/AM-Thinking-vOver the past six months, large language models (LLMs) have demonstrated remarkable improvements in reasoning, particularly in domains such as mathematical problem solving and code generationtasks that require sophisticated logical inference. These advancements are expanding the practical applicability of LLMs across broader range of real-world scenarios. The release of DeepSeek-R1[1] has shown that open-source communities are increasingly capable of building models that rival proprietary systems such as OpenAIs o1[2], Googles Gemini 2.5[3], and Anthropics Claude 3.7[4]. More recently, the emergence of Qwen3-235B-A22B[5] has further advanced the reasoning frontier of open-source models. However, many recent breakthroughs rely on extremely large-scale Mixture-of-Experts (MoE) architectures, which impose significant infrastructure burdens and make model deployment and fine-tuning considerably more complex. In contrast, dense models of moderate size (e.g., 32B) offer better efficiency and deployability, yet often lag behind their MoE counterparts in reasoning performance. This contrast raises critical research question: Can we unlock the reasoning potential of 32B-scale dense modelswithout relying on private data or massive MoE architecturesthrough carefully designed post-training pipeline? To explore this question, we introduce and open-source AM-Thinking-v1, reasoning-optimized language model built upon the publicly available Qwen2.5-32B[6, 7] base model. Our model achieves state-of-the-art performance among dense models of comparable size and even outperforms much larger MoE models in several reasoning benchmarks. Specifically, AM-Thinking-v1 achieves impressive scores of 85.3 and 74.4 on AIME2024[8] and AIME2025[9], two challenging math competition-style benchmarks, and 70.3 on LiveCodeBench[10], widely used benchmark for evaluating code generation. It surpasses DeepSeek-R1 (671B MoE) and approaches or matches the performance of other top-tier MoE models such as Qwen3-235B-A22B and Seed1.5-Thinking[11], despite having only fraction of their parameters. Our success stems from meticulously designed post-training framework that leverages publicly available training queries. We applied strict preprocessing to various open-source queries and instructions, including deduplication, removal of low-quality or multi-modal queries (e.g., those involving images), and thorough decontamination with respect to our evaluation benchmarks. In particular, for mathematical querieswhere we observed high prevalence of noisy itemswe constructed comprehensive data processing pipeline that spans query filtering and ground-truth verification. The post-training pipeline comprises two main stages: Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL). Starting from Qwen2.5-32B base model, we apply SFT using cold-start dataset that encourages "think-then-answer" pattern and builds initial reasoning capability. During RL, we incorporate difficulty-aware query selection and two-stage training procedure to ensure both training stability and progressive improvement in performance. In summary, AM-Thinking-v1 demonstrates that even without large-scale MoE architectures, dense models at the 32B scale can achieve reasoning capabilities comparable to the best available models. We hope this work serves as practical reference for the community, highlighting how careful post-training design can bridge the performance gap while retaining the deployability advantages of moderate-scale models. This offers promising direction for future research at the intersection of scalability, accessibility, and reasoning performance.All queries used in our training come from publicly available datasets. We begin by deduplicating the queries and filtering out low-quality ones. For mathematical queries with ground-truth answers, we further verify the correctness of the provided ground truth. Additionally, we filter model-generated responses based on quality and assign difficulty levels to each query based on the pass rate observed across multiple response attempts. 2 2.1 Data Collection Our training data is collected from multiple publicly available open source datasets, spanning tasks such as mathematical reasoning, code generation, scientific reasoning, instruction follow, and general chat. Mathematical Reasoning During the collection of mathematical data, we ensure that each data point include verifiable ground truth. We incorporate datasets such as OpenR1-Math-220k[12], Big-Math-RL-Verified[13], data_ablation_full59K[14], NuminaMath[15], MetaMathQA[16], 2023_amc_data[17], DeepMath-103K[18], and AIME_1983_2024[19]. Code Generation We ensure that all collected code data include verifiable test cases. this category include PRIME[20], DeepCoder[21], KodCode[22], Datasets selected for liveincode_generation[10], opencoder[25"
[14.05.2025 14:13] Mistral response. {"id": "2da99d80f70a441587b562565428fe58", "object": "chat.completion", "created": 1747231969, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\"Beike (Ke.com)\"]\n```"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1699, "total_tokens": 1715, "completion_tokens": 16}}
[14.05.2025 14:13] Response: ```python
["Beike (Ke.com)"]
```
[14.05.2025 14:13] Deleting PDF ./assets/pdf/2505.08311.pdf.
[14.05.2025 14:13] Success.
[14.05.2025 14:13] Downloading and parsing paper https://huggingface.co/papers/2505.07416.
[14.05.2025 14:13] Extra JSON file exists (./assets/json/2505.07416.json), skip PDF parsing.
[14.05.2025 14:13] Paper image links file exists (./assets/img_data/2505.07416.json), skip HTML parsing.
[14.05.2025 14:13] Success.
[14.05.2025 14:13] Downloading and parsing paper https://huggingface.co/papers/2504.21475.
[14.05.2025 14:13] Extra JSON file exists (./assets/json/2504.21475.json), skip PDF parsing.
[14.05.2025 14:13] Paper image links file exists (./assets/img_data/2504.21475.json), skip HTML parsing.
[14.05.2025 14:13] Success.
[14.05.2025 14:13] Enriching papers with extra data.
[14.05.2025 14:13] ********************************************************************************
[14.05.2025 14:13] Abstract 0. We introduce MiniMax-Speech, an autoregressive Transformer-based Text-to-Speech (TTS) model that generates high-quality speech. A key innovation is our learnable speaker encoder, which extracts timbre features from a reference audio without requiring its transcription. This enables MiniMax-Speech to...
[14.05.2025 14:13] ********************************************************************************
[14.05.2025 14:13] Abstract 1. Instruction following evaluates large language models (LLMs) on their ability to generate outputs that adhere to user-defined constraints. However, existing benchmarks often rely on templated constraint prompts, which lack the diversity of real-world usage and limit fine-grained performance assessme...
[14.05.2025 14:13] ********************************************************************************
[14.05.2025 14:13] Abstract 2. We present gg-bench, a collection of game environments designed to evaluate general reasoning capabilities in language models. Unlike most static benchmarks, gg-bench is a data generating process where new evaluation instances can be generated at will. In particular, gg-bench is synthetically genera...
[14.05.2025 14:13] ********************************************************************************
[14.05.2025 14:13] Abstract 3. Assessing human skill levels in complex activities is a challenging problem with applications in sports, rehabilitation, and training. In this work, we present SkillFormer, a parameter-efficient architecture for unified multi-view proficiency estimation from egocentric and exocentric videos. Buildin...
[14.05.2025 14:13] ********************************************************************************
[14.05.2025 14:13] Abstract 4. Building multimodal language models is fundamentally challenging: it requires aligning vision and language modalities, curating high-quality instruction data, and avoiding the degradation of existing text-only capabilities once vision is introduced. These difficulties are further magnified in the mu...
[14.05.2025 14:13] ********************************************************************************
[14.05.2025 14:13] Abstract 5. Learning navigation in dynamic open-world environments is an important yet challenging skill for robots. Most previous methods rely on precise localization and mapping or learn from expensive real-world demonstrations. In this paper, we propose the Navigation Diffusion Policy (NavDP), an end-to-end ...
[14.05.2025 14:13] ********************************************************************************
[14.05.2025 14:13] Abstract 6. Large language models achieve high task performance yet often hallucinate or rely on outdated knowledge. Retrieval-augmented generation (RAG) addresses these gaps by coupling generation with external search. We analyse how hyperparameters influence speed and quality in RAG systems, covering Chroma a...
[14.05.2025 14:13] ********************************************************************************
[14.05.2025 14:13] Abstract 7. We present AM-Thinking-v1, a 32B dense language model that advances the frontier of reasoning, embodying the collaborative spirit of open-source innovation. Outperforming DeepSeek-R1 and rivaling leading Mixture-of-Experts (MoE) models like Qwen3-235B-A22B and Seed1.5-Thinking, AM-Thinking-v1 achiev...
[14.05.2025 14:13] ********************************************************************************
[14.05.2025 14:13] Abstract 8. Multimodal Review Helpfulness Prediction (MRHP) is an essential task in recommender systems, particularly in E-commerce platforms. Determining the helpfulness of user-generated reviews enhances user experience and improves consumer decision-making. However, existing datasets focus predominantly on E...
[14.05.2025 14:13] ********************************************************************************
[14.05.2025 14:13] Abstract 9. This study addresses the critical gap in Arabic natural language processing by developing an effective Arabic Reverse Dictionary (RD) system that enables users to find words based on their descriptions or meanings. We present a novel transformer-based approach with a semi-encoder neural network arch...
[14.05.2025 14:13] Read previous papers.
[14.05.2025 14:13] Generating reviews via LLM API.
[14.05.2025 14:13] Using data from previous issue: {"categories": ["#optimization", "#multilingual", "#games", "#audio"], "emoji": "ğŸ—£ï¸", "ru": {"title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğµ Ñ€ĞµÑ‡Ğ¸: MiniMax-Speech - ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğ²Ğ¾Ğ¹ ĞºĞ»Ğ¾Ğ½", "desc": "MiniMax-Speech - ÑÑ‚Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ñ€ĞµÑ‡Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¢Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸ĞµĞ¹ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ¾Ğ±ÑƒÑ‡Ğ°
[14.05.2025 14:13] Using data from previous issue: {"categories": ["#training", "#rl", "#alignment", "#optimization", "#benchmark"], "emoji": "ğŸ¤–", "ru": {"title": "ĞœĞ½Ğ¾Ğ³Ğ¾Ğ¼ĞµÑ€Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ†ĞµĞ½ĞºĞµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ
[14.05.2025 14:13] Using data from previous issue: {"categories": ["#benchmark", "#dataset", "#reasoning", "#rl", "#games", "#synthetic"], "emoji": "ğŸ®", "ru": {"title": "gg-bench: ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ†ĞµĞ½ĞºĞµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ³Ñ€Ğ¾Ğ²Ñ‹Ğµ ÑÑ€ĞµĞ´Ñ‹", "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ gg-bench - Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¸Ğ³Ñ€Ğ¾Ğ²Ñ‹Ñ… ÑÑ€ĞµĞ´ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğµ
[14.05.2025 14:13] Using data from previous issue: {"categories": ["#architecture", "#dataset", "#training"], "emoji": "ğŸ¥", "ru": {"title": "SkillFormer: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ¼Ğ°ÑÑ‚ĞµÑ€ÑÑ‚Ğ²Ğ° Ğ¿Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ğ¾Ğ¼Ñƒ Ğ²Ğ¸Ğ´ĞµĞ¾", "desc": "SkillFormer - ÑÑ‚Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¼Ğ°ÑÑ‚ĞµÑ€ÑÑ‚Ğ²Ğ° Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑÑ… Ğ¿Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ². ĞĞ½Ğ° Ğ¸ÑĞ¿
[14.05.2025 14:13] Querying the API.
[14.05.2025 14:13] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Building multimodal language models is fundamentally challenging: it requires aligning vision and language modalities, curating high-quality instruction data, and avoiding the degradation of existing text-only capabilities once vision is introduced. These difficulties are further magnified in the multilingual setting, where the need for multimodal data in different languages exacerbates existing data scarcity, machine translation often distorts meaning, and catastrophic forgetting is more pronounced. To address the aforementioned challenges, we introduce novel techniques spanning both data and modeling. First, we develop a synthetic annotation framework that curates high-quality, diverse multilingual multimodal instruction data, enabling Aya Vision models to produce natural, human-preferred responses to multimodal inputs across many languages. Complementing this, we propose a cross-modal model merging technique that mitigates catastrophic forgetting, effectively preserving text-only capabilities while simultaneously enhancing multimodal generative performance. Aya-Vision-8B achieves best-in-class performance compared to strong multimodal models such as Qwen-2.5-VL-7B, Pixtral-12B, and even much larger Llama-3.2-90B-Vision. We further scale this approach with Aya-Vision-32B, which outperforms models more than twice its size, such as Molmo-72B and LLaMA-3.2-90B-Vision. Our work advances multilingual progress on the multi-modal frontier, and provides insights into techniques that effectively bend the need for compute while delivering extremely high performance.
[14.05.2025 14:13] Response: {
  "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‚Ğ°ÑÑ‚Ñ€Ğ¾Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ·Ğ°Ğ±Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Aya-Vision, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑÑ‚Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğ¼Ğ¸ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ….",
  "emoji": "ğŸŒ",
  "title": "ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹"
}
[14.05.2025 14:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Building multimodal language models is fundamentally challenging: it requires aligning vision and language modalities, curating high-quality instruction data, and avoiding the degradation of existing text-only capabilities once vision is introduced. These difficulties are further magnified in the multilingual setting, where the need for multimodal data in different languages exacerbates existing data scarcity, machine translation often distorts meaning, and catastrophic forgetting is more pronounced. To address the aforementioned challenges, we introduce novel techniques spanning both data and modeling. First, we develop a synthetic annotation framework that curates high-quality, diverse multilingual multimodal instruction data, enabling Aya Vision models to produce natural, human-preferred responses to multimodal inputs across many languages. Complementing this, we propose a cross-modal model merging technique that mitigates catastrophic forgetting, effectively preserving text-only capabilities while simultaneously enhancing multimodal generative performance. Aya-Vision-8B achieves best-in-class performance compared to strong multimodal models such as Qwen-2.5-VL-7B, Pixtral-12B, and even much larger Llama-3.2-90B-Vision. We further scale this approach with Aya-Vision-32B, which outperforms models more than twice its size, such as Molmo-72B and LLaMA-3.2-90B-Vision. Our work advances multilingual progress on the multi-modal frontier, and provides insights into techniques that effectively bend the need for compute while delivering extremely high performance."

[14.05.2025 14:13] Response: ```python
['MULTIMODAL', 'DATA', 'MULTILINGUAL', 'TRAINING']
```
[14.05.2025 14:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Building multimodal language models is fundamentally challenging: it requires aligning vision and language modalities, curating high-quality instruction data, and avoiding the degradation of existing text-only capabilities once vision is introduced. These difficulties are further magnified in the multilingual setting, where the need for multimodal data in different languages exacerbates existing data scarcity, machine translation often distorts meaning, and catastrophic forgetting is more pronounced. To address the aforementioned challenges, we introduce novel techniques spanning both data and modeling. First, we develop a synthetic annotation framework that curates high-quality, diverse multilingual multimodal instruction data, enabling Aya Vision models to produce natural, human-preferred responses to multimodal inputs across many languages. Complementing this, we propose a cross-modal model merging technique that mitigates catastrophic forgetting, effectively preserving text-only capabilities while simultaneously enhancing multimodal generative performance. Aya-Vision-8B achieves best-in-class performance compared to strong multimodal models such as Qwen-2.5-VL-7B, Pixtral-12B, and even much larger Llama-3.2-90B-Vision. We further scale this approach with Aya-Vision-32B, which outperforms models more than twice its size, such as Molmo-72B and LLaMA-3.2-90B-Vision. Our work advances multilingual progress on the multi-modal frontier, and provides insights into techniques that effectively bend the need for compute while delivering extremely high performance."

[14.05.2025 14:13] Response: ```python
['MULTIMODAL', 'SYNTHETIC', 'LOW_RESOURCE']
```
[14.05.2025 14:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper addresses the challenges of building multimodal language models that integrate both vision and language, particularly in a multilingual context. It introduces a synthetic annotation framework to create high-quality, diverse multilingual multimodal instruction data, which helps the Aya Vision models generate human-like responses. Additionally, the authors propose a cross-modal model merging technique to prevent catastrophic forgetting, ensuring that text-only capabilities are maintained while improving multimodal performance. The results show that Aya-Vision models outperform existing multimodal models, demonstrating significant advancements in multilingual multimodal processing.","title":"Advancing Multimodal Language Models for Multilingual Mastery"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper addresses the challenges of building multimodal language models that integrate both vision and language, particularly in a multilingual context. It introduces a synthetic annotation framework to create high-quality, diverse multilingual multimodal instruction data, which helps the Aya Vision models generate human-like responses. Additionally, the authors propose a cross-modal model merging technique to prevent catastrophic forgetting, ensuring that text-only capabilities are maintained while improving multimodal performance. The results show that Aya-Vision models outperform existing multimodal models, demonstrating significant advancements in multilingual multimodal processing.', title='Advancing Multimodal Language Models for Multilingual Mastery'))
[14.05.2025 14:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"æ„å»ºå¤šæ¨¡æ€è¯­è¨€æ¨¡å‹é¢ä¸´è®¸å¤šæŒ‘æˆ˜ï¼ŒåŒ…æ‹¬è§†è§‰å’Œè¯­è¨€æ¨¡æ€çš„å¯¹é½ã€é«˜è´¨é‡æŒ‡ä»¤æ•°æ®çš„æ•´ç†ï¼Œä»¥åŠåœ¨å¼•å…¥è§†è§‰åé¿å…æ–‡æœ¬èƒ½åŠ›çš„é€€åŒ–ã€‚åœ¨å¤šè¯­è¨€ç¯å¢ƒä¸­ï¼Œè¿™äº›å›°éš¾æ›´åŠ çªå‡ºï¼Œå› ä¸ºä¸åŒè¯­è¨€çš„å¤šæ¨¡æ€æ•°æ®éœ€æ±‚åŠ å‰§äº†æ•°æ®ç¨€ç¼ºï¼Œæœºå™¨ç¿»è¯‘å¸¸å¸¸æ‰­æ›²æ„ä¹‰ï¼Œå¹¶ä¸”ç¾éš¾æ€§é—å¿˜ç°è±¡æ›´åŠ æ˜æ˜¾ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†æ–°çš„æŠ€æœ¯ï¼ŒåŒ…æ‹¬ä¸€ä¸ªåˆæˆæ³¨é‡Šæ¡†æ¶ï¼Œç”¨äºæ•´ç†é«˜è´¨é‡ã€å¤šæ ·åŒ–çš„å¤šè¯­è¨€å¤šæ¨¡æ€æŒ‡ä»¤æ•°æ®ï¼Œä½¿Aya Visionæ¨¡å‹èƒ½å¤Ÿåœ¨å¤šç§è¯­è¨€ä¸­å¯¹å¤šæ¨¡æ€è¾“å…¥ç”Ÿæˆè‡ªç„¶ã€ç¬¦åˆäººç±»åå¥½çš„å“åº”ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§è·¨æ¨¡æ€æ¨¡å‹åˆå¹¶æŠ€æœ¯ï¼Œæœ‰æ•ˆå‡è½»ç¾éš¾æ€§é—å¿˜ï¼ŒåŒæ—¶å¢å¼ºå¤šæ¨¡æ€ç”Ÿæˆæ€§èƒ½ã€‚","title":"å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹çš„çªç ´æ€§è¿›å±•"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='æ„å»ºå¤šæ¨¡æ€è¯­è¨€æ¨¡å‹é¢ä¸´è®¸å¤šæŒ‘æˆ˜ï¼ŒåŒ…æ‹¬è§†è§‰å’Œè¯­è¨€æ¨¡æ€çš„å¯¹é½ã€é«˜è´¨é‡æŒ‡ä»¤æ•°æ®çš„æ•´ç†ï¼Œä»¥åŠåœ¨å¼•å…¥è§†è§‰åé¿å…æ–‡æœ¬èƒ½åŠ›çš„é€€åŒ–ã€‚åœ¨å¤šè¯­è¨€ç¯å¢ƒä¸­ï¼Œè¿™äº›å›°éš¾æ›´åŠ çªå‡ºï¼Œå› ä¸ºä¸åŒè¯­è¨€çš„å¤šæ¨¡æ€æ•°æ®éœ€æ±‚åŠ å‰§äº†æ•°æ®ç¨€ç¼ºï¼Œæœºå™¨ç¿»è¯‘å¸¸å¸¸æ‰­æ›²æ„ä¹‰ï¼Œå¹¶ä¸”ç¾éš¾æ€§é—å¿˜ç°è±¡æ›´åŠ æ˜æ˜¾ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†æ–°çš„æŠ€æœ¯ï¼ŒåŒ…æ‹¬ä¸€ä¸ªåˆæˆæ³¨é‡Šæ¡†æ¶ï¼Œç”¨äºæ•´ç†é«˜è´¨é‡ã€å¤šæ ·åŒ–çš„å¤šè¯­è¨€å¤šæ¨¡æ€æŒ‡ä»¤æ•°æ®ï¼Œä½¿Aya Visionæ¨¡å‹èƒ½å¤Ÿåœ¨å¤šç§è¯­è¨€ä¸­å¯¹å¤šæ¨¡æ€è¾“å…¥ç”Ÿæˆè‡ªç„¶ã€ç¬¦åˆäººç±»åå¥½çš„å“åº”ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§è·¨æ¨¡æ€æ¨¡å‹åˆå¹¶æŠ€æœ¯ï¼Œæœ‰æ•ˆå‡è½»ç¾éš¾æ€§é—å¿˜ï¼ŒåŒæ—¶å¢å¼ºå¤šæ¨¡æ€ç”Ÿæˆæ€§èƒ½ã€‚', title='å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹çš„çªç ´æ€§è¿›å±•'))
[14.05.2025 14:13] Using data from previous issue: {"categories": ["#transfer_learning", "#agents", "#diffusion", "#robotics", "#dataset"], "emoji": "ğŸ¤–", "ru": {"title": "NavDP: Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ¸Ğ· ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ NavDP - ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ² Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¹ ÑÑ€ĞµĞ´Ğµ, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ¸ÑĞºĞ»ÑÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒ
[14.05.2025 14:13] Using data from previous issue: {"categories": ["#optimization", "#data", "#benchmark", "#healthcare", "#hallucinations", "#rag"], "emoji": "ğŸ”", "ru": {"title": "ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ RAG: Ğ±Ğ°Ğ»Ğ°Ğ½Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ³Ğ¸Ğ¿ĞµÑ€Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ½Ğ° ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ² ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡
[14.05.2025 14:13] Querying the API.
[14.05.2025 14:13] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

We present AM-Thinking-v1, a 32B dense language model that advances the frontier of reasoning, embodying the collaborative spirit of open-source innovation. Outperforming DeepSeek-R1 and rivaling leading Mixture-of-Experts (MoE) models like Qwen3-235B-A22B and Seed1.5-Thinking, AM-Thinking-v1 achieves impressive scores of 85.3 on AIME 2024, 74.4 on AIME 2025, and 70.3 on LiveCodeBench, showcasing state-of-the-art mathematical and coding capabilities among open-source models of similar scale.   Built entirely from the open-source Qwen2.5-32B base model and publicly available queries, AM-Thinking-v1 leverages a meticulously crafted post-training pipeline - combining supervised fine-tuning and reinforcement learning - to deliver exceptional reasoning capabilities. This work demonstrates that the open-source community can achieve high performance at the 32B scale, a practical sweet spot for deployment and fine-tuning. By striking a balance between top-tier performance and real-world usability, we hope AM-Thinking-v1 inspires further collaborative efforts to harness mid-scale models, pushing reasoning boundaries while keeping accessibility at the core of innovation. We have open-sourced our model on https://huggingface.co/a-m-team/AM-Thinking-v1{Hugging Face}.
[14.05.2025 14:13] Response: {
  "desc": "AM-Thinking-v1 - ÑÑ‚Ğ¾ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ 32 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ DeepSeek-R1 Ğ¸ ĞºĞ¾Ğ½ĞºÑƒÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ MoE-Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ Ğ²Ğ¿ĞµÑ‡Ğ°Ñ‚Ğ»ÑÑÑ‰Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ ĞºĞ¾Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ²Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ…. AM-Thinking-v1 Ğ±Ñ‹Ğ»Ğ° ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Qwen2.5-32B Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰ĞµĞ³Ğ¾ supervised fine-tuning Ğ¸ reinforcement learning. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ 32 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ Ğ±Ğ°Ğ»Ğ°Ğ½Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ ÑƒĞ´Ğ¾Ğ±ÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ.",
  "emoji": "ğŸ§ ",
  "title": "ĞÑ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ 32B-Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¿Ğ»Ğ°Ğ½ĞºÑƒ Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ…"
}
[14.05.2025 14:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"We present AM-Thinking-v1, a 32B dense language model that advances the frontier of reasoning, embodying the collaborative spirit of open-source innovation. Outperforming DeepSeek-R1 and rivaling leading Mixture-of-Experts (MoE) models like Qwen3-235B-A22B and Seed1.5-Thinking, AM-Thinking-v1 achieves impressive scores of 85.3 on AIME 2024, 74.4 on AIME 2025, and 70.3 on LiveCodeBench, showcasing state-of-the-art mathematical and coding capabilities among open-source models of similar scale.   Built entirely from the open-source Qwen2.5-32B base model and publicly available queries, AM-Thinking-v1 leverages a meticulously crafted post-training pipeline - combining supervised fine-tuning and reinforcement learning - to deliver exceptional reasoning capabilities. This work demonstrates that the open-source community can achieve high performance at the 32B scale, a practical sweet spot for deployment and fine-tuning. By striking a balance between top-tier performance and real-world usability, we hope AM-Thinking-v1 inspires further collaborative efforts to harness mid-scale models, pushing reasoning boundaries while keeping accessibility at the core of innovation. We have open-sourced our model on https://huggingface.co/a-m-team/AM-Thinking-v1{Hugging Face}."

[14.05.2025 14:13] Response: ```python
['DATASET', 'TRAINING', 'RL']
```
[14.05.2025 14:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"We present AM-Thinking-v1, a 32B dense language model that advances the frontier of reasoning, embodying the collaborative spirit of open-source innovation. Outperforming DeepSeek-R1 and rivaling leading Mixture-of-Experts (MoE) models like Qwen3-235B-A22B and Seed1.5-Thinking, AM-Thinking-v1 achieves impressive scores of 85.3 on AIME 2024, 74.4 on AIME 2025, and 70.3 on LiveCodeBench, showcasing state-of-the-art mathematical and coding capabilities among open-source models of similar scale.   Built entirely from the open-source Qwen2.5-32B base model and publicly available queries, AM-Thinking-v1 leverages a meticulously crafted post-training pipeline - combining supervised fine-tuning and reinforcement learning - to deliver exceptional reasoning capabilities. This work demonstrates that the open-source community can achieve high performance at the 32B scale, a practical sweet spot for deployment and fine-tuning. By striking a balance between top-tier performance and real-world usability, we hope AM-Thinking-v1 inspires further collaborative efforts to harness mid-scale models, pushing reasoning boundaries while keeping accessibility at the core of innovation. We have open-sourced our model on https://huggingface.co/a-m-team/AM-Thinking-v1{Hugging Face}."

[14.05.2025 14:13] Response: ```python
['REASONING', 'OPEN_SOURCE']
```
[14.05.2025 14:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"AM-Thinking-v1 is a 32 billion parameter dense language model that enhances reasoning capabilities, showcasing the power of open-source collaboration. It surpasses previous models like DeepSeek-R1 and competes with advanced Mixture-of-Experts models, achieving high scores on various benchmarks. The model is built on the Qwen2.5-32B base and utilizes a combination of supervised fine-tuning and reinforcement learning in its post-training process. This work highlights the potential of mid-scale models for practical applications, aiming to inspire further innovation in the open-source community.","title":"Unlocking Reasoning with Open-Source Innovation"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='AM-Thinking-v1 is a 32 billion parameter dense language model that enhances reasoning capabilities, showcasing the power of open-source collaboration. It surpasses previous models like DeepSeek-R1 and competes with advanced Mixture-of-Experts models, achieving high scores on various benchmarks. The model is built on the Qwen2.5-32B base and utilizes a combination of supervised fine-tuning and reinforcement learning in its post-training process. This work highlights the potential of mid-scale models for practical applications, aiming to inspire further innovation in the open-source community.', title='Unlocking Reasoning with Open-Source Innovation'))
[14.05.2025 14:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"æˆ‘ä»¬ä»‹ç»äº†AM-Thinking-v1ï¼Œè¿™æ˜¯ä¸€ä¸ª32Bçš„å¯†é›†è¯­è¨€æ¨¡å‹ï¼Œæ¨åŠ¨äº†æ¨ç†çš„å‰æ²¿ï¼Œä½“ç°äº†å¼€æºåˆ›æ–°çš„åˆä½œç²¾ç¥ã€‚å®ƒåœ¨AIME 2024ã€AIME 2025å’ŒLiveCodeBenchç­‰åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¶…è¶Šäº†DeepSeek-R1ï¼Œå¹¶ä¸é¢†å…ˆçš„ä¸“å®¶æ··åˆæ¨¡å‹å¦‚Qwen3-235B-A22Bå’ŒSeed1.5-Thinkingç›¸åª²ç¾ã€‚AM-Thinking-v1å®Œå…¨åŸºäºå¼€æºçš„Qwen2.5-32BåŸºç¡€æ¨¡å‹ï¼Œç»“åˆç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ ï¼Œå±•ç°äº†å“è¶Šçš„æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬çš„å·¥ä½œè¯æ˜äº†å¼€æºç¤¾åŒºå¯ä»¥åœ¨32Bè§„æ¨¡ä¸Šå®ç°é«˜æ€§èƒ½ï¼Œå¹³è¡¡é¡¶çº§æ€§èƒ½ä¸å®é™…å¯ç”¨æ€§ï¼Œæ¿€åŠ±æ›´å¤šåˆä½œåŠªåŠ›ã€‚","title":"å¼€æºåˆ›æ–°ï¼Œæ¨ç†æ–°é«˜åº¦"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='æˆ‘ä»¬ä»‹ç»äº†AM-Thinking-v1ï¼Œè¿™æ˜¯ä¸€ä¸ª32Bçš„å¯†é›†è¯­è¨€æ¨¡å‹ï¼Œæ¨åŠ¨äº†æ¨ç†çš„å‰æ²¿ï¼Œä½“ç°äº†å¼€æºåˆ›æ–°çš„åˆä½œç²¾ç¥ã€‚å®ƒåœ¨AIME 2024ã€AIME 2025å’ŒLiveCodeBenchç­‰åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¶…è¶Šäº†DeepSeek-R1ï¼Œå¹¶ä¸é¢†å…ˆçš„ä¸“å®¶æ··åˆæ¨¡å‹å¦‚Qwen3-235B-A22Bå’ŒSeed1.5-Thinkingç›¸åª²ç¾ã€‚AM-Thinking-v1å®Œå…¨åŸºäºå¼€æºçš„Qwen2.5-32BåŸºç¡€æ¨¡å‹ï¼Œç»“åˆç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ ï¼Œå±•ç°äº†å“è¶Šçš„æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬çš„å·¥ä½œè¯æ˜äº†å¼€æºç¤¾åŒºå¯ä»¥åœ¨32Bè§„æ¨¡ä¸Šå®ç°é«˜æ€§èƒ½ï¼Œå¹³è¡¡é¡¶çº§æ€§èƒ½ä¸å®é™…å¯ç”¨æ€§ï¼Œæ¿€åŠ±æ›´å¤šåˆä½œåŠªåŠ›ã€‚', title='å¼€æºåˆ›æ–°ï¼Œæ¨ç†æ–°é«˜åº¦'))
[14.05.2025 14:13] Using data from previous issue: {"categories": ["#optimization", "#dataset", "#data", "#multilingual", "#low_resource", "#open_source"], "emoji": "ğŸ‡»ğŸ‡³", "ru": {"title": "Ğ˜Ğ˜ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¾Ñ‚Ğ·Ñ‹Ğ²Ğ¾Ğ² Ğ½Ğ° Ğ²ÑŒĞµÑ‚Ğ½Ğ°Ğ¼ÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ViMRHP Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚Ğ·Ñ‹
[14.05.2025 14:13] Using data from previous issue: {"categories": ["#training", "#data", "#translation", "#dataset", "#architecture", "#multilingual", "#low_resource"], "emoji": "ğŸ“š", "ru": {"title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ°Ñ€Ğ°Ğ±ÑĞºĞ¾Ğ¹ Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸ĞºĞµ: Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ñ‹ Ğ½Ğ° ÑĞ»ÑƒĞ¶Ğ±Ğµ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ", "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ
[14.05.2025 14:13] Loading Chinese text from previous data.
[14.05.2025 14:13] Renaming data file.
[14.05.2025 14:13] Renaming previous data. hf_papers.json to ./d/2025-05-14.json
[14.05.2025 14:13] Saving new data file.
[14.05.2025 14:13] Generating page.
[14.05.2025 14:13] Renaming previous page.
[14.05.2025 14:13] Renaming previous data. index.html to ./d/2025-05-14.html
[14.05.2025 14:13] [Experimental] Generating Chinese page for reading.
[14.05.2025 14:13] Chinese vocab [{'word': 'ä»‹ç»', 'pinyin': 'jiÃ¨ shÃ o', 'trans': 'introduce'}, {'word': 'MiniMax-Speech', 'pinyin': 'MiniMax-Speech', 'trans': 'MiniMax-Speech'}, {'word': 'åŸºäº', 'pinyin': 'jÄ« yÃº', 'trans': 'based on'}, {'word': 'Transformer', 'pinyin': 'Transformer', 'trans': 'Transformer'}, {'word': 'æ–‡æœ¬è½¬è¯­éŸ³', 'pinyin': 'wÃ©n bÄ›n zhuÇn yÇ” yÄ«n', 'trans': 'text-to-speech'}, {'word': 'æ¨¡å‹', 'pinyin': 'mÃ³ xÃ­ng', 'trans': 'model'}, {'word': 'ç”Ÿæˆ', 'pinyin': 'shÄ“ng chÃ©ng', 'trans': 'generate'}, {'word': 'é«˜è´¨é‡', 'pinyin': 'gÄo zhÃ¬ liÃ ng', 'trans': 'high quality'}, {'word': 'è¯­éŸ³', 'pinyin': 'yÇ” yÄ«n', 'trans': 'speech'}, {'word': 'å…³é”®', 'pinyin': 'guÇn jiÃ n', 'trans': 'key'}, {'word': 'åˆ›æ–°', 'pinyin': 'chuÃ ng xÄ«n', 'trans': 'innovation'}, {'word': 'å¯å­¦ä¹ ', 'pinyin': 'kÄ› xuÃ© xÃ­', 'trans': 'learnable'}, {'word': 'æ¼”è®²è€…', 'pinyin': 'yÇn jiÇng zhÄ›', 'trans': 'speaker'}, {'word': 'ç¼–ç å™¨', 'pinyin': 'biÄn mÇ qÃ¬', 'trans': 'encoder'}, {'word': 'ä»', 'pinyin': 'cÃ³ng', 'trans': 'from'}, {'word': 'å‚è€ƒ', 'pinyin': 'cÄn kÇo', 'trans': 'reference'}, {'word': 'éŸ³é¢‘', 'pinyin': 'yÄ«n pÃ­n', 'trans': 'audio'}, {'word': 'æå–', 'pinyin': 'tÃ­ qu', 'trans': 'extract'}, {'word': 'éŸ³è‰²', 'pinyin': 'yÄ«n sÃ¨', 'trans': 'timbre'}, {'word': 'ç‰¹å¾', 'pinyin': 'tÃ¨ zhÄ“ng', 'trans': 'features'}, {'word': 'æ— éœ€', 'pinyin': 'wÃº xÅ«', 'trans': 'no need'}, {'word': 'è½¬å½•', 'pinyin': 'zhuÇn lÃ¹', 'trans': 'transcription'}, {'word': 'é›¶æ ·æœ¬', 'pinyin': 'lÃ­ng yÃ ng bÄ›n', 'trans': 'zero-shot'}, {'word': 'æƒ…å†µ', 'pinyin': 'qÃ­ng kuÃ ng', 'trans': 'situation'}, {'word': 'è¡¨ç°åŠ›', 'pinyin': 'biÇo xiÃ n lÃ¬', 'trans': 'expressiveness'}, {'word': 'å¼º', 'pinyin': 'qiÃ¡ng', 'trans': 'strong'}, {'word': 'ä¸€è‡´', 'pinyin': 'yÄ« zhÃ¬', 'trans': 'consistent'}, {'word': 'æ”¯æŒ', 'pinyin': 'zhÄ« chÃ­', 'trans': 'support'}, {'word': 'ä¸€æ ·æœ¬', 'pinyin': 'yÄ« yÃ ng bÄ›n', 'trans': 'one-shot'}, {'word': 'å£°éŸ³', 'pinyin': 'shÄ“ng yÄ«n', 'trans': 'voice'}, {'word': 'å…‹éš†', 'pinyin': 'kÃ¨ lÃ³ng', 'trans': 'clone'}, {'word': 'é€šè¿‡', 'pinyin': 'tÅng guÃ²', 'trans': 'through'}, {'word': 'Flow-VAE', 'pinyin': 'Flow-VAE', 'trans': 'Flow-VAE'}, {'word': 'åˆæˆ', 'pinyin': 'hÃ© chÃ©ng', 'trans': 'synthesis'}, {'word': 'æ•´ä½“', 'pinyin': 'zhÄ›ng tÇ', 'trans': 'overall'}, {'word': 'è´¨é‡', 'pinyin': 'zhÃ¬ liÃ ng', 'trans': 'quality'}, {'word': 'å¾—åˆ°', 'pinyin': 'dÃ© dÃ o', 'trans': 'obtain'}, {'word': 'æå‡', 'pinyin': 'tÃ­ shÄ“ng', 'trans': 'improvement'}, {'word': 'å¤šç§', 'pinyin': 'duÅ zhÇ’ng', 'trans': 'various'}, {'word': 'è¯„ä¼°', 'pinyin': 'pÃ­ng gÅ«', 'trans': 'evaluation'}, {'word': 'æŒ‡æ ‡', 'pinyin': 'zhÇ biÄo', 'trans': 'metrics'}, {'word': 'è¡¨ç°', 'pinyin': 'biÇo xiÃ n', 'trans': 'performance'}, {'word': 'å‡ºè‰²', 'pinyin': 'chÅ« sÃ¨', 'trans': 'outstanding'}, {'word': 'è¾¾åˆ°', 'pinyin': 'dÃ¡ dÃ o', 'trans': 'achieve'}, {'word': 'æœ€å…ˆè¿›', 'pinyin': 'zuÃ¬ xiÄn jÃ¬n', 'trans': 'state-of-the-art'}]
[14.05.2025 14:13] Renaming previous Chinese page.
[14.05.2025 14:13] Renaming previous data. zh.html to ./d/2025-05-13_zh_reading_task.html
[14.05.2025 14:13] Writing Chinese reading task.
[14.05.2025 14:13] Writing result.
[14.05.2025 14:13] Renaming log file.
[14.05.2025 14:13] Renaming previous data. log.txt to ./logs/2025-05-14_last_log.txt
