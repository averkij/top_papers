[14.05.2025 08:16] Read previous papers.
[14.05.2025 08:16] Generating top page (month).
[14.05.2025 08:16] Writing top page (month).
[14.05.2025 09:12] Read previous papers.
[14.05.2025 09:12] Get feed.
[14.05.2025 09:12] Extract page data from URL. URL: https://huggingface.co/papers/2505.07916
[14.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.07591
[14.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.07215
[14.05.2025 09:12] Extract page data from URL. URL: https://huggingface.co/papers/2505.08712
[14.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.08665
[14.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.07416
[14.05.2025 09:12] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[14.05.2025 09:12] No deleted papers detected.
[14.05.2025 09:12] Downloading and parsing papers (pdf, html). Total: 6.
[14.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.07916.
[14.05.2025 09:12] Downloading paper 2505.07916 from http://arxiv.org/pdf/2505.07916v1...
[14.05.2025 09:12] Extracting affiliations from text.
[14.05.2025 09:12] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 2 1 ] . e [ 1 6 1 9 7 0 . 5 0 5 2 : r MiniMax1 We introduce MiniMax-Speech, an autoregressive Transformer-based Text-to-Speech (TTS) model that generates high-quality speech. key innovation is our learnable speaker encoder, which extracts timbre features from reference audio without requiring its transcription. This enables MiniMax-Speech to produce highly expressive speech with timbre consistent with the reference in zero-shot manner, while also supporting one-shot voice cloning with exceptionally high similarity to the reference voice. In addition, the overall quality of the synthesized audio is enhanced through the proposed Flow-VAE. Our model supports 32 languages and demonstrates excellent performance across multiple objective and subjective evaluations metrics. Notably, it achieves state-of-the-art (SOTA) results on objective voice cloning metrics (Word Error Rate and Speaker Similarity) and has secured the top position on the public TTS Arena leaderboard. Another key strength of MiniMax-Speech, granted by the robust and disentangled representations from the speaker encoder, is its extensibility without modifying the base model, enabling various applications such as: arbitrary voice emotion control via LoRA; text to voice (T2V) by synthesizing timbre features directly from text description; and professional voice cloning (PVC) by fine-tuning timbre features with additional data. We encourage readers to visit https://minimax-ai.github.io/tts_tech_report for more examples. 1. Introduction The advent of codec-based models has catalyzed significant advancements in TTS. These models, when trained on large-scale datasets, demonstrate the remarkable capability to generate high-quality speech from only few seconds of reference audio. This proficiency has fostered their widespread adoption across diverse applications, including conversational AI, audio content creation for blogs, interactive voice assistants, and immersive e-book narration. Two primary modeling"
[14.05.2025 09:12] Response: []
[14.05.2025 09:12] Extracting affiliations from text.
[14.05.2025 09:12] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 2 1 ] . e [ 1 6 1 9 7 0 . 5 0 5 2 : r MiniMax1 We introduce MiniMax-Speech, an autoregressive Transformer-based Text-to-Speech (TTS) model that generates high-quality speech. key innovation is our learnable speaker encoder, which extracts timbre features from reference audio without requiring its transcription. This enables MiniMax-Speech to produce highly expressive speech with timbre consistent with the reference in zero-shot manner, while also supporting one-shot voice cloning with exceptionally high similarity to the reference voice. In addition, the overall quality of the synthesized audio is enhanced through the proposed Flow-VAE. Our model supports 32 languages and demonstrates excellent performance across multiple objective and subjective evaluations metrics. Notably, it achieves state-of-the-art (SOTA) results on objective voice cloning metrics (Word Error Rate and Speaker Similarity) and has secured the top position on the public TTS Arena leaderboard. Another key strength of MiniMax-Speech, granted by the robust and disentangled representations from the speaker encoder, is its extensibility without modifying the base model, enabling various applications such as: arbitrary voice emotion control via LoRA; text to voice (T2V) by synthesizing timbre features directly from text description; and professional voice cloning (PVC) by fine-tuning timbre features with additional data. We encourage readers to visit https://minimax-ai.github.io/tts_tech_report for more examples. 1. Introduction The advent of codec-based models has catalyzed significant advancements in TTS. These models, when trained on large-scale datasets, demonstrate the remarkable capability to generate high-quality speech from only few seconds of reference audio. This proficiency has fostered their widespread adoption across diverse applications, including conversational AI, audio content creation for blogs, interactive voice assistants, and immersive e-book narration. Two primary modeling methodologies are prevalent in large TTS models: autoregressive (AR) language models and non-autoregressive (NAR) diffusion models. NAR diffusion models have gained attention for their rapid inference capabilities. However, such NAR models often employ duration modeling techniques. One approach is explicit phoneme-duration alignment (Gao et al., 2023a; Ju et al., 2024; Le et al., 2023; Mehta et al., 2024), which can constrain naturalness and diversity. An alternative, global duration modeling (Anastassiou et al., 2024; Chen et al., 2024; Eskimez et al., 2024; Jiang et al., 2025; Lee et al., 2024; Wang et al., 2024; Yang et al., 2024), involves the model learning implicit alignments; this, however, can increase modeling complexity and reduce robustness in challenging cases. Conversely, AR models are renowned for their potent modeling capacity, an inherent strength that allows them to generate speech exhibiting superior prosody, intonation, and 1Please send correspondence to model@minimax.io. 2025 MiniMax. All rights reserved MiniMax-Speech: Intrinsic Zero-Shot Text-to-Speech with Learnable Speaker Encoder overall naturalness (Borsos et al., 2023; Casanova et al., 2024; Guo et al., 2024; Wang et al., 2023a, 2025). Most previous AR TTS models (Du et al., 2024b; Wang et al., 2023a) have required both speech and transcription as prompts in voice cloning, methodology categorized as one-shot learning. However, semantic or linguistic mismatches between prompt and target speech, compounded by decoding length limitations, often result in suboptimal generation quality. MiniMax-Speech distinguishes itself by integrating speaker encoder into its AR model. This key feature, based on concepts explored in works like (Betker, 2023), enables zero-shot voice cloning using only speech prompt. This approach eliminates the dependency on reference transcription, thus inherently supporting cross-lingual and multilingual synthesis and avoiding issues from text-speech mismatches. By conditioning solely on vocal characteristics, it allows for wider, more flexible decoding space for prosody and style during generation, leading to richer and more varied outputs. Notably, even in one-shot scenarios where reference transcription is available, this integrated speaker encoder contributes to superior speaker similarity in the synthesized speech. The strategy employed by MiniMax-Speech thus effectively mitigates these common issues and offers greater flexibility for various extensions, such as T2V and PVC. Another technical aspect warranting discussion is the inherent learnability of the speaker encoder we adopted (Betker, 2023; Casanova et al., 2024). Some models often utilize modules pre-trained on Speaker Verification (SV) tasks as their fixed speaker encoders (Du et al., 2024a). However, the training data and optimization objectives inherent to SV tasks can differ from the specific requirements of the TTS task itself. In contrast, our learnable approach involves jointly training the speaker encoder with the AR model. This end-to-end optimization strategy allows the speaker encoder to be better tailored to the demands of the TTS task. Subsequent experimental results compellingly demonstrate that employing learnable speaker encoder yields superior performance in terms of both speaker similarity and intelligibility of the synthesized speech. Flow matching models (Lipman et al., 2023; Tong et al., 2024), when utilized as decoders, are capable of producing high-fidelity speech outputs (Du et al., 2024a,b). prevalent paradigm involves flow matching models first predicting mel-spectrogram, which is subsequently converted to an audio waveform by vocoder. However, the mel-spectrogram can act as an information bottleneck, inherently limiting the ultimate achievable speech quality. In contrast, VAE, benefiting from end-to-end training, demonstrates stronger representation learning capability. Employing VAE-derived representations as the modeling objective for flow matching systems subsequently improves speech quality (Hung et al., 2024; Kim et al., 2021; Tan et al., 2024). Building upon this VAE foundation, MiniMax-Speech innovatively introduces Flow-VAE. This hybrid approach integrates VAE and flow models to enhance the information representation power of the VAE encoder, thereby further improving both audio quality and speaker similarity. We summarize our contributions as follows: 1. We present MiniMax-Speech, TTS model that supports 32 languages and gene"
[14.05.2025 09:12] Mistral response. {"id": "f8f689c76ae94dd0813fd1d8539f5c1d", "object": "chat.completion", "created": 1747213977, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "[]"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1644, "total_tokens": 1646, "completion_tokens": 2}}
[14.05.2025 09:12] Response: []
[14.05.2025 09:12] Deleting PDF ./assets/pdf/2505.07916.pdf.
[14.05.2025 09:12] Success.
[14.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.07591.
[14.05.2025 09:12] Extra JSON file exists (./assets/json/2505.07591.json), skip PDF parsing.
[14.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.07591.json), skip HTML parsing.
[14.05.2025 09:12] Success.
[14.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.07215.
[14.05.2025 09:12] Extra JSON file exists (./assets/json/2505.07215.json), skip PDF parsing.
[14.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.07215.json), skip HTML parsing.
[14.05.2025 09:12] Success.
[14.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.08712.
[14.05.2025 09:12] Downloading paper 2505.08712 from http://arxiv.org/pdf/2505.08712v1...
[14.05.2025 09:13] Extracting affiliations from text.
[14.05.2025 09:13] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 3 1 ] . [ 1 2 1 7 8 0 . 5 0 5 2 : r NavDP: Learning Sim-to-Real Navigation Diffusion Policy with Privileged Information Guidance Wenzhe Cai1, Jiaqi Peng1,2, Yuqiang Yang1, Yujian Zhang3, Meng Wei1,4, Hanqing Wang1, Yilun Chen1, Tai Wang1,, Jiangmiao Pang1, 1Shanghai AI Lab 2Tsinghua University 3Zhejiang University 4The University of Hong Kong Corresponding Authors Abstract: Learning navigation in dynamic open-world environments is an important yet challenging skill for robots. Most previous methods rely on precise localization and mapping or learn from expensive real-world demonstrations. In this paper, we propose the Navigation Diffusion Policy (NavDP), an end-to-end framework trained solely in simulation and can zero-shot transfer to different embodiments in diverse real-world environments. The key ingredient of NavDPs network is the combination of diffusion-based trajectory generation and critic function for trajectory selection, which are conditioned on only local observation tokens encoded from shared policy transformer. Given the privileged information of the global environment in simulation, we scale up the demonstrations of good quality to train the diffusion policy and formulate the critic value function targets with contrastive negative samples. Our demonstration generation approach achieves about 2,500 trajectories/GPU per day, 20 more efficient than real-world data collection, and results in large-scale navigation dataset with 363.2km trajectories across 1244 scenes. Trained with this simulation dataset, NavDP achieves state-of-the-art performance and consistently outstanding generalization capability on quadruped, wheeled, and humanoid robots in diverse indoor and outdoor environments. In addition, we present preliminary attempt at using Gaussian Splatting to make in-domain real-to-sim fine-tuning to further bridge the sim-toreal gap. Experiments show that adding such real-to-sim data can improve the success rate by 30% without hurting its gener"
[14.05.2025 09:13] Response: ```python
["Shanghai AI Lab", "Tsinghua University", "Zhejiang University", "The University of Hong Kong"]
```
[14.05.2025 09:13] Deleting PDF ./assets/pdf/2505.08712.pdf.
[14.05.2025 09:13] Success.
[14.05.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2505.08665.
[14.05.2025 09:13] Extra JSON file exists (./assets/json/2505.08665.json), skip PDF parsing.
[14.05.2025 09:13] Paper image links file exists (./assets/img_data/2505.08665.json), skip HTML parsing.
[14.05.2025 09:13] Success.
[14.05.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2505.07416.
[14.05.2025 09:13] Extra JSON file exists (./assets/json/2505.07416.json), skip PDF parsing.
[14.05.2025 09:13] Paper image links file exists (./assets/img_data/2505.07416.json), skip HTML parsing.
[14.05.2025 09:13] Success.
[14.05.2025 09:13] Enriching papers with extra data.
[14.05.2025 09:13] ********************************************************************************
[14.05.2025 09:13] Abstract 0. We introduce MiniMax-Speech, an autoregressive Transformer-based Text-to-Speech (TTS) model that generates high-quality speech. A key innovation is our learnable speaker encoder, which extracts timbre features from a reference audio without requiring its transcription. This enables MiniMax-Speech to...
[14.05.2025 09:13] ********************************************************************************
[14.05.2025 09:13] Abstract 1. Instruction following evaluates large language models (LLMs) on their ability to generate outputs that adhere to user-defined constraints. However, existing benchmarks often rely on templated constraint prompts, which lack the diversity of real-world usage and limit fine-grained performance assessme...
[14.05.2025 09:13] ********************************************************************************
[14.05.2025 09:13] Abstract 2. We present gg-bench, a collection of game environments designed to evaluate general reasoning capabilities in language models. Unlike most static benchmarks, gg-bench is a data generating process where new evaluation instances can be generated at will. In particular, gg-bench is synthetically genera...
[14.05.2025 09:13] ********************************************************************************
[14.05.2025 09:13] Abstract 3. Learning navigation in dynamic open-world environments is an important yet challenging skill for robots. Most previous methods rely on precise localization and mapping or learn from expensive real-world demonstrations. In this paper, we propose the Navigation Diffusion Policy (NavDP), an end-to-end ...
[14.05.2025 09:13] ********************************************************************************
[14.05.2025 09:13] Abstract 4. Assessing human skill levels in complex activities is a challenging problem with applications in sports, rehabilitation, and training. In this work, we present SkillFormer, a parameter-efficient architecture for unified multi-view proficiency estimation from egocentric and exocentric videos. Buildin...
[14.05.2025 09:13] ********************************************************************************
[14.05.2025 09:13] Abstract 5. Multimodal Review Helpfulness Prediction (MRHP) is an essential task in recommender systems, particularly in E-commerce platforms. Determining the helpfulness of user-generated reviews enhances user experience and improves consumer decision-making. However, existing datasets focus predominantly on E...
[14.05.2025 09:13] Read previous papers.
[14.05.2025 09:13] Generating reviews via LLM API.
[14.05.2025 09:13] Querying the API.
[14.05.2025 09:13] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

We introduce MiniMax-Speech, an autoregressive Transformer-based Text-to-Speech (TTS) model that generates high-quality speech. A key innovation is our learnable speaker encoder, which extracts timbre features from a reference audio without requiring its transcription. This enables MiniMax-Speech to produce highly expressive speech with timbre consistent with the reference in a zero-shot manner, while also supporting one-shot voice cloning with exceptionally high similarity to the reference voice. In addition, the overall quality of the synthesized audio is enhanced through the proposed Flow-VAE. Our model supports 32 languages and demonstrates excellent performance across multiple objective and subjective evaluations metrics. Notably, it achieves state-of-the-art (SOTA) results on objective voice cloning metrics (Word Error Rate and Speaker Similarity) and has secured the top position on the public TTS Arena leaderboard. Another key strength of MiniMax-Speech, granted by the robust and disentangled representations from the speaker encoder, is its extensibility without modifying the base model, enabling various applications such as: arbitrary voice emotion control via LoRA; text to voice (T2V) by synthesizing timbre features directly from text description; and professional voice cloning (PVC) by fine-tuning timbre features with additional data. We encourage readers to visit https://minimax-ai.github.io/tts_tech_report for more examples.
[14.05.2025 09:13] Response: {
  "desc": "MiniMax-Speech - ÑÑ‚Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ñ€ĞµÑ‡Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¢Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸ĞµĞ¹ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ğ¹ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸Ğº Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰ĞµĞ³Ğ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ Ñ‚ĞµĞ¼Ğ±Ñ€Ğ° Ğ¸Ğ· ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ ĞµĞ³Ğ¾ Ñ‚Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ¿Ñ†Ğ¸Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ 32 ÑĞ·Ñ‹ĞºĞ° Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¸Ğ¼ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ² ĞºĞ»Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ³Ğ¾Ğ»Ğ¾ÑĞ°. MiniMax-Speech Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ğ±Ğ»Ğ°Ğ´Ğ°ĞµÑ‚ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµĞ¼Ğ¾ÑÑ‚ÑŒÑ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰ĞµĞ¹ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ÑĞ¼Ğ¾Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ³Ğ¾Ğ»Ğ¾ÑĞ° Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ñ‚ĞµĞ¼Ğ±Ñ€Ğ° Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ.",
  "emoji": "ğŸ—£ï¸",
  "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğµ Ñ€ĞµÑ‡Ğ¸: MiniMax-Speech - ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğ²Ğ¾Ğ¹ ĞºĞ»Ğ¾Ğ½"
}
[14.05.2025 09:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"We introduce MiniMax-Speech, an autoregressive Transformer-based Text-to-Speech (TTS) model that generates high-quality speech. A key innovation is our learnable speaker encoder, which extracts timbre features from a reference audio without requiring its transcription. This enables MiniMax-Speech to produce highly expressive speech with timbre consistent with the reference in a zero-shot manner, while also supporting one-shot voice cloning with exceptionally high similarity to the reference voice. In addition, the overall quality of the synthesized audio is enhanced through the proposed Flow-VAE. Our model supports 32 languages and demonstrates excellent performance across multiple objective and subjective evaluations metrics. Notably, it achieves state-of-the-art (SOTA) results on objective voice cloning metrics (Word Error Rate and Speaker Similarity) and has secured the top position on the public TTS Arena leaderboard. Another key strength of MiniMax-Speech, granted by the robust and disentangled representations from the speaker encoder, is its extensibility without modifying the base model, enabling various applications such as: arbitrary voice emotion control via LoRA; text to voice (T2V) by synthesizing timbre features directly from text description; and professional voice cloning (PVC) by fine-tuning timbre features with additional data. We encourage readers to visit https://minimax-ai.github.io/tts_tech_report for more examples."

[14.05.2025 09:13] Response: ```python
['AUDIO', 'MULTILINGUAL']
```
[14.05.2025 09:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"We introduce MiniMax-Speech, an autoregressive Transformer-based Text-to-Speech (TTS) model that generates high-quality speech. A key innovation is our learnable speaker encoder, which extracts timbre features from a reference audio without requiring its transcription. This enables MiniMax-Speech to produce highly expressive speech with timbre consistent with the reference in a zero-shot manner, while also supporting one-shot voice cloning with exceptionally high similarity to the reference voice. In addition, the overall quality of the synthesized audio is enhanced through the proposed Flow-VAE. Our model supports 32 languages and demonstrates excellent performance across multiple objective and subjective evaluations metrics. Notably, it achieves state-of-the-art (SOTA) results on objective voice cloning metrics (Word Error Rate and Speaker Similarity) and has secured the top position on the public TTS Arena leaderboard. Another key strength of MiniMax-Speech, granted by the robust and disentangled representations from the speaker encoder, is its extensibility without modifying the base model, enabling various applications such as: arbitrary voice emotion control via LoRA; text to voice (T2V) by synthesizing timbre features directly from text description; and professional voice cloning (PVC) by fine-tuning timbre features with additional data. We encourage readers to visit https://minimax-ai.github.io/tts_tech_report for more examples."

[14.05.2025 09:13] Response: ```python
["GAMES", "OPTIMIZATION"]
```
[14.05.2025 09:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"MiniMax-Speech is a cutting-edge Text-to-Speech (TTS) model that utilizes an autoregressive Transformer architecture to generate high-quality speech. A significant feature of this model is its learnable speaker encoder, which captures timbre characteristics from audio samples without needing their text transcriptions. This allows the model to create expressive speech that matches the timbre of the reference audio in a zero-shot manner, and it can also perform one-shot voice cloning with remarkable accuracy. Additionally, MiniMax-Speech enhances audio quality through a Flow-VAE and supports multiple languages, achieving state-of-the-art results in voice cloning metrics and demonstrating versatility for various applications.","title":"Revolutionizing Speech Synthesis with MiniMax-Speech"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='MiniMax-Speech is a cutting-edge Text-to-Speech (TTS) model that utilizes an autoregressive Transformer architecture to generate high-quality speech. A significant feature of this model is its learnable speaker encoder, which captures timbre characteristics from audio samples without needing their text transcriptions. This allows the model to create expressive speech that matches the timbre of the reference audio in a zero-shot manner, and it can also perform one-shot voice cloning with remarkable accuracy. Additionally, MiniMax-Speech enhances audio quality through a Flow-VAE and supports multiple languages, achieving state-of-the-art results in voice cloning metrics and demonstrating versatility for various applications.', title='Revolutionizing Speech Synthesis with MiniMax-Speech'))
[14.05.2025 09:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"MiniMax-Speechæ˜¯ä¸€ç§åŸºäºè‡ªå›å½’Transformerçš„æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å‹ï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„è¯­éŸ³ã€‚å…¶åˆ›æ–°ä¹‹å¤„åœ¨äºå¯å­¦ä¹ çš„è¯´è¯äººç¼–ç å™¨ï¼Œå¯ä»¥ä»å‚è€ƒéŸ³é¢‘ä¸­æå–éŸ³è‰²ç‰¹å¾ï¼Œè€Œæ— éœ€å…¶è½¬å½•ã€‚è¯¥æ¨¡å‹æ”¯æŒé›¶æ ·æœ¬ç”Ÿæˆå…·æœ‰å‚è€ƒéŸ³è‰²çš„è¡¨è¾¾æ€§è¯­éŸ³ï¼Œå¹¶ä¸”åœ¨å•æ ·æœ¬è¯­éŸ³å…‹éš†ä¸­è¡¨ç°å‡ºæé«˜çš„ç›¸ä¼¼åº¦ã€‚æ­¤å¤–ï¼ŒMiniMax-Speechåœ¨å¤šä¸ªå®¢è§‚å’Œä¸»è§‚è¯„ä¼°æŒ‡æ ‡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œæ”¯æŒ32ç§è¯­è¨€ï¼Œå¹¶åœ¨è¯­éŸ³å…‹éš†çš„å®¢è§‚æŒ‡æ ‡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ã€‚","title":"MiniMax-Speechï¼šé«˜è´¨é‡è¯­éŸ³ç”Ÿæˆçš„æ–°çªç ´"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='MiniMax-Speechæ˜¯ä¸€ç§åŸºäºè‡ªå›å½’Transformerçš„æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å‹ï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„è¯­éŸ³ã€‚å…¶åˆ›æ–°ä¹‹å¤„åœ¨äºå¯å­¦ä¹ çš„è¯´è¯äººç¼–ç å™¨ï¼Œå¯ä»¥ä»å‚è€ƒéŸ³é¢‘ä¸­æå–éŸ³è‰²ç‰¹å¾ï¼Œè€Œæ— éœ€å…¶è½¬å½•ã€‚è¯¥æ¨¡å‹æ”¯æŒé›¶æ ·æœ¬ç”Ÿæˆå…·æœ‰å‚è€ƒéŸ³è‰²çš„è¡¨è¾¾æ€§è¯­éŸ³ï¼Œå¹¶ä¸”åœ¨å•æ ·æœ¬è¯­éŸ³å…‹éš†ä¸­è¡¨ç°å‡ºæé«˜çš„ç›¸ä¼¼åº¦ã€‚æ­¤å¤–ï¼ŒMiniMax-Speechåœ¨å¤šä¸ªå®¢è§‚å’Œä¸»è§‚è¯„ä¼°æŒ‡æ ‡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œæ”¯æŒ32ç§è¯­è¨€ï¼Œå¹¶åœ¨è¯­éŸ³å…‹éš†çš„å®¢è§‚æŒ‡æ ‡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ã€‚', title='MiniMax-Speechï¼šé«˜è´¨é‡è¯­éŸ³ç”Ÿæˆçš„æ–°çªç ´'))
[14.05.2025 09:13] Using data from previous issue: {"categories": ["#training", "#rl", "#alignment", "#optimization", "#benchmark"], "emoji": "ğŸ¤–", "ru": {"title": "ĞœĞ½Ğ¾Ğ³Ğ¾Ğ¼ĞµÑ€Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ†ĞµĞ½ĞºĞµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ
[14.05.2025 09:13] Using data from previous issue: {"categories": ["#benchmark", "#dataset", "#reasoning", "#rl", "#games", "#synthetic"], "emoji": "ğŸ®", "ru": {"title": "gg-bench: ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ†ĞµĞ½ĞºĞµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ³Ñ€Ğ¾Ğ²Ñ‹Ğµ ÑÑ€ĞµĞ´Ñ‹", "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ gg-bench - Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¸Ğ³Ñ€Ğ¾Ğ²Ñ‹Ñ… ÑÑ€ĞµĞ´ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğµ
[14.05.2025 09:13] Querying the API.
[14.05.2025 09:13] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Learning navigation in dynamic open-world environments is an important yet challenging skill for robots. Most previous methods rely on precise localization and mapping or learn from expensive real-world demonstrations. In this paper, we propose the Navigation Diffusion Policy (NavDP), an end-to-end framework trained solely in simulation and can zero-shot transfer to different embodiments in diverse real-world environments. The key ingredient of NavDP's network is the combination of diffusion-based trajectory generation and a critic function for trajectory selection, which are conditioned on only local observation tokens encoded from a shared policy transformer. Given the privileged information of the global environment in simulation, we scale up the demonstrations of good quality to train the diffusion policy and formulate the critic value function targets with contrastive negative samples. Our demonstration generation approach achieves about 2,500 trajectories/GPU per day, 20times more efficient than real-world data collection, and results in a large-scale navigation dataset with 363.2km trajectories across 1244 scenes. Trained with this simulation dataset, NavDP achieves state-of-the-art performance and consistently outstanding generalization capability on quadruped, wheeled, and humanoid robots in diverse indoor and outdoor environments. In addition, we present a preliminary attempt at using Gaussian Splatting to make in-domain real-to-sim fine-tuning to further bridge the sim-to-real gap. Experiments show that adding such real-to-sim data can improve the success rate by 30\% without hurting its generalization capability.
[14.05.2025 09:13] Response: {
  "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ NavDP - ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ² Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¹ ÑÑ€ĞµĞ´Ğµ, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ¸ÑĞºĞ»ÑÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ½Ğ° ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸ÑÑ…. NavDP Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ° Ğ´Ğ»Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ĞµĞ¹ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚Ğ¸Ğ¿Ğ°Ğ¼Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¼Ğ¸Ñ€Ğµ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ² ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ĞµĞµ ÑĞ±Ğ¾Ñ€Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ² Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑÑ€ĞµĞ´Ğ°Ñ….",
  "emoji": "ğŸ¤–",
  "title": "NavDP: Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ¸Ğ· ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ"
}
[14.05.2025 09:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Learning navigation in dynamic open-world environments is an important yet challenging skill for robots. Most previous methods rely on precise localization and mapping or learn from expensive real-world demonstrations. In this paper, we propose the Navigation Diffusion Policy (NavDP), an end-to-end framework trained solely in simulation and can zero-shot transfer to different embodiments in diverse real-world environments. The key ingredient of NavDP's network is the combination of diffusion-based trajectory generation and a critic function for trajectory selection, which are conditioned on only local observation tokens encoded from a shared policy transformer. Given the privileged information of the global environment in simulation, we scale up the demonstrations of good quality to train the diffusion policy and formulate the critic value function targets with contrastive negative samples. Our demonstration generation approach achieves about 2,500 trajectories/GPU per day, 20times more efficient than real-world data collection, and results in a large-scale navigation dataset with 363.2km trajectories across 1244 scenes. Trained with this simulation dataset, NavDP achieves state-of-the-art performance and consistently outstanding generalization capability on quadruped, wheeled, and humanoid robots in diverse indoor and outdoor environments. In addition, we present a preliminary attempt at using Gaussian Splatting to make in-domain real-to-sim fine-tuning to further bridge the sim-to-real gap. Experiments show that adding such real-to-sim data can improve the success rate by 30\% without hurting its generalization capability."

[14.05.2025 09:13] Response: ```python
['AGENTS', 'ROBOTICS', 'DATASET']
```
[14.05.2025 09:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Learning navigation in dynamic open-world environments is an important yet challenging skill for robots. Most previous methods rely on precise localization and mapping or learn from expensive real-world demonstrations. In this paper, we propose the Navigation Diffusion Policy (NavDP), an end-to-end framework trained solely in simulation and can zero-shot transfer to different embodiments in diverse real-world environments. The key ingredient of NavDP's network is the combination of diffusion-based trajectory generation and a critic function for trajectory selection, which are conditioned on only local observation tokens encoded from a shared policy transformer. Given the privileged information of the global environment in simulation, we scale up the demonstrations of good quality to train the diffusion policy and formulate the critic value function targets with contrastive negative samples. Our demonstration generation approach achieves about 2,500 trajectories/GPU per day, 20times more efficient than real-world data collection, and results in a large-scale navigation dataset with 363.2km trajectories across 1244 scenes. Trained with this simulation dataset, NavDP achieves state-of-the-art performance and consistently outstanding generalization capability on quadruped, wheeled, and humanoid robots in diverse indoor and outdoor environments. In addition, we present a preliminary attempt at using Gaussian Splatting to make in-domain real-to-sim fine-tuning to further bridge the sim-to-real gap. Experiments show that adding such real-to-sim data can improve the success rate by 30\% without hurting its generalization capability."

[14.05.2025 09:13] Response: ```python
['DIFFUSION', 'TRANSFER_LEARNING']
```
[14.05.2025 09:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces the Navigation Diffusion Policy (NavDP), a novel framework for robot navigation in dynamic environments. Unlike traditional methods that depend on accurate mapping or costly real-world training, NavDP is trained entirely in simulation and can adapt to various robot types in real-world settings. The framework utilizes diffusion-based trajectory generation combined with a critic function to select optimal paths based on local observations. By generating a large dataset of simulated navigation trajectories efficiently, NavDP demonstrates superior performance and generalization across different robotic embodiments and environments.","title":"Efficient Robot Navigation with Simulation-Driven Learning"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper introduces the Navigation Diffusion Policy (NavDP), a novel framework for robot navigation in dynamic environments. Unlike traditional methods that depend on accurate mapping or costly real-world training, NavDP is trained entirely in simulation and can adapt to various robot types in real-world settings. The framework utilizes diffusion-based trajectory generation combined with a critic function to select optimal paths based on local observations. By generating a large dataset of simulated navigation trajectories efficiently, NavDP demonstrates superior performance and generalization across different robotic embodiments and environments.', title='Efficient Robot Navigation with Simulation-Driven Learning'))
[14.05.2025 09:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºå¯¼èˆªæ‰©æ•£ç­–ç•¥ï¼ˆNavDPï¼‰çš„æ–°æ¡†æ¶ï¼Œæ—¨åœ¨å¸®åŠ©æœºå™¨äººåœ¨åŠ¨æ€å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­è¿›è¡Œå¯¼èˆªã€‚è¯¥æ–¹æ³•å®Œå…¨åœ¨æ¨¡æ‹Ÿç¯å¢ƒä¸­è®­ç»ƒï¼Œèƒ½å¤Ÿé›¶-shotè¿ç§»åˆ°ä¸åŒçš„å®é™…åº”ç”¨ä¸­ã€‚NavDPç»“åˆäº†åŸºäºæ‰©æ•£çš„è½¨è¿¹ç”Ÿæˆå’Œç”¨äºè½¨è¿¹é€‰æ‹©çš„è¯„ä»·å‡½æ•°ï¼Œåˆ©ç”¨å…±äº«ç­–ç•¥å˜æ¢å™¨ç¼–ç çš„å±€éƒ¨è§‚å¯Ÿä¿¡æ¯è¿›è¡Œæ¡ä»¶åŒ–ã€‚é€šè¿‡åœ¨æ¨¡æ‹Ÿä¸­è·å–å…¨çƒç¯å¢ƒçš„ç‰¹æƒä¿¡æ¯ï¼Œæˆ‘ä»¬ç”Ÿæˆäº†é«˜è´¨é‡çš„æ¼”ç¤ºæ•°æ®ï¼Œå¹¶åœ¨å¤šç§å®¤å†…å¤–ç¯å¢ƒä¸­å®ç°äº†æœ€å…ˆè¿›çš„å¯¼èˆªæ€§èƒ½å’Œå‡ºè‰²çš„æ³›åŒ–èƒ½åŠ›ã€‚","title":"æœºå™¨äººå¯¼èˆªçš„æ–°çªç ´ï¼šå¯¼èˆªæ‰©æ•£ç­–ç•¥"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºå¯¼èˆªæ‰©æ•£ç­–ç•¥ï¼ˆNavDPï¼‰çš„æ–°æ¡†æ¶ï¼Œæ—¨åœ¨å¸®åŠ©æœºå™¨äººåœ¨åŠ¨æ€å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­è¿›è¡Œå¯¼èˆªã€‚è¯¥æ–¹æ³•å®Œå…¨åœ¨æ¨¡æ‹Ÿç¯å¢ƒä¸­è®­ç»ƒï¼Œèƒ½å¤Ÿé›¶-shotè¿ç§»åˆ°ä¸åŒçš„å®é™…åº”ç”¨ä¸­ã€‚NavDPç»“åˆäº†åŸºäºæ‰©æ•£çš„è½¨è¿¹ç”Ÿæˆå’Œç”¨äºè½¨è¿¹é€‰æ‹©çš„è¯„ä»·å‡½æ•°ï¼Œåˆ©ç”¨å…±äº«ç­–ç•¥å˜æ¢å™¨ç¼–ç çš„å±€éƒ¨è§‚å¯Ÿä¿¡æ¯è¿›è¡Œæ¡ä»¶åŒ–ã€‚é€šè¿‡åœ¨æ¨¡æ‹Ÿä¸­è·å–å…¨çƒç¯å¢ƒçš„ç‰¹æƒä¿¡æ¯ï¼Œæˆ‘ä»¬ç”Ÿæˆäº†é«˜è´¨é‡çš„æ¼”ç¤ºæ•°æ®ï¼Œå¹¶åœ¨å¤šç§å®¤å†…å¤–ç¯å¢ƒä¸­å®ç°äº†æœ€å…ˆè¿›çš„å¯¼èˆªæ€§èƒ½å’Œå‡ºè‰²çš„æ³›åŒ–èƒ½åŠ›ã€‚', title='æœºå™¨äººå¯¼èˆªçš„æ–°çªç ´ï¼šå¯¼èˆªæ‰©æ•£ç­–ç•¥'))
[14.05.2025 09:13] Using data from previous issue: {"categories": ["#architecture", "#dataset", "#training"], "emoji": "ğŸ¥", "ru": {"title": "SkillFormer: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ¼Ğ°ÑÑ‚ĞµÑ€ÑÑ‚Ğ²Ğ° Ğ¿Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ğ¾Ğ¼Ñƒ Ğ²Ğ¸Ğ´ĞµĞ¾", "desc": "SkillFormer - ÑÑ‚Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¼Ğ°ÑÑ‚ĞµÑ€ÑÑ‚Ğ²Ğ° Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑÑ… Ğ¿Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ². ĞĞ½Ğ° Ğ¸ÑĞ¿
[14.05.2025 09:13] Using data from previous issue: {"categories": ["#optimization", "#dataset", "#data", "#multilingual", "#low_resource", "#open_source"], "emoji": "ğŸ‡»ğŸ‡³", "ru": {"title": "Ğ˜Ğ˜ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¾Ñ‚Ğ·Ñ‹Ğ²Ğ¾Ğ² Ğ½Ğ° Ğ²ÑŒĞµÑ‚Ğ½Ğ°Ğ¼ÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ViMRHP Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚Ğ·Ñ‹
[14.05.2025 09:13] Trying to get texts in Chinese.
[14.05.2025 09:13] Mistral request. Model: mistral-large-latest. Prompt: Write simple and brief explanation (4-5 sentences) of an article in Chinese. Use short sentences. Text:

We introduce MiniMax-Speech, an autoregressive Transformer-based Text-to-Speech (TTS) model that generates high-quality speech. A key innovation is our learnable speaker encoder, which extracts timbre features from a reference audio without requiring its transcription. This enables MiniMax-Speech to produce highly expressive speech with timbre consistent with the reference in a zero-shot manner, while also supporting one-shot voice cloning with exceptionally high similarity to the reference voice. In addition, the overall quality of the synthesized audio is enhanced through the proposed Flow-VAE. Our model supports 32 languages and demonstrates excellent performance across multiple objective and subjective evaluations metrics. Notably, it achieves state-of-the-art (SOTA) results on objective voice cloning metrics (Word Error Rate and Speaker Similarity) and has secured the top position on the public TTS Arena leaderboard. Another key strength of MiniMax-Speech, granted by the robust and disentangled representations from the speaker encoder, is its extensibility without modifying the base model, enabling various applications such as: arbitrary voice emotion control via LoRA; text to voice (T2V) by synthesizing timbre features directly from text description; and professional voice cloning (PVC) by fine-tuning timbre features with additional data. We encourage readers to visit https://minimax-ai.github.io/tts_tech_report for more examples.
[14.05.2025 09:13] Mistral response. {"id": "bfa91fa6e0cc4541987984ab1c5eead1", "object": "chat.completion", "created": 1747214011, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "\u6211\u4eec\u4ecb\u7ecd\u4e86MiniMax-Speech\uff0c\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u6587\u672c\u8f6c\u8bed\u97f3\u6a21\u578b\u3002\u5b83\u80fd\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u8bed\u97f3\u3002\u6a21\u578b\u7684\u5173\u952e\u521b\u65b0\u662f\u53ef\u5b66\u4e60\u7684\u6f14\u8bb2\u8005\u7f16\u7801\u5668\uff0c\u4ece\u53c2\u8003\u97f3\u9891\u4e2d\u63d0\u53d6\u97f3\u8272\u7279\u5f81\uff0c\u65e0\u9700\u8f6c\u5f55\u3002\u8fd9\u4f7f\u5f97MiniMax-Speech\u80fd\u5728\u96f6\u6837\u672c\u60c5\u51b5\u4e0b\u751f\u6210\u8868\u73b0\u529b\u5f3a\u3001\u97f3\u8272\u4e00\u81f4\u7684\u8bed\u97f3\uff0c\u5e76\u652f\u6301\u4e00\u6837\u672c\u58f0\u97f3\u514b\u9686\u3002\u901a\u8fc7Flow-VAE\uff0c\u5408\u6210\u97f3\u9891\u7684\u6574\u4f53\u8d28\u91cf\u5f97\u5230\u63d0\u5347\u3002\u6a21\u578b\u652f\u630132\u79cd\u8bed\u8a00\uff0c\u5728\u591a\u79cd\u8bc4\u4f30\u6307\u6807\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 349, "total_tokens": 525, "completion_tokens": 176}}
[14.05.2025 09:13] Response: æˆ‘ä»¬ä»‹ç»äº†MiniMax-Speechï¼Œä¸€ç§åŸºäºTransformerçš„æ–‡æœ¬è½¬è¯­éŸ³æ¨¡å‹ã€‚å®ƒèƒ½ç”Ÿæˆé«˜è´¨é‡çš„è¯­éŸ³ã€‚æ¨¡å‹çš„å…³é”®åˆ›æ–°æ˜¯å¯å­¦ä¹ çš„æ¼”è®²è€…ç¼–ç å™¨ï¼Œä»å‚è€ƒéŸ³é¢‘ä¸­æå–éŸ³è‰²ç‰¹å¾ï¼Œæ— éœ€è½¬å½•ã€‚è¿™ä½¿å¾—MiniMax-Speechèƒ½åœ¨é›¶æ ·æœ¬æƒ…å†µä¸‹ç”Ÿæˆè¡¨ç°åŠ›å¼ºã€éŸ³è‰²ä¸€è‡´çš„è¯­éŸ³ï¼Œå¹¶æ”¯æŒä¸€æ ·æœ¬å£°éŸ³å…‹éš†ã€‚é€šè¿‡Flow-VAEï¼ŒåˆæˆéŸ³é¢‘çš„æ•´ä½“è´¨é‡å¾—åˆ°æå‡ã€‚æ¨¡å‹æ”¯æŒ32ç§è¯­è¨€ï¼Œåœ¨å¤šç§è¯„ä¼°æŒ‡æ ‡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„ç»“æœã€‚
[14.05.2025 09:13] Mistral request. Model: mistral-large-latest. Prompt: Write pinyin transcription for text. Text:

æˆ‘ä»¬ä»‹ç»äº†MiniMax-Speechï¼Œä¸€ç§åŸºäºTransformerçš„æ–‡æœ¬è½¬è¯­éŸ³æ¨¡å‹ã€‚å®ƒèƒ½ç”Ÿæˆé«˜è´¨é‡çš„è¯­éŸ³ã€‚æ¨¡å‹çš„å…³é”®åˆ›æ–°æ˜¯å¯å­¦ä¹ çš„æ¼”è®²è€…ç¼–ç å™¨ï¼Œä»å‚è€ƒéŸ³é¢‘ä¸­æå–éŸ³è‰²ç‰¹å¾ï¼Œæ— éœ€è½¬å½•ã€‚è¿™ä½¿å¾—MiniMax-Speechèƒ½åœ¨é›¶æ ·æœ¬æƒ…å†µä¸‹ç”Ÿæˆè¡¨ç°åŠ›å¼ºã€éŸ³è‰²ä¸€è‡´çš„è¯­éŸ³ï¼Œå¹¶æ”¯æŒä¸€æ ·æœ¬å£°éŸ³å…‹éš†ã€‚é€šè¿‡Flow-VAEï¼ŒåˆæˆéŸ³é¢‘çš„æ•´ä½“è´¨é‡å¾—åˆ°æå‡ã€‚æ¨¡å‹æ”¯æŒ32ç§è¯­è¨€ï¼Œåœ¨å¤šç§è¯„ä¼°æŒ‡æ ‡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„ç»“æœã€‚
[14.05.2025 09:13] Mistral response. {"id": "851955a7599c4511a950c200b4eaae86", "object": "chat.completion", "created": 1747214014, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "\u6211\u4eec\u4ecb\u7ecd\u4e86MiniMax-Speech\uff0c\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u6587\u672c\u8f6c\u8bed\u97f3\u6a21\u578b\u3002\u5b83\u80fd\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u8bed\u97f3\u3002\u6a21\u578b\u7684\u5173\u952e\u521b\u65b0\u662f\u53ef\u5b66\u4e60\u7684\u6f14\u8bb2\u8005\u7f16\u7801\u5668\uff0c\u4ece\u53c2\u8003\u97f3\u9891\u4e2d\u63d0\u53d6\u97f3\u8272\u7279\u5f81\uff0c\u65e0\u9700\u8f6c\u5f55\u3002\u8fd9\u4f7f\u5f97MiniMax-Speech\u80fd\u5728\u96f6\u6837\u672c\u60c5\u51b5\u4e0b\u751f\u6210\u8868\u73b0\u529b\u5f3a\u3001\u97f3\u8272\u4e00\u81f4\u7684\u8bed\u97f3\uff0c\u5e76\u652f\u6301\u4e00\u6837\u672c\u58f0\u97f3\u514b\u9686\u3002\u901a\u8fc7Flow-VAE\uff0c\u5408\u6210\u97f3\u9891\u7684\u6574\u4f53\u8d28\u91cf\u5f97\u5230\u63d0\u5347\u3002\u6a21\u578b\u652f\u630132\u79cd\u8bed\u8a00\uff0c\u5728\u591a\u79cd\u8bc4\u4f30\u6307\u6807\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002\n\nW\u01d2men ji\u00e8sh\u00e0o le MiniMax-Speech, y\u012b zh\u01d2ng j\u012by\u00fa Transformer de w\u00e9nb\u011bn zhu\u01cen y\u01d4y\u012bn m\u00f3x\u00edng. T\u0101 n\u00e9ng sh\u0113ngch\u00e9ng g\u0101o zh\u00ecli\u00e0ng de y\u01d4y\u012bn. M\u00f3x\u00edng de gu\u01cenji\u00e0n chu\u00e0ngx\u012bn sh\u00ec k\u011b xu\u00e9x\u00ed de y\u01cenji\u01cengzh\u011b bi\u0101nm\u01ceq\u00ec, c\u00f3ng c\u0101nk\u01ceo y\u012bnp\u00edn zh\u014dng t\u00edqu\u0101n y\u012bns\u00e8 t\u00e8zh\u0113ng, w\u00fax\u016b zhu\u01cenl\u00f9. Zh\u00e8 sh\u01d0 de MiniMax-Speech n\u00e9ng z\u00e0i l\u00edng y\u00e0ngb\u011bn q\u00edngku\u00e0ng xi\u00e0 sh\u0113ngch\u00e9ng bi\u01ceoxi\u00e0nl\u00ec qi\u00e1ng, y\u012bns\u00e8 y\u012bzh\u00ec de y\u01d4y\u012bn, b\u00ecng zh\u012bch\u00ed y\u012b y\u00e0ngb\u011bn sh\u0113ngy\u012bn k\u00e8l\u00f3ng. T\u014dnggu\u00f2 Flow-VAE, h\u00e9ch\u00e9ng y\u012bnp\u00edn de zh\u011bngt\u01d0 zh\u00ecli\u00e0ng d\u00e9d\u00e0o t\u00edsh\u0113ng. M\u00f3x\u00edng zh\u012bch\u00ed 32 zh\u01d2ng y\u01d4y\u00e1n, z\u00e0i du\u014d zh\u01d2ng p\u00edngg\u016b zh\u01d0bi\u0101o sh\u00e0ng bi\u01ceoxi\u00e0n ch\u016bs\u00e8, d\u00e1d\u00e0o le zu\u00ec xi\u0101nj\u00ecn de ji\u00e9gu\u01d2."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 190, "total_tokens": 736, "completion_tokens": 546}}
[14.05.2025 09:13] Response: æˆ‘ä»¬ä»‹ç»äº†MiniMax-Speechï¼Œä¸€ç§åŸºäºTransformerçš„æ–‡æœ¬è½¬è¯­éŸ³æ¨¡å‹ã€‚å®ƒèƒ½ç”Ÿæˆé«˜è´¨é‡çš„è¯­éŸ³ã€‚æ¨¡å‹çš„å…³é”®åˆ›æ–°æ˜¯å¯å­¦ä¹ çš„æ¼”è®²è€…ç¼–ç å™¨ï¼Œä»å‚è€ƒéŸ³é¢‘ä¸­æå–éŸ³è‰²ç‰¹å¾ï¼Œæ— éœ€è½¬å½•ã€‚è¿™ä½¿å¾—MiniMax-Speechèƒ½åœ¨é›¶æ ·æœ¬æƒ…å†µä¸‹ç”Ÿæˆè¡¨ç°åŠ›å¼ºã€éŸ³è‰²ä¸€è‡´çš„è¯­éŸ³ï¼Œå¹¶æ”¯æŒä¸€æ ·æœ¬å£°éŸ³å…‹éš†ã€‚é€šè¿‡Flow-VAEï¼ŒåˆæˆéŸ³é¢‘çš„æ•´ä½“è´¨é‡å¾—åˆ°æå‡ã€‚æ¨¡å‹æ”¯æŒ32ç§è¯­è¨€ï¼Œåœ¨å¤šç§è¯„ä¼°æŒ‡æ ‡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„ç»“æœã€‚

WÇ’men jiÃ¨shÃ o le MiniMax-Speech, yÄ« zhÇ’ng jÄ«yÃº Transformer de wÃ©nbÄ›n zhuÇn yÇ”yÄ«n mÃ³xÃ­ng. TÄ nÃ©ng shÄ“ngchÃ©ng gÄo zhÃ¬liÃ ng de yÇ”yÄ«n. MÃ³xÃ­ng de guÇnjiÃ n chuÃ ngxÄ«n shÃ¬ kÄ› xuÃ©xÃ­ de yÇnjiÇngzhÄ› biÄnmÇqÃ¬, cÃ³ng cÄnkÇo yÄ«npÃ­n zhÅng tÃ­quÄn yÄ«nsÃ¨ tÃ¨zhÄ“ng, wÃºxÅ« zhuÇnlÃ¹. ZhÃ¨ shÇ de MiniMax-Speech nÃ©ng zÃ i lÃ­ng yÃ ngbÄ›n qÃ­ngkuÃ ng xiÃ  shÄ“ngchÃ©ng biÇoxiÃ nlÃ¬ qiÃ¡ng, yÄ«nsÃ¨ yÄ«zhÃ¬ de yÇ”yÄ«n, bÃ¬ng zhÄ«chÃ­ yÄ« yÃ ngbÄ›n shÄ“ngyÄ«n kÃ¨lÃ³ng. TÅngguÃ² Flow-VAE, hÃ©chÃ©ng yÄ«npÃ­n de zhÄ›ngtÇ zhÃ¬liÃ ng dÃ©dÃ o tÃ­shÄ“ng. MÃ³xÃ­ng zhÄ«chÃ­ 32 zhÇ’ng yÇ”yÃ¡n, zÃ i duÅ zhÇ’ng pÃ­nggÅ« zhÇbiÄo shÃ ng biÇoxiÃ n chÅ«sÃ¨, dÃ¡dÃ o le zuÃ¬ xiÄnjÃ¬n de jiÃ©guÇ’.
[14.05.2025 09:13] Mistral request. Model: mistral-large-latest. Prompt: Write vocab of difficult words for this text as an array of objects with fields 'word', 'pinyin', 'trans'. Return as python list without formatting. Return list and nothing else. Text:

æˆ‘ä»¬ä»‹ç»äº†MiniMax-Speechï¼Œä¸€ç§åŸºäºTransformerçš„æ–‡æœ¬è½¬è¯­éŸ³æ¨¡å‹ã€‚å®ƒèƒ½ç”Ÿæˆé«˜è´¨é‡çš„è¯­éŸ³ã€‚æ¨¡å‹çš„å…³é”®åˆ›æ–°æ˜¯å¯å­¦ä¹ çš„æ¼”è®²è€…ç¼–ç å™¨ï¼Œä»å‚è€ƒéŸ³é¢‘ä¸­æå–éŸ³è‰²ç‰¹å¾ï¼Œæ— éœ€è½¬å½•ã€‚è¿™ä½¿å¾—MiniMax-Speechèƒ½åœ¨é›¶æ ·æœ¬æƒ…å†µä¸‹ç”Ÿæˆè¡¨ç°åŠ›å¼ºã€éŸ³è‰²ä¸€è‡´çš„è¯­éŸ³ï¼Œå¹¶æ”¯æŒä¸€æ ·æœ¬å£°éŸ³å…‹éš†ã€‚é€šè¿‡Flow-VAEï¼ŒåˆæˆéŸ³é¢‘çš„æ•´ä½“è´¨é‡å¾—åˆ°æå‡ã€‚æ¨¡å‹æ”¯æŒ32ç§è¯­è¨€ï¼Œåœ¨å¤šç§è¯„ä¼°æŒ‡æ ‡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„ç»“æœã€‚
[14.05.2025 09:14] Mistral response. {"id": "588ef4423a6f4718977f2effabb9baf6", "object": "chat.completion", "created": 1747214030, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "[\n    {\"word\": \"\u4ecb\u7ecd\", \"pinyin\": \"ji\u00e8 sh\u00e0o\", \"trans\": \"introduce\"},\n    {\"word\": \"MiniMax-Speech\", \"pinyin\": \"MiniMax-Speech\", \"trans\": \"MiniMax-Speech\"},\n    {\"word\": \"\u57fa\u4e8e\", \"pinyin\": \"j\u012b y\u00fa\", \"trans\": \"based on\"},\n    {\"word\": \"Transformer\", \"pinyin\": \"Transformer\", \"trans\": \"Transformer\"},\n    {\"word\": \"\u6587\u672c\u8f6c\u8bed\u97f3\", \"pinyin\": \"w\u00e9n b\u011bn zhu\u01cen y\u01d4 y\u012bn\", \"trans\": \"text-to-speech\"},\n    {\"word\": \"\u6a21\u578b\", \"pinyin\": \"m\u00f3 x\u00edng\", \"trans\": \"model\"},\n    {\"word\": \"\u751f\u6210\", \"pinyin\": \"sh\u0113ng ch\u00e9ng\", \"trans\": \"generate\"},\n    {\"word\": \"\u9ad8\u8d28\u91cf\", \"pinyin\": \"g\u0101o zh\u00ec li\u00e0ng\", \"trans\": \"high quality\"},\n    {\"word\": \"\u8bed\u97f3\", \"pinyin\": \"y\u01d4 y\u012bn\", \"trans\": \"speech\"},\n    {\"word\": \"\u5173\u952e\", \"pinyin\": \"gu\u01cen ji\u00e0n\", \"trans\": \"key\"},\n    {\"word\": \"\u521b\u65b0\", \"pinyin\": \"chu\u00e0ng x\u012bn\", \"trans\": \"innovation\"},\n    {\"word\": \"\u53ef\u5b66\u4e60\", \"pinyin\": \"k\u011b xu\u00e9 x\u00ed\", \"trans\": \"learnable\"},\n    {\"word\": \"\u6f14\u8bb2\u8005\", \"pinyin\": \"y\u01cen ji\u01ceng zh\u011b\", \"trans\": \"speaker\"},\n    {\"word\": \"\u7f16\u7801\u5668\", \"pinyin\": \"bi\u0101n m\u01ce q\u00ec\", \"trans\": \"encoder\"},\n    {\"word\": \"\u4ece\", \"pinyin\": \"c\u00f3ng\", \"trans\": \"from\"},\n    {\"word\": \"\u53c2\u8003\", \"pinyin\": \"c\u0101n k\u01ceo\", \"trans\": \"reference\"},\n    {\"word\": \"\u97f3\u9891\", \"pinyin\": \"y\u012bn p\u00edn\", \"trans\": \"audio\"},\n    {\"word\": \"\u63d0\u53d6\", \"pinyin\": \"t\u00ed qu\", \"trans\": \"extract\"},\n    {\"word\": \"\u97f3\u8272\", \"pinyin\": \"y\u012bn s\u00e8\", \"trans\": \"timbre\"},\n    {\"word\": \"\u7279\u5f81\", \"pinyin\": \"t\u00e8 zh\u0113ng\", \"trans\": \"features\"},\n    {\"word\": \"\u65e0\u9700\", \"pinyin\": \"w\u00fa x\u016b\", \"trans\": \"no need\"},\n    {\"word\": \"\u8f6c\u5f55\", \"pinyin\": \"zhu\u01cen l\u00f9\", \"trans\": \"transcription\"},\n    {\"word\": \"\u96f6\u6837\u672c\", \"pinyin\": \"l\u00edng y\u00e0ng b\u011bn\", \"trans\": \"zero-shot\"},\n    {\"word\": \"\u60c5\u51b5\", \"pinyin\": \"q\u00edng ku\u00e0ng\", \"trans\": \"situation\"},\n    {\"word\": \"\u8868\u73b0\u529b\", \"pinyin\": \"bi\u01ceo xi\u00e0n l\u00ec\", \"trans\": \"expressiveness\"},\n    {\"word\": \"\u5f3a\", \"pinyin\": \"qi\u00e1ng\", \"trans\": \"strong\"},\n    {\"word\": \"\u4e00\u81f4\", \"pinyin\": \"y\u012b zh\u00ec\", \"trans\": \"consistent\"},\n    {\"word\": \"\u652f\u6301\", \"pinyin\": \"zh\u012b ch\u00ed\", \"trans\": \"support\"},\n    {\"word\": \"\u4e00\u6837\u672c\", \"pinyin\": \"y\u012b y\u00e0ng b\u011bn\", \"trans\": \"one-shot\"},\n    {\"word\": \"\u58f0\u97f3\", \"pinyin\": \"sh\u0113ng y\u012bn\", \"trans\": \"voice\"},\n    {\"word\": \"\u514b\u9686\", \"pinyin\": \"k\u00e8 l\u00f3ng\", \"trans\": \"clone\"},\n    {\"word\": \"\u901a\u8fc7\", \"pinyin\": \"t\u014dng gu\u00f2\", \"trans\": \"through\"},\n    {\"word\": \"Flow-VAE\", \"pinyin\": \"Flow-VAE\", \"trans\": \"Flow-VAE\"},\n    {\"word\": \"\u5408\u6210\", \"pinyin\": \"h\u00e9 ch\u00e9ng\", \"trans\": \"synthesis\"},\n    {\"word\": \"\u6574\u4f53\", \"pinyin\": \"zh\u011bng t\u01d0\", \"trans\": \"overall\"},\n    {\"word\": \"\u8d28\u91cf\", \"pinyin\": \"zh\u00ec li\u00e0ng\", \"trans\": \"quality\"},\n    {\"word\": \"\u5f97\u5230\", \"pinyin\": \"d\u00e9 d\u00e0o\", \"trans\": \"obtain\"},\n    {\"word\": \"\u63d0\u5347\", \"pinyin\": \"t\u00ed sh\u0113ng\", \"trans\": \"improvement\"},\n    {\"word\": \"\u591a\u79cd\", \"pinyin\": \"du\u014d zh\u01d2ng\", \"trans\": \"various\"},\n    {\"word\": \"\u8bc4\u4f30\", \"pinyin\": \"p\u00edng g\u016b\", \"trans\": \"evaluation\"},\n    {\"word\": \"\u6307\u6807\", \"pinyin\": \"zh\u01d0 bi\u0101o\", \"trans\": \"metrics\"},\n    {\"word\": \"\u8868\u73b0\", \"pinyin\": \"bi\u01ceo xi\u00e0n\", \"trans\": \"performance\"},\n    {\"word\": \"\u51fa\u8272\", \"pinyin\": \"ch\u016b s\u00e8\", \"trans\": \"outstanding\"},\n    {\"word\": \"\u8fbe\u5230\", \"pinyin\": \"d\u00e1 d\u00e0o\", \"trans\": \"achieve\"},\n    {\"word\": \"\u6700\u5148\u8fdb\", \"pinyin\": \"zu\u00ec xi\u0101n j\u00ecn\", \"trans\": \"state-of-the-art\"}\n]"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 222, "total_tokens": 1546, "completion_tokens": 1324}}
[14.05.2025 09:14] Response: [
    {"word": "ä»‹ç»", "pinyin": "jiÃ¨ shÃ o", "trans": "introduce"},
    {"word": "MiniMax-Speech", "pinyin": "MiniMax-Speech", "trans": "MiniMax-Speech"},
    {"word": "åŸºäº", "pinyin": "jÄ« yÃº", "trans": "based on"},
    {"word": "Transformer", "pinyin": "Transformer", "trans": "Transformer"},
    {"word": "æ–‡æœ¬è½¬è¯­éŸ³", "pinyin": "wÃ©n bÄ›n zhuÇn yÇ” yÄ«n", "trans": "text-to-speech"},
    {"word": "æ¨¡å‹", "pinyin": "mÃ³ xÃ­ng", "trans": "model"},
    {"word": "ç”Ÿæˆ", "pinyin": "shÄ“ng chÃ©ng", "trans": "generate"},
    {"word": "é«˜è´¨é‡", "pinyin": "gÄo zhÃ¬ liÃ ng", "trans": "high quality"},
    {"word": "è¯­éŸ³", "pinyin": "yÇ” yÄ«n", "trans": "speech"},
    {"word": "å…³é”®", "pinyin": "guÇn jiÃ n", "trans": "key"},
    {"word": "åˆ›æ–°", "pinyin": "chuÃ ng xÄ«n", "trans": "innovation"},
    {"word": "å¯å­¦ä¹ ", "pinyin": "kÄ› xuÃ© xÃ­", "trans": "learnable"},
    {"word": "æ¼”è®²è€…", "pinyin": "yÇn jiÇng zhÄ›", "trans": "speaker"},
    {"word": "ç¼–ç å™¨", "pinyin": "biÄn mÇ qÃ¬", "trans": "encoder"},
    {"word": "ä»", "pinyin": "cÃ³ng", "trans": "from"},
    {"word": "å‚è€ƒ", "pinyin": "cÄn kÇo", "trans": "reference"},
    {"word": "éŸ³é¢‘", "pinyin": "yÄ«n pÃ­n", "trans": "audio"},
    {"word": "æå–", "pinyin": "tÃ­ qu", "trans": "extract"},
    {"word": "éŸ³è‰²", "pinyin": "yÄ«n sÃ¨", "trans": "timbre"},
    {"word": "ç‰¹å¾", "pinyin": "tÃ¨ zhÄ“ng", "trans": "features"},
    {"word": "æ— éœ€", "pinyin": "wÃº xÅ«", "trans": "no need"},
    {"word": "è½¬å½•", "pinyin": "zhuÇn lÃ¹", "trans": "transcription"},
    {"word": "é›¶æ ·æœ¬", "pinyin": "lÃ­ng yÃ ng bÄ›n", "trans": "zero-shot"},
    {"word": "æƒ…å†µ", "pinyin": "qÃ­ng kuÃ ng", "trans": "situation"},
    {"word": "è¡¨ç°åŠ›", "pinyin": "biÇo xiÃ n lÃ¬", "trans": "expressiveness"},
    {"word": "å¼º", "pinyin": "qiÃ¡ng", "trans": "strong"},
    {"word": "ä¸€è‡´", "pinyin": "yÄ« zhÃ¬", "trans": "consistent"},
    {"word": "æ”¯æŒ", "pinyin": "zhÄ« chÃ­", "trans": "support"},
    {"word": "ä¸€æ ·æœ¬", "pinyin": "yÄ« yÃ ng bÄ›n", "trans": "one-shot"},
    {"word": "å£°éŸ³", "pinyin": "shÄ“ng yÄ«n", "trans": "voice"},
    {"word": "å…‹éš†", "pinyin": "kÃ¨ lÃ³ng", "trans": "clone"},
    {"word": "é€šè¿‡", "pinyin": "tÅng guÃ²", "trans": "through"},
    {"word": "Flow-VAE", "pinyin": "Flow-VAE", "trans": "Flow-VAE"},
    {"word": "åˆæˆ", "pinyin": "hÃ© chÃ©ng", "trans": "synthesis"},
    {"word": "æ•´ä½“", "pinyin": "zhÄ›ng tÇ", "trans": "overall"},
    {"word": "è´¨é‡", "pinyin": "zhÃ¬ liÃ ng", "trans": "quality"},
    {"word": "å¾—åˆ°", "pinyin": "dÃ© dÃ o", "trans": "obtain"},
    {"word": "æå‡", "pinyin": "tÃ­ shÄ“ng", "trans": "improvement"},
    {"word": "å¤šç§", "pinyin": "duÅ zhÇ’ng", "trans": "various"},
    {"word": "è¯„ä¼°", "pinyin": "pÃ­ng gÅ«", "trans": "evaluation"},
    {"word": "æŒ‡æ ‡", "pinyin": "zhÇ biÄo", "trans": "metrics"},
    {"word": "è¡¨ç°", "pinyin": "biÇo xiÃ n", "trans": "performance"},
    {"word": "å‡ºè‰²", "pinyin": "chÅ« sÃ¨", "trans": "outstanding"},
    {"word": "è¾¾åˆ°", "pinyin": "dÃ¡ dÃ o", "trans": "achieve"},
    {"word": "æœ€å…ˆè¿›", "pinyin": "zuÃ¬ xiÄn jÃ¬n", "trans": "state-of-the-art"}
]
[14.05.2025 09:14] Mistral request. Model: mistral-large-latest. Prompt: Translate this text in English. Text:

æˆ‘ä»¬ä»‹ç»äº†MiniMax-Speechï¼Œä¸€ç§åŸºäºTransformerçš„æ–‡æœ¬è½¬è¯­éŸ³æ¨¡å‹ã€‚å®ƒèƒ½ç”Ÿæˆé«˜è´¨é‡çš„è¯­éŸ³ã€‚æ¨¡å‹çš„å…³é”®åˆ›æ–°æ˜¯å¯å­¦ä¹ çš„æ¼”è®²è€…ç¼–ç å™¨ï¼Œä»å‚è€ƒéŸ³é¢‘ä¸­æå–éŸ³è‰²ç‰¹å¾ï¼Œæ— éœ€è½¬å½•ã€‚è¿™ä½¿å¾—MiniMax-Speechèƒ½åœ¨é›¶æ ·æœ¬æƒ…å†µä¸‹ç”Ÿæˆè¡¨ç°åŠ›å¼ºã€éŸ³è‰²ä¸€è‡´çš„è¯­éŸ³ï¼Œå¹¶æ”¯æŒä¸€æ ·æœ¬å£°éŸ³å…‹éš†ã€‚é€šè¿‡Flow-VAEï¼ŒåˆæˆéŸ³é¢‘çš„æ•´ä½“è´¨é‡å¾—åˆ°æå‡ã€‚æ¨¡å‹æ”¯æŒ32ç§è¯­è¨€ï¼Œåœ¨å¤šç§è¯„ä¼°æŒ‡æ ‡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„ç»“æœã€‚
[14.05.2025 09:14] Mistral response. {"id": "fcad18b60b4241149b8f1fb413af4987", "object": "chat.completion", "created": 1747214041, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "We introduce MiniMax-Speech, a Transformer-based text-to-speech model capable of generating high-quality speech. The key innovation of the model is a learnable speaker encoder that extracts voice characteristics from reference audio without the need for transcription. This enables MiniMax-Speech to generate expressive and voice-consistent speech in zero-shot scenarios and supports one-shot voice cloning. Through Flow-VAE, the overall quality of the synthesized audio is enhanced. The model supports 32 languages and performs excellently across various evaluation metrics, achieving state-of-the-art results."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 188, "total_tokens": 320, "completion_tokens": 132}}
[14.05.2025 09:14] Response: We introduce MiniMax-Speech, a Transformer-based text-to-speech model capable of generating high-quality speech. The key innovation of the model is a learnable speaker encoder that extracts voice characteristics from reference audio without the need for transcription. This enables MiniMax-Speech to generate expressive and voice-consistent speech in zero-shot scenarios and supports one-shot voice cloning. Through Flow-VAE, the overall quality of the synthesized audio is enhanced. The model supports 32 languages and performs excellently across various evaluation metrics, achieving state-of-the-art results.
[14.05.2025 09:14] Renaming data file.
[14.05.2025 09:14] Renaming previous data. hf_papers.json to ./d/2025-05-14.json
[14.05.2025 09:14] Saving new data file.
[14.05.2025 09:14] Generating page.
[14.05.2025 09:14] Renaming previous page.
[14.05.2025 09:14] Renaming previous data. index.html to ./d/2025-05-14.html
[14.05.2025 09:14] [Experimental] Generating Chinese page for reading.
[14.05.2025 09:14] Chinese vocab [{'word': 'ä»‹ç»', 'pinyin': 'jiÃ¨ shÃ o', 'trans': 'introduce'}, {'word': 'MiniMax-Speech', 'pinyin': 'MiniMax-Speech', 'trans': 'MiniMax-Speech'}, {'word': 'åŸºäº', 'pinyin': 'jÄ« yÃº', 'trans': 'based on'}, {'word': 'Transformer', 'pinyin': 'Transformer', 'trans': 'Transformer'}, {'word': 'æ–‡æœ¬è½¬è¯­éŸ³', 'pinyin': 'wÃ©n bÄ›n zhuÇn yÇ” yÄ«n', 'trans': 'text-to-speech'}, {'word': 'æ¨¡å‹', 'pinyin': 'mÃ³ xÃ­ng', 'trans': 'model'}, {'word': 'ç”Ÿæˆ', 'pinyin': 'shÄ“ng chÃ©ng', 'trans': 'generate'}, {'word': 'é«˜è´¨é‡', 'pinyin': 'gÄo zhÃ¬ liÃ ng', 'trans': 'high quality'}, {'word': 'è¯­éŸ³', 'pinyin': 'yÇ” yÄ«n', 'trans': 'speech'}, {'word': 'å…³é”®', 'pinyin': 'guÇn jiÃ n', 'trans': 'key'}, {'word': 'åˆ›æ–°', 'pinyin': 'chuÃ ng xÄ«n', 'trans': 'innovation'}, {'word': 'å¯å­¦ä¹ ', 'pinyin': 'kÄ› xuÃ© xÃ­', 'trans': 'learnable'}, {'word': 'æ¼”è®²è€…', 'pinyin': 'yÇn jiÇng zhÄ›', 'trans': 'speaker'}, {'word': 'ç¼–ç å™¨', 'pinyin': 'biÄn mÇ qÃ¬', 'trans': 'encoder'}, {'word': 'ä»', 'pinyin': 'cÃ³ng', 'trans': 'from'}, {'word': 'å‚è€ƒ', 'pinyin': 'cÄn kÇo', 'trans': 'reference'}, {'word': 'éŸ³é¢‘', 'pinyin': 'yÄ«n pÃ­n', 'trans': 'audio'}, {'word': 'æå–', 'pinyin': 'tÃ­ qu', 'trans': 'extract'}, {'word': 'éŸ³è‰²', 'pinyin': 'yÄ«n sÃ¨', 'trans': 'timbre'}, {'word': 'ç‰¹å¾', 'pinyin': 'tÃ¨ zhÄ“ng', 'trans': 'features'}, {'word': 'æ— éœ€', 'pinyin': 'wÃº xÅ«', 'trans': 'no need'}, {'word': 'è½¬å½•', 'pinyin': 'zhuÇn lÃ¹', 'trans': 'transcription'}, {'word': 'é›¶æ ·æœ¬', 'pinyin': 'lÃ­ng yÃ ng bÄ›n', 'trans': 'zero-shot'}, {'word': 'æƒ…å†µ', 'pinyin': 'qÃ­ng kuÃ ng', 'trans': 'situation'}, {'word': 'è¡¨ç°åŠ›', 'pinyin': 'biÇo xiÃ n lÃ¬', 'trans': 'expressiveness'}, {'word': 'å¼º', 'pinyin': 'qiÃ¡ng', 'trans': 'strong'}, {'word': 'ä¸€è‡´', 'pinyin': 'yÄ« zhÃ¬', 'trans': 'consistent'}, {'word': 'æ”¯æŒ', 'pinyin': 'zhÄ« chÃ­', 'trans': 'support'}, {'word': 'ä¸€æ ·æœ¬', 'pinyin': 'yÄ« yÃ ng bÄ›n', 'trans': 'one-shot'}, {'word': 'å£°éŸ³', 'pinyin': 'shÄ“ng yÄ«n', 'trans': 'voice'}, {'word': 'å…‹éš†', 'pinyin': 'kÃ¨ lÃ³ng', 'trans': 'clone'}, {'word': 'é€šè¿‡', 'pinyin': 'tÅng guÃ²', 'trans': 'through'}, {'word': 'Flow-VAE', 'pinyin': 'Flow-VAE', 'trans': 'Flow-VAE'}, {'word': 'åˆæˆ', 'pinyin': 'hÃ© chÃ©ng', 'trans': 'synthesis'}, {'word': 'æ•´ä½“', 'pinyin': 'zhÄ›ng tÇ', 'trans': 'overall'}, {'word': 'è´¨é‡', 'pinyin': 'zhÃ¬ liÃ ng', 'trans': 'quality'}, {'word': 'å¾—åˆ°', 'pinyin': 'dÃ© dÃ o', 'trans': 'obtain'}, {'word': 'æå‡', 'pinyin': 'tÃ­ shÄ“ng', 'trans': 'improvement'}, {'word': 'å¤šç§', 'pinyin': 'duÅ zhÇ’ng', 'trans': 'various'}, {'word': 'è¯„ä¼°', 'pinyin': 'pÃ­ng gÅ«', 'trans': 'evaluation'}, {'word': 'æŒ‡æ ‡', 'pinyin': 'zhÇ biÄo', 'trans': 'metrics'}, {'word': 'è¡¨ç°', 'pinyin': 'biÇo xiÃ n', 'trans': 'performance'}, {'word': 'å‡ºè‰²', 'pinyin': 'chÅ« sÃ¨', 'trans': 'outstanding'}, {'word': 'è¾¾åˆ°', 'pinyin': 'dÃ¡ dÃ o', 'trans': 'achieve'}, {'word': 'æœ€å…ˆè¿›', 'pinyin': 'zuÃ¬ xiÄn jÃ¬n', 'trans': 'state-of-the-art'}]
[14.05.2025 09:14] Renaming previous Chinese page.
[14.05.2025 09:14] Renaming previous data. zh.html to ./d/2025-05-13_zh_reading_task.html
[14.05.2025 09:14] Writing Chinese reading task.
[14.05.2025 09:14] Writing result.
[14.05.2025 09:14] Renaming log file.
[14.05.2025 09:14] Renaming previous data. log.txt to ./logs/2025-05-14_last_log.txt
