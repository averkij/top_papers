[05.11.2024 22:12] Read previous papers.
[05.11.2024 22:12] Generating top page (month).
[05.11.2024 22:12] Writing top page (month).
[06.11.2024 00:57] Read previous papers.
[06.11.2024 00:57] Get feed.
[06.11.2024 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2410.24024
[06.11.2024 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2411.02355
[06.11.2024 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2411.02337
[06.11.2024 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2411.02395
[06.11.2024 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2411.00860
[06.11.2024 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2411.02265
[06.11.2024 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2411.02385
[06.11.2024 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2411.00836
[06.11.2024 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2411.02336
[06.11.2024 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2411.02319
[06.11.2024 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2411.02397
[06.11.2024 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2411.01747
[06.11.2024 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2411.02335
[06.11.2024 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2411.02327
[06.11.2024 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2411.00918
[06.11.2024 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2411.00743
[06.11.2024 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2411.01798
[06.11.2024 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2411.02394
[06.11.2024 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2411.01192
[06.11.2024 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2411.00359
[06.11.2024 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2411.00785
[06.11.2024 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2411.01106
[06.11.2024 00:57] Get page data from previous paper. URL: https://huggingface.co/papers/2411.00492
[06.11.2024 00:57] ********************************************************************************
[06.11.2024 00:57] Abstract 0. Autonomous agents have become increasingly important for interacting with the real world. Android agents, in particular, have been recently a frequently-mentioned interaction method. However, existing studies for training and evaluating Android agents lack systematic research on both open-source and...
[06.11.2024 00:57] ********************************************************************************
[06.11.2024 00:57] Abstract 1. Despite the popularity of large language model (LLM) quantization for inference acceleration, significant uncertainty remains regarding the accuracy-performance trade-offs associated with various quantization formats. We present a comprehensive empirical study of quantized accuracy, evaluating popul...
[06.11.2024 00:57] ********************************************************************************
[06.11.2024 00:57] Abstract 2. Large language models (LLMs) have shown remarkable potential as autonomous agents, particularly in web-based tasks. However, existing LLM web agents heavily rely on expensive proprietary LLM APIs, while open LLMs lack the necessary decision-making capabilities. This paper introduces WebRL, a self-ev...
[06.11.2024 00:57] ********************************************************************************
[06.11.2024 00:57] Abstract 3. Diffusion models have demonstrated excellent capabilities in text-to-image generation. Their semantic understanding (i.e., prompt following) ability has also been greatly improved with large language models (e.g., T5, Llama). However, existing models cannot perfectly handle long and complex text pro...
[06.11.2024 00:57] ********************************************************************************
[06.11.2024 00:57] Abstract 4. Large-scale deployment of large language models (LLMs) in various applications, such as chatbots and virtual assistants, requires LLMs to be culturally sensitive to the user to ensure inclusivity. Culture has been widely studied in psychology and anthropology, and there has been a recent surge in re...
[06.11.2024 00:57] ********************************************************************************
[06.11.2024 00:57] Abstract 5. In this paper, we introduce Hunyuan-Large, which is currently the largest open-source Transformer-based mixture of experts model, with a total of 389 billion parameters and 52 billion activation parameters, capable of handling up to 256K tokens. We conduct a thorough evaluation of Hunyuan-Large's su...
[06.11.2024 00:57] ********************************************************************************
[06.11.2024 00:57] Abstract 6. OpenAI's Sora highlights the potential of video generation for developing world models that adhere to fundamental physical laws. However, the ability of video generation models to discover such laws purely from visual data without human priors can be questioned. A world model learning the true law s...
[06.11.2024 00:57] ********************************************************************************
[06.11.2024 00:57] Abstract 7. The rapid advancements in Vision-Language Models (VLMs) have shown great potential in tackling mathematical reasoning tasks that involve visual context. Unlike humans who can reliably apply solution steps to similar problems with minor modifications, we found that SOTA VLMs like GPT-4o can consisten...
[06.11.2024 00:57] ********************************************************************************
[06.11.2024 00:57] Abstract 8. Texturing is a crucial step in the 3D asset production workflow, which enhances the visual appeal and diversity of 3D assets. Despite recent advancements in Text-to-Texture (T2T) generation, existing methods often yield subpar results, primarily due to local discontinuities, inconsistencies across m...
[06.11.2024 00:57] ********************************************************************************
[06.11.2024 00:57] Abstract 9. Recent developments in 2D visual generation have been remarkably successful. However, 3D and 4D generation remain challenging in real-world applications due to the lack of large-scale 4D data and effective model design. In this paper, we propose to jointly investigate general 3D and 4D generation by...
[06.11.2024 00:57] ********************************************************************************
[06.11.2024 00:57] Abstract 10. Generating temporally-consistent high-fidelity videos can be computationally expensive, especially over longer temporal spans. More-recent Diffusion Transformers (DiTs) -- despite making significant headway in this context -- have only heightened such challenges as they rely on larger models and hea...
[06.11.2024 00:57] ********************************************************************************
[06.11.2024 00:57] Abstract 11. Existing LLM agent systems typically select actions from a fixed and predefined set at every step. While this approach is effective in closed, narrowly-scoped environments, we argue that it presents two major challenges when deploying LLM agents in real-world scenarios: (1) selecting from a fixed se...
[06.11.2024 00:57] ********************************************************************************
[06.11.2024 00:57] Abstract 12. Activation sparsity denotes the existence of substantial weakly-contributed elements within activation outputs that can be eliminated, benefiting many important applications concerned with large language models (LLMs). Although promoting greater activation sparsity within LLMs deserves deep studies,...
[06.11.2024 00:57] ********************************************************************************
[06.11.2024 00:57] Abstract 13. The past year has witnessed the significant advancement of video-based large language models. However, the challenge of developing a unified model for both short and long video understanding remains unresolved. Most existing video LLMs cannot handle hour-long videos, while methods custom for long vi...
[06.11.2024 00:57] ********************************************************************************
[06.11.2024 00:57] Abstract 14. Mixture of Experts (MoEs) plays an important role in the development of more efficient and effective large language models (LLMs). Due to the enormous resource requirements, studying large scale MoE algorithms remain in-accessible to many researchers. This work develops LibMoE, a comprehensive and m...
[06.11.2024 00:57] ********************************************************************************
[06.11.2024 00:57] Abstract 15. Understanding and mitigating the potential risks associated with foundation models (FMs) hinges on developing effective interpretability methods. Sparse Autoencoders (SAEs) have emerged as a promising tool for disentangling FM representations, but they struggle to capture rare, yet crucial concepts ...
[06.11.2024 00:57] ********************************************************************************
[06.11.2024 00:57] Abstract 16. In Large Language Model (LLM) development, Reinforcement Learning from Human Feedback (RLHF) is crucial for aligning models with human values and preferences. RLHF traditionally relies on the Kullback-Leibler (KL) divergence between the current policy and a frozen initial policy as a reference, whic...
[06.11.2024 00:57] ********************************************************************************
[06.11.2024 00:57] Abstract 17. Modern visual effects (VFX) software has made it possible for skilled artists to create imagery of virtually anything. However, the creation process remains laborious, complex, and largely inaccessible to everyday users. In this work, we present AutoVFX, a framework that automatically creates realis...
[06.11.2024 00:57] ********************************************************************************
[06.11.2024 00:57] Abstract 18. We introduce Swan, a family of embedding models centred around the Arabic language, addressing both small-scale and large-scale use cases. Swan includes two variants: Swan-Small, based on ARBERTv2, and Swan-Large, built on ArMistral, a pretrained Arabic large language model. To evaluate these models...
[06.11.2024 00:57] ********************************************************************************
[06.11.2024 00:57] Abstract 19. This paper describes an efficient algorithm for solving noisy linear inverse problems using pretrained diffusion models. Extending the paradigm of denoising diffusion implicit models (DDIM), we propose constrained diffusion implicit models (CDIM) that modify the diffusion updates to enforce a constr...
[06.11.2024 00:57] ********************************************************************************
[06.11.2024 00:57] Abstract 20. We introduce Image-GOal Representations (IGOR), aiming to learn a unified, semantically consistent action space across human and various robots. Through this unified latent action space, IGOR enables knowledge transfer among large-scale robot and human activity data. We achieve this by compressing v...
[06.11.2024 00:57] ********************************************************************************
[06.11.2024 00:57] Abstract 21. Large multimodal models (LMMs) have recently shown great progress in text-rich image understanding, yet they still struggle with complex, multi-page, visually-rich documents. Traditional methods using document parsers for retrieval-augmented generation suffer from performance and efficiency limitati...
[06.11.2024 00:57] ********************************************************************************
[06.11.2024 00:57] Abstract 22. We present Multi-expert Prompting, a novel enhancement of ExpertPrompting (Xu et al., 2023), designed to improve the large language model (LLM) generation. Specifically, it guides an LLM to fulfill an input instruction by simulating multiple experts, aggregating their responses, and selecting the be...
[06.11.2024 00:57] Read previous papers.
[06.11.2024 00:57] Generating reviews via LLM API.
[06.11.2024 00:57] Using data from previous issue: {"categories": ["#agents", "#benchmark", "#multimodal", "#dataset"], "emoji": "ü§ñ", "ru": {"title": "AndroidLab: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –æ–±—É—á–µ–Ω–∏–∏ Android-–∞–≥–µ–Ω—Ç–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç AndroidLab - —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫—É—é —Å—Ä–µ–¥—É –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –∏ –æ—Ü–µ–Ω–∫–∏ –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –±–∞–∑–µ Android. –≠—Ç–∞ —Å—Ä–µ–¥–∞ –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è –æ–ø–µ—Ä–∞—Ü–∏–æ–Ω–Ω
[06.11.2024 00:57] Using data from previous issue: {"categories": ["#inference", "#optimization"], "emoji": "üî¨", "ru": {"title": "–û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ LLM: –±–∞–ª–∞–Ω—Å –º–µ–∂–¥—É —Ç–æ—á–Ω–æ—Å—Ç—å—é –∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é", "desc": "–≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—é –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –≤—ã–≤–æ–¥–∞. –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ–≤–µ–ª–∏ –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π —ç–º–ø–∏—Ä–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª
[06.11.2024 00:57] Using data from previous issue: {"categories": ["#agents", "#rl", "#rlhf", "#training"], "emoji": "üï∏Ô∏è", "ru": {"title": "WebRL: –û—Ç–∫—Ä—ã—Ç—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç –ø—Ä–æ–ø—Ä–∏–µ—Ç–∞—Ä–Ω—ã–µ –≤ –≤–µ–±-–∑–∞–¥–∞—á–∞—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç WebRL - —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≤–µ–±-–∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ—Ç–∫—Ä—ã—Ç—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. Web
[06.11.2024 00:57] Using data from previous issue: {"categories": ["#diffusion", "#cv", "#long_context"], "emoji": "üé®", "ru": {"title": "–¢–æ—á–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Å–ª–æ–∂–Ω—ã–º —Ç–µ–∫—Å—Ç–∞–º: –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –¥–ª—è DiT", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —Ä–µ–≥–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∏–Ω–≥–∞ –¥–ª—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã Diffusion Transformer (DiT), –ø—Ä–∏–º–µ–Ω–∏–º—ã–π –∫ –º–æ–¥–µ–ª—è–º FLUX.1. –≠—Ç–æ
[06.11.2024 00:57] Using data from previous issue: {"categories": ["#survey", "#multimodal", "#ethics", "#dataset", "#benchmark"], "emoji": "üåç", "ru": {"title": "–ö—É–ª—å—Ç—É—Ä–Ω–∞—è –∏–Ω–∫–ª—é–∑–∏–≤–Ω–æ—Å—Ç—å –≤ —ç–ø–æ—Ö—É –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –≤–Ω–µ–¥—Ä–µ–Ω–∏—é –∫—É–ª—å—Ç—É—Ä–Ω–æ–π –æ—Å–≤–µ–¥–æ–º–ª–µ–Ω–Ω–æ—Å—Ç–∏ –≤ –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (LLM). –ê–≤—Ç–æ—Ä—ã —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç
[06.11.2024 00:57] Using data from previous issue: {"categories": ["#architecture", "#training", "#long_context", "#synthetic", "#optimization"], "emoji": "üß†", "ru": {"title": "Hunyuan-Large: –ì–∏–≥–∞–Ω—Ç—Å–∫–∏–π —à–∞–≥ –≤–ø–µ—Ä–µ–¥ –≤ –æ–±–ª–∞—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π —Å–º–µ—Å–∏ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Hunyuan-Large - –∫—Ä—É–ø–Ω–µ–π—à—É—é –æ—Ç–∫—Ä—ã—Ç—É—é –º–æ–¥–µ–ª—å –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ —Å –∞—Ä—Ö
[06.11.2024 00:57] Using data from previous issue: {"categories": ["#video", "#diffusion", "#reasoning"], "emoji": "üé•", "ru": {"title": "–ì–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –≤–∏–¥–µ–æ–º–æ–¥–µ–ª–∏ –Ω–µ —Ä–∞—Å–∫—Ä—ã–≤–∞—é—Ç —Ñ–∏–∑–∏—á–µ—Å–∫–∏–µ –∑–∞–∫–æ–Ω—ã –ø—Ä–∏ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –∏–∑—É—á–∞—Ç—å —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ñ–∏–∑–∏—á–µ—Å–∫–∏–µ –∑–∞–∫–æ–Ω—ã –∏–∑ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –ê–≤—Ç
[06.11.2024 00:57] Using data from previous issue: {"categories": ["#benchmark", "#math", "#cv"], "emoji": "üßÆ", "ru": {"title": "DynaMath: –ù–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ—Ü–µ–Ω–∫–µ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –ò–ò", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç DynaMath - –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –≤–∏–∑—É–∞–ª—å–Ω—ã–π –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ä–æ–±–∞—Å—Ç–Ω–æ—Å—Ç–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π Vision-Language Models (VLM) –≤ –∑–∞–¥
[06.11.2024 00:57] Using data from previous issue: {"categories": ["#3d", "#benchmark"], "emoji": "üé®", "ru": {"title": "MVPaint: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–º —Ç–µ–∫—Å—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–∏–∏ 3D-–º–æ–¥–µ–ª–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ MVPaint –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç—É—Ä –¥–ª—è 3D-–º–æ–¥–µ–ª–µ–π. –°–∏—Å—Ç–µ–º–∞ —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ —Ç—Ä–µ—Ö –∫–ª—é—á–µ–≤—ã—Ö –º–æ–¥—É–ª–µ–π: —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞
[06.11.2024 00:57] Using data from previous issue: {"categories": ["#dataset", "#data", "#cv", "#3d", "#video"], "emoji": "üé•", "ru": {"title": "GenXD: –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä 3D –∏ 4D —Å—Ü–µ–Ω", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ 3D –∏ 4D —Å—Ü–µ–Ω –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º GenXD. –û–Ω–∏ —Å–æ–∑–¥–∞–ª–∏ –±–æ–ª—å—à–æ–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö —Ä–µ–∞–ª—å–Ω—ã—Ö 4D —Å—Ü–µ–Ω CamVid-30K, –∏—Å–ø–æ–ª
[06.11.2024 00:57] Using data from previous issue: {"categories": ["#video", "#inference", "#optimization"], "emoji": "üöÄ", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —É—Å–∫–æ—Ä—è–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –≤–∏–¥–µ–æ –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ AdaCache –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –≤–∏–¥–µ–æ-–¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ (DiT) –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è. AdaCache –∞–¥–∞–ø—Ç–∏–≤–Ω–æ –∫—ç—à–∏—Ä—É–µ
[06.11.2024 00:57] Using data from previous issue: {"categories": ["#agents", "#plp", "#benchmark"], "emoji": "ü§ñ", "ru": {"title": "–î–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –¥–µ–π—Å—Ç–≤–∏–π –¥–ª—è –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –±–∞–∑–µ LLM: —à–∞–≥ –∫ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–º—É –ò–ò", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM), –∫–æ—Ç–æ—Ä–∞—è –ø–æ–∑–≤–æ–ª—è–µ—Ç –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ —Å–æ–∑–¥–∞–≤–∞—Ç—å –∏
[06.11.2024 00:57] Using data from previous issue: {"categories": ["#architecture", "#training", "#interpretability"], "emoji": "üß†", "ru": {"title": "–†–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç—å –∞–∫—Ç–∏–≤–∞—Ü–∏–π –≤ LLM: –∫–ª—é—á –∫ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∏ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç–∏", "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç—å –∞–∫—Ç–∏–≤–∞—Ü–∏–π –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM). –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –Ω–æ–≤—É—é –º–µ—Ç—Ä–∏–∫—É PPL-p% –¥–ª—è
[06.11.2024 00:57] Using data from previous issue: {"categories": ["#video", "#multimodal", "#training", "#rlhf"], "emoji": "üé•", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤–∏–¥–µ–æ –ª—é–±–æ–π –¥–ª–∏–Ω—ã —Å –ø–æ–º–æ—â—å—é –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –ø—É–ª–∏–Ω–≥–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å PPLLaVA –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫–∞–∫ –∫–æ—Ä–æ—Ç–∫–∏—Ö, —Ç–∞–∫ –∏ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ. –ö–ª—é—á–µ–≤–∞—è –∏–Ω–Ω–æ–≤–∞—Ü–∏—è –∑–∞–∫–ª—é—á–∞–µ—Ç—Å
[06.11.2024 00:57] Using data from previous issue: {"categories": ["#benchmark", "#training", "#architecture"], "emoji": "üß†", "ru": {"title": "LibMoE: –î–µ–º–æ–∫—Ä–∞—Ç–∏–∑–∞—Ü–∏—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π Mixture of Experts –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç LibMoE - –∫–æ–º–ø–ª–µ–∫—Å–Ω—É—é –º–æ–¥—É–ª—å–Ω—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è, –æ–±—É—á–µ–Ω–∏—è –∏ –æ—Ü–µ–Ω–∫–∏ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ Mixture o
[06.11.2024 00:57] Using data from previous issue: {"categories": ["#interpretability", "#dataset", "#training"], "emoji": "üîç", "ru": {"title": "–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä—ã —Ä–∞—Å–∫—Ä—ã–≤–∞—é—Ç —Å–∫—Ä—ã—Ç—ã–µ –∫–æ–Ω—Ü–µ–ø—Ç—ã –≤ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π - –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–µ –∞–≤—Ç–æ—ç–Ω
[06.11.2024 00:57] Using data from previous issue: {"categories": ["#rlhf", "#training", "#benchmark", "#alignment"], "emoji": "üß†", "ru": {"title": "SALSA: –ì–∏–±–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è –ª—É—á—à–µ–π –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ SALSA –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ –æ—Ç —á–µ–ª–æ–≤–µ–∫–∞ (RLHF) –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö
[06.11.2024 00:57] Using data from previous issue: {"categories": ["#cv", "#multimodal", "#video", "#architecture"], "emoji": "üé¨", "ru": {"title": "AutoVFX: –†–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ –≤–∏–¥–µ–æ—ç—Ñ—Ñ–µ–∫—Ç—ã –ø–æ —Ç–µ–∫—Å—Ç–æ–≤—ã–º –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º", "desc": "AutoVFX - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞, –∫–æ—Ç–æ—Ä–∞—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–æ–∑–¥–∞–µ—Ç —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ –≤–∏–¥–µ–æ —Å –≤–∏–∑—É–∞–ª—å–Ω—ã–º–∏ —ç—Ñ—Ñ–µ–∫—Ç–∞–º–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–¥–Ω–æ–≥–æ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –≤–∏–¥
[06.11.2024 00:57] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#multilingual"], "emoji": "ü¶¢", "ru": {"title": "Swan: –ü—Ä–æ—Ä—ã–≤ –≤ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∞—Ä–∞–±—Å–∫–æ–≥–æ —è–∑—ã–∫–∞", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã –º–æ–¥–µ–ª–∏ —Å–µ–º–µ–π—Å—Ç–≤–∞ Swan –¥–ª—è –≤—Å—Ç—Ä–∞–∏–≤–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–æ–≤ –Ω–∞ –∞—Ä–∞–±—Å–∫–æ–º —è–∑—ã–∫–µ. –†–∞–∑—Ä–∞–±–æ—Ç–∞–Ω—ã –¥–≤–∞ –≤–∞—Ä–∏–∞–Ω—Ç–∞: Swan-Small –Ω–∞ –æ—Å–Ω–æ–≤–µ ARBERTv2 –∏ Swan-Large –Ω–∞ –±–∞–∑–µ Ar
[06.11.2024 00:57] Using data from previous issue: {"categories": ["#diffusion", "#inference", "#3d"], "emoji": "üî¨", "ru": {"title": "–£—Å–∫–æ—Ä–µ–Ω–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ –æ–±—Ä–∞—Ç–Ω—ã—Ö –∑–∞–¥–∞—á —Å –ø–æ–º–æ—â—å—é –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã—Ö –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –∑–∞—à—É–º–ª–µ–Ω–Ω—ã—Ö –ª–∏–Ω–µ–π–Ω—ã—Ö –æ–±—Ä–∞—Ç–Ω—ã—Ö –∑–∞–¥–∞—á —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã
[06.11.2024 00:57] Using data from previous issue: {"categories": ["#agents", "#transfer_learning", "#robotics", "#cv"], "emoji": "ü§ñ", "ru": {"title": "–ï–¥–∏–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –¥–µ–π—Å—Ç–≤–∏–π –¥–ª—è –ª—é–¥–µ–π –∏ —Ä–æ–±–æ—Ç–æ–≤", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç IGOR - —Å–∏—Å—Ç–µ–º—É –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –µ–¥–∏–Ω–æ–≥–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –¥–µ–π—Å—Ç–≤–∏–π –¥–ª—è –ª—é–¥–µ–π –∏ —Ä–æ–±–æ—Ç–æ–≤. IGOR 
[06.11.2024 00:57] Using data from previous issue: {"categories": ["#multimodal", "#rag", "#benchmark"], "emoji": "üìÑ", "ru": {"title": "LoCAL: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é LMM", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ LoCAL –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –¥–ª–∏–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –±–æ–ª—å—à–∏–º–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ (LMM). LoCAL –∏—Å–ø–æ–ª—å–∑—É–µ
[06.11.2024 00:57] Using data from previous issue: {"categories": ["#alignment", "#interpretability", "#training"], "emoji": "üß†", "ru": {"title": "–ö–æ–ª–ª–µ–∫—Ç–∏–≤–Ω—ã–π —Ä–∞–∑—É–º —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ—Ç–≤–µ—Ç–æ–≤ –ò–ò", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –º–µ—Ç–æ–¥ Multi-expert Prompting, —É–ª—É—á—à–∞—é—â–∏–π –≥–µ–Ω–µ—Ä–∞—Ü–∏—é —Ç–µ–∫—Å—Ç–∞ –±–æ–ª—å—à–∏–º–∏ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ (LLM). –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ —Å–∏–º—É–ª–∏—Ä—É–µ—Ç —Ä–∞
[06.11.2024 00:57] Loading Chinese text from previous data.
[06.11.2024 00:57] Renaming data file.
[06.11.2024 00:57] Renaming previous data. hf_papers.json to ./d/2024-11-06.json
[06.11.2024 00:57] Saving new data file.
[06.11.2024 00:57] Generating page.
[06.11.2024 00:57] Renaming previous page.
[06.11.2024 00:57] Renaming previous data. index.html to ./d/2024-11-06.html
[06.11.2024 00:57] [Experimental] Generating Chinese page for reading.
[06.11.2024 00:57] Chinese vocab [{'word': 'Ëá™‰∏ª‰ª£ÁêÜ', 'pinyin': 'z√¨zh«î d√†il«ê', 'trans': 'autonomous agent'}, {'word': 'Áé∞ÂÆû‰∏ñÁïå', 'pinyin': 'xi√†nsh√≠ sh√¨ji√®', 'trans': 'real world'}, {'word': '‰∫§‰∫í', 'pinyin': 'jiƒÅoh√π', 'trans': 'interaction'}, {'word': 'Android‰ª£ÁêÜ', 'pinyin': 'Android d√†il«ê', 'trans': 'Android agent'}, {'word': 'Áé∞ÊúâÁ†îÁ©∂', 'pinyin': 'xi√†ny«íu y√°nji≈´', 'trans': 'existing research'}, {'word': 'Áº∫‰πè', 'pinyin': 'quƒìf√°', 'trans': 'lack'}, {'word': 'ÂºÄÊ∫ê', 'pinyin': 'kƒÅiyu√°n', 'trans': 'open source'}, {'word': 'Èó≠Ê∫ê', 'pinyin': 'b√¨yu√°n', 'trans': 'closed source'}, {'word': 'Á≥ªÁªüÁ†îÁ©∂', 'pinyin': 'x√¨t«íng y√°nji≈´', 'trans': 'systematic study'}, {'word': 'AndroidLab', 'pinyin': 'AndroidLab', 'trans': 'AndroidLab'}, {'word': 'Ê°ÜÊû∂', 'pinyin': 'ku√†ngji√†', 'trans': 'framework'}, {'word': 'Â§öÁßçÊ®°ÊÄÅ', 'pinyin': 'du≈çzh«íng m√≥sh√¨', 'trans': 'multimodal'}, {'word': 'Êìç‰ΩúÁéØÂ¢É', 'pinyin': 'cƒÅozu√≤ hu√°nj√¨ng', 'trans': 'operating environment'}, {'word': 'ÂèØÈáçÂ§ç', 'pinyin': 'kƒõ ch√≥ngf√π', 'trans': 'reproducible'}, {'word': 'Âü∫ÂáÜÊµãËØï', 'pinyin': 'jƒ´zh«în c√®sh√¨', 'trans': 'benchmark test'}, {'word': 'ÊîØÊåÅ', 'pinyin': 'zhƒ´ch√≠', 'trans': 'support'}, {'word': 'Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã', 'pinyin': 'd√†x√≠ng y«îy√°n m√≥x√≠ng', 'trans': 'large language model'}, {'word': 'Â§öÊ®°ÊÄÅÊ®°Âûã', 'pinyin': 'du≈ç m√≥sh√¨ m√≥x√≠ng', 'trans': 'multimodal model'}, {'word': '‰ΩøÁî®', 'pinyin': 'sh«êy√≤ng', 'trans': 'use'}, {'word': 'ÁéØÂ¢É', 'pinyin': 'hu√°nj√¨ng', 'trans': 'environment'}, {'word': 'ÂºÄÂèë', 'pinyin': 'kƒÅifƒÅ', 'trans': 'develop'}, {'word': 'AndroidÊåá‰ª§Êï∞ÊçÆÈõÜ', 'pinyin': 'Android zh«êl√¨ng sh√πj√πj√≠', 'trans': 'Android command dataset'}, {'word': 'ËÆ≠ÁªÉ', 'pinyin': 'x√πnli√†n', 'trans': 'train'}, {'word': 'ÂÖ≠‰∏™', 'pinyin': 'li√π g√®', 'trans': 'six'}, {'word': 'ÊòæËëó', 'pinyin': 'xi«énzh√π', 'trans': 'significant'}, {'word': 'ÊèêÈ´ò', 'pinyin': 't√≠gƒÅo', 'trans': 'improve'}, {'word': '‰ªªÂä°ÊàêÂäüÁéá', 'pinyin': 'r√®nw√π ch√©ngg≈çngl«ú', 'trans': 'task success rate'}, {'word': 'GitHub', 'pinyin': 'GitHub', 'trans': 'GitHub'}]
[06.11.2024 00:57] Renaming previous Chinese page.
[06.11.2024 00:57] Renaming previous data. zh.html to ./d/2024-11-05_zh_reading_task.html
[06.11.2024 00:57] Writing result.
[06.11.2024 00:57] Writing Chinese reading task.
[06.11.2024 00:57] Renaming log file.
[06.11.2024 00:57] Renaming previous data. log.txt to ./logs/2024-11-06_last_log.txt
