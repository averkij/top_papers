[23.02.2026 07:49] Read previous papers.
[23.02.2026 07:49] Generating top page (month).
[23.02.2026 07:49] Writing top page (month).
[23.02.2026 08:40] Read previous papers.
[23.02.2026 08:40] Get feed.
[23.02.2026 08:40] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10693
[23.02.2026 08:40] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08354
[23.02.2026 08:40] Get page data from previous paper. URL: https://huggingface.co/papers/2602.18422
[23.02.2026 08:40] Get page data from previous paper. URL: https://huggingface.co/papers/2602.18071
[23.02.2026 08:40] Get page data from previous paper. URL: https://huggingface.co/papers/2602.18432
[23.02.2026 08:40] Extract page data from URL. URL: https://huggingface.co/papers/2602.17186
[23.02.2026 08:40] Get page data from previous paper. URL: https://huggingface.co/papers/2602.18312
[23.02.2026 08:40] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[23.02.2026 08:40] No deleted papers detected.
[23.02.2026 08:40] Downloading and parsing papers (pdf, html). Total: 7.
[23.02.2026 08:40] Downloading and parsing paper https://huggingface.co/papers/2602.10693.
[23.02.2026 08:40] Extra JSON file exists (./assets/json/2602.10693.json), skip PDF parsing.
[23.02.2026 08:40] Paper image links file exists (./assets/img_data/2602.10693.json), skip HTML parsing.
[23.02.2026 08:40] Success.
[23.02.2026 08:40] Downloading and parsing paper https://huggingface.co/papers/2602.08354.
[23.02.2026 08:40] Extra JSON file exists (./assets/json/2602.08354.json), skip PDF parsing.
[23.02.2026 08:40] Paper image links file exists (./assets/img_data/2602.08354.json), skip HTML parsing.
[23.02.2026 08:40] Success.
[23.02.2026 08:40] Downloading and parsing paper https://huggingface.co/papers/2602.18422.
[23.02.2026 08:40] Extra JSON file exists (./assets/json/2602.18422.json), skip PDF parsing.
[23.02.2026 08:40] Paper image links file exists (./assets/img_data/2602.18422.json), skip HTML parsing.
[23.02.2026 08:40] Success.
[23.02.2026 08:40] Downloading and parsing paper https://huggingface.co/papers/2602.18071.
[23.02.2026 08:40] Extra JSON file exists (./assets/json/2602.18071.json), skip PDF parsing.
[23.02.2026 08:40] Paper image links file exists (./assets/img_data/2602.18071.json), skip HTML parsing.
[23.02.2026 08:40] Success.
[23.02.2026 08:40] Downloading and parsing paper https://huggingface.co/papers/2602.18432.
[23.02.2026 08:40] Extra JSON file exists (./assets/json/2602.18432.json), skip PDF parsing.
[23.02.2026 08:40] Paper image links file exists (./assets/img_data/2602.18432.json), skip HTML parsing.
[23.02.2026 08:40] Success.
[23.02.2026 08:40] Downloading and parsing paper https://huggingface.co/papers/2602.17186.
[23.02.2026 08:40] Downloading paper 2602.17186 from https://arxiv.org/pdf/2602.17186v1...
[23.02.2026 08:40] Extracting affiliations from text.
[23.02.2026 08:40] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 9 1 ] . [ 1 6 8 1 7 1 . 2 0 6 2 : r a Seulbi Lee and Sangheum Hwang Department of Data Science, Seoul National University of Science and Technology seulbi@ds.seoultech.ac.kr, shwang@seoultech.ac.kr February 20, "
[23.02.2026 08:40] Response: ```python
["Seoul National University of Science and Technology"]
```
[23.02.2026 08:40] Deleting PDF ./assets/pdf/2602.17186.pdf.
[23.02.2026 08:40] Success.
[23.02.2026 08:40] Downloading and parsing paper https://huggingface.co/papers/2602.18312.
[23.02.2026 08:40] Extra JSON file exists (./assets/json/2602.18312.json), skip PDF parsing.
[23.02.2026 08:40] Paper image links file exists (./assets/img_data/2602.18312.json), skip HTML parsing.
[23.02.2026 08:40] Success.
[23.02.2026 08:40] Enriching papers with extra data.
[23.02.2026 08:40] ********************************************************************************
[23.02.2026 08:40] Abstract 0. VESPO addresses training instability in LLM reinforcement learning by using variational formulation with variance reduction to correct policy divergence without length normalization.  					AI-generated summary 				 Training stability remains a central challenge in reinforcement learning (RL) for lar...
[23.02.2026 08:40] ********************************************************************************
[23.02.2026 08:40] Abstract 1. Large reasoning models can implicitly determine optimal stopping points for thinking, which SAGE-RL enhances by incorporating efficient reasoning patterns into pass@1 inference for improved accuracy and efficiency.  					AI-generated summary 				 Recent advancements in large reasoning models (LRMs) ...
[23.02.2026 08:40] ********************************************************************************
[23.02.2026 08:40] Abstract 2. A human-centric video world model conditioned on tracked head and hand poses is introduced, enabling dexterous interactions through a bidirectional video diffusion model trained for egocentric virtual environment generation.  					AI-generated summary 				 Extended reality (XR) demands generative mo...
[23.02.2026 08:40] ********************************************************************************
[23.02.2026 08:40] Abstract 3. EgoPush enables robot manipulation in cluttered environments through perception-driven policy learning that uses object-centric latent spaces and stage-decomposed rewards for long-horizon tasks.  					AI-generated summary 				 Humans can rearrange objects in cluttered environments using egocentric p...
[23.02.2026 08:40] ********************************************************************************
[23.02.2026 08:40] Abstract 4. A causal transformer-based variational autoencoder combined with flow matching enables real-time, spatially-aware conversational motion for embodied agents in virtual reality applications.  					AI-generated summary 				 As embodied agents become central to VR, telepresence, and digital human applic...
[23.02.2026 08:40] ********************************************************************************
[23.02.2026 08:40] Abstract 5. Visual Information Gain metric quantifies the contribution of visual input to prediction uncertainty, enabling selective training that improves visual grounding and reduces language bias in vision-language models.  					AI-generated summary 				 Large Vision Language Models (LVLMs) have achieved rem...
[23.02.2026 08:40] ********************************************************************************
[23.02.2026 08:40] Abstract 6. Reinforcement learning policies are improved by using action Jacobian penalty to eliminate unrealistic high-frequency signals, with a new Linear Policy Net architecture reducing computational overhead while enabling faster convergence and efficient inference for motion imitation tasks.  					AI-gene...
[23.02.2026 08:40] Read previous papers.
[23.02.2026 08:40] Generating reviews via LLM API.
[23.02.2026 08:40] Using data from previous issue: {"categories": ["#rl", "#training", "#open_source", "#reasoning", "#math", "#optimization"], "emoji": "âš–ï¸", "ru": {"title": "Ğ¡Ñ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ñ€ĞµĞ´ÑƒĞºÑ†Ğ¸Ñ Ğ´Ğ¸ÑĞ¿ĞµÑ€ÑĞ¸Ğ¸", "desc": "VESPO Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ½ĞµÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾
[23.02.2026 08:40] Using data from previous issue: {"categories": ["#training", "#optimization", "#reasoning", "#benchmark", "#rl"], "emoji": "ğŸ§ ", "ru": {"title": "ĞœĞ¾Ğ´ĞµĞ»Ğ¸ ÑĞ°Ğ¼Ğ¸ Ğ·Ğ½Ğ°ÑÑ‚, ĞºĞ¾Ğ³Ğ´Ğ° Ğ¾ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑŒÑÑ: Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°", "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ½ĞµÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹
[23.02.2026 08:40] Using data from previous issue: {"categories": ["#video", "#architecture", "#training", "#multimodal", "#diffusion", "#3d"], "emoji": "ğŸ¥½", "ru": {"title": "Ğ’Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ° Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ğ¾Ğ·Ñ‹ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ñ‹ Ğ¸ Ñ€ÑƒĞº Ğ´Ğ»Ñ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ğ¾Ğ¹ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸", "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ°, Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½
[23.02.2026 08:40] Using data from previous issue: {"categories": ["#cv", "#robotics", "#rl"], "emoji": "ğŸ¤–", "ru": {"title": "ĞœĞ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ¼ Ğ¸Ğ· Ğ¿ĞµÑ€Ğ²Ğ¾Ğ³Ğ¾ Ğ»Ğ¸Ñ†Ğ° Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ½Ğ¾-Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°", "desc": "EgoPush â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ¼ Ğ² Ğ·Ğ°Ğ³Ñ€Ğ¾Ğ¼Ğ¾Ğ¶Ğ´Ñ‘Ğ½Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ… Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ½Ğ¸Ñ. 
[23.02.2026 08:40] Using data from previous issue: {"categories": ["#agents", "#dataset", "#video", "#architecture", "#multimodal", "#3d"], "emoji": "ğŸ­", "ru": {"title": "ĞŸÑ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ°Ñ Ğ¾ÑĞ²ĞµĞ´Ğ¾Ğ¼Ğ»Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ñ‹Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ñ‹ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸", "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ¾ÑĞ²ĞµĞ´Ğ¾Ğ¼Ğ»ĞµĞ½Ğ½Ñ‹Ñ… 
[23.02.2026 08:40] Querying the API.
[23.02.2026 08:40] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Visual Information Gain metric quantifies the contribution of visual input to prediction uncertainty, enabling selective training that improves visual grounding and reduces language bias in vision-language models.  					AI-generated summary 				 Large Vision Language Models (LVLMs) have achieved remarkable progress, yet they often suffer from language bias, producing answers without relying on visual evidence. While prior work attempts to mitigate this issue through decoding strategies, architectural modifications, or curated instruction data, they typically lack a quantitative measure of how much individual training samples or tokens actually benefit from the image. In this work, we introduce Visual Information Gain (VIG), a perplexity-based metric that measures the reduction in prediction uncertainty provided by visual input. VIG enables fine-grained analysis at both sample and token levels, effectively highlighting visually grounded elements such as colors, spatial relations, and attributes. Leveraging this, we propose a VIG-guided selective training scheme that prioritizes high-VIG samples and tokens. This approach improves visual grounding and mitigates language bias, achieving superior performance with significantly reduced supervision by focusing exclusively on visually informative samples and tokens.
[23.02.2026 08:40] Response: ```json
{
  "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ° Visual Information Gain (VIG), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚, Ğ½Ğ°ÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¸Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑÑ‚Ñ€Ğ°Ğ´Ğ°ÑÑ‚ Ğ¾Ñ‚ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ¸ Ğ½Ğµ Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ÑÑ‚ÑÑ Ğ½Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ° Ğ¿Ñ€Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğµ Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ VIG Ğ¾Ğ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑÑ…ĞµĞ¼Ñƒ Ğ¸Ğ·Ğ±Ğ¸Ñ€Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ñ‹ Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ. Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºÑƒ Ğº Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ñƒ, ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ĞµÑ‚ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğµ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğ¼ Ğ¾Ğ±ÑŠÑ‘Ğ¼Ğ¾Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.",
  "emoji": "ğŸ‘ï¸",
  "title": "Ğ˜Ğ·Ğ¼ĞµÑ€ÑĞµĞ¼ Ğ²ĞºĞ»Ğ°Ğ´ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ: Ğ¸Ğ·Ğ±Ğ¸Ñ€Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ°"
}
```
[23.02.2026 08:40] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Visual Information Gain metric quantifies the contribution of visual input to prediction uncertainty, enabling selective training that improves visual grounding and reduces language bias in vision-language models.  					AI-generated summary 				 Large Vision Language Models (LVLMs) have achieved remarkable progress, yet they often suffer from language bias, producing answers without relying on visual evidence. While prior work attempts to mitigate this issue through decoding strategies, architectural modifications, or curated instruction data, they typically lack a quantitative measure of how much individual training samples or tokens actually benefit from the image. In this work, we introduce Visual Information Gain (VIG), a perplexity-based metric that measures the reduction in prediction uncertainty provided by visual input. VIG enables fine-grained analysis at both sample and token levels, effectively highlighting visually grounded elements such as colors, spatial relations, and attributes. Leveraging this, we propose a VIG-guided selective training scheme that prioritizes high-VIG samples and tokens. This approach improves visual grounding and mitigates language bias, achieving superior performance with significantly reduced supervision by focusing exclusively on visually informative samples and tokens."

[23.02.2026 08:40] Response: ```python
["MULTIMODAL", "TRAINING", "BENCHMARK"]
```

**Justification:**

- **MULTIMODAL**: The paper explicitly focuses on Large Vision Language Models (LVLMs) that combine visual and language modalities, addressing how to improve visual grounding in vision-language models.

- **TRAINING**: The paper proposes a VIG-guided selective training scheme that improves model training by prioritizing high-VIG samples and tokens, which is a training methodology improvement.

- **BENCHMARK**: The paper introduces Visual Information Gain (VIG), a perplexity-based metric for quantifying and measuring the contribution of visual input to prediction uncertainty, which serves as an evaluation framework/metric for analyzing model performance.
[23.02.2026 08:40] Error. Failed to parse JSON from LLM. ["MULTIMODAL", "TRAINING", "BENCHMARK"]


**Justification:**

- **MULTIMODAL**: The paper explicitly focuses on Large Vision Language Models (LVLMs) that combine visual and language modalities, addressing how to improve visual grounding in vision-language models.

- **TRAINING**: The paper proposes a VIG-guided selective training scheme that improves model training by prioritizing high-VIG samples and tokens, which is a training methodology improvement.

- **BENCHMARK**: The paper introduces Visual Information Gain (VIG), a perplexity-based metric for quantifying and measuring the contribution of visual input to prediction uncertainty, which serves as an evaluation framework/metric for analyzing model performance.
[23.02.2026 08:40] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Visual Information Gain metric quantifies the contribution of visual input to prediction uncertainty, enabling selective training that improves visual grounding and reduces language bias in vision-language models.  					AI-generated summary 				 Large Vision Language Models (LVLMs) have achieved remarkable progress, yet they often suffer from language bias, producing answers without relying on visual evidence. While prior work attempts to mitigate this issue through decoding strategies, architectural modifications, or curated instruction data, they typically lack a quantitative measure of how much individual training samples or tokens actually benefit from the image. In this work, we introduce Visual Information Gain (VIG), a perplexity-based metric that measures the reduction in prediction uncertainty provided by visual input. VIG enables fine-grained analysis at both sample and token levels, effectively highlighting visually grounded elements such as colors, spatial relations, and attributes. Leveraging this, we propose a VIG-guided selective training scheme that prioritizes high-VIG samples and tokens. This approach improves visual grounding and mitigates language bias, achieving superior performance with significantly reduced supervision by focusing exclusively on visually informative samples and tokens."

[23.02.2026 08:40] Response: ```python
['INTERPRETABILITY', 'OPTIMIZATION']
```

**Justification:**

- **INTERPRETABILITY**: The paper introduces Visual Information Gain (VIG), a metric designed to analyze and explain model behavior by measuring how much visual input contributes to prediction uncertainty. This directly relates to understanding and interpreting model decisions at both sample and token levels.

- **OPTIMIZATION**: The paper proposes a VIG-guided selective training scheme that optimizes the training process by prioritizing high-VIG samples and tokens, thereby improving training efficiency and performance with reduced supervision.
[23.02.2026 08:40] Error. Failed to parse JSON from LLM. ["INTERPRETABILITY", "OPTIMIZATION"]


**Justification:**

- **INTERPRETABILITY**: The paper introduces Visual Information Gain (VIG), a metric designed to analyze and explain model behavior by measuring how much visual input contributes to prediction uncertainty. This directly relates to understanding and interpreting model decisions at both sample and token levels.

- **OPTIMIZATION**: The paper proposes a VIG-guided selective training scheme that optimizes the training process by prioritizing high-VIG samples and tokens, thereby improving training efficiency and performance with reduced supervision.
[23.02.2026 08:40] Response: ParsedChatCompletionMessage[~ResponseFormatT](content='{"desc":"This paper introduces the Visual Information Gain (VIG) metric, which quantifies how much visual input helps reduce uncertainty in predictions made by vision-language models. It addresses the common issue of language bias in large vision-language models (LVLMs) that often rely on textual information rather than visual evidence. By using VIG, the authors can analyze the contribution of individual training samples and tokens, identifying which visual elements enhance model performance. The proposed VIG-guided selective training method focuses on high-VIG samples, leading to improved visual grounding and reduced reliance on language, ultimately enhancing model accuracy with less training data.","title":"Enhancing Vision-Language Models with Visual Information Gain"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper introduces the Visual Information Gain (VIG) metric, which quantifies how much visual input helps reduce uncertainty in predictions made by vision-language models. It addresses the common issue of language bias in large vision-language models (LVLMs) that often rely on textual information rather than visual evidence. By using VIG, the authors can analyze the contribution of individual training samples and tokens, identifying which visual elements enhance model performance. The proposed VIG-guided selective training method focuses on high-VIG samples, leading to improved visual grounding and reduced reliance on language, ultimately enhancing model accuracy with less training data.', title='Enhancing Vision-Language Models with Visual Information Gain'))
[23.02.2026 08:40] Response: ParsedChatCompletionMessage[~ResponseFormatT](content='{"desc":"æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„åº¦é‡æ ‡å‡†ï¼Œç§°ä¸ºè§†è§‰ä¿¡æ¯å¢ç›Šï¼ˆVIGï¼‰ï¼Œç”¨äºé‡åŒ–è§†è§‰è¾“å…¥å¯¹é¢„æµ‹ä¸ç¡®å®šæ€§çš„è´¡çŒ®ã€‚é€šè¿‡ä½¿ç”¨VIGï¼Œç ”ç©¶è€…èƒ½å¤Ÿåœ¨æ ·æœ¬å’Œæ ‡è®°å±‚é¢è¿›è¡Œç»†è‡´åˆ†æï¼Œè¯†åˆ«å‡ºä¸è§†è§‰ç›¸å…³çš„å…ƒç´ ï¼Œå¦‚é¢œè‰²ã€ç©ºé—´å…³ç³»å’Œå±æ€§ã€‚è¯¥æ–¹æ³•é€šè¿‡ä¼˜å…ˆé€‰æ‹©é«˜VIGæ ·æœ¬å’Œæ ‡è®°ï¼Œå®æ–½é€‰æ‹©æ€§è®­ç»ƒï¼Œä»è€Œæ”¹å–„è§†è§‰åŸºç¡€å’Œå‡å°‘è¯­è¨€åè§ã€‚æœ€ç»ˆï¼Œç ”ç©¶è¡¨æ˜ï¼Œè¿™ç§æ–¹æ³•åœ¨å‡å°‘ç›‘ç£çš„åŒæ—¶ï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„æ€§èƒ½ã€‚","title":"æå‡è§†è§‰åŸºç¡€ï¼Œå‡å°‘è¯­è¨€åè§çš„é€‰æ‹©æ€§è®­ç»ƒ"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„åº¦é‡æ ‡å‡†ï¼Œç§°ä¸ºè§†è§‰ä¿¡æ¯å¢ç›Šï¼ˆVIGï¼‰ï¼Œç”¨äºé‡åŒ–è§†è§‰è¾“å…¥å¯¹é¢„æµ‹ä¸ç¡®å®šæ€§çš„è´¡çŒ®ã€‚é€šè¿‡ä½¿ç”¨VIGï¼Œç ”ç©¶è€…èƒ½å¤Ÿåœ¨æ ·æœ¬å’Œæ ‡è®°å±‚é¢è¿›è¡Œç»†è‡´åˆ†æï¼Œè¯†åˆ«å‡ºä¸è§†è§‰ç›¸å…³çš„å…ƒç´ ï¼Œå¦‚é¢œè‰²ã€ç©ºé—´å…³ç³»å’Œå±æ€§ã€‚è¯¥æ–¹æ³•é€šè¿‡ä¼˜å…ˆé€‰æ‹©é«˜VIGæ ·æœ¬å’Œæ ‡è®°ï¼Œå®æ–½é€‰æ‹©æ€§è®­ç»ƒï¼Œä»è€Œæ”¹å–„è§†è§‰åŸºç¡€å’Œå‡å°‘è¯­è¨€åè§ã€‚æœ€ç»ˆï¼Œç ”ç©¶è¡¨æ˜ï¼Œè¿™ç§æ–¹æ³•åœ¨å‡å°‘ç›‘ç£çš„åŒæ—¶ï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„æ€§èƒ½ã€‚', title='æå‡è§†è§‰åŸºç¡€ï¼Œå‡å°‘è¯­è¨€åè§çš„é€‰æ‹©æ€§è®­ç»ƒ'))
[23.02.2026 08:40] Using data from previous issue: {"categories": ["#architecture", "#robotics", "#optimization", "#inference", "#rl"], "emoji": "ğŸ¤–", "ru": {"title": "Ğ“Ğ»Ğ°Ğ´ĞºĞ¸Ğµ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ñ‡ĞµÑ€ĞµĞ· ÑˆÑ‚Ñ€Ğ°Ñ„ ÑĞºĞ¾Ğ±Ğ¸Ğ°Ğ½Ğ° Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹", "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¸Ğ¼Ğ¸Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑˆÑ‚Ñ€Ğ°Ñ„Ğ° Ğ½Ğ° ÑĞºĞ¾Ğ±Ğ¸Ğ°
[23.02.2026 08:40] Renaming data file.
[23.02.2026 08:40] Renaming previous data. hf_papers.json to ./d/2026-02-23.json
[23.02.2026 08:40] Saving new data file.
[23.02.2026 08:40] Generating page.
[23.02.2026 08:40] Renaming previous page.
[23.02.2026 08:40] Renaming previous data. index.html to ./d/2026-02-23.html
[23.02.2026 08:40] Writing result.
[23.02.2026 08:40] Renaming log file.
[23.02.2026 08:40] Renaming previous data. log.txt to ./logs/2026-02-23_last_log.txt
