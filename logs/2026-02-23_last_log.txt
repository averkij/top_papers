[23.02.2026 04:18] Read previous papers.
[23.02.2026 04:18] Generating top page (month).
[23.02.2026 04:18] Writing top page (month).
[23.02.2026 06:01] Read previous papers.
[23.02.2026 06:01] Get feed.
[23.02.2026 06:01] Extract page data from URL. URL: https://huggingface.co/papers/2602.10693
[23.02.2026 06:01] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08354
[23.02.2026 06:01] Get page data from previous paper. URL: https://huggingface.co/papers/2602.18422
[23.02.2026 06:01] Get page data from previous paper. URL: https://huggingface.co/papers/2602.18071
[23.02.2026 06:01] Get page data from previous paper. URL: https://huggingface.co/papers/2602.18432
[23.02.2026 06:01] Get page data from previous paper. URL: https://huggingface.co/papers/2602.18312
[23.02.2026 06:01] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[23.02.2026 06:01] No deleted papers detected.
[23.02.2026 06:01] Downloading and parsing papers (pdf, html). Total: 6.
[23.02.2026 06:01] Downloading and parsing paper https://huggingface.co/papers/2602.10693.
[23.02.2026 06:01] Downloading paper 2602.10693 from https://arxiv.org/pdf/2602.10693v1...
[23.02.2026 06:02] Extracting affiliations from text.
[23.02.2026 06:02] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"VESPO: Variational Sequence-Level Soft Policy Optimization for Stable Off-Policy LLM Training Guobin Shen 1 Chenxiao Zhao 1 Xiang Cheng 1 Lei Huang 1 Xing Yu 1 6 2 0 2 1 1 ] . [ 1 3 9 6 0 1 . 2 0 6 2 : r a "
[23.02.2026 06:02] Response: ```python
[]
```
[23.02.2026 06:02] Extracting affiliations from text.
[23.02.2026 06:02] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"VESPO: Variational Sequence-Level Soft Policy Optimization for Stable Off-Policy LLM Training Guobin Shen 1 Chenxiao Zhao 1 Xiang Cheng 1 Lei Huang 1 Xing Yu 1 6 2 0 2 1 1 ] . [ 1 3 9 6 0 1 . 2 0 6 2 : r aTraining stability remains central challenge in reinforcement learning (RL) for large language models (LLMs). Policy staleness, asynchronous training, and mismatches between training and inference engines all cause the behavior policy to diverge from the current policy, risking training collapse. Importance sampling provides principled correction for this distribution shift but suffers from high variance; existing remedies such as token-level clipping and sequence-level normalization lack unified theoretical foundation. We propose Variational sEquence-level Soft Policy Optimization (VESPO). By incorporating variance reduction into variational formulation over proposal distributions, VESPO derives closedform reshaping kernel that operates directly on sequence-level importance weights without length normalization. Experiments on mathematical reasoning benchmarks show that VESPO maintains stable training under staleness ratios up to 64 and fully asynchronous execution, and delivers consistent gains across both dense and Mixtureof-Experts models. Code is available at https: //github.com/FloyedShen/VESPO. 1. Introduction Reinforcement learning (RL) has become key technique for tackling complex problem-solving tasks with large language models (LLMs), enabling capabilities such as multistep mathematical reasoning and code generation (OpenAI, 2024; Anthropic, 2025; DeepSeek-AI et al., 2025; Yang et al., 2025). In practice, off-policy updates arise naturally in RL pipelines for LLMs. common source is that systems split large rollout batches into mini-batches for sequential updates (Yang 1Xiaohongshu Inc. Shen <floyed shen@outlook.com>, shan2@xiaohongshu.com>. Preprint. February 12, 2026. Correspondence to: Guobin Xing Yu <yuanFigure 1. Left: VESPO reformulates IS weight reshaping as finding proposal that balances proximity to Âµ and Ï€ under variance constraint. Right: Training reward (gbs/mbs=4) on Qwen3-30B-A3B-Base. et al., 2025), causing later batches to become stale relative to the evolving policy. Asynchronous systems (Fu et al., 2025; Zhong et al., 2025; Noukhovitch et al., 2025) amplify this by decoupling rollout from training entirely. Traininginference mismatches introduce further discrepancies, especially in MoE models where routing decisions compound through layers. To stabilize training under such distribution mismatch, existing works adopt truncated importance sampling (TIS) (Liu et al., 2025), mask out off-policy samples from training (Hu, 2025), or replay expert routing for target policy inference (Zheng et al., 2025b; Ma et al., 2025). While PPO (Schulman et al., 2017) enforces trust region constraints via clipping, it does not directly address the variance challenge of sequence-level importance sampling (IS). Existing methods address this challenge through various importance weight transformations. Most operate at the token level: GRPO (Shao et al., 2024; DeepSeek-AI et al., 2025) applies PPO-style clipping to per-token ratios; other methods such as SAPO (Gao et al., 2025) design heuristic transformations for off-policy importance weights (Xi et al., 2025; Zheng et al., 2025c; Dwyer et al., 2025; Roux et al., 2025; Hu et al., 2025). However, token-level transformations are compromise to avoid the multiplicative variance explosion of sequence-level weights, and have been shown to be merely first-order approximation to their sequencelevel counterparts (Zheng et al., 2025b). Sequence-level approaches (Zheng et al., 2025b; Zhao et al., 2025) introduce length normalization to control variance, but this normal1 VESPO: Variational Sequence-Level Soft Policy Optimization ization makes the IS estimator biased; hard clipping is still required on top. Despite these efforts, principled guidance for designing importance weight transformations remains limited. We propose Variational sEquence-level Soft Policy Optimization (VESPO), which takes fundamentally different approach: rather than designing reshaping heuristics, we explicitly incorporate variance reduction for off-policy importance sampling into variational formulation, yielding principled closed-form solution (Figure 1). Additionally, we find that the resulting transformation is particularly friendly to sequence-level optimization: unlike previous methods that rely on length normalization to avoid variance explosion, VESPO operates directly on sequence-level importance weights without approximation or normalization. Our contributions are summarized as follows: We explicitly incorporate variance reduction into variational formulation for importance weight reshaping and derive principled closed-form solution. The resulting algorithm, VESPO, operates directly on sequence-level importance weights, preserving intertoken dependencies while remaining free from lengthdependent bias. Experiments on mathematical reasoning benchmarks demonstrate that VESPO remains stable under staleness ratios up to 64 and in fully asynchronous training, delivering consistent improvements across both dense and MoE architectures. 2. Preliminaries Notation. We consider an autoregressive language model parameterized by Î¸ as policy Ï€Î¸. Let denote query sampled from dataset D. In off-policy settings, responses = (y1, . . . , yT ) are sampled from behavior policy Âµ (e.g., an earlier checkpoint or different inference engine). The likelihood of generating given factorizes as: Ï€Î¸(y x) = (cid:89) t=1 Ï€Î¸(yt x, y<t). (1) We write Ï„ = (x, y) for query-response pair, and R(Ï„ ) for the sequence-level reward assigned to the complete response. Policy Gradient with Off-Policy Correction. The goal is to maximize the expected reward under the current policy: (Î¸) = EÏ„ Ï€Î¸ (cid:2)R(Ï„ )(cid:3). (2) When samples are drawn from behavior policy Âµ instead, importance sampling provides an unbiased correction. Taking the gradient yields the off-policy policy gradient: Î¸J (Î¸) = EÏ„ Âµ [W (Ï„ ) R(Ï„ ) Î¸ log Ï€Î¸(Ï„ )] , (3) where (Ï„ ) = Ï€Î¸(Ï„ )/Âµ(Ï„ ) is the importance weight. This classical policy gradient view (Sutton et al., 1999) reveals that the importance weight serves as gradient weighting factor: it determines how much each sample contributes to the parameter update. More generally, any modification to the importance weight can be understood as defining reshaping function Ï•(W ) that re"
[23.02.2026 06:02] Mistral response. {"id": "021dad259ebc4b368bcd685d7d64cf60", "created": 1771826521, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1578, "total_tokens": 1593, "completion_tokens": 15, "prompt_tokens_details": {"cached_tokens": 0}}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\"Xiaohongshu Inc.\"]\n```"}}]}
[23.02.2026 06:02] Response: ```python
["Xiaohongshu Inc."]
```
[23.02.2026 06:02] Deleting PDF ./assets/pdf/2602.10693.pdf.
[23.02.2026 06:02] Success.
[23.02.2026 06:02] Downloading and parsing paper https://huggingface.co/papers/2602.08354.
[23.02.2026 06:02] Extra JSON file exists (./assets/json/2602.08354.json), skip PDF parsing.
[23.02.2026 06:02] Paper image links file exists (./assets/img_data/2602.08354.json), skip HTML parsing.
[23.02.2026 06:02] Success.
[23.02.2026 06:02] Downloading and parsing paper https://huggingface.co/papers/2602.18422.
[23.02.2026 06:02] Extra JSON file exists (./assets/json/2602.18422.json), skip PDF parsing.
[23.02.2026 06:02] Paper image links file exists (./assets/img_data/2602.18422.json), skip HTML parsing.
[23.02.2026 06:02] Success.
[23.02.2026 06:02] Downloading and parsing paper https://huggingface.co/papers/2602.18071.
[23.02.2026 06:02] Extra JSON file exists (./assets/json/2602.18071.json), skip PDF parsing.
[23.02.2026 06:02] Paper image links file exists (./assets/img_data/2602.18071.json), skip HTML parsing.
[23.02.2026 06:02] Success.
[23.02.2026 06:02] Downloading and parsing paper https://huggingface.co/papers/2602.18432.
[23.02.2026 06:02] Extra JSON file exists (./assets/json/2602.18432.json), skip PDF parsing.
[23.02.2026 06:02] Paper image links file exists (./assets/img_data/2602.18432.json), skip HTML parsing.
[23.02.2026 06:02] Success.
[23.02.2026 06:02] Downloading and parsing paper https://huggingface.co/papers/2602.18312.
[23.02.2026 06:02] Extra JSON file exists (./assets/json/2602.18312.json), skip PDF parsing.
[23.02.2026 06:02] Paper image links file exists (./assets/img_data/2602.18312.json), skip HTML parsing.
[23.02.2026 06:02] Success.
[23.02.2026 06:02] Enriching papers with extra data.
[23.02.2026 06:02] ********************************************************************************
[23.02.2026 06:02] Abstract 0. VESPO addresses training instability in LLM reinforcement learning by using variational formulation with variance reduction to correct policy divergence without length normalization.  					AI-generated summary 				 Training stability remains a central challenge in reinforcement learning (RL) for lar...
[23.02.2026 06:02] ********************************************************************************
[23.02.2026 06:02] Abstract 1. Large reasoning models can implicitly determine optimal stopping points for thinking, which SAGE-RL enhances by incorporating efficient reasoning patterns into pass@1 inference for improved accuracy and efficiency.  					AI-generated summary 				 Recent advancements in large reasoning models (LRMs) ...
[23.02.2026 06:02] ********************************************************************************
[23.02.2026 06:02] Abstract 2. A human-centric video world model conditioned on tracked head and hand poses is introduced, enabling dexterous interactions through a bidirectional video diffusion model trained for egocentric virtual environment generation.  					AI-generated summary 				 Extended reality (XR) demands generative mo...
[23.02.2026 06:02] ********************************************************************************
[23.02.2026 06:02] Abstract 3. EgoPush enables robot manipulation in cluttered environments through perception-driven policy learning that uses object-centric latent spaces and stage-decomposed rewards for long-horizon tasks.  					AI-generated summary 				 Humans can rearrange objects in cluttered environments using egocentric p...
[23.02.2026 06:02] ********************************************************************************
[23.02.2026 06:02] Abstract 4. A causal transformer-based variational autoencoder combined with flow matching enables real-time, spatially-aware conversational motion for embodied agents in virtual reality applications.  					AI-generated summary 				 As embodied agents become central to VR, telepresence, and digital human applic...
[23.02.2026 06:02] ********************************************************************************
[23.02.2026 06:02] Abstract 5. Reinforcement learning policies are improved by using action Jacobian penalty to eliminate unrealistic high-frequency signals, with a new Linear Policy Net architecture reducing computational overhead while enabling faster convergence and efficient inference for motion imitation tasks.  					AI-gene...
[23.02.2026 06:02] Read previous papers.
[23.02.2026 06:02] Generating reviews via LLM API.
[23.02.2026 06:02] Querying the API.
[23.02.2026 06:02] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

VESPO addresses training instability in LLM reinforcement learning by using variational formulation with variance reduction to correct policy divergence without length normalization.  					AI-generated summary 				 Training stability remains a central challenge in reinforcement learning (RL) for large language models (LLMs). Policy staleness, asynchronous training, and mismatches between training and inference engines all cause the behavior policy to diverge from the current policy, risking training collapse. Importance sampling provides a principled correction for this distribution shift but suffers from high variance; existing remedies such as token-level clipping and sequence-level normalization lack a unified theoretical foundation. We propose Variational sEquence-level Soft Policy Optimization (VESPO). By incorporating variance reduction into a variational formulation over proposal distributions, VESPO derives a closed-form reshaping kernel that operates directly on sequence-level importance weights without length normalization. Experiments on mathematical reasoning benchmarks show that VESPO maintains stable training under staleness ratios up to 64x and fully asynchronous execution, and delivers consistent gains across both dense and Mixture-of-Experts models. Code is available at https://github.com/FloyedShen/VESPO
[23.02.2026 06:02] Response: ```json
{
  "desc": "VESPO Ñ€ÐµÑˆÐ°ÐµÑ‚ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ñƒ Ð½ÐµÑÑ‚Ð°Ð±Ð¸Ð»ÑŒÐ½Ð¾ÑÑ‚Ð¸ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ… ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ñ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ Ð¼ÐµÑ‚Ð¾Ð´Ð¾Ð² ÑƒÑÐ¸Ð»ÐµÐ½Ð¸Ñ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ, Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÑ Ð²Ð°Ñ€Ð¸Ð°Ñ†Ð¸Ð¾Ð½Ð½ÑƒÑŽ Ñ„Ð¾Ñ€Ð¼ÑƒÐ»Ð¸Ñ€Ð¾Ð²ÐºÑƒ Ñ Ñ€ÐµÐ´ÑƒÐºÑ†Ð¸ÐµÐ¹ Ð´Ð¸ÑÐ¿ÐµÑ€ÑÐ¸Ð¸. ÐœÐµÑ‚Ð¾Ð´ ÐºÐ¾Ñ€Ñ€ÐµÐºÑ‚Ð¸Ñ€ÑƒÐµÑ‚ Ñ€Ð°ÑÑ…Ð¾Ð¶Ð´ÐµÐ½Ð¸Ðµ Ð¼ÐµÐ¶Ð´Ñƒ Ð¿Ð¾Ð²ÐµÐ´ÐµÐ½Ñ‡ÐµÑÐºÐ¾Ð¹ Ð¿Ð¾Ð»Ð¸Ñ‚Ð¸ÐºÐ¾Ð¹ Ð¸ Ñ‚ÐµÐºÑƒÑ‰ÐµÐ¹ Ð¿Ð¾Ð»Ð¸Ñ‚Ð¸ÐºÐ¾Ð¹ Ð±ÐµÐ· Ð½Ð¾Ñ€Ð¼Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ð¸ Ð¿Ð¾ Ð´Ð»Ð¸Ð½Ðµ Ð¿Ð¾ÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚Ð¸. ÐÐ²Ñ‚Ð¾Ñ€Ñ‹ Ð¿Ñ€Ð¸Ð¼ÐµÐ½ÑÑŽÑ‚ importance sampling Ñ Ð·Ð°Ð¼ÐºÐ½ÑƒÑ‚Ð¾Ð¹ Ñ„Ð¾Ñ€Ð¼Ð¾Ð¹ ÑÐ´Ñ€Ð° Ð¿ÐµÑ€ÐµÑ„Ð¾Ñ€Ð¼ÑƒÐ»Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ, ÐºÐ¾Ñ‚Ð¾Ñ€Ð¾Ðµ Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚ Ð½Ð° ÑƒÑ€Ð¾Ð²Ð½Ðµ Ð¿Ð¾ÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚ÐµÐ¹. Ð­ÐºÑÐ¿ÐµÑ€Ð¸Ð¼ÐµÐ½Ñ‚Ñ‹ Ð¿Ð¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÑŽÑ‚ ÑÑ‚Ð°Ð±Ð¸Ð»ÑŒÐ½Ð¾Ðµ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð¿Ñ€Ð¸ Ð²Ñ‹ÑÐ¾ÐºÐ¸Ñ… ÐºÐ¾ÑÑ„Ñ„Ð¸Ñ†Ð¸ÐµÐ½Ñ‚Ð°Ñ… ÑƒÑÑ‚Ð°Ñ€ÐµÐ²Ð°Ð½Ð¸Ñ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¸ Ð¿Ð¾Ð»Ð½Ð¾ÑÑ‚ÑŒÑŽ Ð°ÑÐ¸Ð½Ñ…Ñ€Ð¾Ð½Ð½Ð¾Ð¼ Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ð¸ Ð½Ð° Ñ€Ð°Ð·Ð»Ð¸Ñ‡Ð½Ñ‹Ñ… Ð°Ñ€Ñ…Ð¸Ñ‚ÐµÐºÑ‚ÑƒÑ€Ð°Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹.",
  "emoji": "âš–ï¸",
  "title": "Ð¡Ñ‚Ð°Ð±Ð¸Ð»ÑŒÐ½Ð°Ñ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð¿Ð¾Ð»Ð¸Ñ‚Ð¸ÐºÐ¸ Ð´Ð»Ñ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ… ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ñ‡ÐµÑ€ÐµÐ· Ð²Ð°Ñ€Ð¸Ð°Ñ†Ð¸Ð¾Ð½Ð½ÑƒÑŽ Ñ€ÐµÐ´ÑƒÐºÑ†Ð¸ÑŽ Ð´Ð¸ÑÐ¿ÐµÑ€ÑÐ¸Ð¸"
}
```
[23.02.2026 06:02] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"VESPO addresses training instability in LLM reinforcement learning by using variational formulation with variance reduction to correct policy divergence without length normalization.  					AI-generated summary 				 Training stability remains a central challenge in reinforcement learning (RL) for large language models (LLMs). Policy staleness, asynchronous training, and mismatches between training and inference engines all cause the behavior policy to diverge from the current policy, risking training collapse. Importance sampling provides a principled correction for this distribution shift but suffers from high variance; existing remedies such as token-level clipping and sequence-level normalization lack a unified theoretical foundation. We propose Variational sEquence-level Soft Policy Optimization (VESPO). By incorporating variance reduction into a variational formulation over proposal distributions, VESPO derives a closed-form reshaping kernel that operates directly on sequence-level importance weights without length normalization. Experiments on mathematical reasoning benchmarks show that VESPO maintains stable training under staleness ratios up to 64x and fully asynchronous execution, and delivers consistent gains across both dense and Mixture-of-Experts models. Code is available at https://github.com/FloyedShen/VESPO"

[23.02.2026 06:02] Response: ```python
["RL", "TRAINING", "MATH"]
```
[23.02.2026 06:02] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"VESPO addresses training instability in LLM reinforcement learning by using variational formulation with variance reduction to correct policy divergence without length normalization.  					AI-generated summary 				 Training stability remains a central challenge in reinforcement learning (RL) for large language models (LLMs). Policy staleness, asynchronous training, and mismatches between training and inference engines all cause the behavior policy to diverge from the current policy, risking training collapse. Importance sampling provides a principled correction for this distribution shift but suffers from high variance; existing remedies such as token-level clipping and sequence-level normalization lack a unified theoretical foundation. We propose Variational sEquence-level Soft Policy Optimization (VESPO). By incorporating variance reduction into a variational formulation over proposal distributions, VESPO derives a closed-form reshaping kernel that operates directly on sequence-level importance weights without length normalization. Experiments on mathematical reasoning benchmarks show that VESPO maintains stable training under staleness ratios up to 64x and fully asynchronous execution, and delivers consistent gains across both dense and Mixture-of-Experts models. Code is available at https://github.com/FloyedShen/VESPO"

[23.02.2026 06:02] Response: ```python
["OPTIMIZATION", "REASONING", "OPEN_SOURCE"]
```
[23.02.2026 06:02] Response: ParsedChatCompletionMessage[~ResponseFormatT](content='{"desc":"VESPO is a method designed to improve the stability of training large language models (LLMs) in reinforcement learning (RL) settings. It addresses issues like policy divergence caused by asynchronous training and mismatches between training and inference. By using a variational approach with variance reduction, VESPO corrects distribution shifts without relying on length normalization. Experiments demonstrate that VESPO can maintain stable training even under extreme conditions and improves performance across various model types.","title":"Stabilizing LLM Training with VESPO"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='VESPO is a method designed to improve the stability of training large language models (LLMs) in reinforcement learning (RL) settings. It addresses issues like policy divergence caused by asynchronous training and mismatches between training and inference. By using a variational approach with variance reduction, VESPO corrects distribution shifts without relying on length normalization. Experiments demonstrate that VESPO can maintain stable training even under extreme conditions and improves performance across various model types.', title='Stabilizing LLM Training with VESPO'))
[23.02.2026 06:02] Response: ParsedChatCompletionMessage[~ResponseFormatT](content='{"desc":"VESPOæ˜¯ä¸€ç§æ–°æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡åž‹ï¼ˆLLMï¼‰åœ¨å¼ºåŒ–å­¦ä¹ ä¸­çš„è®­ç»ƒä¸ç¨³å®šæ€§é—®é¢˜ã€‚å®ƒé€šè¿‡å˜åˆ†å½¢å¼ç»“åˆæ–¹å·®å‡å°‘æŠ€æœ¯ï¼Œæ¥ä¿®æ­£ç­–ç•¥çš„åå·®ï¼Œè€Œä¸éœ€è¦é•¿åº¦å½’ä¸€åŒ–ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿç›´æŽ¥åœ¨åºåˆ—çº§åˆ«çš„é‡è¦æ€§æƒé‡ä¸Šæ“ä½œï¼Œä»Žè€Œæé«˜è®­ç»ƒçš„ç¨³å®šæ€§ã€‚å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒVESPOåœ¨é«˜è¾¾64å€çš„è¿‡æ—¶æ¯”çŽ‡å’Œå®Œå…¨å¼‚æ­¥æ‰§è¡Œä¸‹ï¼Œä»èƒ½ä¿æŒç¨³å®šçš„è®­ç»ƒæ•ˆæžœã€‚","title":"VESPOï¼šç¨³å®šå¤§è¯­è¨€æ¨¡åž‹çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒ"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='VESPOæ˜¯ä¸€ç§æ–°æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡åž‹ï¼ˆLLMï¼‰åœ¨å¼ºåŒ–å­¦ä¹ ä¸­çš„è®­ç»ƒä¸ç¨³å®šæ€§é—®é¢˜ã€‚å®ƒé€šè¿‡å˜åˆ†å½¢å¼ç»“åˆæ–¹å·®å‡å°‘æŠ€æœ¯ï¼Œæ¥ä¿®æ­£ç­–ç•¥çš„åå·®ï¼Œè€Œä¸éœ€è¦é•¿åº¦å½’ä¸€åŒ–ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿç›´æŽ¥åœ¨åºåˆ—çº§åˆ«çš„é‡è¦æ€§æƒé‡ä¸Šæ“ä½œï¼Œä»Žè€Œæé«˜è®­ç»ƒçš„ç¨³å®šæ€§ã€‚å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒVESPOåœ¨é«˜è¾¾64å€çš„è¿‡æ—¶æ¯”çŽ‡å’Œå®Œå…¨å¼‚æ­¥æ‰§è¡Œä¸‹ï¼Œä»èƒ½ä¿æŒç¨³å®šçš„è®­ç»ƒæ•ˆæžœã€‚', title='VESPOï¼šç¨³å®šå¤§è¯­è¨€æ¨¡åž‹çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒ'))
[23.02.2026 06:02] Using data from previous issue: {"categories": ["#training", "#optimization", "#reasoning", "#benchmark", "#rl"], "emoji": "ðŸ§ ", "ru": {"title": "ÐœÐ¾Ð´ÐµÐ»Ð¸ ÑÐ°Ð¼Ð¸ Ð·Ð½Ð°ÑŽÑ‚, ÐºÐ¾Ð³Ð´Ð° Ð¾ÑÑ‚Ð°Ð½Ð¾Ð²Ð¸Ñ‚ÑŒÑÑ: Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð´Ð»Ð¸Ð½Ñ‹ Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ð¹ Ð±ÐµÐ· Ð¿Ð¾Ñ‚ÐµÑ€Ð¸ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð°", "desc": "Ð’ ÑÑ‚Ð°Ñ‚ÑŒÐµ Ð¸ÑÑÐ»ÐµÐ´ÑƒÐµÑ‚ÑÑ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ð° Ð½ÐµÑÑ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ð¾ÑÑ‚Ð¸ Ð´Ð»Ð¸Ð½Ð½Ñ‹Ñ… Ñ†ÐµÐ¿Ð¾Ñ‡ÐµÐº Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ð¹ Ð² Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ… ÑÐ·Ñ‹
[23.02.2026 06:02] Using data from previous issue: {"categories": ["#video", "#architecture", "#training", "#multimodal", "#diffusion", "#3d"], "emoji": "ðŸ¥½", "ru": {"title": "Ð’Ð¸Ð´ÐµÐ¾Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð²Ð¸Ñ€Ñ‚ÑƒÐ°Ð»ÑŒÐ½Ð¾Ð³Ð¾ Ð¼Ð¸Ñ€Ð° Ñ ÑƒÐ¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸ÐµÐ¼ Ñ‡ÐµÑ€ÐµÐ· Ð¿Ð¾Ð·Ñ‹ Ð³Ð¾Ð»Ð¾Ð²Ñ‹ Ð¸ Ñ€ÑƒÐº Ð´Ð»Ñ Ñ€Ð°ÑÑˆÐ¸Ñ€ÐµÐ½Ð½Ð¾Ð¹ Ñ€ÐµÐ°Ð»ÑŒÐ½Ð¾ÑÑ‚Ð¸", "desc": "Ð’ Ñ€Ð°Ð±Ð¾Ñ‚Ðµ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÐµÐ½Ð° Ð²Ð¸Ð´ÐµÐ¾Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð²Ð¸Ñ€Ñ‚ÑƒÐ°Ð»ÑŒÐ½Ð¾Ð³Ð¾ Ð¼Ð¸Ñ€Ð°, Ð¾Ñ€Ð¸ÐµÐ½Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð°Ñ Ð½
[23.02.2026 06:02] Using data from previous issue: {"categories": ["#cv", "#robotics", "#rl"], "emoji": "ðŸ¤–", "ru": {"title": "ÐœÐ°Ð½Ð¸Ð¿ÑƒÐ»ÑÑ†Ð¸Ñ Ñ€Ð¾Ð±Ð¾Ñ‚Ð¾Ð¼ Ð¸Ð· Ð¿ÐµÑ€Ð²Ð¾Ð³Ð¾ Ð»Ð¸Ñ†Ð° Ñ‡ÐµÑ€ÐµÐ· Ð¾Ð±ÑŠÐµÐºÑ‚Ð½Ð¾-Ñ†ÐµÐ½Ñ‚Ñ€Ð¸Ñ‡Ð½Ñ‹Ðµ Ð»Ð°Ñ‚ÐµÐ½Ñ‚Ð½Ñ‹Ðµ Ð¿Ñ€Ð¾ÑÑ‚Ñ€Ð°Ð½ÑÑ‚Ð²Ð°", "desc": "EgoPush â€” ÑÑ‚Ð¾ Ñ„Ñ€ÐµÐ¹Ð¼Ð²Ð¾Ñ€Ðº Ð´Ð»Ñ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ð¿Ð¾Ð»Ð¸Ñ‚Ð¸Ðº Ð¼Ð°Ð½Ð¸Ð¿ÑƒÐ»Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ Ñ€Ð¾Ð±Ð¾Ñ‚Ð¾Ð¼ Ð² Ð·Ð°Ð³Ñ€Ð¾Ð¼Ð¾Ð¶Ð´Ñ‘Ð½Ð½Ñ‹Ñ… ÑÑ†ÐµÐ½Ð°Ñ… Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ ÑÐ³Ð¾Ñ†ÐµÐ½Ñ‚Ñ€Ð¸Ñ‡ÐµÑÐºÐ¾Ð³Ð¾ Ð²Ð¸Ð´ÐµÐ½Ð¸Ñ. 
[23.02.2026 06:02] Using data from previous issue: {"categories": ["#agents", "#dataset", "#video", "#architecture", "#multimodal", "#3d"], "emoji": "ðŸŽ­", "ru": {"title": "ÐŸÑ€Ð¾ÑÑ‚Ñ€Ð°Ð½ÑÑ‚Ð²ÐµÐ½Ð½Ð°Ñ Ð¾ÑÐ²ÐµÐ´Ð¾Ð¼Ð»Ñ‘Ð½Ð½Ð¾ÑÑ‚ÑŒ Ð²Ð¸Ñ€Ñ‚ÑƒÐ°Ð»ÑŒÐ½Ñ‹Ñ… Ð°Ð³ÐµÐ½Ñ‚Ð¾Ð² Ñ‡ÐµÑ€ÐµÐ· Ð¿Ñ€Ð¸Ñ‡Ð¸Ð½Ð½Ñ‹Ðµ Ñ‚Ñ€Ð°Ð½ÑÑ„Ð¾Ñ€Ð¼ÐµÑ€Ñ‹ Ð² Ñ€ÐµÐ°Ð»ÑŒÐ½Ð¾Ð¼ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð¸", "desc": "Ð’ Ñ€Ð°Ð±Ð¾Ñ‚Ðµ Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½ Ð¼ÐµÑ‚Ð¾Ð´ Ð´Ð»Ñ ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ñ Ð¿Ñ€Ð¾ÑÑ‚Ñ€Ð°Ð½ÑÑ‚Ð²ÐµÐ½Ð½Ð¾-Ð¾ÑÐ²ÐµÐ´Ð¾Ð¼Ð»ÐµÐ½Ð½Ñ‹Ñ… 
[23.02.2026 06:02] Using data from previous issue: {"categories": ["#architecture", "#robotics", "#optimization", "#inference", "#rl"], "emoji": "ðŸ¤–", "ru": {"title": "Ð“Ð»Ð°Ð´ÐºÐ¸Ðµ Ð¿Ð¾Ð»Ð¸Ñ‚Ð¸ÐºÐ¸ Ñ‡ÐµÑ€ÐµÐ· ÑˆÑ‚Ñ€Ð°Ñ„ ÑÐºÐ¾Ð±Ð¸Ð°Ð½Ð° Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ð¹", "desc": "Ð’ ÑÑ‚Ð°Ñ‚ÑŒÐµ Ð¿Ñ€ÐµÐ´Ð»Ð°Ð³Ð°ÐµÑ‚ÑÑ Ð¼ÐµÑ‚Ð¾Ð´ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ñ Ð¿Ð¾Ð»Ð¸Ñ‚Ð¸Ðº Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ñ Ð¿Ð¾Ð´ÐºÑ€ÐµÐ¿Ð»ÐµÐ½Ð¸ÐµÐ¼ Ð´Ð»Ñ Ð¸Ð¼Ð¸Ñ‚Ð°Ñ†Ð¸Ð¸ Ð´Ð²Ð¸Ð¶ÐµÐ½Ð¸Ð¹ Ð¿ÑƒÑ‚Ñ‘Ð¼ Ð´Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ð¸Ñ ÑˆÑ‚Ñ€Ð°Ñ„Ð° Ð½Ð° ÑÐºÐ¾Ð±Ð¸Ð°
[23.02.2026 06:02] Renaming data file.
[23.02.2026 06:02] Renaming previous data. hf_papers.json to ./d/2026-02-23.json
[23.02.2026 06:02] Saving new data file.
[23.02.2026 06:02] Generating page.
[23.02.2026 06:02] Renaming previous page.
[23.02.2026 06:02] Renaming previous data. index.html to ./d/2026-02-23.html
[23.02.2026 06:02] Writing result.
[23.02.2026 06:02] Renaming log file.
[23.02.2026 06:02] Renaming previous data. log.txt to ./logs/2026-02-23_last_log.txt
