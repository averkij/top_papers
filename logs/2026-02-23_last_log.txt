[23.02.2026 19:55] Read previous papers.
[23.02.2026 19:55] Generating top page (month).
[23.02.2026 19:55] Writing top page (month).
[23.02.2026 20:37] Read previous papers.
[23.02.2026 20:37] Get feed.
[23.02.2026 20:37] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10693
[23.02.2026 20:37] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08354
[23.02.2026 20:37] Get page data from previous paper. URL: https://huggingface.co/papers/2602.18422
[23.02.2026 20:37] Get page data from previous paper. URL: https://huggingface.co/papers/2602.18292
[23.02.2026 20:37] Get page data from previous paper. URL: https://huggingface.co/papers/2602.15727
[23.02.2026 20:37] Get page data from previous paper. URL: https://huggingface.co/papers/2602.18071
[23.02.2026 20:37] Get page data from previous paper. URL: https://huggingface.co/papers/2602.18432
[23.02.2026 20:37] Get page data from previous paper. URL: https://huggingface.co/papers/2602.17807
[23.02.2026 20:37] Get page data from previous paper. URL: https://huggingface.co/papers/2602.16742
[23.02.2026 20:37] Extract page data from URL. URL: https://huggingface.co/papers/2602.15814
[23.02.2026 20:37] Get page data from previous paper. URL: https://huggingface.co/papers/2602.18312
[23.02.2026 20:37] Get page data from previous paper. URL: https://huggingface.co/papers/2602.17664
[23.02.2026 20:37] Get page data from previous paper. URL: https://huggingface.co/papers/2602.17186
[23.02.2026 20:37] Get page data from previous paper. URL: https://huggingface.co/papers/2602.17080
[23.02.2026 20:37] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10094
[23.02.2026 20:37] Get page data from previous paper. URL: https://huggingface.co/papers/2602.17022
[23.02.2026 20:37] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[23.02.2026 20:37] No deleted papers detected.
[23.02.2026 20:37] Downloading and parsing papers (pdf, html). Total: 16.
[23.02.2026 20:37] Downloading and parsing paper https://huggingface.co/papers/2602.10693.
[23.02.2026 20:37] Extra JSON file exists (./assets/json/2602.10693.json), skip PDF parsing.
[23.02.2026 20:37] Paper image links file exists (./assets/img_data/2602.10693.json), skip HTML parsing.
[23.02.2026 20:37] Success.
[23.02.2026 20:37] Downloading and parsing paper https://huggingface.co/papers/2602.08354.
[23.02.2026 20:37] Extra JSON file exists (./assets/json/2602.08354.json), skip PDF parsing.
[23.02.2026 20:37] Paper image links file exists (./assets/img_data/2602.08354.json), skip HTML parsing.
[23.02.2026 20:37] Success.
[23.02.2026 20:37] Downloading and parsing paper https://huggingface.co/papers/2602.18422.
[23.02.2026 20:37] Extra JSON file exists (./assets/json/2602.18422.json), skip PDF parsing.
[23.02.2026 20:37] Paper image links file exists (./assets/img_data/2602.18422.json), skip HTML parsing.
[23.02.2026 20:37] Success.
[23.02.2026 20:37] Downloading and parsing paper https://huggingface.co/papers/2602.18292.
[23.02.2026 20:37] Extra JSON file exists (./assets/json/2602.18292.json), skip PDF parsing.
[23.02.2026 20:37] Paper image links file exists (./assets/img_data/2602.18292.json), skip HTML parsing.
[23.02.2026 20:37] Success.
[23.02.2026 20:37] Downloading and parsing paper https://huggingface.co/papers/2602.15727.
[23.02.2026 20:37] Extra JSON file exists (./assets/json/2602.15727.json), skip PDF parsing.
[23.02.2026 20:37] Paper image links file exists (./assets/img_data/2602.15727.json), skip HTML parsing.
[23.02.2026 20:37] Success.
[23.02.2026 20:37] Downloading and parsing paper https://huggingface.co/papers/2602.18071.
[23.02.2026 20:37] Extra JSON file exists (./assets/json/2602.18071.json), skip PDF parsing.
[23.02.2026 20:37] Paper image links file exists (./assets/img_data/2602.18071.json), skip HTML parsing.
[23.02.2026 20:37] Success.
[23.02.2026 20:37] Downloading and parsing paper https://huggingface.co/papers/2602.18432.
[23.02.2026 20:37] Extra JSON file exists (./assets/json/2602.18432.json), skip PDF parsing.
[23.02.2026 20:37] Paper image links file exists (./assets/img_data/2602.18432.json), skip HTML parsing.
[23.02.2026 20:37] Success.
[23.02.2026 20:37] Downloading and parsing paper https://huggingface.co/papers/2602.17807.
[23.02.2026 20:37] Extra JSON file exists (./assets/json/2602.17807.json), skip PDF parsing.
[23.02.2026 20:37] Paper image links file exists (./assets/img_data/2602.17807.json), skip HTML parsing.
[23.02.2026 20:37] Success.
[23.02.2026 20:37] Downloading and parsing paper https://huggingface.co/papers/2602.16742.
[23.02.2026 20:37] Extra JSON file exists (./assets/json/2602.16742.json), skip PDF parsing.
[23.02.2026 20:37] Paper image links file exists (./assets/img_data/2602.16742.json), skip HTML parsing.
[23.02.2026 20:37] Success.
[23.02.2026 20:37] Downloading and parsing paper https://huggingface.co/papers/2602.15814.
[23.02.2026 20:37] Downloading paper 2602.15814 from https://arxiv.org/pdf/2602.15814v1...
[23.02.2026 20:37] Extracting affiliations from text.
[23.02.2026 20:37] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 7 1 ] . [ 1 4 1 8 5 1 . 2 0 6 2 : r Published as conference paper at ICLR 2026 AVEY-B Devang Acharya and Mohammad Hammoud Avey AI {dacharya,mhh}@avey.ai "
[23.02.2026 20:37] Response: ```python
["Avey AI"]
```
[23.02.2026 20:37] Deleting PDF ./assets/pdf/2602.15814.pdf.
[23.02.2026 20:38] Success.
[23.02.2026 20:38] Downloading and parsing paper https://huggingface.co/papers/2602.18312.
[23.02.2026 20:38] Extra JSON file exists (./assets/json/2602.18312.json), skip PDF parsing.
[23.02.2026 20:38] Paper image links file exists (./assets/img_data/2602.18312.json), skip HTML parsing.
[23.02.2026 20:38] Success.
[23.02.2026 20:38] Downloading and parsing paper https://huggingface.co/papers/2602.17664.
[23.02.2026 20:38] Extra JSON file exists (./assets/json/2602.17664.json), skip PDF parsing.
[23.02.2026 20:38] Paper image links file exists (./assets/img_data/2602.17664.json), skip HTML parsing.
[23.02.2026 20:38] Success.
[23.02.2026 20:38] Downloading and parsing paper https://huggingface.co/papers/2602.17186.
[23.02.2026 20:38] Extra JSON file exists (./assets/json/2602.17186.json), skip PDF parsing.
[23.02.2026 20:38] Paper image links file exists (./assets/img_data/2602.17186.json), skip HTML parsing.
[23.02.2026 20:38] Success.
[23.02.2026 20:38] Downloading and parsing paper https://huggingface.co/papers/2602.17080.
[23.02.2026 20:38] Extra JSON file exists (./assets/json/2602.17080.json), skip PDF parsing.
[23.02.2026 20:38] Paper image links file exists (./assets/img_data/2602.17080.json), skip HTML parsing.
[23.02.2026 20:38] Success.
[23.02.2026 20:38] Downloading and parsing paper https://huggingface.co/papers/2602.10094.
[23.02.2026 20:38] Extra JSON file exists (./assets/json/2602.10094.json), skip PDF parsing.
[23.02.2026 20:38] Paper image links file exists (./assets/img_data/2602.10094.json), skip HTML parsing.
[23.02.2026 20:38] Success.
[23.02.2026 20:38] Downloading and parsing paper https://huggingface.co/papers/2602.17022.
[23.02.2026 20:38] Extra JSON file exists (./assets/json/2602.17022.json), skip PDF parsing.
[23.02.2026 20:38] Paper image links file exists (./assets/img_data/2602.17022.json), skip HTML parsing.
[23.02.2026 20:38] Success.
[23.02.2026 20:38] Enriching papers with extra data.
[23.02.2026 20:38] ********************************************************************************
[23.02.2026 20:38] Abstract 0. VESPO addresses training instability in LLM reinforcement learning by using variational formulation with variance reduction to correct policy divergence without length normalization.  					AI-generated summary 				 Training stability remains a central challenge in reinforcement learning (RL) for lar...
[23.02.2026 20:38] ********************************************************************************
[23.02.2026 20:38] Abstract 1. Large reasoning models can implicitly determine optimal stopping points for thinking, which SAGE-RL enhances by incorporating efficient reasoning patterns into pass@1 inference for improved accuracy and efficiency.  					AI-generated summary 				 Recent advancements in large reasoning models (LRMs) ...
[23.02.2026 20:38] ********************************************************************************
[23.02.2026 20:38] Abstract 2. A human-centric video world model conditioned on tracked head and hand poses is introduced, enabling dexterous interactions through a bidirectional video diffusion model trained for egocentric virtual environment generation.  					AI-generated summary 				 Extended reality (XR) demands generative mo...
[23.02.2026 20:38] ********************************************************************************
[23.02.2026 20:38] Abstract 3. Abstract Decoding is reinterpreted as a principled optimization layer that balances model scores with structural preferences, recovering existing methods as special cases and enabling the creation of new decoders like Best-of-K that improve accuracy in mathematical reasoning tasks.  					AI-generate...
[23.02.2026 20:38] ********************************************************************************
[23.02.2026 20:38] Abstract 4. Abstract Visual analogy learning via dynamic composition of learned LoRA transformation primitives enables flexible image manipulation with improved generalization over fixed adaptation modules.  					AI-generated summary Visual analogy learning enables image manipulation through demonstration rathe...
[23.02.2026 20:38] ********************************************************************************
[23.02.2026 20:38] Abstract 5. EgoPush enables robot manipulation in cluttered environments through perception-driven policy learning that uses object-centric latent spaces and stage-decomposed rewards for long-horizon tasks.  					AI-generated summary 				 Humans can rearrange objects in cluttered environments using egocentric p...
[23.02.2026 20:38] ********************************************************************************
[23.02.2026 20:38] Abstract 6. A causal transformer-based variational autoencoder combined with flow matching enables real-time, spatially-aware conversational motion for embodied agents in virtual reality applications.  					AI-generated summary 				 As embodied agents become central to VR, telepresence, and digital human applic...
[23.02.2026 20:38] ********************************************************************************
[23.02.2026 20:38] Abstract 7. Abstract A video segmentation model eliminates specialized tracking modules by using a Vision Transformer encoder with query propagation and fusion mechanisms for efficient, high-speed processing.  					AI-generated summary Existing online video segmentation models typically combine a per-frame segm...
[23.02.2026 20:38] ********************************************************************************
[23.02.2026 20:38] Abstract 8. DeepVision-103K dataset enhances multimodal reasoning capabilities of large models through diverse mathematical content and visual elements.  					AI-generated summary 				 Reinforcement Learning with Verifiable Rewards (RLVR) has been shown effective in enhancing the visual reflection and reasoning...
[23.02.2026 20:38] ********************************************************************************
[23.02.2026 20:38] Abstract 9. Abstract Compact pretrained bidirectional encoders based on Avey architecture outperform Transformer-based models on token classification and information retrieval tasks while scaling more efficiently to long contexts.  					AI-generated summary Compact pretrained bidirectional encoders remain the b...
[23.02.2026 20:38] ********************************************************************************
[23.02.2026 20:38] Abstract 10. Reinforcement learning policies are improved by using action Jacobian penalty to eliminate unrealistic high-frequency signals, with a new Linear Policy Net architecture reducing computational overhead while enabling faster convergence and efficient inference for motion imitation tasks.  					AI-gene...
[23.02.2026 20:38] ********************************************************************************
[23.02.2026 20:38] Abstract 11. Abstract Diffusion Language Models suffer from high inference costs due to iterative denoising, prompting the development of Sink-Aware Pruning that identifies and removes unstable attention sinks, improving efficiency without retraining.  					AI-generated summary Diffusion Language Models (DLMs) i...
[23.02.2026 20:38] ********************************************************************************
[23.02.2026 20:38] Abstract 12. Visual Information Gain metric quantifies the contribution of visual input to prediction uncertainty, enabling selective training that improves visual grounding and reduces language bias in vision-language models.  					AI-generated summary 				 Large Vision Language Models (LVLMs) have achieved rem...
[23.02.2026 20:38] ********************************************************************************
[23.02.2026 20:38] Abstract 13. Abstract A new class of optimizers combines orthogonalized momentum with norm-based noise adaptation, achieving improved convergence rates and training performance for large language models.  					AI-generated summary Efficient stochastic optimization typically integrates an update direction that pe...
[23.02.2026 20:38] ********************************************************************************
[23.02.2026 20:38] Abstract 14. Abstract 4RC presents a unified feed-forward framework for 4D reconstruction from monocular videos that learns holistic scene geometry and motion dynamics through a transformer-based encoder-decoder architecture with conditional querying capabilities.  					AI-generated summary We present 4RC, a uni...
[23.02.2026 20:38] ********************************************************************************
[23.02.2026 20:38] Abstract 15. Abstract Conversational agents with tool integration face challenges from user-induced errors, but a test-time intervention method called Reasoning Inception (ReIn) enables error recovery by injecting external reasoning into the agent's decision-making process without modifying model parameters or p...
[23.02.2026 20:38] Read previous papers.
[23.02.2026 20:38] Generating reviews via LLM API.
[23.02.2026 20:38] Using data from previous issue: {"categories": ["#rl", "#training", "#open_source", "#reasoning", "#math", "#optimization"], "emoji": "‚öñÔ∏è", "ru": {"title": "–°—Ç–∞–±–∏–ª—å–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–æ–ª–∏—Ç–∏–∫–∏ –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω—É—é —Ä–µ–¥—É–∫—Ü–∏—é –¥–∏—Å–ø–µ—Ä—Å–∏–∏", "desc": "VESPO —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ
[23.02.2026 20:38] Using data from previous issue: {"categories": ["#training", "#optimization", "#reasoning", "#benchmark", "#rl"], "emoji": "üß†", "ru": {"title": "–ú–æ–¥–µ–ª–∏ —Å–∞–º–∏ –∑–Ω–∞—é—Ç, –∫–æ–≥–¥–∞ –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å—Å—è: –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª–∏–Ω—ã —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞", "desc": "–í —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ –Ω–µ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –¥–ª–∏–Ω–Ω—ã—Ö —Ü–µ–ø–æ—á–µ–∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã
[23.02.2026 20:38] Using data from previous issue: {"categories": ["#video", "#architecture", "#training", "#multimodal", "#diffusion", "#3d"], "emoji": "ü•Ω", "ru": {"title": "–í–∏–¥–µ–æ–º–æ–¥–µ–ª—å –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–≥–æ –º–∏—Ä–∞ —Å —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º —á–µ—Ä–µ–∑ –ø–æ–∑—ã –≥–æ–ª–æ–≤—ã –∏ —Ä—É–∫ –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –≤–∏–¥–µ–æ–º–æ–¥–µ–ª—å –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–≥–æ –º–∏—Ä–∞, –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –Ω
[23.02.2026 20:38] Using data from previous issue: {"categories": ["#inference", "#math", "#reasoning", "#optimization"], "emoji": "üéØ", "ru": {"title": "–î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞–∫ –ø—Ä–∏–Ω—Ü–∏–ø–∏–∞–ª—å–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è: –µ–¥–∏–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –Ω–æ–≤—ã—Ö –¥–µcoders", "desc": "–í —Å—Ç–∞—Ç—å–µ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–µ—Ä–µ–æ—Å–º—ã—Å–ª—è–µ—Ç—Å—è –∫–∞–∫ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π —Å–ª–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏, –∫–æ—Ç–æ—Ä—ã–π –±–∞–ª–∞–Ω—Å–∏—Ä—É–µ—Ç –º–µ–∂–¥—É –æ—Ü–µ–Ω–∫
[23.02.2026 20:38] Using data from previous issue: {"categories": [], "emoji": "üé®", "ru": {"title": "–î–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è –∫–æ–º–ø–æ–∑–∏—Ü–∏—è LoRA –ø—Ä–∏–º–∏—Ç–∏–≤–æ–≤ –¥–ª—è –≥–∏–±–∫–æ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ —á–µ—Ä–µ–∑ –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –∞–Ω–∞–ª–æ–≥–∏–∏", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ LoRWeB –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –≤–∏–∑—É–∞–ª—å–Ω—ã–º –∞–Ω–∞–ª–æ–≥–∏—è–º, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–∞–Ω–∏–ø—É–ª–∏—Ä–æ–≤–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ
[23.02.2026 20:38] Using data from previous issue: {"categories": ["#cv", "#robotics", "#rl"], "emoji": "ü§ñ", "ru": {"title": "–ú–∞–Ω–∏–ø—É–ª—è—Ü–∏—è —Ä–æ–±–æ—Ç–æ–º –∏–∑ –ø–µ—Ä–≤–æ–≥–æ –ª–∏—Ü–∞ —á–µ—Ä–µ–∑ –æ–±—ä–µ–∫—Ç–Ω–æ-—Ü–µ–Ω—Ç—Ä–∏—á–Ω—ã–µ –ª–∞—Ç–µ–Ω—Ç–Ω—ã–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞", "desc": "EgoPush ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –ø–æ–ª–∏—Ç–∏–∫ –º–∞–Ω–∏–ø—É–ª–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–æ–±–æ—Ç–æ–º –≤ –∑–∞–≥—Ä–æ–º–æ–∂–¥—ë–Ω–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ö –Ω–∞ –æ—Å–Ω–æ–≤–µ —ç–≥–æ—Ü–µ–Ω—Ç—Ä–∏—á–µ—Å–∫–æ–≥–æ –≤–∏–¥–µ–Ω–∏—è. 
[23.02.2026 20:38] Using data from previous issue: {"categories": ["#agents", "#dataset", "#video", "#architecture", "#multimodal", "#3d"], "emoji": "üé≠", "ru": {"title": "–ü—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–∞—è –æ—Å–≤–µ–¥–æ–º–ª—ë–Ω–Ω–æ—Å—Ç—å –≤–∏—Ä—Ç—É–∞–ª—å–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ –ø—Ä–∏—á–∏–Ω–Ω—ã–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ-–æ—Å–≤–µ–¥–æ–º–ª–µ–Ω–Ω—ã—Ö 
[23.02.2026 20:38] Using data from previous issue: {"categories": ["#cv", "#video", "#architecture"], "emoji": "üé¨", "ru": {"title": "–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä –±–µ–∑ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è: –±—ã—Å—Ç—Ä–∞—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –≤–∏–¥–µ–æ —á–µ—Ä–µ–∑ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–æ–≤", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ —É–ø—Ä–æ—â—ë–Ω–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ Vision Transformer –±–µ–∑ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–æ–¥
[23.02.2026 20:38] Using data from previous issue: {"categories": ["#rl", "#benchmark", "#multimodal", "#dataset"], "emoji": "üìä", "ru": {"title": "–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è —á–µ—Ä–µ–∑ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç —Å –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–∞—è –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö DeepVision-103K, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö –º—É
[23.02.2026 20:38] Querying the API.
[23.02.2026 20:38] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Abstract Compact pretrained bidirectional encoders based on Avey architecture outperform Transformer-based models on token classification and information retrieval tasks while scaling more efficiently to long contexts.  					AI-generated summary Compact pretrained bidirectional encoders remain the backbone of industrial NLP under tight compute and memory budgets. Their effectiveness stems from self-attention's ability to deliver high-quality bidirectional contextualization with sequence-level parallelism, as popularized by BERT-style architectures. Recently, Avey was introduced as an autoregressive, attention-free alternative that naturally admits an encoder-only adaptation. In this paper, we reformulate Avey for the encoder-only paradigm and propose several innovations to its architecture, including decoupled static and dynamic parameterizations, stability-oriented normalization, and neural compression. Results show that this reformulated architecture compares favorably to four widely used Transformer-based encoders, consistently outperforming them on standard token-classification and information-retrieval benchmarks while scaling more efficiently to long contexts.
[23.02.2026 20:38] Response: ```json
{
  "desc": "–í —Ä–∞–±–æ—Ç–µ –∞–≤—Ç–æ—Ä—ã –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä—É—é—Ç –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É Avey –¥–ª—è —Ä–µ–∂–∏–º–∞ –∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–∞ –±–µ–∑ –º–µ—Ö–∞–Ω–∏–∑–º–∞ –≤–Ω–∏–º–∞–Ω–∏—è, –ø—Ä–µ–¥–ª–æ–∂–∏–≤ –Ω–µ—Å–∫–æ–ª—å–∫–æ —É–ª—É—á—à–µ–Ω–∏–π. –û–Ω–∏ –≤–≤–æ–¥—è—Ç —Ä–∞–∑–≤—è–∑–∞–Ω–Ω—É—é –ø–∞—Ä–∞–º–µ—Ç—Ä–∏–∑–∞—Ü–∏—é, –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –∏ –Ω–µ–π—Ä–æ–Ω–Ω–æ–µ —Å–∂–∞—Ç–∏–µ, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∫–æ–º–ø–∞–∫—Ç–Ω–æ–º—É —ç–Ω–∫–æ–¥–µ—Ä—É —Ä–∞–±–æ—Ç–∞—Ç—å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä-Âü∫–º–æ–¥–µ–ª–∏ –Ω–∞ –∑–∞–¥–∞—á–∞—Ö –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤ –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞. –ì–ª–∞–≤–Ω–æ–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ - –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –¥–ª–∏–Ω–Ω—ã–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–∏ –º–µ–Ω—å—à–µ–º –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤.",
  "emoji": "‚ö°",
  "title": "–ö–æ–º–ø–∞–∫—Ç–Ω—ã–π —ç–Ω–∫–æ–¥–µ—Ä –±–µ–∑ –≤–Ω–∏–º–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π –±—ã—Å—Ç—Ä–µ–µ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤"
}
```
[23.02.2026 20:38] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Abstract Compact pretrained bidirectional encoders based on Avey architecture outperform Transformer-based models on token classification and information retrieval tasks while scaling more efficiently to long contexts.  					AI-generated summary Compact pretrained bidirectional encoders remain the backbone of industrial NLP under tight compute and memory budgets. Their effectiveness stems from self-attention's ability to deliver high-quality bidirectional contextualization with sequence-level parallelism, as popularized by BERT-style architectures. Recently, Avey was introduced as an autoregressive, attention-free alternative that naturally admits an encoder-only adaptation. In this paper, we reformulate Avey for the encoder-only paradigm and propose several innovations to its architecture, including decoupled static and dynamic parameterizations, stability-oriented normalization, and neural compression. Results show that this reformulated architecture compares favorably to four widely used Transformer-based encoders, consistently outperforming them on standard token-classification and information-retrieval benchmarks while scaling more efficiently to long contexts."

[23.02.2026 20:38] Response: ```python
["ARCHITECTURE", "BENCHMARK", "SMALL_MODELS"]
```
[23.02.2026 20:38] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Abstract Compact pretrained bidirectional encoders based on Avey architecture outperform Transformer-based models on token classification and information retrieval tasks while scaling more efficiently to long contexts.  					AI-generated summary Compact pretrained bidirectional encoders remain the backbone of industrial NLP under tight compute and memory budgets. Their effectiveness stems from self-attention's ability to deliver high-quality bidirectional contextualization with sequence-level parallelism, as popularized by BERT-style architectures. Recently, Avey was introduced as an autoregressive, attention-free alternative that naturally admits an encoder-only adaptation. In this paper, we reformulate Avey for the encoder-only paradigm and propose several innovations to its architecture, including decoupled static and dynamic parameterizations, stability-oriented normalization, and neural compression. Results show that this reformulated architecture compares favorably to four widely used Transformer-based encoders, consistently outperforming them on standard token-classification and information-retrieval benchmarks while scaling more efficiently to long contexts."

[23.02.2026 20:38] Response: ```python
['LONG_CONTEXT', 'OPTIMIZATION']
```

**Justification:**

- **LONG_CONTEXT**: The paper explicitly discusses "scaling more efficiently to long contexts" and mentions the ability to handle long contexts as a key advantage of their proposed architecture.

- **OPTIMIZATION**: The paper focuses on creating "compact pretrained bidirectional encoders" that work "under tight compute and memory budgets" and discusses architectural innovations for efficiency, which relates to training and inference optimization.
[23.02.2026 20:38] Error. Failed to parse JSON from LLM. ["LONG_CONTEXT", "OPTIMIZATION"]


**Justification:**

- **LONG_CONTEXT**: The paper explicitly discusses "scaling more efficiently to long contexts" and mentions the ability to handle long contexts as a key advantage of their proposed architecture.

- **OPTIMIZATION**: The paper focuses on creating "compact pretrained bidirectional encoders" that work "under tight compute and memory budgets" and discusses architectural innovations for efficiency, which relates to training and inference optimization.
[23.02.2026 20:38] Response: ParsedChatCompletionMessage[~ResponseFormatT](content='{"desc":"This paper presents a new architecture called Avey, which is designed for token classification and information retrieval tasks in natural language processing (NLP). Avey is a compact, pretrained bidirectional encoder that operates without the traditional self-attention mechanism found in Transformer models, allowing for more efficient scaling to longer text inputs. The authors introduce several enhancements to Avey, such as separating static and dynamic parameters and implementing stability-oriented normalization techniques. Experimental results demonstrate that Avey outperforms popular Transformer-based models on various benchmarks while being more efficient in terms of compute and memory usage.","title":"Avey: Efficient Bidirectional Encoding for NLP Tasks"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper presents a new architecture called Avey, which is designed for token classification and information retrieval tasks in natural language processing (NLP). Avey is a compact, pretrained bidirectional encoder that operates without the traditional self-attention mechanism found in Transformer models, allowing for more efficient scaling to longer text inputs. The authors introduce several enhancements to Avey, such as separating static and dynamic parameters and implementing stability-oriented normalization techniques. Experimental results demonstrate that Avey outperforms popular Transformer-based models on various benchmarks while being more efficient in terms of compute and memory usage.', title='Avey: Efficient Bidirectional Encoding for NLP Tasks'))
[23.02.2026 20:38] Response: ParsedChatCompletionMessage[~ResponseFormatT](content='{"desc":"Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÂü∫‰∫éAveyÊû∂ÊûÑÁöÑÁ¥ßÂáëÂûãÈ¢ÑËÆ≠ÁªÉÂèåÂêëÁºñÁ†ÅÂô®ÔºåËØ•ÁºñÁ†ÅÂô®Âú®Ê†áËÆ∞ÂàÜÁ±ªÂíå‰ø°ÊÅØÊ£ÄÁ¥¢‰ªªÂä°‰∏≠‰ºò‰∫éÂü∫‰∫éTransformerÁöÑÊ®°ÂûãÔºåÂπ∂‰∏îÂú®Â§ÑÁêÜÈïø‰∏ä‰∏ãÊñáÊó∂Êõ¥ÂÖ∑ÊïàÁéá„ÄÇAveyÊû∂ÊûÑÊòØ‰∏ÄÁßçËá™ÂõûÂΩí„ÄÅÊó†Ê≥®ÊÑèÂäõÁöÑÊõø‰ª£ÊñπÊ°àÔºåÈÄÇÂêà‰∫é‰ªÖ‰ΩøÁî®ÁºñÁ†ÅÂô®ÁöÑÈÄÇÈÖç„ÄÇÊàë‰ª¨ÂØπAveyËøõË°å‰∫ÜÈáçÊñ∞ÊûÑÂª∫ÔºåÊèêÂá∫‰∫ÜÂ§öÈ°πÂàõÊñ∞ÔºåÂåÖÊã¨Ëß£ËÄ¶ÁöÑÈùôÊÄÅÂíåÂä®ÊÄÅÂèÇÊï∞Âåñ„ÄÅÁ®≥ÂÆöÊÄßÂØºÂêëÁöÑÂΩí‰∏ÄÂåñ‰ª•ÂèäÁ•ûÁªèÂéãÁº©„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÈáçÊñ∞ÊûÑÂª∫ÁöÑÊû∂ÊûÑÂú®Ê†áÂáÜÁöÑÊ†áËÆ∞ÂàÜÁ±ªÂíå‰ø°ÊÅØÊ£ÄÁ¥¢Âü∫ÂáÜÊµãËØï‰∏≠ÔºåÂßãÁªà‰ºò‰∫éÂõõÁßçÂπøÊ≥õ‰ΩøÁî®ÁöÑÂü∫‰∫éTransformerÁöÑÁºñÁ†ÅÂô®„ÄÇ","title":"AveyÊû∂ÊûÑÔºöÈ´òÊïàÁöÑÂèåÂêëÁºñÁ†ÅÂô®Êñ∞ÈÄâÊã©"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÂü∫‰∫éAveyÊû∂ÊûÑÁöÑÁ¥ßÂáëÂûãÈ¢ÑËÆ≠ÁªÉÂèåÂêëÁºñÁ†ÅÂô®ÔºåËØ•ÁºñÁ†ÅÂô®Âú®Ê†áËÆ∞ÂàÜÁ±ªÂíå‰ø°ÊÅØÊ£ÄÁ¥¢‰ªªÂä°‰∏≠‰ºò‰∫éÂü∫‰∫éTransformerÁöÑÊ®°ÂûãÔºåÂπ∂‰∏îÂú®Â§ÑÁêÜÈïø‰∏ä‰∏ãÊñáÊó∂Êõ¥ÂÖ∑ÊïàÁéá„ÄÇAveyÊû∂ÊûÑÊòØ‰∏ÄÁßçËá™ÂõûÂΩí„ÄÅÊó†Ê≥®ÊÑèÂäõÁöÑÊõø‰ª£ÊñπÊ°àÔºåÈÄÇÂêà‰∫é‰ªÖ‰ΩøÁî®ÁºñÁ†ÅÂô®ÁöÑÈÄÇÈÖç„ÄÇÊàë‰ª¨ÂØπAveyËøõË°å‰∫ÜÈáçÊñ∞ÊûÑÂª∫ÔºåÊèêÂá∫‰∫ÜÂ§öÈ°πÂàõÊñ∞ÔºåÂåÖÊã¨Ëß£ËÄ¶ÁöÑÈùôÊÄÅÂíåÂä®ÊÄÅÂèÇÊï∞Âåñ„ÄÅÁ®≥ÂÆöÊÄßÂØºÂêëÁöÑÂΩí‰∏ÄÂåñ‰ª•ÂèäÁ•ûÁªèÂéãÁº©„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÈáçÊñ∞ÊûÑÂª∫ÁöÑÊû∂ÊûÑÂú®Ê†áÂáÜÁöÑÊ†áËÆ∞ÂàÜÁ±ªÂíå‰ø°ÊÅØÊ£ÄÁ¥¢Âü∫ÂáÜÊµãËØï‰∏≠ÔºåÂßãÁªà‰ºò‰∫éÂõõÁßçÂπøÊ≥õ‰ΩøÁî®ÁöÑÂü∫‰∫éTransformerÁöÑÁºñÁ†ÅÂô®„ÄÇ', title='AveyÊû∂ÊûÑÔºöÈ´òÊïàÁöÑÂèåÂêëÁºñÁ†ÅÂô®Êñ∞ÈÄâÊã©'))
[23.02.2026 20:38] Using data from previous issue: {"categories": ["#architecture", "#robotics", "#optimization", "#inference", "#rl"], "emoji": "ü§ñ", "ru": {"title": "–ì–ª–∞–¥–∫–∏–µ –ø–æ–ª–∏—Ç–∏–∫–∏ —á–µ—Ä–µ–∑ —à—Ç—Ä–∞—Ñ —è–∫–æ–±–∏–∞–Ω–∞ –¥–µ–π—Å—Ç–≤–∏–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –º–µ—Ç–æ–¥ —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–ª–∏—Ç–∏–∫ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –∏–º–∏—Ç–∞—Ü–∏–∏ –¥–≤–∏–∂–µ–Ω–∏–π –ø—É—Ç—ë–º –¥–æ–±–∞–≤–ª–µ–Ω–∏—è —à—Ç—Ä–∞—Ñ–∞ –Ω–∞ —è–∫–æ–±–∏–∞
[23.02.2026 20:38] Using data from previous issue: {"categories": ["#open_source", "#diffusion", "#optimization"], "emoji": "‚úÇÔ∏è", "ru": {"title": "–£–º–Ω–æ–µ —É–¥–∞–ª–µ–Ω–∏–µ –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω—ã—Ö —É–∑–ª–æ–≤ –≤ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –≤—ã–≤–æ–¥–∞", "desc": "–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–µ —Ç—Ä–µ–±—É—é—Ç –≤—ã—Å–æ–∫–∏—Ö –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–∞—Ç—Ä–∞—Ç –∏–∑-–∑–∞ –∏—Ç–µ
[23.02.2026 20:38] Using data from previous issue: {"categories": [], "emoji": "üëÅÔ∏è", "ru": {"title": "–ò–∑–º–µ—Ä—è–µ–º –≤–∫–ª–∞–¥ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: –∏–∑–±–∏—Ä–∞—Ç–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –∑—Ä–µ–Ω–∏—è –∏ —è–∑—ã–∫–∞", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –º–µ—Ç—Ä–∏–∫–∞ Visual Information Gain (VIG), –∫–æ—Ç–æ—Ä–∞—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ–Ω–Ω–æ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç, –Ω–∞—Å–∫–æ–ª—å–∫–æ –≤–∏–∑—É–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è —Å–Ω–∏–∂–∞–µ—Ç –Ω–µ–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ—Å—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π 
[23.02.2026 20:38] Using data from previous issue: {"categories": ["#training", "#optimization", "#architecture"], "emoji": "‚ö°", "ru": {"title": "–û—Ä—Ç–æ–≥–æ–Ω–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π –º–æ–º–µ–Ω—Ç –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è —Å –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–º —à—É–º–æ–º", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω—ã –Ω–æ–≤—ã–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä—ã NAMO –∏ NAMO-D, –∫–æ—Ç–æ—Ä—ã–µ –æ–±—ä–µ–¥–∏–Ω—è—é—Ç –æ—Ä—Ç–æ–≥–æ–Ω–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π –º–æ–º–µ–Ω—Ç —Å –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–π –ø–æ–¥—Å—Ç—Ä–æ–π–∫–æ–π –ø–æ–¥ —É—Ä–æ–≤–µ–Ω—å —à
[23.02.2026 20:38] Using data from previous issue: {"categories": ["#video", "#3d", "#architecture"], "emoji": "üé¨", "ru": {"title": "–ï–¥–∏–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π –≥–µ–æ–º–µ—Ç—Ä–∏–∏ —Å—Ü–µ–Ω—ã –∏–∑ –≤–∏–¥–µ–æ", "desc": "4RC –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –µ–¥–∏–Ω—É—é –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –¥–ª—è 4D —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –∏–∑ –º–æ–Ω–æ–∫—É–ª—è—Ä–Ω—ã—Ö –≤–∏–¥–µ–æ, –∫–æ—Ç–æ—Ä–∞—è –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –∏–∑—É—á–∞–µ—Ç –≥–µ–æ–º–µ—Ç—Ä–∏—é
[23.02.2026 20:38] Using data from previous issue: {"categories": ["#reasoning", "#agents", "#alignment", "#training"], "emoji": "üîß", "ru": {"title": "–í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –æ—à–∏–±–æ–∫ –≤ –¥–∏–∞–ª–æ–≥–æ–≤—ã—Ö –∞–≥–µ–Ω—Ç–∞—Ö –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏", "desc": "–†–∞–±–æ—Ç–∞ –æ–ø–∏—Å—ã–≤–∞–µ—Ç –º–µ—Ç–æ–¥ Reasoning Inception (ReIn) –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –¥–∏–∞–ª–æ–≥–æ–≤—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ –æ—Ç –æ—à–∏–±–æ–∫ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –≤–æ –≤—Ä–µ–º—è –∏–Ω
[23.02.2026 20:38] Renaming data file.
[23.02.2026 20:38] Renaming previous data. hf_papers.json to ./d/2026-02-23.json
[23.02.2026 20:38] Saving new data file.
[23.02.2026 20:38] Generating page.
[23.02.2026 20:38] Renaming previous page.
[23.02.2026 20:38] Renaming previous data. index.html to ./d/2026-02-23.html
[23.02.2026 20:38] Writing result.
[23.02.2026 20:38] Renaming log file.
[23.02.2026 20:38] Renaming previous data. log.txt to ./logs/2026-02-23_last_log.txt
