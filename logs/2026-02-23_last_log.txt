[23.02.2026 12:44] Read previous papers.
[23.02.2026 12:44] Generating top page (month).
[23.02.2026 12:44] Writing top page (month).
[23.02.2026 14:04] Read previous papers.
[23.02.2026 14:04] Get feed.
[23.02.2026 14:04] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10693
[23.02.2026 14:04] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08354
[23.02.2026 14:04] Get page data from previous paper. URL: https://huggingface.co/papers/2602.18422
[23.02.2026 14:04] Get page data from previous paper. URL: https://huggingface.co/papers/2602.18071
[23.02.2026 14:04] Get page data from previous paper. URL: https://huggingface.co/papers/2602.18432
[23.02.2026 14:04] Extract page data from URL. URL: https://huggingface.co/papers/2602.17807
[23.02.2026 14:04] Extract page data from URL. URL: https://huggingface.co/papers/2602.17664
[23.02.2026 14:04] Get page data from previous paper. URL: https://huggingface.co/papers/2602.17186
[23.02.2026 14:04] Get page data from previous paper. URL: https://huggingface.co/papers/2602.16742
[23.02.2026 14:04] Get page data from previous paper. URL: https://huggingface.co/papers/2602.18312
[23.02.2026 14:04] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[23.02.2026 14:04] No deleted papers detected.
[23.02.2026 14:04] Downloading and parsing papers (pdf, html). Total: 10.
[23.02.2026 14:04] Downloading and parsing paper https://huggingface.co/papers/2602.10693.
[23.02.2026 14:04] Extra JSON file exists (./assets/json/2602.10693.json), skip PDF parsing.
[23.02.2026 14:04] Paper image links file exists (./assets/img_data/2602.10693.json), skip HTML parsing.
[23.02.2026 14:04] Success.
[23.02.2026 14:04] Downloading and parsing paper https://huggingface.co/papers/2602.08354.
[23.02.2026 14:04] Extra JSON file exists (./assets/json/2602.08354.json), skip PDF parsing.
[23.02.2026 14:04] Paper image links file exists (./assets/img_data/2602.08354.json), skip HTML parsing.
[23.02.2026 14:04] Success.
[23.02.2026 14:04] Downloading and parsing paper https://huggingface.co/papers/2602.18422.
[23.02.2026 14:04] Extra JSON file exists (./assets/json/2602.18422.json), skip PDF parsing.
[23.02.2026 14:04] Paper image links file exists (./assets/img_data/2602.18422.json), skip HTML parsing.
[23.02.2026 14:04] Success.
[23.02.2026 14:04] Downloading and parsing paper https://huggingface.co/papers/2602.18071.
[23.02.2026 14:04] Extra JSON file exists (./assets/json/2602.18071.json), skip PDF parsing.
[23.02.2026 14:04] Paper image links file exists (./assets/img_data/2602.18071.json), skip HTML parsing.
[23.02.2026 14:04] Success.
[23.02.2026 14:04] Downloading and parsing paper https://huggingface.co/papers/2602.18432.
[23.02.2026 14:04] Extra JSON file exists (./assets/json/2602.18432.json), skip PDF parsing.
[23.02.2026 14:04] Paper image links file exists (./assets/img_data/2602.18432.json), skip HTML parsing.
[23.02.2026 14:04] Success.
[23.02.2026 14:04] Downloading and parsing paper https://huggingface.co/papers/2602.17807.
[23.02.2026 14:04] Downloading paper 2602.17807 from https://arxiv.org/pdf/2602.17807v1...
[23.02.2026 14:04] Extracting affiliations from text.
[23.02.2026 14:04] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"VidEoMT: Your ViT is Secretly Also Video Segmentation Model Narges Norouzi1 Idil Esen Zulfikar2, Niccol`o Cavagnero1, Tommie Kerssies1 Bastian Leibe Gijs Dubbelman1 Daan de Geus1 1Eindhoven University of Technology 2RWTH Aachen University 6 2 0 2 9 ] . [ 1 7 0 8 7 1 . 2 0 6 2 : r a "
[23.02.2026 14:04] Failed to download and parse paper https://huggingface.co/papers/2602.17807: Error code: 529 - {'type': 'error', 'error': {'type': 'overloaded_error', 'message': 'Overloaded'}, 'request_id': 'req_011CYR8LNHBkUduzbrpU2J8m'}
[23.02.2026 14:04] Downloading and parsing paper https://huggingface.co/papers/2602.17664.
[23.02.2026 14:04] Downloading paper 2602.17664 from https://arxiv.org/pdf/2602.17664v1...
[23.02.2026 14:04] Extracting affiliations from text.
[23.02.2026 14:04] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Sink-Aware Pruning for Diffusion Language Models Aidar Myrzakhan, Tianyi Li, Bowei Guo, Shengkun Tang, Zhiqiang Shen VILA Lab, MBZUAI 6 2 0 2 9 1 ] . [ 1 4 6 6 7 1 . 2 0 6 2 : r a "
[23.02.2026 14:04] Failed to download and parse paper https://huggingface.co/papers/2602.17664: Error code: 529 - {'type': 'error', 'error': {'type': 'overloaded_error', 'message': 'Overloaded'}, 'request_id': 'req_011CYR8M9osKECRK9c9FX8ks'}
[23.02.2026 14:04] Downloading and parsing paper https://huggingface.co/papers/2602.17186.
[23.02.2026 14:04] Extra JSON file exists (./assets/json/2602.17186.json), skip PDF parsing.
[23.02.2026 14:04] Paper image links file exists (./assets/img_data/2602.17186.json), skip HTML parsing.
[23.02.2026 14:04] Success.
[23.02.2026 14:04] Downloading and parsing paper https://huggingface.co/papers/2602.16742.
[23.02.2026 14:04] Extra JSON file exists (./assets/json/2602.16742.json), skip PDF parsing.
[23.02.2026 14:04] Paper image links file exists (./assets/img_data/2602.16742.json), skip HTML parsing.
[23.02.2026 14:04] Success.
[23.02.2026 14:04] Downloading and parsing paper https://huggingface.co/papers/2602.18312.
[23.02.2026 14:04] Extra JSON file exists (./assets/json/2602.18312.json), skip PDF parsing.
[23.02.2026 14:04] Paper image links file exists (./assets/img_data/2602.18312.json), skip HTML parsing.
[23.02.2026 14:04] Success.
[23.02.2026 14:04] Enriching papers with extra data.
[23.02.2026 14:04] ********************************************************************************
[23.02.2026 14:04] Abstract 0. VESPO addresses training instability in LLM reinforcement learning by using variational formulation with variance reduction to correct policy divergence without length normalization.  					AI-generated summary 				 Training stability remains a central challenge in reinforcement learning (RL) for lar...
[23.02.2026 14:04] ********************************************************************************
[23.02.2026 14:04] Abstract 1. Large reasoning models can implicitly determine optimal stopping points for thinking, which SAGE-RL enhances by incorporating efficient reasoning patterns into pass@1 inference for improved accuracy and efficiency.  					AI-generated summary 				 Recent advancements in large reasoning models (LRMs) ...
[23.02.2026 14:04] ********************************************************************************
[23.02.2026 14:04] Abstract 2. A human-centric video world model conditioned on tracked head and hand poses is introduced, enabling dexterous interactions through a bidirectional video diffusion model trained for egocentric virtual environment generation.  					AI-generated summary 				 Extended reality (XR) demands generative mo...
[23.02.2026 14:04] ********************************************************************************
[23.02.2026 14:04] Abstract 3. EgoPush enables robot manipulation in cluttered environments through perception-driven policy learning that uses object-centric latent spaces and stage-decomposed rewards for long-horizon tasks.  					AI-generated summary 				 Humans can rearrange objects in cluttered environments using egocentric p...
[23.02.2026 14:04] ********************************************************************************
[23.02.2026 14:04] Abstract 4. A causal transformer-based variational autoencoder combined with flow matching enables real-time, spatially-aware conversational motion for embodied agents in virtual reality applications.  					AI-generated summary 				 As embodied agents become central to VR, telepresence, and digital human applic...
[23.02.2026 14:04] ********************************************************************************
[23.02.2026 14:04] Abstract 5. Abstract A video segmentation model eliminates specialized tracking modules by using a Vision Transformer encoder with query propagation and fusion mechanisms for efficient, high-speed processing.  					AI-generated summary Existing online video segmentation models typically combine a per-frame segm...
[23.02.2026 14:04] ********************************************************************************
[23.02.2026 14:04] Abstract 6. Abstract Diffusion Language Models suffer from high inference costs due to iterative denoising, prompting the development of Sink-Aware Pruning that identifies and removes unstable attention sinks, improving efficiency without retraining.  					AI-generated summary Diffusion Language Models (DLMs) i...
[23.02.2026 14:04] ********************************************************************************
[23.02.2026 14:04] Abstract 7. Visual Information Gain metric quantifies the contribution of visual input to prediction uncertainty, enabling selective training that improves visual grounding and reduces language bias in vision-language models.  					AI-generated summary 				 Large Vision Language Models (LVLMs) have achieved rem...
[23.02.2026 14:04] ********************************************************************************
[23.02.2026 14:04] Abstract 8. DeepVision-103K dataset enhances multimodal reasoning capabilities of large models through diverse mathematical content and visual elements.  					AI-generated summary 				 Reinforcement Learning with Verifiable Rewards (RLVR) has been shown effective in enhancing the visual reflection and reasoning...
[23.02.2026 14:04] ********************************************************************************
[23.02.2026 14:04] Abstract 9. Reinforcement learning policies are improved by using action Jacobian penalty to eliminate unrealistic high-frequency signals, with a new Linear Policy Net architecture reducing computational overhead while enabling faster convergence and efficient inference for motion imitation tasks.  					AI-gene...
[23.02.2026 14:04] Read previous papers.
[23.02.2026 14:04] Generating reviews via LLM API.
[23.02.2026 14:04] Using data from previous issue: {"categories": ["#rl", "#training", "#open_source", "#reasoning", "#math", "#optimization"], "emoji": "‚öñÔ∏è", "ru": {"title": "–°—Ç–∞–±–∏–ª—å–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–æ–ª–∏—Ç–∏–∫–∏ –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω—É—é —Ä–µ–¥—É–∫—Ü–∏—é –¥–∏—Å–ø–µ—Ä—Å–∏–∏", "desc": "VESPO —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ
[23.02.2026 14:04] Using data from previous issue: {"categories": ["#training", "#optimization", "#reasoning", "#benchmark", "#rl"], "emoji": "üß†", "ru": {"title": "–ú–æ–¥–µ–ª–∏ —Å–∞–º–∏ –∑–Ω–∞—é—Ç, –∫–æ–≥–¥–∞ –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å—Å—è: –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª–∏–Ω—ã —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞", "desc": "–í —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ –Ω–µ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –¥–ª–∏–Ω–Ω—ã—Ö —Ü–µ–ø–æ—á–µ–∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã
[23.02.2026 14:04] Using data from previous issue: {"categories": ["#video", "#architecture", "#training", "#multimodal", "#diffusion", "#3d"], "emoji": "ü•Ω", "ru": {"title": "–í–∏–¥–µ–æ–º–æ–¥–µ–ª—å –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–≥–æ –º–∏—Ä–∞ —Å —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º —á–µ—Ä–µ–∑ –ø–æ–∑—ã –≥–æ–ª–æ–≤—ã –∏ —Ä—É–∫ –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –≤–∏–¥–µ–æ–º–æ–¥–µ–ª—å –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–≥–æ –º–∏—Ä–∞, –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –Ω
[23.02.2026 14:04] Using data from previous issue: {"categories": ["#cv", "#robotics", "#rl"], "emoji": "ü§ñ", "ru": {"title": "–ú–∞–Ω–∏–ø—É–ª—è—Ü–∏—è —Ä–æ–±–æ—Ç–æ–º –∏–∑ –ø–µ—Ä–≤–æ–≥–æ –ª–∏—Ü–∞ —á–µ—Ä–µ–∑ –æ–±—ä–µ–∫—Ç–Ω–æ-—Ü–µ–Ω—Ç—Ä–∏—á–Ω—ã–µ –ª–∞—Ç–µ–Ω—Ç–Ω—ã–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞", "desc": "EgoPush ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –ø–æ–ª–∏—Ç–∏–∫ –º–∞–Ω–∏–ø—É–ª–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–æ–±–æ—Ç–æ–º –≤ –∑–∞–≥—Ä–æ–º–æ–∂–¥—ë–Ω–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ö –Ω–∞ –æ—Å–Ω–æ–≤–µ —ç–≥–æ—Ü–µ–Ω—Ç—Ä–∏—á–µ—Å–∫–æ–≥–æ –≤–∏–¥–µ–Ω–∏—è. 
[23.02.2026 14:04] Using data from previous issue: {"categories": ["#agents", "#dataset", "#video", "#architecture", "#multimodal", "#3d"], "emoji": "üé≠", "ru": {"title": "–ü—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–∞—è –æ—Å–≤–µ–¥–æ–º–ª—ë–Ω–Ω–æ—Å—Ç—å –≤–∏—Ä—Ç—É–∞–ª—å–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ –ø—Ä–∏—á–∏–Ω–Ω—ã–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ-–æ—Å–≤–µ–¥–æ–º–ª–µ–Ω–Ω—ã—Ö 
[23.02.2026 14:04] Querying the API.
[23.02.2026 14:04] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Abstract A video segmentation model eliminates specialized tracking modules by using a Vision Transformer encoder with query propagation and fusion mechanisms for efficient, high-speed processing.  					AI-generated summary Existing online video segmentation models typically combine a per-frame segmenter with complex specialized tracking modules. While effective, these modules introduce significant architectural complexity and computational overhead. Recent studies suggest that plain Vision Transformer (ViT) encoders, when scaled with sufficient capacity and large-scale pre-training, can conduct accurate image segmentation without requiring specialized modules. Motivated by this observation, we propose the Video Encoder-only Mask Transformer (VidEoMT), a simple encoder-only video segmentation model that eliminates the need for dedicated tracking modules. To enable temporal modeling in an encoder-only ViT, VidEoMT introduces a lightweight query propagation mechanism that carries information across frames by reusing queries from the previous frame. To balance this with adaptability to new content, it employs a query fusion strategy that combines the propagated queries with a set of temporally-agnostic learned queries. As a result, VidEoMT attains the benefits of a tracker without added complexity, achieving competitive accuracy while being 5x--10x faster, running at up to 160 FPS with a ViT-L backbone. Code: https://www.tue-mps.org/videomt/
[23.02.2026 14:04] Error getting data: Error code: 529 - {'type': 'error', 'error': {'type': 'overloaded_error', 'message': 'Overloaded'}, 'request_id': 'req_011CYR8MTMJvTZPTNFDDqsQX'}
[23.02.2026 14:04] Querying the API.
[23.02.2026 14:04] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Abstract Diffusion Language Models suffer from high inference costs due to iterative denoising, prompting the development of Sink-Aware Pruning that identifies and removes unstable attention sinks, improving efficiency without retraining.  					AI-generated summary Diffusion Language Models (DLMs) incur high inference cost due to iterative denoising, motivating efficient pruning. Existing pruning heuristics largely inherited from autoregressive (AR) LLMs, typically preserve attention sink tokens because AR sinks serve as stable global anchors. We show that this assumption does not hold for DLMs: the attention-sink position exhibits substantially higher variance over the full generation trajectory (measured by how the dominant sink locations shift across timesteps), indicating that sinks are often transient and less structurally essential than in AR models. Based on this observation, we propose {bf Sink-Aware Pruning}, which automatically identifies and prunes unstable sinks in DLMs (prior studies usually keep sinks for AR LLMs). Without retraining, our method achieves a better quality-efficiency trade-off and outperforms strong prior pruning baselines under matched compute. Our code is available at https://github.com/VILA-Lab/Sink-Aware-Pruning.
[23.02.2026 14:05] Error getting data: Error code: 529 - {'type': 'error', 'error': {'type': 'overloaded_error', 'message': 'Overloaded'}, 'request_id': 'req_011CYR8Mvp3ztG2rc4MMxxX4'}
[23.02.2026 14:05] Using data from previous issue: {"categories": [], "emoji": "üëÅÔ∏è", "ru": {"title": "–ò–∑–º–µ—Ä—è–µ–º –≤–∫–ª–∞–¥ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: –∏–∑–±–∏—Ä–∞—Ç–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –∑—Ä–µ–Ω–∏—è –∏ —è–∑—ã–∫–∞", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –º–µ—Ç—Ä–∏–∫–∞ Visual Information Gain (VIG), –∫–æ—Ç–æ—Ä–∞—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ–Ω–Ω–æ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç, –Ω–∞—Å–∫–æ–ª—å–∫–æ –≤–∏–∑—É–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è —Å–Ω–∏–∂–∞–µ—Ç –Ω–µ–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ—Å—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π 
[23.02.2026 14:05] Using data from previous issue: {"categories": ["#rl", "#benchmark", "#multimodal", "#dataset"], "emoji": "üìä", "ru": {"title": "–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è —á–µ—Ä–µ–∑ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç —Å –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–∞—è –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö DeepVision-103K, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö –º—É
[23.02.2026 14:05] Using data from previous issue: {"categories": ["#architecture", "#robotics", "#optimization", "#inference", "#rl"], "emoji": "ü§ñ", "ru": {"title": "–ì–ª–∞–¥–∫–∏–µ –ø–æ–ª–∏—Ç–∏–∫–∏ —á–µ—Ä–µ–∑ —à—Ç—Ä–∞—Ñ —è–∫–æ–±–∏–∞–Ω–∞ –¥–µ–π—Å—Ç–≤–∏–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –º–µ—Ç–æ–¥ —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–ª–∏—Ç–∏–∫ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –∏–º–∏—Ç–∞—Ü–∏–∏ –¥–≤–∏–∂–µ–Ω–∏–π –ø—É—Ç—ë–º –¥–æ–±–∞–≤–ª–µ–Ω–∏—è —à—Ç—Ä–∞—Ñ–∞ –Ω–∞ —è–∫–æ–±–∏–∞
[23.02.2026 14:05] Renaming data file.
[23.02.2026 14:05] Renaming previous data. hf_papers.json to ./d/2026-02-23.json
[23.02.2026 14:05] Saving new data file.
[23.02.2026 14:05] Generating page.
[23.02.2026 14:05] Renaming previous page.
[23.02.2026 14:05] Renaming previous data. index.html to ./d/2026-02-23.html
[23.02.2026 14:05] Writing result.
[23.02.2026 14:05] Renaming log file.
[23.02.2026 14:05] Renaming previous data. log.txt to ./logs/2026-02-23_last_log.txt
