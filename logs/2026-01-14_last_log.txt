[14.01.2026 22:25] Read previous papers.
[14.01.2026 22:25] Generating top page (month).
[14.01.2026 22:25] Writing top page (month).
[14.01.2026 23:20] Read previous papers.
[14.01.2026 23:20] Get feed.
[14.01.2026 23:20] Get page data from previous paper. URL: https://huggingface.co/papers/2601.06789
[14.01.2026 23:20] Get page data from previous paper. URL: https://huggingface.co/papers/2601.07022
[14.01.2026 23:20] Get page data from previous paper. URL: https://huggingface.co/papers/2601.04745
[14.01.2026 23:20] Get page data from previous paper. URL: https://huggingface.co/papers/2601.08225
[14.01.2026 23:20] Get page data from previous paper. URL: https://huggingface.co/papers/2512.24965
[14.01.2026 23:20] Get page data from previous paper. URL: https://huggingface.co/papers/2601.08079
[14.01.2026 23:20] Get page data from previous paper. URL: https://huggingface.co/papers/2601.06487
[14.01.2026 23:20] Get page data from previous paper. URL: https://huggingface.co/papers/2601.08584
[14.01.2026 23:20] Get page data from previous paper. URL: https://huggingface.co/papers/2601.08831
[14.01.2026 23:20] Get page data from previous paper. URL: https://huggingface.co/papers/2601.07264
[14.01.2026 23:20] Get page data from previous paper. URL: https://huggingface.co/papers/2601.08670
[14.01.2026 23:20] Get page data from previous paper. URL: https://huggingface.co/papers/2601.08620
[14.01.2026 23:20] Get page data from previous paper. URL: https://huggingface.co/papers/2601.08303
[14.01.2026 23:20] Get page data from previous paper. URL: https://huggingface.co/papers/2601.08828
[14.01.2026 23:20] Get page data from previous paper. URL: https://huggingface.co/papers/2601.08665
[14.01.2026 23:20] Get page data from previous paper. URL: https://huggingface.co/papers/2601.08587
[14.01.2026 23:20] Get page data from previous paper. URL: https://huggingface.co/papers/2601.08468
[14.01.2026 23:20] Get page data from previous paper. URL: https://huggingface.co/papers/2601.08321
[14.01.2026 23:20] Get page data from previous paper. URL: https://huggingface.co/papers/2601.07290
[14.01.2026 23:20] Get page data from previous paper. URL: https://huggingface.co/papers/2601.06786
[14.01.2026 23:20] Get page data from previous paper. URL: https://huggingface.co/papers/2601.07632
[14.01.2026 23:20] Get page data from previous paper. URL: https://huggingface.co/papers/2601.04582
[14.01.2026 23:20] Get page data from previous paper. URL: https://huggingface.co/papers/2601.02669
[14.01.2026 23:20] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[14.01.2026 23:20] No deleted papers detected.
[14.01.2026 23:20] Downloading and parsing papers (pdf, html). Total: 23.
[14.01.2026 23:20] Downloading and parsing paper https://huggingface.co/papers/2601.06789.
[14.01.2026 23:20] Extra JSON file exists (./assets/json/2601.06789.json), skip PDF parsing.
[14.01.2026 23:20] Paper image links file exists (./assets/img_data/2601.06789.json), skip HTML parsing.
[14.01.2026 23:20] Success.
[14.01.2026 23:20] Downloading and parsing paper https://huggingface.co/papers/2601.07022.
[14.01.2026 23:20] Extra JSON file exists (./assets/json/2601.07022.json), skip PDF parsing.
[14.01.2026 23:20] Paper image links file exists (./assets/img_data/2601.07022.json), skip HTML parsing.
[14.01.2026 23:20] Success.
[14.01.2026 23:20] Downloading and parsing paper https://huggingface.co/papers/2601.04745.
[14.01.2026 23:20] Extra JSON file exists (./assets/json/2601.04745.json), skip PDF parsing.
[14.01.2026 23:20] Paper image links file exists (./assets/img_data/2601.04745.json), skip HTML parsing.
[14.01.2026 23:20] Success.
[14.01.2026 23:20] Downloading and parsing paper https://huggingface.co/papers/2601.08225.
[14.01.2026 23:20] Extra JSON file exists (./assets/json/2601.08225.json), skip PDF parsing.
[14.01.2026 23:20] Paper image links file exists (./assets/img_data/2601.08225.json), skip HTML parsing.
[14.01.2026 23:20] Success.
[14.01.2026 23:20] Downloading and parsing paper https://huggingface.co/papers/2512.24965.
[14.01.2026 23:20] Extra JSON file exists (./assets/json/2512.24965.json), skip PDF parsing.
[14.01.2026 23:20] Paper image links file exists (./assets/img_data/2512.24965.json), skip HTML parsing.
[14.01.2026 23:20] Success.
[14.01.2026 23:20] Downloading and parsing paper https://huggingface.co/papers/2601.08079.
[14.01.2026 23:20] Extra JSON file exists (./assets/json/2601.08079.json), skip PDF parsing.
[14.01.2026 23:20] Paper image links file exists (./assets/img_data/2601.08079.json), skip HTML parsing.
[14.01.2026 23:20] Success.
[14.01.2026 23:20] Downloading and parsing paper https://huggingface.co/papers/2601.06487.
[14.01.2026 23:20] Extra JSON file exists (./assets/json/2601.06487.json), skip PDF parsing.
[14.01.2026 23:20] Paper image links file exists (./assets/img_data/2601.06487.json), skip HTML parsing.
[14.01.2026 23:20] Success.
[14.01.2026 23:20] Downloading and parsing paper https://huggingface.co/papers/2601.08584.
[14.01.2026 23:20] Extra JSON file exists (./assets/json/2601.08584.json), skip PDF parsing.
[14.01.2026 23:20] Paper image links file exists (./assets/img_data/2601.08584.json), skip HTML parsing.
[14.01.2026 23:20] Success.
[14.01.2026 23:20] Downloading and parsing paper https://huggingface.co/papers/2601.08831.
[14.01.2026 23:20] Extra JSON file exists (./assets/json/2601.08831.json), skip PDF parsing.
[14.01.2026 23:20] Paper image links file exists (./assets/img_data/2601.08831.json), skip HTML parsing.
[14.01.2026 23:20] Success.
[14.01.2026 23:20] Downloading and parsing paper https://huggingface.co/papers/2601.07264.
[14.01.2026 23:20] Extra JSON file exists (./assets/json/2601.07264.json), skip PDF parsing.
[14.01.2026 23:20] Paper image links file exists (./assets/img_data/2601.07264.json), skip HTML parsing.
[14.01.2026 23:20] Success.
[14.01.2026 23:20] Downloading and parsing paper https://huggingface.co/papers/2601.08670.
[14.01.2026 23:20] Extra JSON file exists (./assets/json/2601.08670.json), skip PDF parsing.
[14.01.2026 23:20] Paper image links file exists (./assets/img_data/2601.08670.json), skip HTML parsing.
[14.01.2026 23:20] Success.
[14.01.2026 23:20] Downloading and parsing paper https://huggingface.co/papers/2601.08620.
[14.01.2026 23:20] Extra JSON file exists (./assets/json/2601.08620.json), skip PDF parsing.
[14.01.2026 23:20] Paper image links file exists (./assets/img_data/2601.08620.json), skip HTML parsing.
[14.01.2026 23:20] Success.
[14.01.2026 23:20] Downloading and parsing paper https://huggingface.co/papers/2601.08303.
[14.01.2026 23:20] Extra JSON file exists (./assets/json/2601.08303.json), skip PDF parsing.
[14.01.2026 23:20] Paper image links file exists (./assets/img_data/2601.08303.json), skip HTML parsing.
[14.01.2026 23:20] Success.
[14.01.2026 23:20] Downloading and parsing paper https://huggingface.co/papers/2601.08828.
[14.01.2026 23:20] Extra JSON file exists (./assets/json/2601.08828.json), skip PDF parsing.
[14.01.2026 23:20] Paper image links file exists (./assets/img_data/2601.08828.json), skip HTML parsing.
[14.01.2026 23:20] Success.
[14.01.2026 23:20] Downloading and parsing paper https://huggingface.co/papers/2601.08665.
[14.01.2026 23:20] Extra JSON file exists (./assets/json/2601.08665.json), skip PDF parsing.
[14.01.2026 23:20] Paper image links file exists (./assets/img_data/2601.08665.json), skip HTML parsing.
[14.01.2026 23:20] Success.
[14.01.2026 23:20] Downloading and parsing paper https://huggingface.co/papers/2601.08587.
[14.01.2026 23:20] Extra JSON file exists (./assets/json/2601.08587.json), skip PDF parsing.
[14.01.2026 23:20] Paper image links file exists (./assets/img_data/2601.08587.json), skip HTML parsing.
[14.01.2026 23:20] Success.
[14.01.2026 23:20] Downloading and parsing paper https://huggingface.co/papers/2601.08468.
[14.01.2026 23:20] Extra JSON file exists (./assets/json/2601.08468.json), skip PDF parsing.
[14.01.2026 23:20] Paper image links file exists (./assets/img_data/2601.08468.json), skip HTML parsing.
[14.01.2026 23:20] Success.
[14.01.2026 23:20] Downloading and parsing paper https://huggingface.co/papers/2601.08321.
[14.01.2026 23:20] Extra JSON file exists (./assets/json/2601.08321.json), skip PDF parsing.
[14.01.2026 23:20] Paper image links file exists (./assets/img_data/2601.08321.json), skip HTML parsing.
[14.01.2026 23:20] Success.
[14.01.2026 23:20] Downloading and parsing paper https://huggingface.co/papers/2601.07290.
[14.01.2026 23:20] Extra JSON file exists (./assets/json/2601.07290.json), skip PDF parsing.
[14.01.2026 23:20] Paper image links file exists (./assets/img_data/2601.07290.json), skip HTML parsing.
[14.01.2026 23:20] Success.
[14.01.2026 23:20] Downloading and parsing paper https://huggingface.co/papers/2601.06786.
[14.01.2026 23:20] Extra JSON file exists (./assets/json/2601.06786.json), skip PDF parsing.
[14.01.2026 23:20] Paper image links file exists (./assets/img_data/2601.06786.json), skip HTML parsing.
[14.01.2026 23:20] Success.
[14.01.2026 23:20] Downloading and parsing paper https://huggingface.co/papers/2601.07632.
[14.01.2026 23:20] Extra JSON file exists (./assets/json/2601.07632.json), skip PDF parsing.
[14.01.2026 23:20] Paper image links file exists (./assets/img_data/2601.07632.json), skip HTML parsing.
[14.01.2026 23:20] Success.
[14.01.2026 23:20] Downloading and parsing paper https://huggingface.co/papers/2601.04582.
[14.01.2026 23:20] Extra JSON file exists (./assets/json/2601.04582.json), skip PDF parsing.
[14.01.2026 23:20] Paper image links file exists (./assets/img_data/2601.04582.json), skip HTML parsing.
[14.01.2026 23:20] Success.
[14.01.2026 23:20] Downloading and parsing paper https://huggingface.co/papers/2601.02669.
[14.01.2026 23:20] Extra JSON file exists (./assets/json/2601.02669.json), skip PDF parsing.
[14.01.2026 23:20] Paper image links file exists (./assets/img_data/2601.02669.json), skip HTML parsing.
[14.01.2026 23:20] Success.
[14.01.2026 23:20] Enriching papers with extra data.
[14.01.2026 23:20] ********************************************************************************
[14.01.2026 23:20] Abstract 0. MemGovern framework transforms unstructured GitHub data into structured experiential memory for autonomous software engineering agents, improving bug resolution rates through enhanced experience retrieval.  					AI-generated summary 				 While autonomous software engineering (SWE) agents are reshapi...
[14.01.2026 23:20] ********************************************************************************
[14.01.2026 23:20] Abstract 1. Solar Open presents a 102B-parameter bilingual Mixture-of-Experts language model that addresses data scarcity in underserved languages through synthetic data generation, progressive curriculum coordination, and scalable reinforcement learning optimization.  					AI-generated summary 				 We introduc...
[14.01.2026 23:20] ********************************************************************************
[14.01.2026 23:20] Abstract 2. Long-horizon memory benchmarks based on autobiographical narratives evaluate models' ability to infer stable motivations and decision principles through evidence-linked questions spanning factual recall, subjective state attribution, and principle-level reasoning.  					AI-generated summary 				 Exi...
[14.01.2026 23:20] ********************************************************************************
[14.01.2026 23:20] Abstract 3. Large reasoning models enable scalable multi-turn dialogue generation through automated task-oriented simulation and user-oriented behavioral modeling for enhanced human-agent interaction datasets.  					AI-generated summary 				 The recent paradigm shift toward large reasoning models (LRMs) as auto...
[14.01.2026 23:20] ********************************************************************************
[14.01.2026 23:20] Abstract 4. Building intelligent agents capable of dexterous manipulation is essential for achieving human-like automation in both robotics and digital environments. However, existing GUI agents rely on discrete click predictions (x,y), which prohibits free-form, closed-loop trajectories (e.g. dragging a progre...
[14.01.2026 23:20] ********************************************************************************
[14.01.2026 23:20] Abstract 5. Memory management in tool-augmented agents is crucial for maintaining coherent, goal-directed reasoning over extended tasks, requiring explicit mechanisms to track and organize reasoning steps within constrained contexts.  					AI-generated summary 				 Complex reasoning in tool-augmented agent fram...
[14.01.2026 23:20] ********************************************************************************
[14.01.2026 23:20] Abstract 6. Reinforcement learning for large language model agents suffers from discrimination collapse in open-ended tasks due to pointwise scalar scoring, which ArenaRL addresses through relative ranking and pairwise evaluation mechanisms.  					AI-generated summary 				 Reinforcement learning has substantial...
[14.01.2026 23:20] ********************************************************************************
[14.01.2026 23:20] Abstract 7. The Ministral 3 series consists of parameter-efficient dense language models with three sizes (3B, 8B, 14B) and three variants per size, trained using cascade distillation for compute-constrained applications.  					AI-generated summary 				 We introduce the Ministral 3 series, a family of parameter...
[14.01.2026 23:20] ********************************************************************************
[14.01.2026 23:20] Abstract 8. 3AM enhances video object segmentation by integrating 3D-aware features from MUSt3R into SAM2, achieving improved viewpoint consistency with only RGB input at inference.  					AI-generated summary 				 Video object segmentation methods like SAM2 achieve strong performance through memory-based archit...
[14.01.2026 23:20] ********************************************************************************
[14.01.2026 23:20] Abstract 9. Tool-integrated language model agents exhibit different calibration behaviors based on tool type, with a reinforcement learning framework improving both task accuracy and reliable uncertainty estimation across diverse domains.  					AI-generated summary 				 Autonomous agents based on large language...
[14.01.2026 23:20] ********************************************************************************
[14.01.2026 23:20] Abstract 10. Parallel Context-of-Experts Decoding enables multi-document reasoning in retrieval-augmented generation by treating retrieved documents as isolated experts and synchronizing predictions through a retrieval-aware contrastive decoding rule.  					AI-generated summary 				 Retrieval Augmented Generatio...
[14.01.2026 23:20] ********************************************************************************
[14.01.2026 23:20] Abstract 11. ViDoRe v3 is a multimodal retrieval-augmented generation benchmark with diverse document types and languages, revealing that visual retrieval and late interaction models improve performance while current systems still face challenges with non-textual elements and complex queries.  					AI-generated ...
[14.01.2026 23:20] ********************************************************************************
[14.01.2026 23:20] Abstract 12. An efficient diffusion transformer framework for mobile and edge devices that maintains high-generation quality while reducing computational costs through compact architecture, elastic training, and knowledge-guided distillation.  					AI-generated summary 				 Recent advances in diffusion transform...
[14.01.2026 23:20] ********************************************************************************
[14.01.2026 23:20] Abstract 13. Motive is a gradient-based data attribution framework that identifies influential video clips for motion improvement in text-to-video models through motion-weighted loss masking.  					AI-generated summary 				 Despite the rapid progress of video generation models, the role of data in influencing mo...
[14.01.2026 23:20] ********************************************************************************
[14.01.2026 23:20] Abstract 14. VLingNav enhances embodied navigation through linguistic-driven cognition with adaptive reasoning and visual-assisted memory, achieving state-of-the-art performance and zero-shot transfer to real robots.  					AI-generated summary 				 VLA models have shown promising potential in embodied navigation...
[14.01.2026 23:20] ********************************************************************************
[14.01.2026 23:20] Abstract 15. MoCha enables controllable video character replacement using a single frame mask through condition-aware RoPE and a comprehensive data construction pipeline with specialized datasets.  					AI-generated summary 				 Controllable video character replacement with a user-provided identity remains a cha...
[14.01.2026 23:20] ********************************************************************************
[14.01.2026 23:20] Abstract 16. Reinforcement learning with verifiable rewards is enhanced through a judge-then-generate paradigm that improves both efficiency and accuracy in mathematical problem-solving.  					AI-generated summary 				 Reinforcement Learning with Verifiable Rewards (RLVR) has become a standard paradigm for reaso...
[14.01.2026 23:20] ********************************************************************************
[14.01.2026 23:20] Abstract 17. A unified multimodal model for visual text editing that understands natural language instructions and maintains stylistic consistency with reference images through visual language modeling and contextual embedding combination.  					AI-generated summary 				 With the rapid advancement of image gener...
[14.01.2026 23:20] ********************************************************************************
[14.01.2026 23:20] Abstract 18. VideoLoom is a unified video large language model that achieves state-of-the-art performance in spatial-temporal video understanding through a specialized dataset and benchmark.  					AI-generated summary 				 This paper presents VideoLoom, a unified Video Large Language Model (Video LLM) for joint ...
[14.01.2026 23:20] ********************************************************************************
[14.01.2026 23:20] Abstract 19. Training large language models for improved reasoning while maintaining calibration through epistemic learning reduces inference compute requirements and enhances performance on mathematical and coding tasks.  					AI-generated summary 				 Improving the reasoning abilities of large language models ...
[14.01.2026 23:20] ********************************************************************************
[14.01.2026 23:20] Abstract 20. A novel framework for motion understanding and motion-language reasoning that aligns motion token geometry with language model embeddings through orthogonal constraints and sparse projection techniques.  					AI-generated summary 				 Discrete motion tokenization has recently enabled Large Language ...
[14.01.2026 23:20] ********************************************************************************
[14.01.2026 23:20] Abstract 21. A reinforcement learning framework for text-to-visualization generation that improves chart quality and code execution by optimizing multiple objectives using post-execution feedback.  					AI-generated summary 				 Text-to-Visualization (Text2Vis) systems translate natural language queries over tab...
[14.01.2026 23:20] ********************************************************************************
[14.01.2026 23:20] Abstract 22. FactArena is an automated evaluation framework that comprehensively assesses large language models across all stages of the fact-checking pipeline, revealing gaps between claim verification accuracy and overall fact-checking capability.  					AI-generated summary 				 Large Language Models (LLMs) ar...
[14.01.2026 23:20] Read previous papers.
[14.01.2026 23:20] Generating reviews via LLM API.
[14.01.2026 23:20] Using data from previous issue: {"categories": [], "emoji": "üß†", "ru": {"title": "–û–ø—ã—Ç —á–µ–ª–æ–≤–µ–∫–∞ –∫–∞–∫ –ø–∞–º—è—Ç—å –¥–ª—è –∞–≥–µ–Ω—Ç–æ–≤ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ MemGovern, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –Ω–µ—Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ GitHub –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é –ø–∞–º—è—Ç—å –æ–ø—ã—Ç–∞ –¥–ª—è –∞–≤—Ç–æ–Ω–æ–º–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–≥–æ –æ–±–µ—Å–ø–µ—á
[14.01.2026 23:20] Using data from previous issue: {"categories": ["#low_resource", "#training", "#data", "#rlhf", "#synthetic", "#optimization", "#open_source", "#multilingual", "#reasoning", "#architecture"], "emoji": "üåç", "ru": {"title": "–°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –∫–∞–∫ –º–æ—Å—Ç –¥–ª—è —è–∑—ã–∫–æ–≤ —Å –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –±–æ–ª—å—à–∞—è —è–∑—ã–∫–æ–≤–∞—è 
[14.01.2026 23:20] Using data from previous issue: {"categories": ["#reasoning", "#open_source", "#long_context"], "emoji": "üìñ", "ru": {"title": "–ü–æ–Ω–∏–º–∞–Ω–∏–µ –ª–∏—á–Ω–æ—Å—Ç–∏ —á–µ—Ä–µ–∑ –±–∏–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ –Ω–∞—Ä—Ä–∞—Ç–∏–≤—ã: –∫–æ–≥–¥–∞ –ø–æ–∏—Å–∫ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ KnowMeBench –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø–æ–Ω–∏–º–∞—Ç—å –ª–∏—á–Ω–æ—Å—Ç—å —á–µ–ª–æ–≤–µ–∫–∞ –Ω–∞ –æ
[14.01.2026 23:20] Using data from previous issue: {"categories": ["#synthetic", "#reasoning"], "emoji": "üó£Ô∏è", "ru": {"title": "–°–∏–º—É–ª—è—Ü–∏—è —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–≥–æ –ø–æ–≤–µ–¥–µ–Ω–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö –¥–∏–∞–ª–æ–≥–æ–≤ —Å AI-–∞–≥–µ–Ω—Ç–∞–º–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –º–Ω–æ–≥–æ—Ö–æ–¥–æ–≤—ã—Ö –¥–∏–∞–ª–æ–≥–æ–≤ –º–µ–∂–¥—É —á–µ–ª–æ–≤–µ–∫–æ–º –∏ –∞–≥–µ–Ω—Ç–æ–º, –∏—Å–ø–æ–ª—å–∑—É—è –±–æ–ª—å—à
[14.01.2026 23:20] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#agents", "#small_models", "#architecture"], "emoji": "ü§ñ", "ru": {"title": "–ü–ª–∞–≤–Ω—ã–µ –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–∏ –≤ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞—Ö: –ø–æ—Ç–æ–∫–æ–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π –∞–≥–µ–Ω—Ç–æ–≤", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è ShowUI-œÄ ‚Äî –ø–µ—Ä–≤–∞—è –ø–æ—Ç–æ—á–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è
[14.01.2026 23:20] Using data from previous issue: {"categories": ["#benchmark", "#reasoning", "#long_context", "#agents"], "emoji": "üß†", "ru": {"title": "–ö–æ–≥–Ω–∏—Ç–∏–≤–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ: —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –ø–∞–º—è—Ç—å –¥–ª—è –¥–æ–ª–≥–æ–≥–æ–≤–æ—Ä–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∞–≥–µ–Ω—Ç–æ–≤", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ MemoBrain ‚Äî –º–æ–¥–µ–ª—å –∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –ø–∞–º—è—Ç–∏ –¥–ª—è –∞–≥–µ–Ω—Ç–æ–≤, –¥–æ–ø–æ–ª–Ω–µ–Ω–Ω—ã—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º
[14.01.2026 23:20] Using data from previous issue: {"categories": ["#alignment", "#benchmark", "#rl", "#optimization", "#reasoning", "#dataset", "#agents", "#training"], "emoji": "üèÜ", "ru": {"title": "–û—Ç –ø–æ—Ç–æ—á–µ—á–Ω—ã—Ö –æ—Ü–µ–Ω–æ–∫ –∫ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–º—É —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—é: —Ç—É—Ä–Ω–∏—Ä–Ω—ã–π —Å–ø–æ—Å–æ–± –æ–±—É—á–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ ArenaRL –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä
[14.01.2026 23:20] Using data from previous issue: {"categories": [], "emoji": "üöÄ", "ru": {"title": "–ö–æ–º–ø–∞–∫—Ç–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –≤—Å–µ—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã Ministral 3 ‚Äî —Å–µ–º–µ–π—Å—Ç–≤–æ –∫–æ–º–ø–∞–∫—Ç–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Ä–∞–∑–º–µ—Ä–æ–º –æ—Ç 3 –¥–æ 14 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏. –î–ª—è –∫–∞–∂–¥–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ –ø—Ä–µ–¥–ª–æ
[14.01.2026 23:20] Using data from previous issue: {"categories": ["#architecture", "#training", "#3d", "#optimization", "#video", "#cv"], "emoji": "üé¨", "ru": {"title": "–ì–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∞—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –≤ –≤–∏–¥–µ–æ—Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ —Å–∏–Ω—Ç–µ–∑ 3D-–ø—Ä–∏–∑–Ω–∞–∫–æ–≤", "desc": "3AM —É–ª—É—á—à–∞–µ—Ç —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—é –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –≤–∏–¥–µ–æ, –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É—è —Ç—Ä—ë—Ö–º–µ—Ä–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ –º–æ–¥–µ–ª–∏ MUSt3R –≤ –∞
[14.01.2026 23:20] Using data from previous issue: {"categories": ["#benchmark", "#agents", "#rl"], "emoji": "üéØ", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞–¥–µ–∂–Ω–æ –≤—ã—Ä–∞–∂–∞—Ç—å –Ω–µ—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤", "desc": "–†–∞–±–æ—Ç–∞ –∏—Å—Å–ª–µ–¥—É–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏ (—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ –ø—Ä–∞–≤–∏–ª—å–Ω–æ –æ—Ü–µ–Ω–∏–≤–∞—Ç—å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤ —Å–≤–æ–∏—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è—Ö) –≤ LLM-–∞–≥–µ–Ω—Ç–∞—Ö, –∫–æ—Ç
[14.01.2026 23:20] Using data from previous issue: {"categories": [], "emoji": "üéØ", "ru": {"title": "–î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –±–µ–∑ –ø–µ—Ä–µ–∫—Ä—ë—Å—Ç–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –º–µ—Ç–æ–¥ Parallel Context-of-Experts Decoding –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º—ã –º–Ω–æ–≥–æ–¥–æ–∫—É–º–µ–Ω—Ç–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ retrieval-augmented generation. –ü–æ–¥—Ö–æ–¥ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç –∫–∞–∂–¥—ã–π –ø–æ–ª—É—á–µ–Ω–Ω—ã–π
[14.01.2026 23:20] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#multilingual", "#survey", "#open_source", "#rag", "#multimodal", "#low_resource"], "emoji": "üìä", "ru": {"title": "–í–∏–∑—É–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–π –≤ –º–Ω–æ–≥–æ–º–æ–¥–∞–ª—å–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ–º", "desc": "ViDoRe v3 ‚Äî —ç—Ç–æ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–∏
[14.01.2026 23:20] Using data from previous issue: {"categories": ["#architecture", "#small_models", "#training", "#inference"], "emoji": "üì±", "ru": {"title": "–ö–æ–º–ø–∞–∫—Ç–Ω—ã–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã –¥–ª—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤ –Ω–∞ –ª–∞–¥–æ–Ω–∏", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–æ–≥–æ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ –¥–ª—è –º–æ–±–∏–ª—å–Ω—ã—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤, –∫–æ—Ç–æ—Ä–∞—è —Å–æ—á–µ—Ç–∞–µ—Ç –∞–¥–∞–ø
[14.01.2026 23:20] Using data from previous issue: {"categories": ["#video", "#data", "#training"], "emoji": "üé¨", "ru": {"title": "–ê—Ç—Ä–∏–±—É—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –¥–≤–∏–∂–µ–Ω–∏—è –≤ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –≤–∏–¥–µ–æ–º–æ–¥–µ–ª—è—Ö", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω Motive ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∞—Ç—Ä–∏–±—É—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞—Ö, –∫–æ—Ç–æ—Ä—ã–π –≤—ã—è–≤–ª—è–µ—Ç –≤–ª–∏—è—Ç–µ–ª—å–Ω—ã–µ –≤–∏–¥–µ–æ–∫–ª–∏–ø—ã –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è
[14.01.2026 23:20] Using data from previous issue: {"categories": ["#rl", "#transfer_learning", "#multimodal", "#reasoning", "#dataset", "#robotics", "#agents", "#training"], "emoji": "ü§ñ", "ru": {"title": "–†–∞—Å—Å—É–∂–¥–∞—é—â–∞—è –Ω–∞–≤–∏–≥–∞—Ü–∏—è: –∫–æ–≥–¥–∞ —Ä–æ–±–æ—Ç –¥—É–º–∞–µ—Ç, –ø—Ä–µ–∂–¥–µ —á–µ–º –¥–µ–π—Å—Ç–≤–æ–≤–∞—Ç—å", "desc": "VLingNav ‚Äî —ç—Ç–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –Ω–∞–≤–∏–≥–∞—Ü–∏–∏ —Ä–æ–±–æ—Ç–æ–≤, –∫–æ—Ç–æ—Ä–∞
[14.01.2026 23:20] Using data from previous issue: {"categories": ["#rl", "#multimodal", "#data", "#dataset", "#video", "#architecture"], "emoji": "üé¨", "ru": {"title": "–ó–∞–º–µ–Ω–∞ –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π –≤ –≤–∏–¥–µ–æ –ø–æ –æ–¥–Ω–æ–π –º–∞—Å–∫–µ –±–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π", "desc": "MoCha ‚Äî —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –∑–∞–º–µ–Ω—ã –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π –≤ –≤–∏–¥–µ–æ, –∏—Å–ø–æ–ª—å–∑—É—è —Ç–æ–ª—å–∫–æ –æ–¥–Ω—É –º–∞—Å–∫—É –∏–∑ –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω–æ–≥–æ –∫–∞–¥—Ä–∞ –≤–º–µ—Å—Ç–æ
[14.01.2026 23:20] Using data from previous issue: {"categories": ["#rlhf", "#rl", "#optimization", "#reasoning", "#math", "#training"], "emoji": "‚öñÔ∏è", "ru": {"title": "–°—É–¥—å—è —Å–Ω–∞—á–∞–ª–∞, –∑–∞—Ç–µ–º –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä: –æ–±—É—á–µ–Ω–∏–µ –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω JudgeRLVR ‚Äî –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –∏ –ø—Ä–æ–≤–µ—Ä—è
[14.01.2026 23:20] Using data from previous issue: {"categories": ["#dataset", "#cv", "#multimodal"], "emoji": "‚úèÔ∏è", "ru": {"title": "–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–µ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å—Ç–∏–ª—è —á–µ—Ä–µ–∑ –≤–∏–∑—É–∞–ª—å–Ω–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ —è–∑—ã–∫–∞", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è UM-Text ‚Äî —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –Ω–∞
[14.01.2026 23:20] Using data from previous issue: {"categories": ["#multimodal", "#open_source", "#video", "#benchmark", "#synthetic", "#dataset"], "emoji": "üé¨", "ru": {"title": "–ï–¥–∏–Ω–∞—è –≤–∏–¥–µ–æ-–º–æ–¥–µ–ª—å –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è —Ç–æ–≥–æ, —á—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –∏ –≥–¥–µ —ç—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç", "desc": "VideoLoom ‚Äî —ç—Ç–æ –µ–¥–∏–Ω–∞—è –≤–∏–¥–µ–æ –±–æ–ª—å—à–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–Ω–∏–º–∞–µ—Ç –≤–∏–¥–µ–æ —Å—Ä–∞–∑—É –≤
[14.01.2026 23:20] Using data from previous issue: {"categories": ["#small_models", "#reasoning", "#inference", "#training", "#alignment", "#plp", "#math", "#optimization"], "emoji": "üß†", "ru": {"title": "–£—á–∏–º –º–æ–¥–µ–ª–∏ –Ω–µ —Ç–æ–ª—å–∫–æ –¥—É–º–∞—Ç—å, –Ω–æ –∏ –∑–Ω–∞—Ç—å, –∫–æ–≥–¥–∞ –¥–æ–≤–µ—Ä—è—Ç—å —Å–≤–æ–∏–º –º—ã—Å–ª—è–º", "desc": "–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç –º–µ—Ç–æ–¥ EpiCaR –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª
[14.01.2026 23:20] Using data from previous issue: {"categories": [], "emoji": "üï∫", "ru": {"title": "–ï–¥–∏–Ω–∞—è –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∞—è –æ—Å–Ω–æ–≤–∞ –¥–ª—è –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –¥–≤–∏–∂–µ–Ω–∏—è –∏ —è–∑—ã–∫–∞", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –Ω–æ–≤–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –¥–≤–∏–∂–µ–Ω–∏—è –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –æ —Å–≤—è–∑–∏ –º–µ–∂–¥—É –¥–≤–∏–∂–µ–Ω–∏–µ–º –∏ —è–∑—ã–∫–æ–º –ø—É—Ç—ë–º –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –≥–µ–æ–º–µ—Ç—Ä–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤ –¥–≤–∏–∂–µ–Ω–∏—è —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏ —è–∑—ã–∫–æ–≤–æ–π 
[14.01.2026 23:20] Using data from previous issue: {"categories": ["#open_source", "#benchmark", "#rl", "#optimization", "#multimodal", "#training"], "emoji": "üìä", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç RL-Text2Vis ‚Äî –ø–µ—Ä–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏
[14.01.2026 23:20] Using data from previous issue: {"categories": [], "emoji": "üîç", "ru": {"title": "–û—Ç –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∫ —Ü–µ–ª–æ—Å—Ç–Ω–æ–π –æ—Ü–µ–Ω–∫–µ: –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –ø—Ä–æ–≤–µ—Ä—è—Ç—å —Ñ–∞–∫—Ç—ã", "desc": "FactArena –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å–∏—Å—Ç–µ–º—É –æ—Ü–µ–Ω–∫–∏, –∫–æ—Ç–æ—Ä–∞—è –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –≤—Å–µ—Ö —ç—Ç–∞–ø–∞—Ö –ø—Ä
[14.01.2026 23:20] Renaming data file.
[14.01.2026 23:20] Renaming previous data. hf_papers.json to ./d/2026-01-14.json
[14.01.2026 23:20] Saving new data file.
[14.01.2026 23:20] Generating page.
[14.01.2026 23:20] Renaming previous page.
[14.01.2026 23:20] Renaming previous data. index.html to ./d/2026-01-14.html
[14.01.2026 23:20] Writing result.
[14.01.2026 23:20] Renaming log file.
[14.01.2026 23:20] Renaming previous data. log.txt to ./logs/2026-01-14_last_log.txt
