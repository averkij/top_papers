[14.01.2026 07:27] Read previous papers.
[14.01.2026 07:27] Generating top page (month).
[14.01.2026 07:27] Writing top page (month).
[14.01.2026 08:33] Read previous papers.
[14.01.2026 08:33] Get feed.
[14.01.2026 08:33] Get page data from previous paper. URL: https://huggingface.co/papers/2601.07022
[14.01.2026 08:33] Get page data from previous paper. URL: https://huggingface.co/papers/2601.08225
[14.01.2026 08:33] Get page data from previous paper. URL: https://huggingface.co/papers/2512.24965
[14.01.2026 08:33] Get page data from previous paper. URL: https://huggingface.co/papers/2601.06789
[14.01.2026 08:33] Extract page data from URL. URL: https://huggingface.co/papers/2601.04745
[14.01.2026 08:33] Get page data from previous paper. URL: https://huggingface.co/papers/2601.07264
[14.01.2026 08:33] Get page data from previous paper. URL: https://huggingface.co/papers/2601.06487
[14.01.2026 08:33] Get page data from previous paper. URL: https://huggingface.co/papers/2601.08584
[14.01.2026 08:33] Get page data from previous paper. URL: https://huggingface.co/papers/2601.08828
[14.01.2026 08:33] Get page data from previous paper. URL: https://huggingface.co/papers/2601.08665
[14.01.2026 08:33] Get page data from previous paper. URL: https://huggingface.co/papers/2601.08587
[14.01.2026 08:33] Get page data from previous paper. URL: https://huggingface.co/papers/2601.08468
[14.01.2026 08:33] Get page data from previous paper. URL: https://huggingface.co/papers/2601.08303
[14.01.2026 08:33] Get page data from previous paper. URL: https://huggingface.co/papers/2601.04582
[14.01.2026 08:33] Extract page data from URL. URL: https://huggingface.co/papers/2601.02669
[14.01.2026 08:33] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[14.01.2026 08:33] No deleted papers detected.
[14.01.2026 08:33] Downloading and parsing papers (pdf, html). Total: 15.
[14.01.2026 08:33] Downloading and parsing paper https://huggingface.co/papers/2601.07022.
[14.01.2026 08:33] Extra JSON file exists (./assets/json/2601.07022.json), skip PDF parsing.
[14.01.2026 08:33] Paper image links file exists (./assets/img_data/2601.07022.json), skip HTML parsing.
[14.01.2026 08:33] Success.
[14.01.2026 08:33] Downloading and parsing paper https://huggingface.co/papers/2601.08225.
[14.01.2026 08:33] Extra JSON file exists (./assets/json/2601.08225.json), skip PDF parsing.
[14.01.2026 08:33] Paper image links file exists (./assets/img_data/2601.08225.json), skip HTML parsing.
[14.01.2026 08:33] Success.
[14.01.2026 08:33] Downloading and parsing paper https://huggingface.co/papers/2512.24965.
[14.01.2026 08:33] Extra JSON file exists (./assets/json/2512.24965.json), skip PDF parsing.
[14.01.2026 08:33] Paper image links file exists (./assets/img_data/2512.24965.json), skip HTML parsing.
[14.01.2026 08:33] Success.
[14.01.2026 08:33] Downloading and parsing paper https://huggingface.co/papers/2601.06789.
[14.01.2026 08:33] Extra JSON file exists (./assets/json/2601.06789.json), skip PDF parsing.
[14.01.2026 08:33] Paper image links file exists (./assets/img_data/2601.06789.json), skip HTML parsing.
[14.01.2026 08:33] Success.
[14.01.2026 08:33] Downloading and parsing paper https://huggingface.co/papers/2601.04745.
[14.01.2026 08:33] Downloading paper 2601.04745 from https://arxiv.org/pdf/2601.04745v1...
[14.01.2026 08:33] Extracting affiliations from text.
[14.01.2026 08:33] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"2026-01-09 KnowMe-Bench: Benchmarking Person Understanding for Lifelong Digital Companions Tingyu Wu1,2*, Zhisheng Chen1,2*, Ziyan Weng5*, Shuhe Wang8, Chenglong Li4, Shuo Zhang2, Sen Hu2,3, Silin Wu6, Qizhen Lan2,7, Huacan Wang1,2, Ronghao Chen2,3, 1UCAS, 2QuantaAlpha, 3PKU, 4THU, 5CITYU-DG, 6HAINNU, 7UTHealth, 8NUS *These authors contributed equally to this work. Correspondence: Qizhen.Lan@uth.tmc.edu, chenronghao@alumni.pku.edu.cn, wanghuacan17@mails.ucas.ac.cn "
[14.01.2026 08:33] Response: ```python
['UCAS', 'QuantaAlpha', 'PKU', 'THU', 'CITYU-DG', 'HAINNU', 'UTHealth', 'NUS']
```
[14.01.2026 08:33] Deleting PDF ./assets/pdf/2601.04745.pdf.
[14.01.2026 08:33] Success.
[14.01.2026 08:33] Downloading and parsing paper https://huggingface.co/papers/2601.07264.
[14.01.2026 08:33] Extra JSON file exists (./assets/json/2601.07264.json), skip PDF parsing.
[14.01.2026 08:33] Paper image links file exists (./assets/img_data/2601.07264.json), skip HTML parsing.
[14.01.2026 08:33] Success.
[14.01.2026 08:33] Downloading and parsing paper https://huggingface.co/papers/2601.06487.
[14.01.2026 08:33] Extra JSON file exists (./assets/json/2601.06487.json), skip PDF parsing.
[14.01.2026 08:33] Paper image links file exists (./assets/img_data/2601.06487.json), skip HTML parsing.
[14.01.2026 08:33] Success.
[14.01.2026 08:33] Downloading and parsing paper https://huggingface.co/papers/2601.08584.
[14.01.2026 08:33] Extra JSON file exists (./assets/json/2601.08584.json), skip PDF parsing.
[14.01.2026 08:33] Paper image links file exists (./assets/img_data/2601.08584.json), skip HTML parsing.
[14.01.2026 08:33] Success.
[14.01.2026 08:33] Downloading and parsing paper https://huggingface.co/papers/2601.08828.
[14.01.2026 08:33] Extra JSON file exists (./assets/json/2601.08828.json), skip PDF parsing.
[14.01.2026 08:33] Paper image links file exists (./assets/img_data/2601.08828.json), skip HTML parsing.
[14.01.2026 08:33] Success.
[14.01.2026 08:33] Downloading and parsing paper https://huggingface.co/papers/2601.08665.
[14.01.2026 08:33] Extra JSON file exists (./assets/json/2601.08665.json), skip PDF parsing.
[14.01.2026 08:33] Paper image links file exists (./assets/img_data/2601.08665.json), skip HTML parsing.
[14.01.2026 08:33] Success.
[14.01.2026 08:33] Downloading and parsing paper https://huggingface.co/papers/2601.08587.
[14.01.2026 08:33] Extra JSON file exists (./assets/json/2601.08587.json), skip PDF parsing.
[14.01.2026 08:33] Paper image links file exists (./assets/img_data/2601.08587.json), skip HTML parsing.
[14.01.2026 08:33] Success.
[14.01.2026 08:33] Downloading and parsing paper https://huggingface.co/papers/2601.08468.
[14.01.2026 08:33] Extra JSON file exists (./assets/json/2601.08468.json), skip PDF parsing.
[14.01.2026 08:33] Paper image links file exists (./assets/img_data/2601.08468.json), skip HTML parsing.
[14.01.2026 08:33] Success.
[14.01.2026 08:33] Downloading and parsing paper https://huggingface.co/papers/2601.08303.
[14.01.2026 08:33] Extra JSON file exists (./assets/json/2601.08303.json), skip PDF parsing.
[14.01.2026 08:33] Paper image links file exists (./assets/img_data/2601.08303.json), skip HTML parsing.
[14.01.2026 08:33] Success.
[14.01.2026 08:33] Downloading and parsing paper https://huggingface.co/papers/2601.04582.
[14.01.2026 08:33] Extra JSON file exists (./assets/json/2601.04582.json), skip PDF parsing.
[14.01.2026 08:33] Paper image links file exists (./assets/img_data/2601.04582.json), skip HTML parsing.
[14.01.2026 08:33] Success.
[14.01.2026 08:33] Downloading and parsing paper https://huggingface.co/papers/2601.02669.
[14.01.2026 08:33] Downloading paper 2601.02669 from https://arxiv.org/pdf/2601.02669v1...
[14.01.2026 08:33] Extracting affiliations from text.
[14.01.2026 08:33] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Towards Comprehensive Stage-wise Benchmarking of Large Language Models in Fact-Checking Hongzhan Lin, Zixin Chen, Zhiqi Shen, Ziyang Luo, Zhen Ye, Jing Ma, Tat-Seng Chua and Guandong Xu 1 6 2 0 2 ] . [ 1 9 6 6 2 0 . 1 0 6 2 : r AbstractLarge Language Models (LLMs) are increasingly deployed in real-world fact-checking systems, yet existing evaluations focus predominantly on claim verification and overlook the broader fact-checking workflow, including claim extraction and evidence retrieval. This narrow focus prevents current benchmarks from revealing systematic reasoning failures, factual blind spots, and robustness limitations of modern LLMs. To bridge this gap, we present FactArena, fully automated arenastyle evaluation framework that conducts comprehensive, stagewise benchmarking of LLMs across the complete fact-checking pipeline. FactArena integrates three key components: (i) an LLM-driven fact-checking process that standardizes claim decomposition, evidence retrieval via tool-augmented interactions, and justification-based verdict prediction; (ii) an arena-styled judgment mechanism guided by consolidated reference guidelines to ensure unbiased and consistent pairwise comparisons across heterogeneous judge agents; and (iii) an arena-driven claimevolution module that adaptively generates more challenging and semantically controlled claims to probe LLMs factual robustness beyond fixed seed data. Across 16 state-of-the-art LLMs spanning seven model families, FactArena produces stable and interpretable rankings. Our analyses further reveal significant discrepancies between static claim-verification accuracy and endto-end fact-checking competence, highlighting the necessity of holistic evaluation. The proposed framework offers scalable and trustworthy paradigm for diagnosing LLMs factual reasoning, guiding future model development, and advancing the reliable deployment of LLMs in safety-critical fact-checking applications. Index TermsFact-checking, large language mode"
[14.01.2026 08:33] Response: ```python
[]
```
[14.01.2026 08:33] Extracting affiliations from text.
[14.01.2026 08:33] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Towards Comprehensive Stage-wise Benchmarking of Large Language Models in Fact-Checking Hongzhan Lin, Zixin Chen, Zhiqi Shen, Ziyang Luo, Zhen Ye, Jing Ma, Tat-Seng Chua and Guandong Xu 1 6 2 0 2 ] . [ 1 9 6 6 2 0 . 1 0 6 2 : r AbstractLarge Language Models (LLMs) are increasingly deployed in real-world fact-checking systems, yet existing evaluations focus predominantly on claim verification and overlook the broader fact-checking workflow, including claim extraction and evidence retrieval. This narrow focus prevents current benchmarks from revealing systematic reasoning failures, factual blind spots, and robustness limitations of modern LLMs. To bridge this gap, we present FactArena, fully automated arenastyle evaluation framework that conducts comprehensive, stagewise benchmarking of LLMs across the complete fact-checking pipeline. FactArena integrates three key components: (i) an LLM-driven fact-checking process that standardizes claim decomposition, evidence retrieval via tool-augmented interactions, and justification-based verdict prediction; (ii) an arena-styled judgment mechanism guided by consolidated reference guidelines to ensure unbiased and consistent pairwise comparisons across heterogeneous judge agents; and (iii) an arena-driven claimevolution module that adaptively generates more challenging and semantically controlled claims to probe LLMs factual robustness beyond fixed seed data. Across 16 state-of-the-art LLMs spanning seven model families, FactArena produces stable and interpretable rankings. Our analyses further reveal significant discrepancies between static claim-verification accuracy and endto-end fact-checking competence, highlighting the necessity of holistic evaluation. The proposed framework offers scalable and trustworthy paradigm for diagnosing LLMs factual reasoning, guiding future model development, and advancing the reliable deployment of LLMs in safety-critical fact-checking applications. Index TermsFact-checking, large language models, benchmark, trustworthy auditing. I. INTRODUCTION ARGE language models (LLMs) have significantly advanced the field of natural language processing (NLP), demonstrating remarkable improvements across numerous tasks [1], [2]. Previous studies have particularly highlighted the capability of LLMs to perform different tasks in the factchecking pipeline [3][5]. Despite these advancements, LLMs remain vulnerable to factual inaccuracies and are prone to errors in reasoning [6], [7]. Mistakes arising from flawed memorized knowledge or inadequate factual reasoning, can undermine their reliability and effectiveness in fact-checking applications [8][10]. Consequently, it is critical to systematically identify and understand the capabilities of LLMs in factHongzhan Lin, Zixin Chen, Jing Ma are with the Department of Computer Science, Hong Kong Baptist University. (E-mail: cshzlin@comp.hkbu.edu.hk) Zhiqi Shen, Ziyang Luo are with Salesforce AI Research. Zhen Ye is with The Hong Kong University of Science and Technology. Tat-Seng Chua is with National University of Singapore. Guandong Xu is with The Education University of Hong Kong. Fig. 1. The comparison of different focuses between our proposed FactArena and traditional solutions in fact-checking evaluation. Different from the traditional solutions that only focus on claim verification, FactArena aims to scrutinize the full stages of the fact-checking pipeline. checking to ensure their robustness and enhance their overall trustworthiness. As illustrated in Figure 1, the fact-checking process [11] typically consists of three critical stages: (i) claim extraction, where claims requiring verification are identified; (ii) evidence retrieval, where relevant sources supporting or refuting these claims are gathered; and (iii) claim verification, which involves assessing the veracity of the claims based on the retrieved evidence and providing justifications for the resulting verdict. Traditional approaches [12], [13] for evaluating LLM performance in fact-checking have primarily focused on the last stage: claim verification, while largely neglecting the equally critical earlier stages. Benchmarking LLM performance comprehensively across all fact-checking stages is essential, not solely claim verification. LLMs are increasingly deployed throughout the entire fact-checking pipeline, especially when addressing complex real-world claims. Overlooking any stage, especially the two preliminary stages, could severely compromise the integrity and effectiveness of the overall fact-checking systems. Thus, thorough and systematic assessment of LLM capabilities at every stage is indispensable. comprehensive auditing can yield deeper insights into the model limitations, thereby significantly advancing the reliable deployment of LLMs in practical fact-checking scenarios. Designing such an evaluation is inherently challenging, as the different tasks of the fact-checking pipeline may involve open-ended judgments that lack clear-cut ground truth. potential remedy lies in leveraging platforms such as Chatbot Arena [14], which crowdsource annotations from diverse users and employ pairwise comparisons to yield more nuanced and objective assessments of LLM capabilities. However, this strategy requires considerable data preparation and annotation resources [15], rendering it prohibitively costly when applied at scale. To address these challenges, we propose systematic, automated arena-style framework that evaluates LLMs across the entire fact-checking pipeline using unbiased collective judgments. The goal is to facilitate comprehensive and reliable assessments of LLMs fact-checking capacities. Our framework is designed around three key principles: 1) The framework scrutinizes all phases of the fact-checking process rather than limiting evaluation to claim verification, thereby capturing holistic view of model capabilities. 2) The framework must integrate diverse evaluator perspectives to ensure fair assessments of LLM performance. To ensure reliability as well as minimizing subjectivity in single judgment, we draw on the notion of collective intelligence [16], aligning judgments across heterogeneous evaluators by establishing consensus on factuality. 3) The framework automates the evaluation process to enable efficient and large-scale assessments. Inspired by agentic evaluation approaches [17], [18], we modularize the auditing of LLMs, thereby reducing reliance on costly manual annotation. In this work, we"
[14.01.2026 08:33] Mistral response. {"id": "232522ebf2464c64b04a4f00db3bfc01", "created": 1768379618, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1291, "total_tokens": 1344, "completion_tokens": 53}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\n    \"Department of Computer Science, Hong Kong Baptist University\",\n    \"Salesforce AI Research\",\n    \"The Hong Kong University of Science and Technology\",\n    \"National University of Singapore\",\n    \"The Education University of Hong Kong\"\n]\n```"}}]}
[14.01.2026 08:33] Response: ```python
[
    "Department of Computer Science, Hong Kong Baptist University",
    "Salesforce AI Research",
    "The Hong Kong University of Science and Technology",
    "National University of Singapore",
    "The Education University of Hong Kong"
]
```
[14.01.2026 08:33] Deleting PDF ./assets/pdf/2601.02669.pdf.
[14.01.2026 08:33] Success.
[14.01.2026 08:33] Enriching papers with extra data.
[14.01.2026 08:33] ********************************************************************************
[14.01.2026 08:33] Abstract 0. Solar Open presents a 102B-parameter bilingual Mixture-of-Experts language model that addresses data scarcity in underserved languages through synthetic data generation, progressive curriculum coordination, and scalable reinforcement learning optimization.  					AI-generated summary 				 We introduc...
[14.01.2026 08:33] ********************************************************************************
[14.01.2026 08:33] Abstract 1. Large reasoning models enable scalable multi-turn dialogue generation through automated task-oriented simulation and user-oriented behavioral modeling for enhanced human-agent interaction datasets.  					AI-generated summary 				 The recent paradigm shift toward large reasoning models (LRMs) as auto...
[14.01.2026 08:33] ********************************************************************************
[14.01.2026 08:33] Abstract 2. Building intelligent agents capable of dexterous manipulation is essential for achieving human-like automation in both robotics and digital environments. However, existing GUI agents rely on discrete click predictions (x,y), which prohibits free-form, closed-loop trajectories (e.g. dragging a progre...
[14.01.2026 08:33] ********************************************************************************
[14.01.2026 08:33] Abstract 3. MemGovern framework transforms unstructured GitHub data into structured experiential memory for autonomous software engineering agents, improving bug resolution rates through enhanced experience retrieval.  					AI-generated summary 				 While autonomous software engineering (SWE) agents are reshapi...
[14.01.2026 08:33] ********************************************************************************
[14.01.2026 08:33] Abstract 4. Long-horizon memory benchmarks based on autobiographical narratives evaluate models' ability to infer stable motivations and decision principles through evidence-linked questions spanning factual recall, subjective state attribution, and principle-level reasoning.  					AI-generated summary 				 Exi...
[14.01.2026 08:33] ********************************************************************************
[14.01.2026 08:33] Abstract 5. Tool-integrated language model agents exhibit different calibration behaviors based on tool type, with a reinforcement learning framework improving both task accuracy and reliable uncertainty estimation across diverse domains.  					AI-generated summary 				 Autonomous agents based on large language...
[14.01.2026 08:33] ********************************************************************************
[14.01.2026 08:33] Abstract 6. Reinforcement learning for large language model agents suffers from discrimination collapse in open-ended tasks due to pointwise scalar scoring, which ArenaRL addresses through relative ranking and pairwise evaluation mechanisms.  					AI-generated summary 				 Reinforcement learning has substantial...
[14.01.2026 08:33] ********************************************************************************
[14.01.2026 08:33] Abstract 7. The Ministral 3 series consists of parameter-efficient dense language models with three sizes (3B, 8B, 14B) and three variants per size, trained using cascade distillation for compute-constrained applications.  					AI-generated summary 				 We introduce the Ministral 3 series, a family of parameter...
[14.01.2026 08:33] ********************************************************************************
[14.01.2026 08:33] Abstract 8. Motive is a gradient-based data attribution framework that identifies influential video clips for motion improvement in text-to-video models through motion-weighted loss masking.  					AI-generated summary 				 Despite the rapid progress of video generation models, the role of data in influencing mo...
[14.01.2026 08:33] ********************************************************************************
[14.01.2026 08:33] Abstract 9. VLingNav enhances embodied navigation through linguistic-driven cognition with adaptive reasoning and visual-assisted memory, achieving state-of-the-art performance and zero-shot transfer to real robots.  					AI-generated summary 				 VLA models have shown promising potential in embodied navigation...
[14.01.2026 08:33] ********************************************************************************
[14.01.2026 08:33] Abstract 10. MoCha enables controllable video character replacement using a single frame mask through condition-aware RoPE and a comprehensive data construction pipeline with specialized datasets.  					AI-generated summary 				 Controllable video character replacement with a user-provided identity remains a cha...
[14.01.2026 08:33] ********************************************************************************
[14.01.2026 08:33] Abstract 11. Reinforcement learning with verifiable rewards is enhanced through a judge-then-generate paradigm that improves both efficiency and accuracy in mathematical problem-solving.  					AI-generated summary 				 Reinforcement Learning with Verifiable Rewards (RLVR) has become a standard paradigm for reaso...
[14.01.2026 08:33] ********************************************************************************
[14.01.2026 08:33] Abstract 12. An efficient diffusion transformer framework for mobile and edge devices that maintains high-generation quality while reducing computational costs through compact architecture, elastic training, and knowledge-guided distillation.  					AI-generated summary 				 Recent advances in diffusion transform...
[14.01.2026 08:33] ********************************************************************************
[14.01.2026 08:33] Abstract 13. A reinforcement learning framework for text-to-visualization generation that improves chart quality and code execution by optimizing multiple objectives using post-execution feedback.  					AI-generated summary 				 Text-to-Visualization (Text2Vis) systems translate natural language queries over tab...
[14.01.2026 08:33] ********************************************************************************
[14.01.2026 08:33] Abstract 14. FactArena is an automated evaluation framework that comprehensively assesses large language models across all stages of the fact-checking pipeline, revealing gaps between claim verification accuracy and overall fact-checking capability.  					AI-generated summary 				 Large Language Models (LLMs) ar...
[14.01.2026 08:33] Read previous papers.
[14.01.2026 08:33] Generating reviews via LLM API.
[14.01.2026 08:33] Using data from previous issue: {"categories": ["#low_resource", "#training", "#data", "#rlhf", "#synthetic", "#optimization", "#open_source", "#multilingual", "#reasoning", "#architecture"], "emoji": "üåç", "ru": {"title": "–°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –∫–∞–∫ –º–æ—Å—Ç –¥–ª—è —è–∑—ã–∫–æ–≤ —Å –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –±–æ–ª—å—à–∞—è —è–∑—ã–∫–æ–≤–∞—è 
[14.01.2026 08:33] Using data from previous issue: {"categories": ["#synthetic", "#reasoning"], "emoji": "üó£Ô∏è", "ru": {"title": "–°–∏–º—É–ª—è—Ü–∏—è —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–≥–æ –ø–æ–≤–µ–¥–µ–Ω–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö –¥–∏–∞–ª–æ–≥–æ–≤ —Å AI-–∞–≥–µ–Ω—Ç–∞–º–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –º–Ω–æ–≥–æ—Ö–æ–¥–æ–≤—ã—Ö –¥–∏–∞–ª–æ–≥–æ–≤ –º–µ–∂–¥—É —á–µ–ª–æ–≤–µ–∫–æ–º –∏ –∞–≥–µ–Ω—Ç–æ–º, –∏—Å–ø–æ–ª—å–∑—É—è –±–æ–ª—å—à
[14.01.2026 08:33] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#agents", "#small_models", "#architecture"], "emoji": "ü§ñ", "ru": {"title": "–ü–ª–∞–≤–Ω—ã–µ –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–∏ –≤ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞—Ö: –ø–æ—Ç–æ–∫–æ–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π –∞–≥–µ–Ω—Ç–æ–≤", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è ShowUI-œÄ ‚Äî –ø–µ—Ä–≤–∞—è –ø–æ—Ç–æ—á–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è
[14.01.2026 08:33] Using data from previous issue: {"categories": [], "emoji": "üß†", "ru": {"title": "–û–ø—ã—Ç —á–µ–ª–æ–≤–µ–∫–∞ –∫–∞–∫ –ø–∞–º—è—Ç—å –¥–ª—è –∞–≥–µ–Ω—Ç–æ–≤ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ MemGovern, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –Ω–µ—Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ GitHub –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é –ø–∞–º—è—Ç—å –æ–ø—ã—Ç–∞ –¥–ª—è –∞–≤—Ç–æ–Ω–æ–º–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–≥–æ –æ–±–µ—Å–ø–µ—á
[14.01.2026 08:33] Querying the API.
[14.01.2026 08:33] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Long-horizon memory benchmarks based on autobiographical narratives evaluate models' ability to infer stable motivations and decision principles through evidence-linked questions spanning factual recall, subjective state attribution, and principle-level reasoning.  					AI-generated summary 				 Existing long-horizon memory benchmarks mostly use multi-turn dialogues or synthetic user histories, which makes retrieval performance an imperfect proxy for person understanding. We present \BenchName, a publicly releasable benchmark built from long-form autobiographical narratives, where actions, context, and inner thoughts provide dense evidence for inferring stable motivations and decision principles. \BenchName~reconstructs each narrative into a flashback-aware, time-anchored stream and evaluates models with evidence-linked questions spanning factual recall, subjective state attribution, and principle-level reasoning. Across diverse narrative sources, retrieval-augmented systems mainly improve factual accuracy, while errors persist on temporally grounded explanations and higher-level inferences, highlighting the need for memory mechanisms beyond retrieval. Our data is in KnowMeBench{https://github.com/QuantaAlpha/KnowMeBench}.
[14.01.2026 08:33] Response: ```json
{
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ KnowMeBench –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø–æ–Ω–∏–º–∞—Ç—å –ª–∏—á–Ω–æ—Å—Ç—å —á–µ–ª–æ–≤–µ–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–ª–∏–Ω–Ω—ã—Ö –∞–≤—Ç–æ–±–∏–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏—Ö –Ω–∞—Ä—Ä–∞—Ç–∏–≤–æ–≤. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –º–µ—Ç–æ–¥–æ–≤, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏—Ö –¥–∏–∞–ª–æ–≥–∏ –∏–ª–∏ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –∏—Å—Ç–æ—Ä–∏–∏, –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç –ø–ª–æ—Ç–Ω—ã–µ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞ –≤ –≤–∏–¥–µ –¥–µ–π—Å—Ç–≤–∏–π, –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –º—ã—Å–ª–µ–π, –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–ª—è –≤—ã–≤–æ–¥–∞ —É—Å—Ç–æ–π—á–∏–≤—ã—Ö –º–æ—Ç–∏–≤–∞—Ü–∏–π –∏ –ø—Ä–∏–Ω—Ü–∏–ø–æ–≤ –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π. –ë–µ–Ω—á–º–∞—Ä–∫ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ –≤–æ–ø—Ä–æ—Å—ã —Ç—Ä—ë—Ö —É—Ä–æ–≤–Ω–µ–π —Å–ª–æ–∂–Ω–æ—Å—Ç–∏: —Ñ–∞–∫—Ç-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞ –∞—Ç—Ä–∏–±—É—Ü–∏–∏ —Å—É–±—ä–µ–∫—Ç–∏–≤–Ω—ã—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π –∏ –ª–æ–≥–∏–∫–æ-–ø—Ä–∏–Ω—Ü–∏–ø–∏–∞–ª—å–Ω—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —Å–∏—Å—Ç–µ–º—ã —Å –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ —É–ª—É—á—à–∞—é—Ç —Ç–æ–ª—å–∫–æ —Ç–æ—á–Ω–æ—Å—Ç—å —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö –æ—Ç–≤–µ—Ç–æ–≤, –∞ –æ—à–∏–±–∫–∏ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –ø—Ä–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–±—ä—è—Å–Ω–µ–Ω–∏—è—Ö –∏ –≤—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã—Ö –≤—ã–≤–æ–¥–∞—Ö, —á—Ç–æ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å —Ä–∞–∑–≤–∏—Ç–∏—è –º–µ—Ö–∞–Ω–∏–∑–º–æ–≤ –ø–∞–º—è—Ç–∏ —Å–≤–µ—Ä—Ö –ø—Ä–æ—Å—Ç–æ–≥–æ –ø–æ–∏—Å–∫–∞.",
  "emoji": "üìñ",
  "title": "–ü–æ–Ω–∏–º–∞–Ω–∏–µ –ª–∏—á–Ω–æ—Å—Ç–∏ —á–µ—Ä–µ–∑ –±–∏–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ –Ω–∞—Ä—Ä–∞—Ç–∏–≤—ã: –∫–æ–≥–¥–∞ –ø–æ–∏—Å–∫ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ"
}
```
[14.01.2026 08:33] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Long-horizon memory benchmarks based on autobiographical narratives evaluate models' ability to infer stable motivations and decision principles through evidence-linked questions spanning factual recall, subjective state attribution, and principle-level reasoning.  					AI-generated summary 				 Existing long-horizon memory benchmarks mostly use multi-turn dialogues or synthetic user histories, which makes retrieval performance an imperfect proxy for person understanding. We present \BenchName, a publicly releasable benchmark built from long-form autobiographical narratives, where actions, context, and inner thoughts provide dense evidence for inferring stable motivations and decision principles. \BenchName~reconstructs each narrative into a flashback-aware, time-anchored stream and evaluates models with evidence-linked questions spanning factual recall, subjective state attribution, and principle-level reasoning. Across diverse narrative sources, retrieval-augmented systems mainly improve factual accuracy, while errors persist on temporally grounded explanations and higher-level inferences, highlighting the need for memory mechanisms beyond retrieval. Our data is in KnowMeBench{https://github.com/QuantaAlpha/KnowMeBench}."

[14.01.2026 08:33] Response: ```python
["DATASET", "BENCHMARK", "RAG"]
```

**Justification:**
- **DATASET**: The paper introduces "KnowMeBench," a new publicly releasable benchmark built from long-form autobiographical narratives.
- **BENCHMARK**: The paper explicitly proposes a benchmark for evaluating models' ability to infer stable motivations and decision principles through evidence-linked questions.
- **RAG**: The paper evaluates retrieval-augmented systems and discusses how they improve factual accuracy, directly addressing retrieval-augmented generation techniques.
[14.01.2026 08:33] Error. Failed to parse JSON from LLM. ["DATASET", "BENCHMARK", "RAG"]


**Justification:**
- **DATASET**: The paper introduces "KnowMeBench," a new publicly releasable benchmark built from long-form autobiographical narratives.
- **BENCHMARK**: The paper explicitly proposes a benchmark for evaluating models" ability to infer stable motivations and decision principles through evidence-linked questions.
- **RAG**: The paper evaluates retrieval-augmented systems and discusses how they improve factual accuracy, directly addressing retrieval-augmented generation techniques.
[14.01.2026 08:33] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Long-horizon memory benchmarks based on autobiographical narratives evaluate models' ability to infer stable motivations and decision principles through evidence-linked questions spanning factual recall, subjective state attribution, and principle-level reasoning.  					AI-generated summary 				 Existing long-horizon memory benchmarks mostly use multi-turn dialogues or synthetic user histories, which makes retrieval performance an imperfect proxy for person understanding. We present \BenchName, a publicly releasable benchmark built from long-form autobiographical narratives, where actions, context, and inner thoughts provide dense evidence for inferring stable motivations and decision principles. \BenchName~reconstructs each narrative into a flashback-aware, time-anchored stream and evaluates models with evidence-linked questions spanning factual recall, subjective state attribution, and principle-level reasoning. Across diverse narrative sources, retrieval-augmented systems mainly improve factual accuracy, while errors persist on temporally grounded explanations and higher-level inferences, highlighting the need for memory mechanisms beyond retrieval. Our data is in KnowMeBench{https://github.com/QuantaAlpha/KnowMeBench}."

[14.01.2026 08:33] Response: ```python
['LONG_CONTEXT', 'REASONING', 'OPEN_SOURCE']
```
[14.01.2026 08:33] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces a new benchmark called \\\\\\\\BenchName, designed to assess machine learning models\' understanding of long-term memory through autobiographical narratives. Unlike previous benchmarks that relied on dialogues or synthetic histories, \\\\\\\\BenchName uses real-life stories to provide rich context for evaluating models\' abilities to infer motivations and decision-making principles. The benchmark includes questions that require factual recall, understanding of subjective states, and reasoning at a principle level. The findings indicate that while retrieval-augmented systems can improve factual accuracy, they struggle with temporally grounded explanations and higher-level reasoning, suggesting a need for advanced memory mechanisms in AI.","title":"Unlocking Long-Term Memory in AI with Autobiographical Narratives"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="This paper introduces a new benchmark called \\\\BenchName, designed to assess machine learning models' understanding of long-term memory through autobiographical narratives. Unlike previous benchmarks that relied on dialogues or synthetic histories, \\\\BenchName uses real-life stories to provide rich context for evaluating models' abilities to infer motivations and decision-making principles. The benchmark includes questions that require factual recall, understanding of subjective states, and reasoning at a principle level. The findings indicate that while retrieval-augmented systems can improve factual accuracy, they struggle with temporally grounded explanations and higher-level reasoning, suggesting a need for advanced memory mechanisms in AI.", title='Unlocking Long-Term Memory in AI with Autobiographical Narratives'))
[14.01.2026 08:33] Response: ParsedChatCompletionMessage[Article](content='{"desc":"ËøôÁØáËÆ∫Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂü∫ÂáÜÊµãËØïÔºåÂêç‰∏∫\\\\BenchNameÔºåÊó®Âú®ËØÑ‰º∞Êú∫Âô®Â≠¶‰π†Ê®°ÂûãÂú®ÈïøÊó∂Èó¥ËÆ∞ÂøÜÊñπÈù¢ÁöÑË°®Áé∞„ÄÇËØ•Âü∫ÂáÜÊµãËØïÂü∫‰∫éËá™‰º†ÂèôËø∞ÔºåÊèê‰æõ‰∫Ü‰∏∞ÂØåÁöÑËØÅÊçÆÊù•Êé®Êñ≠Á®≥ÂÆöÁöÑÂä®Êú∫ÂíåÂÜ≥Á≠ñÂéüÂàô„ÄÇ‰∏éÁé∞ÊúâÁöÑÂü∫ÂáÜÊµãËØï‰∏çÂêåÔºå\\\\BenchNameÂÖ≥Ê≥®‰∫é‰∫ãÂÆûÂõûÂøÜ„ÄÅ‰∏ªËßÇÁä∂ÊÄÅÂΩíÂõ†ÂíåÂéüÂàôÁ∫ßÊé®ÁêÜÁ≠âÊñπÈù¢„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÁé∞ÊúâÁöÑÊ£ÄÁ¥¢Â¢ûÂº∫Á≥ªÁªüÂú®‰∫ãÂÆûÂáÜÁ°ÆÊÄß‰∏äÊúâÊâÄÊèêÈ´òÔºå‰ΩÜÂú®Êó∂Èó¥Âü∫Á°ÄÁöÑËß£ÈáäÂíåÊõ¥È´òÂ±ÇÊ¨°ÁöÑÊé®ÁêÜ‰∏ä‰ªçÂ≠òÂú®ÈîôËØØÔºåÂº∫Ë∞É‰∫ÜÈúÄË¶ÅË∂ÖË∂äÁÆÄÂçïÊ£ÄÁ¥¢ÁöÑËÆ∞ÂøÜÊú∫Âà∂„ÄÇ","title":"ÈïøÊó∂ËÆ∞ÂøÜËØÑ‰º∞ÁöÑÊñ∞Âü∫ÂáÜÊµãËØï"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='ËøôÁØáËÆ∫Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂü∫ÂáÜÊµãËØïÔºåÂêç‰∏∫\\BenchNameÔºåÊó®Âú®ËØÑ‰º∞Êú∫Âô®Â≠¶‰π†Ê®°ÂûãÂú®ÈïøÊó∂Èó¥ËÆ∞ÂøÜÊñπÈù¢ÁöÑË°®Áé∞„ÄÇËØ•Âü∫ÂáÜÊµãËØïÂü∫‰∫éËá™‰º†ÂèôËø∞ÔºåÊèê‰æõ‰∫Ü‰∏∞ÂØåÁöÑËØÅÊçÆÊù•Êé®Êñ≠Á®≥ÂÆöÁöÑÂä®Êú∫ÂíåÂÜ≥Á≠ñÂéüÂàô„ÄÇ‰∏éÁé∞ÊúâÁöÑÂü∫ÂáÜÊµãËØï‰∏çÂêåÔºå\\BenchNameÂÖ≥Ê≥®‰∫é‰∫ãÂÆûÂõûÂøÜ„ÄÅ‰∏ªËßÇÁä∂ÊÄÅÂΩíÂõ†ÂíåÂéüÂàôÁ∫ßÊé®ÁêÜÁ≠âÊñπÈù¢„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÁé∞ÊúâÁöÑÊ£ÄÁ¥¢Â¢ûÂº∫Á≥ªÁªüÂú®‰∫ãÂÆûÂáÜÁ°ÆÊÄß‰∏äÊúâÊâÄÊèêÈ´òÔºå‰ΩÜÂú®Êó∂Èó¥Âü∫Á°ÄÁöÑËß£ÈáäÂíåÊõ¥È´òÂ±ÇÊ¨°ÁöÑÊé®ÁêÜ‰∏ä‰ªçÂ≠òÂú®ÈîôËØØÔºåÂº∫Ë∞É‰∫ÜÈúÄË¶ÅË∂ÖË∂äÁÆÄÂçïÊ£ÄÁ¥¢ÁöÑËÆ∞ÂøÜÊú∫Âà∂„ÄÇ', title='ÈïøÊó∂ËÆ∞ÂøÜËØÑ‰º∞ÁöÑÊñ∞Âü∫ÂáÜÊµãËØï'))
[14.01.2026 08:33] Using data from previous issue: {"categories": ["#benchmark", "#agents", "#rl"], "emoji": "üéØ", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞–¥–µ–∂–Ω–æ –≤—ã—Ä–∞–∂–∞—Ç—å –Ω–µ—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤", "desc": "–†–∞–±–æ—Ç–∞ –∏—Å—Å–ª–µ–¥—É–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏ (—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ –ø—Ä–∞–≤–∏–ª—å–Ω–æ –æ—Ü–µ–Ω–∏–≤–∞—Ç—å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤ —Å–≤–æ–∏—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è—Ö) –≤ LLM-–∞–≥–µ–Ω—Ç–∞—Ö, –∫–æ—Ç
[14.01.2026 08:33] Using data from previous issue: {"categories": ["#alignment", "#benchmark", "#rl", "#optimization", "#reasoning", "#dataset", "#agents", "#training"], "emoji": "üèÜ", "ru": {"title": "–û—Ç –ø–æ—Ç–æ—á–µ—á–Ω—ã—Ö –æ—Ü–µ–Ω–æ–∫ –∫ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–º—É —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—é: —Ç—É—Ä–Ω–∏—Ä–Ω—ã–π —Å–ø–æ—Å–æ–± –æ–±—É—á–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ ArenaRL –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä
[14.01.2026 08:33] Using data from previous issue: {"categories": [], "emoji": "üöÄ", "ru": {"title": "–ö–æ–º–ø–∞–∫—Ç–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –≤—Å–µ—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã Ministral 3 ‚Äî —Å–µ–º–µ–π—Å—Ç–≤–æ –∫–æ–º–ø–∞–∫—Ç–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Ä–∞–∑–º–µ—Ä–æ–º –æ—Ç 3 –¥–æ 14 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏. –î–ª—è –∫–∞–∂–¥–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ –ø—Ä–µ–¥–ª–æ
[14.01.2026 08:33] Using data from previous issue: {"categories": ["#video", "#data", "#training"], "emoji": "üé¨", "ru": {"title": "–ê—Ç—Ä–∏–±—É—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –¥–≤–∏–∂–µ–Ω–∏—è –≤ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –≤–∏–¥–µ–æ–º–æ–¥–µ–ª—è—Ö", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω Motive ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∞—Ç—Ä–∏–±—É—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞—Ö, –∫–æ—Ç–æ—Ä—ã–π –≤—ã—è–≤–ª—è–µ—Ç –≤–ª–∏—è—Ç–µ–ª—å–Ω—ã–µ –≤–∏–¥–µ–æ–∫–ª–∏–ø—ã –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è
[14.01.2026 08:33] Using data from previous issue: {"categories": ["#rl", "#transfer_learning", "#multimodal", "#reasoning", "#dataset", "#robotics", "#agents", "#training"], "emoji": "ü§ñ", "ru": {"title": "–†–∞—Å—Å—É–∂–¥–∞—é—â–∞—è –Ω–∞–≤–∏–≥–∞—Ü–∏—è: –∫–æ–≥–¥–∞ —Ä–æ–±–æ—Ç –¥—É–º–∞–µ—Ç, –ø—Ä–µ–∂–¥–µ —á–µ–º –¥–µ–π—Å—Ç–≤–æ–≤–∞—Ç—å", "desc": "VLingNav ‚Äî —ç—Ç–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –Ω–∞–≤–∏–≥–∞—Ü–∏–∏ —Ä–æ–±–æ—Ç–æ–≤, –∫–æ—Ç–æ—Ä–∞
[14.01.2026 08:33] Using data from previous issue: {"categories": ["#rl", "#multimodal", "#data", "#dataset", "#video", "#architecture"], "emoji": "üé¨", "ru": {"title": "–ó–∞–º–µ–Ω–∞ –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π –≤ –≤–∏–¥–µ–æ –ø–æ –æ–¥–Ω–æ–π –º–∞—Å–∫–µ –±–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π", "desc": "MoCha ‚Äî —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –∑–∞–º–µ–Ω—ã –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π –≤ –≤–∏–¥–µ–æ, –∏—Å–ø–æ–ª—å–∑—É—è —Ç–æ–ª—å–∫–æ –æ–¥–Ω—É –º–∞—Å–∫—É –∏–∑ –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω–æ–≥–æ –∫–∞–¥—Ä–∞ –≤–º–µ—Å—Ç–æ
[14.01.2026 08:33] Using data from previous issue: {"categories": ["#rlhf", "#rl", "#optimization", "#reasoning", "#math", "#training"], "emoji": "‚öñÔ∏è", "ru": {"title": "–°—É–¥—å—è —Å–Ω–∞—á–∞–ª–∞, –∑–∞—Ç–µ–º –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä: –æ–±—É—á–µ–Ω–∏–µ –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω JudgeRLVR ‚Äî –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –∏ –ø—Ä–æ–≤–µ—Ä—è
[14.01.2026 08:33] Using data from previous issue: {"categories": ["#architecture", "#small_models", "#training", "#inference"], "emoji": "üì±", "ru": {"title": "–ö–æ–º–ø–∞–∫—Ç–Ω—ã–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã –¥–ª—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤ –Ω–∞ –ª–∞–¥–æ–Ω–∏", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–æ–≥–æ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ –¥–ª—è –º–æ–±–∏–ª—å–Ω—ã—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤, –∫–æ—Ç–æ—Ä–∞—è —Å–æ—á–µ—Ç–∞–µ—Ç –∞–¥–∞–ø
[14.01.2026 08:33] Using data from previous issue: {"categories": ["#open_source", "#benchmark", "#rl", "#optimization", "#multimodal", "#training"], "emoji": "üìä", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç RL-Text2Vis ‚Äî –ø–µ—Ä–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏
[14.01.2026 08:33] Querying the API.
[14.01.2026 08:33] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

FactArena is an automated evaluation framework that comprehensively assesses large language models across all stages of the fact-checking pipeline, revealing gaps between claim verification accuracy and overall fact-checking capability.  					AI-generated summary 				 Large Language Models (LLMs) are increasingly deployed in real-world fact-checking systems, yet existing evaluations focus predominantly on claim verification and overlook the broader fact-checking workflow, including claim extraction and evidence retrieval. This narrow focus prevents current benchmarks from revealing systematic reasoning failures, factual blind spots, and robustness limitations of modern LLMs. To bridge this gap, we present FactArena, a fully automated arena-style evaluation framework that conducts comprehensive, stage-wise benchmarking of LLMs across the complete fact-checking pipeline. FactArena integrates three key components: (i) an LLM-driven fact-checking process that standardizes claim decomposition, evidence retrieval via tool-augmented interactions, and justification-based verdict prediction; (ii) an arena-styled judgment mechanism guided by consolidated reference guidelines to ensure unbiased and consistent pairwise comparisons across heterogeneous judge agents; and (iii) an arena-driven claim-evolution module that adaptively generates more challenging and semantically controlled claims to probe LLMs' factual robustness beyond fixed seed data. Across 16 state-of-the-art LLMs spanning seven model families, FactArena produces stable and interpretable rankings. Our analyses further reveal significant discrepancies between static claim-verification accuracy and end-to-end fact-checking competence, highlighting the necessity of holistic evaluation. The proposed framework offers a scalable and trustworthy paradigm for diagnosing LLMs' factual reasoning, guiding future model development, and advancing the reliable deployment of LLMs in safety-critical fact-checking applications.
[14.01.2026 08:33] Response: ```json
{
  "desc": "FactArena –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å–∏—Å—Ç–µ–º—É –æ—Ü–µ–Ω–∫–∏, –∫–æ—Ç–æ—Ä–∞—è –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –≤—Å–µ—Ö —ç—Ç–∞–ø–∞—Ö –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ñ–∞–∫—Ç–æ–≤, –≤–∫–ª—é—á–∞—è –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–π, –ø–æ–∏—Å–∫ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤ –∏ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—é. –§—Ä–µ–π–º–≤–æ—Ä–∫ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç LLM-—É–ø—Ä–∞–≤–ª—è–µ–º—ã–π –ø—Ä–æ—Ü–µ—Å—Å —Ñ–∞–∫—Ç-—á–µ–∫–∏–Ω–≥–∞ —Å –º–µ—Ö–∞–Ω–∏–∑–º–æ–º –∞—Ä–µ–Ω—ã-—Å—Ç–∏–ª—è –¥–ª—è –æ–±—ä–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –∏ –º–æ–¥—É–ª—å —ç–≤–æ–ª—é—Ü–∏–∏ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–π –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã—Ö –ø—Ä–æ–≤–µ—Ä–æ—á–Ω—ã—Ö —Å–ª—É—á–∞–µ–≤. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –Ω–∞ 16 —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö LLM –ø–æ–∫–∞–∑–∞–ª–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏—è –º–µ–∂–¥—É —Ç–æ—á–Ω–æ—Å—Ç—å—é –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–π –∏ –æ–±—â–µ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å—é –≤—ã–ø–æ–ª–Ω—è—Ç—å –ø–æ–ª–Ω—ã–π –∫–æ–Ω–≤–µ–π–µ—Ä –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ñ–∞–∫—Ç–æ–≤. –†–∞–±–æ—Ç–∞ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–µ—Ç –≤–∞–∂–Ω–æ—Å—Ç—å –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ–≥–æ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è LLM –≤ –∫—Ä–∏—Ç–∏—á–Ω—ã—Ö –ø–æ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è—Ö —Ñ–∞–∫—Ç-—á–µ–∫–∏–Ω–≥–∞.",
  "emoji": "üîç",
  "title": "–û—Ç –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∫ —Ü–µ–ª–æ—Å—Ç–Ω–æ–π –æ—Ü–µ–Ω–∫–µ: –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –ø—Ä–æ–≤–µ—Ä—è—Ç—å —Ñ–∞–∫—Ç—ã"
}
```
[14.01.2026 08:33] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"FactArena is an automated evaluation framework that comprehensively assesses large language models across all stages of the fact-checking pipeline, revealing gaps between claim verification accuracy and overall fact-checking capability.  					AI-generated summary 				 Large Language Models (LLMs) are increasingly deployed in real-world fact-checking systems, yet existing evaluations focus predominantly on claim verification and overlook the broader fact-checking workflow, including claim extraction and evidence retrieval. This narrow focus prevents current benchmarks from revealing systematic reasoning failures, factual blind spots, and robustness limitations of modern LLMs. To bridge this gap, we present FactArena, a fully automated arena-style evaluation framework that conducts comprehensive, stage-wise benchmarking of LLMs across the complete fact-checking pipeline. FactArena integrates three key components: (i) an LLM-driven fact-checking process that standardizes claim decomposition, evidence retrieval via tool-augmented interactions, and justification-based verdict prediction; (ii) an arena-styled judgment mechanism guided by consolidated reference guidelines to ensure unbiased and consistent pairwise comparisons across heterogeneous judge agents; and (iii) an arena-driven claim-evolution module that adaptively generates more challenging and semantically controlled claims to probe LLMs' factual robustness beyond fixed seed data. Across 16 state-of-the-art LLMs spanning seven model families, FactArena produces stable and interpretable rankings. Our analyses further reveal significant discrepancies between static claim-verification accuracy and end-to-end fact-checking competence, highlighting the necessity of holistic evaluation. The proposed framework offers a scalable and trustworthy paradigm for diagnosing LLMs' factual reasoning, guiding future model development, and advancing the reliable deployment of LLMs in safety-critical fact-checking applications."

[14.01.2026 08:33] Response: ```python
["BENCHMARK", "AGENTS", "DATASET"]
```

**Justification:**

- **BENCHMARK**: FactArena is explicitly described as "an automated evaluation framework" and "arena-style evaluation framework" that conducts "comprehensive, stage-wise benchmarking of LLMs." It proposes a model evaluation framework for assessing LLMs across the fact-checking pipeline.

- **AGENTS**: The paper mentions "judge agents" and "LLM-driven fact-checking process" where LLMs act as autonomous agents performing tasks like claim decomposition, evidence retrieval via tool-augmented interactions, and verdict prediction.

- **DATASET**: The paper introduces FactArena with an "arena-driven claim-evolution module that adaptively generates more challenging and semantically controlled claims," which constitutes creating/modifying evaluation data for benchmarking purposes.
[14.01.2026 08:33] Error. Failed to parse JSON from LLM. ["BENCHMARK", "AGENTS", "DATASET"]


**Justification:**

- **BENCHMARK**: FactArena is explicitly described as "an automated evaluation framework" and "arena-style evaluation framework" that conducts "comprehensive, stage-wise benchmarking of LLMs." It proposes a model evaluation framework for assessing LLMs across the fact-checking pipeline.

- **AGENTS**: The paper mentions "judge agents" and "LLM-driven fact-checking process" where LLMs act as autonomous agents performing tasks like claim decomposition, evidence retrieval via tool-augmented interactions, and verdict prediction.

- **DATASET**: The paper introduces FactArena with an "arena-driven claim-evolution module that adaptively generates more challenging and semantically controlled claims," which constitutes creating/modifying evaluation data for benchmarking purposes.
[14.01.2026 08:33] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"FactArena is an automated evaluation framework that comprehensively assesses large language models across all stages of the fact-checking pipeline, revealing gaps between claim verification accuracy and overall fact-checking capability.  					AI-generated summary 				 Large Language Models (LLMs) are increasingly deployed in real-world fact-checking systems, yet existing evaluations focus predominantly on claim verification and overlook the broader fact-checking workflow, including claim extraction and evidence retrieval. This narrow focus prevents current benchmarks from revealing systematic reasoning failures, factual blind spots, and robustness limitations of modern LLMs. To bridge this gap, we present FactArena, a fully automated arena-style evaluation framework that conducts comprehensive, stage-wise benchmarking of LLMs across the complete fact-checking pipeline. FactArena integrates three key components: (i) an LLM-driven fact-checking process that standardizes claim decomposition, evidence retrieval via tool-augmented interactions, and justification-based verdict prediction; (ii) an arena-styled judgment mechanism guided by consolidated reference guidelines to ensure unbiased and consistent pairwise comparisons across heterogeneous judge agents; and (iii) an arena-driven claim-evolution module that adaptively generates more challenging and semantically controlled claims to probe LLMs' factual robustness beyond fixed seed data. Across 16 state-of-the-art LLMs spanning seven model families, FactArena produces stable and interpretable rankings. Our analyses further reveal significant discrepancies between static claim-verification accuracy and end-to-end fact-checking competence, highlighting the necessity of holistic evaluation. The proposed framework offers a scalable and trustworthy paradigm for diagnosing LLMs' factual reasoning, guiding future model development, and advancing the reliable deployment of LLMs in safety-critical fact-checking applications."

[14.01.2026 08:33] Response: ```python
['INTERPRETABILITY', 'REASONING', 'HALLUCINATIONS']
```

**Justification:**

- **INTERPRETABILITY**: The paper focuses on analyzing and explaining LLM behavior through comprehensive evaluation across the fact-checking pipeline, revealing systematic reasoning failures and factual blind spots.

- **REASONING**: The paper explicitly addresses "factual reasoning" capabilities and evaluates LLMs' logical reasoning through claim decomposition, evidence retrieval, and verdict prediction stages.

- **HALLUCINATIONS**: The paper addresses "factual blind spots" and "robustness limitations" of LLMs in fact-checking, which relates to understanding and evaluating hallucinations and factual errors in model outputs.
[14.01.2026 08:33] Error. Failed to parse JSON from LLM. ["INTERPRETABILITY", "REASONING", "HALLUCINATIONS"]


**Justification:**

- **INTERPRETABILITY**: The paper focuses on analyzing and explaining LLM behavior through comprehensive evaluation across the fact-checking pipeline, revealing systematic reasoning failures and factual blind spots.

- **REASONING**: The paper explicitly addresses "factual reasoning" capabilities and evaluates LLMs" logical reasoning through claim decomposition, evidence retrieval, and verdict prediction stages.

- **HALLUCINATIONS**: The paper addresses "factual blind spots" and "robustness limitations" of LLMs in fact-checking, which relates to understanding and evaluating hallucinations and factual errors in model outputs.
[14.01.2026 08:34] Response: ParsedChatCompletionMessage[Article](content='{"desc":"FactArena is a new evaluation framework designed to assess large language models (LLMs) throughout the entire fact-checking process, not just their ability to verify claims. It identifies weaknesses in LLMs by examining their performance in claim extraction, evidence retrieval, and justification of verdicts. The framework includes a standardized process for fact-checking, a fair judgment system for comparing models, and a module that generates challenging claims to test LLMs\' robustness. By revealing gaps between claim verification accuracy and overall fact-checking ability, FactArena aims to improve the reliability of LLMs in critical applications.","title":"FactArena: Holistic Evaluation for Reliable Fact-Checking in LLMs"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="FactArena is a new evaluation framework designed to assess large language models (LLMs) throughout the entire fact-checking process, not just their ability to verify claims. It identifies weaknesses in LLMs by examining their performance in claim extraction, evidence retrieval, and justification of verdicts. The framework includes a standardized process for fact-checking, a fair judgment system for comparing models, and a module that generates challenging claims to test LLMs' robustness. By revealing gaps between claim verification accuracy and overall fact-checking ability, FactArena aims to improve the reliability of LLMs in critical applications.", title='FactArena: Holistic Evaluation for Reliable Fact-Checking in LLMs'))
[14.01.2026 08:34] Response: ParsedChatCompletionMessage[Article](content='{"desc":"FactArenaÊòØ‰∏Ä‰∏™Ëá™Âä®ÂåñËØÑ‰º∞Ê°ÜÊû∂ÔºåÂÖ®Èù¢ËØÑ‰º∞Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®‰∫ãÂÆûÊ†∏Êü•ÊµÅÁ®ã‰∏≠ÁöÑÂêÑ‰∏™Èò∂ÊÆµÔºåÊè≠Á§∫‰∫ÜÂ£∞ÊòéÈ™åËØÅÂáÜÁ°ÆÊÄß‰∏éÊï¥‰Ωì‰∫ãÂÆûÊ†∏Êü•ËÉΩÂäõ‰πãÈó¥ÁöÑÂ∑ÆË∑ù„ÄÇÁé∞ÊúâËØÑ‰º∞‰∏ªË¶ÅÈõÜ‰∏≠Âú®Â£∞ÊòéÈ™åËØÅÔºåÂøΩËßÜ‰∫ÜÊõ¥ÂπøÊ≥õÁöÑ‰∫ãÂÆûÊ†∏Êü•Â∑•‰ΩúÊµÅÁ®ãÔºåÂåÖÊã¨Â£∞ÊòéÊèêÂèñÂíåËØÅÊçÆÊ£ÄÁ¥¢„ÄÇFactArenaÈÄöËøáÊ†áÂáÜÂåñÁöÑÂ£∞ÊòéÂàÜËß£„ÄÅÂ∑•ÂÖ∑Â¢ûÂº∫ÁöÑËØÅÊçÆÊ£ÄÁ¥¢ÂíåÂü∫‰∫éÁêÜÁî±ÁöÑÂà§ÂÜ≥È¢ÑÊµãÔºåÊèê‰æõ‰∫ÜÂÖ®Èù¢ÁöÑÈò∂ÊÆµÊÄßÂü∫ÂáÜÊµãËØï„ÄÇËØ•Ê°ÜÊû∂‰∏∫ËØäÊñ≠Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑ‰∫ãÂÆûÊé®ÁêÜÊèê‰æõ‰∫ÜÂèØÊâ©Â±ïÂíåÂèØÈù†ÁöÑËåÉÂºèÔºåÊé®Âä®‰∫ÜÂú®ÂÆâÂÖ®ÂÖ≥ÈîÆÁöÑ‰∫ãÂÆûÊ†∏Êü•Â∫îÁî®‰∏≠ÂèØÈù†ÈÉ®ÁΩ≤Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑËøõÂ±ï„ÄÇ","title":"ÂÖ®Èù¢ËØÑ‰º∞ËØ≠Ë®ÄÊ®°ÂûãÁöÑ‰∫ãÂÆûÊ†∏Êü•ËÉΩÂäõ"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='FactArenaÊòØ‰∏Ä‰∏™Ëá™Âä®ÂåñËØÑ‰º∞Ê°ÜÊû∂ÔºåÂÖ®Èù¢ËØÑ‰º∞Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®‰∫ãÂÆûÊ†∏Êü•ÊµÅÁ®ã‰∏≠ÁöÑÂêÑ‰∏™Èò∂ÊÆµÔºåÊè≠Á§∫‰∫ÜÂ£∞ÊòéÈ™åËØÅÂáÜÁ°ÆÊÄß‰∏éÊï¥‰Ωì‰∫ãÂÆûÊ†∏Êü•ËÉΩÂäõ‰πãÈó¥ÁöÑÂ∑ÆË∑ù„ÄÇÁé∞ÊúâËØÑ‰º∞‰∏ªË¶ÅÈõÜ‰∏≠Âú®Â£∞ÊòéÈ™åËØÅÔºåÂøΩËßÜ‰∫ÜÊõ¥ÂπøÊ≥õÁöÑ‰∫ãÂÆûÊ†∏Êü•Â∑•‰ΩúÊµÅÁ®ãÔºåÂåÖÊã¨Â£∞ÊòéÊèêÂèñÂíåËØÅÊçÆÊ£ÄÁ¥¢„ÄÇFactArenaÈÄöËøáÊ†áÂáÜÂåñÁöÑÂ£∞ÊòéÂàÜËß£„ÄÅÂ∑•ÂÖ∑Â¢ûÂº∫ÁöÑËØÅÊçÆÊ£ÄÁ¥¢ÂíåÂü∫‰∫éÁêÜÁî±ÁöÑÂà§ÂÜ≥È¢ÑÊµãÔºåÊèê‰æõ‰∫ÜÂÖ®Èù¢ÁöÑÈò∂ÊÆµÊÄßÂü∫ÂáÜÊµãËØï„ÄÇËØ•Ê°ÜÊû∂‰∏∫ËØäÊñ≠Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑ‰∫ãÂÆûÊé®ÁêÜÊèê‰æõ‰∫ÜÂèØÊâ©Â±ïÂíåÂèØÈù†ÁöÑËåÉÂºèÔºåÊé®Âä®‰∫ÜÂú®ÂÆâÂÖ®ÂÖ≥ÈîÆÁöÑ‰∫ãÂÆûÊ†∏Êü•Â∫îÁî®‰∏≠ÂèØÈù†ÈÉ®ÁΩ≤Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑËøõÂ±ï„ÄÇ', title='ÂÖ®Èù¢ËØÑ‰º∞ËØ≠Ë®ÄÊ®°ÂûãÁöÑ‰∫ãÂÆûÊ†∏Êü•ËÉΩÂäõ'))
[14.01.2026 08:34] Renaming data file.
[14.01.2026 08:34] Renaming previous data. hf_papers.json to ./d/2026-01-14.json
[14.01.2026 08:34] Saving new data file.
[14.01.2026 08:34] Generating page.
[14.01.2026 08:34] Renaming previous page.
[14.01.2026 08:34] Renaming previous data. index.html to ./d/2026-01-14.html
[14.01.2026 08:34] Writing result.
[14.01.2026 08:34] Renaming log file.
[14.01.2026 08:34] Renaming previous data. log.txt to ./logs/2026-01-14_last_log.txt
