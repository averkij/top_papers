[05.06.2025 17:09] Read previous papers.
[05.06.2025 17:09] Generating top page (month).
[05.06.2025 17:09] Writing top page (month).
[05.06.2025 19:09] Read previous papers.
[05.06.2025 19:09] Get feed.
[05.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2506.03569
[05.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2506.04207
[05.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2506.04089
[05.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.16968
[05.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2506.02921
[05.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2506.04180
[05.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2506.04141
[05.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2506.04142
[05.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2506.04225
[05.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2506.03150
[05.06.2025 19:09] Extract page data from URL. URL: https://huggingface.co/papers/2506.04178
[05.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2506.04158
[05.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2506.01320
[05.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2506.03930
[05.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2506.03295
[05.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2506.04228
[05.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2506.03139
[05.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2506.03517
[05.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.24500
[05.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2506.04108
[05.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2506.02592
[05.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2506.03610
[05.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2506.03099
[05.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2506.03355
[05.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2506.03106
[05.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2506.03956
[05.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21541
[05.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2506.03448
[05.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2506.02945
[05.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2506.02294
[05.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2506.00482
[05.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.23807
[05.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2506.04133
[05.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2506.03837
[05.06.2025 19:09] Extract page data from URL. URL: https://huggingface.co/papers/2506.03566
[05.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2506.03525
[05.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2506.01344
[05.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2506.04034
[05.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2506.03951
[05.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2506.03822
[05.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2506.03614
[05.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2506.02515
[05.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.23564
[05.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2506.04214
[05.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2506.03817
[05.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2506.03538
[05.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2506.02680
[05.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2506.02153
[05.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2506.00618
[05.06.2025 19:09] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[05.06.2025 19:09] No deleted papers detected.
[05.06.2025 19:09] Downloading and parsing papers (pdf, html). Total: 49.
[05.06.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2506.03569.
[05.06.2025 19:09] Extra JSON file exists (./assets/json/2506.03569.json), skip PDF parsing.
[05.06.2025 19:09] Paper image links file exists (./assets/img_data/2506.03569.json), skip HTML parsing.
[05.06.2025 19:09] Success.
[05.06.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2506.04207.
[05.06.2025 19:09] Extra JSON file exists (./assets/json/2506.04207.json), skip PDF parsing.
[05.06.2025 19:09] Paper image links file exists (./assets/img_data/2506.04207.json), skip HTML parsing.
[05.06.2025 19:09] Success.
[05.06.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2506.04089.
[05.06.2025 19:09] Extra JSON file exists (./assets/json/2506.04089.json), skip PDF parsing.
[05.06.2025 19:09] Paper image links file exists (./assets/img_data/2506.04089.json), skip HTML parsing.
[05.06.2025 19:09] Success.
[05.06.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.16968.
[05.06.2025 19:09] Extra JSON file exists (./assets/json/2505.16968.json), skip PDF parsing.
[05.06.2025 19:09] Paper image links file exists (./assets/img_data/2505.16968.json), skip HTML parsing.
[05.06.2025 19:09] Success.
[05.06.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2506.02921.
[05.06.2025 19:09] Extra JSON file exists (./assets/json/2506.02921.json), skip PDF parsing.
[05.06.2025 19:09] Paper image links file exists (./assets/img_data/2506.02921.json), skip HTML parsing.
[05.06.2025 19:09] Success.
[05.06.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2506.04180.
[05.06.2025 19:09] Extra JSON file exists (./assets/json/2506.04180.json), skip PDF parsing.
[05.06.2025 19:09] Paper image links file exists (./assets/img_data/2506.04180.json), skip HTML parsing.
[05.06.2025 19:09] Success.
[05.06.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2506.04141.
[05.06.2025 19:09] Extra JSON file exists (./assets/json/2506.04141.json), skip PDF parsing.
[05.06.2025 19:09] Paper image links file exists (./assets/img_data/2506.04141.json), skip HTML parsing.
[05.06.2025 19:09] Success.
[05.06.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2506.04142.
[05.06.2025 19:09] Extra JSON file exists (./assets/json/2506.04142.json), skip PDF parsing.
[05.06.2025 19:09] Paper image links file exists (./assets/img_data/2506.04142.json), skip HTML parsing.
[05.06.2025 19:09] Success.
[05.06.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2506.04225.
[05.06.2025 19:09] Extra JSON file exists (./assets/json/2506.04225.json), skip PDF parsing.
[05.06.2025 19:09] Paper image links file exists (./assets/img_data/2506.04225.json), skip HTML parsing.
[05.06.2025 19:09] Success.
[05.06.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2506.03150.
[05.06.2025 19:09] Extra JSON file exists (./assets/json/2506.03150.json), skip PDF parsing.
[05.06.2025 19:09] Paper image links file exists (./assets/img_data/2506.03150.json), skip HTML parsing.
[05.06.2025 19:09] Success.
[05.06.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2506.04178.
[05.06.2025 19:09] Downloading paper 2506.04178 from http://arxiv.org/pdf/2506.04178v1...
[05.06.2025 19:09] Extracting affiliations from text.
[05.06.2025 19:09] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Etash Guha*1,2, Ryan Marten*3, Sedrick Keh*4, Negin Raoof*5, Georgios Smyrnis*6, Hritik BansalŒ∂7, Marianna NezhurinaŒ∂8,9,16, Jean MercatŒ∂4, Trung VuŒ∂3, Zayne SpragueŒ∂6, Ashima Suvarna7, Benjamin Feuer10, Liangyu Chen1, Zaid Khan11, Eric Frankel2, Sachin Grover12, Caroline Choi1, Niklas Muennighoff1, Shiye Su1, Wanjia Zhao1, John Yang1, Shreyas Pimpalgaonkar3, Kartik Sharma3, Charlie Cheng-Jie Ji3, Yichuan Deng2, Sarah Pratt2, Vivek Ramanujan2, Jon Saad-Falcon1, Jeffrey Li2, Achal Dave, Alon Albalak13, Kushal Arora4, Blake Wulfe4, Chinmay Hegde10, Greg Durrett6, Sewoong Oh2, Mohit Bansal11, Saadia Gabriel7, Aditya Grover7, Kai-Wei Chang7, Vaishaal Shankar, Aaron Gokaslan14, Mike A. Merrill1, Tatsunori Hashimoto1, Yejin Choi1, Jenia Jitsev8,9,16, Reinhard Heckel15, Maheswaran Sathiamoorthy3, Alexandros G. Dimakis3,5, Ludwig Schmidt1 1Stanford University, 2University of Washington, 3BespokeLabs.ai, 4Toyota Research Institute, 5UC Berkeley, 6UT Austin, 7UCLA, 8JSC, 9LAION, 10NYU, 11UNC Chapel Hill, 12ASU, 13Lila Sciences, 14Cornell Tech 15TUM 16Open-Œ® (Open-Sci) Collective "
[05.06.2025 19:09] Response: ```python
[
    "Stanford University",
    "University of Washington",
    "BespokeLabs.ai",
    "Toyota Research Institute",
    "UC Berkeley",
    "UT Austin",
    "UCLA",
    "JSC",
    "LAION",
    "NYU",
    "UNC Chapel Hill",
    "ASU",
    "Lila Sciences",
    "Cornell Tech",
    "TUM",
    "Open-Œ® (Open-Sci) Collective"
]
```
[05.06.2025 19:09] Deleting PDF ./assets/pdf/2506.04178.pdf.
[05.06.2025 19:09] Success.
[05.06.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2506.04158.
[05.06.2025 19:09] Extra JSON file exists (./assets/json/2506.04158.json), skip PDF parsing.
[05.06.2025 19:09] Paper image links file exists (./assets/img_data/2506.04158.json), skip HTML parsing.
[05.06.2025 19:09] Success.
[05.06.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2506.01320.
[05.06.2025 19:09] Extra JSON file exists (./assets/json/2506.01320.json), skip PDF parsing.
[05.06.2025 19:09] Paper image links file exists (./assets/img_data/2506.01320.json), skip HTML parsing.
[05.06.2025 19:09] Success.
[05.06.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2506.03930.
[05.06.2025 19:09] Extra JSON file exists (./assets/json/2506.03930.json), skip PDF parsing.
[05.06.2025 19:09] Paper image links file exists (./assets/img_data/2506.03930.json), skip HTML parsing.
[05.06.2025 19:09] Success.
[05.06.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2506.03295.
[05.06.2025 19:09] Extra JSON file exists (./assets/json/2506.03295.json), skip PDF parsing.
[05.06.2025 19:09] Paper image links file exists (./assets/img_data/2506.03295.json), skip HTML parsing.
[05.06.2025 19:09] Success.
[05.06.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2506.04228.
[05.06.2025 19:09] Extra JSON file exists (./assets/json/2506.04228.json), skip PDF parsing.
[05.06.2025 19:09] Paper image links file exists (./assets/img_data/2506.04228.json), skip HTML parsing.
[05.06.2025 19:09] Success.
[05.06.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2506.03139.
[05.06.2025 19:09] Extra JSON file exists (./assets/json/2506.03139.json), skip PDF parsing.
[05.06.2025 19:09] Paper image links file exists (./assets/img_data/2506.03139.json), skip HTML parsing.
[05.06.2025 19:09] Success.
[05.06.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2506.03517.
[05.06.2025 19:09] Extra JSON file exists (./assets/json/2506.03517.json), skip PDF parsing.
[05.06.2025 19:09] Paper image links file exists (./assets/img_data/2506.03517.json), skip HTML parsing.
[05.06.2025 19:09] Success.
[05.06.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.24500.
[05.06.2025 19:09] Extra JSON file exists (./assets/json/2505.24500.json), skip PDF parsing.
[05.06.2025 19:09] Paper image links file exists (./assets/img_data/2505.24500.json), skip HTML parsing.
[05.06.2025 19:09] Success.
[05.06.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2506.04108.
[05.06.2025 19:09] Extra JSON file exists (./assets/json/2506.04108.json), skip PDF parsing.
[05.06.2025 19:09] Paper image links file exists (./assets/img_data/2506.04108.json), skip HTML parsing.
[05.06.2025 19:09] Success.
[05.06.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2506.02592.
[05.06.2025 19:09] Extra JSON file exists (./assets/json/2506.02592.json), skip PDF parsing.
[05.06.2025 19:09] Paper image links file exists (./assets/img_data/2506.02592.json), skip HTML parsing.
[05.06.2025 19:09] Success.
[05.06.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2506.03610.
[05.06.2025 19:09] Extra JSON file exists (./assets/json/2506.03610.json), skip PDF parsing.
[05.06.2025 19:09] Paper image links file exists (./assets/img_data/2506.03610.json), skip HTML parsing.
[05.06.2025 19:09] Success.
[05.06.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2506.03099.
[05.06.2025 19:09] Extra JSON file exists (./assets/json/2506.03099.json), skip PDF parsing.
[05.06.2025 19:09] Paper image links file exists (./assets/img_data/2506.03099.json), skip HTML parsing.
[05.06.2025 19:09] Success.
[05.06.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2506.03355.
[05.06.2025 19:09] Extra JSON file exists (./assets/json/2506.03355.json), skip PDF parsing.
[05.06.2025 19:09] Paper image links file exists (./assets/img_data/2506.03355.json), skip HTML parsing.
[05.06.2025 19:09] Success.
[05.06.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2506.03106.
[05.06.2025 19:09] Extra JSON file exists (./assets/json/2506.03106.json), skip PDF parsing.
[05.06.2025 19:09] Paper image links file exists (./assets/img_data/2506.03106.json), skip HTML parsing.
[05.06.2025 19:09] Success.
[05.06.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2506.03956.
[05.06.2025 19:09] Extra JSON file exists (./assets/json/2506.03956.json), skip PDF parsing.
[05.06.2025 19:09] Paper image links file exists (./assets/img_data/2506.03956.json), skip HTML parsing.
[05.06.2025 19:09] Success.
[05.06.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.21541.
[05.06.2025 19:09] Extra JSON file exists (./assets/json/2505.21541.json), skip PDF parsing.
[05.06.2025 19:09] Paper image links file exists (./assets/img_data/2505.21541.json), skip HTML parsing.
[05.06.2025 19:09] Success.
[05.06.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2506.03448.
[05.06.2025 19:09] Extra JSON file exists (./assets/json/2506.03448.json), skip PDF parsing.
[05.06.2025 19:09] Paper image links file exists (./assets/img_data/2506.03448.json), skip HTML parsing.
[05.06.2025 19:09] Success.
[05.06.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2506.02945.
[05.06.2025 19:09] Extra JSON file exists (./assets/json/2506.02945.json), skip PDF parsing.
[05.06.2025 19:09] Paper image links file exists (./assets/img_data/2506.02945.json), skip HTML parsing.
[05.06.2025 19:09] Success.
[05.06.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2506.02294.
[05.06.2025 19:09] Extra JSON file exists (./assets/json/2506.02294.json), skip PDF parsing.
[05.06.2025 19:09] Paper image links file exists (./assets/img_data/2506.02294.json), skip HTML parsing.
[05.06.2025 19:09] Success.
[05.06.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2506.00482.
[05.06.2025 19:09] Extra JSON file exists (./assets/json/2506.00482.json), skip PDF parsing.
[05.06.2025 19:09] Paper image links file exists (./assets/img_data/2506.00482.json), skip HTML parsing.
[05.06.2025 19:09] Success.
[05.06.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.23807.
[05.06.2025 19:09] Extra JSON file exists (./assets/json/2505.23807.json), skip PDF parsing.
[05.06.2025 19:09] Paper image links file exists (./assets/img_data/2505.23807.json), skip HTML parsing.
[05.06.2025 19:09] Success.
[05.06.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2506.04133.
[05.06.2025 19:09] Extra JSON file exists (./assets/json/2506.04133.json), skip PDF parsing.
[05.06.2025 19:09] Paper image links file exists (./assets/img_data/2506.04133.json), skip HTML parsing.
[05.06.2025 19:09] Success.
[05.06.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2506.03837.
[05.06.2025 19:09] Extra JSON file exists (./assets/json/2506.03837.json), skip PDF parsing.
[05.06.2025 19:09] Paper image links file exists (./assets/img_data/2506.03837.json), skip HTML parsing.
[05.06.2025 19:09] Success.
[05.06.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2506.03566.
[05.06.2025 19:09] Downloading paper 2506.03566 from http://arxiv.org/pdf/2506.03566v1...
[05.06.2025 19:09] Extracting affiliations from text.
[05.06.2025 19:09] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 4 ] . [ 1 6 6 5 3 0 . 6 0 5 2 : r POSS:Position Specialist Generates Better Draft for Speculative Decoding Langlin Huang1, Chengsong Huang1, Jixuan Leng2, Di huang1, Jiaxin Huang1 1Washington University in St. Louis 2Carnegie Mellon University h.langlin@wustl.edu, jiaxinh@wustl.edu "
[05.06.2025 19:09] Response: ```python
["Washington University in St. Louis", "Carnegie Mellon University"]
```
[05.06.2025 19:09] Deleting PDF ./assets/pdf/2506.03566.pdf.
[05.06.2025 19:09] Success.
[05.06.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2506.03525.
[05.06.2025 19:09] Extra JSON file exists (./assets/json/2506.03525.json), skip PDF parsing.
[05.06.2025 19:09] Paper image links file exists (./assets/img_data/2506.03525.json), skip HTML parsing.
[05.06.2025 19:09] Success.
[05.06.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2506.01344.
[05.06.2025 19:09] Extra JSON file exists (./assets/json/2506.01344.json), skip PDF parsing.
[05.06.2025 19:09] Paper image links file exists (./assets/img_data/2506.01344.json), skip HTML parsing.
[05.06.2025 19:09] Success.
[05.06.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2506.04034.
[05.06.2025 19:09] Extra JSON file exists (./assets/json/2506.04034.json), skip PDF parsing.
[05.06.2025 19:09] Paper image links file exists (./assets/img_data/2506.04034.json), skip HTML parsing.
[05.06.2025 19:09] Success.
[05.06.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2506.03951.
[05.06.2025 19:09] Extra JSON file exists (./assets/json/2506.03951.json), skip PDF parsing.
[05.06.2025 19:09] Paper image links file exists (./assets/img_data/2506.03951.json), skip HTML parsing.
[05.06.2025 19:09] Success.
[05.06.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2506.03822.
[05.06.2025 19:09] Extra JSON file exists (./assets/json/2506.03822.json), skip PDF parsing.
[05.06.2025 19:09] Paper image links file exists (./assets/img_data/2506.03822.json), skip HTML parsing.
[05.06.2025 19:09] Success.
[05.06.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2506.03614.
[05.06.2025 19:09] Extra JSON file exists (./assets/json/2506.03614.json), skip PDF parsing.
[05.06.2025 19:09] Paper image links file exists (./assets/img_data/2506.03614.json), skip HTML parsing.
[05.06.2025 19:09] Success.
[05.06.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2506.02515.
[05.06.2025 19:09] Extra JSON file exists (./assets/json/2506.02515.json), skip PDF parsing.
[05.06.2025 19:09] Paper image links file exists (./assets/img_data/2506.02515.json), skip HTML parsing.
[05.06.2025 19:09] Success.
[05.06.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.23564.
[05.06.2025 19:09] Extra JSON file exists (./assets/json/2505.23564.json), skip PDF parsing.
[05.06.2025 19:09] Paper image links file exists (./assets/img_data/2505.23564.json), skip HTML parsing.
[05.06.2025 19:09] Success.
[05.06.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2506.04214.
[05.06.2025 19:09] Extra JSON file exists (./assets/json/2506.04214.json), skip PDF parsing.
[05.06.2025 19:09] Paper image links file exists (./assets/img_data/2506.04214.json), skip HTML parsing.
[05.06.2025 19:09] Success.
[05.06.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2506.03817.
[05.06.2025 19:09] Extra JSON file exists (./assets/json/2506.03817.json), skip PDF parsing.
[05.06.2025 19:09] Paper image links file exists (./assets/img_data/2506.03817.json), skip HTML parsing.
[05.06.2025 19:09] Success.
[05.06.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2506.03538.
[05.06.2025 19:09] Extra JSON file exists (./assets/json/2506.03538.json), skip PDF parsing.
[05.06.2025 19:09] Paper image links file exists (./assets/img_data/2506.03538.json), skip HTML parsing.
[05.06.2025 19:09] Success.
[05.06.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2506.02680.
[05.06.2025 19:09] Extra JSON file exists (./assets/json/2506.02680.json), skip PDF parsing.
[05.06.2025 19:09] Paper image links file exists (./assets/img_data/2506.02680.json), skip HTML parsing.
[05.06.2025 19:09] Success.
[05.06.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2506.02153.
[05.06.2025 19:09] Extra JSON file exists (./assets/json/2506.02153.json), skip PDF parsing.
[05.06.2025 19:09] Paper image links file exists (./assets/img_data/2506.02153.json), skip HTML parsing.
[05.06.2025 19:09] Success.
[05.06.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2506.00618.
[05.06.2025 19:09] Extra JSON file exists (./assets/json/2506.00618.json), skip PDF parsing.
[05.06.2025 19:09] Paper image links file exists (./assets/img_data/2506.00618.json), skip HTML parsing.
[05.06.2025 19:09] Success.
[05.06.2025 19:09] Enriching papers with extra data.
[05.06.2025 19:09] ********************************************************************************
[05.06.2025 19:09] Abstract 0. We open-source MiMo-VL-7B-SFT and MiMo-VL-7B-RL, two powerful vision-language models delivering state-of-the-art performance in both general visual understanding and multimodal reasoning. MiMo-VL-7B-RL outperforms Qwen2.5-VL-7B on 35 out of 40 evaluated tasks, and scores 59.4 on OlympiadBench, surpa...
[05.06.2025 19:09] ********************************************************************************
[05.06.2025 19:09] Abstract 1. Inspired by the remarkable reasoning capabilities of Deepseek-R1 in complex textual tasks, many works attempt to incentivize similar capabilities in Multimodal Large Language Models (MLLMs) by directly applying reinforcement learning (RL). However, they still struggle to activate complex reasoning. ...
[05.06.2025 19:09] ********************************************************************************
[05.06.2025 19:09] Abstract 2. AmbiK, a textual dataset of ambiguous instructions for kitchen robots, enables unified comparison of ambiguity detection methods.  					AI-generated summary 				 As a part of an embodied agent, Large Language Models (LLMs) are typically used for behavior planning given natural language instructions ...
[05.06.2025 19:09] ********************************************************************************
[05.06.2025 19:09] Abstract 3. CASS is a dataset and model suite for GPU code transpilation at both source and assembly levels, achieving high accuracy and performance matching with native code.  					AI-generated summary 				 We introduce CASS, the first large-scale dataset and model suite for cross-architecture GPU code transpi...
[05.06.2025 19:09] ********************************************************************************
[05.06.2025 19:09] Abstract 4. LongBioBench is a new benchmark using artificially generated biographies to evaluate long-context language models across understanding, reasoning, and trustworthiness dimensions, addressing limitations in existing frameworks.  					AI-generated summary 				 Existing frameworks for evaluating long-co...
[05.06.2025 19:09] ********************************************************************************
[05.06.2025 19:09] Abstract 5. Long-form text generation remains a significant challenge for large language models (LLMs), particularly in maintaining coherence, ensuring logical consistency, and preserving text quality as sequence length increases. To address these limitations, we propose SuperWriter-Agent, an agent-based framew...
[05.06.2025 19:09] ********************************************************************************
[05.06.2025 19:09] Abstract 6. A new benchmark, MMR-V, is proposed to challenge multimodal large language models with long-range, multi-frame reasoning and hidden information processing in videos, revealing their limitations and inspiring further research.  					AI-generated summary 				 The sequential structure of videos poses a...
[05.06.2025 19:09] ********************************************************************************
[05.06.2025 19:09] Abstract 7. A method called shortcut neuron patching identifies and suppresses shortcut neurons in language models to mitigate data contamination issues in trustworthy evaluations.  					AI-generated summary 				 The development of large language models (LLMs) depends on trustworthy evaluation. However, most cu...
[05.06.2025 19:09] ********************************************************************************
[05.06.2025 19:09] Abstract 8. Voyager is a video diffusion framework that generates world-consistent 3D point-cloud sequences from a single image, enabling long-range, consistent 3D scene exploration with user-defined camera paths.  					AI-generated summary 				 Real-world applications like video gaming and virtual reality ofte...
[05.06.2025 19:09] ********************************************************************************
[05.06.2025 19:09] Abstract 9. IllumiCraft integrates geometric cues in a diffusion framework to generate high-fidelity, temporally coherent videos from textual or image inputs.  					AI-generated summary 				 Although diffusion-based models can generate high-quality and high-resolution video sequences from textual or image input...
[05.06.2025 19:09] ********************************************************************************
[05.06.2025 19:09] Abstract 10. The OpenThoughts project created open-source datasets leading to reasoning models that match or exceed state-of-the-art benchmarks in math, code, and science.  					AI-generated summary 				 Reasoning models have made rapid progress on many benchmarks involving math, code, and science. Yet, there ar...
[05.06.2025 19:09] ********************************************************************************
[05.06.2025 19:09] Abstract 11. While diffusion models have achieved remarkable success in text-to-image generation, they encounter significant challenges with instruction-driven image editing. Our research highlights a key challenge: these models particularly struggle with structurally inconsistent edits that involve substantial ...
[05.06.2025 19:09] ********************************************************************************
[05.06.2025 19:09] Abstract 12. We introduce Psi-Sampler, an SMC-based framework incorporating pCNL-based initial particle sampling for effective inference-time reward alignment with a score-based generative model. Inference-time reward alignment with score-based generative models has recently gained significant traction, followin...
[05.06.2025 19:09] ********************************************************************************
[05.06.2025 19:09] Abstract 13. VisCode-200K, a large-scale dataset for visualization, improves plot generation performance by integrating execution-grounded supervision and iterative code correction, outperforming open-source models and rivaling proprietary ones.  					AI-generated summary 				 Large language models (LLMs) often ...
[05.06.2025 19:09] ********************************************************************************
[05.06.2025 19:09] Abstract 14. Critique Fine-Tuning on a single problem can efficiently enhance the reasoning capabilities of large language models with significant performance gains and reduced computational cost compared to reinforcement learning.  					AI-generated summary 				 We have witnessed that strong LLMs like Qwen-Math...
[05.06.2025 19:09] ********************************************************************************
[05.06.2025 19:09] Abstract 15. LayerFlow is a unified framework for generating layer-aware videos using a text-to-video diffusion transformer and layer embeddings, supporting various video generation tasks with a multi-stage training strategy.  					AI-generated summary 				 We present LayerFlow, a unified solution for layer-awar...
[05.06.2025 19:09] ********************************************************************************
[05.06.2025 19:09] Abstract 16. SVGenius evaluates Large Language Models and Multimodal LLMs for SVG processing using a comprehensive benchmark across three dimensions: understanding, editing, and generation, revealing insights into model capabilities and limitations.  					AI-generated summary 				 Large Language Models (LLMs) an...
[05.06.2025 19:09] ********************************************************************************
[05.06.2025 19:09] Abstract 17. Direct Preference Optimization (DPO) has recently been applied as a post-training technique for text-to-video diffusion models. To obtain training data, annotators are asked to provide preferences between two videos generated from independent noise. However, this approach prohibits fine-grained comp...
[05.06.2025 19:09] ********************************************************************************
[05.06.2025 19:09] Abstract 18. Temporal-aware Hierarchical Cognitive Reinforcement Learning enhances LLMs' social intelligence by addressing the distinct cognitive demands of social domains.  					AI-generated summary 				 Recently, Large Language Models (LLMs) have made significant progress in IQ-related domains that require car...
[05.06.2025 19:09] ********************************************************************************
[05.06.2025 19:09] Abstract 19. Rectified Sparse Attention (ReSA) improves the efficiency of long-sequence generation in Large Language Models by combining block-sparse attention with periodic dense rectification, maintaining high-quality generation.  					AI-generated summary 				 Efficient long-sequence generation is a critical ...
[05.06.2025 19:09] ********************************************************************************
[05.06.2025 19:09] Abstract 20. The DBG score is introduced to measure self-preference bias in large language models by using gold judgments as proxies for response quality, addressing the confounding effect of response quality.  					AI-generated summary 				 Recent studies show that large language models (LLMs) exhibit self-pref...
[05.06.2025 19:09] ********************************************************************************
[05.06.2025 19:09] Abstract 21. Orak is a benchmark for training and evaluating LLM agents across diverse video games, featuring a plug-and-play interface and fine-tuning datasets to enhance agentic modules and gameplay.  					AI-generated summary 				 Large Language Model (LLM) agents are reshaping the game industry, particularly...
[05.06.2025 19:09] ********************************************************************************
[05.06.2025 19:09] Abstract 22. TalkingMachines transforms a pretrained image-to-video model into an audio-driven avatar generator, supports infinite video streaming, and uses engineering optimizations for real-time performance.  					AI-generated summary 				 In this paper, we present TalkingMachines -- an efficient framework tha...
[05.06.2025 19:09] ********************************************************************************
[05.06.2025 19:09] Abstract 23. LEAF, an adversarial finetuning method, enhances the robustness of CLIP text encoders, improving zero-shot accuracy and multimodal retrieval performance under adversarial noise.  					AI-generated summary 				 Adversarial input attacks can cause a significant shift of CLIP embeddings. This can affec...
[05.06.2025 19:09] ********************************************************************************
[05.06.2025 19:09] Abstract 24. Critique-GRPO, an RL framework combining numerical and natural language feedback, enhances LLM reasoning across tasks and outperforms existing methods.  					AI-generated summary 				 Recent advances in reinforcement learning (RL) with numerical feedback, such as scalar rewards, have significantly e...
[05.06.2025 19:09] ********************************************************************************
[05.06.2025 19:09] Abstract 25. Adapting Pre-trained Models before the core CL process (ACL) improves Continual Learning by enhancing plasticity while maintaining stability.  					AI-generated summary 				 Continual Learning (CL) seeks to enable neural networks to incrementally acquire new knowledge (plasticity) while retaining ex...
[05.06.2025 19:09] ********************************************************************************
[05.06.2025 19:09] Abstract 26. DiffDecompose, a diffusion Transformer-based framework, effectively decomposes images into constituent layers with semantic prompts, addressing challenges in transparent layer decomposition.  					AI-generated summary 				 Diffusion models have recently motivated great success in many generation tas...
[05.06.2025 19:09] ********************************************************************************
[05.06.2025 19:09] Abstract 27. RefEdit, an instruction-based editing model trained on synthetic data, outperforms baselines in complex scene editing and referring expression tasks.  					AI-generated summary 				 Despite recent advances in inversion and instruction-based image editing, existing approaches primarily excel at editi...
[05.06.2025 19:09] ********************************************************************************
[05.06.2025 19:09] Abstract 28. LLM-as-a-judge is a framework in which a large language model (LLM) automatically evaluates the output of another LLM. We propose quantitative LLM judges, which align evaluation scores of existing LLM judges to human scores in a given domain using regression models. The models are trained to improve...
[05.06.2025 19:09] ********************************************************************************
[05.06.2025 19:09] Abstract 29. A diffusion-based data augmentation strategy improves robustness in knowledge distillation by generating challenging samples, enhancing accuracy and spurious feature resilience.  					AI-generated summary 				 Large foundation models trained on extensive datasets demonstrate strong zero-shot capabil...
[05.06.2025 19:09] ********************************************************************************
[05.06.2025 19:09] Abstract 30. BenchHub is a dynamic benchmark repository that aggregates and classifies datasets for large language models, facilitating domain-specific evaluations and improving model comparisons.  					AI-generated summary 				 As large language models (LLMs) continue to advance, the need for up-to-date and wel...
[05.06.2025 19:09] ********************************************************************************
[05.06.2025 19:09] Abstract 31. A dynamic layerwise pruning method adaptively determines layer importance by combining model weights and activation information to maintain performance in large language models at high sparsity.  					AI-generated summary 				 Pruning has recently been widely adopted to reduce the parameter scale an...
[05.06.2025 19:09] ********************************************************************************
[05.06.2025 19:09] Abstract 32. A review of trust, risk, and security management in LLM-based agentic multi-agent systems, examining governance, explainability, ModelOps, and privacy/security.  					AI-generated summary 				 Agentic AI systems, built on large language models (LLMs) and deployed in multi-agent configurations, are r...
[05.06.2025 19:09] ********************************************************************************
[05.06.2025 19:09] Abstract 33. HTSC-2025, a benchmark dataset for high-temperature superconducting materials, is presented to facilitate AI-based discovery in this field.  					AI-generated summary 				 The discovery of high-temperature superconducting materials holds great significance for human industry and daily life. In recen...
[05.06.2025 19:09] ********************************************************************************
[05.06.2025 19:09] Abstract 34. Position Specialists (PosS) enhance Large Language Model (LLM) inference by using position-specialized draft layers to improve token prediction accuracy and acceptance rates.  					AI-generated summary 				 Speculative decoding accelerates Large Language Model (LLM) inference by using a small draft ...
[05.06.2025 19:09] ********************************************************************************
[05.06.2025 19:09] Abstract 35. Video-SKoT framework improves domain-adaptive video reasoning by constructing skill-aware Chain-of-Thought supervisions and specialized expert modules.  					AI-generated summary 				 Recent advances in Chain-of-Thought (CoT) reasoning have improved complex video understanding, but existing methods ...
[05.06.2025 19:09] ********************************************************************************
[05.06.2025 19:09] Abstract 36. Flowcharts are a critical tool for visualizing decision-making processes. However, their non-linear structure and complex visual-textual relationships make it challenging to interpret them using LLMs, as vision-language models frequently hallucinate nonexistent connections and decision paths when an...
[05.06.2025 19:09] ********************************************************************************
[05.06.2025 19:09] Abstract 37. Object referring aims to detect all objects in an image that match a given natural language description. We argue that a robust object referring model should be grounded, meaning its predictions should be both explainable and faithful to the visual content. Specifically, it should satisfy two key pr...
[05.06.2025 19:09] ********************************************************************************
[05.06.2025 19:09] Abstract 38. A novel framework, Dual-Arch, enhances Continual Learning by addressing the stability-plasticity dilemma at the architectural level using two specialized networks.  					AI-generated summary 				 The quest for Continual Learning (CL) seeks to empower neural networks with the ability to learn and ada...
[05.06.2025 19:09] ********************************************************************************
[05.06.2025 19:09] Abstract 39. Publication databases rely on accurate metadata extraction from diverse web sources, yet variations in web layouts and data formats present challenges for metadata providers. This paper introduces CRAWLDoc, a new method for contextual ranking of linked web documents. Starting with a publication's UR...
[05.06.2025 19:09] ********************************************************************************
[05.06.2025 19:09] Abstract 40. VLMs exhibit visual stitching, an ability to integrate fragmented visual information, which enables harmful content to evade data moderation and be reconstructed during inference.  					AI-generated summary 				 One way to mitigate risks in vision-language models (VLMs) is to remove dangerous sample...
[05.06.2025 19:09] ********************************************************************************
[05.06.2025 19:09] Abstract 41. A new benchmark called FinChain evaluates multi-step symbolic reasoning in financial tasks with a focus on intermediate reasoning steps, introducing ChainEval as a metric for assessing both final answers and reasoning processes.  					AI-generated summary 				 Multi-step symbolic reasoning is critic...
[05.06.2025 19:09] ********************************************************************************
[05.06.2025 19:09] Abstract 42. The Segment Policy Optimization (SPO) framework improves large language model reasoning via reinforcement learning by offering intermediate granularity advantage estimation, balancing precision and computational efficiency.  					AI-generated summary 				 Enhancing the reasoning capabilities of larg...
[05.06.2025 19:09] ********************************************************************************
[05.06.2025 19:09] Abstract 43. Generating accurate sounds for complex audio-visual scenes is challenging, especially in the presence of multiple objects and sound sources. In this paper, we propose an {\em interactive object-aware audio generation} model that grounds sound generation in user-selected visual objects within images....
[05.06.2025 19:09] ********************************************************************************
[05.06.2025 19:09] Abstract 44. Annotating data is a time-consuming and costly task, but it is inherently required for supervised machine learning. Active Learning (AL) is an established method that minimizes human labeling effort by iteratively selecting the most informative unlabeled samples for expert annotation, thereby improv...
[05.06.2025 19:09] ********************************************************************************
[05.06.2025 19:09] Abstract 45. A novel Asymmetric Dual 3DGS framework improves 3D reconstruction by training dual models with consistency constraints and divergent masking, outperforming existing methods with high efficiency.  					AI-generated summary 				 3D reconstruction from in-the-wild images remains a challenging task due ...
[05.06.2025 19:09] ********************************************************************************
[05.06.2025 19:09] Abstract 46. FLAIR, a novel training-free variational framework, leverages flow-based generative models to enhance inverse problem solutions, achieving superior reconstruction quality and sample diversity.  					AI-generated summary 				 Flow-based latent generative models such as Stable Diffusion 3 are able to ...
[05.06.2025 19:09] ********************************************************************************
[05.06.2025 19:09] Abstract 47. Large language models (LLMs) are often praised for exhibiting near-human performance on a wide range of tasks and valued for their ability to hold a general conversation. The rise of agentic AI systems is, however, ushering in a mass of applications in which language models perform a small number of...
[05.06.2025 19:09] ********************************************************************************
[05.06.2025 19:09] Abstract 48. RIOSWorld is a benchmark for evaluating safety risks of multimodal large language models in real-world computer tasks, revealing significant risks that necessitate safety alignment.  					AI-generated summary 				 With the rapid development of multimodal large language models (MLLMs), they are incre...
[05.06.2025 19:09] Read previous papers.
[05.06.2025 19:09] Generating reviews via LLM API.
[05.06.2025 19:09] Using data from previous issue: {"categories": ["#training", "#rl", "#reasoning", "#multimodal", "#rlhf", "#benchmark", "#dataset", "#open_source"], "emoji": "üß†", "ru": {"title": "–ü—Ä–æ—Ä—ã–≤ –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–º –ò–ò: MiMo-VL —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –Ω–æ–≤—ã–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ –¥–≤–µ –º–æ—â–Ω—ã–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ MiMo-VL-7B-SFT
[05.06.2025 19:09] Using data from previous issue: {"categories": ["#reasoning", "#open_source", "#multimodal", "#training", "#benchmark", "#rl"], "emoji": "üß†", "ru": {"title": "–£–ª—É—á—à–µ–Ω–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π MLLM: –æ—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–æ –º–Ω–æ–≥–æ—ç—Ç–∞–ø–Ω–æ–≥–æ RL", "desc": "–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ —É–ª—É—á—à–µ–Ω–∏—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (MLLM) –∫ —Ä–∞—Å—Å—É–∂–¥
[05.06.2025 19:09] Using data from previous issue: {"categories": ["#interpretability", "#agents", "#data", "#dataset", "#alignment"], "emoji": "üç≥", "ru": {"title": "AmbiK: —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –Ω–µ–æ–¥–Ω–æ–∑–Ω–∞—á–Ω–æ—Å—Ç–∏ –≤ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è—Ö –¥–ª—è —Ä–æ–±–æ—Ç–æ–≤", "desc": "AmbiK - —ç—Ç–æ —Ç–µ–∫—Å—Ç–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç –Ω–µ–æ–¥–Ω–æ–∑–Ω–∞—á–Ω—ã—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –¥–ª—è –∫—É—Ö–æ–Ω–Ω—ã—Ö —Ä–æ–±–æ—Ç–æ–≤, —Å–æ–∑–¥–∞–Ω–Ω
[05.06.2025 19:09] Using data from previous issue: {"categories": ["#dataset", "#low_resource", "#benchmark", "#open_source"], "emoji": "üîÑ", "ru": {"title": "CASS: –ü—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ –±–∞—Ä—å–µ—Ä–æ–≤ –º–µ–∂–¥—É GPU-–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞–º–∏", "desc": "CASS –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∏ –Ω–∞–±–æ—Ä –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ç—Ä–∞–Ω—Å–ø–∏–ª—è—Ü–∏–∏ GPU-–∫–æ–¥–∞ –º–µ–∂–¥—É –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞–º–∏ –∫–∞–∫ –Ω–∞ —É—Ä–æ–≤–Ω–µ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –∫–æ–¥–∞, —Ç–∞
[05.06.2025 19:09] Using data from previous issue: {"categories": ["#benchmark", "#synthetic", "#long_context", "#reasoning", "#interpretability"], "emoji": "üìä", "ru": {"title": "LongBioBench: –ù–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –æ—Ü–µ–Ω–∫–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º", "desc": "LongBioBench - —ç—Ç–æ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º, –∏—Å–ø
[05.06.2025 19:09] Using data from previous issue: {"categories": ["#training", "#optimization", "#agents", "#long_context", "#rlhf", "#benchmark", "#dataset", "#story_generation"], "emoji": "‚úçÔ∏è", "ru": {"title": "–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤", "desc": "SuperWriter-Agent - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤
[05.06.2025 19:09] Using data from previous issue: {"categories": ["#reasoning", "#multimodal", "#long_context", "#benchmark", "#video"], "emoji": "üé•", "ru": {"title": "MMR-V: –ù–æ–≤—ã–π —Ä—É–±–µ–∂ –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è—Ö –ø–æ –≤–∏–¥–µ–æ", "desc": "–ü—Ä–µ–¥–ª–æ–∂–µ–Ω –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ MMR-V –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –∑–∞–¥–∞—á–∞—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –ø–æ –≤–∏–¥–µ–æ. –û–Ω —Ç—Ä
[05.06.2025 19:09] Using data from previous issue: {"categories": ["#data", "#hallucinations", "#benchmark", "#ethics", "#training"], "emoji": "üß†", "ru": {"title": "–ë–æ—Ä—å–±–∞ —Å –∑–∞–≥—Ä—è–∑–Ω–µ–Ω–∏–µ–º –¥–∞–Ω–Ω—ã—Ö –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö —á–µ—Ä–µ–∑ –ø–∞—Ç—á–∏–Ω–≥ –Ω–µ–π—Ä–æ–Ω–æ–≤-—à–æ—Ä—Ç–∫–∞—Ç–æ–≤", "desc": "–ú–µ—Ç–æ–¥ 'shortcut neuron patching' –∏–¥–µ–Ω—Ç–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç –∏ –ø–æ–¥–∞–≤–ª—è–µ—Ç –Ω–µ–π—Ä–æ–Ω—ã-—à–æ—Ä—Ç–∫–∞—Ç—ã –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è
[05.06.2025 19:09] Using data from previous issue: {"categories": ["#games", "#dataset", "#diffusion", "#3d", "#video"], "emoji": "üöÄ", "ru": {"title": "–ò—Å—Å–ª–µ–¥—É–π 3D-–º–∏—Ä—ã –∏–∑ –æ–¥–Ω–æ–≥–æ –∫–∞–¥—Ä–∞", "desc": "Voyager - —ç—Ç–æ —Å–∏—Å—Ç–µ–º–∞ –≤–∏–¥–µ–æ–¥–∏—Ñ—Ñ—É–∑–∏–∏, –∫–æ—Ç–æ—Ä–∞—è –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω—ã–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ 3D-–æ–±–ª–∞–∫–æ–≤ —Ç–æ—á–µ–∫ –∏–∑ –æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è. –û–Ω–∞ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç—å 
[05.06.2025 19:09] Using data from previous issue: {"categories": ["#multimodal", "#diffusion", "#architecture", "#3d", "#video"], "emoji": "üé•", "ru": {"title": "–ì–µ–æ–º–µ—Ç—Ä–∏—è —Å–≤–µ—Ç–∞: –Ω–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å –∫–æ–Ω—Ç—Ä–æ–ª—è –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ", "desc": "IllumiCraft - —ç—Ç–æ –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω–∞—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ, –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É—é—â–∞—è –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–µ –ø–æ–¥—Å–∫–∞–∑–∫–∏ –¥–ª—è —É–ª—É—á
[05.06.2025 19:09] Querying the API.
[05.06.2025 19:09] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

The OpenThoughts project created open-source datasets leading to reasoning models that match or exceed state-of-the-art benchmarks in math, code, and science.  					AI-generated summary 				 Reasoning models have made rapid progress on many benchmarks involving math, code, and science. Yet, there are still many open questions about the best training recipes for reasoning since state-of-the-art models often rely on proprietary datasets with little to no public information available. To address this, the goal of the OpenThoughts project is to create open-source datasets for training reasoning models. After initial explorations, our OpenThoughts2-1M dataset led to OpenThinker2-32B, the first model trained on public reasoning data to match DeepSeek-R1-Distill-32B on standard reasoning benchmarks such as AIME and LiveCodeBench. We then improve our dataset further by systematically investigating each step of our data generation pipeline with 1,000+ controlled experiments, which led to OpenThoughts3. Scaling the pipeline to 1.2M examples and using QwQ-32B as teacher yields our OpenThinker3-7B model, which achieves state-of-the-art results: 53% on AIME 2025, 51% on LiveCodeBench 06/24-01/25, and 54% on GPQA Diamond. All of our datasets and models are available on https://openthoughts.ai.
[05.06.2025 19:09] Response: {
  "desc": "–ü—Ä–æ–µ–∫—Ç OpenThoughts —Å–æ–∑–¥–∞–ª –æ—Ç–∫—Ä—ã—Ç—ã–µ –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä—ã–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç –∏–ª–∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ —ç—Ç–∞–ª–æ–Ω—ã –≤ –º–∞—Ç–µ–º–∞—Ç–∏–∫–µ, –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–∏ –∏ –Ω–∞—É–∫–µ. –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –¥–∞—Ç–∞—Å–µ—Ç OpenThoughts2-1M, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–∏–≤–µ–ª –∫ —Å–æ–∑–¥–∞–Ω–∏—é –º–æ–¥–µ–ª–∏ OpenThinker2-32B, —Å–æ–ø–æ—Å—Ç–∞–≤–∏–º–æ–π —Å DeepSeek-R1-Distill-32B –ø–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–º –ø–æ–∫–∞–∑–∞—Ç–µ–ª—è–º. –î–∞–ª—å–Ω–µ–π—à–µ–µ —É–ª—É—á—à–µ–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ—Ü–µ—Å—Å–∞ –¥–æ 1,2 –º–ª–Ω –ø—Ä–∏–º–µ—Ä–æ–≤ –ø–æ–∑–≤–æ–ª–∏–ª–æ —Å–æ–∑–¥–∞—Ç—å –º–æ–¥–µ–ª—å OpenThinker3-7B, –¥–æ—Å—Ç–∏–≥–∞—é—â—É—é –Ω–∞–∏–ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ —Ç–µ—Å—Ç–∞—Ö AIME, LiveCodeBench –∏ GPQA Diamond. –í—Å–µ –¥–∞—Ç–∞—Å–µ—Ç—ã –∏ –º–æ–¥–µ–ª–∏ –ø—Ä–æ–µ–∫—Ç–∞ –Ω–∞—Ö–æ–¥—è—Ç—Å—è –≤ –æ—Ç–∫—Ä—ã—Ç–æ–º –¥–æ—Å—Ç—É–ø–µ.",
  "emoji": "üß†",
  "title": "–û—Ç–∫—Ä—ã—Ç—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø—Ä–æ—Ä—ã–≤–∞ –≤ –ò–ò-—Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è—Ö"
}
[05.06.2025 19:09] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"The OpenThoughts project created open-source datasets leading to reasoning models that match or exceed state-of-the-art benchmarks in math, code, and science.  					AI-generated summary 				 Reasoning models have made rapid progress on many benchmarks involving math, code, and science. Yet, there are still many open questions about the best training recipes for reasoning since state-of-the-art models often rely on proprietary datasets with little to no public information available. To address this, the goal of the OpenThoughts project is to create open-source datasets for training reasoning models. After initial explorations, our OpenThoughts2-1M dataset led to OpenThinker2-32B, the first model trained on public reasoning data to match DeepSeek-R1-Distill-32B on standard reasoning benchmarks such as AIME and LiveCodeBench. We then improve our dataset further by systematically investigating each step of our data generation pipeline with 1,000+ controlled experiments, which led to OpenThoughts3. Scaling the pipeline to 1.2M examples and using QwQ-32B as teacher yields our OpenThinker3-7B model, which achieves state-of-the-art results: 53% on AIME 2025, 51% on LiveCodeBench 06/24-01/25, and 54% on GPQA Diamond. All of our datasets and models are available on https://openthoughts.ai."

[05.06.2025 19:09] Response: ```python
["DATASET", "BENCHMARK", "DATA", "TRAINING"]
```
[05.06.2025 19:09] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"The OpenThoughts project created open-source datasets leading to reasoning models that match or exceed state-of-the-art benchmarks in math, code, and science.  					AI-generated summary 				 Reasoning models have made rapid progress on many benchmarks involving math, code, and science. Yet, there are still many open questions about the best training recipes for reasoning since state-of-the-art models often rely on proprietary datasets with little to no public information available. To address this, the goal of the OpenThoughts project is to create open-source datasets for training reasoning models. After initial explorations, our OpenThoughts2-1M dataset led to OpenThinker2-32B, the first model trained on public reasoning data to match DeepSeek-R1-Distill-32B on standard reasoning benchmarks such as AIME and LiveCodeBench. We then improve our dataset further by systematically investigating each step of our data generation pipeline with 1,000+ controlled experiments, which led to OpenThoughts3. Scaling the pipeline to 1.2M examples and using QwQ-32B as teacher yields our OpenThinker3-7B model, which achieves state-of-the-art results: 53% on AIME 2025, 51% on LiveCodeBench 06/24-01/25, and 54% on GPQA Diamond. All of our datasets and models are available on https://openthoughts.ai."

[05.06.2025 19:09] Response: ```python
['OPEN_SOURCE', 'REASONING', 'SCIENCE']
```
[05.06.2025 19:09] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The OpenThoughts project focuses on creating open-source datasets to enhance reasoning models in math, code, and science. By developing the OpenThoughts2-1M dataset, they trained the OpenThinker2-32B model, which achieved performance comparable to proprietary models on standard benchmarks. The project further refined its dataset through extensive experimentation, resulting in the OpenThoughts3 dataset and the OpenThinker3-7B model. This model set new state-of-the-art results on multiple reasoning benchmarks, demonstrating the effectiveness of publicly available training data.","title":"Open-Source Datasets for Superior Reasoning Models"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='The OpenThoughts project focuses on creating open-source datasets to enhance reasoning models in math, code, and science. By developing the OpenThoughts2-1M dataset, they trained the OpenThinker2-32B model, which achieved performance comparable to proprietary models on standard benchmarks. The project further refined its dataset through extensive experimentation, resulting in the OpenThoughts3 dataset and the OpenThinker3-7B model. This model set new state-of-the-art results on multiple reasoning benchmarks, demonstrating the effectiveness of publicly available training data.', title='Open-Source Datasets for Superior Reasoning Models'))
[05.06.2025 19:09] Response: ParsedChatCompletionMessage[Article](content='{"desc":"OpenThoughtsÈ°πÁõÆÊó®Âú®ÂàõÂª∫ÂºÄÊ∫êÊï∞ÊçÆÈõÜÔºå‰ª•ËÆ≠ÁªÉÊé®ÁêÜÊ®°ÂûãÔºåËææÂà∞ÊàñË∂ÖËøáÊï∞Â≠¶„ÄÅ‰ª£Á†ÅÂíåÁßëÂ≠¶È¢ÜÂüüÁöÑÊúÄÊñ∞Âü∫ÂáÜ„ÄÇÂ∞ΩÁÆ°Êé®ÁêÜÊ®°ÂûãÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠ÂèñÂæó‰∫ÜÂø´ÈÄüËøõÂ±ïÔºå‰ΩÜ‰ªçÂ≠òÂú®ËÆ∏Â§öÂÖ≥‰∫éÊúÄ‰Ω≥ËÆ≠ÁªÉÊñπÊ≥ïÁöÑÊú™Ëß£ÈóÆÈ¢ò„ÄÇÈÄöËøáÁ≥ªÁªüÂú∞Á†îÁ©∂Êï∞ÊçÆÁîüÊàêÊµÅÁ®ãÔºåOpenThoughtsÈ°πÁõÆÂºÄÂèë‰∫ÜÂ§ö‰∏™ÁâàÊú¨ÁöÑÊï∞ÊçÆÈõÜÔºåÊúÄÁªàÊé®Âá∫‰∫ÜOpenThinker3-7BÊ®°ÂûãÔºåÂèñÂæó‰∫ÜÂú®Â§ö‰∏™Ê†áÂáÜÊé®ÁêÜÂü∫ÂáÜ‰∏äÁöÑÊúÄ‰Ω≥ÊàêÁª©„ÄÇÊâÄÊúâÊï∞ÊçÆÈõÜÂíåÊ®°ÂûãÂùáÂèØÂú®https://openthoughts.ai‰∏äËé∑Âèñ„ÄÇ","title":"ÂºÄÊ∫êÊï∞ÊçÆÈõÜÂä©ÂäõÊé®ÁêÜÊ®°ÂûãÁ™ÅÁ†¥ÊûÅÈôê"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='OpenThoughtsÈ°πÁõÆÊó®Âú®ÂàõÂª∫ÂºÄÊ∫êÊï∞ÊçÆÈõÜÔºå‰ª•ËÆ≠ÁªÉÊé®ÁêÜÊ®°ÂûãÔºåËææÂà∞ÊàñË∂ÖËøáÊï∞Â≠¶„ÄÅ‰ª£Á†ÅÂíåÁßëÂ≠¶È¢ÜÂüüÁöÑÊúÄÊñ∞Âü∫ÂáÜ„ÄÇÂ∞ΩÁÆ°Êé®ÁêÜÊ®°ÂûãÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠ÂèñÂæó‰∫ÜÂø´ÈÄüËøõÂ±ïÔºå‰ΩÜ‰ªçÂ≠òÂú®ËÆ∏Â§öÂÖ≥‰∫éÊúÄ‰Ω≥ËÆ≠ÁªÉÊñπÊ≥ïÁöÑÊú™Ëß£ÈóÆÈ¢ò„ÄÇÈÄöËøáÁ≥ªÁªüÂú∞Á†îÁ©∂Êï∞ÊçÆÁîüÊàêÊµÅÁ®ãÔºåOpenThoughtsÈ°πÁõÆÂºÄÂèë‰∫ÜÂ§ö‰∏™ÁâàÊú¨ÁöÑÊï∞ÊçÆÈõÜÔºåÊúÄÁªàÊé®Âá∫‰∫ÜOpenThinker3-7BÊ®°ÂûãÔºåÂèñÂæó‰∫ÜÂú®Â§ö‰∏™Ê†áÂáÜÊé®ÁêÜÂü∫ÂáÜ‰∏äÁöÑÊúÄ‰Ω≥ÊàêÁª©„ÄÇÊâÄÊúâÊï∞ÊçÆÈõÜÂíåÊ®°ÂûãÂùáÂèØÂú®https://openthoughts.ai‰∏äËé∑Âèñ„ÄÇ', title='ÂºÄÊ∫êÊï∞ÊçÆÈõÜÂä©ÂäõÊé®ÁêÜÊ®°ÂûãÁ™ÅÁ†¥ÊûÅÈôê'))
[05.06.2025 19:09] Using data from previous issue: {"categories": ["#diffusion", "#benchmark", "#cv", "#architecture"], "emoji": "üñºÔ∏è", "ru": {"title": "–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∫–∞–∫ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –ò–ò-–æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∏—Å–∫—É—Å
[05.06.2025 19:09] Using data from previous issue: {"categories": ["#inference", "#alignment", "#rlhf", "#optimization"], "emoji": "üé®", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏–µ –Ω–∞–≥—Ä–∞–¥ –≤ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö —Å –ø–æ–º–æ—â—å—é —É–º–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Psi-Sampler - –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏—è –Ω–∞–≥—Ä–∞–¥ –ø—Ä–∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–µ –≤ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö
[05.06.2025 19:09] Using data from previous issue: {"categories": ["#story_generation", "#data", "#dataset", "#optimization", "#training"], "emoji": "üìä", "ru": {"title": "VisCode-200K: –ë–æ–ª—å—à–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —É–º–Ω–æ–≥–æ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–æ–≤", "desc": "VisCode-200K - —ç—Ç–æ –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∑–∞–¥–∞—á –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –≥—Ä–∞—Ñ–∏–∫–æ–≤ 
[05.06.2025 19:09] Using data from previous issue: {"categories": ["#training", "#rl", "#optimization", "#reasoning"], "emoji": "üß†", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Ä–∞—Å–∫—Ä—ã—Ç–∏–µ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª–∞ –ò–ò —á–µ—Ä–µ–∑ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –∫—Ä–∏—Ç–∏–∫–µ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ Critique Fine-Tuning (CFT) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). CFT 
[05.06.2025 19:09] Using data from previous issue: {"categories": ["#multimodal", "#diffusion", "#synthetic", "#video", "#training"], "emoji": "üéûÔ∏è", "ru": {"title": "LayerFlow: –£–º–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –º–Ω–æ–≥–æ—Å–ª–æ–π–Ω–æ–≥–æ –≤–∏–¥–µ–æ –ø–æ —Ç–µ–∫—Å—Ç–æ–≤—ã–º –ø–æ–¥—Å–∫–∞–∑–∫–∞–º", "desc": "LayerFlow - —ç—Ç–æ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ —Å —É—á–µ—Ç–æ–º —Å–ª–æ–µ–≤, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä –¥–∏
[05.06.2025 19:09] Using data from previous issue: {"categories": ["#reasoning", "#open_source", "#optimization", "#multimodal", "#benchmark", "#interpretability"], "emoji": "üìä", "ru": {"title": "SVGenius: –∫–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –ò–ò –≤ —Ä–∞–±–æ—Ç–µ —Å –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –≥—Ä–∞—Ñ–∏–∫–æ–π", "desc": "SVGenius - —ç—Ç–æ –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –±–æ–ª—å—à–∏—Ö —è
[05.06.2025 19:09] Using data from previous issue: {"categories": ["#video", "#optimization", "#diffusion", "#rlhf"], "emoji": "üé¨", "ru": {"title": "DenseDPO: –¢–æ—á–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ", "desc": "DenseDPO - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ç–µ–∫—Å—Ç-–≤-–≤–∏–¥–µ–æ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –û–Ω —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É —Å–º–µ—â–µ–Ω–∏—è –≤ —Å—Ç–æ—Ä–æ–Ω—É –∫–ª–∏–ø
[05.06.2025 19:09] Using data from previous issue: {"categories": ["#rlhf", "#rl", "#reasoning", "#training"], "emoji": "üß†", "ru": {"title": "–ù–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —Å–æ—Ü–∏–∞–ª—å–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ –º–µ—Ç–æ–¥ Temporal-aware Hierarchical Cognitive Reinforcement Learning (TimeHC-RL) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–æ—Ü–∏
[05.06.2025 19:09] Using data from previous issue: {"categories": ["#architecture", "#inference", "#long_context", "#optimization", "#training"], "emoji": "üöÄ", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞", "desc": "–ú–µ—Ç–æ–¥ Rectified Sparse Attention (ReSA) —É–ª—É—á—à–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –≤ –±–æ
[05.06.2025 19:09] Using data from previous issue: {"categories": ["#benchmark", "#data", "#hallucinations", "#interpretability", "#ethics", "#training"], "emoji": "‚öñÔ∏è", "ru": {"title": "DBG: –ù–æ–≤—ã–π —Å–ø–æ—Å–æ–± –∏–∑–º–µ—Ä–µ–Ω–∏—è –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç–∏ –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∏–∑–º–µ—Ä–µ–Ω–∏—è –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç–∏ —Å–∞–º–æ–ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ
[05.06.2025 19:09] Using data from previous issue: {"categories": ["#games", "#agents", "#video", "#benchmark", "#transfer_learning"], "emoji": "üéÆ", "ru": {"title": "Orak: —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∏–≥—Ä–æ–≤—ã—Ö –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤ –Ω–æ–≤–æ–≥–æ –ø–æ–∫–æ–ª–µ–Ω–∏—è", "desc": "Orak - —ç—Ç–æ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ –æ—Ü–µ–Ω–∫–∏ –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM)
[05.06.2025 19:09] Using data from previous issue: {"categories": ["#architecture", "#multimodal", "#inference", "#games", "#audio", "#video", "#optimization"], "emoji": "üó£Ô∏è", "ru": {"title": "–û–∂–∏–≤–ª—è–µ–º –∞–≤–∞—Ç–∞—Ä—ã: –∞—É–¥–∏–æ-—É–ø—Ä–∞–≤–ª—è–µ–º–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∏–¥–µ–æ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏", "desc": "TalkingMachines - —ç—Ç–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞, –ø—Ä–µ–æ–±—Ä–∞–∑—É—é—â–∞—è –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª
[05.06.2025 19:09] Using data from previous issue: {"categories": ["#multimodal", "#training", "#optimization", "#diffusion", "#rlhf", "#security"], "emoji": "üõ°Ô∏è", "ru": {"title": "LEAF: –ü–æ–≤—ã—à–µ–Ω–∏–µ —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏ CLIP –∫ —Å–æ—Å—Ç—è–∑–∞—Ç–µ–ª—å–Ω—ã–º –∞—Ç–∞–∫–∞–º", "desc": "LEAF - —ç—Ç–æ –º–µ—Ç–æ–¥ —Å–æ—Å—Ç—è–∑–∞—Ç–µ–ª—å–Ω–æ–π –¥–æ–≤–æ–¥–∫–∏ (adversarial finetuning), –∫–æ—Ç–æ—Ä—ã–π –ø–æ–≤—ã—à–∞–µ—Ç —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å —Ç–µ–∫—Å—Ç
[05.06.2025 19:09] Using data from previous issue: {"categories": ["#math", "#rl", "#reasoning", "#optimization", "#rlhf", "#training"], "emoji": "üß†", "ru": {"title": "Critique-GRPO: –£–ª—É—á—à–µ–Ω–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –ò–ò —á–µ—Ä–µ–∑ –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—É—é –æ–±—Ä–∞—Ç–Ω—É—é —Å–≤—è–∑—å", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Critique-GRPO - –Ω–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂
[05.06.2025 19:09] Using data from previous issue: {"categories": ["#benchmark", "#training", "#optimization"], "emoji": "üß†", "ru": {"title": "–ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –∏ –ø–ª–∞—Å—Ç–∏—á–Ω–æ—Å—Ç–∏ –≤ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–º—É –æ–±—É—á–µ–Ω–∏—é (Continual Learning) —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞
[05.06.2025 19:09] Using data from previous issue: {"categories": ["#diffusion", "#open_source", "#cv", "#dataset"], "emoji": "üñºÔ∏è", "ru": {"title": "–£–º–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ —Å–ª–æ–∏ —Å –ø–æ–º–æ—â—å—é –ò–ò", "desc": "DiffDecompose - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–æ–≥–æ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ –¥–ª—è –¥–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —Å–ª–æ–∏. –û–Ω–∞ —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º
[05.06.2025 19:09] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#synthetic", "#cv", "#optimization", "#open_source"], "emoji": "üñºÔ∏è", "ru": {"title": "RefEdit: –ü—Ä–æ—Ä—ã–≤ –≤ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ —Å–ª–æ–∂–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é –ò–ò", "desc": "RefEdit - —ç—Ç–æ –º–æ–¥–µ–ª—å —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π, –æ–±—É—á–µ–Ω–Ω–∞—è –Ω–∞ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫
[05.06.2025 19:09] Using data from previous issue: {"categories": ["#training", "#optimization", "#alignment", "#rlhf", "#dataset"], "emoji": "‚öñÔ∏è", "ru": {"title": "LLM-—Å—É–¥—å–∏: –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ—Ü–µ–Ω–∫–∞ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –ø–æ–º–æ—â—å—é —Ä–µ–≥—Ä–µ—Å—Å–∏–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ LLM-as-a-judge, –≥–¥–µ –±–æ–ª—å—à–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥—Ä—É–≥–æ–π –º–æ–¥
[05.06.2025 19:09] Using data from previous issue: {"categories": ["#data", "#transfer_learning", "#training", "#optimization", "#diffusion", "#dataset"], "emoji": "üß†", "ru": {"title": "–ü–æ–≤—ã—à–µ–Ω–∏–µ —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é —Å–ª–æ–∂–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∏—Ñ—Ñ—É–∑–∏–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–æ
[05.06.2025 19:09] Using data from previous issue: {"categories": ["#optimization", "#dataset", "#survey", "#benchmark"], "emoji": "üìä", "ru": {"title": "BenchHub: –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "BenchHub - —ç—Ç–æ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –±–µ–Ω—á–º–∞—Ä–∫–æ–≤, –∫–æ—Ç–æ—Ä–æ–µ –∞–≥—Ä–µ–≥–∏—Ä—É–µ—Ç –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥
[05.06.2025 19:09] Using data from previous issue: {"categories": ["#optimization", "#inference", "#training"], "emoji": "‚úÇÔ∏è", "ru": {"title": "–£–º–Ω–∞—è –æ–±—Ä–µ–∑–∫–∞ —Å–ª–æ–µ–≤ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–ü—Ä–µ–¥–ª–æ–∂–µ–Ω –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π –ø–æ—Å–ª–æ–π–Ω–æ–π –æ–±—Ä–µ–∑–∫–∏ (DLP) –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. DLP –∞–¥–∞–ø—Ç–∏–≤–Ω–æ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –≤–∞–∂–Ω–æ—Å—Ç—å –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ—è, –∫–æ–º–±–∏–Ω
[05.06.2025 19:09] Using data from previous issue: {"categories": ["#training", "#architecture", "#survey", "#agents", "#multimodal", "#security", "#alignment", "#benchmark", "#interpretability"], "emoji": "ü§ñ", "ru": {"title": "–ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –∏ –¥–æ–≤–µ—Ä–∏–µ –≤ —ç–ø–æ—Ö—É –∞–≥–µ–Ω—Ç–Ω–æ–≥–æ –ò–ò", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–æ–≤–µ—Ä–∏–µ–º, —Ä–∏
[05.06.2025 19:09] Using data from previous issue: {"categories": ["#benchmark", "#dataset", "#open_source", "#science"], "emoji": "‚ö°", "ru": {"title": "HTSC-2025: —ç—Ç–∞–ª–æ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–æ—Ä—ã–≤–∞ –≤ —Å–≤–µ—Ä—Ö–ø—Ä–æ–≤–æ–¥–∏–º–æ—Å—Ç–∏", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö HTSC-2025 –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –≤—ã—Å–æ–∫–æ—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–Ω—ã—Ö —Å–≤–µ—Ä—Ö–ø—Ä–æ–≤–æ–¥—è—â–∏—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤ —Å –ø–æ–º–æ—â—å—é –º–µ—Ç–æ–¥–æ–≤ –∏—Å–∫—É
[05.06.2025 19:09] Querying the API.
[05.06.2025 19:09] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Position Specialists (PosS) enhance Large Language Model (LLM) inference by using position-specialized draft layers to improve token prediction accuracy and acceptance rates.  					AI-generated summary 				 Speculative decoding accelerates Large Language Model (LLM) inference by using a small draft model to predict multiple tokens, and a large target model to verify these tokens in parallel. Recent studies leverage the hidden state of the target model to enhance draft model prediction accuracy. However, existing methods suffer from the degrading quality of draft token predictions at later positions, due to error accumulation in draft model generated features. In this paper, we propose Position Specialists (PosS), which consist of multiple position-specialized draft layers to generate tokens at assigned position(s). Position specialists greatly improve token acceptance rate at later positions per drafting round, as each specialist only needs to focus on handling a certain level of draft model feature deviation. Experiment results on Llama-3-8B-Instruct and Llama-2-13B-chat across six datasets demonstrate that PosS effectively improves over baselines on average acceptance length and speed-up ratio. Our codebase is available at https://github.com/shrango/PosS.
[05.06.2025 19:10] Response: {
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Position Specialists (PosS) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞ –≤—ã–≤–æ–¥–∞ –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM). PosS –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å–ª–æ–µ–≤ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤ –Ω–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö –ø–æ–∑–∏—Ü–∏—è—Ö, —á—Ç–æ –ø–æ–≤—ã—à–∞–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∏ —É—Ä–æ–≤–µ–Ω—å –ø—Ä–∏–Ω—è—Ç–∏—è —Ç–æ–∫–µ–Ω–æ–≤. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –æ—Å–æ–±–µ–Ω–Ω–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–µ–Ω –¥–ª—è —Ç–æ–∫–µ–Ω–æ–≤ –Ω–∞ –±–æ–ª–µ–µ –ø–æ–∑–¥–Ω–∏—Ö –ø–æ–∑–∏—Ü–∏—è—Ö, –≥–¥–µ –æ–±—ã—á–Ω–æ –Ω–∞–±–ª—é–¥–∞–µ—Ç—Å—è –Ω–∞–∫–æ–ø–ª–µ–Ω–∏–µ –æ—à–∏–±–æ–∫. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ –º–æ–¥–µ–ª—è—Ö Llama –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ PosS –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –±–∞–∑–æ–≤—ã–µ –º–µ—Ç–æ–¥—ã –ø–æ —Å—Ä–µ–¥–Ω–µ–π –¥–ª–∏–Ω–µ –ø—Ä–∏–Ω—è—Ç–∏—è –∏ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—É —É—Å–∫–æ—Ä–µ–Ω–∏—è.",
  "emoji": "üéØ",
  "title": "–°–ø–µ—Ü–∏–∞–ª–∏—Å—Ç—ã –ø–æ –ø–æ–∑–∏—Ü–∏—è–º: —Ç–æ—á–Ω–æ—Å—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤ –Ω–∞ –Ω–æ–≤–æ–º —É—Ä–æ–≤–Ω–µ"
}
[05.06.2025 19:10] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Position Specialists (PosS) enhance Large Language Model (LLM) inference by using position-specialized draft layers to improve token prediction accuracy and acceptance rates.  					AI-generated summary 				 Speculative decoding accelerates Large Language Model (LLM) inference by using a small draft model to predict multiple tokens, and a large target model to verify these tokens in parallel. Recent studies leverage the hidden state of the target model to enhance draft model prediction accuracy. However, existing methods suffer from the degrading quality of draft token predictions at later positions, due to error accumulation in draft model generated features. In this paper, we propose Position Specialists (PosS), which consist of multiple position-specialized draft layers to generate tokens at assigned position(s). Position specialists greatly improve token acceptance rate at later positions per drafting round, as each specialist only needs to focus on handling a certain level of draft model feature deviation. Experiment results on Llama-3-8B-Instruct and Llama-2-13B-chat across six datasets demonstrate that PosS effectively improves over baselines on average acceptance length and speed-up ratio. Our codebase is available at https://github.com/shrango/PosS."

[05.06.2025 19:10] Response: ```python
['INFERENCE', 'ARCHITECTURE', 'TRAINING']
```
[05.06.2025 19:10] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Position Specialists (PosS) enhance Large Language Model (LLM) inference by using position-specialized draft layers to improve token prediction accuracy and acceptance rates.  					AI-generated summary 				 Speculative decoding accelerates Large Language Model (LLM) inference by using a small draft model to predict multiple tokens, and a large target model to verify these tokens in parallel. Recent studies leverage the hidden state of the target model to enhance draft model prediction accuracy. However, existing methods suffer from the degrading quality of draft token predictions at later positions, due to error accumulation in draft model generated features. In this paper, we propose Position Specialists (PosS), which consist of multiple position-specialized draft layers to generate tokens at assigned position(s). Position specialists greatly improve token acceptance rate at later positions per drafting round, as each specialist only needs to focus on handling a certain level of draft model feature deviation. Experiment results on Llama-3-8B-Instruct and Llama-2-13B-chat across six datasets demonstrate that PosS effectively improves over baselines on average acceptance length and speed-up ratio. Our codebase is available at https://github.com/shrango/PosS."

[05.06.2025 19:10] Response: ```python
["OPTIMIZATION"]
```
[05.06.2025 19:10] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces Position Specialists (PosS), a method that enhances the performance of Large Language Models (LLMs) during inference. By utilizing position-specialized draft layers, PosS improves the accuracy of token predictions, particularly at later positions where traditional methods struggle due to error accumulation. The approach allows each specialist to focus on specific positions, leading to higher acceptance rates for generated tokens. Experimental results show that PosS outperforms existing methods in both acceptance length and inference speed across multiple datasets.","title":"Enhancing Token Prediction with Position Specialists"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper introduces Position Specialists (PosS), a method that enhances the performance of Large Language Models (LLMs) during inference. By utilizing position-specialized draft layers, PosS improves the accuracy of token predictions, particularly at later positions where traditional methods struggle due to error accumulation. The approach allows each specialist to focus on specific positions, leading to higher acceptance rates for generated tokens. Experimental results show that PosS outperforms existing methods in both acceptance length and inference speed across multiple datasets.', title='Enhancing Token Prediction with Position Specialists'))
[05.06.2025 19:10] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫‰ΩçÁΩÆ‰∏ìÂÆ∂ÔºàPosSÔºâÁöÑÊñ∞ÊñπÊ≥ïÔºåÊó®Âú®ÈÄöËøá‰ΩøÁî®‰ΩçÁΩÆ‰∏ìÁî®ÁöÑËçâÁ®øÂ±ÇÊù•ÊèêÈ´òÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÁöÑÊé®ÁêÜÊÄßËÉΩ„ÄÇ‰ΩçÁΩÆ‰∏ìÂÆ∂ËÉΩÂ§üÂú®ÁâπÂÆö‰ΩçÁΩÆÁîüÊàêÊõ¥ÂáÜÁ°ÆÁöÑÊ†áËÆ∞Ôºå‰ªéËÄåÊèêÈ´òÊ†áËÆ∞ÁöÑÊé•ÂèóÁéáÔºåÂ∞§ÂÖ∂ÊòØÂú®ÂêéÊúü‰ΩçÁΩÆ„ÄÇÈÄöËøá‰∏ìÊ≥®‰∫éÂ§ÑÁêÜËçâÁ®øÊ®°ÂûãÁâπÂæÅÁöÑÂÅèÂ∑ÆÔºåÊØè‰∏™‰∏ìÂÆ∂ÂèØ‰ª•ÊúâÊïàÂáèÂ∞ëÈîôËØØÁ¥ØÁßØÂ∏¶Êù•ÁöÑÂΩ±Âìç„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåPosSÂú®Â§ö‰∏™Êï∞ÊçÆÈõÜ‰∏äÊòæËëó‰ºò‰∫éÂü∫Á∫øÊ®°ÂûãÔºåÊèêÂçá‰∫ÜÂπ≥ÂùáÊé•ÂèóÈïøÂ∫¶ÂíåÂä†ÈÄüÊØî„ÄÇ","title":"‰ΩçÁΩÆ‰∏ìÂÆ∂ÊèêÂçáËØ≠Ë®ÄÊ®°ÂûãÊé®ÁêÜÊïàÁéá"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫‰ΩçÁΩÆ‰∏ìÂÆ∂ÔºàPosSÔºâÁöÑÊñ∞ÊñπÊ≥ïÔºåÊó®Âú®ÈÄöËøá‰ΩøÁî®‰ΩçÁΩÆ‰∏ìÁî®ÁöÑËçâÁ®øÂ±ÇÊù•ÊèêÈ´òÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÁöÑÊé®ÁêÜÊÄßËÉΩ„ÄÇ‰ΩçÁΩÆ‰∏ìÂÆ∂ËÉΩÂ§üÂú®ÁâπÂÆö‰ΩçÁΩÆÁîüÊàêÊõ¥ÂáÜÁ°ÆÁöÑÊ†áËÆ∞Ôºå‰ªéËÄåÊèêÈ´òÊ†áËÆ∞ÁöÑÊé•ÂèóÁéáÔºåÂ∞§ÂÖ∂ÊòØÂú®ÂêéÊúü‰ΩçÁΩÆ„ÄÇÈÄöËøá‰∏ìÊ≥®‰∫éÂ§ÑÁêÜËçâÁ®øÊ®°ÂûãÁâπÂæÅÁöÑÂÅèÂ∑ÆÔºåÊØè‰∏™‰∏ìÂÆ∂ÂèØ‰ª•ÊúâÊïàÂáèÂ∞ëÈîôËØØÁ¥ØÁßØÂ∏¶Êù•ÁöÑÂΩ±Âìç„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåPosSÂú®Â§ö‰∏™Êï∞ÊçÆÈõÜ‰∏äÊòæËëó‰ºò‰∫éÂü∫Á∫øÊ®°ÂûãÔºåÊèêÂçá‰∫ÜÂπ≥ÂùáÊé•ÂèóÈïøÂ∫¶ÂíåÂä†ÈÄüÊØî„ÄÇ', title='‰ΩçÁΩÆ‰∏ìÂÆ∂ÊèêÂçáËØ≠Ë®ÄÊ®°ÂûãÊé®ÁêÜÊïàÁéá'))
[05.06.2025 19:10] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#reasoning", "#video"], "emoji": "üé¨", "ru": {"title": "–£–º–Ω–æ–µ –≤–∏–¥–µ–æ: —Ä–∞—Å—Å—É–∂–¥–∞–µ–º –∫–∞–∫ —ç–∫—Å–ø–µ—Ä—Ç—ã –≤ —Ä–∞–∑–Ω—ã—Ö –æ–±–ª–∞—Å—Ç—è—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ Video-SKoT –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –≤–∏–¥–µ–æ–∞–Ω–∞–ª–∏–∑–∞ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –¥–æ–º–µ–Ω–∞—Ö. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫
[05.06.2025 19:10] Using data from previous issue: {"categories": ["#graphs", "#cv", "#reasoning", "#agents", "#hallucinations", "#multimodal", "#benchmark", "#interpretability"], "emoji": "üîÄ", "ru": {"title": "–¢–æ—á–Ω–∞—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –±–ª–æ–∫-—Å—Ö–µ–º —Å –ø–æ–º–æ—â—å—é –Ω–µ–π—Ä–æ—Å–∏–º–≤–æ–ª–∏—á–µ—Å–∫–æ–≥–æ –∞–≥–µ–Ω—Ç–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∑–∞–¥–∞—á—É —Ç–æ—á–Ω–æ–π –∞—Ç—Ä–∏–±—É—Ü–∏–∏ –±–ª–æ–∫-—Å—Ö–µ–º –∏ –∞–≥–µ–Ω—Ç
[05.06.2025 19:10] Using data from previous issue: {"categories": ["#cv", "#rl", "#training", "#reasoning", "#hallucinations", "#interpretability", "#dataset"], "emoji": "üîç", "ru": {"title": "–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ–µ –æ–±—ä–µ–∫—Ç–Ω–æ–µ —Ä–µ—Ñ–µ—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ —á–µ—Ä–µ–∑ –ø–æ—à–∞–≥–æ–≤—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∑–∞–¥–∞—á–µ –æ–±—ä–µ–∫—Ç–Ω–æ–≥–æ —Ä–µ—Ñ–µ—Ä–∏—Ä–æ–≤–∞–Ω–∏—è –≤ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω
[05.06.2025 19:10] Using data from previous issue: {"categories": ["#architecture", "#training", "#optimization"], "emoji": "üß†", "ru": {"title": "–î–≤–æ–π–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É Dual-Arch –¥–ª—è –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä–∞—è —Ä–µ—à–∞–µ—Ç –¥–∏–ª–µ–º–º—É —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏-–ø–ª–∞—Å—Ç–∏—á–Ω–æ—Å—Ç–∏ –Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç
[05.06.2025 19:10] Using data from previous issue: {"categories": ["#dataset", "#data"], "emoji": "üï∏Ô∏è", "ru": {"title": "CRAWLDoc: –£–º–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∏–∑ –≤–µ–±-–¥–æ–∫—É–º–µ–Ω—Ç–æ–≤", "desc": "CRAWLDoc - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–≥–æ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è —Å–≤—è–∑–∞–Ω–Ω—ã—Ö –≤–µ–±-–¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –ø—É–±–ª–∏–∫–∞—Ü–∏–π. –û–Ω –Ω–∞—á–∏–Ω–∞–µ—Ç —Å URL –ø—É–±–ª–∏–∫–∞—Ü–∏–∏, –∏–∑–≤–ª–µ–∫–∞–µ—Ç —Å–≤—è–∑–∞–Ω–Ω
[05.06.2025 19:10] Using data from previous issue: {"categories": ["#open_source", "#data", "#dataset", "#multimodal", "#cv", "#benchmark", "#security", "#ethics"], "emoji": "üß©", "ru": {"title": "–í–∏–∑—É–∞–ª—å–Ω–æ–µ —Å—à–∏–≤–∞–Ω–∏–µ: —Å–∫—Ä—ã—Ç–∞—è —É–≥—Ä–æ–∑–∞ –≤ –º–æ–¥–µ–ª—è—Ö –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è", "desc": "–≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Ä–∞—Å–∫—Ä—ã–≤–∞–µ—Ç —Ñ–µ–Ω–æ–º–µ–Ω '–≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ —Å—à–∏–≤–∞–Ω–∏—è' –≤ –º–æ–¥–µ–ª—è—Ö –∫–æ–º–ø—å—é—Ç–µ
[05.06.2025 19:10] Using data from previous issue: {"categories": ["#reasoning", "#optimization", "#dataset", "#benchmark"], "emoji": "üíπ", "ru": {"title": "FinChain: –Ω–æ–≤—ã–π —Ä—É–±–µ–∂ –≤ –æ—Ü–µ–Ω–∫–µ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –ò–ò", "desc": "FinChain - —ç—Ç–æ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–Ω–æ–≥–æ—ç—Ç–∞–ø–Ω—ã—Ö —Å–∏–º–≤–æ–ª—å–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –∑–∞–¥–∞—á–∞—Ö, —Ñ–æ–∫—É—Å–∏—Ä—É—é—â–∏–π—Å—è –Ω–∞ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á
[05.06.2025 19:10] Using data from previous issue: {"categories": ["#rl", "#reasoning", "#optimization", "#math", "#training"], "emoji": "üß†", "ru": {"title": "SPO: –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ —Å–µ–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –±–æ–ª—å
[05.06.2025 19:10] Using data from previous issue: {"categories": ["#audio", "#diffusion", "#multimodal"], "emoji": "üîä", "ru": {"title": "–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–≤—É–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∑–≤—É–∫–∞, –æ—Å–Ω–æ–≤–∞–Ω–Ω—É—é –Ω–∞ –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–∞—Ö –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö. –ú–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ
[05.06.2025 19:10] Using data from previous issue: {"categories": ["#training", "#data", "#optimization"], "emoji": "üîç", "ru": {"title": "–†–∞—Å–∫—Ä—ã–≤–∞—è —Å–µ–∫—Ä–µ—Ç—ã –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤ –∞–∫—Ç–∏–≤–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏", "desc": "–≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ –∞–∫—Ç–∏–≤–Ω–æ–º—É –æ–±—É—á–µ–Ω–∏—é (Active Learning, AL) –≤ –º–∞—à–∏–Ω–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏, –∫–æ—Ç–æ—Ä–æ–µ –ø–æ–º–æ–≥–∞–µ—Ç –º–∏–Ω–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å —É—Å–∏–ª–∏—è –ø–æ —Ä–∞–∑–º–µ—Ç–∫–µ –¥–∞–Ω–Ω—ã—Ö. 
[05.06.2025 19:10] Using data from previous issue: {"categories": ["#training", "#3d", "#optimization"], "emoji": "üîç", "ru": {"title": "–î–≤–æ–π–Ω–æ–µ –∑—Ä–µ–Ω–∏–µ –¥–ª—è —Ç–æ—á–Ω–æ–π 3D-—Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏", "desc": "–ü—Ä–µ–¥–ª–æ–∂–µ–Ω –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ Asymmetric Dual 3DGS –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è 3D-—Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –û–Ω –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ –æ–±—É—á–µ–Ω–∏–∏ –¥–≤—É—Ö –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π 3D Gaussian Splatting —Å –æ
[05.06.2025 19:10] Using data from previous issue: {"categories": ["#cv", "#benchmark", "#diffusion", "#data", "#training", "#optimization"], "emoji": "üîÑ", "ru": {"title": "FLAIR: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ —Ä–µ—à–µ–Ω–∏–∏ –æ–±—Ä–∞—Ç–Ω—ã—Ö –∑–∞–¥–∞—á —Å –ø–æ–º–æ—â—å—é –ø–æ—Ç–æ–∫–æ–≤—ã—Ö –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "FLAIR - —ç—Ç–æ –Ω–æ–≤–∞—è –≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø
[05.06.2025 19:10] Using data from previous issue: {"categories": ["#agents", "#agi", "#optimization", "#small_models"], "emoji": "ü§ñ", "ru": {"title": "–ú–∞–ª—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ - –±—É–¥—É—â–µ–µ –∞–≥–µ–Ω—Ç–Ω–æ–≥–æ –ò–ò", "desc": "–°—Ç–∞—Ç—å—è —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –º–∞–ª—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (SLM) –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞. –ê–≤—Ç–æ—Ä—ã —É—Ç–≤–µ—Ä–∂–¥–∞—é—Ç, —á—Ç–æ SLM –¥
[05.06.2025 19:10] Using data from previous issue: {"categories": ["#ethics", "#benchmark", "#multimodal", "#agents", "#security", "#alignment"], "emoji": "üñ•Ô∏è", "ru": {"title": "RIOSWorld: –≤—ã—è–≤–ª–µ–Ω–∏–µ —Ä–∏—Å–∫–æ–≤ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤ –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö", "desc": "RIOSWorld - —ç—Ç–æ –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ä–∏—Å–∫–æ–≤ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏
[05.06.2025 19:10] Loading Chinese text from previous data.
[05.06.2025 19:10] Renaming data file.
[05.06.2025 19:10] Renaming previous data. hf_papers.json to ./d/2025-06-05.json
[05.06.2025 19:10] Saving new data file.
[05.06.2025 19:10] Generating page.
[05.06.2025 19:10] Renaming previous page.
[05.06.2025 19:10] Renaming previous data. index.html to ./d/2025-06-05.html
[05.06.2025 19:10] [Experimental] Generating Chinese page for reading.
[05.06.2025 19:10] Chinese vocab [{'word': 'ÂºÄÊ∫ê', 'pinyin': 'kƒÅi yu√°n', 'trans': 'open source'}, {'word': 'ËßÜËßâ-ËØ≠Ë®ÄÊ®°Âûã', 'pinyin': 'sh√¨ ju√© y«î y√°n m√≥ x√≠ng', 'trans': 'vision-language model'}, {'word': 'Ë°®Áé∞Âá∫Ëâ≤', 'pinyin': 'bi«éo xi√†n ch≈´ s√®', 'trans': 'perform excellently'}, {'word': 'Â§öÊ®°ÊÄÅ', 'pinyin': 'du≈ç m√≥ t√†i', 'trans': 'multimodal'}, {'word': 'Êé®ÁêÜ', 'pinyin': 'tuƒ´ l«ê', 'trans': 'reasoning'}, {'word': 'Ë∂ÖË∂ä', 'pinyin': 'chƒÅo yu√®', 'trans': 'surpass'}, {'word': 'ÂèÇÊï∞Èáè', 'pinyin': 'cƒÅn sh«î li√†ng', 'trans': 'parameter quantity'}, {'word': 'GUI', 'pinyin': 'GUI', 'trans': 'Graphical User Interface'}, {'word': 'Â∫îÁî®', 'pinyin': 'y√¨ng y√≤ng', 'trans': 'application'}, {'word': '‰∏ìÈó®', 'pinyin': 'zhuƒÅn m√©n', 'trans': 'specialized'}, {'word': 'È¢ÑËÆ≠ÁªÉ', 'pinyin': 'y√π x√πn li√†n', 'trans': 'pre-training'}, {'word': 'Ê∑∑Âêà', 'pinyin': 'h√πn h√©', 'trans': 'hybrid'}, {'word': 'Á≠ñÁï•', 'pinyin': 'c√® l√º√®', 'trans': 'strategy'}, {'word': 'Âº∫ÂåñÂ≠¶‰π†', 'pinyin': 'qi√°ng hu√† xu√© x√≠', 'trans': 'reinforcement learning'}, {'word': 'ËØÑ‰º∞', 'pinyin': 'p√≠ng g≈´', 'trans': 'evaluation'}, {'word': 'Â•ó‰ª∂', 'pinyin': 't√†o ji√†n', 'trans': 'suite'}, {'word': 'ÂèØÈáçÂ§çÊÄß', 'pinyin': 'kƒõ ch√≥ng f√π x√¨ng', 'trans': 'reproducibility'}, {'word': 'Êé®Âä®', 'pinyin': 'tuƒ´ d√≤ng', 'trans': 'promote'}, {'word': 'È¢ÜÂüü', 'pinyin': 'l«êng y√π', 'trans': 'field'}, {'word': 'Ê£ÄÊü•ÁÇπ', 'pinyin': 'ji«én ch√° di«én', 'trans': 'checkpoint'}]
[05.06.2025 19:10] Renaming previous Chinese page.
[05.06.2025 19:10] Renaming previous data. zh.html to ./d/2025-06-04_zh_reading_task.html
[05.06.2025 19:10] Writing Chinese reading task.
[05.06.2025 19:10] Writing result.
[05.06.2025 19:10] Renaming log file.
[05.06.2025 19:10] Renaming previous data. log.txt to ./logs/2025-06-05_last_log.txt
