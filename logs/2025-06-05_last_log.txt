[04.06.2025 23:15] Read previous papers.
[04.06.2025 23:15] Generating top page (month).
[04.06.2025 23:15] Writing top page (month).
[05.06.2025 00:55] Read previous papers.
[05.06.2025 00:55] Get feed.
[05.06.2025 00:55] Get page data from previous paper. URL: https://huggingface.co/papers/2505.24726
[05.06.2025 00:55] Get page data from previous paper. URL: https://huggingface.co/papers/2506.02387
[05.06.2025 00:55] Get page data from previous paper. URL: https://huggingface.co/papers/2506.03147
[05.06.2025 00:55] Get page data from previous paper. URL: https://huggingface.co/papers/2506.02096
[05.06.2025 00:55] Get page data from previous paper. URL: https://huggingface.co/papers/2505.24120
[05.06.2025 00:55] Get page data from previous paper. URL: https://huggingface.co/papers/2505.24714
[05.06.2025 00:55] Get page data from previous paper. URL: https://huggingface.co/papers/2506.02397
[05.06.2025 00:55] Get page data from previous paper. URL: https://huggingface.co/papers/2506.00123
[05.06.2025 00:55] Get page data from previous paper. URL: https://huggingface.co/papers/2506.03135
[05.06.2025 00:55] Get page data from previous paper. URL: https://huggingface.co/papers/2506.03143
[05.06.2025 00:55] Get page data from previous paper. URL: https://huggingface.co/papers/2505.23061
[05.06.2025 00:55] Get page data from previous paper. URL: https://huggingface.co/papers/2506.03065
[05.06.2025 00:55] Get page data from previous paper. URL: https://huggingface.co/papers/2506.01674
[05.06.2025 00:55] Get page data from previous paper. URL: https://huggingface.co/papers/2506.00070
[05.06.2025 00:55] Get page data from previous paper. URL: https://huggingface.co/papers/2506.03136
[05.06.2025 00:55] Get page data from previous paper. URL: https://huggingface.co/papers/2506.03131
[05.06.2025 00:55] Get page data from previous paper. URL: https://huggingface.co/papers/2506.03126
[05.06.2025 00:55] Get page data from previous paper. URL: https://huggingface.co/papers/2506.02497
[05.06.2025 00:55] Get page data from previous paper. URL: https://huggingface.co/papers/2506.02528
[05.06.2025 00:55] Get page data from previous paper. URL: https://huggingface.co/papers/2506.01144
[05.06.2025 00:55] Get page data from previous paper. URL: https://huggingface.co/papers/2506.00910
[05.06.2025 00:55] Get page data from previous paper. URL: https://huggingface.co/papers/2506.03123
[05.06.2025 00:55] Get page data from previous paper. URL: https://huggingface.co/papers/2506.01789
[05.06.2025 00:55] Get page data from previous paper. URL: https://huggingface.co/papers/2505.22704
[05.06.2025 00:55] Get page data from previous paper. URL: https://huggingface.co/papers/2506.01716
[05.06.2025 00:55] Get page data from previous paper. URL: https://huggingface.co/papers/2506.00413
[05.06.2025 00:55] Get page data from previous paper. URL: https://huggingface.co/papers/2506.01274
[05.06.2025 00:55] Get page data from previous paper. URL: https://huggingface.co/papers/2506.03096
[05.06.2025 00:55] Get page data from previous paper. URL: https://huggingface.co/papers/2506.03079
[05.06.2025 00:55] Get page data from previous paper. URL: https://huggingface.co/papers/2506.02338
[05.06.2025 00:55] Get page data from previous paper. URL: https://huggingface.co/papers/2506.01004
[05.06.2025 00:55] Get page data from previous paper. URL: https://huggingface.co/papers/2506.00391
[05.06.2025 00:55] Get page data from previous paper. URL: https://huggingface.co/papers/2506.00227
[05.06.2025 00:55] Get page data from previous paper. URL: https://huggingface.co/papers/2505.24273
[05.06.2025 00:55] Get page data from previous paper. URL: https://huggingface.co/papers/2505.18079
[05.06.2025 00:55] Get page data from previous paper. URL: https://huggingface.co/papers/2506.03119
[05.06.2025 00:55] Get page data from previous paper. URL: https://huggingface.co/papers/2506.02678
[05.06.2025 00:55] Get page data from previous paper. URL: https://huggingface.co/papers/2506.02510
[05.06.2025 00:55] Get page data from previous paper. URL: https://huggingface.co/papers/2506.02454
[05.06.2025 00:55] Get page data from previous paper. URL: https://huggingface.co/papers/2506.02295
[05.06.2025 00:55] Get page data from previous paper. URL: https://huggingface.co/papers/2506.01565
[05.06.2025 00:55] Get page data from previous paper. URL: https://huggingface.co/papers/2505.16994
[05.06.2025 00:55] Get page data from previous paper. URL: https://huggingface.co/papers/2506.03144
[05.06.2025 00:55] Get page data from previous paper. URL: https://huggingface.co/papers/2506.02281
[05.06.2025 00:55] Get page data from previous paper. URL: https://huggingface.co/papers/2506.02138
[05.06.2025 00:55] Get page data from previous paper. URL: https://huggingface.co/papers/2506.01265
[05.06.2025 00:55] Get page data from previous paper. URL: https://huggingface.co/papers/2505.24362
[05.06.2025 00:55] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[05.06.2025 00:55] No deleted papers detected.
[05.06.2025 00:55] Downloading and parsing papers (pdf, html). Total: 47.
[05.06.2025 00:55] Downloading and parsing paper https://huggingface.co/papers/2505.24726.
[05.06.2025 00:55] Extra JSON file exists (./assets/json/2505.24726.json), skip PDF parsing.
[05.06.2025 00:55] Paper image links file exists (./assets/img_data/2505.24726.json), skip HTML parsing.
[05.06.2025 00:55] Success.
[05.06.2025 00:55] Downloading and parsing paper https://huggingface.co/papers/2506.02387.
[05.06.2025 00:55] Extra JSON file exists (./assets/json/2506.02387.json), skip PDF parsing.
[05.06.2025 00:55] Paper image links file exists (./assets/img_data/2506.02387.json), skip HTML parsing.
[05.06.2025 00:55] Success.
[05.06.2025 00:55] Downloading and parsing paper https://huggingface.co/papers/2506.03147.
[05.06.2025 00:55] Extra JSON file exists (./assets/json/2506.03147.json), skip PDF parsing.
[05.06.2025 00:55] Paper image links file exists (./assets/img_data/2506.03147.json), skip HTML parsing.
[05.06.2025 00:55] Success.
[05.06.2025 00:55] Downloading and parsing paper https://huggingface.co/papers/2506.02096.
[05.06.2025 00:55] Extra JSON file exists (./assets/json/2506.02096.json), skip PDF parsing.
[05.06.2025 00:55] Paper image links file exists (./assets/img_data/2506.02096.json), skip HTML parsing.
[05.06.2025 00:55] Success.
[05.06.2025 00:55] Downloading and parsing paper https://huggingface.co/papers/2505.24120.
[05.06.2025 00:55] Extra JSON file exists (./assets/json/2505.24120.json), skip PDF parsing.
[05.06.2025 00:55] Paper image links file exists (./assets/img_data/2505.24120.json), skip HTML parsing.
[05.06.2025 00:55] Success.
[05.06.2025 00:55] Downloading and parsing paper https://huggingface.co/papers/2505.24714.
[05.06.2025 00:55] Extra JSON file exists (./assets/json/2505.24714.json), skip PDF parsing.
[05.06.2025 00:55] Paper image links file exists (./assets/img_data/2505.24714.json), skip HTML parsing.
[05.06.2025 00:55] Success.
[05.06.2025 00:55] Downloading and parsing paper https://huggingface.co/papers/2506.02397.
[05.06.2025 00:55] Extra JSON file exists (./assets/json/2506.02397.json), skip PDF parsing.
[05.06.2025 00:55] Paper image links file exists (./assets/img_data/2506.02397.json), skip HTML parsing.
[05.06.2025 00:55] Success.
[05.06.2025 00:55] Downloading and parsing paper https://huggingface.co/papers/2506.00123.
[05.06.2025 00:55] Extra JSON file exists (./assets/json/2506.00123.json), skip PDF parsing.
[05.06.2025 00:55] Paper image links file exists (./assets/img_data/2506.00123.json), skip HTML parsing.
[05.06.2025 00:55] Success.
[05.06.2025 00:55] Downloading and parsing paper https://huggingface.co/papers/2506.03135.
[05.06.2025 00:55] Extra JSON file exists (./assets/json/2506.03135.json), skip PDF parsing.
[05.06.2025 00:55] Paper image links file exists (./assets/img_data/2506.03135.json), skip HTML parsing.
[05.06.2025 00:55] Success.
[05.06.2025 00:55] Downloading and parsing paper https://huggingface.co/papers/2506.03143.
[05.06.2025 00:55] Extra JSON file exists (./assets/json/2506.03143.json), skip PDF parsing.
[05.06.2025 00:55] Paper image links file exists (./assets/img_data/2506.03143.json), skip HTML parsing.
[05.06.2025 00:55] Success.
[05.06.2025 00:55] Downloading and parsing paper https://huggingface.co/papers/2505.23061.
[05.06.2025 00:55] Extra JSON file exists (./assets/json/2505.23061.json), skip PDF parsing.
[05.06.2025 00:55] Paper image links file exists (./assets/img_data/2505.23061.json), skip HTML parsing.
[05.06.2025 00:55] Success.
[05.06.2025 00:55] Downloading and parsing paper https://huggingface.co/papers/2506.03065.
[05.06.2025 00:55] Extra JSON file exists (./assets/json/2506.03065.json), skip PDF parsing.
[05.06.2025 00:55] Paper image links file exists (./assets/img_data/2506.03065.json), skip HTML parsing.
[05.06.2025 00:55] Success.
[05.06.2025 00:55] Downloading and parsing paper https://huggingface.co/papers/2506.01674.
[05.06.2025 00:55] Extra JSON file exists (./assets/json/2506.01674.json), skip PDF parsing.
[05.06.2025 00:55] Paper image links file exists (./assets/img_data/2506.01674.json), skip HTML parsing.
[05.06.2025 00:55] Success.
[05.06.2025 00:55] Downloading and parsing paper https://huggingface.co/papers/2506.00070.
[05.06.2025 00:55] Extra JSON file exists (./assets/json/2506.00070.json), skip PDF parsing.
[05.06.2025 00:55] Paper image links file exists (./assets/img_data/2506.00070.json), skip HTML parsing.
[05.06.2025 00:55] Success.
[05.06.2025 00:55] Downloading and parsing paper https://huggingface.co/papers/2506.03136.
[05.06.2025 00:55] Extra JSON file exists (./assets/json/2506.03136.json), skip PDF parsing.
[05.06.2025 00:55] Paper image links file exists (./assets/img_data/2506.03136.json), skip HTML parsing.
[05.06.2025 00:55] Success.
[05.06.2025 00:55] Downloading and parsing paper https://huggingface.co/papers/2506.03131.
[05.06.2025 00:55] Extra JSON file exists (./assets/json/2506.03131.json), skip PDF parsing.
[05.06.2025 00:55] Paper image links file exists (./assets/img_data/2506.03131.json), skip HTML parsing.
[05.06.2025 00:55] Success.
[05.06.2025 00:55] Downloading and parsing paper https://huggingface.co/papers/2506.03126.
[05.06.2025 00:55] Extra JSON file exists (./assets/json/2506.03126.json), skip PDF parsing.
[05.06.2025 00:55] Paper image links file exists (./assets/img_data/2506.03126.json), skip HTML parsing.
[05.06.2025 00:55] Success.
[05.06.2025 00:55] Downloading and parsing paper https://huggingface.co/papers/2506.02497.
[05.06.2025 00:55] Extra JSON file exists (./assets/json/2506.02497.json), skip PDF parsing.
[05.06.2025 00:55] Paper image links file exists (./assets/img_data/2506.02497.json), skip HTML parsing.
[05.06.2025 00:55] Success.
[05.06.2025 00:55] Downloading and parsing paper https://huggingface.co/papers/2506.02528.
[05.06.2025 00:55] Extra JSON file exists (./assets/json/2506.02528.json), skip PDF parsing.
[05.06.2025 00:55] Paper image links file exists (./assets/img_data/2506.02528.json), skip HTML parsing.
[05.06.2025 00:55] Success.
[05.06.2025 00:55] Downloading and parsing paper https://huggingface.co/papers/2506.01144.
[05.06.2025 00:55] Extra JSON file exists (./assets/json/2506.01144.json), skip PDF parsing.
[05.06.2025 00:55] Paper image links file exists (./assets/img_data/2506.01144.json), skip HTML parsing.
[05.06.2025 00:55] Success.
[05.06.2025 00:55] Downloading and parsing paper https://huggingface.co/papers/2506.00910.
[05.06.2025 00:55] Extra JSON file exists (./assets/json/2506.00910.json), skip PDF parsing.
[05.06.2025 00:55] Paper image links file exists (./assets/img_data/2506.00910.json), skip HTML parsing.
[05.06.2025 00:55] Success.
[05.06.2025 00:55] Downloading and parsing paper https://huggingface.co/papers/2506.03123.
[05.06.2025 00:55] Extra JSON file exists (./assets/json/2506.03123.json), skip PDF parsing.
[05.06.2025 00:55] Paper image links file exists (./assets/img_data/2506.03123.json), skip HTML parsing.
[05.06.2025 00:55] Success.
[05.06.2025 00:55] Downloading and parsing paper https://huggingface.co/papers/2506.01789.
[05.06.2025 00:55] Extra JSON file exists (./assets/json/2506.01789.json), skip PDF parsing.
[05.06.2025 00:55] Paper image links file exists (./assets/img_data/2506.01789.json), skip HTML parsing.
[05.06.2025 00:55] Success.
[05.06.2025 00:55] Downloading and parsing paper https://huggingface.co/papers/2505.22704.
[05.06.2025 00:55] Extra JSON file exists (./assets/json/2505.22704.json), skip PDF parsing.
[05.06.2025 00:55] Paper image links file exists (./assets/img_data/2505.22704.json), skip HTML parsing.
[05.06.2025 00:55] Success.
[05.06.2025 00:55] Downloading and parsing paper https://huggingface.co/papers/2506.01716.
[05.06.2025 00:55] Extra JSON file exists (./assets/json/2506.01716.json), skip PDF parsing.
[05.06.2025 00:55] Paper image links file exists (./assets/img_data/2506.01716.json), skip HTML parsing.
[05.06.2025 00:55] Success.
[05.06.2025 00:55] Downloading and parsing paper https://huggingface.co/papers/2506.00413.
[05.06.2025 00:55] Extra JSON file exists (./assets/json/2506.00413.json), skip PDF parsing.
[05.06.2025 00:55] Paper image links file exists (./assets/img_data/2506.00413.json), skip HTML parsing.
[05.06.2025 00:55] Success.
[05.06.2025 00:55] Downloading and parsing paper https://huggingface.co/papers/2506.01274.
[05.06.2025 00:55] Extra JSON file exists (./assets/json/2506.01274.json), skip PDF parsing.
[05.06.2025 00:55] Paper image links file exists (./assets/img_data/2506.01274.json), skip HTML parsing.
[05.06.2025 00:55] Success.
[05.06.2025 00:55] Downloading and parsing paper https://huggingface.co/papers/2506.03096.
[05.06.2025 00:55] Extra JSON file exists (./assets/json/2506.03096.json), skip PDF parsing.
[05.06.2025 00:55] Paper image links file exists (./assets/img_data/2506.03096.json), skip HTML parsing.
[05.06.2025 00:55] Success.
[05.06.2025 00:55] Downloading and parsing paper https://huggingface.co/papers/2506.03079.
[05.06.2025 00:55] Extra JSON file exists (./assets/json/2506.03079.json), skip PDF parsing.
[05.06.2025 00:55] Paper image links file exists (./assets/img_data/2506.03079.json), skip HTML parsing.
[05.06.2025 00:55] Success.
[05.06.2025 00:55] Downloading and parsing paper https://huggingface.co/papers/2506.02338.
[05.06.2025 00:55] Extra JSON file exists (./assets/json/2506.02338.json), skip PDF parsing.
[05.06.2025 00:55] Paper image links file exists (./assets/img_data/2506.02338.json), skip HTML parsing.
[05.06.2025 00:55] Success.
[05.06.2025 00:55] Downloading and parsing paper https://huggingface.co/papers/2506.01004.
[05.06.2025 00:55] Extra JSON file exists (./assets/json/2506.01004.json), skip PDF parsing.
[05.06.2025 00:55] Paper image links file exists (./assets/img_data/2506.01004.json), skip HTML parsing.
[05.06.2025 00:55] Success.
[05.06.2025 00:55] Downloading and parsing paper https://huggingface.co/papers/2506.00391.
[05.06.2025 00:55] Extra JSON file exists (./assets/json/2506.00391.json), skip PDF parsing.
[05.06.2025 00:55] Paper image links file exists (./assets/img_data/2506.00391.json), skip HTML parsing.
[05.06.2025 00:55] Success.
[05.06.2025 00:55] Downloading and parsing paper https://huggingface.co/papers/2506.00227.
[05.06.2025 00:55] Extra JSON file exists (./assets/json/2506.00227.json), skip PDF parsing.
[05.06.2025 00:55] Paper image links file exists (./assets/img_data/2506.00227.json), skip HTML parsing.
[05.06.2025 00:55] Success.
[05.06.2025 00:55] Downloading and parsing paper https://huggingface.co/papers/2505.24273.
[05.06.2025 00:55] Extra JSON file exists (./assets/json/2505.24273.json), skip PDF parsing.
[05.06.2025 00:55] Paper image links file exists (./assets/img_data/2505.24273.json), skip HTML parsing.
[05.06.2025 00:55] Success.
[05.06.2025 00:55] Downloading and parsing paper https://huggingface.co/papers/2505.18079.
[05.06.2025 00:55] Extra JSON file exists (./assets/json/2505.18079.json), skip PDF parsing.
[05.06.2025 00:55] Paper image links file exists (./assets/img_data/2505.18079.json), skip HTML parsing.
[05.06.2025 00:55] Success.
[05.06.2025 00:55] Downloading and parsing paper https://huggingface.co/papers/2506.03119.
[05.06.2025 00:55] Extra JSON file exists (./assets/json/2506.03119.json), skip PDF parsing.
[05.06.2025 00:55] Paper image links file exists (./assets/img_data/2506.03119.json), skip HTML parsing.
[05.06.2025 00:55] Success.
[05.06.2025 00:55] Downloading and parsing paper https://huggingface.co/papers/2506.02678.
[05.06.2025 00:55] Extra JSON file exists (./assets/json/2506.02678.json), skip PDF parsing.
[05.06.2025 00:55] Paper image links file exists (./assets/img_data/2506.02678.json), skip HTML parsing.
[05.06.2025 00:55] Success.
[05.06.2025 00:55] Downloading and parsing paper https://huggingface.co/papers/2506.02510.
[05.06.2025 00:55] Extra JSON file exists (./assets/json/2506.02510.json), skip PDF parsing.
[05.06.2025 00:55] Paper image links file exists (./assets/img_data/2506.02510.json), skip HTML parsing.
[05.06.2025 00:55] Success.
[05.06.2025 00:55] Downloading and parsing paper https://huggingface.co/papers/2506.02454.
[05.06.2025 00:55] Extra JSON file exists (./assets/json/2506.02454.json), skip PDF parsing.
[05.06.2025 00:55] Paper image links file exists (./assets/img_data/2506.02454.json), skip HTML parsing.
[05.06.2025 00:55] Success.
[05.06.2025 00:55] Downloading and parsing paper https://huggingface.co/papers/2506.02295.
[05.06.2025 00:55] Extra JSON file exists (./assets/json/2506.02295.json), skip PDF parsing.
[05.06.2025 00:55] Paper image links file exists (./assets/img_data/2506.02295.json), skip HTML parsing.
[05.06.2025 00:55] Success.
[05.06.2025 00:55] Downloading and parsing paper https://huggingface.co/papers/2506.01565.
[05.06.2025 00:55] Extra JSON file exists (./assets/json/2506.01565.json), skip PDF parsing.
[05.06.2025 00:55] Paper image links file exists (./assets/img_data/2506.01565.json), skip HTML parsing.
[05.06.2025 00:55] Success.
[05.06.2025 00:55] Downloading and parsing paper https://huggingface.co/papers/2505.16994.
[05.06.2025 00:55] Extra JSON file exists (./assets/json/2505.16994.json), skip PDF parsing.
[05.06.2025 00:55] Paper image links file exists (./assets/img_data/2505.16994.json), skip HTML parsing.
[05.06.2025 00:55] Success.
[05.06.2025 00:55] Downloading and parsing paper https://huggingface.co/papers/2506.03144.
[05.06.2025 00:55] Extra JSON file exists (./assets/json/2506.03144.json), skip PDF parsing.
[05.06.2025 00:55] Paper image links file exists (./assets/img_data/2506.03144.json), skip HTML parsing.
[05.06.2025 00:55] Success.
[05.06.2025 00:55] Downloading and parsing paper https://huggingface.co/papers/2506.02281.
[05.06.2025 00:55] Extra JSON file exists (./assets/json/2506.02281.json), skip PDF parsing.
[05.06.2025 00:55] Paper image links file exists (./assets/img_data/2506.02281.json), skip HTML parsing.
[05.06.2025 00:55] Success.
[05.06.2025 00:55] Downloading and parsing paper https://huggingface.co/papers/2506.02138.
[05.06.2025 00:55] Extra JSON file exists (./assets/json/2506.02138.json), skip PDF parsing.
[05.06.2025 00:55] Paper image links file exists (./assets/img_data/2506.02138.json), skip HTML parsing.
[05.06.2025 00:55] Success.
[05.06.2025 00:55] Downloading and parsing paper https://huggingface.co/papers/2506.01265.
[05.06.2025 00:55] Extra JSON file exists (./assets/json/2506.01265.json), skip PDF parsing.
[05.06.2025 00:55] Paper image links file exists (./assets/img_data/2506.01265.json), skip HTML parsing.
[05.06.2025 00:55] Success.
[05.06.2025 00:55] Downloading and parsing paper https://huggingface.co/papers/2505.24362.
[05.06.2025 00:55] Extra JSON file exists (./assets/json/2505.24362.json), skip PDF parsing.
[05.06.2025 00:55] Paper image links file exists (./assets/img_data/2505.24362.json), skip HTML parsing.
[05.06.2025 00:55] Success.
[05.06.2025 00:55] Enriching papers with extra data.
[05.06.2025 00:55] ********************************************************************************
[05.06.2025 00:55] Abstract 0. A method using self-reflection and reinforcement learning improves the performance of large language models, especially with limited feedback, by rewarding self-reflections that lead to better task performance.  					AI-generated summary 				 We explore a method for improving the performance of larg...
[05.06.2025 00:55] ********************************************************************************
[05.06.2025 00:55] Abstract 1. VS-Bench is a multimodal benchmark designed to evaluate Vision Language Models' strategic reasoning and decision-making in complex multi-agent environments.  					AI-generated summary 				 Recent advancements in Vision Language Models (VLMs) have expanded their capabilities to interactive agent task...
[05.06.2025 00:55] ********************************************************************************
[05.06.2025 00:55] Abstract 2. A unified generative framework called UniWorld uses semantic features from visual-language models for image perception and manipulation, outperforming BAGEL with reduced data.  					AI-generated summary 				 Although existing unified models deliver strong performance on vision-language understanding...
[05.06.2025 00:55] ********************************************************************************
[05.06.2025 00:55] Abstract 3. SynthRL, a scalable pipeline for data synthesis in reinforcement learning with verifiable rewards, enhances visual math reasoning VLMs by generating challenging, verifiable questions.  					AI-generated summary 				 Vision-language models (VLMs) trained via reinforcement learning with verifiable rew...
[05.06.2025 00:55] ********************************************************************************
[05.06.2025 00:55] Abstract 4. A new benchmark, CSVQA, evaluates scientific reasoning in vision-language models through domain-specific visual question answering, highlighting the need for improvement in these models.  					AI-generated summary 				 Vision-Language Models (VLMs) have demonstrated remarkable progress in multimodal...
[05.06.2025 00:55] ********************************************************************************
[05.06.2025 00:55] Abstract 5. FinMME is a comprehensive multimodal dataset for financial research and FinScore is an evaluation system that highlights the challenges faced by even advanced models like GPT-4o in the finance domain.  					AI-generated summary 				 Multimodal Large Language Models (MLLMs) have experienced rapid dev...
[05.06.2025 00:55] ********************************************************************************
[05.06.2025 00:55] Abstract 6. OThink-R1 is introduced to reduce reasoning redundancy in complex problem-solving by classifying reasoning steps as essential or redundant and dynamically switching thinking modes based on task complexity.  					AI-generated summary 				 Recent advanced large reasoning models (LRMs) leverage extende...
[05.06.2025 00:55] ********************************************************************************
[05.06.2025 00:55] Abstract 7. VeBrain is a unified framework that integrates multimodal understanding, visual-spatial reasoning, and physical interaction for legged robots, demonstrating superior performance compared to existing methods across various benchmarks.  					AI-generated summary 				 The remarkable progress of Multimo...
[05.06.2025 00:55] ********************************************************************************
[05.06.2025 00:55] Abstract 8. A comprehensive benchmark called OmniSpatial evaluates vision-language models' understanding of advanced spatial reasoning tasks, revealing significant limitations across various models.  					AI-generated summary 				 Spatial reasoning is a key aspect of cognitive psychology and remains a major bot...
[05.06.2025 00:55] ********************************************************************************
[05.06.2025 00:55] Abstract 9. GUI-Actor, a VLM-based method using attention for coordinate-free GUI grounding, outperforms existing methods with better generalization and efficient fine-tuning.  					AI-generated summary 				 One of the principal challenges in building VLM-powered GUI agents is visual grounding, i.e., localizing...
[05.06.2025 00:55] ********************************************************************************
[05.06.2025 00:55] Abstract 10. DINGO, a dynamic programming-based decoding strategy, enhances diffusion language models by enforcing structured output constraints, significantly improving performance on symbolic math and JSON generation tasks.  					AI-generated summary 				 Diffusion LLMs have emerged as a promising alternative ...
[05.06.2025 00:55] ********************************************************************************
[05.06.2025 00:55] Abstract 11. Sparse-vDiT accelerates Video Diffusion Transformer (vDiT) by leveraging sparsity patterns in attention maps, reducing theoretical FLOPs and improving inference speed without significant loss in visual quality.  					AI-generated summary 				 While Diffusion Transformers (DiTs) have achieved breakth...
[05.06.2025 00:55] ********************************************************************************
[05.06.2025 00:55] Abstract 12. MotionSight, a zero-shot method using object-centric visual spotlight and motion blur as prompts, enhances fine-grained video motion understanding and achieves state-of-the-art performance on MotionVid-QA, a large-scale dataset with hierarchical annotations.  					AI-generated summary 				 Despite a...
[05.06.2025 00:55] ********************************************************************************
[05.06.2025 00:55] Abstract 13. Large Vision-Language Models (LVLMs) have recently shown great promise in advancing robotics by combining embodied reasoning with robot control. A common approach involves training on embodied reasoning tasks related to robot control using Supervised Fine-Tuning (SFT). However, SFT datasets are ofte...
[05.06.2025 00:55] ********************************************************************************
[05.06.2025 00:55] Abstract 14. CURE, a reinforcement learning framework, improves code and unit test generation accuracy without ground-truth supervision, enhancing performance in various coding and testing tasks.  					AI-generated summary 				 We propose CURE, a novel reinforcement learning framework with a dedicated reward des...
[05.06.2025 00:55] ********************************************************************************
[05.06.2025 00:55] Abstract 15. A novel generative model, Native-resolution diffusion Transformer (NiT), synthesizes high-resolution and varied aspect ratio images with state-of-the-art performance and zero-shot generalization capabilities.  					AI-generated summary 				 We introduce native-resolution image synthesis, a novel gen...
[05.06.2025 00:55] ********************************************************************************
[05.06.2025 00:55] Abstract 16. AnimeShooter, a reference-guided multi-shot animation dataset, enhances coherent animated video generation by incorporating comprehensive hierarchical annotations and visual consistency, and AnimeShooterGen leverages MLLMs and video diffusion models to achieve superior results.  					AI-generated su...
[05.06.2025 00:55] ********************************************************************************
[05.06.2025 00:55] Abstract 17. LumosFlow uses LMTV-DM for key frame generation and LOF-DM followed by MotionControlNet for smooth intermediate frame interpolation, ensuring temporally coherent long video generation.  					AI-generated summary 				 Long video generation has gained increasing attention due to its widespread applica...
[05.06.2025 00:55] ********************************************************************************
[05.06.2025 00:55] Abstract 18. RelationAdapter, a lightweight module, enhances Diffusion Transformer models to capture and apply visual transformations effectively using source-target image pairs, improving editing performance and generalization on diverse tasks.  					AI-generated summary 				 Inspired by the in-context learning...
[05.06.2025 00:55] ********************************************************************************
[05.06.2025 00:55] Abstract 19. FlowMo, a training-free method, enhances motion coherence in pre-trained text-to-video diffusion models by leveraging their own predictions to reduce patch-wise temporal variance.  					AI-generated summary 				 Text-to-video diffusion models are notoriously limited in their ability to model tempora...
[05.06.2025 00:55] ********************************************************************************
[05.06.2025 00:55] Abstract 20. ActiveKD integrates active learning with knowledge distillation using large vision-language models to efficiently select diverse, unlabeled samples for annotation.  					AI-generated summary 				 Knowledge distillation (KD) is a widely used framework for training compact, task-specific models by lev...
[05.06.2025 00:55] ********************************************************************************
[05.06.2025 00:55] Abstract 21. Diffusion Models have achieved remarkable results in video synthesis but require iterative denoising steps, leading to substantial computational overhead. Consistency Models have made significant progress in accelerating diffusion models. However, directly applying them to video diffusion models oft...
[05.06.2025 00:55] ********************************************************************************
[05.06.2025 00:55] Abstract 22. High-quality datasets are fundamental to training and evaluating machine learning models, yet their creation-especially with accurate human annotations-remains a significant challenge. Many dataset paper submissions lack originality, diversity, or rigorous quality control, and these shortcomings are...
[05.06.2025 00:55] ********************************************************************************
[05.06.2025 00:55] Abstract 23. A reinforcement learning framework improves code quality in large language models by using automated feedback from program analysis and unit tests.  					AI-generated summary 				 Code generation with large language models (LLMs), often termed vibe coding, is increasingly adopted in production but f...
[05.06.2025 00:55] ********************************************************************************
[05.06.2025 00:55] Abstract 24. The Self-Challenging framework trains intelligent agents using self-generated tasks defined as Code-as-Task, improving performance in multi-turn tool-use benchmarks.  					AI-generated summary 				 Large language models are quickly becoming the foundation for intelligent agents that are capable of u...
[05.06.2025 00:55] ********************************************************************************
[05.06.2025 00:55] Abstract 25. Adaptive parallel decoding (APD) enhances the throughput of diffusion large language models (dLLMs) by dynamically adjusting parallel token generation without significantly diminishing quality.  					AI-generated summary 				 The generation speed of LLMs are bottlenecked by autoregressive decoding, ...
[05.06.2025 00:55] ********************************************************************************
[05.06.2025 00:55] Abstract 26. ReFoCUS uses reinforcement learning to optimize frame selection for video-LLM, enhancing reasoning performance in video QA by aligning with model preferences.  					AI-generated summary 				 Recent progress in Large Multi-modal Models (LMMs) has enabled effective vision-language reasoning, yet the a...
[05.06.2025 00:55] ********************************************************************************
[05.06.2025 00:55] Abstract 27. Contrastive language-image pre-training aligns the features of text-image pairs in a common latent space via distinct encoders for each modality. While this approach achieves impressive performance in several zero-shot tasks, it cannot natively handle multimodal inputs, i.e., encoding image and text...
[05.06.2025 00:55] ********************************************************************************
[05.06.2025 00:55] Abstract 28. ORV, an Occupancy-centric Robot Video generation framework, uses 4D semantic occupancy sequences to produce photorealistic, temporally consistent, and precisely controllable robot videos, enhancing existing methods.  					AI-generated summary 				 Acquiring real-world robotic simulation data through...
[05.06.2025 00:55] ********************************************************************************
[05.06.2025 00:55] Abstract 29. The Long CoT Collection dataset, generated by short CoT LLMs, enhances general reasoning skills and provides a strong foundation for reinforcement learning, achieving quality comparable to R1.  					AI-generated summary 				 With the release of R1, a publicly available large reasoning model (LRM), r...
[05.06.2025 00:55] ********************************************************************************
[05.06.2025 00:55] Abstract 30. MoCA-Video injects semantic features from a reference image into a video object, preserving motion and visual context, and outperforms baselines using novel metrics.  					AI-generated summary 				 We introduce MoCA-Video (Motion-Aware Concept Alignment in Video), a training-free framework bridging ...
[05.06.2025 00:55] ********************************************************************************
[05.06.2025 00:55] Abstract 31. SHARE, an SLM-based Hierarchical Action RECorrection system, enhances LLMs in text-to-SQL by transforming SQL queries into action trajectories and employing a granular refinement process, improving error detection and correction efficiency.  					AI-generated summary 				 Current self-correction app...
[05.06.2025 00:55] ********************************************************************************
[05.06.2025 00:55] Abstract 32. Ctrl-Crash, a controllable car crash video generation model using classifier-free guidance, achieves top performance in video quality and realism compared to existing diffusion-based methods.  					AI-generated summary 				 Video diffusion techniques have advanced significantly in recent years; howe...
[05.06.2025 00:55] ********************************************************************************
[05.06.2025 00:55] Abstract 33. This study investigates the interplay between supervised fine-tuning and reinforcement learning in large language models, focusing on the role of backtracking in enhancing reasoning capabilities across various tasks.  					AI-generated summary 				 Recent breakthroughs in large language models (LLMs...
[05.06.2025 00:55] ********************************************************************************
[05.06.2025 00:55] Abstract 34. The Deep Video Discovery agent uses an autonomous agentic search strategy with large language models to overcome limitations in long-form video understanding, achieving state-of-the-art results on benchmarks like LVBench.  					AI-generated summary 				 Long-form video understanding presents signifi...
[05.06.2025 00:55] ********************************************************************************
[05.06.2025 00:55] Abstract 35. Existing interpolation methods use pre-trained video diffusion priors to generate intermediate frames between sparsely sampled keyframes. In the absence of 3D geometric guidance, these methods struggle to produce plausible results for complex, articulated human motions and offer limited control over...
[05.06.2025 00:55] ********************************************************************************
[05.06.2025 00:55] Abstract 36. Large Language Models (LLMs) have recently achieved remarkable progress by leveraging Reinforcement Learning and extended Chain-of-Thought (CoT) techniques. However, the challenge of performing efficient language reasoning--especially during inference with extremely long outputs--has drawn increasin...
[05.06.2025 00:55] ********************************************************************************
[05.06.2025 00:55] Abstract 37. A new multilingual, multi-sector, and multi-task benchmark, M¬≥FinMeeting, evaluates large language models' performance in understanding financial meetings across different languages and industries.  					AI-generated summary 				 Recent breakthroughs in large language models (LLMs) have led to the d...
[05.06.2025 00:55] ********************************************************************************
[05.06.2025 00:55] Abstract 38. A new framework, Multimodal DeepResearcher, enables Large Language Models to generate high-quality multimodal reports combining text and diverse visualizations through structured textual representations.  					AI-generated summary 				 Visualizations play a crucial part in effective communication of...
[05.06.2025 00:55] ********************************************************************************
[05.06.2025 00:55] Abstract 39. Qari-OCR, a series of fine-tuned vision-language models, achieves state-of-the-art performance in Arabic OCR through iterative optimization on specialized datasets, handling diacritics, fonts, layouts, and low-resolution images.  					AI-generated summary 				 The inherent complexities of Arabic scr...
[05.06.2025 00:55] ********************************************************************************
[05.06.2025 00:55] Abstract 40. Culture is a rich and dynamic domain that evolves across both geography and time. However, existing studies on cultural understanding with vision-language models (VLMs) primarily emphasize geographic diversity, often overlooking the critical temporal dimensions. To bridge this gap, we introduce Hanf...
[05.06.2025 00:55] ********************************************************************************
[05.06.2025 00:55] Abstract 41. A unified large recommender model with intrinsic reasoning capabilities is proposed, facilitating interleaved reasoning and recommendation using a reinforcement learning framework called RecPO.  					AI-generated summary 				 Large recommender models have extended LLMs as powerful recommenders via e...
[05.06.2025 00:55] ********************************************************************************
[05.06.2025 00:55] Abstract 42. Semantic retrieval is crucial for modern applications yet remains underexplored in current research. Existing datasets are limited to single languages, single images, or singular retrieval conditions, often failing to fully exploit the expressive capacity of visual information as evidenced by mainta...
[05.06.2025 00:55] ********************************************************************************
[05.06.2025 00:55] Abstract 43. GAIN-RL leverages angle concentration signals to improve training efficiency and data efficiency in Reinforcement Fine-tuning of Large Language Models.  					AI-generated summary 				 Current Reinforcement Fine-tuning (RFT) paradigms for Large Language Models (LLMs) suffer from sample inefficiency d...
[05.06.2025 00:55] ********************************************************************************
[05.06.2025 00:55] Abstract 44. A specialized LRP method for Transformer explainability considers positional encoding, improving relevance propagation and outperforming existing methods.  					AI-generated summary 				 The development of effective explainability tools for Transformers is a crucial pursuit in deep learning research...
[05.06.2025 00:55] ********************************************************************************
[05.06.2025 00:55] Abstract 45. In-context learning (ICL) is an important yet not fully understood ability of pre-trained large language models (LLMs). It can greatly enhance task performance using a few examples, termed demonstrations, without fine-tuning. Although effective in question answering, ICL often underperforms in long-...
[05.06.2025 00:55] ********************************************************************************
[05.06.2025 00:55] Abstract 46. We investigate whether the success of a zero-shot Chain-of-Thought (CoT) process can be predicted before completion. We discover that a probing classifier, based on LLM representations, performs well even before a single token is generated, suggesting that crucial information about the reasoning pro...
[05.06.2025 00:55] Read previous papers.
[05.06.2025 00:55] Generating reviews via LLM API.
[05.06.2025 00:55] Using data from previous issue: {"categories": ["#optimization", "#training", "#rlhf", "#small_models", "#reasoning", "#rl"], "emoji": "üß†", "ru": {"title": "–°–∞–º–æ–∞–Ω–∞–ª–∏–∑ –∏ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –ø–æ–≤—ã—à–∞—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –º–µ—Ç–æ–¥ —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞–±–æ—Ç—ã –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –ø–æ–º–æ—â—å—é —Å–∞–º
[05.06.2025 00:55] Using data from previous issue: {"categories": ["#reasoning", "#multimodal", "#agents", "#benchmark", "#games"], "emoji": "ü§ñ", "ru": {"title": "VS-Bench: –ù–æ–≤—ã–π —Ä—É–±–µ–∂ –≤ –æ—Ü–µ–Ω–∫–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤", "desc": "VS-Bench - —ç—Ç–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è –∏ –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π
[05.06.2025 00:55] Using data from previous issue: {"categories": ["#dataset", "#cv", "#multimodal", "#open_source", "#benchmark"], "emoji": "üé®", "ru": {"title": "UniWorld: –ú–æ—â–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤", "desc": "UniWorld - —ç—Ç–æ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è –∏ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω
[05.06.2025 00:55] Using data from previous issue: {"categories": ["#reasoning", "#dataset", "#training", "#synthetic", "#rl"], "emoji": "üß†", "ru": {"title": "SynthRL: –°–∏–Ω—Ç–µ–∑ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –ò–ò", "desc": "SynthRL - —ç—Ç–æ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—ã–π –∫–æ–Ω–≤–µ–π–µ—Ä –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ –¥–∞–Ω–Ω—ã—Ö –≤ –æ–±—É—á–µ–Ω–∏–∏ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º —Å –ø—Ä–æ–≤–µ—Ä—è–µ–º—ã–º–∏ –Ω–∞–≥—Ä–∞–¥–∞–º–∏. –û–Ω —É
[05.06.2025 00:55] Using data from previous issue: {"categories": ["#science", "#multimodal", "#benchmark", "#reasoning"], "emoji": "üî¨", "ru": {"title": "CSVQA: –Ω–æ–≤—ã–π —Ä—É–±–µ–∂ –≤ –æ—Ü–µ–Ω–∫–µ –Ω–∞—É—á–Ω–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è –ò–ò", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ CSVQA –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –Ω–∞—É—á–Ω–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö —á–µ—Ä–µ–∑ –∑–∞–¥–∞—á–∏ –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –ø–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º –≤
[05.06.2025 00:55] Using data from previous issue: {"categories": ["#dataset", "#hallucinations", "#science", "#multimodal", "#benchmark"], "emoji": "üìä", "ru": {"title": "FinMME –∏ FinScore: –ù–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–π —Å—Ñ–µ—Ä–µ", "desc": "FinMME - —ç—Ç–æ –æ–±—à–∏—Ä–Ω—ã–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏
[05.06.2025 00:55] Using data from previous issue: {"categories": ["#training", "#math", "#reasoning"], "emoji": "üß†", "ru": {"title": "–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –ò–ò: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ —Ç–æ—á–Ω–æ—Å—Ç–∏", "desc": "OThink-R1 - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π –Ω–∞ —É–º–µ–Ω—å—à–µ–Ω–∏–µ –∏–∑–±—ã—Ç–æ—á–Ω–æ—Å—Ç–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –ø—Ä–∏ —Ä–µ—à–µ–Ω–∏–∏ —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á. –û–Ω –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ
[05.06.2025 00:55] Using data from previous issue: {"categories": ["#games", "#robotics", "#multimodal", "#reasoning", "#benchmark", "#dataset"], "emoji": "ü§ñ", "ru": {"title": "VeBrain: –ï–¥–∏–Ω—ã–π –º–æ–∑–≥ –¥–ª—è –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è, –º—ã—à–ª–µ–Ω–∏—è –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ä–æ–±–æ—Ç–∞–º–∏", "desc": "VeBrain - —ç—Ç–æ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Ä–æ–±–æ—Ç–æ–≤ —Å –Ω–æ–≥–∞–º–∏, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–µ –ø–æ–Ω–∏–º–∞
[05.06.2025 00:55] Using data from previous issue: {"categories": ["#cv", "#benchmark", "#reasoning"], "emoji": "üß†", "ru": {"title": "OmniSpatial: –Ω–æ–≤—ã–π —Ä—É–±–µ–∂ –≤ –æ—Ü–µ–Ω–∫–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è –ò–ò", "desc": "OmniSpatial - —ç—Ç–æ –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π —Ç–µ—Å—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á –º–æ–¥–µ–ª—è–º–∏ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞. 
[05.06.2025 00:55] Using data from previous issue: {"categories": ["#multimodal", "#training", "#cv", "#benchmark", "#optimization", "#games"], "emoji": "üñ±Ô∏è", "ru": {"title": "GUI-Actor: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞ —Å –ø–æ–º–æ—â—å—é VLM", "desc": "GUI-Actor - —ç—Ç–æ –º–µ—Ç–æ–¥ –Ω–∞ –æ—Å–Ω–æ–≤–µ VLM, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –º–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è –±–µ–∑–∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–Ω–æ–π –ª–æ–∫–∞–ª–∏
[05.06.2025 00:55] Using data from previous issue: {"categories": ["#training", "#architecture", "#benchmark", "#optimization", "#diffusion"], "emoji": "üßÆ", "ru": {"title": "DINGO: –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç DINGO - –Ω–æ–≤—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. 
[05.06.2025 00:55] Using data from previous issue: {"categories": ["#optimization", "#architecture", "#video", "#inference", "#diffusion"], "emoji": "üéûÔ∏è", "ru": {"title": "–£—Å–∫–æ—Ä–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –º–µ—Ç–æ–¥ Sparse-vDiT –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è Video Diffusion Transformer (vDiT), –∏—Å–ø–æ–ª—å–∑—É—è —Ä–∞–∑
[05.06.2025 00:55] Using data from previous issue: {"categories": ["#dataset", "#cv", "#multimodal", "#open_source", "#games"], "emoji": "üé•", "ru": {"title": "–ù–æ–≤—ã–π –≤–∑–≥–ª—è–¥ –Ω–∞ –¥–≤–∏–∂–µ–Ω–∏–µ: MotionSight —É–ª—É—á—à–∞–µ—Ç –ø–æ–Ω–∏–º–∞–Ω–∏–µ –≤–∏–¥–µ–æ –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è", "desc": "MotionSight - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –±–µ–∑ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –¥–≤–∏–∂–µ–Ω–∏—è –≤ –≤–∏–¥–µ–æ –º—É–ª—å
[05.06.2025 00:55] Using data from previous issue: {"categories": ["#optimization", "#robotics", "#training", "#rl", "#reasoning"], "emoji": "ü§ñ", "ru": {"title": "Robot-R1: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –æ–±—É—á–µ–Ω–∏–∏ —Ä–æ–±–æ—Ç–æ–≤ —á–µ—Ä–µ–∑ –≤–æ–ø–ª–æ—â–µ–Ω–Ω—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ Robot-R1 –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≤–æ–ø–ª–æ—â–µ–Ω–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–µ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞
[05.06.2025 00:55] Using data from previous issue: {"categories": ["#training", "#optimization", "#agents", "#rl", "#games"], "emoji": "üß†", "ru": {"title": "CURE: —ç–≤–æ–ª—é—Ü–∏—è –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –±–µ–∑ —ç—Ç–∞–ª–æ–Ω–æ–≤", "desc": "CURE - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞ –∏ –º–æ–¥—É–ª—å–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤ –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —ç—Ç–∞–ª–æ–Ω–Ω–æ–≥–æ –∫
[05.06.2025 00:55] Using data from previous issue: {"categories": ["#architecture", "#diffusion", "#cv", "#synthetic", "#benchmark"], "emoji": "üñºÔ∏è", "ru": {"title": "NiT: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –±–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å NiT (Native-resolution diffusion Transformer), —Å–ø–æ—Å–æ–±–Ω–∞—è —Å–∏–Ω—Ç–µ–∑–∏—Ä–æ–≤–∞—Ç
[05.06.2025 00:55] Using data from previous issue: {"categories": ["#video", "#multimodal", "#story_generation", "#diffusion", "#dataset"], "emoji": "üé¨", "ru": {"title": "AnimeShooter: –Ω–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–≤—è–∑–Ω–æ–π –∞–Ω–∏–º–∞—Ü–∏–∏ —Å –æ–ø–æ—Ä–Ω—ã–º–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏", "desc": "AnimeShooter - —ç—Ç–æ –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –º–Ω–æ–≥–æ–∫–∞–¥—Ä–æ–≤–æ–π –∞–Ω–∏–º–∞—Ü–∏–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –æ–ø–æ—Ä
[05.06.2025 00:55] Using data from previous issue: {"categories": ["#open_source", "#video", "#diffusion"], "emoji": "üé¨", "ru": {"title": "LumosFlow: –ø–ª–∞–≤–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–π –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏–∏ –∫–∞–¥—Ä–æ–≤", "desc": "LumosFlow - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞. –°–∏—Å—Ç–µ–º–∞ –ø—Ä–∏–º–µ–Ω—è–µ
[05.06.2025 00:55] Using data from previous issue: {"categories": ["#cv", "#dataset", "#transfer_learning", "#diffusion"], "emoji": "üñºÔ∏è", "ru": {"title": "RelationAdapter: –£–º–Ω–æ–µ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ –ø–∞—Ä–µ –ø—Ä–∏–º–µ—Ä–æ–≤", "desc": "RelationAdapter - —ç—Ç–æ –ª–µ–≥–∫–æ–≤–µ—Å–Ω—ã–π –º–æ–¥—É–ª—å, —É–ª—É—á—à–∞—é—â–∏–π —Ä–∞–±–æ—Ç—É –º–æ–¥–µ–ª–µ–π Diffusion Transformer –¥–ª—è –∑–∞—Ö–≤–∞—Ç–∞ –∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –≤–∏–∑
[05.06.2025 00:55] Using data from previous issue: {"categories": ["#video", "#training", "#diffusion"], "emoji": "üé¨", "ru": {"title": "–£–ª—É—á—à–µ–Ω–∏–µ –¥–≤–∏–∂–µ–Ω–∏—è –≤ –≤–∏–¥–µ–æ –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏", "desc": "FlowMo - —ç—Ç–æ –º–µ—Ç–æ–¥ –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –¥–≤–∏–∂–µ–Ω–∏—è –≤ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –ø–æ —Ç–µ
[05.06.2025 00:55] Using data from previous issue: {"categories": ["#data", "#optimization", "#training", "#dataset", "#transfer_learning"], "emoji": "üß†", "ru": {"title": "ActiveKD: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ —á–µ—Ä–µ–∑ —Å–∏–Ω–µ—Ä–≥–∏—é –∞–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –∑–Ω–∞–Ω–∏–π", "desc": "ActiveKD - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∏–π –∞–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏
[05.06.2025 00:55] Using data from previous issue: {"categories": ["#training", "#video", "#multimodal", "#diffusion", "#optimization", "#architecture"], "emoji": "üé¨", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π —Å–∏–Ω—Ç–µ–∑ –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é –¥–≤—É—Ö —ç–∫—Å–ø–µ—Ä—Ç–æ–≤", "desc": "–ê–≤—Ç–æ—Ä—ã —Å—Ç–∞—Ç—å–∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É—Å–∫–æ—Ä–µ–Ω–∏—é –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ –≤–∏–¥–µ–æ. –û–Ω–∏ –≤—ã—è–≤–∏–ª–∏ –ø—Ä–æ
[05.06.2025 00:55] Using data from previous issue: {"categories": ["#open_source", "#data", "#synthetic", "#dataset"], "emoji": "üìä", "ru": {"title": "DataRubrics: –ù–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –≤ —ç–ø–æ—Ö—É –ò–ò", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ—Ü–µ–Ω–∫–µ –∫–∞—á–µ—Å—Ç–≤–∞ –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã –≤–≤–æ–¥—è—Ç –∫–æ–Ω—Ü–µ–ø—Ü–∏—é DataRubrics - —Å—Ç
[05.06.2025 00:55] Using data from previous issue: {"categories": ["#security", "#rl", "#dataset", "#training", "#optimization"], "emoji": "üîß", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –∫–æ–¥–∞ –æ—Ç LLM", "desc": "–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ REAL, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –∫–æ–¥–∞, –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º–æ–≥–æ –±–æ–ª—å—à–∏–º–∏ —è–∑—ã–∫
[05.06.2025 00:55] Using data from previous issue: {"categories": ["#optimization", "#agi", "#training", "#agents", "#rl"], "emoji": "ü§ñ", "ru": {"title": "–°–∞–º–æ–æ–±—É—á–µ–Ω–∏–µ –ò–ò: –∞–≥–µ–Ω—Ç –±—Ä–æ—Å–∞–µ—Ç –≤—ã–∑–æ–≤ —Å–∞–º–æ–º—É —Å–µ–±–µ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤, –Ω–∞–∑—ã–≤–∞–µ–º—ã–π Self-Challenging. –í —ç—Ç–æ–π –º–æ–¥–µ–ª–∏ –∞–≥–µ–Ω—Ç —Å–∞–º –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∑–∞
[05.06.2025 00:55] Using data from previous issue: {"categories": ["#optimization", "#training", "#benchmark", "#diffusion", "#architecture"], "emoji": "üöÄ", "ru": {"title": "–£—Å–∫–æ—Ä–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è (APD) –¥–ª—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π
[05.06.2025 00:55] Using data from previous issue: {"categories": ["#reasoning", "#multimodal", "#video", "#optimization", "#rl", "#benchmark"], "emoji": "üéûÔ∏è", "ru": {"title": "–£–º–Ω—ã–π –≤—ã–±–æ—Ä –∫–∞–¥—Ä–æ–≤ –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –≤–∏–¥–µ–æ –ò–ò", "desc": "ReFoCUS - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≤—ã–±–æ—Ä–∞ –∫–∞–¥—Ä–æ–≤ –¥–ª—è –≤–∏–¥–µ–æ-LLM —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º. –û
[05.06.2025 00:55] Using data from previous issue: {"categories": ["#architecture", "#dataset", "#games", "#alignment", "#multimodal"], "emoji": "üîÄ", "ru": {"title": "–†–∞–Ω–Ω—è—è —Ñ—å—é–∂–Ω –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –ò–ò", "desc": "FuseLIP - —ç—Ç–æ –Ω–æ–≤–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –≤—Å—Ç—Ä–∞–∏–≤–∞–Ω–∏—è, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è –µ–¥–∏–Ω—É—é —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∏ –∏–∑
[05.06.2025 00:55] Using data from previous issue: {"categories": ["#games", "#robotics", "#optimization", "#video"], "emoji": "ü§ñ", "ru": {"title": "ORV: –¢–æ—á–Ω—ã–π –∫–æ–Ω—Ç—Ä–æ–ª—å —Ä–æ–±–æ—Ç–æ–≤ —á–µ—Ä–µ–∑ 4D —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é –∑–∞–Ω—è—Ç–æ—Å—Ç—å", "desc": "ORV - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ —Å —Ä–æ–±–æ—Ç–∞–º–∏, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ 4D —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –∑–∞–Ω—è—Ç–æ—Å—Ç–∏. –û–Ω –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–æ–∑–¥
[05.06.2025 00:55] Using data from previous issue: {"categories": ["#rl", "#transfer_learning", "#dataset", "#reasoning"], "emoji": "üß†", "ru": {"title": "–î–ª–∏–Ω–Ω—ã–µ —Ü–µ–ø–æ—á–∫–∏ –º—ã—à–ª–µ–Ω–∏—è –¥–ª—è –ò–ò: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –º–æ–¥–µ–ª–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö Long CoT Collection, —Å–æ–∑–¥–∞–Ω–Ω—ã–π —Å –ø–æ–º–æ—â—å—é –∫–æ—Ä–æ—Ç–∫–∏—Ö –º–æ–¥–µ–ª–µ–π —Ü–µ–ø–æ—á–µ–∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω
[05.06.2025 00:55] Using data from previous issue: {"categories": ["#dataset", "#video", "#benchmark", "#diffusion"], "emoji": "üé¨", "ru": {"title": "–£–º–Ω–æ–µ —Å–º–µ—à–∏–≤–∞–Ω–∏–µ –≤–∏–¥–µ–æ –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è", "desc": "MoCA-Video - —ç—Ç–æ –±–µ–∑—Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –≤–Ω–µ–¥—Ä–µ–Ω–∏—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–µ–π –∏–∑ —ç—Ç–∞–ª–æ–Ω–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ –≤–∏–¥–µ–æ–æ–±—ä–µ–∫—Ç. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥
[05.06.2025 00:55] Using data from previous issue: {"categories": ["#low_resource", "#training", "#optimization", "#dataset", "#reasoning", "#small_models", "#data"], "emoji": "üîç", "ru": {"title": "–¢–æ—á–Ω–∞—è –∫–æ—Ä—Ä–µ–∫—Ü–∏—è SQL —Å –ø–æ–º–æ—â—å—é –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –¥–µ–π—Å—Ç–≤–∏–π", "desc": "SHARE - —ç—Ç–æ —Å–∏—Å—Ç–µ–º–∞ –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏ –¥–µ–π—Å—Ç–≤–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ SLM, –∫–æ—Ç–æ—Ä–∞—è —É–ª—É—á—à–∞–µ—Ç —Ä–∞–±–æ—Ç—É –±
[05.06.2025 00:55] Using data from previous issue: {"categories": ["#multimodal", "#inference", "#diffusion", "#video"], "emoji": "üöó", "ru": {"title": "–£–ø—Ä–∞–≤–ª—è–µ–º–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö –≤–∏–¥–µ–æ –∞–≤—Ç–æ–∫–∞—Ç–∞—Å—Ç—Ä–æ—Ñ –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –¥–æ—Ä–æ–∂–Ω–æ–≥–æ –¥–≤–∏–∂–µ–Ω–∏—è", "desc": "Ctrl-Crash - —ç—Ç–æ –º–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –∞–≤—Ç–æ–º–æ–±–∏–ª—å–Ω—ã—Ö –∞–≤–∞—Ä–∏–π, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è —É–ø—Ä–∞–≤–ª—è–µ–º–æ–µ 
[05.06.2025 00:55] Using data from previous issue: {"categories": ["#training", "#rl", "#synthetic", "#reasoning", "#transfer_learning"], "emoji": "üß†", "ru": {"title": "–í–æ–∑–≤—Ä–∞—Ç –≤ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è—Ö: –∫–ª—é—á –∫ —É–ª—É—á—à–µ–Ω–∏—é LLM", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∏–∑—É—á–∞–µ—Ç –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ –º–µ–∂–¥—É –æ–±—É—á–µ–Ω–∏–µ–º —Å —É—á–∏—Ç–µ–ª–µ–º –∏ –æ–±—É—á–µ–Ω–∏–µ–º —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM), —Ñ
[05.06.2025 00:55] Using data from previous issue: {"categories": ["#benchmark", "#video", "#long_context", "#reasoning", "#agents"], "emoji": "üé•", "ru": {"title": "–ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –∞–≥–µ–Ω—Ç –¥–ª—è –≥–ª—É–±–æ–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∞–≥–µ–Ω—Ç–∞ Deep Video Discovery –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –ø–æ–∏—Å
[05.06.2025 00:55] Using data from previous issue: {"categories": ["#dataset", "#diffusion", "#3d", "#cv"], "emoji": "üï∫", "ru": {"title": "3D-—É–ø—Ä–∞–≤–ª—è–µ–º–∞—è –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—è –≤–∏–¥–µ–æ –¥–ª—è —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ–π –∞–Ω–∏–º–∞—Ü–∏–∏ —á–µ–ª–æ–≤–µ–∫–∞", "desc": "PoseFuse3D-KI - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏–∏ –∫–ª—é—á–µ–≤—ã—Ö –∫–∞–¥—Ä–æ–≤ —Å —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º–∏ –¥–≤–∏–∂–µ–Ω–∏—è–º–∏, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è 3D-–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–æ–∑–µ –∏ —Ñ
[05.06.2025 00:55] Using data from previous issue: {"categories": ["#rl", "#benchmark", "#training", "#long_context", "#optimization", "#reasoning"], "emoji": "‚öñÔ∏è", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –≤ LLM: –±–∞–ª–∞–Ω—Å –º–µ–∂–¥—É –∫—Ä–∞—Ç–∫–æ—Å—Ç—å—é –∏ —Ç–æ—á–Ω–æ—Å—Ç—å—é", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM), –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞
[05.06.2025 00:55] Using data from previous issue: {"categories": ["#dataset", "#machine_translation", "#science", "#multilingual", "#benchmark"], "emoji": "üìä", "ru": {"title": "M¬≥FinMeeting: –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –≤—Å—Ç—Ä–µ—á —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ M¬≥FinMeeting –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –±–æ–ª—å—à–∏—Ö —è
[05.06.2025 00:55] Using data from previous issue: {"categories": ["#agents", "#multimodal", "#reasoning", "#optimization", "#games", "#benchmark"], "emoji": "üìä", "ru": {"title": "–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –æ—Ç—á–µ—Ç—ã: –Ω–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å –ø–æ–º–æ—â—å—é –ò–ò", "desc": "–ù–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ Multimodal DeepResearcher –ø–æ–∑–≤–æ–ª—è–µ—Ç –±–æ–ª—å—à–∏–º —è–∑—ã–∫–æ–≤—ã–º –º–æ–¥–µ–ª—è–º (LLM) —Å–æ–∑–¥–∞–≤–∞—Ç—å
[05.06.2025 00:55] Using data from previous issue: {"categories": ["#multilingual", "#low_resource", "#dataset", "#synthetic", "#cv", "#open_source", "#optimization"], "emoji": "üìö", "ru": {"title": "–ü—Ä–æ—Ä—ã–≤ –≤ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–∏ –∞—Ä–∞–±—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–º–æ—â—å—é –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è", "desc": "Qari-OCR –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π —Å–µ—Ä–∏—é –º–æ–¥–µ–ª–µ–π –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è –∏ –æ–±—Ä–∞–±–æ—Ç
[05.06.2025 00:55] Using data from previous issue: {"categories": ["#dataset", "#multimodal", "#benchmark"], "emoji": "üëò", "ru": {"title": "–û—Ü–µ–Ω–∫–∞ –∫—É–ª—å—Ç—É—Ä–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏ –ò–ò —á–µ—Ä–µ–∑ –ø—Ä–∏–∑–º—É —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω–æ–π –∫–∏—Ç–∞–π—Å–∫–æ–π –æ–¥–µ–∂–¥—ã", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Hanfu-Bench - –Ω–æ–≤—ã–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∫—É–ª—å—Ç—É—Ä–Ω—ã—Ö –∞—Å–ø–µ–∫—Ç–æ–≤ –º–æ–¥–µ–ª
[05.06.2025 00:55] Using data from previous issue: {"categories": ["#rl", "#training", "#reasoning", "#architecture", "#optimization"], "emoji": "üß†", "ru": {"title": "–ï–¥–∏–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π", "desc": "–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∫—Ä—É–ø–Ω–∞—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ç–µ–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å —Å –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–º–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è–º–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –ú–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–±—É—á–µ–Ω–∏–µ —Å
[05.06.2025 00:55] Using data from previous issue: {"categories": ["#training", "#dataset", "#benchmark", "#multilingual", "#transfer_learning", "#open_source"], "emoji": "üîç", "ru": {"title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–º –ø–æ–∏—Å–∫–µ: MERIT –∏ Coral", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç MERIT - –ø–µ—Ä–≤—ã–π –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞
[05.06.2025 00:55] Using data from previous issue: {"categories": ["#optimization", "#rl", "#training"], "emoji": "üöÄ", "ru": {"title": "–£—Å–∫–æ—Ä–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –ø–æ–º–æ—â—å—é —É–º–Ω–æ–≥–æ –≤—ã–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö", "desc": "GAIN-RL - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–∏–≥–Ω–∞–ª –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü–∏–∏ —É–≥–ª–æ–≤ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ
[05.06.2025 00:55] Using data from previous issue: {"categories": ["#training", "#interpretability", "#open_source", "#architecture"], "emoji": "üîç", "ru": {"title": "–£–ª—É—á—à–µ–Ω–∏–µ –æ–±—ä—è—Å–Ω–∏–º–æ—Å—Ç–∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ —Å —É—á–µ—Ç–æ–º –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º–µ—Ç–æ–¥ Layer-wise Relevance Propagation (LRP) –¥–ª—è –æ–±—ä—è—Å–Ω–∏–º–æ—Å—Ç–∏ –º–æ
[05.06.2025 00:55] Using data from previous issue: {"categories": ["#data", "#training", "#long_context", "#optimization", "#multimodal"], "emoji": "üìù", "ru": {"title": "LongGuide: —É–ª—É—á—à–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π", "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è –ø–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É (ICL) –¥–ª—è –∫—Ä—É–ø–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π
[05.06.2025 00:55] Using data from previous issue: {"categories": ["#reasoning", "#optimization", "#training", "#rl"], "emoji": "üß†", "ru": {"title": "–†–∞–Ω–Ω–µ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —É—Å–ø–µ—Ö–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –∏–∑—É—á–∞—é—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —É—Å–ø–µ—Ö–∞ –ø—Ä–æ—Ü–µ—Å—Å–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —Å –Ω—É–ª–µ–≤—ã–º –≤—ã—Å—Ç—Ä–µ–ª–æ–º (zero-shot Chain-of-Thought) –¥–æ –µ–≥–æ –∑
[05.06.2025 00:55] Loading Chinese text from previous data.
[05.06.2025 00:55] Renaming data file.
[05.06.2025 00:55] Renaming previous data. hf_papers.json to ./d/2025-06-05.json
[05.06.2025 00:55] Saving new data file.
[05.06.2025 00:55] Generating page.
[05.06.2025 00:55] Renaming previous page.
[05.06.2025 00:55] Renaming previous data. index.html to ./d/2025-06-05.html
[05.06.2025 00:55] [Experimental] Generating Chinese page for reading.
[05.06.2025 00:55] Chinese vocab [{'word': 'Â§öÊ®°ÊÄÅ', 'pinyin': 'du≈ç m√≥ t√†i', 'trans': 'multimodal'}, {'word': 'Âü∫ÂáÜ', 'pinyin': 'jƒ´ zh«în', 'trans': 'benchmark'}, {'word': 'ËßÜËßâËØ≠Ë®ÄÊ®°Âûã', 'pinyin': 'sh√¨ ju√© y«î y√°n m√≥ x√≠ng', 'trans': 'vision-language model'}, {'word': 'Â§çÊùÇ', 'pinyin': 'f√π z√°', 'trans': 'complex'}, {'word': 'Â§öÊô∫ËÉΩ‰Ωì', 'pinyin': 'du≈ç zh√¨ n√©ng t«ê', 'trans': 'multi-agent'}, {'word': 'ÁéØÂ¢É', 'pinyin': 'hu√°n j√¨ng', 'trans': 'environment'}, {'word': 'Á≠ñÁï•Êé®ÁêÜ', 'pinyin': 'c√® l√º√® tuƒ´ l«ê', 'trans': 'strategic reasoning'}, {'word': 'ÂÜ≥Á≠ñ', 'pinyin': 'ju√© c√®', 'trans': 'decision-making'}, {'word': 'ËÉΩÂäõ', 'pinyin': 'n√©ng l√¨', 'trans': 'ability'}, {'word': 'Â±ÄÈôê‰∫é', 'pinyin': 'j√∫ xi√†n y√∫', 'trans': 'limited to'}, {'word': 'ÂçïÊô∫ËÉΩ‰Ωì', 'pinyin': 'dƒÅn zh√¨ n√©ng t«ê', 'trans': 'single-agent'}, {'word': '‰ªÖ', 'pinyin': 'j«ên', 'trans': 'only'}, {'word': 'ÊñáÊú¨', 'pinyin': 'w√©n bƒõn', 'trans': 'text'}, {'word': 'Ê∂µÁõñ', 'pinyin': 'h√°n g√†i', 'trans': 'cover'}, {'word': 'Âêà‰Ωú', 'pinyin': 'h√© zu√≤', 'trans': 'cooperation'}, {'word': 'Á´û‰∫â', 'pinyin': 'j√¨ng zhƒìng', 'trans': 'competition'}, {'word': 'Ê∑∑Âêà', 'pinyin': 'h√πn h√©', 'trans': 'hybrid'}, {'word': 'Âä®Êú∫', 'pinyin': 'd√≤ng jƒ´', 'trans': 'motivation'}, {'word': '‰∫íÂä®', 'pinyin': 'h√π d√≤ng', 'trans': 'interaction'}, {'word': 'Á†îÁ©∂', 'pinyin': 'y√°n ji≈´', 'trans': 'research'}, {'word': 'ÂèëÁé∞', 'pinyin': 'fƒÅ xi√†n', 'trans': 'find'}, {'word': 'È¢ÑÊµã', 'pinyin': 'y√π c√®', 'trans': 'prediction'}, {'word': 'ÂáÜÁ°ÆÊÄß', 'pinyin': 'zh«în qu√® x√¨ng', 'trans': 'accuracy'}, {'word': 'ÂΩí‰∏ÄÂåñ', 'pinyin': 'guƒ´ yƒ´ hu√†', 'trans': 'normalization'}, {'word': 'ÂõûÊä•', 'pinyin': 'hu√≠ b√†o', 'trans': 'reward'}, {'word': 'ÊñπÈù¢', 'pinyin': 'fƒÅng mi√†n', 'trans': 'aspect'}, {'word': 'Â≠òÂú®', 'pinyin': 'c√∫n z√†i', 'trans': 'exist'}, {'word': 'ÊòæËëó', 'pinyin': 'xi«én zh√π', 'trans': 'significant'}, {'word': 'Â∑ÆË∑ù', 'pinyin': 'chƒÅ j√π', 'trans': 'gap'}, {'word': 'Êó®Âú®', 'pinyin': 'zh«ê z√†i', 'trans': 'aim to'}, {'word': 'Ê†áÂáÜÂåñ', 'pinyin': 'biƒÅo zh«în hu√†', 'trans': 'standardize'}, {'word': 'ËØÑ‰º∞', 'pinyin': 'p√≠ng g≈´', 'trans': 'evaluate'}, {'word': 'ÊåáÂá∫', 'pinyin': 'zh«ê ch≈´', 'trans': 'point out'}, {'word': 'Â±ÄÈôêÊÄß', 'pinyin': 'j√∫ xi√†n x√¨ng', 'trans': 'limitation'}, {'word': 'Êé®Âä®', 'pinyin': 'tuƒ´ d√≤ng', 'trans': 'promote'}, {'word': 'Êú™Êù•', 'pinyin': 'w√®i l√°i', 'trans': 'future'}, {'word': '‰ª£Á†Å', 'pinyin': 'd√†i m«é', 'trans': 'code'}, {'word': 'Êï∞ÊçÆ', 'pinyin': 'sh√π j√π', 'trans': 'data'}, {'word': 'Ëé∑Âèñ', 'pinyin': 'hu√≤ q«î', 'trans': 'obtain'}]
[05.06.2025 00:55] Renaming previous Chinese page.
[05.06.2025 00:55] Renaming previous data. zh.html to ./d/2025-06-04_zh_reading_task.html
[05.06.2025 00:55] Writing Chinese reading task.
[05.06.2025 00:55] Writing result.
[05.06.2025 00:55] Renaming log file.
[05.06.2025 00:55] Renaming previous data. log.txt to ./logs/2025-06-05_last_log.txt
