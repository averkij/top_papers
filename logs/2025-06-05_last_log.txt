[05.06.2025 04:21] Read previous papers.
[05.06.2025 04:21] Generating top page (month).
[05.06.2025 04:21] Writing top page (month).
[05.06.2025 05:13] Read previous papers.
[05.06.2025 05:13] Get feed.
[05.06.2025 05:13] Get page data from previous paper. URL: https://huggingface.co/papers/2506.03569
[05.06.2025 05:13] Get page data from previous paper. URL: https://huggingface.co/papers/2506.02921
[05.06.2025 05:13] Get page data from previous paper. URL: https://huggingface.co/papers/2506.04180
[05.06.2025 05:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.24500
[05.06.2025 05:13] Get page data from previous paper. URL: https://huggingface.co/papers/2506.04228
[05.06.2025 05:13] Get page data from previous paper. URL: https://huggingface.co/papers/2506.04158
[05.06.2025 05:13] Get page data from previous paper. URL: https://huggingface.co/papers/2506.03295
[05.06.2025 05:13] Get page data from previous paper. URL: https://huggingface.co/papers/2506.04207
[05.06.2025 05:13] Get page data from previous paper. URL: https://huggingface.co/papers/2506.02592
[05.06.2025 05:13] Get page data from previous paper. URL: https://huggingface.co/papers/2506.03099
[05.06.2025 05:13] Get page data from previous paper. URL: https://huggingface.co/papers/2506.04108
[05.06.2025 05:13] Get page data from previous paper. URL: https://huggingface.co/papers/2506.03106
[05.06.2025 05:13] Extract page data from URL. URL: https://huggingface.co/papers/2506.01320
[05.06.2025 05:13] Get page data from previous paper. URL: https://huggingface.co/papers/2506.04133
[05.06.2025 05:13] Get page data from previous paper. URL: https://huggingface.co/papers/2506.04034
[05.06.2025 05:13] Get page data from previous paper. URL: https://huggingface.co/papers/2506.03448
[05.06.2025 05:13] Get page data from previous paper. URL: https://huggingface.co/papers/2506.02945
[05.06.2025 05:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.23807
[05.06.2025 05:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21541
[05.06.2025 05:13] Get page data from previous paper. URL: https://huggingface.co/papers/2506.02294
[05.06.2025 05:13] Get page data from previous paper. URL: https://huggingface.co/papers/2506.01344
[05.06.2025 05:13] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[05.06.2025 05:13] No deleted papers detected.
[05.06.2025 05:13] Downloading and parsing papers (pdf, html). Total: 21.
[05.06.2025 05:13] Downloading and parsing paper https://huggingface.co/papers/2506.03569.
[05.06.2025 05:13] Extra JSON file exists (./assets/json/2506.03569.json), skip PDF parsing.
[05.06.2025 05:13] Paper image links file exists (./assets/img_data/2506.03569.json), skip HTML parsing.
[05.06.2025 05:13] Success.
[05.06.2025 05:13] Downloading and parsing paper https://huggingface.co/papers/2506.02921.
[05.06.2025 05:13] Extra JSON file exists (./assets/json/2506.02921.json), skip PDF parsing.
[05.06.2025 05:13] Paper image links file exists (./assets/img_data/2506.02921.json), skip HTML parsing.
[05.06.2025 05:13] Success.
[05.06.2025 05:13] Downloading and parsing paper https://huggingface.co/papers/2506.04180.
[05.06.2025 05:13] Extra JSON file exists (./assets/json/2506.04180.json), skip PDF parsing.
[05.06.2025 05:13] Paper image links file exists (./assets/img_data/2506.04180.json), skip HTML parsing.
[05.06.2025 05:13] Success.
[05.06.2025 05:13] Downloading and parsing paper https://huggingface.co/papers/2505.24500.
[05.06.2025 05:13] Extra JSON file exists (./assets/json/2505.24500.json), skip PDF parsing.
[05.06.2025 05:13] Paper image links file exists (./assets/img_data/2505.24500.json), skip HTML parsing.
[05.06.2025 05:13] Success.
[05.06.2025 05:13] Downloading and parsing paper https://huggingface.co/papers/2506.04228.
[05.06.2025 05:13] Extra JSON file exists (./assets/json/2506.04228.json), skip PDF parsing.
[05.06.2025 05:13] Paper image links file exists (./assets/img_data/2506.04228.json), skip HTML parsing.
[05.06.2025 05:13] Success.
[05.06.2025 05:13] Downloading and parsing paper https://huggingface.co/papers/2506.04158.
[05.06.2025 05:13] Extra JSON file exists (./assets/json/2506.04158.json), skip PDF parsing.
[05.06.2025 05:13] Paper image links file exists (./assets/img_data/2506.04158.json), skip HTML parsing.
[05.06.2025 05:13] Success.
[05.06.2025 05:13] Downloading and parsing paper https://huggingface.co/papers/2506.03295.
[05.06.2025 05:13] Extra JSON file exists (./assets/json/2506.03295.json), skip PDF parsing.
[05.06.2025 05:13] Paper image links file exists (./assets/img_data/2506.03295.json), skip HTML parsing.
[05.06.2025 05:13] Success.
[05.06.2025 05:13] Downloading and parsing paper https://huggingface.co/papers/2506.04207.
[05.06.2025 05:13] Extra JSON file exists (./assets/json/2506.04207.json), skip PDF parsing.
[05.06.2025 05:13] Paper image links file exists (./assets/img_data/2506.04207.json), skip HTML parsing.
[05.06.2025 05:13] Success.
[05.06.2025 05:13] Downloading and parsing paper https://huggingface.co/papers/2506.02592.
[05.06.2025 05:13] Extra JSON file exists (./assets/json/2506.02592.json), skip PDF parsing.
[05.06.2025 05:13] Paper image links file exists (./assets/img_data/2506.02592.json), skip HTML parsing.
[05.06.2025 05:13] Success.
[05.06.2025 05:13] Downloading and parsing paper https://huggingface.co/papers/2506.03099.
[05.06.2025 05:13] Extra JSON file exists (./assets/json/2506.03099.json), skip PDF parsing.
[05.06.2025 05:13] Paper image links file exists (./assets/img_data/2506.03099.json), skip HTML parsing.
[05.06.2025 05:13] Success.
[05.06.2025 05:13] Downloading and parsing paper https://huggingface.co/papers/2506.04108.
[05.06.2025 05:13] Extra JSON file exists (./assets/json/2506.04108.json), skip PDF parsing.
[05.06.2025 05:13] Paper image links file exists (./assets/img_data/2506.04108.json), skip HTML parsing.
[05.06.2025 05:13] Success.
[05.06.2025 05:13] Downloading and parsing paper https://huggingface.co/papers/2506.03106.
[05.06.2025 05:13] Extra JSON file exists (./assets/json/2506.03106.json), skip PDF parsing.
[05.06.2025 05:13] Paper image links file exists (./assets/img_data/2506.03106.json), skip HTML parsing.
[05.06.2025 05:13] Success.
[05.06.2025 05:13] Downloading and parsing paper https://huggingface.co/papers/2506.01320.
[05.06.2025 05:13] Downloading paper 2506.01320 from http://arxiv.org/pdf/2506.01320v1...
[05.06.2025 05:13] Extracting affiliations from text.
[05.06.2025 05:13] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 2 ] . [ 1 0 2 3 1 0 . 6 0 5 2 : r Œ®-Sampler: Initial Particle Sampling for SMC-Based Inference-Time Reward Alignment in Score Models Taehoon Yoon Yunhong Min Kyeongmin Yeo Minhyuk Sung KAIST {taehoon,dbsghd363,aaaaa,mhsung}@kaist.ac.kr "
[05.06.2025 05:13] Response: ```python
["KAIST"]
```
[05.06.2025 05:13] Deleting PDF ./assets/pdf/2506.01320.pdf.
[05.06.2025 05:13] Success.
[05.06.2025 05:13] Downloading and parsing paper https://huggingface.co/papers/2506.04133.
[05.06.2025 05:13] Extra JSON file exists (./assets/json/2506.04133.json), skip PDF parsing.
[05.06.2025 05:13] Paper image links file exists (./assets/img_data/2506.04133.json), skip HTML parsing.
[05.06.2025 05:13] Success.
[05.06.2025 05:13] Downloading and parsing paper https://huggingface.co/papers/2506.04034.
[05.06.2025 05:13] Extra JSON file exists (./assets/json/2506.04034.json), skip PDF parsing.
[05.06.2025 05:13] Paper image links file exists (./assets/img_data/2506.04034.json), skip HTML parsing.
[05.06.2025 05:13] Success.
[05.06.2025 05:13] Downloading and parsing paper https://huggingface.co/papers/2506.03448.
[05.06.2025 05:13] Extra JSON file exists (./assets/json/2506.03448.json), skip PDF parsing.
[05.06.2025 05:13] Paper image links file exists (./assets/img_data/2506.03448.json), skip HTML parsing.
[05.06.2025 05:13] Success.
[05.06.2025 05:13] Downloading and parsing paper https://huggingface.co/papers/2506.02945.
[05.06.2025 05:13] Extra JSON file exists (./assets/json/2506.02945.json), skip PDF parsing.
[05.06.2025 05:13] Paper image links file exists (./assets/img_data/2506.02945.json), skip HTML parsing.
[05.06.2025 05:13] Success.
[05.06.2025 05:13] Downloading and parsing paper https://huggingface.co/papers/2505.23807.
[05.06.2025 05:13] Extra JSON file exists (./assets/json/2505.23807.json), skip PDF parsing.
[05.06.2025 05:13] Paper image links file exists (./assets/img_data/2505.23807.json), skip HTML parsing.
[05.06.2025 05:13] Success.
[05.06.2025 05:13] Downloading and parsing paper https://huggingface.co/papers/2505.21541.
[05.06.2025 05:13] Extra JSON file exists (./assets/json/2505.21541.json), skip PDF parsing.
[05.06.2025 05:13] Paper image links file exists (./assets/img_data/2505.21541.json), skip HTML parsing.
[05.06.2025 05:13] Success.
[05.06.2025 05:13] Downloading and parsing paper https://huggingface.co/papers/2506.02294.
[05.06.2025 05:13] Extra JSON file exists (./assets/json/2506.02294.json), skip PDF parsing.
[05.06.2025 05:13] Paper image links file exists (./assets/img_data/2506.02294.json), skip HTML parsing.
[05.06.2025 05:13] Success.
[05.06.2025 05:13] Downloading and parsing paper https://huggingface.co/papers/2506.01344.
[05.06.2025 05:13] Extra JSON file exists (./assets/json/2506.01344.json), skip PDF parsing.
[05.06.2025 05:13] Paper image links file exists (./assets/img_data/2506.01344.json), skip HTML parsing.
[05.06.2025 05:13] Success.
[05.06.2025 05:13] Enriching papers with extra data.
[05.06.2025 05:13] ********************************************************************************
[05.06.2025 05:13] Abstract 0. We open-source MiMo-VL-7B-SFT and MiMo-VL-7B-RL, two powerful vision-language models delivering state-of-the-art performance in both general visual understanding and multimodal reasoning. MiMo-VL-7B-RL outperforms Qwen2.5-VL-7B on 35 out of 40 evaluated tasks, and scores 59.4 on OlympiadBench, surpa...
[05.06.2025 05:13] ********************************************************************************
[05.06.2025 05:13] Abstract 1. LongBioBench is a new benchmark using artificially generated biographies to evaluate long-context language models across understanding, reasoning, and trustworthiness dimensions, addressing limitations in existing frameworks.  					AI-generated summary 				 Existing frameworks for evaluating long-co...
[05.06.2025 05:13] ********************************************************************************
[05.06.2025 05:13] Abstract 2. Long-form text generation remains a significant challenge for large language models (LLMs), particularly in maintaining coherence, ensuring logical consistency, and preserving text quality as sequence length increases. To address these limitations, we propose SuperWriter-Agent, an agent-based framew...
[05.06.2025 05:13] ********************************************************************************
[05.06.2025 05:13] Abstract 3. Temporal-aware Hierarchical Cognitive Reinforcement Learning enhances LLMs' social intelligence by addressing the distinct cognitive demands of social domains.  					AI-generated summary 				 Recently, Large Language Models (LLMs) have made significant progress in IQ-related domains that require car...
[05.06.2025 05:13] ********************************************************************************
[05.06.2025 05:13] Abstract 4. LayerFlow is a unified framework for generating layer-aware videos using a text-to-video diffusion transformer and layer embeddings, supporting various video generation tasks with a multi-stage training strategy.  					AI-generated summary 				 We present LayerFlow, a unified solution for layer-awar...
[05.06.2025 05:13] ********************************************************************************
[05.06.2025 05:13] Abstract 5. While diffusion models have achieved remarkable success in text-to-image generation, they encounter significant challenges with instruction-driven image editing. Our research highlights a key challenge: these models particularly struggle with structurally inconsistent edits that involve substantial ...
[05.06.2025 05:13] ********************************************************************************
[05.06.2025 05:13] Abstract 6. Critique Fine-Tuning on a single problem can efficiently enhance the reasoning capabilities of large language models with significant performance gains and reduced computational cost compared to reinforcement learning.  					AI-generated summary 				 We have witnessed that strong LLMs like Qwen-Math...
[05.06.2025 05:13] ********************************************************************************
[05.06.2025 05:13] Abstract 7. Inspired by the remarkable reasoning capabilities of Deepseek-R1 in complex textual tasks, many works attempt to incentivize similar capabilities in Multimodal Large Language Models (MLLMs) by directly applying reinforcement learning (RL). However, they still struggle to activate complex reasoning. ...
[05.06.2025 05:13] ********************************************************************************
[05.06.2025 05:13] Abstract 8. The DBG score is introduced to measure self-preference bias in large language models by using gold judgments as proxies for response quality, addressing the confounding effect of response quality.  					AI-generated summary 				 Recent studies show that large language models (LLMs) exhibit self-pref...
[05.06.2025 05:13] ********************************************************************************
[05.06.2025 05:13] Abstract 9. TalkingMachines transforms a pretrained image-to-video model into an audio-driven avatar generator, supports infinite video streaming, and uses engineering optimizations for real-time performance.  					AI-generated summary 				 In this paper, we present TalkingMachines -- an efficient framework tha...
[05.06.2025 05:13] ********************************************************************************
[05.06.2025 05:13] Abstract 10. Rectified Sparse Attention (ReSA) improves the efficiency of long-sequence generation in Large Language Models by combining block-sparse attention with periodic dense rectification, maintaining high-quality generation.  					AI-generated summary 				 Efficient long-sequence generation is a critical ...
[05.06.2025 05:13] ********************************************************************************
[05.06.2025 05:13] Abstract 11. Critique-GRPO, an RL framework combining numerical and natural language feedback, enhances LLM reasoning across tasks and outperforms existing methods.  					AI-generated summary 				 Recent advances in reinforcement learning (RL) with numerical feedback, such as scalar rewards, have significantly e...
[05.06.2025 05:13] ********************************************************************************
[05.06.2025 05:13] Abstract 12. We introduce Psi-Sampler, an SMC-based framework incorporating pCNL-based initial particle sampling for effective inference-time reward alignment with a score-based generative model. Inference-time reward alignment with score-based generative models has recently gained significant traction, followin...
[05.06.2025 05:13] ********************************************************************************
[05.06.2025 05:13] Abstract 13. A review of trust, risk, and security management in LLM-based agentic multi-agent systems, examining governance, explainability, ModelOps, and privacy/security.  					AI-generated summary 				 Agentic AI systems, built on large language models (LLMs) and deployed in multi-agent configurations, are r...
[05.06.2025 05:13] ********************************************************************************
[05.06.2025 05:13] Abstract 14. Object referring aims to detect all objects in an image that match a given natural language description. We argue that a robust object referring model should be grounded, meaning its predictions should be both explainable and faithful to the visual content. Specifically, it should satisfy two key pr...
[05.06.2025 05:13] ********************************************************************************
[05.06.2025 05:13] Abstract 15. RefEdit, an instruction-based editing model trained on synthetic data, outperforms baselines in complex scene editing and referring expression tasks.  					AI-generated summary 				 Despite recent advances in inversion and instruction-based image editing, existing approaches primarily excel at editi...
[05.06.2025 05:13] ********************************************************************************
[05.06.2025 05:13] Abstract 16. LLM-as-a-judge is a framework in which a large language model (LLM) automatically evaluates the output of another LLM. We propose quantitative LLM judges, which align evaluation scores of existing LLM judges to human scores in a given domain using regression models. The models are trained to improve...
[05.06.2025 05:13] ********************************************************************************
[05.06.2025 05:13] Abstract 17. A dynamic layerwise pruning method adaptively determines layer importance by combining model weights and activation information to maintain performance in large language models at high sparsity.  					AI-generated summary 				 Pruning has recently been widely adopted to reduce the parameter scale an...
[05.06.2025 05:13] ********************************************************************************
[05.06.2025 05:13] Abstract 18. DiffDecompose, a diffusion Transformer-based framework, effectively decomposes images into constituent layers with semantic prompts, addressing challenges in transparent layer decomposition.  					AI-generated summary 				 Diffusion models have recently motivated great success in many generation tas...
[05.06.2025 05:13] ********************************************************************************
[05.06.2025 05:13] Abstract 19. A diffusion-based data augmentation strategy improves robustness in knowledge distillation by generating challenging samples, enhancing accuracy and spurious feature resilience.  					AI-generated summary 				 Large foundation models trained on extensive datasets demonstrate strong zero-shot capabil...
[05.06.2025 05:13] ********************************************************************************
[05.06.2025 05:13] Abstract 20. Flowcharts are a critical tool for visualizing decision-making processes. However, their non-linear structure and complex visual-textual relationships make it challenging to interpret them using LLMs, as vision-language models frequently hallucinate nonexistent connections and decision paths when an...
[05.06.2025 05:13] Read previous papers.
[05.06.2025 05:13] Generating reviews via LLM API.
[05.06.2025 05:13] Using data from previous issue: {"categories": ["#training", "#rl", "#reasoning", "#multimodal", "#rlhf", "#benchmark", "#dataset", "#open_source"], "emoji": "üß†", "ru": {"title": "–ü—Ä–æ—Ä—ã–≤ –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–º –ò–ò: MiMo-VL —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –Ω–æ–≤—ã–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ –¥–≤–µ –º–æ—â–Ω—ã–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ MiMo-VL-7B-SFT
[05.06.2025 05:13] Using data from previous issue: {"categories": ["#benchmark", "#synthetic", "#long_context", "#reasoning", "#interpretability"], "emoji": "üìä", "ru": {"title": "LongBioBench: –ù–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –æ—Ü–µ–Ω–∫–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º", "desc": "LongBioBench - —ç—Ç–æ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º, –∏—Å–ø
[05.06.2025 05:13] Using data from previous issue: {"categories": ["#training", "#optimization", "#agents", "#long_context", "#rlhf", "#benchmark", "#dataset", "#story_generation"], "emoji": "‚úçÔ∏è", "ru": {"title": "–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤", "desc": "SuperWriter-Agent - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤
[05.06.2025 05:13] Using data from previous issue: {"categories": ["#rlhf", "#rl", "#reasoning", "#training"], "emoji": "üß†", "ru": {"title": "–ù–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —Å–æ—Ü–∏–∞–ª—å–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ –º–µ—Ç–æ–¥ Temporal-aware Hierarchical Cognitive Reinforcement Learning (TimeHC-RL) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–æ—Ü–∏
[05.06.2025 05:13] Using data from previous issue: {"categories": ["#multimodal", "#diffusion", "#synthetic", "#video", "#training"], "emoji": "üéûÔ∏è", "ru": {"title": "LayerFlow: –£–º–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –º–Ω–æ–≥–æ—Å–ª–æ–π–Ω–æ–≥–æ –≤–∏–¥–µ–æ –ø–æ —Ç–µ–∫—Å—Ç–æ–≤—ã–º –ø–æ–¥—Å–∫–∞–∑–∫–∞–º", "desc": "LayerFlow - —ç—Ç–æ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ —Å —É—á–µ—Ç–æ–º —Å–ª–æ–µ–≤, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä –¥–∏
[05.06.2025 05:13] Using data from previous issue: {"categories": ["#diffusion", "#benchmark", "#cv", "#architecture"], "emoji": "üñºÔ∏è", "ru": {"title": "–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∫–∞–∫ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –ò–ò-–æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∏—Å–∫—É—Å
[05.06.2025 05:13] Using data from previous issue: {"categories": ["#training", "#rl", "#optimization", "#reasoning"], "emoji": "üß†", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Ä–∞—Å–∫—Ä—ã—Ç–∏–µ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª–∞ –ò–ò —á–µ—Ä–µ–∑ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –∫—Ä–∏—Ç–∏–∫–µ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ Critique Fine-Tuning (CFT) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). CFT 
[05.06.2025 05:13] Using data from previous issue: {"categories": ["#reasoning", "#open_source", "#multimodal", "#training", "#benchmark", "#rl"], "emoji": "üß†", "ru": {"title": "–£–ª—É—á—à–µ–Ω–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π MLLM: –æ—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–æ –º–Ω–æ–≥–æ—ç—Ç–∞–ø–Ω–æ–≥–æ RL", "desc": "–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ —É–ª—É—á—à–µ–Ω–∏—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (MLLM) –∫ —Ä–∞—Å—Å—É–∂–¥
[05.06.2025 05:13] Using data from previous issue: {"categories": ["#benchmark", "#data", "#hallucinations", "#interpretability", "#ethics", "#training"], "emoji": "‚öñÔ∏è", "ru": {"title": "DBG: –ù–æ–≤—ã–π —Å–ø–æ—Å–æ–± –∏–∑–º–µ—Ä–µ–Ω–∏—è –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç–∏ –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∏–∑–º–µ—Ä–µ–Ω–∏—è –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç–∏ —Å–∞–º–æ–ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ
[05.06.2025 05:13] Using data from previous issue: {"categories": ["#architecture", "#multimodal", "#inference", "#games", "#audio", "#video", "#optimization"], "emoji": "üó£Ô∏è", "ru": {"title": "–û–∂–∏–≤–ª—è–µ–º –∞–≤–∞—Ç–∞—Ä—ã: –∞—É–¥–∏–æ-—É–ø—Ä–∞–≤–ª—è–µ–º–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∏–¥–µ–æ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏", "desc": "TalkingMachines - —ç—Ç–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞, –ø—Ä–µ–æ–±—Ä–∞–∑—É—é—â–∞—è –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª
[05.06.2025 05:13] Using data from previous issue: {"categories": ["#architecture", "#inference", "#long_context", "#optimization", "#training"], "emoji": "üöÄ", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞", "desc": "–ú–µ—Ç–æ–¥ Rectified Sparse Attention (ReSA) —É–ª—É—á—à–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –≤ –±–æ
[05.06.2025 05:13] Using data from previous issue: {"categories": ["#math", "#rl", "#reasoning", "#optimization", "#rlhf", "#training"], "emoji": "üß†", "ru": {"title": "Critique-GRPO: –£–ª—É—á—à–µ–Ω–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –ò–ò —á–µ—Ä–µ–∑ –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—É—é –æ–±—Ä–∞—Ç–Ω—É—é —Å–≤—è–∑—å", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Critique-GRPO - –Ω–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂
[05.06.2025 05:13] Querying the API.
[05.06.2025 05:13] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

We introduce Psi-Sampler, an SMC-based framework incorporating pCNL-based initial particle sampling for effective inference-time reward alignment with a score-based generative model. Inference-time reward alignment with score-based generative models has recently gained significant traction, following a broader paradigm shift from pre-training to post-training optimization. At the core of this trend is the application of Sequential Monte Carlo (SMC) to the denoising process. However, existing methods typically initialize particles from the Gaussian prior, which inadequately captures reward-relevant regions and results in reduced sampling efficiency. We demonstrate that initializing from the reward-aware posterior significantly improves alignment performance. To enable posterior sampling in high-dimensional latent spaces, we introduce the preconditioned Crank-Nicolson Langevin (pCNL) algorithm, which combines dimension-robust proposals with gradient-informed dynamics. This approach enables efficient and scalable posterior sampling and consistently improves performance across various reward alignment tasks, including layout-to-image generation, quantity-aware generation, and aesthetic-preference generation, as demonstrated in our experiments.
[05.06.2025 05:13] Response: {
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Psi-Sampler - –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏—è –Ω–∞–≥—Ä–∞–¥ –ø—Ä–∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–µ –≤ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ—Ü–µ–Ω–æ–∫. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–π –º–µ—Ç–æ–¥ –ú–æ–Ω—Ç–µ-–ö–∞—Ä–ª–æ (SMC) —Å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–µ–π —á–∞—Å—Ç–∏—Ü –∏–∑ –∞–ø–æ—Å—Ç–µ—Ä–∏–æ—Ä–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è, —É—á–∏—Ç—ã–≤–∞—é—â–µ–≥–æ –Ω–∞–≥—Ä–∞–¥—É. –î–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏ –∏–∑ –∞–ø–æ—Å—Ç–µ—Ä–∏–æ—Ä–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤ –≤—ã—Å–æ–∫–æ—Ä–∞–∑–º–µ—Ä–Ω—ã—Ö –ª–∞—Ç–µ–Ω—Ç–Ω—ã—Ö –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞—Ö –≤–≤–æ–¥–∏—Ç—Å—è –∞–ª–≥–æ—Ä–∏—Ç–º preconditioned Crank-Nicolson Langevin (pCNL). –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç —É–ª—É—á—à–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å —É—á–µ—Ç–æ–º –Ω–∞–≥—Ä–∞–¥.",
  "emoji": "üé®",
  "title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏–µ –Ω–∞–≥—Ä–∞–¥ –≤ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö —Å –ø–æ–º–æ—â—å—é —É–º–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏"
}
[05.06.2025 05:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"We introduce Psi-Sampler, an SMC-based framework incorporating pCNL-based initial particle sampling for effective inference-time reward alignment with a score-based generative model. Inference-time reward alignment with score-based generative models has recently gained significant traction, following a broader paradigm shift from pre-training to post-training optimization. At the core of this trend is the application of Sequential Monte Carlo (SMC) to the denoising process. However, existing methods typically initialize particles from the Gaussian prior, which inadequately captures reward-relevant regions and results in reduced sampling efficiency. We demonstrate that initializing from the reward-aware posterior significantly improves alignment performance. To enable posterior sampling in high-dimensional latent spaces, we introduce the preconditioned Crank-Nicolson Langevin (pCNL) algorithm, which combines dimension-robust proposals with gradient-informed dynamics. This approach enables efficient and scalable posterior sampling and consistently improves performance across various reward alignment tasks, including layout-to-image generation, quantity-aware generation, and aesthetic-preference generation, as demonstrated in our experiments."

[05.06.2025 05:13] Response: ```python
['INFERENCE', 'RLHF']
```
[05.06.2025 05:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"We introduce Psi-Sampler, an SMC-based framework incorporating pCNL-based initial particle sampling for effective inference-time reward alignment with a score-based generative model. Inference-time reward alignment with score-based generative models has recently gained significant traction, following a broader paradigm shift from pre-training to post-training optimization. At the core of this trend is the application of Sequential Monte Carlo (SMC) to the denoising process. However, existing methods typically initialize particles from the Gaussian prior, which inadequately captures reward-relevant regions and results in reduced sampling efficiency. We demonstrate that initializing from the reward-aware posterior significantly improves alignment performance. To enable posterior sampling in high-dimensional latent spaces, we introduce the preconditioned Crank-Nicolson Langevin (pCNL) algorithm, which combines dimension-robust proposals with gradient-informed dynamics. This approach enables efficient and scalable posterior sampling and consistently improves performance across various reward alignment tasks, including layout-to-image generation, quantity-aware generation, and aesthetic-preference generation, as demonstrated in our experiments."

[05.06.2025 05:13] Response: ```python
["ALIGNMENT", "OPTIMIZATION"]
```
[05.06.2025 05:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The paper presents Psi-Sampler, a framework that enhances reward alignment during inference by using Sequential Monte Carlo (SMC) methods. It addresses the limitations of traditional particle initialization from Gaussian priors, which often fail to capture important reward-related areas. By employing a reward-aware posterior for initialization, the framework significantly boosts sampling efficiency and alignment performance. Additionally, the introduction of the preconditioned Crank-Nicolson Langevin (pCNL) algorithm allows for effective sampling in complex, high-dimensional spaces, leading to improved results in various generative tasks.","title":"Enhancing Reward Alignment with Psi-Sampler"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='The paper presents Psi-Sampler, a framework that enhances reward alignment during inference by using Sequential Monte Carlo (SMC) methods. It addresses the limitations of traditional particle initialization from Gaussian priors, which often fail to capture important reward-related areas. By employing a reward-aware posterior for initialization, the framework significantly boosts sampling efficiency and alignment performance. Additionally, the introduction of the preconditioned Crank-Nicolson Langevin (pCNL) algorithm allows for effective sampling in complex, high-dimensional spaces, leading to improved results in various generative tasks.', title='Enhancing Reward Alignment with Psi-Sampler'))
[05.06.2025 05:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÂêç‰∏∫Psi-SamplerÁöÑÊ°ÜÊû∂ÔºåËØ•Ê°ÜÊû∂Âü∫‰∫éÂ∫èÂàóËíôÁâπÂç°Ê¥õÔºàSMCÔºâÊñπÊ≥ïÔºåÂπ∂ÁªìÂêà‰∫ÜÂü∫‰∫éÂ•ñÂä±ÁöÑÂàùÂßãÁ≤íÂ≠êÈááÊ†∑Ôºå‰ª•ÂÆûÁé∞‰∏éÂü∫‰∫éÂàÜÊï∞ÁöÑÁîüÊàêÊ®°ÂûãÁöÑÊúâÊïàÊé®ÁêÜÊó∂Èó¥Â•ñÂä±ÂØπÈΩê„ÄÇËøëÂπ¥Êù•ÔºåÂü∫‰∫éÂàÜÊï∞ÁöÑÁîüÊàêÊ®°ÂûãÂú®Êé®ÁêÜÊó∂Èó¥Â•ñÂä±ÂØπÈΩêÊñπÈù¢ÂèóÂà∞‰∫ÜÂπøÊ≥õÂÖ≥Ê≥®ÔºåÊ†áÂøóÁùÄ‰ªéÈ¢ÑËÆ≠ÁªÉÂà∞ÂêéËÆ≠ÁªÉ‰ºòÂåñÁöÑËåÉÂºèËΩ¨Âèò„ÄÇÁé∞ÊúâÊñπÊ≥ïÈÄöÂ∏∏‰ªéÈ´òÊñØÂÖàÈ™åÂàùÂßãÂåñÁ≤íÂ≠êÔºåËøô‰∏çË∂≥‰ª•ÊçïÊçâ‰∏éÂ•ñÂä±Áõ∏ÂÖ≥ÁöÑÂå∫ÂüüÔºåÂØºËá¥ÈááÊ†∑ÊïàÁéáÈôç‰Ωé„ÄÇÊàë‰ª¨Â±ïÁ§∫‰∫Ü‰ªéÂ•ñÂä±ÊÑüÁü•ÂêéÈ™åÂàùÂßãÂåñÊòæËëóÊèêÈ´ò‰∫ÜÂØπÈΩêÊÄßËÉΩÔºåÂπ∂ÂºïÂÖ•‰∫ÜÈ¢ÑÊù°‰ª∂Crank-Nicolson LangevinÔºàpCNLÔºâÁÆóÊ≥ïÔºå‰ª•ÂÆûÁé∞È´òÁª¥ÊΩúÂú®Á©∫Èó¥‰∏≠ÁöÑÂêéÈ™åÈááÊ†∑„ÄÇ","title":"È´òÊïàÁöÑÂ•ñÂä±ÂØπÈΩêÔºöPsi-SamplerÊ°ÜÊû∂"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÂêç‰∏∫Psi-SamplerÁöÑÊ°ÜÊû∂ÔºåËØ•Ê°ÜÊû∂Âü∫‰∫éÂ∫èÂàóËíôÁâπÂç°Ê¥õÔºàSMCÔºâÊñπÊ≥ïÔºåÂπ∂ÁªìÂêà‰∫ÜÂü∫‰∫éÂ•ñÂä±ÁöÑÂàùÂßãÁ≤íÂ≠êÈááÊ†∑Ôºå‰ª•ÂÆûÁé∞‰∏éÂü∫‰∫éÂàÜÊï∞ÁöÑÁîüÊàêÊ®°ÂûãÁöÑÊúâÊïàÊé®ÁêÜÊó∂Èó¥Â•ñÂä±ÂØπÈΩê„ÄÇËøëÂπ¥Êù•ÔºåÂü∫‰∫éÂàÜÊï∞ÁöÑÁîüÊàêÊ®°ÂûãÂú®Êé®ÁêÜÊó∂Èó¥Â•ñÂä±ÂØπÈΩêÊñπÈù¢ÂèóÂà∞‰∫ÜÂπøÊ≥õÂÖ≥Ê≥®ÔºåÊ†áÂøóÁùÄ‰ªéÈ¢ÑËÆ≠ÁªÉÂà∞ÂêéËÆ≠ÁªÉ‰ºòÂåñÁöÑËåÉÂºèËΩ¨Âèò„ÄÇÁé∞ÊúâÊñπÊ≥ïÈÄöÂ∏∏‰ªéÈ´òÊñØÂÖàÈ™åÂàùÂßãÂåñÁ≤íÂ≠êÔºåËøô‰∏çË∂≥‰ª•ÊçïÊçâ‰∏éÂ•ñÂä±Áõ∏ÂÖ≥ÁöÑÂå∫ÂüüÔºåÂØºËá¥ÈááÊ†∑ÊïàÁéáÈôç‰Ωé„ÄÇÊàë‰ª¨Â±ïÁ§∫‰∫Ü‰ªéÂ•ñÂä±ÊÑüÁü•ÂêéÈ™åÂàùÂßãÂåñÊòæËëóÊèêÈ´ò‰∫ÜÂØπÈΩêÊÄßËÉΩÔºåÂπ∂ÂºïÂÖ•‰∫ÜÈ¢ÑÊù°‰ª∂Crank-Nicolson LangevinÔºàpCNLÔºâÁÆóÊ≥ïÔºå‰ª•ÂÆûÁé∞È´òÁª¥ÊΩúÂú®Á©∫Èó¥‰∏≠ÁöÑÂêéÈ™åÈááÊ†∑„ÄÇ', title='È´òÊïàÁöÑÂ•ñÂä±ÂØπÈΩêÔºöPsi-SamplerÊ°ÜÊû∂'))
[05.06.2025 05:13] Using data from previous issue: {"categories": ["#training", "#architecture", "#survey", "#agents", "#multimodal", "#security", "#alignment", "#benchmark", "#interpretability"], "emoji": "ü§ñ", "ru": {"title": "–ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –∏ –¥–æ–≤–µ—Ä–∏–µ –≤ —ç–ø–æ—Ö—É –∞–≥–µ–Ω—Ç–Ω–æ–≥–æ –ò–ò", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–æ–≤–µ—Ä–∏–µ–º, —Ä–∏
[05.06.2025 05:13] Using data from previous issue: {"categories": ["#cv", "#rl", "#training", "#reasoning", "#hallucinations", "#interpretability", "#dataset"], "emoji": "üîç", "ru": {"title": "–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ–µ –æ–±—ä–µ–∫—Ç–Ω–æ–µ —Ä–µ—Ñ–µ—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ —á–µ—Ä–µ–∑ –ø–æ—à–∞–≥–æ–≤—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∑–∞–¥–∞—á–µ –æ–±—ä–µ–∫—Ç–Ω–æ–≥–æ —Ä–µ—Ñ–µ—Ä–∏—Ä–æ–≤–∞–Ω–∏—è –≤ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω
[05.06.2025 05:13] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#synthetic", "#cv", "#optimization", "#open_source"], "emoji": "üñºÔ∏è", "ru": {"title": "RefEdit: –ü—Ä–æ—Ä—ã–≤ –≤ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ —Å–ª–æ–∂–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é –ò–ò", "desc": "RefEdit - —ç—Ç–æ –º–æ–¥–µ–ª—å —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π, –æ–±—É—á–µ–Ω–Ω–∞—è –Ω–∞ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫
[05.06.2025 05:13] Using data from previous issue: {"categories": ["#training", "#optimization", "#alignment", "#rlhf", "#dataset"], "emoji": "‚öñÔ∏è", "ru": {"title": "LLM-—Å—É–¥—å–∏: –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ—Ü–µ–Ω–∫–∞ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –ø–æ–º–æ—â—å—é —Ä–µ–≥—Ä–µ—Å—Å–∏–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ LLM-as-a-judge, –≥–¥–µ –±–æ–ª—å—à–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥—Ä—É–≥–æ–π –º–æ–¥
[05.06.2025 05:13] Using data from previous issue: {"categories": ["#optimization", "#inference", "#training"], "emoji": "‚úÇÔ∏è", "ru": {"title": "–£–º–Ω–∞—è –æ–±—Ä–µ–∑–∫–∞ —Å–ª–æ–µ–≤ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–ü—Ä–µ–¥–ª–æ–∂–µ–Ω –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π –ø–æ—Å–ª–æ–π–Ω–æ–π –æ–±—Ä–µ–∑–∫–∏ (DLP) –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. DLP –∞–¥–∞–ø—Ç–∏–≤–Ω–æ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –≤–∞–∂–Ω–æ—Å—Ç—å –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ—è, –∫–æ–º–±–∏–Ω
[05.06.2025 05:13] Using data from previous issue: {"categories": ["#diffusion", "#open_source", "#cv", "#dataset"], "emoji": "üñºÔ∏è", "ru": {"title": "–£–º–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ —Å–ª–æ–∏ —Å –ø–æ–º–æ—â—å—é –ò–ò", "desc": "DiffDecompose - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–æ–≥–æ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ –¥–ª—è –¥–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —Å–ª–æ–∏. –û–Ω–∞ —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º
[05.06.2025 05:13] Using data from previous issue: {"categories": ["#data", "#transfer_learning", "#training", "#optimization", "#diffusion", "#dataset"], "emoji": "üß†", "ru": {"title": "–ü–æ–≤—ã—à–µ–Ω–∏–µ —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é —Å–ª–æ–∂–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∏—Ñ—Ñ—É–∑–∏–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–æ
[05.06.2025 05:13] Using data from previous issue: {"categories": ["#graphs", "#cv", "#reasoning", "#agents", "#hallucinations", "#multimodal", "#benchmark", "#interpretability"], "emoji": "üîÄ", "ru": {"title": "–¢–æ—á–Ω–∞—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –±–ª–æ–∫-—Å—Ö–µ–º —Å –ø–æ–º–æ—â—å—é –Ω–µ–π—Ä–æ—Å–∏–º–≤–æ–ª–∏—á–µ—Å–∫–æ–≥–æ –∞–≥–µ–Ω—Ç–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∑–∞–¥–∞—á—É —Ç–æ—á–Ω–æ–π –∞—Ç—Ä–∏–±—É—Ü–∏–∏ –±–ª–æ–∫-—Å—Ö–µ–º –∏ –∞–≥–µ–Ω—Ç
[05.06.2025 05:13] Loading Chinese text from previous data.
[05.06.2025 05:13] Renaming data file.
[05.06.2025 05:13] Renaming previous data. hf_papers.json to ./d/2025-06-05.json
[05.06.2025 05:13] Saving new data file.
[05.06.2025 05:13] Generating page.
[05.06.2025 05:13] Renaming previous page.
[05.06.2025 05:13] Renaming previous data. index.html to ./d/2025-06-05.html
[05.06.2025 05:13] [Experimental] Generating Chinese page for reading.
[05.06.2025 05:13] Chinese vocab [{'word': 'Â§öÊ®°ÊÄÅ', 'pinyin': 'du≈ç m√≥ t√†i', 'trans': 'multimodal'}, {'word': 'Âü∫ÂáÜ', 'pinyin': 'jƒ´ zh«în', 'trans': 'benchmark'}, {'word': 'ËßÜËßâËØ≠Ë®ÄÊ®°Âûã', 'pinyin': 'sh√¨ ju√© y«î y√°n m√≥ x√≠ng', 'trans': 'vision-language model'}, {'word': 'Â§çÊùÇ', 'pinyin': 'f√π z√°', 'trans': 'complex'}, {'word': 'Â§öÊô∫ËÉΩ‰Ωì', 'pinyin': 'du≈ç zh√¨ n√©ng t«ê', 'trans': 'multi-agent'}, {'word': 'ÁéØÂ¢É', 'pinyin': 'hu√°n j√¨ng', 'trans': 'environment'}, {'word': 'Á≠ñÁï•Êé®ÁêÜ', 'pinyin': 'c√® l√º√® tuƒ´ l«ê', 'trans': 'strategic reasoning'}, {'word': 'ÂÜ≥Á≠ñ', 'pinyin': 'ju√© c√®', 'trans': 'decision-making'}, {'word': 'ËÉΩÂäõ', 'pinyin': 'n√©ng l√¨', 'trans': 'ability'}, {'word': 'Â±ÄÈôê‰∫é', 'pinyin': 'j√∫ xi√†n y√∫', 'trans': 'limited to'}, {'word': 'ÂçïÊô∫ËÉΩ‰Ωì', 'pinyin': 'dƒÅn zh√¨ n√©ng t«ê', 'trans': 'single-agent'}, {'word': '‰ªÖ', 'pinyin': 'j«ên', 'trans': 'only'}, {'word': 'ÊñáÊú¨', 'pinyin': 'w√©n bƒõn', 'trans': 'text'}, {'word': 'Ê∂µÁõñ', 'pinyin': 'h√°n g√†i', 'trans': 'cover'}, {'word': 'Âêà‰Ωú', 'pinyin': 'h√© zu√≤', 'trans': 'cooperation'}, {'word': 'Á´û‰∫â', 'pinyin': 'j√¨ng zhƒìng', 'trans': 'competition'}, {'word': 'Ê∑∑Âêà', 'pinyin': 'h√πn h√©', 'trans': 'hybrid'}, {'word': 'Âä®Êú∫', 'pinyin': 'd√≤ng jƒ´', 'trans': 'motivation'}, {'word': '‰∫íÂä®', 'pinyin': 'h√π d√≤ng', 'trans': 'interaction'}, {'word': 'Á†îÁ©∂', 'pinyin': 'y√°n ji≈´', 'trans': 'research'}, {'word': 'ÂèëÁé∞', 'pinyin': 'fƒÅ xi√†n', 'trans': 'find'}, {'word': 'È¢ÑÊµã', 'pinyin': 'y√π c√®', 'trans': 'prediction'}, {'word': 'ÂáÜÁ°ÆÊÄß', 'pinyin': 'zh«în qu√® x√¨ng', 'trans': 'accuracy'}, {'word': 'ÂΩí‰∏ÄÂåñ', 'pinyin': 'guƒ´ yƒ´ hu√†', 'trans': 'normalization'}, {'word': 'ÂõûÊä•', 'pinyin': 'hu√≠ b√†o', 'trans': 'reward'}, {'word': 'ÊñπÈù¢', 'pinyin': 'fƒÅng mi√†n', 'trans': 'aspect'}, {'word': 'Â≠òÂú®', 'pinyin': 'c√∫n z√†i', 'trans': 'exist'}, {'word': 'ÊòæËëó', 'pinyin': 'xi«én zh√π', 'trans': 'significant'}, {'word': 'Â∑ÆË∑ù', 'pinyin': 'chƒÅ j√π', 'trans': 'gap'}, {'word': 'Êó®Âú®', 'pinyin': 'zh«ê z√†i', 'trans': 'aim to'}, {'word': 'Ê†áÂáÜÂåñ', 'pinyin': 'biƒÅo zh«în hu√†', 'trans': 'standardize'}, {'word': 'ËØÑ‰º∞', 'pinyin': 'p√≠ng g≈´', 'trans': 'evaluate'}, {'word': 'ÊåáÂá∫', 'pinyin': 'zh«ê ch≈´', 'trans': 'point out'}, {'word': 'Â±ÄÈôêÊÄß', 'pinyin': 'j√∫ xi√†n x√¨ng', 'trans': 'limitation'}, {'word': 'Êé®Âä®', 'pinyin': 'tuƒ´ d√≤ng', 'trans': 'promote'}, {'word': 'Êú™Êù•', 'pinyin': 'w√®i l√°i', 'trans': 'future'}, {'word': '‰ª£Á†Å', 'pinyin': 'd√†i m«é', 'trans': 'code'}, {'word': 'Êï∞ÊçÆ', 'pinyin': 'sh√π j√π', 'trans': 'data'}, {'word': 'Ëé∑Âèñ', 'pinyin': 'hu√≤ q«î', 'trans': 'obtain'}]
[05.06.2025 05:13] Renaming previous Chinese page.
[05.06.2025 05:13] Renaming previous data. zh.html to ./d/2025-06-04_zh_reading_task.html
[05.06.2025 05:13] Writing Chinese reading task.
[05.06.2025 05:13] Writing result.
[05.06.2025 05:13] Renaming log file.
[05.06.2025 05:13] Renaming previous data. log.txt to ./logs/2025-06-05_last_log.txt
