[05.06.2025 03:44] Read previous papers.
[05.06.2025 03:44] Generating top page (month).
[05.06.2025 03:44] Writing top page (month).
[05.06.2025 04:20] Read previous papers.
[05.06.2025 04:20] Get feed.
[05.06.2025 04:20] Get page data from previous paper. URL: https://huggingface.co/papers/2506.03569
[05.06.2025 04:20] Get page data from previous paper. URL: https://huggingface.co/papers/2506.02921
[05.06.2025 04:20] Get page data from previous paper. URL: https://huggingface.co/papers/2506.04180
[05.06.2025 04:20] Get page data from previous paper. URL: https://huggingface.co/papers/2505.24500
[05.06.2025 04:20] Get page data from previous paper. URL: https://huggingface.co/papers/2506.04228
[05.06.2025 04:20] Get page data from previous paper. URL: https://huggingface.co/papers/2506.04158
[05.06.2025 04:20] Extract page data from URL. URL: https://huggingface.co/papers/2506.03295
[05.06.2025 04:20] Get page data from previous paper. URL: https://huggingface.co/papers/2506.02592
[05.06.2025 04:20] Get page data from previous paper. URL: https://huggingface.co/papers/2506.03099
[05.06.2025 04:20] Extract page data from URL. URL: https://huggingface.co/papers/2506.04207
[05.06.2025 04:20] Get page data from previous paper. URL: https://huggingface.co/papers/2506.04108
[05.06.2025 04:20] Get page data from previous paper. URL: https://huggingface.co/papers/2506.03106
[05.06.2025 04:20] Get page data from previous paper. URL: https://huggingface.co/papers/2506.04133
[05.06.2025 04:20] Get page data from previous paper. URL: https://huggingface.co/papers/2506.04034
[05.06.2025 04:20] Get page data from previous paper. URL: https://huggingface.co/papers/2506.03448
[05.06.2025 04:20] Get page data from previous paper. URL: https://huggingface.co/papers/2506.02945
[05.06.2025 04:20] Get page data from previous paper. URL: https://huggingface.co/papers/2506.02294
[05.06.2025 04:20] Get page data from previous paper. URL: https://huggingface.co/papers/2506.01344
[05.06.2025 04:20] Get page data from previous paper. URL: https://huggingface.co/papers/2505.23807
[05.06.2025 04:20] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21541
[05.06.2025 04:20] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[05.06.2025 04:20] No deleted papers detected.
[05.06.2025 04:20] Downloading and parsing papers (pdf, html). Total: 20.
[05.06.2025 04:20] Downloading and parsing paper https://huggingface.co/papers/2506.03569.
[05.06.2025 04:20] Extra JSON file exists (./assets/json/2506.03569.json), skip PDF parsing.
[05.06.2025 04:20] Paper image links file exists (./assets/img_data/2506.03569.json), skip HTML parsing.
[05.06.2025 04:20] Success.
[05.06.2025 04:20] Downloading and parsing paper https://huggingface.co/papers/2506.02921.
[05.06.2025 04:20] Extra JSON file exists (./assets/json/2506.02921.json), skip PDF parsing.
[05.06.2025 04:20] Paper image links file exists (./assets/img_data/2506.02921.json), skip HTML parsing.
[05.06.2025 04:20] Success.
[05.06.2025 04:20] Downloading and parsing paper https://huggingface.co/papers/2506.04180.
[05.06.2025 04:20] Extra JSON file exists (./assets/json/2506.04180.json), skip PDF parsing.
[05.06.2025 04:20] Paper image links file exists (./assets/img_data/2506.04180.json), skip HTML parsing.
[05.06.2025 04:20] Success.
[05.06.2025 04:20] Downloading and parsing paper https://huggingface.co/papers/2505.24500.
[05.06.2025 04:20] Extra JSON file exists (./assets/json/2505.24500.json), skip PDF parsing.
[05.06.2025 04:20] Paper image links file exists (./assets/img_data/2505.24500.json), skip HTML parsing.
[05.06.2025 04:20] Success.
[05.06.2025 04:20] Downloading and parsing paper https://huggingface.co/papers/2506.04228.
[05.06.2025 04:20] Extra JSON file exists (./assets/json/2506.04228.json), skip PDF parsing.
[05.06.2025 04:20] Paper image links file exists (./assets/img_data/2506.04228.json), skip HTML parsing.
[05.06.2025 04:20] Success.
[05.06.2025 04:20] Downloading and parsing paper https://huggingface.co/papers/2506.04158.
[05.06.2025 04:20] Extra JSON file exists (./assets/json/2506.04158.json), skip PDF parsing.
[05.06.2025 04:20] Paper image links file exists (./assets/img_data/2506.04158.json), skip HTML parsing.
[05.06.2025 04:20] Success.
[05.06.2025 04:20] Downloading and parsing paper https://huggingface.co/papers/2506.03295.
[05.06.2025 04:20] Downloading paper 2506.03295 from http://arxiv.org/pdf/2506.03295v1...
[05.06.2025 04:20] Extracting affiliations from text.
[05.06.2025 04:20] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Unleashing the Reasoning Potential of Pre-trained LLMs by Critique Fine-Tuning on One Problem Yubo Wang1,2, Ping Nie5, Kai Zou3, Lijun Wu4, Wenhu Chen1,2 1University of Waterloo, 2Vector Institute, 3Netmind.AI, 4Shanghai AI Lab, 5Independent 5 2 0 2 3 ] . [ 1 5 9 2 3 0 . 6 0 5 2 : r a "
[05.06.2025 04:20] Response: ```python
["University of Waterloo", "Vector Institute", "Netmind.AI", "Shanghai AI Lab", "Independent"]
```
[05.06.2025 04:20] Deleting PDF ./assets/pdf/2506.03295.pdf.
[05.06.2025 04:20] Success.
[05.06.2025 04:20] Downloading and parsing paper https://huggingface.co/papers/2506.02592.
[05.06.2025 04:20] Extra JSON file exists (./assets/json/2506.02592.json), skip PDF parsing.
[05.06.2025 04:20] Paper image links file exists (./assets/img_data/2506.02592.json), skip HTML parsing.
[05.06.2025 04:20] Success.
[05.06.2025 04:20] Downloading and parsing paper https://huggingface.co/papers/2506.03099.
[05.06.2025 04:20] Extra JSON file exists (./assets/json/2506.03099.json), skip PDF parsing.
[05.06.2025 04:20] Paper image links file exists (./assets/img_data/2506.03099.json), skip HTML parsing.
[05.06.2025 04:20] Success.
[05.06.2025 04:20] Downloading and parsing paper https://huggingface.co/papers/2506.04207.
[05.06.2025 04:20] Downloading paper 2506.04207 from http://arxiv.org/pdf/2506.04207v1...
[05.06.2025 04:20] Extracting affiliations from text.
[05.06.2025 04:20] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 4 ] . [ 1 7 0 2 4 0 . 6 0 5 2 : r Advancing Multimodal Reasoning: From Optimized Cold Start to Staged Reinforcement Learning Shuang Chen1, Yue Guo2, Zhaochen Su3 , Yafu Li4, Yulun Wu1, Jiacheng Chen4, Jiayu Chen2, Weijie Wang1, Xiaoye Qu4, Yu Cheng5 1 Zhejiang University 2 Fudan University 3 Soochow University 4 Shanghai AI Laboratory 5 The Chinese University of Hong Kong Code Page: https://github.com/CSfufu/Revisual-R1 Figure 1: Overall performance across five multimodal reasoning benchmarks (MathVerse, MathVision, DynaMath, WeMath, and LogicVista) and four textual reasoning benchmarks (AIME24, AIME25, GPQA, and MATH500). Our ReVisual-R1 achieves better performance than existing works. "
[05.06.2025 04:20] Response: ```python
[
    "Zhejiang University",
    "Fudan University",
    "Soochow University",
    "Shanghai AI Laboratory",
    "The Chinese University of Hong Kong"
]
```
[05.06.2025 04:20] Deleting PDF ./assets/pdf/2506.04207.pdf.
[05.06.2025 04:20] Success.
[05.06.2025 04:20] Downloading and parsing paper https://huggingface.co/papers/2506.04108.
[05.06.2025 04:20] Extra JSON file exists (./assets/json/2506.04108.json), skip PDF parsing.
[05.06.2025 04:20] Paper image links file exists (./assets/img_data/2506.04108.json), skip HTML parsing.
[05.06.2025 04:20] Success.
[05.06.2025 04:20] Downloading and parsing paper https://huggingface.co/papers/2506.03106.
[05.06.2025 04:20] Extra JSON file exists (./assets/json/2506.03106.json), skip PDF parsing.
[05.06.2025 04:20] Paper image links file exists (./assets/img_data/2506.03106.json), skip HTML parsing.
[05.06.2025 04:20] Success.
[05.06.2025 04:20] Downloading and parsing paper https://huggingface.co/papers/2506.04133.
[05.06.2025 04:20] Extra JSON file exists (./assets/json/2506.04133.json), skip PDF parsing.
[05.06.2025 04:20] Paper image links file exists (./assets/img_data/2506.04133.json), skip HTML parsing.
[05.06.2025 04:20] Success.
[05.06.2025 04:20] Downloading and parsing paper https://huggingface.co/papers/2506.04034.
[05.06.2025 04:20] Extra JSON file exists (./assets/json/2506.04034.json), skip PDF parsing.
[05.06.2025 04:20] Paper image links file exists (./assets/img_data/2506.04034.json), skip HTML parsing.
[05.06.2025 04:20] Success.
[05.06.2025 04:20] Downloading and parsing paper https://huggingface.co/papers/2506.03448.
[05.06.2025 04:20] Extra JSON file exists (./assets/json/2506.03448.json), skip PDF parsing.
[05.06.2025 04:20] Paper image links file exists (./assets/img_data/2506.03448.json), skip HTML parsing.
[05.06.2025 04:20] Success.
[05.06.2025 04:20] Downloading and parsing paper https://huggingface.co/papers/2506.02945.
[05.06.2025 04:20] Extra JSON file exists (./assets/json/2506.02945.json), skip PDF parsing.
[05.06.2025 04:20] Paper image links file exists (./assets/img_data/2506.02945.json), skip HTML parsing.
[05.06.2025 04:20] Success.
[05.06.2025 04:20] Downloading and parsing paper https://huggingface.co/papers/2506.02294.
[05.06.2025 04:20] Extra JSON file exists (./assets/json/2506.02294.json), skip PDF parsing.
[05.06.2025 04:20] Paper image links file exists (./assets/img_data/2506.02294.json), skip HTML parsing.
[05.06.2025 04:20] Success.
[05.06.2025 04:20] Downloading and parsing paper https://huggingface.co/papers/2506.01344.
[05.06.2025 04:20] Extra JSON file exists (./assets/json/2506.01344.json), skip PDF parsing.
[05.06.2025 04:20] Paper image links file exists (./assets/img_data/2506.01344.json), skip HTML parsing.
[05.06.2025 04:20] Success.
[05.06.2025 04:20] Downloading and parsing paper https://huggingface.co/papers/2505.23807.
[05.06.2025 04:20] Extra JSON file exists (./assets/json/2505.23807.json), skip PDF parsing.
[05.06.2025 04:20] Paper image links file exists (./assets/img_data/2505.23807.json), skip HTML parsing.
[05.06.2025 04:20] Success.
[05.06.2025 04:20] Downloading and parsing paper https://huggingface.co/papers/2505.21541.
[05.06.2025 04:20] Extra JSON file exists (./assets/json/2505.21541.json), skip PDF parsing.
[05.06.2025 04:20] Paper image links file exists (./assets/img_data/2505.21541.json), skip HTML parsing.
[05.06.2025 04:20] Success.
[05.06.2025 04:20] Enriching papers with extra data.
[05.06.2025 04:20] ********************************************************************************
[05.06.2025 04:20] Abstract 0. We open-source MiMo-VL-7B-SFT and MiMo-VL-7B-RL, two powerful vision-language models delivering state-of-the-art performance in both general visual understanding and multimodal reasoning. MiMo-VL-7B-RL outperforms Qwen2.5-VL-7B on 35 out of 40 evaluated tasks, and scores 59.4 on OlympiadBench, surpa...
[05.06.2025 04:20] ********************************************************************************
[05.06.2025 04:20] Abstract 1. LongBioBench is a new benchmark using artificially generated biographies to evaluate long-context language models across understanding, reasoning, and trustworthiness dimensions, addressing limitations in existing frameworks.  					AI-generated summary 				 Existing frameworks for evaluating long-co...
[05.06.2025 04:20] ********************************************************************************
[05.06.2025 04:20] Abstract 2. Long-form text generation remains a significant challenge for large language models (LLMs), particularly in maintaining coherence, ensuring logical consistency, and preserving text quality as sequence length increases. To address these limitations, we propose SuperWriter-Agent, an agent-based framew...
[05.06.2025 04:20] ********************************************************************************
[05.06.2025 04:20] Abstract 3. Temporal-aware Hierarchical Cognitive Reinforcement Learning enhances LLMs' social intelligence by addressing the distinct cognitive demands of social domains.  					AI-generated summary 				 Recently, Large Language Models (LLMs) have made significant progress in IQ-related domains that require car...
[05.06.2025 04:20] ********************************************************************************
[05.06.2025 04:20] Abstract 4. LayerFlow is a unified framework for generating layer-aware videos using a text-to-video diffusion transformer and layer embeddings, supporting various video generation tasks with a multi-stage training strategy.  					AI-generated summary 				 We present LayerFlow, a unified solution for layer-awar...
[05.06.2025 04:20] ********************************************************************************
[05.06.2025 04:20] Abstract 5. While diffusion models have achieved remarkable success in text-to-image generation, they encounter significant challenges with instruction-driven image editing. Our research highlights a key challenge: these models particularly struggle with structurally inconsistent edits that involve substantial ...
[05.06.2025 04:20] ********************************************************************************
[05.06.2025 04:20] Abstract 6. Critique Fine-Tuning on a single problem can efficiently enhance the reasoning capabilities of large language models with significant performance gains and reduced computational cost compared to reinforcement learning.  					AI-generated summary 				 We have witnessed that strong LLMs like Qwen-Math...
[05.06.2025 04:20] ********************************************************************************
[05.06.2025 04:20] Abstract 7. The DBG score is introduced to measure self-preference bias in large language models by using gold judgments as proxies for response quality, addressing the confounding effect of response quality.  					AI-generated summary 				 Recent studies show that large language models (LLMs) exhibit self-pref...
[05.06.2025 04:20] ********************************************************************************
[05.06.2025 04:20] Abstract 8. TalkingMachines transforms a pretrained image-to-video model into an audio-driven avatar generator, supports infinite video streaming, and uses engineering optimizations for real-time performance.  					AI-generated summary 				 In this paper, we present TalkingMachines -- an efficient framework tha...
[05.06.2025 04:20] ********************************************************************************
[05.06.2025 04:20] Abstract 9. Inspired by the remarkable reasoning capabilities of Deepseek-R1 in complex textual tasks, many works attempt to incentivize similar capabilities in Multimodal Large Language Models (MLLMs) by directly applying reinforcement learning (RL). However, they still struggle to activate complex reasoning. ...
[05.06.2025 04:20] ********************************************************************************
[05.06.2025 04:20] Abstract 10. Rectified Sparse Attention (ReSA) improves the efficiency of long-sequence generation in Large Language Models by combining block-sparse attention with periodic dense rectification, maintaining high-quality generation.  					AI-generated summary 				 Efficient long-sequence generation is a critical ...
[05.06.2025 04:20] ********************************************************************************
[05.06.2025 04:20] Abstract 11. Critique-GRPO, an RL framework combining numerical and natural language feedback, enhances LLM reasoning across tasks and outperforms existing methods.  					AI-generated summary 				 Recent advances in reinforcement learning (RL) with numerical feedback, such as scalar rewards, have significantly e...
[05.06.2025 04:20] ********************************************************************************
[05.06.2025 04:20] Abstract 12. A review of trust, risk, and security management in LLM-based agentic multi-agent systems, examining governance, explainability, ModelOps, and privacy/security.  					AI-generated summary 				 Agentic AI systems, built on large language models (LLMs) and deployed in multi-agent configurations, are r...
[05.06.2025 04:20] ********************************************************************************
[05.06.2025 04:20] Abstract 13. Object referring aims to detect all objects in an image that match a given natural language description. We argue that a robust object referring model should be grounded, meaning its predictions should be both explainable and faithful to the visual content. Specifically, it should satisfy two key pr...
[05.06.2025 04:20] ********************************************************************************
[05.06.2025 04:20] Abstract 14. RefEdit, an instruction-based editing model trained on synthetic data, outperforms baselines in complex scene editing and referring expression tasks.  					AI-generated summary 				 Despite recent advances in inversion and instruction-based image editing, existing approaches primarily excel at editi...
[05.06.2025 04:20] ********************************************************************************
[05.06.2025 04:20] Abstract 15. LLM-as-a-judge is a framework in which a large language model (LLM) automatically evaluates the output of another LLM. We propose quantitative LLM judges, which align evaluation scores of existing LLM judges to human scores in a given domain using regression models. The models are trained to improve...
[05.06.2025 04:20] ********************************************************************************
[05.06.2025 04:20] Abstract 16. A diffusion-based data augmentation strategy improves robustness in knowledge distillation by generating challenging samples, enhancing accuracy and spurious feature resilience.  					AI-generated summary 				 Large foundation models trained on extensive datasets demonstrate strong zero-shot capabil...
[05.06.2025 04:20] ********************************************************************************
[05.06.2025 04:20] Abstract 17. Flowcharts are a critical tool for visualizing decision-making processes. However, their non-linear structure and complex visual-textual relationships make it challenging to interpret them using LLMs, as vision-language models frequently hallucinate nonexistent connections and decision paths when an...
[05.06.2025 04:20] ********************************************************************************
[05.06.2025 04:20] Abstract 18. A dynamic layerwise pruning method adaptively determines layer importance by combining model weights and activation information to maintain performance in large language models at high sparsity.  					AI-generated summary 				 Pruning has recently been widely adopted to reduce the parameter scale an...
[05.06.2025 04:20] ********************************************************************************
[05.06.2025 04:20] Abstract 19. DiffDecompose, a diffusion Transformer-based framework, effectively decomposes images into constituent layers with semantic prompts, addressing challenges in transparent layer decomposition.  					AI-generated summary 				 Diffusion models have recently motivated great success in many generation tas...
[05.06.2025 04:20] Read previous papers.
[05.06.2025 04:20] Generating reviews via LLM API.
[05.06.2025 04:20] Using data from previous issue: {"categories": ["#training", "#rl", "#reasoning", "#multimodal", "#rlhf", "#benchmark", "#dataset", "#open_source"], "emoji": "ğŸ§ ", "ru": {"title": "ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ˜Ğ˜: MiMo-VL ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ñ‹", "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ´Ğ²Ğµ Ğ¼Ğ¾Ñ‰Ğ½Ñ‹Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ MiMo-VL-7B-SFT
[05.06.2025 04:20] Using data from previous issue: {"categories": ["#benchmark", "#synthetic", "#long_context", "#reasoning", "#interpretability"], "emoji": "ğŸ“Š", "ru": {"title": "LongBioBench: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼", "desc": "LongBioBench - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼, Ğ¸ÑĞ¿
[05.06.2025 04:20] Using data from previous issue: {"categories": ["#training", "#optimization", "#agents", "#long_context", "#rlhf", "#benchmark", "#dataset", "#story_generation"], "emoji": "âœï¸", "ru": {"title": "Ğ¡Ñ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²", "desc": "SuperWriter-Agent - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²
[05.06.2025 04:20] Using data from previous issue: {"categories": ["#rlhf", "#rl", "#reasoning", "#training"], "emoji": "ğŸ§ ", "ru": {"title": "ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹", "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Temporal-aware Hierarchical Cognitive Reinforcement Learning (TimeHC-RL) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¾Ñ†Ğ¸
[05.06.2025 04:20] Using data from previous issue: {"categories": ["#multimodal", "#diffusion", "#synthetic", "#video", "#training"], "emoji": "ğŸï¸", "ru": {"title": "LayerFlow: Ğ£Ğ¼Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ»Ğ¾Ğ¹Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ°Ğ¼", "desc": "LayerFlow - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ ÑĞ»Ğ¾ĞµĞ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ Ğ´Ğ¸
[05.06.2025 04:20] Using data from previous issue: {"categories": ["#diffusion", "#benchmark", "#cv", "#architecture"], "emoji": "ğŸ–¼ï¸", "ru": {"title": "Ğ ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ ĞºĞ°Ğº Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ˜Ğ˜-Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°", "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ÑĞºÑƒÑ
[05.06.2025 04:20] Querying the API.
[05.06.2025 04:20] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Critique Fine-Tuning on a single problem can efficiently enhance the reasoning capabilities of large language models with significant performance gains and reduced computational cost compared to reinforcement learning.  					AI-generated summary 				 We have witnessed that strong LLMs like Qwen-Math, MiMo, and Phi-4 possess immense reasoning potential inherited from the pre-training stage. With reinforcement learning (RL), these models can improve dramatically on reasoning tasks. Recent studies have shown that even RL on a single problem can unleash these models' reasoning capabilities. However, RL is not only expensive but also unstable. Even one-shot RL requires hundreds of GPU hours. This raises a critical question: Is there a more efficient way to unleash the reasoning potential of these powerful base LLMs? In this work, we demonstrate that Critique Fine-Tuning (CFT) on only one problem can effectively unleash the reasoning potential of LLMs. Our method constructs critique data by collecting diverse model-generated solutions to a single problem and using teacher LLMs to provide detailed critiques. We fine-tune Qwen and Llama family models, ranging from 1.5B to 14B parameters, on the CFT data and observe significant performance gains across diverse reasoning tasks. For example, with just 5 GPU hours of training, Qwen-Math-7B-CFT show an average improvement of 15% on six math benchmarks and 16% on three logic reasoning benchmarks. These results are comparable to or even surpass the results from RL with 20x less compute. Ablation studies reveal the robustness of one-shot CFT across different prompt problems. These results highlight one-shot CFT as a simple, general, and compute-efficient approach to unleashing the reasoning capabilities of modern LLMs.
[05.06.2025 04:20] Response: {
  "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Critique Fine-Tuning (CFT) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). CFT Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ñ‚Ğ·Ñ‹Ğ²Ğ°Ñ… Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸ÑÑ… Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ-ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ CFT Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ñ… Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ CFT ĞºĞ°Ğº Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¸ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğº Ñ€Ğ°ÑĞºÑ€Ñ‹Ñ‚Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»Ğ° ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… LLM.",
  "emoji": "ğŸ§ ",
  "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°ÑĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»Ğ° Ğ˜Ğ˜ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞµ"
}
[05.06.2025 04:20] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Critique Fine-Tuning on a single problem can efficiently enhance the reasoning capabilities of large language models with significant performance gains and reduced computational cost compared to reinforcement learning.  					AI-generated summary 				 We have witnessed that strong LLMs like Qwen-Math, MiMo, and Phi-4 possess immense reasoning potential inherited from the pre-training stage. With reinforcement learning (RL), these models can improve dramatically on reasoning tasks. Recent studies have shown that even RL on a single problem can unleash these models' reasoning capabilities. However, RL is not only expensive but also unstable. Even one-shot RL requires hundreds of GPU hours. This raises a critical question: Is there a more efficient way to unleash the reasoning potential of these powerful base LLMs? In this work, we demonstrate that Critique Fine-Tuning (CFT) on only one problem can effectively unleash the reasoning potential of LLMs. Our method constructs critique data by collecting diverse model-generated solutions to a single problem and using teacher LLMs to provide detailed critiques. We fine-tune Qwen and Llama family models, ranging from 1.5B to 14B parameters, on the CFT data and observe significant performance gains across diverse reasoning tasks. For example, with just 5 GPU hours of training, Qwen-Math-7B-CFT show an average improvement of 15% on six math benchmarks and 16% on three logic reasoning benchmarks. These results are comparable to or even surpass the results from RL with 20x less compute. Ablation studies reveal the robustness of one-shot CFT across different prompt problems. These results highlight one-shot CFT as a simple, general, and compute-efficient approach to unleashing the reasoning capabilities of modern LLMs."

[05.06.2025 04:20] Response: ```python
['TRAINING', 'RL']
```
[05.06.2025 04:20] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Critique Fine-Tuning on a single problem can efficiently enhance the reasoning capabilities of large language models with significant performance gains and reduced computational cost compared to reinforcement learning.  					AI-generated summary 				 We have witnessed that strong LLMs like Qwen-Math, MiMo, and Phi-4 possess immense reasoning potential inherited from the pre-training stage. With reinforcement learning (RL), these models can improve dramatically on reasoning tasks. Recent studies have shown that even RL on a single problem can unleash these models' reasoning capabilities. However, RL is not only expensive but also unstable. Even one-shot RL requires hundreds of GPU hours. This raises a critical question: Is there a more efficient way to unleash the reasoning potential of these powerful base LLMs? In this work, we demonstrate that Critique Fine-Tuning (CFT) on only one problem can effectively unleash the reasoning potential of LLMs. Our method constructs critique data by collecting diverse model-generated solutions to a single problem and using teacher LLMs to provide detailed critiques. We fine-tune Qwen and Llama family models, ranging from 1.5B to 14B parameters, on the CFT data and observe significant performance gains across diverse reasoning tasks. For example, with just 5 GPU hours of training, Qwen-Math-7B-CFT show an average improvement of 15% on six math benchmarks and 16% on three logic reasoning benchmarks. These results are comparable to or even surpass the results from RL with 20x less compute. Ablation studies reveal the robustness of one-shot CFT across different prompt problems. These results highlight one-shot CFT as a simple, general, and compute-efficient approach to unleashing the reasoning capabilities of modern LLMs."

[05.06.2025 04:20] Response: ```python
["REASONING", "OPTIMIZATION"]
```
[05.06.2025 04:20] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces Critique Fine-Tuning (CFT) as a method to enhance the reasoning abilities of large language models (LLMs) like Qwen and Llama. By focusing on a single problem, CFT generates critique data from various model-generated solutions, which are then used to fine-tune the models. The results show that this approach leads to significant performance improvements on reasoning tasks with much lower computational costs compared to traditional reinforcement learning methods. The findings suggest that CFT is a robust and efficient strategy for maximizing the reasoning potential of LLMs.","title":"Unlocking Reasoning Power with Efficient Critique Fine-Tuning"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper introduces Critique Fine-Tuning (CFT) as a method to enhance the reasoning abilities of large language models (LLMs) like Qwen and Llama. By focusing on a single problem, CFT generates critique data from various model-generated solutions, which are then used to fine-tune the models. The results show that this approach leads to significant performance improvements on reasoning tasks with much lower computational costs compared to traditional reinforcement learning methods. The findings suggest that CFT is a robust and efficient strategy for maximizing the reasoning potential of LLMs.', title='Unlocking Reasoning Power with Efficient Critique Fine-Tuning'))
[05.06.2025 04:20] Response: ParsedChatCompletionMessage[Article](content='{"desc":"æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºæ‰¹è¯„å¾®è°ƒï¼ˆCritique Fine-Tuning, CFTï¼‰çš„æ–¹æ³•ï¼Œæ—¨åœ¨é«˜æ•ˆæå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡å¯¹å•ä¸€é—®é¢˜è¿›è¡Œå¾®è°ƒï¼ŒCFTèƒ½å¤Ÿæ˜¾è‘—æé«˜æ¨¡å‹åœ¨æ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼ŒåŒæ—¶å‡å°‘è®¡ç®—æˆæœ¬ã€‚ç ”ç©¶è¡¨æ˜ï¼Œä½¿ç”¨CFTæ–¹æ³•ï¼Œæ¨¡å‹åœ¨å¤šä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸­å¹³å‡æå‡äº†15%åˆ°16%ã€‚ä¸ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼ŒCFTåœ¨è®¡ç®—èµ„æºä¸Šæ›´åŠ é«˜æ•ˆï¼Œå±•ç¤ºäº†å…¶ä½œä¸ºä¸€ç§ç®€å•ä¸”é€šç”¨çš„æ¨ç†èƒ½åŠ›æå‡ç­–ç•¥çš„æ½œåŠ›ã€‚","title":"æ‰¹è¯„å¾®è°ƒï¼šé«˜æ•ˆé‡Šæ”¾è¯­è¨€æ¨¡å‹æ¨ç†æ½œåŠ›çš„åˆ©å™¨"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºæ‰¹è¯„å¾®è°ƒï¼ˆCritique Fine-Tuning, CFTï¼‰çš„æ–¹æ³•ï¼Œæ—¨åœ¨é«˜æ•ˆæå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡å¯¹å•ä¸€é—®é¢˜è¿›è¡Œå¾®è°ƒï¼ŒCFTèƒ½å¤Ÿæ˜¾è‘—æé«˜æ¨¡å‹åœ¨æ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼ŒåŒæ—¶å‡å°‘è®¡ç®—æˆæœ¬ã€‚ç ”ç©¶è¡¨æ˜ï¼Œä½¿ç”¨CFTæ–¹æ³•ï¼Œæ¨¡å‹åœ¨å¤šä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸­å¹³å‡æå‡äº†15%åˆ°16%ã€‚ä¸ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼ŒCFTåœ¨è®¡ç®—èµ„æºä¸Šæ›´åŠ é«˜æ•ˆï¼Œå±•ç¤ºäº†å…¶ä½œä¸ºä¸€ç§ç®€å•ä¸”é€šç”¨çš„æ¨ç†èƒ½åŠ›æå‡ç­–ç•¥çš„æ½œåŠ›ã€‚', title='æ‰¹è¯„å¾®è°ƒï¼šé«˜æ•ˆé‡Šæ”¾è¯­è¨€æ¨¡å‹æ¨ç†æ½œåŠ›çš„åˆ©å™¨'))
[05.06.2025 04:20] Using data from previous issue: {"categories": ["#benchmark", "#data", "#hallucinations", "#interpretability", "#ethics", "#training"], "emoji": "âš–ï¸", "ru": {"title": "DBG: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ± Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚Ğ¸ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚Ğ¸ ÑĞ°Ğ¼Ğ¾Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ñ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾
[05.06.2025 04:20] Using data from previous issue: {"categories": ["#architecture", "#multimodal", "#inference", "#games", "#audio", "#video", "#optimization"], "emoji": "ğŸ—£ï¸", "ru": {"title": "ĞĞ¶Ğ¸Ğ²Ğ»ÑĞµĞ¼ Ğ°Ğ²Ğ°Ñ‚Ğ°Ñ€Ñ‹: Ğ°ÑƒĞ´Ğ¸Ğ¾-ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸", "desc": "TalkingMachines - ÑÑ‚Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒÑÑ‰Ğ°Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»
[05.06.2025 04:20] Querying the API.
[05.06.2025 04:20] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Inspired by the remarkable reasoning capabilities of Deepseek-R1 in complex textual tasks, many works attempt to incentivize similar capabilities in Multimodal Large Language Models (MLLMs) by directly applying reinforcement learning (RL). However, they still struggle to activate complex reasoning. In this paper, rather than examining multimodal RL in isolation, we delve into current training pipelines and identify three crucial phenomena: 1) Effective cold start initialization is critical for enhancing MLLM reasoning. Intriguingly, we find that initializing with carefully selected text data alone can lead to performance surpassing many recent multimodal reasoning models, even before multimodal RL. 2) Standard GRPO applied to multimodal RL suffers from gradient stagnation, which degrades training stability and performance. 3) Subsequent text-only RL training, following the multimodal RL phase, further enhances multimodal reasoning. This staged training approach effectively balances perceptual grounding and cognitive reasoning development. By incorporating the above insights and addressing multimodal RL issues, we introduce ReVisual-R1, achieving a new state-of-the-art among open-source 7B MLLMs on challenging benchmarks including MathVerse, MathVision, WeMath, LogicVista, DynaMath, and challenging AIME2024 and AIME2025.
[05.06.2025 04:20] Response: {
  "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ñ‚Ñ€Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ñ„ĞµĞ½Ğ¾Ğ¼ĞµĞ½Ğ° Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ: Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ÑÑ‚Ğ°Ğ³Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğ¼ GRPO Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ RL. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¸Ñ… Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğ¹ Ğ±Ñ‹Ğ»Ğ° Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ReVisual-R1, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³ÑˆĞ°Ñ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ÑÑ€ĞµĞ´Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… 7B MLLM Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ….",
  "emoji": "ğŸ§ ",
  "title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ MLLM: Ğ¾Ñ‚ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğ³Ğ¾ RL"
}
[05.06.2025 04:20] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Inspired by the remarkable reasoning capabilities of Deepseek-R1 in complex textual tasks, many works attempt to incentivize similar capabilities in Multimodal Large Language Models (MLLMs) by directly applying reinforcement learning (RL). However, they still struggle to activate complex reasoning. In this paper, rather than examining multimodal RL in isolation, we delve into current training pipelines and identify three crucial phenomena: 1) Effective cold start initialization is critical for enhancing MLLM reasoning. Intriguingly, we find that initializing with carefully selected text data alone can lead to performance surpassing many recent multimodal reasoning models, even before multimodal RL. 2) Standard GRPO applied to multimodal RL suffers from gradient stagnation, which degrades training stability and performance. 3) Subsequent text-only RL training, following the multimodal RL phase, further enhances multimodal reasoning. This staged training approach effectively balances perceptual grounding and cognitive reasoning development. By incorporating the above insights and addressing multimodal RL issues, we introduce ReVisual-R1, achieving a new state-of-the-art among open-source 7B MLLMs on challenging benchmarks including MathVerse, MathVision, WeMath, LogicVista, DynaMath, and challenging AIME2024 and AIME2025."

[05.06.2025 04:20] Response: ```python
['RL', 'TRAINING', 'MULTIMODAL', 'BENCHMARK']
```
[05.06.2025 04:20] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Inspired by the remarkable reasoning capabilities of Deepseek-R1 in complex textual tasks, many works attempt to incentivize similar capabilities in Multimodal Large Language Models (MLLMs) by directly applying reinforcement learning (RL). However, they still struggle to activate complex reasoning. In this paper, rather than examining multimodal RL in isolation, we delve into current training pipelines and identify three crucial phenomena: 1) Effective cold start initialization is critical for enhancing MLLM reasoning. Intriguingly, we find that initializing with carefully selected text data alone can lead to performance surpassing many recent multimodal reasoning models, even before multimodal RL. 2) Standard GRPO applied to multimodal RL suffers from gradient stagnation, which degrades training stability and performance. 3) Subsequent text-only RL training, following the multimodal RL phase, further enhances multimodal reasoning. This staged training approach effectively balances perceptual grounding and cognitive reasoning development. By incorporating the above insights and addressing multimodal RL issues, we introduce ReVisual-R1, achieving a new state-of-the-art among open-source 7B MLLMs on challenging benchmarks including MathVerse, MathVision, WeMath, LogicVista, DynaMath, and challenging AIME2024 and AIME2025."

[05.06.2025 04:20] Response: ```python
["REASONING", "OPEN_SOURCE"]
```
[05.06.2025 04:20] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper explores how to improve reasoning in Multimodal Large Language Models (MLLMs) by analyzing their training processes. It identifies that starting with well-chosen text data can significantly boost reasoning capabilities, even before applying multimodal reinforcement learning (RL). The authors also highlight that traditional gradient-based methods in multimodal RL can lead to stagnation, negatively impacting training effectiveness. By implementing a staged training approach that combines text-only RL after multimodal RL, they introduce ReVisual-R1, which sets new performance records on various complex benchmarks.","title":"Unlocking Reasoning in MLLMs with Smart Training Strategies"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper explores how to improve reasoning in Multimodal Large Language Models (MLLMs) by analyzing their training processes. It identifies that starting with well-chosen text data can significantly boost reasoning capabilities, even before applying multimodal reinforcement learning (RL). The authors also highlight that traditional gradient-based methods in multimodal RL can lead to stagnation, negatively impacting training effectiveness. By implementing a staged training approach that combines text-only RL after multimodal RL, they introduce ReVisual-R1, which sets new performance records on various complex benchmarks.', title='Unlocking Reasoning in MLLMs with Smart Training Strategies'))
[05.06.2025 04:21] Response: ParsedChatCompletionMessage[Article](content='{"desc":"æœ¬æ–‡æ¢è®¨äº†å¦‚ä½•é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æå‡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚ç ”ç©¶å‘ç°ï¼Œè‰¯å¥½çš„å†·å¯åŠ¨åˆå§‹åŒ–å¯¹äºå¢å¼ºMLLMçš„æ¨ç†è‡³å…³é‡è¦ï¼Œå•ç‹¬ä½¿ç”¨ç²¾å¿ƒé€‰æ‹©çš„æ–‡æœ¬æ•°æ®å³å¯è¶…è¶Šè®¸å¤šè¿‘æœŸçš„å¤šæ¨¡æ€æ¨ç†æ¨¡å‹ã€‚æ ‡å‡†çš„GRPOåœ¨å¤šæ¨¡æ€RLä¸­å­˜åœ¨æ¢¯åº¦åœæ»çš„é—®é¢˜ï¼Œå½±å“äº†è®­ç»ƒçš„ç¨³å®šæ€§å’Œæ€§èƒ½ã€‚é€šè¿‡åˆ†é˜¶æ®µçš„è®­ç»ƒæ–¹æ³•ï¼Œç»“åˆæ–‡æœ¬å’Œå¤šæ¨¡æ€RLï¼Œæå‡ºäº†ReVisual-R1ï¼Œè¾¾åˆ°äº†å¼€æº7B MLLMåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­çš„æ–°çŠ¶æ€ã€‚","title":"æå‡å¤šæ¨¡æ€æ¨ç†çš„æ–°æ–¹æ³•"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='æœ¬æ–‡æ¢è®¨äº†å¦‚ä½•é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æå‡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚ç ”ç©¶å‘ç°ï¼Œè‰¯å¥½çš„å†·å¯åŠ¨åˆå§‹åŒ–å¯¹äºå¢å¼ºMLLMçš„æ¨ç†è‡³å…³é‡è¦ï¼Œå•ç‹¬ä½¿ç”¨ç²¾å¿ƒé€‰æ‹©çš„æ–‡æœ¬æ•°æ®å³å¯è¶…è¶Šè®¸å¤šè¿‘æœŸçš„å¤šæ¨¡æ€æ¨ç†æ¨¡å‹ã€‚æ ‡å‡†çš„GRPOåœ¨å¤šæ¨¡æ€RLä¸­å­˜åœ¨æ¢¯åº¦åœæ»çš„é—®é¢˜ï¼Œå½±å“äº†è®­ç»ƒçš„ç¨³å®šæ€§å’Œæ€§èƒ½ã€‚é€šè¿‡åˆ†é˜¶æ®µçš„è®­ç»ƒæ–¹æ³•ï¼Œç»“åˆæ–‡æœ¬å’Œå¤šæ¨¡æ€RLï¼Œæå‡ºäº†ReVisual-R1ï¼Œè¾¾åˆ°äº†å¼€æº7B MLLMåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­çš„æ–°çŠ¶æ€ã€‚', title='æå‡å¤šæ¨¡æ€æ¨ç†çš„æ–°æ–¹æ³•'))
[05.06.2025 04:21] Using data from previous issue: {"categories": ["#architecture", "#inference", "#long_context", "#optimization", "#training"], "emoji": "ğŸš€", "ru": {"title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°", "desc": "ĞœĞµÑ‚Ğ¾Ğ´ Rectified Sparse Attention (ReSA) ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ² Ğ±Ğ¾
[05.06.2025 04:21] Using data from previous issue: {"categories": ["#math", "#rl", "#reasoning", "#optimization", "#rlhf", "#training"], "emoji": "ğŸ§ ", "ru": {"title": "Critique-GRPO: Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ˜Ğ˜ Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Critique-GRPO - Ğ½Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶
[05.06.2025 04:21] Using data from previous issue: {"categories": ["#training", "#architecture", "#survey", "#agents", "#multimodal", "#security", "#alignment", "#benchmark", "#interpretability"], "emoji": "ğŸ¤–", "ru": {"title": "Ğ‘ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ´Ğ¾Ğ²ĞµÑ€Ğ¸Ğµ Ğ² ÑĞ¿Ğ¾Ñ…Ñƒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ¾Ğ²ĞµÑ€Ğ¸ĞµĞ¼, Ñ€Ğ¸
[05.06.2025 04:21] Using data from previous issue: {"categories": ["#cv", "#rl", "#training", "#reasoning", "#hallucinations", "#interpretability", "#dataset"], "emoji": "ğŸ”", "ru": {"title": "Ğ˜Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ½Ğ¾Ğµ Ñ€ĞµÑ„ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµÑ„ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½
[05.06.2025 04:21] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#synthetic", "#cv", "#optimization", "#open_source"], "emoji": "ğŸ–¼ï¸", "ru": {"title": "RefEdit: ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜", "desc": "RefEdit - ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞº
[05.06.2025 04:21] Using data from previous issue: {"categories": ["#training", "#optimization", "#alignment", "#rlhf", "#dataset"], "emoji": "âš–ï¸", "ru": {"title": "LLM-ÑÑƒĞ´ÑŒĞ¸: Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¸", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº LLM-as-a-judge, Ğ³Ğ´Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´Ñ€ÑƒĞ³Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´
[05.06.2025 04:21] Using data from previous issue: {"categories": ["#data", "#transfer_learning", "#training", "#optimization", "#diffusion", "#dataset"], "emoji": "ğŸ§ ", "ru": {"title": "ĞŸĞ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ²", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ¾
[05.06.2025 04:21] Using data from previous issue: {"categories": ["#graphs", "#cv", "#reasoning", "#agents", "#hallucinations", "#multimodal", "#benchmark", "#interpretability"], "emoji": "ğŸ”€", "ru": {"title": "Ğ¢Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ñ Ğ±Ğ»Ğ¾Ğº-ÑÑ…ĞµĞ¼ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ†Ğ¸Ğ¸ Ğ±Ğ»Ğ¾Ğº-ÑÑ…ĞµĞ¼ Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚
[05.06.2025 04:21] Using data from previous issue: {"categories": ["#optimization", "#inference", "#training"], "emoji": "âœ‚ï¸", "ru": {"title": "Ğ£Ğ¼Ğ½Ğ°Ñ Ğ¾Ğ±Ñ€ĞµĞ·ĞºĞ° ÑĞ»Ğ¾ĞµĞ² Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹", "desc": "ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ğ¾ÑĞ»Ğ¾Ğ¹Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€ĞµĞ·ĞºĞ¸ (DLP) Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. DLP Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ÑĞ»Ğ¾Ñ, ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½
[05.06.2025 04:21] Using data from previous issue: {"categories": ["#diffusion", "#open_source", "#cv", "#dataset"], "emoji": "ğŸ–¼ï¸", "ru": {"title": "Ğ£Ğ¼Ğ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° ÑĞ»Ğ¾Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜", "desc": "DiffDecompose - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ° Ğ´Ğ»Ñ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ»Ğ¾Ğ¸. ĞĞ½Ğ° Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼
[05.06.2025 04:21] Loading Chinese text from previous data.
[05.06.2025 04:21] Renaming data file.
[05.06.2025 04:21] Renaming previous data. hf_papers.json to ./d/2025-06-05.json
[05.06.2025 04:21] Saving new data file.
[05.06.2025 04:21] Generating page.
[05.06.2025 04:21] Renaming previous page.
[05.06.2025 04:21] Renaming previous data. index.html to ./d/2025-06-05.html
[05.06.2025 04:21] [Experimental] Generating Chinese page for reading.
[05.06.2025 04:21] Chinese vocab [{'word': 'å¤šæ¨¡æ€', 'pinyin': 'duÅ mÃ³ tÃ i', 'trans': 'multimodal'}, {'word': 'åŸºå‡†', 'pinyin': 'jÄ« zhÇ”n', 'trans': 'benchmark'}, {'word': 'è§†è§‰è¯­è¨€æ¨¡å‹', 'pinyin': 'shÃ¬ juÃ© yÇ” yÃ¡n mÃ³ xÃ­ng', 'trans': 'vision-language model'}, {'word': 'å¤æ‚', 'pinyin': 'fÃ¹ zÃ¡', 'trans': 'complex'}, {'word': 'å¤šæ™ºèƒ½ä½“', 'pinyin': 'duÅ zhÃ¬ nÃ©ng tÇ', 'trans': 'multi-agent'}, {'word': 'ç¯å¢ƒ', 'pinyin': 'huÃ¡n jÃ¬ng', 'trans': 'environment'}, {'word': 'ç­–ç•¥æ¨ç†', 'pinyin': 'cÃ¨ lÃ¼Ã¨ tuÄ« lÇ', 'trans': 'strategic reasoning'}, {'word': 'å†³ç­–', 'pinyin': 'juÃ© cÃ¨', 'trans': 'decision-making'}, {'word': 'èƒ½åŠ›', 'pinyin': 'nÃ©ng lÃ¬', 'trans': 'ability'}, {'word': 'å±€é™äº', 'pinyin': 'jÃº xiÃ n yÃº', 'trans': 'limited to'}, {'word': 'å•æ™ºèƒ½ä½“', 'pinyin': 'dÄn zhÃ¬ nÃ©ng tÇ', 'trans': 'single-agent'}, {'word': 'ä»…', 'pinyin': 'jÇn', 'trans': 'only'}, {'word': 'æ–‡æœ¬', 'pinyin': 'wÃ©n bÄ›n', 'trans': 'text'}, {'word': 'æ¶µç›–', 'pinyin': 'hÃ¡n gÃ i', 'trans': 'cover'}, {'word': 'åˆä½œ', 'pinyin': 'hÃ© zuÃ²', 'trans': 'cooperation'}, {'word': 'ç«äº‰', 'pinyin': 'jÃ¬ng zhÄ“ng', 'trans': 'competition'}, {'word': 'æ··åˆ', 'pinyin': 'hÃ¹n hÃ©', 'trans': 'hybrid'}, {'word': 'åŠ¨æœº', 'pinyin': 'dÃ²ng jÄ«', 'trans': 'motivation'}, {'word': 'äº’åŠ¨', 'pinyin': 'hÃ¹ dÃ²ng', 'trans': 'interaction'}, {'word': 'ç ”ç©¶', 'pinyin': 'yÃ¡n jiÅ«', 'trans': 'research'}, {'word': 'å‘ç°', 'pinyin': 'fÄ xiÃ n', 'trans': 'find'}, {'word': 'é¢„æµ‹', 'pinyin': 'yÃ¹ cÃ¨', 'trans': 'prediction'}, {'word': 'å‡†ç¡®æ€§', 'pinyin': 'zhÇ”n quÃ¨ xÃ¬ng', 'trans': 'accuracy'}, {'word': 'å½’ä¸€åŒ–', 'pinyin': 'guÄ« yÄ« huÃ ', 'trans': 'normalization'}, {'word': 'å›æŠ¥', 'pinyin': 'huÃ­ bÃ o', 'trans': 'reward'}, {'word': 'æ–¹é¢', 'pinyin': 'fÄng miÃ n', 'trans': 'aspect'}, {'word': 'å­˜åœ¨', 'pinyin': 'cÃºn zÃ i', 'trans': 'exist'}, {'word': 'æ˜¾è‘—', 'pinyin': 'xiÇn zhÃ¹', 'trans': 'significant'}, {'word': 'å·®è·', 'pinyin': 'chÄ jÃ¹', 'trans': 'gap'}, {'word': 'æ—¨åœ¨', 'pinyin': 'zhÇ zÃ i', 'trans': 'aim to'}, {'word': 'æ ‡å‡†åŒ–', 'pinyin': 'biÄo zhÇ”n huÃ ', 'trans': 'standardize'}, {'word': 'è¯„ä¼°', 'pinyin': 'pÃ­ng gÅ«', 'trans': 'evaluate'}, {'word': 'æŒ‡å‡º', 'pinyin': 'zhÇ chÅ«', 'trans': 'point out'}, {'word': 'å±€é™æ€§', 'pinyin': 'jÃº xiÃ n xÃ¬ng', 'trans': 'limitation'}, {'word': 'æ¨åŠ¨', 'pinyin': 'tuÄ« dÃ²ng', 'trans': 'promote'}, {'word': 'æœªæ¥', 'pinyin': 'wÃ¨i lÃ¡i', 'trans': 'future'}, {'word': 'ä»£ç ', 'pinyin': 'dÃ i mÇ', 'trans': 'code'}, {'word': 'æ•°æ®', 'pinyin': 'shÃ¹ jÃ¹', 'trans': 'data'}, {'word': 'è·å–', 'pinyin': 'huÃ² qÇ”', 'trans': 'obtain'}]
[05.06.2025 04:21] Renaming previous Chinese page.
[05.06.2025 04:21] Renaming previous data. zh.html to ./d/2025-06-04_zh_reading_task.html
[05.06.2025 04:21] Writing Chinese reading task.
[05.06.2025 04:21] Writing result.
[05.06.2025 04:21] Renaming log file.
[05.06.2025 04:21] Renaming previous data. log.txt to ./logs/2025-06-05_last_log.txt
