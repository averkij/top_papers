[05.06.2025 08:17] Read previous papers.
[05.06.2025 08:17] Generating top page (month).
[05.06.2025 08:17] Writing top page (month).
[05.06.2025 09:13] Read previous papers.
[05.06.2025 09:13] Get feed.
[05.06.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2506.03569
[05.06.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2506.04207
[05.06.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2506.02921
[05.06.2025 09:13] Extract page data from URL. URL: https://huggingface.co/papers/2506.04089
[05.06.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.16968
[05.06.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2506.04180
[05.06.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2506.01320
[05.06.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2506.04228
[05.06.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2506.04225
[05.06.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2506.03139
[05.06.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.24500
[05.06.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2506.03295
[05.06.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2506.04158
[05.06.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2506.04141
[05.06.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2506.04142
[05.06.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2506.03930
[05.06.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2506.03517
[05.06.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2506.03106
[05.06.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2506.03099
[05.06.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2506.02592
[05.06.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2506.04108
[05.06.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2506.03956
[05.06.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2506.02945
[05.06.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21541
[05.06.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2506.03448
[05.06.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2506.00482
[05.06.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.23807
[05.06.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2506.04133
[05.06.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2506.04034
[05.06.2025 09:13] Extract page data from URL. URL: https://huggingface.co/papers/2506.03951
[05.06.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2506.03614
[05.06.2025 09:13] Extract page data from URL. URL: https://huggingface.co/papers/2506.03538
[05.06.2025 09:13] Extract page data from URL. URL: https://huggingface.co/papers/2506.03355
[05.06.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2506.02294
[05.06.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2506.01344
[05.06.2025 09:13] Extract page data from URL. URL: https://huggingface.co/papers/2506.03817
[05.06.2025 09:13] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[05.06.2025 09:13] No deleted papers detected.
[05.06.2025 09:13] Downloading and parsing papers (pdf, html). Total: 36.
[05.06.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2506.03569.
[05.06.2025 09:13] Extra JSON file exists (./assets/json/2506.03569.json), skip PDF parsing.
[05.06.2025 09:13] Paper image links file exists (./assets/img_data/2506.03569.json), skip HTML parsing.
[05.06.2025 09:13] Success.
[05.06.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2506.04207.
[05.06.2025 09:13] Extra JSON file exists (./assets/json/2506.04207.json), skip PDF parsing.
[05.06.2025 09:13] Paper image links file exists (./assets/img_data/2506.04207.json), skip HTML parsing.
[05.06.2025 09:13] Success.
[05.06.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2506.02921.
[05.06.2025 09:13] Extra JSON file exists (./assets/json/2506.02921.json), skip PDF parsing.
[05.06.2025 09:13] Paper image links file exists (./assets/img_data/2506.02921.json), skip HTML parsing.
[05.06.2025 09:13] Success.
[05.06.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2506.04089.
[05.06.2025 09:13] Downloading paper 2506.04089 from http://arxiv.org/pdf/2506.04089v1...
[05.06.2025 09:13] Extracting affiliations from text.
[05.06.2025 09:13] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"AmbiK: Dataset of Ambiguous Tasks in Kitchen Environment Anastasiia Ivanova1,2, Eva Bakaeva2, Zoya Volovikova2,3, Alexey K. Kovalev3,2, Aleksandr I. Panov2,3 1LMU, Munich, Germany 2MIPT, Dolgoprudny, Russia 3AIRI, Moscow, Russia anastasiia.ivanova@campus.lmu.de, kovalev@airi.net 5 2 0 2 4 ] . [ 1 9 8 0 4 0 . 6 0 5 2 : r a "
[05.06.2025 09:13] Response: ```python
["LMU, Munich, Germany", "MIPT, Dolgoprudny, Russia", "AIRI, Moscow, Russia"]
```
[05.06.2025 09:13] Deleting PDF ./assets/pdf/2506.04089.pdf.
[05.06.2025 09:13] Success.
[05.06.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2505.16968.
[05.06.2025 09:13] Extra JSON file exists (./assets/json/2505.16968.json), skip PDF parsing.
[05.06.2025 09:13] Paper image links file exists (./assets/img_data/2505.16968.json), skip HTML parsing.
[05.06.2025 09:13] Success.
[05.06.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2506.04180.
[05.06.2025 09:13] Extra JSON file exists (./assets/json/2506.04180.json), skip PDF parsing.
[05.06.2025 09:13] Paper image links file exists (./assets/img_data/2506.04180.json), skip HTML parsing.
[05.06.2025 09:13] Success.
[05.06.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2506.01320.
[05.06.2025 09:13] Extra JSON file exists (./assets/json/2506.01320.json), skip PDF parsing.
[05.06.2025 09:13] Paper image links file exists (./assets/img_data/2506.01320.json), skip HTML parsing.
[05.06.2025 09:13] Success.
[05.06.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2506.04228.
[05.06.2025 09:13] Extra JSON file exists (./assets/json/2506.04228.json), skip PDF parsing.
[05.06.2025 09:13] Paper image links file exists (./assets/img_data/2506.04228.json), skip HTML parsing.
[05.06.2025 09:13] Success.
[05.06.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2506.04225.
[05.06.2025 09:13] Extra JSON file exists (./assets/json/2506.04225.json), skip PDF parsing.
[05.06.2025 09:13] Paper image links file exists (./assets/img_data/2506.04225.json), skip HTML parsing.
[05.06.2025 09:13] Success.
[05.06.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2506.03139.
[05.06.2025 09:13] Extra JSON file exists (./assets/json/2506.03139.json), skip PDF parsing.
[05.06.2025 09:13] Paper image links file exists (./assets/img_data/2506.03139.json), skip HTML parsing.
[05.06.2025 09:13] Success.
[05.06.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2505.24500.
[05.06.2025 09:13] Extra JSON file exists (./assets/json/2505.24500.json), skip PDF parsing.
[05.06.2025 09:13] Paper image links file exists (./assets/img_data/2505.24500.json), skip HTML parsing.
[05.06.2025 09:13] Success.
[05.06.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2506.03295.
[05.06.2025 09:13] Extra JSON file exists (./assets/json/2506.03295.json), skip PDF parsing.
[05.06.2025 09:13] Paper image links file exists (./assets/img_data/2506.03295.json), skip HTML parsing.
[05.06.2025 09:13] Success.
[05.06.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2506.04158.
[05.06.2025 09:13] Extra JSON file exists (./assets/json/2506.04158.json), skip PDF parsing.
[05.06.2025 09:13] Paper image links file exists (./assets/img_data/2506.04158.json), skip HTML parsing.
[05.06.2025 09:13] Success.
[05.06.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2506.04141.
[05.06.2025 09:13] Extra JSON file exists (./assets/json/2506.04141.json), skip PDF parsing.
[05.06.2025 09:13] Paper image links file exists (./assets/img_data/2506.04141.json), skip HTML parsing.
[05.06.2025 09:13] Success.
[05.06.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2506.04142.
[05.06.2025 09:13] Extra JSON file exists (./assets/json/2506.04142.json), skip PDF parsing.
[05.06.2025 09:13] Paper image links file exists (./assets/img_data/2506.04142.json), skip HTML parsing.
[05.06.2025 09:13] Success.
[05.06.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2506.03930.
[05.06.2025 09:13] Extra JSON file exists (./assets/json/2506.03930.json), skip PDF parsing.
[05.06.2025 09:13] Paper image links file exists (./assets/img_data/2506.03930.json), skip HTML parsing.
[05.06.2025 09:13] Success.
[05.06.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2506.03517.
[05.06.2025 09:13] Extra JSON file exists (./assets/json/2506.03517.json), skip PDF parsing.
[05.06.2025 09:13] Paper image links file exists (./assets/img_data/2506.03517.json), skip HTML parsing.
[05.06.2025 09:13] Success.
[05.06.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2506.03106.
[05.06.2025 09:13] Extra JSON file exists (./assets/json/2506.03106.json), skip PDF parsing.
[05.06.2025 09:13] Paper image links file exists (./assets/img_data/2506.03106.json), skip HTML parsing.
[05.06.2025 09:13] Success.
[05.06.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2506.03099.
[05.06.2025 09:13] Extra JSON file exists (./assets/json/2506.03099.json), skip PDF parsing.
[05.06.2025 09:13] Paper image links file exists (./assets/img_data/2506.03099.json), skip HTML parsing.
[05.06.2025 09:13] Success.
[05.06.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2506.02592.
[05.06.2025 09:13] Extra JSON file exists (./assets/json/2506.02592.json), skip PDF parsing.
[05.06.2025 09:13] Paper image links file exists (./assets/img_data/2506.02592.json), skip HTML parsing.
[05.06.2025 09:13] Success.
[05.06.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2506.04108.
[05.06.2025 09:13] Extra JSON file exists (./assets/json/2506.04108.json), skip PDF parsing.
[05.06.2025 09:13] Paper image links file exists (./assets/img_data/2506.04108.json), skip HTML parsing.
[05.06.2025 09:13] Success.
[05.06.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2506.03956.
[05.06.2025 09:13] Extra JSON file exists (./assets/json/2506.03956.json), skip PDF parsing.
[05.06.2025 09:13] Paper image links file exists (./assets/img_data/2506.03956.json), skip HTML parsing.
[05.06.2025 09:13] Success.
[05.06.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2506.02945.
[05.06.2025 09:13] Extra JSON file exists (./assets/json/2506.02945.json), skip PDF parsing.
[05.06.2025 09:13] Paper image links file exists (./assets/img_data/2506.02945.json), skip HTML parsing.
[05.06.2025 09:13] Success.
[05.06.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2505.21541.
[05.06.2025 09:13] Extra JSON file exists (./assets/json/2505.21541.json), skip PDF parsing.
[05.06.2025 09:13] Paper image links file exists (./assets/img_data/2505.21541.json), skip HTML parsing.
[05.06.2025 09:13] Success.
[05.06.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2506.03448.
[05.06.2025 09:13] Extra JSON file exists (./assets/json/2506.03448.json), skip PDF parsing.
[05.06.2025 09:13] Paper image links file exists (./assets/img_data/2506.03448.json), skip HTML parsing.
[05.06.2025 09:13] Success.
[05.06.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2506.00482.
[05.06.2025 09:13] Extra JSON file exists (./assets/json/2506.00482.json), skip PDF parsing.
[05.06.2025 09:13] Paper image links file exists (./assets/img_data/2506.00482.json), skip HTML parsing.
[05.06.2025 09:13] Success.
[05.06.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2505.23807.
[05.06.2025 09:13] Extra JSON file exists (./assets/json/2505.23807.json), skip PDF parsing.
[05.06.2025 09:13] Paper image links file exists (./assets/img_data/2505.23807.json), skip HTML parsing.
[05.06.2025 09:13] Success.
[05.06.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2506.04133.
[05.06.2025 09:13] Extra JSON file exists (./assets/json/2506.04133.json), skip PDF parsing.
[05.06.2025 09:13] Paper image links file exists (./assets/img_data/2506.04133.json), skip HTML parsing.
[05.06.2025 09:13] Success.
[05.06.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2506.04034.
[05.06.2025 09:13] Extra JSON file exists (./assets/json/2506.04034.json), skip PDF parsing.
[05.06.2025 09:13] Paper image links file exists (./assets/img_data/2506.04034.json), skip HTML parsing.
[05.06.2025 09:13] Success.
[05.06.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2506.03951.
[05.06.2025 09:13] Downloading paper 2506.03951 from http://arxiv.org/pdf/2506.03951v1...
[05.06.2025 09:13] Extracting affiliations from text.
[05.06.2025 09:13] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Rethinking the Stability-Plasticity Trade-off in Continual Learning from an Architectural Perspective Aojun Lu 1 Hangjie Yuan 2 Tao Feng 3 Yanan Sun "
[05.06.2025 09:13] Response: []
[05.06.2025 09:13] Extracting affiliations from text.
[05.06.2025 09:13] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Rethinking the Stability-Plasticity Trade-off in Continual Learning from an Architectural Perspective Aojun Lu 1 Hangjie Yuan 2 Tao Feng 3 Yanan SunThe quest for Continual Learning (CL) seeks to empower neural networks with the ability to learn and adapt incrementally. Central to this pursuit is addressing the stability-plasticity dilemma, which involves striking balance between two conflicting objectives: preserving previously learned knowledge and acquiring new knowledge. While numerous CL methods aim to achieve this tradeoff, they often overlook the impact of network architecture on stability and plasticity, restricting the trade-off to the parameter level. In this paper, we delve into the conflict between stability and plasticity at the architectural level. We reveal that under an equal parameter constraint, deeper networks exhibit better plasticity, while wider networks are characterized by superior stability. To address this architectural-level dilemma, we introduce novel framework denoted DualArch, which serves as plug-in component for CL. This framework leverages the complementary strengths of two distinct and independent networks: one dedicated to plasticity and the other to stability. Each network is designed with specialized and lightweight architecture, tailored to its respective objective. Extensive experiments demonstrate that Dual-Arch enhances the performance of existing CL methods while being up to 87% more compact in terms of parameters. 5 2 0 2 4 ] . [ 1 1 5 9 3 0 . 6 0 5 2 : r 1. Introduction Continual Learning (CL) seeks to enable neural networks to continuously acquire and update knowledge. The primary 1College of Computer Science, Sichuan University, Chengdu, China 2College of Computer Science and Technology, Zhejiang University, Hangzhou, China 3Department of Computer Science and Technology, Tsinghua University, Beijing, China. Correspondence to: Tao Feng <fengtao.hi@gmail.com>. Proceedings of the 42 nd International Conference on Machine Learning, Vancouver, Canada. PMLR 267, 2025. Copyright 2025 by the author(s). challenge in CL is catastrophic forgetting (McCloskey & Cohen, 1989; Goodfellow et al., 2013), i.e., directly updating neural networks to learn new data causes rapid forgetting of previously acquired knowledge. To learn continually without forgetting, neural network must balance plasticity, to learn new concepts, and stability, to retain acquired knowledge. However, emphasizing stability can limit the neural networks ability to acquire new knowledge, while excessive plasticity can lead to severe forgetting, challenge known as the stability-plasticity dilemma (Grossberg, 2013). To enhance CL, most of the research efforts (Li & Hoiem, 2017; Henning et al., 2021; Feng et al., 2022) are centered on developing novel learning methods that achieve better trade-off between stability and plasticity. These methods involve adding loss terms that prevent the model from changing, replaying past data, or explicitly using distinct parts of the network for different tasks, etc (Wang et al., 2024a). In particular, architecture-based methods have achieved great success across various CL scenarios (Rusu et al., 2016; Rosenfeld & Tsotsos, 2018; Wang et al., 2023). Characteristically, this type of method introduces an extra part of the network that is solely trained on the current data, which is then integrated with other parts that have been continuously trained on the previous data (Yan et al., 2021; Zhou et al., 2023b). Since new independent parameter space is used to learn the current data, these methods avoid rewriting the original parameters, thus preserving the old knowledge. In this way, the conflict between stability and plasticity at the parameter level can be significantly mitigated. While studies that focus on expanding and allocating architecture have achieved notable success, research on the basic architectures for CL is still in its infancy. This gap is crucial because, despite the ability of advanced learning methods to optimize parameters effectively, the overall CL performance remains constrained by suboptimal architectures (Lu et al., 2024). In this regard, certain pioneer works have concluded that wider and shallower networks exhibit superior overall CL performance, mainly contributing to enhanced stability (Mirzadeh et al., 2022a;b). However, theoretical analyses and practices (Simonyan & Zisserman, 2014; He et al., 2016; Liang & Srikant, 2017; Raghu et al., 2017; Zhao et al., 2024) have demonstrated that deeper networks posRethinking the Stability-Plasticity Trade-off in Continual Learning from an Architectural Perspective Figure 1: Left. (a) The average forgetting and (b) the accuracy on the new task of ResNet-18 and its wider and shallower variant. Details are presented in Sec. 3. Right. While existing research mainly optimizes weights (represented by node colors) for the stability-plasticity trade-off at the parameter level, this study proposes novel insight for extending this trade-off to the architectural level. sess enhanced representation learning ability, indicating the important role of depth in facilitating plasticity. These findings raise concern regarding whether there is an inherent conflict between stability and plasticity at the architectural level under given parameter count constraint. To investigate this, we conducted comparison between ResNet-18 (He et al., 2016) and its wider yet shallower variant, evaluating their average forgetting and accuracy on the new task. As shown in Fig. 1, ResNet-18 achieves higher accuracy on the new task, indicative of better plasticity, whereas the wider yet shallower variant exhibits lower average forgetting, indicative of greater stability. However, both networks underperform in the other aspect, which indicates there may exist stability-plasticity dilemma at the architectural level as well. Given that existing works (Zhou et al., 2023b; Lu et al., 2024) typically employ uniform architecture for both stability and plasticity, this inherent dilemma may limit CL performance, even when the architecture and parameters are finely optimized. How to balance the stability and plasticity at the architectural level? An intuitive and straightforward solution is to combine two independent models with distinct architectures: one dedicated to plasticity and the other to stability. Previous studies on CL have demonstrated that incorporating an auxiliary model, specifically trained on the current data, can "
[05.06.2025 09:13] Mistral response. {"id": "dd757e64074b4fcea515adec10c029d6", "object": "chat.completion", "created": 1749114818, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\n    \"College of Computer Science, Sichuan University, Chengdu, China\",\n    \"College of Computer Science and Technology, Zhejiang University, Hangzhou, China\",\n    \"Department of Computer Science and Technology, Tsinghua University, Beijing, China\"\n]\n```"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1568, "total_tokens": 1643, "completion_tokens": 75}}
[05.06.2025 09:13] Response: ```python
[
    "College of Computer Science, Sichuan University, Chengdu, China",
    "College of Computer Science and Technology, Zhejiang University, Hangzhou, China",
    "Department of Computer Science and Technology, Tsinghua University, Beijing, China"
]
```
[05.06.2025 09:13] Deleting PDF ./assets/pdf/2506.03951.pdf.
[05.06.2025 09:13] Success.
[05.06.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2506.03614.
[05.06.2025 09:13] Extra JSON file exists (./assets/json/2506.03614.json), skip PDF parsing.
[05.06.2025 09:13] Paper image links file exists (./assets/img_data/2506.03614.json), skip HTML parsing.
[05.06.2025 09:13] Success.
[05.06.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2506.03538.
[05.06.2025 09:13] Downloading paper 2506.03538 from http://arxiv.org/pdf/2506.03538v1...
[05.06.2025 09:13] Extracting affiliations from text.
[05.06.2025 09:13] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 4 ] . [ 1 8 3 5 3 0 . 6 0 5 2 : r Robust Neural Rendering in the Wild with Asymmetric Dual 3D Gaussian Splatting Chengqi Li Department of Computing and Software McMaster University lic222@mcmaster.ca Zhihao Shi Department of Electrical and Computer Engineering McMaster University shiz31@mcmaster.ca Yangdi Lu Department of Computing and Software McMaster University luy100@mcmaster.ca Wenbo He Department of Computing and Software McMaster University hew11@mcmaster.ca Xiangyu Xu Xian Jiaotong University xuxiangyu2014@gmail.com "
[05.06.2025 09:13] Response: ```python
["Department of Computing and Software McMaster University", "Department of Electrical and Computer Engineering McMaster University", "Xian Jiaotong University"]
```
[05.06.2025 09:13] Deleting PDF ./assets/pdf/2506.03538.pdf.
[05.06.2025 09:13] Success.
[05.06.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2506.03355.
[05.06.2025 09:13] Downloading paper 2506.03355 from http://arxiv.org/pdf/2506.03355v1...
[05.06.2025 09:13] Extracting affiliations from text.
[05.06.2025 09:13] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 3 ] . [ 1 5 5 3 3 0 . 6 0 5 2 : r Robustness in Both Domains: CLIP Needs Robust Text Encoder Elias Abad Rocamora , Christian Schlarmann , Naman Deep Singh , Yongtao Wu : LIONS - Ecole Polytechnique Federale de Lausanne, Switzerland , Matthias Hein , Volkan Cevher : Tubingen AI center, University of Tubingen, Germany {name.surname}@{epfl.ch, uni-tuebingen.de} "
[05.06.2025 09:13] Response: ```python
["LIONS - Ecole Polytechnique Federale de Lausanne, Switzerland", "Tubingen AI center, University of Tubingen, Germany"]
```
[05.06.2025 09:13] Deleting PDF ./assets/pdf/2506.03355.pdf.
[05.06.2025 09:13] Success.
[05.06.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2506.02294.
[05.06.2025 09:13] Extra JSON file exists (./assets/json/2506.02294.json), skip PDF parsing.
[05.06.2025 09:13] Paper image links file exists (./assets/img_data/2506.02294.json), skip HTML parsing.
[05.06.2025 09:13] Success.
[05.06.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2506.01344.
[05.06.2025 09:13] Extra JSON file exists (./assets/json/2506.01344.json), skip PDF parsing.
[05.06.2025 09:13] Paper image links file exists (./assets/img_data/2506.01344.json), skip HTML parsing.
[05.06.2025 09:13] Success.
[05.06.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2506.03817.
[05.06.2025 09:13] Downloading paper 2506.03817 from http://arxiv.org/pdf/2506.03817v1...
[05.06.2025 09:14] Extracting affiliations from text.
[05.06.2025 09:14] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"ARXIV PREPRINT: SURVEY OF ACTIVE LEARNING HYPERPARAMETERS 1 Survey of Active Learning Hyperparameters: Insights from Large-Scale Experimental Grid Julius Gonsior Tim Rieß Anja Reusch Claudio Hartmann Maik Thiele Wolfgang Lehner Technische Universitat Dresden, {firstname.lastname}@tu-dresden.de Technion - Israeli Institute of Technology, {firstname}@campus.technion.ac.il Hochschule fur Technik und Wirtschaft Dresden, {firstname.lastname}@htw-dresden.de 5 2 0 2 ] . [ 1 7 1 8 3 0 . 6 0 5 2 : r AbstractAnnotating data is time-consuming and costly task, but it is inherently required for supervised machine learning. Active Learning (AL) is an established method that minimizes human labeling effort by iteratively selecting the most informative unlabeled samples for expert annotation, thereby improving the overall classification performance. Even though AL has been known for decades [1], AL is still rarely used in real-world applications. As indicated in the two community web surveys among the NLP community about AL [2], [3], two main reasons continue to hold practitioners back from using AL: first, the complexity of setting AL up, and second, lack of trust in its effectiveness. We hypothesize that both reasons share the same culprit: the large hyperparameter space of AL. This mostly unexplored hyperparameter space often leads to misleading and irreproducible AL experiment results. In this study, we first compiled large hyperparameter grid of over 4.6 million hyperparameter combinations, second, recorded the performance of all combinations in the so-far biggest conducted AL study, and third, analyzed the impact of each hyperparameter in the experiment results. In the end, we give recommendations about the influence of each hyperparameter, demonstrate the surprising influence of the concrete AL strategy implementation, and outline an experimental study design for reproducible AL experiments with minimal computational effort, thus contributing to more reproducible and trustwo"
[05.06.2025 09:14] Response: ```python
[
    "Technische Universitat Dresden",
    "Technion - Israeli Institute of Technology",
    "Hochschule fur Technik und Wirtschaft Dresden"
]
```
[05.06.2025 09:14] Deleting PDF ./assets/pdf/2506.03817.pdf.
[05.06.2025 09:14] Success.
[05.06.2025 09:14] Enriching papers with extra data.
[05.06.2025 09:14] ********************************************************************************
[05.06.2025 09:14] Abstract 0. We open-source MiMo-VL-7B-SFT and MiMo-VL-7B-RL, two powerful vision-language models delivering state-of-the-art performance in both general visual understanding and multimodal reasoning. MiMo-VL-7B-RL outperforms Qwen2.5-VL-7B on 35 out of 40 evaluated tasks, and scores 59.4 on OlympiadBench, surpa...
[05.06.2025 09:14] ********************************************************************************
[05.06.2025 09:14] Abstract 1. Inspired by the remarkable reasoning capabilities of Deepseek-R1 in complex textual tasks, many works attempt to incentivize similar capabilities in Multimodal Large Language Models (MLLMs) by directly applying reinforcement learning (RL). However, they still struggle to activate complex reasoning. ...
[05.06.2025 09:14] ********************************************************************************
[05.06.2025 09:14] Abstract 2. LongBioBench is a new benchmark using artificially generated biographies to evaluate long-context language models across understanding, reasoning, and trustworthiness dimensions, addressing limitations in existing frameworks.  					AI-generated summary 				 Existing frameworks for evaluating long-co...
[05.06.2025 09:14] ********************************************************************************
[05.06.2025 09:14] Abstract 3. AmbiK, a textual dataset of ambiguous instructions for kitchen robots, enables unified comparison of ambiguity detection methods.  					AI-generated summary 				 As a part of an embodied agent, Large Language Models (LLMs) are typically used for behavior planning given natural language instructions ...
[05.06.2025 09:14] ********************************************************************************
[05.06.2025 09:14] Abstract 4. CASS is a dataset and model suite for GPU code transpilation at both source and assembly levels, achieving high accuracy and performance matching with native code.  					AI-generated summary 				 We introduce CASS, the first large-scale dataset and model suite for cross-architecture GPU code transpi...
[05.06.2025 09:14] ********************************************************************************
[05.06.2025 09:14] Abstract 5. Long-form text generation remains a significant challenge for large language models (LLMs), particularly in maintaining coherence, ensuring logical consistency, and preserving text quality as sequence length increases. To address these limitations, we propose SuperWriter-Agent, an agent-based framew...
[05.06.2025 09:14] ********************************************************************************
[05.06.2025 09:14] Abstract 6. We introduce Psi-Sampler, an SMC-based framework incorporating pCNL-based initial particle sampling for effective inference-time reward alignment with a score-based generative model. Inference-time reward alignment with score-based generative models has recently gained significant traction, followin...
[05.06.2025 09:14] ********************************************************************************
[05.06.2025 09:14] Abstract 7. LayerFlow is a unified framework for generating layer-aware videos using a text-to-video diffusion transformer and layer embeddings, supporting various video generation tasks with a multi-stage training strategy.  					AI-generated summary 				 We present LayerFlow, a unified solution for layer-awar...
[05.06.2025 09:14] ********************************************************************************
[05.06.2025 09:14] Abstract 8. Voyager is a video diffusion framework that generates world-consistent 3D point-cloud sequences from a single image, enabling long-range, consistent 3D scene exploration with user-defined camera paths.  					AI-generated summary 				 Real-world applications like video gaming and virtual reality ofte...
[05.06.2025 09:14] ********************************************************************************
[05.06.2025 09:14] Abstract 9. SVGenius evaluates Large Language Models and Multimodal LLMs for SVG processing using a comprehensive benchmark across three dimensions: understanding, editing, and generation, revealing insights into model capabilities and limitations.  					AI-generated summary 				 Large Language Models (LLMs) an...
[05.06.2025 09:14] ********************************************************************************
[05.06.2025 09:14] Abstract 10. Temporal-aware Hierarchical Cognitive Reinforcement Learning enhances LLMs' social intelligence by addressing the distinct cognitive demands of social domains.  					AI-generated summary 				 Recently, Large Language Models (LLMs) have made significant progress in IQ-related domains that require car...
[05.06.2025 09:14] ********************************************************************************
[05.06.2025 09:14] Abstract 11. Critique Fine-Tuning on a single problem can efficiently enhance the reasoning capabilities of large language models with significant performance gains and reduced computational cost compared to reinforcement learning.  					AI-generated summary 				 We have witnessed that strong LLMs like Qwen-Math...
[05.06.2025 09:14] ********************************************************************************
[05.06.2025 09:14] Abstract 12. While diffusion models have achieved remarkable success in text-to-image generation, they encounter significant challenges with instruction-driven image editing. Our research highlights a key challenge: these models particularly struggle with structurally inconsistent edits that involve substantial ...
[05.06.2025 09:14] ********************************************************************************
[05.06.2025 09:14] Abstract 13. A new benchmark, MMR-V, is proposed to challenge multimodal large language models with long-range, multi-frame reasoning and hidden information processing in videos, revealing their limitations and inspiring further research.  					AI-generated summary 				 The sequential structure of videos poses a...
[05.06.2025 09:14] ********************************************************************************
[05.06.2025 09:14] Abstract 14. A method called shortcut neuron patching identifies and suppresses shortcut neurons in language models to mitigate data contamination issues in trustworthy evaluations.  					AI-generated summary 				 The development of large language models (LLMs) depends on trustworthy evaluation. However, most cu...
[05.06.2025 09:14] ********************************************************************************
[05.06.2025 09:14] Abstract 15. VisCode-200K, a large-scale dataset for visualization, improves plot generation performance by integrating execution-grounded supervision and iterative code correction, outperforming open-source models and rivaling proprietary ones.  					AI-generated summary 				 Large language models (LLMs) often ...
[05.06.2025 09:14] ********************************************************************************
[05.06.2025 09:14] Abstract 16. Direct Preference Optimization (DPO) has recently been applied as a post-training technique for text-to-video diffusion models. To obtain training data, annotators are asked to provide preferences between two videos generated from independent noise. However, this approach prohibits fine-grained comp...
[05.06.2025 09:14] ********************************************************************************
[05.06.2025 09:14] Abstract 17. Critique-GRPO, an RL framework combining numerical and natural language feedback, enhances LLM reasoning across tasks and outperforms existing methods.  					AI-generated summary 				 Recent advances in reinforcement learning (RL) with numerical feedback, such as scalar rewards, have significantly e...
[05.06.2025 09:14] ********************************************************************************
[05.06.2025 09:14] Abstract 18. TalkingMachines transforms a pretrained image-to-video model into an audio-driven avatar generator, supports infinite video streaming, and uses engineering optimizations for real-time performance.  					AI-generated summary 				 In this paper, we present TalkingMachines -- an efficient framework tha...
[05.06.2025 09:14] ********************************************************************************
[05.06.2025 09:14] Abstract 19. The DBG score is introduced to measure self-preference bias in large language models by using gold judgments as proxies for response quality, addressing the confounding effect of response quality.  					AI-generated summary 				 Recent studies show that large language models (LLMs) exhibit self-pref...
[05.06.2025 09:14] ********************************************************************************
[05.06.2025 09:14] Abstract 20. Rectified Sparse Attention (ReSA) improves the efficiency of long-sequence generation in Large Language Models by combining block-sparse attention with periodic dense rectification, maintaining high-quality generation.  					AI-generated summary 				 Efficient long-sequence generation is a critical ...
[05.06.2025 09:14] ********************************************************************************
[05.06.2025 09:14] Abstract 21. Adapting Pre-trained Models before the core CL process (ACL) improves Continual Learning by enhancing plasticity while maintaining stability.  					AI-generated summary 				 Continual Learning (CL) seeks to enable neural networks to incrementally acquire new knowledge (plasticity) while retaining ex...
[05.06.2025 09:14] ********************************************************************************
[05.06.2025 09:14] Abstract 22. LLM-as-a-judge is a framework in which a large language model (LLM) automatically evaluates the output of another LLM. We propose quantitative LLM judges, which align evaluation scores of existing LLM judges to human scores in a given domain using regression models. The models are trained to improve...
[05.06.2025 09:14] ********************************************************************************
[05.06.2025 09:14] Abstract 23. DiffDecompose, a diffusion Transformer-based framework, effectively decomposes images into constituent layers with semantic prompts, addressing challenges in transparent layer decomposition.  					AI-generated summary 				 Diffusion models have recently motivated great success in many generation tas...
[05.06.2025 09:14] ********************************************************************************
[05.06.2025 09:14] Abstract 24. RefEdit, an instruction-based editing model trained on synthetic data, outperforms baselines in complex scene editing and referring expression tasks.  					AI-generated summary 				 Despite recent advances in inversion and instruction-based image editing, existing approaches primarily excel at editi...
[05.06.2025 09:14] ********************************************************************************
[05.06.2025 09:14] Abstract 25. BenchHub is a dynamic benchmark repository that aggregates and classifies datasets for large language models, facilitating domain-specific evaluations and improving model comparisons.  					AI-generated summary 				 As large language models (LLMs) continue to advance, the need for up-to-date and wel...
[05.06.2025 09:14] ********************************************************************************
[05.06.2025 09:14] Abstract 26. A dynamic layerwise pruning method adaptively determines layer importance by combining model weights and activation information to maintain performance in large language models at high sparsity.  					AI-generated summary 				 Pruning has recently been widely adopted to reduce the parameter scale an...
[05.06.2025 09:14] ********************************************************************************
[05.06.2025 09:14] Abstract 27. A review of trust, risk, and security management in LLM-based agentic multi-agent systems, examining governance, explainability, ModelOps, and privacy/security.  					AI-generated summary 				 Agentic AI systems, built on large language models (LLMs) and deployed in multi-agent configurations, are r...
[05.06.2025 09:14] ********************************************************************************
[05.06.2025 09:14] Abstract 28. Object referring aims to detect all objects in an image that match a given natural language description. We argue that a robust object referring model should be grounded, meaning its predictions should be both explainable and faithful to the visual content. Specifically, it should satisfy two key pr...
[05.06.2025 09:14] ********************************************************************************
[05.06.2025 09:14] Abstract 29. A novel framework, Dual-Arch, enhances Continual Learning by addressing the stability-plasticity dilemma at the architectural level using two specialized networks.  					AI-generated summary 				 The quest for Continual Learning (CL) seeks to empower neural networks with the ability to learn and ada...
[05.06.2025 09:14] ********************************************************************************
[05.06.2025 09:14] Abstract 30. VLMs exhibit visual stitching, an ability to integrate fragmented visual information, which enables harmful content to evade data moderation and be reconstructed during inference.  					AI-generated summary 				 One way to mitigate risks in vision-language models (VLMs) is to remove dangerous sample...
[05.06.2025 09:14] ********************************************************************************
[05.06.2025 09:14] Abstract 31. A novel Asymmetric Dual 3DGS framework improves 3D reconstruction by training dual models with consistency constraints and divergent masking, outperforming existing methods with high efficiency.  					AI-generated summary 				 3D reconstruction from in-the-wild images remains a challenging task due ...
[05.06.2025 09:14] ********************************************************************************
[05.06.2025 09:14] Abstract 32. LEAF, an adversarial finetuning method, enhances the robustness of CLIP text encoders, improving zero-shot accuracy and multimodal retrieval performance under adversarial noise.  					AI-generated summary 				 Adversarial input attacks can cause a significant shift of CLIP embeddings. This can affec...
[05.06.2025 09:14] ********************************************************************************
[05.06.2025 09:14] Abstract 33. A diffusion-based data augmentation strategy improves robustness in knowledge distillation by generating challenging samples, enhancing accuracy and spurious feature resilience.  					AI-generated summary 				 Large foundation models trained on extensive datasets demonstrate strong zero-shot capabil...
[05.06.2025 09:14] ********************************************************************************
[05.06.2025 09:14] Abstract 34. Flowcharts are a critical tool for visualizing decision-making processes. However, their non-linear structure and complex visual-textual relationships make it challenging to interpret them using LLMs, as vision-language models frequently hallucinate nonexistent connections and decision paths when an...
[05.06.2025 09:14] ********************************************************************************
[05.06.2025 09:14] Abstract 35. Annotating data is a time-consuming and costly task, but it is inherently required for supervised machine learning. Active Learning (AL) is an established method that minimizes human labeling effort by iteratively selecting the most informative unlabeled samples for expert annotation, thereby improv...
[05.06.2025 09:14] Read previous papers.
[05.06.2025 09:14] Generating reviews via LLM API.
[05.06.2025 09:14] Using data from previous issue: {"categories": ["#training", "#rl", "#reasoning", "#multimodal", "#rlhf", "#benchmark", "#dataset", "#open_source"], "emoji": "🧠", "ru": {"title": "Прорыв в мультимодальном ИИ: MiMo-VL устанавливает новые стандарты", "desc": "Исследователи представили две мощные мультимодальные модели MiMo-VL-7B-SFT
[05.06.2025 09:14] Using data from previous issue: {"categories": ["#reasoning", "#open_source", "#multimodal", "#training", "#benchmark", "#rl"], "emoji": "🧠", "ru": {"title": "Улучшение рассуждений MLLM: от инициализации до многоэтапного RL", "desc": "Статья посвящена улучшению способностей мультимодальных больших языковых моделей (MLLM) к рассужд
[05.06.2025 09:14] Using data from previous issue: {"categories": ["#benchmark", "#synthetic", "#long_context", "#reasoning", "#interpretability"], "emoji": "📊", "ru": {"title": "LongBioBench: Новый стандарт оценки языковых моделей с длинным контекстом", "desc": "LongBioBench - это новый бенчмарк для оценки языковых моделей с длинным контекстом, исп
[05.06.2025 09:14] Querying the API.
[05.06.2025 09:14] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

AmbiK, a textual dataset of ambiguous instructions for kitchen robots, enables unified comparison of ambiguity detection methods.  					AI-generated summary 				 As a part of an embodied agent, Large Language Models (LLMs) are typically used for behavior planning given natural language instructions from the user. However, dealing with ambiguous instructions in real-world environments remains a challenge for LLMs. Various methods for task ambiguity detection have been proposed. However, it is difficult to compare them because they are tested on different datasets and there is no universal benchmark. For this reason, we propose AmbiK (Ambiguous Tasks in Kitchen Environment), the fully textual dataset of ambiguous instructions addressed to a robot in a kitchen environment. AmbiK was collected with the assistance of LLMs and is human-validated. It comprises 1000 pairs of ambiguous tasks and their unambiguous counterparts, categorized by ambiguity type (Human Preferences, Common Sense Knowledge, Safety), with environment descriptions, clarifying questions and answers, user intents, and task plans, for a total of 2000 tasks. We hope that AmbiK will enable researchers to perform a unified comparison of ambiguity detection methods. AmbiK is available at https://github.com/cog-model/AmbiK-dataset.
[05.06.2025 09:14] Response: {
  "desc": "AmbiK - это текстовый датасет неоднозначных инструкций для кухонных роботов, созданный для унифицированного сравнения методов обнаружения неоднозначности. Датасет содержит 1000 пар неоднозначных задач и их однозначных аналогов, классифицированных по типу неоднозначности. AmbiK был собран с помощью больших языковых моделей (LLM) и проверен людьми. Он включает описания окружения, уточняющие вопросы и ответы, намерения пользователей и планы задач.",
  "emoji": "🍳",
  "title": "AmbiK: унифицированный бенчмарк для обнаружения неоднозначности в инструкциях для роботов"
}
[05.06.2025 09:14] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"AmbiK, a textual dataset of ambiguous instructions for kitchen robots, enables unified comparison of ambiguity detection methods.  					AI-generated summary 				 As a part of an embodied agent, Large Language Models (LLMs) are typically used for behavior planning given natural language instructions from the user. However, dealing with ambiguous instructions in real-world environments remains a challenge for LLMs. Various methods for task ambiguity detection have been proposed. However, it is difficult to compare them because they are tested on different datasets and there is no universal benchmark. For this reason, we propose AmbiK (Ambiguous Tasks in Kitchen Environment), the fully textual dataset of ambiguous instructions addressed to a robot in a kitchen environment. AmbiK was collected with the assistance of LLMs and is human-validated. It comprises 1000 pairs of ambiguous tasks and their unambiguous counterparts, categorized by ambiguity type (Human Preferences, Common Sense Knowledge, Safety), with environment descriptions, clarifying questions and answers, user intents, and task plans, for a total of 2000 tasks. We hope that AmbiK will enable researchers to perform a unified comparison of ambiguity detection methods. AmbiK is available at https://github.com/cog-model/AmbiK-dataset."

[05.06.2025 09:14] Response: ```python
['DATASET', 'DATA', 'AGENTS']
```
[05.06.2025 09:14] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"AmbiK, a textual dataset of ambiguous instructions for kitchen robots, enables unified comparison of ambiguity detection methods.  					AI-generated summary 				 As a part of an embodied agent, Large Language Models (LLMs) are typically used for behavior planning given natural language instructions from the user. However, dealing with ambiguous instructions in real-world environments remains a challenge for LLMs. Various methods for task ambiguity detection have been proposed. However, it is difficult to compare them because they are tested on different datasets and there is no universal benchmark. For this reason, we propose AmbiK (Ambiguous Tasks in Kitchen Environment), the fully textual dataset of ambiguous instructions addressed to a robot in a kitchen environment. AmbiK was collected with the assistance of LLMs and is human-validated. It comprises 1000 pairs of ambiguous tasks and their unambiguous counterparts, categorized by ambiguity type (Human Preferences, Common Sense Knowledge, Safety), with environment descriptions, clarifying questions and answers, user intents, and task plans, for a total of 2000 tasks. We hope that AmbiK will enable researchers to perform a unified comparison of ambiguity detection methods. AmbiK is available at https://github.com/cog-model/AmbiK-dataset."

[05.06.2025 09:14] Response: ```python
['ALIGNMENT', 'INTERPRETABILITY']
```
[05.06.2025 09:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The paper introduces AmbiK, a new dataset designed to help researchers evaluate how well different methods can detect ambiguous instructions for kitchen robots. AmbiK contains 1000 pairs of ambiguous and clear tasks, categorized by types of ambiguity such as human preferences and safety concerns. This dataset is unique because it allows for standardized testing of various ambiguity detection techniques, which have previously been difficult to compare due to differing datasets. By providing a common benchmark, AmbiK aims to advance the development of more effective language models in handling real-world instructions.","title":"AmbiK: A Unified Benchmark for Ambiguity Detection in Kitchen Robotics"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='The paper introduces AmbiK, a new dataset designed to help researchers evaluate how well different methods can detect ambiguous instructions for kitchen robots. AmbiK contains 1000 pairs of ambiguous and clear tasks, categorized by types of ambiguity such as human preferences and safety concerns. This dataset is unique because it allows for standardized testing of various ambiguity detection techniques, which have previously been difficult to compare due to differing datasets. By providing a common benchmark, AmbiK aims to advance the development of more effective language models in handling real-world instructions.', title='AmbiK: A Unified Benchmark for Ambiguity Detection in Kitchen Robotics'))
[05.06.2025 09:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"AmbiK是一个针对厨房机器人模糊指令的文本数据集，旨在统一比较模糊性检测方法。该数据集包含1000对模糊任务及其明确对应任务，涵盖人类偏好、常识知识和安全等模糊性类型。AmbiK由大型语言模型（LLMs）协助收集，并经过人工验证，提供环境描述、澄清问题及答案、用户意图和任务计划等信息。我们希望AmbiK能帮助研究人员进行模糊性检测方法的统一比较。","title":"统一比较模糊性检测方法的AmbiK数据集"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='AmbiK是一个针对厨房机器人模糊指令的文本数据集，旨在统一比较模糊性检测方法。该数据集包含1000对模糊任务及其明确对应任务，涵盖人类偏好、常识知识和安全等模糊性类型。AmbiK由大型语言模型（LLMs）协助收集，并经过人工验证，提供环境描述、澄清问题及答案、用户意图和任务计划等信息。我们希望AmbiK能帮助研究人员进行模糊性检测方法的统一比较。', title='统一比较模糊性检测方法的AmbiK数据集'))
[05.06.2025 09:14] Using data from previous issue: {"categories": ["#dataset", "#low_resource", "#benchmark", "#open_source"], "emoji": "🔄", "ru": {"title": "CASS: Преодоление барьеров между GPU-архитектурами", "desc": "CASS представляет собой набор данных и набор моделей для транспиляции GPU-кода между архитектурами как на уровне исходного кода, та
[05.06.2025 09:14] Using data from previous issue: {"categories": ["#training", "#optimization", "#agents", "#long_context", "#rlhf", "#benchmark", "#dataset", "#story_generation"], "emoji": "✍️", "ru": {"title": "Структурированное мышление для улучшения генерации длинных текстов", "desc": "SuperWriter-Agent - это новая система для улучшения качеств
[05.06.2025 09:14] Using data from previous issue: {"categories": ["#inference", "#alignment", "#rlhf", "#optimization"], "emoji": "🎨", "ru": {"title": "Эффективное согласование наград в генеративных моделях с помощью умной выборки", "desc": "Статья представляет Psi-Sampler - новый метод для улучшения согласования наград при инференсе в генеративных
[05.06.2025 09:14] Using data from previous issue: {"categories": ["#multimodal", "#diffusion", "#synthetic", "#video", "#training"], "emoji": "🎞️", "ru": {"title": "LayerFlow: Умная генерация многослойного видео по текстовым подсказкам", "desc": "LayerFlow - это унифицированная система для генерации видео с учетом слоев, использующая трансформер ди
[05.06.2025 09:14] Using data from previous issue: {"categories": ["#games", "#dataset", "#diffusion", "#3d", "#video"], "emoji": "🚀", "ru": {"title": "Исследуй 3D-миры из одного кадра", "desc": "Voyager - это система видеодиффузии, которая генерирует согласованные последовательности 3D-облаков точек из одного изображения. Она позволяет исследовать 
[05.06.2025 09:14] Using data from previous issue: {"categories": ["#reasoning", "#open_source", "#optimization", "#multimodal", "#benchmark", "#interpretability"], "emoji": "📊", "ru": {"title": "SVGenius: комплексная оценка возможностей ИИ в работе с векторной графикой", "desc": "SVGenius - это комплексный бенчмарк для оценки способностей больших я
[05.06.2025 09:14] Using data from previous issue: {"categories": ["#rlhf", "#rl", "#reasoning", "#training"], "emoji": "🧠", "ru": {"title": "Новый метод обучения для повышения социального интеллекта языковых моделей", "desc": "Исследователи представили метод Temporal-aware Hierarchical Cognitive Reinforcement Learning (TimeHC-RL) для улучшения соци
[05.06.2025 09:14] Using data from previous issue: {"categories": ["#training", "#rl", "#optimization", "#reasoning"], "emoji": "🧠", "ru": {"title": "Эффективное раскрытие потенциала ИИ через обучение на критике", "desc": "Статья представляет метод Critique Fine-Tuning (CFT) для улучшения способностей рассуждения больших языковых моделей (LLM). CFT 
[05.06.2025 09:14] Using data from previous issue: {"categories": ["#diffusion", "#benchmark", "#cv", "#architecture"], "emoji": "🖼️", "ru": {"title": "Редактирование изображений как программирование: новый подход к ИИ-обработке визуального контента", "desc": "Исследователи представили новый подход к редактированию изображений с использованием искус
[05.06.2025 09:14] Using data from previous issue: {"categories": ["#reasoning", "#multimodal", "#long_context", "#benchmark", "#video"], "emoji": "🎥", "ru": {"title": "MMR-V: Новый рубеж в мультимодальных рассуждениях по видео", "desc": "Предложен новый бенчмарк MMR-V для оценки мультимодальных языковых моделей в задачах рассуждения по видео. Он тр
[05.06.2025 09:14] Using data from previous issue: {"categories": ["#data", "#hallucinations", "#benchmark", "#ethics", "#training"], "emoji": "🧠", "ru": {"title": "Борьба с загрязнением данных в языковых моделях через патчинг нейронов-шорткатов", "desc": "Метод 'shortcut neuron patching' идентифицирует и подавляет нейроны-шорткаты в языковых моделя
[05.06.2025 09:14] Using data from previous issue: {"categories": ["#story_generation", "#data", "#dataset", "#optimization", "#training"], "emoji": "📊", "ru": {"title": "VisCode-200K: Большие данные для умного построения графиков", "desc": "VisCode-200K - это крупномасштабный набор данных для задач визуализации, который улучшает генерацию графиков 
[05.06.2025 09:14] Using data from previous issue: {"categories": ["#video", "#optimization", "#diffusion", "#rlhf"], "emoji": "🎬", "ru": {"title": "DenseDPO: Точная оптимизация предпочтений для улучшения генерации видео", "desc": "DenseDPO - это новый метод для улучшения текст-в-видео диффузионных моделей. Он решает проблему смещения в сторону клип
[05.06.2025 09:14] Using data from previous issue: {"categories": ["#math", "#rl", "#reasoning", "#optimization", "#rlhf", "#training"], "emoji": "🧠", "ru": {"title": "Critique-GRPO: Улучшение рассуждений ИИ через комбинированную обратную связь", "desc": "Статья представляет Critique-GRPO - новую систему обучения с подкреплением для улучшения рассуж
[05.06.2025 09:14] Using data from previous issue: {"categories": ["#architecture", "#multimodal", "#inference", "#games", "#audio", "#video", "#optimization"], "emoji": "🗣️", "ru": {"title": "Оживляем аватары: аудио-управляемая генерация видео в реальном времени", "desc": "TalkingMachines - это эффективная система, преобразующая предобученные модел
[05.06.2025 09:14] Using data from previous issue: {"categories": ["#benchmark", "#data", "#hallucinations", "#interpretability", "#ethics", "#training"], "emoji": "⚖️", "ru": {"title": "DBG: Новый способ измерения предвзятости в языковых моделях", "desc": "Статья представляет новый метод измерения предвзятости самопредпочтения в больших языковых мо
[05.06.2025 09:14] Using data from previous issue: {"categories": ["#architecture", "#inference", "#long_context", "#optimization", "#training"], "emoji": "🚀", "ru": {"title": "Эффективная генерация длинных текстов без потери качества", "desc": "Метод Rectified Sparse Attention (ReSA) улучшает эффективность генерации длинных последовательностей в бо
[05.06.2025 09:14] Using data from previous issue: {"categories": ["#benchmark", "#training", "#optimization"], "emoji": "🧠", "ru": {"title": "Балансировка стабильности и пластичности в непрерывном обучении", "desc": "Статья представляет новый подход к непрерывному обучению (Continual Learning) с использованием предобученных моделей. Авторы предлага
[05.06.2025 09:14] Using data from previous issue: {"categories": ["#training", "#optimization", "#alignment", "#rlhf", "#dataset"], "emoji": "⚖️", "ru": {"title": "LLM-судьи: автоматическая оценка языковых моделей с помощью регрессии", "desc": "Статья представляет фреймворк LLM-as-a-judge, где большая языковая модель оценивает результаты другой мод
[05.06.2025 09:14] Using data from previous issue: {"categories": ["#diffusion", "#open_source", "#cv", "#dataset"], "emoji": "🖼️", "ru": {"title": "Умное разделение изображений на слои с помощью ИИ", "desc": "DiffDecompose - это новая система на основе диффузионного трансформера для декомпозиции изображений на семантические слои. Она решает проблем
[05.06.2025 09:14] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#synthetic", "#cv", "#optimization", "#open_source"], "emoji": "🖼️", "ru": {"title": "RefEdit: Прорыв в редактировании сложных изображений с помощью ИИ", "desc": "RefEdit - это модель редактирования изображений на основе инструкций, обученная на синтетическ
[05.06.2025 09:14] Using data from previous issue: {"categories": ["#optimization", "#dataset", "#survey", "#benchmark"], "emoji": "📊", "ru": {"title": "BenchHub: Универсальный инструмент для оценки языковых моделей", "desc": "BenchHub - это динамическое хранилище бенчмарков, которое агрегирует и классифицирует наборы данных для больших языковых мод
[05.06.2025 09:14] Using data from previous issue: {"categories": ["#optimization", "#inference", "#training"], "emoji": "✂️", "ru": {"title": "Умная обрезка слоев для эффективных языковых моделей", "desc": "Предложен новый метод динамической послойной обрезки (DLP) для больших языковых моделей. DLP адаптивно определяет важность каждого слоя, комбин
[05.06.2025 09:14] Using data from previous issue: {"categories": ["#training", "#architecture", "#survey", "#agents", "#multimodal", "#security", "#alignment", "#benchmark", "#interpretability"], "emoji": "🤖", "ru": {"title": "Безопасность и доверие в эпоху агентного ИИ", "desc": "Статья представляет структурированный анализ управления доверием, ри
[05.06.2025 09:14] Using data from previous issue: {"categories": ["#cv", "#rl", "#training", "#reasoning", "#hallucinations", "#interpretability", "#dataset"], "emoji": "🔍", "ru": {"title": "Интерпретируемое объектное реферирование через пошаговые рассуждения", "desc": "Статья представляет новый подход к задаче объектного реферирования в компьютерн
[05.06.2025 09:14] Querying the API.
[05.06.2025 09:14] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A novel framework, Dual-Arch, enhances Continual Learning by addressing the stability-plasticity dilemma at the architectural level using two specialized networks.  					AI-generated summary 				 The quest for Continual Learning (CL) seeks to empower neural networks with the ability to learn and adapt incrementally. Central to this pursuit is addressing the stability-plasticity dilemma, which involves striking a balance between two conflicting objectives: preserving previously learned knowledge and acquiring new knowledge. While numerous CL methods aim to achieve this trade-off, they often overlook the impact of network architecture on stability and plasticity, restricting the trade-off to the parameter level. In this paper, we delve into the conflict between stability and plasticity at the architectural level. We reveal that under an equal parameter constraint, deeper networks exhibit better plasticity, while wider networks are characterized by superior stability. To address this architectural-level dilemma, we introduce a novel framework denoted Dual-Arch, which serves as a plug-in component for CL. This framework leverages the complementary strengths of two distinct and independent networks: one dedicated to plasticity and the other to stability. Each network is designed with a specialized and lightweight architecture, tailored to its respective objective. Extensive experiments demonstrate that Dual-Arch enhances the performance of existing CL methods while being up to 87% more compact in terms of parameters.
[05.06.2025 09:14] Response: {
  "desc": "Статья представляет новую архитектуру Dual-Arch для непрерывного обучения, которая решает дилемму стабильности-пластичности на архитектурном уровне. Авторы обнаружили, что при равном количестве параметров более глубокие сети обладают лучшей пластичностью, а более широкие - лучшей стабильностью. Dual-Arch использует две отдельные сети: одну для пластичности, другую для стабильности, каждая со специализированной архитектурой. Эксперименты показали, что Dual-Arch улучшает производительность существующих методов непрерывного обучения, при этом используя до 87% меньше параметров.",
  "emoji": "🧠",
  "title": "Двойная архитектура для эффективного непрерывного обучения"
}
[05.06.2025 09:14] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A novel framework, Dual-Arch, enhances Continual Learning by addressing the stability-plasticity dilemma at the architectural level using two specialized networks.  					AI-generated summary 				 The quest for Continual Learning (CL) seeks to empower neural networks with the ability to learn and adapt incrementally. Central to this pursuit is addressing the stability-plasticity dilemma, which involves striking a balance between two conflicting objectives: preserving previously learned knowledge and acquiring new knowledge. While numerous CL methods aim to achieve this trade-off, they often overlook the impact of network architecture on stability and plasticity, restricting the trade-off to the parameter level. In this paper, we delve into the conflict between stability and plasticity at the architectural level. We reveal that under an equal parameter constraint, deeper networks exhibit better plasticity, while wider networks are characterized by superior stability. To address this architectural-level dilemma, we introduce a novel framework denoted Dual-Arch, which serves as a plug-in component for CL. This framework leverages the complementary strengths of two distinct and independent networks: one dedicated to plasticity and the other to stability. Each network is designed with a specialized and lightweight architecture, tailored to its respective objective. Extensive experiments demonstrate that Dual-Arch enhances the performance of existing CL methods while being up to 87% more compact in terms of parameters."

[05.06.2025 09:14] Response: ```python
['ARCHITECTURE', 'TRAINING']
```
[05.06.2025 09:14] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A novel framework, Dual-Arch, enhances Continual Learning by addressing the stability-plasticity dilemma at the architectural level using two specialized networks.  					AI-generated summary 				 The quest for Continual Learning (CL) seeks to empower neural networks with the ability to learn and adapt incrementally. Central to this pursuit is addressing the stability-plasticity dilemma, which involves striking a balance between two conflicting objectives: preserving previously learned knowledge and acquiring new knowledge. While numerous CL methods aim to achieve this trade-off, they often overlook the impact of network architecture on stability and plasticity, restricting the trade-off to the parameter level. In this paper, we delve into the conflict between stability and plasticity at the architectural level. We reveal that under an equal parameter constraint, deeper networks exhibit better plasticity, while wider networks are characterized by superior stability. To address this architectural-level dilemma, we introduce a novel framework denoted Dual-Arch, which serves as a plug-in component for CL. This framework leverages the complementary strengths of two distinct and independent networks: one dedicated to plasticity and the other to stability. Each network is designed with a specialized and lightweight architecture, tailored to its respective objective. Extensive experiments demonstrate that Dual-Arch enhances the performance of existing CL methods while being up to 87% more compact in terms of parameters."

[05.06.2025 09:14] Response: ```python
["OPTIMIZATION"]
```
[05.06.2025 09:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The paper presents a new framework called Dual-Arch that improves Continual Learning (CL) by tackling the stability-plasticity dilemma through architectural innovations. It highlights that deeper networks are better at learning new information (plasticity), while wider networks excel at retaining old knowledge (stability). By utilizing two specialized networks—one focused on plasticity and the other on stability—Dual-Arch effectively balances these competing needs. Experimental results show that this approach not only enhances the performance of existing CL methods but also reduces the model size by up to 87%.","title":"Dual-Arch: Balancing Stability and Plasticity in Continual Learning"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='The paper presents a new framework called Dual-Arch that improves Continual Learning (CL) by tackling the stability-plasticity dilemma through architectural innovations. It highlights that deeper networks are better at learning new information (plasticity), while wider networks excel at retaining old knowledge (stability). By utilizing two specialized networks—one focused on plasticity and the other on stability—Dual-Arch effectively balances these competing needs. Experimental results show that this approach not only enhances the performance of existing CL methods but also reduces the model size by up to 87%.', title='Dual-Arch: Balancing Stability and Plasticity in Continual Learning'))
[05.06.2025 09:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本文提出了一种新框架Dual-Arch，旨在通过在架构层面解决稳定性与可塑性之间的矛盾来增强持续学习。持续学习的目标是使神经网络能够逐步学习和适应新知识，同时保持对旧知识的记忆。研究表明，在相同参数约束下，深层网络具有更好的可塑性，而宽层网络则表现出更高的稳定性。Dual-Arch框架结合了两个独立网络的优势，一个专注于可塑性，另一个专注于稳定性，从而提高了现有持续学习方法的性能。","title":"双网络架构，平衡学习稳定性与可塑性"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本文提出了一种新框架Dual-Arch，旨在通过在架构层面解决稳定性与可塑性之间的矛盾来增强持续学习。持续学习的目标是使神经网络能够逐步学习和适应新知识，同时保持对旧知识的记忆。研究表明，在相同参数约束下，深层网络具有更好的可塑性，而宽层网络则表现出更高的稳定性。Dual-Arch框架结合了两个独立网络的优势，一个专注于可塑性，另一个专注于稳定性，从而提高了现有持续学习方法的性能。', title='双网络架构，平衡学习稳定性与可塑性'))
[05.06.2025 09:14] Using data from previous issue: {"categories": ["#open_source", "#data", "#dataset", "#multimodal", "#cv", "#benchmark", "#security", "#ethics"], "emoji": "🧩", "ru": {"title": "Визуальное сшивание: скрытая угроза в моделях компьютерного зрения", "desc": "Это исследование раскрывает феномен 'визуального сшивания' в моделях компьюте
[05.06.2025 09:14] Querying the API.
[05.06.2025 09:14] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A novel Asymmetric Dual 3DGS framework improves 3D reconstruction by training dual models with consistency constraints and divergent masking, outperforming existing methods with high efficiency.  					AI-generated summary 				 3D reconstruction from in-the-wild images remains a challenging task due to inconsistent lighting conditions and transient distractors. Existing methods typically rely on heuristic strategies to handle the low-quality training data, which often struggle to produce stable and consistent reconstructions, frequently resulting in visual artifacts. In this work, we propose Asymmetric Dual 3DGS, a novel framework that leverages the stochastic nature of these artifacts: they tend to vary across different training runs due to minor randomness. Specifically, our method trains two 3D Gaussian Splatting (3DGS) models in parallel, enforcing a consistency constraint that encourages convergence on reliable scene geometry while suppressing inconsistent artifacts. To prevent the two models from collapsing into similar failure modes due to confirmation bias, we introduce a divergent masking strategy that applies two complementary masks: a multi-cue adaptive mask and a self-supervised soft mask, which leads to an asymmetric training process of the two models, reducing shared error modes. In addition, to improve the efficiency of model training, we introduce a lightweight variant called Dynamic EMA Proxy, which replaces one of the two models with a dynamically updated Exponential Moving Average (EMA) proxy, and employs an alternating masking strategy to preserve divergence. Extensive experiments on challenging real-world datasets demonstrate that our method consistently outperforms existing approaches while achieving high efficiency. Codes and trained models will be released.
[05.06.2025 09:14] Response: {
  "desc": "Предложен новый метод Asymmetric Dual 3DGS для улучшения 3D-реконструкции изображений. Он основан на обучении двух параллельных моделей 3D Gaussian Splatting с ограничением согласованности и дивергентным маскированием. Метод превосходит существующие подходы, подавляя артефакты и выделяя надежную геометрию сцены. Также представлен облегченный вариант Dynamic EMA Proxy для повышения эффективности обучения.",
  "emoji": "🔍",
  "title": "Двойное зрение для точной 3D-реконструкции"
}
[05.06.2025 09:14] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A novel Asymmetric Dual 3DGS framework improves 3D reconstruction by training dual models with consistency constraints and divergent masking, outperforming existing methods with high efficiency.  					AI-generated summary 				 3D reconstruction from in-the-wild images remains a challenging task due to inconsistent lighting conditions and transient distractors. Existing methods typically rely on heuristic strategies to handle the low-quality training data, which often struggle to produce stable and consistent reconstructions, frequently resulting in visual artifacts. In this work, we propose Asymmetric Dual 3DGS, a novel framework that leverages the stochastic nature of these artifacts: they tend to vary across different training runs due to minor randomness. Specifically, our method trains two 3D Gaussian Splatting (3DGS) models in parallel, enforcing a consistency constraint that encourages convergence on reliable scene geometry while suppressing inconsistent artifacts. To prevent the two models from collapsing into similar failure modes due to confirmation bias, we introduce a divergent masking strategy that applies two complementary masks: a multi-cue adaptive mask and a self-supervised soft mask, which leads to an asymmetric training process of the two models, reducing shared error modes. In addition, to improve the efficiency of model training, we introduce a lightweight variant called Dynamic EMA Proxy, which replaces one of the two models with a dynamically updated Exponential Moving Average (EMA) proxy, and employs an alternating masking strategy to preserve divergence. Extensive experiments on challenging real-world datasets demonstrate that our method consistently outperforms existing approaches while achieving high efficiency. Codes and trained models will be released."

[05.06.2025 09:14] Response: ```python
["3D", "TRAINING"]
```
[05.06.2025 09:14] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A novel Asymmetric Dual 3DGS framework improves 3D reconstruction by training dual models with consistency constraints and divergent masking, outperforming existing methods with high efficiency.  					AI-generated summary 				 3D reconstruction from in-the-wild images remains a challenging task due to inconsistent lighting conditions and transient distractors. Existing methods typically rely on heuristic strategies to handle the low-quality training data, which often struggle to produce stable and consistent reconstructions, frequently resulting in visual artifacts. In this work, we propose Asymmetric Dual 3DGS, a novel framework that leverages the stochastic nature of these artifacts: they tend to vary across different training runs due to minor randomness. Specifically, our method trains two 3D Gaussian Splatting (3DGS) models in parallel, enforcing a consistency constraint that encourages convergence on reliable scene geometry while suppressing inconsistent artifacts. To prevent the two models from collapsing into similar failure modes due to confirmation bias, we introduce a divergent masking strategy that applies two complementary masks: a multi-cue adaptive mask and a self-supervised soft mask, which leads to an asymmetric training process of the two models, reducing shared error modes. In addition, to improve the efficiency of model training, we introduce a lightweight variant called Dynamic EMA Proxy, which replaces one of the two models with a dynamically updated Exponential Moving Average (EMA) proxy, and employs an alternating masking strategy to preserve divergence. Extensive experiments on challenging real-world datasets demonstrate that our method consistently outperforms existing approaches while achieving high efficiency. Codes and trained models will be released."

[05.06.2025 09:14] Response: ```python
["OPTIMIZATION"]
```
[05.06.2025 09:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The paper presents the Asymmetric Dual 3DGS framework, which enhances 3D reconstruction from images taken in varied conditions. It addresses the challenges of inconsistent lighting and distracting elements by training two models simultaneously with a focus on consistency and divergence. The framework uses a unique masking strategy to prevent the models from converging on the same errors, thus improving the quality of the reconstructions. Experimental results show that this approach is more efficient and effective than current methods, leading to better performance in real-world scenarios.","title":"Revolutionizing 3D Reconstruction with Asymmetric Dual Models"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='The paper presents the Asymmetric Dual 3DGS framework, which enhances 3D reconstruction from images taken in varied conditions. It addresses the challenges of inconsistent lighting and distracting elements by training two models simultaneously with a focus on consistency and divergence. The framework uses a unique masking strategy to prevent the models from converging on the same errors, thus improving the quality of the reconstructions. Experimental results show that this approach is more efficient and effective than current methods, leading to better performance in real-world scenarios.', title='Revolutionizing 3D Reconstruction with Asymmetric Dual Models'))
[05.06.2025 09:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本文提出了一种新颖的非对称双重3DGS框架，旨在提高3D重建的效果。该方法通过训练两个模型并施加一致性约束，来减少不一致的视觉伪影。我们引入了多线索自适应掩码和自监督软掩码，确保两个模型在训练过程中保持差异，从而降低共享错误模式。实验结果表明，该方法在处理真实世界数据集时，表现出更高的效率和更好的重建质量。","title":"非对称双重3DGS框架：高效的3D重建新方法"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本文提出了一种新颖的非对称双重3DGS框架，旨在提高3D重建的效果。该方法通过训练两个模型并施加一致性约束，来减少不一致的视觉伪影。我们引入了多线索自适应掩码和自监督软掩码，确保两个模型在训练过程中保持差异，从而降低共享错误模式。实验结果表明，该方法在处理真实世界数据集时，表现出更高的效率和更好的重建质量。', title='非对称双重3DGS框架：高效的3D重建新方法'))
[05.06.2025 09:14] Querying the API.
[05.06.2025 09:14] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

LEAF, an adversarial finetuning method, enhances the robustness of CLIP text encoders, improving zero-shot accuracy and multimodal retrieval performance under adversarial noise.  					AI-generated summary 				 Adversarial input attacks can cause a significant shift of CLIP embeddings. This can affect the downstream robustness of models incorporating CLIP in the pipeline, such as text-to-image generative models or large vision language models. While some efforts have been done towards making the CLIP image encoders robust, the robustness of text encoders remains unexplored. In this work, we cover this gap in the literature. We propose LEAF: an efficient adversarial finetuning method for the text domain, with the ability to scale to large CLIP models. Our models significantly improve the zero-shot adversarial accuracy in the text domain, while maintaining the vision performance provided by robust image encoders. When combined with text-to-image diffusion models, we can improve the generation quality under adversarial noise. When employing our robust CLIP encoders in multimodal retrieval tasks, we improve the recall under adversarial noise over standard CLIP models. Finally, we show that robust text encoders facilitate better reconstruction of input text from its embedding via direct optimization.
[05.06.2025 09:14] Response: {
  "desc": "LEAF - это метод состязательной доводки (adversarial finetuning), который повышает устойчивость текстовых энкодеров CLIP. Он улучшает точность классификации с нулевым обучением (zero-shot accuracy) и производительность мультимодального поиска в условиях состязательного шума. LEAF эффективно масштабируется на большие модели CLIP и сохраняет исходную производительность для визуальных задач. Метод также улучшает качество генерации изображений по тексту и реконструкцию текста из эмбеддингов в условиях шума.",
  "emoji": "🛡️",
  "title": "LEAF: Повышение устойчивости CLIP к состязательным атакам"
}
[05.06.2025 09:14] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"LEAF, an adversarial finetuning method, enhances the robustness of CLIP text encoders, improving zero-shot accuracy and multimodal retrieval performance under adversarial noise.  					AI-generated summary 				 Adversarial input attacks can cause a significant shift of CLIP embeddings. This can affect the downstream robustness of models incorporating CLIP in the pipeline, such as text-to-image generative models or large vision language models. While some efforts have been done towards making the CLIP image encoders robust, the robustness of text encoders remains unexplored. In this work, we cover this gap in the literature. We propose LEAF: an efficient adversarial finetuning method for the text domain, with the ability to scale to large CLIP models. Our models significantly improve the zero-shot adversarial accuracy in the text domain, while maintaining the vision performance provided by robust image encoders. When combined with text-to-image diffusion models, we can improve the generation quality under adversarial noise. When employing our robust CLIP encoders in multimodal retrieval tasks, we improve the recall under adversarial noise over standard CLIP models. Finally, we show that robust text encoders facilitate better reconstruction of input text from its embedding via direct optimization."

[05.06.2025 09:14] Response: ```python
["RLHF", "MULTIMODAL", "TRAINING"]
```
[05.06.2025 09:14] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"LEAF, an adversarial finetuning method, enhances the robustness of CLIP text encoders, improving zero-shot accuracy and multimodal retrieval performance under adversarial noise.  					AI-generated summary 				 Adversarial input attacks can cause a significant shift of CLIP embeddings. This can affect the downstream robustness of models incorporating CLIP in the pipeline, such as text-to-image generative models or large vision language models. While some efforts have been done towards making the CLIP image encoders robust, the robustness of text encoders remains unexplored. In this work, we cover this gap in the literature. We propose LEAF: an efficient adversarial finetuning method for the text domain, with the ability to scale to large CLIP models. Our models significantly improve the zero-shot adversarial accuracy in the text domain, while maintaining the vision performance provided by robust image encoders. When combined with text-to-image diffusion models, we can improve the generation quality under adversarial noise. When employing our robust CLIP encoders in multimodal retrieval tasks, we improve the recall under adversarial noise over standard CLIP models. Finally, we show that robust text encoders facilitate better reconstruction of input text from its embedding via direct optimization."

[05.06.2025 09:14] Response: ```python
['SECURITY', 'OPTIMIZATION', 'DIFFUSION']
```
[05.06.2025 09:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces LEAF, a novel adversarial finetuning method designed to enhance the robustness of CLIP text encoders against adversarial attacks. By addressing the vulnerability of text embeddings, LEAF significantly boosts zero-shot accuracy and improves performance in multimodal retrieval tasks, even under adversarial noise. The method not only preserves the strong performance of image encoders but also enhances the quality of text-to-image generation. Overall, LEAF fills a critical gap in the literature by ensuring that text encoders are as robust as their image counterparts, leading to better model performance in various applications.","title":"Enhancing Text Encoder Robustness with LEAF"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper introduces LEAF, a novel adversarial finetuning method designed to enhance the robustness of CLIP text encoders against adversarial attacks. By addressing the vulnerability of text embeddings, LEAF significantly boosts zero-shot accuracy and improves performance in multimodal retrieval tasks, even under adversarial noise. The method not only preserves the strong performance of image encoders but also enhances the quality of text-to-image generation. Overall, LEAF fills a critical gap in the literature by ensuring that text encoders are as robust as their image counterparts, leading to better model performance in various applications.', title='Enhancing Text Encoder Robustness with LEAF'))
[05.06.2025 09:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"LEAF是一种对抗微调方法，旨在增强CLIP文本编码器的鲁棒性。通过对抗噪声的训练，LEAF显著提高了文本领域的零-shot准确率和多模态检索性能。该方法不仅保持了图像编码器的视觉性能，还能在文本到图像生成模型中提升生成质量。我们的研究填补了文本编码器鲁棒性研究的空白，展示了其在多模态任务中的优势。","title":"LEAF：提升CLIP文本编码器鲁棒性的对抗微调方法"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='LEAF是一种对抗微调方法，旨在增强CLIP文本编码器的鲁棒性。通过对抗噪声的训练，LEAF显著提高了文本领域的零-shot准确率和多模态检索性能。该方法不仅保持了图像编码器的视觉性能，还能在文本到图像生成模型中提升生成质量。我们的研究填补了文本编码器鲁棒性研究的空白，展示了其在多模态任务中的优势。', title='LEAF：提升CLIP文本编码器鲁棒性的对抗微调方法'))
[05.06.2025 09:14] Using data from previous issue: {"categories": ["#data", "#transfer_learning", "#training", "#optimization", "#diffusion", "#dataset"], "emoji": "🧠", "ru": {"title": "Повышение устойчивости моделей через генерацию сложных примеров", "desc": "Статья представляет новую стратегию аугментации данных на основе диффузии для улучшения ро
[05.06.2025 09:14] Using data from previous issue: {"categories": ["#graphs", "#cv", "#reasoning", "#agents", "#hallucinations", "#multimodal", "#benchmark", "#interpretability"], "emoji": "🔀", "ru": {"title": "Точная интерпретация блок-схем с помощью нейросимволического агента", "desc": "Статья представляет задачу точной атрибуции блок-схем и агент
[05.06.2025 09:14] Querying the API.
[05.06.2025 09:14] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Annotating data is a time-consuming and costly task, but it is inherently required for supervised machine learning. Active Learning (AL) is an established method that minimizes human labeling effort by iteratively selecting the most informative unlabeled samples for expert annotation, thereby improving the overall classification performance. Even though AL has been known for decades, AL is still rarely used in real-world applications. As indicated in the two community web surveys among the NLP community about AL, two main reasons continue to hold practitioners back from using AL: first, the complexity of setting AL up, and second, a lack of trust in its effectiveness. We hypothesize that both reasons share the same culprit: the large hyperparameter space of AL. This mostly unexplored hyperparameter space often leads to misleading and irreproducible AL experiment results. In this study, we first compiled a large hyperparameter grid of over 4.6 million hyperparameter combinations, second, recorded the performance of all combinations in the so-far biggest conducted AL study, and third, analyzed the impact of each hyperparameter in the experiment results. In the end, we give recommendations about the influence of each hyperparameter, demonstrate the surprising influence of the concrete AL strategy implementation, and outline an experimental study design for reproducible AL experiments with minimal computational effort, thus contributing to more reproducible and trustworthy AL research in the future.
[05.06.2025 09:14] Response: {
  "desc": "Это исследование посвящено активному обучению (Active Learning, AL) в машинном обучении, которое помогает минимизировать усилия по разметке данных. Авторы провели масштабный эксперимент, изучив более 4,6 миллионов комбинаций гиперпараметров AL. Они проанализировали влияние каждого гиперпараметра на результаты и предложили рекомендации по настройке AL. Исследование направлено на повышение воспроизводимости и надежности экспериментов с AL в будущем.",
  "emoji": "🔍",
  "title": "Раскрывая секреты гиперпараметров в активном обучении"
}
[05.06.2025 09:14] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Annotating data is a time-consuming and costly task, but it is inherently required for supervised machine learning. Active Learning (AL) is an established method that minimizes human labeling effort by iteratively selecting the most informative unlabeled samples for expert annotation, thereby improving the overall classification performance. Even though AL has been known for decades, AL is still rarely used in real-world applications. As indicated in the two community web surveys among the NLP community about AL, two main reasons continue to hold practitioners back from using AL: first, the complexity of setting AL up, and second, a lack of trust in its effectiveness. We hypothesize that both reasons share the same culprit: the large hyperparameter space of AL. This mostly unexplored hyperparameter space often leads to misleading and irreproducible AL experiment results. In this study, we first compiled a large hyperparameter grid of over 4.6 million hyperparameter combinations, second, recorded the performance of all combinations in the so-far biggest conducted AL study, and third, analyzed the impact of each hyperparameter in the experiment results. In the end, we give recommendations about the influence of each hyperparameter, demonstrate the surprising influence of the concrete AL strategy implementation, and outline an experimental study design for reproducible AL experiments with minimal computational effort, thus contributing to more reproducible and trustworthy AL research in the future."

[05.06.2025 09:14] Response: ```python
["DATA", "TRAINING"]
```
[05.06.2025 09:14] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Annotating data is a time-consuming and costly task, but it is inherently required for supervised machine learning. Active Learning (AL) is an established method that minimizes human labeling effort by iteratively selecting the most informative unlabeled samples for expert annotation, thereby improving the overall classification performance. Even though AL has been known for decades, AL is still rarely used in real-world applications. As indicated in the two community web surveys among the NLP community about AL, two main reasons continue to hold practitioners back from using AL: first, the complexity of setting AL up, and second, a lack of trust in its effectiveness. We hypothesize that both reasons share the same culprit: the large hyperparameter space of AL. This mostly unexplored hyperparameter space often leads to misleading and irreproducible AL experiment results. In this study, we first compiled a large hyperparameter grid of over 4.6 million hyperparameter combinations, second, recorded the performance of all combinations in the so-far biggest conducted AL study, and third, analyzed the impact of each hyperparameter in the experiment results. In the end, we give recommendations about the influence of each hyperparameter, demonstrate the surprising influence of the concrete AL strategy implementation, and outline an experimental study design for reproducible AL experiments with minimal computational effort, thus contributing to more reproducible and trustworthy AL research in the future."

[05.06.2025 09:14] Response: ```python
["OPTIMIZATION"]
```
[05.06.2025 09:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper addresses the challenges of using Active Learning (AL) in supervised machine learning, particularly the complexities and trust issues that hinder its adoption. The authors compiled a vast hyperparameter grid with over 4.6 million combinations to analyze how different settings affect AL performance. They conducted the largest AL study to date, recording the results and examining the impact of each hyperparameter on the outcomes. The findings provide insights and recommendations for setting up reproducible AL experiments, aiming to enhance the reliability and effectiveness of AL in real-world applications.","title":"Unlocking Active Learning: Simplifying Setup for Trustworthy Results"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper addresses the challenges of using Active Learning (AL) in supervised machine learning, particularly the complexities and trust issues that hinder its adoption. The authors compiled a vast hyperparameter grid with over 4.6 million combinations to analyze how different settings affect AL performance. They conducted the largest AL study to date, recording the results and examining the impact of each hyperparameter on the outcomes. The findings provide insights and recommendations for setting up reproducible AL experiments, aiming to enhance the reliability and effectiveness of AL in real-world applications.', title='Unlocking Active Learning: Simplifying Setup for Trustworthy Results'))
[05.06.2025 09:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"标注数据是一个耗时且成本高昂的任务，但这是监督学习所必需的。主动学习（Active Learning, AL）是一种通过迭代选择最具信息量的未标记样本来减少人工标注工作的方法，从而提高分类性能。尽管AL已经存在了几十年，但在实际应用中仍然很少被使用。本文研究了AL中超参数空间的复杂性，提出了一个包含460万种超参数组合的大型网格，并分析了每个超参数对实验结果的影响，以促进更可靠的AL研究。","title":"优化主动学习，提升标注效率"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='标注数据是一个耗时且成本高昂的任务，但这是监督学习所必需的。主动学习（Active Learning, AL）是一种通过迭代选择最具信息量的未标记样本来减少人工标注工作的方法，从而提高分类性能。尽管AL已经存在了几十年，但在实际应用中仍然很少被使用。本文研究了AL中超参数空间的复杂性，提出了一个包含460万种超参数组合的大型网格，并分析了每个超参数对实验结果的影响，以促进更可靠的AL研究。', title='优化主动学习，提升标注效率'))
[05.06.2025 09:14] Trying to get texts in Chinese.
[05.06.2025 09:14] Mistral request. Model: mistral-large-latest. Prompt: Write simple and brief explanation (4-5 sentences) of an article in Chinese. Use short sentences. Text:

We open-source MiMo-VL-7B-SFT and MiMo-VL-7B-RL, two powerful vision-language models delivering state-of-the-art performance in both general visual understanding and multimodal reasoning. MiMo-VL-7B-RL outperforms Qwen2.5-VL-7B on 35 out of 40 evaluated tasks, and scores 59.4 on OlympiadBench, surpassing models with up to 78B parameters. For GUI grounding applications, it sets a new standard with 56.1 on OSWorld-G, even outperforming specialized models such as UI-TARS. Our training combines four-stage pre-training (2.4 trillion tokens) with Mixed On-policy Reinforcement Learning (MORL) integrating diverse reward signals. We identify the importance of incorporating high-quality reasoning data with long Chain-of-Thought into pre-training stages, and the benefits of mixed RL despite challenges in simultaneous multi-domain optimization. We also contribute a comprehensive evaluation suite covering 50+ tasks to promote reproducibility and advance the field. The model checkpoints and full evaluation suite are available at https://github.com/XiaomiMiMo/MiMo-VL.
[05.06.2025 09:15] Mistral response. {"id": "e198953a49094d46b0e2dde842ca916c", "object": "chat.completion", "created": 1749114899, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "\u6211\u4eec\u5f00\u6e90\u4e86\u4e24\u4e2a\u5f3a\u5927\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff0cMiMo-VL-7B-SFT\u548cMiMo-VL-7B-RL\uff0c\u5b83\u4eec\u5728\u89c6\u89c9\u7406\u89e3\u548c\u591a\u6a21\u6001\u63a8\u7406\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002MiMo-VL-7B-RL\u572835\u4e2a\u4efb\u52a1\u4e2d\u8d85\u8d8a\u4e86Qwen2.5-VL-7B\uff0c\u5e76\u5728OlympiadBench\u4e0a\u5f97\u520659.4\uff0c\u8d85\u8fc7\u4e86\u53c2\u6570\u91cf\u9ad8\u8fbe78B\u7684\u6a21\u578b\u3002\u5728GUI\u5e94\u7528\u4e2d\uff0c\u5b83\u5728OSWorld-G\u4e0a\u5f97\u520656.1\uff0c\u751a\u81f3\u8d85\u8fc7\u4e86\u4e13\u95e8\u6a21\u578bUI-TARS\u3002\u6211\u4eec\u7684\u8bad\u7ec3\u7ed3\u5408\u4e86\u56db\u9636\u6bb5\u9884\u8bad\u7ec3\u548c\u6df7\u5408\u5728\u7b56\u7565\u5f3a\u5316\u5b66\u4e60\u3002\u6211\u4eec\u8fd8\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5305\u542b50\u591a\u4e2a\u4efb\u52a1\u7684\u5168\u9762\u8bc4\u4f30\u5957\u4ef6\uff0c\u4ee5\u4fc3\u8fdb\u53ef\u91cd\u590d\u6027\u548c\u63a8\u52a8\u9886\u57df\u53d1\u5c55\u3002\u6a21\u578b\u68c0\u67e5\u70b9\u548c\u5b8c\u6574\u8bc4\u4f30\u5957\u4ef6\u53ef\u5728https://github.com/XiaomiMiMo/MiMo-VL\u83b7\u53d6\u3002"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 316, "total_tokens": 581, "completion_tokens": 265}}
[05.06.2025 09:15] Response: 我们开源了两个强大的视觉-语言模型，MiMo-VL-7B-SFT和MiMo-VL-7B-RL，它们在视觉理解和多模态推理方面表现出色。MiMo-VL-7B-RL在35个任务中超越了Qwen2.5-VL-7B，并在OlympiadBench上得分59.4，超过了参数量高达78B的模型。在GUI应用中，它在OSWorld-G上得分56.1，甚至超过了专门模型UI-TARS。我们的训练结合了四阶段预训练和混合在策略强化学习。我们还提供了一个包含50多个任务的全面评估套件，以促进可重复性和推动领域发展。模型检查点和完整评估套件可在https://github.com/XiaomiMiMo/MiMo-VL获取。
[05.06.2025 09:15] Mistral request. Model: mistral-large-latest. Prompt: Write pinyin transcription for text. Text:

我们开源了两个强大的视觉-语言模型，MiMo-VL-7B-SFT和MiMo-VL-7B-RL，它们在视觉理解和多模态推理方面表现出色。MiMo-VL-7B-RL在35个任务中超越了Qwen2.5-VL-7B，并在OlympiadBench上得分59.4，超过了参数量高达78B的模型。在GUI应用中，它在OSWorld-G上得分56.1，甚至超过了专门模型UI-TARS。我们的训练结合了四阶段预训练和混合在策略强化学习。我们还提供了一个包含50多个任务的全面评估套件，以促进可重复性和推动领域发展。模型检查点和完整评估套件可在https://github.com/XiaomiMiMo/MiMo-VL获取。
[05.06.2025 09:15] Mistral response. {"id": "1e9ba16de50446f897a7b9b3f2941ef1", "object": "chat.completion", "created": 1749114902, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "W\u01d2men k\u0101iyu\u00e1nle li\u01ceng g\u00e8 qi\u00e1ngd\u00e0 de sh\u00ecju\u00e9-y\u01d4y\u00e1n m\u00f3x\u00edng, MiMo-VL-7B-SFT h\u00e9 MiMo-VL-7B-RL, t\u0101men z\u00e0i sh\u00ecju\u00e9 l\u01d0ji\u011b h\u00e9 du\u014d m\u00f3sh\u00ec tu\u012bl\u01d0 f\u0101ngmi\u00e0n bi\u01ceoxi\u00e0n ch\u016bs\u00e8. MiMo-VL-7B-RL z\u00e0i 35 g\u00e8 r\u00e8nw\u00f9 zh\u014dng ch\u0101oyu\u00e8le Qwen2.5-VL-7B, b\u00ecng z\u00e0i OlympiadBench sh\u00e0ng d\u00e9f\u0113n 59.4, ch\u0101ogu\u00f2le c\u0101nsh\u00f9 li\u00e0ng g\u0101od\u00e1 78B de m\u00f3x\u00edng. Z\u00e0i GUI y\u00ecngy\u00f2ng zh\u014dng, t\u0101 z\u00e0i OSWorld-G sh\u00e0ng d\u00e9f\u0113n 56.1, sh\u00e8nzh\u00ec ch\u0101ogu\u00f2le zhu\u0101nm\u00e9n m\u00f3x\u00edng UI-TARS. W\u01d2men de x\u00f9nli\u00e0n ji\u00e9h\u00e9le s\u00ec ji\u0113du\u00e0n y\u00f9x\u00f9nli\u00e0n h\u00e9 h\u00f9nh\u00e9 z\u00e0i c\u00e8l\u00fc\u00e8 qi\u00e1ngxu\u00e9xu\u00e9. W\u01d2men h\u00e1i t\u00edg\u014dngle y\u012bg\u00e8 b\u0101oh\u00e1n 50 du\u014d g\u00e8 r\u00e8nw\u00f9 de qu\u00e1nmi\u00e0n p\u00edngg\u01d4 t\u00e0oji\u00e0n, y\u01d0 c\u00f9j\u00ecn k\u011b ch\u00f3ngf\u00f9x\u00ecng h\u00e9 tu\u012bd\u00f2ng l\u01d0ngy\u00f9 f\u0101zh\u01cen. M\u00f3x\u00edng ji\u01cench\u00e1 di\u01cen h\u00e9 w\u00e1nzh\u011bng p\u00edngg\u01d4 t\u00e0oji\u00e0n k\u011b z\u00e0i https://github.com/XiaomiMiMo/MiMo-VL hu\u00f2q\u01d4."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 279, "total_tokens": 742, "completion_tokens": 463}}
[05.06.2025 09:15] Response: Wǒmen kāiyuánle liǎng gè qiángdà de shìjué-yǔyán móxíng, MiMo-VL-7B-SFT hé MiMo-VL-7B-RL, tāmen zài shìjué lǐjiě hé duō móshì tuīlǐ fāngmiàn biǎoxiàn chūsè. MiMo-VL-7B-RL zài 35 gè rènwù zhōng chāoyuèle Qwen2.5-VL-7B, bìng zài OlympiadBench shàng défēn 59.4, chāoguòle cānshù liàng gāodá 78B de móxíng. Zài GUI yìngyòng zhōng, tā zài OSWorld-G shàng défēn 56.1, shènzhì chāoguòle zhuānmén móxíng UI-TARS. Wǒmen de xùnliàn jiéhéle sì jiēduàn yùxùnliàn hé hùnhé zài cèlüè qiángxuéxué. Wǒmen hái tígōngle yīgè bāohán 50 duō gè rènwù de quánmiàn pínggǔ tàojiàn, yǐ cùjìn kě chóngfùxìng hé tuīdòng lǐngyù fāzhǎn. Móxíng jiǎnchá diǎn hé wánzhěng pínggǔ tàojiàn kě zài https://github.com/XiaomiMiMo/MiMo-VL huòqǔ.
[05.06.2025 09:15] Mistral request. Model: mistral-large-latest. Prompt: Write vocab of difficult words for this text as an array of objects with fields 'word', 'pinyin', 'trans'. Return as python list without formatting. Return list and nothing else. Text:

我们开源了两个强大的视觉-语言模型，MiMo-VL-7B-SFT和MiMo-VL-7B-RL，它们在视觉理解和多模态推理方面表现出色。MiMo-VL-7B-RL在35个任务中超越了Qwen2.5-VL-7B，并在OlympiadBench上得分59.4，超过了参数量高达78B的模型。在GUI应用中，它在OSWorld-G上得分56.1，甚至超过了专门模型UI-TARS。我们的训练结合了四阶段预训练和混合在策略强化学习。我们还提供了一个包含50多个任务的全面评估套件，以促进可重复性和推动领域发展。模型检查点和完整评估套件可在https://github.com/XiaomiMiMo/MiMo-VL获取。
[05.06.2025 09:15] Mistral response. {"id": "d32bfa3e4fa64dd88459e28f6204925f", "object": "chat.completion", "created": 1749114912, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "[\n    {\"word\": \"\u5f00\u6e90\", \"pinyin\": \"k\u0101i yu\u00e1n\", \"trans\": \"open source\"},\n    {\"word\": \"\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\", \"pinyin\": \"sh\u00ec ju\u00e9 y\u01d4 y\u00e1n m\u00f3 x\u00edng\", \"trans\": \"vision-language model\"},\n    {\"word\": \"\u8868\u73b0\u51fa\u8272\", \"pinyin\": \"bi\u01ceo xi\u00e0n ch\u016b s\u00e8\", \"trans\": \"perform excellently\"},\n    {\"word\": \"\u591a\u6a21\u6001\", \"pinyin\": \"du\u014d m\u00f3 t\u00e0i\", \"trans\": \"multimodal\"},\n    {\"word\": \"\u63a8\u7406\", \"pinyin\": \"tu\u012b l\u01d0\", \"trans\": \"reasoning\"},\n    {\"word\": \"\u8d85\u8d8a\", \"pinyin\": \"ch\u0101o yu\u00e8\", \"trans\": \"surpass\"},\n    {\"word\": \"\u53c2\u6570\u91cf\", \"pinyin\": \"c\u0101n sh\u01d4 li\u00e0ng\", \"trans\": \"parameter quantity\"},\n    {\"word\": \"GUI\", \"pinyin\": \"GUI\", \"trans\": \"Graphical User Interface\"},\n    {\"word\": \"\u5e94\u7528\", \"pinyin\": \"y\u00ecng y\u00f2ng\", \"trans\": \"application\"},\n    {\"word\": \"\u4e13\u95e8\", \"pinyin\": \"zhu\u0101n m\u00e9n\", \"trans\": \"specialized\"},\n    {\"word\": \"\u9884\u8bad\u7ec3\", \"pinyin\": \"y\u00f9 x\u00f9n li\u00e0n\", \"trans\": \"pre-training\"},\n    {\"word\": \"\u6df7\u5408\", \"pinyin\": \"h\u00f9n h\u00e9\", \"trans\": \"hybrid\"},\n    {\"word\": \"\u7b56\u7565\", \"pinyin\": \"c\u00e8 l\u00fc\u00e8\", \"trans\": \"strategy\"},\n    {\"word\": \"\u5f3a\u5316\u5b66\u4e60\", \"pinyin\": \"qi\u00e1ng hu\u00e0 xu\u00e9 x\u00ed\", \"trans\": \"reinforcement learning\"},\n    {\"word\": \"\u8bc4\u4f30\", \"pinyin\": \"p\u00edng g\u016b\", \"trans\": \"evaluation\"},\n    {\"word\": \"\u5957\u4ef6\", \"pinyin\": \"t\u00e0o ji\u00e0n\", \"trans\": \"suite\"},\n    {\"word\": \"\u53ef\u91cd\u590d\u6027\", \"pinyin\": \"k\u011b ch\u00f3ng f\u00f9 x\u00ecng\", \"trans\": \"reproducibility\"},\n    {\"word\": \"\u63a8\u52a8\", \"pinyin\": \"tu\u012b d\u00f2ng\", \"trans\": \"promote\"},\n    {\"word\": \"\u9886\u57df\", \"pinyin\": \"l\u01d0ng y\u00f9\", \"trans\": \"field\"},\n    {\"word\": \"\u68c0\u67e5\u70b9\", \"pinyin\": \"ji\u01cen ch\u00e1 di\u01cen\", \"trans\": \"checkpoint\"}\n]"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 311, "total_tokens": 939, "completion_tokens": 628}}
[05.06.2025 09:15] Response: [
    {"word": "开源", "pinyin": "kāi yuán", "trans": "open source"},
    {"word": "视觉-语言模型", "pinyin": "shì jué yǔ yán mó xíng", "trans": "vision-language model"},
    {"word": "表现出色", "pinyin": "biǎo xiàn chū sè", "trans": "perform excellently"},
    {"word": "多模态", "pinyin": "duō mó tài", "trans": "multimodal"},
    {"word": "推理", "pinyin": "tuī lǐ", "trans": "reasoning"},
    {"word": "超越", "pinyin": "chāo yuè", "trans": "surpass"},
    {"word": "参数量", "pinyin": "cān shǔ liàng", "trans": "parameter quantity"},
    {"word": "GUI", "pinyin": "GUI", "trans": "Graphical User Interface"},
    {"word": "应用", "pinyin": "yìng yòng", "trans": "application"},
    {"word": "专门", "pinyin": "zhuān mén", "trans": "specialized"},
    {"word": "预训练", "pinyin": "yù xùn liàn", "trans": "pre-training"},
    {"word": "混合", "pinyin": "hùn hé", "trans": "hybrid"},
    {"word": "策略", "pinyin": "cè lüè", "trans": "strategy"},
    {"word": "强化学习", "pinyin": "qiáng huà xué xí", "trans": "reinforcement learning"},
    {"word": "评估", "pinyin": "píng gū", "trans": "evaluation"},
    {"word": "套件", "pinyin": "tào jiàn", "trans": "suite"},
    {"word": "可重复性", "pinyin": "kě chóng fù xìng", "trans": "reproducibility"},
    {"word": "推动", "pinyin": "tuī dòng", "trans": "promote"},
    {"word": "领域", "pinyin": "lǐng yù", "trans": "field"},
    {"word": "检查点", "pinyin": "jiǎn chá diǎn", "trans": "checkpoint"}
]
[05.06.2025 09:15] Mistral request. Model: mistral-large-latest. Prompt: Translate this text in English. Text:

我们开源了两个强大的视觉-语言模型，MiMo-VL-7B-SFT和MiMo-VL-7B-RL，它们在视觉理解和多模态推理方面表现出色。MiMo-VL-7B-RL在35个任务中超越了Qwen2.5-VL-7B，并在OlympiadBench上得分59.4，超过了参数量高达78B的模型。在GUI应用中，它在OSWorld-G上得分56.1，甚至超过了专门模型UI-TARS。我们的训练结合了四阶段预训练和混合在策略强化学习。我们还提供了一个包含50多个任务的全面评估套件，以促进可重复性和推动领域发展。模型检查点和完整评估套件可在https://github.com/XiaomiMiMo/MiMo-VL获取。
[05.06.2025 09:15] Mistral response. {"id": "5919a9ab423e4f76902d0f8f09892548", "object": "chat.completion", "created": 1749114917, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "We have open-sourced two powerful vision-language models, MiMo-VL-7B-SFT and MiMo-VL-7B-RL, which excel in visual understanding and multimodal reasoning. MiMo-VL-7B-RL outperforms Qwen2.5-VL-7B in 35 tasks and scores 59.4 on the OlympiadBench, surpassing models with up to 78B parameters. In GUI applications, it scores 56.1 on OSWorld-G, even outperforming the specialized model UI-TARS. Our training combines four-stage pretraining and hybrid on-policy reinforcement learning. We also provide a comprehensive evaluation suite with over 50 tasks to promote reproducibility and advance the field. Model checkpoints and the complete evaluation suite are available at https://github.com/XiaomiMiMo/MiMo-VL."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 277, "total_tokens": 484, "completion_tokens": 207}}
[05.06.2025 09:15] Response: We have open-sourced two powerful vision-language models, MiMo-VL-7B-SFT and MiMo-VL-7B-RL, which excel in visual understanding and multimodal reasoning. MiMo-VL-7B-RL outperforms Qwen2.5-VL-7B in 35 tasks and scores 59.4 on the OlympiadBench, surpassing models with up to 78B parameters. In GUI applications, it scores 56.1 on OSWorld-G, even outperforming the specialized model UI-TARS. Our training combines four-stage pretraining and hybrid on-policy reinforcement learning. We also provide a comprehensive evaluation suite with over 50 tasks to promote reproducibility and advance the field. Model checkpoints and the complete evaluation suite are available at https://github.com/XiaomiMiMo/MiMo-VL.
[05.06.2025 09:15] Renaming data file.
[05.06.2025 09:15] Renaming previous data. hf_papers.json to ./d/2025-06-05.json
[05.06.2025 09:15] Saving new data file.
[05.06.2025 09:15] Generating page.
[05.06.2025 09:15] Renaming previous page.
[05.06.2025 09:15] Renaming previous data. index.html to ./d/2025-06-05.html
[05.06.2025 09:15] [Experimental] Generating Chinese page for reading.
[05.06.2025 09:15] Chinese vocab [{'word': '开源', 'pinyin': 'kāi yuán', 'trans': 'open source'}, {'word': '视觉-语言模型', 'pinyin': 'shì jué yǔ yán mó xíng', 'trans': 'vision-language model'}, {'word': '表现出色', 'pinyin': 'biǎo xiàn chū sè', 'trans': 'perform excellently'}, {'word': '多模态', 'pinyin': 'duō mó tài', 'trans': 'multimodal'}, {'word': '推理', 'pinyin': 'tuī lǐ', 'trans': 'reasoning'}, {'word': '超越', 'pinyin': 'chāo yuè', 'trans': 'surpass'}, {'word': '参数量', 'pinyin': 'cān shǔ liàng', 'trans': 'parameter quantity'}, {'word': 'GUI', 'pinyin': 'GUI', 'trans': 'Graphical User Interface'}, {'word': '应用', 'pinyin': 'yìng yòng', 'trans': 'application'}, {'word': '专门', 'pinyin': 'zhuān mén', 'trans': 'specialized'}, {'word': '预训练', 'pinyin': 'yù xùn liàn', 'trans': 'pre-training'}, {'word': '混合', 'pinyin': 'hùn hé', 'trans': 'hybrid'}, {'word': '策略', 'pinyin': 'cè lüè', 'trans': 'strategy'}, {'word': '强化学习', 'pinyin': 'qiáng huà xué xí', 'trans': 'reinforcement learning'}, {'word': '评估', 'pinyin': 'píng gū', 'trans': 'evaluation'}, {'word': '套件', 'pinyin': 'tào jiàn', 'trans': 'suite'}, {'word': '可重复性', 'pinyin': 'kě chóng fù xìng', 'trans': 'reproducibility'}, {'word': '推动', 'pinyin': 'tuī dòng', 'trans': 'promote'}, {'word': '领域', 'pinyin': 'lǐng yù', 'trans': 'field'}, {'word': '检查点', 'pinyin': 'jiǎn chá diǎn', 'trans': 'checkpoint'}]
[05.06.2025 09:15] Renaming previous Chinese page.
[05.06.2025 09:15] Renaming previous data. zh.html to ./d/2025-06-04_zh_reading_task.html
[05.06.2025 09:15] Writing Chinese reading task.
[05.06.2025 09:15] Writing result.
[05.06.2025 09:15] Renaming log file.
[05.06.2025 09:15] Renaming previous data. log.txt to ./logs/2025-06-05_last_log.txt
