[20.10.2024 08:15] [Experimental] Generating an image for paper Movie Gen: A Cast of Media Foundation Models.
[20.10.2024 08:15] [Experimental] Image for paper Movie Gen: A Cast of Media Foundation Models already exists.
[20.10.2024 08:15] [Experimental] Generating an image for paper MixEval-X: Any-to-Any Evaluations from Real-World Data Mixtures.
[20.10.2024 08:15] [Experimental] Image for paper MixEval-X: Any-to-Any Evaluations from Real-World Data Mixtures already exists.
[20.10.2024 08:15] [Experimental] Generating an image for paper MobA: A Two-Level Agent System for Efficient Mobile Task Automation.
[20.10.2024 08:15] [Experimental] Image for paper MobA: A Two-Level Agent System for Efficient Mobile Task Automation already exists.
[20.10.2024 08:15] [Experimental] Generating an image for paper JudgeBench: A Benchmark for Evaluating LLM-based Judges.
[20.10.2024 08:15] [Experimental] Image for paper JudgeBench: A Benchmark for Evaluating LLM-based Judges already exists.
[20.10.2024 08:15] [Experimental] Generating an image for paper Roadmap towards Superhuman Speech Understanding using Large Language Models.
[20.10.2024 08:15] [Experimental] Image for paper Roadmap towards Superhuman Speech Understanding using Large Language Models already exists.
[20.10.2024 08:15] [Experimental] Generating an image for paper Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation.
[20.10.2024 08:15] [Experimental] Image for paper Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation already exists.
[20.10.2024 08:15] [Experimental] Generating an image for paper Harnessing Webpage UIs for Text-Rich Visual Understanding.
[20.10.2024 08:15] [Experimental] Image for paper Harnessing Webpage UIs for Text-Rich Visual Understanding already exists.
[20.10.2024 08:15] [Experimental] Generating an image for paper Fluid: Scaling Autoregressive Text-to-image Generative Models with Continuous Tokens.
[20.10.2024 08:15] OpenAI request. Model: gpt-4o-mini. Prompt: Write a text with image prompt in style of surrealism and modern art based on the following paper. Use key themes and elements from it. Add instruction to write a text that reads as brief paper title as a label on some object on an image. Return only prompt and nothing else. Title: 'Fluid: Scaling Autoregressive Text-to-image Generative Models with Continuous Tokens' Text: 'Scaling up autoregressive models in vision has not proven as beneficial as in large language models. In this work, we investigate this scaling problem in the context of text-to-image generation, focusing on two critical factors: whether models use discrete or continuous tokens, and whether tokens are generated in a random or fixed raster order using BERT- or GPT-like transformer architectures. Our empirical results show that, while all models scale effectively in terms of validation loss, their evaluation performance -- measured by FID, GenEval score, and visual quality -- follows different trends. Models based on continuous tokens achieve significantly better visual quality than those using discrete tokens. Furthermore, the generation order and attention mechanisms significantly affect the GenEval score: random-order models achieve notably better GenEval scores compared to raster-order models. Inspired by these findings, we train Fluid, a random-order autoregressive model on continuous tokens. Fluid 10.5B model achieves a new state-of-the-art zero-shot FID of 6.16 on MS-COCO 30K, and 0.69 overall score on the GenEval benchmark. We hope our findings and results will encourage future efforts to further bridge the scaling gap between vision and language models.'
[20.10.2024 08:15] Response: **Image Prompt:** Create a surreal landscape where fluid forms merge and transform into abstract representations of text and imagery. Imagine a giant, flowing ribbon of text that twists and spirals, reflecting the continuous token concept, interspersed with vivid, dreamlike images symbolizing the generative process. Include ethereal shapes resembling both discrete and continuous tokens, with some appearing as clear geometric forms while others are amorphous and shifting. In the background, a distorted clock represents the random order of generation, with the hands swirling chaotically. The color palette should be vibrant and dynamic, conveying a sense of motion and innovation.

**Label:** "Fluid: Scaling Autoregressive Text-to-Image Generative Models with Continuous Tokens"
[20.10.2024 08:15] Generating image by prompt: **Image Prompt:** Create a surreal landscape where fluid forms merge and transform into abstract representations of text and imagery. Imagine a giant, flowing ribbon of text that twists and spirals, reflecting the continuous token concept, interspersed with vivid, dreamlike images symbolizing the generative process. Include ethereal shapes resembling both discrete and continuous tokens, with some appearing as clear geometric forms while others are amorphous and shifting. In the background, a distorted clock represents the random order of generation, with the hands swirling chaotically. The color palette should be vibrant and dynamic, conveying a sense of motion and innovation.

**Label:** "Fluid: Scaling Autoregressive Text-to-Image Generative Models with Continuous Tokens".
[20.10.2024 08:15] Saving generated image from https://fal.media/files/kangaroo/HsGxwzF5s22mAbYOQk7eW.png to 3fa9a449112a391b.jpg.
[20.10.2024 16:08] Read previous papers.
[20.10.2024 16:08] Get feed.
[20.10.2024 16:08] Get page data from previous paper. URL: https://huggingface.co/papers/2410.13720
[20.10.2024 16:08] Get page data from previous paper. URL: https://huggingface.co/papers/2410.13754
[20.10.2024 16:08] Get page data from previous paper. URL: https://huggingface.co/papers/2410.13757
[20.10.2024 16:08] Get page data from previous paper. URL: https://huggingface.co/papers/2410.13268
[20.10.2024 16:08] Get page data from previous paper. URL: https://huggingface.co/papers/2410.13863
[20.10.2024 16:08] Get page data from previous paper. URL: https://huggingface.co/papers/2410.12784
[20.10.2024 16:08] Get page data from previous paper. URL: https://huggingface.co/papers/2410.13848
[20.10.2024 16:08] Get page data from previous paper. URL: https://huggingface.co/papers/2410.13824
[20.10.2024 16:08] Get page data from previous paper. URL: https://huggingface.co/papers/2410.12705
[20.10.2024 16:08] Get page data from previous paper. URL: https://huggingface.co/papers/2410.13830
[20.10.2024 16:08] Get page data from previous paper. URL: https://huggingface.co/papers/2410.13085
[20.10.2024 16:08] Get page data from previous paper. URL: https://huggingface.co/papers/2410.13804
[20.10.2024 16:08] Get page data from previous paper. URL: https://huggingface.co/papers/2410.11842
[20.10.2024 16:08] Get page data from previous paper. URL: https://huggingface.co/papers/2410.13785
[20.10.2024 16:08] Get page data from previous paper. URL: https://huggingface.co/papers/2410.13841
[20.10.2024 16:08] Get page data from previous paper. URL: https://huggingface.co/papers/2410.13639
[20.10.2024 16:08] Get page data from previous paper. URL: https://huggingface.co/papers/2410.09426
[20.10.2024 16:08] Get page data from previous paper. URL: https://huggingface.co/papers/2410.13832
[20.10.2024 16:08] Get page data from previous paper. URL: https://huggingface.co/papers/2410.13360
[20.10.2024 16:08] Get page data from previous paper. URL: https://huggingface.co/papers/2410.09019
[20.10.2024 16:08] Get page data from previous paper. URL: https://huggingface.co/papers/2410.13852
[20.10.2024 16:08] Get page data from previous paper. URL: https://huggingface.co/papers/2410.13198
[20.10.2024 16:08] Get page data from previous paper. URL: https://huggingface.co/papers/2410.13859
[20.10.2024 16:08] Get page data from previous paper. URL: https://huggingface.co/papers/2410.13854
[20.10.2024 16:08] Get page data from previous paper. URL: https://huggingface.co/papers/2410.12781
[20.10.2024 16:08] Get page data from previous paper. URL: https://huggingface.co/papers/2410.12771
[20.10.2024 16:08] Get page data from previous paper. URL: https://huggingface.co/papers/2410.12957
[20.10.2024 16:08] Get page data from previous paper. URL: https://huggingface.co/papers/2410.13334
[20.10.2024 16:08] Get page data from previous paper. URL: https://huggingface.co/papers/2410.13618
[20.10.2024 16:08] Get page data from previous paper. URL: https://huggingface.co/papers/2410.13060
[20.10.2024 16:08] Get page data from previous paper. URL: https://huggingface.co/papers/2410.09347
[20.10.2024 16:08] Get page data from previous paper. URL: https://huggingface.co/papers/2410.12183
[20.10.2024 16:08] Get page data from previous paper. URL: https://huggingface.co/papers/2410.13293
[20.10.2024 16:08] Get page data from previous paper. URL: https://huggingface.co/papers/2410.10210
[20.10.2024 16:08] ********************************************************************************
[20.10.2024 16:08] Abstract 0. We present Movie Gen, a cast of foundation models that generates high-quality, 1080p HD videos with different aspect ratios and synchronized audio. We also show additional capabilities such as precise instruction-based video editing and generation of personalized videos based on a user's image. Our ...
[20.10.2024 16:08] ********************************************************************************
[20.10.2024 16:08] Abstract 1. Perceiving and generating diverse modalities are crucial for AI models to effectively learn from and engage with real-world signals, necessitating reliable evaluations for their development. We identify two major issues in current evaluations: (1) inconsistent standards, shaped by different communit...
[20.10.2024 16:08] ********************************************************************************
[20.10.2024 16:08] Abstract 2. Current mobile assistants are limited by dependence on system APIs or struggle with complex user instructions and diverse interfaces due to restricted comprehension and decision-making abilities. To address these challenges, we propose MobA, a novel Mobile phone Agent powered by multimodal large lan...
[20.10.2024 16:08] ********************************************************************************
[20.10.2024 16:08] Abstract 3. The success of large language models (LLMs) has prompted efforts to integrate speech and audio data, aiming to create general foundation models capable of processing both textual and non-textual inputs. Recent advances, such as GPT-4o, highlight the potential for end-to-end speech LLMs, which preser...
[20.10.2024 16:08] ********************************************************************************
[20.10.2024 16:08] Abstract 4. Scaling up autoregressive models in vision has not proven as beneficial as in large language models. In this work, we investigate this scaling problem in the context of text-to-image generation, focusing on two critical factors: whether models use discrete or continuous tokens, and whether tokens ar...
[20.10.2024 16:08] ********************************************************************************
[20.10.2024 16:08] Abstract 5. LLM-based judges have emerged as a scalable alternative to human evaluation and are increasingly used to assess, compare, and improve models. However, the reliability of LLM-based judges themselves is rarely scrutinized. As LLMs become more advanced, their responses grow more sophisticated, requirin...
[20.10.2024 16:08] ********************************************************************************
[20.10.2024 16:08] Abstract 6. In this paper, we introduce Janus, an autoregressive framework that unifies multimodal understanding and generation. Prior research often relies on a single visual encoder for both tasks, such as Chameleon. However, due to the differing levels of information granularity required by multimodal unders...
[20.10.2024 16:08] ********************************************************************************
[20.10.2024 16:08] Abstract 7. Text-rich visual understanding-the ability to process environments where dense textual content is integrated with visuals-is crucial for multimodal large language models (MLLMs) to interact effectively with structured environments. To enhance this capability, we propose synthesizing general multimod...
[20.10.2024 16:08] ********************************************************************************
[20.10.2024 16:08] Abstract 8. Vision Language Models (VLMs) often struggle with culture-specific knowledge, particularly in languages other than English and in underrepresented cultural contexts. To evaluate their understanding of such knowledge, we introduce WorldCuisines, a massive-scale benchmark for multilingual and multicul...
[20.10.2024 16:08] ********************************************************************************
[20.10.2024 16:08] Abstract 9. Recent advances in customized video generation have enabled users to create videos tailored to both specific subjects and motion trajectories. However, existing methods often require complicated test-time fine-tuning and struggle with balancing subject learning and motion control, limiting their rea...
[20.10.2024 16:08] ********************************************************************************
[20.10.2024 16:08] Abstract 10. Artificial Intelligence (AI) has demonstrated significant potential in healthcare, particularly in disease diagnosis and treatment planning. Recent progress in Medical Large Vision-Language Models (Med-LVLMs) has opened up new possibilities for interactive diagnostic tools. However, these models oft...
[20.10.2024 16:08] ********************************************************************************
[20.10.2024 16:08] Abstract 11. Evaluating large language models (LLMs) is costly: it requires the generation and examination of LLM outputs on a large-scale benchmark of various tasks. This paper investigates how to efficiently reduce the tasks used to benchmark LLMs without affecting the evaluation quality. Our study reveals tha...
[20.10.2024 16:08] ********************************************************************************
[20.10.2024 16:08] Abstract 12. In this work, we upgrade the multi-head attention mechanism, the core of the Transformer model, to improve efficiency while maintaining or surpassing the previous accuracy level. We show that multi-head attention can be expressed in the summation form. Drawing on the insight that not all attention h...
[20.10.2024 16:08] ********************************************************************************
[20.10.2024 16:08] Abstract 13. Alignment of large language models (LLMs) involves training models on preference-contrastive output pairs to adjust their responses according to human preferences. To obtain such contrastive pairs, traditional methods like RLHF and RLAIF rely on limited contrasting patterns, such as varying model va...
[20.10.2024 16:08] ********************************************************************************
[20.10.2024 16:08] Abstract 14. Post-training has emerged as a crucial paradigm for adapting large-scale pre-trained models to various tasks, whose effects are fully reflected by delta parameters (i.e., the disparity between post-trained and pre-trained parameters). While numerous studies have explored delta parameter properties v...
[20.10.2024 16:08] ********************************************************************************
[20.10.2024 16:08] Abstract 15. Enabling Large Language Models (LLMs) to handle a wider range of complex tasks (e.g., coding, math) has drawn great attention from many researchers. As LLMs continue to evolve, merely increasing the number of model parameters yields diminishing performance improvements and heavy computational costs....
[20.10.2024 16:08] ********************************************************************************
[20.10.2024 16:08] Abstract 16. Recently, quantization has been widely used for the compression and acceleration of large language models~(LLMs). Due to the outliers in LLMs, it is crucial to flatten weights and activations to minimize quantization error with the equally spaced quantization points. Prior research explores various ...
[20.10.2024 16:08] ********************************************************************************
[20.10.2024 16:08] Abstract 17. Panoramic image stitching provides a unified, wide-angle view of a scene that extends beyond the camera's field of view. Stitching frames of a panning video into a panoramic photograph is a well-understood problem for stationary scenes, but when objects are moving, a still panorama cannot capture th...
[20.10.2024 16:08] ********************************************************************************
[20.10.2024 16:08] Abstract 18. The development of large language models (LLMs) has significantly enhanced the capabilities of multimodal LLMs (MLLMs) as general assistants. However, lack of user-specific knowledge still restricts their application in human's daily life. In this paper, we introduce the Retrieval Augmented Personal...
[20.10.2024 16:08] ********************************************************************************
[20.10.2024 16:08] Abstract 19. Language models (LMs) have demonstrated expert-level reasoning and recall abilities in medicine. However, computational costs and privacy concerns are mounting barriers to wide-scale implementation. We introduce a parsimonious adaptation of phi-3-mini, MedMobile, a 3.8 billion parameter LM capable o...
[20.10.2024 16:08] ********************************************************************************
[20.10.2024 16:08] Abstract 20. Multi-turn interactions between large language models (LLMs) and users naturally include implicit feedback signals. If an LLM responds in an unexpected way to an instruction, the user is likely to signal it by rephrasing the request, expressing frustration, or pivoting to an alternative task. Such s...
[20.10.2024 16:08] ********************************************************************************
[20.10.2024 16:08] Abstract 21. Generative Error Correction (GEC) has emerged as a powerful post-processing method to enhance the performance of Automatic Speech Recognition (ASR) systems. However, we show that GEC models struggle to generalize beyond the specific types of errors encountered during training, limiting their ability...
[20.10.2024 16:08] ********************************************************************************
[20.10.2024 16:08] Abstract 22. Despite the significant progress in multimodal large language models (MLLMs), their high computational cost remains a barrier to real-world deployment. Inspired by the mixture of depths (MoDs) in natural language processing, we aim to address this limitation from the perspective of ``activated token...
[20.10.2024 16:08] ********************************************************************************
[20.10.2024 16:08] Abstract 23. As the capabilities of Multimodal Large Language Models (MLLMs) continue to improve, the need for higher-order capability evaluation of MLLMs is increasing. However, there is a lack of work evaluating MLLM for higher-order perception and understanding of Chinese visual content. To fill the gap, we i...
[20.10.2024 16:08] ********************************************************************************
[20.10.2024 16:08] Abstract 24. We propose Long-LRM, a generalizable 3D Gaussian reconstruction model that is capable of reconstructing a large scene from a long sequence of input images. Specifically, our model can process 32 source images at 960x540 resolution within only 1.3 seconds on a single A100 80G GPU. Our architecture fe...
[20.10.2024 16:08] ********************************************************************************
[20.10.2024 16:08] Abstract 25. The ability to discover new materials with desirable properties is critical for numerous applications from helping mitigate climate change to advances in next generation computing hardware. AI has the potential to accelerate materials discovery and design by more effectively exploring the chemical s...
[20.10.2024 16:08] ********************************************************************************
[20.10.2024 16:08] Abstract 26. Generating music that aligns with the visual content of a video has been a challenging task, as it requires a deep understanding of visual semantics and involves generating music whose melody, rhythm, and dynamics harmonize with the visual narratives. This paper presents MuVi, a novel framework that...
[20.10.2024 16:08] ********************************************************************************
[20.10.2024 16:08] Abstract 27. Although large language models (LLMs) demonstrate impressive proficiency in various tasks, they present potential safety risks, such as `jailbreaks', where malicious inputs can coerce LLMs into generating harmful content. To address these issues, many LLM developers have implemented various safety m...
[20.10.2024 16:08] ********************************************************************************
[20.10.2024 16:08] Abstract 28. The rapid growth of model scale has necessitated substantial computational resources for fine-tuning. Existing approach such as Low-Rank Adaptation (LoRA) has sought to address the problem of handling the large updated parameters in full fine-tuning. However, LoRA utilize random initialization and o...
[20.10.2024 16:08] ********************************************************************************
[20.10.2024 16:08] Abstract 29. The pervasiveness of proprietary language models has raised privacy concerns for users' sensitive data, emphasizing the need for private inference (PI), where inference is performed directly on encrypted inputs. However, current PI methods face prohibitively higher communication and latency overhead...
[20.10.2024 16:08] ********************************************************************************
[20.10.2024 16:08] Abstract 30. Classifier-Free Guidance (CFG) is a critical technique for enhancing the sample quality of visual generative models. However, in autoregressive (AR) multi-modal generation, CFG introduces design inconsistencies between language and visual content, contradicting the design philosophy of unifying diff...
[20.10.2024 16:08] ********************************************************************************
[20.10.2024 16:08] Abstract 31. Vision-language foundation models (such as CLIP) have recently shown their power in transfer learning, owing to large-scale image-text pre-training. However, target domain data in the downstream tasks can be highly different from the pre-training phase, which makes it hard for such a single model to...
[20.10.2024 16:08] ********************************************************************************
[20.10.2024 16:08] Abstract 32. Many students struggle with math word problems (MWPs), often finding it difficult to identify key information and select the appropriate mathematical operations.Schema-based instruction (SBI) is an evidence-based strategy that helps students categorize problems based on their structure, improving pr...
[20.10.2024 16:08] ********************************************************************************
[20.10.2024 16:08] Abstract 33. As large language models rapidly evolve to support longer context, there is a notable disparity in their capability to generate output at greater lengths. Recent study suggests that the primary cause for this imbalance may arise from the lack of data with long-output during alignment training. In li...
[20.10.2024 16:08] Read previous papers.
[20.10.2024 16:08] Generating reviews via LLM API.
[20.10.2024 16:08] Using data from previous issue: {"desc": "Movie Gen представляет собой набор фундаментальных моделей для генерации высококачественных видео с синхронизированным аудио. Модели устанавливают новый state-of-the-art в нескольких задачах, включая синтез видео по тексту и редактирование видео. Крупнейшая модель имеет 30 миллиардов парам
[20.10.2024 16:08] Using data from previous issue: {"desc": "Статья представляет MixEval-X - первый многомодальный бенчмарк для оценки моделей ИИ в реальных задачах. Авторы предлагают новые методы для создания репрезентативных наборов тестов, охватывающих различные модальности ввода и вывода. Бенчмарк решает проблемы несогласованности стандартов оце
[20.10.2024 16:08] Using data from previous issue: {"desc": "MobA - это новый мобильный агент, основанный на мультимодальных больших языковых моделях. Он использует двухуровневую архитектуру с глобальным агентом для понимания команд и планирования, и локальным агентом для выполнения конкретных действий. Система включает модуль рефлексии для эффектив
[20.10.2024 16:08] Using data from previous issue: {"desc": "В статье предлагается дорожная карта из пяти уровней для развития речевых языковых моделей (LLM), способных обрабатывать как текстовые, так и нетекстовые входные данные. Авторы разработали бенчмарк SAGI для стандартизации оценки различных задач на этих уровнях. Исследование выявило пробелы
[20.10.2024 16:08] Using data from previous issue: {"desc": "Исследователи изучили проблему масштабирования авторегрессионных моделей в контексте генерации изображений по тексту. Они сравнили модели с дискретными и непрерывными токенами, а также с различным порядком генерации и архитектурами трансформеров. Результаты показали, что модели с непрерывн
[20.10.2024 16:08] Using data from previous issue: {"desc": "Статья представляет новую систему оценки судей на основе больших языковых моделей (LLM). Авторы предлагают JudgeBench - набор тестов для оценки LLM-судей в сложных задачах, охватывающих знания, рассуждения, математику и программирование. JudgeBench использует новый подход к созданию сложны
[20.10.2024 16:08] Using data from previous issue: {"desc": "Статья представляет Janus - новую авторегрессивную модель для мультимодального понимания и генерации. В отличие от предыдущих подходов, Janus использует отдельные визуальные энкодеры для задач понимания и генерации, что позволяет оптимизировать работу модели. Единая архитектура трансформер
[20.10.2024 16:08] Using data from previous issue: {"desc": "В статье представлен новый подход к улучшению понимания визуального контекста с текстом для мультимодальных больших языковых моделей. Авторы предлагают синтезировать инструкции из веб-интерфейсов с помощью текстовых языковых моделей. Создан датасет MultiUI, содержащий 7,3 миллиона образцов
[20.10.2024 16:08] Using data from previous issue: {"desc": "Статья представляет новый бенчмарк WorldCuisines для оценки понимания культурно-специфических знаний моделями компьютерного зрения и обработки естественного языка. Бенчмарк включает набор данных для визуального ответа на вопросы (VQA) с парами текст-изображение на 30 языках и диалектах, ох
[20.10.2024 16:08] Using data from previous issue: {"desc": "DreamVideo-2 - это новая система для создания персонализированных видео без дополнительного обучения. Она использует одно изображение и последовательность ограничивающих рамок для генерации видео с заданным объектом и траекторией движения. Система вводит референсное внимание и маскированны
[20.10.2024 16:08] Using data from previous issue: {"desc": "В статье представлена система MMed-RAG, разработанная для повышения фактической точности медицинских крупномасштабных моделей зрения и языка (Med-LVLMs). Система использует механизм поиска с учетом домена, адаптивный метод выбора найденных контекстов и стратегию дообучения на основе RAG. Э
[20.10.2024 16:08] Using data from previous issue: {"desc": "Статья исследует методы эффективного сокращения количества задач для оценки больших языковых моделей (LLM) без ущерба для качества оценки. Авторы предлагают метрику для оценки переносимости между задачами с помощью обучения в контексте (ICL). Анализируя попарную переносимость, можно сократ
[20.10.2024 16:08] Using data from previous issue: {"desc": "Исследователи предлагают новый механизм внимания Mixture-of-Head (MoH), который улучшает эффективность многоголового внимания в трансформерах. MoH позволяет каждому токену выбирать подходящие головы внимания, повышая эффективность вывода без ущерба для точности. Эксперименты на ViT, DiT и 
[20.10.2024 16:08] Using data from previous issue: {"desc": "Статья представляет новый подход к выравниванию больших языковых моделей под названием PopAlign. Этот метод использует разнообразные контрастные паттерны на уровнях промпта, модели и пайплайна для улучшения данных о предпочтениях. PopAlign решает проблемы ограниченности традиционных методо
[20.10.2024 16:08] Using data from previous issue: {"desc": "Статья представляет новый подход к анализу дельта-параметров в пост-обучении моделей машинного обучения, основанный на аппроксимации Римана функции потерь. Авторы классифицируют существующие методы редактирования дельта-параметров на три категории в зависимости от их влияния на производите
[20.10.2024 16:08] Using data from previous issue: {"desc": "Статья исследует механизмы улучшения рассуждений в крупных языковых моделях (LLM) с помощью стратегий вывода. Авторы сравнивают модель o1 от OpenAI с другими методами вычислений во время тестирования на задачах рассуждения в областях математики, программирования и здравого смысла. Исследов
[20.10.2024 16:08] Using data from previous issue: {"desc": "FlatQuant - это новый подход к квантованию после обучения, направленный на улучшение сглаженности весов и активаций в больших языковых моделях (LLM). Метод использует оптимальные аффинные преобразования для каждого линейного слоя, калибруемые за часы с помощью облегченной целевой функции. 
[20.10.2024 16:08] Using data from previous issue: {"desc": "Статья представляет метод синтеза панорамного видео из обычного панорамного видео, снятого вручную. Авторы рассматривают задачу как проблему заполнения пространственно-временного объема, используя генеративные модели видео. Предложенный подход позволяет создавать видеопанорамы для различны
[20.10.2024 16:08] Using data from previous issue: {"desc": "Статья представляет фреймворк Retrieval Augmented Personalization (RAP) для персонализации мультимодальных языковых моделей (MLLM). RAP использует базу данных ключ-значение для хранения информации о пользователе, извлекает релевантные данные с помощью мультимодального ретривера и генерируе
[20.10.2024 16:08] Using data from previous issue: {"desc": "Статья представляет MedMobile - компактную языковую модель для медицинских приложений, способную работать на мобильных устройствах. Модель, основанная на phi-3-mini, имеет всего 3,8 миллиарда параметров, но демонстрирует впечатляющие результаты на тесте MedQA (USMLE), превосходя проходной 
[20.10.2024 16:08] Using data from previous issue: {"desc": "Статья представляет метод ReSpect, позволяющий языковым моделям (LLM) учиться на неявных сигналах обратной связи в ходе взаимодействия с пользователями. Авторы применяют ReSpect в сценарии мультимодального взаимодействия, где люди инструктируют LLM для решения задач абстрактного мышления. 
[20.10.2024 16:08] Using data from previous issue: {"desc": "Статья представляет новый подход DARAG для улучшения генеративной коррекции ошибок (GEC) в системах автоматического распознавания речи (ASR). DARAG использует синтетические данные, созданные с помощью языковых моделей и систем text-to-speech, для расширения обучающего набора данных. Метод 
[20.10.2024 16:08] Using data from previous issue: {"desc": "Статья представляет новый метод gamma-MoD для оптимизации мультимодальных больших языковых моделей (MLLM). Авторы предлагают стратегию адаптации Mixture of Depths (MoD) для существующих MLLM, используя метрику ранга карт внимания (ARank) для определения избыточных слоев. Метод включает общ
[20.10.2024 16:08] Using data from previous issue: {"desc": "CII-Bench - это новый бенчмарк для оценки способностей мультимодальных языковых моделей (MLLM) к восприятию и пониманию китайских изображений высокого уровня. Он включает аутентичные китайские изображения, в том числе отражающие традиционную культуру. Эксперименты показали значительный раз
[20.10.2024 16:08] Using data from previous issue: {"desc": "Long-LRM - это обобщенная модель реконструкции 3D гауссовых распределений, способная восстанавливать большие сцены из длинных последовательностей изображений. Модель может обрабатывать 32 исходных изображения с разрешением 960x540 всего за 1,3 секунды на одном GPU A100 80G. Архитектура соч
[20.10.2024 16:08] Using data from previous issue: {"desc": "Статья представляет новый крупномасштабный открытый датасет Open Materials 2024 (OMat24) и набор предобученных моделей для ускорения открытия новых материалов с помощью искусственного интеллекта. OMat24 содержит более 110 миллионов расчетов методом функционала плотности (DFT), фокусируясь 
[20.10.2024 16:08] Using data from previous issue: {"desc": "MuVi - это новая система для создания музыки, синхронизированной с видеоконтентом. Она анализирует видео с помощью специального визуального адаптера для извлечения релевантных характеристик. Затем эти характеристики используются для генерации музыки, соответствующей настроению, теме, ритму
[20.10.2024 16:08] Using data from previous issue: {"desc": "Статья исследует проблемы безопасности больших языковых моделей (LLM) и методы их обхода. Авторы вводят понятие PCJailbreak, демонстрирующее риски, связанные с намеренными смещениями, внедренными в LLM для обеспечения безопасности. Исследование показывает значительную разницу в успешности 
[20.10.2024 16:08] Using data from previous issue: {"desc": "Статья представляет новый метод эффективной настройки параметров (PEFT) под названием LoLDU. Этот подход использует LDU-разложение для инициализации матриц низкого ранга, что обеспечивает более быструю сходимость и ортогональность. LoLDU значительно сокращает количество обучаемых параметро
[20.10.2024 16:08] Using data from previous issue: {"desc": "Статья представляет комплексный анализ роли нелинейных операций в трансформерных языковых моделях декодер-только архитектуры. Авторы предлагают AERO - четырехэтапную структуру оптимизации архитектуры, которая систематически удаляет нелинейности, такие как LayerNorm и GELU, и сокращает коли
[20.10.2024 16:08] Using data from previous issue: {"desc": "Статья представляет новый метод под названием Condition Contrastive Alignment (CCA) для улучшения качества генерации изображений в авторегрессионных мультимодальных моделях без использования техники Classifier-Free Guidance (CFG). CCA основан на методах выравнивания языковых моделей и напр
[20.10.2024 16:08] Using data from previous issue: {"desc": "Статья представляет TransAgent - фреймворк для улучшения обобщающей способности моделей компьютерного зрения и обработки естественного языка типа CLIP. TransAgent объединяет знания из различных экспертных моделей через мультимодальную дистилляцию знаний. Это позволяет эффективно адаптирова
[20.10.2024 16:08] Using data from previous issue: {"desc": "Авторы предлагают новый подход к решению математических текстовых задач, называемый SBI-RAG (Schema-Based Instruction Retrieval-Augmented Generation). Этот метод сочетает в себе схемо-ориентированное обучение (SBI) и большую языковую модель (LLM) для пошагового рассуждения при решении зада
[20.10.2024 16:08] Using data from previous issue: {"desc": "В статье исследуется проблема неспособности больших языковых моделей генерировать длинные тексты. Авторы предлагают метод дообучения моделей на специально подготовленном наборе данных с длинными ответами. Эксперименты показывают, что даже небольшое количество качественных данных позволяет 
[20.10.2024 16:08] Trying to get texts in Chinese.
[20.10.2024 16:08] Mistral request. Model: mistral-large-latest. Prompt: Write simple and brief explanation (4-5 sentences) of an article in Chinese. Use short sentences. Text:

We present Movie Gen, a cast of foundation models that generates high-quality, 1080p HD videos with different aspect ratios and synchronized audio. We also show additional capabilities such as precise instruction-based video editing and generation of personalized videos based on a user's image. Our models set a new state-of-the-art on multiple tasks: text-to-video synthesis, video personalization, video editing, video-to-audio generation, and text-to-audio generation. Our largest video generation model is a 30B parameter transformer trained with a maximum context length of 73K video tokens, corresponding to a generated video of 16 seconds at 16 frames-per-second. We show multiple technical innovations and simplifications on the architecture, latent spaces, training objectives and recipes, data curation, evaluation protocols, parallelization techniques, and inference optimizations that allow us to reap the benefits of scaling pre-training data, model size, and training compute for training large scale media generation models. We hope this paper helps the research community to accelerate progress and innovation in media generation models. All videos from this paper are available at https://go.fb.me/MovieGenResearchVideos.
[20.10.2024 16:08] Mistral response. {"id": "63fc2308ffae407b8f348db66a3bfa77", "object": "chat.completion", "created": 1729440495, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "content": "\u8fd9\u7bc7\u6587\u7ae0\u4ecb\u7ecd\u4e86Movie Gen\uff0c\u4e00\u79cd\u80fd\u751f\u6210\u9ad8\u8d28\u91cf1080p\u89c6\u9891\u7684\u57fa\u7840\u6a21\u578b\u3002\u5b83\u53ef\u4ee5\u7f16\u8f91\u89c6\u9891\uff0c\u751f\u6210\u4e2a\u6027\u5316\u89c6\u9891\uff0c\u8fd8\u80fd\u540c\u6b65\u97f3\u9891\u3002\u6a21\u578b\u5728\u591a\u9879\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u5982\u6587\u672c\u5230\u89c6\u9891\u5408\u6210\u3001\u89c6\u9891\u7f16\u8f91\u7b49\u3002\u6700\u5927\u7684\u6a21\u578b\u6709300\u4ebf\u53c2\u6570\uff0c\u53ef\u751f\u621016\u79d2\u89c6\u9891\u3002\u7814\u7a76\u56e2\u961f\u5e0c\u671b\u8fd9\u9879\u5de5\u4f5c\u80fd\u63a8\u52a8\u5a92\u4f53\u751f\u6210\u6a21\u578b\u7684\u8fdb\u6b65\u3002\u6240\u6709\u89c6\u9891\u53ef\u5728\u94fe\u63a5\u4e2d\u67e5\u770b\u3002", "tool_calls": null}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 294, "total_tokens": 439, "completion_tokens": 145}}
[20.10.2024 16:08] Response: 这篇文章介绍了Movie Gen，一种能生成高质量1080p视频的基础模型。它可以编辑视频，生成个性化视频，还能同步音频。模型在多项任务上表现出色，如文本到视频合成、视频编辑等。最大的模型有300亿参数，可生成16秒视频。研究团队希望这项工作能推动媒体生成模型的进步。所有视频可在链接中查看。
[20.10.2024 16:08] Mistral request. Model: mistral-large-latest. Prompt: Write pinyin transcription for text. Text:

这篇文章介绍了Movie Gen，一种能生成高质量1080p视频的基础模型。它可以编辑视频，生成个性化视频，还能同步音频。模型在多项任务上表现出色，如文本到视频合成、视频编辑等。最大的模型有300亿参数，可生成16秒视频。研究团队希望这项工作能推动媒体生成模型的进步。所有视频可在链接中查看。
[20.10.2024 16:08] Mistral response. {"id": "0fba792e426a49cf82bce009e9037efd", "object": "chat.completion", "created": 1729440499, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "content": "\u8fd9\u7bc7\u6587\u7ae0\u4ecb\u7ecd\u4e86Movie Gen\uff0c\u4e00\u79cd\u80fd\u751f\u6210\u9ad8\u8d28\u91cf1080p\u89c6\u9891\u7684\u57fa\u7840\u6a21\u578b\u3002\u5b83\u53ef\u4ee5\u7f16\u8f91\u89c6\u9891\uff0c\u751f\u6210\u4e2a\u6027\u5316\u89c6\u9891\uff0c\u8fd8\u80fd\u540c\u6b65\u97f3\u9891\u3002\u6a21\u578b\u5728\u591a\u9879\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u5982\u6587\u672c\u5230\u89c6\u9891\u5408\u6210\u3001\u89c6\u9891\u7f16\u8f91\u7b49\u3002\u6700\u5927\u7684\u6a21\u578b\u6709300\u4ebf\u53c2\u6570\uff0c\u53ef\u751f\u621016\u79d2\u89c6\u9891\u3002\u7814\u7a76\u56e2\u961f\u5e0c\u671b\u8fd9\u9879\u5de5\u4f5c\u80fd\u63a8\u52a8\u5a92\u4f53\u751f\u6210\u6a21\u578b\u7684\u8fdb\u6b65\u3002\u6240\u6709\u89c6\u9891\u53ef\u5728\u94fe\u63a5\u4e2d\u67e5\u770b\u3002\n\nzh\u00e8 pi\u0101n w\u00e9n zh\u0101ng ji\u00e8 sh\u00e0o le Movie Gen, y\u012b zh\u01d2ng n\u00e9ng sh\u0113ng ch\u01cen g\u0101o zh\u00ec li\u00e0ng 1080p sh\u00ec p\u00edn de j\u012b ch\u01d4 m\u00f3 x\u00edng. t\u0101 k\u011b y\u01d0 bi\u01cen j\u00ed sh\u00ec p\u00edn, sh\u0113ng ch\u01cen g\u00e8 x\u00ecng hu\u00e0 sh\u00ec p\u00edn, h\u00e1i n\u00e9ng t\u00f3ng b\u00f9 y\u012bn p\u00edn. m\u00f3 x\u00edng z\u00e0i du\u014d xi\u00e0ng r\u00e8n w\u00f9 sh\u00e0ng bi\u01ceo xi\u01cen ch\u016b s\u00e8, r\u00fa w\u00e9n b\u011bn d\u00e0o sh\u00ec p\u00edn h\u00e9 ch\u00e9ng, sh\u00ec p\u00edn bi\u01cen j\u00ed d\u011bng. zu\u00ec d\u00e0 de m\u00f3 x\u00edng y\u01d2u 300 y\u00ec c\u0101n sh\u01d4, k\u011b sh\u0113ng ch\u01cen 16 mi\u01ceo sh\u00ec p\u00edn. y\u00e1n ji\u016b tu\u00e1n du\u00ec x\u012b w\u00e0ng zh\u00e8 xi\u00e0ng g\u014dng zu\u00f2 n\u00e9ng tu\u012b d\u00f2ng m\u00e9i t\u01d0 sh\u0113ng ch\u01cen m\u00f3 x\u00edng de j\u00ecn b\u00f9. su\u01d2 y\u01d2u sh\u00ec p\u00edn k\u011b z\u00e0i li\u00e1n ji\u0113 zh\u014dng ch\u00e1 k\u00e0n.\n\nHere is the pinyin transcription for the given text.", "tool_calls": null}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 160, "total_tokens": 629, "completion_tokens": 469}}
[20.10.2024 16:08] Response: 这篇文章介绍了Movie Gen，一种能生成高质量1080p视频的基础模型。它可以编辑视频，生成个性化视频，还能同步音频。模型在多项任务上表现出色，如文本到视频合成、视频编辑等。最大的模型有300亿参数，可生成16秒视频。研究团队希望这项工作能推动媒体生成模型的进步。所有视频可在链接中查看。

zhè piān wén zhāng jiè shào le Movie Gen, yī zhǒng néng shēng chǎn gāo zhì liàng 1080p shì pín de jī chǔ mó xíng. tā kě yǐ biǎn jí shì pín, shēng chǎn gè xìng huà shì pín, hái néng tóng bù yīn pín. mó xíng zài duō xiàng rèn wù shàng biǎo xiǎn chū sè, rú wén běn dào shì pín hé chéng, shì pín biǎn jí děng. zuì dà de mó xíng yǒu 300 yì cān shǔ, kě shēng chǎn 16 miǎo shì pín. yán jiū tuán duì xī wàng zhè xiàng gōng zuò néng tuī dòng méi tǐ shēng chǎn mó xíng de jìn bù. suǒ yǒu shì pín kě zài lián jiē zhōng chá kàn.

Here is the pinyin transcription for the given text.
[20.10.2024 16:08] Mistral request. Model: mistral-large-latest. Prompt: Write vocab of difficult words for this text as an array of objects with fields 'word', 'pinyin', 'trans'. Return as python list without formatting. Return list and nothing else. Text:

这篇文章介绍了Movie Gen，一种能生成高质量1080p视频的基础模型。它可以编辑视频，生成个性化视频，还能同步音频。模型在多项任务上表现出色，如文本到视频合成、视频编辑等。最大的模型有300亿参数，可生成16秒视频。研究团队希望这项工作能推动媒体生成模型的进步。所有视频可在链接中查看。
[20.10.2024 16:08] Mistral response. {"id": "2272f2c4ca564698943f42cc1c1c85a3", "object": "chat.completion", "created": 1729440512, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "content": "[\n    {\"word\": \"\u4ecb\u7ecd\", \"pinyin\": \"ji\u00e8sh\u00e0o\", \"trans\": \"introduce\"},\n    {\"word\": \"\u57fa\u7840\u6a21\u578b\", \"pinyin\": \"j\u012bch\u01d4 m\u00f3x\u00edng\", \"trans\": \"foundational model\"},\n    {\"word\": \"\u751f\u6210\", \"pinyin\": \"sh\u0113ngch\u00e9ng\", \"trans\": \"generate\"},\n    {\"word\": \"\u9ad8\u8d28\u91cf\", \"pinyin\": \"g\u0101o zh\u00ecli\u00e0ng\", \"trans\": \"high quality\"},\n    {\"word\": \"1080p\", \"pinyin\": \"y\u012bl\u00edngb\u0101sh\u00ed p\", \"trans\": \"1080p\"},\n    {\"word\": \"\u89c6\u9891\", \"pinyin\": \"sh\u00ecp\u00edn\", \"trans\": \"video\"},\n    {\"word\": \"\u7f16\u8f91\", \"pinyin\": \"bi\u0101nj\u00ed\", \"trans\": \"edit\"},\n    {\"word\": \"\u4e2a\u6027\u5316\", \"pinyin\": \"g\u00e8x\u00ecnghu\u00e0\", \"trans\": \"personalized\"},\n    {\"word\": \"\u540c\u6b65\", \"pinyin\": \"t\u00f3ngb\u00f9\", \"trans\": \"synchronize\"},\n    {\"word\": \"\u97f3\u9891\", \"pinyin\": \"y\u012bnp\u00edn\", \"trans\": \"audio\"},\n    {\"word\": \"\u8868\u73b0\u51fa\u8272\", \"pinyin\": \"bi\u01ceoxi\u00e0n ch\u016bs\u00e8\", \"trans\": \"perform excellently\"},\n    {\"word\": \"\u6587\u672c\u5230\u89c6\u9891\u5408\u6210\", \"pinyin\": \"w\u00e9nb\u011bn d\u00e0o sh\u00ecp\u00edn h\u00e9ch\u00e9ng\", \"trans\": \"text-to-video synthesis\"},\n    {\"word\": \"\u89c6\u9891\u7f16\u8f91\", \"pinyin\": \"sh\u00ecp\u00edn bi\u0101nj\u00ed\", \"trans\": \"video editing\"},\n    {\"word\": \"\u53c2\u6570\", \"pinyin\": \"c\u0101nsh\u00f9\", \"trans\": \"parameters\"},\n    {\"word\": \"\u7814\u7a76\u56e2\u961f\", \"pinyin\": \"y\u00e1nji\u016b tu\u00e1ndu\u00ec\", \"trans\": \"research team\"},\n    {\"word\": \"\u63a8\u52a8\", \"pinyin\": \"tu\u012bd\u00f2ng\", \"trans\": \"promote\"},\n    {\"word\": \"\u5a92\u4f53\u751f\u6210\u6a21\u578b\", \"pinyin\": \"m\u00e9it\u01d0 sh\u0113ngch\u00e9ng m\u00f3x\u00edng\", \"trans\": \"media generation model\"},\n    {\"word\": \"\u8fdb\u6b65\", \"pinyin\": \"j\u00ecnb\u00f9\", \"trans\": \"progress\"},\n    {\"word\": \"\u94fe\u63a5\", \"pinyin\": \"li\u00e0nji\u0113\", \"trans\": \"link\"},\n    {\"word\": \"\u67e5\u770b\", \"pinyin\": \"ch\u00e1k\u00e0n\", \"trans\": \"view\"}\n]", "tool_calls": null}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 192, "total_tokens": 825, "completion_tokens": 633}}
[20.10.2024 16:08] Response: [
    {"word": "介绍", "pinyin": "jièshào", "trans": "introduce"},
    {"word": "基础模型", "pinyin": "jīchǔ móxíng", "trans": "foundational model"},
    {"word": "生成", "pinyin": "shēngchéng", "trans": "generate"},
    {"word": "高质量", "pinyin": "gāo zhìliàng", "trans": "high quality"},
    {"word": "1080p", "pinyin": "yīlíngbāshí p", "trans": "1080p"},
    {"word": "视频", "pinyin": "shìpín", "trans": "video"},
    {"word": "编辑", "pinyin": "biānjí", "trans": "edit"},
    {"word": "个性化", "pinyin": "gèxìnghuà", "trans": "personalized"},
    {"word": "同步", "pinyin": "tóngbù", "trans": "synchronize"},
    {"word": "音频", "pinyin": "yīnpín", "trans": "audio"},
    {"word": "表现出色", "pinyin": "biǎoxiàn chūsè", "trans": "perform excellently"},
    {"word": "文本到视频合成", "pinyin": "wénběn dào shìpín héchéng", "trans": "text-to-video synthesis"},
    {"word": "视频编辑", "pinyin": "shìpín biānjí", "trans": "video editing"},
    {"word": "参数", "pinyin": "cānshù", "trans": "parameters"},
    {"word": "研究团队", "pinyin": "yánjiū tuánduì", "trans": "research team"},
    {"word": "推动", "pinyin": "tuīdòng", "trans": "promote"},
    {"word": "媒体生成模型", "pinyin": "méitǐ shēngchéng móxíng", "trans": "media generation model"},
    {"word": "进步", "pinyin": "jìnbù", "trans": "progress"},
    {"word": "链接", "pinyin": "liànjiē", "trans": "link"},
    {"word": "查看", "pinyin": "chákàn", "trans": "view"}
]
[20.10.2024 16:08] Mistral request. Model: mistral-large-latest. Prompt: Translate this text in English. Text:

这篇文章介绍了Movie Gen，一种能生成高质量1080p视频的基础模型。它可以编辑视频，生成个性化视频，还能同步音频。模型在多项任务上表现出色，如文本到视频合成、视频编辑等。最大的模型有300亿参数，可生成16秒视频。研究团队希望这项工作能推动媒体生成模型的进步。所有视频可在链接中查看。
[20.10.2024 16:08] Mistral response. {"id": "ac38d1bd56d844aab37ae56bd2ea98ce", "object": "chat.completion", "created": 1729440527, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "content": "This article introduces Movie Gen, a foundational model capable of generating high-quality 1080p videos. It can edit videos, generate personalized videos, and synchronize audio. The model performs excellently in various tasks, such as text-to-video synthesis and video editing. The largest model has 300 billion parameters and can generate 16-second videos. The research team hopes that this work will advance the development of media generation models. All videos can be viewed at the provided link.", "tool_calls": null}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 158, "total_tokens": 266, "completion_tokens": 108}}
[20.10.2024 16:08] Response: This article introduces Movie Gen, a foundational model capable of generating high-quality 1080p videos. It can edit videos, generate personalized videos, and synchronize audio. The model performs excellently in various tasks, such as text-to-video synthesis and video editing. The largest model has 300 billion parameters and can generate 16-second videos. The research team hopes that this work will advance the development of media generation models. All videos can be viewed at the provided link.
[20.10.2024 16:08] Renaming data file.
[20.10.2024 16:08] Renaming previous data. hf_papers.json to ./d/2024-10-18.json
[20.10.2024 16:08] Saving new data file.
[20.10.2024 16:08] Generating page.
[20.10.2024 16:08] Renaming previous page.
[20.10.2024 16:08] Renaming previous data. index.html to ./d/2024-10-18.html
[20.10.2024 16:08] [Experimental] Generating Chinese page for reading.
[20.10.2024 16:08] Chinese vocab [{'word': '介绍', 'pinyin': 'jièshào', 'trans': 'introduce'}, {'word': '基础模型', 'pinyin': 'jīchǔ móxíng', 'trans': 'foundational model'}, {'word': '生成', 'pinyin': 'shēngchéng', 'trans': 'generate'}, {'word': '高质量', 'pinyin': 'gāo zhìliàng', 'trans': 'high quality'}, {'word': '1080p', 'pinyin': 'yīlíngbāshí p', 'trans': '1080p'}, {'word': '视频', 'pinyin': 'shìpín', 'trans': 'video'}, {'word': '编辑', 'pinyin': 'biānjí', 'trans': 'edit'}, {'word': '个性化', 'pinyin': 'gèxìnghuà', 'trans': 'personalized'}, {'word': '同步', 'pinyin': 'tóngbù', 'trans': 'synchronize'}, {'word': '音频', 'pinyin': 'yīnpín', 'trans': 'audio'}, {'word': '表现出色', 'pinyin': 'biǎoxiàn chūsè', 'trans': 'perform excellently'}, {'word': '文本到视频合成', 'pinyin': 'wénběn dào shìpín héchéng', 'trans': 'text-to-video synthesis'}, {'word': '视频编辑', 'pinyin': 'shìpín biānjí', 'trans': 'video editing'}, {'word': '参数', 'pinyin': 'cānshù', 'trans': 'parameters'}, {'word': '研究团队', 'pinyin': 'yánjiū tuánduì', 'trans': 'research team'}, {'word': '推动', 'pinyin': 'tuīdòng', 'trans': 'promote'}, {'word': '媒体生成模型', 'pinyin': 'méitǐ shēngchéng móxíng', 'trans': 'media generation model'}, {'word': '进步', 'pinyin': 'jìnbù', 'trans': 'progress'}, {'word': '链接', 'pinyin': 'liànjiē', 'trans': 'link'}, {'word': '查看', 'pinyin': 'chákàn', 'trans': 'view'}]
[20.10.2024 16:08] Renaming previous Chinese page.
[20.10.2024 16:08] Renaming previous data. zh.html to ./d/2024-10-19_zh_reading_task.html
[20.10.2024 16:08] Writing result.
[20.10.2024 16:08] Writing Chinese reading task.
[20.10.2024 16:08] Renaming log file.
[20.10.2024 16:08] Renaming previous data. log.txt to ./logs/2024-10-20_last_log.txt
