[17.03.2025 10:13] Read previous papers.
[17.03.2025 10:13] Generating top page (month).
[17.03.2025 10:13] Writing top page (month).
[17.03.2025 11:09] Read previous papers.
[17.03.2025 11:09] Get feed.
[17.03.2025 11:09] Get page data from previous paper. URL: https://huggingface.co/papers/2503.07677
[17.03.2025 11:09] Get page data from previous paper. URL: https://huggingface.co/papers/2503.11647
[17.03.2025 11:09] Get page data from previous paper. URL: https://huggingface.co/papers/2503.11646
[17.03.2025 11:09] Get page data from previous paper. URL: https://huggingface.co/papers/2503.11224
[17.03.2025 11:09] Get page data from previous paper. URL: https://huggingface.co/papers/2503.11069
[17.03.2025 11:09] Get page data from previous paper. URL: https://huggingface.co/papers/2503.11514
[17.03.2025 11:09] Get page data from previous paper. URL: https://huggingface.co/papers/2503.10781
[17.03.2025 11:09] Get page data from previous paper. URL: https://huggingface.co/papers/2503.10970
[17.03.2025 11:09] Get page data from previous paper. URL: https://huggingface.co/papers/2503.10772
[17.03.2025 11:09] Get page data from previous paper. URL: https://huggingface.co/papers/2503.10632
[17.03.2025 11:09] Get page data from previous paper. URL: https://huggingface.co/papers/2503.09279
[17.03.2025 11:09] Get page data from previous paper. URL: https://huggingface.co/papers/2503.06553
[17.03.2025 11:09] Get page data from previous paper. URL: https://huggingface.co/papers/2503.06542
[17.03.2025 11:09] Get page data from previous paper. URL: https://huggingface.co/papers/2503.10696
[17.03.2025 11:09] Get page data from previous paper. URL: https://huggingface.co/papers/2503.06674
[17.03.2025 11:09] Get page data from previous paper. URL: https://huggingface.co/papers/2503.10624
[17.03.2025 11:09] Get page data from previous paper. URL: https://huggingface.co/papers/2503.05689
[17.03.2025 11:09] Get page data from previous paper. URL: https://huggingface.co/papers/2503.08111
[17.03.2025 11:09] Extract page data from URL. URL: https://huggingface.co/papers/2503.10620
[17.03.2025 11:09] Extract page data from URL. URL: https://huggingface.co/papers/2503.09330
[17.03.2025 11:09] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[17.03.2025 11:09] No deleted papers detected.
[17.03.2025 11:09] Downloading and parsing papers (pdf, html). Total: 20.
[17.03.2025 11:09] Downloading and parsing paper https://huggingface.co/papers/2503.07677.
[17.03.2025 11:09] Extra JSON file exists (./assets/json/2503.07677.json), skip PDF parsing.
[17.03.2025 11:09] Paper image links file exists (./assets/img_data/2503.07677.json), skip HTML parsing.
[17.03.2025 11:09] Success.
[17.03.2025 11:09] Downloading and parsing paper https://huggingface.co/papers/2503.11647.
[17.03.2025 11:09] Extra JSON file exists (./assets/json/2503.11647.json), skip PDF parsing.
[17.03.2025 11:09] Paper image links file exists (./assets/img_data/2503.11647.json), skip HTML parsing.
[17.03.2025 11:09] Success.
[17.03.2025 11:09] Downloading and parsing paper https://huggingface.co/papers/2503.11646.
[17.03.2025 11:09] Extra JSON file exists (./assets/json/2503.11646.json), skip PDF parsing.
[17.03.2025 11:09] Paper image links file exists (./assets/img_data/2503.11646.json), skip HTML parsing.
[17.03.2025 11:09] Success.
[17.03.2025 11:09] Downloading and parsing paper https://huggingface.co/papers/2503.11224.
[17.03.2025 11:09] Extra JSON file exists (./assets/json/2503.11224.json), skip PDF parsing.
[17.03.2025 11:09] Paper image links file exists (./assets/img_data/2503.11224.json), skip HTML parsing.
[17.03.2025 11:09] Success.
[17.03.2025 11:09] Downloading and parsing paper https://huggingface.co/papers/2503.11069.
[17.03.2025 11:09] Extra JSON file exists (./assets/json/2503.11069.json), skip PDF parsing.
[17.03.2025 11:09] Paper image links file exists (./assets/img_data/2503.11069.json), skip HTML parsing.
[17.03.2025 11:09] Success.
[17.03.2025 11:09] Downloading and parsing paper https://huggingface.co/papers/2503.11514.
[17.03.2025 11:09] Extra JSON file exists (./assets/json/2503.11514.json), skip PDF parsing.
[17.03.2025 11:09] Paper image links file exists (./assets/img_data/2503.11514.json), skip HTML parsing.
[17.03.2025 11:09] Success.
[17.03.2025 11:09] Downloading and parsing paper https://huggingface.co/papers/2503.10781.
[17.03.2025 11:09] Extra JSON file exists (./assets/json/2503.10781.json), skip PDF parsing.
[17.03.2025 11:09] Paper image links file exists (./assets/img_data/2503.10781.json), skip HTML parsing.
[17.03.2025 11:09] Success.
[17.03.2025 11:09] Downloading and parsing paper https://huggingface.co/papers/2503.10970.
[17.03.2025 11:09] Extra JSON file exists (./assets/json/2503.10970.json), skip PDF parsing.
[17.03.2025 11:09] Paper image links file exists (./assets/img_data/2503.10970.json), skip HTML parsing.
[17.03.2025 11:09] Success.
[17.03.2025 11:09] Downloading and parsing paper https://huggingface.co/papers/2503.10772.
[17.03.2025 11:09] Extra JSON file exists (./assets/json/2503.10772.json), skip PDF parsing.
[17.03.2025 11:09] Paper image links file exists (./assets/img_data/2503.10772.json), skip HTML parsing.
[17.03.2025 11:09] Success.
[17.03.2025 11:09] Downloading and parsing paper https://huggingface.co/papers/2503.10632.
[17.03.2025 11:09] Extra JSON file exists (./assets/json/2503.10632.json), skip PDF parsing.
[17.03.2025 11:09] Paper image links file exists (./assets/img_data/2503.10632.json), skip HTML parsing.
[17.03.2025 11:09] Success.
[17.03.2025 11:09] Downloading and parsing paper https://huggingface.co/papers/2503.09279.
[17.03.2025 11:09] Extra JSON file exists (./assets/json/2503.09279.json), skip PDF parsing.
[17.03.2025 11:09] Paper image links file exists (./assets/img_data/2503.09279.json), skip HTML parsing.
[17.03.2025 11:09] Success.
[17.03.2025 11:09] Downloading and parsing paper https://huggingface.co/papers/2503.06553.
[17.03.2025 11:09] Extra JSON file exists (./assets/json/2503.06553.json), skip PDF parsing.
[17.03.2025 11:09] Paper image links file exists (./assets/img_data/2503.06553.json), skip HTML parsing.
[17.03.2025 11:09] Success.
[17.03.2025 11:09] Downloading and parsing paper https://huggingface.co/papers/2503.06542.
[17.03.2025 11:09] Extra JSON file exists (./assets/json/2503.06542.json), skip PDF parsing.
[17.03.2025 11:09] Paper image links file exists (./assets/img_data/2503.06542.json), skip HTML parsing.
[17.03.2025 11:09] Success.
[17.03.2025 11:09] Downloading and parsing paper https://huggingface.co/papers/2503.10696.
[17.03.2025 11:09] Extra JSON file exists (./assets/json/2503.10696.json), skip PDF parsing.
[17.03.2025 11:09] Paper image links file exists (./assets/img_data/2503.10696.json), skip HTML parsing.
[17.03.2025 11:09] Success.
[17.03.2025 11:09] Downloading and parsing paper https://huggingface.co/papers/2503.06674.
[17.03.2025 11:09] Extra JSON file exists (./assets/json/2503.06674.json), skip PDF parsing.
[17.03.2025 11:09] Paper image links file exists (./assets/img_data/2503.06674.json), skip HTML parsing.
[17.03.2025 11:09] Success.
[17.03.2025 11:09] Downloading and parsing paper https://huggingface.co/papers/2503.10624.
[17.03.2025 11:09] Extra JSON file exists (./assets/json/2503.10624.json), skip PDF parsing.
[17.03.2025 11:09] Paper image links file exists (./assets/img_data/2503.10624.json), skip HTML parsing.
[17.03.2025 11:09] Success.
[17.03.2025 11:09] Downloading and parsing paper https://huggingface.co/papers/2503.05689.
[17.03.2025 11:09] Extra JSON file exists (./assets/json/2503.05689.json), skip PDF parsing.
[17.03.2025 11:09] Paper image links file exists (./assets/img_data/2503.05689.json), skip HTML parsing.
[17.03.2025 11:09] Success.
[17.03.2025 11:09] Downloading and parsing paper https://huggingface.co/papers/2503.08111.
[17.03.2025 11:09] Extra JSON file exists (./assets/json/2503.08111.json), skip PDF parsing.
[17.03.2025 11:09] Paper image links file exists (./assets/img_data/2503.08111.json), skip HTML parsing.
[17.03.2025 11:09] Success.
[17.03.2025 11:09] Downloading and parsing paper https://huggingface.co/papers/2503.10620.
[17.03.2025 11:09] Downloading paper 2503.10620 from http://arxiv.org/pdf/2503.10620v1...
[17.03.2025 11:09] Extracting affiliations from text.
[17.03.2025 11:09] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"From TOWER to SPIRE: Adding the Speech Modality to Text-Only LLM Kshitij Ambilduke*1, Ben Peters2, Sonal Sannigrahi2,3, Anil Keshwani4, Tsz Kin Lam5, Bruno Martins3,6, Marcely Zanon Boito7, André F.T. Martins2,3,8,9 1Paris-Saclay University 2Instituto de Telecomunicações 3Instituto Superior Técnico, Universidade de Lisboa 4Sapienza University of Rome 5University of Edinburgh 6INESC-ID 7NAVER LABS Europe 8ELLIS Unit Lisbon 9Unbabel Correspondence: benzurdopeters@gmail.com "
[17.03.2025 11:09] Response: ```python
[
    "Paris-Saclay University",
    "Instituto de Telecomunicações",
    "Instituto Superior Técnico, Universidade de Lisboa",
    "Sapienza University of Rome",
    "University of Edinburgh",
    "INESC-ID",
    "NAVER LABS Europe",
    "ELLIS Unit Lisbon",
    "Unbabel"
]
```
[17.03.2025 11:09] Deleting PDF ./assets/pdf/2503.10620.pdf.
[17.03.2025 11:09] Success.
[17.03.2025 11:09] Downloading and parsing paper https://huggingface.co/papers/2503.09330.
[17.03.2025 11:09] Downloading paper 2503.09330 from http://arxiv.org/pdf/2503.09330v1...
[17.03.2025 11:09] Extracting affiliations from text.
[17.03.2025 11:09] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Group-robust Machine Unlearning Thomas De Min1, Subhankar Roy1 Stephane Lathuili`ere2, 3 Elisa Ricci1, 4 Massimiliano Mancini1 1University of Trento 2LTCI, Telecom Paris, Institut Polytechnique de Paris 3Inria Grenoble, Univ. Grenoble Alpes 4Fondazione Bruno Kessler 5 2 0 2 M 2 1 ] . [ 1 0 3 3 9 0 . 3 0 5 2 : r a "
[17.03.2025 11:09] Response: ```python
["University of Trento", "LTCI, Telecom Paris, Institut Polytechnique de Paris", "Inria Grenoble, Univ. Grenoble Alpes", "Fondazione Bruno Kessler"]
```
[17.03.2025 11:09] Deleting PDF ./assets/pdf/2503.09330.pdf.
[17.03.2025 11:09] Success.
[17.03.2025 11:09] Enriching papers with extra data.
[17.03.2025 11:09] ********************************************************************************
[17.03.2025 11:09] Abstract 0. Diffusion models have shown impressive results in generating high-quality conditional samples using guidance techniques such as Classifier-Free Guidance (CFG). However, existing methods often require additional training or neural function evaluations (NFEs), making them incompatible with guidance-di...
[17.03.2025 11:09] ********************************************************************************
[17.03.2025 11:09] Abstract 1. Camera control has been actively studied in text or image conditioned video generation tasks. However, altering camera trajectories of a given video remains under-explored, despite its importance in the field of video creation. It is non-trivial due to the extra constraints of maintaining multiple-f...
[17.03.2025 11:09] ********************************************************************************
[17.03.2025 11:09] Abstract 2. The pursuit of data efficiency, where quality outweighs quantity, has emerged as a cornerstone in robotic manipulation, especially given the high costs associated with real-world data collection. We propose that maximizing the informational density of individual demonstrations can dramatically reduc...
[17.03.2025 11:09] ********************************************************************************
[17.03.2025 11:09] Abstract 3. State Space Models (SSMs) have emerged as a promising alternative to the popular transformer-based models and have been increasingly gaining attention. Compared to transformers, SSMs excel at tasks with sequential data or longer contexts, demonstrating comparable performances with significant effici...
[17.03.2025 11:09] ********************************************************************************
[17.03.2025 11:09] Abstract 4. Large language models (LLMs) have evolved beyond simple text generation to power software agents that directly translate natural language commands into tangible actions. While API-based LLM agents initially rose to prominence for their robust automation capabilities and seamless integration with pro...
[17.03.2025 11:09] ********************************************************************************
[17.03.2025 11:09] Abstract 5. Federated Learning (FL) has emerged as a promising privacy-preserving collaborative model training paradigm without sharing raw data. However, recent studies have revealed that private information can still be leaked through shared gradient information and attacked by Gradient Inversion Attacks (GIA...
[17.03.2025 11:09] ********************************************************************************
[17.03.2025 11:09] Abstract 6. We propose a novel approach for captioning and object grounding in video, where the objects in the caption are grounded in the video via temporally dense bounding boxes. We introduce the following contributions. First, we present a large-scale automatic annotation method that aggregates captions gro...
[17.03.2025 11:09] ********************************************************************************
[17.03.2025 11:09] Abstract 7. Precision therapeutics require multimodal adaptive models that generate personalized treatment recommendations. We introduce TxAgent, an AI agent that leverages multi-step reasoning and real-time biomedical knowledge retrieval across a toolbox of 211 tools to analyze drug interactions, contraindicat...
[17.03.2025 11:09] ********************************************************************************
[17.03.2025 11:09] Abstract 8. Bridging different modalities lies at the heart of cross-modality generation. While conventional approaches treat the text modality as a conditioning signal that gradually guides the denoising process from Gaussian noise to the target image modality, we explore a much simpler paradigm-directly evolv...
[17.03.2025 11:09] ********************************************************************************
[17.03.2025 11:09] Abstract 9. Kolmogorov-Arnold networks (KANs) are a remarkable innovation consisting of learnable activation functions with the potential to capture more complex relationships from data. Although KANs are useful in finding symbolic representations and continual learning of one-dimensional functions, their effec...
[17.03.2025 11:09] ********************************************************************************
[17.03.2025 11:09] Abstract 10. Video Detailed Captioning (VDC) is a crucial task for vision-language bridging, enabling fine-grained descriptions of complex video content. In this paper, we first comprehensively benchmark current state-of-the-art approaches and systematically identified two critical limitations: biased capability...
[17.03.2025 11:09] ********************************************************************************
[17.03.2025 11:09] Abstract 11. As multi-modal large language models (MLLMs) frequently exhibit errors when solving scientific problems, evaluating the validity of their reasoning processes is critical for ensuring reliability and uncovering fine-grained model weaknesses. Since human evaluation is laborious and costly, prompting M...
[17.03.2025 11:09] ********************************************************************************
[17.03.2025 11:09] Abstract 12. Unified models (UniMs) for multimodal understanding and generation have recently received much attention in the area of vision and language. Existing UniMs are designed to simultaneously learn both multimodal understanding and generation capabilities, demanding substantial computational resources, a...
[17.03.2025 11:09] ********************************************************************************
[17.03.2025 11:09] Abstract 13. Visual autoregressive models typically adhere to a raster-order ``next-token prediction" paradigm, which overlooks the spatial and temporal locality inherent in visual content. Specifically, visual tokens exhibit significantly stronger correlations with their spatially or temporally adjacent tokens ...
[17.03.2025 11:09] ********************************************************************************
[17.03.2025 11:09] Abstract 14. Accelerating diffusion model sampling is crucial for efficient AIGC deployment. While diffusion distillation methods -- based on distribution matching and trajectory matching -- reduce sampling to as few as one step, they fall short on complex tasks like text-to-image generation. Few-step generation...
[17.03.2025 11:09] ********************************************************************************
[17.03.2025 11:09] Abstract 15. Fitting a body to a 3D clothed human point cloud is a common yet challenging task. Traditional optimization-based approaches use multi-stage pipelines that are sensitive to pose initialization, while recent learning-based methods often struggle with generalization across diverse poses and garment ty...
[17.03.2025 11:09] ********************************************************************************
[17.03.2025 11:09] Abstract 16. We propose GoalFlow, an end-to-end autonomous driving method for generating high-quality multimodal trajectories. In autonomous driving scenarios, there is rarely a single suitable trajectory. Recent methods have increasingly focused on modeling multimodal trajectory distributions. However, they suf...
[17.03.2025 11:09] ********************************************************************************
[17.03.2025 11:09] Abstract 17. Accurate material retrieval is critical for creating realistic 3D assets. Existing methods rely on datasets that capture shape-invariant and lighting-varied representations of materials, which are scarce and face challenges due to limited diversity and inadequate real-world generalization. Most curr...
[17.03.2025 11:09] ********************************************************************************
[17.03.2025 11:09] Abstract 18. Large language models (LLMs) have shown remarkable performance and generalization capabilities across multiple languages and tasks, making them very attractive targets for multi-modality integration (e.g., images or speech). In this work, we extend an existing LLM to the speech modality via speech d...
[17.03.2025 11:09] ********************************************************************************
[17.03.2025 11:09] Abstract 19. Machine unlearning is an emerging paradigm to remove the influence of specific training data (i.e., the forget set) from a model while preserving its knowledge of the rest of the data (i.e., the retain set). Previous approaches assume the forget data to be uniformly distributed from all training dat...
[17.03.2025 11:09] Read previous papers.
[17.03.2025 11:09] Generating reviews via LLM API.
[17.03.2025 11:09] Using data from previous issue: {"categories": ["#training", "#cv", "#diffusion", "#inference"], "emoji": "🖼️", "ru": {"title": "PLADIS: Эффективное улучшение генерации изображений с помощью разреженного внимания", "desc": "Статья представляет новый метод под названием PLADIS для улучшения предобученных моделей диффузии в задаче г
[17.03.2025 11:09] Using data from previous issue: {"categories": ["#dataset", "#optimization", "#training", "#video", "#games"], "emoji": "🎥", "ru": {"title": "Управление камерой в видео с помощью генеративных моделей", "desc": "ReCamMaster - это генеративная система для изменения траектории камеры в видео. Она использует предобученные модели text-
[17.03.2025 11:09] Using data from previous issue: {"categories": ["#robotics", "#benchmark", "#dataset", "#optimization", "#open_source", "#agents", "#training", "#data"], "emoji": "🤖", "ru": {"title": "Меньше данных, больше эффективности: революция в обучении роботов", "desc": "Статья представляет новый подход к сбору данных для обучения роботов м
[17.03.2025 11:09] Using data from previous issue: {"categories": ["#architecture", "#long_context", "#survey", "#math", "#training"], "emoji": "🧠", "ru": {"title": "SSM: Эффективная альтернатива трансформерам для обработки последовательностей", "desc": "Это обзор моделей пространства состояний (SSM) как альтернативы трансформерам в машинном обучени
[17.03.2025 11:09] Using data from previous issue: {"categories": ["#multimodal", "#survey", "#agents"], "emoji": "🤖", "ru": {"title": "Сравнение API и GUI агентов на основе LLM: путь к гибридным решениям", "desc": "Это исследование сравнивает агентов на основе больших языковых моделей (LLM), работающих через API и через графический интерфейс. Автор
[17.03.2025 11:09] Using data from previous issue: {"categories": ["#leakage", "#benchmark", "#security", "#survey", "#healthcare", "#data"], "emoji": "🛡️", "ru": {"title": "Защита приватности в федеративном обучении: анализ атак с инверсией градиента", "desc": "Статья посвящена анализу атак с инверсией градиента (GIA) в контексте федеративного обуч
[17.03.2025 11:09] Using data from previous issue: {"categories": ["#cv", "#dataset", "#training", "#video", "#data"], "emoji": "🎥", "ru": {"title": "Революция в понимании видео: от автоматической разметки к точной локализации объектов", "desc": "Статья представляет новый подход к генерации подписей и локализации объектов в видео с использованием те
[17.03.2025 11:09] Using data from previous issue: {"categories": ["#alignment", "#healthcare", "#science", "#agents", "#reasoning", "#benchmark", "#multimodal"], "emoji": "💊", "ru": {"title": "TxAgent: ИИ-помощник для точной и персонализированной фармакотерапии", "desc": "TxAgent - это ИИ-агент для персонализированной терапии, использующий многоэта
[17.03.2025 11:09] Using data from previous issue: {"categories": ["#architecture", "#training", "#multimodal", "#diffusion"], "emoji": "🌊", "ru": {"title": "FlowTok: эффективный переход между текстом и изображением через токены", "desc": "FlowTok - это новый подход к генерации изображений из текста и наоборот. Он использует метод согласования поток
[17.03.2025 11:09] Using data from previous issue: {"categories": ["#architecture", "#cv", "#optimization", "#open_source", "#training"], "emoji": "🧠", "ru": {"title": "Новый взгляд на внимание: Колмогоров-Арнольд сети в Vision Transformers", "desc": "Статья представляет новый подход к архитектуре нейронных сетей - Колмогоров-Арнольд внимание (KArAt
[17.03.2025 11:09] Using data from previous issue: {"categories": ["#benchmark", "#training", "#multimodal", "#synthetic", "#alignment"], "emoji": "🦜", "ru": {"title": "Cockatiel: Новый стандарт в детальном описании видео", "desc": "Статья представляет новый подход к детальному описанию видео (VDC) под названием Cockatiel. Авторы разработали трехэта
[17.03.2025 11:09] Using data from previous issue: {"categories": ["#interpretability", "#reasoning", "#multimodal", "#science", "#open_source", "#dataset", "#training", "#benchmark"], "emoji": "🔬", "ru": {"title": "Надежная оценка научных рассуждений мультимодальными ИИ-судьями", "desc": "В статье представлен ProJudgeBench - первый комплексный бенч
[17.03.2025 11:09] Using data from previous issue: {"categories": ["#games", "#optimization", "#multimodal", "#dataset", "#training", "#architecture"], "emoji": "🛡️", "ru": {"title": "ARMOR: Эффективное улучшение мультимодальных моделей", "desc": "ARMOR - это новый подход к созданию унифицированных моделей для мультимодального понимания и генерации.
[17.03.2025 11:09] Using data from previous issue: {"categories": ["#cv", "#video", "#games", "#benchmark", "#optimization", "#architecture", "#training"], "emoji": "🧩", "ru": {"title": "NAR: Революция в авторегрессионном моделировании визуального контента", "desc": "Статья представляет новый подход к авторегрессионному моделированию визуального кон
[17.03.2025 11:09] Using data from previous issue: {"categories": ["#diffusion", "#training", "#cv", "#video"], "emoji": "🚀", "ru": {"title": "TDM: Революция в ускорении диффузионных моделей", "desc": "Статья представляет новый метод ускорения генерации изображений с помощью диффузионных моделей, называемый Trajectory Distribution Matching (TDM). TD
[17.03.2025 11:09] Using data from previous issue: {"categories": ["#3d", "#optimization", "#cv", "#open_source"], "emoji": "👕", "ru": {"title": "Точная подгонка 3D-модели тела к одетому человеку с учетом плотности прилегания одежды", "desc": "Статья представляет новый метод ETCH для подгонки трехмерной модели тела к облаку точек одетого человека. E
[17.03.2025 11:09] Using data from previous issue: {"categories": ["#optimization", "#diffusion", "#agents", "#multimodal"], "emoji": "🚗", "ru": {"title": "GoalFlow: Точное планирование траекторий для автономного вождения", "desc": "GoalFlow - это новый метод автономного вождения для генерации высококачественных мультимодальных траекторий. Он решает
[17.03.2025 11:09] Using data from previous issue: {"categories": ["#synthetic", "#optimization", "#dataset", "#cv", "#3d"], "emoji": "🔍", "ru": {"title": "MaRI: Революция в поиске 3D-материалов", "desc": "Статья представляет MaRI - фреймворк для улучшения поиска реалистичных 3D-материалов. Он создает общее пространство признаков для синтетических и
[17.03.2025 11:09] Querying the API.
[17.03.2025 11:09] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Large language models (LLMs) have shown remarkable performance and generalization capabilities across multiple languages and tasks, making them very attractive targets for multi-modality integration (e.g., images or speech). In this work, we extend an existing LLM to the speech modality via speech discretization and continued pre-training. In particular, we are interested in multilingual LLMs, such as TOWER, as their pre-training setting allows us to treat discretized speech input as an additional translation language. The resulting open-source model, SPIRE, is able to transcribe and translate English speech input while maintaining TOWER's original performance on translation-related tasks, showcasing that discretized speech input integration as an additional language is feasible during LLM adaptation. We make our code and models available to the community.
[17.03.2025 11:09] Response: {
  "desc": "В этой статье описывается расширение возможностей большой языковой модели (LLM) для работы с речью путем дискретизации речевого сигнала и дополнительного предобучения. Авторы интегрируют дискретизированный речевой ввод как дополнительный язык в многоязычную модель TOWER. Полученная модель SPIRE способна транскрибировать и переводить английскую речь, сохраняя при этом исходную производительность TOWER в задачах перевода. Исследование демонстрирует возможность интеграции дискретизированного речевого ввода как дополнительного языка при адаптации LLM.",

  "emoji": "🗣️",

  "title": "Расширение языковых моделей на речевую модальность"
}
[17.03.2025 11:09] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Large language models (LLMs) have shown remarkable performance and generalization capabilities across multiple languages and tasks, making them very attractive targets for multi-modality integration (e.g., images or speech). In this work, we extend an existing LLM to the speech modality via speech discretization and continued pre-training. In particular, we are interested in multilingual LLMs, such as TOWER, as their pre-training setting allows us to treat discretized speech input as an additional translation language. The resulting open-source model, SPIRE, is able to transcribe and translate English speech input while maintaining TOWER's original performance on translation-related tasks, showcasing that discretized speech input integration as an additional language is feasible during LLM adaptation. We make our code and models available to the community."

[17.03.2025 11:09] Response: ```python
['MULTIMODAL', 'MULTILINGUAL']
```
[17.03.2025 11:09] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Large language models (LLMs) have shown remarkable performance and generalization capabilities across multiple languages and tasks, making them very attractive targets for multi-modality integration (e.g., images or speech). In this work, we extend an existing LLM to the speech modality via speech discretization and continued pre-training. In particular, we are interested in multilingual LLMs, such as TOWER, as their pre-training setting allows us to treat discretized speech input as an additional translation language. The resulting open-source model, SPIRE, is able to transcribe and translate English speech input while maintaining TOWER's original performance on translation-related tasks, showcasing that discretized speech input integration as an additional language is feasible during LLM adaptation. We make our code and models available to the community."

[17.03.2025 11:09] Response: ```python
['OPEN_SOURCE', 'TRANSLATION', 'LOW_RESOURCE']
```
[17.03.2025 11:09] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper discusses the enhancement of large language models (LLMs) by integrating speech as a new modality. The authors focus on multilingual LLMs, specifically TOWER, and propose a method to convert speech into a format that the model can understand. They introduce a new model called SPIRE, which can transcribe and translate English speech while preserving the original capabilities of TOWER. The research demonstrates that incorporating discretized speech as an additional language is a viable approach for adapting LLMs, and the authors provide their code and models for public use.","title":"Integrating Speech into Multilingual LLMs for Enhanced Performance"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper discusses the enhancement of large language models (LLMs) by integrating speech as a new modality. The authors focus on multilingual LLMs, specifically TOWER, and propose a method to convert speech into a format that the model can understand. They introduce a new model called SPIRE, which can transcribe and translate English speech while preserving the original capabilities of TOWER. The research demonstrates that incorporating discretized speech as an additional language is a viable approach for adapting LLMs, and the authors provide their code and models for public use.', title='Integrating Speech into Multilingual LLMs for Enhanced Performance'))
[17.03.2025 11:09] Response: ParsedChatCompletionMessage[Article](content='{"desc":"大型语言模型（LLMs）在多种语言和任务中表现出色，具有很强的泛化能力，适合与多模态（如图像或语音）结合。本文将现有的LLM扩展到语音模态，通过语音离散化和持续预训练来实现。我们特别关注多语言LLM，例如TOWER，因为它的预训练设置允许我们将离散化的语音输入视为额外的翻译语言。最终生成的开源模型SPIRE能够转录和翻译英语语音输入，同时保持TOWER在翻译相关任务上的原始性能，证明了在LLM适应过程中将离散语音输入作为额外语言的可行性。","title":"将语音融入大型语言模型的创新之路"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='大型语言模型（LLMs）在多种语言和任务中表现出色，具有很强的泛化能力，适合与多模态（如图像或语音）结合。本文将现有的LLM扩展到语音模态，通过语音离散化和持续预训练来实现。我们特别关注多语言LLM，例如TOWER，因为它的预训练设置允许我们将离散化的语音输入视为额外的翻译语言。最终生成的开源模型SPIRE能够转录和翻译英语语音输入，同时保持TOWER在翻译相关任务上的原始性能，证明了在LLM适应过程中将离散语音输入作为额外语言的可行性。', title='将语音融入大型语言模型的创新之路'))
[17.03.2025 11:09] Querying the API.
[17.03.2025 11:09] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Machine unlearning is an emerging paradigm to remove the influence of specific training data (i.e., the forget set) from a model while preserving its knowledge of the rest of the data (i.e., the retain set). Previous approaches assume the forget data to be uniformly distributed from all training datapoints. However, if the data to unlearn is dominant in one group, we empirically show that performance for this group degrades, leading to fairness issues. This work tackles the overlooked problem of non-uniformly distributed forget sets, which we call group-robust machine unlearning, by presenting a simple, effective strategy that mitigates the performance loss in dominant groups via sample distribution reweighting. Moreover, we present MIU (Mutual Information-aware Machine Unlearning), the first approach for group robustness in approximate machine unlearning. MIU minimizes the mutual information between model features and group information, achieving unlearning while reducing performance degradation in the dominant group of the forget set. Additionally, MIU exploits sample distribution reweighting and mutual information calibration with the original model to preserve group robustness. We conduct experiments on three datasets and show that MIU outperforms standard methods, achieving unlearning without compromising model robustness. Source code available at https://github.com/tdemin16/group-robust_machine_unlearning.
[17.03.2025 11:10] Response: {
  "desc": "Статья посвящена проблеме группового машинного разобучения, когда данные для удаления из модели неравномерно распределены между группами. Авторы предлагают метод MIU, который минимизирует взаимную информацию между признаками модели и информацией о группах. MIU использует перевзвешивание распределения выборки и калибровку взаимной информации с исходной моделью для сохранения групповой устойчивости. Эксперименты на трех наборах данных показывают, что MIU превосходит стандартные методы, достигая разобучения без ущерба для устойчивости модели.",
  "emoji": "🧠",
  "title": "Групповое разобучение без потери устойчивости"
}
[17.03.2025 11:10] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Machine unlearning is an emerging paradigm to remove the influence of specific training data (i.e., the forget set) from a model while preserving its knowledge of the rest of the data (i.e., the retain set). Previous approaches assume the forget data to be uniformly distributed from all training datapoints. However, if the data to unlearn is dominant in one group, we empirically show that performance for this group degrades, leading to fairness issues. This work tackles the overlooked problem of non-uniformly distributed forget sets, which we call group-robust machine unlearning, by presenting a simple, effective strategy that mitigates the performance loss in dominant groups via sample distribution reweighting. Moreover, we present MIU (Mutual Information-aware Machine Unlearning), the first approach for group robustness in approximate machine unlearning. MIU minimizes the mutual information between model features and group information, achieving unlearning while reducing performance degradation in the dominant group of the forget set. Additionally, MIU exploits sample distribution reweighting and mutual information calibration with the original model to preserve group robustness. We conduct experiments on three datasets and show that MIU outperforms standard methods, achieving unlearning without compromising model robustness. Source code available at https://github.com/tdemin16/group-robust_machine_unlearning."

[17.03.2025 11:10] Response: ```python
["DATASET", "DATA", "TRAINING"]
```
[17.03.2025 11:10] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Machine unlearning is an emerging paradigm to remove the influence of specific training data (i.e., the forget set) from a model while preserving its knowledge of the rest of the data (i.e., the retain set). Previous approaches assume the forget data to be uniformly distributed from all training datapoints. However, if the data to unlearn is dominant in one group, we empirically show that performance for this group degrades, leading to fairness issues. This work tackles the overlooked problem of non-uniformly distributed forget sets, which we call group-robust machine unlearning, by presenting a simple, effective strategy that mitigates the performance loss in dominant groups via sample distribution reweighting. Moreover, we present MIU (Mutual Information-aware Machine Unlearning), the first approach for group robustness in approximate machine unlearning. MIU minimizes the mutual information between model features and group information, achieving unlearning while reducing performance degradation in the dominant group of the forget set. Additionally, MIU exploits sample distribution reweighting and mutual information calibration with the original model to preserve group robustness. We conduct experiments on three datasets and show that MIU outperforms standard methods, achieving unlearning without compromising model robustness. Source code available at https://github.com/tdemin16/group-robust_machine_unlearning."

[17.03.2025 11:10] Response: ```python
['ETHICS']
```
[17.03.2025 11:10] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces the concept of group-robust machine unlearning, which addresses the challenge of removing specific training data from a model while maintaining its performance across different groups. It highlights the issue of fairness when the data to be unlearned is not uniformly distributed, leading to performance degradation in dominant groups. The authors propose a novel method called MIU (Mutual Information-aware Machine Unlearning) that minimizes the mutual information between model features and group information, thus enhancing unlearning effectiveness. Through experiments on various datasets, MIU demonstrates superior performance compared to traditional methods, ensuring robust model performance even after unlearning.","title":"Fair and Effective Machine Unlearning for Diverse Data Groups"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper introduces the concept of group-robust machine unlearning, which addresses the challenge of removing specific training data from a model while maintaining its performance across different groups. It highlights the issue of fairness when the data to be unlearned is not uniformly distributed, leading to performance degradation in dominant groups. The authors propose a novel method called MIU (Mutual Information-aware Machine Unlearning) that minimizes the mutual information between model features and group information, thus enhancing unlearning effectiveness. Through experiments on various datasets, MIU demonstrates superior performance compared to traditional methods, ensuring robust model performance even after unlearning.', title='Fair and Effective Machine Unlearning for Diverse Data Groups'))
[17.03.2025 11:10] Response: ParsedChatCompletionMessage[Article](content='{"desc":"机器遗忘是一种新兴的范式，旨在从模型中去除特定训练数据的影响，同时保留其对其他数据的知识。以往的方法假设遗忘数据均匀分布，但如果要遗忘的数据在某一组中占主导地位，模型在该组的性能会下降，导致公平性问题。本文提出了一种简单有效的策略，通过样本分布重加权来缓解主导组的性能损失，解决了非均匀分布遗忘集的问题。我们还提出了MIU（互信息感知机器遗忘），这是首个针对近似机器遗忘的组鲁棒性方法，能够在减少主导组性能下降的同时实现遗忘。","title":"组鲁棒性机器遗忘：公平性与性能的平衡"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='机器遗忘是一种新兴的范式，旨在从模型中去除特定训练数据的影响，同时保留其对其他数据的知识。以往的方法假设遗忘数据均匀分布，但如果要遗忘的数据在某一组中占主导地位，模型在该组的性能会下降，导致公平性问题。本文提出了一种简单有效的策略，通过样本分布重加权来缓解主导组的性能损失，解决了非均匀分布遗忘集的问题。我们还提出了MIU（互信息感知机器遗忘），这是首个针对近似机器遗忘的组鲁棒性方法，能够在减少主导组性能下降的同时实现遗忘。', title='组鲁棒性机器遗忘：公平性与性能的平衡'))
[17.03.2025 11:10] Loading Chinese text from previous data.
[17.03.2025 11:10] Renaming data file.
[17.03.2025 11:10] Renaming previous data. hf_papers.json to ./d/2025-03-17.json
[17.03.2025 11:10] Saving new data file.
[17.03.2025 11:10] Generating page.
[17.03.2025 11:10] Renaming previous page.
[17.03.2025 11:10] Renaming previous data. index.html to ./d/2025-03-17.html
[17.03.2025 11:10] [Experimental] Generating Chinese page for reading.
[17.03.2025 11:10] Chinese vocab [{'word': '讨论', 'pinyin': 'tǎo lùn', 'trans': 'discuss'}, {'word': '名为', 'pinyin': 'míng wéi', 'trans': 'named'}, {'word': '视频', 'pinyin': 'shì pín', 'trans': 'video'}, {'word': '重新', 'pinyin': 'chóng xīn', 'trans': 'again'}, {'word': '渲染', 'pinyin': 'xuàn rán', 'trans': 'render'}, {'word': '框架', 'pinyin': 'kuàng jià', 'trans': 'framework'}, {'word': '摄像机', 'pinyin': 'shè xiàng jī', 'trans': 'camera'}, {'word': '轨迹', 'pinyin': 'guǐ jì', 'trans': 'trajectory'}, {'word': '同时', 'pinyin': 'tóng shí', 'trans': 'simultaneously'}, {'word': '保持', 'pinyin': 'bǎo chí', 'trans': 'maintain'}, {'word': '多帧', 'pinyin': 'duō zhēn', 'trans': 'multi-frame'}, {'word': '外观', 'pinyin': 'wài guǎn', 'trans': 'appearance'}, {'word': '动态', 'pinyin': 'dòng tài', 'trans': 'dynamic'}, {'word': '同步', 'pinyin': 'tóng bù', 'trans': 'synchronization'}, {'word': '方法', 'pinyin': 'fāng fǎ', 'trans': 'method'}, {'word': '利用', 'pinyin': 'lì yòng', 'trans': 'utilize'}, {'word': '预训练', 'pinyin': 'yù xùn liàn', 'trans': 'pre-trained'}, {'word': '文本', 'pinyin': 'wén běn', 'trans': 'text'}, {'word': '生成', 'pinyin': 'shēng chéng', 'trans': 'generate'}, {'word': '能力', 'pinyin': 'néng lì', 'trans': 'ability'}, {'word': '构建', 'pinyin': 'gòu jiàn', 'trans': 'construct'}, {'word': '数据集', 'pinyin': 'shù jù jí', 'trans': 'dataset'}, {'word': '实验', 'pinyin': 'shí yàn', 'trans': 'experiment'}, {'word': '结果', 'pinyin': 'jié guǒ', 'trans': 'result'}, {'word': '显示', 'pinyin': 'xiǎn shì', 'trans': 'display'}, {'word': '稳定', 'pinyin': 'wěn dìng', 'trans': 'stable'}, {'word': '超分辨率', 'pinyin': 'chāo fēn biàn lǜ', 'trans': 'super-resolution'}, {'word': '扩展', 'pinyin': 'kuò zhǎn', 'trans': 'extend'}, {'word': '表现', 'pinyin': 'biǎo xiàn', 'trans': 'performance'}, {'word': '出色', 'pinyin': 'chū sè', 'trans': 'outstanding'}]
[17.03.2025 11:10] Renaming previous Chinese page.
[17.03.2025 11:10] Renaming previous data. zh.html to ./d/2025-03-16_zh_reading_task.html
[17.03.2025 11:10] Writing Chinese reading task.
[17.03.2025 11:10] Writing result.
[17.03.2025 11:10] Renaming log file.
[17.03.2025 11:10] Renaming previous data. log.txt to ./logs/2025-03-17_last_log.txt
