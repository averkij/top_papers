[17.03.2025 11:10] Read previous papers.
[17.03.2025 11:10] Generating top page (month).
[17.03.2025 11:10] Writing top page (month).
[17.03.2025 12:20] Read previous papers.
[17.03.2025 12:20] Get feed.
[17.03.2025 12:20] Get page data from previous paper. URL: https://huggingface.co/papers/2503.11647
[17.03.2025 12:20] Get page data from previous paper. URL: https://huggingface.co/papers/2503.07677
[17.03.2025 12:20] Get page data from previous paper. URL: https://huggingface.co/papers/2503.11646
[17.03.2025 12:20] Get page data from previous paper. URL: https://huggingface.co/papers/2503.11224
[17.03.2025 12:20] Get page data from previous paper. URL: https://huggingface.co/papers/2503.11069
[17.03.2025 12:20] Get page data from previous paper. URL: https://huggingface.co/papers/2503.11514
[17.03.2025 12:20] Get page data from previous paper. URL: https://huggingface.co/papers/2503.10781
[17.03.2025 12:20] Get page data from previous paper. URL: https://huggingface.co/papers/2503.10970
[17.03.2025 12:20] Get page data from previous paper. URL: https://huggingface.co/papers/2503.10772
[17.03.2025 12:20] Get page data from previous paper. URL: https://huggingface.co/papers/2503.10632
[17.03.2025 12:20] Get page data from previous paper. URL: https://huggingface.co/papers/2503.09279
[17.03.2025 12:20] Get page data from previous paper. URL: https://huggingface.co/papers/2503.10696
[17.03.2025 12:20] Get page data from previous paper. URL: https://huggingface.co/papers/2503.06553
[17.03.2025 12:20] Get page data from previous paper. URL: https://huggingface.co/papers/2503.06542
[17.03.2025 12:20] Get page data from previous paper. URL: https://huggingface.co/papers/2503.06674
[17.03.2025 12:20] Get page data from previous paper. URL: https://huggingface.co/papers/2503.10624
[17.03.2025 12:20] Extract page data from URL. URL: https://huggingface.co/papers/2503.10684
[17.03.2025 12:20] Get page data from previous paper. URL: https://huggingface.co/papers/2503.05689
[17.03.2025 12:20] Get page data from previous paper. URL: https://huggingface.co/papers/2503.08111
[17.03.2025 12:20] Extract page data from URL. URL: https://huggingface.co/papers/2503.11651
[17.03.2025 12:20] Get page data from previous paper. URL: https://huggingface.co/papers/2503.10620
[17.03.2025 12:20] Get page data from previous paper. URL: https://huggingface.co/papers/2503.09330
[17.03.2025 12:20] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[17.03.2025 12:20] No deleted papers detected.
[17.03.2025 12:20] Downloading and parsing papers (pdf, html). Total: 22.
[17.03.2025 12:20] Downloading and parsing paper https://huggingface.co/papers/2503.11647.
[17.03.2025 12:20] Extra JSON file exists (./assets/json/2503.11647.json), skip PDF parsing.
[17.03.2025 12:20] Paper image links file exists (./assets/img_data/2503.11647.json), skip HTML parsing.
[17.03.2025 12:20] Success.
[17.03.2025 12:20] Downloading and parsing paper https://huggingface.co/papers/2503.07677.
[17.03.2025 12:20] Extra JSON file exists (./assets/json/2503.07677.json), skip PDF parsing.
[17.03.2025 12:20] Paper image links file exists (./assets/img_data/2503.07677.json), skip HTML parsing.
[17.03.2025 12:20] Success.
[17.03.2025 12:20] Downloading and parsing paper https://huggingface.co/papers/2503.11646.
[17.03.2025 12:20] Extra JSON file exists (./assets/json/2503.11646.json), skip PDF parsing.
[17.03.2025 12:20] Paper image links file exists (./assets/img_data/2503.11646.json), skip HTML parsing.
[17.03.2025 12:20] Success.
[17.03.2025 12:20] Downloading and parsing paper https://huggingface.co/papers/2503.11224.
[17.03.2025 12:20] Extra JSON file exists (./assets/json/2503.11224.json), skip PDF parsing.
[17.03.2025 12:20] Paper image links file exists (./assets/img_data/2503.11224.json), skip HTML parsing.
[17.03.2025 12:20] Success.
[17.03.2025 12:20] Downloading and parsing paper https://huggingface.co/papers/2503.11069.
[17.03.2025 12:20] Extra JSON file exists (./assets/json/2503.11069.json), skip PDF parsing.
[17.03.2025 12:20] Paper image links file exists (./assets/img_data/2503.11069.json), skip HTML parsing.
[17.03.2025 12:20] Success.
[17.03.2025 12:20] Downloading and parsing paper https://huggingface.co/papers/2503.11514.
[17.03.2025 12:20] Extra JSON file exists (./assets/json/2503.11514.json), skip PDF parsing.
[17.03.2025 12:20] Paper image links file exists (./assets/img_data/2503.11514.json), skip HTML parsing.
[17.03.2025 12:20] Success.
[17.03.2025 12:20] Downloading and parsing paper https://huggingface.co/papers/2503.10781.
[17.03.2025 12:20] Extra JSON file exists (./assets/json/2503.10781.json), skip PDF parsing.
[17.03.2025 12:20] Paper image links file exists (./assets/img_data/2503.10781.json), skip HTML parsing.
[17.03.2025 12:20] Success.
[17.03.2025 12:20] Downloading and parsing paper https://huggingface.co/papers/2503.10970.
[17.03.2025 12:20] Extra JSON file exists (./assets/json/2503.10970.json), skip PDF parsing.
[17.03.2025 12:20] Paper image links file exists (./assets/img_data/2503.10970.json), skip HTML parsing.
[17.03.2025 12:20] Success.
[17.03.2025 12:20] Downloading and parsing paper https://huggingface.co/papers/2503.10772.
[17.03.2025 12:20] Extra JSON file exists (./assets/json/2503.10772.json), skip PDF parsing.
[17.03.2025 12:20] Paper image links file exists (./assets/img_data/2503.10772.json), skip HTML parsing.
[17.03.2025 12:20] Success.
[17.03.2025 12:20] Downloading and parsing paper https://huggingface.co/papers/2503.10632.
[17.03.2025 12:20] Extra JSON file exists (./assets/json/2503.10632.json), skip PDF parsing.
[17.03.2025 12:20] Paper image links file exists (./assets/img_data/2503.10632.json), skip HTML parsing.
[17.03.2025 12:20] Success.
[17.03.2025 12:20] Downloading and parsing paper https://huggingface.co/papers/2503.09279.
[17.03.2025 12:20] Extra JSON file exists (./assets/json/2503.09279.json), skip PDF parsing.
[17.03.2025 12:20] Paper image links file exists (./assets/img_data/2503.09279.json), skip HTML parsing.
[17.03.2025 12:20] Success.
[17.03.2025 12:20] Downloading and parsing paper https://huggingface.co/papers/2503.10696.
[17.03.2025 12:20] Extra JSON file exists (./assets/json/2503.10696.json), skip PDF parsing.
[17.03.2025 12:20] Paper image links file exists (./assets/img_data/2503.10696.json), skip HTML parsing.
[17.03.2025 12:20] Success.
[17.03.2025 12:20] Downloading and parsing paper https://huggingface.co/papers/2503.06553.
[17.03.2025 12:20] Extra JSON file exists (./assets/json/2503.06553.json), skip PDF parsing.
[17.03.2025 12:20] Paper image links file exists (./assets/img_data/2503.06553.json), skip HTML parsing.
[17.03.2025 12:20] Success.
[17.03.2025 12:20] Downloading and parsing paper https://huggingface.co/papers/2503.06542.
[17.03.2025 12:20] Extra JSON file exists (./assets/json/2503.06542.json), skip PDF parsing.
[17.03.2025 12:20] Paper image links file exists (./assets/img_data/2503.06542.json), skip HTML parsing.
[17.03.2025 12:20] Success.
[17.03.2025 12:20] Downloading and parsing paper https://huggingface.co/papers/2503.06674.
[17.03.2025 12:20] Extra JSON file exists (./assets/json/2503.06674.json), skip PDF parsing.
[17.03.2025 12:20] Paper image links file exists (./assets/img_data/2503.06674.json), skip HTML parsing.
[17.03.2025 12:20] Success.
[17.03.2025 12:20] Downloading and parsing paper https://huggingface.co/papers/2503.10624.
[17.03.2025 12:20] Extra JSON file exists (./assets/json/2503.10624.json), skip PDF parsing.
[17.03.2025 12:20] Paper image links file exists (./assets/img_data/2503.10624.json), skip HTML parsing.
[17.03.2025 12:20] Success.
[17.03.2025 12:20] Downloading and parsing paper https://huggingface.co/papers/2503.10684.
[17.03.2025 12:20] Downloading paper 2503.10684 from http://arxiv.org/pdf/2503.10684v1...
[17.03.2025 12:20] Extracting affiliations from text.
[17.03.2025 12:20] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 1 1 ] . [ 1 4 8 6 0 1 . 3 0 5 2 : r March 2025 Open-World Skill Discovery from Unsegmented Demonstrations Jingwen Deng1, Zihao Wang1, Shaofei Cai1, Anji Liu2 and Yitao Liang1(cid:66) 1Peking University, 2University of California, Los Angeles, All authors are affiliated with Team CraftJarvis Learning skills in open-world environments is essential for developing agents capable of handling variety of tasks by combining basic skills. Online demonstration videos are typically long but unsegmented, making them difficult to segment and label with skill identifiers. Unlike existing methods that rely on sequence sampling or human labeling, we have developed self-supervised learning-based approach to segment these long videos into series of semantic-aware and skill-consistent segments. Drawing inspiration from human cognitive event segmentation theory, we introduce Skill Boundary Detection (SBD), an annotation-free temporal video segmentation algorithm. SBD detects skill boundaries in video by leveraging prediction errors from pretrained unconditional action-prediction model. This approach is based on the assumption that significant increase in prediction error indicates shift in the skill being executed. We evaluated our method in Minecraft, rich open-world simulator with extensive gameplay videos available online. Our SBD-generated segments improved the average performance of conditioned policies by 63.7% and 52.1% on short-term atomic skill tasks, and their corresponding hierarchical agents by 11.3% and 20.8% on long-horizon tasks. Our method can leverage the diverse YouTube videos to train instruction-following agents. The project page can be found in https://craftjarvis.github.io/SkillDiscovery/. 1. Introduction Most existing LLM-based instruction-following agents adopt two-layer structure of planner and controller to complete tasks, which first convert the instruction into atom skills through the planner, and then use conditioned policy to convert it into action"
[17.03.2025 12:20] Response: ```python
["Peking University", "University of California, Los Angeles"]
```
[17.03.2025 12:20] Deleting PDF ./assets/pdf/2503.10684.pdf.
[17.03.2025 12:20] Success.
[17.03.2025 12:20] Downloading and parsing paper https://huggingface.co/papers/2503.05689.
[17.03.2025 12:20] Extra JSON file exists (./assets/json/2503.05689.json), skip PDF parsing.
[17.03.2025 12:20] Paper image links file exists (./assets/img_data/2503.05689.json), skip HTML parsing.
[17.03.2025 12:20] Success.
[17.03.2025 12:20] Downloading and parsing paper https://huggingface.co/papers/2503.08111.
[17.03.2025 12:20] Extra JSON file exists (./assets/json/2503.08111.json), skip PDF parsing.
[17.03.2025 12:20] Paper image links file exists (./assets/img_data/2503.08111.json), skip HTML parsing.
[17.03.2025 12:20] Success.
[17.03.2025 12:20] Downloading and parsing paper https://huggingface.co/papers/2503.11651.
[17.03.2025 12:20] Downloading paper 2503.11651 from http://arxiv.org/pdf/2503.11651v1...
[17.03.2025 12:20] Extracting affiliations from text.
[17.03.2025 12:20] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 4 1 ] . [ 1 1 5 6 1 1 . 3 0 5 2 : r VGGT: Visual Geometry Grounded Transformer Jianyuan Wang1,2 Minghao Chen1,2 Nikita Karaev1,2 Andrea Vedaldi1,2 Christian Rupprecht David Novotny2 1Visual Geometry Group, University of Oxford 2Meta AI Figure 1. VGGT is large feed-forward transformer with minimal 3D-inductive biases trained on trove of 3D-annotated data. It accepts up to hundreds of images and predicts cameras, point maps, depth maps, and point tracks for all images at once in less than second, which often outperforms optimization-based alternatives without further processing. "
[17.03.2025 12:20] Response: ```python
["Visual Geometry Group, University of Oxford", "Meta AI"]
```
[17.03.2025 12:20] Deleting PDF ./assets/pdf/2503.11651.pdf.
[17.03.2025 12:20] Success.
[17.03.2025 12:20] Downloading and parsing paper https://huggingface.co/papers/2503.10620.
[17.03.2025 12:20] Extra JSON file exists (./assets/json/2503.10620.json), skip PDF parsing.
[17.03.2025 12:20] Paper image links file exists (./assets/img_data/2503.10620.json), skip HTML parsing.
[17.03.2025 12:20] Success.
[17.03.2025 12:20] Downloading and parsing paper https://huggingface.co/papers/2503.09330.
[17.03.2025 12:20] Extra JSON file exists (./assets/json/2503.09330.json), skip PDF parsing.
[17.03.2025 12:20] Paper image links file exists (./assets/img_data/2503.09330.json), skip HTML parsing.
[17.03.2025 12:20] Success.
[17.03.2025 12:20] Enriching papers with extra data.
[17.03.2025 12:20] ********************************************************************************
[17.03.2025 12:20] Abstract 0. Camera control has been actively studied in text or image conditioned video generation tasks. However, altering camera trajectories of a given video remains under-explored, despite its importance in the field of video creation. It is non-trivial due to the extra constraints of maintaining multiple-f...
[17.03.2025 12:20] ********************************************************************************
[17.03.2025 12:20] Abstract 1. Diffusion models have shown impressive results in generating high-quality conditional samples using guidance techniques such as Classifier-Free Guidance (CFG). However, existing methods often require additional training or neural function evaluations (NFEs), making them incompatible with guidance-di...
[17.03.2025 12:20] ********************************************************************************
[17.03.2025 12:20] Abstract 2. The pursuit of data efficiency, where quality outweighs quantity, has emerged as a cornerstone in robotic manipulation, especially given the high costs associated with real-world data collection. We propose that maximizing the informational density of individual demonstrations can dramatically reduc...
[17.03.2025 12:20] ********************************************************************************
[17.03.2025 12:20] Abstract 3. State Space Models (SSMs) have emerged as a promising alternative to the popular transformer-based models and have been increasingly gaining attention. Compared to transformers, SSMs excel at tasks with sequential data or longer contexts, demonstrating comparable performances with significant effici...
[17.03.2025 12:20] ********************************************************************************
[17.03.2025 12:20] Abstract 4. Large language models (LLMs) have evolved beyond simple text generation to power software agents that directly translate natural language commands into tangible actions. While API-based LLM agents initially rose to prominence for their robust automation capabilities and seamless integration with pro...
[17.03.2025 12:20] ********************************************************************************
[17.03.2025 12:20] Abstract 5. Federated Learning (FL) has emerged as a promising privacy-preserving collaborative model training paradigm without sharing raw data. However, recent studies have revealed that private information can still be leaked through shared gradient information and attacked by Gradient Inversion Attacks (GIA...
[17.03.2025 12:20] ********************************************************************************
[17.03.2025 12:20] Abstract 6. We propose a novel approach for captioning and object grounding in video, where the objects in the caption are grounded in the video via temporally dense bounding boxes. We introduce the following contributions. First, we present a large-scale automatic annotation method that aggregates captions gro...
[17.03.2025 12:20] ********************************************************************************
[17.03.2025 12:20] Abstract 7. Precision therapeutics require multimodal adaptive models that generate personalized treatment recommendations. We introduce TxAgent, an AI agent that leverages multi-step reasoning and real-time biomedical knowledge retrieval across a toolbox of 211 tools to analyze drug interactions, contraindicat...
[17.03.2025 12:20] ********************************************************************************
[17.03.2025 12:20] Abstract 8. Bridging different modalities lies at the heart of cross-modality generation. While conventional approaches treat the text modality as a conditioning signal that gradually guides the denoising process from Gaussian noise to the target image modality, we explore a much simpler paradigm-directly evolv...
[17.03.2025 12:20] ********************************************************************************
[17.03.2025 12:20] Abstract 9. Kolmogorov-Arnold networks (KANs) are a remarkable innovation consisting of learnable activation functions with the potential to capture more complex relationships from data. Although KANs are useful in finding symbolic representations and continual learning of one-dimensional functions, their effec...
[17.03.2025 12:20] ********************************************************************************
[17.03.2025 12:20] Abstract 10. Video Detailed Captioning (VDC) is a crucial task for vision-language bridging, enabling fine-grained descriptions of complex video content. In this paper, we first comprehensively benchmark current state-of-the-art approaches and systematically identified two critical limitations: biased capability...
[17.03.2025 12:20] ********************************************************************************
[17.03.2025 12:20] Abstract 11. Visual autoregressive models typically adhere to a raster-order ``next-token prediction" paradigm, which overlooks the spatial and temporal locality inherent in visual content. Specifically, visual tokens exhibit significantly stronger correlations with their spatially or temporally adjacent tokens ...
[17.03.2025 12:20] ********************************************************************************
[17.03.2025 12:20] Abstract 12. As multi-modal large language models (MLLMs) frequently exhibit errors when solving scientific problems, evaluating the validity of their reasoning processes is critical for ensuring reliability and uncovering fine-grained model weaknesses. Since human evaluation is laborious and costly, prompting M...
[17.03.2025 12:20] ********************************************************************************
[17.03.2025 12:20] Abstract 13. Unified models (UniMs) for multimodal understanding and generation have recently received much attention in the area of vision and language. Existing UniMs are designed to simultaneously learn both multimodal understanding and generation capabilities, demanding substantial computational resources, a...
[17.03.2025 12:20] ********************************************************************************
[17.03.2025 12:20] Abstract 14. Accelerating diffusion model sampling is crucial for efficient AIGC deployment. While diffusion distillation methods -- based on distribution matching and trajectory matching -- reduce sampling to as few as one step, they fall short on complex tasks like text-to-image generation. Few-step generation...
[17.03.2025 12:20] ********************************************************************************
[17.03.2025 12:20] Abstract 15. Fitting a body to a 3D clothed human point cloud is a common yet challenging task. Traditional optimization-based approaches use multi-stage pipelines that are sensitive to pose initialization, while recent learning-based methods often struggle with generalization across diverse poses and garment ty...
[17.03.2025 12:20] ********************************************************************************
[17.03.2025 12:20] Abstract 16. Learning skills in open-world environments is essential for developing agents capable of handling a variety of tasks by combining basic skills. Online demonstration videos are typically long but unsegmented, making them difficult to segment and label with skill identifiers. Unlike existing methods t...
[17.03.2025 12:20] ********************************************************************************
[17.03.2025 12:20] Abstract 17. We propose GoalFlow, an end-to-end autonomous driving method for generating high-quality multimodal trajectories. In autonomous driving scenarios, there is rarely a single suitable trajectory. Recent methods have increasingly focused on modeling multimodal trajectory distributions. However, they suf...
[17.03.2025 12:20] ********************************************************************************
[17.03.2025 12:20] Abstract 18. Accurate material retrieval is critical for creating realistic 3D assets. Existing methods rely on datasets that capture shape-invariant and lighting-varied representations of materials, which are scarce and face challenges due to limited diversity and inadequate real-world generalization. Most curr...
[17.03.2025 12:20] ********************************************************************************
[17.03.2025 12:20] Abstract 19. We present VGGT, a feed-forward neural network that directly infers all key 3D attributes of a scene, including camera parameters, point maps, depth maps, and 3D point tracks, from one, a few, or hundreds of its views. This approach is a step forward in 3D computer vision, where models have typicall...
[17.03.2025 12:20] ********************************************************************************
[17.03.2025 12:20] Abstract 20. Large language models (LLMs) have shown remarkable performance and generalization capabilities across multiple languages and tasks, making them very attractive targets for multi-modality integration (e.g., images or speech). In this work, we extend an existing LLM to the speech modality via speech d...
[17.03.2025 12:20] ********************************************************************************
[17.03.2025 12:20] Abstract 21. Machine unlearning is an emerging paradigm to remove the influence of specific training data (i.e., the forget set) from a model while preserving its knowledge of the rest of the data (i.e., the retain set). Previous approaches assume the forget data to be uniformly distributed from all training dat...
[17.03.2025 12:20] Read previous papers.
[17.03.2025 12:20] Generating reviews via LLM API.
[17.03.2025 12:20] Using data from previous issue: {"categories": ["#dataset", "#optimization", "#training", "#video", "#games"], "emoji": "üé•", "ru": {"title": "–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∫–∞–º–µ—Ä–æ–π –≤ –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "ReCamMaster - —ç—Ç–æ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –∏–∑–º–µ–Ω–µ–Ω–∏—è —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –∫–∞–º–µ—Ä—ã –≤ –≤–∏–¥–µ–æ. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ text-
[17.03.2025 12:20] Using data from previous issue: {"categories": ["#training", "#cv", "#diffusion", "#inference"], "emoji": "üñºÔ∏è", "ru": {"title": "PLADIS: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º PLADIS –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–∏—Ñ—Ñ—É–∑–∏–∏ –≤ –∑–∞–¥–∞—á–µ –≥
[17.03.2025 12:20] Using data from previous issue: {"categories": ["#robotics", "#benchmark", "#dataset", "#optimization", "#open_source", "#agents", "#training", "#data"], "emoji": "ü§ñ", "ru": {"title": "–ú–µ–Ω—å—à–µ –¥–∞–Ω–Ω—ã—Ö, –±–æ–ª—å—à–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ –æ–±—É—á–µ–Ω–∏–∏ —Ä–æ–±–æ—Ç–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–±–æ—Ä—É –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Ä–æ–±–æ—Ç–æ–≤ –º
[17.03.2025 12:20] Using data from previous issue: {"categories": ["#architecture", "#long_context", "#survey", "#math", "#training"], "emoji": "üß†", "ru": {"title": "SSM: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞–º –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π", "desc": "–≠—Ç–æ –æ–±–∑–æ—Ä –º–æ–¥–µ–ª–µ–π –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ —Å–æ—Å—Ç–æ—è–Ω–∏–π (SSM) –∫–∞–∫ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞–º –≤ –º–∞—à–∏–Ω–Ω–æ–º –æ–±—É—á–µ–Ω–∏
[17.03.2025 12:20] Using data from previous issue: {"categories": ["#multimodal", "#survey", "#agents"], "emoji": "ü§ñ", "ru": {"title": "–°—Ä–∞–≤–Ω–µ–Ω–∏–µ API –∏ GUI –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM: –ø—É—Ç—å –∫ –≥–∏–±—Ä–∏–¥–Ω—ã–º —Ä–µ—à–µ–Ω–∏—è–º", "desc": "–≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Å—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM), —Ä–∞–±–æ—Ç–∞—é—â–∏—Ö —á–µ—Ä–µ–∑ API –∏ —á–µ—Ä–µ–∑ –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å. –ê–≤—Ç–æ—Ä
[17.03.2025 12:20] Using data from previous issue: {"categories": ["#leakage", "#benchmark", "#security", "#survey", "#healthcare", "#data"], "emoji": "üõ°Ô∏è", "ru": {"title": "–ó–∞—â–∏—Ç–∞ –ø—Ä–∏–≤–∞—Ç–Ω–æ—Å—Ç–∏ –≤ —Ñ–µ–¥–µ—Ä–∞—Ç–∏–≤–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏: –∞–Ω–∞–ª–∏–∑ –∞—Ç–∞–∫ —Å –∏–Ω–≤–µ—Ä—Å–∏–µ–π –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞", "desc": "–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –∞–Ω–∞–ª–∏–∑—É –∞—Ç–∞–∫ —Å –∏–Ω–≤–µ—Ä—Å–∏–µ–π –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ (GIA) –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ —Ñ–µ–¥–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á
[17.03.2025 12:20] Using data from previous issue: {"categories": ["#cv", "#dataset", "#training", "#video", "#data"], "emoji": "üé•", "ru": {"title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ –≤–∏–¥–µ–æ: –æ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π —Ä–∞–∑–º–µ—Ç–∫–∏ –∫ —Ç–æ—á–Ω–æ–π –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏ –æ–±—ä–µ–∫—Ç–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ–¥–ø–∏—Å–µ–π –∏ –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏ –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –≤–∏–¥–µ–æ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ç–µ
[17.03.2025 12:20] Using data from previous issue: {"categories": ["#alignment", "#healthcare", "#science", "#agents", "#reasoning", "#benchmark", "#multimodal"], "emoji": "üíä", "ru": {"title": "TxAgent: –ò–ò-–ø–æ–º–æ—â–Ω–∏–∫ –¥–ª—è —Ç–æ—á–Ω–æ–π –∏ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Ñ–∞—Ä–º–∞–∫–æ—Ç–µ—Ä–∞–ø–∏–∏", "desc": "TxAgent - —ç—Ç–æ –ò–ò-–∞–≥–µ–Ω—Ç –¥–ª—è –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Ç–µ—Ä–∞–ø–∏–∏, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –º–Ω–æ–≥–æ—ç—Ç–∞
[17.03.2025 12:20] Using data from previous issue: {"categories": ["#architecture", "#training", "#multimodal", "#diffusion"], "emoji": "üåä", "ru": {"title": "FlowTok: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –ø–µ—Ä–µ—Ö–æ–¥ –º–µ–∂–¥—É —Ç–µ–∫—Å—Ç–æ–º –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º —á–µ—Ä–µ–∑ —Ç–æ–∫–µ–Ω—ã", "desc": "FlowTok - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏–∑ —Ç–µ–∫—Å—Ç–∞ –∏ –Ω–∞–æ–±–æ—Ä–æ—Ç. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–µ—Ç–æ–¥ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏—è –ø–æ—Ç–æ–∫
[17.03.2025 12:20] Using data from previous issue: {"categories": ["#architecture", "#cv", "#optimization", "#open_source", "#training"], "emoji": "üß†", "ru": {"title": "–ù–æ–≤—ã–π –≤–∑–≥–ª—è–¥ –Ω–∞ –≤–Ω–∏–º–∞–Ω–∏–µ: –ö–æ–ª–º–æ–≥–æ—Ä–æ–≤-–ê—Ä–Ω–æ–ª—å–¥ —Å–µ—Ç–∏ –≤ Vision Transformers", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π - –ö–æ–ª–º–æ–≥–æ—Ä–æ–≤-–ê—Ä–Ω–æ–ª—å–¥ –≤–Ω–∏–º–∞–Ω–∏–µ (KArAt
[17.03.2025 12:20] Using data from previous issue: {"categories": ["#benchmark", "#training", "#multimodal", "#synthetic", "#alignment"], "emoji": "ü¶ú", "ru": {"title": "Cockatiel: –ù–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –≤ –¥–µ—Ç–∞–ª—å–Ω–æ–º –æ–ø–∏—Å–∞–Ω–∏–∏ –≤–∏–¥–µ–æ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –¥–µ—Ç–∞–ª—å–Ω–æ–º—É –æ–ø–∏—Å–∞–Ω–∏—é –≤–∏–¥–µ–æ (VDC) –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Cockatiel. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ —Ç—Ä–µ—Ö—ç—Ç–∞
[17.03.2025 12:20] Using data from previous issue: {"categories": ["#cv", "#video", "#games", "#benchmark", "#optimization", "#architecture", "#training"], "emoji": "üß©", "ru": {"title": "NAR: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω–æ–º –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–∏ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω–æ–º—É –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—é –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –∫–æ–Ω
[17.03.2025 12:20] Using data from previous issue: {"categories": ["#interpretability", "#reasoning", "#multimodal", "#science", "#open_source", "#dataset", "#training", "#benchmark"], "emoji": "üî¨", "ru": {"title": "–ù–∞–¥–µ–∂–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞—É—á–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–º–∏ –ò–ò-—Å—É–¥—å—è–º–∏", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω ProJudgeBench - –ø–µ—Ä–≤—ã–π –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á
[17.03.2025 12:20] Using data from previous issue: {"categories": ["#games", "#optimization", "#multimodal", "#dataset", "#training", "#architecture"], "emoji": "üõ°Ô∏è", "ru": {"title": "ARMOR: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "ARMOR - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–æ–∑–¥–∞–Ω–∏—é —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.
[17.03.2025 12:20] Using data from previous issue: {"categories": ["#diffusion", "#training", "#cv", "#video"], "emoji": "üöÄ", "ru": {"title": "TDM: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ —É—Å–∫–æ—Ä–µ–Ω–∏–∏ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —É—Å–∫–æ—Ä–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –Ω–∞–∑—ã–≤–∞–µ–º—ã–π Trajectory Distribution Matching (TDM). TD
[17.03.2025 12:20] Using data from previous issue: {"categories": ["#3d", "#optimization", "#cv", "#open_source"], "emoji": "üëï", "ru": {"title": "–¢–æ—á–Ω–∞—è –ø–æ–¥–≥–æ–Ω–∫–∞ 3D-–º–æ–¥–µ–ª–∏ —Ç–µ–ª–∞ –∫ –æ–¥–µ—Ç–æ–º—É —á–µ–ª–æ–≤–µ–∫—É —Å —É—á–µ—Ç–æ–º –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏ –ø—Ä–∏–ª–µ–≥–∞–Ω–∏—è –æ–¥–µ–∂–¥—ã", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ ETCH –¥–ª—è –ø–æ–¥–≥–æ–Ω–∫–∏ —Ç—Ä–µ—Ö–º–µ—Ä–Ω–æ–π –º–æ–¥–µ–ª–∏ —Ç–µ–ª–∞ –∫ –æ–±–ª–∞–∫—É —Ç–æ—á–µ–∫ –æ–¥–µ—Ç–æ–≥–æ —á–µ–ª–æ–≤–µ–∫–∞. E
[17.03.2025 12:20] Querying the API.
[17.03.2025 12:20] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Learning skills in open-world environments is essential for developing agents capable of handling a variety of tasks by combining basic skills. Online demonstration videos are typically long but unsegmented, making them difficult to segment and label with skill identifiers. Unlike existing methods that rely on sequence sampling or human labeling, we have developed a self-supervised learning-based approach to segment these long videos into a series of semantic-aware and skill-consistent segments. Drawing inspiration from human cognitive event segmentation theory, we introduce Skill Boundary Detection (SBD), an annotation-free temporal video segmentation algorithm. SBD detects skill boundaries in a video by leveraging prediction errors from a pretrained unconditional action-prediction model. This approach is based on the assumption that a significant increase in prediction error indicates a shift in the skill being executed. We evaluated our method in Minecraft, a rich open-world simulator with extensive gameplay videos available online. Our SBD-generated segments improved the average performance of conditioned policies by 63.7% and 52.1% on short-term atomic skill tasks, and their corresponding hierarchical agents by 11.3% and 20.8% on long-horizon tasks. Our method can leverage the diverse YouTube videos to train instruction-following agents. The project page can be found in https://craftjarvis.github.io/SkillDiscovery.
[17.03.2025 12:20] Response: {
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏—è –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ –Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ —Å–≤—è–∑–∞–Ω–Ω—ã—Ö –Ω–∞–≤—ã–∫–æ–≤. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –∞–ª–≥–æ—Ä–∏—Ç–º Skill Boundary Detection (SBD), –∫–æ—Ç–æ—Ä—ã–π –æ–±–Ω–∞—Ä—É–∂–∏–≤–∞–µ—Ç –≥—Ä–∞–Ω–∏—Ü—ã –Ω–∞–≤—ã–∫–æ–≤ –≤ –≤–∏–¥–µ–æ, –∏—Å–ø–æ–ª—å–∑—É—è –æ—à–∏–±–∫–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏. –ú–µ—Ç–æ–¥ –±—ã–ª –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω –Ω–∞ –≤–∏–¥–µ–æ –∏–≥—Ä–æ–≤–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞ Minecraft –∏ –ø–æ–∫–∞–∑–∞–ª –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –æ–±—É—Å–ª–æ–≤–ª–µ–Ω–Ω—ã—Ö –ø–æ–ª–∏—Ç–∏–∫ –∏ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏—Ö –∞–≥–µ–Ω—Ç–æ–≤. –î–∞–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –≤–∏–¥–µ–æ —Å YouTube –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤, –≤—ã–ø–æ–ª–Ω—è—é—â–∏—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏.",
  "emoji": "üéÆ",
  "title": "–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –≤–∏–¥–µ–æ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤ —Å–ª–æ–∂–Ω—ã–º –Ω–∞–≤—ã–∫–∞–º"
}
[17.03.2025 12:20] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Learning skills in open-world environments is essential for developing agents capable of handling a variety of tasks by combining basic skills. Online demonstration videos are typically long but unsegmented, making them difficult to segment and label with skill identifiers. Unlike existing methods that rely on sequence sampling or human labeling, we have developed a self-supervised learning-based approach to segment these long videos into a series of semantic-aware and skill-consistent segments. Drawing inspiration from human cognitive event segmentation theory, we introduce Skill Boundary Detection (SBD), an annotation-free temporal video segmentation algorithm. SBD detects skill boundaries in a video by leveraging prediction errors from a pretrained unconditional action-prediction model. This approach is based on the assumption that a significant increase in prediction error indicates a shift in the skill being executed. We evaluated our method in Minecraft, a rich open-world simulator with extensive gameplay videos available online. Our SBD-generated segments improved the average performance of conditioned policies by 63.7% and 52.1% on short-term atomic skill tasks, and their corresponding hierarchical agents by 11.3% and 20.8% on long-horizon tasks. Our method can leverage the diverse YouTube videos to train instruction-following agents. The project page can be found in https://craftjarvis.github.io/SkillDiscovery."

[17.03.2025 12:20] Response: ```python
['AGENTS', 'VIDEO']
```
[17.03.2025 12:20] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Learning skills in open-world environments is essential for developing agents capable of handling a variety of tasks by combining basic skills. Online demonstration videos are typically long but unsegmented, making them difficult to segment and label with skill identifiers. Unlike existing methods that rely on sequence sampling or human labeling, we have developed a self-supervised learning-based approach to segment these long videos into a series of semantic-aware and skill-consistent segments. Drawing inspiration from human cognitive event segmentation theory, we introduce Skill Boundary Detection (SBD), an annotation-free temporal video segmentation algorithm. SBD detects skill boundaries in a video by leveraging prediction errors from a pretrained unconditional action-prediction model. This approach is based on the assumption that a significant increase in prediction error indicates a shift in the skill being executed. We evaluated our method in Minecraft, a rich open-world simulator with extensive gameplay videos available online. Our SBD-generated segments improved the average performance of conditioned policies by 63.7% and 52.1% on short-term atomic skill tasks, and their corresponding hierarchical agents by 11.3% and 20.8% on long-horizon tasks. Our method can leverage the diverse YouTube videos to train instruction-following agents. The project page can be found in https://craftjarvis.github.io/SkillDiscovery."

[17.03.2025 12:20] Response: ```python
['GAMES', 'OPEN_SOURCE']
```
[17.03.2025 12:20] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents a novel approach for segmenting long, unstructured demonstration videos into meaningful skill segments using self-supervised learning. The method, called Skill Boundary Detection (SBD), identifies transitions between skills by analyzing prediction errors from a pretrained action-prediction model. By applying this technique, the authors demonstrate significant improvements in the performance of agents trained on these segmented skills in the Minecraft environment. This approach allows for the effective use of diverse online video content to enhance the training of instruction-following agents without the need for manual labeling.","title":"Segmenting Skills for Smarter Agents"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper presents a novel approach for segmenting long, unstructured demonstration videos into meaningful skill segments using self-supervised learning. The method, called Skill Boundary Detection (SBD), identifies transitions between skills by analyzing prediction errors from a pretrained action-prediction model. By applying this technique, the authors demonstrate significant improvements in the performance of agents trained on these segmented skills in the Minecraft environment. This approach allows for the effective use of diverse online video content to enhance the training of instruction-following agents without the need for manual labeling.', title='Segmenting Skills for Smarter Agents'))
[17.03.2025 12:20] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Âú®ÂºÄÊîæ‰∏ñÁïåÁéØÂ¢É‰∏≠Â≠¶‰π†ÊäÄËÉΩÂØπ‰∫éÂºÄÂèëËÉΩÂ§üÂ§ÑÁêÜÂ§öÁßç‰ªªÂä°ÁöÑÊô∫ËÉΩ‰ΩìËá≥ÂÖ≥ÈáçË¶Å„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éËá™ÁõëÁù£Â≠¶‰π†ÁöÑÊñπÊ≥ïÔºåÂèØ‰ª•Â∞ÜÈïøËßÜÈ¢ëÂàÜÂâ≤Êàê‰∏ÄÁ≥ªÂàóËØ≠‰πâÊòéÁ°Æ‰∏îÊäÄËÉΩ‰∏ÄËá¥ÁöÑÁâáÊÆµÔºåËÄåÊó†ÈúÄ‰∫∫Â∑•Ê†áÊ≥®„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÊ£ÄÊµãÈ¢ÑÊµãËØØÂ∑ÆÊù•ËØÜÂà´ÊäÄËÉΩËæπÁïåÔºåÂÅáËÆæÈ¢ÑÊµãËØØÂ∑ÆÁöÑÊòæËëóÂ¢ûÂä†Ë°®ÊòéÊäÄËÉΩÁöÑËΩ¨Âèò„ÄÇÊàë‰ª¨ÁöÑÂÆûÈ™åË°®ÊòéÔºåËøôÁßçÊñπÊ≥ïÂú®Minecraft‰∏≠ÊòæËëóÊèêÈ´ò‰∫ÜÊù°‰ª∂Á≠ñÁï•ÂíåÂ±ÇÊ¨°‰ª£ÁêÜÁöÑË°®Áé∞„ÄÇ","title":"Ëá™ÁõëÁù£Â≠¶‰π†Âä©ÂäõÊäÄËÉΩËæπÁïåÊ£ÄÊµã"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Âú®ÂºÄÊîæ‰∏ñÁïåÁéØÂ¢É‰∏≠Â≠¶‰π†ÊäÄËÉΩÂØπ‰∫éÂºÄÂèëËÉΩÂ§üÂ§ÑÁêÜÂ§öÁßç‰ªªÂä°ÁöÑÊô∫ËÉΩ‰ΩìËá≥ÂÖ≥ÈáçË¶Å„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éËá™ÁõëÁù£Â≠¶‰π†ÁöÑÊñπÊ≥ïÔºåÂèØ‰ª•Â∞ÜÈïøËßÜÈ¢ëÂàÜÂâ≤Êàê‰∏ÄÁ≥ªÂàóËØ≠‰πâÊòéÁ°Æ‰∏îÊäÄËÉΩ‰∏ÄËá¥ÁöÑÁâáÊÆµÔºåËÄåÊó†ÈúÄ‰∫∫Â∑•Ê†áÊ≥®„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÊ£ÄÊµãÈ¢ÑÊµãËØØÂ∑ÆÊù•ËØÜÂà´ÊäÄËÉΩËæπÁïåÔºåÂÅáËÆæÈ¢ÑÊµãËØØÂ∑ÆÁöÑÊòæËëóÂ¢ûÂä†Ë°®ÊòéÊäÄËÉΩÁöÑËΩ¨Âèò„ÄÇÊàë‰ª¨ÁöÑÂÆûÈ™åË°®ÊòéÔºåËøôÁßçÊñπÊ≥ïÂú®Minecraft‰∏≠ÊòæËëóÊèêÈ´ò‰∫ÜÊù°‰ª∂Á≠ñÁï•ÂíåÂ±ÇÊ¨°‰ª£ÁêÜÁöÑË°®Áé∞„ÄÇ', title='Ëá™ÁõëÁù£Â≠¶‰π†Âä©ÂäõÊäÄËÉΩËæπÁïåÊ£ÄÊµã'))
[17.03.2025 12:20] Using data from previous issue: {"categories": ["#optimization", "#diffusion", "#agents", "#multimodal"], "emoji": "üöó", "ru": {"title": "GoalFlow: –¢–æ—á–Ω–æ–µ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π –¥–ª—è –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –≤–æ–∂–¥–µ–Ω–∏—è", "desc": "GoalFlow - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –≤–æ–∂–¥–µ–Ω–∏—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π. –û–Ω —Ä–µ—à–∞–µ—Ç
[17.03.2025 12:20] Using data from previous issue: {"categories": ["#synthetic", "#optimization", "#dataset", "#cv", "#3d"], "emoji": "üîç", "ru": {"title": "MaRI: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –ø–æ–∏—Å–∫–µ 3D-–º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç MaRI - —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–∏—Å–∫–∞ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö 3D-–º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤. –û–Ω —Å–æ–∑–¥–∞–µ—Ç –æ–±—â–µ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –∏
[17.03.2025 12:20] Querying the API.
[17.03.2025 12:20] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

We present VGGT, a feed-forward neural network that directly infers all key 3D attributes of a scene, including camera parameters, point maps, depth maps, and 3D point tracks, from one, a few, or hundreds of its views. This approach is a step forward in 3D computer vision, where models have typically been constrained to and specialized for single tasks. It is also simple and efficient, reconstructing images in under one second, and still outperforming alternatives that require post-processing with visual geometry optimization techniques. The network achieves state-of-the-art results in multiple 3D tasks, including camera parameter estimation, multi-view depth estimation, dense point cloud reconstruction, and 3D point tracking. We also show that using pretrained VGGT as a feature backbone significantly enhances downstream tasks, such as non-rigid point tracking and feed-forward novel view synthesis. Code and models are publicly available at https://github.com/facebookresearch/vggt.
[17.03.2025 12:20] Response: {
  "desc": "VGGT - —ç—Ç–æ –Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å –ø—Ä—è–º–æ–≥–æ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä–∞—è –Ω–∞–ø—Ä—è–º—É—é –≤—ã–≤–æ–¥–∏—Ç –≤—Å–µ –∫–ª—é—á–µ–≤—ã–µ 3D-–∞—Ç—Ä–∏–±—É—Ç—ã —Å—Ü–µ–Ω—ã –∏–∑ –æ–¥–Ω–æ–≥–æ –∏–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –µ—ë –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –ú–æ–¥–µ–ª—å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∏—Ä—É–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –º–µ–Ω–µ–µ —á–µ–º –∑–∞ —Å–µ–∫—É–Ω–¥—É, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã, —Ç—Ä–µ–±—É—é—â–∏–µ –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∏. VGGT –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –ø–µ—Ä–µ–¥–æ–≤—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö 3D-–∑–∞–¥–∞—á–∞—Ö, –≤–∫–ª—é—á–∞—è –æ—Ü–µ–Ω–∫—É –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∫–∞–º–µ—Ä—ã –∏ –≥–ª—É–±–∏–Ω—ã, —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—é –æ–±–ª–∞–∫–∞ —Ç–æ—á–µ–∫ –∏ 3D-—Ç—Ä–µ–∫–∏–Ω–≥. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π VGGT –≤ –∫–∞—á–µ—Å—Ç–≤–µ –æ—Å–Ω–æ–≤—ã –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞–µ—Ç —Ä–∞–±–æ—Ç—É –≤ –∑–∞–¥–∞—á–∞—Ö —Å–∏–Ω—Ç–µ–∑–∞ –Ω–æ–≤—ã—Ö —Ä–∞–∫—É—Ä—Å–æ–≤ –∏ –Ω–µ–∂–µ—Å—Ç–∫–æ–≥–æ —Ç—Ä–µ–∫–∏–Ω–≥–∞ —Ç–æ—á–µ–∫.",

  "emoji": "üîÆ",

  "title": "VGGT: –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –Ω–µ–π—Ä–æ—Å–µ—Ç—å –¥–ª—è –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ 3D-–∞–Ω–∞–ª–∏–∑–∞ —Å—Ü–µ–Ω"
}
[17.03.2025 12:20] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"We present VGGT, a feed-forward neural network that directly infers all key 3D attributes of a scene, including camera parameters, point maps, depth maps, and 3D point tracks, from one, a few, or hundreds of its views. This approach is a step forward in 3D computer vision, where models have typically been constrained to and specialized for single tasks. It is also simple and efficient, reconstructing images in under one second, and still outperforming alternatives that require post-processing with visual geometry optimization techniques. The network achieves state-of-the-art results in multiple 3D tasks, including camera parameter estimation, multi-view depth estimation, dense point cloud reconstruction, and 3D point tracking. We also show that using pretrained VGGT as a feature backbone significantly enhances downstream tasks, such as non-rigid point tracking and feed-forward novel view synthesis. Code and models are publicly available at https://github.com/facebookresearch/vggt."

[17.03.2025 12:20] Response: ```python
["3D", "CV"]
```
[17.03.2025 12:20] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"We present VGGT, a feed-forward neural network that directly infers all key 3D attributes of a scene, including camera parameters, point maps, depth maps, and 3D point tracks, from one, a few, or hundreds of its views. This approach is a step forward in 3D computer vision, where models have typically been constrained to and specialized for single tasks. It is also simple and efficient, reconstructing images in under one second, and still outperforming alternatives that require post-processing with visual geometry optimization techniques. The network achieves state-of-the-art results in multiple 3D tasks, including camera parameter estimation, multi-view depth estimation, dense point cloud reconstruction, and 3D point tracking. We also show that using pretrained VGGT as a feature backbone significantly enhances downstream tasks, such as non-rigid point tracking and feed-forward novel view synthesis. Code and models are publicly available at https://github.com/facebookresearch/vggt."

[17.03.2025 12:20] Response: ```python
["OPTIMIZATION", "OPEN_SOURCE"]
```
[17.03.2025 12:20] Response: ParsedChatCompletionMessage[Article](content='{"desc":"VGGT is a feed-forward neural network designed to extract key 3D attributes from various views of a scene, such as camera parameters and depth maps. Unlike traditional models that focus on single tasks, VGGT efficiently handles multiple 3D computer vision tasks simultaneously. It operates quickly, reconstructing images in under one second while achieving superior results compared to methods that rely on post-processing. Additionally, VGGT can be used as a feature backbone to improve performance in related tasks like point tracking and novel view synthesis.","title":"VGGT: Revolutionizing 3D Scene Understanding with Speed and Efficiency"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='VGGT is a feed-forward neural network designed to extract key 3D attributes from various views of a scene, such as camera parameters and depth maps. Unlike traditional models that focus on single tasks, VGGT efficiently handles multiple 3D computer vision tasks simultaneously. It operates quickly, reconstructing images in under one second while achieving superior results compared to methods that rely on post-processing. Additionally, VGGT can be used as a feature backbone to improve performance in related tasks like point tracking and novel view synthesis.', title='VGGT: Revolutionizing 3D Scene Understanding with Speed and Efficiency'))
[17.03.2025 12:20] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êàë‰ª¨ÊèêÂá∫‰∫ÜVGGTÔºåËøôÊòØ‰∏ÄÁßçÂâçÈ¶àÁ•ûÁªèÁΩëÁªúÔºåÂèØ‰ª•Áõ¥Êé•Êé®Êñ≠Âú∫ÊôØÁöÑÊâÄÊúâÂÖ≥ÈîÆ3DÂ±ûÊÄßÔºåÂåÖÊã¨Áõ∏Êú∫ÂèÇÊï∞„ÄÅÁÇπÂõæ„ÄÅÊ∑±Â∫¶ÂõæÂíå3DÁÇπËΩ®Ëøπ„ÄÇËØ•ÊñπÊ≥ïÂú®3DËÆ°ÁÆóÊú∫ËßÜËßâÈ¢ÜÂüüÂêëÂâçËøàÂá∫‰∫Ü‰∏ÄÊ≠•ÔºåÂÖãÊúç‰∫Ü‰ª•ÂæÄÊ®°Âûã‰ªÖÈôê‰∫éÂçï‰∏Ä‰ªªÂä°ÁöÑÂ±ÄÈôêÊÄß„ÄÇVGGTÁÆÄÂçïÈ´òÊïàÔºåËÉΩÂ§üÂú®‰∏çÂà∞‰∏ÄÁßíÁöÑÊó∂Èó¥ÂÜÖÈáçÂª∫ÂõæÂÉèÔºåÂπ∂‰∏îÂú®Â§ö‰∏™3D‰ªªÂä°‰∏≠Ë°®Áé∞‰ºò‰∫éÈúÄË¶ÅÂêéÂ§ÑÁêÜÁöÑÊõø‰ª£ÊñπÊ°à„ÄÇ‰ΩøÁî®È¢ÑËÆ≠ÁªÉÁöÑVGGT‰Ωú‰∏∫ÁâπÂæÅÈ™®Âπ≤ÊòæËëóÊèêÂçá‰∫Ü‰∏ãÊ∏∏‰ªªÂä°ÁöÑÊÄßËÉΩÔºåÂ¶ÇÈùûÂàöÊÄßÁÇπË∑üË∏™ÂíåÂâçÈ¶àÊñ∞ËßÜÂõæÂêàÊàê„ÄÇ","title":"VGGTÔºöÈ´òÊïàÁöÑ3DÂú∫ÊôØÊé®Êñ≠ÁΩëÁªú"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êàë‰ª¨ÊèêÂá∫‰∫ÜVGGTÔºåËøôÊòØ‰∏ÄÁßçÂâçÈ¶àÁ•ûÁªèÁΩëÁªúÔºåÂèØ‰ª•Áõ¥Êé•Êé®Êñ≠Âú∫ÊôØÁöÑÊâÄÊúâÂÖ≥ÈîÆ3DÂ±ûÊÄßÔºåÂåÖÊã¨Áõ∏Êú∫ÂèÇÊï∞„ÄÅÁÇπÂõæ„ÄÅÊ∑±Â∫¶ÂõæÂíå3DÁÇπËΩ®Ëøπ„ÄÇËØ•ÊñπÊ≥ïÂú®3DËÆ°ÁÆóÊú∫ËßÜËßâÈ¢ÜÂüüÂêëÂâçËøàÂá∫‰∫Ü‰∏ÄÊ≠•ÔºåÂÖãÊúç‰∫Ü‰ª•ÂæÄÊ®°Âûã‰ªÖÈôê‰∫éÂçï‰∏Ä‰ªªÂä°ÁöÑÂ±ÄÈôêÊÄß„ÄÇVGGTÁÆÄÂçïÈ´òÊïàÔºåËÉΩÂ§üÂú®‰∏çÂà∞‰∏ÄÁßíÁöÑÊó∂Èó¥ÂÜÖÈáçÂª∫ÂõæÂÉèÔºåÂπ∂‰∏îÂú®Â§ö‰∏™3D‰ªªÂä°‰∏≠Ë°®Áé∞‰ºò‰∫éÈúÄË¶ÅÂêéÂ§ÑÁêÜÁöÑÊõø‰ª£ÊñπÊ°à„ÄÇ‰ΩøÁî®È¢ÑËÆ≠ÁªÉÁöÑVGGT‰Ωú‰∏∫ÁâπÂæÅÈ™®Âπ≤ÊòæËëóÊèêÂçá‰∫Ü‰∏ãÊ∏∏‰ªªÂä°ÁöÑÊÄßËÉΩÔºåÂ¶ÇÈùûÂàöÊÄßÁÇπË∑üË∏™ÂíåÂâçÈ¶àÊñ∞ËßÜÂõæÂêàÊàê„ÄÇ', title='VGGTÔºöÈ´òÊïàÁöÑ3DÂú∫ÊôØÊé®Êñ≠ÁΩëÁªú'))
[17.03.2025 12:20] Using data from previous issue: {"categories": ["#multilingual", "#translation", "#open_source", "#multimodal", "#low_resource"], "emoji": "üó£Ô∏è", "ru": {"title": "–†–∞—Å—à–∏—Ä–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ —Ä–µ—á–µ–≤—É—é –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç—å", "desc": "–í —ç—Ç–æ–π —Å—Ç–∞—Ç—å–µ –æ–ø–∏—Å—ã–≤–∞–µ—Ç—Å—è —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –±–æ–ª—å—à–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ (LLM) –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ä–µ—á—å—é –ø—É—Ç–µ–º –¥–∏—Å
[17.03.2025 12:20] Using data from previous issue: {"categories": ["#training", "#dataset", "#ethics", "#data"], "emoji": "üß†", "ru": {"title": "–ì—Ä—É–ø–ø–æ–≤–æ–µ —Ä–∞–∑–æ–±—É—á–µ–Ω–∏–µ –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏", "desc": "–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –ø—Ä–æ–±–ª–µ–º–µ –≥—Ä—É–ø–ø–æ–≤–æ–≥–æ –º–∞—à–∏–Ω–Ω–æ–≥–æ —Ä–∞–∑–æ–±—É—á–µ–Ω–∏—è, –∫–æ–≥–¥–∞ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è –∏–∑ –º–æ–¥–µ–ª–∏ –Ω–µ—Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω—ã –º–µ–∂–¥—É –≥—Ä—É–ø–ø–∞–º–∏. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª
[17.03.2025 12:20] Loading Chinese text from previous data.
[17.03.2025 12:20] Renaming data file.
[17.03.2025 12:20] Renaming previous data. hf_papers.json to ./d/2025-03-17.json
[17.03.2025 12:20] Saving new data file.
[17.03.2025 12:20] Generating page.
[17.03.2025 12:20] Renaming previous page.
[17.03.2025 12:20] Renaming previous data. index.html to ./d/2025-03-17.html
[17.03.2025 12:20] [Experimental] Generating Chinese page for reading.
[17.03.2025 12:20] Chinese vocab [{'word': 'ËÆ®ËÆ∫', 'pinyin': 't«éo l√πn', 'trans': 'discuss'}, {'word': 'Âêç‰∏∫', 'pinyin': 'm√≠ng w√©i', 'trans': 'named'}, {'word': 'ËßÜÈ¢ë', 'pinyin': 'sh√¨ p√≠n', 'trans': 'video'}, {'word': 'ÈáçÊñ∞', 'pinyin': 'ch√≥ng xƒ´n', 'trans': 'again'}, {'word': 'Ê∏≤Êüì', 'pinyin': 'xu√†n r√°n', 'trans': 'render'}, {'word': 'Ê°ÜÊû∂', 'pinyin': 'ku√†ng ji√†', 'trans': 'framework'}, {'word': 'ÊëÑÂÉèÊú∫', 'pinyin': 'sh√® xi√†ng jƒ´', 'trans': 'camera'}, {'word': 'ËΩ®Ëøπ', 'pinyin': 'gu«ê j√¨', 'trans': 'trajectory'}, {'word': 'ÂêåÊó∂', 'pinyin': 't√≥ng sh√≠', 'trans': 'simultaneously'}, {'word': '‰øùÊåÅ', 'pinyin': 'b«éo ch√≠', 'trans': 'maintain'}, {'word': 'Â§öÂ∏ß', 'pinyin': 'du≈ç zhƒìn', 'trans': 'multi-frame'}, {'word': 'Â§ñËßÇ', 'pinyin': 'w√†i gu«én', 'trans': 'appearance'}, {'word': 'Âä®ÊÄÅ', 'pinyin': 'd√≤ng t√†i', 'trans': 'dynamic'}, {'word': 'ÂêåÊ≠•', 'pinyin': 't√≥ng b√π', 'trans': 'synchronization'}, {'word': 'ÊñπÊ≥ï', 'pinyin': 'fƒÅng f«é', 'trans': 'method'}, {'word': 'Âà©Áî®', 'pinyin': 'l√¨ y√≤ng', 'trans': 'utilize'}, {'word': 'È¢ÑËÆ≠ÁªÉ', 'pinyin': 'y√π x√πn li√†n', 'trans': 'pre-trained'}, {'word': 'ÊñáÊú¨', 'pinyin': 'w√©n bƒõn', 'trans': 'text'}, {'word': 'ÁîüÊàê', 'pinyin': 'shƒìng ch√©ng', 'trans': 'generate'}, {'word': 'ËÉΩÂäõ', 'pinyin': 'n√©ng l√¨', 'trans': 'ability'}, {'word': 'ÊûÑÂª∫', 'pinyin': 'g√≤u ji√†n', 'trans': 'construct'}, {'word': 'Êï∞ÊçÆÈõÜ', 'pinyin': 'sh√π j√π j√≠', 'trans': 'dataset'}, {'word': 'ÂÆûÈ™å', 'pinyin': 'sh√≠ y√†n', 'trans': 'experiment'}, {'word': 'ÁªìÊûú', 'pinyin': 'ji√© gu«í', 'trans': 'result'}, {'word': 'ÊòæÁ§∫', 'pinyin': 'xi«én sh√¨', 'trans': 'display'}, {'word': 'Á®≥ÂÆö', 'pinyin': 'wƒõn d√¨ng', 'trans': 'stable'}, {'word': 'Ë∂ÖÂàÜËæ®Áéá', 'pinyin': 'chƒÅo fƒìn bi√†n l«ú', 'trans': 'super-resolution'}, {'word': 'Êâ©Â±ï', 'pinyin': 'ku√≤ zh«én', 'trans': 'extend'}, {'word': 'Ë°®Áé∞', 'pinyin': 'bi«éo xi√†n', 'trans': 'performance'}, {'word': 'Âá∫Ëâ≤', 'pinyin': 'ch≈´ s√®', 'trans': 'outstanding'}]
[17.03.2025 12:20] Renaming previous Chinese page.
[17.03.2025 12:20] Renaming previous data. zh.html to ./d/2025-03-16_zh_reading_task.html
[17.03.2025 12:20] Writing Chinese reading task.
[17.03.2025 12:20] Writing result.
[17.03.2025 12:20] Renaming log file.
[17.03.2025 12:20] Renaming previous data. log.txt to ./logs/2025-03-17_last_log.txt
