[17.03.2025 14:11] Read previous papers.
[17.03.2025 14:11] Generating top page (month).
[17.03.2025 14:11] Writing top page (month).
[17.03.2025 15:11] Read previous papers.
[17.03.2025 15:11] Get feed.
[17.03.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.11647
[17.03.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.07677
[17.03.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.11646
[17.03.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.11224
[17.03.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.11069
[17.03.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.11514
[17.03.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.10781
[17.03.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.10970
[17.03.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.10772
[17.03.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.10632
[17.03.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.11651
[17.03.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.09279
[17.03.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.10696
[17.03.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.06542
[17.03.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.06674
[17.03.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.06553
[17.03.2025 15:11] Extract page data from URL. URL: https://huggingface.co/papers/2503.11579
[17.03.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.10624
[17.03.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.10684
[17.03.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.05689
[17.03.2025 15:11] Extract page data from URL. URL: https://huggingface.co/papers/2503.11629
[17.03.2025 15:11] Extract page data from URL. URL: https://huggingface.co/papers/2503.11576
[17.03.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.08111
[17.03.2025 15:11] Extract page data from URL. URL: https://huggingface.co/papers/2503.11207
[17.03.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.10620
[17.03.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.09330
[17.03.2025 15:11] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[17.03.2025 15:11] No deleted papers detected.
[17.03.2025 15:11] Downloading and parsing papers (pdf, html). Total: 26.
[17.03.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2503.11647.
[17.03.2025 15:11] Extra JSON file exists (./assets/json/2503.11647.json), skip PDF parsing.
[17.03.2025 15:11] Paper image links file exists (./assets/img_data/2503.11647.json), skip HTML parsing.
[17.03.2025 15:11] Success.
[17.03.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2503.07677.
[17.03.2025 15:11] Extra JSON file exists (./assets/json/2503.07677.json), skip PDF parsing.
[17.03.2025 15:11] Paper image links file exists (./assets/img_data/2503.07677.json), skip HTML parsing.
[17.03.2025 15:11] Success.
[17.03.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2503.11646.
[17.03.2025 15:11] Extra JSON file exists (./assets/json/2503.11646.json), skip PDF parsing.
[17.03.2025 15:11] Paper image links file exists (./assets/img_data/2503.11646.json), skip HTML parsing.
[17.03.2025 15:11] Success.
[17.03.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2503.11224.
[17.03.2025 15:11] Extra JSON file exists (./assets/json/2503.11224.json), skip PDF parsing.
[17.03.2025 15:11] Paper image links file exists (./assets/img_data/2503.11224.json), skip HTML parsing.
[17.03.2025 15:11] Success.
[17.03.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2503.11069.
[17.03.2025 15:11] Extra JSON file exists (./assets/json/2503.11069.json), skip PDF parsing.
[17.03.2025 15:11] Paper image links file exists (./assets/img_data/2503.11069.json), skip HTML parsing.
[17.03.2025 15:11] Success.
[17.03.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2503.11514.
[17.03.2025 15:11] Extra JSON file exists (./assets/json/2503.11514.json), skip PDF parsing.
[17.03.2025 15:11] Paper image links file exists (./assets/img_data/2503.11514.json), skip HTML parsing.
[17.03.2025 15:11] Success.
[17.03.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2503.10781.
[17.03.2025 15:11] Extra JSON file exists (./assets/json/2503.10781.json), skip PDF parsing.
[17.03.2025 15:11] Paper image links file exists (./assets/img_data/2503.10781.json), skip HTML parsing.
[17.03.2025 15:11] Success.
[17.03.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2503.10970.
[17.03.2025 15:11] Extra JSON file exists (./assets/json/2503.10970.json), skip PDF parsing.
[17.03.2025 15:11] Paper image links file exists (./assets/img_data/2503.10970.json), skip HTML parsing.
[17.03.2025 15:11] Success.
[17.03.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2503.10772.
[17.03.2025 15:11] Extra JSON file exists (./assets/json/2503.10772.json), skip PDF parsing.
[17.03.2025 15:11] Paper image links file exists (./assets/img_data/2503.10772.json), skip HTML parsing.
[17.03.2025 15:11] Success.
[17.03.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2503.10632.
[17.03.2025 15:11] Extra JSON file exists (./assets/json/2503.10632.json), skip PDF parsing.
[17.03.2025 15:11] Paper image links file exists (./assets/img_data/2503.10632.json), skip HTML parsing.
[17.03.2025 15:11] Success.
[17.03.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2503.11651.
[17.03.2025 15:11] Extra JSON file exists (./assets/json/2503.11651.json), skip PDF parsing.
[17.03.2025 15:11] Paper image links file exists (./assets/img_data/2503.11651.json), skip HTML parsing.
[17.03.2025 15:11] Success.
[17.03.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2503.09279.
[17.03.2025 15:11] Extra JSON file exists (./assets/json/2503.09279.json), skip PDF parsing.
[17.03.2025 15:11] Paper image links file exists (./assets/img_data/2503.09279.json), skip HTML parsing.
[17.03.2025 15:11] Success.
[17.03.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2503.10696.
[17.03.2025 15:11] Extra JSON file exists (./assets/json/2503.10696.json), skip PDF parsing.
[17.03.2025 15:11] Paper image links file exists (./assets/img_data/2503.10696.json), skip HTML parsing.
[17.03.2025 15:11] Success.
[17.03.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2503.06542.
[17.03.2025 15:11] Extra JSON file exists (./assets/json/2503.06542.json), skip PDF parsing.
[17.03.2025 15:11] Paper image links file exists (./assets/img_data/2503.06542.json), skip HTML parsing.
[17.03.2025 15:11] Success.
[17.03.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2503.06674.
[17.03.2025 15:11] Extra JSON file exists (./assets/json/2503.06674.json), skip PDF parsing.
[17.03.2025 15:11] Paper image links file exists (./assets/img_data/2503.06674.json), skip HTML parsing.
[17.03.2025 15:11] Success.
[17.03.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2503.06553.
[17.03.2025 15:11] Extra JSON file exists (./assets/json/2503.06553.json), skip PDF parsing.
[17.03.2025 15:11] Paper image links file exists (./assets/img_data/2503.06553.json), skip HTML parsing.
[17.03.2025 15:11] Success.
[17.03.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2503.11579.
[17.03.2025 15:11] Extra JSON file exists (./assets/json/2503.11579.json), skip PDF parsing.
[17.03.2025 15:11] Paper image links file exists (./assets/img_data/2503.11579.json), skip HTML parsing.
[17.03.2025 15:11] Success.
[17.03.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2503.10624.
[17.03.2025 15:11] Extra JSON file exists (./assets/json/2503.10624.json), skip PDF parsing.
[17.03.2025 15:11] Paper image links file exists (./assets/img_data/2503.10624.json), skip HTML parsing.
[17.03.2025 15:11] Success.
[17.03.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2503.10684.
[17.03.2025 15:11] Extra JSON file exists (./assets/json/2503.10684.json), skip PDF parsing.
[17.03.2025 15:11] Paper image links file exists (./assets/img_data/2503.10684.json), skip HTML parsing.
[17.03.2025 15:11] Success.
[17.03.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2503.05689.
[17.03.2025 15:11] Extra JSON file exists (./assets/json/2503.05689.json), skip PDF parsing.
[17.03.2025 15:11] Paper image links file exists (./assets/img_data/2503.05689.json), skip HTML parsing.
[17.03.2025 15:11] Success.
[17.03.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2503.11629.
[17.03.2025 15:11] Extra JSON file exists (./assets/json/2503.11629.json), skip PDF parsing.
[17.03.2025 15:11] Paper image links file exists (./assets/img_data/2503.11629.json), skip HTML parsing.
[17.03.2025 15:11] Success.
[17.03.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2503.11576.
[17.03.2025 15:11] Extra JSON file exists (./assets/json/2503.11576.json), skip PDF parsing.
[17.03.2025 15:11] Paper image links file exists (./assets/img_data/2503.11576.json), skip HTML parsing.
[17.03.2025 15:11] Success.
[17.03.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2503.08111.
[17.03.2025 15:11] Extra JSON file exists (./assets/json/2503.08111.json), skip PDF parsing.
[17.03.2025 15:11] Paper image links file exists (./assets/img_data/2503.08111.json), skip HTML parsing.
[17.03.2025 15:11] Success.
[17.03.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2503.11207.
[17.03.2025 15:11] Downloading paper 2503.11207 from http://arxiv.org/pdf/2503.11207v1...
[17.03.2025 15:11] Extracting affiliations from text.
[17.03.2025 15:11] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 4 1 ] . [ 1 7 0 2 1 1 . 3 0 5 2 : r 126 Can Large Reasoning Models do Analogical Reasoning under Perceptual Uncertainty? Giacomo Camposampiero IBM Research - Zurich and ETH Z√ºrich Michael Hersche IBM Research - Zurich giacomo.camposampiero1@ibm.com michael.hersche@ibm.com Roger Wattenhofer ETH Z√ºrich Abu Sebastian IBM Research - Zurich Abbas Rahimi IBM Research - Zurich wattenhofer@ethz.ch ase@zurich.ibm.com abr@zurich.ibm.com Abstract This work presents first evaluation of two state-of-the-art Large Reasoning Models (LRMs), OpenAIs o3-mini and DeepSeek R1, on analogical reasoning, focusing on well-established nonverbal human IQ tests based on Ravens progressive matrices. We benchmark with the I-RAVEN dataset and its more difficult extension, I-RAVEN-X, which tests the ability to generalize to longer reasoning rules and ranges of the attribute values. To assess the influence of visual uncertainties on these nonverbal analogical reasoning tests, we extend the I-RAVEN-X dataset, which otherwise assumes an oracle perception. We adopt twofold strategy to simulate this imperfect visual perception: 1) we introduce confounding attributes which, being sampled at random, do not contribute to the prediction of the correct answer of the puzzles and 2) smoothen the distributions of the input attributes values. We observe sharp decline in OpenAIs o3-mini task accuracy, dropping from 86.6% on the original I-RAVEN to just 17.0%approaching random chanceon the more challenging I-RAVEN-X, which increases input length and range and emulates perceptual uncertainty. This drop occurred despite spending 3.4 more reasoning tokens. similar trend is also observed for DeepSeek R1: from 80.6% to 23.2%. On the other hand, neuro-symbolic probabilistic abductive model, ARLC, that achieves state-of-the-art performances on IRAVEN, can robustly reason under all these out-of-distribution tests, maintaining strong accuracy with only modest reduction from 98.6% to 88.0%. Our code is available a"
[17.03.2025 15:11] Response: ```python
["IBM Research - Zurich", "ETH Z√ºrich"]
```
[17.03.2025 15:11] Deleting PDF ./assets/pdf/2503.11207.pdf.
[17.03.2025 15:11] Success.
[17.03.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2503.10620.
[17.03.2025 15:11] Extra JSON file exists (./assets/json/2503.10620.json), skip PDF parsing.
[17.03.2025 15:11] Paper image links file exists (./assets/img_data/2503.10620.json), skip HTML parsing.
[17.03.2025 15:11] Success.
[17.03.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2503.09330.
[17.03.2025 15:11] Extra JSON file exists (./assets/json/2503.09330.json), skip PDF parsing.
[17.03.2025 15:11] Paper image links file exists (./assets/img_data/2503.09330.json), skip HTML parsing.
[17.03.2025 15:11] Success.
[17.03.2025 15:11] Enriching papers with extra data.
[17.03.2025 15:11] ********************************************************************************
[17.03.2025 15:11] Abstract 0. Camera control has been actively studied in text or image conditioned video generation tasks. However, altering camera trajectories of a given video remains under-explored, despite its importance in the field of video creation. It is non-trivial due to the extra constraints of maintaining multiple-f...
[17.03.2025 15:11] ********************************************************************************
[17.03.2025 15:11] Abstract 1. Diffusion models have shown impressive results in generating high-quality conditional samples using guidance techniques such as Classifier-Free Guidance (CFG). However, existing methods often require additional training or neural function evaluations (NFEs), making them incompatible with guidance-di...
[17.03.2025 15:11] ********************************************************************************
[17.03.2025 15:11] Abstract 2. The pursuit of data efficiency, where quality outweighs quantity, has emerged as a cornerstone in robotic manipulation, especially given the high costs associated with real-world data collection. We propose that maximizing the informational density of individual demonstrations can dramatically reduc...
[17.03.2025 15:11] ********************************************************************************
[17.03.2025 15:11] Abstract 3. State Space Models (SSMs) have emerged as a promising alternative to the popular transformer-based models and have been increasingly gaining attention. Compared to transformers, SSMs excel at tasks with sequential data or longer contexts, demonstrating comparable performances with significant effici...
[17.03.2025 15:11] ********************************************************************************
[17.03.2025 15:11] Abstract 4. Large language models (LLMs) have evolved beyond simple text generation to power software agents that directly translate natural language commands into tangible actions. While API-based LLM agents initially rose to prominence for their robust automation capabilities and seamless integration with pro...
[17.03.2025 15:11] ********************************************************************************
[17.03.2025 15:11] Abstract 5. Federated Learning (FL) has emerged as a promising privacy-preserving collaborative model training paradigm without sharing raw data. However, recent studies have revealed that private information can still be leaked through shared gradient information and attacked by Gradient Inversion Attacks (GIA...
[17.03.2025 15:11] ********************************************************************************
[17.03.2025 15:11] Abstract 6. We propose a novel approach for captioning and object grounding in video, where the objects in the caption are grounded in the video via temporally dense bounding boxes. We introduce the following contributions. First, we present a large-scale automatic annotation method that aggregates captions gro...
[17.03.2025 15:11] ********************************************************************************
[17.03.2025 15:11] Abstract 7. Precision therapeutics require multimodal adaptive models that generate personalized treatment recommendations. We introduce TxAgent, an AI agent that leverages multi-step reasoning and real-time biomedical knowledge retrieval across a toolbox of 211 tools to analyze drug interactions, contraindicat...
[17.03.2025 15:11] ********************************************************************************
[17.03.2025 15:11] Abstract 8. Bridging different modalities lies at the heart of cross-modality generation. While conventional approaches treat the text modality as a conditioning signal that gradually guides the denoising process from Gaussian noise to the target image modality, we explore a much simpler paradigm-directly evolv...
[17.03.2025 15:11] ********************************************************************************
[17.03.2025 15:11] Abstract 9. Kolmogorov-Arnold networks (KANs) are a remarkable innovation consisting of learnable activation functions with the potential to capture more complex relationships from data. Although KANs are useful in finding symbolic representations and continual learning of one-dimensional functions, their effec...
[17.03.2025 15:11] ********************************************************************************
[17.03.2025 15:11] Abstract 10. We present VGGT, a feed-forward neural network that directly infers all key 3D attributes of a scene, including camera parameters, point maps, depth maps, and 3D point tracks, from one, a few, or hundreds of its views. This approach is a step forward in 3D computer vision, where models have typicall...
[17.03.2025 15:11] ********************************************************************************
[17.03.2025 15:11] Abstract 11. Video Detailed Captioning (VDC) is a crucial task for vision-language bridging, enabling fine-grained descriptions of complex video content. In this paper, we first comprehensively benchmark current state-of-the-art approaches and systematically identified two critical limitations: biased capability...
[17.03.2025 15:11] ********************************************************************************
[17.03.2025 15:11] Abstract 12. Visual autoregressive models typically adhere to a raster-order ``next-token prediction" paradigm, which overlooks the spatial and temporal locality inherent in visual content. Specifically, visual tokens exhibit significantly stronger correlations with their spatially or temporally adjacent tokens ...
[17.03.2025 15:11] ********************************************************************************
[17.03.2025 15:11] Abstract 13. Unified models (UniMs) for multimodal understanding and generation have recently received much attention in the area of vision and language. Existing UniMs are designed to simultaneously learn both multimodal understanding and generation capabilities, demanding substantial computational resources, a...
[17.03.2025 15:11] ********************************************************************************
[17.03.2025 15:11] Abstract 14. Accelerating diffusion model sampling is crucial for efficient AIGC deployment. While diffusion distillation methods -- based on distribution matching and trajectory matching -- reduce sampling to as few as one step, they fall short on complex tasks like text-to-image generation. Few-step generation...
[17.03.2025 15:11] ********************************************************************************
[17.03.2025 15:11] Abstract 15. As multi-modal large language models (MLLMs) frequently exhibit errors when solving scientific problems, evaluating the validity of their reasoning processes is critical for ensuring reliability and uncovering fine-grained model weaknesses. Since human evaluation is laborious and costly, prompting M...
[17.03.2025 15:11] ********************************************************************************
[17.03.2025 15:11] Abstract 16. State-of-the-art transformer-based large multimodal models (LMMs) struggle to handle hour-long video inputs due to the quadratic complexity of the causal self-attention operations, leading to high computational costs during training and inference. Existing token compression-based methods reduce the ...
[17.03.2025 15:11] ********************************************************************************
[17.03.2025 15:11] Abstract 17. Fitting a body to a 3D clothed human point cloud is a common yet challenging task. Traditional optimization-based approaches use multi-stage pipelines that are sensitive to pose initialization, while recent learning-based methods often struggle with generalization across diverse poses and garment ty...
[17.03.2025 15:11] ********************************************************************************
[17.03.2025 15:11] Abstract 18. Learning skills in open-world environments is essential for developing agents capable of handling a variety of tasks by combining basic skills. Online demonstration videos are typically long but unsegmented, making them difficult to segment and label with skill identifiers. Unlike existing methods t...
[17.03.2025 15:11] ********************************************************************************
[17.03.2025 15:11] Abstract 19. We propose GoalFlow, an end-to-end autonomous driving method for generating high-quality multimodal trajectories. In autonomous driving scenarios, there is rarely a single suitable trajectory. Recent methods have increasingly focused on modeling multimodal trajectory distributions. However, they suf...
[17.03.2025 15:11] ********************************************************************************
[17.03.2025 15:11] Abstract 20. We introduce TreeMeshGPT, an autoregressive Transformer designed to generate high-quality artistic meshes aligned with input point clouds. Instead of the conventional next-token prediction in autoregressive Transformer, we propose a novel Autoregressive Tree Sequencing where the next input token is ...
[17.03.2025 15:11] ********************************************************************************
[17.03.2025 15:11] Abstract 21. We introduce SmolDocling, an ultra-compact vision-language model targeting end-to-end document conversion. Our model comprehensively processes entire pages by generating DocTags, a new universal markup format that captures all page elements in their full context with location. Unlike existing approa...
[17.03.2025 15:11] ********************************************************************************
[17.03.2025 15:11] Abstract 22. Accurate material retrieval is critical for creating realistic 3D assets. Existing methods rely on datasets that capture shape-invariant and lighting-varied representations of materials, which are scarce and face challenges due to limited diversity and inadequate real-world generalization. Most curr...
[17.03.2025 15:11] ********************************************************************************
[17.03.2025 15:11] Abstract 23. This work presents a first evaluation of two state-of-the-art Large Reasoning Models (LRMs), OpenAI's o3-mini and DeepSeek R1, on analogical reasoning, focusing on well-established nonverbal human IQ tests based on Raven's progressive matrices. We benchmark with the I-RAVEN dataset and its more diff...
[17.03.2025 15:11] ********************************************************************************
[17.03.2025 15:11] Abstract 24. Large language models (LLMs) have shown remarkable performance and generalization capabilities across multiple languages and tasks, making them very attractive targets for multi-modality integration (e.g., images or speech). In this work, we extend an existing LLM to the speech modality via speech d...
[17.03.2025 15:11] ********************************************************************************
[17.03.2025 15:11] Abstract 25. Machine unlearning is an emerging paradigm to remove the influence of specific training data (i.e., the forget set) from a model while preserving its knowledge of the rest of the data (i.e., the retain set). Previous approaches assume the forget data to be uniformly distributed from all training dat...
[17.03.2025 15:11] Read previous papers.
[17.03.2025 15:11] Generating reviews via LLM API.
[17.03.2025 15:11] Using data from previous issue: {"categories": ["#dataset", "#optimization", "#training", "#video", "#games"], "emoji": "üé•", "ru": {"title": "–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∫–∞–º–µ—Ä–æ–π –≤ –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "ReCamMaster - —ç—Ç–æ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –∏–∑–º–µ–Ω–µ–Ω–∏—è —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –∫–∞–º–µ—Ä—ã –≤ –≤–∏–¥–µ–æ. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ text-
[17.03.2025 15:11] Using data from previous issue: {"categories": ["#training", "#cv", "#diffusion", "#inference"], "emoji": "üñºÔ∏è", "ru": {"title": "PLADIS: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º PLADIS –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–∏—Ñ—Ñ—É–∑–∏–∏ –≤ –∑–∞–¥–∞—á–µ –≥
[17.03.2025 15:11] Using data from previous issue: {"categories": ["#robotics", "#benchmark", "#dataset", "#optimization", "#open_source", "#agents", "#training", "#data"], "emoji": "ü§ñ", "ru": {"title": "–ú–µ–Ω—å—à–µ –¥–∞–Ω–Ω—ã—Ö, –±–æ–ª—å—à–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ –æ–±—É—á–µ–Ω–∏–∏ —Ä–æ–±–æ—Ç–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–±–æ—Ä—É –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Ä–æ–±–æ—Ç–æ–≤ –º
[17.03.2025 15:11] Using data from previous issue: {"categories": ["#architecture", "#long_context", "#survey", "#math", "#training"], "emoji": "üß†", "ru": {"title": "SSM: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞–º –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π", "desc": "–≠—Ç–æ –æ–±–∑–æ—Ä –º–æ–¥–µ–ª–µ–π –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ —Å–æ—Å—Ç–æ—è–Ω–∏–π (SSM) –∫–∞–∫ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞–º –≤ –º–∞—à–∏–Ω–Ω–æ–º –æ–±—É—á–µ–Ω–∏
[17.03.2025 15:11] Using data from previous issue: {"categories": ["#multimodal", "#survey", "#agents"], "emoji": "ü§ñ", "ru": {"title": "–°—Ä–∞–≤–Ω–µ–Ω–∏–µ API –∏ GUI –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM: –ø—É—Ç—å –∫ –≥–∏–±—Ä–∏–¥–Ω—ã–º —Ä–µ—à–µ–Ω–∏—è–º", "desc": "–≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Å—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM), —Ä–∞–±–æ—Ç–∞—é—â–∏—Ö —á–µ—Ä–µ–∑ API –∏ —á–µ—Ä–µ–∑ –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å. –ê–≤—Ç–æ—Ä
[17.03.2025 15:11] Using data from previous issue: {"categories": ["#leakage", "#benchmark", "#security", "#survey", "#healthcare", "#data"], "emoji": "üõ°Ô∏è", "ru": {"title": "–ó–∞—â–∏—Ç–∞ –ø—Ä–∏–≤–∞—Ç–Ω–æ—Å—Ç–∏ –≤ —Ñ–µ–¥–µ—Ä–∞—Ç–∏–≤–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏: –∞–Ω–∞–ª–∏–∑ –∞—Ç–∞–∫ —Å –∏–Ω–≤–µ—Ä—Å–∏–µ–π –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞", "desc": "–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –∞–Ω–∞–ª–∏–∑—É –∞—Ç–∞–∫ —Å –∏–Ω–≤–µ—Ä—Å–∏–µ–π –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ (GIA) –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ —Ñ–µ–¥–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á
[17.03.2025 15:11] Using data from previous issue: {"categories": ["#cv", "#dataset", "#training", "#video", "#data"], "emoji": "üé•", "ru": {"title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ –≤–∏–¥–µ–æ: –æ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π —Ä–∞–∑–º–µ—Ç–∫–∏ –∫ —Ç–æ—á–Ω–æ–π –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏ –æ–±—ä–µ–∫—Ç–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ–¥–ø–∏—Å–µ–π –∏ –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏ –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –≤–∏–¥–µ–æ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ç–µ
[17.03.2025 15:11] Using data from previous issue: {"categories": ["#alignment", "#healthcare", "#science", "#agents", "#reasoning", "#benchmark", "#multimodal"], "emoji": "üíä", "ru": {"title": "TxAgent: –ò–ò-–ø–æ–º–æ—â–Ω–∏–∫ –¥–ª—è —Ç–æ—á–Ω–æ–π –∏ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Ñ–∞—Ä–º–∞–∫–æ—Ç–µ—Ä–∞–ø–∏–∏", "desc": "TxAgent - —ç—Ç–æ –ò–ò-–∞–≥–µ–Ω—Ç –¥–ª—è –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Ç–µ—Ä–∞–ø–∏–∏, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –º–Ω–æ–≥–æ—ç—Ç–∞
[17.03.2025 15:11] Using data from previous issue: {"categories": ["#architecture", "#training", "#multimodal", "#diffusion"], "emoji": "üåä", "ru": {"title": "FlowTok: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –ø–µ—Ä–µ—Ö–æ–¥ –º–µ–∂–¥—É —Ç–µ–∫—Å—Ç–æ–º –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º —á–µ—Ä–µ–∑ —Ç–æ–∫–µ–Ω—ã", "desc": "FlowTok - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏–∑ —Ç–µ–∫—Å—Ç–∞ –∏ –Ω–∞–æ–±–æ—Ä–æ—Ç. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–µ—Ç–æ–¥ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏—è –ø–æ—Ç–æ–∫
[17.03.2025 15:11] Using data from previous issue: {"categories": ["#architecture", "#cv", "#optimization", "#open_source", "#training"], "emoji": "üß†", "ru": {"title": "–ù–æ–≤—ã–π –≤–∑–≥–ª—è–¥ –Ω–∞ –≤–Ω–∏–º–∞–Ω–∏–µ: –ö–æ–ª–º–æ–≥–æ—Ä–æ–≤-–ê—Ä–Ω–æ–ª—å–¥ —Å–µ—Ç–∏ –≤ Vision Transformers", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π - –ö–æ–ª–º–æ–≥–æ—Ä–æ–≤-–ê—Ä–Ω–æ–ª—å–¥ –≤–Ω–∏–º–∞–Ω–∏–µ (KArAt
[17.03.2025 15:11] Using data from previous issue: {"categories": ["#cv", "#open_source", "#3d", "#optimization"], "emoji": "üîÆ", "ru": {"title": "VGGT: –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –Ω–µ–π—Ä–æ—Å–µ—Ç—å –¥–ª—è –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ 3D-–∞–Ω–∞–ª–∏–∑–∞ —Å—Ü–µ–Ω", "desc": "VGGT - —ç—Ç–æ –Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å –ø—Ä—è–º–æ–≥–æ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä–∞—è –Ω–∞–ø—Ä—è–º—É—é –≤—ã–≤–æ–¥–∏—Ç –≤—Å–µ –∫–ª—é—á–µ–≤—ã–µ 3D-–∞—Ç—Ä–∏–±—É—Ç—ã —Å—Ü–µ–Ω—ã –∏–∑ –æ–¥–Ω–æ–≥–æ –∏–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö
[17.03.2025 15:11] Using data from previous issue: {"categories": ["#benchmark", "#training", "#multimodal", "#synthetic", "#alignment"], "emoji": "ü¶ú", "ru": {"title": "Cockatiel: –ù–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –≤ –¥–µ—Ç–∞–ª—å–Ω–æ–º –æ–ø–∏—Å–∞–Ω–∏–∏ –≤–∏–¥–µ–æ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –¥–µ—Ç–∞–ª—å–Ω–æ–º—É –æ–ø–∏—Å–∞–Ω–∏—é –≤–∏–¥–µ–æ (VDC) –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Cockatiel. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ —Ç—Ä–µ—Ö—ç—Ç–∞
[17.03.2025 15:11] Using data from previous issue: {"categories": ["#cv", "#video", "#games", "#benchmark", "#optimization", "#architecture", "#training"], "emoji": "üß©", "ru": {"title": "NAR: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω–æ–º –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–∏ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω–æ–º—É –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—é –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –∫–æ–Ω
[17.03.2025 15:11] Using data from previous issue: {"categories": ["#games", "#optimization", "#multimodal", "#dataset", "#training", "#architecture"], "emoji": "üõ°Ô∏è", "ru": {"title": "ARMOR: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "ARMOR - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–æ–∑–¥–∞–Ω–∏—é —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.
[17.03.2025 15:11] Using data from previous issue: {"categories": ["#diffusion", "#training", "#cv", "#video"], "emoji": "üöÄ", "ru": {"title": "TDM: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ —É—Å–∫–æ—Ä–µ–Ω–∏–∏ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —É—Å–∫–æ—Ä–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –Ω–∞–∑—ã–≤–∞–µ–º—ã–π Trajectory Distribution Matching (TDM). TD
[17.03.2025 15:11] Using data from previous issue: {"categories": ["#interpretability", "#reasoning", "#multimodal", "#science", "#open_source", "#dataset", "#training", "#benchmark"], "emoji": "üî¨", "ru": {"title": "–ù–∞–¥–µ–∂–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞—É—á–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–º–∏ –ò–ò-—Å—É–¥—å—è–º–∏", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω ProJudgeBench - –ø–µ—Ä–≤—ã–π –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á
[17.03.2025 15:11] Querying the API.
[17.03.2025 15:11] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

State-of-the-art transformer-based large multimodal models (LMMs) struggle to handle hour-long video inputs due to the quadratic complexity of the causal self-attention operations, leading to high computational costs during training and inference. Existing token compression-based methods reduce the number of video tokens but often incur information loss and remain inefficient for extremely long sequences. In this paper, we explore an orthogonal direction to build a hybrid Mamba-Transformer model (VAMBA) that employs Mamba-2 blocks to encode video tokens with linear complexity. Without any token reduction, VAMBA can encode more than 1024 frames (640times360) on a single GPU, while transformer-based models can only encode 256 frames. On long video input, VAMBA achieves at least 50% reduction in GPU memory usage during training and inference, and nearly doubles the speed per training step compared to transformer-based LMMs. Our experimental results demonstrate that VAMBA improves accuracy by 4.3% on the challenging hour-long video understanding benchmark LVBench over prior efficient video LMMs, and maintains strong performance on a broad spectrum of long and short video understanding tasks.
[17.03.2025 15:11] Error getting data: Error code: 529 - {'type': 'error', 'error': {'type': 'overloaded_error', 'message': 'Overloaded'}}
[17.03.2025 15:11] Using data from previous issue: {"categories": ["#3d", "#optimization", "#cv", "#open_source"], "emoji": "üëï", "ru": {"title": "–¢–æ—á–Ω–∞—è –ø–æ–¥–≥–æ–Ω–∫–∞ 3D-–º–æ–¥–µ–ª–∏ —Ç–µ–ª–∞ –∫ –æ–¥–µ—Ç–æ–º—É —á–µ–ª–æ–≤–µ–∫—É —Å —É—á–µ—Ç–æ–º –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏ –ø—Ä–∏–ª–µ–≥–∞–Ω–∏—è –æ–¥–µ–∂–¥—ã", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ ETCH –¥–ª—è –ø–æ–¥–≥–æ–Ω–∫–∏ —Ç—Ä–µ—Ö–º–µ—Ä–Ω–æ–π –º–æ–¥–µ–ª–∏ —Ç–µ–ª–∞ –∫ –æ–±–ª–∞–∫—É —Ç–æ—á–µ–∫ –æ–¥–µ—Ç–æ–≥–æ —á–µ–ª–æ–≤–µ–∫–∞. E
[17.03.2025 15:11] Using data from previous issue: {"categories": ["#games", "#video", "#agents", "#open_source"], "emoji": "üéÆ", "ru": {"title": "–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –≤–∏–¥–µ–æ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤ —Å–ª–æ–∂–Ω—ã–º –Ω–∞–≤—ã–∫–∞–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏—è –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ –Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ —Å–≤—è–∑–∞–Ω–Ω—ã—Ö –Ω
[17.03.2025 15:11] Using data from previous issue: {"categories": ["#optimization", "#diffusion", "#agents", "#multimodal"], "emoji": "üöó", "ru": {"title": "GoalFlow: –¢–æ—á–Ω–æ–µ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π –¥–ª—è –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –≤–æ–∂–¥–µ–Ω–∏—è", "desc": "GoalFlow - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –≤–æ–∂–¥–µ–Ω–∏—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π. –û–Ω —Ä–µ—à–∞–µ—Ç
[17.03.2025 15:11] Querying the API.
[17.03.2025 15:11] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

We introduce TreeMeshGPT, an autoregressive Transformer designed to generate high-quality artistic meshes aligned with input point clouds. Instead of the conventional next-token prediction in autoregressive Transformer, we propose a novel Autoregressive Tree Sequencing where the next input token is retrieved from a dynamically growing tree structure that is built upon the triangle adjacency of faces within the mesh. Our sequencing enables the mesh to extend locally from the last generated triangular face at each step, and therefore reduces training difficulty and improves mesh quality. Our approach represents each triangular face with two tokens, achieving a compression rate of approximately 22% compared to the naive face tokenization. This efficient tokenization enables our model to generate highly detailed artistic meshes with strong point cloud conditioning, surpassing previous methods in both capacity and fidelity. Furthermore, our method generates mesh with strong normal orientation constraints, minimizing flipped normals commonly encountered in previous methods. Our experiments show that TreeMeshGPT enhances the mesh generation quality with refined details and normal orientation consistency.
[17.03.2025 15:11] Error getting data: Error code: 529 - {'type': 'error', 'error': {'type': 'overloaded_error', 'message': 'Overloaded'}}
[17.03.2025 15:11] Querying the API.
[17.03.2025 15:11] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

We introduce SmolDocling, an ultra-compact vision-language model targeting end-to-end document conversion. Our model comprehensively processes entire pages by generating DocTags, a new universal markup format that captures all page elements in their full context with location. Unlike existing approaches that rely on large foundational models, or ensemble solutions that rely on handcrafted pipelines of multiple specialized models, SmolDocling offers an end-to-end conversion for accurately capturing content, structure and spatial location of document elements in a 256M parameters vision-language model. SmolDocling exhibits robust performance in correctly reproducing document features such as code listings, tables, equations, charts, lists, and more across a diverse range of document types including business documents, academic papers, technical reports, patents, and forms -- significantly extending beyond the commonly observed focus on scientific papers. Additionally, we contribute novel publicly sourced datasets for charts, tables, equations, and code recognition. Experimental results demonstrate that SmolDocling competes with other Vision Language Models that are up to 27 times larger in size, while reducing computational requirements substantially. The model is currently available, datasets will be publicly available soon.
[17.03.2025 15:11] Error getting data: Error code: 529 - {'type': 'error', 'error': {'type': 'overloaded_error', 'message': 'Overloaded'}}
[17.03.2025 15:11] Using data from previous issue: {"categories": ["#synthetic", "#optimization", "#dataset", "#cv", "#3d"], "emoji": "üîç", "ru": {"title": "MaRI: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –ø–æ–∏—Å–∫–µ 3D-–º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç MaRI - —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–∏—Å–∫–∞ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö 3D-–º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤. –û–Ω —Å–æ–∑–¥–∞–µ—Ç –æ–±—â–µ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –∏
[17.03.2025 15:11] Querying the API.
[17.03.2025 15:11] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

This work presents a first evaluation of two state-of-the-art Large Reasoning Models (LRMs), OpenAI's o3-mini and DeepSeek R1, on analogical reasoning, focusing on well-established nonverbal human IQ tests based on Raven's progressive matrices. We benchmark with the I-RAVEN dataset and its more difficult extension, I-RAVEN-X, which tests the ability to generalize to longer reasoning rules and ranges of the attribute values. To assess the influence of visual uncertainties on these nonverbal analogical reasoning tests, we extend the I-RAVEN-X dataset, which otherwise assumes an oracle perception. We adopt a two-fold strategy to simulate this imperfect visual perception: 1) we introduce confounding attributes which, being sampled at random, do not contribute to the prediction of the correct answer of the puzzles and 2) smoothen the distributions of the input attributes' values. We observe a sharp decline in OpenAI's o3-mini task accuracy, dropping from 86.6% on the original I-RAVEN to just 17.0% -- approaching random chance -- on the more challenging I-RAVEN-X, which increases input length and range and emulates perceptual uncertainty. This drop occurred despite spending 3.4x more reasoning tokens. A similar trend is also observed for DeepSeek R1: from 80.6% to 23.2%. On the other hand, a neuro-symbolic probabilistic abductive model, ARLC, that achieves state-of-the-art performances on I-RAVEN, can robustly reason under all these out-of-distribution tests, maintaining strong accuracy with only a modest reduction from 98.6% to 88.0%. Our code is available at https://github.com/IBM/raven-large-language-models.
[17.03.2025 15:11] Error getting data: Error code: 529 - {'type': 'error', 'error': {'type': 'overloaded_error', 'message': 'Overloaded'}}
[17.03.2025 15:11] Using data from previous issue: {"categories": ["#multilingual", "#machine_translation", "#open_source", "#multimodal", "#low_resource"], "emoji": "üó£Ô∏è", "ru": {"title": "–†–∞—Å—à–∏—Ä–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ —Ä–µ—á–µ–≤—É—é –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç—å", "desc": "–í —ç—Ç–æ–π —Å—Ç–∞—Ç—å–µ –æ–ø–∏—Å—ã–≤–∞–µ—Ç—Å—è —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –±–æ–ª—å—à–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ (LLM) –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ä–µ—á—å—é –ø
[17.03.2025 15:11] Using data from previous issue: {"categories": ["#training", "#dataset", "#ethics", "#data"], "emoji": "üß†", "ru": {"title": "–ì—Ä—É–ø–ø–æ–≤–æ–µ —Ä–∞–∑–æ–±—É—á–µ–Ω–∏–µ –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏", "desc": "–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –ø—Ä–æ–±–ª–µ–º–µ –≥—Ä—É–ø–ø–æ–≤–æ–≥–æ –º–∞—à–∏–Ω–Ω–æ–≥–æ —Ä–∞–∑–æ–±—É—á–µ–Ω–∏—è, –∫–æ–≥–¥–∞ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è –∏–∑ –º–æ–¥–µ–ª–∏ –Ω–µ—Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω—ã –º–µ–∂–¥—É –≥—Ä—É–ø–ø–∞–º–∏. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª
[17.03.2025 15:11] Loading Chinese text from previous data.
[17.03.2025 15:11] Renaming data file.
[17.03.2025 15:11] Renaming previous data. hf_papers.json to ./d/2025-03-17.json
[17.03.2025 15:11] Saving new data file.
[17.03.2025 15:11] Generating page.
[17.03.2025 15:11] Renaming previous page.
[17.03.2025 15:11] Renaming previous data. index.html to ./d/2025-03-17.html
[17.03.2025 15:11] [Experimental] Generating Chinese page for reading.
[17.03.2025 15:11] Chinese vocab [{'word': 'ËÆ®ËÆ∫', 'pinyin': 't«éo l√πn', 'trans': 'discuss'}, {'word': 'Âêç‰∏∫', 'pinyin': 'm√≠ng w√©i', 'trans': 'named'}, {'word': 'ËßÜÈ¢ë', 'pinyin': 'sh√¨ p√≠n', 'trans': 'video'}, {'word': 'ÈáçÊñ∞', 'pinyin': 'ch√≥ng xƒ´n', 'trans': 'again'}, {'word': 'Ê∏≤Êüì', 'pinyin': 'xu√†n r√°n', 'trans': 'render'}, {'word': 'Ê°ÜÊû∂', 'pinyin': 'ku√†ng ji√†', 'trans': 'framework'}, {'word': 'ÊëÑÂÉèÊú∫', 'pinyin': 'sh√® xi√†ng jƒ´', 'trans': 'camera'}, {'word': 'ËΩ®Ëøπ', 'pinyin': 'gu«ê j√¨', 'trans': 'trajectory'}, {'word': 'ÂêåÊó∂', 'pinyin': 't√≥ng sh√≠', 'trans': 'simultaneously'}, {'word': '‰øùÊåÅ', 'pinyin': 'b«éo ch√≠', 'trans': 'maintain'}, {'word': 'Â§öÂ∏ß', 'pinyin': 'du≈ç zhƒìn', 'trans': 'multi-frame'}, {'word': 'Â§ñËßÇ', 'pinyin': 'w√†i gu«én', 'trans': 'appearance'}, {'word': 'Âä®ÊÄÅ', 'pinyin': 'd√≤ng t√†i', 'trans': 'dynamic'}, {'word': 'ÂêåÊ≠•', 'pinyin': 't√≥ng b√π', 'trans': 'synchronization'}, {'word': 'ÊñπÊ≥ï', 'pinyin': 'fƒÅng f«é', 'trans': 'method'}, {'word': 'Âà©Áî®', 'pinyin': 'l√¨ y√≤ng', 'trans': 'utilize'}, {'word': 'È¢ÑËÆ≠ÁªÉ', 'pinyin': 'y√π x√πn li√†n', 'trans': 'pre-trained'}, {'word': 'ÊñáÊú¨', 'pinyin': 'w√©n bƒõn', 'trans': 'text'}, {'word': 'ÁîüÊàê', 'pinyin': 'shƒìng ch√©ng', 'trans': 'generate'}, {'word': 'ËÉΩÂäõ', 'pinyin': 'n√©ng l√¨', 'trans': 'ability'}, {'word': 'ÊûÑÂª∫', 'pinyin': 'g√≤u ji√†n', 'trans': 'construct'}, {'word': 'Êï∞ÊçÆÈõÜ', 'pinyin': 'sh√π j√π j√≠', 'trans': 'dataset'}, {'word': 'ÂÆûÈ™å', 'pinyin': 'sh√≠ y√†n', 'trans': 'experiment'}, {'word': 'ÁªìÊûú', 'pinyin': 'ji√© gu«í', 'trans': 'result'}, {'word': 'ÊòæÁ§∫', 'pinyin': 'xi«én sh√¨', 'trans': 'display'}, {'word': 'Á®≥ÂÆö', 'pinyin': 'wƒõn d√¨ng', 'trans': 'stable'}, {'word': 'Ë∂ÖÂàÜËæ®Áéá', 'pinyin': 'chƒÅo fƒìn bi√†n l«ú', 'trans': 'super-resolution'}, {'word': 'Êâ©Â±ï', 'pinyin': 'ku√≤ zh«én', 'trans': 'extend'}, {'word': 'Ë°®Áé∞', 'pinyin': 'bi«éo xi√†n', 'trans': 'performance'}, {'word': 'Âá∫Ëâ≤', 'pinyin': 'ch≈´ s√®', 'trans': 'outstanding'}]
[17.03.2025 15:11] Renaming previous Chinese page.
[17.03.2025 15:11] Renaming previous data. zh.html to ./d/2025-03-16_zh_reading_task.html
[17.03.2025 15:11] Writing Chinese reading task.
[17.03.2025 15:11] Writing result.
[17.03.2025 15:11] Renaming log file.
[17.03.2025 15:11] Renaming previous data. log.txt to ./logs/2025-03-17_last_log.txt
