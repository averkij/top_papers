[17.03.2025 09:14] Read previous papers.
[17.03.2025 09:14] Generating top page (month).
[17.03.2025 09:14] Writing top page (month).
[17.03.2025 10:12] Read previous papers.
[17.03.2025 10:12] Get feed.
[17.03.2025 10:12] Get page data from previous paper. URL: https://huggingface.co/papers/2503.07677
[17.03.2025 10:12] Get page data from previous paper. URL: https://huggingface.co/papers/2503.11647
[17.03.2025 10:12] Get page data from previous paper. URL: https://huggingface.co/papers/2503.11646
[17.03.2025 10:12] Get page data from previous paper. URL: https://huggingface.co/papers/2503.11224
[17.03.2025 10:12] Get page data from previous paper. URL: https://huggingface.co/papers/2503.11069
[17.03.2025 10:12] Get page data from previous paper. URL: https://huggingface.co/papers/2503.11514
[17.03.2025 10:12] Get page data from previous paper. URL: https://huggingface.co/papers/2503.10970
[17.03.2025 10:12] Get page data from previous paper. URL: https://huggingface.co/papers/2503.10781
[17.03.2025 10:12] Get page data from previous paper. URL: https://huggingface.co/papers/2503.10772
[17.03.2025 10:12] Get page data from previous paper. URL: https://huggingface.co/papers/2503.10632
[17.03.2025 10:12] Get page data from previous paper. URL: https://huggingface.co/papers/2503.09279
[17.03.2025 10:12] Extract page data from URL. URL: https://huggingface.co/papers/2503.10696
[17.03.2025 10:12] Get page data from previous paper. URL: https://huggingface.co/papers/2503.06674
[17.03.2025 10:12] Get page data from previous paper. URL: https://huggingface.co/papers/2503.06553
[17.03.2025 10:12] Get page data from previous paper. URL: https://huggingface.co/papers/2503.06542
[17.03.2025 10:12] Get page data from previous paper. URL: https://huggingface.co/papers/2503.05689
[17.03.2025 10:12] Extract page data from URL. URL: https://huggingface.co/papers/2503.10624
[17.03.2025 10:12] Extract page data from URL. URL: https://huggingface.co/papers/2503.08111
[17.03.2025 10:12] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[17.03.2025 10:12] No deleted papers detected.
[17.03.2025 10:12] Downloading and parsing papers (pdf, html). Total: 18.
[17.03.2025 10:12] Downloading and parsing paper https://huggingface.co/papers/2503.07677.
[17.03.2025 10:12] Extra JSON file exists (./assets/json/2503.07677.json), skip PDF parsing.
[17.03.2025 10:12] Paper image links file exists (./assets/img_data/2503.07677.json), skip HTML parsing.
[17.03.2025 10:12] Success.
[17.03.2025 10:12] Downloading and parsing paper https://huggingface.co/papers/2503.11647.
[17.03.2025 10:12] Extra JSON file exists (./assets/json/2503.11647.json), skip PDF parsing.
[17.03.2025 10:12] Paper image links file exists (./assets/img_data/2503.11647.json), skip HTML parsing.
[17.03.2025 10:12] Success.
[17.03.2025 10:12] Downloading and parsing paper https://huggingface.co/papers/2503.11646.
[17.03.2025 10:12] Extra JSON file exists (./assets/json/2503.11646.json), skip PDF parsing.
[17.03.2025 10:12] Paper image links file exists (./assets/img_data/2503.11646.json), skip HTML parsing.
[17.03.2025 10:12] Success.
[17.03.2025 10:12] Downloading and parsing paper https://huggingface.co/papers/2503.11224.
[17.03.2025 10:12] Extra JSON file exists (./assets/json/2503.11224.json), skip PDF parsing.
[17.03.2025 10:12] Paper image links file exists (./assets/img_data/2503.11224.json), skip HTML parsing.
[17.03.2025 10:12] Success.
[17.03.2025 10:12] Downloading and parsing paper https://huggingface.co/papers/2503.11069.
[17.03.2025 10:12] Extra JSON file exists (./assets/json/2503.11069.json), skip PDF parsing.
[17.03.2025 10:12] Paper image links file exists (./assets/img_data/2503.11069.json), skip HTML parsing.
[17.03.2025 10:12] Success.
[17.03.2025 10:12] Downloading and parsing paper https://huggingface.co/papers/2503.11514.
[17.03.2025 10:12] Extra JSON file exists (./assets/json/2503.11514.json), skip PDF parsing.
[17.03.2025 10:12] Paper image links file exists (./assets/img_data/2503.11514.json), skip HTML parsing.
[17.03.2025 10:12] Success.
[17.03.2025 10:12] Downloading and parsing paper https://huggingface.co/papers/2503.10970.
[17.03.2025 10:12] Extra JSON file exists (./assets/json/2503.10970.json), skip PDF parsing.
[17.03.2025 10:12] Paper image links file exists (./assets/img_data/2503.10970.json), skip HTML parsing.
[17.03.2025 10:12] Success.
[17.03.2025 10:12] Downloading and parsing paper https://huggingface.co/papers/2503.10781.
[17.03.2025 10:12] Extra JSON file exists (./assets/json/2503.10781.json), skip PDF parsing.
[17.03.2025 10:12] Paper image links file exists (./assets/img_data/2503.10781.json), skip HTML parsing.
[17.03.2025 10:12] Success.
[17.03.2025 10:12] Downloading and parsing paper https://huggingface.co/papers/2503.10772.
[17.03.2025 10:12] Extra JSON file exists (./assets/json/2503.10772.json), skip PDF parsing.
[17.03.2025 10:12] Paper image links file exists (./assets/img_data/2503.10772.json), skip HTML parsing.
[17.03.2025 10:12] Success.
[17.03.2025 10:12] Downloading and parsing paper https://huggingface.co/papers/2503.10632.
[17.03.2025 10:12] Extra JSON file exists (./assets/json/2503.10632.json), skip PDF parsing.
[17.03.2025 10:12] Paper image links file exists (./assets/img_data/2503.10632.json), skip HTML parsing.
[17.03.2025 10:12] Success.
[17.03.2025 10:12] Downloading and parsing paper https://huggingface.co/papers/2503.09279.
[17.03.2025 10:12] Extra JSON file exists (./assets/json/2503.09279.json), skip PDF parsing.
[17.03.2025 10:12] Paper image links file exists (./assets/img_data/2503.09279.json), skip HTML parsing.
[17.03.2025 10:12] Success.
[17.03.2025 10:12] Downloading and parsing paper https://huggingface.co/papers/2503.10696.
[17.03.2025 10:12] Downloading paper 2503.10696 from http://arxiv.org/pdf/2503.10696v1...
[17.03.2025 10:12] Extracting affiliations from text.
[17.03.2025 10:12] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 2 1 ] . [ 1 6 9 6 0 1 . 3 0 5 2 : r a Yefei He1,2 Yuanyu He1 Shaoxuan He1 Feng Chen3 Hong Zhou1 Kaipeng Zhang2 Bohan Zhuang1 1Zhejiang University, China 2Shanghai AI Laboratory, China 3The University of Adelaide, Australia Work done during an internship at Shanghai AI Laboratory Equal contribution Corresponding authors Figure 1. Generated samples from NAR. Results are shown for 512 512 text-guided image generation (1st row), 256 256 classconditional image generation (2nd row) and 128 128 class-conditional video generation (3rd row). "
[17.03.2025 10:12] Response: ```python
["Zhejiang University, China", "Shanghai AI Laboratory, China", "The University of Adelaide, Australia"]
```
[17.03.2025 10:12] Deleting PDF ./assets/pdf/2503.10696.pdf.
[17.03.2025 10:12] Success.
[17.03.2025 10:12] Downloading and parsing paper https://huggingface.co/papers/2503.06674.
[17.03.2025 10:12] Extra JSON file exists (./assets/json/2503.06674.json), skip PDF parsing.
[17.03.2025 10:12] Paper image links file exists (./assets/img_data/2503.06674.json), skip HTML parsing.
[17.03.2025 10:12] Success.
[17.03.2025 10:12] Downloading and parsing paper https://huggingface.co/papers/2503.06553.
[17.03.2025 10:12] Extra JSON file exists (./assets/json/2503.06553.json), skip PDF parsing.
[17.03.2025 10:12] Paper image links file exists (./assets/img_data/2503.06553.json), skip HTML parsing.
[17.03.2025 10:12] Success.
[17.03.2025 10:12] Downloading and parsing paper https://huggingface.co/papers/2503.06542.
[17.03.2025 10:12] Extra JSON file exists (./assets/json/2503.06542.json), skip PDF parsing.
[17.03.2025 10:12] Paper image links file exists (./assets/img_data/2503.06542.json), skip HTML parsing.
[17.03.2025 10:12] Success.
[17.03.2025 10:12] Downloading and parsing paper https://huggingface.co/papers/2503.05689.
[17.03.2025 10:12] Extra JSON file exists (./assets/json/2503.05689.json), skip PDF parsing.
[17.03.2025 10:12] Paper image links file exists (./assets/img_data/2503.05689.json), skip HTML parsing.
[17.03.2025 10:12] Success.
[17.03.2025 10:12] Downloading and parsing paper https://huggingface.co/papers/2503.10624.
[17.03.2025 10:12] Downloading paper 2503.10624 from http://arxiv.org/pdf/2503.10624v1...
[17.03.2025 10:12] Extracting affiliations from text.
[17.03.2025 10:12] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 3 1 ] . [ 1 4 2 6 0 1 . 3 0 5 2 : r ETCH: Generalizing Body Fitting to Clothed Humans via Equivariant Tightness Boqian Li1 Haiwen Feng2,3 Zeyu Cai1 Michael J. Black2 Yuliang Xiu1,2 1Westlake University 2Max Planck Institute for Intelligent Systems 3Berkeley AI Research (BAIR) {liboqian, caizeyu, xiuyuliang}@westlake.edu.cn {haiwen.feng, black}@tuebingen.mpg.de boqian-li.github.io/ETCH/ Figure 1. Body Fitting on Clothed Humans. Given 3D clothed humans in any pose and clothing, ETCH accurately fits the body underneath. Our key novelty is modeling cloth-to-body SE(3)-equivariant tightness vectors for clothed humans, abbreviated as ETCH, which resembles etching from the outer clothing down to the inner body. The ground-truth body is shown in blue, our fitted body in green, and ground-truth markers as . "
[17.03.2025 10:12] Response: ```python
["Westlake University", "Max Planck Institute for Intelligent Systems", "Berkeley AI Research (BAIR)"]
```
[17.03.2025 10:12] Deleting PDF ./assets/pdf/2503.10624.pdf.
[17.03.2025 10:12] Success.
[17.03.2025 10:12] Downloading and parsing paper https://huggingface.co/papers/2503.08111.
[17.03.2025 10:12] Downloading paper 2503.08111 from http://arxiv.org/pdf/2503.08111v1...
[17.03.2025 10:12] Extracting affiliations from text.
[17.03.2025 10:12] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 1 1 ] . [ 1 1 1 1 8 0 . 3 0 5 2 : r MaRI: Material Retrieval Integration across Domains Jianhui Wang1* Zhifei Yang2* Yangfan He3* Huixiong Zhang1 Yuxuan Chen4 Jingwei Huang5 1University of Electronic Science and Technology of China 2Peking University 3University of Minnesota 4Fudan University 5Tencent Hunyuan3D https://jianhuiwemi.github.io/MaRI Figure 1. Examples from the MaRI Gallery, showcasing (a) synthetic and (b) real-world datasets we constructed. (c) MaRI: groundbreaking framework for accurately retrieving textures from images, bridging the gap between visual representations and material properties. "
[17.03.2025 10:12] Response: ```python
[
    "University of Electronic Science and Technology of China",
    "Peking University",
    "University of Minnesota",
    "Fudan University",
    "Tencent Hunyuan3D"
]
```
[17.03.2025 10:12] Deleting PDF ./assets/pdf/2503.08111.pdf.
[17.03.2025 10:12] Success.
[17.03.2025 10:12] Enriching papers with extra data.
[17.03.2025 10:12] ********************************************************************************
[17.03.2025 10:12] Abstract 0. Diffusion models have shown impressive results in generating high-quality conditional samples using guidance techniques such as Classifier-Free Guidance (CFG). However, existing methods often require additional training or neural function evaluations (NFEs), making them incompatible with guidance-di...
[17.03.2025 10:12] ********************************************************************************
[17.03.2025 10:12] Abstract 1. Camera control has been actively studied in text or image conditioned video generation tasks. However, altering camera trajectories of a given video remains under-explored, despite its importance in the field of video creation. It is non-trivial due to the extra constraints of maintaining multiple-f...
[17.03.2025 10:12] ********************************************************************************
[17.03.2025 10:12] Abstract 2. The pursuit of data efficiency, where quality outweighs quantity, has emerged as a cornerstone in robotic manipulation, especially given the high costs associated with real-world data collection. We propose that maximizing the informational density of individual demonstrations can dramatically reduc...
[17.03.2025 10:12] ********************************************************************************
[17.03.2025 10:12] Abstract 3. State Space Models (SSMs) have emerged as a promising alternative to the popular transformer-based models and have been increasingly gaining attention. Compared to transformers, SSMs excel at tasks with sequential data or longer contexts, demonstrating comparable performances with significant effici...
[17.03.2025 10:12] ********************************************************************************
[17.03.2025 10:12] Abstract 4. Large language models (LLMs) have evolved beyond simple text generation to power software agents that directly translate natural language commands into tangible actions. While API-based LLM agents initially rose to prominence for their robust automation capabilities and seamless integration with pro...
[17.03.2025 10:12] ********************************************************************************
[17.03.2025 10:12] Abstract 5. Federated Learning (FL) has emerged as a promising privacy-preserving collaborative model training paradigm without sharing raw data. However, recent studies have revealed that private information can still be leaked through shared gradient information and attacked by Gradient Inversion Attacks (GIA...
[17.03.2025 10:12] ********************************************************************************
[17.03.2025 10:12] Abstract 6. Precision therapeutics require multimodal adaptive models that generate personalized treatment recommendations. We introduce TxAgent, an AI agent that leverages multi-step reasoning and real-time biomedical knowledge retrieval across a toolbox of 211 tools to analyze drug interactions, contraindicat...
[17.03.2025 10:12] ********************************************************************************
[17.03.2025 10:12] Abstract 7. We propose a novel approach for captioning and object grounding in video, where the objects in the caption are grounded in the video via temporally dense bounding boxes. We introduce the following contributions. First, we present a large-scale automatic annotation method that aggregates captions gro...
[17.03.2025 10:12] ********************************************************************************
[17.03.2025 10:12] Abstract 8. Bridging different modalities lies at the heart of cross-modality generation. While conventional approaches treat the text modality as a conditioning signal that gradually guides the denoising process from Gaussian noise to the target image modality, we explore a much simpler paradigm-directly evolv...
[17.03.2025 10:12] ********************************************************************************
[17.03.2025 10:12] Abstract 9. Kolmogorov-Arnold networks (KANs) are a remarkable innovation consisting of learnable activation functions with the potential to capture more complex relationships from data. Although KANs are useful in finding symbolic representations and continual learning of one-dimensional functions, their effec...
[17.03.2025 10:12] ********************************************************************************
[17.03.2025 10:12] Abstract 10. Video Detailed Captioning (VDC) is a crucial task for vision-language bridging, enabling fine-grained descriptions of complex video content. In this paper, we first comprehensively benchmark current state-of-the-art approaches and systematically identified two critical limitations: biased capability...
[17.03.2025 10:12] ********************************************************************************
[17.03.2025 10:12] Abstract 11. Visual autoregressive models typically adhere to a raster-order ``next-token prediction" paradigm, which overlooks the spatial and temporal locality inherent in visual content. Specifically, visual tokens exhibit significantly stronger correlations with their spatially or temporally adjacent tokens ...
[17.03.2025 10:12] ********************************************************************************
[17.03.2025 10:12] Abstract 12. Accelerating diffusion model sampling is crucial for efficient AIGC deployment. While diffusion distillation methods -- based on distribution matching and trajectory matching -- reduce sampling to as few as one step, they fall short on complex tasks like text-to-image generation. Few-step generation...
[17.03.2025 10:12] ********************************************************************************
[17.03.2025 10:12] Abstract 13. As multi-modal large language models (MLLMs) frequently exhibit errors when solving scientific problems, evaluating the validity of their reasoning processes is critical for ensuring reliability and uncovering fine-grained model weaknesses. Since human evaluation is laborious and costly, prompting M...
[17.03.2025 10:12] ********************************************************************************
[17.03.2025 10:12] Abstract 14. Unified models (UniMs) for multimodal understanding and generation have recently received much attention in the area of vision and language. Existing UniMs are designed to simultaneously learn both multimodal understanding and generation capabilities, demanding substantial computational resources, a...
[17.03.2025 10:12] ********************************************************************************
[17.03.2025 10:12] Abstract 15. We propose GoalFlow, an end-to-end autonomous driving method for generating high-quality multimodal trajectories. In autonomous driving scenarios, there is rarely a single suitable trajectory. Recent methods have increasingly focused on modeling multimodal trajectory distributions. However, they suf...
[17.03.2025 10:12] ********************************************************************************
[17.03.2025 10:12] Abstract 16. Fitting a body to a 3D clothed human point cloud is a common yet challenging task. Traditional optimization-based approaches use multi-stage pipelines that are sensitive to pose initialization, while recent learning-based methods often struggle with generalization across diverse poses and garment ty...
[17.03.2025 10:12] ********************************************************************************
[17.03.2025 10:12] Abstract 17. Accurate material retrieval is critical for creating realistic 3D assets. Existing methods rely on datasets that capture shape-invariant and lighting-varied representations of materials, which are scarce and face challenges due to limited diversity and inadequate real-world generalization. Most curr...
[17.03.2025 10:12] Read previous papers.
[17.03.2025 10:12] Generating reviews via LLM API.
[17.03.2025 10:12] Using data from previous issue: {"categories": ["#training", "#cv", "#diffusion", "#inference"], "emoji": "🖼️", "ru": {"title": "PLADIS: Эффективное улучшение генерации изображений с помощью разреженного внимания", "desc": "Статья представляет новый метод под названием PLADIS для улучшения предобученных моделей диффузии в задаче г
[17.03.2025 10:12] Using data from previous issue: {"categories": ["#dataset", "#optimization", "#training", "#video", "#games"], "emoji": "🎥", "ru": {"title": "Управление камерой в видео с помощью генеративных моделей", "desc": "ReCamMaster - это генеративная система для изменения траектории камеры в видео. Она использует предобученные модели text-
[17.03.2025 10:12] Using data from previous issue: {"categories": ["#robotics", "#benchmark", "#dataset", "#optimization", "#open_source", "#agents", "#training", "#data"], "emoji": "🤖", "ru": {"title": "Меньше данных, больше эффективности: революция в обучении роботов", "desc": "Статья представляет новый подход к сбору данных для обучения роботов м
[17.03.2025 10:12] Using data from previous issue: {"categories": ["#architecture", "#long_context", "#survey", "#math", "#training"], "emoji": "🧠", "ru": {"title": "SSM: Эффективная альтернатива трансформерам для обработки последовательностей", "desc": "Это обзор моделей пространства состояний (SSM) как альтернативы трансформерам в машинном обучени
[17.03.2025 10:12] Using data from previous issue: {"categories": ["#multimodal", "#survey", "#agents"], "emoji": "🤖", "ru": {"title": "Сравнение API и GUI агентов на основе LLM: путь к гибридным решениям", "desc": "Это исследование сравнивает агентов на основе больших языковых моделей (LLM), работающих через API и через графический интерфейс. Автор
[17.03.2025 10:12] Using data from previous issue: {"categories": ["#leakage", "#benchmark", "#security", "#survey", "#healthcare", "#data"], "emoji": "🛡️", "ru": {"title": "Защита приватности в федеративном обучении: анализ атак с инверсией градиента", "desc": "Статья посвящена анализу атак с инверсией градиента (GIA) в контексте федеративного обуч
[17.03.2025 10:12] Using data from previous issue: {"categories": ["#alignment", "#healthcare", "#science", "#agents", "#reasoning", "#benchmark", "#multimodal"], "emoji": "💊", "ru": {"title": "TxAgent: ИИ-помощник для точной и персонализированной фармакотерапии", "desc": "TxAgent - это ИИ-агент для персонализированной терапии, использующий многоэта
[17.03.2025 10:12] Using data from previous issue: {"categories": ["#cv", "#dataset", "#training", "#video", "#data"], "emoji": "🎥", "ru": {"title": "Революция в понимании видео: от автоматической разметки к точной локализации объектов", "desc": "Статья представляет новый подход к генерации подписей и локализации объектов в видео с использованием те
[17.03.2025 10:12] Using data from previous issue: {"categories": ["#architecture", "#training", "#multimodal", "#diffusion"], "emoji": "🌊", "ru": {"title": "FlowTok: эффективный переход между текстом и изображением через токены", "desc": "FlowTok - это новый подход к генерации изображений из текста и наоборот. Он использует метод согласования поток
[17.03.2025 10:12] Using data from previous issue: {"categories": ["#architecture", "#cv", "#optimization", "#open_source", "#training"], "emoji": "🧠", "ru": {"title": "Новый взгляд на внимание: Колмогоров-Арнольд сети в Vision Transformers", "desc": "Статья представляет новый подход к архитектуре нейронных сетей - Колмогоров-Арнольд внимание (KArAt
[17.03.2025 10:12] Using data from previous issue: {"categories": ["#benchmark", "#training", "#multimodal", "#synthetic", "#alignment"], "emoji": "🦜", "ru": {"title": "Cockatiel: Новый стандарт в детальном описании видео", "desc": "Статья представляет новый подход к детальному описанию видео (VDC) под названием Cockatiel. Авторы разработали трехэта
[17.03.2025 10:12] Querying the API.
[17.03.2025 10:12] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Visual autoregressive models typically adhere to a raster-order ``next-token prediction" paradigm, which overlooks the spatial and temporal locality inherent in visual content. Specifically, visual tokens exhibit significantly stronger correlations with their spatially or temporally adjacent tokens compared to those that are distant. In this paper, we propose Neighboring Autoregressive Modeling (NAR), a novel paradigm that formulates autoregressive visual generation as a progressive outpainting procedure, following a near-to-far ``next-neighbor prediction" mechanism. Starting from an initial token, the remaining tokens are decoded in ascending order of their Manhattan distance from the initial token in the spatial-temporal space, progressively expanding the boundary of the decoded region. To enable parallel prediction of multiple adjacent tokens in the spatial-temporal space, we introduce a set of dimension-oriented decoding heads, each predicting the next token along a mutually orthogonal dimension. During inference, all tokens adjacent to the decoded tokens are processed in parallel, substantially reducing the model forward steps for generation. Experiments on ImageNet256times 256 and UCF101 demonstrate that NAR achieves 2.4times and 8.6times higher throughput respectively, while obtaining superior FID/FVD scores for both image and video generation tasks compared to the PAR-4X approach. When evaluating on text-to-image generation benchmark GenEval, NAR with 0.8B parameters outperforms Chameleon-7B while using merely 0.4 of the training data. Code is available at https://github.com/ThisisBillhe/NAR.
[17.03.2025 10:12] Response: {
  "desc": "Статья представляет новый подход к авторегрессионному моделированию визуального контента, названный Neighboring Autoregressive Modeling (NAR). В отличие от традиционных моделей, использующих растровый порядок предсказания, NAR применяет механизм 'предсказания следующего соседа', учитывающий пространственно-временную близость токенов. Модель начинает с одного токена и последовательно декодирует остальные, расширяя границы декодированной области. Для параллельного предсказания соседних токенов используются специальные декодирующие головки, что значительно ускоряет процесс генерации.",
  "emoji": "🧩",
  "title": "NAR: Революция в авторегрессионном моделировании визуального контента"
}
[17.03.2025 10:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Visual autoregressive models typically adhere to a raster-order ``next-token prediction" paradigm, which overlooks the spatial and temporal locality inherent in visual content. Specifically, visual tokens exhibit significantly stronger correlations with their spatially or temporally adjacent tokens compared to those that are distant. In this paper, we propose Neighboring Autoregressive Modeling (NAR), a novel paradigm that formulates autoregressive visual generation as a progressive outpainting procedure, following a near-to-far ``next-neighbor prediction" mechanism. Starting from an initial token, the remaining tokens are decoded in ascending order of their Manhattan distance from the initial token in the spatial-temporal space, progressively expanding the boundary of the decoded region. To enable parallel prediction of multiple adjacent tokens in the spatial-temporal space, we introduce a set of dimension-oriented decoding heads, each predicting the next token along a mutually orthogonal dimension. During inference, all tokens adjacent to the decoded tokens are processed in parallel, substantially reducing the model forward steps for generation. Experiments on ImageNet256times 256 and UCF101 demonstrate that NAR achieves 2.4times and 8.6times higher throughput respectively, while obtaining superior FID/FVD scores for both image and video generation tasks compared to the PAR-4X approach. When evaluating on text-to-image generation benchmark GenEval, NAR with 0.8B parameters outperforms Chameleon-7B while using merely 0.4 of the training data. Code is available at https://github.com/ThisisBillhe/NAR."

[17.03.2025 10:12] Response: ```python
['CV', 'VIDEO', 'BENCHMARK', 'TRAINING', 'ARCHITECTURE']
```
[17.03.2025 10:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Visual autoregressive models typically adhere to a raster-order ``next-token prediction" paradigm, which overlooks the spatial and temporal locality inherent in visual content. Specifically, visual tokens exhibit significantly stronger correlations with their spatially or temporally adjacent tokens compared to those that are distant. In this paper, we propose Neighboring Autoregressive Modeling (NAR), a novel paradigm that formulates autoregressive visual generation as a progressive outpainting procedure, following a near-to-far ``next-neighbor prediction" mechanism. Starting from an initial token, the remaining tokens are decoded in ascending order of their Manhattan distance from the initial token in the spatial-temporal space, progressively expanding the boundary of the decoded region. To enable parallel prediction of multiple adjacent tokens in the spatial-temporal space, we introduce a set of dimension-oriented decoding heads, each predicting the next token along a mutually orthogonal dimension. During inference, all tokens adjacent to the decoded tokens are processed in parallel, substantially reducing the model forward steps for generation. Experiments on ImageNet256times 256 and UCF101 demonstrate that NAR achieves 2.4times and 8.6times higher throughput respectively, while obtaining superior FID/FVD scores for both image and video generation tasks compared to the PAR-4X approach. When evaluating on text-to-image generation benchmark GenEval, NAR with 0.8B parameters outperforms Chameleon-7B while using merely 0.4 of the training data. Code is available at https://github.com/ThisisBillhe/NAR."

[17.03.2025 10:12] Response: ```python
["OPTIMIZATION", "GAMES"]
```
[17.03.2025 10:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces Neighboring Autoregressive Modeling (NAR), a new approach to visual generation that improves upon traditional autoregressive models by focusing on the spatial and temporal relationships between visual tokens. Instead of predicting the next token in a raster order, NAR uses a \'next-neighbor prediction\' method, decoding tokens based on their proximity to an initial token. This allows for parallel processing of adjacent tokens, significantly speeding up the generation process. Experiments show that NAR achieves much higher throughput and better quality scores in image and video generation compared to existing methods, while also being more efficient in terms of training data usage.","title":"Revolutionizing Visual Generation with Neighboring Autoregressive Modeling"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="This paper introduces Neighboring Autoregressive Modeling (NAR), a new approach to visual generation that improves upon traditional autoregressive models by focusing on the spatial and temporal relationships between visual tokens. Instead of predicting the next token in a raster order, NAR uses a 'next-neighbor prediction' method, decoding tokens based on their proximity to an initial token. This allows for parallel processing of adjacent tokens, significantly speeding up the generation process. Experiments show that NAR achieves much higher throughput and better quality scores in image and video generation compared to existing methods, while also being more efficient in terms of training data usage.", title='Revolutionizing Visual Generation with Neighboring Autoregressive Modeling'))
[17.03.2025 10:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本文提出了一种新的视觉自回归模型，称为邻近自回归建模（NAR），旨在改善传统的基于光栅顺序的“下一个标记预测”方法。NAR通过近邻预测机制，将自回归视觉生成视为一种逐步扩展的过程，从初始标记开始，按曼哈顿距离逐步解码剩余标记。该方法引入了一组面向维度的解码头，允许在空间-时间空间中并行预测多个相邻标记，从而显著减少生成所需的模型前向步骤。实验结果表明，NAR在图像和视频生成任务中均优于现有方法，显示出更高的吞吐量和更好的生成质量。","title":"邻近自回归建模：提升视觉生成效率的创新方法"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本文提出了一种新的视觉自回归模型，称为邻近自回归建模（NAR），旨在改善传统的基于光栅顺序的“下一个标记预测”方法。NAR通过近邻预测机制，将自回归视觉生成视为一种逐步扩展的过程，从初始标记开始，按曼哈顿距离逐步解码剩余标记。该方法引入了一组面向维度的解码头，允许在空间-时间空间中并行预测多个相邻标记，从而显著减少生成所需的模型前向步骤。实验结果表明，NAR在图像和视频生成任务中均优于现有方法，显示出更高的吞吐量和更好的生成质量。', title='邻近自回归建模：提升视觉生成效率的创新方法'))
[17.03.2025 10:12] Using data from previous issue: {"categories": ["#diffusion", "#training", "#cv", "#video"], "emoji": "🚀", "ru": {"title": "TDM: Революция в ускорении диффузионных моделей", "desc": "Статья представляет новый метод ускорения генерации изображений с помощью диффузионных моделей, называемый Trajectory Distribution Matching (TDM). TD
[17.03.2025 10:12] Using data from previous issue: {"categories": ["#interpretability", "#reasoning", "#multimodal", "#science", "#open_source", "#dataset", "#training", "#benchmark"], "emoji": "🔬", "ru": {"title": "Надежная оценка научных рассуждений мультимодальными ИИ-судьями", "desc": "В статье представлен ProJudgeBench - первый комплексный бенч
[17.03.2025 10:12] Using data from previous issue: {"categories": ["#games", "#optimization", "#multimodal", "#dataset", "#training", "#architecture"], "emoji": "🛡️", "ru": {"title": "ARMOR: Эффективное улучшение мультимодальных моделей", "desc": "ARMOR - это новый подход к созданию унифицированных моделей для мультимодального понимания и генерации.
[17.03.2025 10:12] Using data from previous issue: {"categories": ["#optimization", "#diffusion", "#agents", "#multimodal"], "emoji": "🚗", "ru": {"title": "GoalFlow: Точное планирование траекторий для автономного вождения", "desc": "GoalFlow - это новый метод автономного вождения для генерации высококачественных мультимодальных траекторий. Он решает
[17.03.2025 10:12] Querying the API.
[17.03.2025 10:12] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Fitting a body to a 3D clothed human point cloud is a common yet challenging task. Traditional optimization-based approaches use multi-stage pipelines that are sensitive to pose initialization, while recent learning-based methods often struggle with generalization across diverse poses and garment types. We propose Equivariant Tightness Fitting for Clothed Humans, or ETCH, a novel pipeline that estimates cloth-to-body surface mapping through locally approximate SE(3) equivariance, encoding tightness as displacement vectors from the cloth surface to the underlying body. Following this mapping, pose-invariant body features regress sparse body markers, simplifying clothed human fitting into an inner-body marker fitting task. Extensive experiments on CAPE and 4D-Dress show that ETCH significantly outperforms state-of-the-art methods -- both tightness-agnostic and tightness-aware -- in body fitting accuracy on loose clothing (16.7% ~ 69.5%) and shape accuracy (average 49.9%). Our equivariant tightness design can even reduce directional errors by (67.2% ~ 89.8%) in one-shot (or out-of-distribution) settings. Qualitative results demonstrate strong generalization of ETCH, regardless of challenging poses, unseen shapes, loose clothing, and non-rigid dynamics. We will release the code and models soon for research purposes at https://boqian-li.github.io/ETCH/.
[17.03.2025 10:12] Response: {
  "desc": "Статья представляет новый метод ETCH для подгонки трехмерной модели тела к облаку точек одетого человека. ETCH использует локально приближенную SE(3)-эквивариантность для оценки соответствия между поверхностью одежды и телом, кодируя плотность прилегания как векторы смещения. Метод регрессирует разреженные маркеры тела с помощью инвариантных к позе признаков, упрощая задачу подгонки тела. Эксперименты показывают, что ETCH значительно превосходит современные методы по точности подгонки тела и формы, особенно для свободной одежды.",
  "emoji": "👕",
  "title": "Точная подгонка 3D-модели тела к одетому человеку с учетом плотности прилегания одежды"
}
[17.03.2025 10:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Fitting a body to a 3D clothed human point cloud is a common yet challenging task. Traditional optimization-based approaches use multi-stage pipelines that are sensitive to pose initialization, while recent learning-based methods often struggle with generalization across diverse poses and garment types. We propose Equivariant Tightness Fitting for Clothed Humans, or ETCH, a novel pipeline that estimates cloth-to-body surface mapping through locally approximate SE(3) equivariance, encoding tightness as displacement vectors from the cloth surface to the underlying body. Following this mapping, pose-invariant body features regress sparse body markers, simplifying clothed human fitting into an inner-body marker fitting task. Extensive experiments on CAPE and 4D-Dress show that ETCH significantly outperforms state-of-the-art methods -- both tightness-agnostic and tightness-aware -- in body fitting accuracy on loose clothing (16.7% ~ 69.5%) and shape accuracy (average 49.9%). Our equivariant tightness design can even reduce directional errors by (67.2% ~ 89.8%) in one-shot (or out-of-distribution) settings. Qualitative results demonstrate strong generalization of ETCH, regardless of challenging poses, unseen shapes, loose clothing, and non-rigid dynamics. We will release the code and models soon for research purposes at https://boqian-li.github.io/ETCH/."

[17.03.2025 10:12] Response: ```python
["3D", "CV"]
```
[17.03.2025 10:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Fitting a body to a 3D clothed human point cloud is a common yet challenging task. Traditional optimization-based approaches use multi-stage pipelines that are sensitive to pose initialization, while recent learning-based methods often struggle with generalization across diverse poses and garment types. We propose Equivariant Tightness Fitting for Clothed Humans, or ETCH, a novel pipeline that estimates cloth-to-body surface mapping through locally approximate SE(3) equivariance, encoding tightness as displacement vectors from the cloth surface to the underlying body. Following this mapping, pose-invariant body features regress sparse body markers, simplifying clothed human fitting into an inner-body marker fitting task. Extensive experiments on CAPE and 4D-Dress show that ETCH significantly outperforms state-of-the-art methods -- both tightness-agnostic and tightness-aware -- in body fitting accuracy on loose clothing (16.7% ~ 69.5%) and shape accuracy (average 49.9%). Our equivariant tightness design can even reduce directional errors by (67.2% ~ 89.8%) in one-shot (or out-of-distribution) settings. Qualitative results demonstrate strong generalization of ETCH, regardless of challenging poses, unseen shapes, loose clothing, and non-rigid dynamics. We will release the code and models soon for research purposes at https://boqian-li.github.io/ETCH/."

[17.03.2025 10:12] Response: ```python
["OPTIMIZATION", "OPEN_SOURCE"]
```
[17.03.2025 10:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The paper introduces Equivariant Tightness Fitting for Clothed Humans (ETCH), a new method for fitting a body model to a 3D point cloud of a clothed human. ETCH improves upon traditional optimization methods by using a pipeline that leverages SE(3) equivariance to accurately map cloth surfaces to body shapes. This approach simplifies the fitting process by focusing on pose-invariant body features and regressing sparse body markers. Experimental results show that ETCH significantly enhances fitting accuracy for various clothing types and poses, outperforming existing methods in both tightness and shape accuracy.","title":"Revolutionizing Body Fitting with Equivariant Tightness!"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='The paper introduces Equivariant Tightness Fitting for Clothed Humans (ETCH), a new method for fitting a body model to a 3D point cloud of a clothed human. ETCH improves upon traditional optimization methods by using a pipeline that leverages SE(3) equivariance to accurately map cloth surfaces to body shapes. This approach simplifies the fitting process by focusing on pose-invariant body features and regressing sparse body markers. Experimental results show that ETCH significantly enhances fitting accuracy for various clothing types and poses, outperforming existing methods in both tightness and shape accuracy.', title='Revolutionizing Body Fitting with Equivariant Tightness!'))
[17.03.2025 10:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本论文提出了一种新的方法，称为等变紧致拟合（ETCH），用于将身体与3D穿衣人类点云相匹配。传统的方法依赖于多阶段优化，容易受到姿势初始化的影响，而学习型方法在不同姿势和服装类型的泛化能力上存在困难。ETCH通过局部近似的SE(3)等变性来估计布料与身体表面的映射，并将紧致度编码为从布料表面到身体的位移向量。实验结果表明，ETCH在松散衣物的身体拟合精度和形状精度上显著优于现有的最先进方法，展示了其强大的泛化能力。","title":"等变紧致拟合：提升3D穿衣人类拟合精度的创新方法"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本论文提出了一种新的方法，称为等变紧致拟合（ETCH），用于将身体与3D穿衣人类点云相匹配。传统的方法依赖于多阶段优化，容易受到姿势初始化的影响，而学习型方法在不同姿势和服装类型的泛化能力上存在困难。ETCH通过局部近似的SE(3)等变性来估计布料与身体表面的映射，并将紧致度编码为从布料表面到身体的位移向量。实验结果表明，ETCH在松散衣物的身体拟合精度和形状精度上显著优于现有的最先进方法，展示了其强大的泛化能力。', title='等变紧致拟合：提升3D穿衣人类拟合精度的创新方法'))
[17.03.2025 10:12] Querying the API.
[17.03.2025 10:12] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Accurate material retrieval is critical for creating realistic 3D assets. Existing methods rely on datasets that capture shape-invariant and lighting-varied representations of materials, which are scarce and face challenges due to limited diversity and inadequate real-world generalization. Most current approaches adopt traditional image search techniques. They fall short in capturing the unique properties of material spaces, leading to suboptimal performance in retrieval tasks. Addressing these challenges, we introduce MaRI, a framework designed to bridge the feature space gap between synthetic and real-world materials. MaRI constructs a shared embedding space that harmonizes visual and material attributes through a contrastive learning strategy by jointly training an image and a material encoder, bringing similar materials and images closer while separating dissimilar pairs within the feature space. To support this, we construct a comprehensive dataset comprising high-quality synthetic materials rendered with controlled shape variations and diverse lighting conditions, along with real-world materials processed and standardized using material transfer techniques. Extensive experiments demonstrate the superior performance, accuracy, and generalization capabilities of MaRI across diverse and complex material retrieval tasks, outperforming existing methods.
[17.03.2025 10:12] Response: {
  "desc": "Статья представляет MaRI - фреймворк для улучшения поиска реалистичных 3D-материалов. Он создает общее пространство признаков для синтетических и реальных материалов с помощью контрастивного обучения. Авторы также собрали набор данных из высококачественных синтетических материалов и обработанных реальных материалов. Эксперименты показывают, что MaRI превосходит существующие методы в задачах поиска материалов.",
  "emoji": "🔍",
  "title": "MaRI: Революция в поиске 3D-материалов"
}
[17.03.2025 10:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Accurate material retrieval is critical for creating realistic 3D assets. Existing methods rely on datasets that capture shape-invariant and lighting-varied representations of materials, which are scarce and face challenges due to limited diversity and inadequate real-world generalization. Most current approaches adopt traditional image search techniques. They fall short in capturing the unique properties of material spaces, leading to suboptimal performance in retrieval tasks. Addressing these challenges, we introduce MaRI, a framework designed to bridge the feature space gap between synthetic and real-world materials. MaRI constructs a shared embedding space that harmonizes visual and material attributes through a contrastive learning strategy by jointly training an image and a material encoder, bringing similar materials and images closer while separating dissimilar pairs within the feature space. To support this, we construct a comprehensive dataset comprising high-quality synthetic materials rendered with controlled shape variations and diverse lighting conditions, along with real-world materials processed and standardized using material transfer techniques. Extensive experiments demonstrate the superior performance, accuracy, and generalization capabilities of MaRI across diverse and complex material retrieval tasks, outperforming existing methods."

[17.03.2025 10:12] Response: ```python
['DATASET', '3D', 'CV']
```
[17.03.2025 10:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Accurate material retrieval is critical for creating realistic 3D assets. Existing methods rely on datasets that capture shape-invariant and lighting-varied representations of materials, which are scarce and face challenges due to limited diversity and inadequate real-world generalization. Most current approaches adopt traditional image search techniques. They fall short in capturing the unique properties of material spaces, leading to suboptimal performance in retrieval tasks. Addressing these challenges, we introduce MaRI, a framework designed to bridge the feature space gap between synthetic and real-world materials. MaRI constructs a shared embedding space that harmonizes visual and material attributes through a contrastive learning strategy by jointly training an image and a material encoder, bringing similar materials and images closer while separating dissimilar pairs within the feature space. To support this, we construct a comprehensive dataset comprising high-quality synthetic materials rendered with controlled shape variations and diverse lighting conditions, along with real-world materials processed and standardized using material transfer techniques. Extensive experiments demonstrate the superior performance, accuracy, and generalization capabilities of MaRI across diverse and complex material retrieval tasks, outperforming existing methods."

[17.03.2025 10:12] Response: ```python
["SYNTHETIC", "OPTIMIZATION"]
```
[17.03.2025 10:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents MaRI, a novel framework aimed at improving material retrieval for realistic 3D asset creation. It addresses the limitations of existing methods that rely on traditional image search techniques, which often fail to capture the unique properties of materials. By utilizing a contrastive learning strategy, MaRI creates a shared embedding space that aligns visual and material attributes, enhancing the retrieval process. The framework is supported by a comprehensive dataset of synthetic and real-world materials, demonstrating superior performance in accuracy and generalization across various retrieval tasks.","title":"Bridging the Gap in Material Retrieval with MaRI"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper presents MaRI, a novel framework aimed at improving material retrieval for realistic 3D asset creation. It addresses the limitations of existing methods that rely on traditional image search techniques, which often fail to capture the unique properties of materials. By utilizing a contrastive learning strategy, MaRI creates a shared embedding space that aligns visual and material attributes, enhancing the retrieval process. The framework is supported by a comprehensive dataset of synthetic and real-world materials, demonstrating superior performance in accuracy and generalization across various retrieval tasks.', title='Bridging the Gap in Material Retrieval with MaRI'))
[17.03.2025 10:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"准确的材料检索对于创建真实的3D资产至关重要。现有方法依赖于捕捉形状不变和光照变化的材料表示的数据集，这些数据集稀缺且面临多样性不足和现实世界泛化不良的挑战。我们提出了MaRI框架，旨在弥合合成材料和真实材料之间的特征空间差距。通过对比学习策略，MaRI构建了一个共享的嵌入空间，使得相似的材料和图像在特征空间中更接近，同时将不相似的对分开，从而提高了材料检索的性能和准确性。","title":"MaRI：提升材料检索的智能框架"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='准确的材料检索对于创建真实的3D资产至关重要。现有方法依赖于捕捉形状不变和光照变化的材料表示的数据集，这些数据集稀缺且面临多样性不足和现实世界泛化不良的挑战。我们提出了MaRI框架，旨在弥合合成材料和真实材料之间的特征空间差距。通过对比学习策略，MaRI构建了一个共享的嵌入空间，使得相似的材料和图像在特征空间中更接近，同时将不相似的对分开，从而提高了材料检索的性能和准确性。', title='MaRI：提升材料检索的智能框架'))
[17.03.2025 10:13] Loading Chinese text from previous data.
[17.03.2025 10:13] Renaming data file.
[17.03.2025 10:13] Renaming previous data. hf_papers.json to ./d/2025-03-17.json
[17.03.2025 10:13] Saving new data file.
[17.03.2025 10:13] Generating page.
[17.03.2025 10:13] Renaming previous page.
[17.03.2025 10:13] Renaming previous data. index.html to ./d/2025-03-17.html
[17.03.2025 10:13] [Experimental] Generating Chinese page for reading.
[17.03.2025 10:13] Chinese vocab [{'word': '讨论', 'pinyin': 'tǎo lùn', 'trans': 'discuss'}, {'word': '名为', 'pinyin': 'míng wéi', 'trans': 'named'}, {'word': '视频', 'pinyin': 'shì pín', 'trans': 'video'}, {'word': '重新', 'pinyin': 'chóng xīn', 'trans': 'again'}, {'word': '渲染', 'pinyin': 'xuàn rán', 'trans': 'render'}, {'word': '框架', 'pinyin': 'kuàng jià', 'trans': 'framework'}, {'word': '摄像机', 'pinyin': 'shè xiàng jī', 'trans': 'camera'}, {'word': '轨迹', 'pinyin': 'guǐ jì', 'trans': 'trajectory'}, {'word': '同时', 'pinyin': 'tóng shí', 'trans': 'simultaneously'}, {'word': '保持', 'pinyin': 'bǎo chí', 'trans': 'maintain'}, {'word': '多帧', 'pinyin': 'duō zhēn', 'trans': 'multi-frame'}, {'word': '外观', 'pinyin': 'wài guǎn', 'trans': 'appearance'}, {'word': '动态', 'pinyin': 'dòng tài', 'trans': 'dynamic'}, {'word': '同步', 'pinyin': 'tóng bù', 'trans': 'synchronization'}, {'word': '方法', 'pinyin': 'fāng fǎ', 'trans': 'method'}, {'word': '利用', 'pinyin': 'lì yòng', 'trans': 'utilize'}, {'word': '预训练', 'pinyin': 'yù xùn liàn', 'trans': 'pre-trained'}, {'word': '文本', 'pinyin': 'wén běn', 'trans': 'text'}, {'word': '生成', 'pinyin': 'shēng chéng', 'trans': 'generate'}, {'word': '能力', 'pinyin': 'néng lì', 'trans': 'ability'}, {'word': '构建', 'pinyin': 'gòu jiàn', 'trans': 'construct'}, {'word': '数据集', 'pinyin': 'shù jù jí', 'trans': 'dataset'}, {'word': '实验', 'pinyin': 'shí yàn', 'trans': 'experiment'}, {'word': '结果', 'pinyin': 'jié guǒ', 'trans': 'result'}, {'word': '显示', 'pinyin': 'xiǎn shì', 'trans': 'display'}, {'word': '稳定', 'pinyin': 'wěn dìng', 'trans': 'stable'}, {'word': '超分辨率', 'pinyin': 'chāo fēn biàn lǜ', 'trans': 'super-resolution'}, {'word': '扩展', 'pinyin': 'kuò zhǎn', 'trans': 'extend'}, {'word': '表现', 'pinyin': 'biǎo xiàn', 'trans': 'performance'}, {'word': '出色', 'pinyin': 'chū sè', 'trans': 'outstanding'}]
[17.03.2025 10:13] Renaming previous Chinese page.
[17.03.2025 10:13] Renaming previous data. zh.html to ./d/2025-03-16_zh_reading_task.html
[17.03.2025 10:13] Writing Chinese reading task.
[17.03.2025 10:13] Writing result.
[17.03.2025 10:13] Renaming log file.
[17.03.2025 10:13] Renaming previous data. log.txt to ./logs/2025-03-17_last_log.txt
