[17.03.2025 04:14] Read previous papers.
[17.03.2025 04:14] Generating top page (month).
[17.03.2025 04:14] Writing top page (month).
[17.03.2025 05:11] Read previous papers.
[17.03.2025 05:11] Get feed.
[17.03.2025 05:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.07677
[17.03.2025 05:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.11647
[17.03.2025 05:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.11646
[17.03.2025 05:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.11224
[17.03.2025 05:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.11514
[17.03.2025 05:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.11069
[17.03.2025 05:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.10772
[17.03.2025 05:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.10970
[17.03.2025 05:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.09279
[17.03.2025 05:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.10632
[17.03.2025 05:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.05689
[17.03.2025 05:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.06674
[17.03.2025 05:11] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[17.03.2025 05:11] No deleted papers detected.
[17.03.2025 05:11] Downloading and parsing papers (pdf, html). Total: 12.
[17.03.2025 05:11] Downloading and parsing paper https://huggingface.co/papers/2503.07677.
[17.03.2025 05:11] Extra JSON file exists (./assets/json/2503.07677.json), skip PDF parsing.
[17.03.2025 05:11] Paper image links file exists (./assets/img_data/2503.07677.json), skip HTML parsing.
[17.03.2025 05:11] Success.
[17.03.2025 05:11] Downloading and parsing paper https://huggingface.co/papers/2503.11647.
[17.03.2025 05:11] Extra JSON file exists (./assets/json/2503.11647.json), skip PDF parsing.
[17.03.2025 05:11] Paper image links file exists (./assets/img_data/2503.11647.json), skip HTML parsing.
[17.03.2025 05:11] Success.
[17.03.2025 05:11] Downloading and parsing paper https://huggingface.co/papers/2503.11646.
[17.03.2025 05:11] Extra JSON file exists (./assets/json/2503.11646.json), skip PDF parsing.
[17.03.2025 05:11] Paper image links file exists (./assets/img_data/2503.11646.json), skip HTML parsing.
[17.03.2025 05:11] Success.
[17.03.2025 05:11] Downloading and parsing paper https://huggingface.co/papers/2503.11224.
[17.03.2025 05:11] Extra JSON file exists (./assets/json/2503.11224.json), skip PDF parsing.
[17.03.2025 05:11] Paper image links file exists (./assets/img_data/2503.11224.json), skip HTML parsing.
[17.03.2025 05:11] Success.
[17.03.2025 05:11] Downloading and parsing paper https://huggingface.co/papers/2503.11514.
[17.03.2025 05:11] Extra JSON file exists (./assets/json/2503.11514.json), skip PDF parsing.
[17.03.2025 05:11] Paper image links file exists (./assets/img_data/2503.11514.json), skip HTML parsing.
[17.03.2025 05:11] Success.
[17.03.2025 05:11] Downloading and parsing paper https://huggingface.co/papers/2503.11069.
[17.03.2025 05:11] Extra JSON file exists (./assets/json/2503.11069.json), skip PDF parsing.
[17.03.2025 05:11] Paper image links file exists (./assets/img_data/2503.11069.json), skip HTML parsing.
[17.03.2025 05:11] Success.
[17.03.2025 05:11] Downloading and parsing paper https://huggingface.co/papers/2503.10772.
[17.03.2025 05:11] Extra JSON file exists (./assets/json/2503.10772.json), skip PDF parsing.
[17.03.2025 05:11] Paper image links file exists (./assets/img_data/2503.10772.json), skip HTML parsing.
[17.03.2025 05:11] Success.
[17.03.2025 05:11] Downloading and parsing paper https://huggingface.co/papers/2503.10970.
[17.03.2025 05:11] Extra JSON file exists (./assets/json/2503.10970.json), skip PDF parsing.
[17.03.2025 05:11] Paper image links file exists (./assets/img_data/2503.10970.json), skip HTML parsing.
[17.03.2025 05:11] Success.
[17.03.2025 05:11] Downloading and parsing paper https://huggingface.co/papers/2503.09279.
[17.03.2025 05:11] Extra JSON file exists (./assets/json/2503.09279.json), skip PDF parsing.
[17.03.2025 05:11] Paper image links file exists (./assets/img_data/2503.09279.json), skip HTML parsing.
[17.03.2025 05:11] Success.
[17.03.2025 05:11] Downloading and parsing paper https://huggingface.co/papers/2503.10632.
[17.03.2025 05:11] Extra JSON file exists (./assets/json/2503.10632.json), skip PDF parsing.
[17.03.2025 05:11] Paper image links file exists (./assets/img_data/2503.10632.json), skip HTML parsing.
[17.03.2025 05:11] Success.
[17.03.2025 05:11] Downloading and parsing paper https://huggingface.co/papers/2503.05689.
[17.03.2025 05:11] Extra JSON file exists (./assets/json/2503.05689.json), skip PDF parsing.
[17.03.2025 05:11] Paper image links file exists (./assets/img_data/2503.05689.json), skip HTML parsing.
[17.03.2025 05:11] Success.
[17.03.2025 05:11] Downloading and parsing paper https://huggingface.co/papers/2503.06674.
[17.03.2025 05:11] Extra JSON file exists (./assets/json/2503.06674.json), skip PDF parsing.
[17.03.2025 05:11] Paper image links file exists (./assets/img_data/2503.06674.json), skip HTML parsing.
[17.03.2025 05:11] Success.
[17.03.2025 05:11] Enriching papers with extra data.
[17.03.2025 05:11] ********************************************************************************
[17.03.2025 05:11] Abstract 0. Diffusion models have shown impressive results in generating high-quality conditional samples using guidance techniques such as Classifier-Free Guidance (CFG). However, existing methods often require additional training or neural function evaluations (NFEs), making them incompatible with guidance-di...
[17.03.2025 05:11] ********************************************************************************
[17.03.2025 05:11] Abstract 1. Camera control has been actively studied in text or image conditioned video generation tasks. However, altering camera trajectories of a given video remains under-explored, despite its importance in the field of video creation. It is non-trivial due to the extra constraints of maintaining multiple-f...
[17.03.2025 05:11] ********************************************************************************
[17.03.2025 05:11] Abstract 2. The pursuit of data efficiency, where quality outweighs quantity, has emerged as a cornerstone in robotic manipulation, especially given the high costs associated with real-world data collection. We propose that maximizing the informational density of individual demonstrations can dramatically reduc...
[17.03.2025 05:11] ********************************************************************************
[17.03.2025 05:11] Abstract 3. State Space Models (SSMs) have emerged as a promising alternative to the popular transformer-based models and have been increasingly gaining attention. Compared to transformers, SSMs excel at tasks with sequential data or longer contexts, demonstrating comparable performances with significant effici...
[17.03.2025 05:11] ********************************************************************************
[17.03.2025 05:11] Abstract 4. Federated Learning (FL) has emerged as a promising privacy-preserving collaborative model training paradigm without sharing raw data. However, recent studies have revealed that private information can still be leaked through shared gradient information and attacked by Gradient Inversion Attacks (GIA...
[17.03.2025 05:11] ********************************************************************************
[17.03.2025 05:11] Abstract 5. Large language models (LLMs) have evolved beyond simple text generation to power software agents that directly translate natural language commands into tangible actions. While API-based LLM agents initially rose to prominence for their robust automation capabilities and seamless integration with pro...
[17.03.2025 05:11] ********************************************************************************
[17.03.2025 05:11] Abstract 6. Bridging different modalities lies at the heart of cross-modality generation. While conventional approaches treat the text modality as a conditioning signal that gradually guides the denoising process from Gaussian noise to the target image modality, we explore a much simpler paradigm-directly evolv...
[17.03.2025 05:11] ********************************************************************************
[17.03.2025 05:11] Abstract 7. Precision therapeutics require multimodal adaptive models that generate personalized treatment recommendations. We introduce TxAgent, an AI agent that leverages multi-step reasoning and real-time biomedical knowledge retrieval across a toolbox of 211 tools to analyze drug interactions, contraindicat...
[17.03.2025 05:11] ********************************************************************************
[17.03.2025 05:11] Abstract 8. Video Detailed Captioning (VDC) is a crucial task for vision-language bridging, enabling fine-grained descriptions of complex video content. In this paper, we first comprehensively benchmark current state-of-the-art approaches and systematically identified two critical limitations: biased capability...
[17.03.2025 05:11] ********************************************************************************
[17.03.2025 05:11] Abstract 9. Kolmogorov-Arnold networks (KANs) are a remarkable innovation consisting of learnable activation functions with the potential to capture more complex relationships from data. Although KANs are useful in finding symbolic representations and continual learning of one-dimensional functions, their effec...
[17.03.2025 05:11] ********************************************************************************
[17.03.2025 05:11] Abstract 10. We propose GoalFlow, an end-to-end autonomous driving method for generating high-quality multimodal trajectories. In autonomous driving scenarios, there is rarely a single suitable trajectory. Recent methods have increasingly focused on modeling multimodal trajectory distributions. However, they suf...
[17.03.2025 05:11] ********************************************************************************
[17.03.2025 05:11] Abstract 11. Accelerating diffusion model sampling is crucial for efficient AIGC deployment. While diffusion distillation methods -- based on distribution matching and trajectory matching -- reduce sampling to as few as one step, they fall short on complex tasks like text-to-image generation. Few-step generation...
[17.03.2025 05:11] Read previous papers.
[17.03.2025 05:11] Generating reviews via LLM API.
[17.03.2025 05:11] Using data from previous issue: {"categories": ["#training", "#cv", "#diffusion", "#inference"], "emoji": "üñºÔ∏è", "ru": {"title": "PLADIS: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º PLADIS –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–∏—Ñ—Ñ—É–∑–∏–∏ –≤ –∑–∞–¥–∞—á–µ –≥
[17.03.2025 05:11] Using data from previous issue: {"categories": ["#dataset", "#optimization", "#training", "#video", "#games"], "emoji": "üé•", "ru": {"title": "–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∫–∞–º–µ—Ä–æ–π –≤ –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "ReCamMaster - —ç—Ç–æ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –∏–∑–º–µ–Ω–µ–Ω–∏—è —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –∫–∞–º–µ—Ä—ã –≤ –≤–∏–¥–µ–æ. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ text-
[17.03.2025 05:11] Using data from previous issue: {"categories": ["#robotics", "#benchmark", "#dataset", "#optimization", "#open_source", "#agents", "#training", "#data"], "emoji": "ü§ñ", "ru": {"title": "–ú–µ–Ω—å—à–µ –¥–∞–Ω–Ω—ã—Ö, –±–æ–ª—å—à–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ –æ–±—É—á–µ–Ω–∏–∏ —Ä–æ–±–æ—Ç–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–±–æ—Ä—É –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Ä–æ–±–æ—Ç–æ–≤ –º
[17.03.2025 05:11] Using data from previous issue: {"categories": ["#architecture", "#long_context", "#survey", "#math", "#training"], "emoji": "üß†", "ru": {"title": "SSM: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞–º –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π", "desc": "–≠—Ç–æ –æ–±–∑–æ—Ä –º–æ–¥–µ–ª–µ–π –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ —Å–æ—Å—Ç–æ—è–Ω–∏–π (SSM) –∫–∞–∫ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞–º –≤ –º–∞—à–∏–Ω–Ω–æ–º –æ–±—É—á–µ–Ω–∏
[17.03.2025 05:11] Using data from previous issue: {"categories": ["#leakage", "#benchmark", "#security", "#survey", "#healthcare", "#data"], "emoji": "üõ°Ô∏è", "ru": {"title": "–ó–∞—â–∏—Ç–∞ –ø—Ä–∏–≤–∞—Ç–Ω–æ—Å—Ç–∏ –≤ —Ñ–µ–¥–µ—Ä–∞—Ç–∏–≤–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏: –∞–Ω–∞–ª–∏–∑ –∞—Ç–∞–∫ —Å –∏–Ω–≤–µ—Ä—Å–∏–µ–π –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞", "desc": "–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –∞–Ω–∞–ª–∏–∑—É –∞—Ç–∞–∫ —Å –∏–Ω–≤–µ—Ä—Å–∏–µ–π –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ (GIA) –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ —Ñ–µ–¥–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á
[17.03.2025 05:11] Using data from previous issue: {"categories": ["#multimodal", "#survey", "#agents"], "emoji": "ü§ñ", "ru": {"title": "–°—Ä–∞–≤–Ω–µ–Ω–∏–µ API –∏ GUI –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM: –ø—É—Ç—å –∫ –≥–∏–±—Ä–∏–¥–Ω—ã–º —Ä–µ—à–µ–Ω–∏—è–º", "desc": "–≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Å—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM), —Ä–∞–±–æ—Ç–∞—é—â–∏—Ö —á–µ—Ä–µ–∑ API –∏ —á–µ—Ä–µ–∑ –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å. –ê–≤—Ç–æ—Ä
[17.03.2025 05:11] Using data from previous issue: {"categories": ["#architecture", "#training", "#multimodal", "#diffusion"], "emoji": "üåä", "ru": {"title": "FlowTok: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –ø–µ—Ä–µ—Ö–æ–¥ –º–µ–∂–¥—É —Ç–µ–∫—Å—Ç–æ–º –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º —á–µ—Ä–µ–∑ —Ç–æ–∫–µ–Ω—ã", "desc": "FlowTok - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏–∑ —Ç–µ–∫—Å—Ç–∞ –∏ –Ω–∞–æ–±–æ—Ä–æ—Ç. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–µ—Ç–æ–¥ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏—è –ø–æ—Ç–æ–∫
[17.03.2025 05:11] Using data from previous issue: {"categories": ["#alignment", "#healthcare", "#science", "#agents", "#reasoning", "#benchmark", "#multimodal"], "emoji": "üíä", "ru": {"title": "TxAgent: –ò–ò-–ø–æ–º–æ—â–Ω–∏–∫ –¥–ª—è —Ç–æ—á–Ω–æ–π –∏ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Ñ–∞—Ä–º–∞–∫–æ—Ç–µ—Ä–∞–ø–∏–∏", "desc": "TxAgent - —ç—Ç–æ –ò–ò-–∞–≥–µ–Ω—Ç –¥–ª—è –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Ç–µ—Ä–∞–ø–∏–∏, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –º–Ω–æ–≥–æ—ç—Ç–∞
[17.03.2025 05:11] Using data from previous issue: {"categories": ["#benchmark", "#training", "#multimodal", "#synthetic", "#alignment"], "emoji": "ü¶ú", "ru": {"title": "Cockatiel: –ù–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –≤ –¥–µ—Ç–∞–ª—å–Ω–æ–º –æ–ø–∏—Å–∞–Ω–∏–∏ –≤–∏–¥–µ–æ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –¥–µ—Ç–∞–ª—å–Ω–æ–º—É –æ–ø–∏—Å–∞–Ω–∏—é –≤–∏–¥–µ–æ (VDC) –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Cockatiel. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ —Ç—Ä–µ—Ö—ç—Ç–∞
[17.03.2025 05:11] Using data from previous issue: {"categories": ["#architecture", "#cv", "#optimization", "#open_source", "#training"], "emoji": "üß†", "ru": {"title": "–ù–æ–≤—ã–π –≤–∑–≥–ª—è–¥ –Ω–∞ –≤–Ω–∏–º–∞–Ω–∏–µ: –ö–æ–ª–º–æ–≥–æ—Ä–æ–≤-–ê—Ä–Ω–æ–ª—å–¥ —Å–µ—Ç–∏ –≤ Vision Transformers", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π - –ö–æ–ª–º–æ–≥–æ—Ä–æ–≤-–ê—Ä–Ω–æ–ª—å–¥ –≤–Ω–∏–º–∞–Ω–∏–µ (KArAt
[17.03.2025 05:11] Using data from previous issue: {"categories": ["#optimization", "#diffusion", "#agents", "#multimodal"], "emoji": "üöó", "ru": {"title": "GoalFlow: –¢–æ—á–Ω–æ–µ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π –¥–ª—è –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –≤–æ–∂–¥–µ–Ω–∏—è", "desc": "GoalFlow - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –≤–æ–∂–¥–µ–Ω–∏—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π. –û–Ω —Ä–µ—à–∞–µ—Ç
[17.03.2025 05:11] Using data from previous issue: {"categories": ["#diffusion", "#training", "#cv", "#video"], "emoji": "üöÄ", "ru": {"title": "TDM: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ —É—Å–∫–æ—Ä–µ–Ω–∏–∏ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —É—Å–∫–æ—Ä–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –Ω–∞–∑—ã–≤–∞–µ–º—ã–π Trajectory Distribution Matching (TDM). TD
[17.03.2025 05:11] Loading Chinese text from previous data.
[17.03.2025 05:11] Renaming data file.
[17.03.2025 05:11] Renaming previous data. hf_papers.json to ./d/2025-03-17.json
[17.03.2025 05:11] Saving new data file.
[17.03.2025 05:11] Generating page.
[17.03.2025 05:11] Renaming previous page.
[17.03.2025 05:11] Renaming previous data. index.html to ./d/2025-03-17.html
[17.03.2025 05:11] [Experimental] Generating Chinese page for reading.
[17.03.2025 05:11] Chinese vocab [{'word': 'ËÆ®ËÆ∫', 'pinyin': 't«éo l√πn', 'trans': 'discuss'}, {'word': 'Ê≠£ÂàôÂåñ', 'pinyin': 'zh√®ng z√© hu√†', 'trans': 'regularization'}, {'word': 'Áé∞‰ª£', 'pinyin': 'xi√†n d√†i', 'trans': 'modern'}, {'word': 'Á•ûÁªèÁΩëÁªú', 'pinyin': 'sh√©n jƒ´ng w«éng lu√≤', 'trans': 'neural network'}, {'word': '‰ΩúÁî®', 'pinyin': 'zu√≤ y√≤ng', 'trans': 'role'}, {'word': 'Á†îÁ©∂', 'pinyin': 'y√°n ji≈´', 'trans': 'research'}, {'word': 'ÂèëÁé∞', 'pinyin': 'fƒÅ xi√†n', 'trans': 'discover'}, {'word': 'Âç≥‰Ωø', 'pinyin': 'j√≠ sh«ê', 'trans': 'even if'}, {'word': '‰ΩøÁî®', 'pinyin': 'sh«ê y√≤ng', 'trans': 'use'}, {'word': 'Transformers', 'pinyin': 'T√®i hu√†n fƒÅ m«î', 'trans': 'Transformers'}, {'word': 'ÈÄöËøá', 'pinyin': 't≈çng gu√≤', 'trans': 'through'}, {'word': 'ÊäÄÊúØ', 'pinyin': 'j√¨ sh√π', 'trans': 'technique'}, {'word': 'ËææÂà∞', 'pinyin': 'd√° d√†o', 'trans': 'achieve'}, {'word': 'Áõ∏Âêå', 'pinyin': 'xiƒÅng t√≥ng', 'trans': 'same'}, {'word': 'ÊÄßËÉΩ', 'pinyin': 'x√¨ng n√©ng', 'trans': 'performance'}, {'word': 'ÂÖÉÁ¥†Á∫ß', 'pinyin': 'yu√°n s√π j√≠', 'trans': 'element-wise'}, {'word': 'Êìç‰Ωú', 'pinyin': 'cƒÅo zu√≤', 'trans': 'operation'}, {'word': 'ÁÅµÊÑü', 'pinyin': 'l√≠ng g«én', 'trans': 'inspiration'}, {'word': 'Â±Ç', 'pinyin': 'c√©ng', 'trans': 'layer'}, {'word': 'Â∏∏Â∏∏', 'pinyin': 'ch√°ng ch√°ng', 'trans': 'often'}, {'word': '‰∫ßÁîü', 'pinyin': 'ch«én shƒìng', 'trans': 'generate'}, {'word': 'Á±ª‰ºº', 'pinyin': 'l√®i s√¨', 'trans': 'similar'}, {'word': 'tanh', 'pinyin': 't«én h', 'trans': 'tanh'}, {'word': 'SÂΩ¢', 'pinyin': 'S x√≠ng', 'trans': 'S-shaped'}, {'word': 'ËæìÂÖ•-ËæìÂá∫', 'pinyin': 'sh≈´ r√π - sh≈´ ch≈´', 'trans': 'input-output'}, {'word': 'Êò†Â∞Ñ', 'pinyin': 'y√¨ng sh√®', 'trans': 'mapping'}, {'word': 'ÂåπÊïå', 'pinyin': 'p«ê d√≠', 'trans': 'match'}, {'word': 'Ë∂ÖË∂ä', 'pinyin': 'chƒÅo yu√®', 'trans': 'surpass'}, {'word': 'ÊÉÖÂÜµ', 'pinyin': 'q√≠ng ku√†ng', 'trans': 'situation'}, {'word': 'Ë∂ÖÂèÇÊï∞', 'pinyin': 'chƒÅo cƒÅn sh√π', 'trans': 'hyperparameter'}, {'word': 'Ë∞ÉÊï¥', 'pinyin': 'ti√°o zhƒõng', 'trans': 'adjust'}]
[17.03.2025 05:11] Renaming previous Chinese page.
[17.03.2025 05:11] Renaming previous data. zh.html to ./d/2025-03-16_zh_reading_task.html
[17.03.2025 05:11] Writing Chinese reading task.
[17.03.2025 05:11] Writing result.
[17.03.2025 05:11] Renaming log file.
[17.03.2025 05:11] Renaming previous data. log.txt to ./logs/2025-03-17_last_log.txt
