[17.03.2025 15:11] Read previous papers.
[17.03.2025 15:11] Generating top page (month).
[17.03.2025 15:11] Writing top page (month).
[17.03.2025 16:14] Read previous papers.
[17.03.2025 16:14] Get feed.
[17.03.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2503.11647
[17.03.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2503.07677
[17.03.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2503.11646
[17.03.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2503.11224
[17.03.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2503.11069
[17.03.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2503.11514
[17.03.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2503.10781
[17.03.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2503.10772
[17.03.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2503.10970
[17.03.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2503.10632
[17.03.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2503.11651
[17.03.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2503.09279
[17.03.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2503.10696
[17.03.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2503.06553
[17.03.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2503.06542
[17.03.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2503.06674
[17.03.2025 16:14] Extract page data from URL. URL: https://huggingface.co/papers/2503.11579
[17.03.2025 16:14] Extract page data from URL. URL: https://huggingface.co/papers/2503.11576
[17.03.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2503.10624
[17.03.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2503.10684
[17.03.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2503.05689
[17.03.2025 16:14] Extract page data from URL. URL: https://huggingface.co/papers/2503.11629
[17.03.2025 16:14] Extract page data from URL. URL: https://huggingface.co/papers/2503.11207
[17.03.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2503.08111
[17.03.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2503.10620
[17.03.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2503.09330
[17.03.2025 16:14] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[17.03.2025 16:14] No deleted papers detected.
[17.03.2025 16:14] Downloading and parsing papers (pdf, html). Total: 26.
[17.03.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2503.11647.
[17.03.2025 16:14] Extra JSON file exists (./assets/json/2503.11647.json), skip PDF parsing.
[17.03.2025 16:14] Paper image links file exists (./assets/img_data/2503.11647.json), skip HTML parsing.
[17.03.2025 16:14] Success.
[17.03.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2503.07677.
[17.03.2025 16:14] Extra JSON file exists (./assets/json/2503.07677.json), skip PDF parsing.
[17.03.2025 16:14] Paper image links file exists (./assets/img_data/2503.07677.json), skip HTML parsing.
[17.03.2025 16:14] Success.
[17.03.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2503.11646.
[17.03.2025 16:14] Extra JSON file exists (./assets/json/2503.11646.json), skip PDF parsing.
[17.03.2025 16:14] Paper image links file exists (./assets/img_data/2503.11646.json), skip HTML parsing.
[17.03.2025 16:14] Success.
[17.03.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2503.11224.
[17.03.2025 16:14] Extra JSON file exists (./assets/json/2503.11224.json), skip PDF parsing.
[17.03.2025 16:14] Paper image links file exists (./assets/img_data/2503.11224.json), skip HTML parsing.
[17.03.2025 16:14] Success.
[17.03.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2503.11069.
[17.03.2025 16:14] Extra JSON file exists (./assets/json/2503.11069.json), skip PDF parsing.
[17.03.2025 16:14] Paper image links file exists (./assets/img_data/2503.11069.json), skip HTML parsing.
[17.03.2025 16:14] Success.
[17.03.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2503.11514.
[17.03.2025 16:14] Extra JSON file exists (./assets/json/2503.11514.json), skip PDF parsing.
[17.03.2025 16:14] Paper image links file exists (./assets/img_data/2503.11514.json), skip HTML parsing.
[17.03.2025 16:14] Success.
[17.03.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2503.10781.
[17.03.2025 16:14] Extra JSON file exists (./assets/json/2503.10781.json), skip PDF parsing.
[17.03.2025 16:14] Paper image links file exists (./assets/img_data/2503.10781.json), skip HTML parsing.
[17.03.2025 16:14] Success.
[17.03.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2503.10772.
[17.03.2025 16:14] Extra JSON file exists (./assets/json/2503.10772.json), skip PDF parsing.
[17.03.2025 16:14] Paper image links file exists (./assets/img_data/2503.10772.json), skip HTML parsing.
[17.03.2025 16:14] Success.
[17.03.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2503.10970.
[17.03.2025 16:14] Extra JSON file exists (./assets/json/2503.10970.json), skip PDF parsing.
[17.03.2025 16:14] Paper image links file exists (./assets/img_data/2503.10970.json), skip HTML parsing.
[17.03.2025 16:14] Success.
[17.03.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2503.10632.
[17.03.2025 16:14] Extra JSON file exists (./assets/json/2503.10632.json), skip PDF parsing.
[17.03.2025 16:14] Paper image links file exists (./assets/img_data/2503.10632.json), skip HTML parsing.
[17.03.2025 16:14] Success.
[17.03.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2503.11651.
[17.03.2025 16:14] Extra JSON file exists (./assets/json/2503.11651.json), skip PDF parsing.
[17.03.2025 16:14] Paper image links file exists (./assets/img_data/2503.11651.json), skip HTML parsing.
[17.03.2025 16:14] Success.
[17.03.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2503.09279.
[17.03.2025 16:14] Extra JSON file exists (./assets/json/2503.09279.json), skip PDF parsing.
[17.03.2025 16:14] Paper image links file exists (./assets/img_data/2503.09279.json), skip HTML parsing.
[17.03.2025 16:14] Success.
[17.03.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2503.10696.
[17.03.2025 16:14] Extra JSON file exists (./assets/json/2503.10696.json), skip PDF parsing.
[17.03.2025 16:14] Paper image links file exists (./assets/img_data/2503.10696.json), skip HTML parsing.
[17.03.2025 16:14] Success.
[17.03.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2503.06553.
[17.03.2025 16:14] Extra JSON file exists (./assets/json/2503.06553.json), skip PDF parsing.
[17.03.2025 16:14] Paper image links file exists (./assets/img_data/2503.06553.json), skip HTML parsing.
[17.03.2025 16:14] Success.
[17.03.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2503.06542.
[17.03.2025 16:14] Extra JSON file exists (./assets/json/2503.06542.json), skip PDF parsing.
[17.03.2025 16:14] Paper image links file exists (./assets/img_data/2503.06542.json), skip HTML parsing.
[17.03.2025 16:14] Success.
[17.03.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2503.06674.
[17.03.2025 16:14] Extra JSON file exists (./assets/json/2503.06674.json), skip PDF parsing.
[17.03.2025 16:14] Paper image links file exists (./assets/img_data/2503.06674.json), skip HTML parsing.
[17.03.2025 16:14] Success.
[17.03.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2503.11579.
[17.03.2025 16:14] Extra JSON file exists (./assets/json/2503.11579.json), skip PDF parsing.
[17.03.2025 16:14] Paper image links file exists (./assets/img_data/2503.11579.json), skip HTML parsing.
[17.03.2025 16:14] Success.
[17.03.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2503.11576.
[17.03.2025 16:14] Extra JSON file exists (./assets/json/2503.11576.json), skip PDF parsing.
[17.03.2025 16:14] Paper image links file exists (./assets/img_data/2503.11576.json), skip HTML parsing.
[17.03.2025 16:14] Success.
[17.03.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2503.10624.
[17.03.2025 16:14] Extra JSON file exists (./assets/json/2503.10624.json), skip PDF parsing.
[17.03.2025 16:14] Paper image links file exists (./assets/img_data/2503.10624.json), skip HTML parsing.
[17.03.2025 16:14] Success.
[17.03.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2503.10684.
[17.03.2025 16:14] Extra JSON file exists (./assets/json/2503.10684.json), skip PDF parsing.
[17.03.2025 16:14] Paper image links file exists (./assets/img_data/2503.10684.json), skip HTML parsing.
[17.03.2025 16:14] Success.
[17.03.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2503.05689.
[17.03.2025 16:14] Extra JSON file exists (./assets/json/2503.05689.json), skip PDF parsing.
[17.03.2025 16:14] Paper image links file exists (./assets/img_data/2503.05689.json), skip HTML parsing.
[17.03.2025 16:14] Success.
[17.03.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2503.11629.
[17.03.2025 16:14] Extra JSON file exists (./assets/json/2503.11629.json), skip PDF parsing.
[17.03.2025 16:14] Paper image links file exists (./assets/img_data/2503.11629.json), skip HTML parsing.
[17.03.2025 16:14] Success.
[17.03.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2503.11207.
[17.03.2025 16:14] Extra JSON file exists (./assets/json/2503.11207.json), skip PDF parsing.
[17.03.2025 16:14] Paper image links file exists (./assets/img_data/2503.11207.json), skip HTML parsing.
[17.03.2025 16:14] Success.
[17.03.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2503.08111.
[17.03.2025 16:14] Extra JSON file exists (./assets/json/2503.08111.json), skip PDF parsing.
[17.03.2025 16:14] Paper image links file exists (./assets/img_data/2503.08111.json), skip HTML parsing.
[17.03.2025 16:14] Success.
[17.03.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2503.10620.
[17.03.2025 16:14] Extra JSON file exists (./assets/json/2503.10620.json), skip PDF parsing.
[17.03.2025 16:14] Paper image links file exists (./assets/img_data/2503.10620.json), skip HTML parsing.
[17.03.2025 16:14] Success.
[17.03.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2503.09330.
[17.03.2025 16:14] Extra JSON file exists (./assets/json/2503.09330.json), skip PDF parsing.
[17.03.2025 16:14] Paper image links file exists (./assets/img_data/2503.09330.json), skip HTML parsing.
[17.03.2025 16:14] Success.
[17.03.2025 16:14] Enriching papers with extra data.
[17.03.2025 16:14] ********************************************************************************
[17.03.2025 16:14] Abstract 0. Camera control has been actively studied in text or image conditioned video generation tasks. However, altering camera trajectories of a given video remains under-explored, despite its importance in the field of video creation. It is non-trivial due to the extra constraints of maintaining multiple-f...
[17.03.2025 16:14] ********************************************************************************
[17.03.2025 16:14] Abstract 1. Diffusion models have shown impressive results in generating high-quality conditional samples using guidance techniques such as Classifier-Free Guidance (CFG). However, existing methods often require additional training or neural function evaluations (NFEs), making them incompatible with guidance-di...
[17.03.2025 16:14] ********************************************************************************
[17.03.2025 16:14] Abstract 2. The pursuit of data efficiency, where quality outweighs quantity, has emerged as a cornerstone in robotic manipulation, especially given the high costs associated with real-world data collection. We propose that maximizing the informational density of individual demonstrations can dramatically reduc...
[17.03.2025 16:14] ********************************************************************************
[17.03.2025 16:14] Abstract 3. State Space Models (SSMs) have emerged as a promising alternative to the popular transformer-based models and have been increasingly gaining attention. Compared to transformers, SSMs excel at tasks with sequential data or longer contexts, demonstrating comparable performances with significant effici...
[17.03.2025 16:14] ********************************************************************************
[17.03.2025 16:14] Abstract 4. Large language models (LLMs) have evolved beyond simple text generation to power software agents that directly translate natural language commands into tangible actions. While API-based LLM agents initially rose to prominence for their robust automation capabilities and seamless integration with pro...
[17.03.2025 16:14] ********************************************************************************
[17.03.2025 16:14] Abstract 5. Federated Learning (FL) has emerged as a promising privacy-preserving collaborative model training paradigm without sharing raw data. However, recent studies have revealed that private information can still be leaked through shared gradient information and attacked by Gradient Inversion Attacks (GIA...
[17.03.2025 16:14] ********************************************************************************
[17.03.2025 16:14] Abstract 6. We propose a novel approach for captioning and object grounding in video, where the objects in the caption are grounded in the video via temporally dense bounding boxes. We introduce the following contributions. First, we present a large-scale automatic annotation method that aggregates captions gro...
[17.03.2025 16:14] ********************************************************************************
[17.03.2025 16:14] Abstract 7. Bridging different modalities lies at the heart of cross-modality generation. While conventional approaches treat the text modality as a conditioning signal that gradually guides the denoising process from Gaussian noise to the target image modality, we explore a much simpler paradigm-directly evolv...
[17.03.2025 16:14] ********************************************************************************
[17.03.2025 16:14] Abstract 8. Precision therapeutics require multimodal adaptive models that generate personalized treatment recommendations. We introduce TxAgent, an AI agent that leverages multi-step reasoning and real-time biomedical knowledge retrieval across a toolbox of 211 tools to analyze drug interactions, contraindicat...
[17.03.2025 16:14] ********************************************************************************
[17.03.2025 16:14] Abstract 9. Kolmogorov-Arnold networks (KANs) are a remarkable innovation consisting of learnable activation functions with the potential to capture more complex relationships from data. Although KANs are useful in finding symbolic representations and continual learning of one-dimensional functions, their effec...
[17.03.2025 16:14] ********************************************************************************
[17.03.2025 16:14] Abstract 10. We present VGGT, a feed-forward neural network that directly infers all key 3D attributes of a scene, including camera parameters, point maps, depth maps, and 3D point tracks, from one, a few, or hundreds of its views. This approach is a step forward in 3D computer vision, where models have typicall...
[17.03.2025 16:14] ********************************************************************************
[17.03.2025 16:14] Abstract 11. Video Detailed Captioning (VDC) is a crucial task for vision-language bridging, enabling fine-grained descriptions of complex video content. In this paper, we first comprehensively benchmark current state-of-the-art approaches and systematically identified two critical limitations: biased capability...
[17.03.2025 16:14] ********************************************************************************
[17.03.2025 16:14] Abstract 12. Visual autoregressive models typically adhere to a raster-order ``next-token prediction" paradigm, which overlooks the spatial and temporal locality inherent in visual content. Specifically, visual tokens exhibit significantly stronger correlations with their spatially or temporally adjacent tokens ...
[17.03.2025 16:14] ********************************************************************************
[17.03.2025 16:14] Abstract 13. As multi-modal large language models (MLLMs) frequently exhibit errors when solving scientific problems, evaluating the validity of their reasoning processes is critical for ensuring reliability and uncovering fine-grained model weaknesses. Since human evaluation is laborious and costly, prompting M...
[17.03.2025 16:14] ********************************************************************************
[17.03.2025 16:14] Abstract 14. Unified models (UniMs) for multimodal understanding and generation have recently received much attention in the area of vision and language. Existing UniMs are designed to simultaneously learn both multimodal understanding and generation capabilities, demanding substantial computational resources, a...
[17.03.2025 16:14] ********************************************************************************
[17.03.2025 16:14] Abstract 15. Accelerating diffusion model sampling is crucial for efficient AIGC deployment. While diffusion distillation methods -- based on distribution matching and trajectory matching -- reduce sampling to as few as one step, they fall short on complex tasks like text-to-image generation. Few-step generation...
[17.03.2025 16:14] ********************************************************************************
[17.03.2025 16:14] Abstract 16. State-of-the-art transformer-based large multimodal models (LMMs) struggle to handle hour-long video inputs due to the quadratic complexity of the causal self-attention operations, leading to high computational costs during training and inference. Existing token compression-based methods reduce the ...
[17.03.2025 16:14] ********************************************************************************
[17.03.2025 16:14] Abstract 17. We introduce SmolDocling, an ultra-compact vision-language model targeting end-to-end document conversion. Our model comprehensively processes entire pages by generating DocTags, a new universal markup format that captures all page elements in their full context with location. Unlike existing approa...
[17.03.2025 16:14] ********************************************************************************
[17.03.2025 16:14] Abstract 18. Fitting a body to a 3D clothed human point cloud is a common yet challenging task. Traditional optimization-based approaches use multi-stage pipelines that are sensitive to pose initialization, while recent learning-based methods often struggle with generalization across diverse poses and garment ty...
[17.03.2025 16:14] ********************************************************************************
[17.03.2025 16:14] Abstract 19. Learning skills in open-world environments is essential for developing agents capable of handling a variety of tasks by combining basic skills. Online demonstration videos are typically long but unsegmented, making them difficult to segment and label with skill identifiers. Unlike existing methods t...
[17.03.2025 16:14] ********************************************************************************
[17.03.2025 16:14] Abstract 20. We propose GoalFlow, an end-to-end autonomous driving method for generating high-quality multimodal trajectories. In autonomous driving scenarios, there is rarely a single suitable trajectory. Recent methods have increasingly focused on modeling multimodal trajectory distributions. However, they suf...
[17.03.2025 16:14] ********************************************************************************
[17.03.2025 16:14] Abstract 21. We introduce TreeMeshGPT, an autoregressive Transformer designed to generate high-quality artistic meshes aligned with input point clouds. Instead of the conventional next-token prediction in autoregressive Transformer, we propose a novel Autoregressive Tree Sequencing where the next input token is ...
[17.03.2025 16:14] ********************************************************************************
[17.03.2025 16:14] Abstract 22. This work presents a first evaluation of two state-of-the-art Large Reasoning Models (LRMs), OpenAI's o3-mini and DeepSeek R1, on analogical reasoning, focusing on well-established nonverbal human IQ tests based on Raven's progressive matrices. We benchmark with the I-RAVEN dataset and its more diff...
[17.03.2025 16:14] ********************************************************************************
[17.03.2025 16:14] Abstract 23. Accurate material retrieval is critical for creating realistic 3D assets. Existing methods rely on datasets that capture shape-invariant and lighting-varied representations of materials, which are scarce and face challenges due to limited diversity and inadequate real-world generalization. Most curr...
[17.03.2025 16:14] ********************************************************************************
[17.03.2025 16:14] Abstract 24. Large language models (LLMs) have shown remarkable performance and generalization capabilities across multiple languages and tasks, making them very attractive targets for multi-modality integration (e.g., images or speech). In this work, we extend an existing LLM to the speech modality via speech d...
[17.03.2025 16:14] ********************************************************************************
[17.03.2025 16:14] Abstract 25. Machine unlearning is an emerging paradigm to remove the influence of specific training data (i.e., the forget set) from a model while preserving its knowledge of the rest of the data (i.e., the retain set). Previous approaches assume the forget data to be uniformly distributed from all training dat...
[17.03.2025 16:14] Read previous papers.
[17.03.2025 16:14] Generating reviews via LLM API.
[17.03.2025 16:14] Using data from previous issue: {"categories": ["#dataset", "#optimization", "#training", "#video", "#games"], "emoji": "🎥", "ru": {"title": "Управление камерой в видео с помощью генеративных моделей", "desc": "ReCamMaster - это генеративная система для изменения траектории камеры в видео. Она использует предобученные модели text-
[17.03.2025 16:14] Using data from previous issue: {"categories": ["#training", "#cv", "#diffusion", "#inference"], "emoji": "🖼️", "ru": {"title": "PLADIS: Эффективное улучшение генерации изображений с помощью разреженного внимания", "desc": "Статья представляет новый метод под названием PLADIS для улучшения предобученных моделей диффузии в задаче г
[17.03.2025 16:14] Using data from previous issue: {"categories": ["#robotics", "#benchmark", "#dataset", "#optimization", "#open_source", "#agents", "#training", "#data"], "emoji": "🤖", "ru": {"title": "Меньше данных, больше эффективности: революция в обучении роботов", "desc": "Статья представляет новый подход к сбору данных для обучения роботов м
[17.03.2025 16:14] Using data from previous issue: {"categories": ["#architecture", "#long_context", "#survey", "#math", "#training"], "emoji": "🧠", "ru": {"title": "SSM: Эффективная альтернатива трансформерам для обработки последовательностей", "desc": "Это обзор моделей пространства состояний (SSM) как альтернативы трансформерам в машинном обучени
[17.03.2025 16:14] Using data from previous issue: {"categories": ["#multimodal", "#survey", "#agents"], "emoji": "🤖", "ru": {"title": "Сравнение API и GUI агентов на основе LLM: путь к гибридным решениям", "desc": "Это исследование сравнивает агентов на основе больших языковых моделей (LLM), работающих через API и через графический интерфейс. Автор
[17.03.2025 16:14] Using data from previous issue: {"categories": ["#leakage", "#benchmark", "#security", "#survey", "#healthcare", "#data"], "emoji": "🛡️", "ru": {"title": "Защита приватности в федеративном обучении: анализ атак с инверсией градиента", "desc": "Статья посвящена анализу атак с инверсией градиента (GIA) в контексте федеративного обуч
[17.03.2025 16:14] Using data from previous issue: {"categories": ["#cv", "#dataset", "#training", "#video", "#data"], "emoji": "🎥", "ru": {"title": "Революция в понимании видео: от автоматической разметки к точной локализации объектов", "desc": "Статья представляет новый подход к генерации подписей и локализации объектов в видео с использованием те
[17.03.2025 16:14] Using data from previous issue: {"categories": ["#architecture", "#training", "#multimodal", "#diffusion"], "emoji": "🌊", "ru": {"title": "FlowTok: эффективный переход между текстом и изображением через токены", "desc": "FlowTok - это новый подход к генерации изображений из текста и наоборот. Он использует метод согласования поток
[17.03.2025 16:14] Using data from previous issue: {"categories": ["#alignment", "#healthcare", "#science", "#agents", "#reasoning", "#benchmark", "#multimodal"], "emoji": "💊", "ru": {"title": "TxAgent: ИИ-помощник для точной и персонализированной фармакотерапии", "desc": "TxAgent - это ИИ-агент для персонализированной терапии, использующий многоэта
[17.03.2025 16:14] Using data from previous issue: {"categories": ["#architecture", "#cv", "#optimization", "#open_source", "#training"], "emoji": "🧠", "ru": {"title": "Новый взгляд на внимание: Колмогоров-Арнольд сети в Vision Transformers", "desc": "Статья представляет новый подход к архитектуре нейронных сетей - Колмогоров-Арнольд внимание (KArAt
[17.03.2025 16:14] Using data from previous issue: {"categories": ["#cv", "#open_source", "#3d", "#optimization"], "emoji": "🔮", "ru": {"title": "VGGT: Универсальная нейросеть для комплексного 3D-анализа сцен", "desc": "VGGT - это нейронная сеть прямого распространения, которая напрямую выводит все ключевые 3D-атрибуты сцены из одного или нескольких
[17.03.2025 16:14] Using data from previous issue: {"categories": ["#benchmark", "#training", "#multimodal", "#synthetic", "#alignment"], "emoji": "🦜", "ru": {"title": "Cockatiel: Новый стандарт в детальном описании видео", "desc": "Статья представляет новый подход к детальному описанию видео (VDC) под названием Cockatiel. Авторы разработали трехэта
[17.03.2025 16:14] Using data from previous issue: {"categories": ["#cv", "#video", "#games", "#benchmark", "#optimization", "#architecture", "#training"], "emoji": "🧩", "ru": {"title": "NAR: Революция в авторегрессионном моделировании визуального контента", "desc": "Статья представляет новый подход к авторегрессионному моделированию визуального кон
[17.03.2025 16:14] Using data from previous issue: {"categories": ["#interpretability", "#reasoning", "#multimodal", "#science", "#open_source", "#dataset", "#training", "#benchmark"], "emoji": "🔬", "ru": {"title": "Надежная оценка научных рассуждений мультимодальными ИИ-судьями", "desc": "В статье представлен ProJudgeBench - первый комплексный бенч
[17.03.2025 16:14] Using data from previous issue: {"categories": ["#games", "#optimization", "#multimodal", "#dataset", "#training", "#architecture"], "emoji": "🛡️", "ru": {"title": "ARMOR: Эффективное улучшение мультимодальных моделей", "desc": "ARMOR - это новый подход к созданию унифицированных моделей для мультимодального понимания и генерации.
[17.03.2025 16:14] Using data from previous issue: {"categories": ["#diffusion", "#training", "#cv", "#video"], "emoji": "🚀", "ru": {"title": "TDM: Революция в ускорении диффузионных моделей", "desc": "Статья представляет новый метод ускорения генерации изображений с помощью диффузионных моделей, называемый Trajectory Distribution Matching (TDM). TD
[17.03.2025 16:14] Querying the API.
[17.03.2025 16:14] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

State-of-the-art transformer-based large multimodal models (LMMs) struggle to handle hour-long video inputs due to the quadratic complexity of the causal self-attention operations, leading to high computational costs during training and inference. Existing token compression-based methods reduce the number of video tokens but often incur information loss and remain inefficient for extremely long sequences. In this paper, we explore an orthogonal direction to build a hybrid Mamba-Transformer model (VAMBA) that employs Mamba-2 blocks to encode video tokens with linear complexity. Without any token reduction, VAMBA can encode more than 1024 frames (640times360) on a single GPU, while transformer-based models can only encode 256 frames. On long video input, VAMBA achieves at least 50% reduction in GPU memory usage during training and inference, and nearly doubles the speed per training step compared to transformer-based LMMs. Our experimental results demonstrate that VAMBA improves accuracy by 4.3% on the challenging hour-long video understanding benchmark LVBench over prior efficient video LMMs, and maintains strong performance on a broad spectrum of long and short video understanding tasks.
[17.03.2025 16:14] Response: {
  "desc": "Статья представляет новую модель VAMBA, гибрид Mamba и трансформера, для обработки длинных видео. VAMBA использует блоки Mamba-2 для кодирования видеотокенов с линейной сложностью, что позволяет обрабатывать более 1024 кадров на одном GPU. Модель снижает использование памяти GPU на 50% и почти вдвое ускоряет обучение по сравнению с трансформерными моделями. VAMBA показывает улучшение точности на 4.3% на бенчмарке LVBench для понимания часовых видео.",

  "emoji": "🎥",

  "title": "VAMBA: эффективная обработка длинных видео с линейной сложностью"
}
[17.03.2025 16:14] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"State-of-the-art transformer-based large multimodal models (LMMs) struggle to handle hour-long video inputs due to the quadratic complexity of the causal self-attention operations, leading to high computational costs during training and inference. Existing token compression-based methods reduce the number of video tokens but often incur information loss and remain inefficient for extremely long sequences. In this paper, we explore an orthogonal direction to build a hybrid Mamba-Transformer model (VAMBA) that employs Mamba-2 blocks to encode video tokens with linear complexity. Without any token reduction, VAMBA can encode more than 1024 frames (640times360) on a single GPU, while transformer-based models can only encode 256 frames. On long video input, VAMBA achieves at least 50% reduction in GPU memory usage during training and inference, and nearly doubles the speed per training step compared to transformer-based LMMs. Our experimental results demonstrate that VAMBA improves accuracy by 4.3% on the challenging hour-long video understanding benchmark LVBench over prior efficient video LMMs, and maintains strong performance on a broad spectrum of long and short video understanding tasks."

[17.03.2025 16:14] Response: ```python
['VIDEO', 'ARCHITECTURE', 'BENCHMARK']
```
[17.03.2025 16:14] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"State-of-the-art transformer-based large multimodal models (LMMs) struggle to handle hour-long video inputs due to the quadratic complexity of the causal self-attention operations, leading to high computational costs during training and inference. Existing token compression-based methods reduce the number of video tokens but often incur information loss and remain inefficient for extremely long sequences. In this paper, we explore an orthogonal direction to build a hybrid Mamba-Transformer model (VAMBA) that employs Mamba-2 blocks to encode video tokens with linear complexity. Without any token reduction, VAMBA can encode more than 1024 frames (640times360) on a single GPU, while transformer-based models can only encode 256 frames. On long video input, VAMBA achieves at least 50% reduction in GPU memory usage during training and inference, and nearly doubles the speed per training step compared to transformer-based LMMs. Our experimental results demonstrate that VAMBA improves accuracy by 4.3% on the challenging hour-long video understanding benchmark LVBench over prior efficient video LMMs, and maintains strong performance on a broad spectrum of long and short video understanding tasks."

[17.03.2025 16:14] Response: ```python
["LONG_CONTEXT", "OPTIMIZATION"]
```
[17.03.2025 16:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces the Mamba-Transformer model (VAMBA), which addresses the limitations of existing transformer-based large multimodal models (LMMs) in processing long video inputs. Unlike traditional methods that reduce video tokens and often lose information, VAMBA uses Mamba-2 blocks to encode video tokens with linear complexity, allowing it to handle over 1024 frames efficiently. The model significantly reduces GPU memory usage by at least 50% during training and inference, while also increasing training speed nearly twofold compared to standard transformers. Experimental results show that VAMBA not only enhances accuracy on the LVBench benchmark but also performs well across various video understanding tasks.","title":"VAMBA: Efficient Video Processing with Linear Complexity"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper introduces the Mamba-Transformer model (VAMBA), which addresses the limitations of existing transformer-based large multimodal models (LMMs) in processing long video inputs. Unlike traditional methods that reduce video tokens and often lose information, VAMBA uses Mamba-2 blocks to encode video tokens with linear complexity, allowing it to handle over 1024 frames efficiently. The model significantly reduces GPU memory usage by at least 50% during training and inference, while also increasing training speed nearly twofold compared to standard transformers. Experimental results show that VAMBA not only enhances accuracy on the LVBench benchmark but also performs well across various video understanding tasks.', title='VAMBA: Efficient Video Processing with Linear Complexity'))
[17.03.2025 16:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本论文提出了一种新的混合Mamba-Transformer模型（VAMBA），旨在解决现有多模态模型在处理长视频输入时的计算复杂性问题。VAMBA使用Mamba-2模块以线性复杂度编码视频标记，避免了信息损失，并且无需减少标记数量。与传统的变换器模型相比，VAMBA在单个GPU上能够编码超过1024帧的视频，显著提高了训练和推理的速度，并减少了GPU内存使用。实验结果表明，VAMBA在长视频理解基准测试中比之前的高效视频模型提高了4.3%的准确率，同时在多种视频理解任务中保持了强大的性能。","title":"高效视频理解的新突破：VAMBA模型"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本论文提出了一种新的混合Mamba-Transformer模型（VAMBA），旨在解决现有多模态模型在处理长视频输入时的计算复杂性问题。VAMBA使用Mamba-2模块以线性复杂度编码视频标记，避免了信息损失，并且无需减少标记数量。与传统的变换器模型相比，VAMBA在单个GPU上能够编码超过1024帧的视频，显著提高了训练和推理的速度，并减少了GPU内存使用。实验结果表明，VAMBA在长视频理解基准测试中比之前的高效视频模型提高了4.3%的准确率，同时在多种视频理解任务中保持了强大的性能。', title='高效视频理解的新突破：VAMBA模型'))
[17.03.2025 16:14] Querying the API.
[17.03.2025 16:14] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

We introduce SmolDocling, an ultra-compact vision-language model targeting end-to-end document conversion. Our model comprehensively processes entire pages by generating DocTags, a new universal markup format that captures all page elements in their full context with location. Unlike existing approaches that rely on large foundational models, or ensemble solutions that rely on handcrafted pipelines of multiple specialized models, SmolDocling offers an end-to-end conversion for accurately capturing content, structure and spatial location of document elements in a 256M parameters vision-language model. SmolDocling exhibits robust performance in correctly reproducing document features such as code listings, tables, equations, charts, lists, and more across a diverse range of document types including business documents, academic papers, technical reports, patents, and forms -- significantly extending beyond the commonly observed focus on scientific papers. Additionally, we contribute novel publicly sourced datasets for charts, tables, equations, and code recognition. Experimental results demonstrate that SmolDocling competes with other Vision Language Models that are up to 27 times larger in size, while reducing computational requirements substantially. The model is currently available, datasets will be publicly available soon.
[17.03.2025 16:14] Response: {
  "desc": "SmolDocling - это компактная модель обработки документов, сочетающая зрение и язык. Она генерирует универсальную разметку DocTags, захватывающую все элементы страницы с их расположением. Модель показывает надежную производительность в воспроизведении различных особенностей документов, включая листинги кода, таблицы, уравнения и диаграммы. SmolDocling конкурирует с моделями в 27 раз большего размера, существенно снижая вычислительные требования.",
  "emoji": "📄",
  "title": "SmolDocling: компактная модель для комплексной обработки документов"
}
[17.03.2025 16:14] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"We introduce SmolDocling, an ultra-compact vision-language model targeting end-to-end document conversion. Our model comprehensively processes entire pages by generating DocTags, a new universal markup format that captures all page elements in their full context with location. Unlike existing approaches that rely on large foundational models, or ensemble solutions that rely on handcrafted pipelines of multiple specialized models, SmolDocling offers an end-to-end conversion for accurately capturing content, structure and spatial location of document elements in a 256M parameters vision-language model. SmolDocling exhibits robust performance in correctly reproducing document features such as code listings, tables, equations, charts, lists, and more across a diverse range of document types including business documents, academic papers, technical reports, patents, and forms -- significantly extending beyond the commonly observed focus on scientific papers. Additionally, we contribute novel publicly sourced datasets for charts, tables, equations, and code recognition. Experimental results demonstrate that SmolDocling competes with other Vision Language Models that are up to 27 times larger in size, while reducing computational requirements substantially. The model is currently available, datasets will be publicly available soon."

[17.03.2025 16:14] Response: ```python
['DATASET', 'CV', 'MULTIMODAL', 'SMALL_MODELS']
```
[17.03.2025 16:14] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"We introduce SmolDocling, an ultra-compact vision-language model targeting end-to-end document conversion. Our model comprehensively processes entire pages by generating DocTags, a new universal markup format that captures all page elements in their full context with location. Unlike existing approaches that rely on large foundational models, or ensemble solutions that rely on handcrafted pipelines of multiple specialized models, SmolDocling offers an end-to-end conversion for accurately capturing content, structure and spatial location of document elements in a 256M parameters vision-language model. SmolDocling exhibits robust performance in correctly reproducing document features such as code listings, tables, equations, charts, lists, and more across a diverse range of document types including business documents, academic papers, technical reports, patents, and forms -- significantly extending beyond the commonly observed focus on scientific papers. Additionally, we contribute novel publicly sourced datasets for charts, tables, equations, and code recognition. Experimental results demonstrate that SmolDocling competes with other Vision Language Models that are up to 27 times larger in size, while reducing computational requirements substantially. The model is currently available, datasets will be publicly available soon."

[17.03.2025 16:14] Response: ```python
["OPEN_SOURCE", "SCIENCE"]
```
[17.03.2025 16:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"SmolDocling is a compact vision-language model designed for end-to-end document conversion. It generates DocTags, a universal markup format that captures the content, structure, and spatial location of document elements. Unlike traditional methods that use large models or complex pipelines, SmolDocling achieves high accuracy with only 256 million parameters. It performs well across various document types, including business and academic papers, and introduces new datasets for recognizing charts, tables, equations, and code.","title":"SmolDocling: Compact and Powerful Document Conversion"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='SmolDocling is a compact vision-language model designed for end-to-end document conversion. It generates DocTags, a universal markup format that captures the content, structure, and spatial location of document elements. Unlike traditional methods that use large models or complex pipelines, SmolDocling achieves high accuracy with only 256 million parameters. It performs well across various document types, including business and academic papers, and introduces new datasets for recognizing charts, tables, equations, and code.', title='SmolDocling: Compact and Powerful Document Conversion'))
[17.03.2025 16:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"我们介绍了SmolDocling，这是一种超紧凑的视觉-语言模型，旨在实现端到端的文档转换。该模型通过生成DocTags，一种新的通用标记格式，全面处理整个页面，捕捉所有页面元素的完整上下文和位置。与依赖大型基础模型或多个专用模型的手工管道的现有方法不同，SmolDocling提供了一种端到端的转换，准确捕捉文档元素的内容、结构和空间位置。实验结果表明，SmolDocling在性能上与其他高达27倍大小的视觉语言模型竞争，同时显著降低了计算需求。","title":"SmolDocling：高效文档转换的新选择"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='我们介绍了SmolDocling，这是一种超紧凑的视觉-语言模型，旨在实现端到端的文档转换。该模型通过生成DocTags，一种新的通用标记格式，全面处理整个页面，捕捉所有页面元素的完整上下文和位置。与依赖大型基础模型或多个专用模型的手工管道的现有方法不同，SmolDocling提供了一种端到端的转换，准确捕捉文档元素的内容、结构和空间位置。实验结果表明，SmolDocling在性能上与其他高达27倍大小的视觉语言模型竞争，同时显著降低了计算需求。', title='SmolDocling：高效文档转换的新选择'))
[17.03.2025 16:14] Using data from previous issue: {"categories": ["#3d", "#optimization", "#cv", "#open_source"], "emoji": "👕", "ru": {"title": "Точная подгонка 3D-модели тела к одетому человеку с учетом плотности прилегания одежды", "desc": "Статья представляет новый метод ETCH для подгонки трехмерной модели тела к облаку точек одетого человека. E
[17.03.2025 16:14] Using data from previous issue: {"categories": ["#games", "#video", "#agents", "#open_source"], "emoji": "🎮", "ru": {"title": "Автоматическая сегментация видео для обучения ИИ-агентов сложным навыкам", "desc": "Статья представляет новый метод самообучения для сегментации длинных видео на последовательности семантически связанных н
[17.03.2025 16:14] Using data from previous issue: {"categories": ["#optimization", "#diffusion", "#agents", "#multimodal"], "emoji": "🚗", "ru": {"title": "GoalFlow: Точное планирование траекторий для автономного вождения", "desc": "GoalFlow - это новый метод автономного вождения для генерации высококачественных мультимодальных траекторий. Он решает
[17.03.2025 16:14] Querying the API.
[17.03.2025 16:14] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

We introduce TreeMeshGPT, an autoregressive Transformer designed to generate high-quality artistic meshes aligned with input point clouds. Instead of the conventional next-token prediction in autoregressive Transformer, we propose a novel Autoregressive Tree Sequencing where the next input token is retrieved from a dynamically growing tree structure that is built upon the triangle adjacency of faces within the mesh. Our sequencing enables the mesh to extend locally from the last generated triangular face at each step, and therefore reduces training difficulty and improves mesh quality. Our approach represents each triangular face with two tokens, achieving a compression rate of approximately 22% compared to the naive face tokenization. This efficient tokenization enables our model to generate highly detailed artistic meshes with strong point cloud conditioning, surpassing previous methods in both capacity and fidelity. Furthermore, our method generates mesh with strong normal orientation constraints, minimizing flipped normals commonly encountered in previous methods. Our experiments show that TreeMeshGPT enhances the mesh generation quality with refined details and normal orientation consistency.
[17.03.2025 16:14] Response: {
  "desc": "TreeMeshGPT - это авторегрессивный трансформер для генерации высококачественных художественных сеток, согласованных с входными облаками точек. Модель использует новый метод автоматической древовидной последовательности, где следующий токен ввода извлекается из динамически растущей древовидной структуры, построенной на основе смежности треугольников в сетке. Этот подход позволяет сетке локально расширяться от последней сгенерированной треугольной грани на каждом шаге, что снижает сложность обучения и улучшает качество сетки. TreeMeshGPT превосходит предыдущие методы по емкости и точности, генерируя детализированные сетки с сильным ограничением нормалей.",
  "emoji": "🌳",
  "title": "Генерация высококачественных 3D-сеток с помощью древовидной последовательности"
}
[17.03.2025 16:14] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"We introduce TreeMeshGPT, an autoregressive Transformer designed to generate high-quality artistic meshes aligned with input point clouds. Instead of the conventional next-token prediction in autoregressive Transformer, we propose a novel Autoregressive Tree Sequencing where the next input token is retrieved from a dynamically growing tree structure that is built upon the triangle adjacency of faces within the mesh. Our sequencing enables the mesh to extend locally from the last generated triangular face at each step, and therefore reduces training difficulty and improves mesh quality. Our approach represents each triangular face with two tokens, achieving a compression rate of approximately 22% compared to the naive face tokenization. This efficient tokenization enables our model to generate highly detailed artistic meshes with strong point cloud conditioning, surpassing previous methods in both capacity and fidelity. Furthermore, our method generates mesh with strong normal orientation constraints, minimizing flipped normals commonly encountered in previous methods. Our experiments show that TreeMeshGPT enhances the mesh generation quality with refined details and normal orientation consistency."

[17.03.2025 16:14] Response: ```python
['3D', 'ARCHITECTURE']
```
[17.03.2025 16:14] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"We introduce TreeMeshGPT, an autoregressive Transformer designed to generate high-quality artistic meshes aligned with input point clouds. Instead of the conventional next-token prediction in autoregressive Transformer, we propose a novel Autoregressive Tree Sequencing where the next input token is retrieved from a dynamically growing tree structure that is built upon the triangle adjacency of faces within the mesh. Our sequencing enables the mesh to extend locally from the last generated triangular face at each step, and therefore reduces training difficulty and improves mesh quality. Our approach represents each triangular face with two tokens, achieving a compression rate of approximately 22% compared to the naive face tokenization. This efficient tokenization enables our model to generate highly detailed artistic meshes with strong point cloud conditioning, surpassing previous methods in both capacity and fidelity. Furthermore, our method generates mesh with strong normal orientation constraints, minimizing flipped normals commonly encountered in previous methods. Our experiments show that TreeMeshGPT enhances the mesh generation quality with refined details and normal orientation consistency."

[17.03.2025 16:14] Response: ```python
["GAMES", "OPTIMIZATION"]
```
[17.03.2025 16:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"TreeMeshGPT is a novel autoregressive Transformer model that generates artistic meshes from point clouds. It introduces a unique Autoregressive Tree Sequencing method, which builds a dynamic tree structure based on the adjacency of triangular faces, allowing for local mesh extension during generation. This approach not only simplifies the training process but also enhances the quality of the generated meshes by achieving a 22% compression rate through efficient tokenization of triangular faces. Additionally, TreeMeshGPT ensures better normal orientation, reducing the occurrence of flipped normals and improving overall mesh fidelity and detail compared to previous techniques.","title":"Revolutionizing Mesh Generation with TreeMeshGPT"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='TreeMeshGPT is a novel autoregressive Transformer model that generates artistic meshes from point clouds. It introduces a unique Autoregressive Tree Sequencing method, which builds a dynamic tree structure based on the adjacency of triangular faces, allowing for local mesh extension during generation. This approach not only simplifies the training process but also enhances the quality of the generated meshes by achieving a 22% compression rate through efficient tokenization of triangular faces. Additionally, TreeMeshGPT ensures better normal orientation, reducing the occurrence of flipped normals and improving overall mesh fidelity and detail compared to previous techniques.', title='Revolutionizing Mesh Generation with TreeMeshGPT'))
[17.03.2025 16:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"我们介绍了TreeMeshGPT，这是一种自回归Transformer，旨在生成与输入点云对齐的高质量艺术网格。与传统的自回归Transformer的下一个标记预测不同，我们提出了一种新颖的自回归树序列化方法，通过动态增长的树结构来检索下一个输入标记。我们的序列化方法使得网格能够在每一步从最后生成的三角面局部扩展，从而降低训练难度并提高网格质量。此外，我们的模型通过将每个三角面表示为两个标记，实现了约22%的压缩率，生成的网格在细节和法线方向一致性方面优于以往方法。","title":"TreeMeshGPT：高质量艺术网格生成的新方法"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='我们介绍了TreeMeshGPT，这是一种自回归Transformer，旨在生成与输入点云对齐的高质量艺术网格。与传统的自回归Transformer的下一个标记预测不同，我们提出了一种新颖的自回归树序列化方法，通过动态增长的树结构来检索下一个输入标记。我们的序列化方法使得网格能够在每一步从最后生成的三角面局部扩展，从而降低训练难度并提高网格质量。此外，我们的模型通过将每个三角面表示为两个标记，实现了约22%的压缩率，生成的网格在细节和法线方向一致性方面优于以往方法。', title='TreeMeshGPT：高质量艺术网格生成的新方法'))
[17.03.2025 16:14] Querying the API.
[17.03.2025 16:14] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

This work presents a first evaluation of two state-of-the-art Large Reasoning Models (LRMs), OpenAI's o3-mini and DeepSeek R1, on analogical reasoning, focusing on well-established nonverbal human IQ tests based on Raven's progressive matrices. We benchmark with the I-RAVEN dataset and its more difficult extension, I-RAVEN-X, which tests the ability to generalize to longer reasoning rules and ranges of the attribute values. To assess the influence of visual uncertainties on these nonverbal analogical reasoning tests, we extend the I-RAVEN-X dataset, which otherwise assumes an oracle perception. We adopt a two-fold strategy to simulate this imperfect visual perception: 1) we introduce confounding attributes which, being sampled at random, do not contribute to the prediction of the correct answer of the puzzles and 2) smoothen the distributions of the input attributes' values. We observe a sharp decline in OpenAI's o3-mini task accuracy, dropping from 86.6% on the original I-RAVEN to just 17.0% -- approaching random chance -- on the more challenging I-RAVEN-X, which increases input length and range and emulates perceptual uncertainty. This drop occurred despite spending 3.4x more reasoning tokens. A similar trend is also observed for DeepSeek R1: from 80.6% to 23.2%. On the other hand, a neuro-symbolic probabilistic abductive model, ARLC, that achieves state-of-the-art performances on I-RAVEN, can robustly reason under all these out-of-distribution tests, maintaining strong accuracy with only a modest reduction from 98.6% to 88.0%. Our code is available at https://github.com/IBM/raven-large-language-models.
[17.03.2025 16:14] Response: {
  "desc": "Эта работа представляет первую оценку двух современных моделей крупномасштабного рассуждения (LRM) - o3-mini от OpenAI и DeepSeek R1 - в задачах аналогического мышления на основе невербальных тестов IQ, использующих прогрессивные матрицы Равена. Исследователи провели бенчмаркинг на наборах данных I-RAVEN и его более сложном расширении I-RAVEN-X, которые проверяют способность обобщать более длинные правила рассуждения и диапазоны значений атрибутов. Для оценки влияния визуальной неопределенности авторы расширили набор данных I-RAVEN-X, введя случайные атрибуты и сглаживание распределений значений входных атрибутов. Результаты показали значительное снижение точности моделей LRM на более сложных тестах, в то время как нейро-символическая вероятностная модель ARLC продемонстрировала устойчивость к этим изменениям.",
  "emoji": "🧠",
  "title": "Крупные модели рассуждений уступают нейро-символическим подходам в аналогическом мышлении"
}
[17.03.2025 16:14] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"This work presents a first evaluation of two state-of-the-art Large Reasoning Models (LRMs), OpenAI's o3-mini and DeepSeek R1, on analogical reasoning, focusing on well-established nonverbal human IQ tests based on Raven's progressive matrices. We benchmark with the I-RAVEN dataset and its more difficult extension, I-RAVEN-X, which tests the ability to generalize to longer reasoning rules and ranges of the attribute values. To assess the influence of visual uncertainties on these nonverbal analogical reasoning tests, we extend the I-RAVEN-X dataset, which otherwise assumes an oracle perception. We adopt a two-fold strategy to simulate this imperfect visual perception: 1) we introduce confounding attributes which, being sampled at random, do not contribute to the prediction of the correct answer of the puzzles and 2) smoothen the distributions of the input attributes' values. We observe a sharp decline in OpenAI's o3-mini task accuracy, dropping from 86.6% on the original I-RAVEN to just 17.0% -- approaching random chance -- on the more challenging I-RAVEN-X, which increases input length and range and emulates perceptual uncertainty. This drop occurred despite spending 3.4x more reasoning tokens. A similar trend is also observed for DeepSeek R1: from 80.6% to 23.2%. On the other hand, a neuro-symbolic probabilistic abductive model, ARLC, that achieves state-of-the-art performances on I-RAVEN, can robustly reason under all these out-of-distribution tests, maintaining strong accuracy with only a modest reduction from 98.6% to 88.0%. Our code is available at https://github.com/IBM/raven-large-language-models."

[17.03.2025 16:14] Response: ```python
["BENCHMARK", "DATASET", "CV"]
```
[17.03.2025 16:14] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"This work presents a first evaluation of two state-of-the-art Large Reasoning Models (LRMs), OpenAI's o3-mini and DeepSeek R1, on analogical reasoning, focusing on well-established nonverbal human IQ tests based on Raven's progressive matrices. We benchmark with the I-RAVEN dataset and its more difficult extension, I-RAVEN-X, which tests the ability to generalize to longer reasoning rules and ranges of the attribute values. To assess the influence of visual uncertainties on these nonverbal analogical reasoning tests, we extend the I-RAVEN-X dataset, which otherwise assumes an oracle perception. We adopt a two-fold strategy to simulate this imperfect visual perception: 1) we introduce confounding attributes which, being sampled at random, do not contribute to the prediction of the correct answer of the puzzles and 2) smoothen the distributions of the input attributes' values. We observe a sharp decline in OpenAI's o3-mini task accuracy, dropping from 86.6% on the original I-RAVEN to just 17.0% -- approaching random chance -- on the more challenging I-RAVEN-X, which increases input length and range and emulates perceptual uncertainty. This drop occurred despite spending 3.4x more reasoning tokens. A similar trend is also observed for DeepSeek R1: from 80.6% to 23.2%. On the other hand, a neuro-symbolic probabilistic abductive model, ARLC, that achieves state-of-the-art performances on I-RAVEN, can robustly reason under all these out-of-distribution tests, maintaining strong accuracy with only a modest reduction from 98.6% to 88.0%. Our code is available at https://github.com/IBM/raven-large-language-models."

[17.03.2025 16:14] Response: ```python
["REASONING"]
```
[17.03.2025 16:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper evaluates two advanced Large Reasoning Models (LRMs), OpenAI\'s o3-mini and DeepSeek R1, on their ability to perform analogical reasoning using Raven\'s progressive matrices. The study uses the I-RAVEN dataset and its more challenging version, I-RAVEN-X, to test the models\' generalization capabilities under visual uncertainties. The results show a significant drop in accuracy for both LRMs when faced with the more complex tasks, indicating their struggle with longer reasoning rules and perceptual noise. In contrast, a neuro-symbolic model, ARLC, demonstrates robust performance, maintaining high accuracy even under challenging conditions.","title":"Evaluating Reasoning Under Uncertainty in Large Models"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="This paper evaluates two advanced Large Reasoning Models (LRMs), OpenAI's o3-mini and DeepSeek R1, on their ability to perform analogical reasoning using Raven's progressive matrices. The study uses the I-RAVEN dataset and its more challenging version, I-RAVEN-X, to test the models' generalization capabilities under visual uncertainties. The results show a significant drop in accuracy for both LRMs when faced with the more complex tasks, indicating their struggle with longer reasoning rules and perceptual noise. In contrast, a neuro-symbolic model, ARLC, demonstrates robust performance, maintaining high accuracy even under challenging conditions.", title='Evaluating Reasoning Under Uncertainty in Large Models'))
[17.03.2025 16:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本文首次评估了两种先进的大型推理模型（LRMs），OpenAI的o3-mini和DeepSeek R1，在类比推理方面的表现，重点关注基于Raven渐进矩阵的非语言人类智商测试。我们使用I-RAVEN数据集及其更难的扩展版本I-RAVEN-X进行基准测试，后者测试模型对更长推理规则和属性值范围的泛化能力。为了评估视觉不确定性对这些非语言类比推理测试的影响，我们扩展了I-RAVEN-X数据集，并采用了两种策略来模拟不完美的视觉感知。结果显示，OpenAI的o3-mini在I-RAVEN-X上的任务准确率大幅下降，从86.6%降至仅17.0%，而ARLC模型在所有这些分布外测试中保持了强大的准确性。","title":"大型推理模型在类比推理中的挑战与机遇"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本文首次评估了两种先进的大型推理模型（LRMs），OpenAI的o3-mini和DeepSeek R1，在类比推理方面的表现，重点关注基于Raven渐进矩阵的非语言人类智商测试。我们使用I-RAVEN数据集及其更难的扩展版本I-RAVEN-X进行基准测试，后者测试模型对更长推理规则和属性值范围的泛化能力。为了评估视觉不确定性对这些非语言类比推理测试的影响，我们扩展了I-RAVEN-X数据集，并采用了两种策略来模拟不完美的视觉感知。结果显示，OpenAI的o3-mini在I-RAVEN-X上的任务准确率大幅下降，从86.6%降至仅17.0%，而ARLC模型在所有这些分布外测试中保持了强大的准确性。', title='大型推理模型在类比推理中的挑战与机遇'))
[17.03.2025 16:14] Using data from previous issue: {"categories": ["#synthetic", "#optimization", "#dataset", "#cv", "#3d"], "emoji": "🔍", "ru": {"title": "MaRI: Революция в поиске 3D-материалов", "desc": "Статья представляет MaRI - фреймворк для улучшения поиска реалистичных 3D-материалов. Он создает общее пространство признаков для синтетических и
[17.03.2025 16:14] Using data from previous issue: {"categories": ["#multilingual", "#machine_translation", "#open_source", "#multimodal", "#low_resource"], "emoji": "🗣️", "ru": {"title": "Расширение языковых моделей на речевую модальность", "desc": "В этой статье описывается расширение возможностей большой языковой модели (LLM) для работы с речью п
[17.03.2025 16:14] Using data from previous issue: {"categories": ["#training", "#dataset", "#ethics", "#data"], "emoji": "🧠", "ru": {"title": "Групповое разобучение без потери устойчивости", "desc": "Статья посвящена проблеме группового машинного разобучения, когда данные для удаления из модели неравномерно распределены между группами. Авторы предл
[17.03.2025 16:14] Loading Chinese text from previous data.
[17.03.2025 16:14] Renaming data file.
[17.03.2025 16:14] Renaming previous data. hf_papers.json to ./d/2025-03-17.json
[17.03.2025 16:14] Saving new data file.
[17.03.2025 16:14] Generating page.
[17.03.2025 16:14] Renaming previous page.
[17.03.2025 16:14] Renaming previous data. index.html to ./d/2025-03-17.html
[17.03.2025 16:14] [Experimental] Generating Chinese page for reading.
[17.03.2025 16:14] Chinese vocab [{'word': '讨论', 'pinyin': 'tǎo lùn', 'trans': 'discuss'}, {'word': '名为', 'pinyin': 'míng wéi', 'trans': 'named'}, {'word': '视频', 'pinyin': 'shì pín', 'trans': 'video'}, {'word': '重新', 'pinyin': 'chóng xīn', 'trans': 'again'}, {'word': '渲染', 'pinyin': 'xuàn rán', 'trans': 'render'}, {'word': '框架', 'pinyin': 'kuàng jià', 'trans': 'framework'}, {'word': '摄像机', 'pinyin': 'shè xiàng jī', 'trans': 'camera'}, {'word': '轨迹', 'pinyin': 'guǐ jì', 'trans': 'trajectory'}, {'word': '同时', 'pinyin': 'tóng shí', 'trans': 'simultaneously'}, {'word': '保持', 'pinyin': 'bǎo chí', 'trans': 'maintain'}, {'word': '多帧', 'pinyin': 'duō zhēn', 'trans': 'multi-frame'}, {'word': '外观', 'pinyin': 'wài guǎn', 'trans': 'appearance'}, {'word': '动态', 'pinyin': 'dòng tài', 'trans': 'dynamic'}, {'word': '同步', 'pinyin': 'tóng bù', 'trans': 'synchronization'}, {'word': '方法', 'pinyin': 'fāng fǎ', 'trans': 'method'}, {'word': '利用', 'pinyin': 'lì yòng', 'trans': 'utilize'}, {'word': '预训练', 'pinyin': 'yù xùn liàn', 'trans': 'pre-trained'}, {'word': '文本', 'pinyin': 'wén běn', 'trans': 'text'}, {'word': '生成', 'pinyin': 'shēng chéng', 'trans': 'generate'}, {'word': '能力', 'pinyin': 'néng lì', 'trans': 'ability'}, {'word': '构建', 'pinyin': 'gòu jiàn', 'trans': 'construct'}, {'word': '数据集', 'pinyin': 'shù jù jí', 'trans': 'dataset'}, {'word': '实验', 'pinyin': 'shí yàn', 'trans': 'experiment'}, {'word': '结果', 'pinyin': 'jié guǒ', 'trans': 'result'}, {'word': '显示', 'pinyin': 'xiǎn shì', 'trans': 'display'}, {'word': '稳定', 'pinyin': 'wěn dìng', 'trans': 'stable'}, {'word': '超分辨率', 'pinyin': 'chāo fēn biàn lǜ', 'trans': 'super-resolution'}, {'word': '扩展', 'pinyin': 'kuò zhǎn', 'trans': 'extend'}, {'word': '表现', 'pinyin': 'biǎo xiàn', 'trans': 'performance'}, {'word': '出色', 'pinyin': 'chū sè', 'trans': 'outstanding'}]
[17.03.2025 16:14] Renaming previous Chinese page.
[17.03.2025 16:14] Renaming previous data. zh.html to ./d/2025-03-16_zh_reading_task.html
[17.03.2025 16:14] Writing Chinese reading task.
[17.03.2025 16:14] Writing result.
[17.03.2025 16:14] Renaming log file.
[17.03.2025 16:14] Renaming previous data. log.txt to ./logs/2025-03-17_last_log.txt
