[18.10.2024 11:41] Read previous papers.
[18.10.2024 11:41] Get feed.
[18.10.2024 11:41] Get page data from previous paper. URL: https://huggingface.co/papers/2410.13754
[18.10.2024 11:41] Get page data from previous paper. URL: https://huggingface.co/papers/2410.13720
[18.10.2024 11:41] Get page data from previous paper. URL: https://huggingface.co/papers/2410.13824
[18.10.2024 11:41] Extract page data from URL. URL: https://huggingface.co/papers/2410.13757
[18.10.2024 11:41] Get page data from previous paper. URL: https://huggingface.co/papers/2410.13848
[18.10.2024 11:41] Get page data from previous paper. URL: https://huggingface.co/papers/2410.13085
[18.10.2024 11:41] Get page data from previous paper. URL: https://huggingface.co/papers/2410.13841
[18.10.2024 11:41] Get page data from previous paper. URL: https://huggingface.co/papers/2410.13785
[18.10.2024 11:41] Get page data from previous paper. URL: https://huggingface.co/papers/2410.11842
[18.10.2024 11:41] Get page data from previous paper. URL: https://huggingface.co/papers/2410.13830
[18.10.2024 11:41] Get page data from previous paper. URL: https://huggingface.co/papers/2410.13832
[18.10.2024 11:41] Get page data from previous paper. URL: https://huggingface.co/papers/2410.13852
[18.10.2024 11:41] Get page data from previous paper. URL: https://huggingface.co/papers/2410.09426
[18.10.2024 11:41] Get page data from previous paper. URL: https://huggingface.co/papers/2410.09019
[18.10.2024 11:41] Get page data from previous paper. URL: https://huggingface.co/papers/2410.13198
[18.10.2024 11:41] Get page data from previous paper. URL: https://huggingface.co/papers/2410.13618
[18.10.2024 11:41] Get page data from previous paper. URL: https://huggingface.co/papers/2410.13804
[18.10.2024 11:41] Get page data from previous paper. URL: https://huggingface.co/papers/2410.13060
[18.10.2024 11:41] Extract page data from URL. URL: https://huggingface.co/papers/2410.12781
[18.10.2024 11:41] Extract page data from URL. URL: https://huggingface.co/papers/2410.13360
[18.10.2024 11:41] Get page data from previous paper. URL: https://huggingface.co/papers/2410.12957
[18.10.2024 11:41] Get page data from previous paper. URL: https://huggingface.co/papers/2410.13334
[18.10.2024 11:41] Get page data from previous paper. URL: https://huggingface.co/papers/2410.13293
[18.10.2024 11:41] Extract page data from URL. URL: https://huggingface.co/papers/2410.13859
[18.10.2024 11:41] Extract page data from URL. URL: https://huggingface.co/papers/2410.09347
[18.10.2024 11:41] Extract page data from URL. URL: https://huggingface.co/papers/2410.13854
[18.10.2024 11:41] Extract page data from URL. URL: https://huggingface.co/papers/2410.10210
[18.10.2024 11:41] Extract page data from URL. URL: https://huggingface.co/papers/2410.12183
[18.10.2024 11:41] ********************************************************************************
[18.10.2024 11:41] Abstract 0. Perceiving and generating diverse modalities are crucial for AI models to effectively learn from and engage with real-world signals, necessitating reliable evaluations for their development. We identify two major issues in current evaluations: (1) inconsistent standards, shaped by different communit...
[18.10.2024 11:41] ********************************************************************************
[18.10.2024 11:41] Abstract 1. We present Movie Gen, a cast of foundation models that generates high-quality, 1080p HD videos with different aspect ratios and synchronized audio. We also show additional capabilities such as precise instruction-based video editing and generation of personalized videos based on a user's image. Our ...
[18.10.2024 11:41] ********************************************************************************
[18.10.2024 11:41] Abstract 2. Text-rich visual understanding-the ability to process environments where dense textual content is integrated with visuals-is crucial for multimodal large language models (MLLMs) to interact effectively with structured environments. To enhance this capability, we propose synthesizing general multimod...
[18.10.2024 11:41] ********************************************************************************
[18.10.2024 11:41] Abstract 3. Current mobile assistants are limited by dependence on system APIs or struggle with complex user instructions and diverse interfaces due to restricted comprehension and decision-making abilities. To address these challenges, we propose MobA, a novel Mobile phone Agent powered by multimodal large lan...
[18.10.2024 11:41] ********************************************************************************
[18.10.2024 11:41] Abstract 4. In this paper, we introduce Janus, an autoregressive framework that unifies multimodal understanding and generation. Prior research often relies on a single visual encoder for both tasks, such as Chameleon. However, due to the differing levels of information granularity required by multimodal unders...
[18.10.2024 11:41] ********************************************************************************
[18.10.2024 11:41] Abstract 5. Artificial Intelligence (AI) has demonstrated significant potential in healthcare, particularly in disease diagnosis and treatment planning. Recent progress in Medical Large Vision-Language Models (Med-LVLMs) has opened up new possibilities for interactive diagnostic tools. However, these models oft...
[18.10.2024 11:41] ********************************************************************************
[18.10.2024 11:41] Abstract 6. Post-training has emerged as a crucial paradigm for adapting large-scale pre-trained models to various tasks, whose effects are fully reflected by delta parameters (i.e., the disparity between post-trained and pre-trained parameters). While numerous studies have explored delta parameter properties v...
[18.10.2024 11:41] ********************************************************************************
[18.10.2024 11:41] Abstract 7. Alignment of large language models (LLMs) involves training models on preference-contrastive output pairs to adjust their responses according to human preferences. To obtain such contrastive pairs, traditional methods like RLHF and RLAIF rely on limited contrasting patterns, such as varying model va...
[18.10.2024 11:41] ********************************************************************************
[18.10.2024 11:41] Abstract 8. In this work, we upgrade the multi-head attention mechanism, the core of the Transformer model, to improve efficiency while maintaining or surpassing the previous accuracy level. We show that multi-head attention can be expressed in the summation form. Drawing on the insight that not all attention h...
[18.10.2024 11:41] ********************************************************************************
[18.10.2024 11:41] Abstract 9. Recent advances in customized video generation have enabled users to create videos tailored to both specific subjects and motion trajectories. However, existing methods often require complicated test-time fine-tuning and struggle with balancing subject learning and motion control, limiting their rea...
[18.10.2024 11:41] ********************************************************************************
[18.10.2024 11:41] Abstract 10. Panoramic image stitching provides a unified, wide-angle view of a scene that extends beyond the camera's field of view. Stitching frames of a panning video into a panoramic photograph is a well-understood problem for stationary scenes, but when objects are moving, a still panorama cannot capture th...
[18.10.2024 11:41] ********************************************************************************
[18.10.2024 11:41] Abstract 11. Multi-turn interactions between large language models (LLMs) and users naturally include implicit feedback signals. If an LLM responds in an unexpected way to an instruction, the user is likely to signal it by rephrasing the request, expressing frustration, or pivoting to an alternative task. Such s...
[18.10.2024 11:41] ********************************************************************************
[18.10.2024 11:41] Abstract 12. Recently, quantization has been widely used for the compression and acceleration of large language models~(LLMs). Due to the outliers in LLMs, it is crucial to flatten weights and activations to minimize quantization error with the equally spaced quantization points. Prior research explores various ...
[18.10.2024 11:41] ********************************************************************************
[18.10.2024 11:41] Abstract 13. Language models (LMs) have demonstrated expert-level reasoning and recall abilities in medicine. However, computational costs and privacy concerns are mounting barriers to wide-scale implementation. We introduce a parsimonious adaptation of phi-3-mini, MedMobile, a 3.8 billion parameter LM capable o...
[18.10.2024 11:41] ********************************************************************************
[18.10.2024 11:41] Abstract 14. Generative Error Correction (GEC) has emerged as a powerful post-processing method to enhance the performance of Automatic Speech Recognition (ASR) systems. However, we show that GEC models struggle to generalize beyond the specific types of errors encountered during training, limiting their ability...
[18.10.2024 11:41] ********************************************************************************
[18.10.2024 11:41] Abstract 15. The rapid growth of model scale has necessitated substantial computational resources for fine-tuning. Existing approach such as Low-Rank Adaptation (LoRA) has sought to address the problem of handling the large updated parameters in full fine-tuning. However, LoRA utilize random initialization and o...
[18.10.2024 11:41] ********************************************************************************
[18.10.2024 11:41] Abstract 16. Evaluating large language models (LLMs) is costly: it requires the generation and examination of LLM outputs on a large-scale benchmark of various tasks. This paper investigates how to efficiently reduce the tasks used to benchmark LLMs without affecting the evaluation quality. Our study reveals tha...
[18.10.2024 11:41] ********************************************************************************
[18.10.2024 11:41] Abstract 17. The pervasiveness of proprietary language models has raised privacy concerns for users' sensitive data, emphasizing the need for private inference (PI), where inference is performed directly on encrypted inputs. However, current PI methods face prohibitively higher communication and latency overhead...
[18.10.2024 11:41] ********************************************************************************
[18.10.2024 11:41] Abstract 18. We propose Long-LRM, a generalizable 3D Gaussian reconstruction model that is capable of reconstructing a large scene from a long sequence of input images. Specifically, our model can process 32 source images at 960x540 resolution within only 1.3 seconds on a single A100 80G GPU. Our architecture fe...
[18.10.2024 11:41] ********************************************************************************
[18.10.2024 11:41] Abstract 19. The development of large language models (LLMs) has significantly enhanced the capabilities of multimodal LLMs (MLLMs) as general assistants. However, lack of user-specific knowledge still restricts their application in human's daily life. In this paper, we introduce the Retrieval Augmented Personal...
[18.10.2024 11:41] ********************************************************************************
[18.10.2024 11:41] Abstract 20. Generating music that aligns with the visual content of a video has been a challenging task, as it requires a deep understanding of visual semantics and involves generating music whose melody, rhythm, and dynamics harmonize with the visual narratives. This paper presents MuVi, a novel framework that...
[18.10.2024 11:41] ********************************************************************************
[18.10.2024 11:41] Abstract 21. Although large language models (LLMs) demonstrate impressive proficiency in various tasks, they present potential safety risks, such as `jailbreaks', where malicious inputs can coerce LLMs into generating harmful content. To address these issues, many LLM developers have implemented various safety m...
[18.10.2024 11:41] ********************************************************************************
[18.10.2024 11:41] Abstract 22. Many students struggle with math word problems (MWPs), often finding it difficult to identify key information and select the appropriate mathematical operations.Schema-based instruction (SBI) is an evidence-based strategy that helps students categorize problems based on their structure, improving pr...
[18.10.2024 11:41] ********************************************************************************
[18.10.2024 11:41] Abstract 23. Despite the significant progress in multimodal large language models (MLLMs), their high computational cost remains a barrier to real-world deployment. Inspired by the mixture of depths (MoDs) in natural language processing, we aim to address this limitation from the perspective of ``activated token...
[18.10.2024 11:41] ********************************************************************************
[18.10.2024 11:41] Abstract 24. Classifier-Free Guidance (CFG) is a critical technique for enhancing the sample quality of visual generative models. However, in autoregressive (AR) multi-modal generation, CFG introduces design inconsistencies between language and visual content, contradicting the design philosophy of unifying diff...
[18.10.2024 11:41] ********************************************************************************
[18.10.2024 11:41] Abstract 25. As the capabilities of Multimodal Large Language Models (MLLMs) continue to improve, the need for higher-order capability evaluation of MLLMs is increasing. However, there is a lack of work evaluating MLLM for higher-order perception and understanding of Chinese visual content. To fill the gap, we i...
[18.10.2024 11:41] ********************************************************************************
[18.10.2024 11:41] Abstract 26. As large language models rapidly evolve to support longer context, there is a notable disparity in their capability to generate output at greater lengths. Recent study suggests that the primary cause for this imbalance may arise from the lack of data with long-output during alignment training. In li...
[18.10.2024 11:41] ********************************************************************************
[18.10.2024 11:41] Abstract 27. Vision-language foundation models (such as CLIP) have recently shown their power in transfer learning, owing to large-scale image-text pre-training. However, target domain data in the downstream tasks can be highly different from the pre-training phase, which makes it hard for such a single model to...
[18.10.2024 11:41] Read previous papers.
[18.10.2024 11:41] Generating reviews via LLM API.
[18.10.2024 11:41] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç MixEval-X - –ø–µ—Ä–≤—ã–π –º–Ω–æ–≥–æ–º–æ–¥–∞–ª—å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π –ò–ò –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –Ω–æ–≤—ã–µ –º–µ—Ç–æ–¥—ã –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ä–µ–ø—Ä–µ–∑–µ–Ω—Ç–∞—Ç–∏–≤–Ω—ã—Ö –Ω–∞–±–æ—Ä–æ–≤ —Ç–µ—Å—Ç–æ–≤, –æ—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏—Ö —Ä–∞–∑–ª–∏—á–Ω—ã–µ –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–∏ –≤–≤–æ–¥–∞ –∏ –≤—ã–≤–æ–¥–∞. –ë–µ–Ω—á–º–∞—Ä–∫ —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã –Ω–µ—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–≤ –æ—Ü–µ
[18.10.2024 11:41] Using data from previous issue: {"desc": "MovieGen - —ç—Ç–æ –Ω–∞–±–æ—Ä —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –≥–µ–Ω–µ—Ä–∏—Ä—É—é—â–∏—Ö –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –≤–∏–¥–µ–æ –≤ —Ñ–æ—Ä–º–∞—Ç–µ 1080p HD —Å —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –∞—É–¥–∏–æ. –ú–æ–¥–µ–ª–∏ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—é—Ç –Ω–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å –∫–∞—á–µ—Å—Ç–≤–∞ –≤ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∑–∞–¥–∞—á–∞—Ö, –≤–∫–ª—é—á–∞—è —Å–∏–Ω—Ç–µ–∑ –≤–∏–¥–µ–æ –ø–æ —Ç–µ–∫—Å—Ç—É, –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏—é –≤–∏–¥–µ–æ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –∞—É–¥–∏–æ. –ö—Ä—É–ø–Ω–µ–π—à–∞—è –º–æ–¥–µ–ª—å –∏–º–µ
[18.10.2024 11:41] Using data from previous issue: {"desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é –ø–æ–Ω–∏–º–∞–Ω–∏—è –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Å —Ç–µ–∫—Å—Ç–æ–º –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —Å–∏–Ω—Ç–µ–∑–∏—Ä–æ–≤–∞—Ç—å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –∏–∑ –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–≤ —Å –ø–æ–º–æ—â—å—é —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –°–æ–∑–¥–∞–Ω –¥–∞—Ç–∞—Å–µ—Ç MultiUI, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π 7,3 –º–∏–ª–ª–∏–æ–Ω–∞ –æ–±—Ä–∞–∑—Ü–æ–≤
[18.10.2024 11:41] Querying the API.
[18.10.2024 11:41] Got response. {
  "desc": "MobA - —ç—Ç–æ –Ω–æ–≤—ã–π –º–æ–±–∏–ª—å–Ω—ã–π –∞–≥–µ–Ω—Ç, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–≤—É—Ö—É—Ä–æ–≤–Ω–µ–≤—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É —Å –≥–ª–æ–±–∞–ª—å–Ω—ã–º –∞–≥–µ–Ω—Ç–æ–º –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –∫–æ–º–∞–Ω–¥ –∏ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è, –∏ –ª–æ–∫–∞–ª—å–Ω—ã–º –∞–≥–µ–Ω—Ç–æ–º –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π. –°–∏—Å—Ç–µ–º–∞ –≤–∫–ª—é—á–∞–µ—Ç –º–æ–¥—É–ª—å —Ä–µ—Ñ–ª–µ–∫—Å–∏–∏ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–æ–≤—ã—Ö —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞–Ω–∏–π. MobA –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∏ —É—Ä–æ–≤–Ω—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö —É—Å–ª–æ–≤–∏—è—Ö.",
  "categories": ["#multimodal", "#rl", "#nlp", "#cv"],
  "emoji": "üì±",
  "title": "MobA: –£–º–Ω—ã–π –º–æ–±–∏–ª—å–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫ –Ω–æ–≤–æ–≥–æ –ø–æ–∫–æ–ª–µ–Ω–∏—è"
}
[18.10.2024 11:41] Get embedding for a paper via LLM API.
[18.10.2024 11:41] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Janus - –Ω–æ–≤—É—é –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –ø–æ–¥—Ö–æ–¥–æ–≤, Janus –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –æ—Ç–¥–µ–ª—å–Ω—ã–µ –≤–∏–∑—É–∞–ª—å–Ω—ã–µ —ç–Ω–∫–æ–¥–µ—Ä—ã –¥–ª—è –∑–∞–¥–∞—á –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ä–∞–±–æ—Ç—É –º–æ–¥–µ–ª–∏. –ï–¥–∏–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä
[18.10.2024 11:41] Using data from previous issue: {"desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ MMed-RAG, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∑—Ä–µ–Ω–∏—è –∏ —è–∑—ã–∫–∞ (Med-LVLMs). –°–∏—Å—Ç–µ–º–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–µ—Ö–∞–Ω–∏–∑–º –ø–æ–∏—Å–∫–∞ —Å —É—á–µ—Ç–æ–º –¥–æ–º–µ–Ω–∞, –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥ –≤—ã–±–æ—Ä–∞ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤ –∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –¥–æ–æ–±—É—á–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ RAG. –≠
[18.10.2024 11:41] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∞–Ω–∞–ª–∏–∑—É –¥–µ–ª—å—Ç–∞-–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤ –ø–æ—Å—Ç-–æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –∞–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏–∏ –†–∏–º–∞–Ω–∞ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å. –ê–≤—Ç–æ—Ä—ã –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É—é—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –¥–µ–ª—å—Ç–∞-–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –Ω–∞ —Ç—Ä–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∏—Ö –≤–ª–∏—è–Ω–∏—è –Ω–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ
[18.10.2024 11:41] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—é –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º PopAlign. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –Ω–∞ —É—Ä–æ–≤–Ω—è—Ö –ø—Ä–æ–º–ø—Ç–∞, –º–æ–¥–µ–ª–∏ –∏ –ø–∞–π–ø–ª–∞–π–Ω–∞ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –æ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è—Ö. PopAlign —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ—Å—Ç–∏ —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã—Ö –º–µ—Ç–æ–¥–æ
[18.10.2024 11:41] Using data from previous issue: {"desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –Ω–æ–≤—ã–π –º–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è Mixture-of-Head (MoH), –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –º–Ω–æ–≥–æ–≥–æ–ª–æ–≤–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è –≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞—Ö. MoH –ø–æ–∑–≤–æ–ª—è–µ—Ç –∫–∞–∂–¥–æ–º—É —Ç–æ–∫–µ–Ω—É –≤—ã–±–∏—Ä–∞—Ç—å –ø–æ–¥—Ö–æ–¥—è—â–∏–µ –≥–æ–ª–æ–≤—ã –≤–Ω–∏–º–∞–Ω–∏—è, –ø–æ–≤—ã—à–∞—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –≤—ã–≤–æ–¥–∞ –±–µ–∑ —É—â–µ—Ä–±–∞ –¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ ViT, DiT –∏ 
[18.10.2024 11:41] Using data from previous issue: {"desc": "DreamVideo-2 - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–¥–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞—é—â–∏—Ö —Ä–∞–º–æ–∫ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ —Å –∑–∞–¥–∞–Ω–Ω—ã–º –æ–±—ä–µ–∫—Ç–æ–º –∏ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–µ–π –¥–≤–∏–∂–µ–Ω–∏—è. –°–∏—Å—Ç–µ–º–∞ –≤–≤–æ–¥–∏—Ç —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –∏ –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã
[18.10.2024 11:41] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ —Å–∏–Ω—Ç–µ–∑–∞ –ø–∞–Ω–æ—Ä–∞–º–Ω–æ–≥–æ –≤–∏–¥–µ–æ –∏–∑ –æ–±—ã—á–Ω–æ–≥–æ –ø–∞–Ω–æ—Ä–∞–º–Ω–æ–≥–æ –≤–∏–¥–µ–æ, —Å–Ω—è—Ç–æ–≥–æ –≤—Ä—É—á–Ω—É—é. –ê–≤—Ç–æ—Ä—ã —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç –∑–∞–¥–∞—á—É –∫–∞–∫ –ø—Ä–æ–±–ª–µ–º—É –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ-–≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –æ–±—ä–µ–º–∞, –∏—Å–ø–æ–ª—å–∑—É—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≤–∏–¥–µ–æ. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–æ–∑–¥–∞–≤–∞—Ç—å –≤–∏–¥–µ–æ–ø–∞–Ω–æ—Ä–∞–º—ã –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã
[18.10.2024 11:41] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ ReSpect, –ø–æ–∑–≤–æ–ª—è—é—â–∏–π —è–∑—ã–∫–æ–≤—ã–º –º–æ–¥–µ–ª—è–º (LLM) —É—á–∏—Ç—å—Å—è –Ω–∞ –Ω–µ—è–≤–Ω—ã—Ö —Å–∏–≥–Ω–∞–ª–∞—Ö –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ –≤ —Ö–æ–¥–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º–∏. –ê–≤—Ç–æ—Ä—ã –ø—Ä–∏–º–µ–Ω—è—é—Ç ReSpect –≤ —Å—Ü–µ–Ω–∞—Ä–∏–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è, –≥–¥–µ –ª—é–¥–∏ –∏–Ω—Å—Ç—Ä—É–∫—Ç–∏—Ä—É—é—Ç LLM –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á –∞–±—Å—Ç—Ä–∞–∫—Ç–Ω–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è. 
[18.10.2024 11:41] Using data from previous issue: {"desc": "FlatQuant - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—é –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è, –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π –Ω–∞ —É–ª—É—á—à–µ–Ω–∏–µ —Å–≥–ª–∞–∂–µ–Ω–Ω–æ—Å—Ç–∏ –≤–µ—Å–æ–≤ –∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–π –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM). –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –∞—Ñ—Ñ–∏–Ω–Ω—ã–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ª–∏–Ω–µ–π–Ω–æ–≥–æ —Å–ª–æ—è, –∫–∞–ª–∏–±—Ä—É–µ–º—ã–µ –∑–∞ —á–∞—Å—ã —Å –ø–æ–º–æ—â—å—é –æ–±–ª–µ–≥—á–µ–Ω–Ω–æ–π —Ü–µ–ª–µ–≤–æ–π —Ñ—É–Ω–∫—Ü–∏–∏. 
[18.10.2024 11:41] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç MedMobile - –∫–æ–º–ø–∞–∫—Ç–Ω—É—é —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å –¥–ª—è –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π, —Å–ø–æ—Å–æ–±–Ω—É—é —Ä–∞–±–æ—Ç–∞—Ç—å –Ω–∞ –º–æ–±–∏–ª—å–Ω—ã—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞—Ö. –ú–æ–¥–µ–ª—å, –æ—Å–Ω–æ–≤–∞–Ω–Ω–∞—è –Ω–∞ phi-3-mini, –∏–º–µ–µ—Ç –≤—Å–µ–≥–æ 3,8 –º–∏–ª–ª–∏–∞—Ä–¥–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –Ω–æ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –≤–ø–µ—á–∞—Ç–ª—è—é—â–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ —Ç–µ—Å—Ç–µ MedQA (USMLE), –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è –ø—Ä–æ—Ö–æ–¥–Ω–æ–π 
[18.10.2024 11:41] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ DARAG –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–π –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏ –æ—à–∏–±–æ–∫ (GEC) –≤ —Å–∏—Å—Ç–µ–º–∞—Ö –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ—á–∏ (ASR). DARAG –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ, —Å–æ–∑–¥–∞–Ω–Ω—ã–µ —Å –ø–æ–º–æ—â—å—é —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∏ —Å–∏—Å—Ç–µ–º text-to-speech, –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –æ–±—É—á–∞—é—â–µ–≥–æ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö. –ú–µ—Ç–æ–¥ 
[18.10.2024 11:41] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (PEFT) –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º LoLDU. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç LDU-—Ä–∞–∑–ª–æ–∂–µ–Ω–∏–µ –¥–ª—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –º–∞—Ç—Ä–∏—Ü –Ω–∏–∑–∫–æ–≥–æ —Ä–∞–Ω–≥–∞, —á—Ç–æ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –±–æ–ª–µ–µ –±—ã—Å—Ç—Ä—É—é —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å –∏ –æ—Ä—Ç–æ–≥–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å. LoLDU –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–æ–∫—Ä–∞—â–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—É—á–∞–µ–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ
[18.10.2024 11:41] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –º–µ—Ç–æ–¥—ã —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∑–∞–¥–∞—á –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –±–µ–∑ —É—â–µ—Ä–±–∞ –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–∞ –æ—Ü–µ–Ω–∫–∏. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç—Ä–∏–∫—É –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø–µ—Ä–µ–Ω–æ—Å–∏–º–æ—Å—Ç–∏ –º–µ–∂–¥—É –∑–∞–¥–∞—á–∞–º–∏ —Å –ø–æ–º–æ—â—å—é –æ–±—É—á–µ–Ω–∏—è –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ (ICL). –ê–Ω–∞–ª–∏–∑–∏—Ä—É—è –ø–æ–ø–∞—Ä–Ω—É—é –ø–µ—Ä–µ–Ω–æ—Å–∏–º–æ—Å—Ç—å, –º–æ–∂–Ω–æ —Å–æ–∫—Ä–∞—Ç
[18.10.2024 11:41] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ä–æ–ª–∏ –Ω–µ–ª–∏–Ω–µ–π–Ω—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π –≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö –¥–µ–∫–æ–¥–µ—Ä-—Ç–æ–ª—å–∫–æ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç AERO - —á–µ—Ç—ã—Ä–µ—Ö—ç—Ç–∞–ø–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã, –∫–æ—Ç–æ—Ä–∞—è —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏ —É–¥–∞–ª—è–µ—Ç –Ω–µ–ª–∏–Ω–µ–π–Ω–æ—Å—Ç–∏, —Ç–∞–∫–∏–µ –∫–∞–∫ LayerNorm –∏ GELU, –∏ —Å–æ–∫—Ä–∞—â–∞–µ—Ç –∫–æ–ª–∏
[18.10.2024 11:41] Querying the API.
[18.10.2024 11:41] Got response. {
  "desc": "Long-LRM - —ç—Ç–æ –æ–±–æ–±—â–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ 3D –≥–∞—É—Å—Å–æ–≤—ã—Ö —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π, —Å–ø–æ—Å–æ–±–Ω–∞—è –≤–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—Ç—å –±–æ–ª—å—à–∏–µ —Å—Ü–µ–Ω—ã –∏–∑ –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –ú–æ–¥–µ–ª—å –º–æ–∂–µ—Ç –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å 32 –∏—Å—Ö–æ–¥–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ–º 960x540 –≤—Å–µ–≥–æ –∑–∞ 1,3 —Å–µ–∫—É–Ω–¥—ã –Ω–∞ –æ–¥–Ω–æ–º GPU A100 80G. –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Å–æ—á–µ—Ç–∞–µ—Ç –±–ª–æ–∫–∏ Mamba2 –∏ –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –≥–æ—Ä–∞–∑–¥–æ –±–æ–ª—å—à–µ —Ç–æ–∫–µ–Ω–æ–≤, —á–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–µ —Ä–∞–±–æ—Ç—ã. Long-LRM —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∏—Ä—É–µ—Ç –≤—Å—é —Å—Ü–µ–Ω—É –∑–∞ –æ–¥–∏–Ω –ø—Ä–æ—Ö–æ–¥, –¥–æ—Å—Ç–∏–≥–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, —Å—Ä–∞–≤–Ω–∏–º–æ–π —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–æ–Ω–Ω—ã–º–∏ –ø–æ–¥—Ö–æ–¥–∞–º–∏, –Ω–æ –≤ 100 —Ä–∞–∑ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ.",
  "categories": ["#cv", "#3d", "#reconstruction", "#performance"],
  "emoji": "üèôÔ∏è",
  "title": "–ë—ã—Å—Ç—Ä–∞—è 3D —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –±–æ–ª—å—à–∏—Ö —Å—Ü–µ–Ω –∏–∑ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"
}
[18.10.2024 11:41] Get embedding for a paper via LLM API.
[18.10.2024 11:41] Querying the API.
[18.10.2024 11:41] Got response. {
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ Retrieval Augmented Personalization (RAP) –¥–ª—è –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (MLLM). RAP –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö –∫–ª—é—á-–∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ, –∏–∑–≤–ª–µ–∫–∞–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å –ø–æ–º–æ—â—å—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ —Ä–µ—Ç—Ä–∏–≤–µ—Ä–∞ –∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã. –ê–≤—Ç–æ—Ä—ã —Å–æ–∑–¥–∞–ª–∏ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è MLLM –∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ —Å–µ—Ä–∏—é –º–æ–¥–µ–ª–µ–π, —Å–ø–æ—Å–æ–±–Ω—ã—Ö –æ–±–æ–±—â–∞—Ç—å—Å—è –Ω–∞ –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –∫–æ–Ω—Ü–µ–ø—Ü–∏–π –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏. –ú–æ–¥–µ–ª–∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –≤—ã—Å–æ–∫—É—é –≥–∏–±–∫–æ—Å—Ç—å –∏ –∫–∞—á–µ—Å—Ç–≤–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö, –≤–∫–ª—é—á–∞—è –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –≤–∏–∑—É–∞–ª—å–Ω–æ–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ.",
  "categories": ["#multimodal", "#rag", "#personalization", "#dataset"],
  "emoji": "üë§",
  "title": "–ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –ò–ò-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é RAP"
}
[18.10.2024 11:41] Get embedding for a paper via LLM API.
[18.10.2024 11:41] Using data from previous issue: {"desc": "MuVi - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –º—É–∑—ã–∫–∏, —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Å –≤–∏–¥–µ–æ–∫–æ–Ω—Ç–µ–Ω—Ç–æ–º. –û–Ω–∞ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–≥–æ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –∞–¥–∞–ø—Ç–µ—Ä–∞ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫. –ó–∞—Ç–µ–º —ç—Ç–∏ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –º—É–∑—ã–∫–∏, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–π –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—é, —Ç–µ–º–µ, —Ä–∏—Ç–º—É
[18.10.2024 11:41] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –∏ –º–µ—Ç–æ–¥—ã –∏—Ö –æ–±—Ö–æ–¥–∞. –ê–≤—Ç–æ—Ä—ã –≤–≤–æ–¥—è—Ç –ø–æ–Ω—è—Ç–∏–µ PCJailbreak, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—â–µ–µ —Ä–∏—Å–∫–∏, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å –Ω–∞–º–µ—Ä–µ–Ω–Ω—ã–º–∏ —Å–º–µ—â–µ–Ω–∏—è–º–∏, –≤–Ω–µ–¥—Ä–µ–Ω–Ω—ã–º–∏ –≤ LLM –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—É—é —Ä–∞–∑–Ω–∏—Ü—É –≤ —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏ 
[18.10.2024 11:41] Using data from previous issue: {"desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ä–µ—à–µ–Ω–∏—é –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∑–∞–¥–∞—á, –Ω–∞–∑—ã–≤–∞–µ–º—ã–π SBI-RAG (Schema-Based Instruction Retrieval-Augmented Generation). –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ —Å–æ—á–µ—Ç–∞–µ—Ç –≤ —Å–µ–±–µ —Å—Ö–µ–º–æ-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ (SBI) –∏ –±–æ–ª—å—à—É—é —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å (LLM) –¥–ª—è –ø–æ—à–∞–≥–æ–≤–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –ø—Ä–∏ —Ä–µ—à–µ–Ω–∏–∏ –∑–∞–¥–∞
[18.10.2024 11:41] Querying the API.
[18.10.2024 11:42] Got response. {
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ gamma-MoD –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (MLLM). –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ Mixture of Depths (MoD) –¥–ª—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö MLLM, –∏—Å–ø–æ–ª—å–∑—É—è –º–µ—Ç—Ä–∏–∫—É —Ä–∞–Ω–≥–∞ –∫–∞—Ä—Ç –≤–Ω–∏–º–∞–Ω–∏—è (ARank) –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∏–∑–±—ã—Ç–æ—á–Ω—ã—Ö —Å–ª–æ–µ–≤. –ú–µ—Ç–æ–¥ –≤–∫–ª—é—á–∞–µ—Ç –æ–±—â–∏–π –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ç–æ—Ä –¥–ª—è –∑—Ä–µ–Ω–∏—è –∏ —è–∑—ã–∫–∞, –∞ —Ç–∞–∫–∂–µ –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ 9 –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ –ø–æ–≤—ã—à–µ–Ω–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ MLLM –ø—Ä–∏ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–º —Å–Ω–∏–∂–µ–Ω–∏–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.",
  "categories": ["#multimodal", "#nlp", "#cv", "#benchmark"],
  "emoji": "üöÄ",
  "title": "–£—Å–∫–æ—Ä–µ–Ω–∏–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö LLM –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞"
}
[18.10.2024 11:42] Get embedding for a paper via LLM API.
[18.10.2024 11:42] Querying the API.
[18.10.2024 11:42] Got response. {
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Condition Contrastive Alignment (CCA) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ç–µ—Ö–Ω–∏–∫–∏ Classifier-Free Guidance (CFG). CCA –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ –º–µ—Ç–æ–¥–∞—Ö –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∏ –Ω–∞–ø—Ä—è–º—É—é –¥–æ–æ–±—É—á–∞–µ—Ç –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –∂–µ–ª–∞–µ–º–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤—ã–±–æ—Ä–∫–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ CCA –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π –±–µ–∑ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è guidance, —Å–æ–∫—Ä–∞—â–∞—è –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞—Ç—Ä–∞—Ç—ã –≤–¥–≤–æ–µ. –ú–µ—Ç–æ–¥ —Ç–∞–∫–∂–µ –ø–æ–∑–≤–æ–ª—è–µ—Ç –¥–æ—Å—Ç–∏–≥–∞—Ç—å –∫–æ–º–ø—Ä–æ–º–∏—Å—Å–∞ –º–µ–∂–¥—É —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ–º –∏ —Ç–æ—á–Ω–æ—Å—Ç—å—é —Å—ç–º–ø–ª–æ–≤, –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ CFG.",
  "categories": ["#multimodal", "#cv", "#nlp", "#rlhf"],
  "emoji": "üñºÔ∏è",
  "title": "–£–ª—É—á—à–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –±–µ–∑ guidance —á–µ—Ä–µ–∑ –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ"
}
[18.10.2024 11:42] Get embedding for a paper via LLM API.
[18.10.2024 11:42] Querying the API.
[18.10.2024 11:42] Got response. {
  "desc": "CII-Bench - —ç—Ç–æ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (MLLM) –∫ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—é –∏ –ø–æ–Ω–∏–º–∞–Ω–∏—é –∫–∏—Ç–∞–π—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤—ã—Å–æ–∫–æ–≥–æ —É—Ä–æ–≤–Ω—è. –û–Ω –≤–∫–ª—é—á–∞–µ—Ç –∞—É—Ç–µ–Ω—Ç–∏—á–Ω—ã–µ –∫–∏—Ç–∞–π—Å–∫–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, –≤ —Ç–æ–º —á–∏—Å–ª–µ –æ—Ç—Ä–∞–∂–∞—é—â–∏–µ —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—É—é –∫—É–ª—å—Ç—É—Ä—É. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π —Ä–∞–∑—Ä—ã–≤ –º–µ–∂–¥—É –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é MLLM –∏ –ª—é–¥–µ–π –Ω–∞ CII-Bench, –æ—Å–æ–±–µ–Ω–Ω–æ –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω–æ–π –∫–∏—Ç–∞–π—Å–∫–æ–π –∫—É–ª—å—Ç—É—Ä—ã. –ê–≤—Ç–æ—Ä—ã –æ—Ç–º–µ—á–∞—é—Ç, —á—Ç–æ –≤–∫–ª—é—á–µ–Ω–∏–µ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã—Ö –ø–æ–¥—Å–∫–∞–∑–æ–∫ –≤ –ø—Ä–æ–º–ø—Ç—ã —É–ª—É—á—à–∞–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π.",
  "categories": ["#multimodal", "#benchmark", "#cv", "#dataset"],
  "emoji": "üá®üá≥",
  "title": "CII-Bench: –Ω–æ–≤—ã–π —Ä—É–±–µ–∂ –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ –∫–∏—Ç–∞–π—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è MLLM"
}
[18.10.2024 11:42] Get embedding for a paper via LLM API.
[18.10.2024 11:42] Querying the API.
[18.10.2024 11:42] Got response. {
  "desc": "–í —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ –Ω–µ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –¥–ª–∏–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥ –¥–æ–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –Ω–∞ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω–æ–º –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö —Å –¥–ª–∏–Ω–Ω—ã–º–∏ –æ—Ç–≤–µ—Ç–∞–º–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –¥–∞–∂–µ –Ω–µ–±–æ–ª—å—à–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –ø–æ–∑–≤–æ–ª—è–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∏—Ç—å —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤. –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—É–±–ª–∏–∫—É—é—Ç —Å–≤–æ–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö, –∫–æ–¥ –∏ –¥–æ–æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –æ—Ç–∫—Ä—ã—Ç–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞.",
  "categories": ["#nlp", "dataset", "benchmark"],
  "emoji": "üìù",
  "title": "–î–ª–∏–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π: –∫–∞—á–µ—Å—Ç–≤–æ –≤–∞–∂–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞"
}
[18.10.2024 11:42] Get embedding for a paper via LLM API.
[18.10.2024 11:42] Querying the API.
[18.10.2024 11:42] Got response. {
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç TransAgent - —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±–æ–±—â–∞—é—â–µ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ —Ç–∏–ø–∞ CLIP. TransAgent –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –∑–Ω–∞–Ω–∏—è –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—É—é –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—é –∑–Ω–∞–Ω–∏–π. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å CLIP-–ø–æ–¥–æ–±–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∫ –Ω–æ–≤—ã–º –¥–æ–º–µ–Ω–∞–º –¥–∞–Ω–Ω—ã—Ö. –ê–≤—Ç–æ—Ä—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ 11 –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö –ø–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –æ—Å–æ–±–µ–Ω–Ω–æ –≤ —É—Å–ª–æ–≤–∏—è—Ö –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –æ–±—É—á–∞—é—â–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤.",
  "categories": ["#multimodal", "#cv", "#nlp", "#transfer_learning"],
  "emoji": "üß†",
  "title": "TransAgent: –æ–±—ä–µ–¥–∏–Ω—è–µ–º –∑–Ω–∞–Ω–∏—è —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –¥–ª—è –ª—É—á—à–µ–≥–æ –æ–±–æ–±—â–µ–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"
}
[18.10.2024 11:42] Get embedding for a paper via LLM API.
[18.10.2024 11:42] Trying to get texts in Chinese.
[18.10.2024 11:42] Mistral request. Write simple and brief explanation (4-5 sentences) of an article in Chinese. Use short sentences. Text:

Perceiving and generating diverse modalities are crucial for AI models to effectively learn from and engage with real-world signals, necessitating reliable evaluations for their development. We identify two major issues in current evaluations: (1) inconsistent standards, shaped by different communities with varying protocols and maturity levels; and (2) significant query, grading, and generalization biases. To address these, we introduce MixEval-X, the first any-to-any real-world benchmark designed to optimize and standardize evaluations across input and output modalities. We propose multi-modal benchmark mixture and adaptation-rectification pipelines to reconstruct real-world task distributions, ensuring evaluations generalize effectively to real-world use cases. Extensive meta-evaluations show our approach effectively aligns benchmark samples with real-world task distributions and the model rankings correlate strongly with that of crowd-sourced real-world evaluations (up to 0.98). We provide comprehensive leaderboards to rerank existing models and organizations and offer insights to enhance understanding of multi-modal evaluations and inform future research.
[18.10.2024 11:42] Mistral response. {"id": "c0047b821d8341c8b224ca15ca943dd9", "object": "chat.completion", "created": 1729251750, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "content": "\u8fd9\u7bc7\u6587\u7ae0\u8ba8\u8bba\u4e86\u4eba\u5de5\u667a\u80fd\u6a21\u578b\u5728\u5904\u7406\u591a\u79cd\u6a21\u6001\u4fe1\u53f7\u65f6\uff0c\u8bc4\u4f30\u6807\u51c6\u4e0d\u4e00\u81f4\u548c\u5b58\u5728\u504f\u5dee\u7684\u95ee\u9898\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86MixEval-X\uff0c\u8fd9\u662f\u4e00\u4e2a\u8de8\u6a21\u6001\u7684\u5b9e\u9645\u5e94\u7528\u8bc4\u4f30\u57fa\u51c6\u3002MixEval-X\u901a\u8fc7\u6df7\u5408\u548c\u8c03\u6574\u591a\u6a21\u6001\u57fa\u51c6\uff0c\u91cd\u5efa\u5b9e\u9645\u4efb\u52a1\u5206\u5e03\uff0c\u786e\u4fdd\u8bc4\u4f30\u7ed3\u679c\u4e0e\u5b9e\u9645\u5e94\u7528\u76f8\u7b26\u3002\u7814\u7a76\u8868\u660e\uff0c\u8fd9\u79cd\u65b9\u6cd5\u6709\u6548\u5730\u4f7f\u57fa\u51c6\u6837\u672c\u4e0e\u5b9e\u9645\u4efb\u52a1\u5206\u5e03\u4e00\u81f4\uff0c\u5e76\u4e14\u6a21\u578b\u6392\u540d\u4e0e\u4f17\u5305\u5b9e\u9645\u8bc4\u4f30\u7ed3\u679c\u9ad8\u5ea6\u76f8\u5173\u3002\u6587\u7ae0\u8fd8\u63d0\u4f9b\u4e86\u8be6\u7ec6\u7684\u6392\u884c\u699c\uff0c\u91cd\u65b0\u6392\u5217\u73b0\u6709\u6a21\u578b\uff0c\u5e76\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002", "tool_calls": null}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 265, "total_tokens": 475, "completion_tokens": 210}}
[18.10.2024 11:42] Mistral request. Write pinyin transcription for text. Text:

ËøôÁØáÊñáÁ´†ËÆ®ËÆ∫‰∫Ü‰∫∫Â∑•Êô∫ËÉΩÊ®°ÂûãÂú®Â§ÑÁêÜÂ§öÁßçÊ®°ÊÄÅ‰ø°Âè∑Êó∂ÔºåËØÑ‰º∞Ê†áÂáÜ‰∏ç‰∏ÄËá¥ÂíåÂ≠òÂú®ÂÅèÂ∑ÆÁöÑÈóÆÈ¢ò„ÄÇ‰∏∫Ëß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºå‰ΩúËÄÖÊèêÂá∫‰∫ÜMixEval-XÔºåËøôÊòØ‰∏Ä‰∏™Ë∑®Ê®°ÊÄÅÁöÑÂÆûÈôÖÂ∫îÁî®ËØÑ‰º∞Âü∫ÂáÜ„ÄÇMixEval-XÈÄöËøáÊ∑∑ÂêàÂíåË∞ÉÊï¥Â§öÊ®°ÊÄÅÂü∫ÂáÜÔºåÈáçÂª∫ÂÆûÈôÖ‰ªªÂä°ÂàÜÂ∏ÉÔºåÁ°Æ‰øùËØÑ‰º∞ÁªìÊûú‰∏éÂÆûÈôÖÂ∫îÁî®Áõ∏Á¨¶„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåËøôÁßçÊñπÊ≥ïÊúâÊïàÂú∞‰ΩøÂü∫ÂáÜÊ†∑Êú¨‰∏éÂÆûÈôÖ‰ªªÂä°ÂàÜÂ∏É‰∏ÄËá¥ÔºåÂπ∂‰∏îÊ®°ÂûãÊéíÂêç‰∏é‰ºóÂåÖÂÆûÈôÖËØÑ‰º∞ÁªìÊûúÈ´òÂ∫¶Áõ∏ÂÖ≥„ÄÇÊñáÁ´†ËøòÊèê‰æõ‰∫ÜËØ¶ÁªÜÁöÑÊéíË°åÊ¶úÔºåÈáçÊñ∞ÊéíÂàóÁé∞ÊúâÊ®°ÂûãÔºåÂπ∂‰∏∫Êú™Êù•Á†îÁ©∂Êèê‰æõ‰∫ÜËßÅËß£„ÄÇ
[18.10.2024 11:42] Mistral response. {"id": "3d78d3a6db36424eb568668db5abce91", "object": "chat.completion", "created": 1729251761, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "content": "Zh\u00e8 pi\u0101n w\u00e9nzh\u0101ng t\u01ceol\u00f9n le r\u00e9n g\u014dng zh\u00ecn\u00e9ng m\u00f3x\u00edng z\u00e0i ch\u01d4l\u01d0 du\u014d zh\u01d2ng m\u00f3t\u00e0i x\u00ecnh\u00e0o sh\u00ed, p\u00edngg\u016b bi\u0101ozh\u01d4n b\u00f9 y\u012bzh\u00ec h\u00e9 c\u00fanz\u00e0i pi\u00e0nch\u0101 de w\u00e8nti. W\u00e8i ji\u011bju\u00e9 zh\u00e8xi\u0113 w\u00e8nti, zu\u00f2zh\u011b t\u00edch\u016b le MixEval-X, zh\u00e8 sh\u00ec y\u012bg\u00e8 ku\u00e0 m\u00f3t\u00e0i de sh\u00edj\u00ec y\u00ecngy\u00f2ng p\u00edngg\u016b j\u012bzh\u01d4n. MixEval-X t\u014dnggu\u00f2 h\u00f9nh\u00e9 h\u00e9 ti\u00e1ozh\u011bng du\u014d m\u00f3t\u00e0i j\u012bzh\u01d4n, ch\u00f3ngji\u00e0n sh\u00edj\u00ec r\u00e8nw\u00f9 f\u0113nb\u00f9, qu\u00e8b\u01ceo p\u00edngg\u016b ji\u00e9gu\u01d2 y\u01d4 sh\u00edj\u00ec y\u00ecngy\u00f2ng xi\u0101ngf\u00fa. Y\u00e1nji\u016b bi\u01ceom\u00edng, zh\u00e8 zh\u01d2ng f\u0101ngf\u01ce y\u01d2uxi\u00e0o de sh\u01d0 j\u012bzh\u01d4n y\u00e0ngb\u011bn y\u01d4 sh\u00edj\u00ec r\u00e8nw\u00f9 f\u0113nb\u00f9 y\u012bzh\u00ec, \u00e9rqi\u011b m\u00f3x\u00edng p\u00e1im\u00edng y\u01d4 zh\u00f2ngb\u0101o sh\u00edj\u00ec p\u00edngg\u016b ji\u00e9gu\u01d2 g\u0101od\u00f9 xi\u0101nggu\u0101n. W\u00e9nzh\u0101ng h\u00e1i t\u00edg\u014dng le xi\u00e1ngx\u00ec de p\u00e1ih\u00e1ngb\u01ceng, z\u00e0i ch\u00f3ngx\u012bn p\u00e1ili\u00e8 xi\u00e0nz\u00e0i m\u00f3x\u00edng, \u00e9r w\u00e8i w\u00e8il\u00e1i y\u00e1nji\u016b t\u00edg\u014dng le ji\u00e0nji\u011b.", "tool_calls": null}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 225, "total_tokens": 663, "completion_tokens": 438}}
[18.10.2024 11:42] Mistral request. Write vocab of difficult words for this text as an array of objects with fields 'word', 'pinyin', 'trans'. Return as python list without formatting. Return list and nothing else. Text:

ËøôÁØáÊñáÁ´†ËÆ®ËÆ∫‰∫Ü‰∫∫Â∑•Êô∫ËÉΩÊ®°ÂûãÂú®Â§ÑÁêÜÂ§öÁßçÊ®°ÊÄÅ‰ø°Âè∑Êó∂ÔºåËØÑ‰º∞Ê†áÂáÜ‰∏ç‰∏ÄËá¥ÂíåÂ≠òÂú®ÂÅèÂ∑ÆÁöÑÈóÆÈ¢ò„ÄÇ‰∏∫Ëß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºå‰ΩúËÄÖÊèêÂá∫‰∫ÜMixEval-XÔºåËøôÊòØ‰∏Ä‰∏™Ë∑®Ê®°ÊÄÅÁöÑÂÆûÈôÖÂ∫îÁî®ËØÑ‰º∞Âü∫ÂáÜ„ÄÇMixEval-XÈÄöËøáÊ∑∑ÂêàÂíåË∞ÉÊï¥Â§öÊ®°ÊÄÅÂü∫ÂáÜÔºåÈáçÂª∫ÂÆûÈôÖ‰ªªÂä°ÂàÜÂ∏ÉÔºåÁ°Æ‰øùËØÑ‰º∞ÁªìÊûú‰∏éÂÆûÈôÖÂ∫îÁî®Áõ∏Á¨¶„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåËøôÁßçÊñπÊ≥ïÊúâÊïàÂú∞‰ΩøÂü∫ÂáÜÊ†∑Êú¨‰∏éÂÆûÈôÖ‰ªªÂä°ÂàÜÂ∏É‰∏ÄËá¥ÔºåÂπ∂‰∏îÊ®°ÂûãÊéíÂêç‰∏é‰ºóÂåÖÂÆûÈôÖËØÑ‰º∞ÁªìÊûúÈ´òÂ∫¶Áõ∏ÂÖ≥„ÄÇÊñáÁ´†ËøòÊèê‰æõ‰∫ÜËØ¶ÁªÜÁöÑÊéíË°åÊ¶úÔºåÈáçÊñ∞ÊéíÂàóÁé∞ÊúâÊ®°ÂûãÔºåÂπ∂‰∏∫Êú™Êù•Á†îÁ©∂Êèê‰æõ‰∫ÜËßÅËß£„ÄÇ
[18.10.2024 11:43] Mistral response. {"id": "945588c9614d447d958a207864c2d7f5", "object": "chat.completion", "created": 1729251775, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "content": "[\n    {\"word\": \"\u8ba8\u8bba\", \"pinyin\": \"t\u01ceo l\u00f9n\", \"trans\": \"discuss\"},\n    {\"word\": \"\u6a21\u6001\", \"pinyin\": \"m\u00f3 t\u00e0i\", \"trans\": \"modality\"},\n    {\"word\": \"\u4fe1\u53f7\", \"pinyin\": \"x\u00ecn h\u00e0o\", \"trans\": \"signal\"},\n    {\"word\": \"\u8bc4\u4f30\", \"pinyin\": \"p\u00edng g\u016b\", \"trans\": \"evaluate\"},\n    {\"word\": \"\u6807\u51c6\", \"pinyin\": \"bi\u0101o zh\u01d4n\", \"trans\": \"standard\"},\n    {\"word\": \"\u4e0d\u4e00\u81f4\", \"pinyin\": \"b\u00f9 y\u012b zh\u00ec\", \"trans\": \"inconsistent\"},\n    {\"word\": \"\u504f\u5dee\", \"pinyin\": \"pi\u0101n ch\u0101\", \"trans\": \"bias\"},\n    {\"word\": \"\u63d0\u51fa\", \"pinyin\": \"t\u00ed ch\u016b\", \"trans\": \"propose\"},\n    {\"word\": \"\u8de8\u6a21\u6001\", \"pinyin\": \"ku\u00e0 m\u00f3 t\u00e0i\", \"trans\": \"cross-modal\"},\n    {\"word\": \"\u5b9e\u9645\u5e94\u7528\", \"pinyin\": \"sh\u00ed j\u00ec y\u00ecng y\u00f2ng\", \"trans\": \"practical application\"},\n    {\"word\": \"\u8bc4\u4f30\u57fa\u51c6\", \"pinyin\": \"p\u00edng g\u016b j\u012b zh\u01d4n\", \"trans\": \"evaluation benchmark\"},\n    {\"word\": \"\u6df7\u5408\", \"pinyin\": \"h\u00f9n h\u00e9\", \"trans\": \"mix\"},\n    {\"word\": \"\u8c03\u6574\", \"pinyin\": \"ti\u00e1o zh\u011bng\", \"trans\": \"adjust\"},\n    {\"word\": \"\u591a\u6a21\u6001\", \"pinyin\": \"du\u014d m\u00f3 t\u00e0i\", \"trans\": \"multimodal\"},\n    {\"word\": \"\u91cd\u5efa\", \"pinyin\": \"ch\u00f3ng ji\u00e0n\", \"trans\": \"reconstruct\"},\n    {\"word\": \"\u4efb\u52a1\u5206\u5e03\", \"pinyin\": \"r\u00e8n w\u00f9 f\u0113n b\u00f9\", \"trans\": \"task distribution\"},\n    {\"word\": \"\u786e\u4fdd\", \"pinyin\": \"qu\u00e8 b\u01ceo\", \"trans\": \"ensure\"},\n    {\"word\": \"\u76f8\u7b26\", \"pinyin\": \"xi\u0101ng f\u00fa\", \"trans\": \"consistent\"},\n    {\"word\": \"\u6837\u672c\", \"pinyin\": \"y\u00e0ng b\u011bn\", \"trans\": \"sample\"},\n    {\"word\": \"\u4f17\u5305\", \"pinyin\": \"zh\u00f2ng b\u0101o\", \"trans\": \"crowdsourcing\"},\n    {\"word\": \"\u6392\u884c\u699c\", \"pinyin\": \"p\u00e1i h\u00e1ng b\u01ceng\", \"trans\": \"ranking\"},\n    {\"word\": \"\u91cd\u65b0\u6392\u5217\", \"pinyin\": \"ch\u00f3ng x\u012bn p\u00e1i li\u00e8\", \"trans\": \"reorder\"},\n    {\"word\": \"\u89c1\u89e3\", \"pinyin\": \"ji\u00e0n ji\u011b\", \"trans\": \"insight\"}\n]", "tool_calls": null}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 257, "total_tokens": 967, "completion_tokens": 710}}
[18.10.2024 11:43] Mistral request. Translate this text in English. Text:

ËøôÁØáÊñáÁ´†ËÆ®ËÆ∫‰∫Ü‰∫∫Â∑•Êô∫ËÉΩÊ®°ÂûãÂú®Â§ÑÁêÜÂ§öÁßçÊ®°ÊÄÅ‰ø°Âè∑Êó∂ÔºåËØÑ‰º∞Ê†áÂáÜ‰∏ç‰∏ÄËá¥ÂíåÂ≠òÂú®ÂÅèÂ∑ÆÁöÑÈóÆÈ¢ò„ÄÇ‰∏∫Ëß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºå‰ΩúËÄÖÊèêÂá∫‰∫ÜMixEval-XÔºåËøôÊòØ‰∏Ä‰∏™Ë∑®Ê®°ÊÄÅÁöÑÂÆûÈôÖÂ∫îÁî®ËØÑ‰º∞Âü∫ÂáÜ„ÄÇMixEval-XÈÄöËøáÊ∑∑ÂêàÂíåË∞ÉÊï¥Â§öÊ®°ÊÄÅÂü∫ÂáÜÔºåÈáçÂª∫ÂÆûÈôÖ‰ªªÂä°ÂàÜÂ∏ÉÔºåÁ°Æ‰øùËØÑ‰º∞ÁªìÊûú‰∏éÂÆûÈôÖÂ∫îÁî®Áõ∏Á¨¶„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåËøôÁßçÊñπÊ≥ïÊúâÊïàÂú∞‰ΩøÂü∫ÂáÜÊ†∑Êú¨‰∏éÂÆûÈôÖ‰ªªÂä°ÂàÜÂ∏É‰∏ÄËá¥ÔºåÂπ∂‰∏îÊ®°ÂûãÊéíÂêç‰∏é‰ºóÂåÖÂÆûÈôÖËØÑ‰º∞ÁªìÊûúÈ´òÂ∫¶Áõ∏ÂÖ≥„ÄÇÊñáÁ´†ËøòÊèê‰æõ‰∫ÜËØ¶ÁªÜÁöÑÊéíË°åÊ¶úÔºåÈáçÊñ∞ÊéíÂàóÁé∞ÊúâÊ®°ÂûãÔºåÂπ∂‰∏∫Êú™Êù•Á†îÁ©∂Êèê‰æõ‰∫ÜËßÅËß£„ÄÇ
[18.10.2024 11:43] Mistral response. {"id": "08367b2f185e40e4b907a85fe12f3162", "object": "chat.completion", "created": 1729251803, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "content": "This article discusses the issues of inconsistent evaluation standards and biases in artificial intelligence models when handling multimodal signals. To address these problems, the authors propose MixEval-X, a cross-modal practical application evaluation benchmark. MixEval-X reconstructs the actual task distribution by mixing and adjusting multimodal benchmarks, ensuring that evaluation results align with real-world applications. Research shows that this method effectively makes benchmark samples consistent with actual task distributions, and model rankings are highly correlated with crowdsourced actual evaluation results. The article also provides a detailed leaderboard, reordering existing models and offering insights for future research.", "tool_calls": null}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 223, "total_tokens": 359, "completion_tokens": 136}}
[18.10.2024 11:43] Renaming data file.
[18.10.2024 11:43] Renaming previous data. hf_papers.json to 2024-10-17_hf_papers.json
[18.10.2024 11:43] Saving new data file.
[18.10.2024 11:43] Generating page.
[18.10.2024 11:43] Generating Chinese page for reading.
[18.10.2024 11:43] Chinese vocab [{'word': 'ËÆ®ËÆ∫', 'pinyin': 't«éo l√πn', 'trans': 'discuss'}, {'word': 'Ê®°ÊÄÅ', 'pinyin': 'm√≥ t√†i', 'trans': 'modality'}, {'word': '‰ø°Âè∑', 'pinyin': 'x√¨n h√†o', 'trans': 'signal'}, {'word': 'ËØÑ‰º∞', 'pinyin': 'p√≠ng g≈´', 'trans': 'evaluate'}, {'word': 'Ê†áÂáÜ', 'pinyin': 'biƒÅo zh«în', 'trans': 'standard'}, {'word': '‰∏ç‰∏ÄËá¥', 'pinyin': 'b√π yƒ´ zh√¨', 'trans': 'inconsistent'}, {'word': 'ÂÅèÂ∑Æ', 'pinyin': 'piƒÅn chƒÅ', 'trans': 'bias'}, {'word': 'ÊèêÂá∫', 'pinyin': 't√≠ ch≈´', 'trans': 'propose'}, {'word': 'Ë∑®Ê®°ÊÄÅ', 'pinyin': 'ku√† m√≥ t√†i', 'trans': 'cross-modal'}, {'word': 'ÂÆûÈôÖÂ∫îÁî®', 'pinyin': 'sh√≠ j√¨ y√¨ng y√≤ng', 'trans': 'practical application'}, {'word': 'ËØÑ‰º∞Âü∫ÂáÜ', 'pinyin': 'p√≠ng g≈´ jƒ´ zh«în', 'trans': 'evaluation benchmark'}, {'word': 'Ê∑∑Âêà', 'pinyin': 'h√πn h√©', 'trans': 'mix'}, {'word': 'Ë∞ÉÊï¥', 'pinyin': 'ti√°o zhƒõng', 'trans': 'adjust'}, {'word': 'Â§öÊ®°ÊÄÅ', 'pinyin': 'du≈ç m√≥ t√†i', 'trans': 'multimodal'}, {'word': 'ÈáçÂª∫', 'pinyin': 'ch√≥ng ji√†n', 'trans': 'reconstruct'}, {'word': '‰ªªÂä°ÂàÜÂ∏É', 'pinyin': 'r√®n w√π fƒìn b√π', 'trans': 'task distribution'}, {'word': 'Á°Æ‰øù', 'pinyin': 'qu√® b«éo', 'trans': 'ensure'}, {'word': 'Áõ∏Á¨¶', 'pinyin': 'xiƒÅng f√∫', 'trans': 'consistent'}, {'word': 'Ê†∑Êú¨', 'pinyin': 'y√†ng bƒõn', 'trans': 'sample'}, {'word': '‰ºóÂåÖ', 'pinyin': 'zh√≤ng bƒÅo', 'trans': 'crowdsourcing'}, {'word': 'ÊéíË°åÊ¶ú', 'pinyin': 'p√°i h√°ng b«éng', 'trans': 'ranking'}, {'word': 'ÈáçÊñ∞ÊéíÂàó', 'pinyin': 'ch√≥ng xƒ´n p√°i li√®', 'trans': 'reorder'}, {'word': 'ËßÅËß£', 'pinyin': 'ji√†n jiƒõ', 'trans': 'insight'}]
[18.10.2024 11:43] Renaming previous page.
[18.10.2024 11:43] Renaming previous data. index.html to 2024-10-17_hf_papers.html
[18.10.2024 11:43] Renaming previous Chinese page.
[18.10.2024 11:43] Renaming previous data. zh.html to 2024-10-17_zh_reading_task.html
[18.10.2024 11:43] Writing result.
[18.10.2024 11:43] Writing Chinese reading task.
[18.10.2024 11:43] Renaming log file.
[18.10.2024 11:43] Renaming previous data. log.txt to 2024-10-17_last_log.txt
