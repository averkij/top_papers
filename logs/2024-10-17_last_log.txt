[18.10.2024 00:58] Read previous papers.
[18.10.2024 00:58] Get feed.
[18.10.2024 00:58] Get page data from previous paper. URL: https://huggingface.co/papers/2410.11623
[18.10.2024 00:58] Get page data from previous paper. URL: https://huggingface.co/papers/2410.12381
[18.10.2024 00:58] Get page data from previous paper. URL: https://huggingface.co/papers/2410.12787
[18.10.2024 00:58] Get page data from previous paper. URL: https://huggingface.co/papers/2410.12409
[18.10.2024 00:58] Get page data from previous paper. URL: https://huggingface.co/papers/2410.10672
[18.10.2024 00:58] Get page data from previous paper. URL: https://huggingface.co/papers/2410.12613
[18.10.2024 00:58] Get page data from previous paper. URL: https://huggingface.co/papers/2410.12628
[18.10.2024 00:58] Get page data from previous paper. URL: https://huggingface.co/papers/2410.12405
[18.10.2024 00:58] Get page data from previous paper. URL: https://huggingface.co/papers/2410.08968
[18.10.2024 00:58] Get page data from previous paper. URL: https://huggingface.co/papers/2410.11817
[18.10.2024 00:58] Get page data from previous paper. URL: https://huggingface.co/papers/2410.07722
[18.10.2024 00:58] Get page data from previous paper. URL: https://huggingface.co/papers/2410.08584
[18.10.2024 00:58] Get page data from previous paper. URL: https://huggingface.co/papers/2410.11081
[18.10.2024 00:58] Get page data from previous paper. URL: https://huggingface.co/papers/2410.09870
[18.10.2024 00:58] Get page data from previous paper. URL: https://huggingface.co/papers/2410.12391
[18.10.2024 00:58] Get page data from previous paper. URL: https://huggingface.co/papers/2410.11878
[18.10.2024 00:58] Get page data from previous paper. URL: https://huggingface.co/papers/2410.12722
[18.10.2024 00:58] Get page data from previous paper. URL: https://huggingface.co/papers/2410.12109
[18.10.2024 00:58] Get page data from previous paper. URL: https://huggingface.co/papers/2410.12491
[18.10.2024 00:58] Get page data from previous paper. URL: https://huggingface.co/papers/2410.12490
[18.10.2024 00:58] Get page data from previous paper. URL: https://huggingface.co/papers/2410.09724
[18.10.2024 00:58] Extract page data from URL. URL: https://huggingface.co/papers/2410.11843
[18.10.2024 00:58] Extract page data from URL. URL: https://huggingface.co/papers/2410.11900
[18.10.2024 00:58] ********************************************************************************
[18.10.2024 00:58] Abstract 0. Recent advancements in Multi-modal Large Language Models (MLLMs) have opened new avenues for applications in Embodied AI. Building on previous work, EgoThink, we introduce VidEgoThink, a comprehensive benchmark for evaluating egocentric video understanding capabilities. To bridge the gap between MLL...
[18.10.2024 00:58] ********************************************************************************
[18.10.2024 00:58] Abstract 1. Coding tasks have been valuable for evaluating Large Language Models (LLMs), as they demand the comprehension of high-level instructions, complex reasoning, and the implementation of functional programs -- core capabilities for advancing Artificial General Intelligence. Despite the progress in Large...
[18.10.2024 00:58] ********************************************************************************
[18.10.2024 00:58] Abstract 2. Recent advancements in large multimodal models (LMMs) have significantly enhanced performance across diverse tasks, with ongoing efforts to further integrate additional modalities such as video and audio. However, most existing LMMs remain vulnerable to hallucinations, the discrepancy between the fa...
[18.10.2024 00:58] ********************************************************************************
[18.10.2024 00:58] Abstract 3. Autonomous planning has been an ongoing pursuit since the inception of artificial intelligence. Based on curated problem solvers, early planning agents could deliver precise solutions for specific tasks but lacked generalization. The emergence of large language models (LLMs) and their powerful reaso...
[18.10.2024 00:58] ********************************************************************************
[18.10.2024 00:58] Abstract 4. As large language models (LLMs) continue to evolve, efficient evaluation metrics are vital for assessing their ability to compress information and reduce redundancy. While traditional metrics like Matrix Entropy offer valuable insights, they are computationally intensive for large-scale models due t...
[18.10.2024 00:58] ********************************************************************************
[18.10.2024 00:58] Abstract 5. Model merging has become one of the key technologies for enhancing the capabilities and efficiency of Large Language Models (LLMs). However, our understanding of the expected performance gains and principles when merging any two models remains limited. In this work, we introduce model kinship, the d...
[18.10.2024 00:58] ********************************************************************************
[18.10.2024 00:58] Abstract 6. Document Layout Analysis is crucial for real-world document understanding systems, but it encounters a challenging trade-off between speed and accuracy: multimodal methods leveraging both text and visual features achieve higher accuracy but suffer from significant latency, whereas unimodal methods r...
[18.10.2024 00:58] ********************************************************************************
[18.10.2024 00:58] Abstract 7. Large language models (LLMs) have demonstrated impressive capabilities across various tasks, but their performance is highly sensitive to the prompts utilized. This variability poses challenges for accurate assessment and user satisfaction. Current research frequently overlooks instance-level prompt...
[18.10.2024 00:58] ********************************************************************************
[18.10.2024 00:58] Abstract 8. The current paradigm for safety alignment of large language models (LLMs) follows a one-size-fits-all approach: the model refuses to interact with any content deemed unsafe by the model provider. This approach lacks flexibility in the face of varying social norms across cultures and regions. In addi...
[18.10.2024 00:58] ********************************************************************************
[18.10.2024 00:58] Abstract 9. The rapid advancement of text-to-image (T2I) diffusion models has enabled them to generate unprecedented results from given texts. However, as text inputs become longer, existing encoding methods like CLIP face limitations, and aligning the generated images with long texts becomes challenging. To ta...
[18.10.2024 00:58] ********************************************************************************
[18.10.2024 00:58] Abstract 10. Learned Sparse Retrieval (LSR) models use vocabularies from pre-trained transformers, which often split entities into nonsensical fragments. Splitting entities can reduce retrieval accuracy and limits the model's ability to incorporate up-to-date world knowledge not included in the training data. In...
[18.10.2024 00:58] ********************************************************************************
[18.10.2024 00:58] Abstract 11. The efficiency of large vision-language models (LVLMs) is constrained by the computational bottleneck of the attention mechanism during the prefill phase and the memory bottleneck of fetching the key-value (KV) cache in the decoding phase, particularly in scenarios involving high-resolution images o...
[18.10.2024 00:58] ********************************************************************************
[18.10.2024 00:58] Abstract 12. Consistency models (CMs) are a powerful class of diffusion-based generative models optimized for fast sampling. Most existing CMs are trained using discretized timesteps, which introduce additional hyperparameters and are prone to discretization errors. While continuous-time formulations can mitigat...
[18.10.2024 00:58] ********************************************************************************
[18.10.2024 00:58] Abstract 13. Large language models (LLMs) have significantly impacted many aspects of our lives. However, assessing and ensuring their chronological knowledge remains challenging. Existing approaches fall short in addressing the accumulative nature of knowledge, often relying on a single time stamp. To overcome ...
[18.10.2024 00:58] ********************************************************************************
[18.10.2024 00:58] Abstract 14. We study how features emerge, disappear, and persist across models fine-tuned on different domains of text. More specifically, we start from a base one-layer Transformer language model that is trained on a combination of the BabyLM corpus, and a collection of Python code from The Stack. This base mo...
[18.10.2024 00:58] ********************************************************************************
[18.10.2024 00:58] Abstract 15. This paper introduces a new learning paradigm termed Neural Metamorphosis (NeuMeta), which aims to build self-morphable neural networks. Contrary to crafting separate models for different architectures or sizes, NeuMeta directly learns the continuous weight manifold of neural networks. Once trained,...
[18.10.2024 00:58] ********************************************************************************
[18.10.2024 00:58] Abstract 16. Multimodal/vision language models (VLMs) are increasingly being deployed in healthcare settings worldwide, necessitating robust benchmarks to ensure their safety, efficacy, and fairness. Multiple-choice question and answer (QA) datasets derived from national medical examinations have long served as ...
[18.10.2024 00:58] ********************************************************************************
[18.10.2024 00:58] Abstract 17. Large Language Models (LLMs) have made significant strides in text generation and comprehension, with recent advancements extending into multimodal LLMs that integrate visual and audio inputs. However, these models continue to struggle with fine-grained, cross-modal temporal understanding, particula...
[18.10.2024 00:58] ********************************************************************************
[18.10.2024 00:58] Abstract 18. Large language models (LLMs) trained with Reinforcement Learning from Human Feedback (RLHF) have demonstrated remarkable capabilities, but their underlying reward functions and decision-making processes remain opaque. This paper introduces a novel approach to interpreting LLMs by applying inverse re...
[18.10.2024 00:58] ********************************************************************************
[18.10.2024 00:58] Abstract 19. Latent-based image generative models, such as Latent Diffusion Models (LDMs) and Mask Image Models (MIMs), have achieved notable success in image generation tasks. These models typically leverage reconstructive autoencoders like VQGAN or VAE to encode pixels into a more compact latent space and lear...
[18.10.2024 00:58] ********************************************************************************
[18.10.2024 00:58] Abstract 20. Language model calibration refers to the alignment between the confidence of the model and the actual performance of its responses. While previous studies point out the overconfidence phenomenon in Large Language Models (LLMs) and show that LLMs trained with Reinforcement Learning from Human Feedbac...
[18.10.2024 00:58] ********************************************************************************
[18.10.2024 00:58] Abstract 21. Large language models (LLMs) have demonstrated significant potential in the development of intelligent applications and systems such as LLM-based agents and agent operating systems (AIOS). However, when these applications and systems interact with the underlying file system, the file system still re...
[18.10.2024 00:58] ********************************************************************************
[18.10.2024 00:58] Abstract 22. Modern Question Answering (QA) and Reasoning approaches based on Large Language Models (LLMs) commonly use prompting techniques, such as Chain-of-Thought (CoT), assuming the resulting generation will have a more granular exploration and reasoning over the question space and scope. However, such meth...
[18.10.2024 00:58] Read previous papers.
[18.10.2024 00:58] Generating reviews via LLM API.
[18.10.2024 00:58] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç VidEgoThink - –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –ø–æ–Ω–∏–º–∞–Ω–∏—è —ç–≥–æ—Ü–µ–Ω—Ç—Ä–∏—á–µ—Å–∫–æ–≥–æ –≤–∏–¥–µ–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–º–∏ –±–æ–ª—å—à–∏–º–∏ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ (MLLM). –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ —á–µ—Ç—ã—Ä–µ –∫–ª—é—á–µ–≤—ã–µ –≤–∑–∞–∏–º–æ—Å–≤—è–∑–∞–Ω–Ω—ã–µ –∑–∞–¥–∞—á–∏: –æ—Ç–≤–µ—Ç—ã –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –ø–æ –≤–∏–¥–µ–æ, –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–µ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ, –≤–∏–∑—É–∞–ª—å–Ω–∞—è
[18.10.2024 00:58] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ HumanEval-V –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –±–æ–ª—å—à–∏—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π (LMM) –∫ –≤–∏–∑—É–∞–ª—å–Ω–æ–º—É –ø–æ–Ω–∏–º–∞–Ω–∏—é –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é —á–µ—Ä–µ–∑ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –∫–æ–¥–∞. –ë–µ–Ω—á–º–∞—Ä–∫ —Å–æ–¥–µ—Ä–∂–∏—Ç 108 –∑–∞–¥–∞—á –ø–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—é –Ω–∞ Python, –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å –≤–∏–∑—É–∞–ª—å–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º. –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ–≤–µ–ª–∏ –æ—Ü–µ–Ω–∫—É 
[18.10.2024 00:58] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –ø–µ—Ä–≤–æ–µ —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π –≤ –∫—Ä—É–ø–Ω—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö (LMM), –æ—Ö–≤–∞—Ç—ã–≤–∞—é—â–µ–µ —Ç—Ä–∏ –æ—Å–Ω–æ–≤–Ω—ã–µ –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–∏: —è–∑—ã–∫, –≤–∏–∑—É–∞–ª—å–Ω—É—é –∏ –∞—É–¥–∏–æ. –ê–≤—Ç–æ—Ä—ã –≤—ã—è–≤–∏–ª–∏ –¥–≤–µ –∫–ª—é—á–µ–≤—ã–µ –ø—Ä–∏—á–∏–Ω—ã –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π: —á—Ä–µ–∑–º–µ—Ä–Ω–∞—è –æ–ø–æ—Ä–∞ –Ω–∞ –æ–¥–Ω–æ–º–æ–¥–∞–ª—å–Ω—ã–µ –ø—Ä–∏–æ—Ä—ã –∏ –ª–æ–∂–Ω—ã–µ –º–µ–∂–º–æ–¥–∞–ª—å–Ω—ã–µ –∫–æ
[18.10.2024 00:58] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã –≤—ã—è–≤–ª—è—é—Ç –¥–≤–∞ –∫–ª—é—á–µ–≤—ã—Ö —Ñ–∞–∫—Ç–æ—Ä–∞, –ø—Ä–µ–ø—è—Ç—Å—Ç–≤—É—é—â–∏—Ö —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–º—É –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—é –∞–≥–µ–Ω—Ç–æ–≤: –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—É—é —Ä–æ–ª—å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –∏ —É–º–µ–Ω—å—à–∞—é—â–µ–µ—Å—è –≤–ª–∏—è–Ω–∏–µ –≤–æ–ø—Ä–æ—Å–æ–≤. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –¥–∞–∂–µ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏, —Ç–∞–∫
[18.10.2024 00:58] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –º–µ—Ç—Ä–∏–∫—É –æ—Ü–µ–Ω–∫–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) - Matrix Nuclear-Norm. –≠—Ç–∞ –º–µ—Ç—Ä–∏–∫–∞ –ø–æ–∑–≤–æ–ª—è–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –æ—Ü–µ–Ω–∏–≤–∞—Ç—å —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π —Å–∂–∏–º–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏ —É–º–µ–Ω—å—à–∞—Ç—å –∏–∑–±—ã—Ç–æ—á–Ω–æ—Å—Ç—å. –ü–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω–æ–π –º–µ—Ç—Ä–∏–∫–æ–π Matrix Entropy, –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∏–º–µ–µ—Ç –º–µ–Ω—å—à—É—é –≤—ã—á–∏—Å–ª–∏
[18.10.2024 00:58] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –Ω–æ–≤–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –ê–≤—Ç–æ—Ä—ã –≤–≤–æ–¥—è—Ç –ø–æ–Ω—è—Ç–∏–µ '—Ä–æ–¥—Å—Ç–≤–∞ –º–æ–¥–µ–ª–µ–π', –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ–µ –±–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–π —ç–≤–æ–ª—é—Ü–∏–∏, –∏ –∏—Å—Å–ª–µ–¥—É—é—Ç –µ–≥–æ —Å–≤—è–∑—å —Å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é –ø–æ—Å–ª–µ —Å–ª–∏—è–Ω–∏—è. –ù–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–æ–≥–æ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è 'Top-k –ñ–∞–¥–Ω–æ–≥–æ –°
[18.10.2024 00:58] Using data from previous issue: {"desc": "DocLayout-YOLO - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∞–Ω–∞–ª–∏–∑—É –º–∞–∫–µ—Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É—é—â–∏–π —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ —Å–∫–æ—Ä–æ—Å—Ç–∏ –∏ —Ç–æ—á–Ω–æ—Å—Ç–∏. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∞–ª–≥–æ—Ä–∏—Ç–º Mesh-candidate BestFit –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö DocSynth-300K, —á—Ç–æ —É–ª—É—á—à–∞–µ—Ç –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏. –í –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –≤–Ω–µ–¥—Ä–µ–Ω –º–æ–¥—É–ª—å Global-to-
[18.10.2024 00:58] Using data from previous issue: {"desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç ProSA - —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –∫ –ø—Ä–æ–º–ø—Ç–∞–º. –û–Ω–∏ –≤–≤–æ–¥—è—Ç –Ω–æ–≤—É—é –º–µ—Ç—Ä–∏–∫—É PromptSensiScore –∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –æ–±—ä—è—Å–Ω–µ–Ω–∏—è –º–µ—Ö–∞–Ω–∏–∑–º–æ–≤ —Ä–∞–±–æ—Ç—ã LLM. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∫ –ø—Ä
[18.10.2024 00:58] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—é –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Controllable Safety Alignment (CoSA). –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞ '–æ–¥–∏–Ω —Ä–∞–∑–º–µ—Ä –¥–ª—è –≤—Å–µ—Ö', CoSA –ø–æ–∑–≤–æ–ª—è–µ—Ç –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å –º–æ–¥–µ–ª–∏ –∫ —Ä–∞–∑–ª–∏—á–Ω—ã–º —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
[18.10.2024 00:58] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ LongAlign –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ –¥–ª–∏–Ω–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–≤—ã–º –æ–ø–∏—Å–∞–Ω–∏—è–º. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —Å–µ–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ –∏ –¥–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏—é –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –¥–ª—è –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π. –ú–µ—Ç–æ–¥ –ø—Ä–µ–æ–¥–æ–ª–µ–≤–∞–µ—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏
[18.10.2024 00:58] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é –º–æ–¥–µ–ª–µ–π —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ (LSR) –ø—É—Ç–µ–º –æ–±–æ–≥–∞—â–µ–Ω–∏—è –∏—Ö —Å–ª–æ–≤–∞—Ä—è –∫–æ–Ω—Ü–µ–ø—Ü–∏—è–º–∏ –∏ —Å—É—â–Ω–æ—Å—Ç—è–º–∏ –∏–∑ –í–∏–∫–∏–ø–µ–¥–∏–∏. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫—É—é –≥–æ–ª–æ–≤–Ω—É—é —á–∞—Å—Ç—å –º–æ–¥–µ–ª–∏ (DyVo), –∫–æ—Ç–æ—Ä–∞—è –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –≤–ª–æ–∂–µ–Ω–∏—è —Å—É—â–Ω–æ—Å—Ç–µ–π —Å –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–º –ø–æ–∏—Å–∫
[18.10.2024 00:58] Using data from previous issue: {"desc": "ZipVL - —ç—Ç–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –≤—ã–≤–æ–¥–∞ –∫—Ä—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∑—Ä–µ–Ω–∏—è –∏ —è–∑—ã–∫–∞ (LVLM). –û–Ω —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–π –∏ –ø–∞–º—è—Ç–Ω–æ–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ —Å –ø–æ–º–æ—â—å—é –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤–∞–∂–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤. –§—Ä–µ–π–º–≤–æ—Ä–∫ —É—Å–∫–æ—Ä—è–µ—Ç —Ñ–∞–∑—É –ø—Ä–µ–¥–∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è, –≤—ã–ø–æ–ª–Ω—è—è –º–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è —Ç–æ–ª—å–∫–æ –Ω–∞ –≤–∞–∂–Ω—ã—Ö 
[18.10.2024 00:58] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã—Ö –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –º–æ–¥–µ–ª–µ–π –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏ (CMs) –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–∏—Ñ—Ñ—É–∑–∏–∏. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫—É—é –æ—Å–Ω–æ–≤—É, –æ–±—ä—è—Å–Ω—è—é—â—É—é –ø—Ä–∏—á–∏–Ω—ã –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –ø–æ–¥—Ö–æ–¥–æ–≤. –û–Ω–∏ –≤–≤–æ–¥—è—Ç —É–ª—É—á—à–µ–Ω–∏—è –≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏–∑–∞—Ü–∏—é
[18.10.2024 00:58] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç ChroKnowBench - –Ω–æ–≤—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ö—Ä–æ–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –∑–Ω–∞–Ω–∏–π —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç framework ChroKnowledge –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –Ω–µ–ø–∞—Ä–∞–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏—Ö —Ö—Ä–æ–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –∑–Ω–∞–Ω–∏–π LLM. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –∏–∑–≤–ª–µ–∫–∞—Ç—å –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –∑–Ω–∞–Ω–∏—è –∑–∞–≤
[18.10.2024 00:58] Using data from previous issue: {"desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ –∏–∑—É—á–µ–Ω–∏—é –ø–æ—è–≤–ª–µ–Ω–∏—è, –∏—Å—á–µ–∑–Ω–æ–≤–µ–Ω–∏—è –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤ –º–æ–¥–µ–ª—è—Ö, –¥–æ–æ–±—É—á–µ–Ω–Ω—ã—Ö –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –¥–æ–º–µ–Ω–∞—Ö. –ê–≤—Ç–æ—Ä—ã –Ω–∞—á–∏–Ω–∞—é—Ç —Å –±–∞–∑–æ–≤–æ–π –æ–¥–Ω–æ—Å–ª–æ–π–Ω–æ–π –º–æ–¥–µ–ª–∏ Transformer, –æ–±—É—á–µ–Ω–Ω–æ–π –Ω–∞ –∫–æ—Ä–ø—É—Å–µ BabyLM –∏ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ Python-–∫–æ–¥–∞ –∏–∑ The Stack. –ó–∞—Ç–µ–º —ç—Ç–∞ –º–æ–¥–µ–ª—å –∞–¥–∞–ø—Ç–∏—Ä—É–µ—Ç—Å—è –∫ –¥–≤—É
[18.10.2024 00:58] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –ø–∞—Ä–∞–¥–∏–≥–º—É –æ–±—É—á–µ–Ω–∏—è –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Neural Metamorphosis (NeuMeta), –∫–æ—Ç–æ—Ä–∞—è –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–æ–∑–¥–∞–≤–∞—Ç—å —Å–∞–º–æ–∏–∑–º–µ–Ω—è—é—â–∏–µ—Å—è –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏. NeuMeta –æ–±—É—á–∞–µ—Ç –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ –Ω–µ—è–≤–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –≤ –∫–∞—á–µ—Å—Ç–≤–µ –≥–∏–ø–µ—Ä-—Å–µ—Ç–µ–π, –∫–æ—Ç–æ—Ä—ã–µ –≥–µ–Ω–µ—Ä–∏—Ä—É—é—Ç –≤–µ—Å–∞ –¥–ª—è —Å–µ—Ç–µ–π –ª—é–±–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞, –¥–∞–∂–µ –¥–ª—è —Ä–∞–Ω–µ–µ –Ω–µ–≤–∏–¥–∞–Ω
[18.10.2024 00:58] Using data from previous issue: {"desc": "WorldMedQA-V - —ç—Ç–æ –Ω–æ–≤—ã–π –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –∑–¥—Ä–∞–≤–æ–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏. –û–Ω —Å–æ–¥–µ—Ä–∂–∏—Ç 568 –≤–æ–ø—Ä–æ—Å–æ–≤ —Å –≤–∞—Ä–∏–∞–Ω—Ç–∞–º–∏ –æ—Ç–≤–µ—Ç–æ–≤ –∏ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–º–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ –∏–∑ 4 —Å—Ç—Ä–∞–Ω –Ω–∞ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–∞—Ö –∏ –≤ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º –ø–µ—Ä–µ–≤–æ–¥–µ. –î–∞—Ç–∞—Å–µ—Ç –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ—Ü–µ–Ω–∏–≤–∞—Ç—å –ø—Ä–æ–∏
[18.10.2024 00:58] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç OCTAV –∏ –º–æ–¥–µ–ª—å OMCAT –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö. OCTAV —Å–æ–∑–¥–∞–Ω –¥–ª—è –∑–∞—Ö–≤–∞—Ç–∞ –ø–µ—Ä–µ—Ö–æ–¥–æ–≤ —Å–æ–±—ã—Ç–∏–π –º–µ–∂–¥—É –∞—É–¥–∏–æ –∏ –≤–∏–¥–µ–æ –ø–æ—Ç–æ–∫–∞–º–∏. OMCAT –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ RoTE –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≤—Ä–µ–º–µ–Ω–Ω–æ–π –ø—Ä–∏–≤—è–∑–∫–∏ –∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤
[18.10.2024 00:58] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –æ–±—Ä–∞—Ç–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º (IRL) –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –∏—Ö –Ω–µ—è–≤–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø—Ä–æ–≤–æ–¥–∏–ª–∏—Å—å –Ω–∞ LLM —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–æ–≤, –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω—ã—Ö –Ω–∞ —Å–Ω–∏–∂–µ–Ω–∏–µ —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç–∏, —Å –∏–∑–≤–ª–µ—á–µ
[18.10.2024 00:58] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–º—É –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏–∏ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å —Å —ç—Ç–∏–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–º (DiGIT) –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –º–æ–¥–µ–ª–∏ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–π –¥–∏—Ñ—Ñ—É–∑–∏
[18.10.2024 00:58] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –ø–µ—Ä–µ–æ—Ü–µ–Ω–∫–∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM), –æ–±—É—á–µ–Ω–Ω—ã—Ö —Å –ø–æ–º–æ—â—å—é –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ –æ—Ç —á–µ–ª–æ–≤–µ–∫–∞ (RLHF). –ê–≤—Ç–æ—Ä—ã –≤—ã—è–≤–ª—è—é—Ç, —á—Ç–æ –º–æ–¥–µ–ª–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –ü—Ä–æ–∫—Å–∏–º–∞–ª—å–Ω–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø–æ–ª–∏—Ç–∏–∫–∏ (PPO), –∏–º–µ—é—Ç —Å
[18.10.2024 00:58] Querying the API.
[18.10.2024 00:58] Got response. {
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é —Ñ–∞–π–ª–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM-based semantic file system, LSFS). LSFS –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º –∏ –∞–≥–µ–Ω—Ç–∞–º –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–æ–≤–∞—Ç—å —Å —Ñ–∞–π–ª–∞–º–∏ —Å –ø–æ–º–æ—â—å—é –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤, —á—Ç–æ —É–ø—Ä–æ—â–∞–µ—Ç —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ñ–∞–π–ª–∞–º–∏. –°–∏—Å—Ç–µ–º–∞ –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –Ω–∞–±–æ—Ä API –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –æ–ø–µ—Ä–∞—Ü–∏–π —Å —Ñ–∞–π–ª–∞–º–∏, —Ç–∞–∫–∏—Ö –∫–∞–∫ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫, –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π –∏ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è. LSFS –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–µ–∫—Å–æ–≤ —Ñ–∞–π–ª–æ–≤ –∏ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π.",

  "categories": ["#nlp", "#rag", "#dataset", "#multimodal"],

  "emoji": "üìÅ",

  "title": "–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è —Ñ–∞–π–ª–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞: —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ñ–∞–π–ª–∞–º–∏ —Å –ø–æ–º–æ—â—å—é –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞"
}
[18.10.2024 00:58] Get embedding for a paper via LLM API.
[18.10.2024 00:58] Querying the API.
[18.10.2024 00:58] Got response. {
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ä–µ—à–µ–Ω–∏—é –∑–∞–¥–∞—á –≤–æ–ø—Ä–æ—Å–Ω–æ-–æ—Ç–≤–µ—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, –Ω–∞–∑—ã–≤–∞–µ–º—ã–π Faithful Logic-Aided Reasoning and Exploration (FLARE). FLARE –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–µ—à–µ–Ω–∏—è –∏ —Ñ–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –∑–∞–ø—Ä–æ—Å–∞ –≤ –ª–æ–≥–∏—á–µ—Å–∫–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ. –ú–µ—Ç–æ–¥ –≤—ã–ø–æ–ª–Ω—è–µ—Ç –º–Ω–æ–≥–æ—à–∞–≥–æ–≤—ã–π –ø–æ–∏—Å–∫ –ø–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–º—É –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤—É, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ—Ü–µ–Ω–∏—Ç—å –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å –ø—Ä–æ—Ü–µ—Å—Å–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. FLARE –¥–æ—Å—Ç–∏–≥–∞–µ—Ç state-of-the-art —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ 7 –∏–∑ 9 –±–µ–Ω—á–º–∞—Ä–∫–æ–≤ –ø–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—è –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—É—é –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—é –º–µ–∂–¥—É –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å—é –º–æ–¥–µ–ª–∏ –∏ –æ–±—â–µ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é.",
  "categories": ["#nlp", "#rlhf", "#benchmark", "#rag"],
  "emoji": "üß†",
  "title": "FLARE: –õ–æ–≥–∏—á–µ—Å–∫–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ —Å –≥–∞—Ä–∞–Ω—Ç–∏–µ–π –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏"
}
[18.10.2024 00:58] Get embedding for a paper via LLM API.
[18.10.2024 00:58] Loading Chinese text from previous data.
[18.10.2024 00:58] Renaming data file.
[18.10.2024 00:58] Renaming previous data. hf_papers.json to 2024-10-17_hf_papers.json
[18.10.2024 00:58] Saving new data file.
[18.10.2024 00:58] Generating page.
[18.10.2024 00:58] Generating Chinese page for reading.
[18.10.2024 00:58] Chinese vocab [{'word': 'ÁºñÁ†Å', 'pinyin': 'biƒÅn m«é', 'trans': 'coding'}, {'word': '‰ªªÂä°', 'pinyin': 'r√®n w√π', 'trans': 'task'}, {'word': 'ËØÑ‰º∞', 'pinyin': 'p√≠ng g≈´', 'trans': 'evaluation'}, {'word': 'Â§ßÂûã', 'pinyin': 'd√† x√≠ng', 'trans': 'large-scale'}, {'word': 'ËØ≠Ë®ÄÊ®°Âûã', 'pinyin': 'y«î y√°n m√≥ x√≠ng', 'trans': 'language model'}, {'word': 'È´òÁ∫ß', 'pinyin': 'gƒÅo j√≠', 'trans': 'advanced'}, {'word': 'Êåá‰ª§', 'pinyin': 'zh«ê l√¨ng', 'trans': 'instructions'}, {'word': 'Â§çÊùÇ', 'pinyin': 'f√π z√°', 'trans': 'complex'}, {'word': 'Êé®ÁêÜ', 'pinyin': 'tuƒ´ l«ê', 'trans': 'reasoning'}, {'word': 'ÂäüËÉΩ', 'pinyin': 'g≈çng n√©ng', 'trans': 'functional'}, {'word': 'Á®ãÂ∫è', 'pinyin': 'ch√©ng x√π', 'trans': 'program'}, {'word': 'Â§öÊ®°ÊÄÅ', 'pinyin': 'du≈ç m√≥ shu√†i', 'trans': 'multimodal'}, {'word': 'ËßÜËßâ', 'pinyin': 'sh√¨ ju√©', 'trans': 'visual'}, {'word': 'ÊÑüÁü•', 'pinyin': 'g«én zhƒ´', 'trans': 'perception'}, {'word': 'ÁêÜËß£', 'pinyin': 'l«ê jiƒõ', 'trans': 'understanding'}, {'word': 'ËøõÂ±ï', 'pinyin': 'j√¨n zh«én', 'trans': 'progress'}, {'word': '‰∏•Ê†º', 'pinyin': 'y√°n g√©', 'trans': 'strict'}, {'word': 'Âü∫ÂáÜ', 'pinyin': 'jƒ´ zh«în', 'trans': 'benchmark'}, {'word': 'Âº∫Ë∞É', 'pinyin': 'qi√°ng di√†o', 'trans': 'emphasize'}, {'word': 'Á©∫ÁôΩ', 'pinyin': 'k√≤ng b√°i', 'trans': 'gap'}, {'word': 'ÂºïÂÖ•', 'pinyin': 'y«ên r√π', 'trans': 'introduce'}, {'word': '‰∏ìÈó®', 'pinyin': 'zhuƒÅn m√©n', 'trans': 'specifically'}, {'word': 'ËÆæËÆ°', 'pinyin': 'sh√® j√¨', 'trans': 'design'}, {'word': 'Áî®‰∫é', 'pinyin': 'y√≤ng y√∫', 'trans': 'used for'}, {'word': 'ËΩªÈáèÁ∫ß', 'pinyin': 'qƒ´ng li√†ng j√≠', 'trans': 'lightweight'}, {'word': 'ÂàùÁ∫ß', 'pinyin': 'ch≈´ j√≠', 'trans': 'entry-level'}, {'word': 'Á≤æÂøÉ', 'pinyin': 'jƒ´ng xƒ´n', 'trans': 'carefully'}, {'word': 'ËßÜËßâÂÖÉÁ¥†', 'pinyin': 'sh√¨ ju√© yu√°n s√π', 'trans': 'visual elements'}, {'word': 'ËØ¶ÁªÜ', 'pinyin': 'xi√°ng x√¨', 'trans': 'detailed'}, {'word': 'ÊµãËØï', 'pinyin': 'c√® sh√¨', 'trans': 'test'}, {'word': 'Áî®‰æã', 'pinyin': 'y√≤ng l√¨', 'trans': 'case'}, {'word': 'Á†îÁ©∂', 'pinyin': 'y√°n ji≈´', 'trans': 'research'}, {'word': 'ÂèëÁé∞', 'pinyin': 'fƒÅ xi√†n', 'trans': 'find'}, {'word': 'ÂÖàËøõ', 'pinyin': 'xiƒÅn j√¨n', 'trans': 'advanced'}, {'word': 'Èù¢‰∏¥', 'pinyin': 'mi√†n l√≠n', 'trans': 'face'}, {'word': 'ÈáçÂ§ß', 'pinyin': 'zh√≤ng d√†', 'trans': 'major'}, {'word': 'ÊåëÊàò', 'pinyin': 'ti«éo zh√†n', 'trans': 'challenge'}]
[18.10.2024 00:58] Renaming previous page.
[18.10.2024 00:58] Renaming previous data. index.html to 2024-10-17_hf_papers.html
[18.10.2024 00:58] Renaming previous Chinese page.
[18.10.2024 00:58] Renaming previous data. zh.html to 2024-10-17_zh_reading_task.html
[18.10.2024 00:58] Writing result.
[18.10.2024 00:58] Writing Chinese reading task.
[18.10.2024 00:58] Renaming log file.
[18.10.2024 00:58] Renaming previous data. log.txt to 2024-10-17_last_log.txt
