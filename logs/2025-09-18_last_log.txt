[18.09.2025 17:10] Read previous papers.
[18.09.2025 17:10] Generating top page (month).
[18.09.2025 17:10] Writing top page (month).
[18.09.2025 18:16] Read previous papers.
[18.09.2025 18:16] Get feed.
[18.09.2025 18:16] Get page data from previous paper. URL: https://huggingface.co/papers/2509.14008
[18.09.2025 18:16] Get page data from previous paper. URL: https://huggingface.co/papers/2509.14033
[18.09.2025 18:16] Get page data from previous paper. URL: https://huggingface.co/papers/2509.12989
[18.09.2025 18:16] Get page data from previous paper. URL: https://huggingface.co/papers/2509.14232
[18.09.2025 18:16] Get page data from previous paper. URL: https://huggingface.co/papers/2509.13755
[18.09.2025 18:16] Get page data from previous paper. URL: https://huggingface.co/papers/2508.14880
[18.09.2025 18:16] Get page data from previous paper. URL: https://huggingface.co/papers/2509.14142
[18.09.2025 18:16] Get page data from previous paper. URL: https://huggingface.co/papers/2509.14055
[18.09.2025 18:16] Get page data from previous paper. URL: https://huggingface.co/papers/2509.13761
[18.09.2025 18:16] Get page data from previous paper. URL: https://huggingface.co/papers/2509.13523
[18.09.2025 18:16] Get page data from previous paper. URL: https://huggingface.co/papers/2509.13683
[18.09.2025 18:16] Get page data from previous paper. URL: https://huggingface.co/papers/2509.13450
[18.09.2025 18:16] Get page data from previous paper. URL: https://huggingface.co/papers/2509.14026
[18.09.2025 18:16] Get page data from previous paper. URL: https://huggingface.co/papers/2509.14180
[18.09.2025 18:16] Get page data from previous paper. URL: https://huggingface.co/papers/2509.13642
[18.09.2025 18:16] Get page data from previous paper. URL: https://huggingface.co/papers/2509.13353
[18.09.2025 18:16] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[18.09.2025 18:16] No deleted papers detected.
[18.09.2025 18:16] Downloading and parsing papers (pdf, html). Total: 16.
[18.09.2025 18:16] Downloading and parsing paper https://huggingface.co/papers/2509.14008.
[18.09.2025 18:16] Extra JSON file exists (./assets/json/2509.14008.json), skip PDF parsing.
[18.09.2025 18:16] Paper image links file exists (./assets/img_data/2509.14008.json), skip HTML parsing.
[18.09.2025 18:16] Success.
[18.09.2025 18:16] Downloading and parsing paper https://huggingface.co/papers/2509.14033.
[18.09.2025 18:16] Extra JSON file exists (./assets/json/2509.14033.json), skip PDF parsing.
[18.09.2025 18:16] Paper image links file exists (./assets/img_data/2509.14033.json), skip HTML parsing.
[18.09.2025 18:16] Success.
[18.09.2025 18:16] Downloading and parsing paper https://huggingface.co/papers/2509.12989.
[18.09.2025 18:16] Extra JSON file exists (./assets/json/2509.12989.json), skip PDF parsing.
[18.09.2025 18:16] Paper image links file exists (./assets/img_data/2509.12989.json), skip HTML parsing.
[18.09.2025 18:16] Success.
[18.09.2025 18:16] Downloading and parsing paper https://huggingface.co/papers/2509.14232.
[18.09.2025 18:16] Extra JSON file exists (./assets/json/2509.14232.json), skip PDF parsing.
[18.09.2025 18:16] Paper image links file exists (./assets/img_data/2509.14232.json), skip HTML parsing.
[18.09.2025 18:16] Success.
[18.09.2025 18:16] Downloading and parsing paper https://huggingface.co/papers/2509.13755.
[18.09.2025 18:16] Extra JSON file exists (./assets/json/2509.13755.json), skip PDF parsing.
[18.09.2025 18:16] Paper image links file exists (./assets/img_data/2509.13755.json), skip HTML parsing.
[18.09.2025 18:16] Success.
[18.09.2025 18:16] Downloading and parsing paper https://huggingface.co/papers/2508.14880.
[18.09.2025 18:16] Extra JSON file exists (./assets/json/2508.14880.json), skip PDF parsing.
[18.09.2025 18:16] Paper image links file exists (./assets/img_data/2508.14880.json), skip HTML parsing.
[18.09.2025 18:16] Success.
[18.09.2025 18:16] Downloading and parsing paper https://huggingface.co/papers/2509.14142.
[18.09.2025 18:16] Extra JSON file exists (./assets/json/2509.14142.json), skip PDF parsing.
[18.09.2025 18:16] Paper image links file exists (./assets/img_data/2509.14142.json), skip HTML parsing.
[18.09.2025 18:16] Success.
[18.09.2025 18:16] Downloading and parsing paper https://huggingface.co/papers/2509.14055.
[18.09.2025 18:16] Extra JSON file exists (./assets/json/2509.14055.json), skip PDF parsing.
[18.09.2025 18:16] Paper image links file exists (./assets/img_data/2509.14055.json), skip HTML parsing.
[18.09.2025 18:16] Success.
[18.09.2025 18:16] Downloading and parsing paper https://huggingface.co/papers/2509.13761.
[18.09.2025 18:16] Extra JSON file exists (./assets/json/2509.13761.json), skip PDF parsing.
[18.09.2025 18:16] Paper image links file exists (./assets/img_data/2509.13761.json), skip HTML parsing.
[18.09.2025 18:16] Success.
[18.09.2025 18:16] Downloading and parsing paper https://huggingface.co/papers/2509.13523.
[18.09.2025 18:16] Extra JSON file exists (./assets/json/2509.13523.json), skip PDF parsing.
[18.09.2025 18:16] Paper image links file exists (./assets/img_data/2509.13523.json), skip HTML parsing.
[18.09.2025 18:16] Success.
[18.09.2025 18:16] Downloading and parsing paper https://huggingface.co/papers/2509.13683.
[18.09.2025 18:16] Extra JSON file exists (./assets/json/2509.13683.json), skip PDF parsing.
[18.09.2025 18:16] Paper image links file exists (./assets/img_data/2509.13683.json), skip HTML parsing.
[18.09.2025 18:16] Success.
[18.09.2025 18:16] Downloading and parsing paper https://huggingface.co/papers/2509.13450.
[18.09.2025 18:16] Extra JSON file exists (./assets/json/2509.13450.json), skip PDF parsing.
[18.09.2025 18:16] Paper image links file exists (./assets/img_data/2509.13450.json), skip HTML parsing.
[18.09.2025 18:16] Success.
[18.09.2025 18:16] Downloading and parsing paper https://huggingface.co/papers/2509.14026.
[18.09.2025 18:16] Extra JSON file exists (./assets/json/2509.14026.json), skip PDF parsing.
[18.09.2025 18:16] Paper image links file exists (./assets/img_data/2509.14026.json), skip HTML parsing.
[18.09.2025 18:16] Success.
[18.09.2025 18:16] Downloading and parsing paper https://huggingface.co/papers/2509.14180.
[18.09.2025 18:16] Downloading paper 2509.14180 from http://arxiv.org/pdf/2509.14180v1...
[18.09.2025 18:16] Extracting affiliations from text.
[18.09.2025 18:16] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Synthesizing Behaviorally-Grounded Reasoning Chains: Data-Generation Framework for Personal Finance LLMs 5 2 0 2 7 1 ] . [ 1 0 8 1 4 1 . 9 0 5 2 : r a "
[18.09.2025 18:16] Response: []
[18.09.2025 18:16] Extracting affiliations from text.
[18.09.2025 18:16] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Synthesizing Behaviorally-Grounded Reasoning Chains: Data-Generation Framework for Personal Finance LLMs5 2 0 2 7 1 ] . [ 1 0 8 1 4 1 . 9 0 5 2 : r aPersonalized financial advice requires consideration of user goals, constraints, risk tolerance, and jurisdiction. Prior LLM work has focused on support systems for investors and financial planners. Simultaneously, numerous recent studies examine broader personal finance tasks, including budgeting, debt management, retirement, and estate planning, through agentic pipelines that incur high maintenance costs, yielding less than 25% of their expected financial returns. In this study, we introduce novel and reproducible framework that integrates relevant financial context with behavioral finance studies to construct supervision data for end-toend advisors. Using this framework, we create 19k sample reasoning dataset and conduct comprehensive fine-tuning of the Qwen-3-8B model on the dataset. Through held-out test split and blind LLM-jury study, we demonstrate that through careful data curation and behavioral integration, our 8B model achieves performance comparable to significantly larger baselines (14-32B parameters) across factual accuracy, fluency, and personalization metrics while incurring 80% lower costs than the larger counterparts. Keywords: Financial Datasets; Personal Finance; Reasoning Models; Large Language ModelsLegal counseling, healthcare, and finance are among the numerous high-stakes domains in which personalized advice is essential. However, the development of this personalized advice is fraught with obstacles, requiring substantial investments and years of human expertise. Recent research efforts have thoroughly investigated automated decision support systems in various areas, emphasizing their cost-effectiveness. In the financial sector, variety of support systems have been investigated, with particular emphasis on asset recommendations and investment predictions. (Sanz-Cruzado 1 et al., 2024; Luo et al., 2025; Takayanagi et al., 2023) Recent advances in large language models (LLMs) have shown effective performance in acting as decision support systems for investors (Gupta, 2023) and financial planners (Huang et al., 2024). The core advantage of natural language generation presents these automated support systems with unique advantage that was never available in previous applications. This advantage has repeatedly shown its power in linguistic tasks such as streamlining complex financial narratives from extensive documents, corporate discourses, news sources, and social media.(Gueta et al., 2025; Lee and Lay-Ki, 2024) The utility of these models is also being explored in Time series (Liu and Jia, 2025) and Financial reasoning applications (Liu et al., 2025). Notwithstanding this capability, recent research indicates that no model excels across all financial task categories, which include text summarization, sentiment analysis, causal analysis, forecasting, It and text classification (Matlin et al., 2025). has been demonstrated that attaining robust performance frequently necessitates the utilization of large, expensive models, thereby constraining the practicality of these solutions. Due to these inherent limitations and the complexity of financial advisory, many studies focusing on broader financial decision systems have preferred an agentic approach over training financial domain-specific language models. (Okpala et al., 2025; Joshi, 2025; Takayanagi et al., 2025a) Although the initial agentic frameworks focused on answering simple inquiries,(Lakkaraju et al., 2023) recent studies have accelerated the development of these systems to provide practical and actionable advice to the end user (Takayanagi et al., 2025b; Okpala et al., 2025). These agents can now dynamically interact with users and can assist in various tasks such as recommendation, question answering, search, and customer profiling. (Li et al., 2024; Takayanagi et al., 2025a; Han et al., 2024) Although agentic systems demonstrate potential in providing tailored financial advice, their efficacy is hindered by considerable constraints, including the integration with legacy systems, compliance with data security regulations, and high inference (Cemri et al., 2025; Wang et al., 2025). costs. In support of these concerns, recent study by (Meimandi et al., 2025) illustrates that confluence of technical and cost-related factors hinders these applications from realizing even 25% of their anticipated returns. This research also establishes an important differentiation: success in benchmarks does not necessarily equate to success in deployment. In practical terms, these proactive financial advisors frequently encounter swift deterioration in performance within matter of months following their implementation, attributable to the inherent volatility of real-world conditions. Concurrently, studies show that the extent of personalization is often limited by the volume of context and information that can be supplied to an agent, impacting the overall performance. (Zhou et al., 2025; Winder et al., 2024) One of the direct ways to address these limitations is to tune model with domain-specific context that integrates financial, behavioral, and psychological information. This work aims to close this gap by providing reproducible framework to generate financial advice through well-structured chain-of-thought. In particular, the framework constructs supervision data to train models to (a) provide personalized guidance for users financial dilemmas, (b) reliably apply core financial knowledge, and (c) recognize and mitigate user-side behavioral biases by integrating behavioral and historical evidence. To address these limitations, we propose novel, data-centric framework for synthesising behaviorally-grounded reasoning chains. Rather than relying on complex agentic architectures, our approach directly bakes financial, behavioural, and psychological knowledge into the training data itself. Crucially, we treat the inference of the users psychological state not as an afterthought, but as standalone, foundational phase in the reasoning chain. This design choice is directly motivated by recent findings that users trust and engagement are heavily influenced by the persona of the advisor (Takayanagi et al., 2025a), not just the raw accuracy of its advice. By isolating and explicitly modelling this psychological dimension, our framework ensures that personalisation and empathetic fr"
[18.09.2025 18:16] Mistral response. {"id": "7ed911ee7287486bbdb4a76066e38491", "created": 1758219396, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1387, "total_tokens": 1389, "completion_tokens": 2}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "[]"}}]}
[18.09.2025 18:16] Response: []
[18.09.2025 18:16] Deleting PDF ./assets/pdf/2509.14180.pdf.
[18.09.2025 18:16] Success.
[18.09.2025 18:16] Downloading and parsing paper https://huggingface.co/papers/2509.13642.
[18.09.2025 18:16] Extra JSON file exists (./assets/json/2509.13642.json), skip PDF parsing.
[18.09.2025 18:16] Paper image links file exists (./assets/img_data/2509.13642.json), skip HTML parsing.
[18.09.2025 18:16] Success.
[18.09.2025 18:16] Downloading and parsing paper https://huggingface.co/papers/2509.13353.
[18.09.2025 18:16] Extra JSON file exists (./assets/json/2509.13353.json), skip PDF parsing.
[18.09.2025 18:16] Paper image links file exists (./assets/img_data/2509.13353.json), skip HTML parsing.
[18.09.2025 18:16] Success.
[18.09.2025 18:16] Enriching papers with extra data.
[18.09.2025 18:16] ********************************************************************************
[18.09.2025 18:16] Abstract 0. Hala, a family of Arabic-centric instruction and translation models, achieves state-of-the-art results using a translate-and-tune pipeline, slerp merging, and fine-tuning on high-quality bilingual supervision.  					AI-generated summary 				 We present Hala, a family of Arabic-centric instruction an...
[18.09.2025 18:16] ********************************************************************************
[18.09.2025 18:16] Abstract 1. SAIL-VL2, a vision-language foundation model, achieves state-of-the-art performance across diverse benchmarks through data curation, progressive training, and sparse MoE architecture.  					AI-generated summary 				 We introduce SAIL-VL2, an open-suite vision-language foundation model (LVM) for comp...
[18.09.2025 18:16] ********************************************************************************
[18.09.2025 18:16] Abstract 2. Recent advancements in omnidirectional vision, driven by industrial and academic interest, have led to breakthroughs in generation, perception, and understanding, with the proposal of a new system architecture called PANORAMA.  					AI-generated summary 				 Omnidirectional vision, using 360-degree ...
[18.09.2025 18:16] ********************************************************************************
[18.09.2025 18:16] Abstract 3. GenExam is a benchmark for evaluating text-to-image generation in exam-style settings across multiple disciplines, highlighting the challenges in integrating knowledge, reasoning, and generation.  					AI-generated summary 				 Exams are a fundamental test of expert-level intelligence and require in...
[18.09.2025 18:16] ********************************************************************************
[18.09.2025 18:16] Abstract 4. CodeEraser effectively and efficiently removes sensitive memorized information from Code Language Models using machine unlearning techniques without full retraining.  					AI-generated summary 				 While Code Language Models (CLMs) have demonstrated superior performance in software engineering tasks...
[18.09.2025 18:16] ********************************************************************************
[18.09.2025 18:16] Abstract 5. A medical deep research agent using medical knowledge graphs and a custom retrieval engine achieves state-of-the-art performance on medical benchmarks while maintaining competitiveness in general research tasks.  					AI-generated summary 				 Recent developments in Large Language Model (LLM)-based ...
[18.09.2025 18:16] ********************************************************************************
[18.09.2025 18:16] Abstract 6. The MARS2 2025 Challenge focuses on multimodal reasoning with large language models through real-world and specialized scenarios, evaluating 40+ models across three competition tracks using tailored datasets.  					AI-generated summary 				 This paper reviews the MARS2 2025 Challenge on Multimodal R...
[18.09.2025 18:16] ********************************************************************************
[18.09.2025 18:16] Abstract 7. Wan-Animate is a unified framework for character animation and replacement, using spatially-aligned skeleton signals and implicit facial features to generate high-fidelity character videos with seamless environmental integration.  					AI-generated summary 				 We introduce Wan-Animate, a unified fr...
[18.09.2025 18:16] ********************************************************************************
[18.09.2025 18:16] Abstract 8. THOR, a tool-integrated hierarchical optimization framework using RL, enhances mathematical reasoning and code generation by constructing high-quality datasets, optimizing reasoning paths, and correcting errors during inference.  					AI-generated summary 				 Large Language Models (LLMs) have made ...
[18.09.2025 18:16] ********************************************************************************
[18.09.2025 18:16] Abstract 9. AERIS, a large-scale pixel-level Swin diffusion transformer, addresses scaling issues in high-resolution weather forecasting using SWiPe parallelism, achieving high performance and stability.  					AI-generated summary 				 Generative machine learning offers new opportunities to better understand co...
[18.09.2025 18:16] ********************************************************************************
[18.09.2025 18:16] Abstract 10. CARE, a retrieval-augmented reasoning framework, enhances LLMs by integrating in-context evidence, improving retrieval accuracy and answer generation performance.  					AI-generated summary 				 Large language models (LLMs) often struggle with context fidelity, producing inconsistent answers when re...
[18.09.2025 18:16] ********************************************************************************
[18.09.2025 18:16] Abstract 11. SteeringControl evaluates representation steering methods across bias, harmful generation, and hallucination, revealing tradeoffs and entanglement effects on secondary behaviors like sycophancy and commonsense morality.  					AI-generated summary 				 We introduce SteeringControl, a benchmark for ev...
[18.09.2025 18:16] ********************************************************************************
[18.09.2025 18:16] Abstract 12. Quantum variational activation functions (QVAFs) and quantum-inspired Kolmogorov-Arnold networks (QKANs) enhance parameter efficiency and expressivity in quantum machine learning, offering scalability and improved performance in various tasks.  					AI-generated summary 				 Variational quantum circ...
[18.09.2025 18:16] ********************************************************************************
[18.09.2025 18:16] Abstract 13. A novel framework integrates financial context and behavioral finance to fine-tune a Qwen-3-8B model for personalized financial advice, achieving performance comparable to larger models with lower costs.  					AI-generated summary 				 Personalized financial advice requires consideration of user goa...
[18.09.2025 18:16] ********************************************************************************
[18.09.2025 18:16] Abstract 14. LLM-Interleaved (LLM-I) is a flexible framework that uses a central LLM to orchestrate a toolkit of specialized visual tools, achieving state-of-the-art performance in image-text generation through reinforcement learning and a novel scaling strategy.  					AI-generated summary 				 We propose LLM-In...
[18.09.2025 18:16] ********************************************************************************
[18.09.2025 18:16] Abstract 15. Hybrid quantum-classical neural networks outperform classical models in accuracy, training efficiency, and parameter scalability across various datasets, especially for complex vision tasks.  					AI-generated summary 				 This study presents a systematic comparison between hybrid quantum-classical ...
[18.09.2025 18:16] Read previous papers.
[18.09.2025 18:16] Generating reviews via LLM API.
[18.09.2025 18:16] Using data from previous issue: {"categories": ["#small_models", "#data", "#open_source", "#low_resource", "#training", "#multilingual", "#machine_translation", "#dataset"], "emoji": "üïå", "ru": {"title": "–ü—Ä–æ—Ä—ã–≤ –≤ –∞—Ä–∞–±–æ—è–∑—ã—á–Ω–æ–º –ò–ò: –º–æ–¥–µ–ª–∏ Hala —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—é—Ç –Ω–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç", "desc": "Hala - —ç—Ç–æ —Å–µ–º–µ–π—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, 
[18.09.2025 18:16] Using data from previous issue: {"categories": ["#architecture", "#data", "#agi", "#dataset", "#multimodal", "#reasoning", "#open_source", "#training", "#benchmark"], "emoji": "üß†", "ru": {"title": "SAIL-VL2: –ü–µ—Ä–µ–¥–æ–≤–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –∑—Ä–µ–Ω–∏—è –∏ —è–∑—ã–∫–∞", "desc": "SAIL-VL2 - —ç—Ç–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å –¥–ª—è 
[18.09.2025 18:16] Using data from previous issue: {"categories": ["#3d", "#robotics", "#architecture"], "emoji": "üåê", "ru": {"title": "PANORAMA: –Ω–æ–≤—ã–π –≤–∑–≥–ª—è–¥ –Ω–∞ –≤—Å–µ–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–µ –∑—Ä–µ–Ω–∏–µ –≤ —ç–ø–æ—Ö—É –≤–æ–ø–ª–æ—â–µ–Ω–Ω–æ–≥–æ –ò–ò", "desc": "–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç —Ä–∞–∑–≤–∏—Ç–∏–µ –≤—Å–µ–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è –≤ –æ–±–ª–∞—Å—Ç–∏ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç –Ω–æ–≤—É—é —Å–∏—Å—Ç–µ–º–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç
[18.09.2025 18:16] Using data from previous issue: {"categories": ["#games", "#agi", "#reasoning", "#cv", "#benchmark"], "emoji": "üé®", "ru": {"title": "GenExam: –≠–∫–∑–∞–º–µ–Ω –¥–ª—è –ò–ò –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "GenExam - —ç—Ç–æ –Ω–æ–≤—ã–π —ç—Ç–∞–ª–æ–Ω–Ω—ã–π —Ç–µ—Å—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ç–µ–∫—Å—Ç—É –≤ —ç–∫–∑–∞–º–µ–Ω–∞—Ü–∏–æ–Ω–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ –ø–æ —Ä–∞–∑–ª–∏—á–Ω—ã–º –¥–∏—Å—Ü–∏–ø–ª–∏–Ω–∞–º. –û–Ω –≤–∫–ª—é—á–∞–µ
[18.09.2025 18:16] Using data from previous issue: {"categories": ["#leakage", "#inference", "#data", "#training", "#security"], "emoji": "üîê", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Å—Ç–∏—Ä–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ –ò–ò –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç CodeEraser - –º–µ—Ç–æ–¥ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è –∫–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ –º–æ–¥–µ–ª–µ–π –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è (CLM) –±–µ–∑ –ø–æ–ª–Ω–æ–π –ø
[18.09.2025 18:16] Using data from previous issue: {"categories": ["#games", "#agents", "#optimization", "#open_source", "#science", "#dataset", "#training", "#architecture", "#graphs", "#healthcare"], "emoji": "ü©∫", "ru": {"title": "–£–º–Ω—ã–π –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç: –ø—Ä–æ—Ä—ã–≤ –≤ AI-–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ 
[18.09.2025 18:16] Using data from previous issue: {"categories": ["#dataset", "#multimodal", "#reasoning", "#open_source", "#benchmark", "#survey"], "emoji": "ü§ñ", "ru": {"title": "–ù–æ–≤—ã–π —Ä—É–±–µ–∂ –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–º –ò–ò: —Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏–µ MARS2 2025", "desc": "–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç —Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏–µ MARS2 2025 –ø–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–º—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –±–æ–ª—å—à–∏—Ö —è–∑—ã
[18.09.2025 18:16] Using data from previous issue: {"categories": ["#architecture", "#video", "#multimodal", "#open_source", "#cv"], "emoji": "üé≠", "ru": {"title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –∞–Ω–∏–º–∞—Ü–∏–∏ –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π: –æ—Ç —Å–∫–µ–ª–µ—Ç–∞ –¥–æ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ–≥–æ –≤–∏–¥–µ–æ", "desc": "Wan-Animate - —ç—Ç–æ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –∞–Ω–∏–º–∞—Ü–∏–∏ –∏ –∑–∞–º–µ–Ω—ã –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π –≤ –≤–∏–¥–µ–æ. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç
[18.09.2025 18:16] Using data from previous issue: {"categories": ["#inference", "#math", "#rl", "#dataset", "#reasoning", "#training", "#benchmark", "#optimization"], "emoji": "üß†", "ru": {"title": "THOR: –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∏ –∫–æ–¥–∞ —Å –ø–æ–º–æ—â—å—é –ò–ò", "desc": "THOR - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥
[18.09.2025 18:16] Using data from previous issue: {"categories": ["#science", "#diffusion", "#training", "#dataset", "#optimization", "#architecture"], "emoji": "üå¶Ô∏è", "ru": {"title": "AERIS: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –≤—ã—Å–æ–∫–æ—Ç–æ—á–Ω–æ–º –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–∏ –ø–æ–≥–æ–¥—ã —Å –ø–æ–º–æ—â—å—é –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤", "desc": "AERIS - —ç—Ç–æ –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã–π –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä Swin –¥–ª—è 
[18.09.2025 18:16] Using data from previous issue: {"categories": ["#benchmark", "#rag", "#reasoning", "#optimization"], "emoji": "üß†", "ru": {"title": "CARE: –°–∞–º–æ–æ–±—É—á–µ–Ω–∏–µ LLM —Ä–∞–±–æ—Ç–µ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º", "desc": "CARE - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞, –∫–æ—Ç–æ—Ä–∞—è —É–ª—É—á—à–∞–µ—Ç —Ä–∞–±–æ—Ç—É –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –ø—É—Ç–µ–º –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã—Ö –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤ –≤ –ø—Ä–æ—Ü–µ—Å—Å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π
[18.09.2025 18:16] Using data from previous issue: {"categories": ["#data", "#alignment", "#ethics", "#hallucinations", "#benchmark", "#dataset"], "emoji": "üéõÔ∏è", "ru": {"title": "SteeringControl: –ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –º–µ—Ç–æ–¥–æ–≤ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ò–ò", "desc": "SteeringControl - —ç—Ç–æ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–µ—Ç–æ–¥–æ–≤ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è–º–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –∫–ª—é—á–µ–≤—ã—Ö
[18.09.2025 18:16] Using data from previous issue: {"categories": ["#architecture", "#training", "#data"], "emoji": "üî¨", "ru": {"title": "–ö–≤–∞–Ω—Ç–æ–≤—ã–π –ø—Ä–æ—Ä—ã–≤ –≤ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –≤ –∫–≤–∞–Ω—Ç–æ–≤–æ–º –º–∞—à–∏–Ω–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏ - –∫–≤–∞–Ω—Ç–æ–≤—ã–µ –≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω—ã–µ –∞–∫—Ç–∏–≤–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ (QVAF) –∏ –∫–≤–∞–Ω—Ç–æ–≤–æ-–≤–¥–æ—Ö–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ —Å–µ—Ç–∏ –ö–æ–ª
[18.09.2025 18:16] Using data from previous issue: {"categories": ["#agents", "#dataset", "#alignment", "#data", "#optimization", "#training", "#science", "#reasoning"], "emoji": "üí∞", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ —Å–æ–≤–µ—Ç—ã —Å –ø–æ–º–æ—â—å—é –∫–æ–º–ø–∞–∫—Ç–Ω–æ–π –ò–ò-–º–æ–¥–µ–ª–∏", "desc": "–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã
[18.09.2025 18:16] Using data from previous issue: {"categories": ["#rl", "#games", "#multimodal", "#agents", "#optimization", "#benchmark", "#diffusion", "#dataset", "#rag"], "emoji": "üß†", "ru": {"title": "LLM-I: –û—Ä–∫–µ—Å—Ç—Ä –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –ø–æ–¥ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º –ò–ò", "desc": "LLM-Interleaved (LLM-I) - —ç—Ç–æ –≥–∏–±–∫–∞—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è —Å–∏—Å—Ç–µ–º–∞, –∫–æ—Ç–æ—Ä–∞—è –∏—Å–ø–æ–ª—å–∑—É
[18.09.2025 18:16] Using data from previous issue: {"categories": ["#cv", "#optimization", "#training", "#dataset", "#architecture", "#security", "#benchmark"], "emoji": "üî¨", "ru": {"title": "–ö–≤–∞–Ω—Ç–æ–≤–æ-–∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏: –Ω–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –≤ –º–∞—à–∏–Ω–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –≥–∏–±—Ä–∏–¥–Ω—ã—Ö –∫–≤–∞–Ω—Ç
[18.09.2025 18:16] Renaming data file.
[18.09.2025 18:16] Renaming previous data. hf_papers.json to ./d/2025-09-18.json
[18.09.2025 18:16] Saving new data file.
[18.09.2025 18:16] Generating page.
[18.09.2025 18:16] Renaming previous page.
[18.09.2025 18:16] Renaming previous data. index.html to ./d/2025-09-18.html
[18.09.2025 18:16] Writing result.
[18.09.2025 18:16] Renaming log file.
[18.09.2025 18:16] Renaming previous data. log.txt to ./logs/2025-09-18_last_log.txt
