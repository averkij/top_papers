[28.09.2025 18:29] Read previous papers.
[28.09.2025 18:29] Generating top page (month).
[28.09.2025 18:29] Writing top page (month).
[29.09.2025 00:52] Read previous papers.
[29.09.2025 00:52] Get feed.
[29.09.2025 00:52] Get page data from previous paper. URL: https://huggingface.co/papers/2509.19803
[29.09.2025 00:52] Get page data from previous paper. URL: https://huggingface.co/papers/2509.21268
[29.09.2025 00:52] Get page data from previous paper. URL: https://huggingface.co/papers/2509.21320
[29.09.2025 00:52] Get page data from previous paper. URL: https://huggingface.co/papers/2509.21240
[29.09.2025 00:52] Get page data from previous paper. URL: https://huggingface.co/papers/2509.20427
[29.09.2025 00:52] Get page data from previous paper. URL: https://huggingface.co/papers/2509.21245
[29.09.2025 00:52] Get page data from previous paper. URL: https://huggingface.co/papers/2509.21138
[29.09.2025 00:52] Get page data from previous paper. URL: https://huggingface.co/papers/2509.21117
[29.09.2025 00:52] Get page data from previous paper. URL: https://huggingface.co/papers/2509.20712
[29.09.2025 00:52] Get page data from previous paper. URL: https://huggingface.co/papers/2509.20186
[29.09.2025 00:52] Get page data from previous paper. URL: https://huggingface.co/papers/2509.19301
[29.09.2025 00:52] Get page data from previous paper. URL: https://huggingface.co/papers/2509.21114
[29.09.2025 00:52] Get page data from previous paper. URL: https://huggingface.co/papers/2509.21278
[29.09.2025 00:52] Get page data from previous paper. URL: https://huggingface.co/papers/2509.21072
[29.09.2025 00:52] Get page data from previous paper. URL: https://huggingface.co/papers/2509.14662
[29.09.2025 00:52] Get page data from previous paper. URL: https://huggingface.co/papers/2509.20136
[29.09.2025 00:52] Get page data from previous paper. URL: https://huggingface.co/papers/2509.19736
[29.09.2025 00:52] Get page data from previous paper. URL: https://huggingface.co/papers/2509.21318
[29.09.2025 00:52] Get page data from previous paper. URL: https://huggingface.co/papers/2509.21070
[29.09.2025 00:52] Get page data from previous paper. URL: https://huggingface.co/papers/2509.20414
[29.09.2025 00:52] Get page data from previous paper. URL: https://huggingface.co/papers/2509.21302
[29.09.2025 00:52] Get page data from previous paper. URL: https://huggingface.co/papers/2509.21106
[29.09.2025 00:52] Get page data from previous paper. URL: https://huggingface.co/papers/2509.21317
[29.09.2025 00:52] Get page data from previous paper. URL: https://huggingface.co/papers/2509.20293
[29.09.2025 00:52] Get page data from previous paper. URL: https://huggingface.co/papers/2509.21113
[29.09.2025 00:52] Get page data from previous paper. URL: https://huggingface.co/papers/2509.21042
[29.09.2025 00:52] Get page data from previous paper. URL: https://huggingface.co/papers/2509.19228
[29.09.2025 00:52] Get page data from previous paper. URL: https://huggingface.co/papers/2509.20868
[29.09.2025 00:52] Get page data from previous paper. URL: https://huggingface.co/papers/2509.20109
[29.09.2025 00:52] Get page data from previous paper. URL: https://huggingface.co/papers/2509.19676
[29.09.2025 00:52] Get page data from previous paper. URL: https://huggingface.co/papers/2509.19282
[29.09.2025 00:52] Get page data from previous paper. URL: https://huggingface.co/papers/2509.20878
[29.09.2025 00:52] Get page data from previous paper. URL: https://huggingface.co/papers/2509.20706
[29.09.2025 00:52] Get page data from previous paper. URL: https://huggingface.co/papers/2509.20394
[29.09.2025 00:52] Get page data from previous paper. URL: https://huggingface.co/papers/2509.18293
[29.09.2025 00:52] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[29.09.2025 00:52] No deleted papers detected.
[29.09.2025 00:52] Downloading and parsing papers (pdf, html). Total: 35.
[29.09.2025 00:52] Downloading and parsing paper https://huggingface.co/papers/2509.19803.
[29.09.2025 00:52] Extra JSON file exists (./assets/json/2509.19803.json), skip PDF parsing.
[29.09.2025 00:52] Paper image links file exists (./assets/img_data/2509.19803.json), skip HTML parsing.
[29.09.2025 00:52] Success.
[29.09.2025 00:52] Downloading and parsing paper https://huggingface.co/papers/2509.21268.
[29.09.2025 00:52] Extra JSON file exists (./assets/json/2509.21268.json), skip PDF parsing.
[29.09.2025 00:52] Paper image links file exists (./assets/img_data/2509.21268.json), skip HTML parsing.
[29.09.2025 00:52] Success.
[29.09.2025 00:52] Downloading and parsing paper https://huggingface.co/papers/2509.21320.
[29.09.2025 00:52] Extra JSON file exists (./assets/json/2509.21320.json), skip PDF parsing.
[29.09.2025 00:52] Paper image links file exists (./assets/img_data/2509.21320.json), skip HTML parsing.
[29.09.2025 00:52] Success.
[29.09.2025 00:52] Downloading and parsing paper https://huggingface.co/papers/2509.21240.
[29.09.2025 00:52] Extra JSON file exists (./assets/json/2509.21240.json), skip PDF parsing.
[29.09.2025 00:52] Paper image links file exists (./assets/img_data/2509.21240.json), skip HTML parsing.
[29.09.2025 00:52] Success.
[29.09.2025 00:52] Downloading and parsing paper https://huggingface.co/papers/2509.20427.
[29.09.2025 00:52] Extra JSON file exists (./assets/json/2509.20427.json), skip PDF parsing.
[29.09.2025 00:52] Paper image links file exists (./assets/img_data/2509.20427.json), skip HTML parsing.
[29.09.2025 00:52] Success.
[29.09.2025 00:52] Downloading and parsing paper https://huggingface.co/papers/2509.21245.
[29.09.2025 00:52] Extra JSON file exists (./assets/json/2509.21245.json), skip PDF parsing.
[29.09.2025 00:52] Paper image links file exists (./assets/img_data/2509.21245.json), skip HTML parsing.
[29.09.2025 00:52] Success.
[29.09.2025 00:52] Downloading and parsing paper https://huggingface.co/papers/2509.21138.
[29.09.2025 00:52] Extra JSON file exists (./assets/json/2509.21138.json), skip PDF parsing.
[29.09.2025 00:52] Paper image links file exists (./assets/img_data/2509.21138.json), skip HTML parsing.
[29.09.2025 00:52] Success.
[29.09.2025 00:52] Downloading and parsing paper https://huggingface.co/papers/2509.21117.
[29.09.2025 00:52] Extra JSON file exists (./assets/json/2509.21117.json), skip PDF parsing.
[29.09.2025 00:52] Paper image links file exists (./assets/img_data/2509.21117.json), skip HTML parsing.
[29.09.2025 00:52] Success.
[29.09.2025 00:52] Downloading and parsing paper https://huggingface.co/papers/2509.20712.
[29.09.2025 00:52] Extra JSON file exists (./assets/json/2509.20712.json), skip PDF parsing.
[29.09.2025 00:52] Paper image links file exists (./assets/img_data/2509.20712.json), skip HTML parsing.
[29.09.2025 00:52] Success.
[29.09.2025 00:52] Downloading and parsing paper https://huggingface.co/papers/2509.20186.
[29.09.2025 00:52] Extra JSON file exists (./assets/json/2509.20186.json), skip PDF parsing.
[29.09.2025 00:52] Paper image links file exists (./assets/img_data/2509.20186.json), skip HTML parsing.
[29.09.2025 00:52] Success.
[29.09.2025 00:52] Downloading and parsing paper https://huggingface.co/papers/2509.19301.
[29.09.2025 00:52] Extra JSON file exists (./assets/json/2509.19301.json), skip PDF parsing.
[29.09.2025 00:52] Paper image links file exists (./assets/img_data/2509.19301.json), skip HTML parsing.
[29.09.2025 00:52] Success.
[29.09.2025 00:52] Downloading and parsing paper https://huggingface.co/papers/2509.21114.
[29.09.2025 00:52] Extra JSON file exists (./assets/json/2509.21114.json), skip PDF parsing.
[29.09.2025 00:52] Paper image links file exists (./assets/img_data/2509.21114.json), skip HTML parsing.
[29.09.2025 00:52] Success.
[29.09.2025 00:52] Downloading and parsing paper https://huggingface.co/papers/2509.21278.
[29.09.2025 00:52] Extra JSON file exists (./assets/json/2509.21278.json), skip PDF parsing.
[29.09.2025 00:52] Paper image links file exists (./assets/img_data/2509.21278.json), skip HTML parsing.
[29.09.2025 00:52] Success.
[29.09.2025 00:52] Downloading and parsing paper https://huggingface.co/papers/2509.21072.
[29.09.2025 00:52] Extra JSON file exists (./assets/json/2509.21072.json), skip PDF parsing.
[29.09.2025 00:52] Paper image links file exists (./assets/img_data/2509.21072.json), skip HTML parsing.
[29.09.2025 00:52] Success.
[29.09.2025 00:52] Downloading and parsing paper https://huggingface.co/papers/2509.14662.
[29.09.2025 00:52] Extra JSON file exists (./assets/json/2509.14662.json), skip PDF parsing.
[29.09.2025 00:52] Paper image links file exists (./assets/img_data/2509.14662.json), skip HTML parsing.
[29.09.2025 00:52] Success.
[29.09.2025 00:52] Downloading and parsing paper https://huggingface.co/papers/2509.20136.
[29.09.2025 00:52] Extra JSON file exists (./assets/json/2509.20136.json), skip PDF parsing.
[29.09.2025 00:52] Paper image links file exists (./assets/img_data/2509.20136.json), skip HTML parsing.
[29.09.2025 00:52] Success.
[29.09.2025 00:52] Downloading and parsing paper https://huggingface.co/papers/2509.19736.
[29.09.2025 00:52] Extra JSON file exists (./assets/json/2509.19736.json), skip PDF parsing.
[29.09.2025 00:52] Paper image links file exists (./assets/img_data/2509.19736.json), skip HTML parsing.
[29.09.2025 00:52] Success.
[29.09.2025 00:52] Downloading and parsing paper https://huggingface.co/papers/2509.21318.
[29.09.2025 00:52] Extra JSON file exists (./assets/json/2509.21318.json), skip PDF parsing.
[29.09.2025 00:52] Paper image links file exists (./assets/img_data/2509.21318.json), skip HTML parsing.
[29.09.2025 00:52] Success.
[29.09.2025 00:52] Downloading and parsing paper https://huggingface.co/papers/2509.21070.
[29.09.2025 00:52] Extra JSON file exists (./assets/json/2509.21070.json), skip PDF parsing.
[29.09.2025 00:52] Paper image links file exists (./assets/img_data/2509.21070.json), skip HTML parsing.
[29.09.2025 00:52] Success.
[29.09.2025 00:52] Downloading and parsing paper https://huggingface.co/papers/2509.20414.
[29.09.2025 00:52] Extra JSON file exists (./assets/json/2509.20414.json), skip PDF parsing.
[29.09.2025 00:52] Paper image links file exists (./assets/img_data/2509.20414.json), skip HTML parsing.
[29.09.2025 00:52] Success.
[29.09.2025 00:52] Downloading and parsing paper https://huggingface.co/papers/2509.21302.
[29.09.2025 00:52] Extra JSON file exists (./assets/json/2509.21302.json), skip PDF parsing.
[29.09.2025 00:52] Paper image links file exists (./assets/img_data/2509.21302.json), skip HTML parsing.
[29.09.2025 00:52] Success.
[29.09.2025 00:52] Downloading and parsing paper https://huggingface.co/papers/2509.21106.
[29.09.2025 00:52] Extra JSON file exists (./assets/json/2509.21106.json), skip PDF parsing.
[29.09.2025 00:52] Paper image links file exists (./assets/img_data/2509.21106.json), skip HTML parsing.
[29.09.2025 00:52] Success.
[29.09.2025 00:52] Downloading and parsing paper https://huggingface.co/papers/2509.21317.
[29.09.2025 00:52] Extra JSON file exists (./assets/json/2509.21317.json), skip PDF parsing.
[29.09.2025 00:52] Paper image links file exists (./assets/img_data/2509.21317.json), skip HTML parsing.
[29.09.2025 00:52] Success.
[29.09.2025 00:52] Downloading and parsing paper https://huggingface.co/papers/2509.20293.
[29.09.2025 00:52] Extra JSON file exists (./assets/json/2509.20293.json), skip PDF parsing.
[29.09.2025 00:52] Paper image links file exists (./assets/img_data/2509.20293.json), skip HTML parsing.
[29.09.2025 00:52] Success.
[29.09.2025 00:52] Downloading and parsing paper https://huggingface.co/papers/2509.21113.
[29.09.2025 00:52] Extra JSON file exists (./assets/json/2509.21113.json), skip PDF parsing.
[29.09.2025 00:52] Paper image links file exists (./assets/img_data/2509.21113.json), skip HTML parsing.
[29.09.2025 00:52] Success.
[29.09.2025 00:52] Downloading and parsing paper https://huggingface.co/papers/2509.21042.
[29.09.2025 00:52] Extra JSON file exists (./assets/json/2509.21042.json), skip PDF parsing.
[29.09.2025 00:52] Paper image links file exists (./assets/img_data/2509.21042.json), skip HTML parsing.
[29.09.2025 00:52] Success.
[29.09.2025 00:52] Downloading and parsing paper https://huggingface.co/papers/2509.19228.
[29.09.2025 00:52] Extra JSON file exists (./assets/json/2509.19228.json), skip PDF parsing.
[29.09.2025 00:52] Paper image links file exists (./assets/img_data/2509.19228.json), skip HTML parsing.
[29.09.2025 00:52] Success.
[29.09.2025 00:52] Downloading and parsing paper https://huggingface.co/papers/2509.20868.
[29.09.2025 00:52] Extra JSON file exists (./assets/json/2509.20868.json), skip PDF parsing.
[29.09.2025 00:52] Paper image links file exists (./assets/img_data/2509.20868.json), skip HTML parsing.
[29.09.2025 00:52] Success.
[29.09.2025 00:52] Downloading and parsing paper https://huggingface.co/papers/2509.20109.
[29.09.2025 00:52] Extra JSON file exists (./assets/json/2509.20109.json), skip PDF parsing.
[29.09.2025 00:52] Paper image links file exists (./assets/img_data/2509.20109.json), skip HTML parsing.
[29.09.2025 00:52] Success.
[29.09.2025 00:52] Downloading and parsing paper https://huggingface.co/papers/2509.19676.
[29.09.2025 00:52] Extra JSON file exists (./assets/json/2509.19676.json), skip PDF parsing.
[29.09.2025 00:52] Paper image links file exists (./assets/img_data/2509.19676.json), skip HTML parsing.
[29.09.2025 00:52] Success.
[29.09.2025 00:52] Downloading and parsing paper https://huggingface.co/papers/2509.19282.
[29.09.2025 00:52] Extra JSON file exists (./assets/json/2509.19282.json), skip PDF parsing.
[29.09.2025 00:52] Paper image links file exists (./assets/img_data/2509.19282.json), skip HTML parsing.
[29.09.2025 00:52] Success.
[29.09.2025 00:52] Downloading and parsing paper https://huggingface.co/papers/2509.20878.
[29.09.2025 00:52] Extra JSON file exists (./assets/json/2509.20878.json), skip PDF parsing.
[29.09.2025 00:52] Paper image links file exists (./assets/img_data/2509.20878.json), skip HTML parsing.
[29.09.2025 00:52] Success.
[29.09.2025 00:52] Downloading and parsing paper https://huggingface.co/papers/2509.20706.
[29.09.2025 00:52] Extra JSON file exists (./assets/json/2509.20706.json), skip PDF parsing.
[29.09.2025 00:52] Paper image links file exists (./assets/img_data/2509.20706.json), skip HTML parsing.
[29.09.2025 00:52] Success.
[29.09.2025 00:52] Downloading and parsing paper https://huggingface.co/papers/2509.20394.
[29.09.2025 00:52] Extra JSON file exists (./assets/json/2509.20394.json), skip PDF parsing.
[29.09.2025 00:52] Paper image links file exists (./assets/img_data/2509.20394.json), skip HTML parsing.
[29.09.2025 00:52] Success.
[29.09.2025 00:52] Downloading and parsing paper https://huggingface.co/papers/2509.18293.
[29.09.2025 00:52] Extra JSON file exists (./assets/json/2509.18293.json), skip PDF parsing.
[29.09.2025 00:52] Paper image links file exists (./assets/img_data/2509.18293.json), skip HTML parsing.
[29.09.2025 00:52] Success.
[29.09.2025 00:52] Enriching papers with extra data.
[29.09.2025 00:52] ********************************************************************************
[29.09.2025 00:52] Abstract 0. A curriculum reinforcement learning framework dynamically adjusts training sample difficulty based on reward variance, improving LLM performance on mathematical reasoning tasks.  					AI-generated summary 				 Policy-based reinforcement learning currently plays an important role in improving LLMs on...
[29.09.2025 00:52] ********************************************************************************
[29.09.2025 00:52] Abstract 1. Variance-Aware Sampling and large-scale CoT data improve multimodal reasoning models by stabilizing RL fine-tuning and enhancing performance on benchmarks.  					AI-generated summary 				 Large multimodal reasoning models have achieved rapid progress, but their advancement is constrained by two majo...
[29.09.2025 00:52] ********************************************************************************
[29.09.2025 00:52] Abstract 2. A scientific reasoning foundation model pre-trained on diverse scientific data supports multiple tasks and enhances cross-domain generalization and fidelity through specialized training techniques.  					AI-generated summary 				 We present a scientific reasoning foundation model that aligns natural...
[29.09.2025 00:52] ********************************************************************************
[29.09.2025 00:52] Abstract 3. Tree-based Group Relative Policy Optimization (Tree-GRPO) enhances reinforcement learning for large language models by using tree search to improve rollouts and estimate grouped relative advantages, outperforming chain-based methods.  					AI-generated summary 				 Recent advances in reinforcement l...
[29.09.2025 00:52] ********************************************************************************
[29.09.2025 00:52] Abstract 4. Seedream 4.0 is a high-performance multimodal image generation system that integrates text-to-image synthesis, image editing, and multi-image composition using a diffusion transformer and VAE, achieving state-of-the-art results with efficient training and inference.  					AI-generated summary 				 W...
[29.09.2025 00:52] ********************************************************************************
[29.09.2025 00:52] Abstract 5. Hunyuan3D-Omni is a unified 3D asset generation framework that accepts multiple conditioning signals, improving controllability and robustness in production workflows.  					AI-generated summary 				 Recent advances in 3D-native generative models have accelerated asset creation for games, film, and ...
[29.09.2025 00:52] ********************************************************************************
[29.09.2025 00:52] Abstract 6. AutoIntent is an automated machine learning tool for text classification that offers end-to-end automation, including embedding model selection, classifier optimization, and decision threshold tuning, and supports multi-label classification and out-of-scope detection.  					AI-generated summary 				...
[29.09.2025 00:52] ********************************************************************************
[29.09.2025 00:52] Abstract 7. TrustJudge, a probabilistic framework, addresses inconsistencies in LLM-as-a-judge evaluation by using distribution-sensitive scoring and likelihood-aware aggregation, improving accuracy and reliability.  					AI-generated summary 				 The adoption of Large Language Models (LLMs) as automated evalua...
[29.09.2025 00:52] ********************************************************************************
[29.09.2025 00:52] Abstract 8. A novel reinforcement learning algorithm, CE-GPPO, reintroduces gradients from clipped tokens to improve the exploration-exploitation balance in training large language models.  					AI-generated summary 				 Reinforcement learning (RL) has become a powerful paradigm for optimizing large language mo...
[29.09.2025 00:52] ********************************************************************************
[29.09.2025 00:52] Abstract 9. Thinking augmented pre-training improves data efficiency and performance of large language models by augmenting text with automatically generated thinking trajectories.  					AI-generated summary 				 This paper introduces a simple and scalable approach to improve the data efficiency of large langua...
[29.09.2025 00:52] ********************************************************************************
[29.09.2025 00:52] Abstract 10. A residual learning framework combines behavior cloning and reinforcement learning to improve manipulation policies on high-degree-of-freedom systems using sparse binary rewards.  					AI-generated summary 				 Recent advances in behavior cloning (BC) have enabled impressive visuomotor control polic...
[29.09.2025 00:52] ********************************************************************************
[29.09.2025 00:52] Abstract 11. CHARM uses a control-point-based parameterization and autoregressive transformer to generate high-fidelity anime hairstyles efficiently.  					AI-generated summary 				 We present CHARM, a novel parametric representation and generative framework for anime hairstyle modeling. While traditional hair m...
[29.09.2025 00:52] ********************************************************************************
[29.09.2025 00:52] Abstract 12. SHINE is a training-free framework that uses manifold-steered anchor loss and pretrained customization adapters to seamlessly insert objects into new scenes with high fidelity, addressing challenges like complex lighting and diverse inputs.  					AI-generated summary 				 Image composition aims to s...
[29.09.2025 00:52] ********************************************************************************
[29.09.2025 00:52] Abstract 13. Recon-Act, a self-evolving multi-agent framework, improves adaptability and performance on long-horizon web tasks by generating and utilizing generalized tools through reconnaissance and action teams.  					AI-generated summary 				 Recent years, multimodal models have made remarkable strides and pa...
[29.09.2025 00:52] ********************************************************************************
[29.09.2025 00:52] Abstract 14. A novel framework using Schoenfeld's Episode Theory is introduced to analyze the reasoning patterns of Large Reasoning Models in solving math problems, providing a benchmark for machine reasoning.  					AI-generated summary 				 While Large Reasoning Models (LRMs) generate extensive chain-of-thought...
[29.09.2025 00:52] ********************************************************************************
[29.09.2025 00:52] Abstract 15. V-GameGym is a comprehensive benchmark for evaluating code generation in game development, focusing on multimodal evaluation including playability, visual aesthetics, and user engagement.  					AI-generated summary 				 Code large language models have demonstrated remarkable capabilities in programm...
[29.09.2025 00:52] ********************************************************************************
[29.09.2025 00:52] Abstract 16. UserRL framework enhances user-centric RL agents by optimizing reward assignment and user simulation, demonstrating the importance of these factors over model scale.  					AI-generated summary 				 Reinforcement learning (RL) has shown promise in training agentic models that move beyond static bench...
[29.09.2025 00:52] ********************************************************************************
[29.09.2025 00:52] Abstract 17. SD3.5-Flash is an efficient few-step distillation framework that enhances image generation on consumer devices using rectified flow models with innovations like timestep sharing and split-timestep fine-tuning.  					AI-generated summary 				 We present SD3.5-Flash, an efficient few-step distillation...
[29.09.2025 00:52] ********************************************************************************
[29.09.2025 00:52] Abstract 18. ScaleDiff uses an adaptive thinking model to identify and generate difficult mathematical problems, improving the performance of large reasoning models with cost-efficient training.  					AI-generated summary 				 Large Reasoning Models (LRMs) have shown impressive capabilities in complex problem-so...
[29.09.2025 00:52] ********************************************************************************
[29.09.2025 00:52] Abstract 19. SceneWeaver, a reflective agentic framework, uses a language model-based planner to iteratively refine 3D scene synthesis, achieving high physical, visual, and semantic quality across diverse instructions.  					AI-generated summary 				 Indoor scene synthesis has become increasingly important with ...
[29.09.2025 00:52] ********************************************************************************
[29.09.2025 00:52] Abstract 20. QuantVGGT, a quantization framework for Visual Geometry Grounded Transformers, achieves state-of-the-art results with memory reduction and acceleration while maintaining high reconstruction accuracy.  					AI-generated summary 				 Learning-based 3D reconstruction models, represented by Visual Geome...
[29.09.2025 00:52] ********************************************************************************
[29.09.2025 00:52] Abstract 21. BESPOKE is a benchmark for evaluating personalization in search-augmented LLMs using authentic user data and detailed feedback.  					AI-generated summary 				 Search-augmented large language models (LLMs) have advanced information-seeking tasks by integrating retrieval into generation, reducing use...
[29.09.2025 00:52] ********************************************************************************
[29.09.2025 00:52] Abstract 22. IRF, a new recommendation system using natural language commands, improves user satisfaction and business outcomes through a dual-agent architecture and simulation-augmented knowledge distillation.  					AI-generated summary 				 Traditional recommender systems rely on passive feedback mechanisms th...
[29.09.2025 00:52] ********************************************************************************
[29.09.2025 00:52] Abstract 23. LLM-judged benchmarks can produce unreliable rankings due to schema incoherence and factor collapse, which are diagnosed using schematic adherence and psychometric validity.  					AI-generated summary 				 LLM-judged benchmarks are increasingly used to evaluate complex model behaviors, yet their des...
[29.09.2025 00:52] ********************************************************************************
[29.09.2025 00:52] Abstract 24. MOSS-ChatV, a reinforcement learning framework with a DTW-based reward, improves video reasoning consistency and performance across various benchmarks.  					AI-generated summary 				 Video reasoning has emerged as a critical capability for multimodal large language models (MLLMs), requiring models ...
[29.09.2025 00:52] ********************************************************************************
[29.09.2025 00:52] Abstract 25. The causal mask in Transformer decoders induces position-dependent attention patterns, which can interact with explicit positional encodings like RoPE, affecting their relative attention score patterns.  					AI-generated summary 				 While explicit positional encodings such as RoPE are a primary so...
[29.09.2025 00:52] ********************************************************************************
[29.09.2025 00:52] Abstract 26. CompLLM, a soft compression technique for LLMs, divides contexts into segments for efficient, scalable, and reusable compression, enhancing performance and reducing computational costs.  					AI-generated summary 				 Large Language Models (LLMs) face significant computational challenges when proces...
[29.09.2025 00:52] ********************************************************************************
[29.09.2025 00:52] Abstract 27. StyleBench evaluates various reasoning styles across tasks and models, revealing that strategy efficacy depends on model scale and task type.  					AI-generated summary 				 The effectiveness of Large Language Models (LLMs) is heavily influenced by the reasoning strategies, or styles of thought, emp...
[29.09.2025 00:52] ********************************************************************************
[29.09.2025 00:52] Abstract 28. ReflectDrive uses a reflection mechanism with discrete diffusion and pre-trained Diffusion Language Models to generate safe trajectories for autonomous driving systems.  					AI-generated summary 				 End-to-End (E2E) solutions have emerged as a mainstream approach for autonomous driving systems, wi...
[29.09.2025 00:52] ********************************************************************************
[29.09.2025 00:52] Abstract 29. A framework incorporating reasoning into audio classification improves performance through test-time scaling and lightweight retraining of embedding matrices.  					AI-generated summary 				 We propose a framework that enables neural models to "think while listening" to everyday sounds, thereby enha...
[29.09.2025 00:52] ********************************************************************************
[29.09.2025 00:52] Abstract 30. A new benchmark and metric are introduced to evaluate layout-to-image generation models on complex overlapping bounding boxes, along with a fine-tuned model to improve performance.  					AI-generated summary 				 Despite steady progress in layout-to-image generation, current methods still struggle w...
[29.09.2025 00:52] ********************************************************************************
[29.09.2025 00:52] Abstract 31. The study reveals an asymmetry between perceptual optimization and image quality assessment, showing that effective IQA metrics are not always suitable for perceptual optimization, especially under adversarial training, and highlights the importance of discriminator design in optimization.  					AI-...
[29.09.2025 00:52] ********************************************************************************
[29.09.2025 00:52] Abstract 32. MI-Fuse, a denoised label fusion framework, enhances speech emotion recognition in target domains using an API-only LALM and a source-domain SER classifier, achieving better performance than the LALM and other baselines.  					AI-generated summary 				 Large audio-language models (LALMs) show strong...
[29.09.2025 00:52] ********************************************************************************
[29.09.2025 00:52] Abstract 33. The Hazard-Aware System Card (HASC) enhances AI system safety and accountability by integrating security and safety identifiers into a standardized framework.  					AI-generated summary 				 This paper introduces the Hazard-Aware System Card (HASC), a novel framework designed to enhance transparency...
[29.09.2025 00:52] ********************************************************************************
[29.09.2025 00:52] Abstract 34. Evaluation of open-source LLMs for antisemitic content detection using in-context definition and a new Guided-CoT prompt shows improved performance and highlights differences in model utility, explainability, and reliability.  					AI-generated summary 				 Detecting hateful content is a challenging...
[29.09.2025 00:52] Read previous papers.
[29.09.2025 00:52] Generating reviews via LLM API.
[29.09.2025 00:52] Using data from previous issue: {"categories": ["#math", "#training", "#rl", "#optimization", "#reasoning"], "emoji": "üìà", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ —á–µ—Ä–µ–∑ –¥–∏—Å–ø–µ—Ä—Å–∏—é: –∞–¥–∞–ø—Ç–∏–≤–Ω–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å –¥–ª—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è LLM", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ VCRL - –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Ä–µ—à–µ–Ω–∏—é –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á
[29.09.2025 00:52] Using data from previous issue: {"categories": ["#dataset", "#training", "#architecture", "#reasoning", "#benchmark", "#rl", "#optimization", "#multimodal", "#data", "#open_source"], "emoji": "üéØ", "ru": {"title": "–°—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—è RL-–æ–±—É—á–µ–Ω–∏—è —á–µ—Ä–µ–∑ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–∏—Å–ø–µ—Ä—Å–∏–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –º–µ—Ç–æ–¥ Variance
[29.09.2025 00:52] Using data from previous issue: {"categories": ["#dataset", "#training", "#reasoning", "#transfer_learning", "#multimodal", "#data", "#science", "#open_source"], "emoji": "üî¨", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π AI –¥–ª—è –Ω–∞—É—á–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤–æ –≤—Å–µ—Ö –¥–∏—Å—Ü–∏–ø–ª–∏–Ω–∞—Ö", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Å–æ–∑–¥–∞–ª–∏ foundation –º–æ–¥–µ–ª—å –¥–ª—è –Ω–∞—É—á–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, –∫–æ
[29.09.2025 00:52] Using data from previous issue: {"categories": ["#reasoning", "#rlhf", "#rl", "#optimization"], "emoji": "üå≥", "ru": {"title": "–î—Ä–µ–≤–æ–≤–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ –¥–ª—è —É–º–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è AI-–∞–≥–µ–Ω—Ç–æ–≤", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ Tree-GRPO - –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –ø–æ–∏—Å–∫–µ –ø–æ –¥–µ—Ä–µ–≤—É. –ú–µ—Ç–æ–¥ —Ä–µ—à
[29.09.2025 00:52] Using data from previous issue: {"categories": ["#inference", "#training", "#games", "#multimodal", "#cv", "#diffusion"], "emoji": "üé®", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–æ–≤–æ–≥–æ –ø–æ–∫–æ–ª–µ–Ω–∏—è", "desc": "Seedream 4.0 ‚Äî —ç—Ç–æ –≤—ã—Å–æ–∫–æ–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏
[29.09.2025 00:52] Using data from previous issue: {"categories": ["#3d", "#training", "#architecture", "#games", "#synthetic", "#multimodal"], "emoji": "üéÆ", "ru": {"title": "–ú–Ω–æ–≥–æ–º–æ–¥–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç—Ä–æ–ª—å 3D-–≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª—è –∏–≥—Ä–æ–≤–æ–π –∏–Ω–¥—É—Å—Ç—Ä–∏–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ Hunyuan3D-Omni ‚Äî –µ–¥–∏–Ω—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ 3D-–æ–±—ä–µ–∫—Ç–æ–≤ —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º –∫–æ–Ω—Ç—Ä–æ
[29.09.2025 00:52] Using data from previous issue: {"categories": ["#dataset", "#optimization", "#training", "#data"], "emoji": "üéØ", "ru": {"title": "–ü–æ–ª–Ω–∞—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–æ–≤ –æ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–æ —Ä–µ—à–µ–Ω–∏–π", "desc": "AutoIntent –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–æ–≤. –°–∏—Å—Ç–µ–º–∞ –æ–±–µ—Å–ø–µ
[29.09.2025 00:52] Using data from previous issue: {"categories": ["#architecture", "#alignment", "#interpretability", "#data", "#benchmark"], "emoji": "‚öñÔ∏è", "ru": {"title": "–î–µ–ª–∞–µ–º LLM-—Å—É–¥–µ–π —á–µ—Å—Ç–Ω—ã–º–∏: –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –ø—Ä–æ—Ç–∏–≤ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–π –≤ –æ—Ü–µ–Ω–∫–∞—Ö", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –≤—ã—è–≤–∏–ª–∏ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã –≤ —Å–∏—Å—Ç–µ–º–∞—Ö –æ—Ü–µ–Ω–∫–∏, –≥–¥–µ LLM –≤—ã—Å—Ç—É–ø–∞—é—Ç –≤ —Ä–æ–ª–∏ 
[29.09.2025 00:52] Using data from previous issue: {"categories": ["#training", "#reasoning", "#optimization", "#rl"], "emoji": "‚öñÔ∏è", "ru": {"title": "–°–æ—Ö—Ä–∞–Ω—è–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –¥–ª—è –ª—É—á—à–µ–≥–æ –±–∞–ª–∞–Ω—Å–∞ –≤ –æ–±—É—á–µ–Ω–∏–∏ LLM", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º CE-GPPO –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –û—Å–Ω–æ–≤–Ω–∞—è –ø—Ä–æ–±–ª–µ–º–∞ —Å—É—â–µ
[29.09.2025 00:52] Using data from previous issue: {"categories": ["#data", "#training", "#reasoning", "#optimization"], "emoji": "üß†", "ru": {"title": "–î—É–º–∞—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ: –∫–∞–∫ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —É—Å–∫–æ—Ä—è—é—Ç –æ–±—É—á–µ–Ω–∏–µ LLM –≤ —Ç—Ä–∏ —Ä–∞–∑–∞", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è Thinking augmented Pre-Training (TPT), –∫–æ—Ç–æ—Ä–∞—è —É–ª—É—á—à–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏
[29.09.2025 00:52] Using data from previous issue: {"categories": ["#games", "#optimization", "#rl", "#robotics", "#training"], "emoji": "ü§ñ", "ru": {"title": "–û—Å—Ç–∞—Ç–æ—á–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ: –æ—Ç –∏–º–∏—Ç–∞—Ü–∏–∏ –∫ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ—Å—Ç–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥, –∫–æ—Ç–æ—Ä—ã–π –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç behavior cloning –∏ reinforcement learning —á–µ—Ä–µ–∑ –æ—Å—Ç–∞—Ç–æ—á–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ. –ú–µ—Ç–æ–¥ 
[29.09.2025 00:52] Using data from previous issue: {"categories": ["#games", "#synthetic", "#training", "#cv", "#architecture", "#dataset"], "emoji": "üíá", "ru": {"title": "AI —Å–æ–∑–¥–∞—ë—Ç –∞–Ω–∏–º–µ-–ø—Ä–∏—á—ë—Å–∫–∏ —á–µ—Ä–µ–∑ \"—è–∑—ã–∫ –≤–æ–ª–æ—Å\"", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ CHARM - –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∞–Ω–∏–º–µ-–ø—Ä–∏—á—ë—Å–æ–∫ —Å –ø–æ–º–æ—â—å—é AI. –í–º–µ—Å—Ç–æ —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤ –º–æ–¥–µ–ª–∏—Ä
[29.09.2025 00:52] Using data from previous issue: {"categories": ["#training", "#optimization", "#diffusion", "#benchmark", "#cv", "#open_source"], "emoji": "‚ú®", "ru": {"title": "–ë–µ–∑—É–ø—Ä–µ—á–Ω–∞—è –≤—Å—Ç–∞–≤–∫–∞ –æ–±—ä–µ–∫—Ç–æ–≤ –≤ —Å—Ü–µ–Ω—ã –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç SHINE - —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –±–µ—Å—à–æ–≤–Ω–æ–π –≤—Å—Ç–∞–≤–∫–∏ –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –Ω–æ–≤—ã–µ —Å—Ü–µ–Ω—ã —Å –≤—ã—Å–æ–∫–æ–π 
[29.09.2025 00:52] Using data from previous issue: {"categories": ["#dataset", "#agents", "#multimodal", "#optimization", "#agi"], "emoji": "üïµÔ∏è", "ru": {"title": "–†–∞–∑–≤–µ–¥–∫–∞ –∏ –¥–µ–π—Å—Ç–≤–∏–µ: —Å–∞–º–æ–æ–±—É—á–∞—é—â–∏–µ—Å—è –∞–≥–µ–Ω—Ç—ã –¥–ª—è –≤–µ–±-–∑–∞–¥–∞—á", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Recon-Act - —Å–∞–º–æ—ç–≤–æ–ª—é—Ü–∏–æ–Ω–∏—Ä—É—é—â–∏–π –º—É–ª—å—Ç–∏-–∞–≥–µ–Ω—Ç–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã—Ö –∑–∞–¥–∞—á –≤ –≤–µ–±-–±—Ä–∞
[29.09.2025 00:52] Using data from previous issue: {"categories": ["#math", "#reasoning", "#interpretability", "#benchmark"], "emoji": "üß†", "ru": {"title": "–ö–∞—Ä—Ç–æ–≥—Ä–∞—Ñ–∏—è –º—ã—à–ª–µ–Ω–∏—è AI —á–µ—Ä–µ–∑ –ø—Ä–∏–∑–º—É —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–≥–æ –ø–æ–∑–Ω–∞–Ω–∏—è", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–∏–º–µ–Ω–∏–ª–∏ —Ç–µ–æ—Ä–∏—é —ç–ø–∏–∑–æ–¥–æ–≤ –®—ë–Ω—Ñ–µ–ª—å–¥–∞, –∫–ª–∞—Å—Å–∏—á–µ—Å–∫—É—é –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—É—é –º–æ–¥–µ–ª—å —Ä–µ—à–µ–Ω–∏—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á —á–µ–ª–æ–≤–µ–∫–æ–º, 
[29.09.2025 00:52] Using data from previous issue: {"categories": ["#optimization", "#benchmark", "#games", "#multimodal", "#dataset"], "emoji": "üéÆ", "ru": {"title": "–ù–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –æ—Ü–µ–Ω–∫–∏ AI –≤ –≥–µ–π–º–¥–µ–≤–µ: –æ—Ç –∫–æ–¥–∞ –∫ –∏–≥—Ä–∞–±–µ–ª—å–Ω–æ—Å—Ç–∏", "desc": "V-GameGym - —ç—Ç–æ –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞ –≤ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ –∏–≥—Ä, –∫–æ—Ç–æ—Ä—ã–π –≤–∫–ª—é—á–∞–µ—Ç –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—É
[29.09.2025 00:52] Using data from previous issue: {"categories": ["#training", "#open_source", "#reasoning", "#rl", "#rlhf", "#agents", "#optimization"], "emoji": "ü§ù", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ RL –∞–≥–µ–Ω—Ç–æ–≤ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ UserRL - —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ –æ—Ü–µ–Ω–∫–∏ RL –∞–≥–µ–Ω—Ç–æ–≤, –æ—Ä–∏–µ–Ω—Ç
[29.09.2025 00:52] Using data from previous issue: {"categories": ["#dataset", "#training", "#inference", "#optimization", "#data", "#cv", "#diffusion"], "emoji": "‚ö°", "ru": {"title": "–ë—ã—Å—Ç—Ä–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è –≤—Å–µ—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤", "desc": "SD3.5-Flash –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∑–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ —à–∞–≥–æ–≤ –Ω–∞
[29.09.2025 00:52] Using data from previous issue: {"categories": ["#math", "#training", "#optimization", "#transfer_learning", "#reasoning", "#dataset"], "emoji": "üßÆ", "ru": {"title": "–ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–ª–æ–∂–Ω—ã—Ö –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è", "desc": "ScaleDiff –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Å–ª–æ–∂–Ω—ã—Ö –º–∞—Ç–µ–º–∞—Ç–∏—á–µ
[29.09.2025 00:52] Using data from previous issue: {"categories": ["#3d", "#reasoning", "#games", "#alignment", "#agents"], "emoji": "üè†", "ru": {"title": "–£–º–Ω—ã–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç–æ—Ä: AI-–∞–≥–µ–Ω—Ç —Å–æ–∑–¥–∞–µ—Ç —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ 3D –∏–Ω—Ç–µ—Ä—å–µ—Ä—ã —á–µ—Ä–µ–∑ —Å–∞–º–æ—Ä–µ—Ñ–ª–µ–∫—Å–∏—é", "desc": "SceneWeaver - —ç—Ç–æ –∞–≥–µ–Ω—Ç–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ 3D —Å—Ü–µ–Ω, –∫–æ—Ç–æ—Ä–∞—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å-–ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ –¥
[29.09.2025 00:52] Using data from previous issue: {"categories": ["#benchmark", "#3d", "#optimization", "#inference"], "emoji": "üßä", "ru": {"title": "–ö–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –¥–ª—è 3D —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ —Å –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π –ø–æ—Ç–µ—Ä–µ–π –∫–∞—á–µ—Å—Ç–≤–∞", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω QuantVGGT - –ø–µ—Ä–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ Visual Geometry Grounded Transformers, –∫–æ—Ç–æ—Ä—ã
[29.09.2025 00:52] Using data from previous issue: {"categories": ["#survey", "#multimodal", "#dataset", "#alignment", "#benchmark"], "emoji": "üéØ", "ru": {"title": "–ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ–∏—Å–∫–æ–≤—ã—Ö LLM –ø–æ–¥ –∫–∞–∂–¥–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Å–æ–∑–¥–∞–ª–∏ –±–µ–Ω—á–º–∞—Ä–∫ BESPOKE –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏ –≤ –ø–æ–∏—Å–∫–æ–≤—ã—Ö LLM, –∫–æ—Ç–æ—Ä—ã–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏
[29.09.2025 00:52] Using data from previous issue: {"categories": ["#training", "#architecture", "#reasoning", "#optimization", "#multimodal", "#agents"], "emoji": "üó£Ô∏è", "ru": {"title": "–£–ø—Ä–∞–≤–ª—è–π —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏ –≥–æ–ª–æ—Å–æ–º - –≥–æ–≤–æ—Ä–∏ —Å–∏—Å—Ç–µ–º–µ, —á—Ç–æ —Ö–æ—á–µ—à—å —É–≤–∏–¥–µ—Ç—å", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ Interactive Recommendation Feed (IRF) - –Ω–æ–≤—É—é –ø–∞—Ä–∞–¥–∏–≥–º—É —Ä–µ
[29.09.2025 00:52] Using data from previous issue: {"categories": ["#hallucinations", "#interpretability", "#benchmark"], "emoji": "‚öñÔ∏è", "ru": {"title": "LLM-—Å—É–¥—å–∏ –¥–∞—é—Ç –Ω–µ–Ω–∞–¥–µ–∂–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏ –∏–∑-–∑–∞ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è –æ—Ç —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏ —Å–µ—Ä—å–µ–∑–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã –≤ –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö, –≥–¥–µ LLM –≤—ã—Å—Ç—É–ø–∞—é—Ç –≤ —Ä–æ–ª–∏ —Å—É–¥–µ–π –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –¥—Ä—É–≥–∏—Ö –º–æ–¥–µ
[29.09.2025 00:52] Using data from previous issue: {"categories": ["#benchmark", "#reasoning", "#video", "#rl", "#interpretability", "#multimodal"], "emoji": "üé¨", "ru": {"title": "–°–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –æ –≤–∏–¥–µ–æ —á–µ—Ä–µ–∑ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ MOSS-ChatV ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π
[29.09.2025 00:52] Using data from previous issue: {"categories": ["#math", "#optimization", "#architecture", "#interpretability"], "emoji": "üé≠", "ru": {"title": "–ö–∞—É–∑–∞–ª—å–Ω–∞—è –º–∞—Å–∫–∞ –∫–∞–∫ —Å–∫—Ä—ã—Ç—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –¥–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –∫–∞—É–∑–∞–ª—å–Ω–∞—è –º–∞—Å–∫–∞ –≤ –¥–µ–∫–æ–¥–µ—Ä–∞—Ö Transformer —Å–æ–∑–¥–∞—ë—Ç –∑–∞–≤–∏—Å—è—â–∏–µ –æ—Ç –ø–æ–∑–∏—Ü–∏–∏ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –≤–Ω–∏–º–∞–Ω–∏—è 
[29.09.2025 00:52] Using data from previous issue: {"categories": ["#inference", "#training", "#optimization", "#long_context"], "emoji": "üóúÔ∏è", "ru": {"title": "–£–º–Ω–æ–µ —Å–µ–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Å–∂–∞—Ç–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π —Ä–∞–±–æ—Ç—ã LLM", "desc": "CompLLM - —ç—Ç–æ –Ω–æ–≤–∞—è —Ç–µ—Ö–Ω–∏–∫–∞ —Å–∂–∞—Ç–∏—è –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä–∞—è —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ 
[29.09.2025 00:52] Using data from previous issue: {"categories": ["#reasoning", "#open_source", "#benchmark", "#multimodal"], "emoji": "üß†", "ru": {"title": "–†–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏ —Ä–µ—à–∞–µ—Ç, –∫–∞–∫–æ–π —Å—Ç–∏–ª—å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —Ä–∞–±–æ—Ç–∞–µ—Ç –ª—É—á—à–µ", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç StyleBench - –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å—Ç–∏–ª–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö. –ê–≤—Ç–æ—Ä
[29.09.2025 00:52] Using data from previous issue: {"categories": ["#optimization", "#agents", "#diffusion", "#benchmark", "#multimodal", "#rl"], "emoji": "üöó", "ru": {"title": "–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–µ –≤–æ–∂–¥–µ–Ω–∏–µ —á–µ—Ä–µ–∑ —Ä–µ—Ñ–ª–µ–∫—Å–∏—é –∏ –¥–∏—Å–∫—Ä–µ—Ç–Ω—É—é –¥–∏—Ñ—Ñ—É–∑–∏—é", "desc": "ReflectDrive –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –¥–ª—è –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –≤–æ–∂–¥–µ–Ω–∏—è, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –º–µ—Ö–∞–Ω–∏–∑–º —Ä–µ—Ñ–ª–µ–∫—Å
[29.09.2025 00:52] Using data from previous issue: {"categories": ["#audio", "#reasoning", "#multimodal", "#training", "#open_source", "#architecture"], "emoji": "üéß", "ru": {"title": "–ù–∞—É—á–∏—Ç—å AI –¥—É–º–∞—Ç—å –≤–æ –≤—Ä–µ–º—è –ø—Ä–æ—Å–ª—É—à–∏–≤–∞–Ω–∏—è –∑–≤—É–∫–æ–≤", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç –Ω–µ–π—Ä–æ–Ω–Ω—ã–º –º–æ–¥–µ–ª—è–º '–¥—É–º–∞—Ç—å –≤–æ –≤—Ä–µ–º—è –ø—Ä–æ—Å–ª—É—à–∏–≤–∞–Ω–∏—è' –∑–≤—É–∫
[29.09.2025 00:52] Using data from previous issue: {"categories": ["#cv", "#training", "#benchmark"], "emoji": "üß©", "ru": {"title": "–†–µ—à–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º—ã –ø–µ—Ä–µ–∫—Ä—ã–≤–∞—é—â–∏—Ö—Å—è –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –≤—ã—è–≤–∏–ª–∏ –ø—Ä–æ–±–ª–µ–º—É —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ –º–∞–∫–µ—Ç—É - –æ–Ω–∏ –ø–ª–æ—Ö–æ —Å–ø—Ä–∞–≤–ª—è—é—Ç—Å—è —Å –ø–µ—Ä–µ–∫—Ä—ã–≤–∞—é—â–∏–º–∏—Å—è bounding box'–∞–º
[29.09.2025 00:52] Using data from previous issue: {"categories": ["#training", "#cv", "#optimization"], "emoji": "üîÑ", "ru": {"title": "–ê—Å–∏–º–º–µ—Ç—Ä–∏—è –º–µ–∂–¥—É –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –∏ –æ—Ü–µ–Ω–∫–æ–π –∫–∞—á–µ—Å—Ç–≤–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –≤—ã—è–≤–ª—è–µ—Ç –∞—Å–∏–º–º–µ—Ç—Ä–∏—é –º–µ–∂–¥—É –ø–µ—Ä—Ü–µ–ø—Ç—É–∞–ª—å–Ω–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –∏ –æ—Ü–µ–Ω–∫–æ–π –∫–∞—á–µ—Å—Ç–≤–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –ú–µ—Ç—Ä–∏–∫–∏ IQA, –∫–æ—Ç–æ—Ä—ã–µ —Ö–æ—Ä–æ—à–æ —Ä–∞–±–æ—Ç–∞—é—Ç –¥–ª—è –æ—Ü–µ
[29.09.2025 00:52] Using data from previous issue: {"categories": ["#optimization", "#audio", "#multimodal", "#transfer_learning", "#training"], "emoji": "üé≠", "ru": {"title": "–°–ª–∏—è–Ω–∏–µ –∑–Ω–∞–Ω–∏–π –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —ç–º–æ—Ü–∏–π –±–µ–∑ –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ MI-Fuse –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –º–æ–¥–µ–ª–µ–π —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —ç–º–æ—Ü–∏–π –≤ —Ä–µ—á–∏ –∫ –Ω–æ–≤–æ–º—É –¥–æ–º–µ–Ω
[29.09.2025 00:52] Using data from previous issue: {"categories": ["#dataset", "#architecture", "#ethics", "#data", "#security", "#benchmark"], "emoji": "üõ°Ô∏è", "ru": {"title": "–°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ AI —á–µ—Ä–µ–∑ –∫–∞—Ä—Ç–æ—á–∫–∏ —Å–∏—Å—Ç–µ–º —Å –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞–º–∏ —É–≥—Ä–æ–∑", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ Hazard-Aware System Card (HASC), –∫–æ—Ç–æ—Ä–∞—è —Ä–∞—Å—à–∏—Ä—è–µ—Ç
[29.09.2025 00:52] Using data from previous issue: {"categories": ["#alignment", "#training", "#open_source", "#interpretability", "#multimodal", "#dataset", "#ethics", "#benchmark"], "emoji": "üõ°Ô∏è", "ru": {"title": "Guided-CoT –ø—Ä–æ–º–ø—Ç—ã —É–ª—É—á—à–∞—é—Ç –¥–µ—Ç–µ–∫—Ü–∏—é –∞–Ω—Ç–∏—Å–µ–º–∏—Ç—Å–∫–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –≤ open-source LLM", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –æ—Ü–µ–Ω–∏–ª–∏ –≤–æ—Å–µ–º—å open-source LLM 
[29.09.2025 00:52] Renaming data file.
[29.09.2025 00:52] Renaming previous data. hf_papers.json to ./d/2025-09-29.json
[29.09.2025 00:52] Saving new data file.
[29.09.2025 00:52] Generating page.
[29.09.2025 00:52] Renaming previous page.
[29.09.2025 00:52] Renaming previous data. index.html to ./d/2025-09-29.html
[29.09.2025 00:52] Writing result.
[29.09.2025 00:52] Renaming log file.
[29.09.2025 00:52] Renaming previous data. log.txt to ./logs/2025-09-29_last_log.txt
