[29.09.2025 08:17] Read previous papers.
[29.09.2025 08:17] Generating top page (month).
[29.09.2025 08:17] Writing top page (month).
[29.09.2025 09:14] Read previous papers.
[29.09.2025 09:14] Get feed.
[29.09.2025 09:14] Get page data from previous paper. URL: https://huggingface.co/papers/2509.22622
[29.09.2025 09:14] Get page data from previous paper. URL: https://huggingface.co/papers/2509.22611
[29.09.2025 09:14] Get page data from previous paper. URL: https://huggingface.co/papers/2509.22186
[29.09.2025 09:14] Get page data from previous paper. URL: https://huggingface.co/papers/2509.22576
[29.09.2025 09:14] Get page data from previous paper. URL: https://huggingface.co/papers/2509.22637
[29.09.2025 09:14] Get page data from previous paper. URL: https://huggingface.co/papers/2509.21679
[29.09.2025 09:14] Get page data from previous paper. URL: https://huggingface.co/papers/2509.22638
[29.09.2025 09:14] Get page data from previous paper. URL: https://huggingface.co/papers/2509.22647
[29.09.2025 09:14] Get page data from previous paper. URL: https://huggingface.co/papers/2509.22281
[29.09.2025 09:14] Get page data from previous paper. URL: https://huggingface.co/papers/2509.22651
[29.09.2025 09:14] Get page data from previous paper. URL: https://huggingface.co/papers/2509.21880
[29.09.2025 09:14] Get page data from previous paper. URL: https://huggingface.co/papers/2509.22414
[29.09.2025 09:14] Get page data from previous paper. URL: https://huggingface.co/papers/2509.21766
[29.09.2025 09:14] Get page data from previous paper. URL: https://huggingface.co/papers/2509.22644
[29.09.2025 09:14] Get page data from previous paper. URL: https://huggingface.co/papers/2509.22624
[29.09.2025 09:14] Get page data from previous paper. URL: https://huggingface.co/papers/2509.22653
[29.09.2025 09:14] Get page data from previous paper. URL: https://huggingface.co/papers/2509.21710
[29.09.2025 09:14] Get page data from previous paper. URL: https://huggingface.co/papers/2509.21989
[29.09.2025 09:14] Get page data from previous paper. URL: https://huggingface.co/papers/2509.21760
[29.09.2025 09:14] Get page data from previous paper. URL: https://huggingface.co/papers/2509.21799
[29.09.2025 09:14] Get page data from previous paper. URL: https://huggingface.co/papers/2509.22601
[29.09.2025 09:14] Get page data from previous paper. URL: https://huggingface.co/papers/2509.21574
[29.09.2025 09:14] Get page data from previous paper. URL: https://huggingface.co/papers/2509.21500
[29.09.2025 09:14] Get page data from previous paper. URL: https://huggingface.co/papers/2509.22244
[29.09.2025 09:14] Get page data from previous paper. URL: https://huggingface.co/papers/2509.22650
[29.09.2025 09:14] Get page data from previous paper. URL: https://huggingface.co/papers/2509.22642
[29.09.2025 09:14] Get page data from previous paper. URL: https://huggingface.co/papers/2509.22496
[29.09.2025 09:14] Get page data from previous paper. URL: https://huggingface.co/papers/2509.22630
[29.09.2025 09:14] Get page data from previous paper. URL: https://huggingface.co/papers/2509.21559
[29.09.2025 09:14] Get page data from previous paper. URL: https://huggingface.co/papers/2509.19768
[29.09.2025 09:14] Extract page data from URL. URL: https://huggingface.co/papers/2509.20787
[29.09.2025 09:14] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[29.09.2025 09:14] No deleted papers detected.
[29.09.2025 09:14] Downloading and parsing papers (pdf, html). Total: 31.
[29.09.2025 09:14] Downloading and parsing paper https://huggingface.co/papers/2509.22622.
[29.09.2025 09:14] Extra JSON file exists (./assets/json/2509.22622.json), skip PDF parsing.
[29.09.2025 09:14] Paper image links file exists (./assets/img_data/2509.22622.json), skip HTML parsing.
[29.09.2025 09:14] Success.
[29.09.2025 09:14] Downloading and parsing paper https://huggingface.co/papers/2509.22611.
[29.09.2025 09:14] Extra JSON file exists (./assets/json/2509.22611.json), skip PDF parsing.
[29.09.2025 09:14] Paper image links file exists (./assets/img_data/2509.22611.json), skip HTML parsing.
[29.09.2025 09:14] Success.
[29.09.2025 09:14] Downloading and parsing paper https://huggingface.co/papers/2509.22186.
[29.09.2025 09:14] Extra JSON file exists (./assets/json/2509.22186.json), skip PDF parsing.
[29.09.2025 09:14] Paper image links file exists (./assets/img_data/2509.22186.json), skip HTML parsing.
[29.09.2025 09:14] Success.
[29.09.2025 09:14] Downloading and parsing paper https://huggingface.co/papers/2509.22576.
[29.09.2025 09:14] Extra JSON file exists (./assets/json/2509.22576.json), skip PDF parsing.
[29.09.2025 09:14] Paper image links file exists (./assets/img_data/2509.22576.json), skip HTML parsing.
[29.09.2025 09:14] Success.
[29.09.2025 09:14] Downloading and parsing paper https://huggingface.co/papers/2509.22637.
[29.09.2025 09:14] Extra JSON file exists (./assets/json/2509.22637.json), skip PDF parsing.
[29.09.2025 09:14] Paper image links file exists (./assets/img_data/2509.22637.json), skip HTML parsing.
[29.09.2025 09:14] Success.
[29.09.2025 09:14] Downloading and parsing paper https://huggingface.co/papers/2509.21679.
[29.09.2025 09:14] Extra JSON file exists (./assets/json/2509.21679.json), skip PDF parsing.
[29.09.2025 09:14] Paper image links file exists (./assets/img_data/2509.21679.json), skip HTML parsing.
[29.09.2025 09:14] Success.
[29.09.2025 09:14] Downloading and parsing paper https://huggingface.co/papers/2509.22638.
[29.09.2025 09:14] Extra JSON file exists (./assets/json/2509.22638.json), skip PDF parsing.
[29.09.2025 09:14] Paper image links file exists (./assets/img_data/2509.22638.json), skip HTML parsing.
[29.09.2025 09:14] Success.
[29.09.2025 09:14] Downloading and parsing paper https://huggingface.co/papers/2509.22647.
[29.09.2025 09:14] Extra JSON file exists (./assets/json/2509.22647.json), skip PDF parsing.
[29.09.2025 09:14] Paper image links file exists (./assets/img_data/2509.22647.json), skip HTML parsing.
[29.09.2025 09:14] Success.
[29.09.2025 09:14] Downloading and parsing paper https://huggingface.co/papers/2509.22281.
[29.09.2025 09:14] Extra JSON file exists (./assets/json/2509.22281.json), skip PDF parsing.
[29.09.2025 09:14] Paper image links file exists (./assets/img_data/2509.22281.json), skip HTML parsing.
[29.09.2025 09:14] Success.
[29.09.2025 09:14] Downloading and parsing paper https://huggingface.co/papers/2509.22651.
[29.09.2025 09:14] Extra JSON file exists (./assets/json/2509.22651.json), skip PDF parsing.
[29.09.2025 09:14] Paper image links file exists (./assets/img_data/2509.22651.json), skip HTML parsing.
[29.09.2025 09:14] Success.
[29.09.2025 09:14] Downloading and parsing paper https://huggingface.co/papers/2509.21880.
[29.09.2025 09:14] Extra JSON file exists (./assets/json/2509.21880.json), skip PDF parsing.
[29.09.2025 09:14] Paper image links file exists (./assets/img_data/2509.21880.json), skip HTML parsing.
[29.09.2025 09:14] Success.
[29.09.2025 09:14] Downloading and parsing paper https://huggingface.co/papers/2509.22414.
[29.09.2025 09:14] Extra JSON file exists (./assets/json/2509.22414.json), skip PDF parsing.
[29.09.2025 09:14] Paper image links file exists (./assets/img_data/2509.22414.json), skip HTML parsing.
[29.09.2025 09:14] Success.
[29.09.2025 09:14] Downloading and parsing paper https://huggingface.co/papers/2509.21766.
[29.09.2025 09:14] Extra JSON file exists (./assets/json/2509.21766.json), skip PDF parsing.
[29.09.2025 09:14] Paper image links file exists (./assets/img_data/2509.21766.json), skip HTML parsing.
[29.09.2025 09:14] Success.
[29.09.2025 09:14] Downloading and parsing paper https://huggingface.co/papers/2509.22644.
[29.09.2025 09:14] Extra JSON file exists (./assets/json/2509.22644.json), skip PDF parsing.
[29.09.2025 09:14] Paper image links file exists (./assets/img_data/2509.22644.json), skip HTML parsing.
[29.09.2025 09:14] Success.
[29.09.2025 09:14] Downloading and parsing paper https://huggingface.co/papers/2509.22624.
[29.09.2025 09:14] Extra JSON file exists (./assets/json/2509.22624.json), skip PDF parsing.
[29.09.2025 09:14] Paper image links file exists (./assets/img_data/2509.22624.json), skip HTML parsing.
[29.09.2025 09:14] Success.
[29.09.2025 09:14] Downloading and parsing paper https://huggingface.co/papers/2509.22653.
[29.09.2025 09:14] Extra JSON file exists (./assets/json/2509.22653.json), skip PDF parsing.
[29.09.2025 09:14] Paper image links file exists (./assets/img_data/2509.22653.json), skip HTML parsing.
[29.09.2025 09:14] Success.
[29.09.2025 09:14] Downloading and parsing paper https://huggingface.co/papers/2509.21710.
[29.09.2025 09:14] Extra JSON file exists (./assets/json/2509.21710.json), skip PDF parsing.
[29.09.2025 09:14] Paper image links file exists (./assets/img_data/2509.21710.json), skip HTML parsing.
[29.09.2025 09:14] Success.
[29.09.2025 09:14] Downloading and parsing paper https://huggingface.co/papers/2509.21989.
[29.09.2025 09:14] Extra JSON file exists (./assets/json/2509.21989.json), skip PDF parsing.
[29.09.2025 09:14] Paper image links file exists (./assets/img_data/2509.21989.json), skip HTML parsing.
[29.09.2025 09:14] Success.
[29.09.2025 09:14] Downloading and parsing paper https://huggingface.co/papers/2509.21760.
[29.09.2025 09:14] Extra JSON file exists (./assets/json/2509.21760.json), skip PDF parsing.
[29.09.2025 09:14] Paper image links file exists (./assets/img_data/2509.21760.json), skip HTML parsing.
[29.09.2025 09:14] Success.
[29.09.2025 09:14] Downloading and parsing paper https://huggingface.co/papers/2509.21799.
[29.09.2025 09:14] Extra JSON file exists (./assets/json/2509.21799.json), skip PDF parsing.
[29.09.2025 09:14] Paper image links file exists (./assets/img_data/2509.21799.json), skip HTML parsing.
[29.09.2025 09:14] Success.
[29.09.2025 09:14] Downloading and parsing paper https://huggingface.co/papers/2509.22601.
[29.09.2025 09:14] Extra JSON file exists (./assets/json/2509.22601.json), skip PDF parsing.
[29.09.2025 09:14] Paper image links file exists (./assets/img_data/2509.22601.json), skip HTML parsing.
[29.09.2025 09:14] Success.
[29.09.2025 09:14] Downloading and parsing paper https://huggingface.co/papers/2509.21574.
[29.09.2025 09:14] Extra JSON file exists (./assets/json/2509.21574.json), skip PDF parsing.
[29.09.2025 09:14] Paper image links file exists (./assets/img_data/2509.21574.json), skip HTML parsing.
[29.09.2025 09:14] Success.
[29.09.2025 09:14] Downloading and parsing paper https://huggingface.co/papers/2509.21500.
[29.09.2025 09:14] Extra JSON file exists (./assets/json/2509.21500.json), skip PDF parsing.
[29.09.2025 09:14] Paper image links file exists (./assets/img_data/2509.21500.json), skip HTML parsing.
[29.09.2025 09:14] Success.
[29.09.2025 09:14] Downloading and parsing paper https://huggingface.co/papers/2509.22244.
[29.09.2025 09:14] Extra JSON file exists (./assets/json/2509.22244.json), skip PDF parsing.
[29.09.2025 09:14] Paper image links file exists (./assets/img_data/2509.22244.json), skip HTML parsing.
[29.09.2025 09:14] Success.
[29.09.2025 09:14] Downloading and parsing paper https://huggingface.co/papers/2509.22650.
[29.09.2025 09:14] Extra JSON file exists (./assets/json/2509.22650.json), skip PDF parsing.
[29.09.2025 09:14] Paper image links file exists (./assets/img_data/2509.22650.json), skip HTML parsing.
[29.09.2025 09:14] Success.
[29.09.2025 09:14] Downloading and parsing paper https://huggingface.co/papers/2509.22642.
[29.09.2025 09:14] Extra JSON file exists (./assets/json/2509.22642.json), skip PDF parsing.
[29.09.2025 09:14] Paper image links file exists (./assets/img_data/2509.22642.json), skip HTML parsing.
[29.09.2025 09:14] Success.
[29.09.2025 09:14] Downloading and parsing paper https://huggingface.co/papers/2509.22496.
[29.09.2025 09:14] Extra JSON file exists (./assets/json/2509.22496.json), skip PDF parsing.
[29.09.2025 09:14] Paper image links file exists (./assets/img_data/2509.22496.json), skip HTML parsing.
[29.09.2025 09:14] Success.
[29.09.2025 09:14] Downloading and parsing paper https://huggingface.co/papers/2509.22630.
[29.09.2025 09:14] Extra JSON file exists (./assets/json/2509.22630.json), skip PDF parsing.
[29.09.2025 09:14] Paper image links file exists (./assets/img_data/2509.22630.json), skip HTML parsing.
[29.09.2025 09:14] Success.
[29.09.2025 09:14] Downloading and parsing paper https://huggingface.co/papers/2509.21559.
[29.09.2025 09:14] Extra JSON file exists (./assets/json/2509.21559.json), skip PDF parsing.
[29.09.2025 09:14] Paper image links file exists (./assets/img_data/2509.21559.json), skip HTML parsing.
[29.09.2025 09:14] Success.
[29.09.2025 09:14] Downloading and parsing paper https://huggingface.co/papers/2509.19768.
[29.09.2025 09:14] Extra JSON file exists (./assets/json/2509.19768.json), skip PDF parsing.
[29.09.2025 09:14] Paper image links file exists (./assets/img_data/2509.19768.json), skip HTML parsing.
[29.09.2025 09:14] Success.
[29.09.2025 09:14] Downloading and parsing paper https://huggingface.co/papers/2509.20787.
[29.09.2025 09:14] Downloading paper 2509.20787 from http://arxiv.org/pdf/2509.20787v2...
[29.09.2025 09:14] Extracting affiliations from text.
[29.09.2025 09:14] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 6 2 ] . [ 2 7 8 7 0 2 . 9 0 5 2 : r Real-Time Object Detection Meets DINOv Shihua Huang1, Yongjie Hou1, 2, Longfei Liu1, Xuanlong Yu1, Xi Shen1 1 Intellindust AI Lab; 2Xiamen University Equal Contribution; Corresponding author. Project Page: https://intellindust-ai-lab.github.io/projects/DEIMv2 Code & Weights: https://github.com/Intellindust-AI-Lab/DEIMv2 (a) COCO performance v.s. Number of Parameters. (b) COCO performance v.s. FLOPs. Figure 1. Compared with state-of-the-art real-time object detectors on COCO [12], all variants of our proposed DEIMv2 (S, M, L, X) achieve superior performance in terms of average precision (AP) while maintaining similar parameters and less computational cost. "
[29.09.2025 09:14] Response: ```python
["Intellindust AI Lab", "Xiamen University"]
```
[29.09.2025 09:14] Deleting PDF ./assets/pdf/2509.20787.pdf.
[29.09.2025 09:14] Success.
[29.09.2025 09:14] Enriching papers with extra data.
[29.09.2025 09:14] ********************************************************************************
[29.09.2025 09:14] Abstract 0. LongLive is a frame-level autoregressive framework for real-time and interactive long video generation, addressing efficiency and quality challenges through causal attention, KV-recache, streaming long tuning, and short window attention.  					AI-generated summary 				 We present LongLive, a frame-l...
[29.09.2025 09:14] ********************************************************************************
[29.09.2025 09:14] Abstract 1. Quantile Advantage Estimation stabilizes reinforcement learning with verifiable rewards by addressing entropy issues and improving performance on large language models.  					AI-generated summary 				 Reinforcement Learning with Verifiable Rewards (RLVR) strengthens LLM reasoning, but training often...
[29.09.2025 09:14] ********************************************************************************
[29.09.2025 09:14] Abstract 2. MinerU2.5, a 1.2B-parameter document parsing vision-language model, achieves state-of-the-art recognition accuracy with computational efficiency through a coarse-to-fine parsing strategy.  					AI-generated summary 				 We introduce MinerU2.5, a 1.2B-parameter document parsing vision-language model ...
[29.09.2025 09:14] ********************************************************************************
[29.09.2025 09:14] Abstract 3. Entropy-regularized Policy Optimization (EPO) addresses exploration-exploitation challenges in multi-turn environments with sparse rewards, improving performance in tasks like ScienceWorld and ALFWorld.  					AI-generated summary 				 Training LLM agents in multi-turn environments with sparse reward...
[29.09.2025 09:14] ********************************************************************************
[29.09.2025 09:14] Abstract 4. A variational reasoning framework treats thinking traces as latent variables, optimizing them through variational inference to improve language model reasoning.  					AI-generated summary 				 We introduce a variational reasoning framework for language models that treats thinking traces as latent va...
[29.09.2025 09:14] ********************************************************************************
[29.09.2025 09:14] Abstract 5. An automated engine evaluates the factuality of review points in AI conference papers, demonstrating moderate agreement with human experts and higher accuracy at the premise level.  					AI-generated summary 				 Peer review serves as a backbone of academic research, but in most AI conferences, the ...
[29.09.2025 09:14] ********************************************************************************
[29.09.2025 09:14] Abstract 6. Feedback-conditional policy (FCP) enables LLMs to learn from verbal feedback by treating it as a conditioning signal, improving expressiveness over scalar rewards.  					AI-generated summary 				 LLMs are often trained with RL from human or AI feedback, yet such methods typically compress nuanced fe...
[29.09.2025 09:14] ********************************************************************************
[29.09.2025 09:14] Abstract 7. CapRL, a novel reinforcement learning framework, enhances image captioning by using a vision-free language model to evaluate caption quality through multiple-choice questions, leading to improved performance across benchmarks.  					AI-generated summary 				 Image captioning is a fundamental task th...
[29.09.2025 09:14] ********************************************************************************
[29.09.2025 09:14] Abstract 8. MesaTask, an LLM-based framework with a Spatial Reasoning Chain, generates realistic tabletop scenes aligned with task descriptions using DPO algorithms.  					AI-generated summary 				 The ability of robots to interpret human instructions and execute manipulation tasks necessitates the availability...
[29.09.2025 09:14] ********************************************************************************
[29.09.2025 09:14] Abstract 9. VoiceAssistant-Eval is a benchmark for evaluating AI assistants across listening, speaking, and viewing tasks, revealing insights into model performance and identifying areas for improvement.  					AI-generated summary 				 The growing capabilities of large language models and multimodal systems hav...
[29.09.2025 09:14] ********************************************************************************
[29.09.2025 09:14] Abstract 10. RL-ZVP, a novel reinforcement learning algorithm, leverages zero-variance prompts to improve the accuracy and pass rate of Large Language Models in math reasoning tasks.  					AI-generated summary 				 Reinforcement Learning with Verifiable Rewards (RLVR) is a powerful framework for improving the re...
[29.09.2025 09:14] ********************************************************************************
[29.09.2025 09:14] Abstract 11. LucidFlux, a caption-free UIR framework using a diffusion transformer, achieves robust image restoration through adaptive conditioning and SigLIP features without text prompts.  					AI-generated summary 				 Universal image restoration (UIR) aims to recover images degraded by unknown mixtures while...
[29.09.2025 09:14] ********************************************************************************
[29.09.2025 09:14] Abstract 12. UltraHorizon is a new benchmark that evaluates long-horizon and partially observable tasks for autonomous agents, highlighting gaps in their sustained reasoning, planning, memory, and tool use capabilities.  					AI-generated summary 				 Autonomous agents have recently achieved remarkable progress ...
[29.09.2025 09:14] ********************************************************************************
[29.09.2025 09:14] Abstract 13. WebGen-Agent enhances website code generation by integrating visual feedback and GUI-agent testing with a backtracking mechanism and Step-GRPO training to improve accuracy and appearance scores.  					AI-generated summary 				 Agent systems powered by large language models (LLMs) have demonstrated i...
[29.09.2025 09:14] ********************************************************************************
[29.09.2025 09:14] Abstract 14. SPARK, a synergistic policy and reward co-evolving framework, enhances LLMs and LVLMs by recycling rollouts and correctness data to train a generative reward model, reducing reliance on human preferences and external reward models.  					AI-generated summary 				 Recent Large Language Models (LLMs) ...
[29.09.2025 09:14] ********************************************************************************
[29.09.2025 09:14] Abstract 15. See, Point, Fly (SPF) is a training-free aerial vision-and-language navigation framework that treats action prediction as a 2D spatial grounding task, outperforming existing methods in both simulation and real-world evaluations.  					AI-generated summary 				 We present See, Point, Fly (SPF), a tra...
[29.09.2025 09:14] ********************************************************************************
[29.09.2025 09:14] Abstract 16. ToG-3, a novel framework, enhances LLMs with external knowledge using a dynamic, multi-agent system that evolves queries and subgraphs for precise evidence retrieval and reasoning.  					AI-generated summary 				 Retrieval-Augmented Generation (RAG) and Graph-based RAG has become the important parad...
[29.09.2025 09:14] ********************************************************************************
[29.09.2025 09:14] Abstract 17. A novel method disentangles visual and semantic features from diffusion model backbones to quantify and localize visual inconsistencies in subject-driven image generation.  					AI-generated summary 				 We propose a novel approach for disentangling visual and semantic features from the backbones of...
[29.09.2025 09:14] ********************************************************************************
[29.09.2025 09:14] Abstract 18. A pre-trained video diffusion transformer, UniVid, is fine-tuned to handle diverse vision tasks without task-specific modifications, demonstrating cross-modal and cross-source generalization.  					AI-generated summary 				 Large language models, trained on extensive corpora, successfully unify dive...
[29.09.2025 09:14] ********************************************************************************
[29.09.2025 09:14] Abstract 19. D-Artemis, a novel deliberative framework, enhances GUI automation by leveraging app-specific tips, proactive alignment, and reflection, achieving state-of-the-art results with general-purpose multimodal large language models.  					AI-generated summary 				 Graphical User Interface (GUI) agents aim...
[29.09.2025 09:14] ********************************************************************************
[29.09.2025 09:14] Abstract 20. SPEAR, a curriculum-based self-imitation learning method, manages exploration-exploitation balance in reinforcement learning for LLMs by using intrinsic rewards and trajectory-level entropy control.  					AI-generated summary 				 Reinforcement learning (RL) is the dominant paradigm for sharpening s...
[29.09.2025 09:14] ********************************************************************************
[29.09.2025 09:14] Abstract 21. X-Streamer is a unified multimodal framework using dual-transformer architecture for real-time, open-ended interactions across text, speech, and video, leveraging large language-speech models and autoregressive diffusion models.  					AI-generated summary 				 We introduce X-Streamer, an end-to-end ...
[29.09.2025 09:14] ********************************************************************************
[29.09.2025 09:14] Abstract 22. Rubric-based rewards mitigate reward over-optimization in reinforcement fine-tuning by leveraging off-policy examples while maintaining reward reliability.  					AI-generated summary 				 Reinforcement fine-tuning (RFT) often suffers from reward over-optimization, where a policy model hacks the rewa...
[29.09.2025 09:14] ********************************************************************************
[29.09.2025 09:14] Abstract 23. FlashEdit enables real-time, high-fidelity image editing with diffusion models through efficient inversion, background preservation, and localized attention mechanisms.  					AI-generated summary 				 Text-guided image editing with diffusion models has achieved remarkable quality but suffers from pr...
[29.09.2025 09:14] ********************************************************************************
[29.09.2025 09:14] Abstract 24. A new method leverages diffusion transformers' attention scores for referring segmentation without fine-tuning or additional training, improving performance through stop word filtering and attention redistribution.  					AI-generated summary 				 Most existing approaches to referring segmentation ac...
[29.09.2025 09:14] ********************************************************************************
[29.09.2025 09:14] Abstract 25. WoW, a 14-billion-parameter generative world model trained on robot interactions, demonstrates improved physical intuition through SOPHIA's guidance and achieves state-of-the-art performance on physical consistency and causal reasoning in video.  					AI-generated summary 				 Humans develop an unde...
[29.09.2025 09:14] ********************************************************************************
[29.09.2025 09:14] Abstract 26. EAGLE is a lightweight framework that explains token generation in multimodal large language models by attributing tokens to visual regions and quantifying the influence of language and perceptual evidence.  					AI-generated summary 				 Multimodal large language models (MLLMs) have demonstrated re...
[29.09.2025 09:14] ********************************************************************************
[29.09.2025 09:14] Abstract 27. StateX is a post-training pipeline that expands the state size of pre-trained RNNs, enhancing recall and in-context learning without significant additional costs.  					AI-generated summary 				 While Transformer-based models have demonstrated remarkable language modeling performance, their high com...
[29.09.2025 09:14] ********************************************************************************
[29.09.2025 09:14] Abstract 28. X-CoT, an explainable retrieval framework using LLM CoT reasoning, enhances text-to-video retrieval by providing detailed rationales and improving performance.  					AI-generated summary 				 Prevalent text-to-video retrieval systems mainly adopt embedding models for feature extraction and compute c...
[29.09.2025 09:14] ********************************************************************************
[29.09.2025 09:14] Abstract 29. CHURRO, a 3B-parameter open-weight vision-language model, outperforms existing models in historical text recognition using the largest dataset to date, CHURRO-DS, and is more cost-effective.  					AI-generated summary 				 Accurate text recognition for historical documents can greatly advance the st...
[29.09.2025 09:14] ********************************************************************************
[29.09.2025 09:14] Abstract 30. DEIMv2, an extended version of DEIM with DINOv3 features, achieves superior performance-cost trade-offs across diverse deployment scenarios, including GPU, edge, and mobile, by using Spatial Tuning Adapter and HGNetv2 with pruning.  					AI-generated summary 				 Benefiting from the simplicity and e...
[29.09.2025 09:14] Read previous papers.
[29.09.2025 09:14] Generating reviews via LLM API.
[29.09.2025 09:14] Using data from previous issue: {"categories": ["#video", "#training", "#inference", "#diffusion", "#architecture"], "emoji": "🎬", "ru": {"title": "Интерактивная генерация длинных видео в реальном времени", "desc": "LongLive представляет собой авторегрессионную систему для генерации длинных видео в реальном времени на уровне кадро
[29.09.2025 09:14] Using data from previous issue: {"categories": ["#training", "#reasoning", "#rl", "#rlhf", "#optimization"], "emoji": "⚖️", "ru": {"title": "Квантильное преимущество: стабилизация обучения LLM через умный baseline", "desc": "Исследователи предлагают новый подход для улучшения обучения с подкреплением больших языковых моделей, кото
[29.09.2025 09:14] Using data from previous issue: {"categories": ["#training", "#architecture", "#benchmark", "#cv", "#dataset", "#data"], "emoji": "📄", "ru": {"title": "От общего к частному: эффективный парсинг документов в два этапа", "desc": "Представлена MinerU2.5 - vision-language модель с 1.2 миллиардами параметров для парсинга документов. Мо
[29.09.2025 09:14] Using data from previous issue: {"categories": ["#training", "#reasoning", "#rl", "#games", "#optimization"], "emoji": "🌊", "ru": {"title": "Укрощение энтропийного каскада в обучении LLM агентов", "desc": "Исследователи решают проблему обучения LLM агентов в многоходовых средах с разреженными наградами, где для выполнения задачи т
[29.09.2025 09:14] Using data from previous issue: {"categories": ["#optimization", "#rl", "#reasoning", "#training"], "emoji": "🧠", "ru": {"title": "Вариационное рассуждение: унификация вероятностного подхода и RL для языковых моделей", "desc": "Исследователи предлагают новый вариационный фреймворк для улучшения рассуждений языковых моделей, рассма
[29.09.2025 09:14] Using data from previous issue: {"categories": ["#interpretability", "#benchmark", "#dataset", "#ethics", "#data"], "emoji": "🔍", "ru": {"title": "Автоматическая проверка фактов в научных рецензиях с помощью AI", "desc": "Исследователи создали автоматический движок для оценки фактической точности рецензий на научные статьи в облас
[29.09.2025 09:14] Using data from previous issue: {"categories": ["#alignment", "#rl", "#optimization", "#rlhf"], "emoji": "💬", "ru": {"title": "От скалярных наград к богатой вербальной обратной связи", "desc": "Исследователи предлагают новый подход обучения больших языковых моделей на основе вербальной обратной связи вместо скалярных наград. Метод
[29.09.2025 09:14] Using data from previous issue: {"categories": ["#training", "#multimodal", "#dataset", "#rl", "#games", "#rlhf", "#optimization"], "emoji": "📸", "ru": {"title": "Обучение описанию изображений через вопросы и ответы", "desc": "В статье представлен CapRL — новый подход к обучению моделей для генерации описаний изображений с использ
[29.09.2025 09:14] Using data from previous issue: {"categories": ["#3d", "#synthetic", "#agents", "#reasoning", "#dataset"], "emoji": "🤖", "ru": {"title": "Умная генерация сцен для обучения роботов манипуляции объектами", "desc": "Исследователи разработали MesaTask — фреймворк на основе LLM для генерации реалистичных сцен на столешнице, которые соо
[29.09.2025 09:14] Using data from previous issue: {"categories": ["#alignment", "#interpretability", "#multimodal", "#audio", "#benchmark", "#open_source", "#small_models"], "emoji": "🔊", "ru": {"title": "Оценка AI-ассистентов: звук, речь и изображение", "desc": "VoiceAssistant-Eval — это новый бенчмарк для оценки AI-ассистентов, который охватывает
[29.09.2025 09:14] Using data from previous issue: {"categories": ["#training", "#rl", "#optimization", "#math", "#rlhf", "#reasoning"], "emoji": "🧮", "ru": {"title": "Обучение на одинаковых наградах: как извлечь пользу из промптов с нулевой дисперсией", "desc": "В статье представлен новый алгоритм обучения с подкреплением RL-ZVP, который использует
[29.09.2025 09:14] Using data from previous issue: {"categories": ["#cv", "#benchmark", "#open_source", "#dataset", "#diffusion", "#hallucinations"], "emoji": "🔧", "ru": {"title": "Восстановление изображений без текстовых описаний", "desc": "LucidFlux представляет новый подход к универсальному восстановлению изображений, используя диффузионный транс
[29.09.2025 09:14] Using data from previous issue: {"categories": ["#long_context", "#benchmark", "#reasoning", "#agents"], "emoji": "🔭", "ru": {"title": "Долгосрочное мышление AI-агентов: большой разрыв с человеческими способностями", "desc": "UltraHorizon - это новый бенчмарк для оценки автономных AI-агентов в долгосрочных задачах с частичной набл
[29.09.2025 09:14] Using data from previous issue: {"categories": ["#training", "#agents", "#reasoning", "#rlhf", "#optimization"], "emoji": "🌐", "ru": {"title": "Генерация веб-сайтов с визуальной обратной связью", "desc": "В статье представлен WebGen-Agent - новый AI-агент для генерации веб-сайтов, который использует визуальную обратную связь от ск
[29.09.2025 09:14] Using data from previous issue: {"categories": ["#benchmark", "#reasoning", "#architecture", "#rl", "#rlhf", "#optimization", "#training", "#alignment"], "emoji": "🔄", "ru": {"title": "Самообучающиеся модели через синергию политики и вознаграждений", "desc": "SPARK - это новый фреймворк для обучения больших языковых моделей (LLM) 
[29.09.2025 09:14] Using data from previous issue: {"categories": ["#benchmark", "#optimization", "#rl", "#agents", "#games", "#cv"], "emoji": "🚁", "ru": {"title": "Дроны научились летать по текстовым командам через 2D разметку изображений", "desc": "Исследователи представили SPF (See, Point, Fly) - фреймворк для навигации дронов по текстовым инстру
[29.09.2025 09:14] Using data from previous issue: {"categories": ["#multimodal", "#agents", "#benchmark", "#reasoning", "#rag", "#graphs"], "emoji": "🧠", "ru": {"title": "Думай-на-графе: эволюционная мульти-агентная система для точного извлечения знаний", "desc": "ToG-3 представляет новый подход для улучшения работы LLM с внешними знаниями через ди
[29.09.2025 09:14] Using data from previous issue: {"categories": ["#dataset", "#cv", "#benchmark", "#diffusion"], "emoji": "🔍", "ru": {"title": "Разделяй и находи: обнаружение визуальных глитчей в генерации изображений", "desc": "Исследователи предлагают новый метод для разделения визуальных и семантических признаков в диффузионных моделях, что поз
[29.09.2025 09:14] Using data from previous issue: {"categories": ["#cv", "#video", "#multimodal", "#diffusion", "#transfer_learning"], "emoji": "🎬", "ru": {"title": "Один видео трансформер для всех задач компьютерного зрения", "desc": "Исследователи предлагают UniVid - фреймворк, который адаптирует предобученную модель генерации видео для решения р
[29.09.2025 09:14] Using data from previous issue: {"categories": ["#agi", "#multimodal", "#agents", "#benchmark", "#optimization"], "emoji": "🤖", "ru": {"title": "Когнитивный агент для автоматизации GUI с циклом мышления и рефлексии", "desc": "В работе представлен D-Artemis — новый фреймворк для автоматизации действий в графических интерфейсах, кот
[29.09.2025 09:14] Using data from previous issue: {"categories": ["#training", "#rl", "#games", "#rlhf", "#optimization"], "emoji": "🎯", "ru": {"title": "Балансировка исследования и эксплуатации в RL через самоимитацию", "desc": "В статье представлен метод SPEAR - подход к обучению с подкреплением для LLM, основанный на самоимитации и curriculum le
[29.09.2025 09:14] Using data from previous issue: {"categories": ["#alignment", "#architecture", "#multimodal", "#agi", "#interpretability", "#games", "#agents"], "emoji": "🤖", "ru": {"title": "Цифровой человек в реальном времени из одного портрета", "desc": "В статье представлен X-Streamer — фреймворк для создания цифровых человеческих агентов, сп
[29.09.2025 09:14] Using data from previous issue: {"categories": ["#rl", "#alignment", "#optimization", "#rlhf", "#training"], "emoji": "📝", "ru": {"title": "Рубрики против обмана: как избежать переоптимизации наград в fine-tuning", "desc": "Исследование посвящено проблеме чрезмерной оптимизации наград в reinforcement fine-tuning, когда модель начи
[29.09.2025 09:14] Using data from previous issue: {"categories": ["#optimization", "#cv", "#inference", "#open_source", "#diffusion"], "emoji": "⚡", "ru": {"title": "Мгновенное редактирование изображений с диффузионными моделями", "desc": "FlashEdit — это новая система для редактирования изображений с использованием диффузионных моделей в реальном 
[29.09.2025 09:14] Using data from previous issue: {"categories": ["#benchmark", "#multimodal", "#cv", "#diffusion"], "emoji": "🎯", "ru": {"title": "Сегментация без обучения через внимание диффузионных моделей", "desc": "Исследователи предложили новый метод для задач referring segmentation, используя attention scores из диффузионных трансформеров бе
[29.09.2025 09:14] Using data from previous issue: {"categories": ["#reasoning", "#open_source", "#dataset", "#video", "#robotics", "#agents", "#agi", "#hallucinations", "#benchmark"], "emoji": "🤖", "ru": {"title": "Физическая интуиция AI через реальное взаимодействие с миром", "desc": "Исследователи создали WoW — генеративную модель мира с 14 милли
[29.09.2025 09:14] Using data from previous issue: {"categories": ["#hallucinations", "#inference", "#multimodal", "#interpretability", "#open_source"], "emoji": "🦅", "ru": {"title": "Объясняем каждый токен: как MLLM видят и говорят", "desc": "EAGLE - это легковесный фреймворк для объяснения процесса генерации токенов в мультимодальных больших языко
[29.09.2025 09:14] Using data from previous issue: {"categories": ["#optimization", "#training", "#long_context", "#architecture"], "emoji": "🧠", "ru": {"title": "Расширение памяти RNN без лишних затрат", "desc": "В статье представлен StateX - метод пост-обучения для увеличения размера состояния предобученных рекуррентных нейронных сетей (RNN). Проб
[29.09.2025 09:14] Using data from previous issue: {"categories": ["#video", "#interpretability", "#reasoning", "#dataset", "#rag", "#benchmark"], "emoji": "🔍", "ru": {"title": "Объяснимый поиск видео через рассуждения LLM", "desc": "Исследователи предлагают X-CoT - новый фреймворк для поиска видео по текстовому описанию, который использует цепочки 
[29.09.2025 09:14] Using data from previous issue: {"categories": ["#cv", "#multimodal", "#science", "#dataset", "#open_source"], "emoji": "📜", "ru": {"title": "CHURRO: AI для чтения древних текстов и сохранения культурного наследия", "desc": "CHURRO - это специализированная vision-language модель с 3 миллиардами параметров, созданная для распознава
[29.09.2025 09:14] Querying the API.
[29.09.2025 09:14] Claude request. Model: claude-sonnet-4-20250514. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

DEIMv2, an extended version of DEIM with DINOv3 features, achieves superior performance-cost trade-offs across diverse deployment scenarios, including GPU, edge, and mobile, by using Spatial Tuning Adapter and HGNetv2 with pruning.  					AI-generated summary 				 Benefiting from the simplicity and effectiveness of Dense O2O and MAL, DEIM has become the mainstream training framework for real-time DETRs, significantly outperforming the YOLO series. In this work, we extend it with DINOv3 features, resulting in DEIMv2. DEIMv2 spans eight model sizes from X to Atto, covering GPU, edge, and mobile deployment. For the X, L, M, and S variants, we adopt DINOv3-pretrained or distilled backbones and introduce a Spatial Tuning Adapter (STA), which efficiently converts DINOv3's single-scale output into multi-scale features and complements strong semantics with fine-grained details to enhance detection. For ultra-lightweight models (Nano, Pico, Femto, and Atto), we employ HGNetv2 with depth and width pruning to meet strict resource budgets. Together with a simplified decoder and an upgraded Dense O2O, this unified design enables DEIMv2 to achieve a superior performance-cost trade-off across diverse scenarios, establishing new state-of-the-art results. Notably, our largest model, DEIMv2-X, achieves 57.8 AP with only 50.3 million parameters, surpassing prior X-scale models that require over 60 million parameters for just 56.5 AP. On the compact side, DEIMv2-S is the first sub-10 million model (9.71 million) to exceed the 50 AP milestone on COCO, reaching 50.9 AP. Even the ultra-lightweight DEIMv2-Pico, with just 1.5 million parameters, delivers 38.5 AP, matching YOLOv10-Nano (2.3 million) with around 50 percent fewer parameters. Our code and pre-trained models are available at https://github.com/Intellindust-AI-Lab/DEIMv2
[29.09.2025 09:14] Response: ```json
{
  "desc": "DEIMv2 представляет расширенную версию архитектуры DEIM для детекции объектов в реальном времени, использующую features от DINOv3. Модель включает восемь различных размеров от X до Atto для развертывания на GPU, edge-устройствах и мобильных платформах. Для крупных вариантов используется Spatial Tuning Adapter для преобразования одномасштабного выхода DINOv3 в многомасштабные признаки, а для ультралегких моделей применяется HGNetv2 с pruning. DEIMv2 достигает превосходного соотношения производительность-затраты, превосходя YOLO серию и устанавливая новые state-of-the-art результаты в детекции объектов.",
  "emoji": "🎯",
  "title": "Универсальная детекция объектов от GPU до мобильных устройств"
}
```
[29.09.2025 09:14] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"DEIMv2, an extended version of DEIM with DINOv3 features, achieves superior performance-cost trade-offs across diverse deployment scenarios, including GPU, edge, and mobile, by using Spatial Tuning Adapter and HGNetv2 with pruning.  					AI-generated summary 				 Benefiting from the simplicity and effectiveness of Dense O2O and MAL, DEIM has become the mainstream training framework for real-time DETRs, significantly outperforming the YOLO series. In this work, we extend it with DINOv3 features, resulting in DEIMv2. DEIMv2 spans eight model sizes from X to Atto, covering GPU, edge, and mobile deployment. For the X, L, M, and S variants, we adopt DINOv3-pretrained or distilled backbones and introduce a Spatial Tuning Adapter (STA), which efficiently converts DINOv3's single-scale output into multi-scale features and complements strong semantics with fine-grained details to enhance detection. For ultra-lightweight models (Nano, Pico, Femto, and Atto), we employ HGNetv2 with depth and width pruning to meet strict resource budgets. Together with a simplified decoder and an upgraded Dense O2O, this unified design enables DEIMv2 to achieve a superior performance-cost trade-off across diverse scenarios, establishing new state-of-the-art results. Notably, our largest model, DEIMv2-X, achieves 57.8 AP with only 50.3 million parameters, surpassing prior X-scale models that require over 60 million parameters for just 56.5 AP. On the compact side, DEIMv2-S is the first sub-10 million model (9.71 million) to exceed the 50 AP milestone on COCO, reaching 50.9 AP. Even the ultra-lightweight DEIMv2-Pico, with just 1.5 million parameters, delivers 38.5 AP, matching YOLOv10-Nano (2.3 million) with around 50 percent fewer parameters. Our code and pre-trained models are available at https://github.com/Intellindust-AI-Lab/DEIMv2"

[29.09.2025 09:14] Response: ```python
["INFERENCE", "TRAINING", "SMALL_MODELS", "CV", "ARCHITECTURE"]
```
[29.09.2025 09:14] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"DEIMv2, an extended version of DEIM with DINOv3 features, achieves superior performance-cost trade-offs across diverse deployment scenarios, including GPU, edge, and mobile, by using Spatial Tuning Adapter and HGNetv2 with pruning.  					AI-generated summary 				 Benefiting from the simplicity and effectiveness of Dense O2O and MAL, DEIM has become the mainstream training framework for real-time DETRs, significantly outperforming the YOLO series. In this work, we extend it with DINOv3 features, resulting in DEIMv2. DEIMv2 spans eight model sizes from X to Atto, covering GPU, edge, and mobile deployment. For the X, L, M, and S variants, we adopt DINOv3-pretrained or distilled backbones and introduce a Spatial Tuning Adapter (STA), which efficiently converts DINOv3's single-scale output into multi-scale features and complements strong semantics with fine-grained details to enhance detection. For ultra-lightweight models (Nano, Pico, Femto, and Atto), we employ HGNetv2 with depth and width pruning to meet strict resource budgets. Together with a simplified decoder and an upgraded Dense O2O, this unified design enables DEIMv2 to achieve a superior performance-cost trade-off across diverse scenarios, establishing new state-of-the-art results. Notably, our largest model, DEIMv2-X, achieves 57.8 AP with only 50.3 million parameters, surpassing prior X-scale models that require over 60 million parameters for just 56.5 AP. On the compact side, DEIMv2-S is the first sub-10 million model (9.71 million) to exceed the 50 AP milestone on COCO, reaching 50.9 AP. Even the ultra-lightweight DEIMv2-Pico, with just 1.5 million parameters, delivers 38.5 AP, matching YOLOv10-Nano (2.3 million) with around 50 percent fewer parameters. Our code and pre-trained models are available at https://github.com/Intellindust-AI-Lab/DEIMv2"

[29.09.2025 09:14] Response: ```python
["OPTIMIZATION", "OPEN_SOURCE"]
```
[29.09.2025 09:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"DEIMv2 is an advanced version of the DEIM framework that incorporates features from DINOv3, enhancing its performance across various platforms like GPU, edge, and mobile devices. It introduces a Spatial Tuning Adapter (STA) to transform single-scale outputs into multi-scale features, improving object detection accuracy. The model is designed in multiple sizes, from ultra-lightweight to larger variants, optimizing resource usage through techniques like pruning. Notably, DEIMv2 achieves state-of-the-art results with fewer parameters compared to previous models, making it efficient and effective for real-time applications.","title":"DEIMv2: Efficient Object Detection Across All Platforms"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='DEIMv2 is an advanced version of the DEIM framework that incorporates features from DINOv3, enhancing its performance across various platforms like GPU, edge, and mobile devices. It introduces a Spatial Tuning Adapter (STA) to transform single-scale outputs into multi-scale features, improving object detection accuracy. The model is designed in multiple sizes, from ultra-lightweight to larger variants, optimizing resource usage through techniques like pruning. Notably, DEIMv2 achieves state-of-the-art results with fewer parameters compared to previous models, making it efficient and effective for real-time applications.', title='DEIMv2: Efficient Object Detection Across All Platforms'))
[29.09.2025 09:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"DEIMv2是DEIM的扩展版本，结合了DINOv3的特性，能够在GPU、边缘和移动设备等多种部署场景中实现优越的性能与成本平衡。通过使用空间调优适配器和HGNetv2进行剪枝，DEIMv2在不同模型尺寸上表现出色，涵盖了从X到Atto的八种模型。该框架采用DINOv3预训练的骨干网络，并引入了高效的多尺度特征转换，增强了检测的语义和细节。DEIMv2在性能上超越了以往的模型，尤其是其最大的DEIMv2-X模型以仅50.3百万参数达到了57.8的AP，展示了其在资源受限情况下的强大能力。","title":"DEIMv2：性能与成本的最佳平衡"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='DEIMv2是DEIM的扩展版本，结合了DINOv3的特性，能够在GPU、边缘和移动设备等多种部署场景中实现优越的性能与成本平衡。通过使用空间调优适配器和HGNetv2进行剪枝，DEIMv2在不同模型尺寸上表现出色，涵盖了从X到Atto的八种模型。该框架采用DINOv3预训练的骨干网络，并引入了高效的多尺度特征转换，增强了检测的语义和细节。DEIMv2在性能上超越了以往的模型，尤其是其最大的DEIMv2-X模型以仅50.3百万参数达到了57.8的AP，展示了其在资源受限情况下的强大能力。', title='DEIMv2：性能与成本的最佳平衡'))
[29.09.2025 09:14] Renaming data file.
[29.09.2025 09:14] Renaming previous data. hf_papers.json to ./d/2025-09-29.json
[29.09.2025 09:14] Saving new data file.
[29.09.2025 09:14] Generating page.
[29.09.2025 09:14] Renaming previous page.
[29.09.2025 09:14] Renaming previous data. index.html to ./d/2025-09-29.html
[29.09.2025 09:14] Writing result.
[29.09.2025 09:14] Renaming log file.
[29.09.2025 09:14] Renaming previous data. log.txt to ./logs/2025-09-29_last_log.txt
