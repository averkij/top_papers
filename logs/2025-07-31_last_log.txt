[31.07.2025 15:14] Read previous papers.
[31.07.2025 15:14] Generating top page (month).
[31.07.2025 15:14] Writing top page (month).
[31.07.2025 16:16] Read previous papers.
[31.07.2025 16:16] Get feed.
[31.07.2025 16:16] Get page data from previous paper. URL: https://huggingface.co/papers/2507.22827
[31.07.2025 16:16] Get page data from previous paper. URL: https://huggingface.co/papers/2507.21493
[31.07.2025 16:16] Get page data from previous paper. URL: https://huggingface.co/papers/2507.22448
[31.07.2025 16:16] Get page data from previous paper. URL: https://huggingface.co/papers/2507.22607
[31.07.2025 16:16] Get page data from previous paper. URL: https://huggingface.co/papers/2507.20976
[31.07.2025 16:16] Get page data from previous paper. URL: https://huggingface.co/papers/2507.22886
[31.07.2025 16:16] Get page data from previous paper. URL: https://huggingface.co/papers/2507.22565
[31.07.2025 16:16] Get page data from previous paper. URL: https://huggingface.co/papers/2507.22853
[31.07.2025 16:16] Get page data from previous paper. URL: https://huggingface.co/papers/2507.21802
[31.07.2025 16:16] Extract page data from URL. URL: https://huggingface.co/papers/2507.19427
[31.07.2025 16:16] Get page data from previous paper. URL: https://huggingface.co/papers/2507.22062
[31.07.2025 16:16] Get page data from previous paper. URL: https://huggingface.co/papers/2507.13985
[31.07.2025 16:16] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[31.07.2025 16:16] No deleted papers detected.
[31.07.2025 16:16] Downloading and parsing papers (pdf, html). Total: 12.
[31.07.2025 16:16] Downloading and parsing paper https://huggingface.co/papers/2507.22827.
[31.07.2025 16:16] Extra JSON file exists (./assets/json/2507.22827.json), skip PDF parsing.
[31.07.2025 16:16] Paper image links file exists (./assets/img_data/2507.22827.json), skip HTML parsing.
[31.07.2025 16:16] Success.
[31.07.2025 16:16] Downloading and parsing paper https://huggingface.co/papers/2507.21493.
[31.07.2025 16:16] Extra JSON file exists (./assets/json/2507.21493.json), skip PDF parsing.
[31.07.2025 16:16] Paper image links file exists (./assets/img_data/2507.21493.json), skip HTML parsing.
[31.07.2025 16:16] Success.
[31.07.2025 16:16] Downloading and parsing paper https://huggingface.co/papers/2507.22448.
[31.07.2025 16:16] Extra JSON file exists (./assets/json/2507.22448.json), skip PDF parsing.
[31.07.2025 16:16] Paper image links file exists (./assets/img_data/2507.22448.json), skip HTML parsing.
[31.07.2025 16:16] Success.
[31.07.2025 16:16] Downloading and parsing paper https://huggingface.co/papers/2507.22607.
[31.07.2025 16:16] Extra JSON file exists (./assets/json/2507.22607.json), skip PDF parsing.
[31.07.2025 16:16] Paper image links file exists (./assets/img_data/2507.22607.json), skip HTML parsing.
[31.07.2025 16:16] Success.
[31.07.2025 16:16] Downloading and parsing paper https://huggingface.co/papers/2507.20976.
[31.07.2025 16:16] Extra JSON file exists (./assets/json/2507.20976.json), skip PDF parsing.
[31.07.2025 16:16] Paper image links file exists (./assets/img_data/2507.20976.json), skip HTML parsing.
[31.07.2025 16:16] Success.
[31.07.2025 16:16] Downloading and parsing paper https://huggingface.co/papers/2507.22886.
[31.07.2025 16:16] Extra JSON file exists (./assets/json/2507.22886.json), skip PDF parsing.
[31.07.2025 16:16] Paper image links file exists (./assets/img_data/2507.22886.json), skip HTML parsing.
[31.07.2025 16:16] Success.
[31.07.2025 16:16] Downloading and parsing paper https://huggingface.co/papers/2507.22565.
[31.07.2025 16:16] Extra JSON file exists (./assets/json/2507.22565.json), skip PDF parsing.
[31.07.2025 16:16] Paper image links file exists (./assets/img_data/2507.22565.json), skip HTML parsing.
[31.07.2025 16:16] Success.
[31.07.2025 16:16] Downloading and parsing paper https://huggingface.co/papers/2507.22853.
[31.07.2025 16:16] Extra JSON file exists (./assets/json/2507.22853.json), skip PDF parsing.
[31.07.2025 16:16] Paper image links file exists (./assets/img_data/2507.22853.json), skip HTML parsing.
[31.07.2025 16:16] Success.
[31.07.2025 16:16] Downloading and parsing paper https://huggingface.co/papers/2507.21802.
[31.07.2025 16:16] Extra JSON file exists (./assets/json/2507.21802.json), skip PDF parsing.
[31.07.2025 16:16] Paper image links file exists (./assets/img_data/2507.21802.json), skip HTML parsing.
[31.07.2025 16:16] Success.
[31.07.2025 16:16] Downloading and parsing paper https://huggingface.co/papers/2507.19427.
[31.07.2025 16:16] Downloading paper 2507.19427 from http://arxiv.org/pdf/2507.19427v1...
[31.07.2025 16:16] Extracting affiliations from text.
[31.07.2025 16:16] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding StepFun Inc. 5 2 0 2 5 2 ] . [ 1 7 2 4 9 1 . 7 0 5 2 : r Abstract Large language models (LLMs) face low hardware efficiency during decoding, especially for long-context reasoning tasks. This paper introduces Step-3, 321B-parameter VLM with hardware-aware model-system co-design optimized for minimizing decoding costs. Step-3 innovates in two key dimensions: (1) novel Multi-Matrix Factorization Attention (MFA) mechanism that significantly reduces both KV cache size and computation while maintaining high attention expressiveness, and (2) Attention-FFN Disaggregation (AFD), distributed inference system that decouples attention and Feed-Forward Network (FFN) layers into specialized subsystems. This co-design achieves unprecedented cost efficiency: Step-3 significantly reduces theoretical decoding costs compared with models like DeepSeek-V3 and Qwen3 MoE 235B, with the gains widening at longer context. Step-3 achieves low cost while activating 38B parameters per token (more than DeepSeek-V3 and Qwen3 MoE 235B), demonstrating that hardware-aligned attention arithmetic intensity, MoE sparsity, and AFD are critical to cost-effectiveness. We perform head-to-head comparison with DeepSeek-V3 in its favorable scenarios. Our implementation on Hopper GPUs achieves decoding throughput of up to 4,039 tokens per second per GPU under 50ms TPOT SLA (4K context, FP8, no MTP). It is higher than DeepSeek-V3s 2,324 in the same setup and sets new Pareto frontier for LLM decoding. This paper presents the model-system co-design of Step-3, specifically engineered for the test-time scaling paradigm with the primary optimization objective of minimizing decoding costs. Step-3 has 321 billion total parameters, while for each text token, 38B parameters are activated. We will demonstrate that, although Step-3 is in the multi-hundred billion parameter range and the activated parameters are slightly larger than represen"
[31.07.2025 16:16] Response: ```python
["StepFun Inc."]
```
[31.07.2025 16:16] Deleting PDF ./assets/pdf/2507.19427.pdf.
[31.07.2025 16:16] Success.
[31.07.2025 16:16] Downloading and parsing paper https://huggingface.co/papers/2507.22062.
[31.07.2025 16:16] Extra JSON file exists (./assets/json/2507.22062.json), skip PDF parsing.
[31.07.2025 16:16] Paper image links file exists (./assets/img_data/2507.22062.json), skip HTML parsing.
[31.07.2025 16:16] Success.
[31.07.2025 16:16] Downloading and parsing paper https://huggingface.co/papers/2507.13985.
[31.07.2025 16:16] Extra JSON file exists (./assets/json/2507.13985.json), skip PDF parsing.
[31.07.2025 16:16] Paper image links file exists (./assets/img_data/2507.13985.json), skip HTML parsing.
[31.07.2025 16:16] Success.
[31.07.2025 16:16] Enriching papers with extra data.
[31.07.2025 16:16] ********************************************************************************
[31.07.2025 16:16] Abstract 0. A modular multi-agent framework improves UI-to-code generation by integrating vision-language models, hierarchical layout planning, and adaptive prompt-based synthesis, achieving state-of-the-art performance.  					AI-generated summary 				 Automating the transformation of user interface (UI) design...
[31.07.2025 16:16] ********************************************************************************
[31.07.2025 16:16] Abstract 1. BANG is a generative approach that uses latent diffusion models and temporal attention to enable intuitive part-level decomposition and manipulation of 3D objects, enhancing 3D creation workflows.  					AI-generated summary 				 3D creation has always been a unique human strength, driven by our abil...
[31.07.2025 16:16] ********************************************************************************
[31.07.2025 16:16] Abstract 2. Falcon-H1, a new series of large language models with a hybrid architecture combining Transformer-based attention and State Space Models, achieves state-of-the-art performance and efficiency across various tasks and sizes.  					AI-generated summary 				 In this report, we introduce Falcon-H1, a new...
[31.07.2025 16:16] ********************************************************************************
[31.07.2025 16:16] Abstract 3. VL-Cogito, a multimodal reasoning model, uses a Progressive Curriculum Reinforcement Learning framework to improve performance across diverse tasks by dynamically adjusting difficulty and reasoning path length.  					AI-generated summary 				 Reinforcement learning has proven its effectiveness in en...
[31.07.2025 16:16] ********************************************************************************
[31.07.2025 16:16] Abstract 4. A multi-stage, multi-modal knowledge transfer framework using fine-tuned latent diffusion models improves vehicle detection in aerial imagery across different domains.  					AI-generated summary 				 Detecting vehicles in aerial imagery is a critical task with applications in traffic monitoring, urb...
[31.07.2025 16:16] ********************************************************************************
[31.07.2025 16:16] Abstract 5. Omnimodal Referring Audio-Visual Segmentation (OmniAVS) and Omnimodal Instructed Segmentation Assistant (OISA) advance audio-visual segmentation by integrating complex multimodal expressions and leveraging MLLM for reasoning.  					AI-generated summary 				 Referring audio-visual segmentation (RAVS)...
[31.07.2025 16:16] ********************************************************************************
[31.07.2025 16:16] Abstract 6. RLDP, a deep reinforcement learning framework, optimizes differentially private training by dynamically adjusting gradient clipping and noise, enhancing model utility and speed while maintaining privacy.  					AI-generated summary 				 The tension between data privacy and model utility has become th...
[31.07.2025 16:16] ********************************************************************************
[31.07.2025 16:16] Abstract 7. Repair-R1 enhances automated program repair by integrating test cases into the training phase and prioritizing test generation before repair, improving repair success, test generation success, and test coverage.  					AI-generated summary 				 APR (Automated Program Repair) aims to automatically loc...
[31.07.2025 16:16] ********************************************************************************
[31.07.2025 16:16] Abstract 8. MixGRPO, a novel framework integrating SDE and ODE, enhances flow matching models for image generation by optimizing only within a sliding window, improving efficiency and performance.  					AI-generated summary 				 Although GRPO substantially enhances flow matching models in human preference align...
[31.07.2025 16:16] ********************************************************************************
[31.07.2025 16:16] Abstract 9. Step-3, a 321B-parameter VLM, reduces decoding costs through Multi-Matrix Factorization Attention and Attention-FFN Disaggregation, achieving high efficiency and throughput on long-context tasks.  					AI-generated summary 				 Large language models (LLMs) face low hardware efficiency during decodin...
[31.07.2025 16:16] ********************************************************************************
[31.07.2025 16:16] Abstract 10. MetaCLIP 2, trained on worldwide web-scale image-text pairs, improves zero-shot classification and multilingual benchmarks without system-level confounding factors.  					AI-generated summary 				 Contrastive Language-Image Pretraining (CLIP) is a popular foundation model, supporting from zero-shot ...
[31.07.2025 16:16] ********************************************************************************
[31.07.2025 16:16] Abstract 11. DreamScene is an end-to-end framework that generates high-quality, editable 3D scenes from text or dialogue, ensuring automation, 3D consistency, and fine-grained control through a combination of scene planning, graph-based placement, formation pattern sampling, and progressive camera sampling.  			...
[31.07.2025 16:16] Read previous papers.
[31.07.2025 16:16] Generating reviews via LLM API.
[31.07.2025 16:16] Using data from previous issue: {"categories": ["#synthetic", "#open_source", "#training", "#dataset", "#interpretability", "#multimodal", "#agents"], "emoji": "🖥️", "ru": {"title": "Умное превращение дизайна в код с помощью ИИ", "desc": "Статья представляет модульную мультиагентную систему для генерации кода пользовательского инт
[31.07.2025 16:16] Using data from previous issue: {"categories": ["#diffusion", "#games", "#3d", "#reasoning", "#multimodal"], "emoji": "🧩", "ru": {"title": "BANG: Интуитивная декомпозиция 3D-объектов для творческого моделирования", "desc": "BANG - это генеративный подход, использующий латентные диффузионные модели и временное внимание для интуитив
[31.07.2025 16:16] Using data from previous issue: {"categories": ["#small_models", "#training", "#agi", "#architecture", "#science", "#long_context", "#dataset", "#open_source", "#multilingual"], "emoji": "🦅", "ru": {"title": "Falcon-H1: Гибридная мощь в мире языковых моделей", "desc": "Falcon-H1 - это новая серия больших языковых моделей с гибридн
[31.07.2025 16:16] Using data from previous issue: {"categories": ["#multimodal", "#training", "#rlhf", "#reasoning", "#benchmark", "#rl"], "emoji": "🧠", "ru": {"title": "Адаптивное обучение для улучшения мультимодальных рассуждений ИИ", "desc": "VL-Cogito - это мультимодальная модель рассуждений, использующая прогрессивное обучение с подкреплением.
[31.07.2025 16:16] Using data from previous issue: {"categories": ["#synthetic", "#transfer_learning", "#dataset", "#cv", "#data", "#multimodal"], "emoji": "🚗", "ru": {"title": "Генеративный ИИ повышает точность обнаружения автомобилей на аэрофотоснимках", "desc": "Статья представляет новый метод обнаружения транспортных средств на аэрофотоснимках с
[31.07.2025 16:16] Using data from previous issue: {"categories": ["#games", "#cv", "#reasoning", "#dataset", "#multimodal"], "emoji": "🎭", "ru": {"title": "Мультимодальная сегментация: новый уровень понимания аудио-визуального контента", "desc": "OmniAVS - это новый набор данных для сегментации аудио-визуального контента, включающий 2,098 видео и 5
[31.07.2025 16:16] Using data from previous issue: {"categories": ["#healthcare", "#training", "#security", "#rlhf", "#optimization", "#rl"], "emoji": "🛡️", "ru": {"title": "Динамическая оптимизация приватности в обучении языковых моделей", "desc": "RLDP - это фреймворк глубокого обучения с подкреплением, который оптимизирует дифференциально приватн
[31.07.2025 16:16] Using data from previous issue: {"categories": ["#benchmark", "#open_source", "#training", "#dataset", "#optimization", "#rl"], "emoji": "🛠️", "ru": {"title": "Repair-R1: Революция в автоматическом исправлении программ через интеграцию тестирования", "desc": "Repair-R1 - это новый подход к автоматическому исправлению программ (APR
[31.07.2025 16:16] Using data from previous issue: {"categories": ["#alignment", "#optimization", "#training", "#open_source", "#rlhf"], "emoji": "🖼️", "ru": {"title": "Эффективная оптимизация генерации изображений с помощью гибридного подхода SDE-ODE", "desc": "MixGRPO - это новая фреймворк, объединяющий стохастические дифференциальные уравнения (S
[31.07.2025 16:16] Querying the API.
[31.07.2025 16:16] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Step-3, a 321B-parameter VLM, reduces decoding costs through Multi-Matrix Factorization Attention and Attention-FFN Disaggregation, achieving high efficiency and throughput on long-context tasks.  					AI-generated summary 				 Large language models (LLMs) face low hardware efficiency during decoding, especially for long-context reasoning tasks. This paper introduces Step-3, a 321B-parameter VLM with hardware-aware model-system co-design optimized for minimizing decoding costs. Step-3 innovates in two key dimensions: (1) A novel Multi-Matrix Factorization Attention (MFA) mechanism that significantly reduces both KV cache size and computation while maintaining high attention expressiveness, and (2) Attention-FFN Disaggregation (AFD), a distributed inference system that decouples attention and Feed-Forward Network (FFN) layers into specialized subsystems. This co-design achieves unprecedented cost efficiency: Step-3 significantly reduces theoretical decoding costs compared with models like DeepSeek-V3 and Qwen3 MoE 235B, with the gains widening at longer context. Step-3 achieves low cost while activating 38B parameters per token (more than DeepSeek-V3 and Qwen3 MoE 235B), demonstrating that hardware-aligned attention arithmetic intensity, MoE sparsity, and AFD are critical to cost-effectiveness. We perform a head-to-head comparison with DeepSeek-V3 in its favorable scenarios. Our implementation on Hopper GPUs achieves a decoding throughput of up to 4,039 tokens per second per GPU under 50ms TPOT SLA (4K context, FP8, no MTP). It is higher than DeepSeek-V3's 2,324 in the same setup and sets a new Pareto frontier for LLM decoding.
[31.07.2025 16:16] Response: {
  "desc": "Статья представляет Step-3, визуально-языковую модель с 321 миллиардами параметров, оптимизированную для минимизации затрат на декодирование. Модель использует новый механизм внимания с многоматричной факторизацией (MFA), который значительно уменьшает размер KV-кэша и объем вычислений. Step-3 также применяет разделение слоев внимания и Feed-Forward Network (AFD) для повышения эффективности распределенного вывода. Эти инновации позволяют Step-3 достичь беспрецедентной эффективности затрат и высокой пропускной способности при работе с длинными контекстами.",
  "emoji": "🚀",
  "title": "Step-3: Революция в эффективности декодирования больших языковых моделей"
}
[31.07.2025 16:16] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Step-3, a 321B-parameter VLM, reduces decoding costs through Multi-Matrix Factorization Attention and Attention-FFN Disaggregation, achieving high efficiency and throughput on long-context tasks.  					AI-generated summary 				 Large language models (LLMs) face low hardware efficiency during decoding, especially for long-context reasoning tasks. This paper introduces Step-3, a 321B-parameter VLM with hardware-aware model-system co-design optimized for minimizing decoding costs. Step-3 innovates in two key dimensions: (1) A novel Multi-Matrix Factorization Attention (MFA) mechanism that significantly reduces both KV cache size and computation while maintaining high attention expressiveness, and (2) Attention-FFN Disaggregation (AFD), a distributed inference system that decouples attention and Feed-Forward Network (FFN) layers into specialized subsystems. This co-design achieves unprecedented cost efficiency: Step-3 significantly reduces theoretical decoding costs compared with models like DeepSeek-V3 and Qwen3 MoE 235B, with the gains widening at longer context. Step-3 achieves low cost while activating 38B parameters per token (more than DeepSeek-V3 and Qwen3 MoE 235B), demonstrating that hardware-aligned attention arithmetic intensity, MoE sparsity, and AFD are critical to cost-effectiveness. We perform a head-to-head comparison with DeepSeek-V3 in its favorable scenarios. Our implementation on Hopper GPUs achieves a decoding throughput of up to 4,039 tokens per second per GPU under 50ms TPOT SLA (4K context, FP8, no MTP). It is higher than DeepSeek-V3's 2,324 in the same setup and sets a new Pareto frontier for LLM decoding."

[31.07.2025 16:16] Response: ```python
['ARCHITECTURE', 'INFERENCE', 'TRAINING']
```
[31.07.2025 16:16] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Step-3, a 321B-parameter VLM, reduces decoding costs through Multi-Matrix Factorization Attention and Attention-FFN Disaggregation, achieving high efficiency and throughput on long-context tasks.  					AI-generated summary 				 Large language models (LLMs) face low hardware efficiency during decoding, especially for long-context reasoning tasks. This paper introduces Step-3, a 321B-parameter VLM with hardware-aware model-system co-design optimized for minimizing decoding costs. Step-3 innovates in two key dimensions: (1) A novel Multi-Matrix Factorization Attention (MFA) mechanism that significantly reduces both KV cache size and computation while maintaining high attention expressiveness, and (2) Attention-FFN Disaggregation (AFD), a distributed inference system that decouples attention and Feed-Forward Network (FFN) layers into specialized subsystems. This co-design achieves unprecedented cost efficiency: Step-3 significantly reduces theoretical decoding costs compared with models like DeepSeek-V3 and Qwen3 MoE 235B, with the gains widening at longer context. Step-3 achieves low cost while activating 38B parameters per token (more than DeepSeek-V3 and Qwen3 MoE 235B), demonstrating that hardware-aligned attention arithmetic intensity, MoE sparsity, and AFD are critical to cost-effectiveness. We perform a head-to-head comparison with DeepSeek-V3 in its favorable scenarios. Our implementation on Hopper GPUs achieves a decoding throughput of up to 4,039 tokens per second per GPU under 50ms TPOT SLA (4K context, FP8, no MTP). It is higher than DeepSeek-V3's 2,324 in the same setup and sets a new Pareto frontier for LLM decoding."

[31.07.2025 16:16] Response: ```python
["LONG_CONTEXT", "OPTIMIZATION"]
```
[31.07.2025 16:16] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents Step-3, a large 321B-parameter vision-language model (VLM) that enhances decoding efficiency for long-context tasks. It introduces two innovative techniques: Multi-Matrix Factorization Attention (MFA), which reduces the size of the key-value cache and computation while preserving attention quality, and Attention-FFN Disaggregation (AFD), which separates attention and feed-forward network layers into specialized components. These advancements lead to significant reductions in decoding costs compared to existing models like DeepSeek-V3 and Qwen3 MoE 235B, especially as context length increases. The implementation achieves a remarkable decoding throughput of 4,039 tokens per second per GPU, setting a new standard for efficiency in large language models.","title":"Step-3: Revolutionizing Decoding Efficiency in Large Language Models"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper presents Step-3, a large 321B-parameter vision-language model (VLM) that enhances decoding efficiency for long-context tasks. It introduces two innovative techniques: Multi-Matrix Factorization Attention (MFA), which reduces the size of the key-value cache and computation while preserving attention quality, and Attention-FFN Disaggregation (AFD), which separates attention and feed-forward network layers into specialized components. These advancements lead to significant reductions in decoding costs compared to existing models like DeepSeek-V3 and Qwen3 MoE 235B, especially as context length increases. The implementation achieves a remarkable decoding throughput of 4,039 tokens per second per GPU, setting a new standard for efficiency in large language models.', title='Step-3: Revolutionizing Decoding Efficiency in Large Language Models'))
[31.07.2025 16:16] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本文介绍了一种名为Step-3的321B参数的视觉语言模型（VLM），旨在通过多矩阵因子化注意力和注意力-前馈网络分解来降低解码成本。Step-3采用硬件感知的模型-系统协同设计，优化了解码效率，特别是在长上下文推理任务中表现出色。其创新之处在于引入了多矩阵因子化注意力机制，显著减少了KV缓存大小和计算量，同时保持了高效的注意力表达能力。通过分布式推理系统，Step-3实现了前所未有的成本效率，解码吞吐量达到每秒4039个标记，超越了现有模型。","title":"Step-3：高效解码的新纪元"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本文介绍了一种名为Step-3的321B参数的视觉语言模型（VLM），旨在通过多矩阵因子化注意力和注意力-前馈网络分解来降低解码成本。Step-3采用硬件感知的模型-系统协同设计，优化了解码效率，特别是在长上下文推理任务中表现出色。其创新之处在于引入了多矩阵因子化注意力机制，显著减少了KV缓存大小和计算量，同时保持了高效的注意力表达能力。通过分布式推理系统，Step-3实现了前所未有的成本效率，解码吞吐量达到每秒4039个标记，超越了现有模型。', title='Step-3：高效解码的新纪元'))
[31.07.2025 16:16] Using data from previous issue: {"categories": ["#dataset", "#multilingual", "#low_resource", "#cv", "#machine_translation", "#multimodal"], "emoji": "🌐", "ru": {"title": "MetaCLIP 2: Прорыв в мультиязычной классификации", "desc": "В статье представлена новая модель MetaCLIP 2, которая обучена на данных из интернета, включающих из
[31.07.2025 16:16] Using data from previous issue: {"categories": ["#3d", "#games", "#open_source"], "emoji": "🎨", "ru": {"title": "DreamScene: От текста к реалистичным 3D-мирам", "desc": "DreamScene - это комплексная система для генерации качественных и редактируемых 3D-сцен на основе текста или диалога. Она использует модуль планирования сцены с G
[31.07.2025 16:16] Renaming data file.
[31.07.2025 16:16] Renaming previous data. hf_papers.json to ./d/2025-07-31.json
[31.07.2025 16:16] Saving new data file.
[31.07.2025 16:16] Generating page.
[31.07.2025 16:16] Renaming previous page.
[31.07.2025 16:16] Renaming previous data. index.html to ./d/2025-07-31.html
[31.07.2025 16:16] Writing result.
[31.07.2025 16:16] Renaming log file.
[31.07.2025 16:16] Renaming previous data. log.txt to ./logs/2025-07-31_last_log.txt
